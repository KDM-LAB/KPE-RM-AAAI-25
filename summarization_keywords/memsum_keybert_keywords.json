{"9c61f5f51a232049635e6f3441e6397af4d91298": {"ta_keywords": "introduction learning games;learning games;games;introduction learning;learning;introduction", "pdf_keywords": ""}, "a05c3e8bd6dacbd192ffa28543e60e2c93c66d76": {"ta_keywords": "analysing trends twitter;trends twitter;trends twitter popular;information trend analyzer;topics named trending;trending topics;trend analysis used;trend analyzer;tweet retrieval analysing;analysing trends;trending topics shows;named trending topics;trend analyzer work;trend analysis;graph based tweets;trend;trending;share information trend;tweet retrieval;popularity used hashtags;named trending;era tweet retrieval;twitter popular social;information trend;retrieval analysing trends;trends;twitter popular;hashtags data science;popular social media;technology era tweet", "pdf_keywords": ""}, "bfa10ea6a4c9fa585f21f39858da517c31a76343": {"ta_keywords": "persuasive dialogue systems;conversation persuasive dialogue;propose probabilistic dialogue;probabilistic dialogue modeling;method persuasive dialogue;dialogue management;develop dialogue model;persuasive dialogue;dialogue modeling;dialogue modeling method;probabilistic dialogue;dialogue manager task;dialogue model;dialogue systems;needs dialogue management;conversation persuasive;dialogue manager;dialogue systems interact;dialogue management leading;leading conversation persuasive;user needs dialogue;knowledge dialogue manager;dialogue systems evaluate;evaluate knowledge dialogue;develop dialogue;knowledge dialogue;dialogue model assuming;modeling method persuasive;dialogue;needs dialogue", "pdf_keywords": ""}, "0bf2a0a3216c79b62b3664c596f44d7a8add498a": {"ta_keywords": "fiction reviews specifically;specifically science fiction;reviews specifically science;fiction reviews;research literature students;tv fiction reviews;research reviews;science fiction used;undergraduate computer science;computer science curriculum;undergraduate computer;navigate research literature;science fiction;reviews specifically;literature students;literature students learn;research topic stirs;research field learn;students exposed research;fiction used gateway;computer science;critically compare technical;chosen science fiction;gateway research reviews;specifically science;fiction used;learn navigate research;fiction;tv fiction;navigate research", "pdf_keywords": ""}, "a4cb2a401c78bfafee69e823306b0cc9e4d673db": {"ta_keywords": "reviewer assignment instance;fair allocation;fair allocation problem;efficient ef1 allocations;instance fair allocation;reviewer round robin;assignment methods fairness;reviewer assignment;paper assignment methods;model reviewer assignment;item ef1 allocations;fairly distributed papers;allocations real conference;assignment instance fair;ef1 allocations;allocations;mechanism called reviewer;round robin mechanisms;distributed papers;allocation;attainable round robin;ef1 allocations real;round robin mechanism;order papers reviewed;fairness efficiency;distributed papers approach;fairness efficiency runtime;round robin;equally important reviewer;papers reviewed", "pdf_keywords": "reviewer assignment instance;reviewer assignment setting;item reviewer assignment;reviewer assignment;fair allocation envy;optimizing ordering papers;keywords fair allocation;fair allocation;fair allocation problem;reviewer assignment introduction;reviewer round robin;model reviewer assignment;allocation envy;paper assignment methods;allocation envy free;assignment methods fairness;usw reviewer assignment;mechanism called reviewer;instance fair allocation;peer review;ef1 allocations;free item reviewer;allocations real conference;peer review plays;e\ufb03cient ef1 allocations;mechanism optimizing ordering;item reviewer;assignment instance fair;optimizing ordering;\ufb01nds ef1 allocations"}, "242c73ea34833910ad2643ec3a1096bb18c6d04d": {"ta_keywords": "target speaker extraction;speaker extraction neural;target speech extraction;transducer target speaker;improving rnn transducer;transducer rnn improving;speech enhancement uncertainty;rnn transducer target;speech extraction module;speaker extraction;speaker speech extraction;neural network transducer;rnn transducer;transducer rnn;speech extraction model;target speaker speech;speech extraction recurrent;network transducer rnn;tuned target speech;identity speech enhancement;domain target speaker;speech extraction;artifacts target speech;extraction neural uncertainty;speech enhancement;target speaker;neural uncertainty estimation;transducer target;target speech;neural uncertainty module", "pdf_keywords": "speaker extraction neural;target speech extraction;target speaker extraction;rnn target speech;target speaker speech;transducer target speaker;speaker speech extraction;tuned target speech;speaker speech recognition;speaker extraction;extraction module speaker;speech extraction module;speech extraction recurrent;recognize target speaker;rnn transducer target;improving rnn transducer;abstract target speaker;speech extraction model;speech extraction;neural network transducer;target speech;speech recognition;network transducer rnn;noise improving rnn;target speaker;rnn transducer;transducer rnn;neural uncertainty estimation;speech noisy environments;extraction neural uncertainty"}, "a4df5ff749d823905ff9c1a23b522d3f426a1bb6": {"ta_keywords": "personalized pagerank similarity;pagerank similarity;pagerank similarity vectors;personalized pagerank measure;personalized pagerank;graph walks inference;personalized pagerank widely;fields personalized pagerank;pagerank measure roughly;pagerank;pagerank measure;using personalized pagerank;similarity measures nodes;pagerank widely;pagerank widely used;tree structured markov;based graph walks;walks inference markov;similarity measure graph;set personalized pagerank;walks inference;probabilities tree structured;graph walks;structured markov random;similarity measures;mrfs computation similarity;similarity vertices graph;connection inference tree;similarity measure;marginal probabilities tree", "pdf_keywords": ""}, "f32108602fb0dbda29030cac780165a4b89048a3": {"ta_keywords": "comparison relation prediction;predicting comparison relations;relation prediction leveraging;relation prediction text;phrasing knowledge comparison;predict comparison relations;relation prediction;web predict comparison;prediction text sql;comparison relations text;knowledge comparison relation;knowledge comparison;relations text sql;predicting comparison;prediction leveraging adjective;comparison relations;comparison relations limited;noun phrasing knowledge;predict comparison;phrasing knowledge;phrasing knowledge mined;relations text;comparison relation;prediction text;web predict;knowledge mined web;adjective noun phrasing;leveraging adjective noun;prediction leveraging;knowledge existing models", "pdf_keywords": ""}, "04745fe1306d10c915d27a454c157c837dacefce": {"ta_keywords": "independent speaker separation;speaker separation matching;speaker separation;speech enhancement source;based speech enhancement;speech enhancement;enhancement source separation;channel speaker independent;representations source separation;source separation;source separation systems;source separation evaluate;speaker independent;speaker independent speaker;mixture signal phasebook;learning based speech;independent speaker;channel speaker;single channel speaker;corpus single channel;frequency representation mixture;based speech;phasebook;mixture target deep;signal phasebook;phase reconstruction;phase difference mixture;separation matching performance;separation matching;additional phase reconstruction", "pdf_keywords": ""}, "20140fcf0bdd932c1886ff1c7674c23649b1e3b8": {"ta_keywords": "speech synthesis generation;based speech synthesis;speech synthesis;speech synthesis based;improvements synthetic speech;synthetic speech;parameter generation rich;synthetic speech limitations;hmm based speech;improvements hmm based;improve parameter generation;traditional parameter generation;components hmm based;based parameter generation;rich context models;iterative parameter generation;parameter generation methods;synthesis based parameter;parameter generation;component improvements hmm;better generated parameters;improvements hmm;parameter generation method;parameters parameter generation;synthesis generation;synthesis generation method;uses parameters generated;generated parameters;components hmm;generation method rich", "pdf_keywords": ""}, "e32177e38060637ac8a2ebc9990d43d1ab8bdb8a": {"ta_keywords": "community based recommendations;latent similarities community;similarities community based;recommendation systems relatively;recommendation systems;recommendation systems work;social networks information;homophily social networks;similarities community;recommendations solution;social networks;dimensions social networks;based recommendations solution;help recommendation systems;inferences recommend items;use social networks;recommend items users;networks use social;based latent similarities;social networks use;similarities users study;similarities users;latent similarities;networks capture similarities;use communities;social networks capture;problem similarities users;communities;recommend items;based recommendations", "pdf_keywords": ""}, "6680b1e863c394f00307cb3818f7c7d75c9919aa": {"ta_keywords": "codes non multicast;designing network codes;consider distributed storage;distributed storage;distributed storage problem;network codes;multicast;minimizing repair bandwidth;repair bandwidth approach;non multicast;network codes non;distributed;stored network nodes;nodes data collector;network nodes data;multicast setting;non multicast setting;multicast setting example;network nodes;consider distributed;data stored network;repair bandwidth;data connecting nodes;stored network;data collector;bandwidth approach;interference alignment;node replicate;node replicate data;node minimizing repair", "pdf_keywords": ""}, "c096ec97ecc4f8325f6db7f32398445d6a39f959": {"ta_keywords": "fairness aware recommendation;group fairness aware;fairness aware applications;fairness metrics;complexities fairness aware;dynamically rebalancing fairness;group fairness;fairness systems;multiple fairness metrics;supporting multiple fairness;multi group fairness;fairness aware;rebalancing fairness concerns;rebalancing fairness;fairness metrics argue;fairness systems exhibit;fairness concerns;multiple fairness;properties fairness systems;complexities fairness;accuracy fairness;notions fairness;world complexities fairness;fairness assumptions;fairness multiply defined;trade accuracy fairness;notions fairness assumptions;fairness multiply;fairness assumptions recognize;fairness", "pdf_keywords": "recommendation fairness;recommendation fairness using;fairness recommendation fairness;algorithmic fairness recommendation;adaptation recommendation fairness;recommendation fairness particular;choice ranking fairness;fairness recommendation;ranking fairness;ranking fairness scruf;mechanisms choosing fairness;algorithmic fairness;conceptualize algorithmic fairness;social choice ranking;choosing fairness;lottery based mechanisms;fairness using social;recommendation problem arbitrating;social choice framework;choice ranking;choosing fairness concerns;recommender systems social;fairness using;framework formulate lottery;arbitrating preferences;fairness scruf;computing recommendation;fairness scruf conclusion;social choice arbitrate;arbitrate different ranking"}, "27636090a87fab750fccff4c6ede161ab62bcab4": {"ta_keywords": "vehicular ad hoc;hoc networks vanet;network vehicular;networks vanet;ad hoc networks;vanet challenging research;networks vanet challenging;belonging network vehicular;network vehicular ad;vehicle belonging network;vanet challenging;vanet;hoc networks;increasing network visibility;beacons inserting neighbor;mobile ad hoc;network visibility;safety messages beacons;vehicles reaching junction;vehicular;messages beacons;hoc networks decrease;vehicular ad;network visibility taking;visibility taking information;junctions vehicle prior;beacons;road junctions vehicle;neighbor table;vehicles reaching", "pdf_keywords": "vehicular ad hoc;beacons vehicular;gathered beacons vehicular;beacons vehicular ad;hoc networks vanet;ad hoc networks;beacon piggybacking neighbor;vanet challenging research;repetition beacon piggybacking;networks vanet challenging;networks vanet;coded repetition beacon;visibility gained beacon;mobile ad hoc;repetition beacon;vanet challenging;beacon piggybacking;hoc networks realized;vanet;increasing visibility network;beacon order improve;hoc networks;information gathered beacons;beacon piggybacking ghassan;beacon;visibility network;visibility network using;increasing network visibility;beacon order;beacons"}, "c3fc0b1041dcdd5b47ffaa0d584e40aa841628bf": {"ta_keywords": "documents set expansion;set expansion implemented;wrappers set expansion;language expands entities;expands entities;set expansion;set expansion refers;does set expansion;expander language;expands entities automatically;structured documents set;learn binary relational;set expansion paper;structured documents;expansion seal set;character level wrappers;semi structured documents;binary relational concepts;binary relational;set expansion seal;language expands;set expander language;expansion implemented;seal set expander;partial set seed;relational concepts mayor;expander language expands;expansion seal;expands;seal learn binary", "pdf_keywords": ""}, "7f79ac114d30c2c7dae91075210fbfda90c9d76f": {"ta_keywords": "adversarial games notably;games popular diplomacy;adversarial games;diplomacy bots;press diplomacy bots;diplomacy bots unexploitable;successes adversarial games;games involving cooperation;diplomacy equilibrium search;search prior ai;regret minimization techniques;ai successes adversarial;external regret minimization;press diplomacy equilibrium;regret minimization external;purely adversarial;diplomacy combines supervised;press variant diplomacy;adversarial purely;search external regret;variant diplomacy;regret minimization;anonymous games;adversarial purely cooperative;human players;cooperation agent greatly;minimization external regret;press diplomacy;cooperation agent;purely adversarial purely", "pdf_keywords": "dataset diplomacy games;policies trained imitation;diplomacy games;game press diplomacy;imitation learning regret;press diplomacy bots;games popular diplomacy;diplomacy bots;diplomacy games anecdotal;imitation learning;play dataset diplomacy;trained imitation learning;diplomacy bots unexploitable;regret minimization agent;learning regret minimization;world championship diplomacy;dataset diplomacy;press variant diplomacy;trained imitation;diplomacy longstanding benchmark;neural policies trained;diplomacy combines supervised;search regret minimization;games expert human;policies trained;regret minimization search;variant diplomacy;press diplomacy;diplomacy paper agent;neural policies"}, "70dc18bb6607e408ec1cd3f71c0fdac3534c288d": {"ta_keywords": "lstm speech enhancement;speech enhancement lstm;enhancement lstm recurrent;enhancement lstm;rnn based speech;speech enhancement;based speech enhancement;lstm rnns;objective speech enhancement;speech recognition asr;memory lstm rnns;speech enhancement used;lstm speech;robust automatic speech;noise robust asr;lstm rnns discriminatively;demonstrate lstm speech;asr demonstrate lstm;chime speech recognition;rnns;optimal speech reconstruction;rnns discriminatively;speech recognition;developments recurrent neural;rnns discriminatively trained;neural network rnn;speech enhancement light;speech reconstruction;lstm;lstm recurrent neural", "pdf_keywords": ""}, "6e07fb796c75cac6432cdf0c314b933d0f9f45e5": {"ta_keywords": "recognition gene names;biomedical text mining;related gene names;gene names;gene entity related;names actual genes;gene names actual;mining recognition gene;actual gene entity;gene entity;entity related gene;know gene refers;gene refers;refers actual gene;gene refers actual;task biomedical text;recognition gene;ambiguity way names;biomedical text;text mining;know gene;important know gene;gene;related gene;text mining recognition;genes common task;actual gene;common task biomedical;task biomedical;actual genes", "pdf_keywords": ""}, "24fcdaf969089e6a411f7cebc9274bbc53c25e42": {"ta_keywords": "counterfactually augmented datasets;counterfactually augmented data;generating counterfactually augmented;efficacy counterfactually augmented;counterfactually augmented;models trained augmented;generating counterfactually;augmented datasets interestingly;augmented data models;explaining efficacy counterfactually;causal thinking casting;draws causal thinking;augmented datasets;trained augmented;efficacy counterfactually;edits interventions;causal thinking;trained augmented original;causal features make;underlying causal model;casting edits interventions;underlying causal;counterfactually;outcomes underlying causal;causal features;noise causal features;learning models reliant;non causal features;edits interventions relying;explain efficacy counterfactually", "pdf_keywords": "noise causal features;causal features make;causal features lead;causal features degrade;causal features;adding noise causal;non causal features;causal spans models;domain generalization reliance;feature attribution;noise causal;causal models;better domain generalization;generalization reliance spurious;causal models measurement;noise non causal;causal spans;feature attribution technique;relationships causal models;noise domain generalization;generated style transfer;technique identi\ufb01ed causal;domain performance noise;domain generalization;identi\ufb01ed causal spans;determining feature attribution;spans models trained;style transfer methods;domain generalization cases;causal"}, "1d5c07e7415a7e9be078717197ddf9f3c70a2875": {"ta_keywords": "bert pretrained clinical;sensitive information bert;information bert trained;information bert;health records ehr;train bert model;ehr train bert;bert model;pretrained models clinicalbert;bert pretrained;ehr researchers;bert model similar;pretrained clinical notes;deidentified ehr researchers;does bert pretrained;ehr researchers access;electronic health records;bert trained mimic;predictive clinical tasks;trained bert;health records;phi trained bert;personal health information;records ehr;records ehr afforded;bert trained;notes electronic health;clinical notes reveal;corpus ehr;tasks does bert", "pdf_keywords": "sensitive information bert;information bert trained;information bert model;information bert;language models bert;models bert;bert model weights;bert model;extract sensitive information;bert trained mimic;trained bert work;reveal sensitive information;notes ehr reveal;ehr reveal sensitive;sensitive patient information;models patient ehr;corpus ehr work;masked language models;trained bert;ehr data propose;bert pretrained standard;health records ehr;sensitive information;ehr data;bert work;bert pretrained;corpus ehr;masked language modeling;bert;personal health information"}, "ca9047c78d48b606c4e4f0c456b1dda550de28b2": {"ta_keywords": "recurrent neural networks;networks rnns temporal;rnns temporal convolutions;networks rnns;neural networks rnns;deep learning models;recurrent neural;deep learning;rnns temporal;simple deep neural;deep neural;rnns;temporal convolutions neural;deep neural network;learning models time;continuous time memorization;sequences lssl outperforms;memory linear state;layers simple deep;memory linear;temporal convolutions;regression tasks speech;convolutions neural differential;long range memory;time memorization;families deep learning;sequential image classification;range memory linear;neural networks;time memorization introduce", "pdf_keywords": "continuous time memorization;rnns temporal convolutions;long term memory;recurrent neural networks;networks rnns temporal;recurrent convolutional continuous;temporal convolutions neural;deep learning models;learning models time;recurrent convolutional;combining recurrent convolutional;temporal convolutions;rnns temporal;time memorization;recurrent neural;deep learning;neural networks rnns;networks rnns;time memorization empirically;abstract recurrent neural;continuous time models;convolutional continuous time;time memorization introduce;long range memory;memorization introduce trainable;combining recurrent;memorization;convolutions neural;speed computing krylov;time models"}, "b2a090506264bc9706dc9bcc5d61b4965ae919e7": {"ta_keywords": "knowledge graph;included knowledge graph;knowledge graph knowledge;uncertain extractions entities;knowledge graph identification;extractions entities relations;knowledge graph proposed;probabilistic soft logic;extractions 70k ontological;transformed knowledge graph;extractions entities;extraction graph refer;linked data corpus;graph knowledge graph;uncertain extractions;facts included knowledge;synthetic linked data;extraction graph;soft logic;facts useful knowledge;70k ontological relations;linked data;data corpus;graph knowledge;collections interrelated facts;uses probabilistic soft;interrelated facts unfortunately;70k ontological;probabilistic soft;set extractions", "pdf_keywords": ""}, "8ca5a1e6cec68ef515ac1eb28d069a23dc9c14df": {"ta_keywords": "global graph clustering;graph clustering;sense frame induction;graph clustering applications;semantic frame induction;datasets watset local;semantic frame;linguistics journal watset;watset local global;watset coli synsets;datasets watset;analysis watset coli;runtime analysis watset;local global graph;watset local;analysis watset;sense frame;semantic class induction;watset coli classes;watset coli triframes;watset coli;global graph;clustering applications sense;zip semantic frame;article watset local;computational linguistics;clustering;coli synsets;journal watset coli;issues watset coli", "pdf_keywords": ""}, "a75c2d26ca6a06cbee62a8d1dad5993356d96793": {"ta_keywords": "seed entities given;ranking algorithms used;various ranking algorithms;seed entities;ranking algorithms;ranking method small;using seed entities;performance various ranking;ranking method;various ranking;ranking;entities using web;entities automatically utilizing;expands entities automatically;expands entities;expansion named entities;entities given larger;performance using seed;provide seeds bootstrapping;entities automatically;language expands entities;performance seeds user;iseal choice ranking;seeds seal expansion;seeds bootstrapping;entities given;performance seeds;larger set seeds;set expansion performance;seeds seal", "pdf_keywords": ""}, "a2ce385fc8d5068e8c87ebe4699c8f9b295cad5e": {"ta_keywords": "entity recognition languages;adapting word embeddings;phonological subword representations;named entity recognition;nlp resource rich;corpora bilingual dictionaries;morphemes graphemes adapting;new languages morphological;word embeddings new;embeddings new languages;language processing nlp;entity recognition;exploiting subwords transfer;bilingual dictionaries provides;graphemes adapting word;subwords transfer learning;word embeddings;subword representations;processing nlp resource;resourced languages adapting;nlp resource;subword representations work;bilingual dictionaries;resourced languages challenging;challenging exploiting subwords;languages adapting;languages morphological;languages morphological phonological;parallel corpora bilingual;morphological phonological subword", "pdf_keywords": "adapting word embeddings;phonological subword representations;entity recognition languages;named entity recognition;word embeddings new;embeddings new languages;new languages morphological;lowresourced languages adapting;especially phonological morphological;phonemes morphemes graphemes;word embeddings;leveraging subword information;generalization lowresourced languages;phonological morphological representations;phonemes morphemes;entity recognition;classifying named entities;resourced languages challenging;subword representations;languages adapting;learning leveraging subword;languages morphological phonological;leveraging subword;units phonemes morphemes;morphological phonological subword;languages morphological;word representations;phonological subword;linguistically motivated subword;nlp resource rich"}, "a0e92f6e9564b8c38b6649ae71b892ddfb988faa": {"ta_keywords": "parser exemplar retrieval;semantic parser exemplar;exemplars retrieval;related exemplars retrieval;parser exemplar;controllable semantic parser;exemplar retrieval;semantic parser;exemplars retrieval index;semantic parsing want;semantic parsing;exemplar retrieval casper;query parser retrieves;query parser;parser retrieves related;model manipulating retrieval;retrieves related exemplars;generative seq2seq model;parser retrieves;training preferable exemplars;generative seq2seq;parsing want rapidly;applications semantic parsing;input query parser;preferable exemplars;propose controllable semantic;manipulating retrieval;exemplars;controllable semantic;related exemplars", "pdf_keywords": "augmented semantic parser;semantic parser exemplar;parser exemplar retrieval;retrieval augmented semantic;casper parse queries;domain bootstrapping parse;semantic parser;exemplar retrieval casper;semantic parser uses;semantic parsing task;controllable semantic parser;semantic parsing;retrieval casper domain;augmented semantic;adapt new semantic;retrieval casper additionally;new semantic schemas;bootstrapping parse guiding;presented casper retrieval;casper parse examples;new semantic schema;parse guiding schema;new domain retrieval;domain retrieval;casper retrieval augmented;retrieval casper;semantic schema semantic;retrieval index casper;bootstrapping parse;introduction semantic parsing"}, "a30d7a3aa5e50d0b7abb448b6692e419b84018b8": {"ta_keywords": "attention based sequence;prefix boosting sequence;accurate prefix boosting;prefix boosting;boosting sequence sequence;sequence sequence asr;sequence asr;boosting sequence;sequence seq2seq asr;prefix boosting papb;boosting papb discriminative;papb discriminative training;sequence sequence seq2seq;promising accurate prefix;seq2seq asr training;sequence seq2seq;sequence sequence;based sequence sequence;attention based;discriminative training;training technique attention;sequence;discriminative training technique;sequence obtained beam;technique attention based;attention;prefix;asr training procedure;seq2seq asr;correct sequence", "pdf_keywords": "risk mbr softmax;performance mbr softmax;mbr softmax;mbr softmax margin;compared mbr objective;bayes risk mbr;mbr objectives consistent;mbr objectives;ce mbr objectives;mbr objective;risk mbr;better model predictions;comparison papb mbr;margin loss objectives;mbr objective method;compared ce mbr;training objectives better;margin comparison papb;softmax margin loss;performance papb justify;loss objectives comparable;nde pre\ufb01x training;softmax margin comparison;training ntr decoding;papb mbr;compared mbr;set compared mbr;predict character work;performance mbr;training objectives"}, "a41c81e5c3f86e18217069b94b44ceaf281e451c": {"ta_keywords": "impromptu deployment wireless;deployment wireless sensor;cycling wireless sensor;placement location backtracking;forwarding protocols sleep;location backtracking;cycling wireless;locations deploys relays;sensor networks;deployment wireless network;wireless sensor networks;line deployment wireless;deployment wireless;deploys relays locations;wake cycling wireless;heuristic placement policies;locations connect sensor;sensor networks paper;sensor networks provide;outage deployment objective;path outage;path outage deployment;relays locations;simple heuristic placement;protocols sleep;impromptu deployment;inclusion path outage;deploys relays;protocols sleep wake;heuristic placement", "pdf_keywords": "deployment wireless relay;approaches deployment wireless;locations deploying relay;forwarding protocols sleep;placement location backtracking;wireless relay networks;deployment wireless network;location backtracking;deployment wireless;cycling wireless;relay networks assuming;cycling wireless sensor;relay networks;outage deployment objective;placement locations deploying;deploying relay;wireless sensor networks;heuristic placement policies;sensor networks;wireless relay;wake cycling wireless;protocols sleep;known forwarding protocols;path outage deployment;simple heuristic placement;forwarding protocols;locations deploying;relay;sensor networks present;deploying relay provide"}, "f20654f481843ec9eb11bcd00e418aec2470dfa5": {"ta_keywords": "distributed storage codes;constructing distributed storage;efficient distributed storage;distributed storage;distributed storage systems;fault tolerance storage;storage codes efficient;erasure codes storage;codes storage efficient;deployed distributed storage;codes storage;tolerance storage efficient;storage efficient manner;piggybacking constructing distributed;storage codes;storage efficient result;storage efficient;storage systems;tolerance storage;storage codes paper;storage systems instead;erasure codes extensively;replication;efficient distributed;download efficient distributed;storage;traditional erasure codes;locality rebuilding framework;constructing distributed;replication achieve", "pdf_keywords": "distributed storage codes;storage codes ef\ufb01cient;ef\ufb01cient distributed storage;codes smallest repairlocality;storage codes;distributed storage;designing distributed storage;repair parity nodes;ef\ufb01cient repair parity;node repair piggybacking;data centers binary;binary mds codes;node repair;nodes existing codes;exist mds codes;storage codes rashmi;mds codes smallest;codes exist mds;required node repair;mds codes exist;parity nodes;centers binary mds;codes ef\ufb01cient dataread;read download repair;codes meeting constraints;codes entail smallest;repair parity;smallest data read;binary mds;smallest repairlocality"}, "6e05d35d072cd73fa039fd60696a8fe110f1d6cd": {"ta_keywords": "contextual recommendation path;recommendation path constrained;walks recommendation increasingly;contextual recommendation;data contextual recommendation;random walks recommendation;recommendation path;reading recommendation tasks;walks recommendation;recommendation tasks;recommendation tasks leveraging;recommendation systems;based reading recommendation;recommendation increasingly;recommendation systems deal;challenges recommendation systems;recommendation increasingly important;citation based history;collaborative filtering;reading recommendation;collaborative filtering approaches;representation publication databases;based collaborative filtering;content based collaborative;meta data contextual;citation based;publication databases rich;graph representation publication;publication databases;history based reading", "pdf_keywords": ""}, "3c4dfc252c214d559fadb5e3159bcc9c7db08fbc": {"ta_keywords": "lesions dental fluorosis;fluorosis extracted teeth;dental fluorosis appears;teeth suspected fluorosis;mild fluorosis visible;caries lesions dental;measured microct fluorosis;areas fluorosis investigated;lesions fluorosis extracted;fluorosis visible;dental fluorosis;caries lesions;fluorosis imaged microct;fluorosis result pitted;lesions dental;lesions fluorosis appeared;fluorosis investigated study;microct fluorosis;fluorosis investigated;lesions fluorosis;dynamics lesions fluorosis;\u03bcm areas fluorosis;surface severe fluorosis;fluorosis extracted;arrested caries lesions;microct fluorosis causes;areas fluorosis;fluorosis imaged;suspected fluorosis imaged;fluorosis visible faint", "pdf_keywords": ""}, "294f8307f26eb3ec7bbf19f15092f3c473ece821": {"ta_keywords": "named entity recognition;entity recognition classification;entity recognition;training relation extractors;named entity classification;training named entity;entity classification;entity classification propose;relation extractors textbound;classifier relation extractor;relation extractors;distantly supervised approaches;performance distantly supervised;relation extractor;domain relation extractor;relation extractor using;distantly supervised;extractors textbound annotation;named entity classifier;relations knowledge base;named entity;entity classifier relation;large textual corpus;theshelf named entity;distant supervision approaches;entity classifier;supervised nerc;entity;training intended domain;textual corpus", "pdf_keywords": ""}, "6a2c4a0f04c6ba2f6fbc171dcea8730423a298e5": {"ta_keywords": "locality sensitive hashing;neighbor retrieval metric;nearest neighbor retrieval;approximate nearest neighbor;neighbor retrieval;nearest neighbor;locality sensitive;learning prune approaches;retrieval metric;learning prune;effective learning prune;locality;probe locality sensitive;prune approaches density;multi probe locality;retrieval metric non;hashing;sensitive hashing;tree learned pruner;prune approaches;nearest;approximate nearest;probe locality;learned pruner compared;learned pruner;pruner compared recently;retrieval;pruner compared;data sets metric;hashing lsh", "pdf_keywords": ""}, "20d4105b276da6d6d38ed3c1bfc436f76198c240": {"ta_keywords": "datasets political event;event datasets political;political event dataset;datasets political;political event;causal relationships;pairs event datasets;potential causal relationships;cause effect association;pairwise associations events;effect association pairs;discovering potential causal;association pairs events;associations events conduct;association event pairs;large event datasets;effect association event;causal relationships critical;estimating cause effect;event datasets evaluate;events conduct;event dataset involving;event datasets discovering;pertaining actor identities;event datasets;dataset involving interaction;world event datasets;politics financial analysis;associations events;actor identities", "pdf_keywords": ""}, "695d4c04f6e4f7ba5f771ac7853fdbaa81713ae8": {"ta_keywords": "latent topics generate;learn latent topics;academic search network;network embeddings experiments;word network embeddings;latent topic space;shared latent topic;researchers knowledge base;open knowledge bases;latent topics;knowledge bases;bayesian embedding model;modal bayesian embedding;concepts shared latent;embeddings experiments datasets;network embeddings;bayesian embedding;data embeddings;embeddings experiments;knowledge base;topics generate;online academic search;topics generate word;online social networks;genvector learn latent;embedding model genvector;latent topic;embedding model;social network;search network", "pdf_keywords": "modal bayesian embeddings;modal bayesian embedding;topic models embeddings;bayesian embeddings learning;bayesian embeddings;bayesian embedding model;embeddings learning social;word embeddings network;bayesian embedding;embeddings shared latent;social knowledge graphs;embedding model learn;latent topics generate;learn latent topics;latent topic space;embeddings learning;multi modal embeddings;word embeddings;modal embeddings shared;embeddings network embeddings;modal embeddings;data embeddings;embedding model jointly;embeddings shared;generate word embeddings;multi modal bayesian;topic models;connected knowledge bases;embeddings network;network embeddings"}, "3b8494614903dc47579da30477b21b109b29f8cd": {"ta_keywords": "machine learning;machine learning proceedings;learning;rutgers university;rutgers;conference rutgers university;international conference rutgers;machine;conference rutgers;rutgers university new;nj usa;brunswick nj;nj;eleventh international conference;international conference;brunswick nj usa;brunswick;1994;new brunswick nj;university new brunswick;usa;eleventh international;eleventh;new brunswick;conference;university;13 1994;usa july 10;proceedings eleventh;learning proceedings eleventh", "pdf_keywords": ""}, "74c881830a9cd7ea49795faa5c582b7ec56bd0bf": {"ta_keywords": "speech recognition asr;speech recognition reverberant;speech enhancement asr;multichannel speech recognition;speech recognition;noisy speech recognition;speech recognition task;end multichannel speech;recognition asr;automatic speech recognition;asr composing neural;multichannel speech;automatic speech;s2s asr module;recognition asr ability;far field asr;optimize conventional asr;entire multichannel speech;reverberant tasks;reverberant tasks dirha;modules s2s asr;reverb sequence sequence;conventional asr;asr components s2s;reverb sequence;noisy speech;enhancement asr components;asr module;applied noisy speech;noisy reverberant tasks", "pdf_keywords": "speech recognition reverberant;speech recognition asr;end multichannel speech;multichannel speech recognition;speech enhancement asr;speech recognition;multichannel speech;end asr model;speech processing;entire multichannel speech;automatic speech recognition;recognition asr;recognition reverberant mismatch;optimize conventional asr;automatic speech;asr composing neural;recognition asr ability;speech processing johns;end end asr;asr model work;language speech processing;end asr;recognition reverberant;asr model;paradigm automatic speech;end asr objective;conventional asr;reverberant mismatched environments;s2s asr module;reverberant mismatched"}, "3d3b1300c7cd6a820a6d08605248f875a3ad20b9": {"ta_keywords": "hop reasoning models;interpretability multi hop;hop reasoning widely;multi hop reasoning;hop reasoning model;hop reasoning really;hop reasoning;interpretable link prediction;reasoning models satisfactory;reasoning models advance;global interpretability evaluation;interpretability global interpretability;quantitatively evaluate interpretability;metrics using interpretability;reasoning models;benchmark multi hop;global interpretability;interpretability multi;interpretability evaluation multi;information multi hop;performance interpretability points;evaluate interpretability multi;evaluation multi hop;reasoning models terms;interpretability global;performance interpretability;interpretability evaluation;interpretability scores rules;evaluate interpretability;recall local interpretability", "pdf_keywords": "hop reasoning models;interpretability multi hop;multi hop reasoning;hop reasoning model;hop reasoning;reasoning models advance;reasoning models;reasoning models terms;multihop reasoning model;rule interpretability score;prediction performance interpretability;reasoning model improve;information multihop reasoning;rule interpretability;improve performance interpretability;information multi hop;rule based models;reasoning model;interpretability multi;model interpretability automatically;generation rule mining;metrics using interpretability;evaluate interpretability multi;performance interpretability points;accurately evaluate interpretability;rule mining;performance interpretability;interpretability scores rules;global interpretability evaluation;methods rule interpretability"}, "e0c54e18cf2372414042bf67eed0272b0a432190": {"ta_keywords": "weblogs social media;weblogs social;conference weblogs social;international conference weblogs;weblogs;conference weblogs;social media icwsm;social media;media icwsm;media icwsm 2010;icwsm 2010;fourth international conference;icwsm;international conference;icwsm 2010 washington;social;conference;media;fourth international;proceedings fourth;proceedings fourth international;23 26 2010;fourth;international;26 2010;2010 washington dc;2010;2010 washington;proceedings;usa", "pdf_keywords": ""}, "7da967be8f6367f6174bf99d0d019ff545ac5966": {"ta_keywords": "semantic annotation lingvosemantic;annotation lingvosemantic corpus;annotation lingvosemantic;semantic annotation;lingvosemantic corpus presented;lingvosemantic corpus;annotation approach significantly;annotation approach;annotation developed;methodology semantic annotation;annotation;creating annotated;annotation developed methodology;annotated data;corpus presented methodology;annotated data presented;creating annotated data;annotated data reasonable;corpus presented;corpus;annotated;style annotation approach;improved efficiency annotation;recommendations creating annotated;sufficient annotated data;obtain sufficient annotated;semantic;style annotation;sufficient annotated;efficiency annotation", "pdf_keywords": ""}, "49db57f300b270f16cbcb1891ca39e16981d42b5": {"ta_keywords": "tracking covid activity;public health surveillance;tracking covid;covidcast api tracks;covidcast api present;health surveillance;signals covidcast api;health surveillance signals;covidcast api;public health data;relevant tracking covid;covid 19 activity;data pandemic relevant;data pandemic;covidcast;covid activity;covid 19 pandemic;signals covidcast;public health reporting;search trends covid;health data sources;health data;covid activity augmenting;indicators covid 19;indicators covid;covidcast api attempt;trends covid 19;trends covid;covid 19 indicators;pandemic relevant public", "pdf_keywords": ""}, "bad416f073a08086ee428e5a264eac3a7d3251e5": {"ta_keywords": "structured literature image;literature image finder;image finder parsing;text figures biomedical;parsing text figures;figures biomedical literature;structured literature;literature image;biomedical literature;figures biomedical;image finder;finder parsing text;finder parsing;text figures;structured;parsing text;parsing;literature;biomedical;image;text;figures;finder", "pdf_keywords": ""}, "d9dbdd254b02ef1af2769af403cba373c1b1bcb1": {"ta_keywords": "based speaker diarization;outputs speaker diarization;speaker diarization;speaker diarization method;speaker diarization results;speaker diarization problem;clustering speaker representations;speaker label permutation;extraction clustering speaker;formulate speaker diarization;clustering speaker;overlapping speech training;speaker representations;network based speaker;speech training inference;speech training;speaker label;speaker representations furthermore;model formulate speaker;formulate speaker;outputs speaker;recorded speech;domain adaptation;overlapping speech;directly outputs speaker;suffered speaker label;speaker;handle overlapping speech;based speaker;real recorded speech", "pdf_keywords": "based speaker diarization;speaker diarization model;neural speaker diarization;speaker diarization method;speaker diarization;simulated speech mixtures;end neural speaker;speech mixtures evaluated;speech mixtures compared;speech mixtures;speech data;network based speaker;real speech data;speech data achieved;evaluated simulated speech;using simulated speech;method simulated speech;simulated speech;neural speaker;adaptation real speech;diarization model eend;based speaker;diarization model;diarization error oriented;speaker;diarization method directly;directly optimized diarization;optimized diarization error;optimized diarization;end neural network"}, "5931c8ac145baf17cec9effc25c051049b7dfd4c": {"ta_keywords": "spatial grounding dialogue;grounding dialogue task;grounded neural dialogue;reference game dialogue;grounding dialogue;game dialogue agent;dialogue agent accurately;dialogue task;neural dialogue;game dialogue;dialogue agent;neural dialogue model;dialogue model;dialogue;dialogue model successfully;reference game;observable reference game;referents partner utterances;grounded neural;self play evaluations;referents using recurrent;accurately grounds referents;partner utterances;dialogue task udagawa;agent accurately grounds;present grounded neural;pragmatic generation;utterances using structured;partner utterances using;recurrent memory", "pdf_keywords": "grounded collaborative dialogue;collaborative dialogue;grounded neural dialogue;neural dialogue model;dialogue model;dialogue model successfully;neural dialogue;dialogue task;dialogue task \ufb01nding;models grounded collaborative;collaborative dialogue daniel;evaluations dialogue task;human evaluations dialogue;grounded collaborative;evaluations dialogue;dialogue;grounded neural;centric models grounded;model successfully collaborates;collaborative;models grounded;present grounded neural;observable reference game;reference game implementation;reference centric models;abstract present grounded;reference game;task \ufb01nding cooperates;collaborates people partially;collaborates people"}, "6c4258f6a6a4bee7b9d914379c44aea6073cdc37": {"ta_keywords": "energy disaggregation adaptive;building energy disaggregation;energy disaggregation;energy disaggregation problem;disaggregation adaptive filtering;disaggregation adaptive;power consumption signals;power consumption signal;particular disaggregation;particular disaggregation problem;adaptive filtering;filtering paper disaggregation;disaggregation problem reformulated;disaggregation;adaptive filtering paper;level power consumption;disaggregation problem solved;disaggregation problem;adaptive filtering problem;disaggregation problem recovering;problem particular disaggregation;reformulated adaptive filtering;consumption signal building;aggregate power consumption;power consumption;consumption signal;consumption signals aggregate;signals aggregate power;signal building energy;consumption signals", "pdf_keywords": "energy disaggregation adaptive;disaggregation building energy;energy disaggregation;power consumption patterns;energy disaggregation problem;power consumption signals;power consumption data;abstract energy disaggregation;power consumption signal;disaggregation adaptive filtering;introduction power consumption;disaggregation adaptive;novel disaggregation algorithm;aggregate power consumption;building energy data;level power consumption;disaggregation algorithm;power consumption;disaggregation building;building particular disaggregation;device power consumption;energy data;disaggregation algorithm better;consumption signal building;energy data using;algorithm disaggregation building;consumption signals aggregate;models paper disaggregation;particular disaggregation;develop algorithm disaggregation"}, "3dcf9c900f5f28e082a2fcdea4763b6063a76f09": {"ta_keywords": "improving defeasible reasoning;defeasible reasoning modeling;reasoning modeling;defeasible reasoning;reasoning modeling question;reasoning mode reasoning;defeasible reasoning suggests;defeasible reasoning mode;reflexively defeasible reasoning;reasoning suggests person;question model;scenario answering questions;model scenario answering;reasoning mode;scenario answering reflexively;literature defeasible reasoning;given question model;scenario answering;modeling question scenario;answering questions approach;reasoning suggests;reasoning conclusions;question explicitly model;question model create;reasoning;evidence existing cognitive;questions approach;answering reflexively defeasible;guiding think question;improving defeasible", "pdf_keywords": "learning reasoning tasks;defeasible reasoning datasets;reasoning datasets inference;learning reasoning;reasoning datasets;reasoning modeling;reasoning tasks overall;reasoning tasks;defeasible reasoning modeling;datasets inference graphs;reasoning datasets experiments;inference graphs;improves defeasible reasoning;inference graphs formulation;inference graph;inference graph adapted;based learning reasoning;reasoning modeling question;scenario inference graph;datasets inference;defeasible reasoning;graph adapted cognitive;inference;different defeasible reasoning;reasoning;question scenario inference;model scenario answering;scenario answering;scenario inference;art defeasible reasoning"}, "dbdb7f25f1538c2a2885d3992e5320e2ee5c23a1": {"ta_keywords": "cognitive tutors platform;cognitive tutors;simstudent furthermore tutoring;cognitive tutor;cognitive tutor authoring;facilitate tutor learning;tuning cognitive tutors;tutoring;facilitate tutor;furthermore tutoring;tutor learning;authored cognitive tutor;extracted cognitive tutor;tutors platform;equation solving teaching;tutoring interface;furthermore tutoring interface;tutor;solving teaching computer;tutor learning effect;teaching computer agent;tutoring interface used;pedagogical teachable agent;tutor authored cognitive;learning teaching simstudent;tutors;cognitive tutor authored;teachable agent;factors facilitate tutor;tools simstudent pedagogical", "pdf_keywords": ""}, "660119405bb48777cd71d85caa5ec2e90a336caf": {"ta_keywords": "historical text normalization;text normalization systems;text normalization;text normalization large;normalization techniques;normalization techniques analysing;proposed normalization techniques;normalization;normalization large scale;proposed normalization;normalization systems;normalization large;categories proposed normalization;statistical machine translation;machine translation neural;translation neural encoder;machine translation;translation neural;approach historical text;character based statistical;comparison historical text;metrics character based;historical text;decoder models;decoder models studies;languages comparing;encoder decoder models;neural encoder decoder;experiments languages comparing;report experiments languages", "pdf_keywords": "historical text normalization;text normalization systems;text normalization critically;text normalization;normalization techniques analysing;stemming useful tool;normalization techniques;stemming useful;normalization critically;normalization models;normalization;analysis normalization models;proposed normalization techniques;categories proposed normalization;normalization critically survey;stemming;normalization systems marcel;proposed normalization;normalization systems;normalization models reveal;error analysis normalization;shows stemming useful;study shows stemming;analysis normalization;word accuracy cer;word accuracy;approach historical text;shows stemming;comparison historical text;languages case study"}, "98554bd8a15172e9a6ef3cc3db3bc52504110fc9": {"ta_keywords": "bandits regret minimization;stochastic bandits regret;objectives stochastic bandits;stochastic bandits precisely;identification stochastic bandits;stochastic bandits;bandits regret;frontier regret minimization;bandits precisely;bandits precisely establish;regret minimization rm;lower bounds regret;regret achievable algorithm;regret minimization;regret minimization best;bounds regret achievable;bandits;bounds regret;pareto frontier regret;optimally rm bai;perform optimally;exploitation exploration crucial;simultaneously perform optimally;optimally rm;wise optimal;perform optimally rm;exploitation exploration;optimal;achieving optimal;archetypal objectives stochastic", "pdf_keywords": "identi\ufb01cation stochastic bandits;stochastic bandits;exploration adversarial bandits;adversarial bandits;bandits similar stochastic;stochastic bandits references;adversarial bandits similar;exploitation exploration adversarial;frontier regret minimization;bandits;bandits references;regret minimization;optimally rm;bandits similar;regret minimization best;optimally rm bai;perform optimally rm;exploration adversarial;exploitation exploration;bandits references abbasi;optimally pareto frontier;pareto frontier regret;simultaneously perform optimally;perform optimally;optimally pareto;perform optimally pareto;pareto frontier rm;algorithm compromise rm;optimally;algorithm perform optimally"}, "6b3fa9157a8120a6eb86ae06a93611a1fcd9e219": {"ta_keywords": "hardening soft information;database formulate hardening;databases soft;resulting databases soft;soft database noisy;databases soft contain;underlying hard database;soft database;particular soft database;soft database formulate;soft information;hard database cases;hard database;model soft database;soft information sources;feature approach hardening;formally model soft;hardening optimization;hard database given;database noisy;database noisy version;hardening soft;hardening optimization problem;soft contain inconsistencies;formulate hardening optimization;approach hardening global;hardening global sources;hardening global;approach hardening;heuristically extract structured", "pdf_keywords": ""}, "ef9ddbc35676ce8ffc2a8067044473727839dbac": {"ta_keywords": "softmax distributed word;distributed word embeddings;problem expressiveness softmax;neural language models;limited softmax;softmax;expressiveness softmax;language modeling matrix;models limited softmax;limited softmax bottleneck;expressiveness softmax based;softmax bottleneck;practice softmax distributed;softmax based;model natural language;softmax based models;word embeddings;practice softmax;softmax bottleneck given;softmax distributed;language models;formulate language modeling;natural language highly;majority neural language;language models limited;factorization problem expressiveness;word embeddings does;treebank;language modeling;implies practice softmax", "pdf_keywords": "softmax based recurrent;limited softmax;aforementioned softmax based;softmax bottleneck propose;models limited softmax;aforementioned softmax;softmax;learning softmax;recurrent language models;softmaxes;limited softmax bottleneck;problem expressiveness softmax;softmax based models;recurrent language model;softmax bottleneck;expressiveness aforementioned softmax;breaking softmax bottleneck;learning softmax based;rnn language model;breaking softmax;softmax based;expressiveness softmax;softmax bottleneck formulating;address softmax bottleneck;neural language models;expressiveness softmax based;softmaxes mos;2018 breaking softmax;language modeling matrix;identify softmax bottleneck"}, "d29f155060f96becef0247ee77dc038f96b2d983": {"ta_keywords": "translation processing time;speech translation systems;translation speed;speech translation considering;accuracy speech translation;conventional speech translation;speech translation;translation processing;translation speed accuracy;machine translation module;translation systems;translation systems difficult;time translation processing;machine translation;effect translation speed;translation start time;improve translation start;shortening translation unit;translation module;time machine translation;phrase base translation;shortening translation;start time translation;method shortening translation;translation module concretely;base translation propose;translation unit;base translation;improve translation;translation start", "pdf_keywords": ""}, "d415b724fbc35afcc8dd91738123edfa6a5db634": {"ta_keywords": "deep policy gradients;deep policy gradient;policy gradient algorithms;policy gradients;progress deep policy;policy gradient;policy gradients case;algorithms proximal policy;proximal policy optimization;deep policy;policy optimization;policy optimization trpo;region policy optimization;algorithmic progress deep;policy optimization ppo;trust region policy;optimization ppo trust;matters deep policy;gradient algorithms case;implementation matters deep;gradient algorithms;progress deep;reward trpo fundamentally;code level optimizations;secondary importance optimizations;importance optimizations turn;popular algorithms proximal;cumulative reward trpo;optimizations algorithm augmentations;region policy", "pdf_keywords": "deep policy gradients;deep policy gradient;policy gradient methods;policy gradient algorithms;policy gradients;progress deep policy;powering deep policy;policy gradient;deep reinforcement learning;policy gradients case;proximal policy optimization;algorithms proximal policy;region policy optimization;policy optimization;policy optimization trpo;deep reinforcement;gains deep reinforcement;deep policy;policy optimization ppo;popular deep policy;gradient methods trust;performance gains deep;algorithmic progress deep;reinforcement learning \ufb01rst;trust region policy;reward underlying algorithmic;reinforcement learning;matters deep policy;reward underlying;optimization ppo trust"}, "3315dee45b1edb8f8286816629de7b8c31d270d6": {"ta_keywords": "influences political information;affect influence information;political information search;search behavior participants;influence information search;political information;social signals affect;search strategies political;influence information;signals affect influence;judgments subjects voting;social influences political;social signals;participants neutral information;strategies political judgments;political judgments subjects;social cues;task social influences;voting task social;liked disliked information;affect influence;search behavior;social cues fact;political judgments;simple social signals;influences political;subjects voting;participants subjects social;information search;information search strategies", "pdf_keywords": ""}, "387754dc8d4185fadd7c3c15e43956a4d085e8fe": {"ta_keywords": "nearest neighbor search;approximate nearest neighbor;nearest neighbor;permutation search methods;nearest neighbors;point permutation search;nearest neighbors examining;distance point permutation;distance permutations good;neighbor search possible;permutation search;points permutations similar;distance permutations;proximity graph based;neighbor search;graph based retrieval;sorted distance;true nearest neighbors;sorted distance point;efficient faster search;permutation methods reasonably;similar permutation query;search possible efficiently;faster search possible;points permutation methods;spaces distance permutations;faster search;data points permutations;proximity graph;similar permutation", "pdf_keywords": "highaccuracy retrieval methods;graph based retrieval;retrieval methods generic;retrieval methods;retrieval;highaccuracy retrieval;implementation proximity graph;based retrieval;lsh mplsh recall;based retrieval 16;proximity graph based;retrieval 16;focus highaccuracy retrieval;mplsh recall larger;proximity graph;mplsh recall;recall larger;napp baseline methods;implementation proximity;multi probe lsh;recall;napp baseline;probe lsh;algorithm vp tree;retrieval 16 current;implementation multi probe;probe lsh mplsh;added implementation proximity;recall larger 95;benchmark focus highaccuracy"}, "60a121c55b5144bfe3aef5b6ea8959a9f6dd12ae": {"ta_keywords": "speech enhancement algorithms;learning speech enhancement;multiple speech enhancement;speech enhancement noisy;speech enhancement based;speech enhancement;problem speech enhancement;enhancement noisy mixture;ensemble learning speech;noisy mixture ensemble;mixture ensemble learning;enhancement noisy;ensemble learning;enhancement algorithms;ensemble learning framework;mixture ensemble;problem ensemble learning;clean speech based;enhancement algorithms formulating;ensemble;applied noisy mixture;enhancement based;speech based outputs;noisy mixture;obtained clean speech;multiple speech;various machine learning;problem ensemble;speech based;learning speech", "pdf_keywords": ""}, "ba56bb1eb67b188a89060058ef8ad02ce3c660ac": {"ta_keywords": "workshop asian translation;asian translation wat2016;asian translation;translation wat2016;3rd workshop asian;workshop asian;translation;asian;proceedings 3rd workshop;3rd workshop;workshop;wat2016;proceedings;proceedings 3rd;3rd", "pdf_keywords": ""}, "9fc33c53d1f59aa9fd7f1b642c3859900865b0e3": {"ta_keywords": "structured datasets web;semi structured datasets;table hyponym data;structured data web;semi structured data;hyponym data;structured datasets;datasets web collectively;datasets web;structured data;hyponym data small;table hyponym;data web;web collectively representing;available semi structured;collection table hyponym;representing semi structured;datasets;representation large collection;large collection table;semi structured;web collectively;data;low dimensional representation;collectively representing semi;data small;single low dimensional;hyponym;large collection;low dimensional", "pdf_keywords": ""}, "d7729f2ff21f97d56d10c54adc1f1f5ffbec9e5c": {"ta_keywords": "treatment oral leukoplakia;oral leukoplakia using;lasers management oral;treat oral leukoplakia;oral leukoplakia;oral leukoplakia reported;premalignant lesions leukoplakia;oral premalignant lesions;lesions leukoplakia;surgicaltechniques diode laser;leukoplakia using;management oral premalignant;lesions leukoplakia erythroplakia;leukoplakia using different;diode laser;leukoplakia reported effective;leukoplakia reported;treatment oral;oral premalignant;treat oral;techniques treat oral;leukoplakia;outcomes treatment oral;leukoplakia erythroplakia;leukoplakia erythroplakia remain;modality oral premalignant;recurrence oral premalignant;application different lasers;different lasers;different lasers management", "pdf_keywords": ""}, "649eb9fd9a18f9601270b7fcde8d6548bfc6ec75": {"ta_keywords": "speech recognizing separated;speech separation recognition;speech recognition end;overlapping speech recognizing;dataset speech separation;speech recognizing;speaker speech recognition;separating overlapping speech;2mix dataset speech;automatic speech;speech recognition;speech separation;dataset speech;multi speaker speech;automatic speech recognition;speech mixture corresponding;end automatic speech;speech recognition model;monaural multi speaker;speech mixture;speech generated wsj;speaker scheduled sampling;separated speaker scheduled;overlapping speech;module separated speaker;mixture speech corresponding;speech corresponding labels;separation recognition benchmark;requires speech mixture;separated speaker", "pdf_keywords": "end speech recognition;speech recognizing separated;speech separation recognition;speech recognition joint;overlapping speech recognizing;end multispeaker speech;dataset speech separation;speech recognizing;multispeaker speech recognition;2mix dataset speech;speech recognition;speech recognition model;dataset speech;separating overlapping speech;automatic speech recognition;end end multispeaker;end automatic speech;automatic speech;end end speech;speech generated wsj;multispeaker speech;end speech;speech separation;end multi speaker;end multispeaker;multi speaker end;separated speaker scheduled;overlapping speech;multi speaker asr;speaker end end"}, "5884948777dfc003ba49e1513420830616281839": {"ta_keywords": "bilingual lexicon induction;learns unified multilingual;muse bilingual lexicon;representations multilingual bert;unified multilingual representations;trained monolingual representations;methods muse bilingual;multilingual bert;cross lingual alignment;multilingual representations;contextualized representations multilingual;cross lingual tasks;bilingual lexicon;monolingual representations shared;lingual alignment;monolingual representations;lingual ner benchmark;lingual alignment vs;multilingual representations using;representations using monolingual;jointly cross lingual;muse bilingual;monolingual cross lingual;cross lingual objectives;independently trained monolingual;trained monolingual;multilingual bert produces;unified multilingual;representations multilingual;diverse cross lingual", "pdf_keywords": "lingual transfer learning;cross lingual alignment;bilingual lexicon induction;learning multilingual representations;learns uni\ufb01ed multilingual;cross lingual objectives;learning multilingual;monolingual cross lingual;cross lingual transfer;cross lingual;trained monolingual representations;muse bilingual lexicon;multilingual representations text;lingual alignment;multilingual representations;bilingual lexicon;method cross lingual;independently trained monolingual;methods muse bilingual;lingual alignment vs;using monolingual cross;monolingual representations shared;uni\ufb01ed multilingual representations;2020 cross lingual;multilingual representations using;trained monolingual;lingual transfer;lingual objectives jointly;abstract learning multilingual;monolingual representations"}, "56bc2a1eebedab3e452a7ca3969aa1e4dd5946c3": {"ta_keywords": "diversity influence maximization;diversity influential node;embedding diversified ranking;influential node mining;diversified ranking utility;node diversity influence;diversified ranking;influence spread diversity;influence maximization im;influence maximization;heuristics embedding diversified;embedding diversified;incorporate node diversity;maximization result diversification;influential node;node diversity;diversified im optimization;diversification formulate diversified;ranking utility maximization;incorporating diversity influential;results diversified;diversity influence;similarity selected nodes;spread diversity factors;diversified;spread diversity;results diversified im;result diversification;diversified im;formulate diversified", "pdf_keywords": ""}, "af0adbaa0c1ea6abaed4b3d21f1dc4121c35fb30": {"ta_keywords": "communicative agents trained;language coordination agents;communicative agents;conversational partners language;current communicative agents;partners language abilities;language coordination;quickly adapting communicate;shot language coordination;agents trained selfplay;adapting communicate;communicate unseen agents;adapting conversational partners;adapting communicate unseen;human communication;language coordination modeling;agents different linguistic;coordination agents quickly;component human communication;referential game language;modeling listeners mental;adapting conversational;linguistic abilities quickly;explicitly modeling communication;quickly adapting conversational;game language;communicative goal;agents quickly adapting;language abilities;modeling communication", "pdf_keywords": "language coordination agents;shot language coordination;language coordination;referential game language;language coordination modeling;game language navigation;multilingual referential games;communicative agents trained;game language;conversational partners language;quickly adapting communicate;task shot language;quickly adapting conversational;language coordination settings;adapting conversational;agents different linguistic;communicative agents;language navigation task;language abilities;vision language navigation;adapting communicate;linguistic abilities quickly;language comprehension recursive;perform shot language;shot language;referential game;adapting communicate unseen;current communicative agents;vision language;performance referential game"}, "3c57a1aa483d8bffe1339914b80d2913f2dc8376": {"ta_keywords": "generative adversarial networks;adversarial networks gans;generative adversarial;good semi supervised;semi supervised;semisupervised learning requires;semi supervised learning;bad gan theoretically;semisupervised learning;semi supervised classification;good semisupervised learning;gans obtained;gans obtained strong;networks gans obtained;requires bad gan;networks gans;based generative adversarial;bad gan;objective good semisupervised;adversarial networks;generative;good semisupervised;gan theoretically given;semisupervised;gans;adversarial;gan theoretically;discriminator objective good;gan;generator good semi", "pdf_keywords": "discriminator formulation gan;discriminator standard gan;feature matching gans;matching gans;matching gans obtaining;categorical generative adversarial;adversarial networks catgan;gans obtaining;generative adversarial networks;generative adversarial;gans obtaining state;formulation gan;gans;gan based;formulation gan based;networks catgan;gan multi class;gan;discriminator generator objectives;generative;categorical generative;semi supervised;standard gan;networks catgan substitutes;good semi supervised;gan multi;adversarial;adversarial networks;semi supervised learning;proposes categorical generative"}, "866f231970f93f4a201febc2fb46aff06f501e4b": {"ta_keywords": "normalization historical spelling;normalization historical language;automatic normalization historical;normalizing words interactive;normalizing words;rules normalizing words;normalization historical;modern word forms;automatic normalization;semi automatic normalization;historical language data;modern language tool;historical spelling case;normalization preferring forms;historical spelling;rules normalizing;normalizing;normalization;historical modern word;guidelines distinguish normalization;normalization preferring;word forms;mapping historical modern;modern language;use mapping historical;historical language;spelling case studies;modern word;distinguish normalization preferring;rewrite rules normalizing", "pdf_keywords": ""}, "4d96ec46cda5d3b223fc7d33a920ab85864ea36d": {"ta_keywords": "features proteins amino;features proteins;reductase hco superfamily;important features proteins;database proteins;applicable database proteins;hco superfamily providing;database proteins chose;characterized proteins;descriptions performs protein;superfamily providing insight;protein families;hco superfamily;characterized proteins heme;proteins;members hco superfamily;protein annotation primary;protein annotation;proteins amino acid;proteins amino;previously characterized proteins;amino acid sequence;reductases heme copper;amino acid sequences;proteins heme copper;performs protein annotation;role proteins amino;proteins heme;divergent protein families;superfamily applied discover", "pdf_keywords": ""}, "6c170fe3fec5a477c938d07fa00935bb6f7b87cc": {"ta_keywords": "voice conversion;based voice conversion;voice conversion using;based speech synthesis;quality converted speech;speech synthesis;speech synthesis using;sample based speech;gmm based voice;voice conversion vc;converted speech;high quality speech;speech features formulation;converted speech significantly;speech feature vectors;speech tts synthesis;speech rich context;based text speech;joint speech feature;individual speech features;quality speech keeping;text speech;natural speech rich;utilizes individual speech;speech features;speech feature;quality speech introducing;model hmm based;original hmm based;quality speech", "pdf_keywords": ""}, "adac290d72c86c186837a884aae922bee4dee684": {"ta_keywords": "transpositions harder misspellings;surprisal human readers;words cause reading;cause reading difficulty;readers unimpaired comprehension;misspelling error rates;reading difficulty;human readers unimpaired;model human reading;harder misspellings contain;harder misspellings;human reading;human reading presence;naturally occurring misspelling;human readers;word based surprisal;reading difficulty correct;increased reading times;occurring misspelling;words difficultto predict;misspellings contain unexpected;misspelling;character based surprisal;misspellings;misspellings contain;words contain errors;increased reading;difficulty correct words;eye tracking study;readers unimpaired", "pdf_keywords": "text effect reading;predicts human reading;human reading affected;reading behavior;effect reading behavior;experimentally investigate reading;human reading;determine human reading;human reading times;observed reading experiment;effect reading;eye tracking experiment;reading behavior high;observed reading;reading dif\ufb01culty misspellings;misspellings explained using;results eye tracking;reading experiment;eye tracking;transpositions cause reading;reading affected;investigate reading face;reading experiment present;rate observed reading;reading face errors;investigate reading;word based surprisal;misspellings explained;reading times;reading"}, "23918ed366c60ae0ef85b0c80def63127f035e02": {"ta_keywords": "cloud training;model cloud training;inference edge device;inference accuracy shredder;dnn accuracy tradeoff;edge formally shredder;shredder maximizes privacy;edge device;network learns;shredder enables accurate;accuracy shredder;cloud training process;accuracy shredder enables;edge device merely;minimal impact dnn;cloud;offline learning;model cloud;edge device takes;dnns text processing;learns additive noise;trained network learns;dnn accuracy;proposed offline learning;shredder;trained network;impact dnn accuracy;data cloud;shredder offers speedup;network learns additive", "pdf_keywords": ""}, "e7e1f5a713d20cdf31e732022731fdf0d8fb4fc5": {"ta_keywords": "sentence pair classification;natural language inference;explanations natural language;supervised sentence pair;generate explanations classifiers;sentences generating token;explanations classifiers;explanations predictions classifiers;tagging sentence pairs;language inference nli;explanations classifiers operating;single sentence tagging;sentence pairs nli;sentence tagging;generating explanations predictions;inference nli widely;level explanations nli;token level explanations;generating explanations;inference nli;anchor explanations task;explanations nli;supervised sentence;generate explanations;pairs sentences generating;natural language;task natural language;sentence tagging sentence;sentences generating;nli task natural", "pdf_keywords": "sentence attention;cross sentence attention;sentence attention applied;relation attention thresholding;entailment relation attention;nli explanations generated;neural entailment;neural entailment model;attention;thresholded attention improved;attention thresholding;attention thresholding approach;using thresholded attention;attention matrix neural;single sentence tagging;nli explanations;matrix neural entailment;generating explanations using;regularizers generating explanations;relation attention;pairs nli explanations;generating explanations;attention improved precision;thresholded attention;sentence tagging;sentence pairs nli;attention improved;token level explanations;anchor explanations;fully supervised baseline"}, "37ef7941909527aaf123d7b8f90adbf4606f4917": {"ta_keywords": "gibbs sampling machine;learn speech tags;iterative gibbs sampling;implementations iterative gibbs;gibbs sampling;blocked gibbs sampling;speech tags newswire;study infinite hmm;speech tags;model learn speech;infinite hmm;hmm parameters learnt;sampling machine learning;iterative gibbs;learning algorithm probabilistic;gibbs sampling step;infinite hmm parameters;algorithm probabilistic model;performance opposed nlp;learn speech;algorithm probabilistic;hmm parameters;learning algorithm;distributed implementations iterative;instance blocked gibbs;learnt using;nlp;tags newswire text;speech;machine learning algorithm", "pdf_keywords": ""}, "58e5ce12c23f815e9b394220044eaf99b28cfffe": {"ta_keywords": "language processing crisis;processing crisis information;crisis information 2013;workshop language processing;crisis information;language processing;crisis;processing crisis;proceedings workshop language;language;workshop language;information 2013;processing;proceedings workshop;workshop;proceedings;information;2013", "pdf_keywords": ""}, "cf2fcb73e2effff29ceb5a5b89bbca34d2d27c1a": {"ta_keywords": "deceptive attention masks;deceptive attention;based explanations deceive;produce deceptive attention;attention based explanations;predictions attention mechanisms;explanations deceive people;models produce deceptive;predictive accuracy attention;predictions attention;explanations deceive;attention weights;attention weights claimed;biased gender;model biased gender;accuracy attention weights;accuracy attention;deceive people thinking;manipulated attention based;biased gender minorities;confer interpretability purportedly;use attention mechanisms;attention based;based explanations;confer interpretability;attention mechanisms;drive predictions attention;people thinking predictions;manipulated attention;attention masks", "pdf_keywords": "learning deceive attention;deceive attention based;attention based explanations;deceive attention;based explanations deceive;explanations deceive people;deceptive attention;deceptive attention masks;explanations deceive;predictive accuracy attention;produce deceptive attention;attention weights;accuracy attention weights;attention weights claimed;models produce deceptive;confer interpretability purportedly;learning deceive;deceive people thinking;prediction use attention;accuracy attention;abstract attention mechanisms;confer interpretability;biased gender;based explanations;interpretability purportedly useful;model biased gender;people thinking predictions;claimed confer interpretability;interpretability purportedly;use attention mechanisms"}, "79655bfc45039b4d7cfe6cc86d52a4ced492f43a": {"ta_keywords": "evaluating learning rank;learning rank methods;learning rank;performance learning rank;rank methods web;relevance signals using;trec web adhoc;par learning rank;using trec web;methods using trec;trec web;rank methods performance;web track adhoc;rank methods specifically;using trec;experiment learning rank;rank methods;relevance signals;trec data;methods web track;set relevance signals;relevance signals limitation;rank methods using;specific relevance signals;compared performance learning;trec data set;web track;trec;web adhoc;performance learning", "pdf_keywords": ""}, "5c3cc301a892094d5bfca3c41a78a3a8ebd755f8": {"ta_keywords": "boosted regression trees;additive regression trees;regression trees known;prediction accuracy diverse;datasets dart outperforms;ensemble model boosted;regression trees;regression trees mart;high prediction accuracy;regression classification tasks;boosted regression;deliver high prediction;ensemble;prediction instances make;model boosted regression;dart ranking regression;model boosted;prediction instances;trees mart ensemble;regression trees evaluate;boosted;ranking regression classification;specialization trees added;dart outperforms;high prediction;specialization trees;accuracy diverse tasks;dart dropouts;impact prediction instances;dropouts", "pdf_keywords": "datasets dart outperforms;dart ranking regression;dart outperforms;learning deep;deep neural networks;results dart outperforms;deep neural;dart outperforms mart;learning deep neural;dropouts;dart ranking;datasets dart;employing dropouts;way employing dropouts;random forest tasks;available datasets dart;neural networks hinton;neural networks;outperforms mart tasks;algorithm dart overcomes;evaluate dart ranking;context learning deep;dropouts tool recently;algorithm dart;mart random forest;employing dropouts tool;results dart;random forest;employing dropouts mart;mart resulting dart"}, "9eecfdb7c8ad9af4f3863e9f6ed857211fb710e7": {"ta_keywords": "user explanation leveraging;explanation generation;natural language explanations;explanation generation markov;language explanations actions;explanation leveraging existing;explanations actions suggested;advising markov;academic advising markov;explanations actions;advising markov decision;english language explanations;generate salient explanations;language explanations;explanation leveraging;explanations mdp based;language argumentation interface;trust user explanation;interface explanation generation;interactively generates conversational;language explanations order;explanations order build;argumentation interface;based explanation ported;explanations order;generates conversational english;salient explanations mdp;argumentation interface explanation;policies interactively;natural language", "pdf_keywords": ""}, "dfd8fc9966ca8ec5c8bdc2dfc94099285f0e07a9": {"ta_keywords": "privacy preserving text;privacy utility tradeoff;empirical privacy;benchmark privacy utility;benchmark privacy;mitigate empirical privacy;privacy utility;common benchmark privacy;empirical privacy guarantee;empirical privacy risk;privacy preserving;privacy;art empirical privacy;differentially private mechanisms;approach privacy preserving;private mechanisms text;privacy guarantee utilitarian;differentially private;utilitarian approach privacy;private mechanisms parameterizes;private mechanisms;privacy guarantee;approach privacy;privacy risk propose;class differentially private;added differentially private;price kept private;privacy risk;private;real text classification", "pdf_keywords": "privacy utility tradeoff;privacy preserving text;privacy utility;improve privacy utility;privacy preserving;empirically improve privacy;benchmark privacy utility;differentially private mechanisms;common benchmark privacy;benchmark privacy;differentially private;privacy;approach privacy preserving;neighbor random selection;improve privacy;utilitarian approach privacy;private mechanisms;nearest neighbor noised;private mechanisms text;nearest neighbor random;abstract differentially private;approach privacy;nearest neighbors proposed;words use nearest;nearest neighbor;neighbor random;neighbor noised embedding;natural generalization;generalization extend selection;mechanism second nearest"}, "e8c4a4e81084e17b0c71a6a69bdf1e4e2b6f6af1": {"ta_keywords": "speech separation multi;sequence learning conditional;speech data composed;multi sequence learning;sequential sources mixture;speech recognition conditional;speech separation;speech data;sources mixture sequence;speech data primary;multiple sequential sources;observed speech data;learning conditional chain;sequence learning;including speech separation;conditional multi sequence;sequence transduction;multi sequence models;multi speaker speech;mapping mixture signals;sequential sources;single output sequence;waves neural sequence;mixture signals;sequence transduction problems;sequence single output;speech recognition;neural sequence sequence;sequence sequence models;extracting multiple sequential", "pdf_keywords": "speech separation recognition;tasks speech separation;separation multispeaker speech;speech separation;speech separation multispeaker;recognition conditional chain;experiments speech separation;multispeaker speech recognition;separation recognition conditional;speech recognition model;multispeaker speech;speech recognition;conditional multi sequence;sequence model conditional;multiple output sequences;multi sequence model;separation multispeaker;separation recognition;speci\ufb01c tasks speech;output sequences probabilistic;sequences explicit modeling;recognition conditional;sequences probabilistic chain;sequence multi sequence;sequence model explicitly;sequence multi;sequence sequence model;sequence transduction;tasks speech;multiple sequences explicit"}, "59121b847fd7eb4cf92cbfccb54f1705733d8b65": {"ta_keywords": "dereverberation speech recognizer;recognition reverberant speech;reverberant speech dereverberation;speech dereverberation preprocessing;speech recognizer;reverberant speech;performance automatic speech;compensation recognition reverberant;speech dereverberation;reverberant speech received;reverberation prior recognition;noise reverberation;interconnection dereverberation speech;speech recognition;noise reverberation chapter;recognition reverberant;speech recognizer introduce;problem recognition reverberant;automatic speech recognition;automatic speech;variance compensation recognition;reverberation;dereverberation speech;gaussian variances acoustic;reduce reverberation prior;reverberation prior;reduce reverberation;speech recognition severely;variances acoustic model;model adaptation", "pdf_keywords": ""}, "cec37cd54a940bec818db7216cc1086672f3fec0": {"ta_keywords": "synsets sense inventory;lexical substitutions crowdsourcing;synsets representing concepts;synsets inventory sense;duplicate synsets inventory;sense inventory induction;potentially duplicate synsets;duplicate synsets;duplicate synsets having;synsets sense;number duplicate synsets;sense inventory;synsets inventory;mistakes synsets sense;substitutions crowdsourcing;inventory induction topical;sense inventory alignment;using lexical;alignment using lexical;lexical;synsets;synsets having exactly;eliminating potentially duplicate;mistakes synsets;synsets having;crowdsourcing;lexical substitutions;synsets representing;actually mistakes synsets;set synsets", "pdf_keywords": ""}, "fa10752ab1768d1633001420b48be5e2518a4f80": {"ta_keywords": "learning data bounded;learning data;data bounded inconsistency;bounded inconsistency theoretical;bounded inconsistency;inconsistency theoretical;learning;inconsistency;data bounded;inconsistency theoretical experimental;data;theoretical experimental results;theoretical experimental;theoretical;results;experimental results;experimental;bounded", "pdf_keywords": ""}, "9fcfbc662d4095d72eb9a4e1c4f5ae8f0ffc4222": {"ta_keywords": "teeth thermal dehydration;teeth assessing lesion;extracted teeth thermal;lesions teeth assessing;vivo thermal imaging;teeth thermal;imaging studies enamel;lesion activity thermal;thermal imaging studies;lesions teeth;teeth assessing;lesions extracted teeth;thermal imaging;thermal dehydration measurement;imaging coupled dehydration;secondary lesions teeth;optical coherence tomography;thermal imaging coupled;assessing natural lesion;vitro vivo thermal;vivo thermal;coherence tomography;diffusion porous lesion;dehydration measurement optical;studies enamel root;thermal dehydration;activity thermal imaging;lesions assessed optical;assessing lesion;coherence tomography secondary", "pdf_keywords": ""}, "5801974fcebc11b4a8085fb02e77f792454caf7c": {"ta_keywords": "automated social skills;social skills training;social skill speech;social skills trainer;social skills;social skills experimental;improve social skills;social skill;training developing dialogue;performing social skills;provides social skills;process social skills;autistic traits social;skill speech language;skills training human;acquire social skills;social interaction;skill speech;training human computer;social interaction acquire;traits social skills;automated social;developing dialogue;human computer interaction;autistic traits;skills trainer provides;skills training proposed;interaction acquire social;improve skill using;named automated social", "pdf_keywords": ""}, "20f166f7809d1af9065cd1c71ec1e38d5d92993f": {"ta_keywords": "deep reinforcement learners;deep reinforcement learning;catastrophes practical reinforcement;reinforcement learners;fear learned reward;reinforcement learners periodically;policy avoiding catastrophic;intrinsic fear learned;learned reward shaping;reinforcement learning guards;deep reinforcement;reinforcement learning;catastrophic states optimal;policies periodic catastrophes;problems deep reinforcement;avoiding catastrophic states;learned reward;fear learned;accelerates deep reinforcement;reinforcement learning problems;optimal policy;practical reinforcement learning;states intrinsic fear;catastrophic states intrinsic;avoiding catastrophic;reinforcement;contain catastrophic states;states optimal policy;catastrophic states;reward shaping", "pdf_keywords": ""}, "c43d9d868f5288738cd625d365f0b3a5c18d4a20": {"ta_keywords": "simultaneous interpretation corpus;professional simultaneous interpreters;interpretation corpus;interpretation corpus collected;english simultaneous interpretation;interpreter experience;simultaneous interpreters;japanese english simultaneous;interpreter experience objective;simultaneous interpretation data;simultaneous interpreters different;talk translated text;interpreters;using simultaneous interpretation;translated text time;particular talk translated;describes english japanese;effect interpreter experience;differences interpreters;interpreter;translation data speech;interpreters different;talk translated;simultaneous interpretation;translated text;compare differences interpreters;differences interpreters different;interpreters different levels;corpus;translation data", "pdf_keywords": ""}, "3be5e7310b1bec9b4431ad0f1264f536b6a39f14": {"ta_keywords": "machine translation models;pivot based translation;translation models;level machine translation;character level translation;pivot language translation;language translation quality;crowdsourced movie subtitles;translation models used;machine translation;translation quality;translation sparse;translation quality analyzing;level translation sparse;subtitles experiments characterlevel;translation sparse noisy;language translation;movie subtitles;subtitles experiments;movie subtitles experiments;translation applied sparse;subtitles;level translation;datasets crowdsourced movie;characterlevel models;based translation;pivot language;untranslated words;based translation applied;crowdsourced movie", "pdf_keywords": "machine translation models;pivot based translation;translation models;outperforms cascaded translation;pivot language translation;level machine translation;machine translation;language translation quality;character level translation;translation models used;translation sparse;translation quality;cascaded translation models;translation models use;translation generation;level translation generation;level translation sparse;pivot language effectiveness;language translation;translation quality explore;translation sparse noisy;cascaded translation setups;translation generation synthetic;translation setups;translation setups finally;translation applied sparse;cascaded translation;crowdsourced movie subtitles;level translation;compare cascaded translation"}, "6e7e095f46deb297713dcde05991faf635768d29": {"ta_keywords": "conceptualizations race fairness;race fairness research;algorithmic fairness researchers;race fairness;adopted algorithmic fairness;fairness researchers;argue algorithmic fairness;fairness research;fairness researchers need;algorithmic fairness frameworks;algorithmic fairness;race theory sociological;fairness frameworks argue;fairness frameworks;conceptualizations race;sociological work race;race racial categories;conceptualizing operationalizing race;conceptualization race;fairness research drawing;produce racial inequality;ground conceptualizations race;race racial;adopting conceptualization race;racial inequality;race focus social;conceptualization race fixed;racial categories;race theory;racial categories adopted", "pdf_keywords": "race fairness research;conceptualizations race fairness;race fairness;fairness researchers;algorithmic fairness researchers;fairness research;suggestions algorithmic fairness;algorithmic fairness;algorithmic fairness literature;review algorithmic fairness;race theory sociological;limitations algorithmic fairness;fairness researchers moving;algorithmic fairness methodologies;fairness methodologies;racial categories disciplines;sociological work race;fairness literature light;racial categories;fairness methodologies adopt;fairness literature;fairness research drawing;conceptualizations race;professional topics race;topics race;topics race ethnicity;use racial categories;naturalize racial categories;naturalize racial;racial categories turn"}, "4d10d7c02ce01d71f11c296b09b389c6f20b354b": {"ta_keywords": "data labeling crowdsourcing;pricing crowdsourcing;labeling crowdsourcing;pricing crowdsourcing practice;dynamic pricing crowdsourcing;labeling crowdsourcing shared;crowdsourcing practice efficient;labeling public crowdsourcing;public crowdsourcing marketplaces;crowdsourcing marketplaces;crowdsourcing marketplaces present;crowdsourcing marketplaces crowd;crowdsourcing practice;crowdsourcing;largest crowdsourcing marketplaces;crowdsourcing shared;public crowdsourcing;largest crowdsourcing;project largest crowdsourcing;crowdsourcing tutorial;crowdsourcing tutorial present;crowdsourcing shared leading;efficient data labeling;data labeling aggregation;efficient label collection;labeling aggregation;industrial expertise crowdsourcing;efficient label;crowd performers annotating;expertise crowdsourcing", "pdf_keywords": ""}, "191169031c7646c02ecb1aaa9c8a6b6e05009730": {"ta_keywords": "graphene rgo ng;graphene rgo;doped graphene rgo;graphene oxide ni;reduced graphene oxide;constructed graphene oxide;graphene oxide;oxide doped graphene;dielectric loss graphene;ng constructed graphene;loss graphene film;graphene film;graphene oxide doped;graphene;graphene spheres graphene;graphene film gf;synthesis hollow graphene;doped graphene;reduced graphene;loss graphene;graphene spheres;gf reduced graphene;constructed graphene;spheres graphene;hollow graphene;hollow graphene spheres;graphene sheets;spheres graphene sheets;electromagnetic absorption rgo;graphene sheets exhibits", "pdf_keywords": ""}, "19b6e7158ee4f13caa004a0b6c6a6e0ef965ea8f": {"ta_keywords": "data robust speech;challenges robust speech;chime challenges robust;robust speech recognition;robust speech;robust automatic speech;modelling chime challenges;speech recognition everyday;modelling chime;statistical modelling chime;speech recognition development;speech recognition;chime challenge;chime challenges;chime challenge series;automatic speech;automatic speech recognition;overview chime series;speech recognition use;discriminative acoustic language;overview chime;acoustic language modelling;chime;chime series;chime series including;discriminative acoustic;data training;simulated data training;acoustic language;provides overview chime", "pdf_keywords": ""}, "53f6c82035d43a19b9c8be0de651cae25bdd4bda": {"ta_keywords": "transcripts automatic speech;automatic transformation spoken;spoken word transcript;automatic speech;transcripts automatic;make transcripts automatic;transformation spoken word;speaking style transformation;automatic speech recognition;speech recognition results;transcript style language;speech recognition;word transcript style;transcript style;clean transcriptstyle text;word transcript;clean transcriptstyle;make transcripts;attempting make transcripts;transcriptstyle;transcriptstyle text;words wfst based;product clean transcriptstyle;disfluency deletion transformation;transcripts;transcriptstyle text paper;framework speaking style;transcript;dropped words wfst;speaking style", "pdf_keywords": ""}, "821532ecef5bc2252823b190c35f1e4c44ddc41c": {"ta_keywords": "word alignment parallel;embeddings parallel corpora;alignment parallel corpora;learning parallel text;learning translation lexicons;parallel corpora;translation lexicons cross;tuning parallel text;contextualized word embeddings;tuning embeddings parallel;lexicons cross lingual;learning translation;parallel corpora wide;embeddings parallel;word alignment;parallel text objectives;translation lexicons;outputs word alignment;word embeddings;alignment parallel;unsupervised learning parallel;word alignment task;learning parallel;word alignment fine;including learning translation;parallel corpora paper;parallel text;word embeddings derived;trained language models;explicit training parallel", "pdf_keywords": "word alignment parallel;alignment parallel corpora;embeddings parallel corpora;multilingual word aligners;learning translation lexicons;learning translation;translation lexicons cross;parallel corpora;abstract word alignment;word alignment;effectively extract alignments;translation lexicons;tuning embeddings parallel;parallel corpora wide;parallel text objectives;lexicons cross lingual;\ufb01netuning parallel text;word alignment fine;including learning translation;train multilingual word;parallel corpora zi;embeddings parallel;language processing tools;alignment parallel;transfer language processing;improve alignment quality;fine tuning embeddings;extract alignments;word aligners;language pairs"}, "2b4edb9515a26561ea3f9ee2a63a506721c8369e": {"ta_keywords": "aspect based sentiment;aspect sentiments reviews;dataset aspect prediction;aspect prediction;aspect based sentiments;aspect prediction used;aspect sentiments;sentiments reviews make;use aspect sentiments;sentiments reviews;aspects sentiments;obtain aspects sentiments;sentiment analysis scientific;sentiments obtained review;sentiment analysis;aspects sentiments entire;sentiments entire dataset;based sentiment analysis;based sentiment;reviews comments paper;reviews comments;reviews make intriguing;aspect based;analysis scientific reviews;peer reviews comments;based sentiments obtained;reviews able extract;sentiments obtained;sentiment;dataset aspect", "pdf_keywords": "dataset aspect prediction;aspect based sentiment;aspect prediction;aspect prediction used;aspect sentiments;aspects sentiments;aspects sentiments entire;aspect working dataset;aspect sentiments peer;obtain aspects sentiments;reviewing aspects;sentiment analysis scientific;reviewing aspects used;aspect based;latent aspect sentiments;annotation framework;annotate 500 review;review sentences;annotation;sentiment analysis;sentiments entire dataset;500 review sentences;review sentences levels;aspects present sentence;aspects;learning framework annotate;identified aspect working;sentiment associated identified;associated identified aspect;identified aspect"}, "f6d6c4dd0115386c234a0b027dd38f7aa9d9df2f": {"ta_keywords": "nlp endangered languages;endangered languages documentation;challenges language documentation;nlp endangered;focus nlp endangered;current nlp approaches;nlp practitioners;languages documentation revitalization;communities documentary linguists;nlp approaches tutorial;nlp practitioners work;language documentation;nlp tasks;nlp tasks ultimate;language communities documentary;documentary linguists;nlp approaches;limitations current nlp;specific nlp tasks;motivate nlp practitioners;current nlp;specific nlp;motivate nlp;documentary linguists map;nlp;map specific nlp;languages documentation;documentation revitalization;language documentation showing;challenges language", "pdf_keywords": ""}, "31c53acd2a43dcec4342d9c42d0ffbfbef36e855": {"ta_keywords": "label shift estimation;miscalibration estimation;mlls include calibration;information coarse calibration;characterization likelihood based;recognition datasets;shift estimation;label shift describes;estimation error label;image recognition datasets;image recognition;recognition;recognition datasets analysis;calibration classifier;classifier confusion matrix;loss information coarse;label distribution;label shift;label distribution change;shift estimation paper;characterization likelihood;classifier;error label shift;equivalent mlls;cifar10 image recognition;roughly equivalent mlls;theoretical characterization likelihood;miscalibration estimation error;calibration classifier confusion;view label shift", "pdf_keywords": "label shift estimation;predictors label shift;shift label distribution;estimating label marginal;approaches estimating label;estimating label;shift estimation contributions;label shift;label distribution change;shift label;label shift label;view label shift;shift estimation;label distribution;abstract label shift;label marginal;predictors label;shift estimation saurabh;characterization mlls;label marginal paper;view label;label;theoretical characterization mlls;characterization mlls conclusion;mlls particular choice;equivalent mlls;shelf predictors label;re\ufb02ecting miscalibration estimation;uni\ufb01ed view label;roughly equivalent mlls"}, "ccfaccf36b9cd7c0c05af2285ec90ecf5f51a34c": {"ta_keywords": "optimal capacity relay;optimal relay locations;optimal relay;optimal relay location;formulas optimal relay;allocation source relay;relay location optimal;provides optimal relay;multi relay channel;relays derive optimization;relay channel exponential;optimal placement relay;source relay channel;relay channel;optimal power allocation;constraint source relays;relay node placement;source relays derive;placement relay nodes;relay nodes;capacity relay node;capacity relay;multi relay;relay channel study;multiple relay;relay nodes uniformly;source node relay;formulas multi relay;relays derive;source relay", "pdf_keywords": ""}, "d8d12c922fc571d081bae27c67fcf50cdbb17d90": {"ta_keywords": "lingual transfer learning;historical text summarisation;text summarisation dataset;summarisation model trained;summarisation documents historical;text summarisation documents;summarisation dataset;summarisation documents;standard text summarisation;text summarisation;language summarised corresponding;summarisation dataset consists;techniques propose summarisation;cross lingual historical;language summarised;summarisation model;transfer learning techniques;cross lingual transfer;propose summarisation model;transfer learning;historians digital humanities;summarised corresponding modern;lingual transfer;based cross lingual;lingual historical;cross lingual;trained cross lingual;routine historians digital;historical text;lingual historical modern", "pdf_keywords": "cross lingual summarisation;lingual summarisation modern;lingual summarisation;modern language summarisation;summarising historical text;language summarisation task;text summarisation dataset;summarisation documents historical;historical text summarisation;language summarisation;chinese summarising historical;language summarised corresponding;text summarisation documents;standard text summarisation;summarisation modern;german chinese summarising;language summarised;summarisation documents;text summarisation;chinese summarising;summarisation dataset;summarisation modern modern;summarisation dataset consists;summarisation task standard;summarised corresponding modern;summarising historical;lingual benchmarks task;summarisation task;forms language summarised;task historical text"}, "c4607387ee863d5c5e5dc9f8adfbe7930508e286": {"ta_keywords": "machine learning icml;machine learning society;machine learning journal;conference machine learning;international machine learning;machine learning thank;conference international machine;machine learning;thank machine learning;international conference machine;machine learning volume;icml annual conference;learning icml 2008;best paper icml;field machine learning;learning icml;learning thank machine;icml 2008;conference machine;international machine;conferences icml;icml;conferences icml annual;paper icml;machine;annual conference international;icml 1998;25th international conference;paper icml 1998;best paper", "pdf_keywords": ""}, "7f4fa7c6f16f2965a104fa45071ea0c92b4366fe": {"ta_keywords": "lamina emergent joint;stiffness lamina emergent;emergent joint origami;stiffness lamina;joint origami mortise;varying stiffness lamina;joint origami;emergent joint;lamina emergent;origami mortise tenon;mortise tenon structure;origami mortise;stiffness;varying stiffness;origami;joint;tenon structure;lamina;mortise tenon;tenon;emergent;mortise;structure;varying", "pdf_keywords": ""}, "7634b0cf93169d2a95d4d7193f47f97a61e3b4b2": {"ta_keywords": "behavior diagnostic games;games elicit distinguishable;player traits markov;diagnostic games elicit;eliciting distinguishable behavior;games successfully distinguish;diagnostic games;traits markov decision;elicit distinguishable behavior;interacting machine learning;behavior mutual information;parameterize games;games elicit;successfully distinguish players;distinguishable behavior mutual;learning systems game;varying player traits;personalized human interacting;markov decision processes;eliciting distinguishable;traits human behavior;model varying player;distinguishable behavior paper;information maximization;designing behavior diagnostic;game design eliciting;inferring latent psychological;player traits;design eliciting distinguishable;distinguish players different", "pdf_keywords": "games elicit distinguishable;games designed heuristics;behavior diagnostic games;behavior diagnostic game;parameterize games reward;diagnostic games elicit;diagnostic games;diagnostic game design;parameterize games;reward designed heuristics;games elicit;games reward designed;behavior mutual information;processes parameterize games;comparing games;designed heuristics optimization;diagnostic game;heuristics 1a optimization;comparing games designed;designed heuristics;heuristics optimization;elicit distinguishable behavior;markov decision processes;games reward;mutual information maximization;player traits markov;games designed;heuristics 1a;information maximization;games"}, "1ccf412212873ae1b020762b8b86291e1fb11f65": {"ta_keywords": "datasets crowdsourced audio;crowdsourced audio transcription;crowdsourced audio;benchmark datasets crowdsourced;datasets crowdsourced;data collection crowdsourcing;image classification crowdsourcing;collection crowdsourcing;classification crowdsourcing;classification crowdsourcing standard;collection crowdsourcing applicability;crowdsourcing standard tools;crowdsourcing complex tasks;crowdspeech voxdiy benchmark;crowdsourced;crowdsourcing;crowdsourcing standard;crowdsourcing complex;applicability crowdsourcing;applicability crowdsourcing complex;novel aggregation methods;crowdsourcing applicability;crowdsourcing applicability crowdsourcing;voxdiy benchmark datasets;existing novel aggregation;benchmark datasets;audio transcription evaluation;novel aggregation;audio transcription;designing aggregation methods", "pdf_keywords": "crowdsourced audio transcriptions;datasets crowdsourced audio;dataset crowdsourced audio;crowdsourced audio;crowdsourced audio annotations;synthetic datasets crowdsourced;constructing datasets crowdsourced;datasets crowdsourced;dataset crowdsourced;audio transcriptions resourced;data collection crowdsourcing;audio annotations resourced;voxdiy counterpart crowdspeech;audio annotations;audio transcriptions;voice assistants;synthetic datasets;collection crowdsourcing;semi synthetic datasets;scale dataset crowdsourced;crowdsourcing introduction speech;crowdsourced;collection crowdsourcing introduction;voice assistants siri;areas voice assistants;audio transcriptions novel;crowdspeech russian language;counterpart crowdspeech russian;crowdsourcing;crowdsourcing introduction"}, "3ba529f732d3c4a31e9ce57f1c78ddf911846bf4": {"ta_keywords": "labels weak supervision;recent weak supervision;bottleneck labeling training;weak supervision sources;labeling training;generated weak supervision;noisy supervision sources;labeling training data;weak supervision ws;weak supervision;potentially noisy supervision;learning synthesizing labels;supervision ws approaches;classification sequence tagging;easing bottleneck labeling;supervision sources modular;supervision sources;supervision sources used;noisy supervision;supervision sources second;sequence tagging;bottleneck labeling;labels weak;sequence tagging range;supervision ws;labeling;terms labels weak;machine learning synthesizing;supervision;synthesizing labels", "pdf_keywords": "io bio tagging;bio tagging schema;bio tagging scheme;bio tagging;label model io;tagging schema;tagging schema compare;tagging scheme adopted;wrench community driven;io bio;comparision io bio;wrench community;tagging scheme;tagging;model io bio;wrench;performance label model;label model;initiative comparision io;comparision io;model io;io;schema compare performance;compare performance label;bio;performance label;label;schema compare;schema;source initiative comparision"}, "64bc7fe1c46c4d4106afba4621ff1bd4376c077a": {"ta_keywords": "electrolaryngeal speech enhancement;speech enhancement;speech enhancement based;approach electrolaryngeal speech;electrolaryngeal speech;enhancement based noise;noise reduction statistical;noise reduction;based noise reduction;statistical excitation prediction;excitation prediction evaluation;enhancement based;reduction statistical excitation;hybrid approach electrolaryngeal;excitation prediction;approach electrolaryngeal;based noise;enhancement;statistical excitation;electrolaryngeal;excitation prediction kou;speech;reduction statistical;noise;prediction evaluation hybrid;prediction evaluation;evaluation hybrid;excitation;evaluation hybrid approach;evaluation", "pdf_keywords": ""}, "04db62a14f78f693d6bd14a4803b9b73325b36bb": {"ta_keywords": "detecting fake news;fake news subgraph;knowledge graph news;fake news detection;news subgraph classification;news content relations;news subgraph;relations extracted news;knowledge graph recent;fake news experiments;external knowledge graph;types fake news;existing knowledge graphs;news detection external;fake news existing;knowledge based fake;knowledge graph;knowledge graphs entity;entities extracted news;knowledge graphs;detection external knowledge;graph news item;graphs entity prediction;single knowledge graph;news detection;graph news;news existing approaches;news content;based fake news;news item represented", "pdf_keywords": "detecting fake news;fake news subgraph;news subgraph classification;fake news detection;knowledge graph news;news subgraph;modal fake news;knowledge graph;subgraph knowledge;news detection;subgraph knowledge enhanced;relations extracted news;single knowledge graph;represented subgraph knowledge;knowledge textual content;external knowledge graph;graph news item;knowledge based fake;news detection does;graph news;news detection arxiv;types fake news;knowledge graph addition;subgraph classification;news item represented;extracted knowledge textual;knowledge textual;subgraph classification task;textual content social;news item"}, "9bd6cdae71506eb307507e44df7abe0c285b3ca7": {"ta_keywords": "attentional machine translation;translation addition reranking;translation models experiments;statistical machine translation;machine translation models;machine translation;translation models;machine translation addition;asian translation based;workshop asian translation;neural mt reranking;translation based;translation based syntax;neural attentional machine;translation addition;asian translation;improvement grammatical correctness;improvements lexical;grammatical correctness output;attentional machine;neural attentional;translation;using neural attentional;improvements lexical choice;stating neural mt;opposed improvements lexical;main contributions neural;neural mt;improvement grammatical;contributions neural models", "pdf_keywords": "neural mt reranker;reranking baseline syntax;neural mt reranking;machine translation;machine translation languages;based machine translation;reordering phrases insertion;grammatical structure target;general reordering phrases;reordering phrases;mt reranking baseline;reordering phrases sentence;agreement neural mt;regarding reordering phrases;mt reranking framework;verb agreement neural;structure target sentence;reranking framework accuracy;translation languages;phrases insertion deletion;phrases insertion;reranking baseline;translation languages table;target sentence;structures verb agreement;grammatical structure;transfer correct grammatical;target sentence prominent;neural mt;applying neural mt"}, "e0a0b3438aef008fece5b8bbf76105b470f10f25": {"ta_keywords": "convertible codes enabling;coded data distributed;convertible codes;conversion coded data;coded data;efficient conversion coded;conversion coded;codes enabling efficient;data distributed storage;distributed storage;enabling efficient conversion;coded;codes enabling;storage;efficient conversion;data distributed;codes;conversion;data;convertible;distributed;enabling efficient;enabling;efficient", "pdf_keywords": ""}, "1817c9f0fd8a17e31c65963dd8cee9783059495b": {"ta_keywords": "controllable text content;text content manipulation;controllable text;content manipulation;text content;text;controllable;manipulation;content", "pdf_keywords": ""}, "167adafac25ee108ca99c688cceded8bca710bb1": {"ta_keywords": "constancy increases age;size constancy increases;size constancy;constancy function age;age level experiments;size distance stimuli;size constancy represents;investigators size constancy;increases age presented;age presented evidence;ages tested;range ages tested;size judgment different;relationship size constancy;studies constancy;increases age;constancy increases;distances function age;judgment different distances;experiments differ considerably;age presented;function age level;age yielded inconsistent;size judgment;function age;range ages;studies constancy function;size distance;function age yielded;distance stimuli", "pdf_keywords": ""}, "538466f2a69271617bf4f5b0df4e5fd854c11c35": {"ta_keywords": "sparse graph codes;testing based sparse;adaptive group testing;groups items computational;low complexity decoder;group testing scheme;test group items;complexity decoding;complexity decoder systematic;group testing;complexity decoder;group testing based;saffron sparse graph;computational complexity decoding;complexity decoding algorithm;items computational complexity;methodology robustify saffron;graph codes framework;saffron sparse;low complexity;group testing problem;negative group testing;decoding algorithm;graph codes;robustify saffron reliably;subsets items defective;group subsets items;group testing non;test group;items group defective", "pdf_keywords": "sparse graph codes;probabilistic group testing;adaptive group testing;sparse graph coding;group testing code;testing based sparse;saffron sparse graph;graph coding density;group testing powerful;tools sparse graph;group testing;graph codes framework;modern sparse graph;group testing based;based sparse graph;coding theory group;sparse graph;theory group testing;graph coding theory;graph codes;graph coding;framework group testing;coding density;coding theoretic tools;codes framework group;coding density evolution;introduce saffron sparse;theoretic tools sparse;coding theory;coding theory described"}, "7e358ffc2731a82420d84a7f0bedb155a487c39d": {"ta_keywords": "hop question answering;hop questions composition;hop question composition;hop questions musique;hop questions using;question answering datasets;questions composition;questions composition single;hop questions single;hop questions process;questions single hop;hop questions allows;qa dataset musique;multihop question composition;question composition;question composition single;hop questions;question composition use;musique multi hop;single hop questions;questions musique multi;answering datasets propose;composition single hop;multi hop questions;single hop datasets;hop datasets;questions musique;question answering;dataset musique ans;hops composition", "pdf_keywords": "comprehension qa datasets;qa dataset musique;multihop reading comprehension;multihop qa datasets;multihop reasoning introduction;multihop qa dataset;reasoning introduction multihop;question composition;musique multihop questions;datasets help nlp;hop question composition;qa datasets carefully;multihop reasoning remains;abstract multihop reasoning;dataset musique ans;hop questions make;challenging multihop reading;multihop reasoning;qa datasets designed;information musique multihop;introduction multihop qa;dataset musique;multihop questions single;new multihop qa;help nlp community;genuine multihop reasoning;qa datasets;reading comprehension qa;multihop qa;musique multihop"}, "61cce75554a6d1bb802f26758c3b0ba97de6918d": {"ta_keywords": "graph attention networks;graph attentional;graph attentional networks;graph attention;termed graph attentional;graph graph neural;graph neural;enable graph attention;predicting graph context;graph neural networks;task predicting graph;networks positional embeddings;graphs model trained;predictive graph context;graph context;predicting graph;graph locality;attention networks gat;graph context plugged;attentional networks positional;graph context work;information graph locality;positional embeddings learned;attention networks;networks positional;attentional networks;networks gnns;node classification tasks;predictive graph;classification tasks gnns", "pdf_keywords": "embeddings graph attention;graph attentional;graph attention networks;graph attention;graph attentional networks;termed graph attentional;called graph attention;attention networks positional;positional embeddings graph;attentional networks positional;node positional embeddings;networks positional embeddings;learning node positional;attention networks;learn positional embeddings;attentional networks;attention computation;positional embeddings learning;attention computation gat;gat positional embeddings;embeddings graph;graph inspired positional;positional embeddings develop;gats positional embeddings;information attention computation;positional embeddings;positional embeddings gat;attention;positional embedding;attentional"}, "b81acc013c42796a5eea0fc20cfb04846da3a589": {"ta_keywords": "linguistic annotation backend;linguistic annotation;purpose linguistic annotation;nlp tools;nlp tools new;advances natural language;nlp technology;language processing nlp;natural language processing;multilingual neural networks;processing nlp;adapt nlp tools;linguists upload data;annotation backend;natural language;processing nlp technology;corpus management;linguists upload;ease language documentation;portion documentary linguists;nlp technology based;language documentation process;documentary linguists;language documentation inherently;glossing corpus management;documentary linguists work;using linguists;language documentation;use natural language;glossing corpus", "pdf_keywords": "linguistic annotation backend;linguistic annotation;nlp tools;nlp tools new;language processing nlp;language documentation inherently;language documentation process;nlp technology;language documentation;purpose linguistic annotation;ease language documentation;processing nlp;adapt nlp tools;natural language processing;processing nlp technology;multilingual neural networks;corpus management;glossing corpus management;glossing corpus;natural language;linguists work backend;nlp technology based;introduction language documentation;linguists upload data;advances massively multilingual;annotation backend;language processing;documentary linguists;linguist uploads;documentary linguists work"}, "398a0625e8707a0b41ac58eaec51e8feb87dd7cb": {"ta_keywords": "agents learn abstract;text embodied environments;enables agents learn;scene embodied agents;embodied agents require;agents learn;agent abstract knowledge;embodied agents;learned textworld;concretely alfworld enables;text embodied;embodied environments interactive;learned textworld corresponds;executing concretely alfworld;environments interactive learning;agent abstract;knowledge learned textworld;simulator enables agents;alfworld enables creation;interactive learning kitchen;visual environment alfworld;introducing alfworld;interactive learning;introducing alfworld simulator;embodied environments;alfworld simulator;abstract knowledge learned;learn abstract text;alfworld enables;scene embodied", "pdf_keywords": "generalization embodied tasks;agent textworld;agent textworld zero;trained abstract text;agents complete embodied;complete embodied tasks;embodied tasks;generalization embodied;embodied tasks novel;embodied settings agents;butler agent textworld;embodied tasks alfred;trained abstract;generalization unseen tasks;embodied world conclusion;interact learn abstract;complete embodied;shot generalization embodied;embodied worlds;trained corpora demonstrations;agents trained corpora;unseen embodied settings;interactive text environment;embodied worlds pre;better unseen embodied;agents explore interact;unseen embodied;aligned embodied worlds;embodied;demonstrations scratch embodied"}, "4f7bbcef3d40cafad17936fdf562a121667af1e8": {"ta_keywords": "vessel tree reconstruction;reconstructing vector fields;reconstruct complete vessel;principle reconstructing vector;veins unsupervised method;divergent arteries;pattern divergent arteries;complete vessel trees;fields modelling blood;reconstructing vector;regularization principle reconstructing;vessel filters;vector fields modelling;flow pattern divergent;vector fields based;arteries convergent veins;vessel trees;modelling blood flow;vessel tree;convergent veins unsupervised;vector fields;blood flow pattern;divergent arteries convergent;standard vessel filters;flow orientations;ambiguity flow orientations;complete vessel;flow orientations produced;modelling blood;convergent veins", "pdf_keywords": "divergence prior vessel;vessel tree reconstruction;prior vessel tree;reconstruction combines divergence;divergence constraints robust;convergent veins vessel;divergence constraints;pattern divergent arteries;regularization principle reconstructing;combines divergence constraints;principle reconstructing vector;veins vessel tree;robust curvature regularization;curvature regularization;prior vessel;divergent arteries;reconstructing vector \ufb01elds;vessel tree centerline;divergence prior;reconstructing vector;new geometric regularization;divergence previously ignored;geometric regularization principle;vessel tree;arteries convergent veins;geometric regularization;tree centerline reconstruction;divergent arteries convergent;convergent veins;veins vessel"}, "0431f60546381a9e91fb156236c3c7056f57081f": {"ta_keywords": "singing voice synthesis;generate singing better;generate singing;voice synthesis;learning based singing;based pitch augmentation;singing better qualities;singing quality;pitch augmentation mix;singing quality limited;pitch augmentation;voice synthesis svs;singing voice;singing databases;reasonable singing quality;flexibly generate singing;singing databases demonstrate;singing better;augmentation methods boost;based singing voice;data augmentation methods;mix augmentation neural;augmentation neural systems;based singing;singing;data augmentation;public singing databases;augmentation neural;augmentation mix augmentation;augmentation methods", "pdf_keywords": "augmentation methods boost;singing datasets extensive;singing datasets;data augmentation methods;pitch augmentation mix;data augmentation techniques;based pitch augmentation;augmentation techniques pitch;data augmentation;techniques pitch augmentation;proposed data augmentation;pitch augmentation;public singing datasets;method improve singing;simple data augmentation;different data augmentation;data augmentation policies;improve singing quality;singing databases;improve singing;singing quality;augmentation methods;singing databases demonstrate;augmentation mix augmentation;proposed augmentation methods;singing quality limited;augmentation methods stabilizing;mix augmentation;augmentation mix method;augmentation mix"}, "dbe87b171bfb789e1d22a047aeeee69105e6fd02": {"ta_keywords": "t5 sentence embeddings;bertbased sentence embeddings;sentence embeddings t5;sentence embeddings outperforming;sentence embeddings transfer;sentence embeddings encoder;embeddings outperforming sentence;sentence embeddings utilize;produce sentence embeddings;sentence embeddings;scalable sentence encoders;sentence encoders;embeddings transfer tasks;using sentence embeddings;embeddings utilize t5;embeddings t5 achieves;embeddings t5;outperforms bertbased sentence;sentence encoders pre;outperforming sentence bert;embeddings encoder decoder;embeddings transfer;embeddings encoder;embeddings outperforming;embeddings utilize;extracting t5 sentence;sentence bert reimers;embeddings;t5 encoder decoder;sentence t5 scalable", "pdf_keywords": "sentence encoders t5;tuning sentence encoders;sentence representation transfer;sentence encoders pre;scalable sentence encoders;t5 sentence embeddings;sentence encoders;t5 sentence embedding;contrastive learning effective;sentence transfer tasks;sentence embedding models;sentence embeddings utilize;stage contrastive learning;contrastive learning;sentence embedding model;sentence transfer;tuning encoder st5;contrastive learning recipe;trained t5 encoder;sentence embeddings;sentence transfer sts;encoders pre trained;contrastive learning method;perform sentence transfer;embeddings utilize t5;decoder sentence embedding;sentence embedding;improvement sentence transfer;encoder st5;encoder st5 models"}, "95788ed8affd06c0c2c6159c26ff7c123c4f2e0a": {"ta_keywords": "neural speaker diarization;speaker diarization methods;speaker diarization;diarization speaker wise;end speaker diarization;speaker diarization speaker;diarization speaker;speaker speech activity;neural speaker;estimated speakers speech;speaker wise conditional;speakers neural speaker;speaker wise chain;speakers neural;speaker wise;number speakers neural;speakers speech activities;proposed method speaker;speaker audio recordings;neural diarization eend;method multi speaker;end speaker;neural diarization;end neural diarization;speakers speech;method speaker speech;speaker speech;method speaker;speaker audio;multi speaker", "pdf_keywords": "speaker diarization methods;speaker diarization;end speaker diarization;variable speaker scenarios;speaker wise conditional;variable speaker;method variable speaker;speaker scenarios;speaker wise;method multi speaker;proposed speaker wise;end speaker;neural diarization method;variable number speakers;speaker audio recordings;multi speaker;speaker issue novel;speaker audio;end neural diarization;speaker scenarios conclusions;number speaker;neural diarization;endto end speaker;diarization methods;novel speaker wise;proposed speaker;speaker;issue novel speaker;speakers outperforms state;number speaker issue"}, "84908a28a03d0d7c467d9556ed36f0e416de7171": {"ta_keywords": "semantic parsing grammars;recognition written utterances;meaning recognition written;semantic parsing;algorithm semantic parsing;written utterances goal;utterances goal computer;parsing grammars;parsing grammars automatically;parsing;written utterances;utterances goal;utterances;description given utterance;approaches meaning recognition;meaning recognition;utterance;given utterance;given utterance utilizes;context free grammars;grammars key idea;grammars automatically;novel algorithm semantic;utterance utilizes;free grammars;free grammars key;grammars;algorithm semantic;recognition written;grammars key", "pdf_keywords": ""}, "b46be3ac246499655cc442e93c5878e7a9640ae3": {"ta_keywords": "timeline construction;timeline construction compare;produced timelines;timelines using;produced timelines using;timelines using tree;timeline concrete representation;timelines;news identifying temporally;define timeline;hand produced timelines;timeline concrete;timeline;unsupervised methods timeline;methods timeline construction;events episodes sagas;sagas understanding news;related story sequences;events news interfaces;story sequences;related events news;representation saga longrunning;identifying temporally related;temporally related story;news interfaces;episodes sagas understanding;methods timeline;previous events episodes;events episodes;events news", "pdf_keywords": ""}, "8da992b611df508b1803f66ffa53bd1fb741a76c": {"ta_keywords": "question answer hierarchies;generating question answer;reading comprehension datasets;question answer game;classify questions;classify questions existing;answer hierarchies use;existing reading comprehension;challenging text generation;answer hierarchies novel;qa hierarchies crowdsourced;answer hierarchies;text generation task;comprehension datasets;generated qa hierarchies;question taxonomy loosely;question taxonomy;comprehension datasets general;questions existing reading;1978 classify questions;reading comprehension;text generation;hierarchies crowdsourced;generation task;generated qa;hierarchies crowdsourced experiments;answer game student;using question taxonomy;challenging text;qa hierarchies", "pdf_keywords": "question answer hierarchies;text generation task;challenging text generation;generated qa hierarchies;answer hierarchies;answer hierarchies novel;text generation;qa hierarchies crowdsourced;answering trained combination;classify questions;generation task;generation general questions;classify questions existing;generated qa;answering trained;question taxonomy;reading comprehension datasets;document modelgenerated hierarchy;novel text generation;question taxonomy loosely;input document hierarchy;generation task converts;qa hierarchies;comprehension datasets;hierarchies crowdsourced;1978 classify questions;existing reading comprehension;document hierarchy;questions representative;hierarchies crowdsourced experiments"}, "2eea63f896deed47cc0c0000e1482ec5c860fd0b": {"ta_keywords": "detection controversy detection;controversy detection task;controversy detection;detecting controversial posts;controversy detection previous;level controversy detection;controversy detection controversy;network controversy detection;detecting controversial;controversial posts web;sentiment information;sentiment information integrate;controversial posts;relationship sentiment information;information fusion graph;measuring influence news;semantic information fusion;news alleviating polarized;structure relationship sentiment;scale structural semantic;fusion graph convolutional;structural semantic information;detection controversy;graph convolutional network;convolutional network controversy;topics comments information;posts web social;relationship sentiment;information fusion;structural semantic", "pdf_keywords": ""}, "881ce19455a9923e4798e9d77d2d8623ca9d2e03": {"ta_keywords": "clustering speech recognition;estimation clustering speech;bayesian predictive classification;clustering speech;distribution bayesian predictive;speech recognition vbec;speech recognition;variational bayesian estimation;predictive distribution bayesian;bayesian predictive distribution;bayesian estimation clustering;bpc speech recognition;speech recognition experimentally;robust data sparseness;based bayesian predictive;data sparseness;named variational bayesian;bayesian predictive;variational bayesian;recognition address sparse;problem speech recognition;sparse data;predictive classification;proposed total bayesian;robust classification;total bayesian framework;introduce robust classification;vb posterior distributions;posterior distributions vb;sparseness", "pdf_keywords": ""}, "6d9603be7e79ff33677327a0edd5bd3f7da6347b": {"ta_keywords": "\u968e\u5c64\u7684\u30d5\u30ec\u30fc\u30ba\u30d9\u30fc\u30b9\u7ffb\u8a33\u306b\u304a\u3051\u308b\u30d4\u30dc\u30c3\u30c8\u7ffb\u8a33\u624b\u6cd5\u306e\u5fdc\u7528 \u8a00\u8a9e\u7406\u89e3\u3068\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3 \u7b2c6\u56de\u96c6\u5408\u77e5\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0;\u968e\u5c64\u7684\u30d5\u30ec\u30fc\u30ba\u30d9\u30fc\u30b9\u7ffb\u8a33\u306b\u304a\u3051\u308b\u30d4\u30dc\u30c3\u30c8\u7ffb\u8a33\u624b\u6cd5\u306e\u5fdc\u7528 \u8a00\u8a9e\u7406\u89e3\u3068\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3;\u8a00\u8a9e\u7406\u89e3\u3068\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3 \u7b2c6\u56de\u96c6\u5408\u77e5\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0;\u968e\u5c64\u7684\u30d5\u30ec\u30fc\u30ba\u30d9\u30fc\u30b9\u7ffb\u8a33\u306b\u304a\u3051\u308b\u30d4\u30dc\u30c3\u30c8\u7ffb\u8a33\u624b\u6cd5\u306e\u5fdc\u7528;\u7b2c6\u56de\u96c6\u5408\u77e5\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0;\u8a00\u8a9e\u7406\u89e3\u3068\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3", "pdf_keywords": ""}, "81af4e14050c410e2afee226be583088a9791ddf": {"ta_keywords": "learn argument embeddings;argument embeddings based;argument embeddings;argument embeddings context;argument embeddings according;bias argument embeddings;semantic role induction;embeddings unsupervised semantic;unsupervised semantic role;representations unsupervised semantic;embeddings based dependency;embeddings according dependency;embeddings context explicitly;embeddings context;learn argument;embeddings according;unsupervised semantic;embeddings unsupervised;incorporating dependency relations;embeddings based;embeddings;state art embeddings;role induction conll;semantic role;task multiplicative representations;dependency relations multiplicative;dependency relations;multiplicative representations unsupervised;role induction;representations unsupervised", "pdf_keywords": ""}, "ec99cf93ef22a0c0d669abe90c9509f642b2cf69": {"ta_keywords": "downsampling near semantic;segmentation learning downsampling;learning downsampling near;semantic boundaries adaptive;content adaptive downsampling;boundaries adaptive sampling;downsampling technique learns;sampling improving;learning downsampling;uniform sampling improving;adaptive sampling;sampling gives segmentation;semantic boundaries target;good semantic segmentation;semantic segmentation critical;adaptive downsampling;near semantic boundaries;sampling;downsampling near;sampling locations;segmentation better;favor sampling locations;gives segmentation better;adaptive sampling gives;segmentation better quality;adaptive downsampling technique;downsampling;semantic boundaries;semantic segmentation;sampling locations near", "pdf_keywords": "content adaptive downsampling;content adaptive sampling;sampling ground truth;downsampling technique learns;sampling gives segmentation;adaptive sampling ground;sampling improving;better segmentation quality;aware adaptive downsampling;semantic boundaries target;uniform sampling improving;semantic boundaries costperformance;hypothesize better segmentation;segmentation better quality;gives segmentation better;objects uniform downsampling;classes adaptive sampling;adaptive sampling;better segmentation;near semantic boundaries;segmentation better;semantic boundaries;adaptive downsampling;adaptive sampling gives;segmentation quality;driven semantic boundaries;learning content adaptive;sampling ground;sampling;semantic boundaries fig"}, "48e8e8085907192d501eb2bcc582035e90431a2f": {"ta_keywords": "lingual sequence tagging;sequence tagging;deep hierarchical recurrent;sequence tagging extend;deep gated recurrent;hierarchical recurrent neural;recurrent neural network;cross lingual sequence;network sequence tagging;hierarchical recurrent;sequence words model;sequence tagging scratch;recurrent neural;encode morphology context;deep hierarchical;lingual sequence;predict tags;predict tags present;encode morphology;layer predict tags;tags present deep;task cross lingual;word levels encode;present deep hierarchical;neural network sequence;levels encode morphology;words model;gated recurrent;gated recurrent units;cross lingual", "pdf_keywords": "lingual sequence tagging;sequence tagging tasks;sequence tagging based;multiple sequence tagging;sequence tagging;model sequence tagging;tagging tasks language;multi task crosslingual;network sequence tagging;task cross lingual;cross lingual sequence;tagging ner english;sequence tagging scratch;language pos tagging;tagging chunking ner;deep hierarchical recurrent;lingual sequence;task multiple languages;sequence tagging evaluate;tagging tasks;task crosslingual;recurrent neural network;tagging chunking;multiple languages ner;chunking ner english;hierarchical recurrent;hierarchical recurrent neural;tagging ner;tasks languages;cross lingual"}, "5431098723db5858c4553f0259921cbbdd6492d5": {"ta_keywords": "covid 19 languages;19 translation initiative;translation initiative covid;languages tico 19;19 data translated;data translated languages;development data translation;19 languages;languages team targeting;translation initiative;translation memories tmxs;languages team;forming translation initiative;translated languages;languages tico;tico 19 translation;data translation;translated languages represented;data translated;information covid 19;localizers languages tico;data translation memories;initiative covid 19;19 languages addition;pivot languages team;covid 19 data;different languages;covid 19 tico;collaborators forming translation;19 translation", "pdf_keywords": "coronavirus quarantine coronavirus;quarantine coronavirus;coronavirus quarantine;quarantine contagious coronavirus;quarantine 2019 coronavirus;coronavirus precautions coronavirus;coronavirus outbreak coronavirus;precautions coronavirus;coronavirus pandemic coronavirus;outbreak coronavirus pandemic;coronavirus precautions;coronavirus pandemic;coronavirus news coronavirus;coronavirus outbreak;coronavirus 2019;coronavirus exposure epidemic;pandemic coronavirus;outbreak coronavirus outbreak;outbreak coronavirus;2019 coronavirus 2019;outbreak outbreak coronavirus;quarantine coronavirus sos;medicines coronavirus news;protection coronavirus quarantine;epidemic coronavirus exposure;precautions coronavirus prevention;coronavirus 2020;coronavirus epidemic coronavirus;coronavirus 2019ncov 2020;epidemic coronavirus"}, "a72e732f2d11075aa0103b72b4f9884ddcaaaa85": {"ta_keywords": "learnability inductive logic;learnability logic programs;efficient learnability logic;polynomial learnability inductive;learnability logic;inductive logic programming;programming polynomial learnability;learnability inductive;logic programming polynomial;polynomial learnability;efficient learnability;years efficient learnability;learnability results exist;learnability results;practice inductive logic;negative learnability results;inductive logic;learnability;negative learnability;positive negative learnability;logic programs studied;logic programming methods;logic programs;logic programming;logic programs closely;classes logic programs;programming polynomial;practice inductive;inductive;classes logic", "pdf_keywords": ""}, "015dc5b71894dd4d05e7668d015e545ab2e162ba": {"ta_keywords": "speech processing toolkit;kaldi automatic speech;automatic speech;text speech e2e;unified espnet asr;text speech;source speech processing;speech recognition asr;espnet toolkit;open source speech;speech e2e tts;toolkit espnet;toolkit espnet toolkit;automatic speech recognition;recognition asr toolkit;toolkit named espnet;espnet toolkit supports;speech processing;speech recognition;end text speech;asr toolkit;learning asr tts;espnet asr;e2e tts toolkit;speech e2e;espnet tts extension;evaluation comparison toolkits;tts toolkit;processing toolkit espnet;tts asr based", "pdf_keywords": "toolkit espnet tts;text speech toolkit;speech e2e tts;speech toolkit;text speech e2e;speech processing toolkit;speech toolkit tomoki;e2e tts toolkit;asr toolkit espnet;espnet tts extension;espnet tts unified;kaldi automatic speech;tts toolkit;automatic speech;end text speech;espnet tts;text speech;espnet toolkit;e2e tts models;tts asr based;speech e2e;source speech processing;learning asr tts;toolkit named espnet;toolkit espnet;named espnet tts;toolkit espnet toolkit;espnet toolkit supports;espnet asr;tts fastspeech provides"}, "3122a2d7799ba585b993e432b3deb47659b3f3c1": {"ta_keywords": "answering models;question answering lfqa;answering models recently;question answering;form question answering;question answering models;generated answer quality;text generation tasks;answer quality easily;answer quality;paraphrased form training;generated answers;task long form;generated answers actually;text generation;task long;generation tasks;answering lfqa involves;generate paragraph;answering;generate paragraph length;trends generated answers;used text generation;answering lfqa;paragraph length answer;using generate paragraph;paraphrased form;questions occur paraphrased;lfqa paper task;evaluations used text", "pdf_keywords": "retrieval generation quality;retrieval generation;retrieval augmented generation;question answering dataset;retrieval;retrievals;effect retrieval generation;question answering;generated answer quality;attention contrastive retriever;retrieval augmented;retriever learning;present retrieval augmented;retriever learning achieve;contrastive retriever learning;present retrieval;conclusion present retrieval;relies sparse attention;answering dataset;sparse attention;performance eli5 lfqa;answer quality;answering dataset analysis;sparse attention contrastive;form question answering;usage retrievals;model usage retrievals;human evaluation task;performance eli5 long;retrievals section"}, "15d643f4c27d373aa46f26a760051e76fde81dc2": {"ta_keywords": "answer entities train;text answer entities;annotated question entities;answer entities;question answering knowledge;models question answering;answering knowledge graphs;question entities;answering knowledge;question answering;question entities supplied;resolution question answering;entities train;question answering using;task entity resolution;question text answer;end entity resolution;entity resolution er;trivial task entity;e2e trained models;end entity;entity resolution;entities train delivers;knowledge graphs kgqa;e2e learning kgqa;entities;task entity;end end entity;entity;knowledge graphs", "pdf_keywords": "answer entities;entity resolution inference;answering knowledge graphs;reach answer entities;question answering knowledge;end entity resolution;resolution question answering;models question answering;notably entity resolution;answer entities vector;answering knowledge;entity resolution er;entity resolution;perform entity resolution;question answering;entity resolution component;end entity;question semantic parsing;learns perform entity;answering using knowledge;end end entity;knowledge graphs kgqa;entities mentioned;entities;knowledge graph kgqa;entity;entity resolution question;end learning weakly;semantic parsing component;inference end end"}, "5f9d8fe21efb3c2b241427869a333472ab09a22d": {"ta_keywords": "recommending music crawling;music crawling web;filtering recommending music;web collaborative filtering;recommending music;collaborative filtering;music crawling;collaborative filtering recommending;filtering recommending;crawling web;recommending;web collaborative;music;web;collaborative;crawling;filtering", "pdf_keywords": ""}, "ea6547e877c1cc3d37229a6f488ac04e9a11de18": {"ta_keywords": "docking predictions complex;docking predictions;protein interfaces performed;protein interfaces;interface water predictions;water positions protein;docking models indicating;accurate modelling protein;submitting docking predictions;protein protein interfaces;docking models;quality docking models;docking models good;docking performance;models good docking;interface water positions;interfacial water molecules;quality docking;docking;docking performance se;interface water;protein capri target;good docking performance;modelling protein;submitting docking;best interface water;high quality docking;positions interfacial water;predicted interactions capri;water predictions achieved", "pdf_keywords": ""}, "fd1b59d22eb1fb32e0360d4fcbe58dc4ebb25af9": {"ta_keywords": "detection audio deepfakes;deep fakes survey;research audio deepfakes;fake news detection;detecting deepfakes evaluation;audio deepfakes created;deepfakes particularly detection;methods audio deepfakes;creating detecting deepfakes;audio deepfakes;detecting deepfakes;deep fakes;audio deepfakes particularly;generation video deepfakes;detection including fake;deepfakes deep fakes;generation fake content;text deepfakes generation;networks gan;video deepfakes;audio deepfakes deep;fake content;video deepfakes text;deepfakes generation methods;adversarial;networks gan convolutional;gan convolutional neural;deepfakes evaluation;cnn;cnn deep", "pdf_keywords": "audio deepfake generation;generation video deepfakes;detection audio deepfakes;research audio deepfakes;audio deepfake survey;audio deepfakes particularly;audio deepfake;audio deepfakes;deepfake generation detection;focused audio deepfake;deepfake generation;video deepfakes;deepfakes particularly detection;deepfakes focusing audio;video deepfakes particular;focus video deepfakes;audio deepfakes focusing;focusing audio deepfake;generative image modeling;generative image;particularly detection audio;deepfake survey focused;proposed framework stylegan;generation video;deepfakes particular generation;deepfake survey;particular generation video;unconditional generative image;detection audio;generative"}, "86b91922923b03c66497accfa88c638299fc8d26": {"ta_keywords": "variational encoder decoder;space variational encoder;variational encoder;variables variational encoder;encoder decoder trained;encoder decoder;encoder decoder msved;multi space variational;encoder;decoder trained semi;latent variables variational;decoder trained;space variational;decoder msved;decoder;decoder msved method;semi supervised;variational;trained semi supervised;variables variational;task sigmorphon 2017;shared task sigmorphon;task sigmorphon;semi supervised fashion;discrete latent variables;latent variables;supervised;sigmorphon 2017;continuous discrete latent;sigmorphon 2017 discuss", "pdf_keywords": ""}, "f35a01c1e5d5375453c39e6161526633492fb574": {"ta_keywords": "distributed data storage;archival storage systems;data storage;reliability archival storage;data storage systems;data storage methods;storage systems data;latency mds queue;archival storage;study data storage;data storage additionally;data replication;codes storage frequently;storage systems;improved reliability archival;storage methods;mds queue analysing;storage frequently;mds queue analyse;data latency;data replication use;simple data replication;evolving data storage;storage frequently accessed;performance erasure codes;codes mds queue;codes storage;hot data latency;mds queue;reliability archival", "pdf_keywords": "performance mds queues;mds queue analytically;mds queue analysing;mds queues;mds queue analyse;mds queue understand;mds queue;latency performance mds;queue analysing latency;queueing theory using;queueing theory;compare queueing theoretic;queueing theoretic viewpoint;2013 mds queue;mds queues present;queueing theoretic;distributed data storage;compare queueing;queueing;queue analysing;understand compare queueing;framework mds queue;queue analytically characterize;queue analyse;queue analytically;data storage systems;mds queues illustrated;latency performance erasure;data storage;queues"}, "9b71542ef5d5178041048b9a330309053bb0bcfc": {"ta_keywords": "speech separation enhancement;novel speech separation;domain speech separation;performance speech separation;speech separation;speech separation input;paradigm speech separation;synthesize separated speech;speech separation usually;separated speech;separated speech high;separation enhancement model;separation enhancement;speech mixture using;speech quality interference;speech mixture;separation input mixtures;high speech quality;speech resynthesized deep;improved performance speech;separation enhancement related;domain speech;separation input;target speech resynthesized;speech quality;speech high speech;speech resynthesized;truth speech mixture;cocktail party discretization;separation", "pdf_keywords": "speech separation model;speech separation enhancement;domain speech separation;novel speech separation;speech speech separation;performance speech separation;speech separation input;speech separation;paradigm speech separation;speech separation usually;speech mixture using;speech mixture;domain speech;separation enhancement model;separation input mixtures;improved performance speech;speech frontend related;separation enhancement;speech frontend;separation model;truth speech mixture;separation input;paradigm speech frontend;discretization synthesis;separation model sequence;time domain speech;discretization synthesis alternative;convert paradigm speech;performance speech;speech speech"}, "79ab3a0d6dc5d6fd3b466ea2814fdbb93a3672d0": {"ta_keywords": "inference explanation regeneration;multi hop inference;hop inference;textgraphs 2021 shared;textgraphs 2021;textgraphs;hop inference explanation;inference;explanation regeneration;task multi hop;inference explanation;regeneration;hop;2021 shared task;multi hop;task multi;shared task multi;task;shared task;explanation;2021 shared;2021;multi;shared", "pdf_keywords": ""}, "da10c4bc1de7b9b7ddbb21d70ff5092a15cb866f": {"ta_keywords": "domain adaptation;supervised transductive approache;problem domain adaptation;supervised transductive;adaptation protein extraction;subproblem domain adaptation;transfer learning;domain adaptation protein;problem transfer learning;labeled target data;transfer learning particular;support vector machines;art supervised transductive;state art supervised;protein extraction;vector machines maximum;pseudo label based;machines maximum entropy;supervised;protein extraction define;vector machines;label based rescaling;maximum entropy models;learning particular subproblem;labeled target;novel maximum entropy;maximum entropy based;label based;labeled;pseudo label", "pdf_keywords": ""}, "9db77e925015ad02efc9beeab233bedbfe04e4b7": {"ta_keywords": "accelerating reinforcement learning;challenging reinforcement learning;policy gradient;reinforcement learning rl;policy gradient es;popular policy gradient;evolution strategy benchmark;reinforcement learning directional;reinforcement learning;challenging reinforcement;strategy solving dynamical;objective reinforcement learning;accelerating reinforcement;smoothing evolution strategy;objective reinforcement;promise challenging reinforcement;evolution strategy;smoothing evolutionary strategy;evolution strategy es;strategy benchmark rl;gradient es approaches;accelerate rl training;optimal strategy solving;learning rl optimal;reinforcement;learning rl tasks;variation reward function;fluctuation accelerating reinforcement;evolutionary strategy dgs;rl optimal strategy", "pdf_keywords": "blackbox optimization algorithms;popular policy gradient;policy gradient es;blackbox optimization;policy gradient;optimize functions gradient;gradient es approaches;family blackbox optimization;smoothing evolutionary strategy;evolution used optimize;variation reward function;gaussian smoothing evolutionary;variation reward;gradient estimates high;search direction policy;provide gradient estimates;accelerate rl training;gradient information;gradient estimates;smoothing evolutionary;functions gradient information;gradient estimators suggest;faster convergence baselines;evolutionary strategy dgs;nonlocal search direction;evolutionary strategy;gradient estimators;gradient information inaccessible;carlo type gradient;scale variation reward"}, "af0aa62d243c761b56a83369bc9b1f75805003cf": {"ta_keywords": "structured text networks;text corpus labeled;corpus labeled directed;representing text corpus;text corpus;text networks;walk structured text;corpus labeled;labeled directed graph;graph based similarity;corpus;words use supervised;derived dependency parsing;dependency parsing;text networks cmu;walks derive similarity;similarity measure words;structured text;nodes represent words;task learning walk;words weighted edges;dependency parsing given;parsing given graph;labeled directed;based similarity measure;learning walk structured;similarity measure based;derived similarity measure;represent words weighted;directed graph nodes", "pdf_keywords": ""}, "f765b23f0b0d2a196bc0fe562ad24278d0c9cee4": {"ta_keywords": "eve gradient based;training deep;locally eve gradient;adaptive gradient methods;gradient methods stochastic;adaptive gradient;tasks adaptive gradient;adam popular methods;learning rate locally;training deep neural;adaptive learning rates;eve gradient;deep neural;outperforms adam;methods training deep;networks like convolutional;learning rates;deep neural networks;globally adaptive learning;adapts learning rate;recurrent neural networks;learning rate parameter;adam;neural networks like;neural networks image;neural networks language;learning rate;algorithm adapts learning;adjust learning rate;optimization adjust learning", "pdf_keywords": ""}, "ccbc17d42f2b260079eee702fd97a75de705d8ac": {"ta_keywords": "generates threeword phrases;phrases predetermined syntactic;threeword phrases predetermined;syntactic structures decoupling;predetermined syntactic structures;search synthesis;phrases predetermined;generates threeword;predetermined syntactic;decomposition search synthesis;synthesis decomposition search;successfully generates threeword;vectors matched words;syntactic;syntactic structures;search synthesis phase;threeword phrases;text vector decomposed;synthesis decomposition;matched words vocabulary;structures decoupling task;task phases synthesis;monolingual setting;matched words;words vocabulary using;decomposition search;synthesis;vocabulary using;monolingual;kind monolingual setting", "pdf_keywords": ""}, "75b13e7131997ff6fd21325d68a2222d2c1b7157": {"ta_keywords": "beamforming speech separation;speech separation enhancement;speech enhancement separation;neural beamforming speech;spectral separation beamforming;separation beamforming based;speaker separation model;reverberant speech enhancement;beamforming speech;speech separation;speaker separation;separation beamforming;use speaker separation;speech enhancement;frame neural beamforming;sequential neural beamforming;neural beamforming;separation neural networks;neural networks separation;separate real recordings;challenging reverberant speech;neural separation multi;enhancement separation tasks;sequence neural separation;based spectral separation;separation multi frame;neural beamforming alternates;speech recognition;beamforming based;beamforming alternates neural", "pdf_keywords": ""}, "00b874f8346cedadc2a6366c4b72e60140f99556": {"ta_keywords": "trained paraphrase pairs;semantic textual similarity;2016 trained paraphrase;trained paraphrase;textual similarity;paraphrase pairs using;phrase word embeddings;word embeddings;word embeddings wieting;textual similarity sts;paraphrase pairs;english semantic textual;semantic textual;similarity sts task;network english semantic;paragram phrase word;task semeval2016 competition;attention based convolutional;neural network english;paraphrase;textual;similarity;similarity sts;english semantic;embeddings wieting;paragram phrase;attention based input;embeddings wieting et;using paragram phrase;embeddings", "pdf_keywords": ""}, "43c844c30765f3fa25bfabd83490ef826b9ceca1": {"ta_keywords": "adversarial misspellings robust;combating adversarial misspellings;adversarial misspellings;adversarial spelling mistakes;combat adversarial spelling;adversarial spelling;misspellings robust;misspellings robust word;classifier outperforming adversarial;spell checkers trained;combating adversarial;adversarial training shelf;downstream classifier bert;outperforming adversarial;robust word recognition;classifier bert;outperforming adversarial training;adversarial training;trained recognize words;adversarial;robustness downstream classifier;adversarially chosen character;adversarially;combat adversarial;spell checkers;classifier bert model;single adversarially;recognize words corrupted;word recognition;75 combating adversarial", "pdf_keywords": "combating adversarial misspellings;adversarial misspellings;adversarial misspellings robust;adversarial spelling mistakes;combat adversarial spelling;adversarial spelling;combating adversarial;adversarially;augmentation adversarial;adversarial training;adversarial training fourth;augmentation adversarial training;adversarial;combat adversarial;misspellings robust;misspellings robust word;adversarially chosen 1character;adversarially chosen;methods faced adversarially;robust adversarial;50 adversarial training;69 adversarial training;models robust adversarial;data augmentation adversarial;adversarial attacks incorporating;adversarial attacks;robust adversarial attacks;restored 69 adversarial;defenses bert models;64 adversarially"}, "25ae911c13da7ef9def56ee30170920ebd48a668": {"ta_keywords": "predicting argument argument;predicting argument;ranking arguments topic;web arguments convincingness;predicting convincingness web;ranking arguments;computational argumentation;computational argumentation investigate;convincingness web arguments;convincing ranking arguments;tasks predicting argument;arguments topic based;argumentation investigate;convincingness web;argumentation;classification pair arguments;web arguments;analyzing predicting convincingness;argumentation investigate qualitative;field computational argumentation;arguments convincingness analyzing;arguments topic;predicting convincingness;topic based convincingness;arguments 32 topics;relation classification;arguments using bidirectional;arguments having stance;problem relation classification;bidirectional lstm propose", "pdf_keywords": ""}, "b48f0652605f981b5d407496aba3d9756725264f": {"ta_keywords": "decision based preferences;trust ai systems;preferences computer science;decisions study preferences;trust ai;people trust ai;widely preference handling;decision makers properties;based preferences;based preferences optimization;preferences optimization criteria;computer science ai;cp net formalism;preference handling;ai systems need;preference handling community;net formalism;ai;preferences optimization;study preferences computer;ai systems;net formalism convenient;preferences used widely;criteria decision makers;model preferences;science ai;widely preference;decision ethical;decisions study;preference", "pdf_keywords": ""}, "a636768c2fc6cadccd8bb4d704f651dd54dad395": {"ta_keywords": "emotion recognition indonesian;emotion indonesian conversational;recognition indonesian conversational;analysis emotion indonesian;indonesian conversational speech;emotion indonesian;classifies indonesian speech;recognition analysis emotion;indonesian speech;svm classifies indonesian;emotion recognition;indonesian conversational;study emotion recognition;emotional utterances;emotion based acoustic;speech terms emotion;indonesian speech terms;emotional utterances importance;conversational speech recognition;colorful emotional utterances;recognition indonesian;speech recognition;speech recognition analysis;analysis emotion;conversational speech;classifies indonesian;acoustic features construct;features construct corpus;emotion based;emotional aspect human", "pdf_keywords": ""}, "f664e6635d0514b0cb398a713f08bab90b4a3d81": {"ta_keywords": "topics sparse word;sparse word graphs;documents topics sparse;topics sparse;correlations topic models;statistical topic models;topic models;capturing word correlations;sparse word;topic models work;topic models latent;word graphs scalable;word correlations;word correlations topic;word graphs;called sparse word;word graphs statistical;topics semantically meaningful;topics semantically;models latent dirichlet;ap corpus;words representation documents;bag words representation;documents topics;experiments ap corpus;summarize large document;modeling word corre;sparse mod eling;latent dirichlet al;corpus", "pdf_keywords": ""}, "bd1bdb3c5f28001a4cee92c0e1669512d0f06a35": {"ta_keywords": "heaps law generalized;heap law generalized;derivation heaps law;derivation heap law;heaps law;heap law;law generalized zipf;generalized zipf law;zipf law reproduce;simple derivation heap;zipf law;derivation heaps;zipf law previously;derivation heap;formal derivation heaps;generalized zipf;heap;heaps;zipf;law generalized;simple formal derivation;formal derivation;law reproduce simple;simple derivation;law reproduce;derivation;law;reproduce simple formal;law previously;published russian", "pdf_keywords": "heap law generalized;heaps law generalized;heap law;generalized zipf law;heaps law;derivation heap law;derivation heaps law;law generalized zipf;words derivations big;original zipf law;number unique words;zipf law;zipf law previously;zipf law probability;generalized zipf;words derivations;zipf law leonid;unique words;unique words ex;text words derivations;frequent word wi;word rank;heaps;proportional word rank;according original zipf;derivation heaps;heap;word rank pi;word wi inversely;simple derivation heap"}, "a9b9404962760731d6d2fc2ecbc6da7bc2f21be7": {"ta_keywords": "voice activity detection;voice activity;speech gmms;based voice activity;speech gmms constructed;model based voice;non speech gmms;speech period based;drichlet prior statistical;environmental adapted speech;voice;identify speech;prior statistical model;using drichlet prior;gaussian reduction;prior statistical;gaussian distribution propose;vad identify speech;reducing unimportant gaussian;introduce drichlet prior;adapted speech;switching kalman filter;speech period;activity detection;method prior probabilities;speech non speech;activity detection vad;prior probabilities;drichlet prior;constructed switching kalman", "pdf_keywords": ""}, "32367e7587d5b2de0391cff9ad2d600ff8624e60": {"ta_keywords": "human social skills;social skills training;skills social interaction;social skill training;interaction social skills;social skills;features audiovisual;evaluate social skills;features audiovisual features;audiovisual features;social skills trainers;considering audiovisual features;audiovisual features regarding;simulating social skills;appropriate skills social;social skill;effectiveness social skill;skills social;computer interaction social;features human social;training using audio;audio features audiovisual;based social skills;using audio features;parts social skills;computer based social;audio features;considering audiovisual;audiovisual;linguistic features human", "pdf_keywords": ""}, "22f4eb19be4031e63194bbd7c355914533004918": {"ta_keywords": "dynamometer driving schedule;urban dynamometer driving;dynamometer driving;gear shift behavior;analyzes gear shift;transmission considered vehicle;environment urban dynamometer;gear shifts;urban dynamometer;dynamometer;gear shift;motor em transmission;ecms optimal control;simulation model passenger;car longitudinal dynamics;driving schedule;driving schedule udds;standard ecms optimal;vehicle environment;car longitudinal;number gear shifts;transmission considered;analyzes gear;vehicle application;considered vehicle application;passenger car longitudinal;ecms optimal;cost optimized dht;vehicle environment urban;power flow using", "pdf_keywords": ""}, "ea77b71385648f5c6ea533a0e3685f0e76302eba": {"ta_keywords": "entity recognition ner;named entity recognition;language minimizing annotator;lingual transfer learning;entity recognition;named entity recognizers;targeted annotation;annotation uncertain entity;entity recognizers;targeted annotation uncertain;languages little annotation;entity recognizers proposed;minimizing annotator effort;performing targeted annotation;human annotation;human annotation settle;lingual transferred model;target language minimizing;annotator effort;languages active learning;real human annotation;highly resourced languages;spans target language;entity spans target;target language;entity spans;annotation;named entity;annotation uncertain;lower resourced languages", "pdf_keywords": "resource entity recognizers;entity recognizer low;lingual transfer learning;entity recognizers;entity recognizers use;bootstrapping low resource;model trained language;entity recognizer;low resource languages;dataset target language;low resource language;target language labeled;wasteful annotate sequences;named entity recognizer;language low resource;language labeled dataset;annotations english labeled;trained language low;annotate sequences small;ef\ufb01ciently bootstrap ner;label target language;resource language small;learning english labeled;study ef\ufb01ciently bootstrap;resource languages provide;target language;low resource entity;spans target language;bootstrap ner systems;recognizer low resource"}, "0822f8d7e6a72a65e65f147d3a8d8fccd485da40": {"ta_keywords": "word embeddings efficiently;language modeling transformers;better language modeling;embeddings efficiently produces;embeddings efficiently;model short subsequences;word embeddings;position embeddings queries;position embeddings introduce;shortformer better language;instead word embeddings;language modeling;position embeddings;embeddings queries;short subsequences;shorter inputs;relative position embeddings;embeddings introduce simple;embeddings introduce;short subsequences moving;shortformer better;using shorter inputs;progress language modeling;modeling using shorter;embeddings;embeddings queries keys;absolute position embeddings;tokens generating sequences;language modeling using;recurrence methods transformers", "pdf_keywords": "transformer language models;recurrence methods transformers;subsequences improve runtime;input subsequences improve;transformer language;inference longer subsequences;shorter input subsequences;longer subsequences achieve;models using subsequences;subsequences achieve better;subsequences improve;longer subsequences;nonoverlapping subsequences evaluated;affects transformer language;subsequence lengths highly;multiple nonoverlapping subsequences;nonoverlapping subsequences;input subsequence length;subsequences achieve;transformers let models;input subsequences;tokens generating sequences;subsequences evaluated independently;subsequence length;manually choosing subsequence;subsequence length affects;choosing subsequence lengths;language models;transformers;improve ef\ufb01ciency recurrence"}, "253ad629cd2d396201d71aa605bec233bff66dca": {"ta_keywords": "clustering vbec speech;clustering triphone hmm;tree clustering triphone;clustering triphone;model speech recognition;vbec speech recognition;acoustic model speech;estimation clustering vbec;speech recognition gmm;decision tree clustering;clustering based gaussian;gaussian mixture model;model speech;bayesian estimation clustering;speech recognition;based gaussian mixture;triphone hmm states;monophone hidden markov;vbec speech;clustering vbec;finding appropriate acoustic;acoustic model topology;complicated acoustic model;speech recognition using;acoustic model;gaussian mixture;hidden markov model;tree clustering based;triphone hmm;estimation clustering", "pdf_keywords": ""}, "fac95cc5f52f954fe89b3aa4b75895568ff6a6d4": {"ta_keywords": "text support normalization;normalization rules texts;wordforms applying normalization;rules texts luther;normalization rules;luther bible weighted;versions luther bible;maps historical wordforms;historical wordforms;texts luther results;modern wordforms applying;luther bible;modern wordforms;historical wordforms modern;applying normalization rules;texts luther;rules texts;support normalization;normalization;support normalization certain;aligned versions luther;versions luther;diverse language data;wordforms modern wordforms;normalization certain;exact matches baseline;bible weighted;bible weighted according;language data roughly;matches baseline", "pdf_keywords": ""}, "8b73e226815d57bf66fc94905ebd063e4957b449": {"ta_keywords": "adversary reviewers peer;computing adversary reviewers;privacy peer review;adversary inferring reviewer;adversary reviewers;calibration privacy peer;optimal calibration privacy;privacy preventing adversary;reviewers peer;reviewer identity utility;calibration privacy;inferring reviewer identity;privacy peer;map computing adversary;calibration privacy provide;inferring reviewer;involving reviewers papers;preventing adversary inferring;problem calibration privacy;reviewers peer review;computing adversary;algorithms prove pareto;reviewers papers map;privacy;adversary inferring;involving reviewers;reviewer identity;model involving reviewers;peer review;preventing adversary", "pdf_keywords": "calibration privacy optimally;algorithms calibration privacy;privacy preventing adversary;calibration privacy;adversary inferring reviewer;privacy optimally;preventing adversary inferring;calibration privacy provide;privacy optimally trades;problem calibration privacy;adversary inferring;adversary provide explicit;preventing adversary;error adversary provide;adversary provide;paper error adversary;algorithms prove pareto;error adversary;reviewer identity utility;privacy;inferring reviewer identity;case error adversary;adversary;pareto optimal;adversary terms guessing;tradeo\ufb00 privacy;pareto optimal based;privacy preventing;error adversary terms;tradeo\ufb00 privacy preventing"}, "f249e3a7d4f7f964e9a4ca6e633ac31410a91dd8": {"ta_keywords": "lingual transfer morphological;transfer morphological inflection;cross lingual transfer;lingual transfer;lingual transfer single;cross lingual;monolingual data hallucination;effects cross lingual;languages monolingual data;lingual;resource morphological inflection;monolingual data;transfer morphological;morphological inflection typological;attention architecture inflection;success cross lingual;languages monolingual;morphological inflection;morphological inflection macro;multiple languages monolingual;monolingual;inflection decoder;architecture inflection decoder;multiple languages;languages;inflection typological similarity;low resource morphological;single multiple languages;common representation languages;long tail languages", "pdf_keywords": "lingual transfer morphological;monolingual data hallucination;language genetic similarity;cross lingual transfer;lingual transfer;cross lingual;lingual transfer single;lingual training using;related languages;languages monolingual data;cross lingual training;hallucination identify language;lingual;monolingual data;lingual training;success cross lingual;factor cross lingual;languages monolingual;related languages leads;transfer morphological;multiple languages monolingual;e\ufb00ects cross lingual;multiple languages;using related languages;language genetic;languages addition investigate;transfer morphological in\ufb02ection;common representation languages;languages leads better;identify language genetic"}, "a309cb82c27233948f9b09f440be171a8d24ffff": {"ta_keywords": "peer selection agents;accuracy peer selection;algorithm impartial peer;impartial peer selection;prize peernomination;peer selection peernomination;selection peernomination provide;selection peernomination;prize peernomination relaxing;award prize peernomination;peer selection;algorithms impartial individual;novel algorithm impartial;algorithms impartial;algorithm impartial;impartial peer;increased accuracy peer;accuracy peer;peernomination provide;peernomination;peernomination provide theoretical;selection agents self;peernomination relaxing exactness;selection agents choose;agents choose subset;impartial individual agent;peer;selection agents;subset award prize;peernomination relaxing", "pdf_keywords": "accuracy results peernomination;selection peernomination provide;results peernomination improves;peer selection peernomination;selection peernomination;method peer selection;impartial peer selection;results peernomination;performance peernomination underlying;algorithm impartial peer;compare performance peernomination;performance peernomination;peernomination improves;peer selection;peernomination underlying;peer selection mechanisms;truth ranking agent;peernomination provide;peer selection outcomes;ranking agent rankings;peernomination;agent rankings;ranking agent;impartial peer;compare method peer;peernomination improves current;peernomination underlying ground;peernomination provide theoretical;truth ranking;method peer"}, "04a94c15fec43e7563d58be697246a0dd6c57021": {"ta_keywords": "platforms think myspace;outraged users platforms;content providers;social media landscape;social activity internet;information content providers;censor content;social media;web walled;content tweets;content providers provision;services facebook twitter;facebook;facebook twitter;modern social media;responsibilities platforms ought;facebook twitter youtube;twitter;twitter youtube actively;users platforms;think myspace facebook;platforms think;services facebook;raged responsibilities platforms;twitter youtube;providers liability content;responsibilities platforms;organize censor content;twitter youtube tiktok;intended protect telecommunications", "pdf_keywords": "social media landscape;social media giants;modern social media;media giants facebook;social media;curate content nature;content tweets;curate content;blogging platforms;blogging platforms sharing;articles blogging platforms;user content rapidly;blogging;facebook twitter;used curate content;twitter;facebook;technology used curate;intermediaries creators;facebook twitter tra\ufb03c;content tweets photos;content nature;articles blogging;content rapidly;giants facebook twitter;tweets;content providers regulating;information content providers;tweets photos;curate"}, "a99de68ee8d6729eee5ca5943b152aba7e4738ee": {"ta_keywords": "neural editor;neural editor edit;distributed representations edits;combining neural editor;representations edits;representations edits evaluation;edit encoder models;structure semantics edits;editor edit encoder;learning distributed representations;editor edit;editor;encoder models learn;semantics edits experiment;semantics edits;edits experiment natural;edit encoder;distributed representations;code edit data;source code edit;edits evaluation;edits experiment;edits evaluation yields;encoder models;salient information edit;code edit;neural;learning distributed;edits new inputs;edits new", "pdf_keywords": "distributed representations edits;learning representations edits;representation edit neural;learning represent edits;neural editor;edit neural;neural editor trained;edit neural editor;sentences editing prototypes;representations edits;construct edit representation;structure semantics edits;editor trained construct;representations edits experiment;language sentences editing;sentences editing;representations edits present;represent edits;semantics edits;edit encoder trained;edit representation;learning distributed representations;model natural language;editor trained;semantics edits 2017;trained construct edit;distributed representations;trained compute representation;editing;handle edit data"}, "7f20366098665cd508fe82255cc1a65e1e733a14": {"ta_keywords": "use speech enhancement;speech enhancement techniques;enhanced speech features;speech enhancement;speech enhancement technique;enhanced speech;mismatch enhanced speech;speech recognition;speech features acoustic;automatic speech recognition;performance automatic speech;improvement recognition performance;speech features;features acoustic model;improvement recognition;features acoustic;discriminative criterion adaptation;limiting improvement recognition;speech recognition suffers;automatic speech;criterion adaptation provided;reduction compared spectral;estimate feature variance;criterion adaptation;recognition performance;recognition performance performance;recognition limiting improvement;error rate reduction;compared spectral subtraction;enhancement technique prior", "pdf_keywords": ""}, "22d2f8030221bd0c27bfb9416eeffe4e86633780": {"ta_keywords": "fast forward indexes;facilitates ranking documents;fast forward index;efficient neural ranking;ranking documents;improvements hybrid indexes;ranking documents using;index facilitates ranking;query processing fast;indexes provide superior;facilitates ranking;neural ranking;indexes rely efficient;lexical semantic scores;forward indexes propose;indexes performance query;search fast forward;hybrid indexes performance;forward indexes;indexes based nearest;neural ranking using;replacement contextual rankers;dense indexes based;search fast;semantic similarity computation;semantic similarity;indexes based;indexes performance;based semantic similarity;ranking performance using", "pdf_keywords": "document retrieval ranking;facilitates ranking documents;ranking long documents;retrieval ranking;ranking documents;retrieval ranking achieve;ranking documents using;semantic similarity scores;similarity scores ranking;fast forward indexes;combines document retrieval;facilitates ranking;lexical matching scores;indexing approach ranks;ranks documents based;fast forward index;document retrieval;propose forward indexing;index facilitates ranking;matching scores retrieval;ranking phase lexical;lexical semantic scores;semantic similarity document;semantic scores replacement;ranking long;ranks documents;replacement contextual rankers;forward indexing approach;approach ranks documents;indexing"}, "4e0610ac4c5e055ac56b2ae0d91386a10ffbd325": {"ta_keywords": "simulates human learning;human learning math;knowledge addition;learning math;domain knowledge addition;human learning artificial;learning artificial intelligence;learning agent currently;learning agent;manual knowledge encoding;manual knowledge;constructing learning agent;intelligent agent simulates;integration prior knowledge;intelligence constructing learning;prior domain knowledge;prior knowledge;learning artificial;constructing learning;learning math science;domain knowledge;solved human students;model human acquisition;human learning;acquisition prior knowledge;knowledge form deep;level intelligence constructing;knowledge encoding;understanding human learning;prior knowledge form", "pdf_keywords": ""}, "8d019c77989100a51385e4b4a5fa5250445d8f1d": {"ta_keywords": "speech recognition task;speech recognition;discriminative training;sequential discriminative training;vocabulary speech recognition;discriminative training framework;generalized discriminative training;lvcsr task corpus;discriminative training criterion;neural networks discriminative;discriminative feature;corpus spontaneous japanese;balance sequential discriminative;combination encompasses acoustic;discriminative feature transformation;generalized discriminative;proposes generalized discriminative;task corpus spontaneous;task corpus;sequential discriminative;mixture models deep;networks discriminative feature;discriminative;networks discriminative;chime challenge track;corpus;acoustic modeling gaussian;training framework combination;gaussian mixture models;models deep neural", "pdf_keywords": ""}, "1b97c38d0156dc8bf300b41c2ba5c0463c3a2c00": {"ta_keywords": "data augmentation;speech recognizer;data augmentation strategies;data augmentation simple;robustness speech recognizer;learned dnn based;improve robustness speech;autoencoder;augmented training data;denoising autoencoder;including noising reverberation;dnn based denoising;denoising autoencoder proposing;speech recognizer deployed;autoencoder proposing novel;vector extractor ssnn;noising reverberation tested;noising reverberation;sequence summarizing neural;augmentation simple efficient;autoencoder proposing;based denoising autoencoder;summarizing neural network;acoustic summary utterance;robustness speech;dereverberation learned dnn;dnn based;selection augmented training;development data augmentation;augmentation simple", "pdf_keywords": ""}, "3df825e086b00dd4132c34ecbf638f9a6dc4320d": {"ta_keywords": "learns procedural knowledge;learning agent;learning agent simstudent;learns procedural;learning synthetic student;acquiring procedural knowledge;simstudent learns procedural;agent simstudent learns;intelligent agent simulates;human learning artificial;procedural knowledge;procedural knowledge using;learning artificial intelligence;learns;transfer learning synthetic;machine learning agent;learning artificial;knowledge using transfer;procedural knowledge examples;intelligent agent;human learning;building intelligent agent;human level learning;agent simulates;transfer learning;simstudent learns;learning synthetic;using transfer learning;artificial intelligence creating;integrating transfer learning", "pdf_keywords": ""}, "3e59b3e1e3ef65f9574a0fe30f18ba7a815ea0af": {"ta_keywords": "dialogue policy learning;efficient exploration dialogue;learning greedy exploration;policy learning bbq;greedy boltzmann exploration;exploration dialogue policy;policy learning;exploration dialogue;boltzmann exploration;rewards sparse action;based thompson sampling;dialogue tasks;learning bbq networks;task oriented dialogue;actions rewards sparse;greedy exploration;harvest dialogue tasks;greedy exploration inefficient;bbq networks replay;thompson sampling;dialogue tasks make;learning bbq;spiking replay;dialogue systems;networks replay;boltzmann exploration poses;approaches greedy boltzmann;replay buffer experiences;dialogue policy;replay buffer spiking", "pdf_keywords": ""}, "790eb7e93f1d3fce470c0222fd2be83bab55a428": {"ta_keywords": "language models rnn;rnn language models;word based rnn;end speech recognition;speech recognition asr;models rnn lms;speech recognition word;rnn lms performance;based rnn language;speech recognition;models rnn;based rnn lm;rnn language;automatic speech recognition;rnn lms;rnn lm allows;word based lm;rnn lm;automatic speech;end automatic speech;based rnn;trained used decoding;rnn;recognition asr;language models;word probabilities predict;predict characters instead;character based lm;lms performance end;recognition word based", "pdf_keywords": "language model rnn;speech recognition asr;rnn language model;word based rnn;rnn language;based rnn language;attention based encoder;language modeling decoding;modeling decoding end;speech recognition;recognition asr;model rnn lm;end asr systems;language models;model rnn;language modeling;asr systems benchmark;automatic speech recognition;based rnn lm;automatic speech;ctc attention based;language model;end end asr;end automatic speech;language models hybrid;modeling decoding;hybrid ctc attention;end asr;based language models;rnn lm"}, "83cf7b9611fabe9da2d08722445039023f1b19e9": {"ta_keywords": "\u7d71\u8a08\u7684\u624b\u6cd5\u306b\u57fa\u3065\u304f\u6b4c\u58f0\u306e\u77e5\u899a\u5e74\u9f62\u5236\u5fa1\u6cd5 \u30dd\u30b9\u30bf\u30fc\u30bb\u30c3\u30b7\u30e7\u30f3 \u97f3\u5b66\u30b7\u30f3\u30dd\u30b8\u30a6\u30e02014;\u7d71\u8a08\u7684\u624b\u6cd5\u306b\u57fa\u3065\u304f\u6b4c\u58f0\u306e\u77e5\u899a\u5e74\u9f62\u5236\u5fa1\u6cd5 \u30dd\u30b9\u30bf\u30fc\u30bb\u30c3\u30b7\u30e7\u30f3;\u7d71\u8a08\u7684\u624b\u6cd5\u306b\u57fa\u3065\u304f\u6b4c\u58f0\u306e\u77e5\u899a\u5e74\u9f62\u5236\u5fa1\u6cd5;\u30dd\u30b9\u30bf\u30fc\u30bb\u30c3\u30b7\u30e7\u30f3 \u97f3\u5b66\u30b7\u30f3\u30dd\u30b8\u30a6\u30e02014;\u30dd\u30b9\u30bf\u30fc\u30bb\u30c3\u30b7\u30e7\u30f3;\u97f3\u5b66\u30b7\u30f3\u30dd\u30b8\u30a6\u30e02014", "pdf_keywords": ""}, "9b09ff09b88bb793b161f284ca6e66031bc5a992": {"ta_keywords": "parallel distributed cloud;distributed cloud;distributed cloud computing;ural workshop parallel;workshop parallel distributed;cloud computing young;cloud computing;cloud;ural workshop;parallel distributed;distributed;computing young scientists;1st ural workshop;workshop parallel;ural pdc 2015;ural pdc;computing;young scientists proceedings;computing young;parallel;workshop;young scientists;ural;2015 1st ural;1st ural;pdc;proceedings 1st ural;scientists proceedings;pdc 2015;pdc 2015 1st", "pdf_keywords": ""}, "68f2f32e0e8fc868920971077a11042784be2616": {"ta_keywords": "rating ranking;field ranking rating;ranking rating;ranking rating receiving;ranking;field ranking;rating ranking science;selecting matchups sports;sports tournaments book;tournaments book;sports tournaments;science rating ranking;tournaments book comprises;tournaments;science ranking rating;matchups sports tournaments;ranking rating amy;rating receiving;ranking science ranking;ranking science;matchups sports;rating;entities movies sports;sports teams book;rating receiving increasing;science ranking;movies sports;recommendations selecting matchups;movies sports teams;teams book", "pdf_keywords": ""}, "a3ca4893ae941bd1601322aface4840e47339761": {"ta_keywords": "crowdsourcing setting agents;crowdsourcing;crowdsourcing setting;important crowdsourcing;important crowdsourcing setting;study important crowdsourcing;crowd sourcing;crowd sourcing settings;applying crowd sourcing;ubiquitous peer review;peer review;agents misreport reviews;funding scientists selecting;agents evaluate based;evaluations subset agents;distributing awards team;agents evaluate;funding scientists;challenge applying crowd;distributing awards;reviews increase chances;allocating funding scientists;subset agents selected;setting agents evaluate;agents selected;applying crowd;peer review used;awards team allocating;sourcing settings agents;scientists selecting", "pdf_keywords": ""}, "a810d2f4a1fefd4175d8cdda9702ee1b829e5831": {"ta_keywords": "adipose tissue bat;obesity natural compounds;bat white adipose;anti obesity natural;treat obesity;obesity increasing energy;fat diet;prevent treat obesity;fat diet hfd;anti obesity;obesity natural;drugs treat obesity;high fat diet;treat obesity increasing;adipose;white adipose;screen anti obesity;adipose tissue;reduce body weight;white adipose tissue;adipose tissue wat;brown adipose;fed mice results;obesity;weight gain maintained;brown adipose tissue;body weight gain;fed mice significantly;reduced body weight;weight gain", "pdf_keywords": ""}, "66081634c17b089cb47fd1b0ad7ad842c7fb3f87": {"ta_keywords": "simstudent teachable agent;tutee tutor learning;tutee simstudent learning;use teachable agent;teachable agent technology;teachable agent interactive;tutor learning;simstudent teachable;tutors tutees simstudent;teachable agent;simstudent learning;using simstudent teachable;interactive peer learning;tutor human;learning tutors tutees;simstudent learning af;tutor;learning tutors;successful learning tutors;peer learning;fected tutor human;tutor learning introduction;tutor human student;tween tutee tutor;tutee tutor;peer learning environment;af fected tutor;fected tutor;tutors;use teachable", "pdf_keywords": ""}, "d706645fbbc6edfad5fb642b1dfc3019fcabbd99": {"ta_keywords": "mechanical turk evaluate;crowdsourced human judgments;using mechanical turk;amazon mechanical turk;mechanical turk amt;story evaluation experiments;human generated references;mechanical turk;human judgments text;text human generated;crowdsourced human;judgments text quality;collecting crowdsourced human;judgments text;story evaluation;collecting crowdsourced;generated text human;crowdsourced;choices collecting crowdsourced;turk evaluate;text generation papers;text generation;human judgments;ended text generation;worker judgments improve;evaluate automatically researchers;turk evaluate open;model generated text;generated references models;ratings perils using", "pdf_keywords": "transfer paraphrase generation;generative architecture paraphrase;controlled paraphrase generation;paraphrase generation;paraphrase generation complexity;paraphrase generation steep;architecture paraphrase generation;model sentence alignment;story generation embedding;controllable story generation;style transfer paraphrase;text generation evaluating;text generation;text generation research;paraphrase generation pun;sparse text generation;nlp generative;story generation evaluations;content analysis neural;semantics humor prediction;preordering controlled paraphrase;story reasoning generation;explainable nlp generative;ended text generation;models avoid sentences;sentiment style transfer;transfer paraphrase;neural syntactic preordering;corpus benchmarks metrics;recent text generation"}, "ad7129af0644dbcafa9aa2f111cb76526ea444a1": {"ta_keywords": "neural fake news;defenses neural fake;generate neural fake;classify neural fake;detection neural fake;neural fake;fake news targeted;adversaries generate neural;robust defenses neural;news targeted propaganda;fake news requires;trustworthy human written;human written disinformation;defenses neural;propaganda closely mimics;careful threat modeling;targeted propaganda closely;fake news investigate;targeted propaganda;developing robust defenses;written disinformation;news targeted;identifying potential threats;disinformation;threats given headline;human written news;threat modeling identifying;adversaries generate;vaccines autism grover;bias sampling strategies", "pdf_keywords": "neural fake news;headline vaccines;neural fake;classify neural fake;detection neural fake;article generation;real machinegenerated text;article generation investigate;based disinformation threats;machinegenerated text present;vaccines cause autism;disinformation threats;news real human;human written news;research shows vaccines;pretrained language models;autism kids authors;vaccinations cause autism;finds childhood vaccinations;based disinformation;headline vaccines bigger;headline new research;evolving conversation ml;examples article generation;machinegenerated text;autism authors;childhood vaccinations;disinformation threats countered;deep pretrained language;shows vaccines cause"}, "946e5e31b0779fc33550e8681994e7afd8d549a5": {"ta_keywords": "clinical gait analysis;measurement clinical gait;gait analysis;clinical gait;motion measurement clinical;automated motion measurement;motion measurement;gait;automated motion;measurement clinical;motion;measurement;automated;clinical;analysis", "pdf_keywords": ""}, "81d4357afae9680e64a645cbb36aa090c3619b19": {"ta_keywords": "category index results;improve category results;hoc diversity task;category results;diversity task abstract;category results ad;ad hoc diversity;results obtained category;diversity task;category index;obtained category index;hoc diversity;improve category;complete category anchor;results ad hoc;category anchor;category anchor text;unable improve category;obtained category;category;complete category;merged results obtained;abstract merged results;merged results;index results;built complete category;diversity;index results obtained;results obtained index;results ad", "pdf_keywords": ""}, "a7d6b5e61024127bf4fe8f04c0182a16ff97bccf": {"ta_keywords": "complexity probabilistic lobbying;probabilistic lobbying;lobbying probabilistic;models lobbying probabilistic;lobbying probabilistic environment;lobbying stochastic;lobbying stochastic environment;problems lobbying stochastic;various models lobbying;models lobbying;lobbying issue;lobbying issue weighting;lobbying;problems lobbying;depending given bribery;forms lobbying;analysis problems lobbying;lobby seeks influence;bribery methods;resulting forms lobbying;bribery methods formally;given bribery;forms lobbying issue;bribery;criteria bribery;bribery evaluation;given bribery evaluation;criteria bribery methods;bribery evaluation criteria;lobby seeks", "pdf_keywords": "complexity probabilistic lobbying;probabilistic lobbying;lobbying probabilistic;models lobbying probabilistic;lobbying probabilistic environment;lobbying stochastic;lobbying stochastic environment;problems lobbying stochastic;probabilistic lobbying daniel;models lobbying;propose models lobbying;lobbying issue weighting;lobbying issue;problems lobbying;lobbying;forms lobbying;analysis problems lobbying;resulting forms lobbying;forms lobbying issue;depending given bribery;lobby seeks in\ufb02uence;bribery methods;lobby seeks;lobbying daniel binkele;bribery methods formally;criteria bribery;given bribery;criteria bribery methods;complexity probabilistic;bribery"}, "419e714f22c3fa2599abebd630cae5595c70bdef": {"ta_keywords": "e2e automatic speech;speech recognition enhanced;recognition enhanced speech;features enhanced speech;speech enhancement se;recognition speech enhancement;speech enhancement self;speech recognition asr;enhanced speech input;integraded speech recognition;integration speech recognition;speech recognition;automatic speech recognition;speech recognition speech;speech recognition called;speech enhancement;robust speech recognition;including speech enhancement;automatic speech;recognition speech;speech input self;enhanced speech;modules including speech;enhanced speech used;recognition asr model;speech input;used speech recognition;robust speech;e2e asr models;recognition asr", "pdf_keywords": "recognition speech enhancement;speech recognition enhanced;trained speech enhancement;e2e automatic speech;recognition enhanced speech;speech enhancement model;integration speech recognition;speech enhancement se;integraded speech recognition;speech recognition asr;speech enhancement self;enhanced speech input;speech enhancement;automatic speech recognition;speech recognition;including speech enhancement;robust speech recognition;speech recognition speech;speech recognition called;enhanced speech;speech recognition achieves;e2e asr models;module robust speech;automatic speech;recognition asr model;recognition speech;modules including speech;speech input self;targetting robust speech;e2e asr"}, "888c81cd3d1e953e2b7f8cc4ce68ca9f908c1e8d": {"ta_keywords": "nlp privacy;nlp privacy gains;applying nlp privacy;concept differential privacy;differential privacy;privacy frameworks differential;differential privacy non;privacy gains traction;privacy preserving;differential privacy dp;privacy preserving methods;frameworks differential privacy;privacy non trivial;approaches privacy preserving;privacy dp compelling;privacy frameworks;favorite privacy frameworks;approaches privacy;privacy;methods favorite privacy;privacy dp;privacy loss guarantees;differentially private;various approaches privacy;differentially private despite;privacy gains;privacy loss;favorite privacy;privacy non;violates privacy loss", "pdf_keywords": "private text representation;differentially private text;private text;recent nlp papers;computational linguistics acl;computational linguistics;text representation learning;differentially private formally;learning using dptext;differentially private;linguistics acl 2022;nlp papers;proposing text representation;recent nlp;text representation;linguistics acl;association computational linguistics;private formally analyze;analyze recent nlp;nlp papers proposing;nlp;representation learning ivan;linguistics;claims differentially private;privacy loss guarantees;representation learning;privacy;private formally;violates privacy loss;function dptext"}, "596b46dbe4fa8eee72e517ea9fd5f8ef83c9c64e": {"ta_keywords": "question answering;wikipedia quizbowl;approach playing quizbowl;incremental question answering;playing quizbowl involves;quizbowl;quizbowl involves determining;answer quizbowl;playing quizbowl;research quizbowl;quizbowl involves;learning research quizbowl;question answering qa;answering qa players;quizbowl collecting curating;answer answer quizbowl;demonstrates superiority answering;question answering make;quizbowl question;quizbowl question consists;research question answering;entity wikipedia quizbowl;research quizbowl collecting;answer quizbowl scholastic;superiority answering;quizbowl collecting;wikipedia quizbowl case;answering qa;quizbowl scholastic;superiority answering correctly", "pdf_keywords": "question answering;learning challenges factoid;vibrant trivia community;incremental question answering;question answering paper;curating large factoid;trivia community contributed;challenges factoid qa;mastery question answering;factoid qa dataset;challenges factoid;collaborations vibrant trivia;answering paper collaborations;large factoid qa;factoid qa thousands;test knowledge intelligence;trivia competitions;scholastic trivia competitions;factoid qa;natural language processing;factoid;knowledge intelligence;large factoid;machine learning challenges;learning natural language;natural language;wikipedia like answers;computer science courant;learning challenges;answering paper"}, "650f2afca6d72d6b6e2e08849e1224f1e8b7900c": {"ta_keywords": "rating estimation graph;binary rating estimation;rating matrix graph;information social graphs;graph information social;rating estimation;estimation graph information;quality graph information;ratings aid graph;rating estimation problem;social graphs;information binary rating;graph information best;graph information binary;graph information;rating matrix;social graphs work;recover rating matrix;reveal graph information;graph information reduces;estimation graph;quality graph;binary rating;study binary rating;graph information detailed;rating matrix called;matrix graph characterize;users unknown ratings;model rating matrix;matrix graph", "pdf_keywords": ""}, "932404745d960291925b3f27b71734dff5b23633": {"ta_keywords": "induce class discrimination;treatment disparity algorithms;treatment disparity undermining;discrimination;class discrimination;implement treatment disparity;class discrimination iii;discrimination iii general;discrimination iii;disparity require treatment;discrimination law;disparate learning;disparity undermining;exhibit treatment disparity;treatment disparity;ml impact disparity;employment discrimination;proposing disparate learning;disparity outcomes;discrimination law notions;impact disparity outcomes;disparity formally treat;disparate learning processes;treatment disparity formally;disparity undermining policy;precedent employment discrimination;require treatment disparity;disparity widely discussed;disparity outcomes differ;employment discrimination law", "pdf_keywords": ""}, "7bbd132f40c7630aeebf6379b00e307c3fff738c": {"ta_keywords": "stored nodes distributed;nodes distributed manner;replication;exact replication;nodes distributed;repair failed node;bandwidth required repair;data connecting nodes;failed node data;stored nodes;nodes;data stored nodes;replication failed systematic;permit exact replication;distributed;nodes provide;systematic node;node data;distributed manner;failed systematic node;nodes provide explicit;connecting nodes;reconstruction entire data;exact replication failed;replication failed;connecting nodes provide;failed node;node;node data stored;minimizing bandwidth", "pdf_keywords": "minimizing repair bandwidth;storage nodes;data storage nodes;storage nodes distributed;mds code node;bandwidth exact regeneration;regeneration systematic nodes;repair bandwidth;nodes existence construction;systematic nodes existence;codes achieve bound;bound 2k coding;regeneration failed node;repair bandwidth exact;nodes distributed manner;nodes existence;2k coding scheme;existence construction codes;systematic nodes;nodes distributed;nodes;mds code;coding scheme;code node;code node stores;data storage;codes achieve;minimizing repair;type mds code;2k coding"}, "e8c3090e66fdb05a2c169a12c52dd94bb8786fb5": {"ta_keywords": "natural language explanations;generating explanations;pointers explanations persuasive;language explanations leverage;generating explanations propose;words explanations central;explanations persuasive;words explanations;content words explanations;language explanations;explanations persuasive arguments;explanations argument persuasive;methods generating explanations;explanations selectively reuse;explanations central everyday;word level prediction;explanations propose;naturally occurring explanations;echoing word explanation;topic growing ai;investigate explanations selectively;explanations central;investigate explanations;explanations selectively;occurring explanations;crucial predicting echoing;occurring explanations argument;explanations propose novel;word explanation enhance;task investigate explanations", "pdf_keywords": "explanations pointer networks;generating explanations pointer;generating explanations using;generating explanations;natural language explanations;language explanations demonstrate;explanations pointer;language explanations;word embeddings improve;generating explanations assemble;features word embeddings;methods generating explanations;comment predicting echoing;words vanilla lstms;echoing word explanation;nouns shaping explanations;explanations importance contextual;word level prediction;performance generating explanations;pointer networks;predicting echoing content;pointer networks provide;pointer networks investigate;embeddings improve generation;comment predicting;word embeddings;explanations using pointer;explanations identify;embeddings improve;shaping explanations importance"}, "ed535e93d5b5a8b689e861e9c6083a806d1535c2": {"ta_keywords": "systematic generalization transformers;transformers systematic generalization;neural networks generalize;generalization transformers;generalization transformers calls;generalization ability neural;performance transformers systematic;neural networks scan;transformer variants drastically;systematic generalization ability;models improve accuracy;improve performance transformers;transformers systematic;proper generalization validation;generalization validation sets;performance transformers;improve systematic generalization;systematic generalization recently;generalization validation;embedding universal transformer;generalization ability;systematic generalization;neural networks;test systematic generalization;universal transformer variants;networks generalize systematically;networks generalize;accuracy length split;ability neural networks;transformer variants", "pdf_keywords": "transformers systematic generalization;systematic generalization transformers;generalization transformers;generalization transformers r\u00f3bert;transformers typically trained;transformers systematic;performance transformers systematic;transformer variants drastically;embedding universal transformer;baseline transformers;generalization models improve;companion baseline transformers;generalization ability neural;improve performance transformers;baseline transformers typically;universal transformer variants;performance transformers;universal transformers absolute;transformer variants;universal transformers;universal transformer;transformers absolute positional;standard universal transformers;systematic generalization ability;transformers typically;transformers absolute;systematic generalization models;positional embedding largely;outperform models trained;transformers"}, "9abb50813e05de849dbbd89535bc7d0206f5e36a": {"ta_keywords": "semantic verb clustering;models verb clustering;verb clustering;verb clustering assess;processing nlp lexical;mixture models verb;dirichlet process mixture;natural language processing;processing nlp;metric dirichlet process;nlp lexical;mixture models learning;lexical semantic verb;nlp lexical semantic;language processing nlp;task natural language;dirichlet process;verb classes;semantic verb;verb classes using;language processing;lexical semantic;apply dirichlet process;process mixture models;vmeasure metric dirichlet;dirichlet;learning task;add human supervision;semantic;models learning task", "pdf_keywords": ""}, "0bbfa6ab7451aea5cbb842cce97b54500bafdfc7": {"ta_keywords": "inverse decision theory;preferences inverse reinforcement;inverse decision;setting inverse decision;human preferences inverse;inverse reinforcement learning;learning human preferences;binary decisions uncertainty;preferences inverse;preference learning difficult;human optimal decision;preference learning cases;preference learning;make preference learning;identify preferences decision;decisions uncertainty;inverse reinforcement;sequential binary decisions;improving preference learning;understand preference learning;binary decisions;decisions uncertainty statistical;uncertain decision problems;preferences sample complexities;preferences decision;optimal decision;improving preference;optimal decision maker;preference learning methods;uncertain decision", "pdf_keywords": "inverse decision theory;inverse decision;decision theory idt;binary decisions uncertainty;setting inverse decision;sequential binary decisions;binary decisions;understand preference learning;decisions observed learn;preference learning cases;preference learning;reward learning idt;decision theory;decisions observed;decisions uncertainty;human decisions reveal;observed learn tradeoff;decisions uncertainty appendix;number decisions observed;human decisions;understand preference;setting human decisions;decisions reveal;reveal important preferences;observed learn;learning idt;preferences characterizing sample;better understand preference;decisions reveal important;hypothesis class"}, "a31ab366b0a349ee5f341f1179810bc9805d32a4": {"ta_keywords": "regenerating codes operating;msr regenerating codes;repair regenerating codes;regenerating codes;secure msr regenerating;security exact repair;regenerating codes paper;storage regenerating msr;matrix construction secure;secure msr;data file leaked;codes operating;\u21131 nodes repair;minimum storage regenerating;storage regenerating;codes operating minimum;msr regenerating;regenerating msr;exact repair regenerating;construction secure msr;repair regenerating;codes paper consider;nodes repair;file leaked presence;file leaked;regenerating;consider security exact;codes paper;regenerating msr point;leaked presence eavesdropper", "pdf_keywords": ""}, "04d18fc81cc232b3d3dece0994c0fa8aaabaf4b7": {"ta_keywords": "japanese morphological analysis;approach japanese morphological;japanese morphological;domain morphological task;domain morphological;speech pos tagging;morpheme gram models;general domain morphological;segmentation speech pos;adaptation smaller annotations;domain adaptation;effective domain adaptation;adaptation minimum annotation;word segmentation speech;word segmentation;pointwise approach japanese;gram models;morphological analysis;domain adaptation minimum;crfs morpheme gram;process word segmentation;segmentation speech;domain adaptation smaller;cost domain adaptation;approach japanese;addition domain adaptation;morpheme gram;pos tagging;tagging pointwise approach;morphological task proposed", "pdf_keywords": ""}, "d5dcbb144a2be999610b4838d94cc3fb228f837c": {"ta_keywords": "network slicing deployment;deploy network slicing;network slicing quality;network slicing;model network slicing;cost network slicing;network slicing high;network slice backup;network slice;slicing deployment;slicing deployment benefit;grid network slice;network virtualization;network virtualization scenario;5g smart grid;slicing quality chip;slicing quality;slice backup;slicing high quality;slice backup method;communication network virtualization;slicing;reinforcement learning deployment;future 5g smart;deployment cost network;5g future 5g;deploy network;chip deployment based;deployment based nfv;future 5g", "pdf_keywords": ""}, "df689bdc6c497949e9ab3b7ba19950d9fade7180": {"ta_keywords": "\u5bfe\u8a71\u30b7\u30b9\u30c6\u30e0\u306b\u304a\u3051\u308b\u5358\u8a9e\u9593\u306e\u95a2\u4fc2\u6027\u3092\u7528\u3044\u305f\u8a71\u984c\u8a98\u5c0e\u5fdc\u7b54\u6587\u751f\u6210 \u8a00\u8a9e\u30e2\u30c7\u30eb \u97f3\u58f0\u5bfe\u8a71;\u5bfe\u8a71\u30b7\u30b9\u30c6\u30e0\u306b\u304a\u3051\u308b\u5358\u8a9e\u9593\u306e\u95a2\u4fc2\u6027\u3092\u7528\u3044\u305f\u8a71\u984c\u8a98\u5c0e\u5fdc\u7b54\u6587\u751f\u6210 \u8a00\u8a9e\u30e2\u30c7\u30eb;\u5bfe\u8a71\u30b7\u30b9\u30c6\u30e0\u306b\u304a\u3051\u308b\u5358\u8a9e\u9593\u306e\u95a2\u4fc2\u6027\u3092\u7528\u3044\u305f\u8a71\u984c\u8a98\u5c0e\u5fdc\u7b54\u6587\u751f\u6210;\u8a00\u8a9e\u30e2\u30c7\u30eb \u97f3\u58f0\u5bfe\u8a71;\u8a00\u8a9e\u30e2\u30c7\u30eb;\u97f3\u58f0\u5bfe\u8a71", "pdf_keywords": ""}, "d7fe9b46f96ae9df7fa64e1c575c7114e5ef0aaa": {"ta_keywords": "optimal tensor methods;accelerated tensor method;new tensor method;optimal tensor;tensor method;problems optimal tensor;tensor methods;tensor methods 3rd;performance tensor methods;tensor methods smooth;accelerated tensor;known accelerated tensor;tensor method closes;performance tensor;propose new tensor;compare performance tensor;new tensor;tensor;methods smooth convex;optimal method;proposed method accelerated;convex uniformly convexoptimization;method accelerated;uniformly convexoptimization;convexoptimization;3rd order method;proposed optimal method;method accelerated additional;methods 3rd order;iteration complexity bounds", "pdf_keywords": "optimal tensor methods;accelerated tensor method;performance tensor methods;performance accelerated tensor;new tensor method;faster accelerated tensor;optimal tensor;tensor method proposed;tensor methods 3rd;tensor methods;tensor method;propose optimal tensor;tensor method nesterov;tensor methods make;performance tensor;accelerated tensor;compare performance tensor;tensor method closes;propose new tensor;tensor;new tensor;optimal method faster;method faster accelerated;iteration complexity bounds;upper iteration complexity;method optimal method;optimal method;method optimal;iteration complexity;proposed method optimal"}, "4c6f7fb5c2e1bd12899c3ec2788f9ce7eb2f8a5c": {"ta_keywords": "syntactic information pretraining;syntactic generalization performance;syntactic structural probes;higher syntactic information;targeted syntactic evaluation;impact syntactic capabilities;syntactic capabilities;size syntactic generalization;syntactic capabilities roberta;syntactic generalization;encode syntactic knowledge;syntactic information;syntactic knowledge;syntactic knowledge perform;data language models;targeted syntactic;language models;syntactic evaluation analyze;parsing paraphrase identification;syntactic structural;perform targeted syntactic;learn syntax calls;encode higher syntactic;tagging dependency parsing;data encode syntactic;use syntactic structural;higher syntactic;language models need;speech tagging;syntactic evaluation", "pdf_keywords": "syntactic generalization performance;targeted syntactic evaluation;syntactic structural probes;syntactic information experiments;higher syntactic information;targeted syntactic;impact syntactic capabilities;speech tagging dependency;syntactic generalization;perform targeted syntactic;size syntactic generalization;syntactic capabilities;syntactic evaluation analyze;speech tagging;encode syntactic knowledge;syntactic knowledge perform;syntactic information;syntactic capabilities roberta;tagging dependency parsing;syntactic structural;syntactic knowledge;syntactic evaluation;higher syntactic;encode higher syntactic;use syntactic structural;performance different syntactic;dependency parsing paraphrase;applications speech tagging;explore impact syntactic;dependency parsing"}, "b73191adcc938cfcf20ce0327cf5cd1f539f7f81": {"ta_keywords": "extracting keyphrases scientific;scientific information extraction;keyphrases scientific articles;entity recognition scientific;supervised neural tagging;extracting keyphrases;neural tagging annotated;keyphrases scientific;information extraction semi;extraction semi supervised;neural tagging model;neural tagging;named entity recognition;tagging annotated;entity recognition;tagging annotated training;recognition scientific information;information extraction;unannotated articles;problem extracting keyphrases;sequence tagging;methods neural tagging;scientific articles categorizing;introduce semi supervised;semi supervised methods;keyphrases;semi supervised algorithm;semi supervised neural;sequence tagging introduce;scientific articles", "pdf_keywords": "scienti\ufb01c information extraction;information extraction semi;extraction semi supervised;extracting keyphrases scienti\ufb01c;information extraction;information extraction task;introduce semi supervised;introducing semi supervised;semi supervised learning;information extraction semeval;supervised neural tagging;introduced semi supervised;extracting keyphrases;neural tagging;crf neural tagging;semi supervised model;semi supervised;neural tagging model;neural tagging models;neural tagging approach;advances neural tagging;semi supervised neural;keyphrases scienti\ufb01c articles;keyphrases scienti\ufb01c;tagging models;initialization semi supervised;tagging model task;extraction semeval task;extraction task sequence;sequence tagging"}, "06d77cc8970b59102a0caffb5e4c5b7a3242563a": {"ta_keywords": "disambiguated sense embedding;sense embedding model;sense embedding;scaled gumbel softmax;gumbel softmax;softmax;self disambiguated sense;disambiguated sense;embedding model;embedding model scaled;differentiable self disambiguated;embedding;self disambiguated;model scaled gumbel;scaled gumbel;disambiguated;gumbel;sense;differentiable self;model scaled;model;self;scaled;differentiable", "pdf_keywords": ""}, "4715ee17ca4f52762fdf67c9a8ef8fb751c88484": {"ta_keywords": "energy disaggregation framework;energy disaggregation;disaggregation framework blind;identification arx models;bigger energy disaggregation;framework blind identification;model power consumption;disaggregation framework;blind identification known;output measurements blind;measurements blind identification;blind identification arx;arx models piecewise;blind identification;consumption electrical appliances;disaggregation;corresponding arx model;identifying arx model;arx model output;power consumption electrical;model output measurements;task identifying arx;identifying systems;consumption electrical;power consumption;model power;arx models;arx model approximates;task identifying systems;identifying systems turned", "pdf_keywords": "energy disaggregation framework;energy disaggregation;model power consumption;bigger energy disaggregation;present disaggregation algorithm;disaggregation algorithm;arx models piecewise;estimating arx model;arx model piecewise;disaggregation algorithm utilizes;blind identi\ufb01cation arx;identi\ufb01cation arx models;disaggregation framework;corresponding arx model;consumption electrical appliances;estimating arx;arx model output;models individual appliances;arx models;disaggregation;disaggregation framework conclusion;arx model solely;arx model approximates;power consumption electrical;power consumption;identifying arx model;model power;model output measurements;arx model;consumption electrical"}, "cac008e541af58f738407c7f2ee86d547053188f": {"ta_keywords": "\u56fd\u4f1a\u97f3\u58f0\u8a8d\u8b58\u30b7\u30b9\u30c6\u30e0\u306e\u97f3\u97ff \u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u534a\u81ea\u52d5\u66f4\u65b0 \u97f3\u58f0\u8a00\u8a9e\u60c5\u5831\u51e6\u7406;\u56fd\u4f1a\u97f3\u58f0\u8a8d\u8b58\u30b7\u30b9\u30c6\u30e0\u306e\u97f3\u97ff \u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u534a\u81ea\u52d5\u66f4\u65b0;\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u534a\u81ea\u52d5\u66f4\u65b0 \u97f3\u58f0\u8a00\u8a9e\u60c5\u5831\u51e6\u7406 slp;\u56fd\u4f1a\u97f3\u58f0\u8a8d\u8b58\u30b7\u30b9\u30c6\u30e0\u306e\u97f3\u97ff;\u97f3\u58f0\u8a00\u8a9e\u60c5\u5831\u51e6\u7406 slp;\u97f3\u58f0\u8a00\u8a9e\u60c5\u5831\u51e6\u7406 slp vol;\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u534a\u81ea\u52d5\u66f4\u65b0 \u97f3\u58f0\u8a00\u8a9e\u60c5\u5831\u51e6\u7406;\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u534a\u81ea\u52d5\u66f4\u65b0;\u97f3\u58f0\u8a00\u8a9e\u60c5\u5831\u51e6\u7406;slp 84;2010 slp 84;2010 slp;slp;slp vol 2010;vol 2010 slp;slp vol;vol 2010;2010;84;vol", "pdf_keywords": ""}, "1e2ef0c9a494c7949f38940ee735a88c56355202": {"ta_keywords": "sensor selection dynamic;selection dynamic sensor;sensor activation centralized;dynamic sensor subset;sensor subset selection;selection centralized tracking;active sensor selection;sensor selection;dynamic sensor activation;centralized tracking problem;centralized tracking iid;sensor networks cyber;centralized tracking;sensor networks;activation centralized tracking;energy efficient tracking;sensor subset;stochastic approximation learning;dynamic sensor;sampling stochastic approximation;tracking problem minimizing;internet things sensor;stochastic approximation;efficient tracking;sampling stochastic;subset selection centralized;selection dynamic;selection centralized;problem dynamic sensor;active sensors", "pdf_keywords": ""}, "00c3a86551f1bc812b676025210e295021853f66": {"ta_keywords": "book enjoyable answer;enjoyable answer maybe;history question intriguing;book playful;answer book enjoyable;book playful walk;playful walk history;question intriguing;book enjoyable;enjoyable answer;day book playful;walk history question;answer book;walk history;playful walk;history question;playful;answer maybe;answer maybe end;intriguing;history;day book;book;enjoyable;end day book;answer;walk;maybe;question;maybe end day", "pdf_keywords": ""}, "69d5579955a5a8859d78a70b3d1afede0f91fa09": {"ta_keywords": "energy disaggregation task;perform energy disaggregation;approach energy disaggregation;energy disaggregation;energy disaggregation known;energy disaggregation paper;intrusive load monitoring;building energy data;energy data building;energy data;energy consumption behavior;disaggregated data consumer;energy data individual;aggregate energy data;disaggregated data;providing disaggregated data;load monitoring;data individual appliances;disaggregation task;improves energy consumption;disaggregation task studies;energy consumption;load monitoring nilm;power consumed device;data building energy;disaggregation known;disaggregation;power consumed;disaggregation known non;individual appliances", "pdf_keywords": "energy disaggregation task;perform energy disaggregation;approach energy disaggregation;energy disaggregation;energy disaggregation known;abstract energy disaggregation;building energy data;energy data building;nonintrusive load monitoring;providing energy usage;energy usage behavior;energy data;energy data individual;aggregate energy data;energy usage;device usage disaggregation;disaggregation provides feasible;load monitoring;energy disaggregation roy;data building energy;power consumed device;disaggregation implementation;disaggregation implementation methods;data individual appliances;disaggregation task;dynamical framework disaggregation;building energy;framework perform energy;framework disaggregation implementation;power consumed"}, "834d68b9befcc6c68415b460b33435a1822799fb": {"ta_keywords": "argumentation mining;argumentation mining user;argumentation mining evolving;argumentation argumentation mining;analyzing people argumentation;data adapting argumentation;people argumentation argumentation;goal argumentation mining;argumentation model;adapting argumentation;argumentation theories argumentation;adapting argumentation model;argumentation phenomena;argumentation model tested;argumentation argumentation;people argumentation;argumentation theories;argumentation phenomena encountered;discourse goal argumentation;argumentation;normative argumentation theories;theories argumentation phenomena;normative argumentation;generated web discourse;theories argumentation;argument components;web discourse goal;identify argument;argument components deal;identify argument components", "pdf_keywords": "argumentation mining user;argumentation mining;argumentation mining evolving;gurevych argumentation mining;analyzing people argumentation;goal argumentation mining;people argumentation;web discourse methods;generated web discourse;argumention usergenerated web;argumention usergenerated;argumention;argumentation;web discourse propose;people argumentation 128;web discourse;discourse methods features;gurevych argumentation;discourse methods;discourse propose model;habernal gurevych argumentation;level argumention;web discourse ivan;argumentation 128;discourse propose;level argumention usergenerated;argumentation 128 habernal;discourse;goal argumentation;2017 goal argumentation"}, "972a74968d2522908b06c5bd1e26266194c5a9ee": {"ta_keywords": "automatically decontextualize sentences;decontextualize sentences taking;decontextualize sentences;sentence decontextualization;sentence decontextualization taking;dialogue agents summarization;sentences taking excerpts;question answering dialogue;agents summarization interpret;question answering;summarization interpret;wikipedia corpus use;agents summarization;decontextualization important subtask;decontextualization taking sentence;summarization interpret meaning;wikipedia corpus;preserving meaning annotation;answering dialogue agents;automatically decontextualize;data wikipedia corpus;problem sentence decontextualization;models question answering;decontextualization taking;summarization;taking excerpts;answering dialogue;decontextualization;decontextualize;taking excerpts text", "pdf_keywords": "sentence decontextualization user;decontextualization making sentences;sentence decontextualization;sentence decontextualization taking;question answering;models question answering;question answering dialogue;dialogue agents summarization;domain question answering;generating retrieval corpus;answering dialogue agents;value sentence decontextualization;decontextualization taking sentence;decontextualization demonstrate decontextualization;decontextualization user facing;problem sentence decontextualization;decontextualization making;answering dialogue;retrieval corpus;use decontextualization;agents summarization;decontextualization taking;second use decontextualization;decontextualization;generating retrieval;agents summarization interpret;decontextualization user;decontextualization used;decontextualization used user;decontextualization demonstrate"}, "6b387d18bae978202af501c4795f37a0c73781a6": {"ta_keywords": "proximal extragradient method;accelerated proximal extragradient;hessian optimized;extragradient method;gradient hessian optimized;tensor method derivatives;hessian evaluations optimized;optimized function tensor;proposed tensor method;extragradient method 2013;extragradient method constructive;hessian optimized function;existing tensor methods;tensor method 2018;function tensor method;tensor method;tensor methods;proximal extragradient;tensor methods order;hybrid proximal extragradient;comparable newton nesterov;svaiter method optimal;gradient hessian evaluations;hessian evaluations;smooth convex optimization;outer accelerated proximal;convex optimization;convex optimization problems;iteration comparable newton;hessian", "pdf_keywords": ""}, "2bb1e1a5b9a16f6828fe94736cea5dab264533a6": {"ta_keywords": "fully emulate semantic;emulate semantic;semantic representations assertions;semantic emulation;emulate semantic representations;semantic transparency;enable semantic emulation;assertions enable semantic;semantic emulation languages;ungrounded language models;notion semantic transparency;model natural language;semantic transparency study;preserving semantic;abstract language models;nlp tasks formalize;language models understand;semantic representations;preserving semantic relations;representations preserving semantic;language models trained;representations assertions enable;future language models;enable semantic;semantic;assertions code language;strong notion semantic;setting semantic;ways ungrounded language;abstract language", "pdf_keywords": "fully emulate semantic;semantic emulation;emulate semantic;semantic emulation languages;enable semantic emulation;notion semantic transparency;semantic transparency results;semantic transparency;assertions enable semantic;emulate semantic representations;emulation uncomputable formalize;ungrounded language models;contexts emulation uncomputable;preserving semantic;language provide suf\ufb01cient;uncomputable formalize;model natural language;emulation languages satisfy;contexts emulation;strong notion semantic;preserving semantic relations;uncomputable formalize ways;language models;modeling linguistic;acquired natural language;representations preserving semantic;enable semantic;different contexts emulation;ways ungrounded language;semantic representations"}, "96b32b204a62777bef66eea595de2c47b4e9d6e9": {"ta_keywords": "data deep;deep neural;generalization data;superficial statistics training;learning robust representations;generalization data sets;learning robust;generalization methods;domain generalization data;domain generalization methods;data deep neural;learn representations;predictable learning robust;generalization methods explicitly;representation predictable learning;robust representations;deep neural networks;model learn representations;distribution training;predictable learning;learn representations glcm;target distribution training;heavily superficial statistics;generalization;holdout data deep;extract patterns prior;robust representations projecting;training data;model learn;neural networks depend", "pdf_keywords": "captures textural information;textural information;extracts textural information;textural information images;training domain fusion;textural information image;synthetic datasets da;captures textural;neural;datasets da dg;training domain;datasets da;block neural;alexnets training domain;model learn;extracts textural;matrix captures textural;differentiable neural network;textural;building block neural;textural information forces;new differentiable neural;synthetic datasets;model learn representations;differentiable neural;projects textural information;neural network;alexnets training;neural network building;learn representations"}, "636611068825cb4b7bdab6ad16ef415adf4fb96c": {"ta_keywords": "multi domain learning;multidomain learning algorithms;multidomain learning;domain learning approaches;domain learning improvements;domain learning suggests;domain learning;questions multidomain learning;practice multi domain;biases multi domain;multi domain;label biases multi;domain specific class;class label biases;success multi domain;multi domain settings;ensemble learning effects;existing multi domain;ensemble learning;result ensemble learning;multidomain;domain specific;resemble ensemble learning;ensemble learning algorithms;biases multi;label biases;respect questions multidomain;questions multidomain;balanced class label;learning approaches", "pdf_keywords": ""}, "6f902b8128b218563b276c1ebff46ef668dcb185": {"ta_keywords": "vehicular crowd sensing;vehicular mobile crowd;crowd sensing colluding;vehicular crowd;incentivize drivers agents;mechanism crowd sensing;problem vehicular crowd;mobile crowd sensing;crowd sensing;sensing colluding agents;incentive mechanism crowd;crowd sensing fast;vehicles taxis incentive;crowd sensing design;drivers agents collect;data multiple vehicles;agents collect data;taxis incentive mechanism;sensors vehicles taxis;budget vehicular mobile;drivers agents;budget vehicular;given budget vehicular;taxis incentive;sensing colluding;colluding agents;incentive mechanism;mobile crowd;agents non colluding;important problem vehicular", "pdf_keywords": "vehicular crowd sensing;crowd sensing colluding;vehicular mobile crowd;mobile crowd sensing;crowd sensing;crowd sensing fast;crowd sensing problem;mechanism crowd sensing;vehicular crowd;problem vehicular crowd;incentive mechanism crowd;crowd sensing design;community sensing;community sensing paradigm;rising community sensing;crowd sensing mcs;urban sensing;incentivize drivers agents;diverse urban sensing;sensors vehicles taxis;sensing colluding agents;urban sensing data;mobile crowd;novel incentive mechanism;introduction mobile crowd;insensetive mechanism crowd;data multiple vehicles;drivers agents collect;abstract vehicular mobile;incentive mechanism"}, "7650d705b85dc399112a5b6a79e9c6f81c7c6146": {"ta_keywords": "extractive question answering;supervised question answering;question answering;large annotated corpora;question answering large;specific annotated corpora;annotated corpora work;annotated corpora;question answering qa;annotated corpora limited;large annotated;questions base documents;effective semi supervised;semi supervised;models task extractive;documents labelled examples;answering large domain;learning models task;semi supervised question;specific annotated;questions base;domain specific annotated;deep learning;corpora work envision;availability large annotated;task extractive;annotated;deep learning models;answering qa;corpora work", "pdf_keywords": "extractive question answering;semi supervised qa;supervised question answering;question answering;question answering qa;large annotated corpora;supervised qa;training qa models;extractive qa;model extractive qa;present semi supervised;supervised qa requires;ly semi supervised;effective semi supervised;annotated corpora;semi supervised;supervised qa set;annotated corpora construct;unlabeled corpus;models task extractive;text unlabeled corpus;semi supervised question;large annotated;question answering bhuwan;unlabeled corpus use;learning models task;answering qa;questions predicting missing;cloze questions improves;corpus use generated"}, "58834a447c749758e7f57498c6dd88a281af41a0": {"ta_keywords": "parsers exploration;training constituency parsers;parsers dynamic oracle;dynamic oracle supervision;constituency parsers exploration;alternative parsers dynamic;parser transition;alternative parsers;agnostic alternative parsers;given parser transition;oracle supervision explore;parsers exploration custom;parsers dynamic;parser agnostic alternative;oracle supervision;parsers;constituency parsers;parser;gradient method parser;parser agnostic;given parser;defined given parser;method parser agnostic;dynamic oracles provide;dynamic oracles;afforded dynamic oracle;supervision training constituency;supervision explore using;oracles provide strong;dynamic oracle", "pdf_keywords": "training constituency parsers;oracles constituency parsing;constituency parsers exploration;based parsers trained;parsers trained;transition based parsers;parsers trained english;constituency parsers languages;parsers exploration;constituency parsers;dynamic oracle supervision;constituency parsing;parser transition explore;parser transition;given parser transition;based parsers;parsers languages;parsers;parser agnostic alternative;dynamic oracles constituency;oracle supervision;gradient method parser;parser;f1 constituency parsers;parsing;parser agnostic;oracle likelihood training;treebank;parsers exploration custom;given parser"}, "1fa32503bce4f01ab2ccb65dedd374310c488fe8": {"ta_keywords": "fair machine learning;voluntarily adopt fairness;adopt fairness;partial compliance employers;adopt fairness promoting;employers partial compliance;fairness promoting intervention;fair employers;compliance outcomes;compliance outcomes gap;severe fair employers;incentive effects outcomes;outcomes fair machine;partial compliance based;compliant employers partial;fairness promoting;partial compliance;compliance employers;does partial compliance;allocation outcomes fair;learning partial compliance;compliance employers result;fair employers match;fairness desiderata compliant;incentive effects;compliant employers;outcomes fair;partial compliance finally;proportional progress compliance;effects incentive effects", "pdf_keywords": "compliance employers;partial compliance employers;partial compliance hiring;fair employers;employers partial compliance;compliance hiring generally;severe fair employers;compliance hiring;compliance employers result;fairness promoting measures;fair employers match;compliance explored;partial compliance explored;compliance outcomes;compliant employers;compliance explored paper;implications auditors;fair machine learning;effects outcomes auditing;compliance;compliance outcomes gap;implications auditors insights;compliance poses challenge;compliance design;model employment market;compliance based;discuss implications auditors;compliant employers partial;outcomes auditing;employment market leveraging"}, "6ea353ada2b89763f58d8068a74b2e6def526948": {"ta_keywords": "ddi corpus annotation;ddi corpus;corpus annotation;ddi corpus used;texts selected drugbank;corpus annotation guidelines;manually annotated corpus;annotated corpus consisting;annotated corpus;ddis biomedical texts;65 ddi corpus;annotation process;drugbank database 233;drugbank database;manually annotated;pharmacological substances detection;classification pharmacological;classification pharmacological names;annotation;selected drugbank database;annotation guidelines;corpus;annotation guide;ognition classification pharmacological;annotation guide lines;substances detection ddis;ddis medline database;detection ddis biomedical;inter annotator agreement;annotation guidelines free", "pdf_keywords": "ddi corpus annotated;annotated corpus ddi;annotated corpus ddis;corpus ddi extraction;ddi corpus;ddi corpus developed;annotated corpus pharmacological;ddi corpus used;annotation complex drug;biomedical texts ddi;texts ddi corpus;corpus ddis biomedical;manually annotated corpus;corpus developed ddi;corpus ddis;corpus ddi;corpus annotated;annotated corpus;ddis biomedical texts;corpus annotated corpus;elsevier ddi corpus;producing annotated corpus;pharmacological text processing;ddi extraction 2013;ddi extraction explored;information extraction;annotated resource pharmacological;information extraction techniques;corpus;manually annotated"}, "c507ad8b7bec5d29da7cf0ee92e2bf4361a5c92f": {"ta_keywords": "deep quantization neural;deep quantization;effort deep quantization;approach deep quantization;quantization neural networks;dnn computation storage;quantization neural;deep quantization lead;reduce dnn computation;deep networks alexnet;networks dnns;significantly reduce dnn;networks deep;networks dnns typically;deep networks;quantization significantly reduce;neural networks deep;dnn computation;quantization large;reduce dnn;neural networks dnns;quantization levels layer;discovering quantization levels;deep neural networks;deep neural;networks deep neural;deep reinforcement learning;quantization levels;quantization significantly;carefully selecting quantization", "pdf_keywords": ""}, "2899eb53cddf050e3a34f07bbc0bc0ee7907d5d0": {"ta_keywords": "additions word segmentation;speech tagging;word segmentation;tagging problem japanese;speech tagging problem;adding annotated sentences;annotated sentence addition;problem speech tagging;partially annotated sentences;annotated sentences training;word segmentation problem;addition training corpus;training corpus better;sentence addition training;training corpus;sentences training corpus;corpus better;annotated sentences;segmentation problem speech;language resource additions;add new words;second adding annotated;occurrences partially annotated;adding annotated;partially annotated;corpus better entries;training corpus experimental;addition dictionary annotated;sentences training;annotated sentence", "pdf_keywords": ""}, "626f8a50a7bd24d869f25bddb6fbaa59b090268c": {"ta_keywords": "pattern recognition apparatus;provided pattern recognition;pattern recognition;pattern recognition methods;recognition apparatus generating;pattern recognition device;recognition device pattern;differs pattern recognition;recognition apparatus;recognition methods;systems improve recognition;generating plurality systems;recognition device;recognition performance comprising;device pattern recognition;recognition;comprising discriminative training;plurality systems;performance comprising discriminative;combination plurality systems;recognition performance;plurality systems combination;plurality systems improve;systems combination plurality;discriminative training;apparatus generating plurality;discriminative training session;pattern;comprising discriminative;systems", "pdf_keywords": ""}, "3681456f29398e42cc2baafb0b72d166070a3cf1": {"ta_keywords": "games policy gradient;quadratic dynamics games;sequential policy;sublinear convergence nash;stackelberg leadership model;policy gradient based;convergence nash equilibrium;sequential algorithms linear;policy gradient;free sequential algorithms;sequential algorithms;convergence nash;projection free sequential;stackelberg leadership;dynamics games policy;sequential policy updates;gradient descent ascent;adopting sequential policy;free sequential;natural gradient descent;algorithms akin stackelberg;descent ascent proposed;linear quadratic dynamics;nash equilibrium;lq games;dynamics games;stabilization indefinite cost;ascent proposed;gradient descent;nash equilibrium propose", "pdf_keywords": "quadratic dynamic games;quadratic dynamics games;policy gradient sequential;convergence policy gradient;policy global quadratic;control optimization convergence;lq dynamic games;sequential policy;global convergence policy;policy gradient;global quadratic convergence;quasi newton policy;convergence policy;dynamic games;newton policy global;optimization convergence;sequential algorithms linear;algorithms lq dynamic;sequential algorithms;dynamics games;sequential policy updates;free sequential algorithms;gradient sequential;optimization convergence analysis;iterative algorithms lq;linear quadratic dynamics;quadratic convergence rate;lq games;projection free sequential;player maximizing cost"}, "a711e02f85fa52c15df0a830a8ba88df2c3928ec": {"ta_keywords": "based dialog retrieval;dialog retrieval;dialog retrieval using;example based dialog;distributed word representations;based dialog;dialog;word representations recursive;retrieval using distributed;word representations;distributed word;representations recursive autoencoders;using distributed word;retrieval using;retrieval;recursive autoencoders;robust example based;example based;robust example;representations recursive;autoencoders;representations;example;distributed;robust;using distributed;word;recursive;using;based", "pdf_keywords": ""}, "4c67c129dab9805ab248407b77a6d542c2e40d41": {"ta_keywords": "crowdsourcing robust rank;crowdsourcing robust;robust rank matrix;adversarial crowdsourcing robust;reconstructing rank matrix;large adversarial crowdsourcing;robust rank;rank matrix completion;adversarial crowdsourcing;reconstructing rank;problem reconstructing rank;classification crowdsourced;classification crowdsourced data;rank matrix revealed;graph particular adversarial;matrix completion;particular adversarial workers;particular adversarial;crowdsourced data assumption;crowdsourced data;large adversarial;adversarial workers;original rank matrix;matrix completion results;problem classification crowdsourced;arbitrarily large adversarial;adversarial workers make;recover original rank;crowdsourcing;rank matrix particular", "pdf_keywords": "classi\ufb01cation crowdsourced data;classi\ufb01cation crowdsourced;problem classi\ufb01cation crowdsourced;rank matrix completion;methods crowdsourcing;crowdsourced data;matrix completion;crowdsourced data assumption;introduction matrix completion;outperforms adversarial;strongly outperforms adversarial;crowdsourced;art methods crowdsourcing;crowdsourcing;matrix completion 10;crowdsourcing literature synthetic;robust rank matrix;algorithm robust rank;matrix completion regime;outperforms adversarial setting;art methods adversarial;methods crowdsourcing literature;methods adversarial;adversarial;robust rank;adversarial setting;methods adversarial scenario;crowdsourcing literature;adversarial scenario;provably optimal results"}, "840fabc2a7773e1bd771f152f76210b2ea5845b9": {"ta_keywords": "beam search stochastic;stochastic beam search;search stochastic;sequence generation tasks;search stochastic process;nlp conditional poisson;generation tasks nlp;poisson stochastic beam;sequence generation;stochastic beam;efficient estimators sbs;model beam search;sequence models;beam search experiments;beam search taking;beam search;decoding strategy sequence;iteration samplek candidates;conditional poisson sampling;samplek candidates replacement;improvements high entropy;turning beam search;sets sequence models;search taking;conditional poisson stochastic;poisson sampling;efficient estimators;search experiments;estimators sample diverse;poisson stochastic", "pdf_keywords": "stochastic beam search;beam search stochastic;stochastic beam;poisson stochastic beam;decoding algorithm stochastic;search stochastic;search stochastic process;beam search;modi\ufb01cation beam search;turning beam search;beam search simple;algorithm stochastic;mainstay decoding algorithm;algorithm stochastic process;decoding algorithm;poisson stochastic;conditional poisson stochastic;decoding;stochastic;mainstay decoding;beam search turn;stochastic process build;sequence models;stochastic process;beam;turn mainstay decoding;build consistent estimators;search;sets sequence models;stochastic process conditional"}, "509b42fc150a057a64c4608f64e779ef04fdff47": {"ta_keywords": "entity recognition nlp;named entity recognition;entity recognition;nlp tasks;nlp tasks similar;entity recognition analysis;recognition nlp tasks;temporal information documents;recognition nlp;natural language processing;use temporally diverse;predictions text data;text data evolves;text empirically;improvement named entity;language processing models;described text empirically;nlp;better use temporally;named entity;temporal information analyze;temporally diverse;natural language;temporally diverse training;text empirically demonstrate;language processing;predictions text;temporal drift performance;diverse training data;performance temporal information", "pdf_keywords": ""}, "11c6d0851152b6bec34726be40d90bea8d8a90f0": {"ta_keywords": "instances annotators crowdsourcing;annotators crowdsourcing;annotators crowdsourcing provides;annotation noise common;annotation noise;new crowdsourcing;crowdsourcing;crowdsourcing provides;noise annotation;noise annotation use;realize new crowdsourcing;new crowdsourcing model;shared annotators;crowdsourcing model end;crowdsourcing provides practical;decompose annotation noise;crowdsourcing model;layers shared annotators;difficulty annotator expertise;annotator expertise;annotator expertise instance;source noise annotation;instances annotators;annotators;shared annotators capture;annotators capture commonly;expertise instance annotator;respect instances annotators;instance difficulty annotator;annotators capture", "pdf_keywords": "annotation noise sources;annotation noise common;annotation noise;noise annotation use;noise annotation;crowdsourcing datasets;noise based annotators;crowdsourcing datasets image;crowdsourcing;public crowdsourcing datasets;new crowdsourcing;public crowdsourcing;new crowdsourcing model;crowdsourcing model;crowdsourcing model end;source noise annotation;decompose annotation noise;annotators empirically;based annotators instances;realize new crowdsourcing;hypothesis public crowdsourcing;instances annotators empirically;based annotators;shared annotators;annotators;annotators instances;annotators empirically evaluate;annotator expertise;annotators capture commonly;layers shared annotators"}, "6c520d983923dbe1e437c01086424fdcdd8f430a": {"ta_keywords": "speech synthesis quality;synthetic speech quality;improvements synthetic speech;speech synthesis;parametric speech synthesis;synthesis synthetic speech;speech synthesis synthetic;gv generated speech;speech synthesis including;speech synthesis offers;concatenative speech synthesis;synthetic speech;ms synthetic speech;generated speech parameter;generated speech;synthetic speech close;speech quality proposed;statistical parametric speech;voice conversion;speech quality;quality natural speech;tts voice conversion;modify ms utterance;speech quality good;voice conversion vc;generation algorithm hmm;speech tts voice;speech parameter;effective hmm based;text speech", "pdf_keywords": ""}, "5f46d8e18138fe572b6fae897475a2ad645a3e1a": {"ta_keywords": "models automatic speech;external language models;speech recognition asr;lstms hidden markov;asr based deep;audio transcript training;model hmm;rnn asr model;transducer rnn asr;generalize voice search;voice search;automatic speech;speech recognition;voice search data;specifically rnn model;model hmm framework;target domain rnn;transducer rnn;rnn model trained;asr specifically rnn;domain rnn lm;automatic speech recognition;rnn model;matched domain rnn;network transducer rnn;language models lms;domain rnn;audio transcript data;rnn asr;recurrent neural network", "pdf_keywords": "rnn model voice;model voice search;voice search cross;transducer rnn asr;transducer rnn;lstms hidden markov;voice search speech;rnn asr model;network transducer rnn;asr based deep;search speech data;trained rnn model;voice search audio;voice search;rnn model;end rnn model;model hmm;rnn asr;youtube voice search;youtube trained rnn;100 voice search;trained rnn;target domain rnn;matched domain rnn;dnns lstms;rnn model applied;rnn model followed;neural network transducer;model voice;voice search text"}, "2ce3428ba8777c723b9b12e9f8eaeb2c87a5a793": {"ta_keywords": "rationale individually learns;supervision boosting;rationales sentence level;direct supervision boosting;annotation rationales;supervision target task;supervision boosting performance;annotation rationales employ;supervision costly annotation;task based rationale;trustworthiness model prediction;best identifying textual;costly annotation rationales;supervision target;task evaluating trustworthiness;output faithful rationales;textual spans;identifying textual;faithful rationales sentence;identifying textual spans;textual;selecting correct rationales;learns;based rationale individually;learns assign;target task evaluating;sentence level;sentence level solely;learns assign high;learning", "pdf_keywords": "bert reasoning tasks;rationale supervision;rationale supervision applied;experimentally rationale supervision;reasoning tasks supervision;standard bert reasoning;bert reasoning;supervising target rationale;supervising rationale;supervision target task;supervising rationale analysis;tasks supervision improve;reasoning tasks;rationale predicts label;predict faithful explanation;best rationale predicts;performance supervising rationale;tasks supervision;rationales sentence level;rationale predicts;extract faithful rationales;selects best rationale;target rationale;bert;outputting faithful rationales;supervision target;quantify correct predictions;evidence predict;exploiting knowledge selected;correct evidence predict"}, "cd1d915604826e5fb0ba2bbcdf8479a9b90fb289": {"ta_keywords": "wireless relay placement;relay placement random;sequential wireless relay;relay placement;optimal sequential wireless;placement random lattice;wireless relay;sequential wireless;random lattice path;relay;random lattice;placement random;lattice path;optimal sequential;lattice;wireless;optimal;placement;random;sequential;path", "pdf_keywords": "optimal place relay;relay constraint optimal;impromptu deployment relay;multihop wireless path;impromptu optimal deployment;distance threshold policy;deployment multihop wireless;wireless path packet;deployment relay nodes;relay constraint;wireless network random;given relay constraint;deployment relay;communication path control;multihop wireless network;relay nodes;path packet communication;relay nodes establishing;impromptu deployment multihop;optimal deployment multihop;packet communication path;threshold distance optimally;network random lattice;distance optimally chosen;wireless path;create multihop wireless;crossing optimal;crossing optimal place;distance optimally;place relay"}, "a425a11b9b249cb768d0f54d4a32f4f1d007e279": {"ta_keywords": "online learning algorithms;pass online learning;pass online learners;traditional batch learning;online lear;online lear ners;practice online lear;comparable batch learners;batch learning propose;online learning methods;tasks online learning;batch learners;online learning training;batch learning;online learning;svm nlp;batch learning methods;svm nlp tasks;single pass online;training single pass;online learners;linear svm nlp;learning training single;learners traditional batch;batch learners investigate;passes training data;pass online;nlp tasks online;lear ners frequently;linear svm", "pdf_keywords": ""}, "2d71fb62c71e49479c1b6ce832ee1bb88df20556": {"ta_keywords": "description logics computing;description logics;subsumer set descriptions;common subsumer understood;common subsumer attribute;subsumer pair descriptions;subsumer attribute chain;computing common subsumer;computation common subsumer;subsumer attribute;common subsumer set;operation description logics;set commonalities descriptions;subsumer understood problem;logics computing common;common subsumer;subsumer understood;operation inductive learning;attribute chain equalities;structural subsumption operation;inductive learning;subsumer set;case structural subsumption;structural subsumption;common subsumer pair;tractability computing common;set descriptions;logics computing;inductive;subsumption operation", "pdf_keywords": ""}, "6fa85c46ea68c754ef903edc70058ba525f1fc4d": {"ta_keywords": "representation learning skill;like intelligent agent;intelligent agent;skill learning human;integrating representation learning;learning human like;learning human;learning skill learning;skill learning;representation learning;learning skill;human like intelligent;agent;representation;like intelligent;intelligent;integrating representation;learning;skill;human like;human;like;integrating", "pdf_keywords": ""}, "0c89b1ec80de46222ed7efc6261c03e52a1e2c54": {"ta_keywords": "neural description model;description decoder;propose neural description;neural description;phrase natural language;learning unknown phrases;context encoders description;encoders description decoder;description decoder contrast;dictionaries definitions search;natural language;contexts humans;global contexts;contexts humans figure;dictionaries definitions;phrases local global;describing;definitions search;context consult dictionaries;contexts;global context;global contexts humans;description model;meaning expressions;wordnet oxford;urban dictionaries;phrases local;oxford urban dictionaries;wordnet;definitions search documents", "pdf_keywords": ""}, "b2b9b0d7afd85c5d708b79a61d9a000c6c906d8c": {"ta_keywords": "similarity measures parsed;walk based similarity;parsed text corpus;word similarity measure;learning graph walk;syntactic relations graph;measures parsed text;text corpus;similarity measure graph;word similarity;graph walk based;text corpus instance;graph walks;relations graph walks;specific word similarity;nodes represent words;consider parsed text;corpus;similarity measures;based similarity measures;words weighted directed;labelled directed graph;graph walk;similarity measure;graph learning;parsed text propose;parsed text;directed graph nodes;graph learning graph;graph walks combined", "pdf_keywords": ""}, "3042bc348d6cc7959cd574756f720e5afad236de": {"ta_keywords": "deep drawing paperboard;drawing paperboard trays;paperboard deep drawing;drawing paperboard used;drawing paperboard;paperboard used paper;paperboard trays;paperboard process;paperboard process parameters;type paperboard process;paperboard trays tray;type paperboard;paperboard used;paperboard deep;study type paperboard;paperboard;factors paperboard deep;paper anisotropic mechanical;influence factors paperboard;used paper anisotropic;factors paperboard;paper anisotropic;tray shallow rectangle;trays tray shallow;used paper;tray shallow;paper;suitable deep drawing;indented walls experimental;shallow rectangle box", "pdf_keywords": ""}, "39c5cfc0ff6660a17364cb4af1eb071d6efa463d": {"ta_keywords": "mechanical turk variety;amazon mechanical turk;mechanical turk;tasks noise ordinal;comparative ordinal measurements;ordinal measurements better;models ordinal measurements;eliciting judgements humans;prediction tasks noise;noise ordinal measurements;turk variety tasks;ordinal measurements characterize;noise ordinal;ordinal measurements sufficiently;better compare score;scoring cardinal comparative;judgements humans unknown;ordinal approach results;eliciting judgements;selection measurement;ordinal measurements;comparative ordinal;selection measurement scheme;compare score;models ordinal;tasks pairwise comparative;judgements humans;prediction tasks;ordinal approach;humans unknown quantity", "pdf_keywords": "amazon mechanical turk;mechanical turk variety;mechanical turk;tasks noise ordinal;crowdsourcing choice evaluation;evaluators using crowdsourcing;crowdsourcing;crowdsourcing choice;turk variety tasks;using crowdsourcing choice;using crowdsourcing;prediction tasks noise;noise ordinal;collection estimate noise;noise ordinal measurements;estimating;expert human evaluators;estimating unknown;comparative ordinal measurements;cardinal measurement models;ordinal measurements better;models ordinal;ordinal measurements suf\ufb01ciently;noise levels ordinal;models quantify noise;error rates estimating;prediction tasks;estimate noise data;empirical evidence;provide empirical evidence"}, "67b29c3fe6f110125a8892e8ed128d20b23957ea": {"ta_keywords": "lingual entity linking;entity linking xel;entity linking experiments;entity linking;entities source language;linking accuracy xel;cross lingual entity;linking xel;candidate generation disambiguation;generation disambiguation;lingual entity;resource cross lingual;disambiguation make better;low resource languages;entity candidate generation;generation disambiguation make;disambiguation;named entities source;linking accuracy;entities source;xel challenging languages;wikipedia zero resource;improvements entity candidate;end linking accuracy;named entities;linking;resource languages;entity candidate;english knowledge base;resource languages model", "pdf_keywords": "lingual entity linking;entity linking xel;entity linking;resource cross lingual;entities source language;lingual entity;cross lingual entity;disambiguation model;disambiguation model baseline;resource languages lowresource;high resource languages;generation disambiguation;resource languages;entity linking shuyan;xel high resource;languages lowresource;lowresource languages;generation model disambiguation;model disambiguation model;languages lowresource languages;lowresource languages work;linking xel;generation disambiguation make;disambiguation;candidate generation disambiguation;model disambiguation;named entities source;entities source;disambiguation make better;abstract cross lingual"}, "2a64da1ed300e49f2d665312146c8bb2f66920b7": {"ta_keywords": "translation smt optimization;statistical machine translation;machine translation survey;maximize translation accuracy;machine translation smt;machine translation;translation accuracy fundamental;translation accuracy;parameters maximize translation;maximize translation;translation survey;translation smt;batch online optimization;losses optimization statistical;optimization smt;optimization statistical machine;smt optimization;discriminative models;smt optimization parameters;minimization ranking;discriminative models och;optimization statistical;minimizing losses optimization;online optimization;minimization ranking appropriate;error minimization maximum;risk minimization ranking;online optimization specifically;research optimization smt;error minimization", "pdf_keywords": ""}, "d530a007ae0493ef6a8167c25bd007104623c504": {"ta_keywords": "decompiled code renaming;decompiled identifier naming;decompiled identifier renaming;code renaming;naming propose decompiled;propose decompiled identifier;decompiled identifier;identifier renaming engine;code renaming use;tools examining binaries;identifier renaming;approach decompiled identifier;decompiled code;identifier naming;identifier naming propose;predict variable names;examining binaries corresponding;examining binaries;binaries generated projects;corresponding source code;binaries generated;models decompiled code;generating corpora;variable names;variable names identical;binaries corresponding source;technique generating corpora;source code;generating corpora suitable;decompiler common tools", "pdf_keywords": "decompiled code renaming;names decompiled code;variable names decompiled;names decompiled;decompiled identi\ufb01er renaming;code renaming;code renaming use;predict variable names;variable names;generating corpora;decompiled code propose;code propose decompiled;generating corpora suitable;technique generating corpora;tools examining binaries;variable names identical;standard variable names;decompiled code;models decompiled code;identi\ufb01er renaming;variable names ii;identi\ufb01er renaming engine;renaming;corresponding source code;propose decompiled identi\ufb01er;training lexical;parse trees annotated;renaming use;lexical structural information;examining binaries corresponding"}, "bc1bf0a21d7838ec167e77c76163afc1f5f76c3d": {"ta_keywords": "multi channel eeg;channel eeg observed;channel eeg;eeg observed signal;channel eeg signal;eeg signal;removing noise event;eeg observed;eeg signal according;noise removal single;noise removal;noise event related;noise event;removing noise;background noise removal;eeg;separated multiple signals;covariance matrices frequency;potentials erps recorded;multi channel wiener;observed signal separated;grouped covariance;related potentials erps;erps recorded multi;signal separated multiple;event related potentials;erps recorded;grouped covariance matrices;channel wiener filter;signal separated", "pdf_keywords": ""}, "4c9f20ce99f9b93527fd76ec04a44fcef9082005": {"ta_keywords": "fairness interactive recommendation;recommendation reinforcement learning;recommendation reinforcement;interactive recommendation reinforcement;accuracy fairness interactive;fairness interactive;fairrec improve fairness;user preferences fairness;recommendation quality fairrec;fairness irs user;accuracy fairness irs;accuracy fairness balancing;fairrec dynamically;fairness preserving good;balancing accuracy fairness;fairness balancing accuracy;fairrec dynamically maintain;improve fairness preserving;preferences fairness;preferences fairness status;fairness preserving;accuracy fairness;fairness irs;combines accuracy fairness;learning interactive recommender;fairness status constantly;validate fairrec improve;fairness balancing;based framework fairrec;fairrec aims maximizing", "pdf_keywords": "fairness aware recommendation;accuracy fairness irs;fairness status irs;fairness irs;fairness proposed framework;accuracy fairness proposed;model fairness aware;propose fairness aware;fairness aware;fairrec dynamically;fairness irs conclusion;current fairness status;fairness status;fairness proposed;state space reward;model fairness;accuracy fairness;based framework fairrec;fairrec dynamically maintain;reward function user;accuracy fairness long;preference state ups;aware recommendation irs;current fairness;propose fairness;accuracy fairness experimental;combine accuracy fairness;utilized current fairness;framework fairrec dynamically;decision process irs"}, "cb153d8469ac466606032ea457b934bc61ae86ae": {"ta_keywords": "detection fake news;detecting fake news;fake news detection;news detection fake;emotion fake news;fake news detectors;dual emotion fake;news detectors effectively;people emotions news;news detectors;dual emotion features;news detection;news detection exhibit;mining dual emotion;news mining dual;emotion distinctive fake;emotions news comments;fake news evokes;emotions news;fake news mining;news detectors enhancement;emotion features;news evokes;emotion fake;dual emotion distinctive;existing fake news;emotion features represent;emotion appears fake;news mining;represent dual emotion", "pdf_keywords": ""}, "029fa34b291c3f60b8a00cdf386e6048d45c394d": {"ta_keywords": "membership clustering spectral;mixed membership clustering;membership clustering;clustering spectral;alternative spectral clustering;clustering spectral clustering;spectral clustering;node clustering;spectral clustering methods;node clustering methods;based node clustering;membership alternative spectral;clustering methods elegant;clustering;clustering methods;clustering methods allow;node centric representation;clustering methods general;representation edge centric;centric representation edge;competitive mixed membership;mixed membership alternative;approach mixed membership;graph based node;graph based;representation edge;effective graph based;mixed membership;membership alternative;node centric", "pdf_keywords": ""}, "04b876e95ac3e4754c8f0c8a9355e7acc3dc70b9": {"ta_keywords": "addition japanese morphological;japanese morphological analysis;japanese morphological;language resource addition;resource addition japanese;segmentation speech tagging;training corpus better;corpus better addition;speech tagging;training corpus;additivity language resources;addition annotated sentences;corpus better;speech tagging strategy;sentences training corpus;language resources task;addition japanese;annotated sentences training;language resource;trained language resources;various language resource;training corpus comparative;adding annotated sentences;corpus;word segmentation speech;task word segmentation;corpora methods language;word segmentation;morphological analysis;dictionaries corpora methods", "pdf_keywords": ""}, "7393d2618c7478d937112865458862e8d5f10475": {"ta_keywords": "domain reasoning template;sequence models domains;domains study prompttemplate;reasoning template filling;reasoning template;sequence sequence models;cross domain reasoning;domain reasoning;sequence models perform;sequence models;reasoning cross domain;study prompttemplate filling;pretrained sequence sequence;prompt template filling;prompttemplate;prompt template;models domains;pretrained sequence;enable sequence sequence;prompttemplate filling;perform crossdomain reasoning;crossdomain reasoning cross;enables pretrained sequence;study prompttemplate;models domains paper;template filling approach;filling present prompt;present prompt template;health wellbeing domains;approach enable sequence", "pdf_keywords": "reasoning commonsense health;cross domain reasoning;domain reasoning task;domain reasoning template;domain reasoning \ufb01lling;reasoning commonsense;reasoning task study;domain reasoning challenge;domain reasoning;commonsense health domain;reasoning challenge prompt;reasoning template;health domain propose;reasoning \ufb01lling;concepts commonsense food;health domain;reasoning task;health wellbeing domains;commonsense food habits;domains study prompttemplate;casestudy commonsense health;usecase reasoning commonsense;commonsense health;reasoning template filling;model cross domain;reasoning \ufb01lling templates;commonsense health wellbeing;reasoning present;reasoning challenge;reasoning"}, "7137a842d496a1a5581db31ad946fa0c0827e663": {"ta_keywords": "learning robust nonlocalmodels;robust nonlocalmodels;driven learning robust;nonlocalmodels;learning robust;data driven learning;driven learning;data driven;robust;learning;driven;data", "pdf_keywords": ""}, "f735f5f55cbc5a9d372ea1cd9b4e81d35f043a00": {"ta_keywords": "models pairwise comparisons;model pairwise comparisons;pairwise comparisons;pairwise comparisons statistical;pairwise comparisons probabilities;analyzing pairwise comparison;models analyzing pairwise;pairwise comparison data;comparisons statistical computational;transitive models pairwise;flexible model pairwise;stochastically transitive models;pairwise comparison;models pairwise;models computing minimax;comparisons statistical;comparisons probabilities outcomes;model pairwise;minimax rate stochastically;stochastically transitive model;various parametric models;rate stochastically transitive;thresholding algorithm statistically;comparisons probabilities;value thresholding;transitive models;comparison data;stochastic transitivity;parametric models analyzing;stochastic transitivity despite", "pdf_keywords": "broader stochastically transitive;stochastically transitive model;stochastically transitive class;stochastic transitivity;estimator stochastically transitive;classes stochastically transitive;stochastically transitive;stochastic transitivity section;models computing minimax;form stochastic transitivity;transitive model;minimax optimal estimator;optimal estimator stochastically;models broader stochastically;algorithms achieve minimax;transitive model non;minimax optimal;model pairwise comparisons;sub classes stochastically;computing minimax optimal;broader stochastically;classes stochastically;\ufb02exible model pairwise;minimax rate;achieve minimax rate;transitive class;minimax rate interesting;minimax;pairwise comparisons probabilities;achieve minimax"}, "380278716f4d78ad9dcc3ece9e12b235ca1d1569": {"ta_keywords": "reasoning deep learning;logic called tensorlog;tensorlog classes logical;tensorlog;logical reasoning deep;called tensorlog;deep learning infrastructure;tensorflow;tensorlog classes;infrastructure tensorflow theano;called tensorlog classes;results tensorlog;deep learning frameworks;infrastructure tensorflow;parameters probabilistic logic;tensorflow theano;probabilistic logic;reasoning deep;tensorlog scales;deep learning;network infrastructure tensorflow;high performance deep;probabilistic logical;logical queries compiled;neural network infrastructure;experimental results tensorlog;probabilistic order logic;results tensorlog scales;probabilistic logical reasoning;tensorlog scales problems", "pdf_keywords": "probabilistic deductive databases;deductive databases;deductive knowledge graphs;deductive databases containing;deductive databases introduce;deductive dbs;deductive databases prddbs;deductive knowledge graph;dkg deductive databases;programs deductive databases;logic programs deductive;background deductive dbs;logic called tensorlog;deductive dbs section;probabilistic deductive;knowledge graphs ptree;deductive knowledge;tensorlog classes logical;de\ufb01nitions logic programs;tensorlog deep;tensorlog deep learning;knowledge graph dkg;tensorlog;term deductive knowledge;stochastic 23 deductive;23 deductive knowledge;knowledge graphs;logic programs;tensorlog classes;inference ptree sdkgs"}, "8a880680b28dee5642ac88431b3ae1085b911f96": {"ta_keywords": "consistently improves translation;neural machine translation;improves translation quality;translation models little;penalize translations different;penalize translations;improves translation;machine translation models;translation quality;translation models;vietnamese translation task;translation quality multiple;machine translation;languages best regularizer;frequencies penalize translations;translations;translation task;low resource languages;translation task simply;translations different;translations different input;language pairs neural;sentences consistently improves;iwslt15 english vietnamese;differences punctuation regularizer;ter score language;resource languages best;punctuation regularizer;score language pairs;english vietnamese", "pdf_keywords": ""}, "4f7b108830de2e7964b6e1a89bf1c2da60140a34": {"ta_keywords": "latent representation learning;representation learning data;effectively variational autoencoder;representation learning;representation learning compared;variational autoencoder;representation learning framework;effective representation learning;reconstruction latent representation;variational autoencoder vae;likelihood reconstruction latent;deep latent variable;latent representation;autoencoder vae powerful;trained effectively variational;powerful language model;latent variable modeling;modeling text;deep latent;autoencoder vae;language model effective;model effective representation;fix deep latent;autoencoder;variable modeling text;language model;likelihood reconstruction;goals representation learning;modeling text combination;reconstruction latent", "pdf_keywords": "vae training inference;variational autoencoders;variational autoencoders vaes;learning pretraining inference;modeling learning representations;pretraining inference;learning latent variable;pretraining inference network;particular variational autoencoders;improving vae learning;vae learning pretraining;training inference;vae learning;autoencoders vaes;learning latent;modeling learning;training inference network;autoencoders;representation learning data;language modeling learning;learned latent space;learning representations;latent variable models;autoencoder objective;autoencoders vaes kingma;autoencoder;inference network;learning representations particular;representation learning;biasing learning"}, "14119210e5f9e0d962e329c833557dfb5524c4bd": {"ta_keywords": "electrolyte structure ices;lithium oxygen batteries;3d sio2 nanofibers;cathode electrolyte structure;integrated cathode electrolyte;oxygen batteries sslobs;electrolytes flexible integrated;state lithium oxygen;lithium oxygen;oxygen batteries lobs;cathode electrolyte;sio2 nanofibers;electrolyte structure porous;liquid electrolytes flexible;solid polymer electrolyte;oxygen batteries;electrolyte structure;electrolyte supporting 3d;cathode electrolyte supporting;polymer electrolyte;polymer electrolyte ability;sio2 nanofibers nfs;electrolytes flexible;solid state lithium;intimate cathode electrolyte;organic liquid electrolytes;porous sio2 nfs;liquid electrolytes;sslobs making batteries;batteries sslobs", "pdf_keywords": ""}, "de8ded0d66f3227d99751a89fdd5f4b438d6e8ee": {"ta_keywords": "separation robust speech;source separation robust;robust speech analysis;robust speech;separation robust;speech analysis recognition;source separation;speech analysis;application source separation;analysis recognition;separation;recognition;robust;speech;analysis;application source;application;source", "pdf_keywords": ""}, "e74d7523d7d96ab65f05f059284f9d0a994bb074": {"ta_keywords": "annotated treebank ted;annotated syntactic parsing;annotated treebank;manually annotated treebank;talk treebank;ted talk treebank;treebank ted talks;syntactic parsing resources;talk treebank paucity;manually annotated syntactic;annotated syntactic;syntactic parsing;treebank ted;treebank;parsing resources speech;treebank paucity manually;syntactic parsing fundamental;treebank paucity;syntax speechrelated applications;syntax speechrelated;parsing resources;parsing;speech translation naist;natural language processing;related speech translation;sentence segmentation;interaction syntax speechrelated;speech translation;modeling sentence segmentation;machine translation language", "pdf_keywords": ""}, "3efee0095cb578659dfaaf0d87a616f133ecf85c": {"ta_keywords": "speech recognition chime;data augmentation approaches;recognition chime challenge;chime challenge recognize;recognition chime;microphone arrays finally;advanced chime;tools advanced chime;data augmentation;augmentation approaches neural;speech recognition;speech dereverberation beamforming;speech recorded multiple;university speech recognition;microphone arrays;advanced chime recipe;multiple microphone arrays;architectures end speech;chime challenge;acoustic modeling efforts;summarizes acoustic modeling;augmentation approaches;chime recipe;speech recorded;microphone;explore data augmentation;chime;party speech recorded;chime recipe explore;acoustic modeling", "pdf_keywords": ""}, "9896a68e999298410bf16ffd08e8e67a54ad6a91": {"ta_keywords": "orchestrating natural language;software distributed cloud;software integrate cloud;processing software cloud;use software distributed;software distributed;orchestration natural language;language processing tools;language processing software;distributed cloud computing;integrate cloud computing;distributed data processing;software cloud;gearman framework middleware;distributed cloud;software cloud computing;middleware orchestrating natural;document collection distributed;integrate cloud;data processing tools;middleware orchestrating;cloud computing;gearman framework;specific middleware orchestrating;cloud computing environment;processing software integrate;natural language processing;processing tools relatively;processing tools;software integrate", "pdf_keywords": ""}, "52c040c4b1786166325a0d930af94a529e2b5023": {"ta_keywords": "network speaker adaptation;speaker adaptation;dnn adaptation technique;speaker adaptation similarly;propose dnn adaptation;dnn adaptation;neural network speaker;adapted dnn;fmllr adapted dnn;summarizing neural network;acoustic summary utterance;sequence summarizing neural;vector extractor ssnn;representing acoustic summary;summarizing neural;utterance appending vector;replaced sequence summarizing;ssnn produces summary;summary vector fbank;network speaker;summary utterance appending;neural network ssnn;extractor ssnn;summary utterance;adaptation similarly vector;adaptation technique vector;vector fbank features;ssnn vector;sequence summarizing;ssnn vector appended", "pdf_keywords": ""}, "da564ff902a5490088f60c9fb100531fc9f97288": {"ta_keywords": "inference large database;probabilistic logic;probabilistic language;probabilistic language suited;pagerank locally groundable;order probabilistic logic;programming personalized pagerank;order probabilistic language;probabilistic order logic;database facts groundings;inference independent database;probabilistic logic key;grounding query mapping;large database facts;information extraction;personalized pagerank;grounding query;grounding particular query;making inference learning;propositional inference large;personalized pagerank locally;groundable order probabilistic;queries typically;formalized inference;pagerank;retrieval information extraction;entity resolution;query mapping propositional;information extraction information;entity resolution task", "pdf_keywords": "inference personalized pagerank;computation personalized pagerank;stochastic logic programs;personalized pagerank conclusions;variant personalized pagerank;inference personalized;personalized pagerank ppr;personalized pagerank process;personalized pagerank;pagerank conclusions described;pagerank ppr;pagerank ppr 12;logic programs inference;extension stochastic logic;pagerank conclusions;pagerank;related personalized pagerank;version inference personalized;personalized pagerank vector;approximate inference scheme;stochastic logic;pagerank process;pagerank vector v0;approximate inference;pagerank process graph;e\ufb03cient inference rapid;inference rapid;pagerank vector;derivations related personalized;proof language"}, "27724bd19946d6a824d06cdca3cdfe5d40f71003": {"ta_keywords": "predicting edit completions;predicting edit;predict completion edit;modeling edits directly;modeling edits;trained past edits;edit completions based;likelihood edit learning;syntactic models learn;edit completions;edit learning;learn generate edited;predict completion;completions based learned;accuracy syntactic models;opposed modeling edits;generate edited code;problem predicting edit;abstract syntax tree;edits given code;editcompletion task;neural crfs allows;completion edit;syntax tree;past edits;edit learning likelihood;syntactic models;target edit structural;neural crfs;editcompletion", "pdf_keywords": ""}, "0cee58946a13a5c2845647b4af8b9d2bf52a8b6b": {"ta_keywords": "entity recognition distant;entity recognition ner;named entity recognition;entity recognition;models distant supervision;annotations yields highly;language models bert;recognition distant supervision;trained language models;labels external knowledge;models bert;language model ner;bond bert assisted;manual annotations yields;trained language model;annotations;annotations yields;ner models distant;external knowledge bases;distant labels external;named entity;bond bert;manual annotations;distant labels;language models;performance bond bert;bert assisted open;prediction performance ner;using distant labels;recognition ner", "pdf_keywords": "trained language models;trained language model;distantly supervised ner;language models task;language models bert;supervised ner methods;existing distantly supervised;supervised ner;language models;language model mlm;trained language;noisy incomplete annotation;distantly supervised;language model ner;label iteratively language;predicting missing words;pre trained language;distant label iteratively;iteratively language model;language model;improve recall precision;prediction performance ner;models bert;soft labels data;supervised;annotation;incomplete annotation;language model \ufb01rst;recall precision second;model ner tasks"}, "8274799029bfac4402685e1efd995a8aeb9e7426": {"ta_keywords": "sequence sequence autoencoder;sequence autoencoder;sentence compression sequences;sequence autoencoder seq\u02c63;compressed sentences human;sequence compressed sentence;abstractive sentence compression;compressed sentences;latent word sequences;sentence compression;neural sequence sequence;encourages compressed sentences;sequence compressed;input reconstructed sentences;sequence sequence models;pretrained language model;neural sequence;input neural sequence;compressed sentence;sequences encourages compressed;word sequences;sequence models;sequence models currently;autoencoder;prior latent sequences;autoencoder seq\u02c63 consisting;autoencoder seq\u02c63;compressed sentence present;compression sequences;reconstructed sentences", "pdf_keywords": "abstractive sentence compression;sequence sequence autoencoder;sequence autoencoder;sequence compressed sentence;sentence compression;sentence compression sciences;sequence autoencoder seq3;sequence seq3 autoencoder;sentence compression additional;abstract neural sequence;unsupervised abstractive sentence;sequence compressed;sequence autoencoder dubbed;input reconstructed sentences;compressed sentence;sentence compression \ufb01rst;neural sequence sequence;sequence sequence models;autoencoder seq3 consisting;reconstructed sentences;seq3 unsupervised abstractive;compressed sentence present;2019 abstract neural;compression \ufb01rst sequences;neural sequence;seq3 autoencoder trained;autoencoder seq3;autoencoder trained parallel;sequence models;performance unsupervised abstractive"}, "927efd299cffcfca3716efefcc904331b70c153e": {"ta_keywords": "question answering dataset;graph question answering;question answering;bilingual qa dataset;conversational bilingual qa;diverse question answering;question answering qa;answering qa datasets;interpretable reasoning graph;qa models trained;models qa tasks;generating reasoning graph;noahqa conversational bilingual;bilingual qa;learning models qa;develop interpretable reasoning;generating reasoning;qa datasets noahqa;answering qa;reasoning interpretable graph;answering dataset;reasoning graph metric;measure answer quality;qa tasks;reasoning graph;qa tasks existing;interpretable reasoning;reasoning interpretable;reasoning graph reasoning;existing qa datasets", "pdf_keywords": "reasoning graphs;evaluating reasoning graph;bilingual qa dataset;interpretable reasoning graph;truth reasoning graphs;conversational bilingual qa;generating reasoning graph;graph evaluating reasoning;bilingual qa;reasoning graph introduce;reasoning graph evaluating;reasoning graph metric;reasoning graph;qa datasets noahqa;evaluating reasoning;generating reasoning;reasoning graph reasoning;qa models trained;questions interpretable reasoning;new qa datasets;noahqa conversational bilingual;semantic similarity;math23k test english;existing qa datasets;structural semantic similarity;graph reasoning graph;interpretable reasoning;qa datasets complex;qa models;qa datasets"}, "6a116b897569fe4d6ea9ad4c3ba9a18825b96f49": {"ta_keywords": "learning logical rules;differentiable learning logical;rules knowledge base;logical rules knowledge;learning logical;knowledge base completion;rules knowledge;probabilistic logical rules;completion learned models;learned models composed;perform knowledge base;differentiable model learning;knowledge base;completion differentiable learning;learns sequentially;rules unfortunately learning;base completion learned;learned models;neural controller learns;level logical reasoning;composed probabilistic logical;tasks knowledge base;model learning sets;high level logical;probabilistic logical;logical rules;differentiable learning;logical rules useful;reasoning tasks;learns sequentially compose", "pdf_keywords": "neural logic programming;neural logic;framework neural logic;logic programming combines;representations logical rules;inductive logic programming;knowledge base reasoning;logic programming;base reasoning tasks;rules design neural;logic programming outperforms;learn operator memory;model logical rules;operator memory attention;operations neural controller;tensorlog;operations used tensorlog;learn parameters structure;operations neural;used tensorlog;neural controller learn;compose operations neural;order logical rules;learns compose operations;memory attention vectors;knowledge base;natural language jointly;neural lp;representations logical;model logical"}, "3c0e8f7337491ca4f714de14021eb23ca43d1d5e": {"ta_keywords": "speech recognition reverberant;speech recognition asr;robust speech recognition;aspire automatic speech;robustness automatic speech;robust speech;recognition reverberant environments;speech recognition;speech enhancement neural;mismatch robust speech;speech enhancement;speech recognition unknown;automatic speech recognition;recognition reverberant;acoustic model adaptation;microphones noisy reverberant;data telephone speech;automatic speech;reverberant environments challenge;recognition asr systems;includes speech enhancement;reverberant rooms training;recognition asr;reverberant noisy;reverberant noisy conditions;noisy reverberant rooms;recognition unknown reverberant;degradation noisy audio;noisy reverberant;reverberant environments", "pdf_keywords": ""}, "6c78bac2dd71efb89951d9bab72c8129bbc07f67": {"ta_keywords": "regularize topic models;topic models latent;content reviews regularization;regularize topic;reviews regularization framework;regularization exploiting sentiment;reviews regularization;topic models;topic models overly;used regularize topic;modeled topic models;topic models prefer;sparsity topic distributions;permitted topic models;topic distributions;regularization technique latent;sentiment indicative features;variable based regularization;exploiting sentiment indicative;topic distributions documents;regularization introduced paper;regularization framework;based regularization;regularization introduced;regularization;exploiting sentiment;regularization framework used;reviews based content;based content reviews;utility regularization", "pdf_keywords": ""}, "ce45aa1c64da82bfd02db0e147efa268da6980e4": {"ta_keywords": "sub carriers ofdm;carriers ofdm;carriers ofdm studied;bits sub carriers;ofdm studied article;ofdm;ofdm studied;bit loading algorithms;sub carriers;allocate numbers transmitted;carriers;numbers transmitted bits;transmitted bits sub;bit loading;loading algorithms investigated;transmitted bits;allocate numbers;loading algorithms;numbers transmitted;classic bit loading;bits sub;greedy algorithm analyzed;greedy algorithm;investigated greedy algorithm;loading algorithms classic;algorithms classic bit;article bit loading;algorithms investigated greedy;bits;algorithm", "pdf_keywords": ""}, "d32fb57467d64bb82dce60e904ddc5c18b3f0f91": {"ta_keywords": "spatial parking demand;parking demand quantified;curbside parking demand;parking demand;parking demand case;curbside parking increasingly;spatial parking;similar spatial parking;analysis curbside parking;curbside parking;parking increasingly;management curbside parking;demand quantified spatial;repeatability gaussian mixture;parking;parking increasingly important;data driven spatio;park harnessing data;driven spatio temporal;gaussian mixture model;transportation develop gaussian;spatio temporal analysis;drivers looking park;gaussian mixture;develop gaussian mixture;based repeatability gaussian;spatio temporal;park;repeatability gaussian;seattle department transportation", "pdf_keywords": "spatial parking demand;consistency parking demand;similar spatial parking;spatial parking;characteristics parking demand;temporal characteristics parking;parking demand;metricize consistency parking;parking demand supplement;parking demand demonstrate;parking demand quanti\ufb01ed;characteristics parking;consistency parking;repeatability gaussian mixture;observed occupancy patterns;parking;spatial demand consistent;curbside parking;similar spatial demand;gaussian mixture model;spatial demand;occupancy patterns;experiments spatial demand;spatio temporal characteristics;develop gaussian mixture;demand quanti\ufb01ed spatial;gaussian mixture;curbside parking belltown;transportation develop gaussian;parking belltown"}, "ab5c6703fceb3dce6558be309cc65a4a8615c774": {"ta_keywords": "graph based search;metric data neighborhood;approximate neighborhood graph;non symmetric distances;neighborhood graph directly;data neighborhood graphs;mapping distance symmetrization;symmetric distances resorting;distance symmetrization;neighborhood graphs;neighborhood graphs straightforward;neighborhood graph;symmetric distances;symmetrized distance;fast retrieval complex;data neighborhood;metric non symmetric;challenging non metric;distances resorting metric;fast retrieval;modified symmetrized distance;symmetrized distance improve;graphs straightforward metrization;non metric data;accurate fast retrieval;distance symmetrization turn;search algorithm relying;search algorithm;non metric;approximate neighborhood", "pdf_keywords": "searching non metric;graph based retrieval;non symmetric distances;distance symmetrization;mapping distance symmetrization;graph based search;metric data neighborhood;non symmetric distance;index neighborhood graph;symmetric distances resorting;distance metrization symmetrization;challenging non metric;non metric distances;neighborhood proximity graphs;neighborhood graph directly;metric distances directly;data neighborhood graphs;nn search graph;search graph;symmetric distances;proximity graphs deal;non metric distance;approximate neighborhood graph;metric non symmetric;symmetric distance;distances resorting metricspace;quasisymmetrization performance brute;search graph based;neighborhood graphs;index neighborhood"}, "d6741241efb9ffd933df974b43d7109c72238371": {"ta_keywords": "track music generation;music generation transformer;generating multi track;music machine;multi track music;music generation;track music machine;track music;musical events track;tracks interleaved create;music machine mmm;concatenate tracks;tracks interleaved;track music contrast;track concatenate tracks;sequence musical events;multi track;concatenate tracks single;musical events corresponding;tracks single sequence;propose multi track;conditional multi track;different tracks interleaved;musical events;track instrumentation;represents musical material;corresponding different tracks;musical material;tracks;sequence musical", "pdf_keywords": "music generation transcription;polyphonic music generation;track music modeling;music modeling temporal;track musical material;music modeling;representing musical material;generating multi track;track music machine;music machine;approach representing musical;music generation;multi track music;representing musical;track music;track musical;multi track musical;application polyphonic music;music machine mmm;representation multi track;polyphonic music;multi track;track instrumentation;musical material;control track instrumentation;propose multi track;application polyphonic;generative based transformer;musical;music"}, "d41216f2f809e9fe26a684392f0ded4778f79e74": {"ta_keywords": "language tracking speech;tracking speech recognizer;end language tracking;tracking language utterance;monolithic multilingual asr;language tracking;dynamic tracking language;tracking language;speech recognition asr;speech recognizer mixed;tracking speech;speech recognizer;multilingual asr;recognizer mixed language;multilingual asr language;language speech code;switches language utterance;speech code switching;asr language independent;automatic speech;speech corpus;language speech corpus;recognize mixed language;automatic speech recognition;asr language;speech recognition;recognition asr;mixed language speech;speech code;language speech model", "pdf_keywords": ""}, "66cbda3e730285cb572c4792edcef209af32c564": {"ta_keywords": "retriever question answering;question answering;question answering traditional;knowledge reader retriever;distilling knowledge reader;learn retriever models;question answering paper;domain question answering;query documents challenge;information retrieval;task information retrieval;query documents;knowledge reader;knowledge distillation;retrieval;technique learn retriever;natural language processing;learn retriever;retrieval important component;retriever models;retriever models downstream;query support documents;component natural language;pairs query documents;distilling knowledge;inspired knowledge distillation;information retrieval important;retriever model corresponding;knowledge distillation does;data train retriever", "pdf_keywords": "retriever question answering;knowledge reader retriever;question answering gautier;learn retriever models;question answering;retrieval module downstream;information retrieval;retrieval module;distilling knowledge reader;information retrieval module;queries documents annotations;learn retriever systems;question answering benchmarks;queries documents approach;train information retrieval;domain question answering;task information retrieval;retrieval;retrieval important component;queries documents;query documents;based retrieved documents;knowledge reader;training reader retriever;knowledge distillation retriever;retrieved documents;component natural language;procedure learn retriever;retriever models;pairs queries documents"}, "7b96f6165ce5f686e46868c53b111b8e43b93de3": {"ta_keywords": "cite older papers;forgetting cite older;papers cite recent;recent papers cite;older papers analysis;forgetting older literature;cite older;years cited;citations papers;cite recent work;papers cite;papers analysis acl;years cited remained;analysis acl anthology;citations;15 years cited;stable forgetting cite;acl anthology;forgetting cite;citations papers published;tendency recent papers;age outgoing citations;acl anthology represents;older papers;cite recent;rate papers older;older literature field;outgoing citations papers;cited remained;cited remained relatively", "pdf_keywords": ""}, "863b2b38b33ffc4e5462adbc7aaf84aeb93adda8": {"ta_keywords": "language projecting annotation;simultaneously projects annotation;annotation multiple tasks;relying parallel corpora;annotation projection;projecting annotation task;projects annotation multiple;parallel corpora;annotation projection limited;multiple source languages;annotation task;projecting annotation;parallel corpora available;annotation multiple;single source language;work annotation projection;projects annotation;source language projecting;languages relying parallel;speech dependency projection;annotation;annotation task time;algorithm simultaneously projects;tasks multiple source;indoeuropean languages using;work annotation;source language;dependency projection multiple;source languages;programming ilp", "pdf_keywords": "computational linguistics;relying parallel corpora;association computational linguistics;computational linguistics edges;addressed tagging parsing;tagging parsing;annotations word alignments;languages relying parallel;parallel corpora;using parallel corpora;tagging parsing low;parallel corpora available;dependency annotations word;tags source languages;tags syntactic dependencies;languages joint;computational linguistics pages;resource languages joint;linguistics edges pos;languages using parallel;dependency annotations;labels dependency annotations;approach transferring annotations;transferring annotations word;parsing low resource;annotations word;annotation multiple tasks;syntactic dependencies;multiple source languages;languages joint cross"}, "d16d24dd5135f148556df1b2304b3747eee19e00": {"ta_keywords": "signature reply lines;reply lines email;extract signature reply;reply lines;reply lines plain;based mail classifiers;text email messages;email messages analysis;mail classifiers;plain text email;text email;mail classifiers email;classifiers email;email text speech;email text;lines email;email represented sequence;blocks reply lines;preprocessing email text;signature reply;learning extract signature;signature blocks reply;message email represented;representation email message;automatically identifying signature;lines email method;email messages;sequential representation email;anonymization email corpora;messages analysis", "pdf_keywords": ""}, "7f54429be66319dc19a42c0c9fceda3ac33fc92d": {"ta_keywords": "building cognitive tutors;tool cognitive tutors;cognitive tutors building;cognitive tutors;embedded cognitive tutor;cognitive tutor;tutors building intelligent;cognitive tutors highly;cognitive tutor applying;cognitive tutors primary;computer based tutoring;based tutoring;authoring tool cognitive;tutor applying programming;demonstration intelligent authoring;tutoring;tutor;intelligent authoring;tutors building;tutors;intelligent authoring tool;building intelligent authoring;programming demonstration intelligent;write cognitive model;simulated student;tutors highly;tutors highly successful;simulated student embedded;tool cognitive;write cognitive", "pdf_keywords": ""}, "c97500763de8a0871f1b83b1f968fcf4a8b31aee": {"ta_keywords": "stance recognition natural;development automatic stance;stance tasks;natural speech ataros;speaking styles tasks;automatic stance recognition;stance recognition;stance tasks consistent;strong stance tasks;acoustic signals stance;signals stance taking;corpus unscripted conversations;stance taking;stances high involvement;speech ataros technical;automatic stance;signals stance;stance taking existing;frequency stance taking;speaking styles;speech ataros;stances;natural speech;compare speaking styles;preliminary assessment corpus;stance;involvement strong stances;subjectivity creating audio;audio corpus;corpus examine fully", "pdf_keywords": ""}, "2ded680be56e03c8c17a04065deaac8ea6d4fa12": {"ta_keywords": "novel hla allele;new hla allele;hla allele;characterization new hla;hla allele 15;novel hla;identified typing donors;typing donors anhui;new hla;hla;donors anhui;donors anhui province;typing donors;allele;allele 15 179;allele 15;donors;02 identified typing;identified typing;province china characterization;anhui;02 identified;179 02 identified;china characterization new;anhui province china;anhui province;china characterization;identified;province china;province", "pdf_keywords": ""}, "753d10503a3cf340e41552109087ffd15ec96446": {"ta_keywords": "local entrepreneurial network;entrepreneurial network experimental;structure local entrepreneurial;entrepreneurial network;local entrepreneurial;network experimental economics;entrepreneurial;experimental economics study;experimental economics;economics study;economics;network experimental;local;network;structure local;experimental;detailed structure local;structure;detailed structure;study;detailed", "pdf_keywords": ""}, "998bd8862ab4193e672bb16fe1aae4d446f7536e": {"ta_keywords": "learning unpaired image;image synthesis;contrastive learning unpaired;effective image synthesis;learning unpaired;making contrastive learning;image image translation;image translation;image synthesis setting;setting contrastive learning;contrastive learning;contrastive learning effective;unpaired image;unpaired image image;learning effective image;synthesis setting contrastive;image translation addition;domain single image;image translation patch;contrastive learning method;learned feature space;based contrastive learning;maximizing mutual information;learned feature;making contrastive;single image;setting contrastive;doing maximizing mutual;referred negatives image;negatives image image", "pdf_keywords": "unpaired image translation;learning unpaired image;translation unpaired image;contrastive learning unpaired;output contrastive learning;image translation;preservation unpaired image;image image translation;image translation patches;learning unpaired;contrastive loss;making contrastive learning;image synthesis;contrastive loss important;unpaired image image;\ufb01nd image synthesis;internal contrastive learning;contrastive learning;e\ufb00ective image synthesis;unpaired image;image translation setting;image translation problems;image synthesis setting;learning e\ufb00ective image;contrastive learning e\ufb00ective;based contrastive learning;contrastive learning demonstrate;contrastive learning explore;output contrastive;choices contrastive loss"}, "d1206ccabd1980848f14472d6548251c2fab7963": {"ta_keywords": "predicting transferability nlp;transferability nlp tasks;transferability nlp;language models transferring;speech tagging transfers;results transfer learning;transfer learning;transfer learning beneficial;transferable source tasks;nlp tasks develop;target task speech;nlp tasks;task speech tagging;exploring predicting transferability;predict transferable source;predicting transferability;transferring downstream tasks;advances nlp demonstrate;develop task embeddings;task embeddings;source tasks;tagging transfers;advances nlp;predict transferable;source task small;tasks results transfer;task embeddings used;especially target task;used predict transferable;recent advances nlp", "pdf_keywords": "nlp tasks broad;nlp tasks;embeddings new tasks;different nlp tasks;nlp tasks study;models task embeddings;nlp tasks perform;33 nlp tasks;task embeddings;transferability different nlp;task embeddings new;computes task embeddings;transfer learning;results transfer learning;transferability 33 nlp;task embeddings 33;embeddings 33 nlp;exploring task transferability;source tasks;answering sequence labeling;transferability exploring task;bene\ufb01ts transfer learning;task transferability;tagging transfers;task transferability shed;source target tasks;pretrained models task;data source tasks;question answering sequence;representations tasks"}, "e9d26b9f5e6b619bbb759a67560cb949a9f034ba": {"ta_keywords": "ascent learning dynamics;local minmax equilibria;local minmax equilibrium;game theoretic equilibria;asymptotic saddle avoidance;zero sum games;ascent learning;minmax equilibria;discrete time gradient;sum games minimizing;gradient descent ascent;minmax equilibria class;learning dynamics;time gradient descent;minmax equilibrium;strict local minmax;based learning nonconvex;descent ascent learning;nonconvex optimization;minmax equilibrium contrast;games assess convergence;equilibria class games;learning nonconvex;learning dynamics timescale;unconstrained continuous action;learning nonconvex p\u0142;local minmax;natural game theoretic;maximizing player optimizes;maximizing player", "pdf_keywords": ""}, "6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91": {"ta_keywords": "multimodal machine learning;advances multimodal;characterized multimodal;multimodal;recent advances multimodal;specific multimodal;advances multimodal machine;challenges faced multimodal;modalities multimodal;multimodal machine;modalities multimodal machine;interpret multimodal;able interpret multimodal;multiple modalities multimodal;multimodal applications;specific multimodal applications;multimodal signals;faced multimodal machine;characterized multimodal includes;multimodal includes multiple;faced multimodal;multimodal includes;late fusion categorization;fusion learning new;fusion learning;fusion categorization;interpret multimodal signals;problem characterized multimodal;multimodal signals typical;multimodal applications paper", "pdf_keywords": "multimodal machine learning;challenges facing multimodal;data multimodal fusion;challenges surrounding multimodal;multimodal fusion broad;multimodal emotion recognition;multimodal fusion;multimodal machine;data multimodal;potential multimodal machine;models multimodal;multimodal;models multimodal machine;applications multimodal machine;vision models multimodal;research \ufb01eld multimodal;potential multimodal;applications multimodal;facing multimodal machine;multimodal emotion;main applications multimodal;\ufb01eld multimodal machine;facing multimodal;multimodal objects;\ufb01eld multimodal;world multimodal;heterogeneity data multimodal;visual speech recognition;experience world multimodal;surrounding multimodal"}, "480d545ac4a4ffff5b1bc291c2de613192e35d91": {"ta_keywords": "dynamic declaration network;toolkit implementing neural;code computes network;dynet dynamic neural;network structure dynet;defines computation graph;theano cntk tensorflow;dynet dynamic declaration;dynet toolkit implementing;network architectures dynet;computation graph defined;cntk tensorflow;computes network;neural network toolkit;declaration network structure;dynet toolkit;architectures dynet specifically;tensorflow;dynamic neural network;dynamic declaration facilitates;computation graph symbolic;dynamic declaration strategy;architectures dynet;computation graph construction;tensorflow user defines;symbolic computation graph;dynamic neural;implementing neural network;derivatives dynet toolkit;declaration network", "pdf_keywords": "declaration execution network;network architectures sequences;programming model dynet;neural network toolkit;architectures sequences;implementing neural networks;paradigm implementing neural;execution network architecture;creation dynamic neural;dynamic neural networks;programming model dynamic;dynamically structured network;architectures sequences variable;dynamic neural network;recursive neural networks;dynamic neural;execution network;declaration execution programming;implementing neural;declaration toolkits;network architecture refer;dynamic declaration;dynet dynamic neural;recursive neural;architecture refer static;alternative programming model;network architectures;structured network architectures;static declaration toolkits;structured recursive neural"}, "aeccb1d53e08adcfe271d1e4b08c0a2cdc3c42b4": {"ta_keywords": "open information extraction;sentence level extractions;extractions based corpus;information extraction global;information extraction;entity relation phrases;information extraction systems;knowledge bases supervision;extractions translating based;extraction global structure;external knowledge bases;extractions translating;tuple extractions based;supervision open information;based corpus;segmenting entity relation;level extractions translating;tuple extractions;level tuple extractions;sentence level tuple;sentences based local;extraction systems relation;knowledge bases;based corpus level;corpus;relation phrases individual;framework distant supervision;individual sentences based;unify segmenting entity;extraction global", "pdf_keywords": ""}, "824cd8db8a68732db04f4d8b7139eb4475e59ff2": {"ta_keywords": "generation nlg evaluation;nlg evaluation metrics;language generation nlg;nlg evaluation;benchmark natural language;generation nlg;progress nlg relies;natural language generation;benchmark help nlg;measuring progress nlg;nlg research;nlg relies constantly;nlg research multilingual;nlg relies;progress nlg;help nlg research;language generation;nlg;gem living benchmark;natural language;help nlg;multilingual evolve challenge;datasets human evaluation;progress introduce gem;introduce gem;addressing limitation gem;evaluation metrics moving;evaluation metrics;automated metrics;gem provides", "pdf_keywords": "nlg challenges;nlg challenges paper;gem generation evaluation;entire nlg community;nlg community;nlg community participate;range nlg challenges;human automatic evaluation;nlg;automatic evaluation;automatic evaluation aims;generation evaluation metrics;entire nlg;benchmark called gem;wide range nlg;evaluation model outputs;gem generation;generation evaluation;construction gem datasets;2020 gem;called gem generation;range nlg;evaluation metrics;2020 gem focuses;evaluation metrics aims;evaluation;gem;selection construction gem;evaluation aims uncover;gem datasets"}, "508e9bb13fcb1fa0c4dbac47288e8a3c2487bfc2": {"ta_keywords": "generalize proof tree;proof tree generalizing;learning generalize proof;generalizing number automata;generalization explanation based;perform generalization explanation;generalize proof;algorithm generalizing;systems perform generalization;generalizing number;frameworks generalizing number;perform generalization;tree generalizing;algorithm generalizing number;learning generalize;generalization algorithms;number automata based;generalization explanation;generalizing;generalization;generalization algorithms illustration;outputs algorithm generalizing;number automata;proof tree;explanation based learning;generalize;automata based;number desiderata generalization;generalizing number desiderata;automata", "pdf_keywords": ""}, "7a733a8d8f8649cc07e3ea9091f454ae117573af": {"ta_keywords": "evaluations attentionmesh provides;model attentionmesh;evaluations attentionmesh;attentionmesh utilizes deep;attentionmesh provides;model attentionmesh utilizes;end model attentionmesh;human evaluations attentionmesh;attentionmesh provides high;attentionmesh utilizes;deep learning attention;attentionmesh;biomedical text;terms biomedical text;accuracy speed attention;attention;learning attention;attention mechanism index;biomedical text facilitate;model associate textual;speed attention;labeled relevant words;associate textual;words given mesh;learning attention mechanism;information retrieval;word level final;textual;annotations;subject headings mesh", "pdf_keywords": ""}, "abb9b27440719ca44db5947a537fde07f0547973": {"ta_keywords": "repair bandwidth distributed;reducing repair bandwidth;bandwidth distributed storage;repair bandwidth nodes;distributed storage;reduces repair bandwidth;storage nodes;reduction repair bandwidth;storage nodes explicit;distributed storage way;marginal distributed storage;stored storage nodes;repair bandwidth;distributed storage setting;bandwidth nodes feasible;case repair bandwidth;storage way implement;mds code node;repair bandwidth set;repair bandwidth remaining;bandwidth remaining nodes;bandwidth set nodes;nodes explicit codes;bandwidth nodes;storage way;code reduces repair;nodes store data;bandwidth distributed;nodes store;distributed", "pdf_keywords": ""}, "808a9c9dece4c21be50f41e6caf50101f2b24b47": {"ta_keywords": "modeling human preferences;human preferences employing;preference models;preference models leverages;preferences employing;accurately preferences humans;preferences humans subjects;verification preference models;preferences humans;human preferences;accurately preferences;assumption accurately preferences;decision makers preferences;qualitative preference statements;mathematical model preferences;preference statements;human experiments controlled;choice model usually;choice model;formalisms human experiments;qualitative preference;choices observed;preferences employing number;human experiments;model preferences;functions qualitative preference;preference statements constraint;preferences verification preference;experiments controlled;preferences consistent", "pdf_keywords": ""}, "104f75283ae9027eb478e7984bd26b680277ce6f": {"ta_keywords": "navigation language pretraining;language pretraining stochastic;actions training sampled;vision language navigation;robust navigation language;training sampled actions;sequential action decoding;pretraining stochastic sampling;building robust instruction;robust instruction representations;action decoding;language navigation;navigation language;robust instruction;test agent learn;room benchmark;room room benchmark;training sampled;language pretraining;instructions environments robust;expert actions training;actions training;action decoding core;environments robust navigation;pretraining stochastic;agent learn correct;agent learn;instruction representations action;language navigation vln;navigation vln challenge", "pdf_keywords": "training sampled actions;actions training sampled;stochastic action sampling;training sampled;action sampling;test agent learn;forcing training agent;action sampling press;sampled actions test;stochastic sampling;propose stochastic sampling;expert actions training;agent learn correct;pretrained language models;forcing training;agent learn;stochastic sampling scheme;sampled actions;student forcing training;actions test agent;actions training;language models stochastic;sampling;sequential action decoding;sampling press;vln pretrained language;sampling press demonstrates;pretrained language;training agent;test agent"}, "f83ef3250ba1166d7c1c7585da7dd78e0641fae7": {"ta_keywords": "networks gans music;cooperative music generation;symbolic music generation;track music generation;gans music;music generation given;gans music art;music generation;ai cooperative music;music generation framework;music applied generate;networks symbolic music;musegan multi track;music generation accompaniment;generation accompaniment trained;generate coherent music;track sequential generative;multi track music;generate additional tracks;sequential generative adversarial;track music;generative adversarial networks;symbolic music;networks gans;generative adversarial;cooperative music;generation accompaniment;musegan multi;music;sequential generative", "pdf_keywords": "abstract generating music;generating music;symbolic music generation;track sequential generative;track music generation;networks symbolic music;music generation framework;music generation accompaniment;music generation;multi track polyphonic;polyphonic music;generating music notable;multi track music;generate coherent music;musegan multi track;track polyphonic music;sequential generative;generation framework gans;symbolic multi track;sequential generative adversarial;generate multi track;symbolic music;track polyphonic;music harmonic rhythmic;generation accompaniment;track sequence generation;polyphonic music harmonic;generation framework generative;novel generative model;track music"}, "199f383e9acd62649121ccde1e06631ce62c89e9": {"ta_keywords": "algorithm secret sharing;distributed way secret;threshold secret sharing;distributed algorithm sneak;secret sharing network;secret sharing general;sneak algorithm secret;secret sharing;secret sharing dealer;communication complexity sneak;complexity sneak algorithm;way secret sharing;sneak algorithm furthermore;sneak algorithm;problem secret sharing;algorithm sneak;algorithm sneak algorithm;generalization cryptographic protocols;protocols secure multiparty;secret sharing important;algorithm secret;secure multiparty computation;complexity sneak;cryptographic protocols large;satisfying kpropagating dealer;efficient generalization cryptographic;sharing dealer;satisfy propagating dealer;sharing general networks;efficient distributed algorithm", "pdf_keywords": ""}, "645bc7a5347a299a1e8aa965867bd097f6f4bddd": {"ta_keywords": "agent simulate answers;agent simulate navigation;navigating agent;introduce agent task;agent navigates asks;navigating agent simulate;models navigating agent;agent navigates;navigating agent models;agent answers inspired;agent answers;guiding agent answers;question answer generation;questions guiding agent;agent task;recursive mental model;guiding agent simulate;steps generate answers;answer generation demonstrate;answer generation;progress agents;propose recursive mental;agent models guiding;agent simulate;simulate navigation steps;simulate answers;recursive mental;agent task agent;generate answers;progress agents make", "pdf_keywords": "language guided robots;model dialogue navigation;recursive mental model;questioner recursive mental;language guided;dialogue navigation;introduce recursive mental;mental model dialogue;model dialogue;recursive mental;abstract language guided;propose recursive mental;rmm recursive mental;generated questioner recursive;dialogue navigation homero;path planner;guided robots;mental model agent;questioner recursive;mind propose recursive;guided robots able;steps navigation;model introduce recursive;propagate feedback navigation;rmm trained reinforcement;path planner target;steps navigation according;robots able ask;shortest path planner;trained reinforcement"}, "d00a403028eb0786915dab7a76692e5eeadf60be": {"ta_keywords": "transductive transfer learning;unsupervised transductive transfer;transfer learning labeled;transfer learning information;transfer learning;transfer learning protein;problem transfer learning;transductive transfer;methods transductive transfer;transfer learning previous;learning protein extraction;unsupervised transductive;inductive transductive approaches;learning protein;case unsupervised transductive;transductive approaches;transductive approaches adapt;transductive;learning labeled data;learning labeled;inductive transductive;study methods transductive;methods transductive;art inductive transductive;models problem transfer;learning information;learning information gained;domain available training;protein extraction;labeled data target", "pdf_keywords": ""}, "7abcc79e10ff651ef59dea84d347fa64c51e11b2": {"ta_keywords": "organic heterostructures sequential;axial branching ohss;branching axial ohss;engineering organic heterostructures;organic heterostructures;interactions sequential crystal;heterostructures sequential;branching ohss including;heterostructures sequential self;channel photon transportation;axial ohss modulating;branching ohss;heterostructures;sequential crystal nucleation;multi branching axial;single branching axial;axial branching;sequential crystal;synthesizing kinds ohss;branching axial;types axial branching;charge transfer intermolecular;axial ohss;multi channel photon;transfer intermolecular interactions;photon transportation;unilateral axial ohss;transfer intermolecular;channel photon;crystal nucleation", "pdf_keywords": ""}, "31b3e84f0a66e27c53c7fe403a0c6cd2319ed797": {"ta_keywords": "logic tensorlog probabilistic;probabilistic logic tensorlog;tensorlog probabilistic database;logic tensorlog;reasoning deep learning;logic called tensorlog;tensorlog probabilistic;tensorlog;neuralnetwork infrastructure tensorflow;tensorlog classes logical;logical reasoning deep;called tensorlog;neuralnetwork infrastructure;deep learning infrastructure;tensorlog classes;tensorflow;results tensorlog;called tensorlog classes;infrastructure tensorflow theano;functions neuralnetwork infrastructure;infrastructure tensorflow;deep learning frameworks;tensorflow theano;parameters probabilistic logic;reasoning deep;probabilistic logic;tensorlog scales;deep learning;neuralnetwork;experimental results tensorlog", "pdf_keywords": ""}, "805e49c7282b847faee048a63c1f43ceb08f5257": {"ta_keywords": "elections examine condorcet;voting rules instances;voting rules manipulation;manipulations elections;voting data empirical;empirical study voting;votes existing theories;voting systems takes;elections examine;study voting rules;voting systems;coalition size vote;study voting systems;manipulations elections borda;election data testing;position elections examine;voting rules;elections borda rule;vote deficit manipulations;paradox compare votes;elections;strictly ordered voting;consensus different voting;ordered voting;generate election;election data;different voting rules;generate election data;voting data;study voting", "pdf_keywords": ""}, "641af3bc3cc17993dc72098725d2eb9c0d98049d": {"ta_keywords": "tree estimates tda;density tree estimates;density clustering present;density clustering;estimates tda;estimates tda provides;tree estimates;confidence sets density;density tree;tda provides data;sets density tree;tda;techniques used tda;confidence sets;tda provides;dive density clustering;tda established theoretical;clustering present results;clustering;construct confidence sets;clustering present;statistical perspective overview;tda established;properties statistical perspective;statistical;used tda;statistical perspective;properties statistical;analyze data sample;used tda established", "pdf_keywords": ""}, "84f2cfbc142ad3165ea3bcacd189a3d1110660e0": {"ta_keywords": "tex math inline;inline formula wsj;inline formula tex;wer inline formula;inline formula best;formula wsj eval92;math inline;wsj eval92 test;text tex math;math inline formula;best wer inline;inline formula inline;inline formula;word error rate;formula inline;reduction inline formula;wer reduction inline;wsj eval92;corpora respectively outperforms;wer inline;formula tex;formula wsj;formula inline formula;error rate wer;tex math;relative word error;inline formula ami;array corpora;inline;tex math notation", "pdf_keywords": "rnn based encoders;encoder ctc attention;architecure rnnbased cnn;level fusion wav;fusion wav;cnn rnn based;cnn rnn;training encoder ctc;training encoder;rnnbased cnn rnn;joint training encoder;encoders subsampling convolutional;acoustics stream attention;fusion wav alignment;carried parallel encoders;encoder;rnnbased cnn;encoders subsampling;encoder ctc;based encoders subsampling;parallel encoders various;parallel encoders;wordlevel prediction fusion;encoders different architectures;layers characterized speech;stream attention achieved;prediction fusion;encoders;encoders various con\ufb01gurations;based encoders"}, "ceefd51b4b391668e313afe8edb3588197002e37": {"ta_keywords": "based speech synthesis;generated speech parameter;speech synthesis;speech synthesis generated;synthesis generated speech;ms speech parameter;speech parameter trajectory;speech synthesis necessary;speech parameter;spectrum ms speech;speech parameter sequence;modulation spectrum hmm;speech synthesis order;generated speech;spectrum hmm based;hmm based speech;spectrum hmm;smoothing global variance;degradation hmm based;postfilter modify modulation;ms speech;based speech;modify modulation spectrum;postfilter compensate modulation;ms postfilter modify;quality degradation hmm;postfilter modify;compensate modulation spectrum;degradation hmm;introduce modulation spectrum", "pdf_keywords": ""}, "2ec99c834bd67ac64ec04b426e5f9fd04f639024": {"ta_keywords": "datasets crowdsourced audio;dataset crowdsourced audio;crowdsourced audio transcriptions;crowdsourced audio;constructing datasets crowdsourced;datasets crowdsourced;dataset crowdsourced;datasets using crowdsourcing;data collection crowdsourcing;scale dataset crowdsourced;collection crowdsourcing higher;language crowdsourcing;russian language crowdsourcing;collection crowdsourcing;crowdsourcing higher;crowdsourcing standard tools;crowdsourced;crowdsourcing higher level;language crowdsourcing standard;crowdsourcing;voxdiy counterpart crowdspeech;crowdsourcing standard;large scale dataset;high quality datasets;using crowdsourcing;crowdsourcing develop;crowdsourcing develop principled;counterpart crowdspeech russian;audio transcriptions;constructing datasets", "pdf_keywords": ""}, "4375cccdfaf2ce3d013e4129d39f7801ef8a468e": {"ta_keywords": "translation task;workshop asian translation;paper translation subtasks;translation results submitted;translation results;translation task 400;commentary translation task;translation subtasks;translation wat2019 including;patent translation subtasks;task 400 translation;400 translation results;asian translation wat2019;scientific paper translation;translation subtasks hi;translation wat2019;paper translation;translation subtasks ja;asian translation;translation;patent translation;news commentary translation;en patent translation;commentary translation;6th workshop asian;workshop asian;400 translation;submitted automatic evaluation;tasks 6th workshop;evaluated wat2019", "pdf_keywords": ""}, "50a1dd504037463578f6ba8ee40afe4143f3d6fa": {"ta_keywords": "bound expectation norm;bounds expectation norm;uniform measure sphere;uniformly distributed sphere;expectation norm vector;expectation norm;bound expectation;norm vector uniformly;upper bound expectation;vector uniformly distributed;bounds expectation;upper bounds expectation;measure sphere;uniformly distributed euclidean;measure sphere consider;sphere phenomenon concentration;euclidean unit sphere;vector uniformly;concentration uniform measure;distributed sphere;unit sphere;distributed euclidean unit;norm;distributed sphere phenomenon;uniform measure;concentration uniform;sphere;expectation;distributed euclidean;sphere consider problem", "pdf_keywords": "optimal algorithm bandit;algorithm bandit;algorithm bandit zero;bandit zero order;order convex optimization;convex optimization;convex optimization point;bandit zero;optimization point feedback;bandit;optimal algorithm;optimization point;optimal;optimization;zero order convex;convex;order convex;journal machine learning;machine learning research;point feedback;machine learning;learning;algorithm;point feedback journal;learning research;feedback journal machine;feedback;zero order;order;point"}, "fac2368c2ec81ef82fd168d49a0def2f8d1ec7d8": {"ta_keywords": "challenging entity mentions;predicted coreference links;named entity recognition;entity recognition;information extraction tasks;entity mentions;relation extraction event;entity recognition relation;predicted coreference;representations predicted coreference;coreference links;sentence relationships framework;embeddings like bert;tasks named entity;recognition relation extraction;extraction tasks named;relation extraction;entities adjacent sentences;disambiguate challenging entity;entity mentions perform;context contextualized embeddings;contextualized embeddings;cross sentence relationships;extraction event extraction;event extraction;information extraction;event extraction framework;coreference;scoring text spans;contextualized embeddings like", "pdf_keywords": "event extraction contextualized;relation event extraction;relation extraction event;event extraction;extraction contextualized span;extraction event extraction;information extraction tasks;extraction contextualized;entity recognition relation;recognition relation extraction;entity recognition;relation extraction;sentence representations graph;named entity recognition;information extraction;event extraction dygie;bert embeddings graph;multi sentence representations;extraction tasks named;bert embeddings;sentence representations;leveraging bene\ufb01ts contextualization;context relevant tasks;contextual language models;contextualized span representations;language models task;tasks named entity;entity relation event;abilities bert embeddings;contextualized span"}, "25ee819bc444b02db43fcbeced982c975edee033": {"ta_keywords": "crowdsourced labeling worker;labeling worker task;crowdsourced labeling type;types crowdsourced labeling;crowdsourced labeling;consider crowdsourced labeling;labeling type worker;labeling worker;unmatched types crowdsourced;worker task specialization;types crowdsourced;type worker task;binary task labels;matched type tasks;worker task types;tasks matched type;type tasks;worker task;information worker task;crowdsourced;answer tasks matched;worker task associated;tasks matched;clustering worker skill;task specialization model;consider crowdsourced;task labels;task specialization;reliable answer tasks;binary task", "pdf_keywords": "binary task labels;inference algorithm recovers;recovers binary task;clustering worker skill;worker task types;binary task;worker clustering;targeted recovery accuracy;task types;using worker clustering;clustering worker;state art inference;worker clustering worker;worker task;recovery accuracy;task types achieves;estimation weighted majority;worker skill estimation;task labels given;skill estimation weighted;recovery accuracy best;recovery accuracy using;task labels;based semide\ufb01nite programming;information worker task;semide\ufb01nite programming;weighted majority voting;accuracy using worker;inference algorithms;designed inference algorithm"}, "6b13c4ac18f621155a550238a037a670bdce8969": {"ta_keywords": "\u8aac\u5f97\u5bfe\u8a71\u30b3\u30fc\u30d1\u30b9\u306e\u69cb\u7bc9\u3068\u5206\u6790 \u97f3\u58f0\u5bfe\u8a71 \u7b2c15\u56de\u97f3\u58f0\u8a00\u8a9e\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0;\u97f3\u58f0\u5bfe\u8a71 \u7b2c15\u56de\u97f3\u58f0\u8a00\u8a9e\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0;\u7b2c15\u56de\u97f3\u58f0\u8a00\u8a9e\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0;\u8aac\u5f97\u5bfe\u8a71\u30b3\u30fc\u30d1\u30b9\u306e\u69cb\u7bc9\u3068\u5206\u6790 \u97f3\u58f0\u5bfe\u8a71;\u8aac\u5f97\u5bfe\u8a71\u30b3\u30fc\u30d1\u30b9\u306e\u69cb\u7bc9\u3068\u5206\u6790;\u97f3\u58f0\u5bfe\u8a71", "pdf_keywords": ""}, "06d0af396fb08caa6a665dd476380aa16b6199b2": {"ta_keywords": "computer question answering;question answering;proceedings workshop human;workshop human computer;human computer;human computer question;workshop human;proceedings workshop;human;answering;workshop;computer;question;proceedings;computer question", "pdf_keywords": ""}, "3e33c988969b4c9f1d9af8c1c0f7644a30d0311f": {"ta_keywords": "speech translation medical;medical corpus translation;design speech translation;reliability speech translation;corpus translation experiments;medical corpus;speech translation;translation medical;translation medical domain;language barriers medical;corpus translation;translation experiments;collection medical corpus;corpus high reliability;translation experiments performed;high reliability speech;speech translation aims;performed corpus;reliability speech;design speech;corpus;experiments performed corpus;language barriers;barriers medical situations;overall design speech;barriers medical;speech;performed corpus high;caused language barriers;translation", "pdf_keywords": ""}, "6fb3d5a48be16fe1a4cff5e83093b77fbcd1013b": {"ta_keywords": "utility privacy tradeoff;grid operations privacy;utility privacy;quantifying utility privacy;control consumer privacy;control monitoring privacy;tradeoff smart grid;privacy tradeoff;privacy tradeoff direct;smart grid operations;smart grid;operations privacy consumers;consumer privacy;privacy consumers;monitoring privacy;privacy metric assumes;operations privacy;consumer privacy paper;privacy;privacy metric;privacy paper;monitoring privacy metric;privacy paper consider;load control consumer;infer private parameter;private parameter;private parameter independent;ability infer private;adversary model provides;adversary model", "pdf_keywords": ""}, "032e660447156a045ad6cf50272bca46246f4645": {"ta_keywords": "neural machine translation;adaptation personalized neural;machine translation;extreme adaptation personalized;adaptation personalized;softmax;perform machine translation;machine translation mt;softmax particular user;personalized neural machine;softmax particular;output softmax;bias output softmax;output softmax particular;perform translation captured;approximation extreme adaptation;personalized neural;translation captured;translation captured standard;translation mt variations;perform translation;translation;adapting bias;adapting bias output;extreme adaptation;effect perform translation;parameter efficient adaptation;translation mt;person speaks writes;neural machine", "pdf_keywords": "task domain adaptation;domain adaptation;domain adaptation showed;domain adaptation adjust;approaches domain adaptation;dataset speaker annotated;softmax bias;softmax;domain adaptation problem;parameters softmax bias;new dataset speaker;bias vector softmax;softmax bias parameters;softmax layer learn;form domain adaptation;dataset speaker;personal linguistic variations;speaker annotated;softmax layer;learn bias directly;parameters softmax;layer learn bias;learn bias;linguistic variations translation;baseline language pairs;vector softmax;personal linguistic;vector softmax layer;baseline language;modeling speakerspeci\ufb01c variations"}, "2aad7765250f7d9e312c9382f929ea5239b0fd73": {"ta_keywords": "scientist guide cell;use biology;ways use biology;use biology biological;cell biology;biology;guide cell biology;reprogramming cells ways;cells ways use;small reprogramming cells;biology biological;reprogramming cells;biological experiments bioinformatics;cells;cells ways;cells work complexity;biology biological experiments;cells work;cell;experiments bioinformatics;experiments bioinformatics computer;bioinformatics;biological experiments;bioinformatics computer scientist;biological;bioinformatics computer;guide cell;scientist guide;computer scientist guide;complexity living things", "pdf_keywords": ""}, "a1d578646cf42f2f69ee996742af484d03cc9121": {"ta_keywords": "multilingual nlp tasks;multilingual language models;mt5 massively multilingual;training massively multilingual;multilingual nlp;lingual tasks nmt5;tasking language modeling;machine translation pre;multi tasking language;massively multilingual language;translation pre training;massively multilingual;performance downstream multilingual;cross lingual tasks;nlp tasks gains;massively multilingual version;variety multilingual nlp;language modeling objectives;multilingual;multilingual cross lingual;multilingual language;downstream multilingual;nlp tasks;tasks nmt5 parallel;multilingual version t5;tasking language;machine translation;downstream multilingual cross;lingual tasks;language models", "pdf_keywords": "machine translation pretraining;translation pretraining;translation pretraining straightforward;translation language modeling;training massively multilingual;neural machine translation;machine translation objective;multilingual language models;tasking language modeling;language id models;performance downstream multilingual;translation objective pre;objectives machine translation;language pairs mt5;machine translation;multi tasking language;language modeling tlm;language modeling objectives;translation language;cross lingual tasks;massively multilingual;training nmt5 parallel;massively multilingual language;downstream multilingual;translation objective;multilingual;downstream multilingual cross;multilingual cross lingual;precision language id;multilingual cross"}, "0ba3e29dac0857100935b6eb22bce9cee4afcf17": {"ta_keywords": "similarity local names;domain normalization databases;names correspond entities;information retrieval;normalization databases;statistical information retrieval;global domain normalization;names correspond;information retrieval cases;instead names given;normalization databases contain;names measured using;domain normalization;explicitly similarity local;local names measured;global domains easily;names measured;explicitly similarity;similarity local;retrieval cases;correspond entities;names given natural;matching plausible global;retrieval;assume instead names;outperforming exact matching;detailed knowledge world;instead names;place names correspond;global domains", "pdf_keywords": ""}, "cec6de30eea5b4a5a414cf99830fbdb5c56a481c": {"ta_keywords": "parallel vod content;vod content distribution;vod content;storage bandwidth constraints;massively parallel vod;network bandwidth storage;highly distributed algorithm;bandwidth storage;distributed algorithm;architecture massively parallel;highly heterogeneous nodes;parallel vod;reliability storage bandwidth;vod massively;bandwidth constraints;vod massively scaled;distributed;bandwidth constraints vod;storage bandwidth;distributed algorithm provably;bandwidth storage capacity;heterogeneous nodes perform;highly distributed;packet level simulation;stream movie large;constraints vod massively;content distribution;content distribution architect;server use markov;device able stream", "pdf_keywords": ""}, "987658ba918710bbce5de8d92eb44bd127cf72c5": {"ta_keywords": "allophone graphs language;transcriptions phone phoneme;learned allophone graphs;phoneme mappings learnable;allophone graphs used;phoneme mappings;allophone graphs;phonemic transcriptions;phonemic transcriptions phone;allophone graphs building;allophone mappings;language specific phoneme;transcriptions phone;supervision phonemic transcriptions;annotations universal phone;phone phoneme mappings;evaluate allophone mappings;universal speech recognition;phone based lexicons;systems learned allophone;languages speech annotations;differentiable allophone graphs;speech annotations language;producing phonological;phonological units spoken;producing phonological units;language universal speech;learned allophone;speech annotations;shared languages speech", "pdf_keywords": "languages predicting phones;phoneme mappings language;predicting phonemes;transcriptions phone phoneme;model predicting phonemes;phoneme mappings learnable;probabilistic phone phoneme;language speci\ufb01c phonemes;allophone graphs multilingual;predicting phonemes seen;phone based speech;allophone graphs training;phonemes seen languages;phones unseen languages;phonemic transcriptions phone;transcriptions phone;phonemic transcriptions;phoneme mappings;interpretable probabilistic phone;supervision phonemic transcriptions;phone phoneme mappings;represent phone phoneme;phone phoneme rules;speci\ufb01c phonemes incorporate;speci\ufb01c phonemes;allophone graphs using;predicting phones;phonemes;phone realizations;languages predicting"}, "c0a32c68b992b44f1812492c95ac91fb62a6df37": {"ta_keywords": "gradient based bandits;guarantees competitive gradient;gradient reinforcement learning;reinforcement learning gradient;policy gradient reinforcement;games correspond gradient;gradient reinforcement;competitive gradient;competitive gradient based;policy gradient;agents employing gradient;learning schemes competitive;online convex optimization;local nash equilibria;nash equilibria specifically;including policy gradient;algorithms including policy;framework competitive gradient;gradient learning schemes;nash equilibria unstable;reinforcement learning;agent case gradient;learning surely;algorithms information gradient;learning gradient;nash equilibria strict;bandits certain online;sense nash equilibria;learning schemes;nash equilibria", "pdf_keywords": ""}, "01138945dc9de691cd559d09a46597cca7659efb": {"ta_keywords": "technology formulations fairness;concerns fairness bias;peer review research;fairness bias particularly;protocols peer review;fairness bias;peer review;fairness bias defined;formulations fairness bias;concerns fairness;peer review distributed;data science interacts;bias subjectivity;genetics data management;systems auditing;auditing;formulations fairness;deep peer review;measures data science;ethical norms;subjectivity miscalibration dishonest;biases;human evaluations explore;evaluations explore;bias;bias defined;data science;ethical;data management;bias particularly", "pdf_keywords": ""}, "ca7cd3a90d2953b2f8e45686afa3e79eb3a39add": {"ta_keywords": "predicting edit completions;neural model editcompletion;predict completion edit;predicting edit;edit completions based;modeling edits directly;modeling edits;edit completions;model editcompletion task;trained past edits;editcompletion task;model editcompletion;predict completion;completion edit;instead modeling edits;completions based learned;editcompletion task conduct;editcompletion;editcompletion task present;syntax tree;syntactic models learn;abstract syntax tree;task editcompletion task;learn generate edited;completion edit rest;accuracy syntactic models;task editcompletion;problem predicting edit;edits given code;snippet partially edited", "pdf_keywords": "predict edit completions;predicting edit completions;predict completion edit;predicting edit;editcompletion task predict;edit completions based;predict edit;modeling edits;modeling edits directly;trained past edits;edit completions;predict completion;completions based learned;syntactic models learn;task predict edit;completion edit;introduce editcompletion task;introduce editcompletion;directly introduce editcompletion;problem predicting edit;accuracy syntactic models;editcompletion task;learn generate edited;opposed modeling edits;editcompletion;completions based;past edits experiments;past edits;syntactic models;higher accuracy syntactic"}, "d7bebb71635cb818d2f5e0ca0a70434283deb4b6": {"ta_keywords": "performance imitation learning;imitation learning effective;imitation learning;stronger performance imitation;regularized search chess;performance imitation;chess regularizing search;imitationlearned policy results;search chess;imitationlearned policy;divergence imitationlearned policy;search chess regularizing;play learning search;predicting human actions;kl divergence imitationlearned;imitation;imitationlearned;higher human prediction;self play learning;divergence imitationlearned;expert humans self;learning search;human actions match;chess regularizing;strong humanlike policies;humanlike policies;humanlike policies multi;human prediction;human like gameplay;human behavior modeling", "pdf_keywords": "imitation learning chess;prediction accuracy imitation;imitation learning winning;accuracy imitation learning;imitation learned policy;performance imitation learning;imitation learning substantially;divergence imitation learned;imitation learning;surpassing imitation learning;accuracy imitation;policy human imitation;imitation learned;divergence imitationlearned policy;imitationlearned policy results;kl divergence imitation;regret minimization algorithm;stronger performance imitation;divergence imitation;novel regret minimization;accuracy surpassing imitation;human imitation learned;imitation learned anchor;imitationlearned policy;regret minimization;chess regularizing search;performance imitation;prediction accuracy chess;learning chess;carlo tree search"}, "4d7a50f6cfd8f27ebd4d5201fad6c5ef42c33733": {"ta_keywords": "predicting hospital mortality;hospital mortality predictions;mortality prediction mimic;accurate hospital mortality;predictions predicting hospital;benchmark intensive care;mortality prediction;mortality predictions predicting;standard mortality prediction;predicting hospital;hospital mortality;mortality prediction attaining;mortality predictions;bert model patient;patient clinical notes;information icu patients;combining clinical notes;hospital mortality combining;clinical notes paper;clinical notes time;intensive care;clinical notes consisting;clinical notes;intensive care units;mortality combining clinical;bert model;devices clinical notes;make accurate hospital;new standard mortality;summaries doctors recorded", "pdf_keywords": ""}, "1cf2e9e198feef3893da2800a7949f6880ddc084": {"ta_keywords": "explainable leaderboard nlp;leaderboard nlp;nlp research leaderboards;implementation nlp evaluation;leaderboard nlp common;nlp evaluation;nlp evaluation explainaboard;nlp tasks effective;systems various nlp;implementation nlp;rapid development nlp;development nlp;nlp tasks;various nlp tasks;development nlp research;nlp research;nlp common errors;various nlp;explainable leaderboard;conceptualization implementation nlp;nlp;nlp common;leaderboards emerged tool;leaderboards;evaluation tool;explainaboard explainable leaderboard;functionality standard leaderboard;leaderboard;research leaderboards;leaderboard allows researchers", "pdf_keywords": "leaderboard development nlp;nlp evaluation explainaboard;implementation nlp evaluation;nlp evaluation;implementation nlp;development nlp;methods natural language;natural language processing;conceptualization implementation nlp;nlp;natural language;leaderboard development;development nlp proceedings;nlp proceedings 2020;conference natural language;leaderboard;functionality standard leaderboard;language processing emnlp;nlp proceedings;leaderboard allows researchers;paradigm leaderboard development;language processing;new paradigm leaderboard;leaderboard allows;standard leaderboard allows;standard leaderboard;evaluation explainaboard;paradigm leaderboard;language processing 9th;2020 conference empirical"}, "5665d864d0f1bce6672d6d2bf9f8d8646093cb37": {"ta_keywords": "automated fact checking;fact check challenge;fact checking demonstration;semantic parsing claim;parsing claim identification;automated fact;present automated fact;fact checking;parsing claim;verification numerical claims;fact check;expressions knowledge bases;semantic parsing;simple numerical claims;numerical claims population;numerical claims;temporal expressions knowledge;check challenge;expressions knowledge;claim identification;parsing;furious fact check;work semantic parsing;claims population;knowledge bases;verification numerical;claims population germany;handle temporal expressions;check challenge focused;framework verification numerical", "pdf_keywords": ""}, "e050cd9cec5eed73bd56cb2c9726ea85e985384b": {"ta_keywords": "incremental sentence compression;sentence compression improving;sentence compression techniques;current sentence compression;sentence compression;performance sentence compression;performing sentence compression;rnn incremental sentence;sentence compression using;recurrent networks performance;sentence compression making;term memory lstm;short term memory;lstm recurrent networks;recurrent networks;recurrent neural networks;memory lstm recurrent;lstm recurrent;using lstm recurrent;compression using lstm;lstm recurrent neural;lstm;memory lstm;performing incremental sentence;using lstm;dependency tree representations;incremental sentence;recurrent neural;networks rnn;term memory", "pdf_keywords": ""}, "63567f348231abed171c02f99d4c49c2892a2ade": {"ta_keywords": "imbalances loose privacy;fairness differential privacy;differential privacy;differential privacy exacerbate;differential privacy specifically;biases data disparate;accuracy fairness;loose privacy guarantees;accuracy fairness decisions;affect accuracy fairness;imbalance utility fairness;loose privacy;privacy guarantees cause;biases data;privacy guarantees;shown differential privacy;existing biases data;privacy exacerbate existing;privacy;leaks models trained;deep learning different;fair impact data;utility fairness;data crowd sourced;existing biases;privacy exacerbate;privacy specifically;levels privacy;levels privacy recently;fairness differential", "pdf_keywords": "private deep learning;imbalanced stricter privacy;privacy model accuracy;differentially private deep;differential privacy;differential privacy model;impact differential privacy;stricter privacy guarantees;shown differential privacy;privacy guarantees;differential privacy exacerbate;stricter privacy;privacy preserving;privacy;privacy privacy;privacy guarantees used;privacy privacy preserving;different levels privacy;levels privacy;privacy model;privacy privacy protections;private deep;security privacy;differentially private;privacy protections;security privacy privacy;accuracy fairness;privacy paper;effects differentially private;accuracy fairness decisions"}, "d558c6b953e0267781ed5da90a35c122ba360f10": {"ta_keywords": "complex word identification;language dependent features;word identification multiple;identification multiple languages;task identifying words;identifying words phrases;cross lingual model;best monolingual models;inconsistencies annotation data;monolingual models;lingual model;cross lingual;best cross lingual;multi task learning;generalise cross lingual;lingual;complex word;identifying words;inconsistencies annotation;lingual model used;cross lingual setting;language dependent;monolingual best;word identification cwi;monolingual best monolingual;monolingual models relied;baselines complex word;identification cwi task;discuss inconsistencies annotation;lingual setting", "pdf_keywords": "complex word identi\ufb01cation;cross lingual models;word identi\ufb01cation cwi;lingual models;lingual cwi models;word identi\ufb01cation;identi\ufb01cation multiple languages;identifying words phrases;complex word;compare cross lingual;word identi\ufb01cation multiple;task identifying words;cross lingual cwi;lingual cwi;lingual models state;lingual;monolingual cross lingual;best cross lingual;monolingual systems;abstract complex word;cross lingual;identifying words;identi\ufb01cation cwi task;words phrases;monolingual systems exist;identi\ufb01cation cwi;monolingual spanish german;multiple languages;monolingual carefully selected;monolingual carefully"}, "d33d6c16d7c34dd387841efca74b457b7e60933a": {"ta_keywords": "recursive logic programs;learning recursive logic;logic programming learning;inductive logic programming;programming learning recursive;recursive logic;logic programs examples;logic programs;logic programs formally;learning recursive;class recursive logic;logic programming;examples pac learning;programming learning;inductive logic;simulation learn twoclause;pac learning;algorithm variant valiant;problem inductive logic;pac learning restricted;simulation learn;recursive;forced simulation learn;programs formally force2;pac learning algorithm;learn twoclause closed;learn twoclause;restricted class recursive;programs examples;programs formally", "pdf_keywords": ""}, "68ca176c7566067ae4b3311957cc4a134bfbc819": {"ta_keywords": "neural cognitive architecture;cognitive architecture;cognition learn continuously;cognitive architecture nca;cognition learn;propose neural cognitive;learn solve future;learning solve evolving;propose neural;learning designed implemented;neural cognitive;learn continuously;learn continuously solve;human cognition learn;deep learning ending;neural;learning ending learning;end propose neural;learning solve past;deep learning;learning designed;learning ending;ending learning designed;experience learning solve;learning;learning solve;ending learning;self supervision;artificial simulated world;cognition", "pdf_keywords": ""}, "163c6b06d948d0869eb8173b537c441c9a786977": {"ta_keywords": "mdp congestion game;mdp congestion games;congestion game;model tolls incentives;congestion games;congestion payoff strategy;tolls incentives reward;congestion games continuous;congestion games equivalent;process congestion games;tolls incentives enforced;imposing tolls incentives;tolls incentives;congestion game extension;classic congestion games;congestion games demonstrate;constraints mdp congestion;congestion payoff;tolling constraint satisfaction;population selfish agents;selfish agents solves;decision processes congestion;mdp congestion;tolling constraint;congestion;decision process congestion;processes congestion payoff;continuous population selfish;game equilibrium maximum;games continuous population", "pdf_keywords": "mdp congestion games;mdp congestion game;congestion game equilibrium;classical congestion game;process congestion games;congestion games equivalent;congestion games mdpcg;constraints mdp congestion;congestion game;congestion games continuous;congestion games;congestion payoff strategy;congestion game extension;classic congestion games;cooperative games markov;nonatomic routing games;games markov decision;tolling constraint satisfaction;mdp congestion;decision processes congestion;tolls incentives reward;congestion payoff;decision process congestion;process mdp congestion;tolling constraint;congestion games sarah;game equilibrium;processes congestion payoff;routing games;imposing tolls incentives"}, "2b63812db40152b12925ce4a848b929fa591b858": {"ta_keywords": "sequence models tutorial;neural machine translation;translation neural sequence;machine translation;translation neural;machine translation neural;sequence sequence models;sequence models;neural sequence sequence;handling human language;neural sequence;natural language;language processing;neural machine;human language;implementation exercise readers;sequential;language powerful tool;sequential data;test understood content;called neural machine;natural language processing;neural networks natural;model sequential data;basics math programming;mathematical understand concretely;model sequential;tutorial assumes reader;sequence;sequence sequence", "pdf_keywords": "machine translation described;language machine translation;machine translation technology;input machine translation;machine translation;machine translation source;machine translation section;translation technology;machine translation practitioner;techniques machine translation;translate human language;translation source language;translation device;translation technology used;goal machine translation;language machine;language input machine;universal translation device;translation web;target language machine;translation described task;words source converting;online translation web;described task converting;language output language;online translation;language input;translation web sites;native language machine;source language"}, "8b608ad2ec6d0300b6a0bb8f616d4a2b01150693": {"ta_keywords": "topic tracking model;automatic meeting analyzer;topic tracking;analyzer topic tracking;topic word extraction;track changes topics;meeting analyzer topic;word extraction meeting;changes topics based;recognition topic word;topic information automatic;applies topic tracking;topics based;speech recognition topic;estimated topic models;information automatic meeting;topic changes;changes topics;topic models;extraction meeting analysis;topic models online;meeting analyzer;automatic meeting;language model adaptation;tracking model language;model adaptation speech;adaptation speech recognition;model adaptively track;estimated topic;topic word", "pdf_keywords": ""}, "0a4b8b161931799d5c6bc3ecf07c53bae0e9e502": {"ta_keywords": "text data newspapers;corpus high quality;corpora language models;training corpora language;texts language models;language models 021;language preferred quality;school newspaper articles;language models increasingly;language models;privileging corpus;diverse text data;data newspapers;corpora language;training corpora;data newspapers 012;diverse text;newspaper articles;web text suitable;construct training corpora;like wikipedia books;newspaper articles written;school newspaper;text suitable language;corpus;like wikipedia;018 privileging corpus;web dumps diverse;texts language;books 004 news", "pdf_keywords": "dataset school newspapers;laissez faire data;popular internet content;school newspapers;high school newspapers;wikipedia newswire;wikipedia newswire books;implicitly favors content;education statistics;faire data;newswire books popular;internet content;internet content reference;education statistics begin;empirical evidence laissez;evidence laissez faire;quality text implicitly;quality \ufb01lter text;demographic data;school newspapers \ufb01nd;quality text;corpora language models;counties demographic data;training corpora language;web data sources;demographic data census;newspapers;\ufb01lter text data;newswire books;data sources"}, "7a737872a6693ba3f0c99651191b93dad0dadcee": {"ta_keywords": "speech diarization challenge;speech diarization;dihard speech diarization;diarization based subsystems;neural diarization based;neural diarization;diarization based;diarization challenge outputs;diarization challenge;achieved diarization error;diarization error rates;achieved diarization;end neural diarization;diarization;diarization error;combination achieved diarization;ensemble results subsystems;subsystems vector based;vector based subsystems;results subsystems vector;subsystems vector;subsystems hybrid;based subsystems hybrid;challenge outputs ensemble;based subsystems;subsystems;subsystems hybrid subsystem;based subsystems end;outputs ensemble;results subsystems", "pdf_keywords": "layers utterance normalization;diarization performance hybrid;diarization performance;improved diarization performance;utterance normalization;instead utterance normalization;utterance normalization ln;inference improved diarization;utterance normalization log10;normalization ln compression;improved diarization;res2net batch normalization;layers utterance;normalization log10 compression;23 layers utterance;diarization results used;based vector extractor;extractor similar res2net;tdnn based vector;tdnn based xvectors;obtained tdnn based;vector extractor;diarization results;ln compression fusion;layer instead utterance;vector clustering eend;diarization;multiple diarization results;re\ufb01ne diarization results;compression fusion"}, "4d44f2c3f269ea6cbc840b99c3f8119a13829509": {"ta_keywords": "phrase alignment extraction;machine translation preprint;joint phrase alignment;statistical machine translation;phrase alignment;alignment extraction statistical;machine translation;translation preprint;alignment extraction;joint phrase;extraction statistical;extraction statistical machine;preprint;translation;alignment;statistical machine;phrase;statistical;joint;extraction;machine", "pdf_keywords": ""}, "94c0a8be74d69787a1f3f6e91dcb480a2fd0dd56": {"ta_keywords": "reasoning enhancement pre;natural language reasoning;language reasoning;benchmarks demonstrate poet;reasoning enhancement;performance natural language;reasoning knowledge;language reasoning numerical;language models programs;reasoning furthermore poet;example poet improves;models inadequate reasoning;poet new pretraining;existing language models;reasoning studies;reasoning knowledge possessed;training language models;harvest reasoning knowledge;language models harvest;pre training language;poet improves;reasoning like program;poet improves f1;reasoning studies shown;language models;hop reasoning studies;training language;execution results poet;language models inadequate;language models pushing", "pdf_keywords": "boosting reasoning capability;reasoning pre training;paradigm boosting reasoning;language models reasoning;boosting reasoning;models reasoning skills;natural language reasoning;reasoning capability language;language reasoning;reasoning ability models;reasoning knowledge;reasoning capability;reasoning skills;language models programs;reasoning skills including;language reasoning numerical;reasoning ability;harvest reasoning knowledge;training language models;reasoning pre;reasoning knowledge possessed;existing language models;language models harvest;benchmarks demonstrate poet;imitating program executors;performance natural language;language models imitating;datasets demonstrate poet;pre training language;models inadequate reasoning"}, "de41f897ea6ca5447cfae81e9505f94ccf50e6a5": {"ta_keywords": "annual meeting jsmi;meeting jsmi;jsmi;77th annual meeting;holding 77th annual;annual meeting;77th annual;holding 77th;meeting;holding;77th;annual", "pdf_keywords": ""}, "0a4dd1e51616b422aa2d437610dbfbdd3733a114": {"ta_keywords": "based dialog modeling;dialog modeling;dialog systems;dialog agent;constructing dialog examples;dialog management;dialog examples available;approaches dialog management;dialog human human;dialog agent utilizes;dialog examples;constructing dialog;dialog human;example based dialog;dialog management including;dialog modeling ebdm;dialog systems studies;dialog pairs;conversation log databases;human conversation log;propose constructing dialog;work dialog agent;based dialog;deploying dialog systems;dialog;conversational mapping user;approaches dialog;human conversation examples;conversation log;human conversation example", "pdf_keywords": ""}, "d5ec188a5a39e504788c1fe33457eeb816a99f31": {"ta_keywords": "learning dependency grammars;learning phrasal dependency;grammar induction focuses;dependency grammars improving;constituency grammar induction;visual semantic role;concreteness visual semantic;dependency induction lens;dependency structure grammars;constituency dependency parsing;phrasal dependency structure;dependency grammars;dependency induction;learning dependency;dependency parsing experiments;unsupervised grammar induction;word concreteness visual;grammar induction model;visual semantic;dependency parsing;grammar induction;grammar induction propose;visually grounded syntax;multimodal information leading;grammars signal;grounded syntax models;structure grammars signal;grammars signal provided;phrasal dependency;grammars improving direct", "pdf_keywords": "models constituency parsing;constituency dependency parsing;parsing demonstrate concreteness;dependency parsing demonstrate;words dependency parsing;constituency parsing task;constituency parsing;dependency structure grammars;dependency parsing model;dependency parsing;constituency parsing smaller;concreteness visual semantic;word level concreteness;pcfg constituency parsing;word concreteness structural;parsing smaller grammar;unsupervised grammar induction;phraselevel vision based;word concreteness visual;phraselevel vision;constituency structure dependency;learn constituency structure;grammar induction model;words phraselevel vision;structure grammars finally;grammatical structure language;leverages word concreteness;introduction grammar induction;unsupervised grammar;structure grammars"}, "d878828c2345b665ab9651f20fb0e60e1ffe9de5": {"ta_keywords": "gaas heterostructrued sample;heterostructrued sample al;gaas heterostructrued;7as gaas heterostructrued;strains al0 3ga0;detecting strains al0;al nuclear spins;sample al nuclear;heterostructrued sample;strains caused heterostructure;3ga0 7as gaas;nmr technique analyzing;strains al0;resonance nmr technique;custom spectrometer succeeded;al0 3ga0 7as;spectrometer succeeded;nmr technique;magnetic resonance nmr;resonance nmr;spectrometer;sample observation strains;custom spectrometer;heterostructure;caused heterostructure interfaces;heterostructrued;caused heterostructure;al0 3ga0;nuclear spins based;sensitivity custom spectrometer", "pdf_keywords": ""}, "84e566e326b64b105cabf0c47dff336c4f632a1c": {"ta_keywords": "sunpy python;python package solar;package solar physics;sunpy python package;solar physics stuart;sunpy;solar physics;solar;package solar;panda6;python package;python;panda6 matt earnshaw6;panda6 matt;asish panda6;dacie30 sanjeev dubey6;asish panda6 matt;physics stuart;mueller6 sudarshan konge6;hayes5 asish panda6;dacie30 sanjeev;chanda17;physics stuart mumford;sudarshan konge6;sally dacie30 sanjeev;jain6 tannmay;sanjeev dubey6;mueller6 sudarshan;hill6 arthur eigenbrot35;mishra6 shashank", "pdf_keywords": ""}, "18268bdfc8a6e0a51f373bc4acf65c8b9a7bd6a0": {"ta_keywords": "neural machine translation;machine translation models;translation models;binarized prediction;translation models using;machine translation;using binarized prediction;binarized prediction error;models using binarized;prediction error correction;using binarized;error correction;binarized;neural machine;translation;prediction error;neural;models;correction;prediction;models using;machine;error;using", "pdf_keywords": ""}, "9e77f94e5a12cb33b8b464dc834fd81da1a609e2": {"ta_keywords": "delay dynamical;lyapunov krasovskii functional;stability problem delay;stability results delay;problem delay dynamical;delay dynamical dds;delay product type;lyapunov krasovskii;delay dynamical novel;improved delay product;condition improved delay;general lyapunov krasovskii;asymptotic stability;results delay dynamical;delay product;asymptotic stability problem;focuses asymptotic stability;krasovskii functional lkf;construct general lyapunov;lyapunov;general lyapunov;stability results;new stability results;delay;stability;improved delay;krasovskii functional;stability problem;functional lkf;problem delay", "pdf_keywords": ""}, "24219135d563b1cb24523bf522366c91a55d7604": {"ta_keywords": "label classifiers f1;multi label classifiers;thresholding multi label;classifiers f1 optimal;classifier f1 optimal;label classifiers;classifiers f1;classifier f1;binary classifier f1;performance multi label;multi label;f1 optimal thresholding;f1 score thresholds;multi label setting;optimal thresholding multi;classifiers;thresholding multi;binary classifier;optimal threshold best;f1 metric;classifier;f1 metric used;label;achievable f1 score;utilized f1 metric;thresholding predict instances;threshold best achievable;best achievable f1;optimal thresholding;optimal threshold", "pdf_keywords": ""}, "274b4ad4840b0a8a70c5bac3fe4b4861ce5fbb95": {"ta_keywords": "shot relation classification;relation classification dataset;supervised shot relation;relation classification;shot relation;methods relation classification;sentences 100 relations;relation classification conduct;relations derived wikipedia;shot learning methods;shot learning models;relations;relation sentence recognized;shot learning;humans relation sentence;100 relations;relation sentence;relation;present shot relation;supervised shot;competitive shot learning;task fewrel;distant supervision methods;large scale supervised;100 relations derived;art shot learning;relations derived;humans relation;compared humans relation;wikipedia annotated crowdworkers", "pdf_keywords": "supervised shot relation;shot relation classi\ufb01cation;fewshot learning empirical;relation classi\ufb01cation dataset;fewshot learning;benchmark fewshot learning;shot relation;shot learning models;learning models meta;competitive shot learning;large scale supervised;shot learning methods;sentences 100 relations;supervised shot;shot learning;models meta network;classi\ufb01cation dataset fewrel;baselines shot learning;new benchmark fewshot;benchmark fewshot;supervised;learning methods relation;dataset fewrel;meta learner;present shot relation;meta learn ing;learning models struggle;dataset fewrel consisting;meta learn;art shot learning"}, "78e838bcd2268260ddce6be6db4907df6f29f04f": {"ta_keywords": "expressive speech text;anime corpus;emotions expressed text;anime corpus automatic;comics anime corpus;emotion recognition speech;speech text;text balloons based;automatic speech;generation text balloons;speech text systems;expressive speech;balloons based linguistic;corpus automatic speech;expressive text;generate text balloons;linguistic acoustic features;acoustic features comics;text systems conveys;text balloons;reveal expressive text;features comics anime;expressive text preferable;construct expressive speech;features based comic;emotions expressed;text balloons propose;emotion recognition;verbal message emotional;text based", "pdf_keywords": ""}, "66f7d22d6373af5032074b25828331958b07e7f9": {"ta_keywords": "dataset differential privacy;private training dnns;differentially private training;differential privacy;differential privacy meets;privacy meets interpretability;better privacy utility;differentially private;privacy utility;diagnosis differentially private;privacy;providing better privacy;privacy utility trade;better privacy;data training deep;privacy meets;dnns surging importance;dnns especially medical;dp training dnns;training dnns especially;private training;training dnns;neural networks dnns;training dnns surging;aptos dataset differential;training deep neural;networks dnns;training deep;personal data training;dataset differential", "pdf_keywords": "privacy explanation quality;benchmark exploring interpretability;privacy explanation;maps dnns trained;privacy budget;dnns trained using;dp various privacy;levels privacy budget;privacy;activation maps dnns;dp training dnns;exploring interpretability speci\ufb01cally;training dnns especially;different levels privacy;maps dnns;dnns especially medical;training dnns;levels privacy;interpretability speci\ufb01cally class;privacy budget effect;various privacy;exploring interpretability;interpretability speci\ufb01cally;interpretability;dnns trained;dnns especially;interpretablity utility;2016 interpretability;privacy regimes;2016 interpretability method"}, "26a238217321008cd1daaa649683d461e16e7574": {"ta_keywords": "historical text normalization;text normalization;policy gradient training;text normalization prohibitive;learning policy gradient;text normalization albeit;models policy gradient;training neural sequence;normalization albeit outperformed;normalizations long;normalization;gradient training;accurate normalizations long;policy gradient;normalizations long unseen;accurate normalizations;phrase based models;leads accurate normalizations;normalizations;reinforcement learning policy;neural sequence sequence;sequence sequence models;policy gradient fine;datasets historical text;neural sequence;normalization prohibitive;sequence models;scratch reinforcement learning;normalization prohibitive scratch;approach historical text", "pdf_keywords": ""}, "eec490a41bdc716fccf98f4a7996c1d31334985a": {"ta_keywords": "genre samples dataset;cross genre samples;symbolic music generation;music generation including;genre samples;music generation;generation including dataset;components music generation;library symbolic music;muspy dataset management;muspy dataset;easier muspy dataset;symbolic music;cross dataset generalizability;cross genre;datasets;music generation conduct;commonly used datasets;representative cross genre;dataset;cross dataset;datasets currently;dataset analysis;datasets future research;samples dataset analysis;datasets future;components music;dataset management;datasets datasets;dataset generalizability experiment", "pdf_keywords": "music generation systems;music generation including;symbolic music generation;tools developing music;models recurrent neural;lstm;generation including dataset;components music generation;music generation;developing music generation;recurrent neural network;models recurrent;music generation provides;memory lstm;autoregressive models recurrent;shortterm memory lstm;training autoregressive model;training autoregressive;memory lstm network;rnn long shortterm;lstm network;developing music;easier muspy dataset;improve generalizability machine;cross dataset generalizability;muspy symbolic music;muspy dataset management;dataset;recurrent neural;long shortterm memory"}, "dc26c3775d233a5fa9516d21fee12aa5b46f8a25": {"ta_keywords": "recommendations extracting scientific;scientific recommendations extracting;information extraction scientific;extraction scientific literature;leveraging large unannotated;extracting scientific terms;leveraging natural language;processing scientific recommendation;recommendations extracting;scientific term extractor;extracting scientific;terms knowledge graph;large unannotated papers;information extraction;semi supervised approaches;approaches information extraction;knowledge graph;limited annotated resources;making scientific recommendations;extraction scientific;knowledge graph way;annotated resources;scientific papers organizing;annotated resources relatively;small annotated data;organizing terms knowledge;unannotated papers;finding relevant papers;terms unsupervised relational;supervised approaches information", "pdf_keywords": "scienti\ufb01c information extraction;recommendations extracting scienti\ufb01c;information extraction scienti\ufb01c;scienti\ufb01c recommendations extracting;scienti\ufb01c knowledge graph;extraction scienti\ufb01c literature;extracting scienti\ufb01c;information extraction;extracting scienti\ufb01c terms;semisupervised approaches scienti\ufb01c;2018 information extraction;extraction scienti\ufb01c;extraction develop unsupervised;recommendations extracting;leveraging large unannotated;information extraction develop;semisupervised approaches;terms knowledge graph;scienti\ufb01c term extractor;extracted terms unsupervised;semi supervised approaches;terms unsupervised relational;unsupervised approaches knowledge;knowledge graph leveraging;approaches scienti\ufb01c information;knowledge graph discover;making scienti\ufb01c recommendations;term extractor;knowledge graph;unsupervised relational"}, "ef2e2f3a847667000b591c8708b543eaf259113b": {"ta_keywords": "speaker adaptation;speech recognition lstms;based speaker adaptation;channel speech recognition;estimation based speaker;multi channel speech;speaker adaptation prepare;acoustic modeling language;threat speech recognition;speech recognition;acoustic model language;channel speech;rescoring based lstm;challenge employed lstm;beamforming strategy acoustic;recognition lstms;bidirectional lstm;including bidirectional lstm;lstms way;recognition lstms way;lstms;dnn hybrid acoustic;lstm mask estimation;processing acoustic modeling;dnn rnnlm based;acoustic modeling;hybrid acoustic model;lstm;lstm triple threat;bidirectional lstm blstm", "pdf_keywords": ""}, "b176a46ec214b9f75df751dcd2c894f0a7a72a9a": {"ta_keywords": "semi supervised argumentation;supervised argumentation mining;supervised argumentation;argumentation mining;argumentation mining evolving;argumentation mining user;argument component identification;attention argumentation mining;exploiting debate portals;debate portals;debate portals based;debate portals semi;attention argumentation;discourse asked leveraging;exploiting debate;generated web discourse;gained attention argumentation;argumentation;argument component;arguments user generated;representation analyzing arguments;diverse annotated corpora;web discourse recently;analyzing arguments user;large diverse annotated;data debate portals;annotated corpora;web discourse;analyzing arguments;discourse recently gained", "pdf_keywords": ""}, "83145b7a391b792e24d8d38f74ed6b6ae7a149dc": {"ta_keywords": "neural machine translation;translation systems;machine translation systems;context aware word;improves translation quality;translation systems use;machine translation;level machine translation;improves translation;increases context usage;context usage improves;utilize translation time;translation quality;machine translation demonstrated;usage improves translation;actually utilize translation;aware word dropout;utilize translation;context models;translation quality according;usage context models;context aware models;increases context;context context sentences;inter sentential context;sentential context context;context sentences;context context aware;context sentences currently;context aware", "pdf_keywords": "context aware word;source context conditioning;context context aware;aware word dropout;context models;translation systems use;longer context diminishing;context aware;context aware models;context diminishing;context conditioning longer;translation systems;usage context models;source target context;context sizes;longer context;machine translation systems;context conditioning;machine translation;target different context;referenced source context;context sizes source;source context;conditioning longer context;context models using;context target;context referenced source;target context referenced;target context;context broader settings"}, "45eea76ac46b402f3a209de57e469275419fdc9e": {"ta_keywords": "native language reading;unknown word detection;language reading;gaze based unknown;word detection;eye gaze based;word detection non;eye gaze;language reading considering;gaze based;non native language;native language;reading considering speech;gaze;detection non native;speech tag personalization;eye;speech tag;non native;based unknown word;reading;reading considering;considering speech tag;native;language;tag personalization;unknown word;word;personalization;speech", "pdf_keywords": ""}, "94a11c9425bf5f4f9b8ed1b07ea1d15a81b96e9f": {"ta_keywords": "crowdsourcing software development;crowdsourcing software;development crowdsourced applications;crowdsourced applications crowdsourcing;crowdsourced applications;development crowdsourced;crowdsourcing applications;crowdsourcing applications amazon;applications crowdsourcing;applications crowdsourcing used;crowdsourcing;crowdsourced;crowdsourcing used increasingly;crowdsourcing used;worldwide term crowdsourcing;online crowdsourcing;term crowdsourcing;open platforms crowdsourcing;platforms crowdsourcing applications;design development crowdsourced;platforms crowdsourcing;frameworks online crowdsourcing;crowdsourcing coined;crowdsourcing topics investigated;crowdsourcing topics;online crowdsourcing topics;term crowdsourcing coined;crowdsourcing coined journalist;amazon mechanical turk;collaborative software", "pdf_keywords": ""}, "222ae836430ad0c922b47a9345c17212f9584097": {"ta_keywords": "orthognathic surgery laser;surgery treat gummy;lip repositioning surgery;laser assisted lip;orthognathic surgery;conventional orthognathic surgery;treat gummy smile;assisted lip repositioning;upper lip treated;alternative conventional orthognathic;smile excessive gingival;gummy smile excessive;lip repositioning;smile major esthetic;conventional orthognathic;gingival recontouring modified;coupled gingival recontouring;lip treated;surgery laser assisted;gingival recontouring;treat gummy;gummy smile;assisted lip;known gummy smile;repositioning surgery treat;orthognathic;surgery laser;gummy smile major;excessive gingival display;lip treated modified", "pdf_keywords": ""}, "723770d9ac418e923db5e087ae18c04702f5986e": {"ta_keywords": "boosting like rulesets;rule learners ensemble;rule learner generates;built rule learners;rulesets repeatedly boosting;rule learner;new rule learner;learner generates rulesets;rule learners;learners ensemble rules;generates rulesets;rule builder;greedy rule builder;rule builder possible;rule builder use;ensemble rules created;rulesets built rule;generates rulesets repeatedly;ensemble rules;rulesets built;boosting simple;rulesets;repeatedly boosting simple;confidence rated boosting;built rule;like rulesets built;boosting simple greedy;boosting;rules created slipper;rulesets repeatedly", "pdf_keywords": ""}, "b9ede62d1d586e1a3b1ef7ec046f09e4e35639bf": {"ta_keywords": "term based retrieval;term based search;term associations retrieval;effective retrieval pipelines;retrieval algorithm similarity;retrieval nn search;search using similarity;nn retrieval algorithm;designing effective retrieval;retrieval algorithm;based retrieval nn;effective retrieval;retrieval pipelines commonly;retrieval pipelines;generic nn retrieval;associations retrieval pipelines;based retrieval;nn search candidates;nn search using;based search generic;nn retrieval;associations retrieval;based search obtain;based search;nn search;search candidates;force nn search;search obtain candidate;search generic nn;retrieval", "pdf_keywords": "search powered term;search algorithm similarity;nearest neighbor search;text search;generic nn search;approximate nn search;nn search using;similarity function challenging;search nn search;text search powered;scores text search;nn search;similarity function;nn search algorithm;similarity;non symmetric similarity;similarity function weighted;term aliases;unlimited term aliases;terms phrases locations;search algorithm;algorithm similarity function;document terms phrases;nearest neighbor;nn search end;neighbor search;phrases locations text;algorithm similarity;search using;neighbor search nn"}, "6dafc41e9bbd3aa476a0a1c15ca2c459eaef6b98": {"ta_keywords": "morphological inflection models;morphological inflection lemma;neural seq2seq models;morphological inflection;evaluate morphological inflection;inflection unseen lemmas;solving morphological inflection;propose evaluate morphological;inflection models employing;inflection models;evaluate morphological;neural seq2seq;languages lose 10;low resourced languages;morphological;generic neural seq2seq;ranked systems sigmorphon;splits challenge generalization;solving morphological;clearly generalizing inflection;generalizing inflection unseen;languages lose;accuracy scores languages;languages drop high;languages task;languages drop;seq2seq models little;generalizing inflection;highresourced languages lose;average solving morphological", "pdf_keywords": "models generalization abilities;examines models generalization;morphological in\ufb02ection models;split examines models;in\ufb02ates models performance;morphological;models performance;sigmorphon shared tasks;models generalization;morphology;models employing harder;splits challenge generalization;performance in\ufb02ection models;sophisticated modeling instance;performance new splitting;models performance omer;propose evaluate morphological;morphological in\ufb02ection;morphology in\ufb02ection fundamental;tasks new split;generalization abilities;examines models;morphology in\ufb02ection;generalization capacity models;systems fared similarly;similarly split sigmorphon;generalization abilities conditions;models;evaluate morphological;generalization capacity"}, "21066ab388b386f3d3552a4a4c25322e0ca69632": {"ta_keywords": "extraction hypernyms based;hypernymy extraction based;supervised relation extraction;extraction hypernyms;improves hypernymy extraction;approach extraction hypernyms;hypernymy extraction;relation extraction;learning word embeddings;examples hypernym prediction;relation extraction impact;word embeddings;hypernym prediction;hypernym prediction studied;examples supervised relation;hypernyms based projection;hypernyms based;word embeddings natural;positive examples hypernym;examples hypernym;projection learning word;supervised relation;sampling improves hypernymy;hypernyms;examples supervised;learning word;training examples supervised;hypernym;embeddings;improves hypernymy", "pdf_keywords": "extraction hypernymy relations;hypernymy extraction based;approach hypernymy extraction;hypernymy extraction;extraction hypernymy;context hypernymy prediction;hypernymy relations based;model extraction hypernymy;distributional word vectors;hypernymy relations;hypernymy prediction task;hypernymy prediction;experiments context hypernymy;word vectors experiments;context hypernymy;novel approach hypernymy;word vectors;approach hypernymy;projection distributional word;prediction task english;hypernymy;synonyms hyponyms;relations types synonyms;explicit negative training;synonyms hyponyms enforces;synonyms;negative training instances;distributional word;task english russian;types synonyms hyponyms"}, "900ce63d71dce47059434cdf2d5e1d77bc716e8d": {"ta_keywords": "sdns clouds;sdns clouds big;sdns;clouds big data;big data mutual;data mutual opportunities;big data;clouds big;clouds;data mutual;data;mutual opportunities;opportunities;mutual;big", "pdf_keywords": ""}, "8ae392fc9acbada67a4288a6affc2a77f83befcd": {"ta_keywords": "variational autoencoder vq;speech machine translation;variational autoencoder;quantized variational autoencoder;neural machine translation;autoencoder vq vae;autoencoder vq;nmt automatic speech;text speech machine;machine translation nmt;autoencoder;speech machine;automatic speech;discretalk text speech;machine translation;speech recognition asr;autoregressive vector quantized;text speech;text speech e2e;speech e2e tts;vae model learns;speech recognition;vae model learn;end text speech;automatic speech recognition;mapping function speech;vae model autoregressive;speech waveform;function speech waveform;translation nmt vq", "pdf_keywords": "variational autoencoder vqvae;speech machine translation;autoencoder vqvae model;autoencoder vqvae;variational autoencoder;vq vae neural;neural machine translation;speech machine;text speech machine;vae neural machine;quantized variational autoencoder;autoencoder;machine translation nmt;discretalk text speech;shallow fusion language;introduce advanced decoding;text speech;advanced decoding;end text speech;fusion language model;speech e2etts model;decoding techniques;advanced decoding techniques;vae neural;model autoregressive transformer;machine translation;decoding;tts parallel wavegan;text speech e2etts;autoregressive transformer"}, "317d95f99ef62237f6c7d7834d1d19027166b392": {"ta_keywords": "evaluation imitation learning;imitation learning structured;e2e nlg challenge;human evaluation imitation;imitation learning;end language generation;evaluation imitation;language generation second;imitation learning model;language generation;nlg challenge;structured prediction explore;learning structured prediction;generate sequences words;employs imitation learning;learning structured;structured prediction;e2e nlg;imitation;sheffield structured prediction;trueskill scores encoder;prediction explore;decoder architectures generate;structured prediction approaches;encoder decoder model;prediction explore large;scores encoder decoder;nlg;encoder;best naturalness clusters", "pdf_keywords": ""}, "db190db2567c334b772fd653dca10f300074e421": {"ta_keywords": "speaker adaptation end;speech synthesis pretrained;speech data training;end speech synthesis;supervised speaker adaptation;text speech data;speaker adaptation;semi supervised speaker;speech synthesis;speech tts models;speech data;text speech tts;paired text speech;end text speech;speech recognition asr;automatic speech;text speech;transcriptions pretrained asr;speaker corresponding text;minutes speech recordings;end automatic speech;corresponding transcriptions pretrained;speech recordings;pretrained asr models;speech tts;speech recordings target;supervised speaker;automatic speech recognition;pretrained asr;synthesis pretrained models", "pdf_keywords": ""}, "02cbb0db288af2c83b48a023f245812bd22a2408": {"ta_keywords": "text generation metrics;reference generated texts;text models wikibio;evaluate information extraction;generated texts semi;generated texts;wikibio parent correlates;information extraction;text models;existing text generation;texts semi structured;table text models;texts diverge information;text semi structured;text generation;information extraction based;datasets generating text;grams reference generated;wikibio parent;models wikibio parent;recall metrics rely;generating text;wikibio contain reference;generating text semi;reference generated;judgments references diverge;human judgments references;reference texts diverge;solely reference texts;contain reference texts", "pdf_keywords": "generating natural language;reference generated texts;language descriptions structured;sequence sequence models;natural language descriptions;sequence models provide;metrics references elicited;reference texts elicited;references elicited humans;descriptions structured data;human judgments references;introduction task generating;sequence models;natural language;descriptions structured;generated texts;judgments existing metrics;tables generating text;judgments references diverge;reference generated;generated texts semi;judgments references;texts semi structured;references elicited;data webnlg challenge;elicited humans webnlg;language descriptions;solely reference texts;encoding tables generating;webnlg challenge introduction"}, "20937a0f03bcb845afbedda901a6d4e93a2b5c34": {"ta_keywords": "terrorism analysis export;terrorism analysis;terrorism;analysis export networks;export networks;analysis export;export;networks;analysis", "pdf_keywords": ""}, "b8e2e764ac82f81a5bc645c818d0d5ad7806e806": {"ta_keywords": "chime speech separation;speech separation;speech separation recognition;chime speech;chime;separation recognition challenge;separation recognition;speech;separation;recognition challenge analysis;recognition;recognition challenge;challenge analysis;challenge analysis outcomes;challenge;analysis outcomes;analysis;outcomes", "pdf_keywords": ""}, "7c976b0b54ace7d13b87e8feefe6f29c0599d78d": {"ta_keywords": "semantic word network;inducing semantic word;inducing semantic;semantic relations;lexical relations wisdom;relations semantic word;relation extraction;method inducing semantic;lexical relations;outperformed relation extraction;relations individual words;relations semantic;relation extraction methods;semantic relations individual;word network;datasets russian language;represents semantic relations;resources distributional semantics;constructing semantic word;semantics methods relations;semantic word;distributional semantics methods;contexts constructing semantic;constructing semantic;unsupervised method inducing;semantic;lrwc lexical relations;dictionary resources distributional;semantics methods;dictionary resources", "pdf_keywords": ""}, "a83bbc7bf70b1beedbfe0140d24d556e2dc5acc8": {"ta_keywords": "adversarial robustness perceptually;improve adversarial robustness;kernels improve adversarial;adversarial robustness;robustness perceptually aligned;improve adversarial;adversarial;perceptually aligned gradients;robustness perceptually;smooth kernels improve;smooth kernels;robustness;kernels improve;perceptually aligned;aligned gradients;gradients;kernels;smooth;perceptually;improve;aligned", "pdf_keywords": ""}, "2aaf2ee779cd4ff0f26bb73958ea9fb0faa61907": {"ta_keywords": "subcellular location image;image text features;text features training;text images journal;features extracted images;histogram features;location image finder;image finder;subcellular location patterns;text features extracted;protein subcellular location;subcellular location;images online journal;histogram features used;text images;features training;words text features;text features;features extracted text;microscope images;microscope images online;features training algorithm;patterns text images;binary classification;images journal;images journal articles;fluorescence microscope images;features extracted;image text;intensity histogram features", "pdf_keywords": ""}, "08f199ebfd27a5f9ada79edd07ac41e46c7278d5": {"ta_keywords": "factor analysis autism;communication socialization scores;analysis autism spectrum;analysis autism;socialization implemented movie;communication socialization implemented;human communication socialization;communication skills factor;autism spectrum;contributing communication socialization;communication socialization;socialization scores non;autism;communication socialization real;non verbal communication;socialization scores;autism spectrum quotient;relationship communication socialization;enhance human communication;verbal communication skills;communication skills;verbal communication;non verbal information;information contributing communication;contributing communication;ipad application nocoa;socialization implemented;human communication;movie data development;movie data", "pdf_keywords": ""}, "34d5d2f75934caff89311ef20d18a275da5abb47": {"ta_keywords": "explanation based learning;learning multiple examples;examples explanation based;generalizing number learning;number learning multiple;multiple examples explanation;multiple examples;examples explanation;generalizing number;explanation based;learning multiple;number learning;generalizing;examples;learning;based learning;explanation;number;multiple;based", "pdf_keywords": ""}, "fbf2a6a887ea92311cf207d522c535daf867a6ba": {"ta_keywords": "text speech synthesis;trained text embeddings;speech tts synthesis;speech synthesis;end text speech;trained embeddings text;pre trained text;text speech;text speech tts;pre trained embeddings;speech tts;enhanced text speech;speech synthesis hypothesize;synthesis hypothesize text;text embeddings;text embeddings enhanced;trained text;hypothesize text embeddings;embeddings enhanced text;pre training language;embeddings text;supervised representations text;text embeddings contain;tts synthesis model;trained embeddings;tts synthesis;natural prosody;training language understanding;self supervised representations;produce natural prosody", "pdf_keywords": ""}, "da06caf4f340ebc81395f092f9dc3a3101827506": {"ta_keywords": "el speech enhancement;speech enhancement;electrolaryngeal el speech;speech enhancement method;el speech based;laryngectomee el speech;speech enhancement methods;prediction el speech;el speech preserving;el speech using;quality el speech;el speech demonstrated;speech proficient laryngectomees;intelligible el speech;method laryngectomees;proposed el speech;speech using device;laryngectomee el;conversation electrolarynx device;sounds generated electrolarynx;speech demonstrated method;single laryngectomee el;el speech proficient;el speech highly;proficiency laryngectomee;proficiency laryngectomee necessary;el speech;face conversation electrolarynx;method laryngectomees evaluate;laryngectomees produce intelligible", "pdf_keywords": ""}, "acc2ad56a9c68c799747e08d978f9803997c1527": {"ta_keywords": "predicting precursors perovskite;literature trained neural;generate syntheses inorganic;inorganic synthesis insights;syntheses inorganic materials;trained generate syntheses;literature inorganic synthesis;reported syntheses leveraging;scientific literature inorganic;syntheses leveraging new;inorganic materials synthesis;materials synthesis;predicting precursors;syntheses leveraging;literature inorganic;materials synthesis planning;syntheses inorganic;synthesis insights;generate syntheses;precursors perovskite;precursors perovskite materials;technique predicting precursors;inorganic synthesis;synthesis insights potential;reported syntheses;neural networks starting;embeddings language models;materials design discovery;perovskite materials;trained neural", "pdf_keywords": "learns representations materials;predicting precursors perovskite;generate synthesis predictions;embedding model materials;synthesis predictions;synthesis predictions variety;variational autoencoder;model learns representations;materials synthesis;embeddings language models;conditional variational autoencoder;model materials science;variational autoencoder cvae;learns representations;representations materials;model perform synthesizability;precursors perovskite;precursors perovskite materials;representations materials corresponding;predictions variety materials;adapted materials science;materials synthesis planning;autoencoder;proposed novel perovskite;model learns;perovskite materials;synthesizability;materials corresponding synthesis;autoencoder cvae;word embedding model"}, "2aea6cc6c42101b2615753c2933a33e57dd665f2": {"ta_keywords": "walks knowledge base;knowledge base graph;knowledge base;beliefs knowledge base;knowledge base containing;clause learning inference;learning inference;walks knowledge;learning inference large;scale knowledge base;learning inference method;performing learning inference;soft inference;random walks knowledge;large scale knowledge;target relations tuning;inference large scale;clause learning;soft inference procedure;coverage soft inference;horn clause learning;inference tasks;knowledge incomplete coverage;target relations;inference large;path ranking;path ranking algorithm;specifically learn infer;scale knowledge;inference", "pdf_keywords": ""}, "1134ec4cdfc1c2161d157b0f4e03dec85d8c4c8a": {"ta_keywords": "convnets trained loss;canals convnets trained;canals convnets;convolutional networks reconstruct;irrigation canals convnets;convnets trained;convnets;training deep;training deep convolutional;predicted roads;roads prevents predicting;connectivity roads canals;function training deep;predicted roads prevents;convolutional networks;deep convolutional;networks reconstruct;networks reconstruct network;gaps predicted roads;deep convolutional networks;canals aerial images;connectivity oriented loss;roads canals;predicted road;road touch prediction;loss function training;recover road connectivity;roads canals penalizing;connectivity roads;trained loss", "pdf_keywords": "convolutional networks reconstruct;canals convnets trained;binary segmentation convnets;canals convnets;irrigation canals convnets;convolutional neural net;convnets purpose road;convnets trained;segmentation convnets;connectivity reconstructed networks;convnets trained loss;drainage canal networks;segmentation convnets purpose;reconstructed networks;convolutional networks;canal networks;convnets;training deep convolutional;training deep;networks reconstruct network;networks reconstruct;deep convolutional networks;connectivity reconstructed;function training deep;deep convolutional;neural net;convnets purpose;neural net fully;canals simple net;road network delineation"}, "b9b83860bc0d79b3b629b3035c4b7b7f9f71b5af": {"ta_keywords": "propagation forest trail;deployment wireless sensor;propagation forest;impromptu deployment wireless;locations deploys relays;deployment wireless relay;deploys relays locations;relay networks experiences;networks experiences forest;sensor networks extract;node walks forest;sensor networks;wireless sensor networks;deployment wireless;wireless relay networks;deploys relays;deployment algorithms;experiences deployment algorithms;link quality measurements;deployment algorithm;locations connect sensor;walks forest trail;node impromptu deployment;relay networks;model propagation forest;relays locations;node walks;wireless sensor;propagation;relays locations connect", "pdf_keywords": "propagation forest trail;deployment wireless relay;propagation forest;impromptu deployment wireless;relay networks experiences;deployment wireless sensor;wireless relay networks;deployment wireless;asyou deployment wireless;wireless relay;experimentation deployment algorithms;deployment algorithms;relay networks;deployment algorithms taken;networks experiences forest;wireless sensor networks;sensor networks;deployment algorithm;experiences forest trail;forest trail;sensor networks paper;choice deployment algorithm;deployment algorithm parameters;forest trail arxiv;algorithms heuexplorelim optexplorelimlearning;model propagation forest;relay;algorithms experimentally forest;propagation;impromptu deployment"}, "386bfd0e411dee4f512a8737c55dd84846981182": {"ta_keywords": "tables text answering;tabular representation learning;learns exclusively tabular;tables associated text;provides embeddings table;table based prediction;tables underperforms tasks;embeddings table substructures;embeddings table;answering questions tables;jointly models tables;tables associated;pretrained language models;tables text;language models bert;tables;representation learning jointly;learning jointly models;tables underperforms;paired tables text;tabbie provides embeddings;learning jointly;table substructures cells;questions tables underperforms;prediction tasks existing;table;text answering;models bert;models tables associated;operate tables", "pdf_keywords": "pretrained representations tabular;tabular representationlearning;tabular representationlearning jointly;learns exclusively tabular;tabbie pretrained representations;work tabular representationlearning;representations tabular data;tablebased prediction tasks;deeper tabbie representations;tabular information embedding;representations tabular;tablebased prediction;tabbie representations;tabbie representations comparing;table extraction decomposition;table extraction;tables associated text;tabular substructures cells;representations comparing tabert;tables tabular information;tasks table extraction;representations different tabular;jointly models tables;exclusively tabular data;speci\ufb01cally tables tabular;representationlearning jointly models;tabular information;cells corrupt tabbie;tabular data hiroshi;tables tabular"}, "466865aaeb8902f6f8ed93ceeb5fbf9fc8b593b1": {"ta_keywords": "online speech enhancement;stftdomain deep learning;speech enhancement task;speech enhancement short;speech enhancement;reverberant speech enhancement;based speech enhancement;online enhancement deep;speech enhancement incurs;enhancement deep neural;enhancement deep;istft stftdomain deep;stftdomain deep;dnn perform frame;microphone complex;stft shorter output;microphone complex spectral;deep learning;microphone;multi microphone complex;network dnn trained;better enhancement performance;network dnn;stft istft;stftdomain achieve better;singleor multi microphone;based stft istft;multi microphone;stft shorter;frame online speech", "pdf_keywords": "speech enhancement complex;online speech enhancement;speech enhancement;neural speech enhancement;speech enhancement low;speech enhancement short;based speech enhancement;enhancement based stft;microphone complex spectral;online enhancement deep;enhancement complex spectral;stft domain neural;domain beamforming dnns;microphone complex;domain neural speech;spectral mapping microphone;enhancement deep neural;beamforming dnns easily;multi microphone complex;enhancement deep;beamforming dnns;dnns easily integrated;dnn perform frame;deep learning addition;microphone array processing;enhancement complex;singleor multi microphone;microphone;frame online speech;stft istft con\ufb01guration"}, "4759aaacd71fbb2b5ca253aa13ccceac0bc7fe8a": {"ta_keywords": "argumentation explanations;computational argumentation explanations;argumentation explanations convincingness;task computational argumentation;computational argumentation;web argument convincingness;annotating 26k explanations;argument convincingness challenging;explanations convincingness arguments;argumentation;arguments experiments feature;arguments certain controversial;crowd sourced corpus;argument convincingness;flaws convincing arguments;argument pair strengths;rich svm learners;convincingness arguments;explain argument;arguments;learners bidirectional lstm;explanations convincingness;empirical manner annotating;svm learners bidirectional;argument pairs;convincingness challenging;arguments experiments;convincing arguments experiments;convincingness challenging task;arguments certain", "pdf_keywords": ""}, "2cf21fc85af45512bf34d710f325872dca8a5331": {"ta_keywords": "traffic prediction china;prediction china openstreetmap;traffic prediction;osm predict traffic;region traffic prediction;predict traffic;predict traffic conditions;china traffic data;china openstreetmap;openstreetmap;map openstreetmap;shanghai china traffic;baidu map openstreetmap;china traffic;china openstreetmap novel;urban planning transportation;openstreetmap novel useful;traffic data;model graphical traffic;traffic condition data;map openstreetmap osm;traffic data available;openstreetmap osm;traffic conditions nearly;planning transportation;traffic conditions;openstreetmap osm free;openstreetmap novel;prediction china;region traffic", "pdf_keywords": ""}, "fb0a68981dae15f31cbcf5442509a3b8279b264c": {"ta_keywords": "enhance mfcc vectors;mfcc enhancement using;noise features proposed;stereo data based;corrupted noise features;mfcc enhancement;noise feature space;mfcc vectors dynamic;mfcc vectors;method enhance mfcc;noise features;enhance mfcc;stereo data;algorithm vts enhancement;corrupted noise feature;vts enhancement method;cost mfcc enhancement;employs stereo data;noise feature;vector corrupted noise;joint corrupted noise;splice algorithm vts;linear discriminant analysis;discriminant analysis effectively;computational cost mfcc;use linear discriminant;known splice algorithm;based feature mapping;noise environments proposed;linear discriminant", "pdf_keywords": ""}, "4d41c2fa74dd018e39ddb3cbbfead1b42615612c": {"ta_keywords": "neural architectures deepnets;architectures deep;deep architectures deep;unseen deep architectures;deep architectures;architectures deepnets;architectures deep learning;graphs neural architectures;networks parameter prediction;deep learning directly;deepnets;prediction unseen deep;deep learning successful;training networks parameter;10 imagenet leveraging;architectures deepnets 1m;deep learning;imagenet leveraging;graph neural networks;advances graph neural;neural architectures enabling;imagenet leveraging advances;graphs neural;training networks;propose hypernetwork predict;parameter prediction unseen;graph neural;deepnets 1m;networks model learns;hypernetwork predict", "pdf_keywords": "neural architectures deepnets;architectures deepnets;imagenet model learns;pass introduce deepnets;deepnets;architectures deepnets 1m;introduce deepnets;introduce deepnets 1m;deepnets 1m;initializing neural networks;10 imagenet model;neural network architectures;imagenet model;deepnets 1m use;imagenet;neural architectures;10 imagenet;graphs neural architectures;neural networks single;deepnets 1m standardized;useful initializing neural;learns strong representation;neural networks introduce;diverse feedforward neural;strong representation neural;model learns;neural networks;representation neural;cifar 10 imagenet;model learns strong"}, "709f0a4229e40339b595072ae9fbd3a1ae1fd93e": {"ta_keywords": "batching dynamic computation;instance computations batching;computations batches;organize computations batches;computations batches necessary;computations batching;architectures impractical batch;efficient batched operations;computationally efficient batched;operation batching dynamic;efficient batched;computations tensorflow cntk;computations batching algorithm;batching algorithm seamlessly;dynamic computation graphs;batched operations variety;batching dynamic;computations tensorflow;batched operations;instance learning architectures;minibatch computations;tensorflow;tensorflow cntk;batches necessary exploiting;minibatch computations aggregations;neural network toolkits;operation batching;tensorflow cntk theano;write minibatch computations;declared computations tensorflow", "pdf_keywords": "instance computations batching;computations batching;ef\ufb01cient batched operations;minibatch computations;computationally ef\ufb01cient batched;minibatch computations aggregations;computations batching algorithm;write minibatch computations;batched operations variety;batched operations;batching operations extend;batches arithmetic operations;automatically batching operations;architectures impractical batch;batching algorithm seamlessly;batching operations;gpus evaluate batches;batches arithmetic;single instance computations;using dynet autobatch;dynet autobatch;implementation dynet toolkit;evaluate batches arithmetic;dynet autobatch command;batches comparable speedups;batching algorithm;ef\ufb01cient batched;instance computations;batches;evaluate batches"}, "d745ba895cf8dcba5670fb01feea931fc72f9c77": {"ta_keywords": "deep reinforcement learning;reinforcement learning drl;deep reinforcement;source deep reinforcement;learning policy epresentations;transfer learning experiments;complexity learning policy;reinforcement learning;learning learning policy;sample complexity learning;learning policy;transfer learning learning;transfer learning;learning policy scratch;layers trained game;complexity learning;learning experiments;benefits transfer learning;reinforcement;trained game;learning learning;trained game related;learning;layers trained;set transfer learning;transfer simpler environments;drl sample complexity;learning drl;learning drl sample;complex downstream tasks", "pdf_keywords": ""}, "af6c6e66fe0a9ba19c304665e01db1c5a5fba1e4": {"ta_keywords": "constrained markov decision;confidence reinforcement learning;reinforcement learning known;reinforcement learning settings;known constrained markov;markov decision processes;reinforcement learning;constrained markov;reward function constraints;markov decision;upper confidence reinforcement;constrained upper confidence;stochastic decision problems;stochastic decision;confidence reinforcement;cost constraints constrained;learning settings reward;cost constraints;learning known dynamics;constraints described cost;auxiliary cost constraints;class stochastic decision;priori transition kernel;reinforcement;constraints constrained upper;decision processes;cost functions unknown;kernel known constrained;select policy;described cost functions", "pdf_keywords": ""}, "3389b6b8ee5a1ef0395df9f383e771650087b828": {"ta_keywords": "information retrieval pre;classical information retrieval;information retrieval;information retrieval systems;retrieval systems answer;information retrieval search;turn information retrieval;retrieval search engine;documents corpus trained;retrieval systems;answers pre trained;trained language models;domain expert turn;domain expert;retrieval pre trained;retrieval search;domain experts;language models synthesized;language models;pre trained language;engage domain expert;corpus trained;systems answer information;retrieval pre;retrieval;domain experts true;corpus trained paper;documents corpus;trained language;authoritative answers pre", "pdf_keywords": "ir systems nlp;retrieval question answering;information retrieval pre;information retrieval;modern ir nlp;systems nlp models;information retrieval meant;question answering;question answering summarization;trained language models;domain expert response;expert response quality;retrieval ranking;information retrieval framework;language models synthesized;classical information retrieval;retrieval framework proposed;based information retrieval;language models;summarization new tasks;systems nlp;ir nlp;retrieval ranking components;nlp models;nlp models achieve;retrieval framework;retrieval pre trained;expert quality answers;text corpus model;document retrieval"}, "f432d10677897cf72f9594ba7bd4c9199b270fb3": {"ta_keywords": "classification performance measures;classification performance;performance measures;performance measures used;analysis classification performance;adequately evaluating classification;used evaluating classification;performance measures formally;evaluating classification results;results accuracy measure;evaluating classification;classification results accuracy;accuracy measure;properties performance measures;classification results finally;classification results;balanced accuracy;systematic analysis classification;measure best;measures used evaluating;classification;classification literature believe;used classification;analysis classification;classification literature;balanced accuracy previously;used classification literature;symmetric balanced accuracy;results accuracy;ideally choose measure", "pdf_keywords": "good classi\ufb01cation measures;classi\ufb01cation measures;performance measures used;types performance measures;classi\ufb01cation measures arxiv;performance measures;classi\ufb01cation results accuracy;precision evaluate classi\ufb01cation;performance measures threshold;abstract performance measures;class accuracy;evaluating classi\ufb01cation results;accuracy measure family;accuracy probability measures;evaluate classi\ufb01cation results;results accuracy measure;classi\ufb01cation results compare;used evaluating classi\ufb01cation;evaluating classi\ufb01cation;accuracy measure;class accuracy probability;classi\ufb01cation results;evaluate classi\ufb01cation;measures used evaluating;loss ranking measures;analyze multiclass;rigorously analyze multiclass;ranking measures;analyze multiclass scenario;types performance"}, "da9ec5053c8ad8854bdd2ddc3f9c3d82a4114d71": {"ta_keywords": "endangered languages ocr;endangered language texts;correction endangered language;languages ocr;languages ocr post;transcriptions scanned books;setting endangered languages;ocr tools;endangered languages;endangered languages present;endangered language;languages develop ocr;models endangered languages;dataset transcriptions scanned;ocr tools robust;critically endangered languages;transcriptions scanned;ocr post correction;purpose ocr tools;benchmark dataset transcriptions;endangered languages develop;post correction endangered;develop ocr post;develop ocr;dataset transcriptions;ocr;correction endangered;general purpose ocr;language texts;ocr post", "pdf_keywords": "endangered language text;develop ocr postcorrection;transcription endangered language;ocr postcorrection method;benchmark dataset ocr;ocr postcorrection;endangered languages methods;ocr tools;ocr systems;ocr tools robust;dataset ocr;setting endangered languages;ocr systems recent;purpose ocr tools;endangered languages present;endangered languages;dataset ocr post;endangered languages use;pass ocr systems;language text image;language text;endangered language;benchmark dataset transcriptions;ocr;transcriptions scanned books;critically endangered languages;develop ocr;general purpose ocr;dataset transcriptions scanned;endangered languages ainu"}, "f66c82ca087b435463ef4fa0de49825c4eb55885": {"ta_keywords": "semantic parsing;semantic parsing developed;context free parser;method semantic parsing;tree based semantic;semantic annotation learns;parser using context;parsing;parsing developed reliable;semantic annotation;domain atis corpus;parser;parsing developed;parser using;novel parser;based semantic annotation;free parser using;free parser;atis corpus;novel parser extends;annotation learns data;atis corpus uses;annotation learns;uses novel parser;parser extends general;semantic;parser extends;corpus;annotation;data formalization", "pdf_keywords": ""}, "b37d073109cfcf913cf53aded3872e6158e828a0": {"ta_keywords": "vision language navigation;vision language;ground visual objects;visual objects;visual environment looking;task vision language;vln task vision;door ground visual;visual environment;ground visual;door routes visual;visual counterparts;visual features;object detections ensemble;visual features actually;outperform visual counterparts;visual features outperform;including object detections;discover visual features;language navigation vln;environments benchmark room;visual;features outperform visual;surprisingly discover visual;visual objects turn;ablating visual features;state art vln;routes visual environment;discover visual;benchmark room", "pdf_keywords": "visual representation ground;ground language visual;visual textual attention;attention image features;visual attention image;features textual attention;attention image;visual attention;pretrained object detection;agent resnet cnn;visual models mixture;language visual context;cnn features visual;visual context generalizable;textual attention;language visual;object based visual;attention;attention mechanism visual;visual context;features visual agent;resnet cnn;visual agent resnet;visual representation;visual models;ensemble visual;mechanism visual attention;attention mechanism route;representation ground language;resnet cnn features"}, "e2ef0dc26a669ed764e2d70257b162298b8b608e": {"ta_keywords": "maximizing secrecy rate;maximizing secrecy;multiantenna dfrc secrecy;dfrc secrecy channel;maximize secrecy rate;algorithm maximizing secrecy;secrecy channel multiple;maximize secrecy;secrecy channel;channel multiple eavesdroppers;multiple eavesdroppers;approach maximize secrecy;eavesdroppers numerical experiments;multiple eavesdroppers numerical;eavesdroppers;eavesdroppers numerical;dfrc secrecy;secrecy rate;radar channels dfrc;secrecy rate dfrc;surfaces radar channels;eavesdropping messages aimed;dual function radarcommunications;systems susceptibility eavesdropping;eavesdropper;eavesdropper ed;susceptibility eavesdropping;act eavesdropper;act eavesdropper ed;radarcommunications dfrc systems", "pdf_keywords": "multi antenna secrecy;antenna dfrc secrecy;multiantenna dfrc secrecy;antenna secrecy rate;optm3sec optimizing multicast;dfrc secrecy channel;optimizing multicast irs;antenna secrecy;secrecy rate optm3sec;multicast multi antenna;radarcommunications dfrc;multi antenna dfrc;function radarcommunications dfrc;radarcommunications dfrc systems;optimizing multicast;secrecy channel multiple;dfrc secrecy rate;dual function radarcommunications;channel multiple eavesdroppers;multicast irs;multicast irs aided;dfrc secrecy;multiple ed multicast;ed multicast multi;multiple eavesdroppers;antenna dfrc;radarcommunications;multicast multi;function radarcommunications;aided multiantenna dfrc"}, "243880fde63abfc287bd1356c2e1dbf68a1a0aac": {"ta_keywords": "indigenous language transliteration;indigenous language technologies;language technologies canada;learning indigenous language;language learning indigenous;indigenous language;indigenous languages;languages spoken canada;indigenous languages spoken;spoken canada assess;horizons indigenous language;60 indigenous languages;language transliteration text;spoken canada;language technologies;transliteration text prediction;translation speech recognition;language transliteration;transliteration text;transliteration;learning indigenous;speech image technologies;text speech image;text speech;translation speech;machine translation speech;technologies canada;speech image;aided language;speech recognition", "pdf_keywords": "indigenous language technologies;canada indigenous language;indigenous language transliteration;technologies indigenous languages;language technologies canada;indigenous language;indigenous languages;development indigenous language;indigenous languages based;spoken canada indigenous;indigenous languages spoken;horizons indigenous language;indigenous1 languages;indigenous1 languages 10;60 indigenous languages;languages spoken canada;language technologies;60 indigenous1 languages;educational technologies indigenous;language transliteration text;language technologies faces;transliteration text prediction;spoken canada;language transliteration;currently spoken canada;canada indigenous;translation speech recognition;technologies indigenous;transliteration text;language learning document"}, "9364eff879a9dcb34fe3dfdd0843e69c14dd333b": {"ta_keywords": "optimizing models interpretability;optimize interpretability directly;optimize interpretability;models predictive interpretable;predictive interpretable;models interpretability;models interpretable accurate;models interpretable;work optimize interpretability;models interpretability relied;interpretability directly;interpretability directly including;interpretability sparsity;predictive interpretable demonstrate;interpretability;interpretable accurate;interpretable;interpretability relied easy;proxies interpretability sparsity;optimizing models;interpretability relied;desire models interpretable;interpretable accurate work;quantify proxies interpretability;interpretability sparsity number;including humans optimization;models predictive;work optimizing models;user studies models;proxies interpretability", "pdf_keywords": "optimize models human;models predictive interpretable;interpretability alongside prediction;predictive interpretable;optimization evaluate interpretability;models human interpretability;optimize models;predictive neural networks;ef\ufb01ciently optimize models;predictive interpretable demonstrate;neural networks jointly;alongside prediction directly;prediction directly including;alongside prediction;models predictive;prediction directly;predictive neural;human interpretability alongside;optimize collection predictive;model based optimization;collection predictive neural;interpretability model user;jointly optimize;networks jointly optimize;inference identifying models;including humans optimization;human interpretability;humans optimization;high likelihood models;humans optimization loop"}, "64c575bb8b3e11097605028de5c289b0b2d839a4": {"ta_keywords": "speech synthesis preserving;lingual speech synthesis;adaptation phonetic;speech synthesis preserves;adaptation phonetic correction;model adaptation phonetic;native speech synthesis;speech synthesis;synthesis preserves speaker;naturalness preserving speaker;based speech synthesis;synthesis preserving speaker;speaker individuality synthetic;speech synthesis based;speech synthesis improve;preserves speaker individuality;preserve speaker individuality;preserving speaker individuality;individuality synthetic speech;synthetic speech compared;synthetic speech;correction prosodic phonetic;speaker individuality synthesized;synthesis based voice;based voice conversion;voice conversion;synthesized target speech;phonetic correction method;speech synthesis synthesizes;preserving speaker", "pdf_keywords": ""}, "5af9ab65d186e4e1e0b1cef1962ca15336f37931": {"ta_keywords": "semantic parser corpus;dependent semantic parsing;semantic parsing;semantic parser;corpus imitation learning;semantic parsing mrl;develop semantic parser;parser corpus adapting;parser corpus;corpus imitation;corpus adapting imitation;new corpus imitation;semantic parsing develop;context dependent semantic;parsing develop semantic;corpus context dependent;parsing;corpus adapting;available corpus context;corpus;dependent semantic;new corpus;training new corpus;corpus context;semantic;parser;parsing mrl;corpora assume utterances;adapting imitation learning;available corpus", "pdf_keywords": ""}, "4f8e1a4247ce06a15760fc2692c6849601d41b6f": {"ta_keywords": "textual entailment datasets;knowledge textual entailment;textual entailment task;textual entailment;textual entailment fundamental;task textual entailment;encoded useful entailment;text based entailment;entailment datasets;entailment datasets use;entailment models information;multiple textual entailment;entailment fundamental task;entailment task;entailment models;based entailment models;useful entailment;entailment task using;infusing knowledge textual;knowledge textual;entailment fundamental;based entailment;entailment;task natural language;knowledge graphs kgs;capture structural semantic;useful entailment evaluate;natural language processing;semantic information kgs;external knowledge sources", "pdf_keywords": "knowledge textual entailment;textual entailment task;external knowledge textual;textual entailment;capture structural semantic;contextual subgraphs;textual entailment fundamental;infusing knowledge textual;abstract textual entailment;contextually relevant subgraphs;knowledge textual;generate contextual subgraphs;text based entailment;networks infusing knowledge;structural semantic information;entailment models information;entailment fundamental task;structural semantic;entailment task;entailment models;based entailment models;semantic information kgs;entailment task using;knowledge graph;graph convolutional networks;contextual subgraphs reduced;encoded graph convolutional;based entailment;task natural language;entailment fundamental"}, "9cf75483deee77b3c0ee4f996d808437ab4a7435": {"ta_keywords": "thermal gelation chicken;breast myofibrillar protein;myofibrillar protein;chicken breast myofibrillar;gelation chicken breast;process thermal gelation;thermal gelation;breast myofibrillar;gelation chicken;myofibrillar;protein;process thermal;oxidation process thermal;gelation;chicken breast;chicken;thermal;effect oxidation process;oxidation process;effect oxidation;oxidation;breast;process;effect", "pdf_keywords": ""}, "0ce184bd55a4736ec64e5d82a85421298e0373ea": {"ta_keywords": "rnn speech applications;transformer vs rnn;rnn speech;vs rnn speech;transformer conventional recurrent;networks rnn;neural networks rnn;benchmarks comparison rnn;comparison rnn;rnn;speech applications;recurrent neural networks;speech applications experiments;multilingual asr;vs rnn;asr multilingual asr;multilingual asr st;networks rnn total;asr multilingual;comparison rnn undertook;transformer vs;15 asr multilingual;15 asr benchmarks;datasets asr;conventional recurrent neural;study transformer vs;available datasets asr;asr benchmarks;rnn total;st tts benchmarks", "pdf_keywords": "transformer rnn speech;rnn speech applications;speech recognition asr;outperforms rnn multilingual;transformer vs rnn;study transformer rnn;recognition asr speech;vs rnn speech;transformer outperforms rnn;transformer rnn;rnn multilingual end;using transformer rnn;rnn multilingual;rnn speech;speech applications shigeki;asr speech translation;various corpora asr;transformer rnn following;hmm based asr;multilingual asr;end speech applications;speech processing;speech applications various;speech recognition;speech applications;speech applications using;speech translation st;asr multilingual asr;automatic speech;multilingual asr st"}, "448e15e267b20bee1644034e18630da2e68cf36e": {"ta_keywords": "seismic response analysis;seismic response;response analysis subway;sand using ale;analysis subway station;loose sand using;seismic;subway station deep;analysis subway;deep loose sand;loose sand;using ale method;station deep loose;ale method;subway station;subway;sand using;using ale;station deep;ale;response analysis;sand;deep loose;loose;station;response;method;deep;analysis;using", "pdf_keywords": ""}, "4c94dc1b2391d78c9cfdd69955d20b56d7a16982": {"ta_keywords": "systems erasure codes;linear mds codes;mds convertible codes;tuning code redundancy;erasure codes;distributed storage systems;storage systems erasure;mds codes;mds codes valid;distributed storage;code redundancy;erasure codes used;code redundancy observed;storage systems;redundancy requires code;reduce storage cost;code conversion;scale distributed storage;data tuning code;encoded data tuning;codes valid parameters;code dimension;storage cost;convertible codes valid;conversion linear mds;convertible codes specific;storage cost large;convertible codes;code dimension length;code conversion change", "pdf_keywords": "linear mds codes;mds convertible codes;convertible codes access;codes access optimal;convertible codes inconsequential;convertible codes valid;design convertible codes;convertible codes;linear mds convertible;conversion linear mds;construction linear mds;mds codes;analysis code construction;optimal linear mds;codes valid parameters;code construction;code construction arxiv;mds codes valid;lower bounds access;bound access cost;bounds access cost;lower bound access;linear mds;access cost conversion;access optimal;access optimal parameter;access optimal linear;proposed lower bounds;explicit construction access;cost conversion linear"}, "c55d5805a6eb8b1482f21581fe893484eaf9ffb5": {"ta_keywords": "perceived age singing;age singing voices;singing voice age;age singing voice;singer prosodic features;singer perceived age;voice age;age singer perceived;voice timbre control;controlling voice timbre;voice conversion perceived;age singing;voice timbre based;voice age singer;age singer prosodic;age spectral prosodic;features proposed voice;singing voice conversion;voice timbre;proposed voice timbre;singing voices;song voice timbre;singing voices corresponds;singing voice voice;controlling voice;words singing voice;voice conversion;features prosodic features;singing voice;change singer perceived", "pdf_keywords": ""}, "4275d4c4bd10742b321467f175f16198ed7d17d7": {"ta_keywords": "ai cooperative music;track music generation;domain music generation;music creation;music generation using;music generation;cooperative music creation;music models;music models learn;music creation given;generate coherent music;midi;music generation accompaniment;midi files generate;track sequential generative;rock music models;networks gans symbolic;symbolic domain music;multi track music;generate additional tracks;learn noisy midi;track music;accompaniment multi track;cooperative music;networks gans;midi bars;sequential generative adversarial;study generative adversarial;generative adversarial networks;midi files", "pdf_keywords": ""}, "802ddaf5bd731b91e64d8cee43f7fb614b42c1df": {"ta_keywords": "online allocation perishable;fair online allocation;allocation perishable goods;online allocation;goods application electric;electric vehicle charging;allocation perishable;perishable goods application;vehicle charging proceedings;vehicle charging;electric vehicle;fair online;perishable goods;application electric vehicle;allocation;goods application;walsh fair online;goods;charging proceedings;charging;artificial intelligence 2019;artificial intelligence;charging proceedings 28th;application electric;conference artificial intelligence;joint conference artificial;conference artificial;electric;intelligence 2019;vehicle", "pdf_keywords": ""}, "75f90cbbf3c27a8b27567d6a9c8c4538743c8fff": {"ta_keywords": "toolkit text generation;text generation texar;text generation tasks;text generation;extensible toolkit text;generation texar;generation texar model;paradigms texar modularized;texar modularized versatile;texar modularized;versatile extensible toolkit;extensible toolkit;texar particularly suitable;set text generation;natural language machine;machine translation;generation tasks;toolkit text;texar model architecture;language machine translation;dialog content manipulation;introduce texar;source toolkit;inputs natural language;algorithmic paradigms texar;open source toolkit;texar model;generation tasks transform;toolkit;texar open source", "pdf_keywords": ""}, "7e386158f474a395618c5e065ac55844b507007c": {"ta_keywords": "superb speech processing;speech processing universal;speech processing tasks;adaptation superb speech;wide range speech;range speech processing;general speech processing;lightweight prediction heads;speech processing;universal performance benchmark;lightweight prediction;learning general speech;introduce speech processing;specialized lightweight prediction;challenge leaderboard benchmark;superb leaderboard benchmark;task specialized lightweight;benchmark performance shared;leaderboard benchmark toolkit;learning task specialized;performance benchmark superb;leaderboard benchmark performance;processing universal performance;superb tasks learning;benchmark toolkit;leaderboard benchmark;accessibility superb tasks;benchmark superb;benchmark performance;performance benchmark", "pdf_keywords": "speech processing universal;universal performance benchmark;general speech processing;learning general speech;release speech processing;speech processing;challenge leaderboard1 benchmark;lightweight prediction heads;generalizability pretrained models;evaluating generalizability pretrained;processing universal performance;specialized lightweight prediction;introduce speech processing;generalizable usable pretrained;leaderboard1 benchmark toolkit2;speech processing introduce;leaderboard1 benchmark;generalizability pretrained;lightweight prediction;benchmark toolkit2;performance benchmark superb;performance benchmark;learning task specialized;universal performance;task specialized lightweight;processing introduce speech;benchmark;pretrained models;benchmark superb;superb tasks learning"}, "9976ed0d88a4156ecdd3ebe39714c5fb4a5a0246": {"ta_keywords": "kaldi speech recognition;speech recognition toolkit;vocabulary speech recognition;speech recognition systems;speech recognition;corpus spontaneous japanese;speech recognition laborious;matrix adaptation evolution;large vocabulary speech;optimization optimized training;japanese large vocabulary;optimized training script;evolution strategy cma;based large vocabulary;hmm based large;tsubame supercomputer automation;matrix adaptation;tune dnn hmm;human experts tuning;experts tuning;dnn hmm based;tsubame supercomputer;adaptation evolution strategy;experts tuning numerous;speech recognition performed;using tsubame supercomputer;japanese csj using;manually tuned;optimized training;experiments corpus spontaneous", "pdf_keywords": ""}, "814421bb20ba1fba88928fc168db1b7175cca6ac": {"ta_keywords": "control electrolarynx based;f0 control electrolarynx;control electrolarynx;electrolarynx based;electrolarynx based real;electrolarynx;f0 control;real time excitation;direct f0 control;excitation prediction;time excitation prediction;excitation;control;time excitation;direct f0;f0;based real time;implementation direct f0;real time;prediction;based real;implementation direct;implementation;direct;real;based;time", "pdf_keywords": ""}, "e8bd03ff376ab3c863f72f931c91e90eeb9b2be9": {"ta_keywords": "intellectual property knowledge;asian intellectual property;intellectual property;property knowledge network;property knowledge;knowledge network;south asian intellectual;asian intellectual;knowledge;south asian;intellectual;network;property;asian;south", "pdf_keywords": ""}, "36c95e3ef362742a5c1844257e8b79d3251a781e": {"ta_keywords": "useful robotic language;modeling vision language;language 3d objects;shapenet annotated referring;language useful robotic;vision language useful;robotic language understanding;benchmark shapenet annotated;vision language;robotic language;objects referred language;shapenet annotated;visual language;identifying objects referred;visual language 3d;referred language robot;jointly modeling vision;new benchmark shapenet;objects referred;benchmark shapenet;distinguishing objects demonstrate;language requests robot;language robot;identifying objects;non visual language;snare identifying objects;language 3d;distinguishing objects;models distinguishing objects;annotated referring expressions", "pdf_keywords": "vision language models;shapenet annotated referring;modeling vision language;benchmark shapenet annotated;useful robotic language;robotic language understanding;language 3d objects;language useful robotic;referent objects trained;vision language useful;objects referred language;vision language;pretrained vision language;robotic language;new benchmark shapenet;objects trained;jointly modeling vision;benchmark shapenet;shapenet annotated;annotated referring expressions;objects referred;referred language robot;visual language 3d;identifying objects referred;language grounding object;language grounding models;object referents natural;visual language;estimation language grounding;annotated referring"}, "807e421679d4a9d629d2fad1f60f28787dca60e7": {"ta_keywords": "generated questions training;training question answering;model generate questions;generate questions;generated questions human;generate questions based;generative domain adaptive;question answering models;human generated questions;questions training;generated questions;training framework generative;question answering;questions human generated;answering models;domain adaptation;answering models propose;domain adaptive nets;model generated questions;generative model generate;questions based unlabeled;generative domain;novel domain adaptation;domain adaptive;framework generative domain;domain adaptation algorithms;framework train generative;generative model;generative;questions training question", "pdf_keywords": "generative domain adaptive;domain adaptation;novel domain adaptation;generative domain;incorporate domain adaptation;propose generative domain;domain adaptation algorithms;domain adaptation techniques;domain adaptive nets;domain adaptive;generative models reinforcement;generative models semi;adaptation techniques generative;generative models;called generative domain;employ domain adaptation;generative model;generative;discriminative model adversarial;semi supervised;semi supervised learning;techniques generative models;models semi supervised;propose generative;tune generative model;techniques generative;generative model minimize;tune generative;combination generative models;framework called generative"}, "d06493373421c86ba33dbb8834ccb725105a665f": {"ta_keywords": "lexical distinctions extracting;grained lexical distinctions;grained distinctions vocabulary;rules governing lexical;fine grained lexical;rules language learning;distinctions extracting rules;grained lexical;governing lexical selection;lexical distinctions;lexical selection;distinctions vocabulary items;fine grained distinctions;governing lexical;distinctions extracting;learning fine grained;extracting rules explaining;rules explaining distinctions;extracted rules language;rules language;language learning;grained distinctions;distinctions vocabulary;extracting rules governing;lexical selection learning;extracting rules;lexical;translations wall pared;vocabulary items;extracted rules", "pdf_keywords": "grained lexical distinctions;grained distinctions vocabulary;grained lexical;extract lexical semantic;lexical choices extract;lexical distinctions present;lexical selection;governing lexical selection;easier interpretation lexical;lexical distinctions;lexical distinction;lexical semantic features;word extract lexical;extract lexical;rules governing lexical;l1 different lexical;distinguishes lexical choices;lexical semantic;lexical distinction work;\ufb01ne grained lexical;lexical choices;interpretation lexical distinction;distinguishes lexical;l2 learners concise;distinctions vocabulary items;learners concise descriptions;model distinguishes lexical;encoding lexical choice;lexical choice target;lexical choice"}, "ece62ada00cef99d9fc7a60e7d4b773f6d87c8f9": {"ta_keywords": "tasks machine translation;text speech ariel;machine translation mt;machine translation;human language technologies;speech ariel cmu;resource human language;ariel cmu submissions;translation mt entity;speech ariel;evaluations tasks machine;language technologies lorehlt;describes ariel cmu;ariel cmu systems;human language;ariel cmu;situation frames text;language technologies;text speech;frames text speech;tasks machine;paper describes ariel;lorehlt 2018 evaluations;ariel;cmu systems lorehlt18;describes ariel;sf text speech;systems lorehlt18;speech sf text;2018 evaluations tasks", "pdf_keywords": "lexicons training multilingual;lingual corpus;multi lingual corpus;multilingual corpus;bilingual lexicons training;annotators monolingual english;bilingual lexicons;language training data;multilingual corpus collected;training multilingual word;lingual corpus ni;nmt multilingual corpus;speaker annotators monolingual;annotators monolingual;multilingual word vectors;monolingual english text;training data multilingual;multi lingual;translation training data;monolingual sinhala text;lexicons training;data multilingual word;bilingual dictionaries;english bilingual dictionaries;deriving bilingual lexicons;languages multi lingual;training multilingual;speech annotation;linguistics human language;sinhala english data"}, "73635c9dc0ffb61c2eac79234108c6eee1362c1b": {"ta_keywords": "bandits correlated markovian;bandit problem dynamic;bandit problem;multi armed bandits;armed bandit problem;bandits correlated;smoothed reward feedback;multi armed bandit;armed bandits correlated;smoothed reward;observes smoothed reward;bandits;armed bandits;environments smoothed reward;bandit;reward time;discounted rewards epoch;reward feedback proposed;armed bandit;epoch reward discount;reward time average;rewards evolve correlated;epoch reward time;reward feedback;average rewards epoch;rewards epoch reward;correlated markovian;epoch reward;reward discount;reward feedback following", "pdf_keywords": "bandits correlated markovian;correlated markovian rewards;bandit problems environments;bandit problem dynamic;markovian rewards introduction;markovian rewards;bandit algorithms problems;solving bandit problems;bandit problems;approach solving bandit;bandit algorithms;bandit problem;solving bandit;based bandit algorithms;armed bandit problem;bandits correlated;multi armed bandits;armed bandits correlated;problems correlated markovian;multi armed bandit;correlated markovian;correlated markovian environments;rewards correlated;bandits;armed bandits;bandits play;future rewards correlated;armed bandits play;rewards correlated past;bandit"}, "18289b2b04fc8a7a86f474236e55a3b1070a98ad": {"ta_keywords": "\u6587\u732e\u6284\u9332 \u6d44\u6c34\u51e6\u7406\u306b\u304a\u3051\u308b\u819c\u30d5\u30a1\u30a6\u30ea\u30f3\u30b0\u30b3\u30f3\u30c8\u30ed\u30fc\u30eb\u306e\u305f\u3081\u306e\u51dd\u96c6\u53ca\u3073\u9178\u5316\u51e6\u7406 \u6d78\u6f2c\u819c\u69fd\u3078\u306e\u4f4e\u6fc3\u5ea6\u30aa\u30be\u30f3\u6ce8\u5165\u306e\u9069\u7528;\u6d44\u6c34\u51e6\u7406\u306b\u304a\u3051\u308b\u819c\u30d5\u30a1\u30a6\u30ea\u30f3\u30b0\u30b3\u30f3\u30c8\u30ed\u30fc\u30eb\u306e\u305f\u3081\u306e\u51dd\u96c6\u53ca\u3073\u9178\u5316\u51e6\u7406 \u6d78\u6f2c\u819c\u69fd\u3078\u306e\u4f4e\u6fc3\u5ea6\u30aa\u30be\u30f3\u6ce8\u5165\u306e\u9069\u7528;\u6587\u732e\u6284\u9332 \u6d44\u6c34\u51e6\u7406\u306b\u304a\u3051\u308b\u819c\u30d5\u30a1\u30a6\u30ea\u30f3\u30b0\u30b3\u30f3\u30c8\u30ed\u30fc\u30eb\u306e\u305f\u3081\u306e\u51dd\u96c6\u53ca\u3073\u9178\u5316\u51e6\u7406;\u6d44\u6c34\u51e6\u7406\u306b\u304a\u3051\u308b\u819c\u30d5\u30a1\u30a6\u30ea\u30f3\u30b0\u30b3\u30f3\u30c8\u30ed\u30fc\u30eb\u306e\u305f\u3081\u306e\u51dd\u96c6\u53ca\u3073\u9178\u5316\u51e6\u7406;\u6d78\u6f2c\u819c\u69fd\u3078\u306e\u4f4e\u6fc3\u5ea6\u30aa\u30be\u30f3\u6ce8\u5165\u306e\u9069\u7528;\u6587\u732e\u6284\u9332", "pdf_keywords": ""}, "db500c4e746897e5d5adafbf222b959c512445ad": {"ta_keywords": "training data adversarial;data adversarial;data adversarial attacks;adversarial attacks alter;adversarial;adversarial attacks;attacks alter nlp;data poisoning attack;nlp model predictions;new data poisoning;data poisoning work;data poisoning;customizing triggers concealed;poison examples sentiment;allows adversary control;concealed data poisoning;adversary control;model predictions perturbing;poisoning attack allows;triggers concealed data;adversary control model;poisoning attack;predictions manipulated;predictions perturbing;predictions desired trigger;predict positive input;attack cost prediction;proposing defenses mitigate;triggers;understood predictions manipulated", "pdf_keywords": "vulnerabilities nlp models;new vulnerabilities nlp;vulnerabilities nlp;poisoned models trained;vulnerability nlp models;vulnerabilities machine learning;new vulnerability nlp;vulnerability nlp;poisoned models;poisoning attack exposes;poison examples data;examples data poison;construct data poisoning;data poisoning attack;non poisoned models;dangerous new vulnerabilities;new data poisoning;identify novel vulnerabilities;poisoned examples;adversary inserts;data poison;novel vulnerabilities;new vulnerabilities;poisoning attack;poisoning attack allows;nlp models;data poisoning;overlap poison examples;poison examples;debug adversary inserts"}, "00936aa7c8f64fc919dd4dcee6192ccc83e0d26e": {"ta_keywords": "lesions proximal reflectance;extracted teeth imaged;swir transillumination reflectance;transillumination reflectance imaging;teeth imaged micro;swir occlusal reflectance;proximal transillumination reflectance;occlusal reflectance performed;transillumination occlusal reflectance;interproximal lesions tooth;occlusal transillumination reflectance;occlusal reflectance images;proximal reflectance useful;interproximal lesions radiography;reflectance useful imaging;infrared swir transillumination;reflectance imaging device;reflectance imaging;imaging interproximal lesions;swir imaging radiography;transillumination reflectance multispectral;reflectance multispectral imaging;transillumination reflectance;proximal reflectance;lesions radiography using;transillumination reflectance device;radiography detect lesions;occlusal reflectance;reflectance performed;reflectance images useful", "pdf_keywords": ""}, "58b0800ef48da2678e15e5e8bc1d786e24190742": {"ta_keywords": "speech enhancement separation;field speech processing;speech enhancement recognition;learning speech enhancement;far field speech;deep learning speech;speech enhancement;deals speech enhancement;speech recognition;speech recognition asr;speech processing;recognition asr far;enhancement separation recognition;speech processing active;signal enhancement extraction;separation recognition special;separation recognition;audio acoustics speech;automatic speech recognition;era deep learning;deep learning;recognition using microphones;field speech;speech processing era;recognition asr;asr far field;enhancement extraction separation;end signal enhancement;amazon echo challenge;microphones", "pdf_keywords": ""}, "901fbb51d6fb9078e572c83a446b408da4de9b2b": {"ta_keywords": "unsupervised frame induction;uncontextualized word embeddings;frame induction using;frame induction;induction using contextualized;unsupervised frame;word embeddings;contextualized uncontextualized word;contextualized uncontextualized;using contextualized uncontextualized;contextualized;using contextualized;embeddings;uncontextualized word;uncontextualized;unsupervised;frame;induction;induction using;word;using", "pdf_keywords": ""}, "d0a58b6da9f7788534aa9963a78c24a87038e4fc": {"ta_keywords": "computational social choice;empirical risk minimization;norm empirical risk;learning community aggregate;choosing choose papers;important novelty community;empirical risk;choose papers common;community embraced paper;novelty community;community aggregate mapping;computational social;novelty community embraced;choose papers;risk minimization learning;choice identify desirable;community aggregate;risk minimization;specifically characterize choice;norm empirical;minimization learning community;desirable values specifically;social choice;empirical;based norm empirical;desirable values;identify desirable values;characterize choice;social choice identify;identify desirable", "pdf_keywords": ""}, "490c31b460316b7f68e9b8f5ff0d26aef2f7f45f": {"ta_keywords": "mdp congestion game;atomic congestion game;congestion game decision;congestion game equilibria;sensitivity mdp congestion;decision process congestion;process congestion games;selfish optimization states;mdp congestion;congestion game;non atomic congestion;selfish optimization;congestion games;game equilibria uncertainty;performs selfish optimization;congestion games analyze;atomic congestion;congestion effects state;cost influence congestion;congestion;process congestion;mdp decision makers;common mdp decision;markov decision process;game equilibria;influence congestion;mdp decision;optimization states common;optimization states;costs sensitivity analysis", "pdf_keywords": "congestion game equilibria;mdp congestion game;congestion game formulated;game equilibria uncertainty;congestion game framework;sensitivity mdp congestion;perspective mdp congestion;congestion game;mdp congestion;introduce mdp congestion;game equilibria analysis;optimal user equilibrium;game equilibria;user equilibrium;equilibria uncertainty;equilibria traf\ufb01c assignment;sensitivity wardrop equilibria;user equilibrium 12;congestion;inequality style game;style game equilibria;wardrop equilibria traf\ufb01c;wardrop equilibria;stochastic braess paradox;paradox sensitivity analysis;equilibria uncertainty perturbations;work wardrop equilibria;sensitivity mdp;paradox sensitivity;equilibria analysis"}, "2e0b1484740047d6d6fb6bd2c9d8816b54b33811": {"ta_keywords": "ordinal rankings reviewers;rankings reviewers;rankings reviewers goal;peer review;peer review process;assessment peer review;review process;review process subsequent;review process provide;review process including;review process novel;reviewers;reviewers goal;collected review process;reviewers goal check;quality assessment peer;collecting ordinal rankings;conference machine learning;review;data collected review;ordinal rankings;collected review;soundness review process;rankings;reviewers 100;design review process;process subsequent conferences;conference machine;reviewers 100 terms;terms reviewers 100", "pdf_keywords": "reviewers nips;reviewers nips 2016;processing systems nips;analysis nips 2016;nips 2016 review;analysis nips;disagreement reviewers nips;review process quantify;reviews identi\ufb01ed automated;reviewers detect;systems nips;design analysis nips;reviewers paper;reviewers quantifying;collected review process;common reviewers detect;review process;review process review;hoc analysis nips;review process including;information processing;pools reviewers quantifying;machine learning tools;review process enumerated;review data analysis;review data;process review;nips 2016 compared;2016 review process;processing systems"}, "2444be7584d1f5a7e2aa9f65078de09154f14ea1": {"ta_keywords": "teachers knowledge distillation;teacher outputs predicted;confidence weighted teacher;bans outperform teachers;dark knowledge permuted;dark knowledge;knowledge permuted predictions;models train students;learning model teacher;predicted classes;knowledge distillation kd;knowledge distillation;students overpower teachers;outperform teachers;overpower teachers;weighted teacher;outperform teachers significantly;models train;predicted classes present;teachers knowledge;teachers significantly;model teacher;overpower teachers additional;ii dark knowledge;weighted teacher max;non predicted classes;model teacher student;teachers significantly computer;learning;compressing models train", "pdf_keywords": "knowledge distillation;disentangling bene\ufb01ts training;knowledge distillation kd;compressing models train;models train;teacher model converges;abstract knowledge distillation;models train students;learning model teacher;learning model;teacher model;model teacher;transferring knowledge;transferring knowledge machine;train students parameterized;simple training;model compression;compression commonly teacher;arti\ufb01cial neural networks;learning;compressing models;model teacher student;train students;simple training procedure;training technique;student train;training procedure teacher;bene\ufb01ts training technique;training;parameterized identically teachers"}, "f17ee5b9d3120960eddd2bdb9af2f4f689cebb3a": {"ta_keywords": "automatic relation extraction;supervision relation extraction;relation extraction;embeds relation mentions;relation extraction types;relation extraction study;relation extraction using;relation mentions types;jointly embeds relation;text corpora efficient;automatic relation;entity mention pairs;exploiting semantic evidence;relation mentions;pairs automatic relation;dataset indirect supervision;jointly embeds;massive text corpora;embeds relation;qa entity mention;source supervision relation;mention pairs text;indirect source supervision;mentions types qa;semantic evidence qa;pairs text features;relation types semantically;entity mention;answer pairs automatic;exploiting semantic", "pdf_keywords": "embeds relation mentions;embeds information training;supervision relation extraction;jointly embeds relation;relation extraction;entity mention pairs;relation mentions types;exploiting semantic evidence;jointly embeds;learned infer relation;linking knowledge bases;relation extraction study;richer semantic knowledge;mentionfeature qa pair;embeds relation;relation mentions;embeddings learned infer;captures richer semantic;better feature embeddings;embeddings learned;qa entity mention;feature embeddings learned;linking knowledge;exploiting semantic;mention pairs text;entity mention;model mentionfeature qa;pairs text features;generated linking knowledge;semantic evidence qa"}, "90af87c1e4fba127d6db8f5e1f9e1ef3472507e8": {"ta_keywords": "hmm\u97f3\u58f0\u5408\u6210\u306b\u304a\u3051\u308b\u5909\u8abf\u30b9\u30da\u30af\u30c8\u30eb\u306b\u57fa\u3065\u304f\u30dd\u30b9\u30c8\u30d5\u30a3\u30eb\u30bf \u5408\u6210;hmm\u97f3\u58f0\u5408\u6210\u306b\u304a\u3051\u308b\u5909\u8abf\u30b9\u30da\u30af\u30c8\u30eb\u306b\u57fa\u3065\u304f\u30dd\u30b9\u30c8\u30d5\u30a3\u30eb\u30bf \u5408\u6210 \u5909\u63db;hmm\u97f3\u58f0\u5408\u6210\u306b\u304a\u3051\u308b\u5909\u8abf\u30b9\u30da\u30af\u30c8\u30eb\u306b\u57fa\u3065\u304f\u30dd\u30b9\u30c8\u30d5\u30a3\u30eb\u30bf;\u5909\u63db \u5408\u6210;\u5408\u6210 \u751f\u6210 \u97fb\u5f8b;\u5408\u6210 \u5909\u63db \u5408\u6210;\u5909\u63db \u5408\u6210 \u751f\u6210;\u5408\u6210 \u5909\u63db;\u751f\u6210 \u97fb\u5f8b \u97f3\u58f0\u4e00\u822c;\u751f\u6210 \u97fb\u5f8b;\u5408\u6210 \u751f\u6210;\u5408\u6210;\u97fb\u5f8b \u97f3\u58f0\u4e00\u822c;\u97f3\u58f0\u4e00\u822c;\u97fb\u5f8b;\u5909\u63db;\u751f\u6210", "pdf_keywords": ""}, "39365d95992c8294ba32d85c69d337040ddb8e54": {"ta_keywords": "neural machine translation;translations tree;translations tree based;quality translations tree;dependency parse trees;trees constructed linguistic;machine translation nmt;machine translation experiments;linguistically inspired tree;syntactic information nmt;parse trees;tree based decoder;machine translation;nmt adding syntactic;tree structures like;target tree structures;arbitrary tree structure;tree structures;constituency dependency parse;translation experiments;tree structure;tree structure target;tree based;arbitrary tree;decoder neural machine;syntactic information;tree;inspired tree structures;improve quality translations;topology arbitrary tree", "pdf_keywords": "machine translation nmt;neural machine translation;tree topology translation;nmt generates translations;tree guide translation;syntactic information nmt;translations target tree;tree structures nmt;tree based decoder;nmt adding syntactic;machine translation;translation nmt;translation nmt adding;based decoder nmt;decoder nmt;decoder nmt generates;generates translations;machine translation xinyi;guide translation process;decoder neural machine;trdec novel tree;incorporating tree structures;structures nmt trdec;improve quality translations;decoder neural;translation process;tree structures;generated tree guide;tree structure;arbitrary tree structure"}, "162515d87256f13888d9d7ba95275ac4b6c35396": {"ta_keywords": "adversarial misspellings robust;combating adversarial misspellings;adversarial misspellings;adversarial spelling mistakes;combat adversarial spelling;adversarial spelling;misspellings robust;misspellings robust word;classifier outperforming adversarial;robust word recognition;combating adversarial;adversarial training shelf;outperforming adversarial;adversarial training;spell checkers defense;outperforming adversarial training;classifier bert;downstream classifier bert;trained recognize words;adversarial;spell checkers;robustness downstream classifier;adversarially;adversarially chosen character;single adversarially;combat adversarial;recognize words corrupted;classifier bert model;word recognition;robust word", "pdf_keywords": ""}, "615358de8e9a7cf318c172afafc2a303eab93d98": {"ta_keywords": "fashion coordinates recommender;recommender clothing coordinates;photographs fashion coordinates;photographs fashion models;photograph fashion item;recommender using photographs;recommender clothing;photographs fashion magazines;photographs fashion;fashion coordinates;models clothing coordinates;given photographs fashion;features fashion item;photograph fashion;photographs fashion magazine;using photographs fashion;body photographs fashion;fashion style sharing;propose recommender clothing;fashion magazines task;fashion models clothing;visual features fashion;features fashion;clothing coordinates using;real photographs fashion;fashion models;clothing coordinates;clothing coordinates serve;given photograph fashion;models clothing", "pdf_keywords": ""}, "58c04126a5196deb57ae31d6174cd4aae154f138": {"ta_keywords": "example annotation costs;learners solicit labels;examples active learning;labels hard examples;annotation costs;required naively annotate;naively annotate dataset;naively annotate;annotate dataset active;annotation budget;active learning partial;learning partial labels;example annotation;propose active learning;standard active learners;active learning;receive real annotation;real annotation presents;annotation typically;30 annotation budget;active learners given;real annotation;dataset active learning;choose example label;example label;active learners solicit;active learners;annotation budget required;annotation;annotate dataset", "pdf_keywords": "active learning partial;learners fully labels;validate active learning;example annotation costs;cheaply annotated harder;abstract active learning;learning partial labels;examples cheaply annotated;active learning;annotation costs;receive real annotation;cheaply annotated;annotated harder;real annotation;real annotation presents;annotated harder data;example annotation;learning partial feedback;learners solicit labels;labels hard examples;active learners;annotation;classes form annotation;annotation typically;annotation costs alter;annotation presents;active learners solicit;active learners diagnostic;annotated;active learning papers"}, "bdfb9f1c79ad726049a3563c741311391e18532a": {"ta_keywords": "speech style manipulation;entrainment speech recognition;using entrainment speech;entrainment speech;manipulation using entrainment;speech style;speech recognition;style manipulation;style manipulation using;using entrainment;speech;entrainment;manipulation using;style;manipulation;recognition;using", "pdf_keywords": ""}, "eadf5023c90a6af8a0f8e8605bd8050cc13c23a3": {"ta_keywords": "speech recorded kinect;kinect microphone arrays;speech recognition asr;kinect microphone;asr chime challenge;microphone pairs challenge;recorded kinect microphone;microphone arrays binaural;conversational speech recorded;arrays binaural microphone;robust automatic speech;microphone capture;array synchronization speech;microphone arrays;speech recognition;synchronization speech;speech recorded;synchronization speech enhancement;recognition asr;automatic speech;microphone;binaural microphone pairs;automatic speech recognition;distant microphone capture;chime challenge;binaural microphone;recognition asr technology;asr chime;microphone capture vs;recorded kinect", "pdf_keywords": "chime speech separation;multimicrophone conversational asr;distant multimicrophone conversational;speech recognition asr;speech separation recognition;chime challenge considers;chime challenge;multimicrophone conversational;chime speech;5th chime challenge;task distant multimicrophone;chime challenge series;\ufb01fth chime speech;abstract chime challenge;conversational asr;speech recognition;speech processing;speech separation;conversational speech recorded;recognition asr;conversational asr real;speech recorded kinect;robust automatic speech;speech processing johns;automatic speech;machine learning speech;speech recorded;automatic speech recognition;chime;distant multimicrophone"}, "11465566a1f5ec7d4176bb7ab8edd26a154a1b60": {"ta_keywords": "privacy contracts electric;proposing privacy contracts;privacy costs operation;privacy contracts;consumer valuation privacy;probability privacy;valuations privacy costs;privacy costs;valuations privacy;different valuations privacy;depending probability privacy;valuation privacy;proposing privacy;valuation privacy time;privacy;problem proposing privacy;probability privacy breach;contracts electric utilities;electric utilities consumers;utilities consumers goal;utilities consumers;privacy time;utility contracts;fair electric utility;privacy time benefits;privacy breach;sharing consumption data;utility needs data;sharing consumption;operation utility contracts", "pdf_keywords": "privacy contracts electric;proposing privacy contracts;privacy contracts;privacy service contract;privacy costs operation;valuations privacy costs;probability privacy;privacy costs;valuations privacy;depending probability privacy;different valuations privacy;proposing privacy;privacy breach mathematical;considering privacy service;problem proposing privacy;privacy service;contracts electric utilities;privacy;probability privacy breach;contract electric utility;considering privacy;electric utilities consumers;framework considering privacy;utilities consumers;utilities consumers goal;service contract electric;electric utility consumer;privacy breach;service contracts;contracts electric"}, "ba00cbd314dc52b299a8b0c34f1887bcd43cdc12": {"ta_keywords": "using synonymy dictionaries;graph synonyms extracted;synonymy dictionaries;synsets graph synonyms;synonymy dictionaries word;synonyms extracted;synsets using synonymy;synonyms extracted commonly;dictionaries word embeddings;word sense induction;weighted graph synonyms;lexical resources;lexical resources watset;constructed lexical resources;synonyms;graph synonyms;word embeddings;manually constructed lexical;using synonymy;word embeddings build;lexical;apply word sense;dictionaries word;available resources wiktionary;resources wiktionary;automatic induction synsets;constructed lexical;dictionaries;synonymy;datasets english russian", "pdf_keywords": "synsets graph synonyms;synsets using synonymy;lexical resources watset;dictionaries word embeddings;manually constructed lexical;synonymy dictionaries;using synonymy dictionaries;constructed lexical resources;word embeddings;synonymy dictionaries word;lexical resources;analysis lexical resources;automatic induction synsets;datasets english russian;graph synonyms;graph synonyms dmitry;lexical resources available;synonyms;analysis lexical;lexical;constructed lexical;word embeddings despite;compared wordnet terms;english datasets;datasets english;comparative analysis lexical;induction synsets graph;score english datasets;compared wordnet;wordnet terms"}, "48220433a2fb07761b26b2d6aa59b615289a3d4c": {"ta_keywords": "fooling graph neural;node adversarial;single node adversarial;adversarial example node;node adversarial example;attack fooling graph;attacker force gnn;gnn types graphsage;adversarial;gnns vulnerable;adversarial example;fooling graph;gnns vulnerable extremely;graph neural networks;networks gnns;graphsage gcn;graph neural;attacker node single;attacker node;single node attack;node attack fooling;specific attacker node;node attack;networks gnns shown;paper gnns vulnerable;variety domains attack;force gnn;graphsage gcn gat;gnn classify;neural networks gnns", "pdf_keywords": "single attacker node;attacker node adversary;multiple attacker nodes;choose attacker node;attacker nodes;node adversary;attacker node attack;attacker node;attacker node perturb;speci\ufb01c attacker node;attacker node example;attacker choice adversary;label attacker choice;single independent attacker;attacker nodes table;approaches choosing attacker;node adversary allowed;attacker choice;effective multiple attacker;choosing attacker;attacker force gnn;node attack effective;node attack;single attacker;pick speci\ufb01c attacker;scenario single attacker;independent attacker effective;choosing attacker white;choose attacker;attacker effective multiple"}, "25c50ef5a902586a06099ceb29e7f34e2172020a": {"ta_keywords": "neural networks wireless;wireless networks techniques;wireless networks;networks wireless networks;networks wireless;networks techniques applications;networks techniques;neural networks;wireless;networks;neural;techniques applications guidelines;applications guidelines;guidelines;techniques applications;techniques;applications", "pdf_keywords": ""}, "e79bd5d5ad084009233c8524b02ac887029c5fe2": {"ta_keywords": "variable oil damper;damper using design;oil damper using;oil damper;damper using;structure variable oil;35 design base;design base isolated;damper;isolated structure variable;variable oil;base isolated structure;35 design;design optimization method;base isolated;design optimization;using design optimization;isolated structure;structure variable;design base;optimization method;base;35;using design;isolated;optimization;design;oil;structure;variable", "pdf_keywords": ""}, "f1b52bf723d7f5c4b68c8551c4d168ed1224f016": {"ta_keywords": "mitochondrial genome sinocyclocheilus;genome sinocyclocheilus wenshanensis;mitochondrial genome mitogenome;sinocyclocheilus wenshanensis cypriniformes;wenshanensis cyprinid fish;wenshanensis cypriniformes cyprinidae;sinocyclocheilus wenshanensis cyprinid;genome mitogenome wenshanensis;cyprinidae phylogenetic analysis;mitochondrial genome;cyprinidae phylogenetic;sinocyclocheilus wenshanensis;fishes showed wenshanensis;cypriniformes cyprinidae phylogenetic;wenshanensis cypriniformes;mitogenome wenshanensis;abstract sinocyclocheilus wenshanensis;complete mitochondrial genome;mitogenome wenshanensis generation;wenshanensis cyprinid;mitogenome 26 cyprinidae;cyprinid fish species;cyprinidae fishes;wenshanensis generation sequencing;sinocyclocheilus fishes;genome sinocyclocheilus;reported sinocyclocheilus fishes;wenshanensis closely related;cyprinidae fishes showed;26 cyprinidae fishes", "pdf_keywords": ""}, "c14fb834ac6ede13f94f71cfaf5649b55e70a2c2": {"ta_keywords": "resulting data market;data market;quality data purchase;data purchase effort;data cheaply reproduced;data played increasingly;riding data aggregators;data aggregators;share data aggregator;data purchasers sellers;data aggregators directly;data aggregator;data purchasers;data cheaply;equilibria resulting data;mechanisms ensure data;data purchase;socially inefficient data;multiple data purchasers;quality data;single data aggregator;data sources share;sources share data;quality data single;data played;inefficient data cheaply;data sources;data market recent;settings data aggregators;ensure data sources", "pdf_keywords": "equilibria data market;data aggregator incentivizes;aggregator incentivizes data;incentivizes data;resulting data market;incentivizes data source;buyers data aggregator;data buyers data;coupling data buyers;data market;data data aggregators;data aggregators;nash equilibria data;data aggregators bene\ufb01t;data purchasers sellers;data buyers;equilibria data;data market identify;data aggregator;data market analysis;buyers data;data aggregator \ufb01nd;equilibria resulting data;data purchasers;single data aggregator;multiple data purchasers;second data aggregator;aggregator incentivizes;quality data data;data aggregator enters"}, "7c085d7f50a76cf1a09a114986206256e0ee1931": {"ta_keywords": "\u70b9\u4e88\u6e2c\u3068\u7cfb\u5217\u4e88\u6e2c\u306e2\u6bb5\u968e\u5316\u306b\u3088\u308b\u54c1\u8a5e\u63a8\u5b9a\u306e\u7cbe\u5ea6\u5411\u4e0a \u60c5\u5831\u57fa\u790e\u3068\u30a2\u30af\u30bb\u30b9\u6280\u8853 ifat;\u60c5\u5831\u57fa\u790e\u3068\u30a2\u30af\u30bb\u30b9\u6280\u8853 ifat;\u60c5\u5831\u57fa\u790e\u3068\u30a2\u30af\u30bb\u30b9\u6280\u8853 ifat vol;\u70b9\u4e88\u6e2c\u3068\u7cfb\u5217\u4e88\u6e2c\u306e2\u6bb5\u968e\u5316\u306b\u3088\u308b\u54c1\u8a5e\u63a8\u5b9a\u306e\u7cbe\u5ea6\u5411\u4e0a \u60c5\u5831\u57fa\u790e\u3068\u30a2\u30af\u30bb\u30b9\u6280\u8853;\u70b9\u4e88\u6e2c\u3068\u7cfb\u5217\u4e88\u6e2c\u306e2\u6bb5\u968e\u5316\u306b\u3088\u308b\u54c1\u8a5e\u63a8\u5b9a\u306e\u7cbe\u5ea6\u5411\u4e0a;\u60c5\u5831\u57fa\u790e\u3068\u30a2\u30af\u30bb\u30b9\u6280\u8853;ifat 101;2011 ifat 101;ifat;2011 ifat;ifat vol 2011;ifat vol;vol 2011 ifat;101;vol 2011;2011;vol", "pdf_keywords": ""}, "293ed3367027c99a81ead6ff3f31be7de43bce9c": {"ta_keywords": "strategyproof peer selection;peer selection using;peer selection;strategyproof peer;randomization partitioning apportionment;selection using randomization;randomization partitioning;using randomization partitioning;randomization;peer;using randomization;selection;strategyproof;partitioning apportionment;partitioning;selection using;apportionment;using", "pdf_keywords": "strategyproof peer selection;strategyproof peer;2019 strategyproof peer;randomization partitioning apportionment;randomization partitioning;peer selection;using randomization partitioning;allocation;peer selection using;new allocation mechanism;allocation mechanism;dividing continuous resource;ideas partition mechanism;new allocation;randomization;science new strategyproof;fraction allocation;strategyproof mechanism;turning fraction allocation;selection using randomization;partition mechanism literature;continuous resource 18;combined new allocation;apr 2019 strategyproof;using randomization;resource 18;new strategyproof mechanism;new strategyproof;strategyproof mechanism incorporates;strategyproof"}, "adeed0816a2cab763e3bee769957ff1849985759": {"ta_keywords": "normalization historical texts;normalization variant word;automatic normalization historical;normalization historical;automatic normalization;texts using distance;normalization variant;compares approaches normalization;forms modern spellings;semi automatic normalization;approaches normalization;modern spellings;texts semi automatic;modern spellings greatly;historical texts using;approaches normalization focus;normalization;variant word forms;word forms modern;historical texts typically;texts using;string distance measures;based string distance;types historical texts;word forms;normalization focus methods;tagging lemmatization paper;historical texts;tagging lemmatization;normalization focus", "pdf_keywords": ""}, "bd8334c1246adbd47f80eea60249c30a74925d7a": {"ta_keywords": "emergency wireless networks;learning algorithms deployment;asynchronous stochastic approximation;deployment emergency wireless;algorithms deployment wireless;stochastic approximation based;stochastic approximation;learning algorithms asymptotically;wireless networks;stochastic approximation single;deployment wireless relay;optimal policies deployment;wireless relay networks;deployment wireless;fast deployment emergency;emergency wireless;wireless networks human;relay networks;wireless networks motivated;asynchronous stochastic;deployment agent explores;learning algorithms;theory asynchronous stochastic;approach deployment agent;agents robots unmanned;stochastic;deployment agent forward;networks human agents;vehicles uavs agent;wireless relay", "pdf_keywords": "emergency wireless networks;learning algorithms deployment;deployment emergency wireless;asynchronous stochastic approximation;algorithms deployment wireless;stochastic approximation based;wireless networks;stochastic approximation;deployment wireless relay;learning algorithms asymptotically;deployment wireless;wireless relay networks;emergency wireless;stochastic approximation single;wireless networks propose;wireless networks human;optimal policies deployment;learning algorithms;based asynchronous stochastic;fast deployment emergency;relay networks;stochastic;asynchronous stochastic;convergence optimal policy;develop learning algorithms;propose learning algorithms;networks propose learning;wireless relay;agents robots unmanned;stochastic approximation prove"}, "7e0eb21f4903c2fe860d1c4f213879e99d7cd23c": {"ta_keywords": "electrolaryngeal speech enhancement;approach electrolaryngeal speech;speech enhancement;speech enhancement based;electrolaryngeal speech;electrolaryngeal el speech;sounds enable laryngectomees;enable laryngectomees produce;voice conversion;laryngectomees produce el;el speech minimizing;enable laryngectomees;voice conversion method;speech minimizing degradation;parameters voice conversion;parameters proficient laryngectomees;produce el speech;listenability electrolarynx device;generates excitation sounds;degradation listenability electrolarynx;listenability electrolarynx;improving naturalness electrolaryngeal;excitation sounds enable;electrolarynx device artificially;proficient laryngectomees produce;el speech sounds;laryngectomees produce;parameters voice;spectral parameters voice;noise reduction", "pdf_keywords": ""}, "1d56a0b8fb560a79ca28b44bfd6f1e645a36549a": {"ta_keywords": "measurement function thermocouple;temperature collection thermocouple;function thermocouple heating;thermocouple accurate research;function thermocouple;thermal control device;thermocouple judge temperature;thermocouple accurate;collection thermocouple accurate;thermocouple heating belt;thermocouple heating;signals thermocouple;based thermal control;voltage signals thermocouple;collection thermocouple;automatic resistance measurement;thermal control;signals thermocouple judge;automatic measurement resistance;thermocouple;value based thermal;temperature collection;thermocouple judge;temperature collect;interface room temperature;temperature;resistance measurement;judge temperature;based thermal;measurement resistance value", "pdf_keywords": ""}, "ead3182dd47bdd8da98476cca1cfe0373dfc2edc": {"ta_keywords": "clustering vbec speech;clustering triphone hmm;tree clustering triphone;clustering triphone;model speech recognition;vbec speech recognition;acoustic model speech;estimation clustering vbec;speech recognition gmm;decision tree clustering;clustering based gaussian;gaussian mixture model;model speech;bayesian estimation clustering;speech recognition;based gaussian mixture;triphone hmm states;monophone hidden markov;vbec speech;clustering vbec;finding appropriate acoustic;acoustic model topology;complicated acoustic model;speech recognition using;acoustic model;gaussian mixture;hidden markov model;tree clustering based;triphone hmm;estimation clustering", "pdf_keywords": ""}, "b593be8ff3c09c6994657678fcde0c5adf43328e": {"ta_keywords": "unsupervised syntactic parsing;improves constituency parsing;parse tree annotation;constituency parsing;treebank;syntactic parsing;constituency parsing english;wsj penn treebank;syntactic parsing nonetheless;annotation span constraints;penn treebank;parsing;tree annotation span;treebank f1;parse tree;parsing english wsj;parsing english;unsupervised syntactic;approach unsupervised syntactic;constraints compared parse;induction distant supervision;structural annotation text;supervision span constraints;compared parse tree;annotation span;structural annotation;parse;penn treebank f1;parsing nonetheless;tree annotation", "pdf_keywords": "supervised constituency parsing;supervised unsupervised parsing;improves constituency parsing;unsupervised parsing;constituency parsing;effective improving parsing;improving parsing;parsing distant supervision;improving parsing performance;unsupervised parsing distant;treebank;constituency parsing english;penn treebank;parsing;parsing performance diora;wsj penn treebank;treebank f1;constituency parsing stern;parsing english;penn treebank f1;parsing performance;leverages structured svm;parsing english wsj;treebank f1 niculae;supervision span constraints;parsing distant;structured svm;supervised constituency;used supervised constituency;diora leverages structured"}, "07a5536c0570804f816fdb5a0a5ae890630e61bd": {"ta_keywords": "electrolaryngeal speech enhancement;el speech enhancement;speech enhancement;speech enhancement based;speech enhancement method;speech enhancement noise;approach electrolaryngeal speech;el speech noise;electrolaryngeal el speech;statistical voice conversion;voice conversion;adding el speech;electrolaryngeal speech;parameters el speech;produce el speech;voice conversion method;sounds enable laryngectomees;voices converted acoustic;reduction statistical voice;enhancement noise reduction;laryngectomees produce el;el speech using;speech noise;el speech hybrid;parameters statistical voice;enable laryngectomees produce;enhancement based noise;enhancement noise;acoustic parameters el;enable laryngectomees", "pdf_keywords": ""}, "d14afc470cd90521147130e153c0d3e1324cd104": {"ta_keywords": "machine translation learns;learns translate language;translation learns;language representations typology;nlp neural models;translation learns translate;mystery neural nlp;typological databases learning;neural nlp;infer syntactic phonological;learns translate;neural machine translation;neural models know;neural nlp neural;learning language representations;nlp neural;language representations;language does learn;databases learning language;syntactic phonological;learning language;able infer syntactic;syntactic phonological phonetic;languages geographic phylogenetic;english use predict;representations typology prediction;infer syntactic;learn syntax semantics;languages;typological databases", "pdf_keywords": "multi lingual neural;inferring linguistic typology;training multilingual neural;learning language vector;multilingual neural;multilingual neural language;typological features language;neural language models;inferring linguistic;linguistic typology parallel;lingual neural machine;infer syntactic phonological;language models;infer typological features;language training multilingual;networks learn linguistic;massively multi lingual;features language experiments;vectors typology prediction;language aggregating representations;training multilingual;language models stling;lingual neural;specifying language training;neural language;language vector;phonological classes features;languages geographic phylogenetic;linguistic typology;representations infer typological"}, "6aac35ec3bfaf7e835ac633414419c9623838007": {"ta_keywords": "speech features;dimensional speech features;auditory features based;cochleograms dimensional speech;speech features derived;phoneme sequence recognition;use auditory features;cochleogram spectrogram features;cochleogram spectrogram feature;auditory features;spectrogram features best;features based cochleograms;spectrogram features using;cochleogram features;dimensional cochleogram spectrogram;spectrogram feature combination;spectrogram features;neural network dnn;cochleogram spectrogram;combine cochleogram features;dnn single features;cochleogram features log;dimensional speech;timit phoneme sequence;spectrogram feature;use auditory;features using cnn;deep neural network;reveal cochleogram spectrogram;auditory", "pdf_keywords": ""}, "9895531c6dc3854f082de1a1ec651a9e179bbd07": {"ta_keywords": "tonal transcription language;context tonal prediction;tonal languages yongning;highlighting phonetic phonemic;modelling phonemes tones;phonetic phonemic facts;phonemic tonal transcription;phonemic context tonal;phonetic phonemic;jointly modelling phonemes;phonemes tones;modelling phonemes;tonal languages;phonemes tones versus;tonal prediction;experiments tonal languages;phonemic facts linguistic;signal highlighting phonetic;highlighting phonetic;tonal transcription;transcription language;transcription language documentation;temporal classification loss;connectionist temporal classification;phonetic;phonemic tonal;function phonemic tonal;context tonal;phonemes;importance pitch information", "pdf_keywords": ""}, "8d1fd086a76d30343d2224b61cb7ddab2125d0b2": {"ta_keywords": "symbol level learning;level learning programs;level learning sll;learning programs;learning programs uses;level learning;sll learning;level learning use;free learning theory;learning improve performance;free learning;approach symbol level;sll learning improve;machine learning;learning theory analyze;learning sll learning;symbol level;learning theory;distribution free learning;problem symbol level;research machine learning;learning sll;machine learning focused;solution path caching;learning;path caching mechanisms;learning use sort;path caching;program given examples;improve performance program", "pdf_keywords": ""}, "3256198d819f23f82640490b9160e85139627d6c": {"ta_keywords": "crossing representation wavelet;wavelet transform;signal reconstruction;representation wavelet transform;feature signal reconstruction;algorithm signal reconstruction;signal reconstruction new;signal reconstruction problem;wavelet transform domain;representation wavelet;zero crossing representation;wavelet;stabilized zero crossing;zero crossing;operation based edge;iterative algorithm signal;based edge intensity;edge intensity reduce;minimum norm optimization;reconstruction problem reduces;edge intensity;algorithm signal;image processing;norm optimization;reconstruction problem;threshold operation based;crossing representation experimentally;image processing furthermore;demonstrate threshold operation;norm optimization problem", "pdf_keywords": ""}, "f9a86c2df17f408105c2d3e9429410cdc376c6f0": {"ta_keywords": "semantic spaces distributional;distributional semantics;distributional lexical semantics;distributional semantics including;distributional methods nlp;semantic space models;semantic spaces models;semantic space;distributional lexical;dedicated distributional lexical;semantic spaces;space models lexical;art distributional semantics;research semantic spaces;rich lexical semantic;lexical semantics;lexical semantic;applications semantic space;spaces distributional methods;spaces distributional;semantic;models rich lexical;models lexical;adoption semantic spaces;distributional;models lexical acquisition;methods nlp;methods nlp pushing;semantics;lexical semantic resources", "pdf_keywords": ""}, "66713fbcb8d5e48a9eb6425bd7fdbb53751e60b1": {"ta_keywords": "vectorization better compression;faster decoding;novel vectorized scheme;decoding billions integers;integers second vectorization;vectorized scheme simd;faster decoding time;compression propose new;simd fastpfor compression;compression decompression decoding;new vectorized scheme;previously proposed vectorized;compression propose;better compression propose;novel vectorized;times faster decoding;decompression decoding billions;vectorization better;introduce novel vectorized;vectorized scheme;fastest schemes desktop;saves bits int;propose new vectorized;better compression;new vectorized;second vectorization better;decoding billions;saves bits;decompression decoding;vectorized approaches nearly", "pdf_keywords": "coding decoding speed;compression techniques 32;encoding speed;fastest decoding algorithms;speeds fastest decoding;fastest decoding;compression propose new;compression ratios encoding;decoding speed binary;faster decoding;compression propose;better compression propose;compression ratio coding;simd fastpfor compression;encoding speed results;compression techniques;ratios encoding speed;decoding speed;fastpfor better compression;better compression;coding decoding;times faster decoding;fastpfor compression;decoding algorithms reported;speed binary packing;ratio coding decoding;techniques 32 bit;compression;offering competitive compression;speed binary"}, "3a2446c47000c3d0681b2cdf6d8b87a11ff630e2": {"ta_keywords": "insulation board installation;insulation board mounting;wall insulation boards;wall insulation board;insulation boards designed;installation results insulation;insulation board;exterior wall insulation;installation engineering exterior;insulation boards;board installation engineers;results insulation board;wall insulation;engineering exterior wall;workers installation engineering;installation engineers;installation engineering;building exterior wall;artificial installation results;installation device building;design installation device;construction workers installation;artificial installation;mounting device building;device building exterior;insulation board finally;compare artificial installation;engineering exterior;results insulation;board mounting device", "pdf_keywords": ""}, "5ce3148ed36a1ea034da2c05b8cde9efbaf43e6a": {"ta_keywords": "field speech recognition;speech recognition recently;speech recognition;deep beamforming bf;far field speech;speech noise scm;beamforming networks;meeting transcription;beamforming networks using;field speech;meeting transcription task;deep beamforming;beamforming bf network;transcription task shows;bands beamforming networks;deep learning framework;beamforming bf;ami meeting transcription;speech noise;recently deep beamforming;deep learning;transcription task;bf requires speech;requires speech noise;predict bf weights;transcription;features far field;beamforming;bf weights phase;frequency bands beamforming", "pdf_keywords": ""}, "981dbdf6f87f13f3f3047a925c519fc39a35202b": {"ta_keywords": "neural probabilistic language;probabilistic language model;word embeddings;concatenates word embeddings;language modeling;probabilistic language;language modeling driven;word level language;language model benchmarks;language model;word embeddings fixed;predict word;revisit neural probabilistic;network predict word;level language model;neural probabilistic;progress language modeling;advances neural architectures;predict word scaled;short input contexts;language model nplm;neural architectures;neural architectures hardware;embeddings;neural;level language;advances neural;word level;recent progress language;expected word level", "pdf_keywords": "neural probabilistic language;level language modeling;wordlevel language modeling;language modeling;language modeling lens;language model;attention layer;probabilistic language model;simplest neural architecture;language modeling datasets;neural architecture;self attention layer;proposed language modeling;neural architecture proposed;language model nplm;word level language;classic language model;advances neural architecture;neural architecture design;layer self attention;probabilistic language;wordlevel language;attention \ufb01rst layer;transformer word level;language model paper;perplexity word level;architecture proposed language;revisit neural probabilistic;neural probabilistic;simplest neural"}, "d92e0443768ec3715205cb232ef1a1917372b0af": {"ta_keywords": "speech processing toolkit;end speech processing;speech processing asr;nlp tasks espnet;automatic speech;asr text speech;automatic speech processing;downstream natural language;speech processing tasks;toolkit espnet widely;speech processing;text speech;asr nlu models;processing toolkit espnet;various speech processing;spoken language understanding;speech translation;text speech tts;tts speech translation;toolkit espnet;speech tts;tasks espnet slu;language understanding slu;speech translation st;espnet slu project;processing nlp tasks;tasks espnet;nlp tasks;nlu models;speech tts speech", "pdf_keywords": "speech processing toolkit;source speech processing;asr nlu models;speech processing tasks;speech processing;various slu benchmarks;slu benchmarks;various speech processing;e2e slu toolkit;infrastructure various speech;open source speech;nlu models;slu toolkit;processing toolkit espnet;implementations various slu;slu benchmarks enable;popular espnet toolkit;different asr nlu;slu toolkit built;espnet toolkit;slu toolkit objective;toolkit espnet 14;toolkit espnet;espnet slu new;nlu models release;new e2e slu;source speech;source e2e slu;benchmarks;evaluation enhance toolkit"}, "04e3a3ee41c1ee977e023052435bbb5f4c680f66": {"ta_keywords": "freshippo stores nanjing;new retail stores;stores nanjing city;study freshippo stores;stores nanjing;oriented new retail;freshippo stores;retail stores;stores case study;new retail;retail stores case;location choice;nanjing city;location choice optimization;retail;nanjing;stores;case study freshippo;freshippo;city;study freshippo;choice optimization development;location;development community oriented;development community;community oriented new;stores case;optimization development community;community oriented;choice optimization", "pdf_keywords": ""}, "eba4c6b0b860a34461ffb8544111c89a3ef0d8b7": {"ta_keywords": "noisy pairwise comparisons;ranking problem;ranking;social choice crowdsourcing;analysis ranking problem;pairwise comparisons;pairwise comparisons pair;search social choice;algorithm problem competitive;comparison noise constrained;analysis ranking;algorithms problem competitive;given pairwise comparisons;comparisons pair items;competitive ratio algorithms;items noisy pairwise;recommender systems web;items comparison noise;web search social;pairwise comparisons setting;competitive analysis ranking;choice crowdsourcing;noisy pairwise;known algorithms problem;strong stochastic transitivity;search social;comparison noise;comparisons pair;recommender systems;known algorithms", "pdf_keywords": "noisy pairwise comparisons;ranking problem;ranking;analysis ranking problem;ranking problem xi;algorithms problem competitive;pairwise comparisons;comparison noise constrained;pairwise comparisons pair;competitive analysis ranking;analysis ranking;comparisons pair items;items noisy pairwise;given pairwise comparisons;items comparison noise;strong stochastic transitivity;known algorithms problem;stochastic transitivity sst;social choice crowdsourcing;search social choice;noisy pairwise;stochastic transitivity;pairwise comparisons solve;comparison noise;comparisons pair;best possible algorithm;known algorithms;problem competitive ratios;pair items comparison;competitive ratios"}, "9d628e420922cc23a8944de1511ca5d3309f5d58": {"ta_keywords": "leveraging transcripts;marking noteworthy utterances;leveraging transcripts train;noteworthy utterances support;generates summary sentences;extracts noteworthy utterances;pipelines leveraging transcripts;noteworthy utterances;noteworthy utterances multi;soap notes annotations;noteworthy utterances section;clusters noteworthy utterances;utterances support summary;notes annotations;annotations marking noteworthy;note conversation approach;soap note conversation;soap notes;improvements extraction;soap sentence generated;summary sentences;model generate notes;improvements extraction provide;records consisting transcripts;consisting transcripts;sentence generated oracle;annotations;generates soap note;sentence best performing;note conversation", "pdf_keywords": ""}, "40ba59c9945e7c19d06dadfa8f496da5810ee30d": {"ta_keywords": "speech faster beam;vectorization hypotheses speech;faster beam search;hypotheses speech faster;batch multiple speech;speech faster;beam search encoder;traverse multiple utterances;speech recognition;utterances line recognition;beam search algorithm;beam search;beam search accelerates;decoder based speech;right beam search;multiple speech;multiple speech utterances;vectorizing multiple hypotheses;attention based encoder;original beam search;batch shallow fusion;rnnlm ctc batch;search process vectorizing;multiple utterances;hypotheses speech;technique beam search;speech utterances;search encoder decoder;search encoder;faster beam", "pdf_keywords": "corpus achieved speedup;core vectorization utterance;vectorization utterances;vectorization utterances hypotheses;vectorization utterance;utterances achieved speed;vectorization utterance hypothesis;librispeech corpus speed;vectorization multiple utterances;corpus speed;speed csj corpus;utterances batch enables;corpus speed csj;utterances batch;speed librispeech corpus;network utterances batch;hypothesis speech vectorization;speech vectorization;rnnlm execution cpu;cpu core vectorization;speech vectorization method;corpus achieved;rnnlm ctc computation;execution vectorization;execution attention decoder;encoder network utterances;multiple utterances achieved;execution vectorization multiple;network utterances;utterances achieved"}, "ccd33442fef058c7c0eafc57d2c6e6a4cde10a3b": {"ta_keywords": "3d models proteins;graphs protein model;convolutions molecular graphs;spherical graph convolutional;models proteins represented;graphs framework protein;graphs protein;molecular graphs protein;protein model quality;models proteins;represented molecular graphs;molecular graphs framework;spherical convolutions molecular;spherical graph;molecular graphs;protein model;propose spherical graph;structure prediction;framework protein model;graph convolutional network;proteins represented molecular;proteins represented;geometric learning;structure prediction casp;graph convolutional;convolutions molecular;spherical convolutions;approach spherical convolutions;proteins;convolutional network gcn", "pdf_keywords": "represented molecular graphs;vorocnn graph convolutional;3d models proteins;spherical graph convolutional;graph convolutional network;molecular graphs;molecular graph incorporate;structure molecular graph;molecular graph using;molecular graph;tessellation proq3 neural;vorocnn graph;models proteins represented;level molecular graph;convolutional network gcn;molecular graphs section;\ufb01nally vorocnn graph;3d structure molecular;graph convolutional;proteins represented molecular;convolutional network built;models proteins;represented molecular;spherical architectures gcn;protein model quality;convolutional network;graph using convolution;spherical graph;protein model;proteins represented"}, "5dd34d2781ca702d0e3cd1224517ff60d6c3e2ee": {"ta_keywords": "decipherment informal romanization;informal romanization idiosyncratic;informal romanization;character mappings substantially;phonetic visual priors;phonetic visual similarity;observed romanized text;romanization idiosyncratic process;latin script observed;character mappings;script observed romanized;latin character sets;priors character mappings;romanization idiosyncratic;latin script languages;romanization;romanized text unsupervised;script languages latin;encode non latin;visual priors character;unsupervised fashion phonetic;bias phonetic visual;non latin script;languages latin character;phonetic visual;languages character pairs;associated phonetic visual;fashion phonetic visual;languages character;latin script", "pdf_keywords": "decode informal romanization;informal romanization parallel;relying transliterated data;learns decode informal;romanized data languages;transliterated data language;informal romanization;text relying transliterated;relying transliterated;romanization parallel text;romanization;transliterated data;observed romanized text;collecting dataset romanized;romanization parallel;transliterated;learns decode;dataset romanized russian;romanized text unsupervised;romanized data;directly romanized data;decode romanized;language processing;script observed romanized;model decode romanized;romanized text;new dataset romanized;romanized russian collected;unsupervised models arabic;dataset romanized"}, "054ba27fe5cc6085d20ea2707de886db6865dbed": {"ta_keywords": "experts generalize pagerank;pagerank;generalize pagerank;generalize pagerank measure;edges relational retrieval;pagerank measure;relational retrieval;labeled edges relational;pagerank measure popular;proximity queries graph;relational retrieval using;learnable proximity measure;retrieval named entity;entity recognition;named entity recognition;ad hoc retrieval;typed proximity queries;entity recognition ner;edges relational;learnable proximity;proximity queries;label sequence proximity;experts allow rankings;popular entity experts;entity experts;measure popular entity;novel learnable proximity;sequence labeled edges;popular proximity measure;hoc retrieval named", "pdf_keywords": ""}, "7621bfe36cc649a5876cea587366201e158a8b38": {"ta_keywords": "annotations domain adaptation;semantic role labeling;sequence labeling neural;domain adaptation;domain adaptation remains;domain adaptation srl;domain adaptation biological;approaches domain adaptation;resource corpus biological;role labeling srl;role labeling;labeling neural;architecture domain adaptation;labeling neural network;adaptation biological domain;sequence labeling;labeling srl systems;richer annotations domain;training lstm crf;low resource corpus;datasets adapting;semantic role;annotations domain;layer sequence labeling;use semantic role;resource corpus;biological domain involves;datasets adapting low;lstm crf based;biological domain", "pdf_keywords": ""}, "c1a4c5380d90dc77064de6003cfb9611ad218600": {"ta_keywords": "neural dialogue generation;dialogue response generation;dialogue generation;dialogue generator;generating dialogue;sentiment controlled dialogue;response generating dialogue;dialogue generator assisted;neural dialogue response;controlled dialogue generator;controlled neural dialogue;generating dialogue history;neural dialogue;dialogue response;training sentiment;seq2seq model adversarial;response generation;training sentiment controlled;method neural dialogue;generator assisted adversarial;learning training sentiment;sentiment controlled neural;controlled dialogue;conditional adversarial;response generation allows;conditional variational autoencoder;dialogue;variational autoencoder;responses according dialogue;conditional adversarial learning", "pdf_keywords": "sentimentcontrolled dialogue generation;dialogue response generation;neural dialogue generation;sentiment controlled dialogue;dialogue generation;dialogue generator;generating dialogue;sentimentcontrolled dialogue;response generating dialogue;neural dialogue response;sentiment label adversarial;dialogue generator assisted;dialogue generation \ufb02exibility;conditional adversarial;controlled neural dialogue;generating dialogue history;controlled dialogue responses;controlled dialogue generator;training sentiment;dialogue responses;neural dialogue;training sentiment controlled;learning training sentiment;conditional adversarial learning;generator assisted adversarial;conditional generative adversarial;method neural dialogue;dialogue response;adversarial discriminator assesses;framework sentimentcontrolled dialogue"}, "9d06638df32f8feefb95ef5a4769adbb1ae6297d": {"ta_keywords": "boosting like rulesets;rule learners ensemble;effective rule learner;rule learner generates;rulesets repeatedly boosting;built rule learners;rule learner;new rule learner;learner generates rulesets;rule learners;learners ensemble rules;rule builder;generates rulesets;greedy rule builder;rule builder possible;ensemble rules created;rule builder use;rulesets built rule;ensemble rules;generates rulesets repeatedly;confidence rated boosting;rulesets built;rulesets;boosting simple;fast effective rule;repeatedly boosting simple;built rule;rules created slipper;boosting;boosting simple greedy", "pdf_keywords": ""}, "4c78943e11195fb72a3c878a03b248bc317180e0": {"ta_keywords": "learnable description logics;known description logics;known learnable description;learnable description;learnability restricted;description logics;logics known description;learnability restricted order;instead learnability restricted;description logics called;description logic;simple description logic;learnability order representations;description logics subsets;learnable sublanguage;learnability;description logic summarize;learnability order;languages learnable sublanguage;type languages learnable;analyze learnability;learning concepts expressed;instead learnability;polynomial learnability;learnable sublanguage appears;terminological logics kl;terminological logics;languages learnable;polynomial learnability order;logics called terminological", "pdf_keywords": ""}, "f0baf134f0a2ee6e99f6f2287791109cf93305e7": {"ta_keywords": "subcellular localization text;figure text matching;depicting protein subcellular;images depicting protein;images location proteomics;ocr techniques caption;location proteomics data;recognition ocr;protein subcellular localization;character recognition ocr;depicting protein;localization text images;location proteomics;proteomics data;captions literature;figure text;understanding figure text;text matching;information text images;text images online;protein subcellular;localization type protein;accompanying captions literature;extracting information protein;text images location;text images;protein subcellular patterns;captions literature present;proteomics;information protein subcellular", "pdf_keywords": ""}, "db253b17043b6a86e02173b6aa597664b0c7f256": {"ta_keywords": "visual character embedding;character embedding;embeddings characters based;embeddings characters;creating embeddings characters;words creating character;compositionality effect character;visual character;characters based visual;modeled compositionality words;character embedding previous;compositionality words creating;produce visual character;character running convolutional;character level models;creating embeddings;creating image character;image character;characters based;characters languages;compositionality words;creating character;embeddings;rare characters languages;characters;characters languages chinese;instances rare characters;embedding;effect creating embeddings;words writing systems", "pdf_keywords": "visual embeddings characters;characters using cnn;characters convolutional neural;learn representation encoding;learns visual embeddings;modeling compositionality characters;learn embeddings;learn embeddings compositional;embeddings characters;characters convolutional;visual embeddings;appearance characters convolutional;embeddings characters using;compositionality characters language;learn representation;compositionality characters way;representation image;representation encoding;compositionality characters;embeddings;embeddings compositional;networks learn embeddings;embeddings compositional component;model learns visual;cnn include visual;representation image using;networks recurrent;learns visual;visual semantic;characters language"}, "f516c98c3d2dde5b31931715fbc48bbbc0580e27": {"ta_keywords": "predicting leadership roles;predicting leadership;message email speech;measure predicting leadership;team members textual;email speech;email speech acts;textbased email patterns;workgroups leaders;email patterns;email patterns trac;leadership roles inferred;email messages;messages exchanged team;email collection workgroups;investigate team leadership;based textbased email;leadership positions predicted;members textual patterns;team leadership;team leadership roles;workgroups leaders previously;team members;group leader performance;represented message email;inferred collection email;inferred language usage;automatically inferred language;textbased email;collection email messages", "pdf_keywords": ""}, "553b74de8cb7ebca42a686e2a3a2d6aae170946e": {"ta_keywords": "speech enhancement diffuse;speech enhancement based;based speech enhancement;speech enhancement;study speech enhancement;enhancement based diffusion;clean speech signals;model based speech;quality audio waveform;enhancement diffuse model;speech signals noisy;images raw audio;high quality audio;quality audio;raw audio waveforms;based diffusion probabilistic;audio waveform generation;recover clean speech;diffusion probabilistic models;audio waveform;propose diffusion probabilistic;diffusion probabilistic model;enhancement diffuse;waveforms paired diffusion;diffusion probabilistic;enhancement based;audio waveforms;speech signals;clean signals based;diffwave high quality", "pdf_keywords": "speech enhancement diffuse;based speech enhancement;speech enhancement based;speech enhancement;study speech enhancement;enhancement based diffusion;clean speech signals;model based speech;diffwave highquality audio;enhancement diffuse model;audio generative models;speech signals noisy;audio generative;audio waveform generation;audio waveform;enhancement diffuse;speech signals;based diffusion probabilistic;images raw audio;raw audio waveforms;enhancement based;models standardized voice;related audio generative;recover clean speech;audio waveforms;highquality audio waveform;diffusion probabilistic models;propose diffusion probabilistic;diffusion probabilistic model;waveforms paired diffusion"}, "616cc6826066184a8c77c3f2562e4e891ce42911": {"ta_keywords": "intrinsic fear dqns;deep reinforcement learning;reinforcement learning wild;deep reinforcement;fear dqns solve;reinforcement learning sisyphean;combating reinforcement learning;learn reward shaping;use deep reinforcement;reinforcement learning;combating reinforcement;reward shaping;reinforcement;fear dqns;agent avoid catastrophic;dqns solve toy;reward shaping accelerates;curse intrinsic fear;learn reward;introduce intrinsic fear;intrinsic fear;atari games;improve atari games;intrinsic fear new;environments improve atari;atari games seaquest;shaping accelerates learning;learning wild hope;learning wild;intrinsic fear use", "pdf_keywords": "safe reinforcement learning;safety reinforcement learning;fear learned reward;safe reinforcement;intrinsic fear learned;learning avoid catastrophic;reinforcement learning;notions safety reinforcement;avoiding catastrophic states;learned reward shaping;algorithms avoiding catastrophic;reinforcement;intrinsic fear model;avoid catastrophic states;learned reward;fear learned;supervised fear model;safety reinforcement;reinforcement learning literature;reinforcement learning following;supervised fear;fear model;introduce intrinsic fear;train supervised fear;reward shaping;intrinsic fear;treatment safe reinforcement;fear model suggest;avoiding catastrophic;fear model predicts"}, "7e82015c386726f4b8f6f686b6e6bb7d1e7564bb": {"ta_keywords": "controversy detection;controversy detection existing;identifying controversial posts;level controversy detection;comment graph convolutional;controversial posts social;controversial posts;comment graph;mining public sentiment;content related posts;generalizability identifying controversial;post comment graph;identifying controversial;related posts;topics posts comments;topic unrelated features;handle posts topics;structure content topics;content related;topics posts;content topics posts;posts topics;topic post comment;semantic information content;content topics;sentiment assessing;post level controversy;reply relationship modeling;posts topics dissimilar;posts social media", "pdf_keywords": "controversy detection social;controversy detection;controversy detection improve;level controversy detection;network controversy detection;controversy detection conclusion;identifying controversial posts;controversy detection lei;comment graph convolutional;controversial posts social;topicpost comment graph;comment graph;abstract identifying controversial;inter topic detection;mining public sentiment;controversial posts;identifying controversial;topic detection;topic detection propose;sentiment assessing in\ufb02uence;detection social media;sentiment assessing;public sentiment assessing;graph structure content;information graph convolutional;social media fundamental;graph convolutional network;posts social media;semantic structural information;controversy"}, "b8cabd2f7fbf816d667701c5d756b5fcb982e6fe": {"ta_keywords": "player non convex;separation gradient descent;gradient descent ascent;timescale separation gradient;maximizing player approximately;players sharing learning;gamma_1 gradient descent;learning rate player;tau gradient descent;sum games learning;tau maximizing player;timescale separation training;maximizing player;learning rate tau;rate tau maximizing;gamma_1 learning rate;local minmax equilibria;descent ascent provably;games learning rate;sharing learning rate;minmax equilibria finite;stochastic gradient feedback;player approximately converging;gamma_1 learning;tau maximizing;gradient descent;local minmax;finite timescale separation;ascent provably converges;descent ascent", "pdf_keywords": "player non convex;regularized learning dynamics;introduction gradient penalty;gradient descent ascent;local minmax equilibria;gradient penalty based;gradient penalty;adversarial network formulation;learning dynamics;\u03b31 learning;learning rate player;minmax equilibria finite;minmax equilibria;learning rates \u03b31;learning dynamics remain;rate gradient descent;gradient descent;wasserstein cost function;strict local minmax;convergence rate gradient;limiting regularized learning;local minmax;descent ascent provably;parameter learning dynamics;penalty based regularization;\u03b31 learning rate;descent ascent player;stochastic gradient feedback;ascent provably converges;non convex"}, "c5a323f8744838093ee36bee3739dea599ce62f0": {"ta_keywords": "29 nirs\u3092\u7528\u3044\u305f\u8133\u6d3b\u52d5\u8a08\u6e2c\u306b\u3088\u308b\u8a8d\u77e5\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306e\u5ba2\u89b3\u7684\u8a55\u4fa1 15;15 29 nirs\u3092\u7528\u3044\u305f\u8133\u6d3b\u52d5\u8a08\u6e2c\u306b\u3088\u308b\u8a8d\u77e5\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306e\u5ba2\u89b3\u7684\u8a55\u4fa1;nirs\u3092\u7528\u3044\u305f\u8133\u6d3b\u52d5\u8a08\u6e2c\u306b\u3088\u308b\u8a8d\u77e5\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306e\u5ba2\u89b3\u7684\u8a55\u4fa1 15 \u6559\u80b2\u5de5\u5b66;29 nirs\u3092\u7528\u3044\u305f\u8133\u6d3b\u52d5\u8a08\u6e2c\u306b\u3088\u308b\u8a8d\u77e5\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306e\u5ba2\u89b3\u7684\u8a55\u4fa1;nirs\u3092\u7528\u3044\u305f\u8133\u6d3b\u52d5\u8a08\u6e2c\u306b\u3088\u308b\u8a8d\u77e5\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306e\u5ba2\u89b3\u7684\u8a55\u4fa1 15;nirs\u3092\u7528\u3044\u305f\u8133\u6d3b\u52d5\u8a08\u6e2c\u306b\u3088\u308b\u8a8d\u77e5\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306e\u5ba2\u89b3\u7684\u8a55\u4fa1;15 \u6559\u80b2\u5de5\u5b66 \u4e00\u822c\u30bb\u30c3\u30b7\u30e7\u30f3;\u6559\u80b2\u5de5\u5b66 \u4e00\u822c\u30bb\u30c3\u30b7\u30e7\u30f3;15 \u6559\u80b2\u5de5\u5b66;\u4e00\u822c\u30bb\u30c3\u30b7\u30e7\u30f3;\u6559\u80b2\u5de5\u5b66;15 29;29;15", "pdf_keywords": ""}, "e82eff0f3e3d150617f9a721f83046940a963c03": {"ta_keywords": "learning algorithm abstract;basic learning loop;learning loop;model pac learnability;learning loop induction;learning based;basic learning;algorithm integrated learning;learning algorithm;learning algorithm described;integrating basic learning;learning based extension;learnability;problem learning control;learning algorithms explicitly;problem learning;perceptron learning algorithm;learning control rules;investigates learning algorithms;concept learners presented;learning standard explanation;learning algorithms;pac learnability;learnability valiant;applied problem learning;explanation based generalization;hypothesis space learning;examples using abstraction;pac learnability valiant;abstraction functions based", "pdf_keywords": ""}, "c8171eaa3a3aac78c3b37351412101bc06e5f359": {"ta_keywords": "languages monolingual annotators;monolingual annotators;hindi comparable corpora;captions images languages;monolingual annotators method;languages getting captions;english hindi comparable;comparable corpora;machine translation;pairs acceptable translations;hindi comparable;evaluations english hindi;tasks machine translation;pairs translations;machine translation dictionary;translations;translation dictionary extraction;comparable corpora created;resource languages monolingual;comparable training data;languages monolingual;low resource languages;translation dictionary;getting captions;source target languages;acceptable translations;getting captions images;target languages;captions images;human evaluations english", "pdf_keywords": "parallel corpus creation;training machine translation;parallel corpus;corpora ways bilingual;parallel corpora;machine translation;machine translation dictionary;translation systems;parallel corpora ways;machine translation systems;replacement parallel corpora;translation dictionary extraction;tasks machine translation;translation pairs;translation systems situations;bilingual speakers source;corpus creation significantly;process parallel corpus;corpus creation;acceptable translation pairs;translation dictionary;corpus;translation pairs apply;ways bilingual speakers;corpora ways;corpora;hindi source language;ways bilingual;language pair hindi;bilingual speakers"}, "51c33a79e05425b6335c8676a166a0f4e178c0a2": {"ta_keywords": "automatic coding;semi automatic coding;coding allows assessment;items skill coding;skill coding;automatic coding line;skill coding allows;text classification;test items skill;coding;novel text classification;coding line test;line test items;text classification approach;test items;skewed data sets;evaluate novel text;assessment;gaps students;related semi automatic;classification;knowledge gaps students;test performance;performance skewed data;gaps students struggling;assessment occur;skewed data;classification approach improving;test;students struggling", "pdf_keywords": ""}, "2c1cb736df7bf526fc3facecd078980e007abceb": {"ta_keywords": "ca2 yeast calmodulin;concentration yeast calmodulin;activity yeast calmodulin;yeast calmodulin vertebrate;ca2 vertebrate calmodulin;yeast calmodulin;calmodulin baker yeast;yeast calmodulin mol;vertebrate calmodulin maximum;vertebrate calmodulin added;calmodulin maximum activity;calmodulin vertebrate calmodulin;ca2 yeast;lower vertebrate calmodulin;vertebrate calmodulin;vertebrate calmodulin mol;calmodulin vertebrate;calmodulin added maximum;number ca2 yeast;calmodulin maximum;maximum activity yeast;higher concentration yeast;affinity ca2 vertebrate;concentration yeast;activity yeast;yeast saccharomyces cerevisiae;saccharomyces cerevisiae;calmodulin added;light chain kinase;baker yeast saccharomyces", "pdf_keywords": ""}, "62d1a3137b01a69443bebf4d92c1990ec512a6a1": {"ta_keywords": "data extraction attack;attack gpt language;evaluate extraction attack;extraction attack;trained private datasets;extracting training data;extraction attack understand;language models trained;training data extracting;adversary perform training;large language models;data large language;data extracting training;trained scrapes public;extract hundreds verbatim;extracting training;training data extraction;extraction attack recover;verbatim text sequences;demonstrate attack gpt;hundreds verbatim text;model trained scrapes;language model trained;training examples querying;private datasets comprehensively;scrapes public internet;attack possible sequences;models trained private;private datasets;examples querying language", "pdf_keywords": "private machine learning;extracting verbatim sequences;extracting verbatim;query access neural;language model extract;access neural;method extracting verbatim;verbatim text sequences;language model training;sequences model training;private stochastic gradient;text sequences model;verbatim sequences language;language model trained;extract hundreds verbatim;natural language processing;neural network language;verbatim sequences;address language modeling;language models;sequences language model;text sequences;learning models trained;stochastic gradient descent;introduction language models;natural language;access neural network;modeling language models;trained scrapes public;hundreds verbatim text"}, "bc4cb14af1023123b3122a5f0b6f3bb76334ffb4": {"ta_keywords": "kev extracted beam;conversion ion source;ion source built;lowenergy beam transport;pop ion source;80 kev beams;ion source new;conversion ion;lansce proton storage;kev lowenergy beam;alamos neutron scattering;alamos neutron;kev beams enable;beam transport lebt;ion source;ion source operated;proton storage ring;beam transport;beam emittance 80;surface conversion ion;lansce proton;kev beams;proton storage;80 kev extracted;psr pop ion;extracted beam;extracted beam measured;lowenergy beam;ma design beam;center lansce proton", "pdf_keywords": ""}, "073798fde720d5f08dccfbb0c1917a064828c399": {"ta_keywords": "cytomegalovirus subverts macrophage;cytomegalovirus subverts;cytomegalovirus;macrophage identity;subverts macrophage identity;subverts macrophage;macrophage;identity;subverts", "pdf_keywords": ""}, "81f5ef41dfa72679cb7cb38999a41a1c534c3871": {"ta_keywords": "memory casual relationships;casual relationships book;relationships book review;casual relationships;relationships book;relationships;book review;memory casual;book;creating memory casual;casual;memory;review;creating memory;creating", "pdf_keywords": ""}, "8d69f466bdf56ce6663c2f809514577e79dd3bed": {"ta_keywords": "iot based lab;recognition interfaced iot;iot;iot based;gesture controlled environment;wearable device;interfaced iot based;smart tools based;interfaced iot;wearable device comprises;projector gesture controlled;smart tools;technologies like image;projector gesture;having smart tools;gesture controlled;processing internet things;camera projector gesture;like gesture recognition;existing technologies like;device comprises camera;internet things;devices present lab;gesture recognition;existing technologies;lab controls remotely;internet things sixth;gesture recognition color;devices present;building interface", "pdf_keywords": "gesture controlled environment;sixth sense technology;sense wearable device;sense technology implementation;iot based;sense technology;technology implementation iot;iot;iot based lab;gesture recognition;smart tools based;gesture controlled;implementation iot;like gesture recognition;sense wearable;sixth sense wearable;gesture recognition color;interfaced iot based;recognition interface;projector gesture controlled;internet things sixth;implementation iot shubhankar;learning interfaced iot;smart tools;interfaced iot;iot shubhankar;projector gesture;wearable device;iot shubhankar mohan;wearable device comprises"}, "79c6713c41b4fedf9c7454b7e2bb48d0aeb1ae0f": {"ta_keywords": "filling spectral designs;space filling designs;designs arbitrary dimensions;sample designs arbitrary;space filling sample;filling sample designs;spectral designs;optimal space filling;designs refer space;sample designs;space filling spectral;quality space filling;spectral designs paper;quantify space filling;randomness sample designs;technique quantify space;space filling properties;designs arbitrary;filling designs using;filling designs;properties sample designs;high quality space;filling designs refer;evaluate space filling;spatial spectral analysis;space filling;quality space;dimensions image reconstruction;optimal space;measure design performance", "pdf_keywords": "fusion icf simulation;reconstruction surrogate modeling;fusion icf simulator;surrogate modeling benchmark;inertial con\ufb01nement fusion;surrogate models inertial;image reconstruction surrogate;con\ufb01nement fusion icf;surrogate modeling;benchmark optimization functions;reconstruction surrogate;cube surrogate model;con\ufb01nement fusion;surrogate model design;fusion icf;optimization functions inertial;benchmark optimization;designing surrogate models;dimensions image reconstruction;surrogate models;cube surrogate;icf simulation code;rosenbrock cube surrogate;icf simulation;applications image reconstruction;\ufb01lling spectral designs;fusion;icf simulator developed;model design inertial;modeling benchmark optimization"}, "2821db8962fce43265215a9c4b8d66af02e16ae7": {"ta_keywords": "scheduling redundant requests;schedule redundant requests;scheduling redundant;scheduler cancellation overhead;optimal static scheduler;appropriately schedule redundant;significantly optimal scheduling;schedule redundant;optimal dynamic scheduler;redundant requests job;scheduler performs optimal;optimal scheduling;actual optimal scheduling;optimal dynamic scheduling;scheduling policy overhead;jobs schedule redundant;static scheduler proposed;dynamic scheduling policies;optimal scheduling policy;dynamic scheduling;scheduling policies appropriately;scheduling;dynamic scheduler cancellation;scheduler proposed;static scheduler performs;scheduler proposed models;scheduling policies;job latency cancellation;static scheduler;scheduling policy", "pdf_keywords": ""}, "94245856c88e3e08777c876fc038ed1adf8f3285": {"ta_keywords": "classic description logic;description logic theoretical;description logic;learning classic description;logic theoretical;logic theoretical experimental;logic;classic description;description;learning classic;learning;theoretical;classic;theoretical experimental;experimental;results;theoretical experimental results;experimental results", "pdf_keywords": ""}, "9ef4f6a070c750b746fe98ef34083d6a08c9ba42": {"ta_keywords": "cost players control;control nash equilibrium;leader pricing;feedback control nash;nash equilibrium followers;pricing mechanisms;control strategy selfish;incurred leader pricing;linear quadratic game;control nash;follower cost function;quadratic game dynamically;equilibrium followers convex;followers convex feasibility;use pricing mechanisms;coupled nash followers;dynamically coupled nash;strategy selfish agents;pricing;nash equilibrium;pricing mechanisms means;convex feasibility problem;quadratic game;coupled nash;cost incurred leader;control strategy;actions follower cost;convex feasibility;cost players;feedback control strategy", "pdf_keywords": ""}, "ff86133b3b49974f06fc881548c6f3c7a8ceffee": {"ta_keywords": "perceived age singing;singing voice age;singing voice conversion;singing voice experimental;age control singing;age singing voice;statistical singing voice;convert singing voice;singer perceived age;use statistical singing;age singer perceived;possible convert singing;age singing;singing voice timbre;convert singing;statistical singing;control singing voice;voice age singer;target singer statistical;voice age;voice conversion technique;change singer perceived;voice timbre singers;singer statistical approach;singing voice;voice conversion;voice experimental;singer statistical;voice timbre control;voice timbre arbitrary", "pdf_keywords": ""}, "43d82bc8203c09edc7eb6b2bedcf4ab500690852": {"ta_keywords": "lingual adjustment contextual;effectiveness cross lingual;multilingual language models;lingual transfer capabilities;transfer english models;cross lingual adjustment;nli cross lingual;understanding cross lingual;cross lingual transfer;transfer cross lingual;lingual adjustment various;lingual transfer;cross lingual adjusted;lingual adjustment;impact cross lingual;improves nli languages;cross lingual;lingual adjustment mbert;nlp tasks contrast;nli languages ner;various nlp tasks;languages closer keeping;mbert improves nli;lingual adjusted;tune cross lingual;language models effectiveness;large multilingual language;multilingual;shot transfer english;lingual", "pdf_keywords": "word embeddings related;languages nlp tasks;different languages nlp;improves nli languages;mbert improves nli;lingual adjustment mbert;transfer english models;unrelated words languages;nli languages ner;similarities contextualized word;cross lingual adjustment;languages nlp;contextualized word embeddings;nli languages point;parallel corpus similar;lingual adjustment;word embeddings;corpus similar;following cross lingual;nli languages;nlp tasks;small parallel corpus;cross lingual;parallel corpus;english models;nlp tasks original;hindi nlp tasks;lingual;languages point ner;nlp tasks qa"}, "b57da3ccf214e8dad49116c8db9590c2c89629f5": {"ta_keywords": "entity recognition african;african continent nlp;continent nlp research;named entity recognition;entity recognition ner;recognition african languages;continent nlp;african languages analyze;entity recognition;ner african languages;african languages;nlp research bringing;nlp research;addressing representation african;recognition ner african;recognition african;ner african;representation african continent;nlp;languages help researchers;named entity;representation african;languages analyze datasets;african continent;dataset named entity;african;masakhaner named entity;recognition ner;languages help;entity", "pdf_keywords": "entities african languages;african languages ner;ner african languages;african continent nlp;african languages evaluate;representation african languages;continent nlp research;named entities african;language annotators;annotators develop ner;dataset curators nlp;corpora language annotators;african languages;language annotators develop;recognition ner african;african languages paper;ken african languages;languages ner task;continent nlp;entities african;addressing representation african;african languages huggingface;languages ner;news corpora language;ner african;nlp research bringing;nlp research;develop ner datasets;languages huggingface model;recognition ner"}, "05c8f15dbdd7c6661b9176638262bbc1e11de85f": {"ta_keywords": "interpretable sense embeddings;word sense embeddings;learns sense embeddings;sense embeddings learn;multi sense embeddings;interpretability sense embeddings;sense embeddings using;sense embeddings;sense embeddings discover;interpretable embeddings learn;sense embeddings previous;produce interpretable embeddings;sense embeddings represent;interpretable embeddings;embeddings discover interpretability;learning word sense;softmax;sense specific vectors;embeddings learn;sense selection;sense groups;modified gumbel softmax;gumbel softmax;learns sense;multiple sense specific;produce interpretable sense;sense selection methods;softmax function allows;sense groups method;softmax function", "pdf_keywords": "embeddings duplicated senses;existing sense embeddings;sense embeddings;interpretable sense representations;sense embeddings duplicated;sense representations actually;sense representations;evaluation sense embeddings;sense representations coherent;sense embeddings minimal;learns duplicate senses;discovering interpretable sense;uncover sense representations;capture interpretable senses;duplicate senses words;interpretable senses;contextual representations better;attention sense induction;duplicate senses;interpretable senses sec;duplicated senses;senses words;contextual representations;duplicated senses human;existing sense;coherent existing sense;senses human model;sense induction optimized;attention sense;modern contextual representations"}, "58737fba500075136ee0f33f7801a5ac7f82ab68": {"ta_keywords": "bm25 considering variants;differences including lucene;document length bm25;bm25 likely refer;implementation lucene;lucene;researchers speak bm25;bm25 mean;lucene maligned approximation;reproducibility study bm25;length bm25 mean;length bm25;lucene open source;study bm25;study bm25 considering;refer implementation lucene;bm25 entirely;including lucene maligned;including lucene;bm25 considering;bm25;bm25 likely;bm25 entirely clear;implementation lucene open;lucene maligned;study scoring variants;lucene open;scoring variants experiments;practitioners speak bm25;scoring variants", "pdf_keywords": ""}, "14919b6a453a71f2a007d5fa57241887a982575f": {"ta_keywords": "representation independent learnability;learning coreclassic theorem;coreclassic pat learnable;hardness learning coreclassic;learnable usual complexity;coreclassic theorem;independent learnability;representation independent hardness;independent learnability core;learnability core;independent hardness learning;coreclassic theorem error;hardness learning;learnability;learning coreclassic;learnability core classic;coreclassic pat;coreclassic;learnable;pat learnable;learnable usual;pat learnable usual;proof representation independent;representation independent;thank coreclassic pat;question representation independent;complexity assumptions;representation;particular constructed functions;constructed functions satisfy", "pdf_keywords": ""}, "fb089347919e8dada9335b4bac01f16eea758c56": {"ta_keywords": "ethics artificial intelligence;teaching ethics artificial;ethical questions artificial;ethics artificial;story examine ethical;teaching ethics;examine ethical;ethical;ethical theories;ethical questions;examine ethical issues;key ethical questions;stops teaching ethics;common ethical theories;ethical theories indicate;ethics;ethical issues;technologies teach;ethical issues related;questions artificial intelligence;view common ethical;games media students;intelligence computer science;common ethical;intelligence computer;technologies teach particular;key ethical;machine stops teaching;computer games;mentioning computer games", "pdf_keywords": ""}, "e2c05b3abf77900ec82ffa8a95aa774308d2780f": {"ta_keywords": "code switching twitter;level language detection;language detection;estimating code switching;language detection necessary;code switched text;switching specific languages;language detection technique;text multiple languages;analyzing code switched;code switching;restricted code switching;code switching specific;multiple languages mixed;code switched;multiple languages;word level language;switching twitter;languages mixed;specific languages;languages;switched text;switched text arbitrarily;language;languages fail;specific languages fail;estimating code;level language;switching twitter novel;languages mixed sentence", "pdf_keywords": ""}, "719916251f7e36d2e7a40e70f89f20ab97a8bc29": {"ta_keywords": "channel speech enhancement;speech enhancement masks;based speech enhancement;speech enhancement applications;various speech enhancement;speech enhancement;mask based speech;speech enhancement severely;mask based beamformer;single channel speech;learning blstm mask;spectral mask estimation;speech recognition student;speech signal make;enhancement techniques mask;channel speech;mask obtained multichannel;applied multichannel enhancement;multichannel enhancement techniques;spectral mask;multichannel enhancement;blstm mask based;speech signal;unsuitable speech recognition;enhancement masks;speech recognition;enhancement masks used;techniques mask based;mask estimation using;blstm mask", "pdf_keywords": "different speech enhancement;various speech enhancement;channel speech enhancement;based speech enhancement;speech enhancement;speech enhancement showed;speech enhancement using;performance speech enhancement;multichannel speech enhancement;speech enhancement 16;speech enhancement applications;speech enhancement input;speech enhancement aswin;speech enhancement addition;speech enhancement metrics;quality speech masks;speech enhancement properties;mask based speech;speech mask;speech enhanced speech;noisy speech enhanced;learning blstm mask;speech masks;speech mask target;speech enhanced;enhanced speech obtained;enhanced speech;evaluation speech quality;terms speech mask;enhancement using mask"}, "0909fee90833e20913adb553bf6667c9a3b854b0": {"ta_keywords": "wrapper learning systems;wrapper learning;earlier wrapper learning;look like database;additionally learning modular;like database;wrapper management;learning systems;like database called;learning described;database called wrapper;learner;data representations;text asm;learning modular;learning described industrial;tabular data representations;learning modular easily;systems learning described;learner broader;faster learning;data representations visual;wrapper;learning;additionally learning;learner broader coverage;asm rendered program;database;wrapper management active;rendered program", "pdf_keywords": ""}, "4a45ace1f8c6a30ba00201b30acd93844b9797eb": {"ta_keywords": "center histidine trypsin;histidine trypsin;histidine trypsin use;amino heptanone chloromethyl;heptanone chloromethyl ketone;tosylamido amino heptanone;heptanone chloromethyl;chloro tosylamido amino;center histidine;active center histidine;reagent chloro tosylamido;chloromethyl ketone derived;chloromethyl ketone;trypsin use specific;n\u03b1 tosyl lysine;tosylamido amino;histidine;tosyl lysine;amino heptanone;trypsin;trypsin use;chloromethyl;chloro tosylamido;specific reagent chloro;reagent chloro;amino;ketone derived n\u03b1;heptanone;specific reagent;derived n\u03b1 tosyl", "pdf_keywords": ""}, "dd3ba828dbbb17cf478f6840a37954f6ebc81770": {"ta_keywords": "bidirectional attention lstms;bidirectional attentionbased lstms;bidirectional attention based;term memory lstm;attention based lstms;short term memory;attention lstms;attention lstms understand;lstms understand speaker;using bidirectional attention;attentionbased lstms;bidirectional attentionbased;attentionbased lstms present;bidirectional attention;proposed bidirectional attentionbased;use bidirectional attention;memory lstm recurrent;memory lstm;lstms train roledependent;recurrent neural networks;lstms proposed bidirectional;lstm recurrent;rnns lstms proposed;spoken language understanding;lstm recurrent neural;term memory;rnns lstms;lstms understand;lstm;regression rnns lstms", "pdf_keywords": ""}, "81e57827bebb04305cc9e6c85e96c83951244ec2": {"ta_keywords": "knowledge graph embedding;embedding vectors predict;effects knowledge graph;graph embedding;knowledge graph;knowledge graph link;knowledge graph like;effects link prediction;new knowledge graph;data knowledge graph;graph embedding technique;link prediction;vectors predict polypharmacy;graph link prediction;embedding model based;link prediction task;perform link prediction;embedding model;knowledge graph formulates;predict adverse effects;embedding vectors;multi embedding vectors;link prediction solved;tensor decomposition models;using embedding model;task knowledge graph;polypharmacy effects knowledge;multi embedding;predict polypharmacy effects;reliably predict adverse", "pdf_keywords": "knowledge graph embedding;embedding vectors predict;embedding model learn;vectors predict polypharmacy;vector representations drugs;graph embedding model;predict polypharmacy effects;embedding model;graph embedding;related knowledge graph;predict polypharmacy;graph embedding technique;representations drugs effects;tensor factorisation based;embedding vectors;knowledge graph;learn vector representations;representations drugs;multi embedding vectors;new knowledge graph;factorisation based knowledge;tensor factorisation;model learn vector;use tensor factorisation;vector representations;multi embedding;embedding technique;vectors predict;embedding technique instead;link prediction"}, "e98621050e52e9d8c60829d8d861e81ac86a8617": {"ta_keywords": "space speaker adaptation;speaker adaptation;speaker adaptation consistently;model space speaker;space speaker;speaker;adaptation consistently employing;adaptation;employing map estimation;map estimation;adaptation consistently;prior shared feature;feature model space;shared feature model;consistently employing map;estimation;shared feature;feature;feature model;model space;map;prior shared;employing map;model;prior;shared;consistently;space;consistently employing;employing", "pdf_keywords": ""}, "e2b097bce656db9215505659357263c43190194b": {"ta_keywords": "optimal reviewer splitting;near optimal reviewer;optimal reviewer;dividing reviewers phases;random reviewer split;reviewer splitting phase;reviewer splitting;dividing reviewers;data dividing reviewers;paper reviewing conference;reviewer split suitable;reviewers phases;reviewer split;reviewers phases conditions;assigned additional reviewers;paper reviewing;random reviewer;paper review process;reviewing conference;chairs random reviewer;additional reviewers;additional reviewers initial;reviewing conference experiment;paper review;phase paper reviewing;split suitable conference;reviewing;random allows assignment;review process papers;conference experiment design", "pdf_keywords": "assignment conference experiment;paper assignment conference;random allows assignment;dividing reviewers phases;assignment conference;paper assignment problem;optimal assignment;conference experiment design;dividing reviewers;optimal assignment uniformly;oracle optimal assignment;data dividing reviewers;stage paper assignment;assignment uniformly random;conference experiment;random reviewer split;paper assignment;reviewers phases conditions;random choice practical;reviewers phases;split suitable conference;conference data dividing;random choice;conferences applications;conference program chairs;allows assignment nearly;phase paper assignment;chairs random reviewer;random strategy;random reviewer"}, "c0c1950fb0a129b71a218ffa8b9fbc6d088cba2d": {"ta_keywords": "computer science higher;approaches computer science;innovative approaches computer;computer science;approaches computer;science higher education;higher education;innocse 2017 innovative;higher education proceedings;2017 innovative approaches;higher education chelyabinsk;science higher;workshop innovative approaches;innovative approaches;2017 innovative;innocse 2017;approaches;education chelyabinsk russia;international workshop innovative;education proceedings;innovative;education chelyabinsk;workshop innovative;education proceedings 1st;1st international workshop;education;international workshop;computer;russia 26th 2017;workshop", "pdf_keywords": ""}, "27f9b91bd7c70a99f578c8a5cb52d37e4123da47": {"ta_keywords": "tensor regression layers;activation tensors fully;tensor contraction layers;activation tensors;tensor regression;activation tensor output;activation tensor;introduce tensor regression;order activation tensors;tensors fully;incorporating tensor;order activation tensor;tensor output;tensors;layers regularize networks;mri convolutional layers;output tensor;tensor;regularize networks;output tensor arbitrary;flattened activation vectors;tensor output tensor;structure using tensor;imagenet;regression layers;convolutional layers;tensors fully connected;using tensor;tensor contraction;incorporating tensor algebraic", "pdf_keywords": "tensor regression layers;tensor contraction layers;tensor structure network;tensor structure activations;leveraging tensor structure;dataset introduce tensor;tensor regression;preserving tensor structure;introduce tensor regression;activation tensor output;activation tensor;leveraging tensor;contraction tensor regression;preserving leveraging tensor;tensor structure;preserving tensor;propose tensor contraction;tensor;tensor output;tensor contraction experiments;modal tensor structure;structure tensor contraction;introduce tensor contraction;tensor contraction;order activation tensor;tensor structure tensor;propose tensor;structure using tensor;tensor output tensor;output tensor"}, "b09d49c3eacd93782a32ad16ab52f98a21ecc206": {"ta_keywords": "\u30ea\u30a2\u30eb\u306a \u8a00\u8a9e\u51e6\u7406 anpi_nlp;\u4e0d\u81ea\u7136\u8a00\u8a9e\u51e6\u7406 \u67a0\u306b\u53ce\u307e\u3089\u306a\u3044 \u30ea\u30a2\u30eb\u306a;\u30ea\u30a2\u30eb\u306a \u8a00\u8a9e\u51e6\u7406;\u8a00\u8a9e\u51e6\u7406 anpi_nlp nlp\u6280\u8853\u3092\u5fdc\u7528\u3057\u305f\u9707\u707d\u6642\u5b89\u5426\u60c5\u5831\u78ba\u8a8d\u652f\u63f4;\u67a0\u306b\u53ce\u307e\u3089\u306a\u3044 \u30ea\u30a2\u30eb\u306a \u8a00\u8a9e\u51e6\u7406;\u8a00\u8a9e\u51e6\u7406 anpi_nlp;\u30ea\u30a2\u30eb\u306a;\u67a0\u306b\u53ce\u307e\u3089\u306a\u3044 \u30ea\u30a2\u30eb\u306a;anpi_nlp nlp\u6280\u8853\u3092\u5fdc\u7528\u3057\u305f\u9707\u707d\u6642\u5b89\u5426\u60c5\u5831\u78ba\u8a8d\u652f\u63f4;\u4e0d\u81ea\u7136\u8a00\u8a9e\u51e6\u7406 \u67a0\u306b\u53ce\u307e\u3089\u306a\u3044;\u8a00\u8a9e\u51e6\u7406;\u4e0d\u81ea\u7136\u8a00\u8a9e\u51e6\u7406;nlp\u6280\u8853\u3092\u5fdc\u7528\u3057\u305f\u9707\u707d\u6642\u5b89\u5426\u60c5\u5831\u78ba\u8a8d\u652f\u63f4;anpi_nlp;\u67a0\u306b\u53ce\u307e\u3089\u306a\u3044", "pdf_keywords": ""}, "1bf49ef0b33bf8fcc3ebdd16326db419f3af65d8": {"ta_keywords": "representations soft nearest;maximizing entanglement representations;nearest neighbor loss;soft nearest neighbor;improving representations soft;entanglement representations;entanglement class manifolds;neighbor loss measure;neighbors predicted class;representations soft;maximizing entanglement;neighbor loss data;neighbor loss hidden;class independent similarity;entanglement class;maximizing soft nearest;entanglement representations different;representations identify class;outlier;improving representations;loss data training;estimates uncertainty outlier;nearest neighbor;textit maximizing entanglement;uncertainty outlier;encourages representations identify;discrimination final layer;loss hidden layers;similarity structures maximizing;layers beneficial discrimination", "pdf_keywords": "maximizing representation entanglement;entanglement hidden representations;maximizing entanglement hidden;deliberately maximizing entanglement;maximizing entanglement;deep nearest neighbors;representation entanglement;hidden representations classi\ufb01er;entanglement hidden layers;gans semantics captured;hidden representations;deep nearest;uncertainty deep nearest;gans semantics;nearest neighbors dknn;promote entanglement hidden;representation entanglement adding;entanglement hidden;maximizing representation;neighbors dknn approach;nearest neighbor loss;representations classi\ufb01er;discriminator used gans;representations classi\ufb01er contributions;used gans semantics;dknn approach relies;entanglement;promote entanglement;neighbor loss training;practice promote entanglement"}, "3a4f39dbb5e06a5fc55622315797da7a97cc76f6": {"ta_keywords": "word level quality;predicting words output;recurrent neural networks;feed forward recurrent;sentence machine generated;translation predicting words;source sentence machine;sentence machine;forward recurrent neural;predicting words;word using neural;generated translation predicting;machine generated translation;translation predicting;recurrent neural;sentence making predictions;contextual information target;global contextual;global context sentence;context sentence making;contextual;global contextual information;forward recurrent;context information target;sentence making;neural;local global contextual;quality estimation;contextual information;level quality estimation", "pdf_keywords": "additions bidirectional rnns;concatenated word embeddings;bidirectional rnns;context encoding quality;word embeddings target;rnns;benchmark wmt18 word;bidirectional rnns applied;word embeddings;convolution concatenated word;encoding context word;better sequence prediction;word using neural;wmt18 word level;context word level;neural architecture context;context encoding;words handcrafted features;better encoding context;encoding quality estimation;rnns applied capture;contextual;encoding context;global contextual;architecture context encoding;contextual information target;wmt18 word;neural;benchmark wmt18;effectively encode"}, "b9a701c90f3d3df27366f5b29a97f798eb940ac7": {"ta_keywords": "narrative chapterbreak challenge;long segment narrative;sampled narrative chapterbreak;segment narrative ends;narrative chapterbreak;chapterbreak challenge dataset;range language models;language models lrlms;segment narrative;language models;level language understanding;narrative ends chapter;segments sampled narrative;introduce chapterbreak challenge;chapterbreak challenge;discourse level language;long range language;narrative ends;language models numerous;language understanding capabilities;lrlm long segment;chapterbreak existing lrlms;language understanding;level language;challenge dataset long;long range context;chapterbreak;discourse level;context substantially underperforming;end introduce chapterbreak", "pdf_keywords": "chapterbreak challenge dataset;long segment narrative;challenge dataset chapterbreak;narrative understanding long;introduce chapterbreak challenge;frequency narrative shifts;segment narrative ends;narrative dataset extracted;chapterbreak challenge;language models lrlms;narrative dataset;range language models;narrative ends chapter;segment narrative;discourselevel understanding long;chapter transitions dataset;dataset chapterbreak;language models;global narrative understanding;high frequency narrative;narrative shifts;dataset chapterbreak automatically;narrative shifts pointof;segments sampled narrative;long form narratives;sampled narrative dataset;detecting chapter bound;chapterbreak;introduce chapterbreak;level language understanding"}, "2559417f8a3d6ab922cfa824b43f9f0c642a1dae": {"ta_keywords": "named entity recognition;entity recognition ner;named entity extraction;entity recognition systems;entity recognition;improving named entity;dictionaries named entity;similarity extracted entities;entity extraction;entity extraction combining;entities external dictionary;names exploiting dictionaries;named entity;external dictionaries specifically;entity useful similarity;improving named;external dictionaries;exploiting dictionaries named;using external dictionaries;performance named entity;extracted entities entities;dictionaries named;extracted entities;external dictionary;information similarity extracted;markov extraction processes;sequentially classifying words;sequential word classification;word classification difficult;semi markov extraction", "pdf_keywords": ""}, "3ad3ba8d7fc793a19dfe6a87e32453449195c074": {"ta_keywords": "text speech autoencoders;speech text autoencoders;speech autoencoders;decoders automatic speech;speech text training;text autoencoders;text autoencoders leverage;speech recognition asr;training datasets autoencoders;asr text speech;speech tts encoder;speech text datasets;datasets autoencoders learn;autoencoders learn;autoencoders;end speech recognition;datasets autoencoders;autoencoders learn features;automatic speech;text autoencoders share;autoencoders leverage;features speech text;automatic speech recognition;models build speech;build speech text;speech recognition;text speech;large speech text;encoders decoders automatic;using text speech", "pdf_keywords": ""}, "765bdcf27ebc1eb03a14f1e47aefa4dda1e03073": {"ta_keywords": "word representations robust;training noisy texts;machine translation model;neural machine translation;translate moderately noisy;learn representations robust;moderately noisy texts;noisy texts;translation model;representations robust training;translation nmt models;translation systems;noisy texts state;machine translation;noise break neural;machine translation nmt;character convolutional neural;translation model based;end translation systems;models alleviate vocabulary;robust training noisy;representations robust;learn morphology closer;noisy texts humans;invariant word representations;character convolutional;translation systems explore;break neural;issues learn morphology;learn morphology", "pdf_keywords": "word representations robust;representations robust training;learn representations robust;representations robust;robust training noisy;robustness machine translation;training noisy texts;robust training;training adversarial;word representation ensemble;character convolutional neural;ensemble training adversarial;character based cnn;training adversarial examples;character convolutional;invariant word representations;representation ensemble training;representations robust multiple;based character convolutional;adversarial;adversarial examples;invariant word representation;noisy texts;cnn learn;model robustness;word representations;representation ensemble;noisy texts common;adversarial examples different;learn representations"}, "1263e36598dd95cc4becf0e18398f832bb5cf337": {"ta_keywords": "language training attention;related language training;adversarial language network;language training;language network;loss adversarial language;low resource languages;attention based encoder;corpora low resource;language vector injection;adversarial language;training attention based;language network alongside;related language;languages critical augment;language vector;hallucination language vector;resource languages;languages;training attention;languages help develop;language;languages good;resource languages critical;corpora low;attention based;existing corpora low;languages critical;resource languages help;corpora", "pdf_keywords": ""}, "978582ad754eab481856d62bdc7b0ee5bcf21811": {"ta_keywords": "federated unsupervised clustering;iterative federated clustering;clusters federated unsupervised;clustering generative models;unsupervised clustering generative;federated clustering;unlabeled datasets federated;clustering generative;federated clustering algorithm;clustering unlabeled datasets;datasets federated;models captures cluster;unsupervised clustering;datasets federated environment;clusters federated;supervised datasets;clustering unlabeled;generative models ifca;federated unsupervised;different clusters federated;cluster information individual;cluster information;clustering algorithm ifca;recover cluster information;unlabeled datasets;uifca using generative;generative models;cluster;clusters;training models fedavg", "pdf_keywords": ""}, "675098c4611b13920d163a9a9b972da7751460cb": {"ta_keywords": "spoken dialog systems;efficient learning spoken;spoken dialog;learning spoken;rnn architecture spoken;learning spoken language;recurrent neural network;components spoken dialog;networks proposed rnn;dialog systems;spoken language understanding;dialog systems performance;tasks word embedding;language understanding tasks;neural network rnn;network rnn;recurrent neural;rnn architecture;word embedding based;network rnn architecture;word embedding latent;language understanding slu;spoken language;dialog;architecture spoken language;word embedding;topic models pre;latent topic models;entire dialog;performance neural", "pdf_keywords": ""}, "dd64013273eb4398821bf2fc8f024735466e5a1d": {"ta_keywords": "simulate human learning;simulation human learning;learns procedural knowledge;constructing learning agent;intelligent agents simulate;learning agent;simstudent learns procedural;procedural knowledge;student simstudent learns;simulated student simstudent;human learning math;learning agent currently;simulated student;algorithm simulated student;procedural knowledge example;simstudent learns;learns procedural;develop intelligent agents;humans acquire knowledge;acquire knowledge students;acquire knowledge;knowledge students;agents simulate;constructing learning;education improving understanding;intelligent agents;human learning;improving understanding humans;agents simulate human;knowledge students vary", "pdf_keywords": ""}, "1abbe9b6bf3f134ce86e618bba83bf5c94f60f03": {"ta_keywords": "prediction parametric agents;prediction parametric;parametric prediction;prediction algorithms;prediction algorithms designed;customers prediction winner;parametric prediction parametric;potential customers prediction;desires predict parameter;customers prediction;designed assumption training;parametric agents;predict parameter known;error parametric prediction;prediction error parametric;predict parameter;minimizes overall prediction;design problem prediction;algorithms designed assumption;prediction future demand;formulate optimally;surveying potential voters;challenge incentivization procedure;problem prediction algorithms;prediction;problem prediction;winner election surveying;assumption training;assumption training data;algorithms designed", "pdf_keywords": "incentive mechanism prediction;cost prediction elicitation;optimal joint incentive;elicitation jointly optimizes;prediction parametric agents;prediction elicitation jointly;joint incentive mechanism;information incentivize agents;prediction elicitation;incentive mechanism;prediction elicitation analysis;joint incentive;cost prediction;incentivize agents;incentive;agents private information;agents prediction;cope cost prediction;jointly optimizes;elicit heterogeneous agents;elicitation jointly;information order elicit;private information incentivize;rational agents private;information incentivize;agents di\ufb00erent capabilities;heterogeneous agents private;incentivize agents di\ufb00erent;elicitation analysis;prediction based opinions"}, "a2ea2261bd56ae2505750d7571b501d9836175f0": {"ta_keywords": "algorithm discriminative training;discriminative training framework;discriminative training;discriminative training methods;algorithm discriminative;training data complementary;discriminative training adjust;complementary models optimizing;update algorithm discriminative;requirements discriminative training;unify requirements discriminative;speech recognition;discriminative;construct complementary models;automatic speech;automatic speech recognition;requirements discriminative;speech recognition reference;relationship boosting methods;complementary models;boosting methods;performance automatic speech;construct complementary systems;relationship boosting;criterion construct complementary;data complementary;close relationship boosting;generate complementary hypotheses;mutual information base;recognition reference", "pdf_keywords": ""}, "39456ca31a530d85ec182b2676dc94266dada597": {"ta_keywords": "compositional distributional semantics;distributional semantics methods;distributional semantics;distributional semantics work;tensors standard semantic;composing word vectors;semantic similarity tasks;semantic similarity;similarity tasks syntactic;tensors using lowrank;standard semantic similarity;representing meaning composing;words feature vectors;learning models compositional;word vectors;word vectors produce;explores compositional distributional;adjectives represented matrices;syntactic constructions adjectives;semantics methods mapping;semantics methods;semantic;transitive verbs represented;models compositional distributional;constructions adjectives represented;tensors low rank;compositional distributional;syntactic constructions;represented order tensors;tasks syntactic constructions", "pdf_keywords": ""}, "bc8e67d532693818eb33aa8e401260fe2b774a18": {"ta_keywords": "wrapper induction;wrapper induction complex;induction complex documents;tabular data web;complex documents application;documents application tabular;complex documents;data web;tabular;tabular data;wrapper;induction;application tabular data;application tabular;induction complex;documents;documents application;web;data;complex;application", "pdf_keywords": ""}, "29dc4b10e60ed1e2b84598cc6f2622c786841fdf": {"ta_keywords": "temporal logic control;planning specifications robotic;temporal logic specifications;specification temporal logic;functions temporal logic;linear temporal logic;motion planning specifications;robots subject temporal;defined specification temporal;temporal logic;control mobile robots;robots using barrier;temporal logic encompasses;specification temporal;barrier functions temporal;specifications robotic;specifications robotic complex;planning specifications;temporal logic sequence;subject temporal logic;logic control barrier;control barrier functions;motion planning;prioritization based control;functions temporal;using linear temporal;barrier functions prioritization;logic control;feasibility controller control;mobile robots subject", "pdf_keywords": "control mobile robots;robots using barrier;planning speci\ufb01cations robotic;temporal logic speci\ufb01cations;control mobile robotic;linear temporal logic;functions temporal logic;control barrier functions;using control barrier;speci\ufb01cations robotic;temporal logic;robots subject temporal;veri\ufb01cation control networked;speci\ufb01cations using control;temporal logic encompasses;dynamical systems autonomy;mobile robotic systems;control architecture;barrier functions temporal;control barrier;control networked cyber;motion planning speci\ufb01cations;mobile robots;function based controller;mobile robotic;veri\ufb01cation control;mobile robots subject;robotic systems subject;robotic systems;control architecture control"}, "3f0f6c19c6f5d4e4d5066984c5f3e922a2c2ff85": {"ta_keywords": "language abstraction reward;language based reward;learned language abstraction;instruction complexities reward;abstraction reward shaping;exploration learned language;agents reinforcement learning;abstraction reward;agents reinforcement;tasks learning policies;reinforcement learning environments;agents capable understanding;instruction following agents;language abstraction;reinforcement;following agents reinforcement;learned language;reinforcement learning;human ai;based reward shaping;sparsereward tasks learning;tasks learning;learning policies;complexities reward sparsity;enrich sparse rewards;tasks ella exploration;reward shaping;capable understanding language;synthetic language instructions;level tasks ella", "pdf_keywords": "language abstraction reward;abstraction reward shaping;abstraction based reward;abstraction reward;sparse reward environments;learned language abstraction;exploration learned language;sparse reward task;complex sparse reward;reward shaping framework;sparse reward;reward environments;instruction complexities reward;complexities reward sparsity;based reward shaping;language abstraction ella;reward shaping;level language task;language abstraction;reward shaping approach;level language;complexities reward;intermediate rewards agent;ella reward shaping;tasks babyai platform;reward sparsity ella;reward task;ef\ufb01ciency sparse reward;abstraction ella provides;approach learns"}, "81ea04f822a1d5317e5846783900ac424a8f7528": {"ta_keywords": "automatic identification autism;children autism spectrum;identification autism spectrum;linguistic acoustic features;classifiers linguistic acoustic;differences children autism;autism spectrum;children autism;autism spectrum disorders;identification autism;automatic classifiers linguistic;linguistic acoustic;autism;classifiers linguistic;disorders terms linguistic;categories prosody voice;linguistic cues;acoustic features automatic;categories prosody;disorders children narrative;terms linguistic acoustic;word categories prosody;children narrative;children narrative examine;acoustic features mention;turns linguistic cues;prosody voice quality;prosody voice;voice quality differences;acoustic features", "pdf_keywords": ""}, "98fdc7e1e167eb465cdb1c8ee0800db750101155": {"ta_keywords": "statistical voice conversion;conversion statistical voice;utterance spectral distance;conversion metric speaker;spectral variation utterances;speaker spectral variation;voice conversion;spectral distance utterances;utterance utterance spectral;utterance spectral;speaker spectral parameter;spectral conversion statistical;intra speaker spectral;statistical voice;metric speaker utters;predicts intra speaker;speaker spectral;predicting variation prosodic;speech samples;spectral conversion metric;using speech samples;voice conversion vc;variation utterances;evaluations using speech;differences corresponding utterances;distance utterances;observed prosodic changes;parameters vary utterance;improvement spectral conversion;utters sentence spectral", "pdf_keywords": ""}, "ae25ca24eb6c2772ef88e2d0315fc428feb8553e": {"ta_keywords": "unsupervised information extraction;websets unsupervised information;information extraction;information extraction approach;meaningful sets websets;sets websets unsupervised;websets unsupervised;existing knowledge base;extract sets entities;unsupervised information;categories added knowledge;sets entities web;information extraction exploits;added knowledge base;knowledge base;entities meaningful sets;propose unsupervised information;websets;entities web;sets websets;meaningful sets entities;approach extract sets;existing categories;extract sets;categories;entities web extended;knowledge base candidate;category names;frequently occurring entities;sets entities", "pdf_keywords": ""}, "689ab475e8a0f552bf6e39a2f774d9d20e50b9cb": {"ta_keywords": "145 embedded systems;power snns;embedded systems design;embedded systems;report power snns;snns;systems design project;systems design;embedded;145 embedded;237 145 embedded;design project final;design project;systems;cse 237 145;design;cse 237;project report power;final project report;project final project;project final;project report;cse;final project;power;237 145;145;237;report power;project", "pdf_keywords": ""}, "38a73e6f48d057cb58264f5148f8b05522d0d030": {"ta_keywords": "semantic parsing corpus;parsing corpus semantic;corpus semantic parsing;dependent semantic parsing;annotation semantic parsing;semantic parsing;parsing annotation semantic;semantic parsing annotation;semantic parsing task;semantic parsing possible;semantic parsing paper;corpus semantic;parsing corpus;context dependent semantic;progress semantic parsing;parsing annotation;annotation semantic;corpus context dependent;nl utterances machineinterpretable;natural language nl;machineinterpretable meaning representation;parsing;parsing possible;corpus context;new corpus context;dependent semantic;utterances machineinterpretable meaning;corpora progress semantic;natural language;semantic", "pdf_keywords": ""}, "045f90129a8d7148eec4a58770bc4166b51330ca": {"ta_keywords": "modeling parking demand;demand parking consistent;model demand parking;temporal modeling parking;parking demand;demand parking;congestion caused parking;park data driven;modeling parking;spatial demand quantified;parking consistent;demand quantified spatial;parking performance based;parking consistent time;similar spatial demand;repeatability gaussian mixture;parking performance;parking;spatial demand;park data;spatio temporal modeling;gaussian mixture model;data driven spatio;looking park data;mixture model demand;caused parking performance;transportation develop gaussian;driven spatio temporal;develop gaussian mixture;gaussian mixture", "pdf_keywords": ""}, "fd0aa185be4e1f1fe3975779aec179348ec19ea8": {"ta_keywords": "double blind conferences;online arxiv review;reviewers tier;blind conferences;papers visibility;submissions 4699 reviewers;arxiv review process;blind conferences engaged;4699 reviewers ec;reviewers particular papers;reviewers tier double;papers arxiv pros;reviewers ec 2021;surveys reviewers tier;ranked a\ufb03liations visibility;papers visibility 18;conferences icml 2021;surveys reviewers;4699 reviewers;conducted surveys reviewers;submissions 190 reviewers;science conferences icml;2021 5361 submissions;arxiv review;ec reviewers self;reviewers self;ec reviewers;papers online arxiv;review independently authors;conferences icml", "pdf_keywords": "double blind reviewing;blind reviewing;blind reviewing 2021;submissions 4699 reviewers;online arxiv review;internet review;paper internet review;submissions 190 reviewers;surveys reviewers;2021 5361 submissions;reviewers tier;peer review;disciplines peer review;blind computer science;4699 reviewers;reviewers ec 2021;4699 reviewers ec;reviewers arxiv;reviewers tier double;conducted surveys reviewers;surveys reviewers tier;seen paper reviewing;2021 498 submissions;reviewers seen;internet review process;double blind conferences;paper reviewing;reviewers;reviewers seen paper;reviewers self"}, "f837bf72e5b864e1c162e924fed59b778e946e23": {"ta_keywords": "visual guessing games;guesswhat imagination module;encoders learns context;auto encoders learns;learns context aware;visual guessing;learns context;scene training inference;objects scene training;learn conceptual representations;encoders learns;guessing games;conceptual representations objects;attributes imagination;aware latent embeddings;players learn conceptual;imagination module boosts;compguesswhat visual guessing;category aware latent;object scene asking;attributes imagination module;imagination module;embeddings relying category;imagination module based;guessing games guesser;latent embeddings relying;representations objects;embeddings relying;objects discriminative expressive;imagination module outperforms", "pdf_keywords": "grounded conceptual representations;imagination embeddings;imagination embeddings help;object scene training;learning grounded conceptual;encoders learns context;models learning grounded;learns context;scene training inference;learns context aware;showed imagination embeddings;embeddings relying category;aware latent embeddings;conceptual representations;latent embedding object;auto encoders learns;aware latent embedding;imagination models;new imagination models;embeddings relying;latent embeddings relying;embedding object directly;category aware latent;imagination module;conceptual representations fail;imagination module based;modal representations relying;latent embeddings;learning grounded;novel imagination module"}, "df4e3aa275b8f81e22a5332ab550805083094dae": {"ta_keywords": "neural generation translation;neural machine translation;document generation translation;workshop neural generation;machine translation nmt;neural generation;generation translation;machine translation;efficient document generation;generate summaries structured;document generation;generation translation held;workshop neural;translation nmt participants;systems generate summaries;generate summaries;generation translation dgt;findings workshop neural;translation nmt;tasks efficient neural;summaries structured;efficient neural machine;efficient neural;language findings workshop;language processing emnlp;neural machine;text language findings;natural language processing;tasked creating nmt;neural", "pdf_keywords": "neural machine translation;translation generation;neural generation translation;generation translation task;document generation translation;translation generation single;purposes translation generation;translation task tested;machine translation nmt;machine translation;generation translation;nlg tasks;testbed nlg mt;mt nlg tasks;translation task;textual accuracy rg;single testbed nlg;testbed nlg;translation nmt participants;generation translation held;generation translation dgt;nlg tasks report;neural generation;systems generate summaries;workshop neural generation;level generation translation;document generation;textual accuracy;translation nmt;new document generation"}, "e5efd7e2087e58c5a8860398dfcf143aa9dc865e": {"ta_keywords": "supervised sound event;sound event detection;weakly supervised sound;supervised sound;sound event;acoustic driven event;beled event detection;event classes sound;acoustic class inference;weakly labeled training;event boundary detection;event detection task;event classification;sound categories;accurate event localization;segmentation recognition events;weakly supervised;inference weakly supervised;event detection;detection supervised label;data event classification;2018 dcase challenge;recognition events providing;inference using deep;event classification methods;recognition events;detection supervised;event localization;boundary detection supervised;dcase challenge presents", "pdf_keywords": "supervised acoustic event;segmentation sound events;annotate sound events;sound event detection;detection supervised acoustic;acoustic event classi\ufb01cation;sound events;sound events complex;events complex acoustic;supervised acoustic;acoustic driven event;sound event;sound events occur;complex acoustic scenes;segmentation sound;acoustic event;acoustic scenes;acoustic scenes present;annotate sound;classi\ufb01cation annotate sound;event boundary detection;method sound event;acoustic driven analysis;event detection details;event detection;boundary detection supervised;guide segmentation sound;detection supervised;proposed event detection;detection supervised label"}, "0ae93646ad058853eb6424c1dc0ec1559414e5af": {"ta_keywords": "models automatic rumour;rumour verification models;automatic rumour verification;uncertainty rumour verification;predictive uncertainty rumour;automatic rumour;rumour verification estimates;rumour verification;uncertainty rumour;estimates natural language;model performance rumour;performance rumour unfolds;human fact checker;fact checker estimating;rumour;rumour unfolds;natural language processing;performance rumour;verification models uncertainty;estimating predictive uncertainty;language processing models;checker estimating predictive;natural language;fact checker;models uncertainty estimates;model data uncertainty;prioritised human fact;models uncertainty;predictions likely erroneous;verification models", "pdf_keywords": "models automatic rumour;estimates natural language;automatic rumour veri\ufb01cation;uncertainty outperforms unsupervised;rumour veri\ufb01cation models;predictive uncertainty rumour;rumour veri\ufb01cation uncertainty;uncertainty rumour veri\ufb01cation;estimating predictive uncertainty;uncertainty based instance;automatic rumour;instance rejection supervised;predictive uncertainty data;uncertainty rumour;rejection supervised unsupervised;natural language processing;supervised unsupervised consider;uncertainty estimates imbalanced;epistemic uncertainty outperforms;data uncertainty aleatoric;model data uncertainty;natural language;model uncertainty epistemic;data uncertainty estimates;supervised unsupervised;types predictive uncertainty;uncertainty outperforms;uncertainty epistemic estimates;language processing models;rejection supervised"}, "0c12e4c611b32997f8be5811021ead80395a7e5c": {"ta_keywords": "learning speaker role;neural conversation models;task learning speaker;speaker role;speaker role based;based neural conversation;conversation models;neural conversation;learning speaker;multi task learning;role based neural;conversation;task learning;speaker;multi task;role;role based;multi;neural;task;based neural;learning;models;based", "pdf_keywords": ""}, "9af2264799bdc3490e4650e2f5d126762caf420f": {"ta_keywords": "end speech recognition;attention based end;ctc attention based;attention based encoder;ctc attention model;speech recognition directly;attention model multi;joint ctc attention;learning attention model;speech recognition;connectionist temporal classification;attention model;temporal classification ctc;task learning attention;ctc attention;end end speech;learning attention;speech text predefined;speech recognition improve;long input sequences;end speech;speech text;transcribes speech text;recognition directly transcribes;decoder framework learns;alignments approach attention;attention based;directly transcribes speech;multi task learning;task learning", "pdf_keywords": "end speech recognition;attention models cer;ctc attention encoder;ctc attention models;attention model multi;attention encoder;speech recognition improve;ctc attention model;attention encoder decoder;attention based encoder;ctc attention based;attention models;outperforms ctc attention;multi task learning;speech recognition;attention model;chime tasks outperforms;using ctc attention;joint ctc attention;task learning;chime tasks;end end speech;ctc attention;wsj chime tasks;end speech;training shared encoder;advantages ctc attention;encoder using auxiliary;speech recognition method;task learning framework"}, "9d555ed29496850c4ef8a3facd7dce734c86aae7": {"ta_keywords": "predict comments java;prediction programmer comments;comment completion tool;programmer comments;predicting code mainly;predict comments;predicting code;comment completion;comments java source;work predict comments;comment completion capability;models comment completion;using comment completion;comments java;similar codecompletion tools;task predicting code;codecompletion tools built;codecompletion tools;comment typing statistical;comment typing;statistical language models;similar codecompletion;language models programming;codecompletion;editors using comment;prediction programmer;code editors;language models;completion tool;typing statistical language", "pdf_keywords": ""}, "7d9863258ef44ca8a6b87b68be738f7a83ac849a": {"ta_keywords": "speech enhancement asr;end speech recognition;speech recognition asr;text speech enhancement;asr encompass microphone;converts multichannel speech;multichannel speech;end asr architecture;multichannel speech signal;end automatic speech;speech enhancement;speech recognition neural;automatic speech;noisy asr tasks;speech recognition;signal text speech;challenging noisy asr;asr components acoustic;speech enhancement experimental;automatic speech recognition;enhancement asr components;neural beamformer end;enhancement asr;allows speech enhancement;speech signal text;neural beamformer;asr architecture directly;asr architecture integrating;architecture integrating asr;recognition asr", "pdf_keywords": ""}, "c6c18ad62f39060e2547a0b683525e83312d0700": {"ta_keywords": "incentives reviewers bidding;reviewers bidding;incentives reviewers;reviewers price paper;reviewers bidding phase;reviewer bid;conferences bidding scheme;conferences bidding;peer review paper;assigning budgets reviewers;bidding scheme peer;bidding phase reviewers;real conferences bidding;analysis incentives reviewers;reviewers private costs;assignment paper reviews;budgets reviewers price;inspired bidding scheme;market inspired bidding;reviewers price;peer review;paper reviews;bidding scheme substantially;bidding;provide analysis incentives;analysis incentives;demand paper;extensive simulations bidding;papers match budget;budgets reviewers", "pdf_keywords": "incentives reviewers bidding;reviewers bidding;bidding mechanism reviewers;incentives reviewers;reviewers price paper;reviewer bid;reviewers bidding phases;papers reach bidding;bidding phases reviewers;assigning budgets reviewers;paper reviewers;peer review paper;assignment paper reviews;reflects paper demand;alternative bidding mechanism;analysis incentives reviewers;paper demand;papers match budget;conferences paper reviewers;bidding scheme peer;simple alternative bidding;demand paper;demand paper want;reviewers private costs;review paper assignment;inspired bidding scheme;budgets reviewers price;budgets reviewers;alternative bidding;bid favorite papers"}, "be4d47a61fee83d332ca2f3fe097f19f63863d6c": {"ta_keywords": "node clustering graphs;clustering graphs empirical;clustering graphs;node clustering;node clustering performances;bioinformatics social network;analysis node clustering;graphs empirical study;compares node clustering;families undirected graphs;social network analysis;graphs empirical;clustering performances;undirected graphs important;graph analysis node;network analysis node;modeling networks;undirected graphs;clustering performances state;network analysis;social network;graphs;graph analysis;modeling networks active;algorithms probabilistic spectral;clustering;graphs important;networks;graph;node", "pdf_keywords": ""}, "554eade16fb6040bbd21a72bacf903245d7458f1": {"ta_keywords": "human machine intelligence;capabilities ai;capabilities lacking ai;similar capabilities ai;ai;ai research;ai embedding causal;stimulate ai research;lacking ai;human capabilities;capabilities ai embedding;machine intelligence;advance ai;lacking ai instance;human capabilities lacking;stimulate ai;ai research community;ai instance;ai draws inspiration;theories human decision;machine intelligence paper;understanding human machine;cognitive theories human;ai embedding;direction advance ai;human decision making;intelligence;intelligence paper proposes;consider stimulate ai;ai instance adaptability", "pdf_keywords": "convene ai research;provide ai desired;provide ai;ai understanding human;ai;theories human reasoning;signi\ufb01cant advances ai;advances ai understanding;ai desired;human intelligence;advances ai;ai research;human reasoning;ai multi agent;human intelligence precisely;ai understanding;stimulate ai research;human machine intelligence;human intelligence currently;understanding human intelligence;stimulate ai;trying provide ai;human reasoning decision;ai systems self;ai researchers convene;world ai;researchers convene ai;cognitive theories human;ai multi;action ai"}, "b8f5f3c8816ab389c2f366fd8a45603550ea9667": {"ta_keywords": "biomedical text mining;text mining deep;automatic biomedical text;lstm text mining;text mining training;text mining building;text mining answers;biomedical text;database deep reinforcement;articles knowledge automatic;text mining;efficiency biomedical text;text mining flexibility;mining deep network;learning massive knowledge;mining answers;mining deep;reliability text mining;pubmed selecting articles;scientific literature biological;knowledge automatic;association database deep;reliability biomedical text;database deep;massive knowledge collected;mining answers challenge;intelligent reader automatically;knowledge automatic human;behaviors querying pubmed;discovered automatic biomedical", "pdf_keywords": ""}, "5e327c2285ddf2a76d08e5c00d16c7358bc5412c": {"ta_keywords": "strategyproofing peer assessment;form peer assessment;peer assessment;peer assessment peer;peer assessment partitioning;peer assessment employees;assessment peer;strategyproofing peer;assessment peer grading;peer grading;papers peer assessment;strategyproofness strategyproofing peer;peer grading homeworks;individuals assigning evaluate;assignment evaluators submissions;evaluators submissions maximizes;provide dishonest evaluations;peer review;strategyproofness compromise assigned;assigned evaluators expertise;form peer;competition submissions evaluating;peer review scientific;evaluating provide dishonest;evaluators expertise;conference peer review;dishonest evaluations;dishonest evaluations increase;assignment evaluators;evaluators submissions", "pdf_keywords": "strategyproo\ufb01ng peer assessment;peer assessment partitioning;peer assessment;peer assessment peer;form peer assessment;papers peer assessment;peer grading;assessment peer grading;assessment peer;peer assessment employees;peer grading homeworks;algorithms strategyproof assignment;strategyproof assignment assignment;assignment quality guarantees;peer review;assignment assignment quality;strategyproo\ufb01ng peer;assignment quality;conference peer review;peer review scienti\ufb01c;review conference peer;peer review establish;peer;strategyproof assignment;form peer;conference peer;papers peer;algorithm empirical evaluation;grading homeworks grant;algorithms strategyproof"}, "f878a7c756b90c0ed612838492fbbc02ecaaab70": {"ta_keywords": "learning agent;solving problems interleaved;learns cognitive;learning agent examination;learns cognitive skills;simstudent tutored automatic;learning agent learns;agent learns cognitive;automatic tutors given;automatic tutors;tutored automatic;correction learning agent;tutors given problems;agent learns;learns;tutored automatic tutors;simstudent tutored;tutors;does solving problems;solving problems;problem solving experience;problems used teach;orders suggests learning;cognitive skills examples;tutors given;cognitive skills;learning;tutored;simstudent provide computational;examples problem solving", "pdf_keywords": ""}, "1ee276db29ba9127e81d9a7d9cb08f5138339412": {"ta_keywords": "distributed matrix multiplication;speeding distributed computing;coded matrix;coded matrix matrix;novel coded matrix;matrix multiplication scheme;coded computation;distributed matrix;distributed computing;coded computation addressing;large scale computation;parallel computing;distributed computing allows;distributed computing comes;parallel computing massive;redundant computing;codes straggler proofing;scale distributed matrix;matrix multiplication;enabling parallel computing;optimal computation communication;matrix matrix multiplication;introduction redundant computing;codes straggler;redundant computing combat;computing comes stragglers;matrix multiplication dimensional;computation communication costs;product codes straggler;dubbed coded computation", "pdf_keywords": ""}, "34cb1f081c1d1d6b3dc16a9278940a9ee85fb2e0": {"ta_keywords": "estimate interpreter performance;predict interpreter confidence;predicting simultaneous interpreter;simultaneous interpreter performance;interpreter performance;machine translation output;interpreter confidence;interpreter performance building;interpreter performance approximated;predict interpreter;qe machine translation;machine translation;methods predict interpreter;estimate interpreter;translation output methods;interpreter confidence adequacy;interpretation translation spoken;interpreter;simultaneous interpreter;simultaneous interpretation translation;translation output;computer assisted interpretation;pipeline estimate interpreter;translation spoken word;interpretation translation;assisted interpretation interfaces;quality estimation qe;interpretation strategy evaluation;accuracy simultaneous interpretation;translation spoken", "pdf_keywords": "estimate interpreter performance;estimating interpreter performance;predicting simultaneous interpreter;estimating interpreter;translation output experiments;machine translation output;qe machine translation;simultaneous interpreter performance;interpreter performance;interpreter performance building;interpreter performance turn;machine translation;assistance struggling interpreters;struggling interpreters;method estimating interpreter;evaluate interpreter output;interpreter performance approximated;struggling interpreters propose;machine translation mt;qe evaluate interpreter;interpreter output;interpreters;evaluate interpreter;interpreters propose task;simultaneous interpreter;interpreter;estimate interpreter;translation output;interpreters propose;interpreter output immediately"}, "839cbcf5c13d5875e952e40ec2da14b19eee2202": {"ta_keywords": "unconditional optimization;convex optimization;smooth convex optimization;solving unconditional optimization;unconditional optimization problems;gradient free methods;convex optimization problems;propose accelerated descent;accelerated gradient free;accelerated directional search;gradient free method;conditional optimization;proposed accelerated gradient;accelerated descent method;accelerated descent;approach accelerated gradient;gradient free;conditional optimization problems;norm prox structure;approach conditional optimization;norm prox;free method accelerated;optimization problems gradient;accelerated gradient;gradient available numerical;associated norm prox;gradient available;optimization;sparse distance;euclidean prox structure", "pdf_keywords": "nonsmooth convex optimization;stochastic nonsmooth convex;solving stochastic nonsmooth;stochastic nonsmooth;methods solving stochastic;convex optimization problems;convex optimization;nonsmooth convex;lagunovskaya gradient free;solving stochastic;free point methods;gradient free point;gradient free;stochastic;lagunovskaya gradient;optimization problems small;point methods solving;optimization problems;non random noises;point methods;free point;gasnikov lagunovskaya gradient;optimization;nonsmooth;convex;gradient;random noises autom;non random;noises autom;small non random"}, "ebcb425ed1a51e8f1a6eca422882abd454fe04f2": {"ta_keywords": "learners english vocabulary;information learning vocabulary;learning vocabulary introduce;assist esl learners;learning vocabulary;words learning link;english vocabulary addition;target words learning;vocabulary addition;words learning;english vocabulary;vocabulary introduce aimed;vocabulary introduce;lexical information collocations;information assist esl;lexical information;vocabulary addition word;grammar encyclopedic information;rich lexical information;second language learners;provide rich lexical;learners english;language learners english;vocabulary;target words;esl learners;assist esl;link grammar encyclopedic;grammar encyclopedic;set target words", "pdf_keywords": "vocabulary learning linggle;vocabulary learning;learning english1 provides;language l2 learners;vocabulary learning resulting;learning extract vocabulary;extract vocabulary encyclopedic;reading improve language;knowledge target vocabulary;learning content;extract vocabulary;assist esl learners;vocabulary encyclopedic information;paced vocabulary learning;improve language;vocabulary encyclopedic;l2 learners acquiring;prototype vocabulary learning;self paced vocabulary;language skills esl;learning linggle;target vocabulary present;improve language skills;vocabulary present prototype;assist second language;learning linggle booster;skills esl learners;esl learners;learning content rich;l2 learners"}, "4bff8862ba7956fdc2288e8399fb187b9595982b": {"ta_keywords": "gene mention recognition;gene mention task;corresponding gene mentions;results gene mention;gene mention;sentences corresponding gene;ii gene mention;systems identify substrings;identify substrings sentences;s2 genome;gene mentions;s2 genome biology;gene mentions present;presented results gene;mention recognition;identify substrings;results gene;biocreative ii gene;s2 s2 genome;s2 http genomebiology;substrings sentences corresponding;genomebiology com 2008;genomebiology com;corresponding gene;genome biology 2008;http genomebiology com;genomebiology;genome;ii gene;mention recognition demonstrate", "pdf_keywords": ""}, "d8aeb318f68f4635b34c72aa1a0369fadcd79450": {"ta_keywords": "topicspecific sentiments behaviors;user topic distribution;cbs topic model;user communities microblogging;topic model probabilistic;topic model cbs;topic model;topic distribution;communities microblogging;topicspecific sentiments;modeling topics;communities microblogging networks;associates topicspecific sentiments;modeling topics outperforms;topic distribution demonstrate;topics derive user;user experiments twitter;twitter datasets cbs;topics community;derive user communities;experiments twitter datasets;sentiments behaviors user;networks based sentiments;microblogging networks based;emotional topics community;microblogging networks;microblogging;topics community addition;derive user topic;adopt topic model", "pdf_keywords": ""}, "8442f9fd620ea34e1de3128b9388bddd1263f29b": {"ta_keywords": "infeasibility conic optimization;conic optimization minimization;method conic optimization;conic optimization named;conic optimization;conic optimization iterates;dual infeasibility conic;infeasibility conic;gradient method conic;conic constraints;integral projected gradient;optimization minimization convex;minimization convex quadratic;method conic;optimization minimization;order method conic;minimization convex;optimization named extrapolated;optimization;subject conic constraints;projected gradient method;optimality conditions generate;optimization named;projected gradient;minimization;gradient method expipg;primal dual optimality;dual optimality conditions;function subject conic;optimality conditions", "pdf_keywords": "conic optimization minimization;method conic optimization;conic optimization named;abstract conic optimization;conic optimization arxiv;conic optimization;conic constraints;integral projected gradient;gradient method conic;order method conic;convex objective;optimization minimization;optimization named;di\ufb00erentiable convex objective;optimization;optimization arxiv;projected gradient method;optimization arxiv 2108;convex objective function;method conic;subject conic constraints;dual gap constraint;optimization minimization di\ufb00erentiable;optimization named proportional;minimization;projected gradient;primal dual;minimization di\ufb00erentiable convex;novel primal dual;ensures primal dual"}, "ee24fb876e6f1b345d492101c499bc5dd6b8196b": {"ta_keywords": "eeg signal enhancement;probabilistic eeg signal;propose probabilistic eeg;eeg signal separation;probabilistic eeg;enhance eeg signals;observed eeg signals;eeg component probabilistic;eeg signals enhance;channel eeg signals;electroencephalogram eeg;signals enhance eeg;enhance eeg;eeg signals make;eeg signals;multi channel eeg;erps electroencephalogram eeg;eeg signals easily;observed eeg;blind eeg signal;eeg signal;channel eeg;electroencephalogram eeg used;proposed blind eeg;activities eeg signals;model observed eeg;brain activities eeg;erps electroencephalogram;performance eeg signal;improve performance eeg", "pdf_keywords": ""}, "ce0fce520c639af010c71cc6adf57cdeb2790322": {"ta_keywords": "optimization automatic speech;black box optimization;speech recognition;input speech recognition;automatic speech recognition;speech recognition asr;speech recognition performance;automatic speech;probabilistic black box;speech recognition experiments;box optimization automatic;vocabulary speech recognition;parameters input speech;input speech;optimization automatic;optimization using gaussian;recognition asr systems;bayesian optimization;bayesian optimization using;es bayesian optimization;box optimization performance;box optimization techniques;box optimization;recognition asr;optimization techniques covariance;effectiveness black box;black box;optimal systems automatically;gaussians hmms;process black box", "pdf_keywords": ""}, "516a0faeab9ec3a68bc6e7ec13a2df235a27ab52": {"ta_keywords": "deep network models;cnn model suitable;cnn model;unstructured clinical data;clinical notes prediction;deep network;features unstructured clinical;model learn semantic;dataset consisting clinical;cnn;learn semantic features;clinical data;imply cnn model;medical meaningful features;network model learn;results imply cnn;traditional deep network;neural network;convolutional neural network;neural network model;unstructured clinical notes;representing complex medical;semantic features unstructured;consisting clinical data;parameters traditional deep;unstructured clinical;electronic health records;health records;supporting diagnosis decision;proposed convolutional neural", "pdf_keywords": "networks medical diagnosis;clinical notes prediction;diagnosis prediction admission;medical diagnosis admission;diseases keywords convolutional;unstructured clinical data;neural networks medical;diagnosis admission notes;admission notes convolutional;features unstructured clinical;dataset consistsing clinical;discharge diagnosis prediction;clinical data;diagnosis admission;diagnosis prediction;supporting diagnosis decision;unstructured clinical notes;electronic health records;health records;learn semantic features;model learn semantic;consistsing clinical data;supporting diagnosis;neural network text;2017 convolutional neural;representing complex medical;unstructured clinical;keywords convolutional neural;health records ehrs;intensive care units"}, "d3793ae5b3b31f72605978b749e41811e6dcacd4": {"ta_keywords": "inverse reinforcement learning;inverse reinforcement;contextual bandit orchestrator;contextual bandit based;contextual bandit;uses inverse reinforcement;use contextual bandit;reward environment learn;society contextual bandit;reward maximizing constrained;reinforcement learning learn;learning learn maximize;agents maximize reward;maximize reward environment;demonstrations task reinforcement;actions reward maximizing;learn maximize environment;maximizing constrained policy;reinforcement learning;bandit based orchestrator;reward environment;reinforcement;maximize environment rewards;learn maximize;task reinforcement learning;agent able learn;bandit orchestrator allows;reward maximizing;allow agents maximize;task reinforcement", "pdf_keywords": "inverse reinforcement learning;inverse reinforcement;way inverse reinforcement;obtained inverse reinforcement;reinforcement learning demonstrations;uses inverse reinforcement;learning demonstrations;demonstrations task reinforcement;reinforcement learning learn;learning demonstrations humans;policies contextual bandit;reward maximizing policies;reward maximizing policy;reinforcement learning;agent learn policies;task reinforcement learning;learning learn maximize;learn maximize environment;reinforcement;maximizing policies contextual;task reinforcement;agent learn;contextual bandit based;maximize environment rewards;reinforcement learning used;contextual bandit;use contextual bandit;contextual bandit orchestrate;enables agent learn;learn maximize"}, "405c0d9b7cf5482d2e1197167f690e7b7801b9bd": {"ta_keywords": "hop question answering;pretraining qa generated;answering question generation;question answering;question generation explore;question generation;generates questions;generates questions selecting;question answering question;qa generated data;pretraining qa;respectively pretraining qa;human annotated training;qg generates questions;hop training data;generating relevant information;demand human annotated;qa generated;annotated training data;generated training data;annotated training;hop qa achieves;training data mqa;using generated training;hop qa;human annotated;mqa qg unsupervised;unsupervised multi hop;generated training;learning performance hybridqa", "pdf_keywords": "hop question generator;pretraining qa generated;question generator;unsupervised fewshot learning;hop qa unsupervised;hop qa pretraining;qa unsupervised shot;human annotated training;hop training data;fewshot learning;pretraining qa;qa pretraining qa;question generator mqa;questions help train;qa generated data;generated training data;unsupervised shot learning;qa pretraining;demand human annotated;multi hop training;unsupervised multi hop;annotated training;annotated training data;generated training;generate relevant information;multi hop qa;training data greatly;fewshot learning setting;qa generated;unsupervised fewshot"}, "7bf2620188c0a66e1d0e779083cf61960a2f3e2f": {"ta_keywords": "synthesis optimization combinational;synthesis optimization multilevel;synthesis optimization;logic timing constraints;automation synthesis optimization;optimization combinational logic;multilevel logic timing;generating combinational logic;correctness synthesis optimization;synthesis capable generating;synthesis capable;synthesis quality;optimization multilevel logic;timing constraints automation;logic timing;automation synthesis;capable generating combinational;combinational logic;timing constraints;defined timing constraints;synthesis quality measured;constraints automation synthesis;synthesis;multilevel logic;socrates synthesis capable;optimization combinational;logic given technology;generating combinational;combinational logic given;timing constraints imposed", "pdf_keywords": ""}, "d04c91bbb043666ebd6dae51995ee5bbc4291ddf": {"ta_keywords": "double man;double;man", "pdf_keywords": ""}, "b209f2acbf0fbc62a3fd19ad6c13abbd46547736": {"ta_keywords": "microsoft speaker diarization;voxceleb speaker recognition;speaker diarization voxceleb;diarization voxceleb speaker;speaker diarization;speaker recognition challenge;speaker recognition;speaker diarization monaural;speaker embedding extractor;res2net based speaker;track voxceleb speaker;microsoft speaker;voxceleb speaker;based speaker embedding;2020 microsoft speaker;speaker embedding;describes microsoft speaker;continuous speech separation;multi talker recordings;speech separation;speech separation leakage;recognition challenge voxsrc;talker recordings wild;talker recordings;based speaker;challenge voxsrc;diarization output voting;voxsrc 2020 microsoft;challenge voxsrc 2020;diarization track voxceleb", "pdf_keywords": "microsoft speaker diarization;speaker diarization;speaker diarization voxceleb;speaker recognition challenge;speaker diarization monaural;voxceleb speaker recognition;speaker recognition;challenge microsoft speaker;speaker embedding extractor;diarization voxceleb speaker;microsoft speaker;describes microsoft speaker;based speaker embedding;speaker embedding;res2net based speaker;continuous speech separation;multi talker recordings;speech separation leakage;track voxceleb speaker;speech separation;talker recordings wild;talker audio;multi talker audio;talker recordings;talker audio collected;voxceleb speaker;recognition challenge voxsrc;diarization output voting;based speaker;diarization track challenge"}, "9e3e6ddf958c2005f7041cc9dd5fe050a0dbd02e": {"ta_keywords": "multiscale wavelet transform;wavelet transform;wavelet transform idea;points wavelet transform;wavelet transform mwt;multiscale wavelet;result multiscale wavelet;using wavelet transform;crossing points wavelet;signal correspondence method;wavelet transform domain;correspondence result multiscale;multiple resolution analysis;quality signal correspondence;signal correspondence;analysis using wavelet;points wavelet;signal correspondence process;correspondence signals scanning;resolution analysis using;function signal correspondence;wavelet;correspondence signals accuracy;using wavelet;signals accuracy correspondence;correspondence signals;resolution analysis;point transform signal;applied correspondence signals;crossing point transform", "pdf_keywords": ""}, "4ec1d3407a5136c525b53f703c803571200902a4": {"ta_keywords": "hmm\u97f3\u58f0\u5408\u6210\u306b\u304a\u3051\u308b\u5206\u6563\u5171\u6709\u30d5\u30eb\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30e2\u30c7\u30eb\u306b\u3088\u308bf0\u30d1\u30bf\u30fc\u30f3\u751f\u6210 \u4e00\u822c\u30bb\u30c3\u30b7\u30e7\u30f3;hmm\u97f3\u58f0\u5408\u6210\u306b\u304a\u3051\u308b\u5206\u6563\u5171\u6709\u30d5\u30eb\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30e2\u30c7\u30eb\u306b\u3088\u308bf0\u30d1\u30bf\u30fc\u30f3\u751f\u6210 \u4e00\u822c\u30bb\u30c3\u30b7\u30e7\u30f3 \u97f3\u58f0;hmm\u97f3\u58f0\u5408\u6210\u306b\u304a\u3051\u308b\u5206\u6563\u5171\u6709\u30d5\u30eb\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30e2\u30c7\u30eb\u306b\u3088\u308bf0\u30d1\u30bf\u30fc\u30f3\u751f\u6210;\u4e00\u822c\u30bb\u30c3\u30b7\u30e7\u30f3 \u97f3\u58f0;\u4e00\u822c\u30bb\u30c3\u30b7\u30e7\u30f3 \u97f3\u58f0 \u8a00\u8a9e;\u4e00\u822c\u30bb\u30c3\u30b7\u30e7\u30f3;\u5bfe\u8a71;\u97f3\u58f0;\u5bfe\u8a71 \u4e00\u822c;\u97f3\u58f0 \u8a00\u8a9e \u5bfe\u8a71;\u97f3\u58f0 \u8a00\u8a9e;\u8a00\u8a9e \u5bfe\u8a71 \u4e00\u822c;\u8a00\u8a9e \u5bfe\u8a71;\u4e00\u822c;\u8a00\u8a9e", "pdf_keywords": ""}, "f2818da69bb72526fff9d601677db38f24a62ecc": {"ta_keywords": "user satisfaction prediction;predicts user satisfaction;improving user satisfaction;satisfaction prediction methods;based dialogue modeling;dialogue modeling;dialogue modeling ebdm;satisfaction prediction prediction;user satisfaction adapt;based satisfaction prediction;selecting response utterances;dialog example databases;satisfaction prediction;responses user feedback;response candidates prediction;example based dialog;user satisfaction;user satisfaction select;estimate user satisfaction;propose response selection;user satisfaction score;user feedback response;based dialog;methods user satisfaction;response utterances;dialog example;user query response;example based dialogue;dialog;improve dialogue quality", "pdf_keywords": ""}, "febb305a854d02b138250a8a19af956ffa0ada4f": {"ta_keywords": "gradient dynamics games;linear quadratic games;quadratic games existence;convergence nash equilibria;policy gradient algorithms;quadratic games satisfy;quadratic games;gradient avoid nash;nash equilibria continuous;global nash equilibrium;local convergence nash;convergence nash;policy gradient;policy gradient avoid;avoid nash equilibrium;convergence multi agent;nash equilibrium generate;global nash;counterexample policy gradient;solving reinforcement learning;nash equilibrium;unique global nash;nash equilibrium solving;reinforcement learning setting;classic reinforcement learning;equilibria continuous action;gradient algorithms guarantees;reinforcement learning problems;games satisfy conditions;dynamics games", "pdf_keywords": ""}, "f21a9d70319ca99227300349d7bcab5dee5869cd": {"ta_keywords": "incomplete multilingual corpus;incomplete multilingual corpora;machine translation missing;multilingual corpora complete;missing source translations;corpus translations missing;multilingual corpus translations;machine translation nmt;translation missing data;incomplete multilingual;incomplete corpora training;translations missing practice;use incomplete multilingual;neural machine translation;real incomplete multilingual;using incomplete multilingual;multilingual corpus;multilingual corpora multi;multilingual corpora;incomplete corpora;practice multilingual corpora;use incomplete corpora;corpus translations;translations missing;translation nmt using;missing practice multilingual;source translations;translation missing;machine translation;multilingual corpora ted", "pdf_keywords": "incomplete multilingual corpus;incomplete multilingual corpora;multilingual corpus missing;incomplete multilingual;using incomplete multilingual;use incomplete multilingual;actual incomplete multilingual;multilingual corpus;multilingual corpora multi;multilingual corpora;multilingual corpora simple;multilingual corpus uses;missing source translations;nmt using incomplete;represent missing sentences;multilingual;translations training;approaches multisource nmt;multisource nmt using;available translations training;nmt experts better;source nmt using;multisource nmt;multi encoder nmt;corpus missing input;corpora multi;corpora multi encoder;missing input sentences;corpus missing;translations training test"}, "b168fc72fa39e9669567bd099bab179549a15e14": {"ta_keywords": "acl a\u03b22gpi antibodies;a\u03b22gpi antibodies evaluation;a\u03b22gpi antibodies clinical;\u03b22glycoprotein iga antibodies;\u03b22glycoprotein a\u03b22gpi antibodies;a\u03b22gpi antibodies;igm acl a\u03b22gpi;anti \u03b22glycoprotein a\u03b22gpi;antibodies evaluation aps;\u03b22glycoprotein a\u03b22gpi;association a\u03b22gpi iga;anti \u03b22glycoprotein iga;acl a\u03b22gpi;aps thrombosis;related aps thrombosis;\u03b22glycoprotein iga;a\u03b22gp1 iga;a\u03b22gpi iga;aps thrombosis 28;iga antibodies;a\u03b22gp1 iga useful;iga antibodies diagnosis;association a\u03b22gpi;confirmed a\u03b22gp1 iga;detection apl iga;significant association a\u03b22gpi;determination iga acl;a\u03b22gpi iga directed;a\u03b22gpi;a\u03b22gp1 iga exploration", "pdf_keywords": ""}, "de43afd166a79c24b3a7dd16c5695059d9f0aa71": {"ta_keywords": "children understand transitivity;child piaget theory;transitivity understood children;cognitive development child;cognitive development;overview cognitive development;physical analogy train;age child piaget;learning versus age;child infancy;cognitive learning versus;understood children age;development child infancy;concept transitivity understood;child infancy adulthood;concept transitivity;infancy;children understand;versus age child;help children understand;children using physical;cognitive learning;child piaget;analogy train shown;timescale cognitive learning;cognitive;infancy adulthood;overview cognitive;age child;abstract concept transitivity", "pdf_keywords": ""}, "ab94fae3d49cd7016a47020469dc257d8090f5bb": {"ta_keywords": "speaker separation deep;separation deep clustering;speaker separation db;improvement speaker separation;db speaker separation;challenging speech separation;multi speaker separation;speaker separation using;speaker separation;speech separation;separation using deep;deep clustering extend;deep clustering;baseline db speaker;separation deep;using deep clustering;deep clustering recently;training clustering enhancement;channel multi speaker;db speaker;sdr improvement speaker;performance challenging speech;improvement speaker;multi speaker;deep learning;clustering enhancement stages;clustering enhancement;separation db;deep learning architecture;end training clustering", "pdf_keywords": "challenging speech separation;speaker separation db;speech separation extend;improvement speaker separation;training clustering enhancement;deep clustering;speech separation;db speaker separation;extended deep clustering;quality metrics speech;speaker separation;deep clustering framework;metrics speech recognition;training signal reconstruction;clustering enhancement stages;baseline db speaker;performance challenging speech;clustering enhancement;end training clustering;speech recognition;sdr improvement speaker;speaker separation signi\ufb01cant;automatic speech recognition;speech recognition error;automatic speech;improvement speaker;signal reconstruction quality;metrics speech;using automatic speech;incorporating better regularization"}, "6c477a65f0922d405c3665e31581eaa0f269116e": {"ta_keywords": "distributional relational semantics;relational objective wordnet;analogy tests parsing;objective wordnet preliminary;objective wordnet;wordnet preliminary results;parsing word representations;wordnet;word representations;relational semantics;wordnet preliminary;hypothesis word representations;word representations trained;text relational objective;knowledge base completion;text relational;semantics;parsing word;incorporate distributional relational;analogy tests;raw text relational;distributional relational;completion analogy tests;word representations ought;tests parsing word;knowledge base;parsing;relational objective;analogy;results knowledge base", "pdf_keywords": "distributional semantics objective;objectives distributional semantics;distributional semantics using;distributional semantics;implement distributional semantics;wordnet joint objective;objective wordnet joint;semantics using neural;relational objective wordnet;objective wordnet;distributional relational objectives;relational objectives distributional;wordnet joint;distributional objective raw;semantics objective;neural language model;optimizes distributional objective;distributional relational;semantics objective implement;neural language;wordnet;text relational objective;optimizes distributional;distributional objective;objectives distributional;language model nlm;vector representations words;\ufb02exibly optimizes distributional;objective implement distributional;using neural language"}, "10085f7fb0871329d34529cc54df0a8f75756fce": {"ta_keywords": "performance automatic speech;automatic speech;automatic transcription japanese;speech recognition asr;automatically generate spoken;automatic speech recognition;automatic transcription;update automatic transcription;generate spoken style;generate spoken;speech recognition;spoken style training;update acoustic language;acoustic language models;framework asr japanese;recognition asr;speaking style transformation;transcription japanese;transcription japanese national;recognition asr systems;training texts labels;asr japanese;asr accuracy based;verbatim transcripts framework;training texts;asr japanese national;transcripts framework;spoken style;speaking style;congress update acoustic", "pdf_keywords": ""}, "248824ec5d9b4ddf0c36cdc51b6b57af6e881328": {"ta_keywords": "transfer languages ranking;transfer languages better;cross lingual learning;good transfer languages;nlp low resource;transfer languages cross;choosing transfer languages;transfer languages;different nlp tasks;optimal transfer languages;representative nlp tasks;cross lingual transfer;transfer language;languages cross lingual;nlp tasks;languages ranking;nlp tasks inform;lingual transfer;transfer language used;lingual learning;performance natural language;nlp tasks demonstrate;processing nlp low;language processing nlp;cross lingual;languages ranking problem;lingual transfer high;nlp low;low resource languages;languages cross", "pdf_keywords": "transfer nlp tasks;lingual transfer nlp;different nlp tasks;nlp tasks;representative nlp tasks;machine translation entity;nlp tasks inform;transfer nlp;speech tagging dependency;nlp tasks machine;translation entity linking;tasks machine translation;tagging dependency parsing;linking speech tagging;language processing nlp;nlp tasks demonstrate;entity linking speech;nlp techniques low;better transfer languages;speech tagging;good transfer languages;language dataset features;transfer languages better;machine translation;dependency parsing train;features dependency parsing;informative different nlp;processing nlp;dependency parsing task;natural language processing"}, "70170035ef870df1c064cc52804178a52f6a69ef": {"ta_keywords": "incremental learning text;text question answering;learning text question;incremental learning;question answering;learning text;incremental;answering;learning;text question;text;question", "pdf_keywords": ""}, "75c4aefc55bf0b345587740cad0a4e994f29962a": {"ta_keywords": "sound event detection;sound event activity;segments sound event;sound activity detection;polyphonic sound event;sound events;sound activity;sound event;sound event dependent;model sound event;using sound activity;sound events occurs;model hmm frame;identify segments sound;recurrent neural network;processing using sound;sequence sequence detection;sequence detection;model hmm;short term memory;hidden markov;approach polyphonic sound;segments sound;recurrent neural;approach polyphonic;based hidden markov;hybrid approach polyphonic;markov model hmm;errors sound events;hidden markov model", "pdf_keywords": ""}, "cc19de8d0782917098029ed20261cbe0b0c62bf5": {"ta_keywords": "biases review text;detects biases review;biases review ratings;biases text peer;quantify biases text;biases review;quantify bias text;biases text;biases text written;biases evidence biases;evidence biases review;bias text;biases evidence;bias text caused;evidence biases;societal biases evidence;framework quantify bias;framework quantify biases;quantify bias;quantify biases;estimated bias;bias;societal biases;biases;detects biases;text peer reviews;link estimated bias;bias visibility;bias visibility subgroup;change biases text", "pdf_keywords": "biases review text;detects biases review;biases review ratings;biases text peer;quantify biases text;dataset peer reviews;quantify bias text;text peer reviews;biases review;biases text;evidence biases review;bias text;peer reviews;truth bias;double blind reviewing;blind reviewing policy;truth bias inferred;bias text caused;evidence biases;blind reviewing;bias inferred evaluate;blind reviewing 2018;bias identify;bias visibility subgroup;bias inferred;review text;peer reviews international;peer reviews reputed;bias visibility;review ratings"}, "8484fdb56e4690927dc0191ede11c2d24bc5e2ef": {"ta_keywords": "text generation models;distribution text generation;text generation model;text generation directly;text generation;modern text generation;ended text generation;generated text scales;divergences quantized embedding;text using divergence;generation tasks;generation tasks mauve;generation models;learnt distribution text;generated text;neural text;neural text human;distribution human written;generation directly compares;generation model distribution;distributional evaluation metrics;compares learnt distribution;generation directly;generation models computing;learnt distribution;information divergences;gap neural text;computing information divergences;existing distributional evaluation;generation", "pdf_keywords": "generated text human;machine generated text;machine human text;distribution text generation;text generation model;text generation directly;distinguish human text;text generation;human text results;ended text generation;distribution human written;human text;text generation introduce;text human text;text human;masked language model;generated text;generated text respect;language model bert;learnt distribution text;human written text;text lower discrimination;machine generated;compares learnt distribution;trained discriminate machine;discrimination accuracy variety;based masked language;text length model;language model;generation harder distinguish"}, "aff5d7f43823e06bb68220db41de3bc82e2f3990": {"ta_keywords": "cellular networks throughput;tier cellular networks;tier poisson network;networks high mobility;cellular networks;doubly stochastic poisson;stochastic poisson;poisson network model;cell networks high;networks throughput;small cell networks;poisson network;cell networks;data rate mobile;networks throughput maximization;stochastic poisson process;tier cellular;frequent handoff;heterogeneous tier network;high mobility users;downlink data rate;mobile moving users;network model;mobile moving;maximization static mobile;handoff outage;tier poisson;static mobile users;periods tier cellular;average throughput types", "pdf_keywords": ""}, "45cdf5e239a1f0057c350f6654ccd348fb4e2332": {"ta_keywords": "uncertainty agents preferences;stable matching uncertain;uncertainty lottery model;models uncertainty lottery;uncertain linear preferences;matching setting uncertainty;matching uncertain;uncertainty lottery;stable matching;stable stable matching;stable matching setting;matching uncertain linear;model lottery preference;matching highest probability;distribution linear preferences;sided stable matching;preferences compact indifference;lottery preference profiles;probability given matching;probability stable;probability stable stable;lottery preference;highest probability stable;linear preferences;weak preference order;setting uncertainty agents;indifference model agent;probability model lottery;linear preferences compact;indifference model", "pdf_keywords": "matchings independent uncertainty;stable matchings independent;stable matchings;stable matching matching;stable matching;certainly stable matchings;compute stable matching;matchings independent;models uncertainty lottery;probability stable existscertainlystablematching;uncertainty lottery model;matching highest probability;matching matching agents;preference pro\ufb01les models;matching \ufb01nding matching;\ufb01nding matching;probability given matching;uncertainty lottery;matching \ufb01nding;probability stable;\ufb01nding matching highest;given matching \ufb01nding;stable marriage problem;matchings;matching agents prefer;highest probability stable;matching matching;matching agents;given matching;model lottery preference"}, "4731f89169604cd0d8b5352380baa1b4728bca0b": {"ta_keywords": "translation error analysis;machine translation error;machine translation mt;error analysis discriminative;machine translation;tool machine translation;discriminative language models;translation mt systems;regularized discriminative language;translations analyzing;analysis discriminative language;correct translations analyzing;analysis machine translation;error analysis mt;translations analyzing correct;translation error;discriminative language;error analysis machine;language models lms;mt error analysis;frequently erroneous word;use regularized discriminative;language models;occur correct translations;correct translations;translations;regularized discriminative;language models tool;error analysis;frequently erroneous", "pdf_keywords": ""}, "601408d6617bf72894c9f41ae54cf9c17905903a": {"ta_keywords": "string machine translation;efficient accurate translation;machine translation results;machine translation mt;machine translation;inferior machine translation;t2s translation theoretically;string t2s translation;tree string t2s;t2s translation;translation mt methods;tree string machine;accurate tree string;parsing alignment search;systems including parsing;translation results;phrase based hierarchical;string t2s;hierarchical phrase based;accurate translation previous;parsing alignment;including parsing alignment;tree string;translation theoretically holds;based hierarchical phrase;translation theoretically;translation results indicate;translation previous reports;accurate translation;accuracy t2s", "pdf_keywords": ""}, "3dc20be709818630e2249ab28b35b0666b4b544d": {"ta_keywords": "paralinguistic information languages;analyze paralinguistic information;paralinguistic information focus;paralinguistic information;translate content utterance;ignore paralinguistic information;bilingual corpora;paralinguistic information included;speech s2s translation;types paralinguistic information;analyze paralinguistic;english bilingual corpora;emphasis japanese english;utterance ignore paralinguistic;various types paralinguistic;paper analyze paralinguistic;ignore paralinguistic;emphasis japanese;speech speech s2s;translation systems;s2s translation systems;translation systems translate;types paralinguistic;japanese english bilingual;paralinguistic;speech s2s;english bilingual;translate content;s2s translation;analysis emphasis japanese", "pdf_keywords": ""}, "ead6323f137c2f99ef0ffcfa34fa6eb1c6eca3c6": {"ta_keywords": "chime speech separation;speech separation recognition;unsegmented multispeaker speech;speech recognition track;speech recognition unsegmented;track unsegmented multispeaker;multispeaker speech recognition;chime challenge segmented;challenge segmented multispeaker;segmented multispeaker speech;tackle unsegmented multispeaker;multi microphone conversational;recognition challenge chime;unsegmented multispeaker;diarization speech recognition;speech diarization recognition;multispeaker speech;6th chime speech;speech separation;recognition unsegmented recordings;previous chime challenge;microphone conversational;conversational speech diarization;tackling multispeaker speech;microphone conversational speech;speech recognition modules;speech recognition scenario;challenge tackling multispeaker;enhancement speaker diarization;chime challenge tackling", "pdf_keywords": "unsegmented multispeaker speech;track unsegmented multispeaker;speech recognition track;multispeaker speech recognition;tackle unsegmented multispeaker;challenge segmented multispeaker;segmented multispeaker speech;speech recognition unsegmented;unsegmented multispeaker;multispeaker speech;challenge tackling multispeaker;tackling multispeaker speech;speech recognition;speech recognition scenario;proposed challenge tracks;speech separation recognition;speech recognition modules;recognition unsegmented recordings;diarization speech recognition;chime challenge segmented;challenge tracks;chime speech separation;unsegmented recordings 1shinji;segmented multispeaker;recognition challenge chime;speaker diarization;speaker diarization speech;multispeaker;diarization speech;unsegmented recordings"}, "af5c4b80fbf847f69a202ba5a780a3dd18c1a027": {"ta_keywords": "grounded commonsense inference;language inference commonsense;commonsense inference unifying;commonsense inference;inference commonsense;inference commonsense reasoning;natural language inference;commonsense reasoning;task grounded commonsense;grounded commonsense;aggressive adversarial filtering;commonsense reasoning account;aggressive adversarial;language inference;account aggressive adversarial;inference unifying natural;commonsense;datasets propose adversarial;unifying natural language;adversarial;adversarial filtering af;adversarial filtering;propose adversarial;language models massively;biases existing datasets;propose adversarial filtering;inference unifying;stylistic classifiers;biased dataset;adversarial filtering use", "pdf_keywords": "aggressive adversarial \ufb01ltering;challenging testbed nli;nli models dataset;aggressive adversarial;testbed nli models;account aggressive adversarial;grounded commonsense inference;adversarial \ufb01ltering;datasets propose adversarial;nli models;language models massively;challenge grounded commonsense;adversarial \ufb01ltering use;adversarial filtering;testbed nli;adversarial;adversarial filtering af;commonsense inference easy;commonsense inference;language models;biases existing datasets;propose adversarial filtering;art nli models;biased dataset;propose adversarial;human biases;human biases existing;theart language models;training ensemble stylistic;constructs biased dataset"}, "d15eb5744474cec2d0634651bb30000b3873a309": {"ta_keywords": "time expression normalization;normalization rules training;rules time expressions;expressions research normalization;recognition time expressions;normalization rules time;expression normalization;time expressions research;time expressions common;generate normalization rules;automatic rule generation;expression normalization paper;generates normalization rules;normalized temporal;expression normalization sequence;time expressions;normalization rules;rule generation time;automatic rule;automatically generate normalization;rules grammars designed;construct normalized temporal;normalization;normalization sequence operations;rules grammars;rules training data;normalized;rule generation;normalized temporal value;time expression", "pdf_keywords": "normalization rules training;time expression normalization;tweets benchmark achieves;methods tweets benchmark;tweets benchmark;generating normalization rules;time expression recognition;artime generating normalization;rules time expressions;generate normalization rules;rule generation time;normalization rules ranking;automatic rule generation;obtains normalization rules;automatically generate normalization;expression normalization;normalized temporal;expression normalization operation;normalization rules;sota methods tweets;generating normalization;artime time expression;construct normalized temporal;expression normalization sequence;normalization speci\ufb01cally artime;rules training data;time expressions common;tempeval benchmark end;tempeval benchmark;tempeval benchmark present"}, "3b563c16e9a918631d63a20027dad735b625625a": {"ta_keywords": "text generation achieved;text generation tasks;text generation;toolbox text generation;machine translation texar;set text generation;versatile texar modularized;texar modularized versatile;translation texar designed;flexible versatile texar;versatile texar;texar modularized;generation tasks;extensible toolbox text;machine translation;translation texar;source toolkit;texar open source;neural machine translation;open source toolkit;generation achieved abstracting;versatile extensible toolbox;generation achieved;existing toolkits;introduce texar;toolkits;existing toolkits specialized;texar designed highly;toolkit;texar", "pdf_keywords": "text generation toolkit;toolkit text generation;text generation tasks;text generation applications;purpose text generation;text generation;support text generation;generation toolkit;extensible toolkit text;especially text generation;natural language machine;text generation zhiting;purpose toolkit tensorflow;machine translation;language machine translation;machine translation summarization;general purpose toolkit;general purpose text;toolkit text;set text generation;purpose toolkit;toolkit tensorflow;extensible toolkit;natural language;texar general purpose;generation tasks;2019 texar modularized;texar modularized versatile;versatile extensible toolkit;abstract introduce texar"}, "a8372f7cb2e482a455b06c3e47f65aec5c7a924b": {"ta_keywords": "induction pump alip;em pump design;molten lead bismuth;liquid lead bismuth;pump efficiency circulation;lead bismuth target;induction pump;pump design experimental;linear induction pump;numerical study pump;study pump efficiency;pump efficiency conducted;pump alip numerical;lead bismuth eutectic;pump design;pump efficiency;future em pump;low pump efficiency;lead bismuth;bismuth target circuit;accelerator driven ads;efficiency circulation liquid;bismuth target;study pump;bismuth eutectic lbe;circulation liquid alloy;pump alip;em pump;mw accelerator driven;power mw accelerator", "pdf_keywords": ""}, "0c07cc7ba1b862556f5cfee0d5d849866d21a693": {"ta_keywords": "oblivious updates storage;communication oblivious updates;capacity oblivious updates;oblivious update process;oblivious updates;necessary oblivious update;oblivious update;oblivious updates terms;linear update protocols;limits communication oblivious;communication necessary oblivious;communication oblivious;establish capacity oblivious;capacity oblivious;updates storage networks;update protocols investigate;update protocols;lower bound communication;update algorithm meets;updated nodes access;stale node needs;communication necessary storage;bound communication;necessary oblivious;updates storage;bound communication necessary;data updated nodes;stale node access;stale data information;giving updated nodes", "pdf_keywords": "oblivious updates storage;communication oblivious updates;linear update protocols;oblivious update process;abstract distributed storage;updates storage networks;storage networks;systems storage nodes;communication necessary storage;distributed storage;storage nodes;lower bound communication;oblivious updates;distributed storage systems;perform oblivious update;oblivious update stale;limits communication oblivious;necessary oblivious update;updates storage;stale node needs;bound communication;oblivious update;storage code linear;storage networks preetum;bound communication necessary;necessary storage code;replication stale node;update protocols;storage nodes intermittently;update algorithm meets"}, "9650dbe79d34498113371770dcdb48f1bd7c9711": {"ta_keywords": "topic map;topic map help;creation topic map;information retrieval;term similarity creation;research papers words;heatmaps visualize;term extraction;maps computer science;visual exploration research;heatmaps visualize profile;term extraction using;help heatmaps visualize;term similarity;extensible term extraction;heatmap;word phrase similarity;heatmaps;methods information retrieval;natural language processing;phrase similarity;similarity creation topic;visual exploration;research topics field;map help heatmaps;phrase similarity calculated;words phrases paper;titles cities map;similarity creation;papers words phrases", "pdf_keywords": "topic map;visualization large relational;creation topic map;topic map novel;maps based papers;functional visualization large;functional visualization;maps computer science;large scale bibliographic;fully functional visualization;information retrieval;term similarity creation;bibliographic data;interactive visualization;bibliographic data natural;visualization making possible;interactive visualization making;data natural language;practical approach visualizing;database maps based;term extraction;visualization uses;extensible term extraction;natural language processing;database maps;explorative visualization uses;functional visualization mocs;term extraction using;visualization making;explorative visualization"}, "889c3b4394826639d483c039467cd9a05e68e73c": {"ta_keywords": "learning models music;composing piece music;models music;write music nonlinear;ancestral sampling based;gibbs sampling greatly;models music typically;composing piece;human composers;human composers write;previously gibbs sampling;write music;ancestral sampling;composers write music;composition chronological;composition chronological process;blocked gibbs sampling;gibbs sampling;partial musical scores;music typically;gibbs sampling analogue;sampling analogue rewriting;piece music;music nonlinear;samples ancestral sampling;musical scores explore;partial musical;composers write;composers;music nonlinear fashion", "pdf_keywords": "learning models music;generative model musical;modeling musical scores;scores counterpoint convolution;model musical counterpoint;composing piece music;coconet deep convolutional;models music;modeling musical;coconet deep;introduce coconet deep;approach modeling musical;trained reconstruct partial;models music typically;composing piece;reconstruct partial scores;convolutional model trained;musical counterpoint;musical counterpoint form;model musical;piece music;musical scores;deep convolutional model;convolutional approach modeling;composition chronological;musical scores based;model trained reconstruct;coconet;introduce coconet;generative model"}, "68af273e04906e0450a5d01d5606c8313da01453": {"ta_keywords": "sensor subset selection;optimal sensor subset;subset possible sensors;sensor subset;sensors estimation fusion;sensor selection;sensor selection problem;estimation large sensor;developed sensor subset;distribution optimal sensing;sensors estimation;optimal sensor;optimal sensing data;employed optimal sensor;gibbs sampling based;possible sensors estimation;sensing data estimation;estimation fusion;sensor networks;gibbs sampling;optimal sensing;large sensor network;estimation fusion center;sensor networks necessitates;scale sensor networks;sensor network numerical;practical sensor networks;problem gibbs sampling;conjunction gibbs sampling;subset selection exploiting", "pdf_keywords": "estimation large sensor;gibbs sampling stochastic;sensing data estimation;based gibbs sampling;proposed gibbs sampling;sampling stochastic approximation;gibbs sampling based;optimal sensing data;sensors estimation fusion;gibbs sampling;sensors estimation;gibbs sampling derived;optimal sensing;presented gibbs sampling;conjunction gibbs sampling;exploiting stochastic approximation;possible sensors estimation;algorithm based gibbs;large sensor network;sampling stochastic;estimation fusion;sensor networks;ef\ufb01cient data estimation;approximation conjunction gibbs;sampling derived constrained;scale sensor networks;subset possible sensors;unconstrained optimization estimation;sensors proved convergence;stochastic approximation"}, "04f8f739924a19c01d196a48783b914554ac0fe5": {"ta_keywords": "hessian convex optimization;composite convex optimization;algorithms composite convex;minimizing convex functions;convex optimization;continuous hessian convex;hessian convex;domain newton methods;counter minimizing convex;minimizing convex;convex optimization called;lipschitz continuous hessian;continuous hessian;second order algorithms;hessian;newton methods algorithms;convex optimization based;composite convex;gradient method variant;contracting domain newton;newton methods;second order generalization;convex functions lipschitz;functional residual iteration;optimization based global;conditional gradient method;convex;generalization conditional gradient;convex functions;domain newton", "pdf_keywords": "composite convex optimization;second order optimization;algorithms composite convex;convex optimization;convex optimization called;contractingdomain newton method;domain newton methods;aggregating newton method;empirical risk minimization;convex optimization based;domain newton method;risk minimization;second order algorithms;newton method algorithm;algorithm aggregating newton;optimization called contracting;gradient method variant;order optimization;order optimization algorithm;risk minimization problem;optimization algorithm called;conditional gradient method;contracting domain newton;generalization conditional gradient;newton method section;newton methods incorporate;newton methods;newton method;second order generalization;composite convex"}, "86ae1161026f23f9df691a867fd7453cee56fd28": {"ta_keywords": "change meaning phylogenies;lexicon particularly change;studying semantic change;studying change lexicon;semantic change;lexical cognate models;lexical phylogenetic inference;lexical phylogenetic;semantic change rely;change lexicon;change lexicon particularly;lexical cognate;appropriate lexical phylogenetic;meaning phylogenies;language change;phylogenies;language change point;stratified word embeddings;temporally stratified word;approach language change;lexical;illustrations lexical cognate;particularly change meaning;phylogenetic inference particularly;phylogenetic inference;lexicon particularly;change meaning;phylogenetic;lexicon;variation studying change", "pdf_keywords": "semantic change phylogeny;studying semantic change;semantic change;semantic shifts lexical;semantic change semantic;lexicon particularly change;studying change lexicon;semantic shifts;study semantic change;change semantic stability;change semantic;change meaning phylogenies;modeling semantic shifts;identifying meaning change;semantic change macro;approaches language change;change lexicon;change lexicon particularly;phylogeny identifying lexical;shifts lexical;shifts lexical replacement;lexical stability;identifying lexical stability;evolutionary approaches language;lexical replacement;semantic stability variation;semantic stability;language change;change phylogeny identifying;reconstructing meaning identifying"}, "5b8eaaf660b9e2d6a19886991350fffa1320b372": {"ta_keywords": "learn entities relations;learn entities;identifying entities relations;entities relations sentences;approaches identifying entities;identifying entities;classifiers ilp inference;entities relations;inference based training;classifiers;classifiers ilp;strategies learn entities;classifiers study;classifiers study examine;local classifiers ilp;relations sentences;classifiers second;entities;classifiers second uses;uses local classifiers;local classifiers study;just local classifiers;local classifiers;ilp inference uses;lastly local classifiers;local classifiers second;relations sentences compare;sentences;followed local classifiers;inference uses", "pdf_keywords": ""}, "781e0e81834119c135091c8bdfcd1966c10b09ab": {"ta_keywords": "speed integer compression;integer compression;processing integers slow;integer compression schemes;cpus techniques conjunctive;integers slow effort;million query track;integers slow;queries double speed;trec million query;optimize intersection posting;compression schemes exploit;modern cpus techniques;compression schemes;queries;retrieval conference trec;simd instruction compare;fact simd instruction;subsequent processing integers;instruction multiple data;million query;lists introduce simd;intersection posting lists;retrieval;processing integers;cpus techniques;simd instructions ubiquitous;text retrieval;data simd instructions;trec text collections", "pdf_keywords": "128 bit simd;cpus techniques conjunctive;bit simd vectors;simd instructions ubiquitous;bit simd;approach 128 bit;instruction bsr bit;bit scan;trec million query;queries double speed;modern cpus techniques;million query track;128 bit;using simd;simd instructions;techniques conjunctive queries;using simd instructions;bit scan reverse;logs trec million;bsr bit scan;million query;queries;cpus techniques;using logs trec;simd vectors;logs trec;simd;simd vectors introduce;bsr bit;instruction bsr"}, "65f632cbac465633a13b1e3f8c8c410c2f3aec3d": {"ta_keywords": "multi agent reinforcement;agent reinforcement;agent reinforcement learning;reinforcement learning marl;learns better policy;deterministic policy gradient;multi agent deep;agent deep deterministic;stackelberg multi agent;reinforcement learning;deep deterministic policy;policy gradient;reinforcement;simple competitive robotics;policy gradient st;competitive robotics;stackelberg game;learning emergent behaviors;competitive multi agent;agent deep;stackelberg maddpg learning;total policy gradient;problem stackelberg game;competitive robotics environment;policy gradient meaning;multi agent;follower hierarchical interaction;policy exploiting;stackelberg game player;policy exploiting information", "pdf_keywords": ""}, "76862a851bd2c17dcf6bfc2cecbf4af186730123": {"ta_keywords": "segmentation grayscale document;segmenting nontext objects;segmenting nontext;text object segmentation;method segmenting nontext;grayscale document images;grayscale document image;segmentation grayscale;segmenting halftone images;object segmentation grayscale;grayscale document;document images maxtree;document images;directly grayscale document;segmenting;segmentation;document image;solution aimed segmenting;object segmentation;images maxtree structure;unconventional method segmenting;segmenting halftone;aimed segmenting;document image making;thresholding;aimed segmenting halftone;images maxtree;objects directly grayscale;nontext objects;thresholding method connected", "pdf_keywords": ""}, "1a20d6c6891f3a0462515ff9560bc37e66eb422a": {"ta_keywords": "ismw fruct conference;fruct conference;conference hosted;conferences international school;conferences international;itmo university known;itmo university;state university itmo;university itmo university;fruct conference time;university itmo;conferences;conference;events conference hosted;ismw fruct;petersburg state university;conference hosted ground;events conference;time conferences international;cooperation universities globe;ainl ismw fruct;leaders ict;conference time conferences;regional leaders ict;search ainl ismw;time conferences;ict;cooperation universities;leaders ict long;international school organized", "pdf_keywords": ""}, "68258e0541132027ef86f872b92406de1c6edab3": {"ta_keywords": "millimeter wave d2d;wave d2d communication;d2d communication presence;d2d communication;relays millimeter wave;wave d2d;millimeter wave;explore relays millimeter;transmission explore relays;d2d;relays millimeter;continue transmission explore;explore relays;transmission explore;relays;transmission;communication presence dynamic;continue transmission;communication;communication presence;wave;dynamic obstacles;obstacles;presence dynamic obstacles;millimeter;explore;presence dynamic;presence;dynamic;continue", "pdf_keywords": ""}, "b1d309073623d46548e55269fb73485a3b7f11a8": {"ta_keywords": "embryology pretrained language;language model embryology;pretrained language model;pretrained language;pretraining pretrained language;knowledge pretrained;language model findings;knowledge pretrained model;refer embryology pretrained;embryology pretrained;learning speeds pretraining;suggest knowledge pretrained;totipotent language model;pretrained model;pretrained model varies;pretraining;learns;generally improve pretraining;pretrained;language model;improve pretraining;pretrain;model varies pretraining;pretraining pretrained;linguistic knowledge;speech pos different;learning speeds;pretrain steps;pretraining having pretrain;having pretrain", "pdf_keywords": "embryology pretrained language;language model embryology;pretrained language model;pretrained language models;pretrained language;behaviors pretrained language;pretraining \ufb01nd linguistic;pretrained models reproduce;pretraining rarely studied;refer embryology pretrained;embryology pretrained;pretrained models;knowledge pretrained;knowledge pretrained model;pretrained model varies;behaviors pretrained;model varies pretraining;pretraining rarely;language models;pretrained model;abstract behaviors pretrained;language model;learning speeds pretraining;language model results;improve pretraining;generally improve pretraining;pretraining;totipotent1 language model;suggest knowledge pretrained;happened pretraining rarely"}, "0110abf15bf0ee1bdf28061ad05f85b1c9f6e1c3": {"ta_keywords": "similarity reasoning built;similarity score knowledge;similarity measures text;knowledge integration structured;structured information sources;similarity reasoning;type similarity reasoning;combining information relations;built type similarity;knowledge integration;overall similarity score;type knowledge integration;similarity measures;knowledge integration problem;similarity score;overall similarity;type similarity;integration structured information;studied similarity measures;predicate answers query;structured information;text observation web;web based databases;according overall similarity;similarity;text extension datalog;based studied similarity;built predicate answers;information sources containing;score knowledge integration", "pdf_keywords": ""}, "a3da7028a1b721e392c421c2f15096abb1a71afb": {"ta_keywords": "atherosclerotic plaque vulnerability;increases atherosclerotic plaque;hba1c increases atherosclerotic;intravascular optical coherence;atherosclerotic plaque;a1c increases atherosclerotic;coronary intervention pci;elevated hba1c thinner;assessment lipid macrophage;increases atherosclerotic;percutaneous coronary intervention;assessment lipid;including assessment lipid;percutaneous coronary;coronary intervention;greater lipid index;fct elevation hemoglobin;low density lipoprotein;pci elevated hba1c;coronary intervention non;undergoing percutaneous coronary;higher macrophage index;lipid profiles patients;elevated hba1c linearly;elevation hemoglobin a1c;elevated hba1c;elevated hba1c increases;lipid index;patients elevated hba1c;high density lipoprotein", "pdf_keywords": ""}, "3ed07f6643856b9ac4687b3bc667767f3ab4b563": {"ta_keywords": "voice quality control;selecting voice quality;selection voice quality;corresponding voice quality;converted voice quality;voice quality expression;multiple voice quality;quality voice quality;voice quality;voice quality voice;performance voice quality;better voice quality;quality voice;selecting voice;controlling converted voice;scores corresponding voice;achieve better voice;selection voice;method selecting voice;depends selection voice;converted voice;proposed performance voice;speakers statistically modeling;using multiple voice;performance voice;results performance voice;multiple voice;corresponding voice;gaussian mixture model;regression gaussian mixture", "pdf_keywords": ""}, "ecde7c041e9ac48bccef7a8d078a3f80239b0479": {"ta_keywords": "object detection videos;object detection video;improving object detection;detection videos captures;detection videos;breakthroughs object detection;object detection static;video objects dataset;videos captures temporal;detection video;captures temporal context;youtube video objects;videos captures;images object detection;detection video offers;video objects;context frames regularization;object detection;frames weak supervision;supervision context frames;network object detection;context frames;videos;supervision target frames;train recurrent neural;vast amounts video;labeled frames subsequently;labeled frames optimizes;frames regularization;labeled frames", "pdf_keywords": "video utilizing contextual;frames train recurrent;captures temporal context;object detection video;object detection videos;object detection trained;context frames regularization;videos captures temporal;improving object detection;predictions exploiting contextual;detection videos;detection videos captures;weaksupervision context frames;detection video;train recurrent neural;learning image;labeled video frames;domain video frames;learning image domain;context frames;recognition localization video;exploiting contextual;localization video domain;frames weaksupervision context;detection video utilizing;image domain video;labeled video;re\ufb01ne object detection;videos captures;detection trained"}, "085072963b33367b842369b9ce81394d32ac8843": {"ta_keywords": "speech separation noisy;deep learning separation;channel speech separation;speech separation systems;speech separation;separate noise training;separation noisy oracle;learning separation;learning separation models;noise training deep;separation noisy;mixtures noisy speech;separate noise;training noisy single;additionally separate noise;noisy speech requiring;noise training;noisy speech;training synthetic mixtures;clean speech preventing;single channel speech;training noisy conditions;separation systems improved;using noise synthetically;speech requiring network;noise synthetically;clean speech;training noisy;training using mixtures;separation models", "pdf_keywords": "training speech separation;speech separation noisy;speech separation systems;channel speech separation;separation noisy oracle;speech separation;trained separation systems;separation systems trained;trained separation;noisy oracle speech;noise separation;separation systems noisy;separation noisy;effectively training speech;noise separation errors;training better separation;sisdr trained separation;noisy speech paradigm;better separation systems;noisy speech signals;speech sources training;oracle speech;training noisy single;clean noisy oracle;separation systems improved;single channel speech;noisy speech;training speech;oracle speech sources;speech processing"}, "76fe5f80dd25078eefa522e59a7763bc5d5da826": {"ta_keywords": "adaptive spelling error;adaptive spelling;spelling error correction;correction models learner;error correction models;correction models;learner english;models learner english;error correction;spelling error;spelling;learner;adaptive;models learner;english;correction;models;error", "pdf_keywords": ""}, "9165d5e99b2106825dd00b9f5daf60e454434399": {"ta_keywords": "simultaneous interpretation corpus;interpretation corpus comparing;interpretation corpus;simultaneous interpretation interpreter;english simultaneous interpretation;simultaneous interpretation data;professional simultaneous interpreters;simultaneous interpretation systems;japanese english simultaneous;translation data simultaneous;interpretation interpreter;construct simultaneous interpretation;interpretation interpreter possible;simultaneous interpreters;compare translation data;interpretations lectures news;simultaneous interpretation;simultaneous interpreters different;corpus professional simultaneous;recorded interpretations lectures;translation data;interpretations lectures;compare better interpretations;compare translation;interpretation systems;interpretations styles;data simultaneous interpretation;possible compare translation;interpretations good;corpus helpful analyze", "pdf_keywords": ""}, "23d299b35366c18e397faeb2c8687c20f8e17688": {"ta_keywords": "deception attack adversarial;detection deception attack;attack adversarial image;image classifier attacker;attack deep neural;deception attack deep;dnn malicious deception;attack adversarial;adversarial image;malicious deception attack;detection deception;classification autonomous cyber;adversarial;deception attack;classifier attacker;vulnerability dnn malicious;detection cyber;image detection cyber;modified images dnn;images dnn based;classifier attacker model;adversarial image detection;dnn based image;vulnerability dnn;images dnn;detection cyber physical;dnn based classifier;shown vulnerability dnn;malicious deception;dnn malicious", "pdf_keywords": ""}, "72302d8c5cdcf59b6df96290ffc874d3613fe6b1": {"ta_keywords": "cancer pathology classification;pathology classification comparing;pathology classification;cancer pathology;classification comparing sets;classification comparing;pathology;sets high dimensions;classification;cancer;comparing sets high;comparing sets;high dimensions;sets;high dimensions version;sets high;dimensions version;dimensions;2011 committee;comparing;20 2011 committee;dimensions version april;committee;high;2011;april 20 2011;20 2011;version;version april 20;april", "pdf_keywords": ""}, "12e9d005c77f76e344361f79c4b008034ae547eb": {"ta_keywords": "embed textual sequences;charagram embedding words;models embed textual;charagram embeddings outperform;charagram embeddings simple;charagram embeddings;embed textual;present charagram embeddings;charagram embedding;embedding words sentences;demonstrate charagram embeddings;embedding words;tasks charagram embedding;character based compositional;embedding demonstrate charagram;compositional models embed;character gram count;similarity tasks charagram;character gram;sentences character grams;embeddings outperform;textual sequences;embeddings;character level recurrent;using character gram;sentences character;words sentences character;embeddings simple approach;learning character;embeddings outperform complex", "pdf_keywords": "embed textual sequences;charagram embedding words;embedding words sentences;charagram embeddings outperform;charagram embeddings simple;charagram embeddings;models embed textual;embedding words;embed textual;present charagram embeddings;demonstrate charagram embeddings;charagram embedding;representing textual sequences;systems charagram embedding;embeddings outperform;text representation;character based compositional;embeddings outperform complex;sentences character grams;textual sequences words;words sentences character;embeddings;embeddings simple approach;representing textual;sentences character;textual sequences;embeddings simple;compositional models embed;learning character;character level recurrent"}, "f053137323a88eb932d590bcdfc959ee805e2520": {"ta_keywords": "dependency parsing stack;transition based parsers;parsers yielding efficient;parsing stack;complexity stackptr parser;dependency parsing;parsing stack pointer;based parsers yielding;parse tree;stackptr parser;projective parse tree;architecture dependency parsing;parsers;based parsers;parsing;stack pointer networks;stackptr parser benefits;parser;pointer networks stackptr;parsers yielding;parse tree linear;parse;non projective parse;parser benefits;projective parse;pointer networks;search pointer networks;parser benefits information;dependency tree;pointer networks vinyals", "pdf_keywords": "dependency parsing stack;dependency parsing stackpointer;dependency parsing;networks dependency parsing;pointer networks dependency;parsing stackpointer networks;parsing stack;stack pointer networks;pointer networks stackptr;architecture dependency parsing;parsing stack pointer;parsing stackpointer;stackptr parser;pointer networks;pointer networks internal;stackptr parser able;combining pointer networks;maintaining linear parsing;based parsers;transition based parsers;based parsers maintaining;pointer networks vinyals;parsing;parsers;dependency tree;linear parsing;dependency parsing xuezhe;procedure stackptr parser;linear parsing steps;parsers maintaining"}, "a4b1afd75bd2da0b21df58cd4ae1649fefabd8dd": {"ta_keywords": "functions fitness game;utility functions fitness;players utility functions;agent utility functions;fitness game;agents utility maximizers;mobile fitness game;strategy learned utility;desirable objectives game;fitness model agents;equilibrium strategy learned;functions fitness;model agents utility;players utility;parameters players utility;fitness game players;learned utility functions;utility maximizers;computing parameters players;game theoretic framework;learned utility;method simulated game;fitness game hope;privacy receiving reward;game theoretic;correlated equilibrium strategy;parametric agent utility;agents utility;simulated game;agent utility", "pdf_keywords": ""}, "cbf9a2560eac548e7b3d5eb7074c40b7bb861909": {"ta_keywords": "subtasks speaker diarization;speaker diarization eend;speaker diarization conditioned;neural speaker diarization;detection subtasks speaker;speaker diarization based;model speaker diarization;optimize speaker diarization;speaker diarization;end speaker diarization;speaker diarization outperforms;diarization conditioned speech;speech activity overlap;end neural speaker;subtasks speaker;conditioned speech activity;neural speaker;effectively model speaker;overlap detection subtasks;speech activity;conditional multitask learning;diarization eend;end speaker;optimize speaker;diarization eend end;diarization based probabilistic;conditioned speech;diarization outperforms;model speaker;diarization conditioned", "pdf_keywords": "subtasks speaker diarization;subtask speaker diarization;detection subtasks speaker;speaker diarization probabilistic;speaker diarization based;speaker diarization conditioned;model speaker diarization;optimize speaker diarization;speaker diarization;speaker diarization outperforms;speaker diarization sc;formulate speaker diarization;diarization conditioned speech;eend optimize speaker;subtask speaker;subtasks speaker;speech activity overlap;framework subtask speaker;data formulate speaker;conditioned speech activity;conditional multitask learning;speech activity;diarization based probabilistic;effectively model speaker;overlap detection subtasks;diarization probabilistic model;multitask learning;diarization probabilistic;rule multitask learning;diarization sc eend"}, "e9dfccd86b6116f7601d44590985de2df434a094": {"ta_keywords": "tutor learning;tutor learning lessons;tutoring strategy;tutors interactively teach;students explanations tutoring;tutoring;tutoring appropriateness tutoring;appropriateness tutoring strategy;tutoring appropriateness;tutor solve problems;contribute tutor learning;explanations tutoring;tutors interactively;help students tutor;tutor learning making;appropriateness tutoring;tutoring strategy problem;students tutor;interactively teach simstudent;explanations tutoring appropriateness;tutor;act tutors interactively;students tutor solve;interactively teach;tutors;tutor solve;simstudent learning;simstudent learns skills;adaptive help students;students learning", "pdf_keywords": ""}, "c933fed82e7b5cbf7230f0f970b69590b40f86a1": {"ta_keywords": "decentralized sgd methods;theory decentralized sgd;decentralized sgd;variety decentralized sgd;decentralized sgd changing;sgd methods;sgd federated averaging;sgd methods far;universal convergence rates;averaging local sgd;unified convergence analysis;coorperative sgd federated;convergence rates smooth;linear convergence rates;unified theory decentralized;introduce unified convergence;unified convergence;convergence rates special;convergence rates;sgd federated;recovering linear convergence;federated averaging local;large variety decentralized;sgd paper introduce;theory decentralized;sgd changing topology;instance coorperative sgd;federated averaging;convergence analysis covers;local sgd", "pdf_keywords": "decentralized sgd methods;decentralized stochastic optimization;theory decentralized sgd;decentralized sgd changing;decentralized sgd;high decentralized sgd;decentralized stochastic;abstract decentralized stochastic;sgd methods;sgd methods achieve;averaging local sgd;sgd changing topology;sgd federated averaging;adaptive network topology;sgd updates synchronous;local sgd updates;gossip updates adaptive;universal convergence rates;stochastic optimization methods;synchronous pairwise gossip;local sgd;sgd updates;stochastic optimization;coorperative sgd federated;data locality;pairwise gossip updates;convergence rates smooth;sgd changing;cost data locality;data locality communication"}, "91d98b0a175237b48122e7560010e87a968fb6e0": {"ta_keywords": "separation speech enhancement;separation recognition speech;speech enhancement problems;mask prediction networks;neural networks separation;noise robust speech;robust speech recognition;robust speech;speech separation;mask prediction;speech enhancement;speech separation speech;recognition speech challenging;recently speech separation;speech recognition environments;networks separation recognition;speech recognition;speech challenging environments;separation recognition;recognition speech;enhancement deep neural;enhancement deep;separation speech;nonnegative matrix factorization;useful noise robust;deep computational architectures;deep learning;deep computational;noise robust;neural networks deep", "pdf_keywords": ""}, "cf8f2ca0c2d618104bc8724a6effc509088f16c4": {"ta_keywords": "neverending learning paradigm;neverending learning;propose neverending learning;ending learner;ending language learner;ending learner discuss;learning read web;acquired knowledge base;learner;current machine learning;machine learning better;language learner;knowledge base;learning systems acquire;far acquired knowledge;learning systems;learning paradigm;learning;learning read;knowledge;type learning;synthesizing new relational;encompassing type learning;learner discuss lessons;properties ending learner;learning better;acquired knowledge;learner discuss;additionally learned reason;learning paradigm machine", "pdf_keywords": ""}, "cc7858e74a79edceb5a42c30fc5c2dc5117f365b": {"ta_keywords": "deep reinforcement learning;advances deep reinforcement;learned model planning;deep reinforcement;dqn learns;learns environment model;algorithm learns environment;dqn learns function;learn model atari;reinforcement learning rl;learns environment;network dqn learns;deep network dqn;learned model;algorithm learns;model atari environments;reinforcement learning;generative adversarial tree;rl algorithm learns;generative models rl;deep generative;deep generative models;learns;adversarial tree search;atari environments gats;atari environments;adversarial tree;learns function;design deep generative;learn model", "pdf_keywords": "arcade learning environment;games arcade learning;learn model atari;model atari environments;arcade learning;latest arcade learning;atari environments;learned model planning;atari games arcade;learns model environment;atari environments study;algorithm learns environment;learns model;atari games;model atari;learns environment model;learns environment;adversarial tree search;use atari games;generative adversarial tree;learned model;dif\ufb01culties atari games;algorithm learns model;adversarial tree;atari;use atari;deep generative;atari games work;rl algorithm learns;generative models rl"}, "82cb0c428f5edb1db6e733dc4b1b20023a2ce15f": {"ta_keywords": "voting rules instances;voting rules strictly;voting rules examine;strictly ordered voting;ordered voting;voting systems;study voting systems;ordered voting data;voting rules;voting systems takes;evaluation voting rules;empirical evaluation voting;different voting rules;voting data derive;voting data;evaluation voting;dataset evaluate elections;study voting;consensus different voting;different voting;election data testing;elections existing studies;evaluate elections;rav voting rules;voting;evaluate elections plurality;election data;elections;repeated alternative vote;elections existing", "pdf_keywords": ""}, "f0bbc7b84c166e2258b6ba4f9d9835ecac04e842": {"ta_keywords": "speech recognition bayesian;spontaneous speech recognition;modeling spontaneous speech;bayesian acoustic modeling;estimation clustering speech;acoustic modeling spontaneous;clustering speech recognition;clustering speech;speech fluctuation modeling;approach bayesian acoustic;vbec spontaneous speech;bayesian acoustic;speech recognition;speech recognition vbec;variational bayesian estimation;given speech data;spontaneous speech;variations speaker variances;task spontaneous speech;speech data;variational bayesian;framework variational bayesian;speech recognition requires;acoustic modeling;bayesian framework variational;construction spontaneous speech;bayesian estimation clustering;speaking variations speaker;speech recognition paper;variations speaker", "pdf_keywords": ""}, "19b6537012412bee0a36e3e271f84b95868fe859": {"ta_keywords": "ad hominem arguments;ad hominem argument;opponent ad hominem;ad hominem using;arguments punished arguers;hominem arguments;hominem argument existing;ad hominem;hominem argument;typology ad hominem;hominem arguments potential;arguers lapse attacking;hominem using explainable;punished arguers lapse;arguments punished;debating rules;punished arguers;fallacious arguments punished;debating rules strictly;debating;triggers ad hominem;arguing;enforced fallacious arguments;debate;annotation studies;arguers;arguing committing fallacy;architectures arguing;arguers lapse;arguing committing", "pdf_keywords": "ad hominem discussions;ad hominem dialogical;ad hominem arguments;dialogical argumentation estimated;ad hominem presented;ad hominem based;ad hominem fallacies;linguistic rhetorical triggers;classi\ufb01cation ad hominem;hominem arguments crowdsourced;ad hominem;ad hominem properties;typology ad hominem;triggering ad hominem;exchange ad hominem;direct ad hominem;hominem dialogical;potential linguistic rhetorical;rhetorical triggers ad;hominem dialogical exchange;genuine dialogical argumentation;complexity ad hominem;faceted ad hominem;dialogical argumentation;argumentation estimated;hominem arguments;linguistic rhetorical;rhetorical triggers;discourse context looked;hominem discussions"}, "36a5e0e0a8ce67e4cd9077d86e3b4d50fdcff15f": {"ta_keywords": "mose2 ni3se2 nickel;achieve mose2 ni3se2;ni3se2 nickel foam;electrocatalysts overall water;dual functional electrocatalysts;functional electrocatalysts overall;functional electrocatalysts;mose2 ni3se2;nickel foam efficient;electrocatalysts;electrocatalysts overall;achieve mose2;ni3se2 nickel;nickel foam;mose2;ni3se2;water splitting;route achieve mose2;overall water splitting;foam efficient dual;nickel;foam efficient;foam;splitting;water;efficient dual functional;dual functional;overall water;efficient dual;dual", "pdf_keywords": ""}, "d3e13d2514edaf74b863bfbe45a739c32a7689e1": {"ta_keywords": "neural code generation;code examples neural;code generation tasks;representing code tree;code natural language;code tree;examples neural code;code generation;code generation model;code tree structure;neural code;subtree predicted code;representing code;program source code;predicted code;generate complex code;language representing code;sentence similarity scoring;source code natural;retrieve sentences similar;code examples;existing code examples;source code;reference existing code;programming based sentence;based sentence similarity;subtree retrieval makes;structures retrieve sentences;performance code generation;sentence similarity", "pdf_keywords": "neural code generation;code examples neural;code natural language;code generation subtask;natural language code;code generation tasks;examples neural code;code generation models;language code generation;representing code tree;code generation model;code generation;code tree;neural code;methods neural code;code tree structure;generation subtask semantic;subtask semantic parsing;language representing code;retrieval methods neural;based neural code;representing code;method code generation;existing code examples;neural model generating;code generation problem;nl descriptions code;code examples;general purpose code;descriptions code ling"}, "ba3322280992d0425bc9e2b4c59de24857e5f4e7": {"ta_keywords": "performative risk minimization;risk minimization performative;deploying learning algorithms;minimizers performative risk;performative prediction seek;repeated risk minimization;performative prediction;risk minimization;prediction seek classify;classification decisiondependent distributions;risk minimization perturbed;strategic classification decisiondependent;behavior deploying learning;risk minimization consider;prediction seek;learning algorithms explicitly;strategic classification;learning algorithms;classification decisiondependent;minimization performative;classification;classifier;flows performative risk;performative risk;minimizers performative;classifier data distribution;deploying learning;works performative prediction;mapping classifier;minimization performative example", "pdf_keywords": "risk minimization perturbed;gradient descent \ufb02ow;repeated gradient descent;repeated risk minimization;performative risk minimizers;performative risk minimization;risk minimization performative;gradient descent;minimization performative risk;prm gradient \ufb02ow;risk minimization;gradient descent method;gradient \ufb02ow performative;risk minimizers;gradient \ufb02ows performative;perturbation prm gradient;function prm gradient;risk minimization identi\ufb01ed;minimization perturbed trajectories;prm gradient;descent \ufb02ow perturbation;perturbed trajectories gradient;convergence repeated risk;\ufb02ow performative risk;trajectories gradient \ufb02ows;\ufb02ow perturbation prm;minimization perturbed;\ufb02ows performative risk;risk minimization analyzed;minimization analyzed gradient"}, "3c6670ecdfccd4633755c4b19d774453bfb77de3": {"ta_keywords": "fairness patients;types fairness patients;fairness patients regions;fairness deciding;ensure fairness decisions;fairness decisions;consider fairness deciding;decide fairness;decisions consider fairness;fairness decisions consider;challenges decide fairness;ensure fairness;need ensure fairness;decide fairness means;different types fairness;match organs donated;fairness deciding match;consider fairness;patients organs algorithms;deciding match organs;fairness;organs donated deceased;types fairness;match organs;fairness means particular;list organs donated;donors patients challenges;organs donated;waiting list organs;fairness means", "pdf_keywords": ""}, "d79b613a67cf79740e1c08037f7d054585a12284": {"ta_keywords": "nar decoder auxiliary;decoder nar models;model nar decoders;models accelerate decoding;nar decoders;nar decoder;nar decoder parallel;orthros nar decoder;decoder nar;generated nar decoder;cmlm decoder nar;masked language model;speech translation e2e;nar decoders orthros;accelerate decoding;improvements translation quality;decoder auxiliary;large improvements translation;encoder;end speech translation;enhance cmlm decoder;conditional masked language;temporal classification ctc;translation quality;connectionist temporal classification;accelerate decoding speed;decoder shared encoder;nar model auxiliary;decoder;decoding speed", "pdf_keywords": "model nar decoders;nar decoders;nar decoder;orthros nar decoder;nar decoder parallel;nar decoders orthros;nar decoder auxiliary;generated nar decoder;increased decoding speed;51 nar decoders;encoder increased decoding;increased decoding;increase decoding speed;masked language model;decoder;decoding speed;decoder shared encoder;increase decoding;decoder parallel;speech encoder;decoders;encoder;decoder shared speech;decoding;decoders orthros;decoder parallel parallel;shared speech encoder;ar decoder;decoder shared;model increase decoding"}, "fd9e38e240b4372c49b9205d6f909d070ff3804c": {"ta_keywords": "similarity based reasoning;general similarity based;unlabeled background knowledge;text classification;similarity based;knowledge improved text;background knowledge improved;whirl background knowledge;information retrieval;classification intelligent use;improved text classification;based reasoning tasks;statistical similarity measures;databases manipulate textual;inductive classification intelligent;textual data;general similarity;similarity measures;classification intelligent;background knowledge;using statistical similarity;information retrieval community;statistical similarity;textual data using;developed information retrieval;designed general similarity;similarity measures developed;similarity;reasoning tasks competitive;inductive classification", "pdf_keywords": ""}, "cd96cae0f8eabc7bb327c6f30151741bfdd62ee0": {"ta_keywords": "new officers sigai;officers sigai;officers sigai particular;coordination officer anuj;conference coordination officer;new conference coordination;new information officer;new editor ai;excellent service sigai;sigai;service sigai;new conference;new leadership;officer anuj karpatne;new leadership team;integrating new leadership;coordination officer;officer anuj;conference coordination;new officers;changes appointed officers;editor ai;information officer;editor ai matters;dennis new conference;information officer taking;sigai particular;leadership team changes;challenges new officers;team changes appointed", "pdf_keywords": ""}, "dec6bb3c7bb671c86296a2a089e0e38aa3f69279": {"ta_keywords": "machine translation nat;autoregressive machine translation;translation nat systems;performance nat models;translation quality knowledge;accuracy nat models;translation nat;improve performance nat;machine translation;nat systems predict;nat models;gains accuracy nat;nat models usually;translation quality;existing nat models;nat model variations;best translation quality;nat model complexity;models knowledge distillation;performance nat;knowledge distillation empirically;nat model;data pretrained autoregressive;accuracy nat;nat models reason;nat;knowledge distillation reduce;autoregressive models knowledge;knowledge distillation;pretrained autoregressive model", "pdf_keywords": ""}, "6bfeb25ea4bb41ab0840bb1be09f9b2de7eea8e4": {"ta_keywords": "gpcmv encoded protein;infection hypothesize gp33;cellular signaling viral;pathogenesis presence gp33;pig cmv gpcmv;signaling viral;pig cytomegalovirus encoded;gp33 mediated signaling;guinea pig cytomegalovirus;gpcmv genome used;gpcmv genome;pig cytomegalovirus;signaling viral growth;containing gpcmv genome;cmv gpcmv encoded;gpcmv encoded;gpcmv r33;animals viral;signaling activates cytokine;gp33 gene;pregnant animals viral;guinea pig cmv;viral growth inflammation;viral growth pathogenesis;gp33 guinea pig;hypothesize gp33 mediated;placentas roles gp33;cytomegalovirus encoded protein;rescued gpcmv r33;gp33 mediated", "pdf_keywords": ""}, "609010cb866a19dd996281d00818c3fc7363ec94": {"ta_keywords": "entity recognition ner;unsupervised cross lingual;lingual named entity;cross lingual ner;named entity recognition;lingual ner model;entity recognition;lingual ner;ner knowledge language;entity recognition paper;ner tasks languages;cross lingual named;relying bilingual dictionary;resource cross lingual;cross lingual;bilingual dictionary parallel;recognition ner tasks;lingual named;recognition ner;way relying bilingual;relying bilingual;transfer ner knowledge;bilingual dictionary;lingual;languages need manually;language pair;ner knowledge;named entity;languages need;language completely unsupervised", "pdf_keywords": "entity recognition ner;lingual ner methods;cross lingual models;zeroresource cross lingual;unsupervised cross lingual;lingual ner model;cross lingual ner;lingual named entity;lingual neural ner;lingual models;lingual ner;lingual models use;ner knowledge language;named entity recognition;ner tasks languages;entity recognition saiful;entity recognition;resource cross lingual;refer cross lingual;relying bilingual dictionary;neural ner model;cross lingual named;neural ner;cross lingual neural;cross lingual;recognition ner tasks;bilingual dictionary parallel;target language model;transfer ner knowledge;recognition ner"}, "a1c4ce9de92338646c6ee93c7c2e5ee366784b1a": {"ta_keywords": "neural semantic parsing;existing semantic parsing;representation semantic parsing;semantic parsing;semantic parsing datasets;semantic parsing understood;meaning representations program;semantic parsing approaches;neural semantic;meaning representations identifying;benchmark meaning representations;meaning representation semantic;different meaning representations;parsing approaches exhibit;meaning representations researchers;parsing approaches;representation semantic;parsing;parsing understood;meaning representations;parsing datasets completing;existing semantic;parsing datasets;integrating existing semantic;datasets meaning representations;semantic;reveals neural semantic;program alias grammar;meaning representations integrating;generate different meaning", "pdf_keywords": ""}, "facefd2fc4b718c6a0d8096b4eb02866028a04c2": {"ta_keywords": "answer passage supervision;answer span retrieved;conversations span answers;answers open retrieval;answering qa conversational;answerable single span;answer span;truth answer span;question answering qa;span answers;question answering;paraphrased span known;retrieval retrieves evidence;span known answer;conversations challenging span;span retrieved passages;conversational qa convqa;answering qa;span answers satisfactory;paraphrased span;identify paraphrased span;passage supervision;conversational qa;collection extracts answers;open retrieval convqa;retrieval convqa;qa conversational;extracts answers;qa conversational qa;answer passage", "pdf_keywords": "leveraging diverse paraphraser;question answering;paraphrased span known;question answering qa;answering qa conversational;conversations span answers;paraphrased span;span answers;diverse paraphraser;identify paraphrased span;diverse paraphraser 19;answer retrieved passage;approach identify paraphrased;paraphraser 19 generate;learned weak supervision;span known answer;answering qa;open retrieval;datasets open retrieval;conversational qa convqa;identify paraphrased;span answers satisfactory;answers generated;known answer retrieved;paraphraser;collection extracts answers;conversational qa;extracts answers;answer retrieved;emphasize role retrieval"}, "75d33c125eba966b50d4dccd359a2f6aa4e0e2e7": {"ta_keywords": "estimators contextual bandits;assessment contextual bandits;contextual bandits;contextual bandits ii;contextual bandits paper;lipschitz risk estimates;bounds lipschitz risk;lipschitz risk functionals;risk estimates converge;risk estimates;introduce lipschitz risk;cdf estimators contextual;lipschitz risk;bandits;estimates target policy;bandits ii error;bandits ii;risk functionals broad;estimators contextual;inequalities cdf estimators;bandits paper introduce;bandits paper;risk functionals;risk assessment contextual;framework estimates;policy risk;framework estimates target;policy cdf;policy risk assessment;target policy cdf", "pdf_keywords": "lipschitz risk functionals;risk functionals;risk functionals practical;risk functionals introduce;risk functionals broad;subsumes risk functionals;introduce lipschitz risk;risk cumulative prospect;risk bounded;risk functionals encompass;risk functionals novel;norm lipschitz risk;objectives risk depends;lipschitz risk;risk cumulative;di\ufb00erences risk bounded;risk bounded sup;risk depends cdf;cdf rewards;objectives risk;depends cdf rewards;value risk cumulative;policy risk assessment;policy cdf;bounded rewards;conditional value risk;cdf rewards prove;di\ufb00erences cdf rewards;policy risk;cdf rewards ii"}, "cb0de2de79533d4faada3d745f43702eb89d1a60": {"ta_keywords": "datasets nlp gem;generation developing documentation;documentation templates;reusable documentation templates;nlp tools;developing documentation;templates guides documenting;documentation templates huggingface;developing documentation guidelines;documenting datasets models;reusable documentation;develop reusable documentation;documenting datasets;descriptions nlp datasets;practices field nlp;nlp gem;guides documenting datasets;guides documenting;nlp gem benchmark;documenting;documentation practices;processing nlp tools;standard documentation practices;documentation guidelines easy;documentation;descriptions nlp;detailed descriptions nlp;standard documentation;card datasets nlp;documentation practices field", "pdf_keywords": "documentation models;documentation models gem;datasets nlp gem;documentation datasets;nlp tools documentation;documentation datasets cases;processing nlp hugging;descriptions nlp datasets;models natural language;documenting datasets models;documentation templates huggingface;refer documentation models;nlp tools;refer documentation datasets;documentation templates;developing documentation;guides natural language;nlp datasets models;developing documentation guidelines;card datasets nlp;nlp gem;documenting datasets;creating documentation;documentation templates guides;practices \ufb01eld nlp;templates guides documenting;descriptions nlp;nlp gem benchmark;guides documenting datasets;datasets nlg models"}, "13b6c8cce3b4557ad7a3188f2d54636e755e8145": {"ta_keywords": "deep unfolding multichannel;unfolding multichannel source;separation multichannel mixtures;multichannel gaussian mixture;source separation multichannel;multichannel source separation;unfolding multichannel;multichannel mixtures;multichannel audio architecture;multichannel audio;multichannel mixtures simultaneous;separation multichannel;domain multichannel audio;mixtures simultaneous speakers;unfold multichannel;multichannel source;unfold multichannel gaussian;source separation;audio architecture;deep unfolding;approaches deep unfolding;gaussian mixture model;mixture model mcgmm;deep mcgmm computational;deep mcgmm;resulting deep mcgmm;multichannel gaussian;deep unfolding recently;audio architecture defined;multichannel", "pdf_keywords": ""}, "77c63e8f102465e3fc4a46e0b07c32fa8d2f8a54": {"ta_keywords": "unsupervised grammar induction;unsupervised ccg parsers;grammar induction;grammar induction probing;grammar induction help;ccg parsers;unsupervised grammar;progress grammar induction;ccg parsers evaluating;work grammar induction;limitations unsupervised grammar;induction probing linguistic;syntactic structure discoverable;parsers;parsers evaluating labeled;word tag sequences;grammar;syntactic structure;parsers evaluating;unsupervised ccg;necessary progress grammar;tag sequences;labeled dependencies ccgbank;dependencies ccgbank hinting;errors unsupervised ccg;tag sequences paper;progress grammar;syntactic;shed light syntactic;ccgbank hinting", "pdf_keywords": ""}, "c065f9997794b13565dd49a6e475fc5e8c9d54ce": {"ta_keywords": "torsional joints based;tensile stiffness lamina;torsional joints;bending stiffness tensile;compliant joints;torsional let joint;performances compliant joints;tensile stiffness dl;emergent torsional joints;joints based double;joint layer flexible;bending stiffness;compliant joints finally;improve tensile stiffness;double laminated let;tensile stiffness;stiffness tensile;stiffness lamina;stiffness tensile stiffness;based double laminated;similar bending stiffness;stiffness lamina emergent;joints similar bending;derived flexibility accuracy;material structure joint;stiffness dl let;kinetostatic model joint;utilizing double laminated;laminated let dl;lamina emergent torsional", "pdf_keywords": ""}, "112eb8a8273ab725d47789efb87237edbc4f02db": {"ta_keywords": "description logics learnability;logics learnability;logics learnability results;known description logics;propositional conjunctions learnable;logics known description;description logics;learnability restricted;learnability restricted order;description logic;simple description logic;conjunctions learnable;learnability subsets;description logics subsets;order logic learned;considers learnability subsets;considers learnability;logic learned;learnability;learnability subsets order;learnability results;learnability haussler;description logic summarize;learning concepts conjunctive;logic learned valiant;conjunctions learnable paper;learnability results hold;restricted order logics;analyze learnability;study learnability restricted", "pdf_keywords": ""}, "e6accbbb366387faf817126dc7b0260c450bd2e6": {"ta_keywords": "testing based sparse;testing algorithms approximate;group testing algorithms;sparse graph codes;algorithms approximate recovery;testing algorithms;tools modern sparse;modern sparse graph;sparse graph coding;sample complexity leveraging;algorithms approximate;modern sparse;approximate recovery order;based sparse graph;sparse graph;algorithm observed recover;noisy test results;optimal sample complexity;based sparse;group testing;graph coding;approximate recovery;noisy test;sparse;offsets group testing;sample complexity;complexity leveraging;algorithms;graph codes;group testing based", "pdf_keywords": ""}, "ca7a67aa29c67b006017f651601091145644f243": {"ta_keywords": "localize speakers reverberation;speaker localization;localization speech detection;speaker localization speech;speech localization;describes speaker localization;speaker localization value;speech localization speech;localize speakers;integrates speech localization;hard localize speakers;localization speech;712 speaker localization;speech detection development;speech detection;speech detection techniques;speech detection using;743 speech detection;utterances rooms;rooms leaked utterances;leaked utterances rooms;accuracy 712 speaker;speakers reverberation;method integrates speech;utterances rooms rejected;speaker;method calibrates localization;speakers reverberation causes;value 743 speech;reverberation", "pdf_keywords": ""}, "a61aebbfe029c4b8eafae4042e6242cdca8f54b7": {"ta_keywords": "relational qa dataset;wikipedia hyperlinks;relations wikidata;hyperlinks existing qa;generate relational qa;wikipedia hyperlinks existing;qa datasets extremely;target answer entity;triplets wikipedia hyperlinks;answer entity;relations wikidata triplets;extractive qa target;webquestions answering;existing qa datasets;answer entity generate;webquestions answering complex;wikidata;extractive qa;particularly rgpt qa;entities pre train;latent relations;qa datasets;range relations wikidata;entity generate relational;improves significantly questions;infer latent relations;understanding latent relations;dense passage retriever;qa dataset;wikidata triplets", "pdf_keywords": "human annotated qa;relational qa dataset;annotated qa dataset;generate relational qa;target answer entity;annotated qa;answer entity;extractive qa target;answer entity pre;extractive qa;hyperlinks rgpt qa;answer entity \ufb01rst;relations wikidata;qa dataset;wikipedia hyperlinks rgpt;relational qa;stateof art qa;wikipedia hyperlinks;qa provides good;entity pre train;qa provides;passage retriever;dense passage retriever;qa target answer;rgpt qa provides;generate relational;qa target;relations wikidata triplets;webquestions pre train;qa improves"}, "2d1f442578feb7034aa2b68bbf95f608f2342256": {"ta_keywords": "group fairness bandit;fairness bandit arm;fairness bandit;bandit arm selection;group fairness;fairness picking;group fairness contextual;fairness picking arms;group group fairness;fairness equal group;multi armed bandit;fairness contextual multi;form fairness picking;fairness arbitrary;fairness contextual;formulation group fairness;fairness arbitrary number;explore definitions fairness;notions fairness arbitrary;bandit arm;fairness equal;definitions fairness;bounds regret algorithm;bandit cmab;notions fairness;provide bounds regret;bandit;accommodate notions fairness;expected payout groups;fairness", "pdf_keywords": "group fairness bandits;fairness bandits biased;fairness bandits;bandits biased feedback;group fairness contextual;group fairness biased;bandits biased;fairness machine learning;fairness biased feedback;group fairness;fairness contextual multi;fairness contextual;fairness machine;multi armed bandit;bandit;fairness biased;formulation group fairness;reward regret capture;biased feedback contextual;bandits;work fairness machine;bandit cmab;resources fairly groups;bias;reward regret;bandit cmab setting;notions reward;reward;armed bandit;notions reward regret"}, "147ba336fcba32fadca470e14a858ce069375475": {"ta_keywords": "speech synthesis;based speech synthesis;f0 contour generation;contour generation using;contour generation;hmm based speech;models hmm;context models hmm;models hmm based;f0 contour;based speech;contour;synthesis;speech;hmm based;rich context models;context models;generation using;f0;models;generation using rich;using rich context;rich context;context;generation;hmm;using;based;using rich;rich", "pdf_keywords": ""}, "00c8d88abef116d8d3d673a28ff4098115cf8da3": {"ta_keywords": "cooperative persuasive dialogue;persuasive dialogue policies;dialogue management;created dialogue management;persuasive dialogue able;dialogue policies using;automatic cooperative persuasive;learning cooperative persuasive;dialogue policies;dialogue management module;dialogue manager;text based cooperative;persuasive dialogue;persuasive dialogue tendency;dialogue manager controlled;dialogue able persuade;based cooperative persuasive;nlg natural language;effective cooperative persuasive;natural language generation;module cooperative persuasive;cooperative persuasive;language generation nlg;dialogue;nlg modules allows;dialogue able;generation nlg natural;nlg modules;reinforcement learning cooperative;nlu nlg modules", "pdf_keywords": ""}, "04a7d9f0388ded93c1ec16e36a6df3cd44cb95b0": {"ta_keywords": "multilingual entity linking;single entity retrieval;multilingual entity;entity linking language;entity retrieval;entity retrieval model;linking 100 languages;entity linking;new multilingual dataset;language specific mentions;formulation multilingual entity;multilingual dataset http;multilingual dataset;entity linking 100;linking language specific;mining auxiliary entity;base entity linking;large new multilingual;linking language;multilingual;auxiliary entity pairing;new multilingual;single entity;knowledge base entity;entities;new formulation multilingual;million entities;representation negative mining;entity;entity pairing", "pdf_keywords": "multilingual entity linking;multilingual entity;entity linking language;single entity retrieval;entity linking seeks;languages entities;entity retrieval;new multilingual dataset1;large new multilingual;linking 100 languages;formulation multilingual entity;entity linking better;multilingual dataset1 matched;entity linking;entity retrieval model;multilingual dataset1;entities mewsli dataset;linking language;set languages entities;entities mewsli;multilingual el task;proposed multilingual el;entity linking 100;linking language speci\ufb01c;new multilingual;million entities mewsli;proposed multilingual;languages entities conclusion;multilingual;mining auxiliary entity"}, "9768d7ba9d09ac3bf3d52ec674bde1a6e615daad": {"ta_keywords": "minimax risk estimation;bern adaptive estimators;risk estimation pairwise;minimax risk measure;adaptive estimators bernoulli;estimation pairwise comparison;risk measure worst;risk estimator large;minimax risk;estimators bernoulli probabilities;studied minimax risk;risk estimator captures;probabilities pairwise comparisons;adaptive estimators;risk estimation;risk estimator;measuring worst case;estimators bernoulli;pairwise comparison probabilities;estimation pairwise;measuring worst;relative oracle estimator;oracle estimator prior;probabilities future comparisons;case risk estimator;measure worst case;oracle estimator study;bern adaptive;estimator adapts;strong stochastic transitivity", "pdf_keywords": "bern adaptive estimators;adaptive estimators bernoulli;adaptive estimators;estimation sst matrices;computationally achievable adaptivity;bern adaptive;computationally e\ufb03cient estimator;estimator achieve adaptivity;stochastic transitivity sst;introduce adaptivity index;achieve adaptivity index;matrices adaptivity index;estimators bernoulli;strong stochastic transitivity;sst matrices adaptivity;estimation sst;achievable adaptivity propose;adaptivity index;pairwise comparisons;adaptivity index smaller;estimators bernoulli probabilities;optimal computationally achievable;achievable adaptivity;logarithmic adaptivity index;probabilities pairwise comparisons;pairwise comparisons arxiv;squares estimator achieve;stochastic transitivity;estimato estimation sst;squares estimator"}, "b7ffc8f44f7dafd7f51e4e7500842ec406b8e239": {"ta_keywords": "gating reading comprehension;reading comprehension experiments;reading comprehension tasks;grained gating reading;like reading comprehension;gating reading;paragraphs reading comprehension;comprehension tasks achieving;reading comprehension;performance reading comprehension;comprehension tasks;reading comprehension present;comprehension experiments;questions paragraphs reading;combines word level;tasks like reading;comprehension experiments approach;fine grained gating;combine word level;grained gating modeling;paragraphs reading;grained gating;word level;improve performance reading;word level character;character level representations;like reading;comprehension present fine;idea fine grained;gating", "pdf_keywords": "reading comprehension tasks;tackle reading comprehension;comprehension tasks extend;reading comprehension;reading comprehension present;gating document query;comprehension tasks;document query gating;settings reading comprehension;including reading comprehension;word character gating;comprehension tasks general;grained word character;character gating document;tasks including reading;combine word level;gating document;word level characterlevel;documents queries;characterlevel representations word;query gating approaches;paragraph token query;character level representations;word level;query gating;grained gating approach;word level character;reading;character gating;\ufb01ne grained gating"}, "90357a6dc817e2f7cec477a51156675fbf545cf1": {"ta_keywords": "learns multimodal script;learns multimodal;multimodal reasoning recognition;multimodal reasoning;multimodal reasoning time;performing multimodal reasoning;visual commonsense reasoning;model learns multimodal;multimodal script knowledge;contextually performing multimodal;multimodal neural script;pretraining video corpus;multimodal neural;stack multimodal reasoning;visual commonsense;video corpus;multimodal;encourage stack multimodal;visual world contextually;multimodal script;script knowledge watching;videos transcribed speech;performing multimodal;neural script knowledge;video corpus using;videos transcribed;merlot multimodal neural;world contextually;learns match images;importance training videos", "pdf_keywords": "learns multimodal;learning multimodal reasoning;learns multimodal script;multimodal representations;learning multimodal;pretraining video corpus;videos transcribed speech;multimodal reasoning recognition;model learns multimodal;multimodal representations single;powerful multimodal representations;multimodal reasoning capacity;video corpus;learning powerful multimodal;videos transcripts;videos transcribed;youtube videos transcribed;videos accompanying transcripts;collecting videos transcripts;multimodal reasoning;goal learning multimodal;videos transcripts youtube;captions train;multimodal script knowledge;video corpus using;multimodal;transcripts youtube;stack multimodal reasoning;literal captions train;encourage stack multimodal"}, "ba3f39606cfd4150ea80fec1b2e1137933c6d143": {"ta_keywords": "pcr setup forensic;automated pcr;normalization wizard pcr;pcr setup robotic;automated pcr setup;setup forensic casework;setup forensic;forensic casework samples;pcr;pcr setup;wizard pcr setup;forensic casework;wizard pcr;forensic;normalization wizard;casework samples using;setup robotic methods;using normalization wizard;samples using normalization;casework samples;setup robotic;normalization;using normalization;robotic methods;automated;robotic;samples using;samples;setup;casework", "pdf_keywords": ""}, "8ae4a584539a8f30d654e2678dde64a8334461b7": {"ta_keywords": "personalized product reviews;classify reviews;write classify reviews;model classify reviews;classify reviews identifying;classify reviews typically;reviews typically modeled;rnn generates personalized;recurrent neural network;sentiment rating remarkable;product reviews;author review product;review product;character level recurrent;recurrent neural;review product category;level recurrent neural;sentiment rating;remarkable accuracy recommender;reviews typically;product category sentiment;product reviews additional;reviews identifying;neural network rnn;user rate product;reviews identifying author;generates personalized product;identifying author review;generative concatenative nets;accuracy recommender", "pdf_keywords": "learning generate text;generative concatenative nets;rnn generates personalized;recommender basic task;classify reviews;generative concatenative network;write classify reviews;learn write classify;personalized product reviews;review generation;recommender systems assist;use generative;learning generate;generative concatenative;generation traditional supervised;recommender basic;generative model;generative;classify reviews arxiv;supervised;generates personalized;design characterlevel recurrent;lstm rnn;items introduction recommender;recommender systems;simultaneously learning generate;reviews using simple;use generative model;rnn scoring mechanism;ef\ufb01cacy review generation"}, "b01ecfd2322437fcc9c7ce6605d6f5a50f67ec50": {"ta_keywords": "training successor model;outperform training al;training successor;reliably models tasks;training al;training al shown;subsequently training successor;al strategies findings;models tasks;successor model;alternative al strategies;reliably models;outperform training;successor model actively;consistently outperform training;al strategies;model actively acquired;training dataset model;generalize reliably models;actively acquired dataset;retrospective evaluations;models tasks problematic;models;inherent al;subsequently training;downsides inherent al;alternative al;compare alternative al;acquired dataset does;approaches generalize reliably", "pdf_keywords": "training dataset acquisition;dataset acquisition model;training successor model;performance model acquisition;model domain acquisition;model acquisition;model acquisition functions;training dataset model;classi\ufb01cation sequence tagging;standard nlp tasks;consistently outperform training;outperform training;actively sampled training;nlp tasks;dataset acquisition;outperform training report;reliably models tasks;training dataset;acquisition functions tables;training successor;training report performance;actively acquired dataset;sampled training;models tasks;performance function train;sequence tagging;generalize reliably models;learning;acquired dataset;acquisition model"}, "2177bf060aaf2c0c2b551d3e805779cb35c19bb1": {"ta_keywords": "ceramic k2sif6 mn4;phosphor ceramic k2sif6;red phosphor ceramic;ceramic k2sif6;k2sif6 mn4;phosphor ceramic;new red phosphor;red phosphor;k2sif6;phosphor;mn4;ceramic;new red;red;new", "pdf_keywords": ""}, "2068825cabd94c951a0282ed731a8b8f2da1721c": {"ta_keywords": "semantic parsing learns;supervised semantic parsing;models semantic parsing;semi supervised semantic;parsing learns;parsing learns limited;semantic parsing;semantic parsing task;utterances annotating nl;annotating nl utterances;transducing natural language;nl utterances annotating;supervised semantic;natural language nl;parsing task transducing;parsing;tree structured latent;model semi supervised;models semi supervised;semi supervised;nl utterances;unlabeled nl utterances;supervised models semantic;nl utterances corresponding;utterances annotating;variational auto encoding;formal meaning representations;language nl utterances;annotating nl;parsing task", "pdf_keywords": "structvae semantic parsing;semantic parsing learns;utterances generated treestructured;semisupervised semantic parsing;parsing learns;supervised semantic parsing;inference model parsing;semantic parser functions;semantic parser;semantic parsing task;parsing learns limited;semantic parsing;semantic parsing atis;model parsing observed;model parsing;latent meaning representations;nl utterances generated;model semisupervised semantic;transducing natural language;nl utterance latent;structvae semantic;formal meaning representations;structvae based generative;surface nl utterances;abstract semantic parsing;parsing task transducing;shelf semantic parser;parsing observed nl;meaning representations reconstruction;semisupervised semantic"}, "8ca58f3f6e59a6d243f3da6c196e9f730e6e9993": {"ta_keywords": "speaker diarization eend;neural speaker diarization;speaker diarization;end neural speaker;speaker tracing buffer;speaker tracing;extension speaker tracing;handling overlapping speech;overlapping speech flexible;overlapping speech;handle overlapping speech;neural speaker;variable numbers speakers;diarization handling overlapping;numbers speakers end;neural diarization handling;flexible numbers speakers;diarization handle overlapping;number speakers;numbers speakers;number speakers fixed;end neural diarization;diarization eend;diarization handling;diarization eend model;extension speaker;speech flexible numbers;speakers end;speaker;neural diarization", "pdf_keywords": "streaming speaker diarization;speaker diarization method;speaker diarization;online streaming speaker;online diarization;number speakers chunks;speaker tracing;online diarization problem;speaker tracing buffer;streaming speaker;chunk wise diarization;solution online diarization;handles overlapping speech;diarization methods based;overlapping speech \ufb02exible;speakers chunks;overlapping speech;diarization methods;speakers speaker tracing;numbers speakers experiments;\ufb02exible numbers speakers;numbers speakers;numbers speakers proposed;number speakers;wise diarization methods;speakers flex stb;numbers speakers flex;numbers speakers speaker;speakers experiments callhome;speech \ufb02exible numbers"}, "ca6d5c7829a76d10069fa3aa6776c35cc044b7ba": {"ta_keywords": "course advising tool;advising tool;advising software tools;advising software;courses selection advising;tool advising software;advising tool advising;creating course advising;tool advising;selection advising;selection advising career;course advising;selection tool undergraduates;advising career paths;course selection tool;advising career;advising;advising session;course selection;testing course selection;students human advisors;person advising session;person advising;courses selection;concerning courses selection;course planning;tool undergraduates;tools help students;basic course planning;preferences concerning courses", "pdf_keywords": ""}, "6e3f8187f8fef3e11578a73f32da07d33dbf8235": {"ta_keywords": "domain semantic parsing;semantic parsing spoken;text generation dataset;extracting semantic triples;extracting semantic;semantic parsing;parsing spoken dialogue;exploiting semantic;open domain semantic;parsing spoken;domain semantic;procedure extracting semantic;data text annotations;semantic triples tables;exploiting semantic dependencies;record text generation;domain structured data;structures exploiting semantic;source structured data;text generation;text datasets facilitates;semantic triples;sentence conversion;generation dataset;tree ontology annotation;data text datasets;semantic dependencies table;text annotations;semantic dependencies;text datasets", "pdf_keywords": "ontology annotation tables;domain semantic parsing;extracting semantic triples;tree structured semantic;extracting semantic;annotated tree ontologies;semantic triples tables;structured semantic;procedure extracting semantic;semantic parsing;tree ontology annotation;tree ontologies converted;semantic parsing dialogue;domain corpus structured;open domain semantic;domain semantic;structure semantic triple;ontologies converted table;semantic triples;annotation tables converts;ontology annotation;annotation tables;exploiting semantic;ontology structure semantic;semantic triple;structured semantic frame;corpus structured data;tree ontologies;semantic dependencies table;ontologies e2e webnlg"}, "bdbf635476477eec5be5a292b494e20b8902cc35": {"ta_keywords": "translation synthetic noise;robustness machine translation;machine translation synthetic;enhance robustness mt;noise clean data;human generated text;machine translation;synthesizing noise;translation synthetic;robustness mt;synthesizing noise manner;improving robustness machine;generated text particularly;improving robustness;synthetic noise;robustness machine;social media typos;noise human generated;robustness mt systems;synthetic noise human;enhance robustness;dialect idiolect noise;typos slang dialect;noise clean;generated text;typos slang;text particularly;noise partially mitigating;accuracy mt;media typos slang", "pdf_keywords": "machine translation noisy;noise use translation;noisy data social;translation noisy text;naturally noisy data;noisy text mtnt;translation noisy;noisy text;noisy data;noised data;social media text;robustness mt;noise clean data;arti\ufb01cially noised data;synthesizing natural noise;enhance robustness mt;introduce synthetic noise;unique social media;naturally noisy;data social media;noised data work;machine translation;synthetic noise induction;based machine translation;improved resilience vanilla;noise induction model;resilience vanilla mt;improving resilience vanilla;emulate target noise;media based text"}, "b3979990dc2080138021cb3d767c7ec6d3e96194": {"ta_keywords": "characterizing literary relationships;literary relationships;dynamic fictional relationships;literary relationships relies;fictional relationship characters;understanding fictional relationship;fictional relationships;characterizing literary;trajectories understanding fictional;fictional relationship;raw text novels;learning dynamic fictional;work characterizing literary;novel unsupervised;relationship descriptors;text novels;present novel unsupervised;text novels present;relationship characters;descriptors relationship dataset;relationship descriptors trajectory;understanding fictional;global relationship descriptors;relationships relies plot;novels;literary;challenge digital humanities;novels present novel;novels present;dynamic fictional", "pdf_keywords": ""}, "e12c52fb542f76b3f0d29178842428d6a4edfe1e": {"ta_keywords": "biased users deferring;fairness accuracy learning;external decision maker;decision maker experiments;accuracy learning defer;improve accuracy fairness;improving fairness accuracy;learning defer;agents decision making;proposing learning defer;model external decision;generalizes rejection learning;biased users;external decision makers;fairness accuracy;rejection learning;accuracy fairness;rejection learning considering;responsibly improving fairness;improving fairness;decision maker;demonstrate learning defer;agents decision;defer generalizes rejection;external decision;learning defer generalizes;inconsistent biased users;effect agents decision;decision makers predict;learning defer make", "pdf_keywords": "learning holds fairness;external decision maker;model external decision;improve accuracy fairness;external decision makers;rejection learning framework;proposing learning defer;learning defer;learning defer adaptive;external decision;fairness experimentally;decision makers embedding;accuracy fairness;accuracy fairness experimentally;learning defer generalizes;generalizes rejection learning;dm rejection learning;rejection learning generalizes;learning generalizes rejection;decision maker;fairness experimentally equivalence;method learning defer;adaptive rejection learning;defer adaptive rejection;defer generalizes rejection;rejection learning;held external decision;rejection learning holds;agents decision making;rejection learning considering"}, "4ce47dd7a8674f8ffd53f1883bc57e62460a83f0": {"ta_keywords": "differential nash equilibria;nash equilibria games;characterization nash equilibria;differential nash;policy regulations energy;nash equilibria;games non convex;demand smart grid;nash equilibria termed;regulations energy ecosystem;convex strategy spaces;characterization nash;energy ecosystem propose;equilibria games;nash equilibria structurally;create characterization nash;non convex strategy;smart grid consider;smart grid;degenerate differential nash;termed differential nash;equilibria games non;energy ecosystem;regulations energy;strategy spaces;convex strategy;demand smart;efficiency vulnerability tradeoff;utility learning incentive;strategy spaces amenable", "pdf_keywords": ""}, "fa9b043ae8da3cc60c975762ae9066d2fb010f41": {"ta_keywords": "efficient graph clustering;graph clustering;graph clustering makes;graph clustering algorithms;applications graph clustering;graph based representations;unsupervised nlp tasks;unsupervised nlp;graph based;clustering;efficient graph;methodology unsupervised nlp;data graph based;natural language processing;clustering algorithms;present efficient graph;structure data graph;clustering makes possible;processing nlp;data graph;clustering makes;clustering algorithms strengths;graph;language processing nlp;processing nlp tasks;nlp tasks discussed;nlp tasks;nlp tasks evaluation;applications graph;language processing", "pdf_keywords": ""}, "52540497682c4209b8e20125c8255358b22d0fa7": {"ta_keywords": "knowledge novice algebra;constructing learning agent;learning agent cognitively;learning agent currently;learning agent;skill knowledge harder;simulate human learning;human learning math;generality learning agent;learned problem representations;algorithm simulated student;knowledge harder problems;problems constructing learning;using learned problem;constructing learning;problems using learned;acquire skill knowledge;cognitive tutors;acquires representation knowledge;use cognitive tutors;novice algebra students;simulated student;intelligent agents simulate;efficient skill acquisition;simulated student simstudent;problem representations;skill knowledge;learned problem;knowledge novice;using learned", "pdf_keywords": ""}, "2ed6f376e9e7eee6d833ad7b6aba63d7ad40c0f8": {"ta_keywords": "cool blanket cfetr;blanket cfetr;thermal hydraulic design;research thermal hydraulic;design cool blanket;thermal hydraulic;cool blanket;research thermal;hydraulic design cool;cfetr;hydraulic design;thermal;blanket;design cool;hydraulic;design;cool;research", "pdf_keywords": ""}, "bd0db679d595399b91c5acca1db33a2803697d53": {"ta_keywords": "cues online political;political information internet;online political information;online content voters;encountering political information;political information search;political information;content voters encountering;internet learn politics;online political;social cues online;content voters;voters encountering political;voters reach judgments;information search evaluation;news online information;voters encountering;online information colored;subjects social cues;interact online content;effect social cues;social cues;information search;encountering political;americans turning internet;encouraged interact online;judgments similar informed;information internet typically;search evaluation subjects;online information", "pdf_keywords": ""}, "79f47ebf896b848e7c981c8aa6862ca1a7e5e7e5": {"ta_keywords": "adaptive geodesic kernels;kernel clustering limitations;locally adaptive kernels;kernel clustering reduces;kernel clustering;kernels kernel clustering;adaptive kernels;understanding kernel clustering;kernels prove bias;geodesic kernels;adaptive kernels directly;geodesic kernels kernel;theoretical understanding kernel;locally adaptive geodesic;clustering limitations principled;bias density mode;locally adaptive;data density inhomogeneity;bias density;adaptive geodesic;density equalization implicitly;clustering limitations;clustering reduces continuous;density equalization;class kernels;common class kernels;understanding kernel;kernels;data density;clustering reduces", "pdf_keywords": ""}, "0222a48657d554b2a5a3d7ec3bb0b6833b8970a1": {"ta_keywords": "hit diagnosis heparin;heparin induced thrombocytopenia;induced thrombocytopenia hit;diagnosis heparin induced;thrombocytopenia hit;thrombocytopenia aim study;diagnosis heparin;thrombocytopenia hit severe;induced thrombocytopenia aim;thrombocytopenia aim;hit performed serum;1087 heparin induced;induced thrombocytopenia;heparin treatments associated;immunoassay stic expert;heparin induced;heparin treatments;diagnosis hit npv;severe complication heparin;thrombocytopenia;complication heparin treatments;score blind antibody;complication heparin;abstract 1087 heparin;flow immunoassay stic;antibody test results;heparin;blind antibody test;patients suspected hit;hit diagnosis", "pdf_keywords": ""}, "8c5ba1c914eab16b705da03352fe69d5bcfc72ea": {"ta_keywords": "summarization framework trained;abstractive summarization models;summarization models suffer;abstractive summaries generating;generates abstractive summaries;abstractive text summarization;summarization models;document summarization models;abstractive summarization;summarization address challenges;summarization aims compressing;novel abstractive summaries;abstractive summaries;structure induction summarization;techniques abstractive summarization;single document summarization;summaries generating;document summarization;summarization framework;text summarization;summary summarization framework;summary summarization;text summarization aims;summarization address;induction summarization address;condensed summary summarization;abstractive summaries iii;summarization;summaries generating novel;summarization aims", "pdf_keywords": ""}, "7fc0097f6a51282dc1e9020d7c28e12cecaef519": {"ta_keywords": "xstream", "pdf_keywords": ""}, "3cfb319689f06bf04c2e28399361f414ca32c4b3": {"ta_keywords": "transfer learning nlp;learning unified text;transfer learning unified;format transfer learning;learning nlp;learning nlp release;learning techniques nlp;transfer learning;nlp introducing unified;transfer learning techniques;answering text classification;question answering text;techniques nlp;techniques nlp introducing;learning unified;transfer learning model;processing nlp;nlp release dataset;clean crawled corpus;limits transfer learning;nlp combining insights;nlp introducing;unified text text;question answering;nlp release;nlp combining;language processing nlp;unified text;text format transfer;data rich task", "pdf_keywords": "nlp tasks;transfer learning translate;learning techniques nlp;transfer learning;abstract transfer learning;transfer learning techniques;processing nlp tasks;nlp tasks requires;transfer learning uni\ufb01ed;learning uni\ufb01ed text;processing nlp;techniques nlp;transfer learning model;nlp problems including;techniques nlp introducing;nlp introducing uni\ufb01ed;nlp problems;language processing nlp;based nlp problems;nlp introducing;learning translate;english based nlp;natural language processing;limits transfer learning;nlp;nlp paper;processing nlp paper;learning takeaways text;based nlp;perform natural language"}, "d19e097f388ca12ff111989b2bac7d3cc3cf15ca": {"ta_keywords": "narratives capitol riots;riots detect networks;uncover coordinated messaging;text similarity graph;disinformation narratives;disinformation narratives related;coordinated messaging analysis;similar textual content;capitol riots parler;clusters posting similar;different disinformation narratives;riots parler;parler coordinating narratives;messaging analysis;user clusters posting;coordinated messaging;similar textual;clusters posting;posting similar textual;capitol riots;capitol riots detect;graph text text;messaging analysis user;user coordination network;text similarity;graph text;textual content;coordinating narratives capitol;text text similarity;user totext graph", "pdf_keywords": "coordinated messaging users;analyze coordinated messaging;uncover coordinated messaging;analyzed social media;coordinated messaging analysis;user clusters posting;messaging analysis;riots users;messaging analysis user;user coordination graph;social media data;riots users openly;social media usage;riots detect networks;clusters posting similar;textual content social;content social media;coordinated messaging;6th riots users;user coordination network;social media;clusters posting;coordinated user clusters;unusual online activity;messaging users;user user coordination;user coordination;accounts analyzed social;user clusters;analyzed activity users"}, "2c9158e20f58df04a6c5cd54dd3ee7d8df656421": {"ta_keywords": "sparse representations lucene;extending nmslib retrieval;nmslib retrieval systems;nmslib new retrieval;new retrieval toolkit;retrieval toolkit flexneuart;nmslib retrieval;retrieval toolkit;representations lucene;neural ranking;classic neural ranking;retrieval systems;representations lucene purely;nlp community nmslib;lucene purely dense;new retrieval;retrieval;retrieval systems work;neural ranking signals;sparse representations weights;lucene purely;purely sparse representations;sparse representations;lucene;ranking signals library;dense sparse representations;flexneuart efficiently retrieve;toolkit candidate generation;mixing ranking;flexible toolkit candidate", "pdf_keywords": "flexible retrieval nmslib;retrieval nmslib flexneuart;nmslib new retrieval;similarity search library;retrieval nmslib;space searching nmslib;search library nmslib;searching nmslib extendible;similarity search methods;metric space searching;retrieval toolkit flexneuart;new retrieval toolkit;similarity search;retrieval toolkit;space library nmslib;nn search library;evaluation similarity search;flexible retrieval;searching nmslib;search methods distance;metric space library;search library toolkit;platform similarity search;toolkit evaluation similarity;nmslib extendible library;nmslib non metric;space searching;library nmslib;library nmslib ef\ufb01cient;new search methods"}, "d864944df8e765d597484ace12dbc3ac99e950a9": {"ta_keywords": "learning proximal policy;proximal policy optimization;policy optimization heavy;policy gradient algorithms;modern policy gradient;policy optimization;policy gradient;policy optimization ppo;surrogate reward function;algorithms proximal policy;gradients ppo surrogate;ppo surrogate reward;surrogate reward;gradients especially actor;advantages surrogate reward;robust estimator ppo;gradient algorithms proximal;heavy tailed gradients;increases agent policy;learning proximal;optimization heavy tailed;agent policy diverges;gradients ppo;surrogate reward main;loss clipping gradient;tailed gradients;optimization ppo rely;tailedness increases agent;optimization ppo;tailed nature gradients", "pdf_keywords": "ppo clipping heuristics;ppo implementations gradient;tailedness ppo gradients;gradients ppo surrogate;ppo surrogate reward;clipping heuristics primarily;ppo gradients investigate;robust estimator ppo;gradient clipping actor;clipping heuristics;ppo clipping;clipping heuristics present;implementations gradient clipping;gradients ppo;ppo gradients;gradients demonstrated ppo;ppo substitute clipping;heavy tailedness ppo;ppo heuristics demonstrate;heavy tailedness gradients;standard ppo clipping;tailedness gradients perform;nature gradients ppo;ppo heuristics;tailedness ppo;demonstrated ppo clipping;surrogate reward function;surrogate reward;demonstrate clipping heuristics;ppo objective"}, "e6239cc789da289929d49ffed2c0a562213d4703": {"ta_keywords": "welding residual stress;welding ring ribbed;welding ring;alloy ring stiffened;stiffened cylindrical shell;ring stiffened cylindrical;fillet welding;welding residual;deformation introduced welding;structure influence welding;introduced welding ring;effect welding residual;ring stiffened;caused fillet welding;stiffened cylindrical;abstract ring stiffened;welding position constraint;titanium alloy ring;welding position;cylindrical shell results;residual stress deformation;stresses distortions titanium;effect welding;cylindrical shell;welding;stress deformation analyzed;cylindrical shell main;analysis residual stresses;alloy ring;influence welding", "pdf_keywords": ""}, "60e339d25d43c026cf96395aa8accf34eae744a5": {"ta_keywords": "crowdsourced pairwise comparisons;evaluation dataset crowdsourced;dataset crowdsourced pairwise;crowdsourced pairwise;pairs annotated crowdsourcing;dataset crowdsourced;crowdsourcing platform dataset;comparisons datasets capture;pairwise comparisons datasets;evaluation large scale;comparisons datasets;crowdsourcing squad ms;evaluation dataset;annotated crowdsourcing;dataset evaluating pairwise;crowdsourcing squad;pairwise comparison tasks;annotated crowdsourcing platform;crowdsourced;using crowdsourcing squad;crowdsourcing platform;crowdsourcing;evaluating pairwise comparisons;large scale dataset;dataset evaluating;dataset built compare;ms coco imagenet;comparison tasks;comprehensive evaluation large;pairs annotated", "pdf_keywords": "crowdsourced pairwise comparisons;dataset crowdsourced pairwise;evaluation dataset crowdsourced;dataset evaluation pairwise;dataset evaluation;dataset crowdsourced;dataset peopleage;evaluation dataset;genders people photos;dataset evaluating pairwise;crowdsourced pairwise;dataset evaluating;people photos imdb;crowdsourcing platform dataset;open dataset crowdsourced;dataset peopleage 12;dataset uses age;toolkit crowdsourcing 11;scale dataset evaluation;wiki 10 dataset;people photos;crowdsourcing 11;pairs annotated crowdsourcing;datasets;evaluation large scale;benchmark computer vision;large scale dataset;dataset built compare;dataset;pairwise comparisons building"}, "4ef77ef9b8ca8c8a96f1e62fa86a988feb582bd1": {"ta_keywords": "domain adaptation neural;offs domain adaptation;domain adaptation;language model adaptation;adaptation neural language;adaptation concepts machine;neural language models;language models;model adaptation concepts;model adaptation;adaptation neural;language models work;small domain set;concepts machine learning;language model;adaptation concepts;training model set;outof domain set;domain set small;connects language model;domain set;learning theory;neural language;training model;set small domain;sets distance underlying;learning;model set depends;learning theory consider;concepts machine", "pdf_keywords": "domain adaptation;domain adaptation neural;language model adaptation;offs domain adaptation;domain adaptation techniques;analysis domain adaptation;adaptation neural language;adaptation concepts machine;importance sampling contrastive;selection importance sampling;language modeling domain;importance sampling intelligent;derived importance sampling;methods importance sampling;importance sampling;neural language models;language models;adaptation neural;model adaptation;model adaptation concepts;importance sampling future;adaptation concepts;combines language modeling;training combines language;language modeling;concepts machine learning;contrastive data selection;data selection importance;language model;importance sampling owen"}, "09ec8d8e2251e079abb0e109979f33ee120211fa": {"ta_keywords": "proximal extragradient method;accelerated proximal extragradient;tensor method derivatives;hessian optimized;gradient hessian optimized;hessian evaluations optimized;proposed tensor method;optimized function tensor;tensor method 2018;extragradient method;existing tensor methods;tensor methods order;tensor method;tensor methods;function tensor method;hessian optimized function;extragradient method 2013;svaiter method optimal;proximal extragradient;hessian evaluations;gradient hessian evaluations;hybrid proximal extragradient;smooth convex optimization;iteration comparable newton;outer accelerated proximal;convex optimization;convex optimization problems;method optimal;accelerated proximal;hessian", "pdf_keywords": ""}, "a8a863e85a95919773868204d672f1260e0058ce": {"ta_keywords": "neural machine translation;machine translation parameter;translation parameter generator;translation nmt models;machine translation nmt;monolingual data training;machine translation;translation parameter;translation nmt;universal model translate;generation universal neural;contextual parameter generator;language embeddings input;target language embeddings;model translate;zero shot translation;contextual parameter generation;language specific parameterization;universal neural machine;neural network contextual;language embeddings;parameters weights neural;translate multiple languages;parameters encoder decoder;parameter generation universal;existing neural machine;modification existing neural;generates parameters encoder;domain adaptation;parameters encoder", "pdf_keywords": "neural machine translation;parameters shared translation;embeddings context translation;machine translation;contextual parameter generator;translation model language;machine translation emmanouil;generation universal neural;shared translation model;monolingual data training;translation model;machine translation nmt;contextual parameter generation;parameter generation universal;universal model translate;translation nmt models;universal language encoder;translation uses generate;language speci\ufb01c parameterization;universal neural machine;context translation uses;model translate;context translation;new contextual parameter;zero shot translation;language embeddings context;shared translation;shot translation able;learns language embeddings;model language pairs"}, "d82592f3a110308366dfc7c42565d437b5bf59af": {"ta_keywords": "automated social skills;social skills training;social skill speech;social skills trainer;skills social skills;social skills;social skills social;skill speech language;skills social;training developing dialogue;social skill;provides social skills;performing social skills;improve social skills;skills training human;process social skills;interaction appropriate skills;skill speech;autistic traits;social interaction;training human computer;features relationship autistic;developing dialogue;recognizes user speech;user speech;skills trainer provides;automated social;human computer interaction;autistic;skills training proposed", "pdf_keywords": ""}, "5dab371fecc43904c0b785a50136d20cee43a99a": {"ta_keywords": "machine translation trained;translated speech attention;speech machine translation;translation trained parallel;translation corpora models;speech translation experiments;recognition translation tasks;recognition machine translation;translation tasks;translation trained;translation tasks propose;machine translation corpora;machine translation;trainable models attention;trainable recognition translation;end speech translation;speech translation;corpus translated speech;attention passing models;speech attention;models attention;speech translation traditionally;translation experiments;trained parallel texts;translation corpora;trained corpus;formulation model attention;model attention;trained corpus transcribed;translated speech", "pdf_keywords": "machine translation trained;attentional models models;direct attentional models;attentional models;trainable models attention;speech machine translation;tasktrainable recognition translation;translation trained parallel;recognition translation tasks;translation tasks;models attention;translation trained;translation tasks propose;machine translation;model attention;formulation model attention;model attention stages;attention passing model;trained parallel texts;models attention mechanisms;speech translation;abstract speech translation;speech translation traditionally;attentional;trained corpus;trained corpus transcribed;transcribed speech machine;attention;recognition translation;variant proposed attention"}, "461188735d46dc1062f5d1d382d940a24c355fad": {"ta_keywords": "automatically extract relationship;relation classifier;relation classifier aggregating;extract relationship sentence;entity pairs;sentence level relation;extract relationship;level relation classifier;entity pairs large;corpus locatednear relation;scores entity pairs;pairs large corpus;relationship sentence level;relation kind commonsense;relation;locatednear relation kind;corpus;large corpus;relationship sentence;commonsense knowledge describing;relation kind;locatednear relation;entity;commonsense knowledge;aggregating scores entity;knowledge describing;sentence level;kind commonsense knowledge;relationship;classifier aggregating", "pdf_keywords": ""}, "f6be5d90199d1644b85e6b41a7a7f42fb29dbc9a": {"ta_keywords": "learning spatial abilities;children ospt ability;spatial abilities required;spatial ability stem;spatial abilities;spatial ability;training children ospt;ospt training children;spatial ability general;contribution spatial ability;young children ospt;children ospt performance;ospt ability relation;significantly children ospt;children ospt;ospt training;differences ospt ability;stem learning spatial;ospt ability;ospt assessment;ospt ability general;appropriate ospt assessment;ability stem learning;probably spatial ability;objects ability;ospt ability 1st;receive ospt training;learning spatial;specific skill debate;age appropriate ospt", "pdf_keywords": ""}, "a278c07c8bd2921e59dd862cd91a0540dd340030": {"ta_keywords": "estimation treatment effects;estimation treatment;estimate treatment;estimate treatment effect;regularization induces bias;networks estimation treatment;targeted regularization induces;propensity score estimation;neural networks estimation;procedure targeted regularization;literature estimation treatment;final estimate treatment;targeted regularization;treatment propensity score;treatment propensity;probability treatment propensity;regularization procedure targeted;regularization induces;treatment effects observational;induces bias models;bias models;regularization;adjustment second regularization;training neural networks;neural networks used;regularization procedure;treatment effects fit;neural networks;bias models non;training neural", "pdf_keywords": "estimation treatment effects;estimate treatment effect;estimation treatment;estimation adjustment neural;adjustment neural networks;estimate treatment;adapting neural networks;neural networks estimation;networks estimation treatment;\ufb01nal estimate treatment;treatment effects observational;literature estimation treatment;adapting neural;adjustment neural;propensity score estimation;neural networks used;effects observational data;training neural networks;treatment effect;estimation adjustment;neural networks;treatment effects;effects observational;models downstream estimator;networks estimation;2019 adapting neural;treatment effects \ufb01rst;use neural networks;training neural;data propose adaptations"}, "9c5c794094fbf5da8c48df5c3242615dc0b1d245": {"ta_keywords": "learning disentangled representations;unsupervised learning disentangled;disentanglement learned representations;disentanglement learning explicit;learning disentangled;disentanglement learning;disentanglement learned;disentangled representations fundamentally;enforcing disentanglement learned;disentangled representations;disentangled models;disentangled models seemingly;work disentanglement learning;losses disentangled models;disentangled representations real;biases implicit supervision;recovered unsupervised learning;disentangled;enforcing disentanglement;idea unsupervised learning;losses disentangled;unsupervised learning;increased disentanglement;learned representations;theoretically unsupervised learning;corresponding losses disentangled;benefits enforcing disentanglement;learning downstream tasks;impossible inductive biases;furthermore increased disentanglement", "pdf_keywords": "unsupervised learning disentangled;disentanglement learned representations;learning disentangled representations;unsupervised disentanglement learning;disentanglement learning explicit;disentanglement learning;learning disentangled;disentanglement learned;disentangled representations fundamentally;disentangled representations;work disentanglement learning;enforcing disentanglement learned;unsupervised disentanglement;disentangled representations arxiv;disentangled representations real;biases implicit supervision;disentanglement methods disentanglement;unsupervised learning;recovered unsupervised learning;protocol unsupervised disentanglement;disentanglement methods;disentangled;bene\ufb01ts enforcing disentanglement;increased disentanglement;idea unsupervised learning;state art disentanglement;learned representations;learning downstream tasks;disentanglement;theoretically unsupervised learning"}, "2a2d03a1534b365c5b048c824c0886e16ccf7dfa": {"ta_keywords": "text knowledge base;parsed text knowledge;extracting relational knowledge;relational knowledge text;learns syntactic semantic;relations knowledge base;binary relation prediction;syntactic semantic inference;large knowledge base;semantic inference rules;freebase reading relational;extracting relational;knowledge base freebase;text knowledge;knowledge base;knowledge text;syntactic patterns extraction;existing knowledge base;reading relational information;learns syntactic;web text corpus;semantic inference;knowledge base previous;text existing knowledge;relational information large;knowledge relations knowledge;model learns syntactic;knowledge text potential;inference rules binary;relational knowledge", "pdf_keywords": ""}, "535c58ec8020782d41ed3ca72cf94aff7fd65120": {"ta_keywords": "acoustic models speech;models speech recognition;acoustic models;speech recognition research;models speech;automatic speech recognition;speech recognition;prospects automatic speech;speech recognition recent;automatic speech;acoustic;models;recognition research;recognition;speech;recognition recent progress;recognition recent;future prospects automatic;automatic;prospects automatic;future prospects;progress future prospects;progress future;recent progress future;prospects;future;recent progress;progress;research;recent", "pdf_keywords": ""}, "4c93bcc05d6b41cb3df451d703f34bab9c9e9201": {"ta_keywords": "differentially private text;private text generation;add privacy preserving;privacy utility differentially;differential privacy;privacy amplification;private text mechanisms;privacy utility;privacy preserving noise;privacy preserving;form differential privacy;private text;designing differentially private;quantifiable privacy;add privacy;differential privacy provide;tradeoff privacy utility;quantifiable privacy guarantees;privacy;noise privacy amplification;ensuring quantifiable privacy;privacy provide guarantees;differentially private;privacy guarantees;privacy guarantees provides;balancing tradeoff privacy;privacy amplification step;mechanisms add privacy;defers noise privacy;privacy provide", "pdf_keywords": "privacy preserving noise;privacy outputs curator;privacy outputs;defers noise privacy;privacy preserving;add privacy preserving;noise privacy;es privacy outputs;analysis introduce privacy;privacy framework euclidean;privacy ampli cation;noise privacy ampli;introduce privacy ampli;privacy;privacy ampli;noisy vectors propose;built d\u03c7 privacy;d\u03c7 privacy framework;introduce privacy;d\u03c7 privacy;noisy vectors;di erent methods;preserving noise;preserving noise vectorial;add privacy;di erent techniques;projection noisy vectors;es privacy;privacy framework;noise data"}, "96abcdded2985bd44b9514e28f5b8da4fa1e4371": {"ta_keywords": "mythos model interpretability;model interpretability;interpretability machine learning;model interpretability machine;concept interpretability important;concept interpretability;interpretability;interpretability important;interpretability machine;interpretability important slippery;learning concept interpretability;machine learning concept;mythos model;machine learning;mythos;model;learning concept;learning;concept;important slippery;slippery;machine;important", "pdf_keywords": ""}, "18e8646001fc53465fdc8f8eb01523e24c134493": {"ta_keywords": "scores induced ranking;estimators rely ranking;cardinal scores arbitrary;cardinal scores induced;induced ranking;ranking consider cardinal;induced ranking work;consider cardinal scores;consistent induced ranking;rely ranking;rankings rates approaches;cardinal scores;rating based estimators;rankings;ranking;bias scores;scores arbitrary;rankings rates;rely ranking consider;induced ranking paper;biases bias scores;ratings strictly uniformly;ranking work provides;using rankings rates;using rankings;scores arbitrary adversarially;higher using rankings;ranking work;practice cardinal scores;ordinal data ranks", "pdf_keywords": ""}, "f78e5aaf34cc1e4874490e9155c640b73c630021": {"ta_keywords": "games gradient conjectures;learning incorporates opponent;gradient conjectures strategic;sum games gradient;multi agent learning;games gradient;opponents optimize surrogate;agent learning;agent learning review;learning games;strategic multi agent;dynamic behavior opponents;general sum games;strategy belief;behavior opponents optimize;opponents optimize;conjectures strategic;strategy belief opposing;incorporates opponent behavior;sum games;conjectures strategic multi;learning games primarily;update strategy belief;opponent behavior;opponent behavior continuous;opposing players;multi agent;learning rules recent;conjectural learning demonstrate;learning rules", "pdf_keywords": ""}, "f132f6534ec326a1ba61870b701015cd3d1560a2": {"ta_keywords": "tuning domain adaptation;translation domain adaptation;domain adaptation neural;domain adaptation;selection domain classifiers;machine translation domain;selection language modeling;pretraining selected data;selection fine tuning;contrastive data selection;fine tuning domain;tuning domain;selection domain;data selection domain;training fine tuning;pretraining selected;modeling machine translation;complementarity data selection;selected data training;adaptation neural networks;adaptation neural;selection language;domain classifiers effective;tuning complementarity data;pretraining performance;translation domain;data selection language;domain classifiers;machine translation;training phases pretraining", "pdf_keywords": "tuning domain adaptation;domain adaptation neural;domain adaptation;domain adaptation dan;domain adaption neural;abstract domain adaptation;contrastive data selection;adaptation neural;neural machine translation;complementarity data selection;adaption neural language;machine translation complementarity;translation complementarity data;selection methods contrastive;adaptation neural networks;data selection discriminative;selection language modeling;tuning dataset ii;fine tuning domain;tuning domain;data selection language;selection fine tuning;tuning dataset;selected data training;adaption neural;outperform contrastive scoring;pretraining selected data;machine translation conditions;language modeling neural;selection discriminative"}, "835587dbe94b70adeb0b16384e10bb6e0e29de84": {"ta_keywords": "stable matching;stable matching uncertain;uncertain pairwise preferences;matching uncertain pairwise;pairwise preferences;matching uncertain;matching;uncertain pairwise;pairwise;preferences;stable;uncertain", "pdf_keywords": ""}, "97bcea32979ed602fd404448a4e4cedad4171d79": {"ta_keywords": "predict nonverbal behavior;generate nonverbal behaviors;individual nonverbal behaviors;nonverbal behaviors spoken;nonverbal behaviors match;labelled nonverbal behaviors;nonverbal behaviors;nonverbal behaviors body;nonverbal behavior label;generating individual nonverbal;models predict nonverbal;nonverbal behavior;nonverbal behavior varies;language nonverbal behavior;personality virtual agent;individual nonverbal;predict nonverbal;automatically generate nonverbal;nonverbal behaviors big;generate nonverbal;spoken language nonverbal;personality virtual;expected personality virtual;language nonverbal;important generate nonverbal;labelled nonverbal;nonverbal;created dialogue corpus;behaviors spoken language;dialogue corpus", "pdf_keywords": ""}, "89c148d3d4edcb7b13c35da36b97ffb881c38058": {"ta_keywords": "el speech enhancement;speech enhanced el;enhanced el speech;el speech enhanced;speech enhancement;microphone enhanced el;speech enhanced;electrolaryngeal el speech;el speech framework;speech enhancement methods;microphone enhanced;sounds enable laryngectomees;prediction electrolarynx device;feature prediction electrolarynx;el speech recorded;proposed el speech;speech framework effective;speech recorded microphone;excitation prediction effective;enable laryngectomees;enable laryngectomees produce;laryngectomees produce electrolaryngeal;speech framework;speech methods;electrolarynx based;control electrolarynx based;excitation sounds enable;statistical excitation prediction;microphone;recorded microphone enhanced", "pdf_keywords": ""}, "b8dcc2ae3346e41a421232169c2ca07957c654c4": {"ta_keywords": "evolutionary game theory;coevolution agents games;evolutionary game;infinitely populations agents;evolutionary learning dynamic;initializations agents games;games play evolve;dynamic agents;population dynamic agents;game theory;game theoretic;evolve strategically time;chaotic coevolution agents;paradigm evolutionary game;archetypal game theoretic;play evolve strategically;game theoretic setting;dynamic agents interact;competition evolves adversarially;learning dynamic replicator;evolve strategically;competition evolves;agents static games;sum competition evolves;coevolution agents;evolves adversarially;play evolve;learning dynamic;game theory generally;generalizations studied evolutionary", "pdf_keywords": ""}, "135ace829b6ad2ec9db040d8e5fd137034e83665": {"ta_keywords": "crfs semi markov;crfs conditionally trained;semi crfs conditionally;features semi crfs;semi markov chains;algorithms semi crfs;semi markov;semi crfs;fields semi crfs;crfs conditionally;semi markov conditional;inference algorithms semi;crfs semi;random fields semi;version semi markov;semi crfs measure;crfs polynomial time;intuitively semi crf;exact learning inference;conventional crfs semi;conditional random fields;segment non markovian;crfs;exact learning;crfs measure;semi crf;semi crfs polynomial;crfs measure properties;markov conditional random;semi crf input", "pdf_keywords": ""}, "635932ee917d71e01f07211c0359abf3e0e65e47": {"ta_keywords": "speech recognition asr;end automatic speech;automatic speech;speech recognition;automatic speech recognition;speech recognition e2e;field automatic speech;recognition asr;sequence connectionist temporal;connectionist temporal classification;optimal asr end;recognition asr formulation;sequence connectionist;token sequence connectionist;predict applied asr;training insertion based;temporal classification ctc;context optimal asr;right decoding;asr end end;optimal asr;insertion based models;left right decoding;asr end;recognition e2e models;right decoding consider;decoding;training insertion;output token sequence;token sequence addition", "pdf_keywords": "insertion transformer trained;performance autoregressive transformer;transformer trained;transformer trained l2r;training insertionbased models;autoregressive transformer;autoregressive transformer beam;autoregressive transformer similar;transformer similar decoding;competitive autoregressive transformer;strong autoregressive transformer;insertion transformer;training insertionbased model;transformer beam search;joint modeling connectionist;joint training insertionbased;modeling connectionist temporal;insertion based models;trained l2r prior;insertionbased models ctc;training insertionbased;beam search models;search models trained;insertionbased models;trained bbt prior;models trained;models trained bbt;transformer;modeling connectionist;indigo insertion transformer"}, "b30195763eb103e2e5564228119f3810ab423b2e": {"ta_keywords": "labeled documents learning;documents learning unlabeled;classified text classification;text classification;labeled documents training;supervised words;classification using labeled;text classification methods;current text classification;classification models unlabeled;learning unlabeled;data supervised words;learning unlabeled data;using labeled documents;labeled documents;categories classified text;labeled examples based;classification using label;unlabeled data supervised;human labeled documents;sentiment classification;labeled documents humans;classified text;topic sentiment classification;text classification using;labeled examples;label class train;using labeled;label names;data using labeled", "pdf_keywords": "label text classi\ufb01cation;words label names;related words label;lotclass model label;words label;text classi\ufb01cation methods;text classi\ufb01cation built;label names supervision;classi\ufb01cation using label;labeled documents training;semantically related words;word level classi\ufb01cation;words label using;label names language;text classi\ufb01cation;text classi\ufb01cation using;\ufb01cation label names;current text classi\ufb01cation;vocabulary class;correlated words label;semantically correlated words;names language model;text classi \ufb01cation;construct category vocabulary;using label names;classi \ufb01cation label;label names;label replacement word;category vocabulary;category vocabulary class"}, "002c256d30d6be4b23d365a8de8ae0e67e4c9641": {"ta_keywords": "powerful language models;language models;language models form;language modelling lm;million parameter models;language modelling;recurrent architectures;form recurrent architectures;models form recurrent;data language modelling;training data language;formulation neural networks;parameter models;increased memorization training;training compute model;recurrent architectures graves;computations training;additional computations training;models;increased memorization;memorization training;training inference time;neural networks;modelling lm;training inference;powerful language;parameter models seminal;formulation neural;model parameters benefits;computations training inference", "pdf_keywords": "scaling language models;language models massive;augmenting language models;improving language models;retrieval large text;suggest retrieval large;retrieval large;large text database;massive scale memory;language models;language models using;language models explicit;scaling language;models explicit memory;retrieval enhanced;memory unprecedented scale;general knowledge extraction;scale memory;knowledge extraction;improving language;speci\ufb01cally suggest retrieval;retrieval;augmenting language;retrieving databases trillions;suggest retrieval;models massive scale;present retrieval enhanced;databases trillions;model size database;explicit memory unprecedented"}, "933396f5b9111f6acdd76710ee6ab4d24e8673dd": {"ta_keywords": "speech text datasets;asr semi supervised;unpaired speech text;speech text data;text data speech;speech recognition asr;semi supervised training;encoded speech text;end automatic speech;automatic speech;novel semi supervised;speech data intermediate;speech data;automatic speech recognition;data speech;data speech data;autoencoding text data;paired speech text;shared encoder improves;representations speech text;encoder improves feature;encoded speech;recognition asr semi;semi supervised;large unpaired speech;proposed semi supervised;speech recognition;representation speech text;speech text sequences;dissimilarity encoded speech", "pdf_keywords": ""}, "4c49e9b57c8fc58b0df29a27ecca8cc2e376f02b": {"ta_keywords": "collaborative filtering cf;web spider lists;useful collaborative filtering;web lists semantically;collaborative filtering;collects web lists;web search engines;web lists;spider lists used;spider lists;search engines;data useful collaborative;search engines pages;autonomous web;using autonomous web;pages importantly spider;spider collects web;web search;collects web;useful collaborative;autonomous web spider;cf spider collects;commercial web search;lists semantically related;lists pages importantly;cf spider uses;sites data;spider collect useful;lists pages;sites data collected", "pdf_keywords": ""}, "7d5f83cb234a5640487bd258ace06d9dc967d222": {"ta_keywords": "statistical relational learning;information extraction reasoning;relational learning;joint information extraction;information extraction;extraction reasoning scalable;relational learning approach;learning reasoning;relational learning model;learning reasoning tasks;relational learning involves;information extraction errors;statistical relational;knowledge base;extraction reasoning key;extraction reasoning;joint learning inference;scalable statistical relational;propose statistical relational;constructs knowledge base;learning inference;performs learning reasoning;wikipedia datasets;world wikipedia datasets;knowledge base kb;wikipedia datasets multiple;learning inference sl;pipeline statistical relational;domains joint learning;propagate reasoning task", "pdf_keywords": ""}, "f1c7419b87cbf853e691e500643f71720b68fb86": {"ta_keywords": "stacked graphical learning;online learning stacked;learning stacked learning;collective classification widely;stacked learning;collective classification methods;learning stacked;collective classification;existing collective classification;online stacked graphical;stacked learning save;learning scheme stacked;online stacked;overhead online stacked;graphical learning;large streaming datasets;stacked graphical;graphical models learning;graphical learning gives;maintaining large graphs;online learning scheme;graphical learning able;large graphs;streaming datasets;labels simultaneously relational;simultaneously relational data;networks data relational;online learning;social networks data;streaming datasets minimal", "pdf_keywords": ""}, "9b3fd2525a2d1abc44145308e013f117d3d7bdee": {"ta_keywords": "el speech enhancement;enhanced el speech;speech enhancement;microphone enhanced el;speech enhancement technique;speech enhancement methods;develop el speech;el speech produced;speech produced laryngectomee;el speech keeping;microphone enhanced;el speech recorded;speech produced;proposed el speech;speech keeping intelligibility;el speech f0;speech recorded microphone;laryngectomee articulation excitation;recorded microphone enhanced;voice conversion;statistical voice conversion;el speech;el speech presented;using statistical voice;enhanced el;statistical excitation prediction;microphone;speech recorded;speech presented loudspeaker;statistical voice", "pdf_keywords": ""}, "1afa3ab80abda57920b8d456a6513e6f01cc82e7": {"ta_keywords": "event prediction conversations;event forecasting conversations;forecasting conversations regression;survival text regression;prediction conversations;forecasting conversations;conversations regression tasks;prediction conversations allows;predicting length conversation;conversations regression;event prediction tasks;conversation modelling applications;conversation modelling;text regression;prediction tasks common;prediction tasks;text regression time;common conversation modelling;survival regression commonly;predictions regression tasks;known survival regression;time event prediction;survival regression;event prediction;engineering survival text;survival text;tasks common conversation;event forecasting;regression tasks recent;classification tasks", "pdf_keywords": ""}, "5cfdb162ffa4dce18f7c576d352bd459b6a11292": {"ta_keywords": "driven personalized coupons;personalized coupons challenging;personalized coupons;promotions exploit consumers;coupons challenging;discount coupons;limited discount coupons;based party coupon;coupons challenging discover;coupons;temporal dynamics purchase;coupon;urgency boost sales;discount coupons partner;coupon provider;time limited promotions;coupon provider specifically;purchases incorporates temporal;shopping triggering;coupons partner;party coupon;party coupon provider;captures correlations purchases;time limited discount;future purchases based;forecast future purchases;promotions exploit;limited promotions exploit;dynamics purchase behavior;consumers sense urgency", "pdf_keywords": ""}, "1ddcc9671dd6486e34cefadfe71bbbc1bc55035a": {"ta_keywords": "neural machine translation;pretrained word embeddings;word embeddings low;source word embeddings;machine translation;embeddings utilizing subword;machine translation achieved;subtitles naacl hlt;subtitles naacl;subword character level;corpus ted subtitles;word embeddings;standard word embeddings;subtitles;subword information consistently;word embeddings work;model pretrained word;morphologically rich language;ted subtitles naacl;data word embeddings;embeddings work arabic;embeddings word similarity;word embeddings word;arabic morphologically rich;embeddings low resource;subword character;embeddings word;experiments small corpus;level models nlp;word based nmt", "pdf_keywords": ""}, "33d2ebe41477811296abc4077bf9ce09b927ef98": {"ta_keywords": "voice conversion effectively;voice conversion;voice conversion framework;propose voice conversion;voice conversion studies;framework voice conversion;utterances speaker model;content spoken speakers;integrates speaker gmm;model parallel utterances;density model speaker;spoken speakers;framework integrates speaker;parallel utterances speaker;speaker model achieve;utterances speaker;speaker model nonparallel;speaker gmm;voice;transform function speakers;speaker gmm target;model speaker model;integrates speaker;model speaker;gaussian mixture model;speaker model;parallel utterances;source target speakers;target speakers widely;based gaussian mixture", "pdf_keywords": ""}, "c0e8846eb5ce574a6dca3f3a600e82b184339254": {"ta_keywords": "rewards language context;inferring rewards language;infers rewards language;rewards language pragmatically;rewards language;contexts inferring rewards;language conveys information;language conveys;utterances elicit desired;language context;language context present;choose utterances elicit;language pragmatically reasoning;language;utterances elicit;language pragmatically;inferring rewards;elicit desired actions;pragmatically reasoning speakers;language like;infers rewards;actions new contexts;user underlying reward;utterances;model infers rewards;choose utterances;conveys information;instruction following language;language like like;conveys information user", "pdf_keywords": "language rewards flightpref;rewards language;infers rewards language;rewards language pragmatically;mapping language rewards;language improve reward;language rewards;task natural language;language actions;booking task natural;language descriptions actions;model infer reward;infer reward functions;improve reward estimates;utterances elicit desired;infer reward;language actions instruction;booking game;\ufb02ight booking game;infers rewards predicts;actions rewards inverse;rewards inverse reinforcement;booking game evaluate;maps language actions;inferred rewards accurately;interactive \ufb02ight booking;accurately infers rewards;actions rewards;use inferred rewards;inferred rewards"}, "a3cc75975a5998d5a7dd494e70a479ba0a550013": {"ta_keywords": "tutoring authoring demonstration;authoring tutoring decision;tutoring decision;model generated tutoring;tutoring tends precise;generated tutoring;generated authoring tutoring;tutoring decision making;expert model authoring;tutor tutoring simstudent;tutoring simstudent;generated tutoring makes;simstudent performance tutoring;tutoring simstudent applied;building intelligent tutoring;tutoring authoring;authoring demonstration approach;existing cognitive tutor;cognitive tutor tutoring;authoring tutoring;intelligent tutoring;tutoring;cognitive tutor;expert model generated;performance tutoring;precise authoring demonstration;tutoring makes correct;authoring tutoring tends;expert model;authoring demonstration requires", "pdf_keywords": ""}, "04f4e55e14150b7c48b0287ba77c7443df76ed45": {"ta_keywords": "physical commonsense reasoning;reasoning physical commonsense;task physical commonsense;answer physical commonsense;physical commonsense natural;question answering abstract;commonsense reasoning corresponding;physical commonsense;commonsense reasoning;physical commonsense questions;interaction question answering;commonsense natural language;question answering;progress question answering;question answering piqa;kind physical commonsense;natural language understanding;learn reliably answer;reliably answer physical;commonsense natural;commonsense pose challenge;ai systems learn;models bert progress;answering abstract;reasoning physical;models bert;questions experiencing physical;commonsense questions experiencing;pretrained models bert;natural language", "pdf_keywords": "physical commonsense knowledge;physical commonsense reasoning;physical commonsense understanding;task physical commonsense;modeling physical commonsense;progress physical commonsense;commonsense reasoning corresponding;physical commonsense;commonsense reasoning;commonsense knowledge;aicompleteness including robots;interaction question answering;dataset physical interaction;commonsense understanding;including robots interact;question answering;answering piqa dataset;language models useful;commonsense understanding goal;question answering piqa;commonsense knowledge major;understand natural language;robots interact world;models useful nlp;language models;including robots;progress language representations;knowledge existing models;commonsense;robots interact"}, "5e0f2f82b4a28d59f1aa8b8ffe497790de1cdf9d": {"ta_keywords": "reinforcement learning wild;deep reinforcement learning;deep reinforcement;learn reward shaping;reinforcement learning;use deep reinforcement;agent avoid catastrophic;reward shaping;shaping reward function;reinforcement;shaping reward;learn reward;reward shaping accelerates;catastrophes bad agents;objective shaping reward;structure learn reward;reward function away;reward function;learning wild hope;learning wild;predict probability catastrophe;policies repeated catastrophes;shaping accelerates learning;catastrophic states use;learning objective shaping;penalize learning;catastrophic states;learning predict probability;avoid catastrophic;accelerates learning guards", "pdf_keywords": ""}, "7a9f4a8a99f9a38e9213da890f9eab6150ae928e": {"ta_keywords": "learnable proximity measure;personalized pagerank;personalized pagerank known;based personalized pagerank;learnable proximity;novel learnable proximity;pagerank known random;pagerank;pagerank known;learning similarity measures;proximity measure based;learning similarity;similarity measures;similarity measures based;edge label sequence;proximity measure;based random walks;random walks;edge label;sequence learning similarity;known random walk;label sequence learning;measure based personalized;edge label prior;weight edge label;similarity;random walk;proximity;random walk reset;label sequence", "pdf_keywords": ""}, "ebaae38a09c5a4909049e16af759c71db9cc87dc": {"ta_keywords": "question answering;language inference reduces;question answering natural;natural language inference;answering natural language;applications question answering;natural language processing;removing words possible;language inference;words model confidence;words model;inference reduces inputs;neural models crucial;removing words;weaknesses neural models;just words model;inference reduces;tasks removing words;language processing tasks;inputs just words;words possible input;weaknesses neural;neural models;natural language;language processing;extreme natural language;answering natural;exposing weaknesses neural;models crucial improving;neural", "pdf_keywords": ""}, "b17fa6625681d99370122145ba9911f701dd92cb": {"ta_keywords": "semantic parsing context;parsing context;parsing context present;parsing context received;typical context modeling;world semantic parsing;13 context modeling;decoding semantic parser;parsing context evaluate;study semantic parsing;semantic parsing;context modeling;semantic parser;semantic parser adapt;context modeling methods;13 context;decoding semantic;frequent contextual;study context modeling;challenging complex contextual;parsing;contextual;recently semantic parsing;evaluate 13 context;complex contextual;parser;context evaluate 13;context;world semantic;parser adapt typical", "pdf_keywords": "13 context modeling;modeling contextual clues;typical context modeling;modeling contextual;inference modeling contextual;context modeling;pronouns inference modeling;decoding semantic parser;decoding semantic;context modeling methods;generalize frequent contextual;semantic parser;frequent contextual;contextual clues explicit;semantic parser adapt;contextual;13 context;contextual clues;summarize frequent contextual;grammar based decoding;better pronouns inference;evaluate 13 context;based decoding semantic;sense better pronouns;context;pronouns inference;parser;frequent contextual phenomena;adapt typical context;grained analysis representative"}, "bf4da952df7a6ef9c0b2be8b4b4b69ad63848b8f": {"ta_keywords": "predict traffic;traffic speed prediction;available predict traffic;historical traffic data;predict traffic speed;spatiotemporal features traffic;learning intelligent transportation;transfer learning framework;features traffic speed;features traffic;transfer learning;historical traffic;traffic data;propose transfer learning;speed prediction;traffic data available;traffic;traffic speed;traffic speed target;speed prediction benefit;historical data target;data available predict;little historical traffic;intelligent transportation systems;intelligent transportation;prediction;prediction benefit wide;utilize historical data;machine learning intelligent;transportation", "pdf_keywords": ""}, "879cd78b0d4413aef614bc6b6cce075e8e6ad4be": {"ta_keywords": "existing streaming codes;existing streaming code;achieving streaming code;streaming codes best;streaming codes;streaming code;streaming codes designed;models streaming codes;streaming codes incorporates;adversarial packet erasure;streaming code constructions;packet erasure channels;arrivals streaming codes;streaming codes class;codes encode stream;model streaming codes;encode stream;packet erasure channel;achieving streaming;streaming codes variable;existing streaming;stream source packets;encode stream source;streaming;construction streaming codes;stream;stream source;constructions adversarial packet;size arrivals streaming;using existing streaming", "pdf_keywords": ""}, "bd318e959236b0d33a7567b6d3afc8d5e92b8ea3": {"ta_keywords": "bounded ai ethical;ethically bounded ai;ai ethical properties;ethical boundaries ai;ai ethical;specifications ai agents;boundaries ai agents;building ethically bounded;ai agents;goal specifications ai;ai concrete examples;modular approach ai;inherent making ai;notion ethically bounded;boundaries ai;ethically bounded;making ai;specifications ai;bounded ai concrete;bounded ai;ai;making ai robust;ai agents combine;ethical properties component;approach ai;ai robust flexible;humans ai;ai agents deployed;reason ethical boundaries;ethical properties", "pdf_keywords": "bounded ai ethical;ethically bounded ai;ai ethical;ai ethical properties;building ethically bounded;ai social;ai social sciences;ai agents;bounded ai;ai;ethically bounded;ai world;ai world humans;ai concrete examples;notion ethically bounded;ai research;ai drive ai;directions ai social;abstract ai agents;humans ai;social sciences ai;bounded ai concrete;approach ai;world humans ai;ai agents deployed;discussion directions ai;abstract ai;drive ai research;ai systems work;use ai"}, "4a93f7654f795871ed99dece2e1805e4950fd194": {"ta_keywords": "learning spatial language;referent spatial utterances;utterances spatial;understand utterances spatial;spatial utterances;learning spatial;spatial language;utterances spatial configurations;referent spatial;spatial utterances level;learns understand utterances;referents landmarks space;spatial language evaluate;utterances physical locations;spatial;symbolic representations utterances;latent semantic relations;latent semantic;representations latent semantic;framework learning spatial;representations utterances;landmarks space;recovers referent spatial;comprehension data human;understand utterances;utterances;generative probabilistic framework;generative probabilistic;representations utterances physical;human english speakers", "pdf_keywords": ""}, "9e172f35b2b0ebcff090f01d40e61fa5aecefa68": {"ta_keywords": "adversarial losses discriminative;adversarial loss divergence;losses discriminative adversarial;adversarial loss functions;adversarial losses;adversarial loss;various adversarial loss;understanding adversarial losses;functions adversarial loss;adversarial training objectives;adversarial training;adversarial losses combining;set adversarial losses;adversarial losses decoupling;component functions adversarial;designing adversarial training;discriminative adversarial;discriminative adversarial network;deeper understanding adversarial;understanding adversarial;adversarial;adversarial network;functions adversarial;various adversarial;adversarial network setting;designing adversarial;loss functions training;training generative discriminative;set adversarial;proposed various adversarial", "pdf_keywords": ""}, "ebe04f06580abab035408c4c2e65245b3950934e": {"ta_keywords": "variable oil damper;oil damper;method considering earthquake;earthquake input levels;damper;structure variable oil;considering earthquake input;earthquake input;isolated structure variable;21231 design method;base isolated structure;base isolated;levels base isolated;isolated structure;considering earthquake;21231 design;variable oil;earthquake;structure variable;levels base;design method;input levels base;isolated;base;design method considering;21231;structure;design;input levels;variable", "pdf_keywords": ""}, "dd36aca034312a266d6f10b37414d3342c3b9c79": {"ta_keywords": "embeddings knowledge base;knowledge base queries;embeddings knowledge;base queries supplementary;faithful embeddings knowledge;queries supplementary material;queries supplementary;knowledge base;base queries;embeddings;faithful embeddings;queries;supplementary material;knowledge;supplementary;base;faithful;material", "pdf_keywords": ""}, "17351cfeac949c266f4d1ff86c515250b931bdc2": {"ta_keywords": "structure learning logic;learning logic;learning logic programs;inductive logic programming;markov logic networks;logic networks;logic networks mlns;logic programs graphs;learn structures;scalable probabilistic logic;approach learn structures;structures large knowledge;probabilistic logic;logic programming;generated parameter learning;learn structures large;structure learning;efficient structure learning;inductive logic;logic programs;structure learning parameter;methods inductive logic;proppr structure learning;probabilistic logic order;structure learning method;large knowledge base;learning parameter learning;parameter learning;probabilistic logic called;novel structure learning", "pdf_keywords": ""}, "394e17f5ee5e8a734b2714795b7da3cd704716da": {"ta_keywords": "ordinal evaluations peer;evaluations peer grading;peer grading;ordinal evaluations;evaluations peer;students evaluate;peer evaluation;grading;scores robustness ordinal;evaluation students;peer evaluation answers;robustness ordinal evaluations;evaluation students severe;ordinal approach significantly;set students evaluate;moocs highly;feedback evaluation students;evaluation feedback peer;feedback peer evaluation;ordinal;ordinal approach;moocs highly successful;students evaluate present;number students;moocs;cardinal scores robustness;evidence ordinal approach;robustness ordinal;evaluations;evidence ordinal", "pdf_keywords": ""}, "57e4074c588c0e27e4c0bc89f12512ccdb900d79": {"ta_keywords": "unsupervised machine translation;transfer machine translation;unsupervised style transfer;deep generative;deep generative model;generative objectives backtranslation;language translation probabilistic;machine translation;present deep generative;backtranslation adversarial loss;translation techniques probabilistic;objectives backtranslation adversarial;backtranslation adversarial;including sentiment transfer;text style transfer;generative model unsupervised;parallel corpus hmm;non generative techniques;non generative baselines;machine translation techniques;translation probabilistic;sequence model learns;style transfer tasks;recurrent language model;generative baselines;sentiment transfer;translation probabilistic formulation;parallel corpus;generative techniques;formulation unsupervised text", "pdf_keywords": "unsupervised machine translation;unsupervised style transfer;machine translation;language translation experiments;including sentiment transfer;machine translation task;text sequence transduction;sentiment transfer formality;translation experiments suite;sentiment transfer;translation experiments;text style transfer;non generative techniques;deep generative;style transfer tasks;formulation unsupervised text;generative model unsupervised;model unsupervised text;present deep generative;deep generative model;unsupervised text style;proposed non generative;transfer word decipherment;non generative;generative techniques;unsupervised text;style transfer junxian;translation task;style transfer uni\ufb01es;style transfer"}, "8a99e1eb3285f127eed7169441679d47be7f1633": {"ta_keywords": "text generation splicing;text generation tasks;data text generation;text generation;generation splicing;context free grammar;generation splicing nearest;generates text token;generates text;splicing retrieved;directly splicing retrieved;reduced parsing;free grammar;generation tasks directly;splicing retrieved segments;parsing;derivation reduced parsing;splicing;directly splicing;generations data text;generation tasks;splicing nearest neighbors;text neighbor source;tasks directly splicing;neighbors generates text;splicing nearest;derivation generation;tackle data text;segments neighbor text;reduced parsing particular", "pdf_keywords": "text generation splicing;generationby splicing approach;implement generationby splicing;text generation tasks;generationby splicing;data text generation;generation splicing nearest;generation splicing;text generation;context free grammar;splicing approach training;generation neighbors oracle;splicing approach;generation tasks directly;tasks directly splicing;splicing nearest neighbors;actions obtained parsing;directly splicing retrieved;splicing retrieved;partially constructed generation;splicing;splicing retrieved segments;obtained parsing;generation tasks;directly splicing;parsing;methodologically implement generationby;sequence insert actions;splicing nearest;free grammar"}, "3c40fc36217a56aafb0abc735ff7d132b17e83a0": {"ta_keywords": "automatic melody harmonization;melody harmonization;automatic melody;best automatic melody;melody harmonization aims;melody harmonization triad;sequence harmonic accompaniment;task automatic melody;melody sequence;harmonization triad chords;harmonic accompaniment;chord sequence harmonic;melody sequence paper;harmonic accompaniment given;bar melody sequence;generates chord sequence;algorithm deep;harmonization;genetic algorithm deep;chords;chords comparative study;generates chord;melody;algorithm deep learning;multiple bar melody;bar melody;chords comparative;harmonization triad;model generates chord;chord sequence", "pdf_keywords": "melody harmonization models;automatic melody harmonization;melody harmonization;generation automatic melody;melody harmonization including;harmonization including deep;automatic melody;melody harmonization functional;music generation automatic;method melody harmonization;\ufb01ve melody harmonization;rnn models;symbolic music generation;recurrent neural network;music generation;keywords symbolic music;melody;deep recurrent neural;network rnn models;model deep learning;neural network rnn;harmonization models;variants deep recurrent;method melody;model hmm;harmonization functional harmony;pianoroll triad dataset;deep recurrent;deep learning;based model deep"}, "77efa3102456c9f921b05b95eefe845d2ce6bc4b": {"ta_keywords": "regularized discriminative training;mred regularized discriminative;regularized discriminative;relative entropy discrimination;rive regularized discriminative;discriminative training method;discriminative training methods;minimum relative entropy;entropy discrimination mred;conventional discriminative training;discriminative training;entropy discrimination;training methods regularization;mred speech recognition;speech recognition proposed;regularization techniques;speech recognition;conventional discriminative;recognition proposed approximation;entropy discrimination order;relative entropy;discriminative;regularization;methods regularization techniques;methods regularization;training method acoustic;regularized;mred optimization method;rive regularized;interpretations conventional discriminative", "pdf_keywords": ""}, "803a0d2677a7d6b20c3964533595775fa5c7c750": {"ta_keywords": "testing ranked preference;pairwise comparison models;testing ranked;sample testing ranked;ranking dataset statistically;ranking data furthermore;ranked preference data;pairwise comparisons;test pairwise comparison;total ranking data;rankings provided people;total ranking dataset;ranked preference;comparison models;pairwise comparison data;partial total ranking;ranking data;rankings;conclude ratings rankings;total ranking;ranking dataset;ranking;pairwise comparisons applying;consisting pairwise comparisons;comparison data;world pairwise comparison;pairwise comparison;comparisons;ratings rankings;statistically", "pdf_keywords": "testing ranked preference;pairwise comparisons rankings;comparisons rankings;comparison data ranking;sample testing ranked;pairwise comparison models;testing ranked;comparisons rankings motivation;ranked preference data;testing preference data;tests pairwise comparison;ranked preference;pairwise comparisons generally;testing preference;ranking data furthermore;comparison models;pairwise comparisons;ranking data;sample tests pairwise;pairwise comparison data;rankings;data ranking data;comparison models wst;preference data investigate;partial total rankings;ranking;data ranking;tests pairwise;comparisons generally;total rankings"}, "c859416a8e5682bee3c35df29bc02e02a22de072": {"ta_keywords": "transcription language documentation;automatic transcription language;automatic transcriptions useful;manually transcribed speech;china automatic speech;automatic transcriptions;speech transcription tool;automatic transcription;transcribed speech practical;speech recognition tools;integrating automatic transcription;manually transcribed;facilitating language documentation;speech transcription;transcription language;automatic speech;transcription tool;automatic speech recognition;linguist using persephone;transcribed speech;transcription tool trained;linguist using;17 automatic transcriptions;speaker speech transcription;yongning na language;language documentation;language documentation workflow;useful canvas linguist;transcriptions useful;speech recognition", "pdf_keywords": ""}, "90705ece92a71efcf256cd047da53cbc1d4e5295": {"ta_keywords": "gear meshing frequency;gear meshing impact;gear mesh impact;gear vibration data;signal gear meshing;gear vibration;meshing impact load;gear meshing;gear mesh;point gear vibration;load meshing impacts;band meshing impacts;modulated signal gear;meshing impacts transient;meshing frequency modulated;meshing frequency;meshing frequency component;frequency band meshing;mesh impact theory;relationship gear meshing;meshing impacts;vibration data;meshing impact;vibration data acquired;mesh impact;gear rotates heavy;resonance meshing frequency;causes meshing impacts;paper gear mesh;stronger meshing impacts", "pdf_keywords": ""}, "30fa01df767339a6c8bd37c32160992fcb19ed18": {"ta_keywords": "regret empirical games;bounding regret empirical;games bounding regret;bandit problem;empirical games formulate;empirical games;bandit problem new;empirical game theoretic;empirical game;armed bandit problem;games formulate deviation;approximate nash equilibria;bounding regret;game theoretic analysis;approximate nash;multi armed bandit;games bounding;output approximate nash;scale games bounding;large scale games;regret empirical;bandit;armed bandit;game theoretic;games formulate;scale games;nash equilibria;games;techniques solving large;deviation gain computation", "pdf_keywords": ""}, "196be0bdec3b7bcb3ee35cd126fb2730a9d742d6": {"ta_keywords": "simstudent educational;teaching simstudent;simstudent educational software;teaching simstudent interactive;simstudent allows students;learning teaching simstudent;simstudent interactive;simstudent;learning environment simstudent;simstudent allows;simstudent interactive event;environment simstudent;environment simstudent allows;tutor effect;tutor;tutor effect phenomenon;peers tutor effect;educational software;allows students learn;teaching computer agent;leverage tutor effect;instead peers tutor;tutor effect line;peers tutor;teaching computer;leverage tutor;learning teaching;designed leverage tutor;teach learning;educational software infrastructure", "pdf_keywords": ""}, "46619f0547b1a9c2e7649d0e5c931e9aa857a938": {"ta_keywords": "covid tracking project;covid tracking;covid;tracking;tracking project;project", "pdf_keywords": ""}, "8963602d4b9c3b1054a5ed6fb2a2088dec774824": {"ta_keywords": "personal information management;management personal information;improve personal information;machine learning used;learning information management;ways machine learning;information management;information management promising;machine learning;information management tools;machine learning information;machine learning techniques;learning information;personal information email;personal information;learning techniques effectively;track task;evidence machine learning;information email messages;management personal;learning used;searches require awareness;management tools;learning used improve;directions machine learning;email messages;searches;losing track task;searches notably searches;information email", "pdf_keywords": ""}, "daedf33077099f7c808e9f4022469e15bf224ad7": {"ta_keywords": "biopsy transcriptome expression;transcriptome expression profiling;biopsy transcriptome;transplants risk chronic;kidney transplants risk;profiling identify kidney;identify kidney transplants;transcriptome expression;kidney transplants;transplants risk;transcriptome;risk chronic injury;chronic injury multicentre;chronic injury;expression profiling identify;identify kidney;transplants;injury multicentre prospective;expression profiling;risk chronic;kidney;profiling identify;biopsy;injury multicentre;chronic;profiling;injury;multicentre prospective;multicentre prospective study;prospective study", "pdf_keywords": ""}, "59d487d6ef839c82ae128550e35fa44058b03d37": {"ta_keywords": "trained model code2vec;code2vec paper;model code2vec paper;code2vec;model code2vec;implementation data trained;data trained;trained model;data trained model;implementation data;implementation;trained;model;paper;data", "pdf_keywords": ""}, "60ce57713261b41fe2e3d222f1d4530c4fc69241": {"ta_keywords": "computational complexity agent;randomized rules assignment;agents expected utility;rule probabilistic serial;complexity agent manipulating;serial rule probabilistic;probabilistic serial rule;complexity agent;probabilistic serial ps;response manipulating probabilistic;randomized rules;polynomial time algorithm;prominent randomized rules;probabilistic serial;rule probabilistic;expected utility best;polynomial time;manipulating probabilistic serial;computational complexity;rules assignment problem;utility best response;computing expected utility;response np hard;utility better response;expected utility better;assignment problem computing;rule prominent randomized;rules assignment;agents expected;agent manipulating ps", "pdf_keywords": "agent sequential allocation;computational complexity agent;preferences agents;complexity agent manipulating;complexity agent;knowledge preferences agents;agent sequential;response agent sequential;utility better response;agent complete knowledge;agent manipulating ps;best response agent;response multiple agents;\ufb01nding optimal manipulation;multiple agents;expected utility better;manipulable \ufb01nding optimal;agent manipulating;response agent ps;optimal manipulation;ordinal preferences best;polynomial time algorithm;agents present polynomial;utility better;complete knowledge preferences;\ufb01nding optimal;response agent;computational complexity;optimal;consistent ordinal preferences"}, "74d8a998269bcdd087a21840b0e28d86c256c121": {"ta_keywords": "hyperbolic discounting learnable;discounting learnable pac;discounting learnable;general preference models;preference models provide;preference models;discounted utility models;preference models particular;class preference models;preference models defined;models choice time;models intertemporal choice;learnability models choice;learnability models;analyze preference models;known learning bounds;learner improve guarantees;quasi hyperbolic discounting;provided pac learning;pac learning;utility models intertemporal;learning membership;known learning;hyperbolic discounting;learnable pac;dealing learnability models;utility models;learning bounds general;learnable pac setting;bounds general preference", "pdf_keywords": "quasihyperbolic discounting learnable;discounted utility models;models choice time;models intertemporal choice;hyperbolic quasihyperbolic discounting;quasihyperbolic discounting;utility models intertemporal;discounting learnable;general preference models;preference models agent;time dependent choice;preference models;intertemporal choice exponential;utility models;discounting learnable pac;preference models de\ufb01ned;preference models preference;learning time dependent;class preference models;intertemporal choice state;weighting decisions time;preference model;preference model satis\ufb01es;important discounted utility;models preference model;discounted utility;choice time particular;set discounted utility;choice time;intertemporal choice"}, "c994372b3c33bbc1ad6b504c5efb5afd515a5009": {"ta_keywords": "target speech extraction;retraining target speech;speech extraction supervision;supervision based speaker;speech extraction;recordings auxiliary loss;segments spoken target;forcing estimated speech;speaker identity loss;speech extraction recognition;speech recognition gain;speech recognition;spoken target speaker;function target speech;estimated speech;target speech;suitable speech recognition;estimated speech correct;recognition weak supervision;loss speaker;recordings auxiliary;loss speaker characteristics;proposed loss speaker;auxiliary loss;loss function retraining;extraction recognition weak;real recordings auxiliary;propose auxiliary loss;signals suitable speech;speaker proposed loss", "pdf_keywords": ""}, "09d88e0bb8863fd402030aeb625c52c0492c4fef": {"ta_keywords": "reverberation deep recurrent;speech recognition asr;recognition asr reverberated;reverberation deep;asr reverberated environments;deep recurrent noising;space reverberation deep;feature space reverberation;recurrent noising auto;robust automatic speech;auto encoders dae;recognition asr;asr reverberated;noising auto encoders;speech recognition;recurrent noising;reverberation;2014 reverb challenge;reverb challenge development;reverberated environments;encoders dae;automatic speech;auto encoders;encoders dae early;automatic speech recognition;proposed asr achieves;reverb challenge;training feature transformations;space reverberation;deep recurrent", "pdf_keywords": ""}, "320278b24a3c53a44f95e8ef5465bebe56f24225": {"ta_keywords": "dependency parsing pause;parsing pause prediction;pause prediction dependency;parsing pause;prediction dependency parsing;annotated pauses;explicitly annotated pauses;pause prediction;pause prediction experiments;joint dependency parsing;dependency structure pause;model pause prediction;pause prediction prediction;structure pause prediction;dependency parsing;performs dependency parsing;dependency parsing disfluency;prediction prosodic information;annotated pauses resulting;prediction prosodic;prediction prediction prosodic;dependency parsing using;parsing disfluency detection;prosodic information text;pauses;pause;prediction dependency;speech related;prosodic information;joint model pause", "pdf_keywords": ""}, "568462ab0a0a59a2575b70db2cd9022572526f3f": {"ta_keywords": "learning dynamics stackelberg;learning dynamics;learning continuous games;dynamics stackelberg games;learning dynamics mitigate;implicit learning dynamics;stackelberg games equilibria;learning dynamics emulating;stackelberg games;empirically learning dynamics;descent implicit learning;games using deterministic;stackelberg game;gradient descent implicit;implicit learning;structure stackelberg game;converge stackelberg equilibria;stackelberg game using;simultaneous gradient descent;zero sum games;continuous games;general sum games;gradient descent;based learning dynamics;training generative;games equilibria characterization;game using implicit;gradient based learning;novel gradient;learning continuous", "pdf_keywords": ""}, "92622a58377a4671b2ba59e8e59b19b0ab5119bb": {"ta_keywords": "ontology aware partitioning;partitioning knowledge graph;knowledge graph identification;aware partitioning knowledge;partitioning knowledge;knowledge graph construction;ontology aware;knowledge graph;knowledge graphs built;knowledge graphs provide;knowledge graphs;incorporates ontology graph;ontology graph;scaling knowledge graphs;ontology graph distribution;require knowledge graphs;challenges knowledge graph;results knowledge graph;constraints knowledge graphs;ontological constraints knowledge;technique knowledge graph;graphs noisy extractions;aware partitioning;scaling knowledge;ontology;entities relationships automatically;incorporates ontology;hash based approaches;model incorporates ontology;relationships automatically constructing", "pdf_keywords": ""}, "2ab481028dda04197283c03115bb5f46f5998cc3": {"ta_keywords": "russian thesauri linked;thesauri linked open;russian thesauri;linked open data;thesauri linked;russian;open data;linked open;thesauri;linked;data;open", "pdf_keywords": ""}, "582089a00a6c9fb534f16d1dbbafc50cc4e3912a": {"ta_keywords": "natural language queries;language queries enterprise;schema aware semantic;natural language query;aware semantic reasoning;specialized query language;tasks ontology reasoner;ontology reasoner;semantic reasoning interpreting;handling bi queries;language query interfaces;queries enterprise;semantic reasoning helps;semantic reasoning framework;bi queries;query language;focused bi queries;schema aware;semantic reasoning;building schema aware;solvable tasks ontology;ontology reasoner apply;empirically schema aware;query interfaces;question answering athena;tasks ontology;query language data;language queries;aware semantic;know specialized query", "pdf_keywords": ""}, "1adadbfa95e43a70fcd17e6ce947a0652b86bfc3": {"ta_keywords": "clean crawled corpus;crawled corpus large;crawled corpus;large webtext corpora;crawled corpus c4;corpus large language;larger text corpora;text corpora train;webtext corpora;benchmark nlp datasets;text corpora;remarkable progress nlp;corpus large;nlp datasets;crawled;benchmark nlp;corpus;examples benchmark nlp;clean crawled;documenting large webtext;progress nlp;large language models;webtext corpora case;nlp tasks researchers;colossal clean crawled;nlp tasks;corpora case study;large webtext;corpus c4;progress nlp tasks", "pdf_keywords": "clean crawled corpus;crawled webtext corpora;crawled corpus;crawled corpus c4;passively crawled webtext;crawled webtext;bias jewish arab;webtext corpora;crawled;negative sentiment explore;sentiment explore;sentiment explore content;associated positive sentiment;nlp datasets work;bias jewish;clean crawled;observed bias jewish;origins negative sentiment;using passively crawled;nlp datasets;passively crawled;webtext corpora commoncrawl;corpus;benchmark nlp datasets;corpora;ethnicity jewish;corpus c4;negative sentiment;positive sentiment;black arab"}, "90129b0733ac48ead26b7c86e8b4df917568e208": {"ta_keywords": "integration heterogeneous databases;heterogeneous databases common;heterogeneous databases;databases common domains;databases common;queries based textual;databases;domains using queries;based textual similarity;textual similarity;integration heterogeneous;queries based;using queries based;common domains using;common domains;heterogeneous;similarity;using queries;queries;based textual;domains using;domains;textual;integration;common;based;using", "pdf_keywords": ""}, "02d98ca8f4ecd1a2b885d6867f4c1407ae8d1007": {"ta_keywords": "weakly supervised parsers;datasets weak supervision;especially weakly supervised;extra supervision queries;weakly supervised;queries especially weakly;query manual annotations;annotating examples improve;weakly supervised baseline;supervision active learning;annotations extra supervision;supervision demands parsers;supervision experiments wikisql;wikisql annotating examples;supervision queries;supervised parsers learnt;weak supervision experiments;demands parsers search;supervised parsers;weak supervision active;trained weak supervision;wikitablequestions human annotators;parsers learnt;active queries especially;actively select examples;experiments wikisql annotating;wikisql annotating;active queries;annotating examples;human annotators", "pdf_keywords": "training semantic parser;supervision semantic parsing;active supervision semantic;train semantic parser;parser weak supervision;weakly supervised parsers;semantic parser human;semantic parser weak;parser human annotator;supervision semantic;training semantic;semantic parsers;semantic parsers requires;semantic parsing;semantic parsing ansong;learning semantic parsers;semantic parser;supervised parsers;supervised parsers learnt;abstract semantic parser;annotations extra supervision;annotator extra supervision;weak active supervision;semantic parser maps;introduction semantic parsing;parsers learnt;natural language commands;semantic parsing maps;parser human;supervision active learning"}, "601398838250a4e69c69cc339d65f5c51e727ad1": {"ta_keywords": "role question generation;generating contextualized questions;questions semantic role;question generation;question generation approach;approaches question generation;question generation given;asking generating contextualized;semantic roles predicate;contextualized questions semantic;possible semantic roles;semantic roles;contextualized questions;generating contextualized;questions semantic;semantic role;roles predicate develop;roles predicate;passage asking generating;question prototype role;asking possible semantic;generation given predicate;producing set questions;questions situation inherent;role revises contextually;text asking questions;independent question prototype;semantic role unlike;question prototype;passage asking", "pdf_keywords": "natural language questions;querying information expressed;information extraction tasks;\ufb02uent natural language;semantic relations;questions corresponding;context passage leveraging;questions targeting predicate;questions corresponding known;targeting predicate semantic;semantic relations bring;passage leveraging syntactic;syntactic structure qa;natural language text;questions essential communicative;information extraction;natural language;predicate semantic role;representing querying information;leveraging syntactic;information expressed text;generates questions varied;arguments produce questions;information asking questions;natural language question;produce questions;language questions targeting;generates questions;leveraging syntactic structure;information seeking questions"}, "c783bc02f5f901e4604eb3b0d504a036369afd91": {"ta_keywords": "perovskite layer srlaalo4;layer srlaalo4 sr2tio4;sr2tio4 comparative study;srlaalo4 sr2tio4 comparative;srlaalo4 sr2tio4;sr2tio4 comparative;sr2tio4;layer srlaalo4;srlaalo4;study laalo3 srtio3;srtio3;energy perovskite layer;laalo3 srtio3;energy perovskite;intensity energy perovskite;perovskite layer;mn4 line intensity;mn4;mn4 line;perovskite;comparative study laalo3;study laalo3;laalo3;line intensity energy;intensity energy;line intensity;energy;layer;intensity;line", "pdf_keywords": ""}, "a3452276ada37727d0008dad8ca7c27bbbee6984": {"ta_keywords": "rumor detection;network rumor classification;existing rumor detection;rumor classification;rumor classification enhances;rumor detection methods;demonstrate rumor detection;detecting rumors;datasets demonstrate rumor;rumor detection method;attention network rumor;detecting rumors early;capacity detecting rumors;demonstrate rumor;network rumor;posts represent conversation;twitter datasets demonstrate;graph conversation structures;claims existing rumor;extensive experiments twitter;represent conversation thread;experiments twitter datasets;rumors early stages;hierarchical graph attention;attention network;graph attention network;experiments twitter;twitter datasets;graph conversation;conversation structures", "pdf_keywords": "rumor detection twitter;posts rumor detection;interactions rumor detection;rumor detection crucial;attention network rumor;rumor detection;detecting rumors twitter;rumor detection end;early rumor detection;detecting rumors;informative posts rumor;detection twitter claim;twitter benchmarks rumor;rumors twitter enhances;twitter claim guided;gat detecting rumors;network rumor classi\ufb01cation;rumor indicative features;graph attentionbased embeddings;rumor classi\ufb01cation enhances;multi level rumor;user interactions rumor;benchmarks rumor classi\ufb01cation;graph attention network;rumor classi\ufb01cation early;rumor classi\ufb01cation;abstract rumors;graph attention networks;interactions rumor;hierarchical graph attention"}, "19be8dd52d949fed1a3e5aca7630669da2575d73": {"ta_keywords": "uncertainty agents preferences;stable matching uncertain;uncertainty lottery model;models uncertainty lottery;uncertain linear preferences;matching setting uncertainty;matching uncertain;uncertainty lottery;stable matching;stable stable matching;matching uncertain linear;matching highest probability;stable matching setting;model lottery preference;distribution linear preferences;sided stable matching;preferences compact indifference;lottery preference profiles;probability given matching;probability stable;lottery preference;probability stable stable;highest probability stable;linear preferences;weak preference order;setting uncertainty agents;indifference model agent;probability model lottery;linear preferences compact;indifference model", "pdf_keywords": ""}, "35ee53492c7f32dbb3b4ed7ba4d1395218b13ee9": {"ta_keywords": "everyday algorithm auditing;auditing behaviors;facilitate auditing behaviors;algorithm auditing;auditing behaviors emerge;everyday users algorithmic;organic auditing behaviors;algorithm auditing process;systems formal auditing;auditing approaches greatly;everyday use algorithmic;auditing approaches;facilitate auditing;tools facilitate auditing;formal auditing approaches;formal auditing;auditing approaches organic;auditing process users;users algorithmic systems;auditing;auditing regardless users;users algorithmic;approaches organic auditing;auditing process;everyday algorithm;algorithm auditing drawing;organic auditing;problematic machine behaviors;machine behaviors day;everyday use systems", "pdf_keywords": "everyday algorithm auditing;everyday algorithm audits;facilitate auditing behaviors;auditing behaviors;algorithm auditing;algorithm audits;algorithm auditing proposed;auditing behaviors drawn;facilitate auditing;algorithm audits involving;everyday audits;everyday audits community;algorithm auditing process;algorithm auditing outlining;support everyday audits;tools facilitate auditing;auditing proposed;auditing;auditing process users;auditing proposed process;auditing regardless users;audits community guidance;auditing process;algorithm auditing drawing;audits community;audits involving initiation;machine behaviors day;everyday algorithm;auditing outlining;audits"}, "1f38ba33063f118f574cf57ff9f1a0e7de2857ff": {"ta_keywords": "russian semantic similarity;semantic similarity russian;similarity russian nouns;similarity russian;overview russian semantic;russian semantic;similarity evaluation russe;semantic similarity evaluation;semantic similarity;russian nouns exploring;semantic similarity analysis;studies semantic similarity;russian nouns;russian language interesting;task semantic similarity;similarity evaluation;russian language paper;gives overview russian;russian language;overview russian;directly applicable russian;similarity analysis measures;russian exist lot;similarity;applicable russian exist;problem russian language;exploring problem russian;applicable russian;similarity analysis;performed russian language", "pdf_keywords": "russian semantic similarity;semantic similarity panchenko;evaluating semantic similarity;semantic similarity measures2;datasets russian language;semantic similarity;benchmark datasets russian;russian semantic;workshop russian semantic;russian language analysis;test datasets russian;similarity measures2 russe;similarity panchenko;datasets russian;unsupervised skip gram;russian best results;evaluating semantic;datasets russian open;lexical morphological semantic;english distributional skip;skip gram models;gram models directly;similarity measures2;gram model trained;gram models;tool evaluating semantic;russian best;distributional skip gram;russian language;similarity"}, "857036a25401c19e484cc32d974c90cd9a46cd66": {"ta_keywords": "approximation local nash;computing local nash;local optimality nonlinear;computation local nash;local nash equilibria;local optimality;conditions local optimality;local convergence algorithm;nash equilibria games;optimality nonlinear programming;optimality nonlinear;nash equilibria continuous;nonlinear programming optimal;iterative steepest descent;local nash;optimal control theory;programming optimal control;nonlinear programming;numerical approximation local;equilibria continuous games;descent algorithm numerical;equilibria games;nash equilibria;steepest descent algorithm;optimal control;propose iterative steepest;equilibria games played;nash equilibria provide;iterative steepest;optimality", "pdf_keywords": ""}, "d473ff103565d8c76e0cbfa33bdd4b0db1cbb23f": {"ta_keywords": "bid auction games;auction games;complex bayesian games;bayesian games characterized;bayesian games;equilibria finite strategy;auction games distinct;strategy generation;evolution strategies nes;incremental strategy generation;agent strategies parametric;equilibrium computation methods;finite strategy;strategy generator;evolution strategies;agent strategies;search strategies function;loop regret minimization;nes strategy generator;strategies parametric form;regret minimization;strategies function space;strategies nes;strategies function;strategy generation framework;natural evolution strategies;nes strategy;finite strategy set;equilibrium computation adopt;games characterized", "pdf_keywords": "bayesian games pure;bayesian games;bid auction games;symmetric bayesian games;auction games;auction games distinct;games pure equilibrium;equilibria \ufb01nite strategy;black box optimization;methods simultaneousauction games;simultaneousauction games qualitatively;robust games;simultaneousauction games;deep models nes;bid auction;auction;sealed bid auction;games qualitatively;games pure;robust games smoother;pure equilibrium computation;method minimax nes;mixed equilibria;games distinct;\ufb01nite strategy;games smoother topology;equilibrium computation method;games qualitatively different;mbne calculate equilibria;appeared robust games"}, "ecd2b355f250abfd4eb9d6c7c598c33c7cd6bcb0": {"ta_keywords": "conditional entropy crowdsourcing;entropy crowdsourcing;crowdsourced labels principle;crowdsourced labels;noisy crowdsourced labels;labels provided crowdsourcing;entropy crowdsourcing paper;crowdsourcing datasets binary;truth noisy crowdsourced;minimax conditional entropy;crowdsourcing datasets;real crowdsourcing datasets;noisy crowdsourced;probabilistic labeling;real crowdsourcing;crowdsourcing;probabilistic labeling model;unique probabilistic labeling;crowdsourcing workers;variety real crowdsourcing;provided crowdsourcing workers;crowdsourced;provided crowdsourcing;crowdsourcing paper;regularized minimax conditional;crowdsourcing paper propose;propose minimax conditional;crowdsourcing workers usually;labeling model jointly;minimax conditional", "pdf_keywords": ""}, "0fc01a915cc7bf7025f80d44f805bd54b6425a33": {"ta_keywords": "load monitoring nilm;measurement noise sampling;nonintrusive load monitoring;load monitoring;power consumption signal;monitoring nilm algorithm;monitoring nilm;noise sampling;aggregate power consumption;power consumption;consumption signal specialize;consumption signal;noise sampling rate;device usage;scenarios function measurement;arbitrary nonintrusive load;rate device usage;nilm algorithm analyze;device usage introduce;measurement noise;sampling rate device;bounds probability distinguishing;function measurement noise;monitoring;general nilm algorithm;nilm algorithm seek;nilm algorithm;sampling;data derive bounds;probability distinguishing scenarios", "pdf_keywords": "nilm algorithms;general nilm algorithm;nilm algorithm;limits nilm algorithms;nilm algorithm analyze;noise sampling;measurement noise sampling;noise sampling rate;nilm;additive gaussian noise;gaussian noise;gaussian noise independent;algorithms;limits nilm;measurement noise;bound probability distinguishing;function measurement noise;studying general nilm;noise independent;general nilm;error additive gaussian;scenarios function measurement;sampling;containing number devices;bounds probability distinguishing;noise independent underlying;sampling rate device;algorithm;probability distinguishing scenarios;additive gaussian"}, "16457c13a40aa589fa06d8533a47b3f96aede474": {"ta_keywords": "canopy mounting articles;anchoring canopy;building canopy;canopy mounted;canopy;canopy mounted free;canopy wall;canopy mounting;building canopy mounting;wall building canopy;frame anchoring canopy;anchoring canopy wall;canopy wall building;frame having stakes;covering securing means;covering securing;securing means;away securing means;protected vegetables graphical;flexible covering securing;mounting articles protected;frame carrying;securing means provided;anchoring;away securing;securing;ground building shield;frame anchoring;main frame carrying;having stakes", "pdf_keywords": ""}, "639cc01afcc1c78063f7a6bbdae998cd147911c4": {"ta_keywords": "robust utility learning;parametric utility learning;utility learning scheme;utility learning framework;utility learning;defined estimated utility;estimated utility functions;estimated utility;methods robust utility;robust parametric utility;robust utility;learning framework inverse;gradient boosting ensemble;utility functions demonstrate;gradient boosting;utility functions;game defined estimated;boosting ensemble methods;boosting;extend robust utility;encourage energy efficient;heteroskedastic inference improve;estimation heteroskedastic inference;energy efficient behavior;boosting ensemble;utility;inference improve forecasting;data social game;heteroskedastic inference;learning scheme", "pdf_keywords": "robust utility learning;correlated utility learning;utility learning framework;parametric utility learning;utility learning scheme;utility learning;utility learning using;estimated utility functions;novel correlated utility;utility learning errors;constructing correlated utility;robust utility;learning framework inverse;robust parametric utility;estimated utility;extend robust utility;de\ufb01ned estimated utility;learning using heteroskedastic;framework robust utility;heteroskedastic inference adaptation;correlated utility;estimation heteroskedastic inference;utility functions;utility functions demonstrate;feasible generalized;generalized squares estimation;gradient boosting ensemble;heteroskedastic inference;gradient boosting;feasible generalized squares"}, "03d2af05e41aac58ebae4ab37f09155e53d4b35b": {"ta_keywords": "bit matrix completion;bit observations general;noisy bit observations;bit observations;estimate optimizing convex;theory matrix completion;probit observation models;log likelihood concave;matrix completion extreme;generating bit measurements;matrix completion;applications bit matrix;optimizing convex program;bit matrix;likelihood concave;logistic probit observation;binary bit measurements;bit measurements generated;likelihood estimate optimizing;bit measurements eliminate;probit observation;impossible maximum likelihood;convex program addition;estimate suitable constraint;observing subset;maximum likelihood;estimate optimizing;optimizing convex;maximum likelihood estimate;practical applications bit", "pdf_keywords": "bit matrix completion;unquantized matrix completion;matrix completion problem;matrix completion;bit compressed sensing;compressed sensing;matrix convex programming;accuracy recover matrix;compressed sensing described;recover matrix;bit matrix;recover rank matrix;recover matrix distribution;unknown matrix accurately;rank matrix convex;programs bit matrix;unquantized matrix;convex programs bit;recovered binary measurements;matrix accurately;inspired unquantized matrix;matrix accurately e\ufb03ciently;matrix convex;matrix distribution observations;convex programming;rd binary observations;convex programming theory;pair convex programs;binary observations;convex programs"}, "3bfa1fe0a8d031d59dfc0cfa4975c296951ee56c": {"ta_keywords": "speech enhancement recognition;integrated speech enhancement;speech enhancement;rooms integrated speech;speech recognition;integrated speech;speech recognition living;temporal modeling sounds;modeling sounds;recognition living rooms;enhancement recognition based;living rooms integrated;based spatial spectral;enhancement recognition;rooms integrated;spectral temporal modeling;spatial spectral temporal;recognition based spatial;living rooms;spatial spectral;rooms;recognition based;spectral temporal;speech;temporal modeling;enhancement;recognition;spectral;recognition living;based spatial", "pdf_keywords": ""}, "8873d1369590249113e1f0491ce49d1502395b9c": {"ta_keywords": "video text compliance;compliance labels video;text compliance activity;trainable compliance network;compliancenet;video compliance;trainable compliance;automatic compliance;given video compliance;compliance network improves;video compliance associated;text compliance;compliancenet novel;present compliancenet;finally present compliancenet;present compliancenet novel;compliance activity verification;compliance associated text;compliance activity;modal compliance;compliance network;compliancenet novel end;multi modal compliance;end trainable compliance;automatic compliance checking;compliance vtc dataset;compliance;text instructions compliance;text compliance vtc;enable automatic compliance", "pdf_keywords": ""}, "7488429131b8970425a66f3410920d98ff6e9c36": {"ta_keywords": "regularization debiasing evaluations;biased evaluations;debiasing evaluations biased;evaluations biased evaluations;appropriate regularization debiasing;debiasing evaluations;evaluations biased;rating course;biases given ratings;regularization debiasing;ratings information outcome;rating course receive;rating reviews papers;higher rating course;submissions evaluate quality;propose debiasing method;given ratings information;evaluate quality reviews;higher rating reviews;rate propose debiasing;submissions evaluate;rating reviews;propose debiasing;rate teaching quality;regularized optimization;ratings information;reviews applications students;students higher rating;rate teaching;debiasing method", "pdf_keywords": "debiasing evaluations biased;biased evaluations;regularized optimization;debiasing evaluations;evaluations biased evaluations;evaluations biased;regularized optimization problem;chooses appropriate regularization;propose debiasing method;appropriate regularization;solving regularized optimization;regularized;2020 debiasing evaluations;appropriate regularization formulate;debiasing method;bias help;regularization;debiasing method solving;biased evaluations jingyan;rate propose debiasing;bias propose;bias help common;cross validation algorithm;method solving regularized;bias;bias propose estimator;propose debiasing;regularization formulate;solving regularized;induced bias propose"}, "579e01c3c864cc98e57c728f84fcf553c5b1bcba": {"ta_keywords": "audible murmur enhancement;murmur enhancement method;murmur enhancement;non audible murmur;murmur nam microphone;silent speech interfaces;audible murmur;microphones noisy environments;soft whispered voice;promising medium silent;microphone;microphone detect;conductive microphones noisy;silent speech communication;medium silent speech;microphones noisy;whispered voice developed;noise contamination speaking;nam microphone detect;microphone detect extremely;audible murmur nam;body conductive microphones;quiet noisy environments;voice developed experimental;extremely soft whispered;evaluation silent speech;nam microphone;microphones;whispered voice;soft whispered", "pdf_keywords": ""}, "4f1eef4acaf0164593b9e654dba4b8cd3e72421d": {"ta_keywords": "stacked graphical learning;graphical learning efficient;learner collective classification;datasets stacked graphical;graphical learning meta;datasets stacked;collective classification classes;collective classification widely;learning efficient inference;separately collective classification;collective classification;graphical learning;classification relational datasets;propose stacked graphical;classification relational;meta learning scheme;base learner collective;stacked graphical;learning efficient;efficient inference markov;learning meta learning;experiments datasets stacked;meta learning;related instances predicting;used classification relational;efficient inference;faster gibbs sampling;graphical learning 40;instances predicting;random fields stacked", "pdf_keywords": ""}, "31884a623af77136413d997049b5787b394db461": {"ta_keywords": "encoderdecoder\u30e2\u30c7\u30eb\u3092\u8a13\u7df4\u3057\u305f\u306e\u3061 \u63a2\u7d22\u6642\u306b\u9577\u3055\u306e\u5165\u529b\u3092\u53d7 \u3051\u53d6\u308b;\u5217\u306e\u9577\u3055\u306e\u6307\u5b9a\u3092\u5165\u529b\u3068\u3057\u3066\u53d7\u3051\u53d6\u308b\u3088\u3046\u30e2\u30c7\u30eb\u306e\u62e1\u5f35\u3092 \u884c\u3044 \u5b66\u7fd2\u3092\u901a\u3057\u3066\u51fa\u529b\u9577\u306e\u5236\u5fa1\u80fd\u529b\u3092\u7372\u5f97\u3059\u308b;\u6a19\u6e96\u7684\u306a encoderdecoder\u30e2\u30c7\u30eb\u3092\u8a13\u7df4\u3057\u305f\u306e\u3061 \u63a2\u7d22\u6642\u306b\u9577\u3055\u306e\u5165\u529b\u3092\u53d7;encoderdecoder\u30e2\u30c7\u30eb\u3092\u8a13\u7df4\u3057\u305f\u306e\u3061 \u63a2\u7d22\u6642\u306b\u9577\u3055\u306e\u5165\u529b\u3092\u53d7;encoder decoder\u30e2\u30c7\u30eb\u306b\u304a\u3044\u3066\u51fa\u529b\u9577\u3092\u5236 \u5fa1\u3092\u3059\u308b\u305f\u3081\u306e;\u30d3\u30fc\u30e0\u63a2\u7d22\u306b\u5909\u66f4\u3092\u52a0\u3048\u308b\u624b\u6cd5\u3067\u3042\u308a \u6a19\u6e96\u7684\u306a encoderdecoder\u30e2\u30c7\u30eb\u3092\u8a13\u7df4\u3057\u305f\u306e\u3061;\u6b8b\u308a\u306e\u4e8c\u624b\u6cd5\u306f\u5b66\u7fd2\u306b\u57fa\u3065\u304f\u3082\u306e\u3067\u3042\u308a \u51fa\u529b\u7cfb \u5217\u306e\u9577\u3055\u306e\u6307\u5b9a\u3092\u5165\u529b\u3068\u3057\u3066\u53d7\u3051\u53d6\u308b\u3088\u3046\u30e2\u30c7\u30eb\u306e\u62e1\u5f35\u3092;encoder decoder\u30e2\u30c7\u30eb\u306b\u3088\u308b\u6587\u8981\u7d04\u306b\u53d6\u308a\u7d44\u3093\u3060\u5f93\u6765\u306e\u7814 \u7a76\u3067\u306f;encoder decoder\u30e2\u30c7\u30eb\u306b\u3088\u308b\u6587\u8981\u7d04\u306b\u53d6\u308a\u7d44\u3093\u3060\u5f93\u6765\u306e\u7814;\u5b66\u7fd2\u3092\u901a\u3057\u3066\u51fa\u529b\u9577\u306e\u5236\u5fa1\u80fd\u529b\u3092\u7372\u5f97\u3059\u308b \u8981\u7d04\u5668\u304c\u6301\u3064\u3079\u304d\u91cd\u8981\u306a\u80fd\u529b\u306e\u4e00\u3064\u3068\u3057\u3066 \u51fa\u529b\u3059\u308b\u8981;encoder decoder\u30e2\u30c7\u30eb\u306b\u304a\u3044\u3066\u51fa\u529b\u9577\u3092\u5236;decoder\u30e2\u30c7\u30eb\u306b\u3088\u308b\u6587\u8981\u7d04\u306b\u53d6\u308a\u7d44\u3093\u3060\u5f93\u6765\u306e\u7814 \u7a76\u3067\u306f \u3053\u306e\u51fa\u529b\u9577\u306e\u5236\u5fa1\u3068\u3044\u3046\u70b9\u306b\u3064\u3044\u3066\u660e\u78ba\u306b\u53d6\u308a\u7d44;decoder\u30e2\u30c7\u30eb\u306b\u304a\u3044\u3066\u51fa\u529b\u9577\u3092\u5236 \u5fa1\u3092\u3059\u308b\u305f\u3081\u306e 4\u3064\u306e\u624b\u6cd5\u3092\u63d0\u6848\u3059\u308b;\u305d\u306e\u91cd\u8981\u3055\u306b\u95a2\u308f\u3089\u305a encoder decoder\u30e2\u30c7\u30eb\u306b\u3088\u308b\u6587\u8981\u7d04\u306b\u53d6\u308a\u7d44\u3093\u3060\u5f93\u6765\u306e\u7814;\u63a2\u7d22\u6642\u306b\u9577\u3055\u306e\u5165\u529b\u3092\u53d7 \u3051\u53d6\u308b \u6b8b\u308a\u306e\u4e8c\u624b\u6cd5\u306f\u5b66\u7fd2\u306b\u57fa\u3065\u304f\u3082\u306e\u3067\u3042\u308a;\u5b9f\u969b \u6587\u66f8\u8981\u7d04\u5206\u91ce\u306b\u304a\u3051\u308b \u5b9f\u9a13\u8a2d\u5b9a\u3067\u306f\u539f\u6587\u66f8\u3068\u540c\u6642\u306b\u8981\u7d04\u306e\u9577\u3055\u5236\u9650\u3092\u5165\u529b\u3059\u308b;\u51fa\u529b\u7cfb \u5217\u306e\u9577\u3055\u306e\u6307\u5b9a\u3092\u5165\u529b\u3068\u3057\u3066\u53d7\u3051\u53d6\u308b\u3088\u3046\u30e2\u30c7\u30eb\u306e\u62e1\u5f35\u3092 \u884c\u3044;\u884c\u3044 \u5b66\u7fd2\u3092\u901a\u3057\u3066\u51fa\u529b\u9577\u306e\u5236\u5fa1\u80fd\u529b\u3092\u7372\u5f97\u3059\u308b \u8981\u7d04\u5668\u304c\u6301\u3064\u3079\u304d\u91cd\u8981\u306a\u80fd\u529b\u306e\u4e00\u3064\u3068\u3057\u3066;\u5b9f\u9a13\u8a2d\u5b9a\u3067\u306f\u539f\u6587\u66f8\u3068\u540c\u6642\u306b\u8981\u7d04\u306e\u9577\u3055\u5236\u9650\u3092\u5165\u529b\u3059\u308b;\u30e6\u30fc\u30b6\u306b\u3088\u308a\u5165\u529b\u3055\u308c\u305f\u6240\u671b\u306e\u9577\u3055\u306b\u5fdc\u3058 \u3066\u67d4\u8edf\u306b\u51fa\u529b\u3059\u308b\u8981\u7d04\u9577\u3092\u5236\u5fa1\u3067\u304d\u308b\u3053\u3068\u306f\u975e\u5e38\u306b\u91cd\u8981;\u51fa\u529b\u7cfb \u5217\u306e\u9577\u3055\u306e\u6307\u5b9a\u3092\u5165\u529b\u3068\u3057\u3066\u53d7\u3051\u53d6\u308b\u3088\u3046\u30e2\u30c7\u30eb\u306e\u62e1\u5f35\u3092;\u30e6\u30fc\u30b6\u306b\u3088\u308a\u5165\u529b\u3055\u308c\u305f\u6240\u671b\u306e\u9577\u3055\u306b\u5fdc\u3058 \u3066\u67d4\u8edf\u306b\u51fa\u529b\u3059\u308b\u8981\u7d04\u9577\u3092\u5236\u5fa1\u3067\u304d\u308b\u3053\u3068\u306f\u975e\u5e38\u306b\u91cd\u8981 \u3067\u3042\u308a", "pdf_keywords": ""}, "2818bd090206ef33f9d7e1be03bc4f742c6762d1": {"ta_keywords": "agglutinative languages morpheme;languages morpheme based;agglutinative language words;morpheme based language;languages morpheme;words formed suffixes;morpheme based;vocabulary agglutinative languages;uyghur language agglutinative;suffix asr;morpheme referred prefix;agglutinative language;stem suffix asr;language agglutinative language;word morpheme;vocabulary asr built;agglutinative languages;language words formed;language agglutinative;vocabulary agglutinative;suffixes;morpheme;suffix asr results;morpheme referred;word morpheme syllable;vocabulary asr;formed suffixes;large vocabulary asr;units word morpheme;morpheme syllable", "pdf_keywords": ""}, "8f99f9409f254134aa32fbf072475100f688d613": {"ta_keywords": "codes distributed storage;secure regenerating codes;theoretic secrecy capacity;secrecy capacity;information theoretic secrecy;regenerating codes distributed;secrecy capacity setting;information theoretically secure;storage networks;codes distributed;theoretically secure regenerating;distributed storage networks;distributed storage;storage networks provide;class codes distributed;storage nodes;secure regenerating;threat model eavesdropper;constructions regenerating codes;secrecy;theoretic secrecy;storage nodes possibly;regenerating codes achieve;eavesdropper gain access;regenerating codes;subset storage nodes;storage;theoretically secure;repair information theoretically;codes achieve information", "pdf_keywords": "theoretic secrecy capacity;secrecy capacity;information theoretic secrecy;secrecy capacity setting;presented optimal secure;information theoretically secure;secure mbr code;optimal secure msr;information eavesdropper decode;optimal secure;symbols information eavesdropper;constructions regenerating codes;theoretic secrecy;secure msr code;information eavesdropper;threat model eavesdropper;theoretically secure mbr;eavesdropper decode random;eavesdropper decode;secrecy;regenerating codes achieve;eavesdropper gain access;eavesdropper gains access;codes achieve information;parameters 2k secure;symbols eavesdropper;capacity store symbols;secure mbr;regenerating codes;theoretically secure"}, "b130b6387b105ecd9b4718b179b1e128157f9516": {"ta_keywords": "paraphrase models score;paraphrase models;paraphrase model trained;phrase paraphrase tasks;parametric paraphrase models;paraphrase database;paraphrase pairs accurately;model paraphrase database;paraphrase database ppdb;paraphrase model paraphrase;paraphrase model;paraphrase database compositional;compositional paraphrase model;paraphrase tasks;paraphrase pairs;using paraphrase model;score paraphrase pairs;database compositional paraphrase;build parametric paraphrase;improving coverage paraphrase;models score paraphrase;coverage paraphrase database;phrase pairs ppdb;short phrase paraphrase;paraphrase tasks unclear;phrase paraphrase;compositional paraphrase;using paraphrase;parametric paraphrase;phrase pairs", "pdf_keywords": ""}, "191543c7cb084d3af6a48ae771ca3dfd0588ab22": {"ta_keywords": "learning transform recommendation;user review target;text reviews target;recently deep learning;target user review;text reviews;representation text reviews;recommendation tasks recently;reviews target user;especially review text;recommendation tasks;reviews target item;learn latent representation;text reviews written;improve performance recommender;review target item;review text;user review;recommendation unsurprisingly predictive;especially review;reviews target;review text available;deepconn uses neural;text comes reviews;tasks recently deep;recent model deepconn;performance recommendation tasks;review target;deep learning;deep learning methods", "pdf_keywords": "recently deep learning;networks deepconn;learning transform recommendation;deepconn uses neural;neural networks deepconn;recent model deepconn;learn latent representation;representation text reviews;model deepconn;transnets learning;deep cooperative neural;model deep cooperative;model deepconn uses;recommendation tasks recent;user text reviews;recent model deep;deep learning;text reviews;nets learn latent;neural nets learn;recommendation tasks;deep learning methods;text reviews wri;transform recommendation arxiv;text reviews target;transnets learning transform;deepconn uses;combines latent representations;representations regression layer;model deep"}, "c72cdb5ce7e0911c7f442ab503652d6fdeef35e0": {"ta_keywords": "ccg parsers semantic;parsers semantic slot;syntactic ccg parsers;freebase semantic parsing;grounded semantic parsing;semantic parsing;parsers grounded semantic;parsers semantic;semantic parsing declarative;induced ccg parsers;ccg parsers;spades semantic parsing;semantic parsing dataset;parsing declarative sentences;ccg parsers grounded;parsing declarative;syntactic ccg;explore syntactic supervision;parsers;parsing;syntactic supervision;parsers grounded;different syntactic ccg;syntactic supervision required;downstream semantic analysis;required downstream semantic;grounded semantic;unsupervised grammar induction;semantics captured;semantic slot filling", "pdf_keywords": "supervised ccg parsers;semantic parsing;freebase semantic parsing;grounded semantic parsing;semantic parsing task;grammars learning semantically;semantic parsing declarative;spades semantic parsing;sentences annotated ccg;ccg parsers;parsing declarative sentences;semantic parsing dataset;parsing declarative;parsers;ccg categories sentences;parsing;ccg categories lexicon;learning semantically useful;learning semantically;semantic evaluation;semantic evaluation induced;\ufb01rst semantic evaluation;input supervision lexicon;grammars learning;grounded semantic;ccg parsers equal;evaluation induced grammars;annotated ccg;induced grammars learning;weakly supervised ccg"}, "61d2dda8d96a10a714636475c7589bd149bda053": {"ta_keywords": "review networks caption;encoders review networks;rnn encoders review;review networks;rnn decoders cnn;cnn rnn encoders;rnn encoders;decoders cnn rnn;networks caption generation;cnn rnn;review network;rnn decoders;called review network;consider rnn decoders;caption generation;attention mechanism encoder;review network performs;review network review;attention mechanism decoder;review steps attention;networks caption;network review network;encoders review;decoders cnn;review;rnn;called review;input attention;attention;review network generic", "pdf_keywords": ""}, "12239e761e8c7cd05e12e18f43dba7b46dfd8ac1": {"ta_keywords": "translation data augmentation;multi source translation;neural machine translation;source translation systems;multi source neural;machine translation data;source neural machine;machine translation;machine translation nmt;translation data;translation nmt multi;translation systems;source translation;data augmentation approach;source neural;translation systems translate;propose data augmentation;data augmentation;data augmentation experiments;text multiple sources;corpora parallel text;translate multiple languages;sources target language;necessary corpora parallel;multi source;augmentation approach incomplete;translation nmt;translate multiple;corpora parallel;nmt multi source", "pdf_keywords": "multi source translation;neural machine translation;translation data augmentation;machine translation nmt;source translation systems;machine translation data;machine translation outputs;corpus missing translations;machine translation;\ufb01lled machine translation;multi source neural;translation nmt trained;translation systems;source neural machine;translation data;translation systems translate;incomplete multilingual corpora;using translation nmt;missing translations;translation nmt;incorporate incomplete multilingual;source neural;translations \ufb01lled machine;source translation;incomplete multilingual;multi source examples;translation outputs;trained multi source;missing translations \ufb01lled;translation outputs using"}, "d2a2be6ce932a0f1939f31cfff4d64ea3d76723d": {"ta_keywords": "adversarial attacks human;robustness adversarial attacks;robustness adversarial;confers robustness adversarial;adversarial attacks present;set robustness adversarial;adversarial;adversarial attacks;datasets confers robustness;training set robustness;classification robust contemporary;classification robust;makes classification robust;robust contemporary classifiers;uncertainty explicit training;explicit training dataset;attacks human uncertainty;uncertainty makes classification;contemporary classifiers;contemporary classifiers fail;distribution human labels;increasingly training distribution;generalization increasingly training;confers robustness;robustness;training distribution test;explicit training;classifiers;supports improved generalization;benchmark dataset cifar10h", "pdf_keywords": "confers robustness adversarial;robustness adversarial;distributional shift adversarial;robustness adversarial attacks;adversarial;shift adversarial;adversarial attacks;adversarial attacks work;adversarial attacks \ufb01rst;shift adversarial attacks;datasets confers robustness;uncertainty explicit training;human labels models;human category uncertainty;labels models trained;similarity paradigms training;generalization human uncertainty;distribution human labels;human uncertainty image;models trained;category uncertainty imagelevel;uncertainty image classi\ufb01cation;generalization increasingly training;explicit training dataset;approximate human uncertainty;evaluation generalization human;increasingly training distribution;distributions approximate human;label distributions approximate;human perceptual similarity"}, "1668b0b9cc631cdfc0dfaf77b71627f5524a866c": {"ta_keywords": "accelerated directional derivative;stochastic convex optimization;smooth stochastic convex;convex optimization;method smooth stochastic;accelerated directional;stochastic convex;directional derivative method;derivative method smooth;smooth stochastic;directional derivative;optimization;derivative method;method smooth;convex;accelerated;stochastic;derivative;directional;smooth;method", "pdf_keywords": "stochastic convex optimization;smooth stochastic convex;stochastic convex strongly;strongly convex optimization;derivative free optimization;convex optimization corollaries;stochastic convex;methods smooth stochastic;convex optimization;method smooth stochastic;accelerated directional derivative;bound accelerated gradient;convex optimization pavel;convex optimization problems;optimization corollaries derivative;directional derivatives objective;novel directional derivative;free optimization;free optimization prove;accelerated gradient;smooth stochastic;complexity bound accelerated;optimization prove complexity;directional derivative methods;convex strongly convex;accelerated gradient based;derivatives objective function;strongly convex;derivative methods smooth;consider smooth stochastic"}, "3e398bad2d8636491a1034cc938a5e024c7aa881": {"ta_keywords": "cnn transformer;advantages cnn transformer;advantages cnn;dense prediction convolutions;cnns achieved great;cnns achieved;vision tasks convolutions;cnn backbones;pyramid vision transformer;replacement cnn;direct replacement cnn;cnn transformer making;replacement cnn backbones;pyramid vision;networks cnns achieved;introduce pyramid vision;dense prediction tasks;cnn;networks cnns;convolution free backbone;cnns;prediction convolutions;prediction convolutions convolutional;inherits advantages cnn;maps pyramid vision;simpler convolution free;convolutions convolutional neural;convolutions;vision transformer;convolution free", "pdf_keywords": "shrinking pyramid spatial;spatial reduction attention;pyramid spatial reduction;reduction attention layer;progressive shrinking pyramid;reduction attention;shrinking pyramid;attention layer;pyramid spatial;spatial reduction;attention layer obtain;segmenting transparent object;segmenting transparent;scale feature maps;highresolution multi scale;attention;pyramid;multi scale feature;develop progressive shrinking;resources segmenting transparent;feature maps limited;feature maps;object wild transformer;transparent object wild;memory resources segmenting;maps limited computation;scale feature;progressive shrinking;segmenting;highresolution multi"}, "115f1366318e7622f89f3a870e5863282670b1ad": {"ta_keywords": "preoperative use statins;statin therapy protective;use statins protective;statins protective;statins protective factor;pre operative statin;statins prevention;operative statin therapy;statins prevention post;statin therapy pc;mechanism statins prevention;statin therapy;statin therapy background;acute kidney injury;statins reviewed;contrast acute kidney;mechanism statins reviewed;use statins;kidney injury pc;pci serum creatinine;specific mechanism statins;operative statin;kidney injury;statins achieved;kidney injury multicenter;mechanism statins;statins;statin therapy independent;statin;affected type statins", "pdf_keywords": ""}, "d516daff247f7157fccde6649ace91d969cd1973": {"ta_keywords": "model interpretability supervised;interpretability supervised machine;model interpretability;interpretability supervised;mythos model interpretability;interpretability;remarkable predictive capabilities;predictive capabilities work;predictive capabilities;machine learning models;supervised machine learning;remarkable predictive;learning models boast;predictive;models;supervised;models boast;supervised machine;learning models;models boast remarkable;boast remarkable predictive;machine learning;mythos model;model;capabilities;capabilities work;mythos;learning;capabilities work deployment;machine", "pdf_keywords": "interpretable deep neural;models interpretable deep;interpretable deep;interpretable models;underlying interpretability;models interpretable;interpretability \ufb01nding diverse;underlying interpretability \ufb01nding;interpretable models diverse;motivations underlying interpretability;descriptions interpretable models;linear models interpretable;interpretability;interpretability identifying;interpretability \ufb01nding;presently investigated interpretability;confer interpretability;suggesting interpretability;investigated interpretability;motivations interpretability;interpretability identifying transparency;confer interpretability identifying;models interpretable identify;interpretability technical descriptions;interpretable;descriptions interpretable;interpretability techniques;interpretability technical;interpretability refers concept;thought confer interpretability"}, "5ba57ff3c3e6e319586b86a990b6e082f4ecf972": {"ta_keywords": "bayesian speech recognition;bayesian approaches speech;bayesian techniques speech;fully bayesian speech;bayesian speech;estimation clustering speech;clustering speech recognition;speech recognition vbec;modeling speech classification;clustering speech;speech recognition;approaches speech recognition;acoustic modeling speech;speech recognition framework;speech recognition fully;speech classification;modeling speech;effectiveness speech recognition;techniques speech recognition;bayesian estimation clustering;variational bayesian estimation;speaker adaptation;vbec variational bayesian;variational bayesian approaches;speech recognition shown;introduces variational bayesian;speech recognition paper;speaker adaptation tasks;experimentally speaker adaptation;variational bayesian", "pdf_keywords": ""}, "e5d143ae82ede67726aa1a9aeac3de4bf53d8920": {"ta_keywords": "vision language pretraining;knowledgeaware representations improve;knowledge based vision;pretraining approach knowledge;knowledge graph embeddings;knowledgeaware representations;enhance learning semantically;aligned knowledgeaware representations;semantically aligned knowledgeaware;learning semantically;language pretraining;uses knowledge graph;advantage commonsense knowledge;multimodal tasks;knowledge;representations improve models;representations various multimodal;vision language;knowledge graph;based vision language;commonsense knowledge logical;various multimodal tasks;commonsense knowledge;language pretraining kb;nlvr okvqa tasks;learning semantically aligned;learning cross model;pretraining approach;cross model representations;novel pretraining approach", "pdf_keywords": ""}, "7f52e3914a61994f68583635e43bc1bb9203e3b3": {"ta_keywords": "markers genetic toxicity;genetic toxicity;genetic toxicity counting;health effects tobacco;effects tobacco habits;subjects experimental smokers;effects tobacco;fumes cofs smoking;environment cytogenetic consequences;dna damage;biological markers genetic;workers occupationally exposed;cytogenetic consequences food;tobacco habits;dna damage frequency;risk aggravates genetic;environment cytogenetic;cofs smoking habits;occupationally exposed cooking;experimental smokers;experimental smokers non;working environment cytogenetic;markers genetic;cytogenetic consequences;mn dna damage;tobacco habits working;smoking habits;health effects;toxicity counting;smokers non smokers", "pdf_keywords": ""}, "60a4ad8e8f4389f317d109550f5da2a571cbb515": {"ta_keywords": "question answering searching;background corpus answering;question answering;answering searching relevant;corpus answering cloze;answer large corpus;answering searching;corpus answering;factoid question answering;questions retrieval;questions retrieval extracting;cloze questions retrieval;natural language query;large corpus text;retrieval extracting relevant;sentences documents corpus;text answer query;answer query reading;documents corpus;extracting relevant sentences;retrieval extracting;questions answers;large corpus;answer query posts;corpus text quasar;documents corpus given;comprehend natural language;language query extract;text quasar dataset;relevant sentences documents", "pdf_keywords": "question answering search;datasets question answering;question answering;answering search reading;answering search;answer large corpus;natural language query;passages extract answers;searching large corpus;extract answers;large corpus text;question answer context;tasks qa searching;qa searching large;questions answers;reading question answer;qa searching;comprehend natural language;search reading;language query extract;answer context excerpt;answer context;large corpus;search reading question;questions answers obtained;natural language;scripting quasar dataset;reading passages extract;answering;extract answer large"}, "15513c732d6af975f312307be3b5e2bd674ac0ef": {"ta_keywords": "speech recognition errors;speech recognition asr;word error rate;perfect transcription utterances;transcription utterances;impact speech recognition;utterances trained tuned;transcription utterances trained;automatic speech;speech recognition;utterances trained;humans perceived error;automatic speech recognition;impact asr errors;perceived error differently;brain processes language;brain potential erp;utterances;recognition errors;recognition asr;perceived error;language perceives effect;minimizing word error;potential erp studies;recognition asr systems;impact speech;potential erp;deletions insertions words;recognition errors framework;insertions words treated", "pdf_keywords": ""}, "7261b088c48be7eca10263e765739f7347665481": {"ta_keywords": "commodity flow markovian;multi commodity flow;markovian network equilibrium;commodity flow;flow markovian network;network equilibrium;demand multi commodity;flow markovian;variable demand;variable demand multi;markovian network;equilibrium;multi commodity;demand multi;demand;flow;commodity;markovian;network;multi;variable", "pdf_keywords": "equilibrium network games;markovian network equilibrium;network equilibrium generalizes;network equilibrium arxiv;equilibrium network;network equilibrium;network equilibrium model;commodity flow markovian;wardrop equilibrium network;network equilibrium considering;illustrate network equilibrium;multi commodity ride;flow markovian network;commodity ride sharing;multi commodity flow;markovian network;network games;equilibrium generalizes;equilibrium arxiv;model multi commodity;commodity ride;equilibrium;abstract markovian network;flow markovian;commodity flow;equilibrium model multi;classical wardrop equilibrium;ride sharing example;demand multi commodity;equilibrium generalizes classical"}, "4fb8009422903f7cb6f9a929409264b7fbca55e3": {"ta_keywords": "noise feature vectors;feature enhancement;make feature enhancement;feature enhancement method;corrupted noise features;proposes feature enhancement;noise features;high speech recognition;clean feature vectors;feature enhancement process;feature vectors best;corrupted noise feature;noise piecewise linear;noise feature;extended feature vectors;frames corrupted noise;stereo based piecewise;corrupted feature vectors;feature vectors;noise features order;changes noise piecewise;speech recognition;learns piecewise linear;known stereo based;piecewise linear transformation;piecewise linear compensation;environments splice algorithm;stereo based;feature vectors subspace;feature vectors corresponding", "pdf_keywords": ""}, "359dfdfea38f645d5fa49efc846a3b5ebce317fe": {"ta_keywords": "injecting machine learning;build interpretable models;machine learning matter;interpretable models;machine learning software;machine learning;interpretable models express;does know loan;learning software agent;software agent does;interpretable;discomfort machine learning;build interpretable;know loan decide;know loan;software agent;social harm;learning;cautious injecting machine;arms build interpretable;learning matter;applications significant risk;causing social harm;risk causing social;agent does know;loan decide qualifies;ought cautious injecting;social harm calls;cautious injecting;models", "pdf_keywords": "interpretable machine learning;interpretable machine;machine learning practitioners;\ufb01eld interpretable machine;data \ufb01eld interpretable;machine learning audience;interpretable;\ufb01eld interpretable;risk mortality recognizing;automated lending stakeholders;doctors interested predicting;machine learning papers;curiously interpretable machine;predictive models deeply;practitioners integrate predictive;mortality recognizing;mortality recognizing cancer;machine learning touted;machine learning advance;machine learning;recognizing cancer;decision maker understand;conceivable interpretation technique;predicting risk mortality;accept curiously interpretable;2016 machine learning;predictive;salvage supervised learning;understanding conceivable interpretation;predictive models"}, "2c94bc68388517aa4a2d2dfc7d35df95ce24b1a8": {"ta_keywords": "adversarial invariant feature;generalization adversarial invariant;adversarial invariant;learning representations invariant;invariant feature learning;better generalization adversarial;classifications bias free;generalization adversarial;fair classifications bias;classifications bias;adversarial;independent image classification;invariant feature;tasks fair classifications;fair classifications;learning representations;representations invariant specific;learning meaningful representations;feature learning;representations invariant;bias free;machine learning benchmark;learning benchmark tasks;classification;image classification;induces invariant representation;invariant representation;problem learning representations;learning benchmark;benchmark tasks fair", "pdf_keywords": ""}, "5a0c9bbf0432dac8bd357a4aabf82b83a6c95524": {"ta_keywords": "document similarity measures;document similarity;based document similarity;document similarity approach;aspect based similarity;similarity aspect information;document similarity leads;citations indicate aspect;traditional document similarity;document classification;document classification task;pairwise document classification;similarity aspect;aspect based document;similar dissimilar documents;paper citations;papers paper citations;extend similarity aspect;paper extend similarity;citations;similarity measures;similarity measures provide;aspect information;pairs acl anthology;paper citations indicate;based similarity;similarity;results indicates aspect;dissimilar documents;aspect based", "pdf_keywords": "semantic textual similarity;textual similarity;textual similarity agirre;textual similarity combining;targeted disambiguation;computing semantic textual;language models wikipedia;content similarity measures;pilot semantic textual;document summarization scienti\ufb01c;semantic textual;summarization scienti\ufb01c articles;document similarity;improving question answering;document similarity development;question answering;2007 targeted disambiguation;based document similarity;generative language models;word sense disambiguation;multi document summarization;language models;question answering linking;statistical machine translation;document summarization;sense disambiguation;content similarity;summarization scienti\ufb01c;machine translation;multiple content similarity"}, "4f74be7e5dd4b8e9113e86132cf792da2c32ca3d": {"ta_keywords": "\u30d1\u30e9\u30d5\u30ec\u30fc\u30ba\u3092\u8003\u616e\u3057\u305f\u6a5f\u68b0\u7ffb\u8a33\u306e\u8aa4\u308a\u7b87\u6240\u9078\u629e \u8a00\u8a9e\u7406\u89e3\u3068\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3 \u7b2c6\u56de\u96c6\u5408\u77e5\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0;\u30d1\u30e9\u30d5\u30ec\u30fc\u30ba\u3092\u8003\u616e\u3057\u305f\u6a5f\u68b0\u7ffb\u8a33\u306e\u8aa4\u308a\u7b87\u6240\u9078\u629e \u8a00\u8a9e\u7406\u89e3\u3068\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3;\u8a00\u8a9e\u7406\u89e3\u3068\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3 \u7b2c6\u56de\u96c6\u5408\u77e5\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0;\u30d1\u30e9\u30d5\u30ec\u30fc\u30ba\u3092\u8003\u616e\u3057\u305f\u6a5f\u68b0\u7ffb\u8a33\u306e\u8aa4\u308a\u7b87\u6240\u9078\u629e;\u8a00\u8a9e\u7406\u89e3\u3068\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3;\u7b2c6\u56de\u96c6\u5408\u77e5\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0", "pdf_keywords": ""}, "90720bba46dd79bc340359617a7a07fcecc890c1": {"ta_keywords": "local nash equilibria;generic local nash;differential nash equilibria;nash equilibria generic;degenerate differential nash;equilibria games smooth;characterize local nash;equilibria generic local;differential nash;nash equilibria games;nash equilibria open;local nash;nash equilibria;player costs smooth;equilibria games;games smooth costs;smooth perturbations player;perturbations player costs;games smooth;stability non degenerate;equilibria structurally stable;equilibria generic;player costs implies;smooth costs continuous;genericity structural stability;continuous strategy spaces;equilibria open dense;perturbations player;costs smooth functions;equilibria structurally", "pdf_keywords": ""}, "834fb0d09e764b88ef76ee77e0befb8faeaad7fe": {"ta_keywords": "speech parameters generated;proposed speech parameters;based speech synthesis;speech synthesis;generated fitted speech;speech parameters;speech parameter;synthesis proposed speech;initial speech parameters;speech parameter sequence;speech synthesis unit;hmm based speech;synthesis initial speech;synthetic speech quality;improvements synthetic speech;model hmm based;segments speech parameter;fitted speech probability;speech synthesis enables;speech parameters tend;speech quality proposed;synthetic speech;traditional hmm based;model hmm;modeling acoustic features;speech quality;speech quality use;speech quality compared;better synthetic speech;combining hmm based", "pdf_keywords": ""}, "2507a6924007efbe0c3116048a85108398f23007": {"ta_keywords": "translations interlinear glossed;automatic glossing models;interlinear glossed text;collect translations;easy collect translations;language documentation projects;glossed text;collect translations interlinear;creating automatic glossing;glossing models;glossing models using;automatic glossing;language documentation;glossed text igt;source neural models;interlinear glossed;information language documentation;documentation projects scholarly;glossing;linguistic information language;translations;translations interlinear;documentation projects;multi source neural;linguistic information;source neural;encoding linguistic information;text igt widely;format encoding linguistic;glossed", "pdf_keywords": ""}, "5ad44a9d6b850405da42f989711af431427425b5": {"ta_keywords": "lingual systems hindi;mixed language language;mixed language;cross lingual systems;code mixed hindi;mixing multiple languages;script mapping phonetic;native script language;language language tts;language code mixed;hindi target language;mixed hindi english;experiments cross lingual;mixed hindi;languages used conversation;lingual systems synthesis;bilingual speakers languages;lingual systems;multiple languages;language tts database;prefer cross lingual;language code;listening tests bilingual;phonetic space mixed;language language;target language code;language written native;tests bilingual speakers;language tts;cross lingual", "pdf_keywords": ""}, "d95b66901d72a13d0c96c7e9bfd4a999ed7fb19c": {"ta_keywords": "smt systems english;gram language models;language models;language models systems;germanenglish interpolated gram;splitting germanenglish interpolated;splitting germanenglish;word splitting germanenglish;network language models;neural network language;interpolated gram language;phrase based translation;language models hypotheses;germanenglish interpolated;systems english german;based translation formalisms;gram language;translation formalisms;systems english;english mt tasks;germanenglish;ntt naist smt;iwslt 2014 evaluation;2014 evaluation campaign;recurrent neural network;based translation;word splitting;preordering phrase based;syntactic preordering phrase;syntactic preordering", "pdf_keywords": ""}, "ad21f0709a3e5d7db91606e2f67926f93e01838d": {"ta_keywords": "minimize contextual regret;contextual games;contexts game outcomes;contextual games type;players contextual regrets;contextual regret;games driven contextual;class contextual games;contextual regret individual;optimal contextual welfare;contextual regrets;contexts game;repeated games driven;different contexts game;players contextual;optimal contextual;game theoretic notions;correlations present game;repeated games;games cces optimal;game outcomes propose;game theoretic;game outcomes;approached players contextual;cce optimal contextual;driven contextual information;contextual regrets vanish;regret individual players;type repeated games;correlations minimize contextual", "pdf_keywords": "ef\ufb01ciency contextual games;contextual games regret;equilibria optimal contextual;contextual games de\ufb01ning;contextual games;contextual games type;play contextual games;contextual regret contextual;vanishing contextual regret;regret contextual coarse;contextual regret;optimal contextual welfare;class contextual games;players contextual regrets;contextual regret natural;identify contextual regret;game theoretic notions;play contextual;regret contextual;games cces optimal;games notion coarse;de\ufb01ne game theoretic;algorithms play contextual;optimal contextual;players contextual;games regret;contextual regrets;games notion;equilibria ef\ufb01ciency contextual;cce optimal contextual"}, "f2d5861a24b7aa33036208ba81e11bb9b2090e7c": {"ta_keywords": "paraphrase generation models;multilingual paraphrasing model;multilingual paraphrasing;shot paraphrase generation;zero shot paraphrasing;paraphrase generation architectural;corpora generate paraphrases;end multilingual paraphrasing;output paraphrase generation;paraphrase generation;generate paraphrases;paraphrasing model trained;generate paraphrases meaning;paraphrasing baselines evaluated;shot paraphrasing;zero shot paraphrase;shot paraphrasing baselines;paraphrasing baselines;paraphrasing;paraphrasing model;paraphrases meaning;paraphrases meaning spaces;output paraphrase;paraphrases;shot paraphrase;continuous output paraphrase;paraphrase;word embeddings;layer word embeddings;monolingual rewriting facilitates", "pdf_keywords": "unsupervised paraphrasing embedding;training paraphrasing models;multilingual paraphrasing model;paraphrasing embedding;paraphrasing embedding outputs;multilingual paraphrasing;paraphrasing model trained;shot paraphrase generation;paraphrasing models generating;diversity unsupervised paraphrasing;training paraphrasing;paraphrasing models;corpora generate paraphrases;end multilingual paraphrasing;paraphrase generation work;generate paraphrases;paraphrase generation;unsupervised paraphrasing;generate paraphrases meaning;paraphrasing model;method training paraphrasing;paraphrasing;paraphrases meaning spaces;paraphrases meaning;zero shot paraphrase;paraphrases;shot paraphrase;word embeddings;generating embedding;models generating embedding"}, "0e02765103001a792b20242b4dee6dc81917b850": {"ta_keywords": "gene recognizer flybase;entity recognition biomedical;annotated biomedical text;named entity recognition;manually annotated biomedical;bootstrapping gene recognizer;gene recognizer;annotated noisy text;annotated biomedical;automatically annotated noisy;annotation bootstrapping;consistent annotation bootstrapping;automatically annotated;manually annotated;entity recognition;curation automatically annotated;recognizer flybase curation;based annotation scheme;annotated noisy;general manually annotated;based annotation;annotation;annotation bootstrapping evaluating;enabling consistent annotation;annotation scheme;consistent annotation;distinguishes gene names;task based annotation;recognizer flybase;gene mentions", "pdf_keywords": ""}, "5b94512a17483595c5dffc503a16ba0b46c347e5": {"ta_keywords": "extracting disambiguated hypernymy;hypernyms sets synonyms;disambiguated hypernymy relationships;improve hypernymy extraction;hypernymy extraction;relationships propagate hypernyms;propagate hypernyms sets;hypernyms sets;hypernymy relationships propagate;disambiguated hypernymy;synonyms synsets;synonyms synsets constructs;sets synonyms synsets;extracting disambiguated;propagate hypernyms;recognizes hypernymy relationships;method extracting disambiguated;patterns wiktionary datasets;successfully recognizes hypernymy;wiktionary datasets;sets synonyms;hypernymy relationships standard;synonyms;recognizes hypernymy;hypernyms;unsupervised sense representations;hypernymy relationships;relationships matching synsets;synsets constructs embeddings;wiktionary datasets respective", "pdf_keywords": ""}, "0e52ce6cfd1385e1e9304dcf71d66b53fdc2d4bd": {"ta_keywords": "nlp applied news;challenge news ir;challenges journalism relate;challenges journalism;data challenge news;journalism relate information;papers challenges journalism;journalism environment workshop;news ir readers;integration news sources;community news ir;information retrieval ir;processing news content;stories entity detection;challenge news;workshop newsir;processing news;workshop newsir 16;news content;ir nlp applied;problems ir nlp;journalism relate;relate information retrieval;ir natural language;time processing news;processing nlp;news ir;nlp fields;information retrieval;journalism", "pdf_keywords": ""}, "e705255814756178dba75638c29b602095c3cdf4": {"ta_keywords": "deep reinforcement learning;learned deep agents;deep reinforcement;reinforcement learning drl;source deep reinforcement;deep agents;deep agents drl;reinforcement learning;transfer learning experiments;transferable representations learned;layers trained game;learning policy experiments;complexity learning policy;representations learned;representations learned deep;transfer learning;trained game;transfer learning learning;learning policy scratch;sample complexity learning;learned deep;learning policy;learning learning policy;complexity learning;reinforcement;trained game related;learning experiments;layers trained;agents drl agents;benefits transfer learning", "pdf_keywords": "agents trained atari;learned deep agents;trained atari;deep reinforcement learning;deep reinforcement;transferable representations learned;agents atari games;source deep reinforcement;deep agents;transfer learning experiments;agents trained;trained atari 2600;agents atari;deep agents compare;representations learned;training learns;experiments agents trained;models training learns;representations learned deep;rainbow agents atari;transferability features learned;learns;learning experiments agents;transfer learning;learned deep;111 transfer learning;learned rainbow agents;atari games;reinforcement learning;reinforcement learning drl"}, "51203e9d5620abdcdf6c9be93b1e221e79cda67d": {"ta_keywords": "external language model;transfer learning external;external language;languages using external;attention based s2s;s2s model adaptation;learning external text;language independent asr;transfer learning;using external language;target language;target languages;target languages compared;low resource languages;simple transfer learning;build language independent;language model lm;target language experimental;lm fusion transfer;language independent;adaptation perform lm;learning external;framework transfer learning;model adaptation;shared vocabulary languages;better adaptation methods;better adaptation;context target language;performances target languages;language model", "pdf_keywords": "transfer learning language;language model fusion;external language model;lm adaptation language;language independent s2s;language independent asr;languages adaptation;asr language model;adaptation language;vocabulary languages adaptation;transfer learning external;adaptation language independent;languages adaptation perform;external lm adaptation;attention based s2s;language model lm;external language;s2s model adaptation;languages using external;learning language independent;language independent end;transfer learning;low resource languages;simple transfer learning;language model;learning external text;asr language;lm adaptation;sequence s2s architecture;lm fusion transfer"}, "8319786ed7b9cb13e29130b5617bf0aef586cd6f": {"ta_keywords": "cognitive tutor authoring;performance authoring tutoring;generated authoring tutoring;model cognitive tutor;create cognitive tutors;tutor authoring;authoring tutoring better;generated tutoring;tutor authoring tools;tutoring author interactively;cognitive tutor tutoring;cognitive tutors;author interactively tutors;authoring tutoring;cognitive tutor;model generated tutoring;generated tutoring benefits;cognitive tutors heavy;demonstrated tutoring;interactively tutors simstudent;authoring tutoring requires;tutoring better;tutoring;interactively tutors;tutoring author;authoring tutoring author;tutoring requires;tutor tutoring simstudent;expert model cognitive;tutoring simstudent", "pdf_keywords": ""}, "c37c40db51ccfd6f93004e788102ede72578e5d8": {"ta_keywords": "extraction reliable information;collaborative extraction reliable;efficient information extraction;information extraction;information extraction filtering;collaborative extraction;reliable information;extracted information;tool collaborative extraction;confirm extracted information;reliable information given;information present workers;verification deciding information;deciding information present;deciding information;efficient information;machine learning;worker confirm extracted;human worker confirm;reliability important information;manual verification deciding;extracted information framework;machine learning techniques;efficiency manual verification;using machine learning;extraction filtering situations;filtering situations;necessary human worker;important information combed;workers using machine", "pdf_keywords": ""}, "92259193a9d7377368790bf8517cd9798f30caae": {"ta_keywords": "question answering xqa;answering qa interfaces;explainable question answering;question answering qa;question answering solution;question answering;answering qa;information pollution web;solution information pollution;pollution web requires;answering xqa;qa interfaces;answering xqa alleviate;questions xqa ii;following questions xqa;answering solution information;questions xqa;access information web;information roadmap explainable;information pollution provides;qa interfaces simplified;tackle question answering;pollution web;xqa explainable;information web;qa;user friendly interfaces;information pollution;xqa explainable question;xqa ii need", "pdf_keywords": ""}, "0e1a665334b1ec35d77ab1cd4f21bd0da9745548": {"ta_keywords": "ripper sleeping experts;ripperand sleeping experts;sleeping experts phrases;learning algorithms ripperand;algorithms ripperand sleeping;sleeping experts;sleeping experts differ;text categorization;differences ripper sleeping;sleeping experts perform;text categorization problems;categorization problems generally;classifiers;experts phrases evaluated;large text categorization;ripper sleeping;categorization;experts phrases;categorization problems;classifier;ripperand sleeping;classification;classifiers allow;algorithms construct classifiers;classifiers allow context;categorization problems spite;algorithms ripperand;machine learning algorithms;classification recently;classification recently implemented", "pdf_keywords": ""}, "9ca95a09c8bf2d7d28234ff37ece182836dd8632": {"ta_keywords": "imitation learning parser;transition based parsing;learning parser learns;parser learns;learning parser;learning amr parsing;representation parsing task;task imitation learning;parsing algorithm abstract;parser learns statistical;representation parsing;meaning representation parsing;imitation learning amr;exact imitation learning;parsing task;parsing;parser;imitation learning;parsing algorithm;parsing alpha;2016 task imitation;abstract meaning representation;based parsing algorithm;task imitation;amr parsing;imitation learning algorithm;amr parsing alpha;parsing task using;use imitation learning;based parsing", "pdf_keywords": ""}, "0aa0131253b832fdba27ac43f8fa78a322763191": {"ta_keywords": "transformation paralinguistic information;lingual transformation paralinguistic;transformation paralinguistic;paralinguistic information input;proposed methods paralinguistic;methods paralinguistic information;paralinguistic information;text speech synthesis;translation text speech;speech translation model;speech translation technology;methods paralinguistic;paralinguistic information evaluate;channels speech translation;speech synthesis;text speech;speech source language;paralinguistic features;output speech;different possible paralinguistic;possible paralinguistic features;paralinguistic;paralinguistic features handle;speech output;speech output speech;speech translation;speech target language;speech synthesis components;input speech source;possible paralinguistic", "pdf_keywords": ""}, "a94ec1cd89839aa5132118916849d46dff861914": {"ta_keywords": "mind modeling situated;mind modeling;embodied ai agents;modeling situated interactions;embodied ai;theory mind modeling;developing embodied ai;human collaborative behaviors;human collaborative;situated interactions;collaborate human;collaborative behaviors;situated interactions introduce;collaborative behaviors situated;theory mind tasks;able collaborate human;behaviors situated language;agents human world;collaborative tasks;autonomous agents human;collaborative tasks performed;belief states collaborative;world interaction;mind tasks;ai agents;mind tasks enable;beliefs world interaction;collaborate human terms;situated language communication;ai agents able", "pdf_keywords": "mind modeling situated;situated dialogue collaborative;modeling situated dialogue;mind modeling;dialogue collaborative tasks;theory mind modeling;mindcraft theory mind;dialogue collaborative;belief states collaborative;collaborative interactions;collaborative agent;situ collaborative agent;predicting mutual belief;ground mindcraft theory;situated dialogue;mindcraft theory;collaborative tasks cristian;common ground mindcraft;human mental states;interaction discourse visual;viewpoint agent task;collaborative tasks;states collaborative interactions;interaction discourse;task hand collaborative;mental state viewpoint;ground mindcraft;human mental;player mental states;situated environments"}, "8278e5c2a894793e2c93c6c9f0e7535109e7858f": {"ta_keywords": "sowed herbs landscape;herbs landscape conditions;density sowed herbs;herbs landscape;sowed herbs;standing density sowed;landscape conditions;herbs;landscape;standing density;density sowed;dependence standing density;sowed;standing;density;dependence standing;dependence;conditions", "pdf_keywords": ""}, "c637636a2afd7968bdb893af8d2fd220fd39df8f": {"ta_keywords": "meeting transcription utterances;time meeting recognition;meeting recognition;monitoring meetings experimental;real time meeting;monitoring meetings;meeting transcription;meeting recognition understanding;meeting analyzer;recognize speaking online;meeting analyzer monitoring;captures utterances face;automatically recognize speaking;time meeting transcription;time meeting analyzer;speech recognizer;utterances face pose;continuously captures utterances;meetings experimental;meetings experimental results;latency monitoring meetings;face pose speaker;captures utterances;pose speaker using;monitoring conversations;monitoring conversations ongoing;camera center meeting;transcribed speech recognizer;distant microphone;recognize speaking", "pdf_keywords": ""}, "ffac42087ee4ad50df9203762db715dedd209c0b": {"ta_keywords": "grammar semantic tags;parsing tags;free grammar semantic;tag propagation;beneficial parsing tags;parsing tags spread;tags beneficial parsing;tags spread parse;tag propagation mechanism;context free grammars;context free grammar;grammar semantic;parse tree propagation;mechanism tag propagation;parsing;free grammars;grammars enriched semantic;free grammar;easily extracted parse;semantic tags developed;beneficial parsing;free grammars enriched;parse;semantic tags;parse tree usually;rules context free;parse tree;extracted parse tree;enriched semantic tags;propagation instruction semantic", "pdf_keywords": ""}, "919c929dfa665cb0595a835b4380f96da4cd0143": {"ta_keywords": "sampling spatial fields;proposed sampling spatial;sampling spatial;reconstruction field samples;sampling locations;sampling locations approximately;fields mobile sensors;field reconstruction;mobile sensing;assumes sampling locations;field reconstruction field;field samples;proposed sampling;mobile sensing recently;spatial fields mobile;explores multiple sampling;sampling strategies;sampling strategies performance;recently proposed sampling;sampling;multiple sampling strategies;multiple sampling;reconstruction field;paths sample reconstruct;sensing matrices;sample reconstruct dimensional;sample reconstruct;sampling strategies random;sampling typically;classical contemporary sampling", "pdf_keywords": "sensing proposed sampling;sampling spatial \ufb01elds;mobile sensing proposed;bandlimited \ufb01eld sampling;mobile sensing dimensional;sensing dimensional bandlimited;mobile sensing;proposed sampling spatial;sampling spatial;sampling random paths;mobile sensing recently;recently mobile sensing;explores multiple sampling;\ufb01eld sampling;abstract mobile sensing;multiple sampling strategies;proposed sampling;sampling strategies;sensing proposed;paths sample reconstruct;\ufb01elds mobile sensors;sampling strategies recently;random paths sample;multiple sampling;sampling;sensing matrices;recently proposed sampling;paths reconstruction interpolation;mobile sensors;reconstruct dimensional bandlimited"}, "59bdf61a81e46a6c9a96c0c5f96f2f77b82ab09f": {"ta_keywords": "pipeline subsurface detonation;steel pipeline subsurface;x70 steel pipeline;steel pipeline;buried x70 steel;subsurface detonation;pipeline subsurface;weld geometry parameters;behavior buried x70;effect weld geometry;weld geometry;x70 steel;buried x70;effect weld;detonation;pipeline;weld;dynamic behavior buried;steel;buried;subsurface;x70;geometry parameters dynamic;behavior buried;geometry parameters;parameters dynamic behavior;parameters dynamic;dynamic;dynamic behavior;parameters", "pdf_keywords": ""}, "c340b89e7b7fa84fac85cdcf38ba7007e2e71930": {"ta_keywords": "speaker diarization mainly;speaker embeddings clustering;speaker diarization;dealing speaker diarization;clustering speaker embeddings;dynamics speaker diarization;speaker diarization results;based clustering speaker;outputs speaker diarization;clustering speaker;speaker diarization problem;global speaker characteristics;speaker embeddings;capture global speaker;activity dynamics speaker;global speaker;end neural diarization;local speech activity;speech activity dynamics;speech activity;speaker characteristics;talker recording;neural diarization;multi talker recording;speaker characteristics addition;short term memory;dealing speaker;dynamics speaker;self attention capture;diarization mainly", "pdf_keywords": "end neural diarization;global speaker characteristics;neural diarization model;self attention mechanism;representation self attention;based neural diarization;speaker characteristics visualizing;self attention blocks;neural diarization;neural diarization showed;attention mechanism end;speech activity dynamics;end end neural;speech activity;end neural;local speech activity;self attention capture;attention mechanism;self attention;speaker characteristics;blstm based neural;attention blocks;attention blocks actually;global speaker;speaker characteristics addition;captured global speaker;diarization showed self;latent representation self;incorporated self attention;capture global speaker"}, "0fcdf20477f907aa50578876226f5fabf5e074ea": {"ta_keywords": "citations bias propagate;relevant citations bias;citations bias;citation networks;citation networks methods;systems suggest citations;link prediction methods;citation networks authors;world citation networks;link prediction;study link prediction;example citation networks;suggest citations;suggest citations academic;trained link predictors;link predictors;citations;truly relevant citations;citations academic papers;citations academic;bias propagate naively;relevant citations;real world citation;prediction methods frequently;cite preferentially;networks exposure bias;naively trained link;bias arise users;cite;bias propagate", "pdf_keywords": "citations empirically;citations empirically validate;relevant citations empirically;citation networks;citation networks methods;world citation networks;citations;real world citation;citation data microsoft;citation data;truly relevant citations;relevant citations;world citation data;citation;world citation;recommend relevant papers;probabilities mitigate bias;true link probabilities;estimators true risk;empirically proposed estimators;estimators leverage known;bias consequent feedback;estimates true risk;link probabilities;propensity scores estimation;academic graph;microsoft academic graph;empirically validate;propose estimators leverage;bias consequent"}, "da1296f071bb2b65ae9e0b016d914d24b4edb2d2": {"ta_keywords": "financial stability financials;stability financials;frontier capital markets;financial stability;stability financials systems;defines financial stability;stability undeveloped capital;undeveloped capital markets;metrics frontier capital;frontiercapital market designation;markets achievement frontiercapital;frontiercapital market;global investment;investment theinternational monetary;market development stability;capital market development;capital markets;global investment encouragedby;capital market;liquid capital market;investment theinternational;capital markets network;market promote stability;capital markets achievement;undeveloped capital;basedanalysis capital market;achievement frontiercapital market;foreign direct investment;capital market promote;investment", "pdf_keywords": ""}, "916c8553beb3ba4e0d20ba6d7eb2bca365d820c8": {"ta_keywords": "stance detection bidirectional;stance detection;01 stance detection;detection bidirectional conditional;bidirectional conditional encoding;stance;ep 01 stance;detection bidirectional;01 stance;bidirectional conditional;conditional encoding;bidirectional;detection;encoding;conditional;ep 01;ep;01", "pdf_keywords": ""}, "7f613ab03d776f996eb582f04d258a51868dca03": {"ta_keywords": "lipase catalyzed hydrazine;alkyl benzohydrazides hydrazine;lipase organic chemistry;lipase catalyzed synthesis;synthesis alkyl benzohydrazides;benzohydrazides hydrazine;benzohydrazides hydrazine insertion;potential lipase catalyzed;chemistry alkyl benzohydrazides;catalyzed lipase optimal;biocatalyst lipase catalyzed;insertion catalyzed lipase;biocatalyst lipase;catalyzed lipase;lipase catalyzed;alkyl benzohydrazides;lipase used biocatalyst;hydrazine insertion catalyzed;used biocatalyst lipase;catalyzed hydrazine;catalyzed hydrazine insertion;benzohydrazides classic organic;lipase organic;hydrazine insertion synthesis;utilization lipase organic;lipase optimal;benzohydrazides;alkyl benzohydrazides classic;porcine pancreatic lipase;alkyl benzohydrazides extend", "pdf_keywords": ""}, "2aa85084315a4107e2b9b935506b4e9f11428601": {"ta_keywords": "enamel surfaces treatment;bovine enamel surfaces;remineralization occurred monitoring;exposure remineralization;exposure remineralization solution;enamel surfaces;study determine remineralization;period exposure remineralization;changes swir reflectance;determine remineralization occurred;days exposure remineralization;determine remineralization;incipient caries lesions;caries lesions;caries lesions extended;swir reflectance;air bovine enamel;swir reflectance measurements;bovine enamel;remineralization occurred;reflectance;reflectance measurements;images samples dried;light images samples;remineralization;reflected light images;enamel;remineralization solution changes;simulated incipient caries;remineralization solution extended", "pdf_keywords": ""}, "542ad79bb96fc10c46086778aaafc8f3509c5c18": {"ta_keywords": "alternating gradient descent;gradient descent ascent;nonconvex nonconcave minimax;gradient descent;nonconvex nonconcave objectives;nonconcave minimax;inequality alternating gradient;nonconcave objectives satisfying;adversarial learning;networks adversarial learning;nonconcave objectives;ascent agda algorithm;optimization class nonconvex;nonconcave minimax problems;descent ascent agda;nonconvex minimax;adversarial networks;adversarial networks adversarial;emerging machine learning;networks adversarial;descent ascent;alternating gradient;adversarial;generative adversarial networks;subclass nonconvex nonconcave;nonconvex minimax problems;ascent agda;minimax problems nonconvex;applications generative adversarial;class nonconvex nonconcave", "pdf_keywords": "alternating gradient descent;gradient descent ascent;nonconvex nonconcave minimax;nonconcave minimax;agda subclass minimax;ascent agda algorithm;abstract nonconvex minimax;nonconcave minimax problems;nonconvex nonconcave objectives;optimization class nonconvex;descent ascent agda;nonconvex minimax;gradient descent;agda algorithm converges;nonconvex minimax problems;nonconcave objectives satisfying;nonconcave objectives;ascent agda;inequality alternating gradient;stochastic agda;adversarial learning;optimization class;subclass minimax;stochastic agda achieves;class nonconvex nonconcave;networks adversarial learning;adversarial learning conclusion;subclass nonconvex nonconcave;minimax problems represented;minimax"}, "69ee9b3a915951cc84b74599a3a2699a66d4004f": {"ta_keywords": "imitation11 learning;conditioned imitation11 learning;imitation11 learning agent;language conditioned imitation11;robotic manipulation experiments;robotic manipulation;robotic;robots ability manipulate;cliport pathways robotic;conditioned imitation11;robots;manipulate objects;real world tasks;learn transferable concepts;quickly learn transferable;semantic understanding clip;pathways robotic manipulation;robots ability;tasks packing unseen;learn multi task;task models cliport;ability manipulate objects;manipulation experiments 16;imitation11;cliport language;manipulate objects pre1;world tasks;unseen objects 14;cliport language conditioned;objects 14", "pdf_keywords": "manipulation semantic spatial;imitationlearning agent combines;language conditioned imitationlearning;languageconditioned imitation learning;imitationlearning agent;manipulation semantic;cliport languageconditioned imitation;vision language grounding;imitation learning agent;conditioned imitationlearning agent;conditioned imitationlearning;imitationlearning;semantic understanding clip;conference robot learning;imitation learning;vision language;languageconditioned imitation;robot learning corl;robot learning;agent integrates semantic;semantic spatial pathways;learning agent;semantic spatial;vision based manipulation;learning \ufb01ne grained;learning agent integrates;architecture manipulation semantic;robot;integrates semantic understanding;cliport language conditioned"}, "fa3826770207f7bf8bd85a8e97c9ac437f46b061": {"ta_keywords": "comparisons baseline procedures;multiple comparisons baseline;multiple comparisons;comparisons baseline;adjust multiple comparisons;multiple comparisons ensuring;involve multiple comparisons;adjustment permutation test;maxt permutation test;compared unadjusted tests;test permutation;test permutation based;unadjusted tests particular;comparisons account largest;permutation test values;unadjusted tests;comparisons ensuring;taking comparisons;comparisons;obtained taking comparisons;procedures adjust multiplicity;permutation test;permutation test permutation;comparisons ensuring probability;bonferroni adjustment permutation;individually significant results;tests particular;inference procedures small;available test collections;taking comparisons account", "pdf_keywords": ""}, "ce89ee7aaeeea2c9d474707690f3ea9d948776a3": {"ta_keywords": "machine translation noisy;dataset machine translation;translation noisy;translation noisy text;machine translation mt;modern machine translation;machine translation;noisy inputs translations;translations english comments;translation mt systems;sourced translations;professionally sourced translations;noisy text mtnt;37k sentences language;inputs translations;translations;french japanese comments;noisy comments;consisting noisy comments;comments french japanese;english comments french;comments english;noisy comments reddit;comments french;translation mt;japanese comments english;noisy text;translations english;english comments;comments english order", "pdf_keywords": "machine translation noisy;testbed machine translation;machine translation mt;translation noisy text;modern machine translation;translation noisy;dataset machine translation;machine translation;translation mt systems;noisy text mtnt;noisy text mt;translations english comments;handling noisy text;noisy text paul;professionally sourced translations;translation mt;noisy text;sourced translations;consisting noisy comments;text mtnt;collect noisy comments;sentences professionally sourced;37k sentences language;text mtnt consisting;noisy comments;translations;professional translators;text mt;translators;noisy comments reddit"}, "7f85b7ee0fc6cdb5b92417035a7049247729545a": {"ta_keywords": "imbalance performance classifiers;class imbalance performance;performance classifiers increasing;performance random forest;classifiers increasing;classifiers empirical study;random forest study;classifiers increasing number;imbalance performance;increased class imbalance;class imbalance dataset;maximize performance classifiers;performance classifiers kappa;class imbalance;classifiers;classifier adversely;classifiers kappa;performance classifiers;classifier adversely affected;classifiers benchmark;classifiers empirical;classifiers benchmark datasets;performed classifiers benchmark;imbalance dataset;random forest;datasets balanced class;classifiers kappa coefficient;performed classifiers;power classifier adversely;classifier", "pdf_keywords": ""}, "ce5a57c0ccc8993f4a8e3a07101140a757024d9f": {"ta_keywords": "public speeches memorable;features public speeches;speech automatic memorable;spoken quote detection;speeches memorable;automatic memorable spoken;public speeches;distinguish memorable spoken;natural expressive speech;accuracy public speeches;speech public talks;speeches;speeches ted talks;public speeches ted;speeches memorable inspirational;expressive speech public;expressive speech automatic;memorable spoken quotes;quote detection;speeches ted;quote detection results;expressive speech;quotes non memorable;speech;speech public;speech automatic;memorable spoken quote;non memorable spoken;spoken quotes;talks study natural", "pdf_keywords": ""}, "61ad8a0778598022e71c0ee3ba9bc53ddd616517": {"ta_keywords": "parsing question answering;question answering;questions answers matching;semantic parsing;semantic parsing question;decomposed question sequences;question answering focused;answers matching;associated table answering;questions contain coreferences;question sequences collect;question answer pairs;complicated question intents;parsing;question sequences inquire;question intents expressed;coreferences previous questions;table answering;question intents;question sequences;answers matching words;parsing question;work semantic parsing;table answering complicated;questions answers;066 question sequences;coreferences;answering complicated;inquire semi structured;answering focused long", "pdf_keywords": "sequential question answering;semantic parsing dataset;sqa semantic parsing;question answering;semantic parsing;question answering collect;semantic parser;semantic parsing called;question answering natural;semantic parser design;inter question coreferences;questions wikipedia tables;\ufb01rst semantic parsing;traditional semantic parser;question coreferences;wikipedia tables;question coreferences baseline;parsing dataset sequential;wikipedia tables 17;parsing;coreferences baseline experiments;parsing dataset;sqa semantic;structured tables wikipedia;parser design;coreferences;parsing called;\ufb02oating parser;systems sqa semantic;questions answer"}, "9837207d3f4ee8c493375a97077c6f8b22cadac9": {"ta_keywords": "durational residency tests;invalidity durational residency;durational residency;residency tests disarray;residency tests;national state citizenship;state citizenship;supreme court reasoning;residency;court doctrinal explanations;citizenship;validity invalidity durational;conclusion court doctrinal;constitutional commentary;court reasoning;constitutional commentary combines;court reasoning claim;invalidity durational;despair supreme court;form constitutional commentary;court doctrinal;constitutional;validity;conclusion court;supreme court;validity invalidity;equal treatment newcomers;form constitutional;newcomers core meaning;doctrinal explanations validity", "pdf_keywords": ""}, "de9d3a28f9e112a248d097d72ba6ad41a71c8a78": {"ta_keywords": "learning indeterminate clauses;cryptographic limitations learning;learning clause logic;learning learning logic;learning logic programs;learning logic;learning clause;clause logic programs;limitations learning clause;formally problem learning;learning learning indeterminate;learning indeterminate;logic programs examples;logic programs;problem learning single;limitations learning;learning single;logic programs paper;single horn clause;clause logic;variables equivalent learning;depth determinate clauses;clause focus generalizations;clauses used practical;clauses indeterminate variables;problem learning;horn clause;generalizations language constant;predictionpreserving reducibilities introduced;learning single horn", "pdf_keywords": ""}, "bcde1ba141078cf37a69a691fd329d8fd7e70b9b": {"ta_keywords": "criterion active learning;active learning;stopping criterion active;stopping criterion;criterion active;learning;stopping;criterion;active", "pdf_keywords": ""}, "83d6b4bfa8701578c291e55f5f1e5e6508aff313": {"ta_keywords": "medical ethics technology;ethics technology ethics;technology ethics;ethics consider computer;ethics technology;patient autonomy;medical ethics;medical ethics consider;patients autonomy;patients autonomy messy;patient autonomy ability;technology ethics leverage;patient autonomy model;matter patient autonomy;concepts medical ethics;focus patient autonomy;capacity informed consent;consent decisional capacity;informed consent decisional;difference medical ethics;concepts informed consent;ethics consider;consent discussion technology;consent decisional;informed consent;autonomy;ethics;autonomy messy;informed consent discussion;autonomy ability", "pdf_keywords": ""}, "1bc87dba9838b3028b636f456084252f2beac108": {"ta_keywords": "cooperative game;non cooperative game;stackelberg game;behavior occupants distributing;particle swarm optimization;cooperative game present;stackelberg game multiple;efficient behavior occupants;occupants distributing;particle swarm;reversed stackelberg game;occupants distributing points;building manager occupants;nash equilibrium game;game encouraging energy;using particle swarm;swarm optimization;manager occupants;occupants solve;play non cooperative;equilibrium game;utility leader;game multiple followers;occupants solve resulting;nash equilibrium;bilevel optimization problem;greater utility leader;occupants;swarm optimization method;equilibrium game using", "pdf_keywords": ""}, "f016ac107259d6d222c9f52b37208fca4fa1d6bc": {"ta_keywords": "modeling ads scenarios;ads scenarios integrating;use case modeling;ads scenarios use;specifying ads scenarios;specify ads scenarios;use case models;case modeling rucm;used modeling ads;case modeling;driving systems adss;ads scenarios potential;ads scenarios;rucm4ads used modeling;modeling ads;case modeling methodology;requirements adss method;scenarios use case;specifying requirements adss;applicability rucm4ads;applicability specifying ads;scenarios integrating;evaluate applicability rucm4ads;case models;requirements adss;model owm ontology;traffic environments;scenarios use;domain operational world;complex traffic environments", "pdf_keywords": ""}, "36c770b79937db2e3416204b8cf177d0c9881f54": {"ta_keywords": "rotorcraft flight visualization;visualization using hums;rotorcraft flight;flight visualization using;flight visualization;rotorcraft;using hums data;hums data;using hums;hums;visualization using;visualization;flight;data;using", "pdf_keywords": ""}, "0a485fd94b2cb554e281d0f8d7e9f71db4891ce0": {"ta_keywords": "attention vision transformers;novel token downsampling;tokens minimizing reconstruction;softmax attention;token downsampling;complexity attention vision;token downsampling method;vision transformers limit;state art downsampling;exploiting redundancies images;vision transformers;attention vision;attention major computation;token pooling efficiently;quadratic complexity attention;assumptions softmax attention;images intermediate token;complexity attention;token representations;vision transformers self;token pooling significantly;token representations experiments;improves cost accuracy;softmax;pooling significantly improves;softmax attention acts;pooling efficiently;downsampling existing methods;intermediate token representations;requirements vision transformers", "pdf_keywords": "token pooling vision;tokens minimizing reconstruction;novel token downsampling;token downsampling;token downsampling method;token representations;intermediate token representations;images intermediate token;pooling vision transformers;token representations new;token pooling ef\ufb01ciently;tokens minimizing;softmax attention;propose token pooling;token pooling;data aware downsampling;token pooling novel;exploiting redundancies images;pooling vision;softmax;set tokens minimizing;aware downsampling operator;vision transformers limit;aware downsampling;clustering propose token;approximates set tokens;vision transformers;assumptions softmax attention;called token pooling;mild assumptions softmax"}, "d1f32060e921b6e06badd7fdb2b750638b2d131c": {"ta_keywords": "acoustic modeling classification;acoustic modeling networks;feature extraction acoustic;acoustic processing;acoustic processing including;beamforming acoustic modeling;acoustic modeling components;stages acoustic processing;performs acoustic modeling;beamforming acoustic;extraction acoustic modeling;acoustic modeling;network performs acoustic;beamforming feature extraction;parameters beamforming acoustic;features derived microphone;joint training networks;classification parameters beamforming;represent stages acoustic;training networks;extraction acoustic;training networks proposed;stages acoustic;microphone channels;beamforming feature;processing including beamforming;including beamforming feature;microphone;networks trained jointly;beam estimated network", "pdf_keywords": ""}, "98290fb02a844108df202e9a6dc3461e3f14ee32": {"ta_keywords": "stationary points optimization;convex compact objective;stationary point surrogate;point surrogate problem;bounds nearly optimal;surrogate problem guarantee;compact objective function;surrogate problem remains;points optimization problems;resulting surrogate problem;optimization problems;surrogate problem;points optimization;optimization problems form;optimization;order taylor approximation;optimal;approximate order stationary;nearly optimal;point resulting surrogate;approximation finding;point surrogate;objective function smooth;compact objective;taylor approximation;approximation finding near;finding approximate order;optimal aforementioned reduction;smooth assumed convex;problem finding approximate", "pdf_keywords": "nonconcave composite minimax;nonconvex nonconcave min;nonconcave min max;stationary points optimization;min max optimization;composite minimax problems;nonconcave min;structured nonconvex nonconcave;composite minimax;nonconvex nonconcave;small maximization domain;max optimization small;nonconvex nonconcave composite;minimax problems decentralized;optimization small maximization;max optimization;minimax problems;small maximization;points optimization problems;maxy sets convex;minimax;method structured nonconvex;structured nonconvex;gradient descent ascent;minx maxy sets;maximization domain;optimization problems form;points optimization;method saddle point;optimization problems"}, "ad26e5105b6019ff68404962e39ea3a1dfb5931d": {"ta_keywords": "incentive design temporal;algorithm synthesize incentive;synthesize incentive sequence;optimal behavior incentives;optimal sequence incentives;incentive sequence minimizes;incentive design;incentive sequence;sequence incentives principal;formula incentive design;desired behavior incentives;sequence incentives;temporal logic objectives;incentives realizes principal;incentives principal;logic formula incentive;incentives certain;behavior incentives;behavior incentives certain;agent optimal behavior;incentives principal offer;agent finite decision;design temporal logic;behavior incentives realizes;behavior markov decision;incentives;markov decision process;formula incentive;synthesize incentive;incentive", "pdf_keywords": "optimal trajectory agent;motion planning;ensuring optimal agent;algorithm synthesize incentive;optimal agent behavior;motion planning examples;agent markov decision;optimal agent;method motion planning;\ufb01nite planning horizon;optimal trajectory;trajectory agent providing;markov decision process;sequential decision making;planning horizon;\ufb01nite planning;trajectory agent;sequence incentives minimizes;end \ufb01nite planning;model sequential decision;agent markov;synthesize sequence incentives;agent objective maximize;synthesize incentive design;markov decision;process markov decision;agent providing incentives;sequential decision;decision process mdp;planning examples"}, "89a8edbc0fe2ea8b9ee703ca37e5d5d6d34c571a": {"ta_keywords": "neural machine translation;end speech translation;predict paraphrased transcriptions;paraphrased transcriptions auxiliary;speech translation e2e;improves translation performance;training automatic speech;consistently improves translation;speech translation;translation performance effectiveness;machine translation;machine translation nmt;paraphrased transcriptions;translation nmt tasks;speech translation propose;translation performance;source transcription;source transcription pre;improves translation;transcriptions auxiliary task;automatic speech;transcriptions auxiliary;train bilingual e2e;end train bilingual;model predict paraphrased;transcription pre training;translation propose bidirectional;transcription;predict paraphrased;source target bidirectional", "pdf_keywords": "neural machine translation;predict paraphrased transcriptions;paraphrased transcriptions auxiliary;paraphrased transcriptions;end speech translation;speech translation e2e;machine translation nmt;machine translation;translation nmt tasks;training automatic speech;model predict paraphrased;transcriptions auxiliary task;speech translation;training bilingual e2e;predict paraphrased;train bilingual e2e;source transcription;transcriptions auxiliary;end train bilingual;source transcription pre;transcription pre training;generated source paraphrases;source paraphrases target;asr neural machine;bidirectional knowledge distillation;target bidirectional knowledge;transcription;paraphrases target;leverage source transcription;source target bidirectional"}, "daa7e6af585d03e9cb05487413a6495f23400398": {"ta_keywords": "assignment wireless networks;assignment problems wireless;assignment wireless;permutation matrices deep;matrices deep learning;resource assignment wireless;feasible assignment solutions;generating feasible assignment;feasible assignment;assignment solutions;binary assignment problems;set permutation matrices;variables permutation matrices;binary assignment;network learns;approach binary assignment;matrices deep;permutation matrices;assignment problems;deep learning;networks identifies binary;task set permutation;resource assignment;wireless networks identifies;binary variables permutation;network training;wireless networks;sinkhorn neural network;wireless networks poses;problems wireless networks", "pdf_keywords": "binary constraints snn;constraints snn architecture;assignment wireless networks;convex assignment problems;constraints snn;projections permutation matrix;assignment problem solvers;non convex assignment;network assignment problems;convex projections permutation;practical network assignment;binary assignment problems;assignment problems wireless;snn architecture proposed;snn architecture;assignment wireless;network assignment scenarios;network assignment;permutation matrices unsupervised;variables permutation matrices;snn designed solve;set permutation matrices;matrices unsupervised training;projections permutation;convex assignment;layer snn designed;binary assignment;approach binary assignment;permutation matrices;assignment problems numerical"}, "f11ed27f4640dd8785ea7c4aff9705ddaad2b24d": {"ta_keywords": "news detection fake;multimedia fake news;fake news detection;detection fake news;detecting multimedia fake;visual content fake;content fake news;content fake;content important fake;multimedia fake;issues multimedia fake;detection fake;technology fake news;fake news attempts;multimedia technology fake;news detection social;news detection;news detection effectively;proliferation fake news;fake news increasing;important fake news;detection social media;fake news including;detecting multimedia;videos attract mislead;images videos attract;content images;fake news;news attempts utilize;technology fake", "pdf_keywords": ""}, "ad4b09832454a821e925e45e96e769f0c01bd3d6": {"ta_keywords": "topics sparse word;sparse word graphs;documents topics sparse;topics sparse;correlations topic models;statistical topic models;topic models;capturing word correlations;sparse word;topic models work;topic models latent;word graphs scalable;word correlations;word correlations topic;word graphs;called sparse word;word graphs statistical;topics semantically meaningful;topics semantically;models latent dirichlet;ap corpus;words representation documents;bag words representation;documents topics;experiments ap corpus;summarize large document;modeling word corre;sparse mod eling;latent dirichlet al;corpus", "pdf_keywords": ""}, "1f133158a8973fb33fea188f20517cd7e69bfe7f": {"ta_keywords": "attention sublayer transformer;transformer encoder architectures;sublayer transformer encoder;transformer encoder;encoder architectures massively;encoder architectures;transformer encoder standard;accurate efficient transformers;encoder;efficient transformers;efficient transformers long;attention sublayer;self attention sublayer;self attention sublayers;attention sublayers;fnet scales efficiently;attention sublayers simple;fast tpus fnet;efficiently long inputs;benchmark training;sublayer transformer;accuracy bert;bert glue benchmark;92 accuracy bert;encoder standard unparameterized;encoder standard;transformer;transformers;transformers long range;transformers long", "pdf_keywords": "attention sublayer parameterized;attention sublayer;attention sublayers;attention sublayers simple;self attention sublayers;small fnet encoders;fnet encoders;encoder architectures sped;fnet encoders outperform;transformer encoder architectures;replace attention sublayer;attention;transfer learning;encoders outperform small;fnet bert;\ufb01rst replace attention;transformer encoder;encoders outperform;replace attention;sentence prediction nsp;encoders;compare fnet bert;abstract transformer encoder;baselines transfer learning;encoder architectures;encoder;masked language modelling;fnet bert devlin;sentence prediction;mlm sentence prediction"}, "c9ce3889c03fee2990b2277423bbc0fb4366df53": {"ta_keywords": "discriminative language modeling;discriminative language;language modeling;modeling structured classification;language modeling structured;abstract discriminative language;structured classification;structured classification problem;discriminative;log linear models;continuous space model;abstract discriminative;classification;modeling structured;linear models;model continuous space;space model;models;word error rate;classification problem;problem abstract discriminative;language;model continuous;linear models previously;stand model continuous;space model yields;lower word error;structured;model;publications users recognise", "pdf_keywords": ""}, "2de8019fd7d04e3d1305d5efaeeb591f0d966550": {"ta_keywords": "speech recognition asr;speech recognition;speech recognition especially;recognition asr;new decoding strategy;automatic speech recognition;automatic speech;speedup new decoding;new decoding;deep transformers outperformed;recognition asr cmlm;term memory networks;short term memory;structure automatic speech;memory networks;deep transformers;autoregressive transformer 7x;autoregressive transformer;input speech;decoding;context input speech;large margin speech;non autoregressive transformer;autoregressive network asr;decoding strategy proposed;margin speech recognition;recently deep transformers;outperformed kaldi asr;asr benchmarks;decoder", "pdf_keywords": ""}, "ca2144b895cf6812eec535261df9294896417425": {"ta_keywords": "relation extraction classification;semantic relation extraction;relation classification task;relation extraction task;relation extraction;relation extraction model;relation classification;senerio relation extraction;second relation classification;end relation extraction;semantic relation;extraction classification scientific;task semantic relation;classification scientific papers;extraction task subtask;classification task subtask;extraction classification;shared task semantic;relation;concept candidate embeddings;task semantic;semeval 2018 task;semantic;second relation;candidate embeddings;extraction task;subtask senerio relation;classification task;encoding attention;end relation", "pdf_keywords": "semantic relation extraction;scienti\ufb01c relation extraction;relation extraction selectively;relation extraction;relation extraction model;relation extraction classi\ufb01cation;end relation extraction;information extraction;models information extraction;information extraction proposing;extraction classi\ufb01cation scienti\ufb01c;concept embeddings;incorporated concept embeddings;concept candidate embeddings;semantic relation;extraction classi\ufb01cation;leverage semantic information;candidate embeddings;large scholarly dataset;task semantic relation;scholarly dataset extend;semantic information;semantic information concepts;trained large scholarly;classi\ufb01cation scienti\ufb01c papers;concept selection;neural models information;extraction proposing concept;information concepts pre;scholarly dataset"}, "3638e5dfc79ba3fb757900f46ac0c7e7f6dadb05": {"ta_keywords": "context aware cameraphone;experience personal photography;personal photography;aware cameraphone application;cameraphone use image;cameraphone application mobile;cameraphone application;image capture sharing;mobile media sharing;technology photographic practices;image sharing;people use photos;aware cameraphone;cameraphone use;cameraphone;use image sharing;emerging technology photographic;technology photographic;media sharing;mobile media;photographic practices developments;photographic practices;photography;media sharing relate;use photos;application mobile media;photographic;reducing barriers cameraphone;barriers cameraphone use;capture sharing", "pdf_keywords": ""}, "86eb740bbc54a6d734242be28fccf76fd4d2c1ba": {"ta_keywords": "nash equilibrium multiagent;equilibrium multiagent control;nash derive convergence;convergence guarantees gradient;game hessian symmetric;quadratic dynamic game;cooperative multi agent;equilibrium multiagent;bound differential nash;game hessian;known nash equilibrium;values game hessian;multiagent control;guarantees gradient based;multiagent control problem;differential nash;guarantees gradient;finite time convergence;derive convergence guarantees;algorithms non cooperative;differential nash derive;nash equilibrium;convergence guarantees;learning algorithms non;multiagent;gradient based learning;convergence guarantee linear;multi agent;dynamic game known;hessian", "pdf_keywords": ""}, "0fdc3efc11526995d192f18e19f07fba062a76f7": {"ta_keywords": "weak supervision labeling;supervision labeling training;programmatic weak supervision;supervision labeling;weak supervision paradigms;labeling training;labeling training data;weak supervision;noisy supervision sources;various weak supervision;supervision paradigms programmatic;weak supervision pws;supervision sources survey;potentially noisy supervision;synthesizing training labels;noisy supervision;training labels;supervision sources;limited labeled data;labeled data;labeling bottleneck programmatically;labeling;limited labeled;supervision paradigms;labeled;training labels multiple;manual labeling;programmatically synthesizing training;manual labeling bottleneck;supervision pws achieved", "pdf_keywords": "programmatic weak supervision;labeling training;abstract labeling training;learning representation;learning representation learning;deep learning representation;labeling training data;weak supervision;representation learning approaches;representation learning;component pws learning;deep learning;introduction pws learning;weak supervision jieyu;pws learning;labeled;labeled data;labeling;limited labeled data;limited labeled;pws learning paradigm;pws learning work\ufb02ow;labeled data scenarios;learning;training data;supervision jieyu zhang1;abstract labeling;representation;feature;supervision"}, "66340a93813d8f816a8c82354a8f39fa985de27f": {"ta_keywords": "domain question answering;question answering;question answering qa;answering science exam;textual entailment trained;knowledge conceptnet;answering science;background knowledge conceptnet;questions selected challenging;knowledge conceptnet tandem;conceptnet;ai nlp emerging;challenge dataset contains;arc challenge dataset;challenging current qa;questions authored grade;entailment trained scitail;ai nlp;answering qa;exam questions using;science exam;entailment trained;chance answering science;exams questions selected;science exams;textual entailment;challenge dataset;answering qa important;generic textual entailment;qa task", "pdf_keywords": ""}, "2ac6b8ade2a5e1ac89b99012ca6548eca4f8323f": {"ta_keywords": "topological layer based;topological features input;novel topological layer;topological layer;efficient topological layer;exploit underlying topological;underlying topological features;topological layer general;topological features;underlying topological;persistent landscapes efficiently;layer based persistent;based persistent landscapes;persistent landscapes;persistent homology arbitrary;persistent homology;novel topological;general persistent homology;layer general deep;propose novel topological;persistent landscapes provide;information topological features;layer robust;efficient topological;inputs general persistent;layers improve learnability;topological;models based persistent;filtration efficient topological;layer robust noise", "pdf_keywords": "topological layer based;novel topological layer;topological layer feeds;exploit topological features;topological layer;topological features input;develop topological layer;topological features effectively;topological features underlying;exploit topological;topological features;topological layer paper;developing novel topological;layers improve learnability;information topological features;novel topological;develop topological;learnability deep learning;harness develop topological;weighted persistence landscape;topological;learnability deep;signi\ufb01cant topological features;landscape exploit topological;layer robust;learnability networks;learnability networks given;information topological;enhance learnability deep;pllay novel topological"}, "1fa02e5a5adffe82a41225f61f5f8ce86cf229d0": {"ta_keywords": "label mrf segmentation;mrf segmentation;mrf segmentation energies;mrf normalized cut;segmentation energies benefit;segmentation energies;normalized cut;label combinatorial optimization;new segmentation;spectral cut kernel;segmentation;propose new segmentation;normalized cut nc;spectral cut;multi label mrf;techniques spectral cut;mrf constraints;segmentation clustering;efficient multi label;mrf normalized;segmentation clustering model;new segmentation clustering;kernel cut using;field mrf normalized;label mrf;mrf constraints enforcing;random field mrf;dimensional image features;optimization techniques spectral;kernel cut", "pdf_keywords": ""}, "1cfd9b1db68fc320698da05fc6876dd0ea96fc9b": {"ta_keywords": "speech recognition asr;speech recognition;asr based connectionist;connectionist temporal classification;temporal classification ctc;recognition asr model;recognition asr;automatic speech recognition;trained model depth;automatic speech;training pruning;layer pruning;pruning method asr;classification ctc allows;asr model mobile;classification ctc;based connectionist temporal;training pruning method;ctc model pruned;practice layer pruning;present training pruning;end automatic speech;improving real time;temporal classification;trained model;connectionist;connectionist temporal;layer pruning demand;transformer ctc;asr based", "pdf_keywords": "speech recognition asr;speech recognition;ctc model pruned;recognition asr model;transformer ctc model;method transformer ctc;transformer ctc;asr based connectionist;trained model depth;training pruning;automatic speech recognition;training pruning method;pruning method asr;recognition asr;layer pruning;automatic speech;present training pruning;asr model mobile;layer pruning demand;pruning present depth;model depth run;trained model;ctc stochastic depth;classi\ufb01cation ctc allows;end automatic speech;model mobile embedded;temporal classi\ufb01cation ctc;improving real time;ctc allows reduction;model pruned"}, "acbb4495dd698b3190db6899d7d35b0817e0a85e": {"ta_keywords": "gradient descent ascent;minimax learning;standard gradient descent;training non convex;ascent gda proximal;trained minimax;networks gans;networks gans observed;gradient descent;minimax learning problems;problems generative adversarial;success minimax learning;non concave minimax;performance trained minimax;generalization performance trained;generative adversarial networks;minimax optimization;descent ascent gda;concave minimax;depend minimax optimization;gans observed;adversarial networks gans;generative adversarial;generalization performance stochastic;gans observed depend;trained minimax model;learning problems generative;minimax optimization algorithm;descent ascent;risk convex", "pdf_keywords": "learned minimax models;generalization minimax;minimax learning;learned minimax;minimax models learned;analyzing generalization minimax;performance learned minimax;concave gan problems;minimax learners;minimax learning problems;generalization properties minimax;non concave gan;gradient based minimax;concave gan;based minimax learners;minimax optimization;minimax learners farzan;success minimax learning;gan problems;generalization minimax settings;gradient descent ascent;non concave minimax;depend minimax optimization;minimax optimization algorithm;networks gans observed;concave minimax;standard gradient descent;learners non convex;minimax models;networks gans"}, "0053f75b7053f43b9787a9955426281e672b147b": {"ta_keywords": "outside recursive autoencoder;recursive autoencoder;recursive autoencoder diora;syntax simultaneously learns;recursive auto encoders;discovering syntax simultaneously;discovering syntax;trees sentence inference;parse approach predicts;autoencoder;learns representations;tree induction deep;autoencoder diora fully;learns representations constituents;autoencoder diora;unsupervised latent tree;scoring parse approach;auto encoders;parse;scoring parse;latent tree induction;simultaneously learns representations;parse approach;sentence unsupervised latent;predicts word input;outside recursive auto;introduce deep inside;highest scoring parse;inside outside recursive;induction deep", "pdf_keywords": "unsupervised parser training;unsupervised parser;unsupervised parsing;parser trained autoencoder;parser training;outside recursive autoencoders;parser trained;experiments unsupervised parsing;parser training extract;recursive autoencoders diora;learns syntactic structure;unsupervised parsing chunking;recursive autoencoders;learns syntactic;extract shallow parses;shallow parses;method unsupervised parser;recursive autoencoders andrew;unlabeled constituency parsing;chart parser trained;entities syntactic trees;parses noun phrases;syntax simultaneously learns;constituency parsing unlabeled;syntactic trees domain;syntactic trees;parser diora achieves;objective learns syntactic;parsing unlabeled;phrase representations"}, "95e8edd26744ecc2bc23996cfaa68fe6252442a9": {"ta_keywords": "fake news detection;news detection graph;automatic fake news;graph based semantic;veracity news claim;graph structure learning;news detection evidences;semantic structure mining;evidence aware fake;detection graph neural;aware fake news;fake news critical;evidence based fake;graph neural;grained semantic representations;news claim solve;news claim;veracity news;probe veracity news;perniciousness fake news;semantic structure;graph neural networks;semantic representations fed;graph structured data;grained semantic;based fake news;semantic representations;news detection;claims evidences sequences;based semantic structure", "pdf_keywords": "claim evidence graphs;graph based semantic;semantic structure mining;graph structure learning;fake news detection;evidence graphs;graph based fake;semantic structure;semantic structure obtaining;complex semantic structure;based semantic structure;graph structured data;structure obtaining contextual;structure mining framework;graphs longdistance semantic;structure mining;structure learning;graph structured;contextual semantic information;structure learning module;effective structure learning;semantic dependency dispersed;semantic dependency captured;graph structure;performing graph structure;structure learning specifically;distance semantic dependency;information propagation;grained semantics;obtaining contextual semantic"}, "452059171226626718eb677358836328f884298e": {"ta_keywords": "memory networks natural;question answering facebook;question answering;datasets question answering;modeling speech tagging;sentiment treebank sequence;dynamic memory networks;treebank sequence modeling;memory networks;iterative attention;tasks natural language;ask dynamic memory;iterative attention process;speech tagging;sequence modeling speech;dynamic memory network;natural language processing;question answering qa;networks natural language;model condition attention;attention inputs;introduce dynamic memory;dynamic memory;trigger iterative attention;sentiment treebank;stanford sentiment treebank;treebank sequence;cast question answering;speech tagging wsj;memory network", "pdf_keywords": "question answering sequence;question answering;answering sequence modeling;memory networks natural;question answering tasks;episodic memory module;architecture episodic memory;datasets question answering;memory networks;dynamic memory networks;general question answering;ask dynamic memory;question answering facebook;sequence modeling speech;treebank sequence modeling;episodic memory;episodic memory mod;analysis episodic memory;sentiment treebank sequence;modeling speech tagging;dynamic memory network;classi\ufb01cation question answering;questions forms episodic;recurrent sequence model;propose dynamic memory;architecture variety nlp;model multiple nlp;hierarchical recurrent;episodic memories;answering tasks trained"}, "c39ac49e2d3feec992e84868256cb0a0ff028346": {"ta_keywords": "decentralized distributed convex;distributed convex optimization;distributed convex;advances decentralized distributed;decentralized distributed;algorithms non distributed;non distributed setup;distributed setup lower;convex optimization;distributed setup;distributed;optimal algorithms;optimal algorithms non;lower bounds communications;based optimal algorithms;bounds communications rounds;theoretical advances decentralized;communications rounds oracle;advances decentralized;bounds communications;non distributed;optimal;optimization;convex;rounds oracle calls;explained based optimal;communications rounds;algorithms;algorithms non;decentralized", "pdf_keywords": "stochastic optimization;inequalities recently stochastic;optimization method consensus;optimization variational inequalities;recently stochastic optimization;consensus subroutine time;stochastic optimization method;optimization variational;time varying graphs;variational inequalities recently;consensus subroutine;variational inequalities;stochastic;method consensus subroutine;framework optimization variational;varying graphs requiring;optimization;varying graphs;recently stochastic;model framework optimization;method consensus;graphs requiring n\u00b5\u03b5;\u00b5\u03c7 communications proposed;graphs;optimization method;communications proposed;graphs requiring;\u00b5\u03c7 communications;calls \u00b5\u03c7 communications;subroutine time varying"}, "affb8d759af00540458c19696532220dd1c1373a": {"ta_keywords": "dnn hmm models;recognizing vocabulary oov;word text recognition;models dnn hmm;markov models dnn;hmm models;speech recognition asr;vocabulary oov words;hmm optical model;recognition asr machine;speech recognition lvcsr;text recognition;asr methods ocr;sub word text;recognition asr;ocr recent approaches;speech recognition;dnn hmm optical;recognition lvcsr tasks;challenge recognizing vocabulary;ocr;continuous speech recognition;recognition using asr;hmm models explored;using dnn hmm;vocabulary oov;deep neural network;explored text recognition;oov words;models dnn", "pdf_keywords": ""}, "5c5bedaf66cadebbcd9116f38acd3df9ed43d816": {"ta_keywords": "paul wavelet transform;wavelet transform;used wavelet functions;wavelet functions;optimized paul wavelet;wavelet based;wavelet transform wt;wavelet;wavelet functions flexibility;paul wavelet based;wavelet transform synthetic;use paul wavelet;wavelet based continuous;definition paul wavelet;wavelet real valued;commonly used wavelet;wavelet real;used wavelet;paul wavelet;seismic data;cwt seismic data;seismic time frequency;seismic data help;seismic trace analysis;r\u00e9nyi entropy measurement;time frequency analysis;seismic time;paul wavelet real;entropy measurement;gas accumulation seismic", "pdf_keywords": ""}, "bc247abf8180f583a42de392e4f7d2b2a41ad72d": {"ta_keywords": "parser independent interactive;natural language interfaces;enhancing text sql;questions mean parser;systems text sql;state art parsers;text sql technique;language interfaces databases;datasets wikisql complex;databases systems text;datasets wikisql;parsers;text sql;novel parser independent;novel parser;natural language questions;domain datasets wikisql;parser;arbitrary parsers;work arbitrary parsers;wikisql;users query databases;parsers experiments conducted;query databases using;parsers experiments;wikisql complex;query databases;arbitrary parsers experiments;databases using natural;parser independent", "pdf_keywords": "text sql parser;sql parser;enhancing text sql;parser independent interactive;question generator nl;text sql complex;complex text sql;natural language interfaces;text sql technique;question generator;systems text sql;enhance text sql;tokens question generator;text sql;sql complex text;sql parser user;locator question generator;natural language questions;language interfaces databases;arbitrary parsers;parsers achieve;parsers achieve goal;parsers;help parsers;arbitrary parsers achieve;parser user nl;novel parser independent;work arbitrary parsers;abstract natural language;parser"}, "c5bcb690b0aa85ad0a5fd7e7aa4b8c468cd8c69a": {"ta_keywords": "speech recognition;speech recognition results;new speech recognition;error weighted objective;discriminative training based;discriminative training;mmi objective function;error weighted;framework new speech;mutual information mmi;information mmi objective;approach discriminative training;mpe style loss;mmi objective;differenced mmi dmmi;using differenced mmi;differenced mmi;dmmi mpe statistics;expressed margin based;training based integrals;mmi dmmi;corpus;dmmi compared mpe;difference mmi;weighted objective function;error expressed margin;margin based minimum;weighted objective;forward backward algorithm;world corpus", "pdf_keywords": ""}, "f300a62d0522d9a623b62f1305052928d8d7170c": {"ta_keywords": "dataset irony detection;irony detection propose;sensitive irony detection;structures irony detection;irony detection;irony detection important;emoji sensitive irony;dataset irony;irony detection sunny;cues emojis social;balanced dataset irony;nonverbal cues emojis;emojis social media;cues emojis;structures irony;emojis social;sensitive irony;role structures irony;emojis;day emoji;emoji sensitive;day emoji sensitive;irony;emoji;identification online abuse;sunny day emoji;online abuse harassment;ubiquitous use nonverbal;use nonverbal cues;use nonverbal", "pdf_keywords": ""}, "50851e9e16b52e14c422b6e937cfd3ed063b6998": {"ta_keywords": "multilingual bidirectional encoders;multilingual bidirectional encoder;cross lingual encoders;learning multilingual encoders;lingual transfer learning;multilingual encoders;multilingual encoders amber;lingual encoders;transfer learning nlp;multilingual bidirectional;aligned multilingual bidirectional;learning multilingual;lingual encoders mbert;align multilingual representations;multilingual representations;trained cross lingual;objectives multilingual bidirectional;cross lingual;bidirectional encoder pre;cross lingual transfer;amber aligned multilingual;bidirectional encoder;alignment objectives multilingual;bidirectional encoders;resource languages amber;lingual transfer;high resource languages;encoder pre trained;aligned multilingual;multilingual representations different", "pdf_keywords": "multilingual bidirectional encoders;cross lingual encoders;multilingual bidirectional encoder;learning multilingual encoders;lingual transfer learning;multilingual encoders;aligned multilingual bidirectional;align multilingual representations;multilingual encoders amber;learning multilingual;multilingual bidirectional;lingual encoders;crosslingual transfer performance;multilingual representations;transfer learning nlp;trained cross lingual;cross lingual;cross lingual transfer;alignment objectives multilingual;aligned multilingual;objectives multilingual bidirectional;crosslingual transfer;align multilingual;amber aligned multilingual;lingual encoders mbert;multilingual representations different;lingual transfer;method learning multilingual;objectives align multilingual;bidirectional encoders junjie"}, "37e06f3622c17dc6194b547c944462b2a513b878": {"ta_keywords": "consistency generated summaries;corrections generated summaries;generated abstractive summaries;neural abstractive summarization;generated summaries span;abstractive summarization systems;generated summaries;news summarization performance;summarization systems dominated;summarization performance;summaries span;summarization systems;factual correction models;generating incorrect facts;summaries span selection;factual inconsistency generating;news summarization;generated summaries sacrificing;learned question answering;abstractive summarization;strategies news summarization;summarization performance terms;abstractive summaries;factual consistency generated;question answering models;boost factual consistency;question answering;sacrificing summary quality;summaries sacrificing summary;span fact suite", "pdf_keywords": "improve factuality summaries;factuality summaries generated;consistency generated summaries;improve summary factual;factual correction models;corrections generated summaries;correctors improve summary;generated summaries span;summary factual correctness;factuality summaries;factual correctors improve;generated summaries sacri\ufb01cing;factual consistency generated;generated summaries;models improve factuality;summarization systems huge;summaries span;boost factual consistency;abstractive summarization systems;spanfact suite factual;summarization systems;improve factuality;summaries generated;summaries span selection;summarization aims shorten;suite factual correction;learned question answering;summaries generated state;neural based factual;sacri\ufb01cing summary quality"}, "e487f2508e5f62b2745a2e56ceb3c601c286d2e3": {"ta_keywords": "double elimination tournaments;elimination tournaments;fixing balanced knockout;balanced knockout double;balanced knockout;knockout double elimination;knockout double;tournaments;knockout;fixing balanced;elimination;balanced;double elimination;double;fixing", "pdf_keywords": ""}, "705e6b53f88ec733e3c186c6232c41b268248c01": {"ta_keywords": "social choice model;preferences expressed ratings;like netflix ratings;netflix prize dataset;netflix ratings;behavioral social choice;ratings real raters;computational social choice;choice model dependence;social choice literature;netflix prize;social choice outcomes;ratings rankings;raters discuss behavioral;movie ratings;subcollection netflix prize;inference behavioral modeling;social choice;data like netflix;choice model;behavioral predictions conclusions;movie ratings real;ratings rankings ballots;behavioral predictions;conclusions behavioral predictions;predictions conclusions behavioral;social choice contribute;human preferences;human preferences preferences;expressed ratings rankings", "pdf_keywords": ""}, "740182c3aa9a3045fcd9370269d446455ae9f623": {"ta_keywords": "string transduction models;finite state transducers;state transducer neural;string transduction;finite state transducer;transducer neural finite;state transducers rational;transduction tasks;state transducers;transducers rational relations;neural finite state;state transducer;transducer neural;different transduction tasks;family string transduction;transduction models;state transducers nfsts;transduction models defining;recurrent neural;recurrent neural network;function recurrent neural;transduction tasks compete;transduction;neural finite;alignments probability string;probability string pair;distributions pairs strings;offering interpretable paths;introduce neural finite;interpretable paths", "pdf_keywords": ""}, "ba159dbf205193d0cb7c9c18dd01f830d2f56eb8": {"ta_keywords": "task translating historical;translating historical text;automatic linguistic annotation;linguistic annotation;contemporary natural language;shared task translating;modern text goal;contemporary language improving;language improving automatic;improving automatic linguistic;clin27 shared task;translating historical;text contemporary language;natural language processing;task translating;automatic linguistic;language improving;effect translating historical;language processing tools;text goal improving;tools appl clin27;language processing;text modern text;annotation;modern text;historical text contemporary;text modern;text contemporary;natural language;contemporary language", "pdf_keywords": ""}, "b694472c13420acb599a5b1d25d5f2bd42eb8c1b": {"ta_keywords": "dna sequencing shot;novo dna sequencing;sequencing shot gun;efficiency novo dna;sequencing shot;dna sequencing;novo dna assembly;novo dna;dna sequence reconstructed;shot gun data;gun data underlying;novo assembly algorithm;gun data;dna assembly;sequencing;data efficiency novo;unknown dna sequence;problem novo dna;dna sequence;dna assembly novel;assembly novel algorithmic;sequence propose novo;sequence reconstructed;minimum data efficient;sequence reconstructed short;propose novo assembly;assembly algorithm;assembly algorithm requires;underlying unknown dna;data efficient", "pdf_keywords": ""}, "71d649dcb3dee2ca57d0775a9679cb68f82f22d5": {"ta_keywords": "network speaker adaptation;speaker adaptation;dnn adaptation technique;speaker adaptation similarly;propose dnn adaptation;dnn adaptation;neural network speaker;fmllr adapted dnn;adapted dnn;summarizing neural network;acoustic summary utterance;sequence summarizing neural;vector extractor ssnn;representing acoustic summary;summary vector fbank;summarizing neural;utterance appending vector;ssnn produces summary;network speaker;replaced sequence summarizing;summary utterance appending;neural network ssnn;function summarizing neural;extractor ssnn;vector fbank features;summary utterance;adaptation similarly vector;adaptation technique vector;loss function summarizing;vector fbank", "pdf_keywords": ""}, "0be998fffc5f44496042f7757fb2ffa8924e54cd": {"ta_keywords": "training adaptive scorer;adaptive scorer efficiently;scorer network learnable;adaptive scorer;scorer intuitive reward;updates scorer intuitive;training adaptive;scorer intuitive;trained better scorer;training data efficiently;adapts current learning;scorer efficiently challenging;training data;learning model;training process optimizing;scorer efficiently;learning model potentially;formulate scorer network;dds updates scorer;model trained;current learning;data instance training;instance training adaptive;training data instance;intuitive reward;updates scorer;rewards specifically dds;learnable function training;scorer network;differentiable rewards specifically", "pdf_keywords": "training adaptive scorer;training adaptive;problem training adaptive;formulates scorer learnable;scorer learnable function;scorer learnable;training scorer network;optimizing training;optimizing training data;learnable function training;usage training scorer;training examples update;trained optimizing data;training data ef\ufb01ciently;trained optimizing;training data;adaptive scorer weights;adaptive scorer;usage differentiable rewards;training data usage;model trained optimizing;ef\ufb01ciently training scorer;training scorer;training examples;function current training;current training examples;general reinforcement learning;model trained;function ef\ufb01ciently training;differentiable rewards"}, "c5bb38b8e3ce21063670dfd81ac64dcb2ecf10b2": {"ta_keywords": "frequencies spectral notches;spectral notches hrtfs;compute spectral notches;spectral notches pinna;pinna spectral notches;obtaining spectral notches;spectral notches proposed;overlay spectral notches;spectral notches;spectral notches prominent;notches pinna images;spectral notches high;notches hrtfs using;head related impulse;reconstruct head related;modelling pinna spectral;batteaus reflection model;developing individualized head;head related transfer;reconstruct head;computation frequencies spectral;notches prominent features;use batteaus reflection;important reconstruct head;notches proposed fast;individualized head related;notches hrtfs;individualized head;head related;used compute spectral", "pdf_keywords": ""}, "8abd724b770348bd21b16b9aaf2ba0a77596b2ed": {"ta_keywords": "speaker diarization eend;speech activity detector;external speech activity;speaker diarization;approach speaker diarization;results external speech;end neural diarization;pipeline approach speaker;external speech;neural diarization eend;speech activity;neural diarization;diarization eend;estimated diarization;eend encoder decoder;eda eend encoder;diarization eend methods;eend encoder;estimated diarization results;introduce encoder;decoder based attractor;encoder;encoder decoder;introduce encoder decoder;encoder decoder based;approach speaker;diarization eend method;end end neural;end neural;speaker overlap handling", "pdf_keywords": ""}, "af787fda38ce6fa1d14ad2fb8568088faf973a21": {"ta_keywords": "icon based agreement;evaluating icon based;evaluating icon;method evaluating icon;icon based;agreement level design;icon;based agreement level;agreement level;concept intended mobile;based agreement;intended mobile phone;design concept;design concept intended;agreement;design;mobile phone;intended mobile;mobile;level design concept;evaluating;concept intended;method evaluating;concept;level design;phone;method;based;level;intended", "pdf_keywords": ""}, "be312e930f6739a709e60547aa0dfb9c3dc44497": {"ta_keywords": "multilingual neural machine;multilingual training neural;multilingual lexicon encoding;multilingual neural;neural machine translation;languages multilingual neural;machine translation soft;improvements strong multilingual;data multilingual training;translation soft decoupled;multilingual nmt baselines;strong multilingual nmt;multilingual lexicon;sde multilingual lexicon;encoding sde multilingual;strong multilingual;translation soft;multilingual training;multilingual nmt;data multilingual;machine translation nmt;multilingual;languages multilingual;lexicon encoding;machine translation;resource languages multilingual;sde multilingual;low resource languages;lexicon encoding framework;translation nmt systems", "pdf_keywords": "2019 multilingual neural;multilingual lexicon encoding;multilingual neural machine;multilingual neural;multilingual training neural;neural machine translation;multilingual lexicon representation;translation soft decoupled;multilingual lexicon;sde multilingual lexicon;machine translation soft;lexicon encoding;abstract multilingual training;lexicon encoding framework;iclr 2019 multilingual;encoding sde multilingual;softly decoupling lexical;decoupling lexical semantic;multilingual;multilingual training;2019 multilingual;decoupling lexical;abstract multilingual;soft decoupled encoding;translation soft;languages softly decoupling;machine translation;sde multilingual;share lexicallevel information;lexicon representation"}, "e961c8de1df75f70254656e98ca82f9d9fbd640c": {"ta_keywords": "compressive phase retrieval;algorithms compressive phase;phasecode based sparse;efficient compressive phase;efficient algorithms compressive;compressive phase;fast efficient compressive;phasecode fast efficient;general compressive phase;phase retrieval based;phase retrieval;sparse graph coding;algorithms compressive;efficient compressive;phase retrieval problems;vector phasecode fast;phasecode algorithm provably;phase retrieval problem;sparse graph codes;optimal decoding time;phasecode fast;optimal decoding;row vector phasecode;phasecode algorithm;vector phasecode;phasecode based;decoding time optimal;introduce phasecode;limit optimal decoding;phasecode", "pdf_keywords": "compressive phase retrieval;sparsegraph coding;approaching compressive phase;based sparsegraph coding;compressive phase;sparsegraph coding framework;approaching compressive;measurements compressive phase;phasecode recover random;signal phasecode recover;capacity approaching compressive;compressive;phasecode algorithm recover;phase retrieval algorithm;noiseless case phasecode;sparsegraph;algorithms based sparsegraph;based sparsegraph;recovering signal exactly;phase retrieval;phasecode recover;zero sample complexity;phase retrieval problem;phasecode algorithm;signal phasecode;number measurements compressive;measurements compressive;optimal complexity memory;recovering signal;distribution introduce phasecode"}, "e2a4e1a9f8e66baf12a49a3e5d8e33291f9347e7": {"ta_keywords": "text entity linking;semantic matching short;neural semantic matching;entity linking better;entity linking;aggregated semantic matching;entity linking aims;link reference knowledge;short text entity;semantic matching models;semantic matching;linking;linking better;base aggregated semantic;task entity linking;aggregated semantic;jointly disambiguation rank;matching short text;tweet datasets;reference knowledge base;jointly disambiguation;semantic matching asm;neural semantic;disambiguation rank aggregation;text entity;based neural semantic;text fragments link;linking better utilize;public tweet datasets;reference knowledge", "pdf_keywords": ""}, "f951aad88e244182b37e4918c3d570560108c68c": {"ta_keywords": "adversarially robust;constructing adversarially robust;adversarially robust classifiers;adversarially trained;adversarially;perceptually aligned gradients;adversarially trained neural;constructing adversarially;demonstrated adversarially;demonstrated adversarially trained;means constructing adversarially;2019 demonstrated adversarially;robust classifiers;robust classifiers 2019;property robust classifiers;gradients occur randomized;perceptually aligned;classifiers;aligned gradients general;images uncannily resemble;aligned gradients;classifiers 2019;classifiers 2019 demonstrated;standard convolutional neural;robust;gradients general;gradients;aligned gradients occur;trained neural;optimization produces images", "pdf_keywords": "randomized smoothing neural;smoothing neural;smoothed neural networks;smoothing neural network;smoothed neural;neural network smoothed;perceptually aligned gradients;original randomized smoothing;randomized smoothing;perceptual quality generated;paper smoothed neural;aligned gradients perceptually;gradients perceptually aligned;classi\ufb01ers randomized smoothing;gaussian data augmentation;gradients perceptually;data augmentation smoothadv;network smoothed convolution;perceptual quality;augmentation smoothadv;randomized smoothing paper;smoothed convolution gaussian;quality generated images;smoothadv original randomized;training gaussian;smoothed convolution;augmentation smoothadv original;proposed training gaussian;trained using gaussian;effect perceptual quality"}, "5eaa425af39339e0ae30202b348cc6e253813993": {"ta_keywords": "scientific information retrieval;russian patent databases;experts relevant documents;information retrieval;information retrieval designed;expert evaluation grant;retrieval technology analysis;citation indexes developed;information retrieval technology;patent databases citation;retrieval designed russian;corpus information retrieval;technological forecasting systems;databases citation indexes;citation indexes;patent databases;technological forecasting;technology analysis forecasting;patents research;english russian patent;expert evaluation;retrieval designed;expertise process;analyze research industry;grade technological forecasting;patents research papers;russian patent;indexes developed;text corpus information;uses patents research", "pdf_keywords": ""}, "0761a69310f7b8f4ab01495f31a30c6fe53d83b8": {"ta_keywords": "adaptive discriminative speech;adaptation acoustic model;multiscale adaptation;incremental adaptation;multiscale properties speech;language model adaptive;incremental adaptations multiscale;time incremental adaptations;original incremental adaptation;formulation incremental adaptation;multiscale adaptation potential;adaptations multiscale adaptation;method adaptation acoustic;continuous speech recognition;adaptation acoustic;incremental adaptation assumes;discriminative speech modeling;incremental adaptation scheme;speech modeling;incremental adaptations;model adaptive discriminative;acoustic model language;speech recognition;multiple time scales;adaptive discriminative;speech recognition experiments;single time incremental;single time scale;model adaptive;properties speech recognition", "pdf_keywords": ""}, "225767ce707781d0114815068c355622869ee642": {"ta_keywords": "markov model audio;cognitive systems asynchronous;cognition living systems;asynchronous hidden markov;artificial cognitive systems;hidden markov;artificial cognitive;hidden markov model;cognitive systems;visual speech recognition;speech recognition;audio visual speech;understanding cognition living;visual speech;cognition living;progress understanding cognition;understanding cognition;construction artificial cognitive;cognition;markov;systems asynchronous hidden;markov model;living systems;cognitive;living systems new;model audio visual;recognition;audio visual;systems asynchronous;computer science neuroscience", "pdf_keywords": ""}, "91184a2d40be8a0171b5c926b336666ed717ec6e": {"ta_keywords": "issues peer review;digital library analyses;peer review;peer review biases;library analyses;grants peer review;peer review insightful;scientific digital library;peer review entire;issues peer;library analyses library;research grants peer;systemic issues peer;digital library;grants peer;analyses library;review biases;researchers;scientific digital;review biases fraud;peer;analyses library tutorial;insightful experiments computational;experiments computational;billions dollars research;researchers detailed;library;research;resulting scientific digital;researchers detailed writeup", "pdf_keywords": ""}, "7d148b46f45e935765e56887d720492b2b716e55": {"ta_keywords": "mapping constructed spiking;constructed spiking patterns;revealing physical connectivity;collective dynamics generate;spiking patterns;neurons coupling;spiking patterns model;reconstruction networks biology;event space mapping;neurons coupling acts;pair neurons coupling;constructed spiking;networks biology;synapses pair neurons;intrinsic collective dynamics;physical connectivity;neural circuits reveal;pair neurons;collective dynamics;spiking;model neural circuits;timings event space;networked systems event;neurons;event intervals reveal;networks biology social;event space;neural circuits;reconstruction networks;synapses", "pdf_keywords": "synaptic connectivity model;synaptic connectivity;revealed synaptic connectivity;conductance based synapses;connectivity based neuron;synapses pair neurons;spiking patterns model;neurons coupling;spiking patterns;theory revealed synaptic;constructed spiking patterns;revealing physical connectivity;neurons coupling acts;synapses;absence synapses;based synapses;synapses ii excitatory;pair neurons coupling;irregular spiking patterns;synapses iii instantaneous;physical connectivity;synapses pair;absence synapses pair;neural circuits reveal;based synapses ii;mapping constructed spiking;model neural circuits;synapses iii;neuron;collective dynamics generate"}, "d3dd80269f2542cc173afb3a1df24b582a1e4af2": {"ta_keywords": "languages struggle transformers;languages parity;parity language bit;attention transformers remarkably;language bit strings;using languages parity;languages parity language;1s language bit;parity language;attention transformers;regular languages struggle;self attention transformers;generalization machine translation;language bit;improves length generalization;limitation using languages;looking regular languages;generalize longer strings;transformer recognizes parity;recognizes parity;regular languages;machine translation;length generalization machine;bit strings;approaching bit string;number 1s language;symbol transformer classification;1s language;languages struggle;strings offer simple", "pdf_keywords": "parity language bit;recognizes parity;languages parity;transformer recognizes parity;language bit strings;parity language;using languages parity;languages parity language;1s language bit;bit strings odd;bit strings;recognizes parity perfect;parity perfect accuracy;parity;limitation suggested hahn;language bit;entropy models arbitrarily;bit strings starting;parity perfect;limitation using languages;bring cross entropy;1s language;strings odd number;entropy models;entropy;cross entropy;number 1s language;strings odd;constructing transformer recognizes;cross entropy models"}, "920257774e2caee8a8c74968c64c10bcb79a136c": {"ta_keywords": "function transmission lines;genetic algorithm vector;propagation function transmission;transmission lines;genetic algorithm;hybrid genetic algorithm;improved genetic algorithm;transmission lines hybrid;genetic algorithm hybrid;algorithm hybrid genetic;lines hybrid genetic;algorithm vector fitting;approximation propagation function;proposed approximation propagation;method improved genetic;vector fitting based;propagation function;approach approximation propagation;fitting based approach;vector fitting;function transmission;approximation propagation;guessing number poles;poles adjusting corresponding;delay times conventional;corresponding delay times;propagation;adjusting corresponding delay;number poles adjusting;corresponding delay", "pdf_keywords": ""}, "147b954ba0881d643706c918e017f7d66a15b827": {"ta_keywords": "cognitive model discovery;discover cognitive models;build cognitive model;finds cognitive models;cognitive models fit;algebra learning agent;cognitive models using;cognitive model predict;cognitive model human;efficient cognitive model;cognitive models discovered;better cognitive model;cognitive models;measured cognitive models;model human learning;cognitive model;learning agent;learning agent simstudent;discover cognitive;matically discover cognitive;cognitive models depending;quality cognitive models;learning agent simstu;learning agent better;model human experts;predict human learning;human learning curve;build cognitive;approach finds cognitive;agent better cognitive", "pdf_keywords": ""}, "3426f000673aae995a55ade9273c842bb484ad18": {"ta_keywords": "multilingual phoneme recognizers;phoneme recognizers monolingual;crosslingual phoneme recognizers;detection phoneme boundaries;automatic detection phoneme;detection phoneme;phoneme recognizers;phoneme boundaries audio;phoneme identities detection;detect boundaries phonemes;detected segments phoneme;phoneme recognizers interested;phonemes classify;phonemes classify detected;recordings unknown language;boundaries phonemes classify;multilingual phoneme;language detect boundaries;segments phoneme;monoand multilingual phoneme;phoneme boundaries detected;phoneme boundaries;recognizers interested phoneme;language detect;identities detection phoneme;detected phoneme identities;boundaries detected phoneme;phoneme boundaries finally;detected phoneme;multilingual crosslingual phoneme", "pdf_keywords": ""}, "89c2cbdf1a5049a4068ca9215aa8859a1a97b1a3": {"ta_keywords": "contextual bandit learning;algorithm contextual bandit;practical contextual bandit;algorithm contextual bandits;bandit learning algorithm;contextual bandit;contextual bandits method;bandit learning;contextual bandits;bandit learning problem;bandits method;bandit;bandits method assumes;statistically optimal regret;bandits;optimal regret;optimal regret guarantee;context observes reward;reward action;observes reward action;problem learner repeatedly;reward action taming;observes reward;learner repeatedly takes;learning problem;achieves statistically optimal;learning algorithm approaches;algorithm contextual;simple algorithm contextual;learner repeatedly", "pdf_keywords": "algorithm contextual bandits;contextual bandits alekh;contextual bandits;statistically optimal regret;optimal regret guarantee;bandits alekh;bandits;bandits alekh agarwal1;optimal regret;achieves statistically optimal;nearly optimal certain;nearly optimal;prove nearly optimal;statistically optimal;class optimization;certain class optimization;optimal;optimal certain;convex program used;class optimization based;oracle high probability;optimal certain class;prediction performance;oracle calls rounds;computational prediction performance;convex program;similar convex program;cost sensitive classi\ufb01cation;online variant algorithm;regret guarantee"}, "89f7db77a755d44d3aabdbcc7549b743d7debcc5": {"ta_keywords": "conditional preference networks;preference networks;preference networks cp;generalization conditional preference;conditional preference;uncertainty preference statements;express uncertainty preference;preference statements;uncertainty preference;preferences features;preferences features particular;statements preferences;statements preferences set;expression preferences features;conditional statements cp;qualitative conditional statements;cp statements preferences;nets incorporates uncertainty;preferences set objects;qualitative conditional;objects expression preferences;conditional statements;generalization conditional;expression preferences;model qualitative conditional;cp nets formal;express uncertainty;incorporates uncertainty inherently;conditional;nets formal", "pdf_keywords": ""}, "18a82459d495fa3ad22a60bd7c9527df8bd55e1e": {"ta_keywords": "distributed regret learning;distributed regret;distributed optimization;propose distributed regret;regret analysis distributed;distributed learning network;distributed learning algorithm;distributed learning;regret learning algorithm;analysis distributed learning;learning network games;algorithm network games;network games dual;games dual averaging;appears distributed optimization;distributed optimization setup;actions distributed learning;regret bound analysis;network games;regret learning;dual averaging approach;dual averaging;rate convergence network;log regret bound;method dual averaging;dual averaging locally;learning network;convergence network connectivity;network games using;regret bound", "pdf_keywords": ""}, "341f6353547f4a58fdf11fbcc9de3a31083a619b": {"ta_keywords": "imaging lesions tooth;lesions tooth surfaces;lesions tooth;swir imaging lesions;tooth surfaces;imaging lesions;swir imaging;lesions;tooth;imaging;swir;surfaces", "pdf_keywords": ""}, "7a79099447bef9a3ea13b1dc409d04b3dff57320": {"ta_keywords": "automatic music composition;sequence midi like;sequence midi;generative modeling music;score sequence midi;midi like events;revamped midi;modeling music;generating music;modeling music symbolic;task automatic music;remi revamped midi;generating music rhythm;automatic music;composing music;music composition;revamped midi derived;midi derived events;midi;midi like;music plausible rhythmic;context modeling rhythmic;music composition entails;piano music plausible;composing music long;transformer generating music;composes pop piano;modeling rhythmic;pop piano music;music transformer generating", "pdf_keywords": "automatic music composition;composition expressive piano;pop piano compositions;piano music pop;music composition;expressive pop piano;expressive piano music;piano compositions;pop piano music;music composition using;piano music;music information retrieval;remi novel midi;novel midi;music transformer composing;midi;proposed automatic music;expressive piano;pop piano;novel midi derived;human knowledge music;midi derived;automatic music;piano music validated;piano compositions arxiv;midi derived event;music validated objective;piano;components music information;music information"}, "e9d8db4f5b5c106c43a268f635788c0a94b2916a": {"ta_keywords": "stochastic gradient descent;descent ascent methods;distributed methods compression;gradient descent ascent;distributed variants compression;descent ascent sgda;descent ascent unified;ascent methods far;reduction coordinate randomization;gradient descent;randomization distributed variants;ascent methods;ascent sgda;variants compression extensively;sgda prominent algorithms;sampling variance reduction;new distributed methods;variants compression;stochastic gradient;tasks stochastic gradient;optimization variational;method coordinate randomization;max optimization variational;descent ascent;methods compression;new efficient methods;ascent sgda prominent;randomization sega sgda;variance reduction;reduced method svrgda", "pdf_keywords": "distributed methods compression;method coordinate randomization;variance reduced method;new distributed methods;randomization sega sgda;distributed methods;methods compression qsgda;methods compression;coordinate randomization;reduced method svrgda;randomization;sgda new variance;coordinate randomization sega;sgda new method;variance reduced;sgda vr;variants sgda;new variants sgda;compression qsgda;reduced method;method svrgda;new variance reduced;compression qsgda diana;variants sgda new;svrgda new distributed;sgda;randomization sega;sgda new;sgda vr diana;compression"}, "b9913ddf94245c864509f0b94847bdbe77899b46": {"ta_keywords": "tonal transcription language;phonetic phonemic facts;phonetic phonemic;highlighting phonetic phonemic;modelling phonemes tones;context tonal prediction;phonemic tonal transcription;phonemic context tonal;jointly modelling phonemes;phonemes tones;tonal languages yongning;modelling phonemes;phonemes tones versus;tonal languages;phonemic facts linguistic;experiments tonal languages;tonal prediction;phonetic;signal highlighting phonetic;transcription language;tonal transcription;highlighting phonetic;phonemic tonal;connectionist temporal classification;temporal classification loss;transcription language documentation;phonemes;function phonemic tonal;context tonal;languages yongning na", "pdf_keywords": ""}, "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a": {"ta_keywords": "words transformers image;transformers image recognition;convolutional networks used;image classification tasks;cnns;cnns necessary;classification tasks transformer;convolutional networks;cnns necessary pure;components convolutional networks;scale reliance cnns;16x16 words transformers;words transformers;recognition scale;convolutional networks keeping;vision attention;image recognition scale;recognition scale reliance;reliance cnns;image classification;reliance cnns necessary;recognition;vision attention applied;convolutional;perform image classification;transformers image;image recognition;limited vision attention;components convolutional;attention", "pdf_keywords": "transformers image recognition;recognition benchmarks;image recognition benchmarks;recognition benchmarks train;cnns necessary;imagenet;image classi\ufb01cation tasks;transformer encoder;cnns;cnns necessary pure;imagenet 21k dataset;image classi\ufb01cation supervised;trained public imagenet;transformer encoder used;image recognition;nlp pre trained;reliance cnns necessary;inference backprop speed;reliance cnns;transformers image;inference speed;multiple image recognition;standard transformer encoder;recognition;encoder;application transformers image;model image classi\ufb01cation;inference speed main;image recognition purpose;image sequence patches"}, "4533fd4cf13d2f4dd105edaf612934a1bd85ad5a": {"ta_keywords": "multi channel electroencephalogram;channel electroencephalogram;channel electroencephalogram addressed;electroencephalogram addressed observed;electroencephalogram addressed;electroencephalogram;observed signal separated;noise removal singletrial;prior information covariance;separated multiple signals;method noise removal;noise removal;spatial correlation prior;frequency dependent covariance;signal separated multiple;separated signal;amplitude separated signal;signal separated;correlation prior;potentials recorded multi;addressed observed signal;event related potentials;posteriori estimation spatial;related potentials recorded;estimation spatial correlation;information covariance;multiple signals;observed signal;event related potential;separated signal time", "pdf_keywords": ""}, "b62430b9f8810da4d9f28842ac0ca899aa66d422": {"ta_keywords": "\u5e83\u6771\u7701dinghushan\u56fd\u7acb\u81ea\u7136\u4fdd\u8b77\u533a\u5916\u6765\u4fb5\u5165\u690d\u7269\u8abf\u67fb jst \u4eac\u5927\u6a5f\u68b0\u7ffb\u8a33;\u5e83\u6771\u7701dinghushan\u56fd\u7acb\u81ea\u7136\u4fdd\u8b77\u533a\u5916\u6765\u4fb5\u5165\u690d\u7269\u8abf\u67fb jst;\u5e83\u6771\u7701dinghushan\u56fd\u7acb\u81ea\u7136\u4fdd\u8b77\u533a\u5916\u6765\u4fb5\u5165\u690d\u7269\u8abf\u67fb;jst \u4eac\u5927\u6a5f\u68b0\u7ffb\u8a33;\u4eac\u5927\u6a5f\u68b0\u7ffb\u8a33;jst", "pdf_keywords": ""}, "d1c41eb99824e8f4752190da1b815378be23b4b9": {"ta_keywords": "catastrophic forgetting;catastrophic forgetting significantly;mitigating catastrophic forgetting;sequence tasks autoregressive;alleviates catastrophic forgetting;catastrophic forgetting scheduled;forgetting scheduled sampling;form catastrophic forgetting;neural machine translation;autoregressive models trained;forgetting significantly;forgetting significantly improves;machine translation despite;forgetting;translation tasks demonstrate;translation tasks;sequence tasks;14 translation tasks;machine translation;tasks autoregressive models;sequenceto sequence tasks;consolidation neural machine;forgetting scheduled;prefixes used training;tasks autoregressive;models trained;consolidation neural;scheduled sampling elastic;weight consolidation neural;model generated prefixes", "pdf_keywords": "sequence tasks autoregressive;catastrophic forgetting;autoregressive models trained;mitigating catastrophic forgetting;catastrophic forgetting scheduled;forgetting scheduled sampling;tasks autoregressive models;trained scheduled sampling;bias retaining;models trained scheduled;neural machine translation;tasks autoregressive;sequence tasks;models trained;bias retaining output;forgetting;process models trained;input sequence model;strong performance sequenceto;sequence model generated;bias maintaining output;sequenceto sequence tasks;bias maintaining;models trained maximum;scheduled sampling elastic;autoregressive models;retaining output quality;consolidation neural machine;performance sequenceto sequence;sequence model"}, "175b58fe7e49bb5c0c771b73f8834bcff21b59c7": {"ta_keywords": "natural language inference;language inference nli;language inference evaluation;evaluation natural language;sentence encoder models;natural language hypothesis;determining natural language;language inference;inference nli task;evaluation sentence encoder;sentence encoder;inference nli;natural language;encoder models stress;challenging linguistic;models stress tests;datasets nli achieving;automatically constructed stress;challenging linguistic phenomena;datasets nli;nli task determining;nli task;test evaluation natural;constructed stress tests;stress test evaluation;language hypothesis inferred;linguistic;linguistic phenomena suggests;standard datasets nli;stress tests reveals", "pdf_keywords": "language inference nli;nli tests robustness;natural language inference;tests robustness nli;natural language hypothesis;stress test spelling;language inference;sentence encoder models;inference nli task;test spelling errors;inference nli;challenging linguistic;evaluation natural language;tests nli \ufb01rst;robustness nli models;test spelling;sentence encoder model;determining natural language;linguistic phenomena evaluation;sentence encoder;tests nli tests;challenging linguistic phenomena;models speci\ufb01c linguistic;language inference aakanksha;stress tests nli;evaluation sentence encoder;nli tests;tests nli;nli models;robustness nli"}, "49edf7f0dbad8b8c101af9ef95c72f62f545591e": {"ta_keywords": "compact topic embeddings;correlated topic modeling;captures topic correlations;learns compact topic;topic vectors correlated;topic embeddings;topic correlations;linear topic size;topic correlations closeness;closeness topic vectors;topic modeling;topic vectors;topic embeddings captures;embeddings captures topic;topic modeling limited;vectors correlated topic;topic size;correlated topic;topic size paper;compact topic;low dimensional embedding;correlations closeness topic;document classification retrieval;captures topic;document classification;complexity linear topic;inference low dimensional;dimensional embedding;larger existing correlation;embeddings", "pdf_keywords": "topic vectors embedding;correlated topic model;correlated topics pubmed;topic model induces;topic vectors;topic model;representations latent topics;correlated topics;latent topics characterizes;topic occurrence method;closeness topic vectors;100k correlated topics;latent topics;latent topics sharing;topic occurrence;structures latent topics;sparsity topic occurrence;topics pubmed speedup;topics characterizes correlations;variational inference fast;linear topic size;pubmed speedup variational;speedup variational inference;embedding based correlation;correlated topic;topics characterizes;new correlated topic;topic size extensive;topics pubmed;complexity topic size"}, "db79a3e55690c5c86cfd0ec97712ed4ad1e47b3b": {"ta_keywords": "ranking pairwise comparisons;analyze sequential ranking;sequential ranking;comparisons active ranking;sequential ranking algorithm;sequential active ranking;pairwise ranking based;pairwise ranking;ranking pairwise;recovering ranking;active ranking pairwise;recovering ranking using;ranking algorithm counts;succeeds recovering ranking;ranking algorithm;active ranking;ranking based parametric;ranking;ranking based;ranking set items;ranking using;work pairwise ranking;ranking using number;active ranking set;noisy pairwise comparisons;ranking set;pairwise comparisons active;pairwise comparisons;comparisons optimal;comparisons active", "pdf_keywords": ""}, "3c37b9ec2ff1828877575acc600b73c3bcde138f": {"ta_keywords": "bandits policy recommender;multi armed bandits;multi armed bandit;bandits policy;bandit setup;bandits;recommender in\ufb02uences rewards;armed bandits policy;bandit setup captures;bandit;armed bandits;armed bandit setup;armed bandit;recommendations naive approaches;reward proba bility;optimal traditionally recommender;in\ufb02uences rewards;reward proba;recommendations naive;algorithm achieves regret;corresponds unknown reward;unknown reward proba;policy recommender in\ufb02uences;policy dependent horizons;ef\ufb01cient learning algorithm;policy recommender;regret number users;unknown reward;ef\ufb01cient learning;reward", "pdf_keywords": "bandit setup;multi armed bandit;bandit setup captures;bandit;ef\ufb01cient optimal planning;armed bandit setup;optimal planning;\ufb01nding optimal policy;optimal policy \ufb01nite;optimal policy;ef\ufb01cient optimal;armed bandit;optimal planning algorithm;\ufb01nding optimal;policy \ufb01nite horizon;additive approximation run;afninaalglloy ef\ufb01cient optimal;planning algorithm multiple;planning algorithm;problem \ufb01nding optimal;captures policydependent horizons;optimal;planning;policydependent horizons;algorithm multiple user;policydependent horizons particularly;approximation run;2o additive approximation;additive approximation;approximation run time"}, "f6f4d30e4740bd92b31acd297a15872d490e7f11": {"ta_keywords": "supervised learning ssl;semi supervised learning;modeling semi supervised;classification tasks ssl;learning ssl algorithms;ssl heuristics;constraints semi supervised;individual ssl heuristics;ssl heuristics including;semi supervised;ssl heuristics multiple;tasks ssl heuristics;domain specific heuristics;supervised classification;multiple heuristics automatically;heuristics automatically combined;supervised learning;heuristics automatically;supervised classification tasks;traditional supervised classification;bayesian optimization;heuristics multiple heuristics;known heuristics training;known heuristics;using bayesian optimization;modeling traditional supervised;using graphs classifiers;including known heuristics;classification tasks;bayesian optimization methods", "pdf_keywords": "classi\ufb01cation relation extraction;relation extraction tasks;relation extraction;relation extraction modest;modeling ssl constraints;results relation extraction;learners declarative ssl;tasks ssl heuristics;ssl heuristics;modeling ssl;ssl heuristics including;classi\ufb01cation tasks ssl;individual ssl heuristics;ssl constraints speci\ufb01cally;ssl framework learner;representing individual ssl;declarative ssl framework;ssl constraints;approach modeling ssl;declarative ssl;supervised learners declarative;text classi\ufb01cation relation;ssl heuristics multiple;semi supervised learners;link based classi\ufb01cation;supervised classi\ufb01cation tasks;specifying semi supervised;based text classi\ufb01cation;declarative language modeling;language modeling"}, "dab261b25ff8ccd2c9144a5cb3a46b39ac0ac4bd": {"ta_keywords": "compromising ml intellectual;ai broadly scientific;ml intellectual foundations;chastised ai community;ml intellectual;machine learning owes;mcdermott chastised ai;ai broadly;research compromising ml;ai community;ai community abandoning;history ai broadly;chastised ai;ai;rigorous research;intellectual foundations 1976;history ai;intellectual foundations;broadly scientific research;intellectual;flawed scholarship threatens;learning;cyclically history ai;criticize;machine learning;flawed scholarship;rigorous research date;scientific research;compromising ml;learning owes large", "pdf_keywords": "machine learning scholarship;misuse machine learning;learning ml researchers;machine learning suggestive;ml researchers;learning ml;algorithms misuse language;machine learning ml;language misuse;highlevel semantic reasoning;learning suggestive de\ufb01nitions;semantic reasoning actually;highlevel semantic;learning suggestive;semantic reasoning;understanding empirically rigorous;reader choosing terminology;collectively machine learning;terminology avoid;suggestive de\ufb01nitions overloaded;de\ufb01nitions overloaded terminology;obtain understanding empirically;choosing terminology avoid;low level syntactic;intuition aid reader;language misuse machine;learnable obtain understanding;misuse language;driven algorithms misuse;language empower reader"}, "b79dcc5304e557ce200b161d2a884c0ff77f34ec": {"ta_keywords": "spelling correction systems;toolkit spelling correction;isolated misspellings;existing spelling correction;misspellings ii use;engineering isolated misspellings;misspellings multiple sources;misspelt token training;occurring misspellings;misspellings multiple;naturally occurring misspellings;isolated misspellings ii;models using spelling;source toolkit spelling;occurring misspellings multiple;misspellings;misspellings ii;toolkit spelling;representations boosts correction;misspelt token;correction english systems;spelling correction;context misspelt token;using spelling errors;existing spelling;spelling errors context;spelling correction english;neuspell open source;context misspelt;spelling errors", "pdf_keywords": "neural spell correctors;spelling correction toolkit;toolkit spelling correction;neural spelling correction;training data spelling;systems using misspellings;data spelling correction;toolkit spelling;source toolkit spelling;neuspell neural spelling;character level adversarial;spell correctors;adversarial attacks supplement;correction toolkit;using misspellings context;using misspellings;train neural spell;spelling correction context;neural spelling;neuspell neuspell neural;neuspell open source;misspellings context using;neural spell;level adversarial;neuspell neural;misspellings context;com neuspell neuspell;level adversarial attacks;spell correctors \ufb01rst;correction toolkit sai"}, "9b621c0bcd2029006b389bc51395fb6604f9a855": {"ta_keywords": "question generation;captioner requiring supervised;human answerers;questions game synthetic;synthetic human answerers;question generation question;questions improve communicative;goal natural language;questions game;dialogue model;captioner;image captioner requiring;generation question examples;captioner requiring;misunderstandings dialogue model;supervised question answer;clarification question generation;question asking model;question answer data;20 questions game;question examples propose;image captioner;dialogue model uses;questions improve;informative questions shelf;pose questions;natural language;question examples;dialogue;pose questions improve", "pdf_keywords": "clari\ufb01cation question generation;pretrained image captioner;question generation;human answerers;model image captioning;generating clari\ufb01cation questions;synthetic human answerers;clari\ufb01cation questions visually;questions visually;questions visually grounded;captioning;image captioning data;captioner;image captioning;goal natural language;question driven communication;captioning data;clari\ufb01cation dialogue stivers;image captioner;focus computational challenge;question driven;challenge generating clari\ufb01cation;question generation question;captioning data deriving;natural language;question selection belief;clari\ufb01cation dialogue;human machine interactions;natural language processing;question selection"}, "a84c319fef32b2514af9541576189a1735aac507": {"ta_keywords": "knowledge base completion;knowledge base;base completion searching;solution knowledge base;web deriving language;base completion;completion searching web;completion searching;deriving language pattern;searching web deriving;language independent solution;deriving language;language pattern;independent solution knowledge;searching web;solution knowledge;web deriving;language independent;knowledge;completion;searching;language;base;web;pattern;deriving;solution;independent solution;independent", "pdf_keywords": ""}, "f2de2c9d83b9058695e3272a4fbb7c30e04a1476": {"ta_keywords": "learning word second;second language learning;second language predictors;l2 word learning;language predictors l2;investigating second language;ease learning word;language duolingo mobile;learn second language;word learning accuracy;learning english second;word second language;words harder learn;users learning english;language predictors;learning word;english semantic density;language using large;predictors l2 word;second language duolingo;language duolingo;language learning;word learning;second language;l2 word;duolingo mobile;second language using;english second language;duolingo mobile app;learning accuracy big", "pdf_keywords": "l2 word learning;learning word second;word learning accuracy;predictors l2 word;word learning regression;ease learning word;word second language;word learning large;accurate word learning;learning word;english semantic density;l2 word;language duolingo mobile;word learning;learning english second;users learning english;predict accurate word;language duolingo;second language;second language duolingo;app predictors l2;distributional models lexical;examining word level;english second language;effects cross linguistic;status word learning;models lexical;factors l2 word;cross linguistic semantic;models lexical semantics"}, "5b6c1e9dddc4b55036a5629227ae2cc7d49eb6d0": {"ta_keywords": "literature image finder;structured literature image;relevance feedback images;images papers;latent topic models;feedback images papers;topic models;literature image;image finder;images papers classified;topic models model;interactive relevance;findings interactive relevance;text image;biomedical literature;biomedical literature make;text image processing;interactive relevance feedback;combination text image;model papers;vastness biomedical literature;structured literature;image finder tackles;images;latent topic;relevance feedback;slif project;model papers multiple;feedback images;models model papers", "pdf_keywords": ""}, "a1babdf55a6bff96d533fd0c9bc44864283ec107": {"ta_keywords": "governance banks;depositors aware governance;governance banks purpose;depositors risk aversion;depositors risk;aware governance banks;aversion consider banks;chairperson depositors aware;bank respect corporate;corporate governance variables;chairperson depositors;shows depositors risk;president chairperson depositors;consider banks managed;banks managed;depositors opening account;banks managed based;bank president chairperson;particular bank respect;depositors aware;corporate governance;banks;consider banks;corporate governance standpoint;depositors;banks purpose;bank president existence;account particular bank;bank president;stockholders corporation stakeholders", "pdf_keywords": ""}, "3af5e203368fa2c7959d035493571d181a8682af": {"ta_keywords": "dataset multimodal music;multimodal music analysis;music performance dataset;multimodal music;music information retrieval;visual analysis music;music audio datasets;music analysis;musical score midi;music analysis challenges;analysis music performances;performance dataset multimodal;score midi;audio visual analysis;score midi format;music performance;recordings individual tracks;individual tracks audio;midi;performances individual tracks;audio datasets;multitrack classical music;separately recorded performances;music information;analysis music;dataset facilitating audio;existing music information;multimodal tasks dataset;evaluation existing music;dataset multimodal", "pdf_keywords": "multimodal music performance;music performance analysis;dataset multimodal music;music performance dataset;music performances benchmark;modal music analysis;multimodal music;visual analysis music;audio visual analysis;music analysis;music analysis challenges;analysis music performances;multi pitch analysis;music audio datasets;multi modal music;audio datasets conclusion;music performance;score alignment music;multi modal mir;pitch analysis;audio visual;separation music transcription;audio score alignment;alignment music performance;modal mir tasks;mir tasks visually;analysis music;benchmark existing mir;modal music;informed multi pitch"}, "8f43b63ca400a0ea1fdd272f8c83fd67f01d0182": {"ta_keywords": "gene mention tagging;tagging crfs parsing;gene mention task;fields syntactic parsing;mention tagging using;mention tagging;random fields syntactic;crfs syntactic parsing;tagging using conditional;syntactic parsing;mention tagging crfs;parsing tackling biocreative;tagging using;tagging;approach gene mention;syntactic parsing taking;conditional random fields;parsing;tagging crfs;gene mention;fields crfs syntactic;crfs parsing;biocreative gene mention;fields syntactic;crfs parsing tackling;parsing taking advantage;crfs syntactic;syntactic parsing did;parsing did use;parsing taking", "pdf_keywords": ""}, "14a09a04c5c295a93ff25492516112cd86fa0114": {"ta_keywords": "decoders semi supervised;transduction semi supervised;labeled sequence transduction;semi supervised labeled;variational encoder decoders;supervised labeled sequence;space variational encoder;variational encoder;semi supervised;encoder decoders semi;semi supervised learning;sequence transduction semi;encoder;sequence transduction;encoder decoders;sequence transduction task;model labeled sequence;supervised labeled;decoders semi;data labeled sequence;labeled sequence;provides powerful supervised;encoder decoders new;decoders;transduction semi;supervised;unlabeled data labeled;powerful supervised;decoders new model;supervised learning", "pdf_keywords": "transduction semi supervised;decoders semi supervised;labeled sequence transduction;sequence transduction tasks;supervised labeled sequence;space variational encoderdecoders;semi supervised;sequence transduction semi;model labeled sequence;semi supervised labeled;space variational encoder;variational encoderdecoders;sequence transduction task;variational encoder decoders;variational encoderdecoders new;variational encoder;semi supervised learning;sequence transduction chunting;framework labeled sequence;sequence transduction;encoder decoders semi;sequence transduction problems;encoderdecoders;decoders msved generative;labeled sequence;abstract labeled sequence;encoderdecoders new model;transduction tasks;decoders semi;encoder"}, "5e63e47cb3386b032ec43a92ce5980466228c761": {"ta_keywords": "coded distributed storage;distributed storage;erasure coded distributed;distributed storage systems;storage code;storage systems study;data recovery erasure;new storage code;storage;storage systems;erasure coded data;storage optimal;cluster storing;storage efficiency require;reliably storing;provide optimal storage;optimal storage;storage efficiency;data recovery;optimal storage efficiency;storage optimal supporting;challenges data recovery;erasure codes;storage code using;recovery erasure coded;present new storage;erasure coded;new storage;recovery operations erasure;day cluster storing", "pdf_keywords": "coded distributed storage;distributed file hdfs;hdfs underway solution;distributed storage systems;distributed storage;proposed code hadoop;erasure coded distributed;hdfs implementation proposed;code employed hdfs;hdfs;code hadoop distributed;storage systems study;employed hdfs implementation;storage code;hdfs implementation;hadoop distributed file;code hadoop;file hdfs underway;new storage code;storage systems;hadoop distributed;data recovery erasure;hdfs underway;file hdfs;erasure coded data;employing traditional erasurecodes;erasure codes;storage code using;traditional erasurecodes;traditional erasurecodes rs"}, "af44f5db5b4396e1670cda07eff5ad84145ba843": {"ta_keywords": "question answering;question answering paragraphs;factoid question answering;modeling textual compositionality;neural network factoid;modeling textual;textual compositionality neural;question answering typically;learns word phrase;tasks like factoid;sentences reason entities;textual compositionality;answering paragraphs;input modeling textual;text classification;phrase level representations;trivia competition called;answering paragraphs unlike;competition called quiz;textual;like factoid;representations combine sentences;learns word;called quiz;reason input modeling;questions trivia competition;like factoid question;words representations;qanta learns word;text classification methods", "pdf_keywords": ""}, "1bd43c91ecbf46098ef2b521c5367e849819960e": {"ta_keywords": "neural machine translation;monolingual data neural;machine translation;translation improve model;conducting translation improve;monolingual data translate;machine translation nmt;data translate crucial;translation nmt iteratively;translation improve;iteratively conducting translation;domain adaptation low;utilize monolingual data;monolingual data;text selecting monolingual;domain adaptation;conducting translation;translation proven effective;selecting monolingual data;utilize monolingual;data translate;translate crucial;selecting monolingual;models domain adaptation;translation nmt;method utilize monolingual;translate crucial require;quality sentence improvement;general domain text;monolingual", "pdf_keywords": "neural machine translation;iterative translation baseline;weighting iterative translation;competitive iterative translation;strong iterative translation;monolingual data neural;machine translation;translation baseline;translation baseline bleu;translation improve model;iterative translation;conducting translation improve;machine translation nmt;domain adaptation low;domain adaptation;translation nmt iteratively;baselines introduction translation;utilize monolingual data;iteratively conducting translation;translation improve;models domain adaptation;monolingual data;iterative translation zi;methods domain adaptation;translation proven effective;improving performance neural;using monolingual data;english datasets strategies;utilize monolingual;monolingual data delivering"}, "16f0c508aa54e26aa18e3b0f3c91b0c143c6a605": {"ta_keywords": "representation disparity minority;disparity minority groups;disparity minority;loss fairness demographics;higher loss fairness;loss fairness;improvements minority;fairness demographics;speech recognizers;fails improvements minority;risk minority;improvements minority group;risk minority group;minority groups;fairness demographics repeated;speech recognizers usually;minority group user;amplifies representation disparity;fair models unfair;learning models speech;representation disparity;results representation disparity;disparity amplification examples;minority group;prevents disparity amplification;minority;fair models;minimization erm amplifies;minority group time;models unfair", "pdf_keywords": "representation disparity minority;disparity ampli\ufb01cation unfairness;disparity minority groups;disparity minority;ampli\ufb01cation unfairness;ampli\ufb01cation unfairness paper;fairness demographics;incurred minority;prevents disparity ampli\ufb01cation;improvements minority group;fairness demographics repeated;incurred minority groups;improvements minority;fails improvements minority;minimization distributive justice;erm results disparity;minority group user;disparity ampli\ufb01cation examples;fair models unfair;ampli\ufb01es representation disparity;minority groups;minority groups performs;results disparity ampli\ufb01cation;unfairness;representation disparity;fair models;results representation disparity;risk incurred minority;speech recognizers;disparity ampli\ufb01cation"}, "5c159745fce2b87e8b00307b76f0948b9fa8b1d7": {"ta_keywords": "translation translation tasks;language model rnnlm;translation tasks;translation tasks resulting;rnnlm allows fluent;translation input;syntactic parser forest;syntactic analyses translated;workshop asian translation;f2s translation input;translated target language;rescoring using recurrent;recurrent neural network;model rnnlm allows;model rnnlm;parser forest;language model;string f2s translation;f2s translation;neural network language;language pairs targeted;evaluation language pairs;forest possible syntactic;parser forest possible;translation input sentence;syntactic parser;using recurrent neural;translation translation;rnnlm allows;language pairs", "pdf_keywords": ""}, "7a1a202e268ccc910e8044be556e56aa9eb5a94f": {"ta_keywords": "automated dependence plots;model automated dependence;usefulness automated dependence;dependence plots;automated dependence;dependence plots demonstrate;dependence plots adp;dependence;model selection bias;selection bias detection;exploring latent space;plots demonstrate usefulness;exploring latent;bias detection;including model selection;latent space generative;generative model;model automated;generative model address;space generative model;behavior exploring latent;applications machine learning;machine learning;machine learning necessary;model selection;feature space latent;demonstrate usefulness automated;generative;usefulness automated;arising generative model", "pdf_keywords": ""}, "c674ce6454d69a87f00797f3ec90d1b38b451063": {"ta_keywords": "distributed convex optimization;method distributed convex;dual stochastic gradient;distributed convex;stochastic gradient oracle;primal dual stochastic;convex optimization;stochastic gradient;convex optimization problems;dual stochastic;method rate convergence;gradient oracle method;method distributed;oracle method distributed;optimization problems networks;gradient oracle;convergence terms duality;bound distance iteration;duality gap probability;convex;distributed;iteration sequence optimal;rate convergence;introduce primal dual;optimization;distance iteration sequence;primal dual;batch size guarantee;optimal point;distance iteration", "pdf_keywords": "distributed convex optimization;distributed stochastic convex;method distributed convex;stochastic convex optimization;dual approach distributed;dual stochastic gradient;approach distributed stochastic;distributed convex;distributed stochastic;primal dual stochastic;stochastic gradient oracle;stochastic convex;dual stochastic;convex optimization networks;stochastic gradient;convex optimization;convex optimization problems;primal dual approach;approach distributed;method distributed;oracle method distributed;method rate convergence;optimization networks darina;gradient oracle method;optimization problems networks;optimization networks;bound distance iteration;dual approach;convergence terms duality;gradient oracle"}, "2aecdd190066a57db8fea1e1143dc5fc288050e0": {"ta_keywords": "utility privacy tradeoff;quantifying utility privacy;iot privacy consumers;collected iot privacy;utility privacy;quantifying tradeoff iot;iot privacy;iot promises;privacy tradeoff internet;iot promises advantages;privacy tradeoff;things iot promises;tradeoff smart grid;tradeoff iot;privacy metric assumes;privacy metric;smart grid;collected iot;data collected iot;proof concept privacy;smart grid operations;privacy consumers;privacy;tradeoff internet things;tradeoff iot focus;concept privacy metric;smart grid application;concept privacy;iot;adversary model provides", "pdf_keywords": ""}, "242cf2e991f0eed4b1309a2a9dff548e8b95900f": {"ta_keywords": "beamforming methods speech;robust speech recognition;speech recognition;learning based beamforming;benchmarking speech recognition;methods speech recognition;speech recognition study;chime benchmarking speech;benchmarking speech;speech recognition task;robust speech;based beamforming methods;designed robust speech;predicts beamforming weights;beamforming methods specifically;beamforming methods;predicts beamforming;beamforming weights generalized;estimation beamforming weights;likelihood estimation beamforming;based beamforming;beamforming;estimation beamforming;beamforming weights;trained gaussian mixture;response beamforming weights;response beamforming;network predicts beamforming;methods speech;beamforming weights maximum", "pdf_keywords": ""}, "b3bd90f630b2d19856ef031b3dddfcb9b041b243": {"ta_keywords": "strategies learning solving;learns cognitive skills;learning solving problems;learns cognitive;compare learning strategies;tutor learning generalizing;cognitive skills examples;learning strategies;learning strategies learning;program learns cognitive;strategies learning;tutored problem solving;hints tutor learning;outperformed learning strategies;tutor learning;learning solving;learning program learns;learning tutored problem;solving outperformed learning;learning tutored;examples skills;showed learning tutored;cognitive skills;study advantage tutored;results showed learning;learns;advantage tutored;learning generalizing worked;examples exhaustively learning;skills examples conducted", "pdf_keywords": ""}, "dd3770b2dbc9668578fefdc078d37457ba9c0b9a": {"ta_keywords": "electrolaryngeal speech enhancement;el speech enhancement;speech enhancement electrolarynx;speech enhancement;speech enhancement evaluation;approach electrolaryngeal speech;electrolaryngeal speech;electrolaryngeal el speech;voiced prediction hybrid;sounds enable laryngectomees;excitation prediction electrolaryngeal;voice conversion;parameters voice conversion;voice conversion method;unvoiced voiced prediction;voiced prediction;removing micro prosody;noise reduction;produce el speech;enable laryngectomees;parameters voice;enable laryngectomees produce;prediction electrolaryngeal el;excitation sounds enable;enhancement evaluation excitation;laryngectomees produce el;avoiding unvoiced voiced;noise reduction method;prediction hybrid enhancement;excitation feature prediction", "pdf_keywords": ""}, "44cabe32482d4b622d9ca00bf23b3ee7950e2710": {"ta_keywords": "human ml judgments;human ml predictive;human machine decision;ml predictive decisions;formalizing human ml;human ml complementarity;ml predictive decision;models human ml;human ml based;ml judgments;ml judgments informed;machine learning human;machine decision making;context human ml;predictive decisions aggregated;combining human ml;machine decision;criteria human machine;ml better predictive;refer human ml;predictive decisions;combine human ml;ml complementarity proposed;ml predictive;predictive decision;ml complementarity;better predictive decision;predictive decision making;human ml;ml based predictive", "pdf_keywords": "human machine decision;human machine decisions;human machine predictions;formalization human ml;human ml predictions;human ml complementarity;machine decision;human ml predictive;machine decisions;machine decision making;machine predictions unifying;ml complementarity formalize;machine decisions various;ml predictive decision;machine decisions focus;models human ml;aggregation human machine;human ml based;predictions unifying optimization;optimal aggregation human;predictive decision;machine predictions;predictive decisions;ml complementarity;combining human ml;formalization human;formally characterizes optimal;predictive decision making;ml predictions;context human ml"}, "3ac59132297f4e50d5e83852555392f9ff05d8b4": {"ta_keywords": "decentralized distributed optimization;distributed optimization;composite optimization applications;composite optimization;optimization applications decentralized;method composite optimization;derivative free method;optimization applications;decentralized distributed;free method composite;optimization;applications decentralized distributed;method composite;derivative free;distributed;composite;applications decentralized;free method;decentralized;derivative;free;method;applications", "pdf_keywords": "convex composite optimization;gradient sliding method;composite optimization;order sliding algorithm;sliding algorithm zosa;solving convex composite;convex composite;oracles composite optimization;sliding algorithm;convex composite problem;analysis gradient sliding;zeroth order sliding;sliding method;composite optimization problem;based sliding algorithm;method based sliding;2019 convex composite;zosa solving convex;gradient sliding;order sliding;sliding algorithm lan;stochastic zeroth order;order oracle smooth;order oracle nonsmooth;sliding method lan;solving convex;order oracles composite;2016 2019 convex;zeroth order oracle;algorithm zosa"}, "ff3b83ef0a153ed376556057269f3a61da3a103a": {"ta_keywords": "multitrack music assuming;parts voices instruments;instruments notes solo;multitrack music;symbolic multitrack music;automatic instrumentation learning;solo music arrangements;genres ensembles;genres ensembles bach;music assuming mixture;notes solo music;learning separate parts;voices instruments tracks;voices instruments;assistive composing tools;different genres ensembles;multiple instruments;play multiple instruments;separate parts voices;instrumentation learning;instrumentation learning separate;assigning instruments notes;ensembles bach chorales;instruments tracks mixture;solo music;solo music performance;parts voices;instruments notes;instruments;instruments tracks", "pdf_keywords": "separation multitrack music;voice separation model;voice separation;music dataset multilayer;multitrack music;multitrack music examined;music datasets;music dataset;separating mixture parts;separation sequential multi;music datasets possibly;game music datasets;datasets possibly violins;adapt voice separation;algorithm pop music;pop music dataset;task separation multitrack;separation sequential;separation multitrack;task separation sequential;string quartets dataset;arrangement separating mixture;separating mixture;quartets dataset;new task separation;separating;notes sequences labels;separation;quartets game music;separation model"}, "cb90d5ea3a95b4c6ec904f622f51d752f506636e": {"ta_keywords": "siamese networks metric;metric learning structured;cpmetric deep siamese;networks metric learning;siamese networks;cpmetric deep;metric cp nets;metric learning;metric learning value;learning structured preferences;learning value alignment;preferences ai;deep siamese networks;ai systems preferences;metric distance preferences;structured preferences;learn approximation metric;nets propose neural;metric cpmetric;preferences preferences ai;paper metric learning;deep siamese;structured preferences preferences;metric cpmetric using;preferences ethical systems;recommendations consistent preferences;networks metric;orderings actions ai;proposed metric cp;metric cp", "pdf_keywords": ""}, "d4f5f1a196e203226e4a69d52a04d46823f32fb3": {"ta_keywords": "crawled monolingual corpora;parallel corpus;parallel corpus using;centric parallel corpus;parallel corpora additionally;available parallel corpora;parallel sentences web;parallel corpora collection;parallel corpora;quality parallel sentences;sentence pairs web;million sentence pairs;crawled monolingual;web crawled monolingual;corpora tools;multilingual nmt models;monolingual corpora document;monolingual corpora;documents multilingual representation;corpora additionally;multilingual representation models;extracting sentences scanned;large collection sentences;english centric parallel;corpora tools methods;corpus using english;combining corpora tools;sentences scanned;web combining corpora;corpus", "pdf_keywords": "parallel corpus mining;multilingual models translation;crawled monolingual corpora;advances multilingual representation;multilingual representation models;parallel corpus;explore multilingual models;recent advances multilingual;multilingual models;parallel sentences leveraging;multilingual representation learning;indic languages parallel;languages parallel sentences;documents multilingual representation;multilingual models spanning;indic language translation;parallel sentences web;multilingual representation;crawled monolingual;demonstrating parallel corpus;multilingual single script;scanned documents multilingual;web crawled monolingual;advances multilingual;monolingual corpora;monolingual corpora document;documents multilingual;multilingual;explore multilingual;languages parallel"}, "d301054c2819e1a21480800fdabbe5ae909abe09": {"ta_keywords": "synthesis inferring programs;program synthesis;effective program synthesis;inductive program synthesis;program synthesis inferring;libraries neurally guided;program synthesis depends;libraries neurally;learning libraries neurally;synthesis inductive program;abstraction program search;library learning dreamcoder;laps language abstraction;learning dreamcoder laps;inferring programs examples;language abstraction program;abstract reasoning scenes;program search;programs examples desired;neurally guided search;scenes natural language;build programs;functions build programs;inferring programs;learning dreamcoder;models synthesis;language abstraction;composition abstract reasoning;programs examples;search models synthesis", "pdf_keywords": "learned program synthesis;language learned synthesis;generative models programs;hierarchical program induction;models program learning;abstraction program search;program synthesis;learning libraries neurally;libraries neurally guided;learned synthesis;learning hierarchical program;dataset synthesis tasks;generalizability learned program;libraries neurally;learned synthesis work;program search model;learning generalizable library;search models synthesis;introducing jointly generative;language abstraction program;natural language programs;programs language learned;program learning;models programs language;reusable program abstractions;language abstraction;synthesis tasks;learning library conditional;program induction model;laps language abstraction"}, "27b7489bd54dfd585edd2ba0da3920a31e7fd8b5": {"ta_keywords": "continuous sensorimotor games;sensorimotor games;sensorimotor games arise;game interacting machine;machine behavior beliefs;human sensorimotor learning;games theoretically experimentally;interacting machine humans;humans machines predictions;learning behaviors humans;model interaction game;human sensorimotor;agents humans machines;game interacting;economic game;economic game theory;sensorimotor learning;games arise humans;learning using teleoperation;interacting agents humans;game theory results;state learning behaviors;continuous sensorimotor;observations human sensorimotor;learning behaviors;functional humans machines;game theory;beliefs machine behavior;interaction game interacting;sensorimotor learning using", "pdf_keywords": ""}, "af38829cdb55ee7b71d49399f71397d975e40a95": {"ta_keywords": "conditional answers dataset;answers dataset conditionalqa;answering qa dataset;knowing answers dataset;question answering;questions conditional answers;question answering qa;answers dataset features;answering complex questions;answers dataset;conditional answers;contains complex questions;selecting answer conditions;answering qa;answerable questions questions;research answering complex;questions conditional;conditional answers addition;answerable questions;conditionalqa challenging existing;addition conditional answers;answers answerable questions;answers addition conditional;research answering;questions long documents;answer conditions answers;long context documents;answering complex;answering;conditionalqa conditionalqa challenging", "pdf_keywords": "conditional answers dataset;conditionalqa contains questions;questions conditional answers;answer conditions dataset;answers conditions questions;conditional answers;contains questions conditional;answers conditions;questions conditional;selecting answer conditions;answering complex questions;conditionalqa challenging existing;challenging dataset conditionalqa;answers dataset features;answers dataset;dataset conditionalqa challenging;conditionalqa challenging;contains questions;extractive questions;conditional answers addition;answerable questions questions;challenging existing qa;compositional logical reasoning;answer conditions;addition conditional answers;conditionalqa dataset;reasoning combination extractive;answers addition conditional;correct answers conditions;combination extractive questions"}, "2c2234548de4694b6455a19cd0d85a9d6c473456": {"ta_keywords": "library similarity searching;similarity searching;similarity searching fewer;metric spaces tools;approximate search methods;approximate search;library similarity;search methods spaces;metric space access;measure search quality;tools measure search;describes library similarity;metric spaces methods;spaces methods approximate;spaces tools measure;new search methods;search methods;spaces tools;search quality;metric spaces document;methods non metric;measure search;non metric spaces;searching fewer exact;non metric metric;similarity;search methods non;spaces methods;non metric;art approximate search", "pdf_keywords": "similarity search library;metric space searching;similarity search methods;searching generic metric;metric space library;crossplatform similarity search;similarity search;comprehensive toolkit searching;toolkit evaluation similarity;evaluation similarity search;space library nmslib;metric spaces fastmap;space searching;toolkit searching;history non metric;search library toolkit;non metric;metric non metric;library nmslib;crossplatform similarity;extendable crossplatform similarity;library nmslib e\ufb03cient;metric non;spaces fastmap fast;traditional multimedia datasets;metric;support non metric;non metric spaces;generic metric;search library"}, "c00ba15810496669d47d2ed5b627e6c7d2b1f6aa": {"ta_keywords": "unsupervised multi lingual;multi document paraphrasing;texts languages conditioning;document paraphrasing;document paraphrasing objective;masked language modeling;reconstruction target text;jointly learn retrieval;learn retrieval reconstruction;language modeling;paraphrasing;paraphrasing objective;lingual multi document;discriminative generative tasks;summarization information retrieval;multi document summarization;generative tasks languages;pre trained sequence;sequence model learned;multi lingual;generative tasks;texts languages;document summarization;captures aspects paraphrase;translation multi document;language modeling paradigm;trained sequence sequence;learn retrieval;multi lingual multi;related texts languages", "pdf_keywords": "multilingual autoencoder retrieves;language understanding generation;multilingual autoencoder;language modeling provide;marge multilingual autoencoder;language modeling;models natural language;masked language modeling;generative tasks languages;reconstruction target text;autoencoder retrieves generates;autoencoder retrieves;texts languages conditioning;generative tasks;autoencoder;discriminative generative tasks;generation using retrieved;pre trained models;pre training models;sequence model separately;source sequence sequence;multi source sequence;language understanding;related texts languages;natural language understanding;source sequence;retrieved document decodes;document decodes target;sequence sequence model;texts languages"}, "4b890b6ded71f005414e55adb87c23efd437ef95": {"ta_keywords": "speech synthesis quality;synthetic speech quality;parametric speech synthesis;speech synthesis synthetic;synthesis synthetic speech;speech synthesis;improvements synthetic speech;gv generated speech;speech synthesis offers;concatenative speech synthesis;speech synthesis including;synthetic speech;generated speech parameter;voice conversion;ms synthetic speech;tts voice conversion;generated speech;statistical parametric speech;voice conversion vc;synthetic speech close;speech quality proposed;speech tts voice;text speech tts;modify ms utterance;utterance level postfilter;speech parameter;generation algorithm hmm;parametric speech;speech quality;quality natural speech", "pdf_keywords": ""}, "1006d191e9eb5b4dbc35fc0bb389328ddc75cba7": {"ta_keywords": "byte level models;text preprocessing pipelines;token free models;longer token sequences;raw text bytes;tokens corresponding word;byte character sequences;trained language models;process text language;text bytes characters;language models;prone text preprocessing;text bytes;language models operate;text language;token sequences;pre trained language;spelling pronunciation byte;byte sequences;raw text characterize;directly raw text;process byte sequences;bytes characters;byte character;pronunciation byte;character sequences longer;text preprocessing;inference speed byte;pronunciation byte character;sequences longer token", "pdf_keywords": "word level tasks;byte level models;generative tasks multilingual;multilingual tasks;bytelevel models;nlp pipeline;nlp pipeline doing;demonstrate bytelevel models;byte level encoder;text preprocessing tokenization;tasks multilingual tasks;multilingual tasks language;multilingual t5;tasks multilingual;tasks sensitive spelling;word level;variant multilingual t5;preprocessing tokenization;bytelevel models signi\ufb01cantly;tokenization;multilingual t5 xue;free variant multilingual;simpli\ufb01es nlp pipeline;generative tasks;preprocessing tokenization paper;counterparts demonstrate bytelevel;vocabulary building;vocabulary building text;multilingual;decoder models"}, "041b2510e54b2504890cb9f58b9bbc5601f35e3e": {"ta_keywords": "level nlp course;nlp course designed;graduate level nlp;incrementally assignment assignments;nlp course;assignments designed interactive;level research nlp;assignments;level nlp;assignment assignments;incrementally assignment;research nlp;assignments build;assignments build incrementally;assignments graduate level;assignment assignments goal;nlp end course;research nlp end;course designed assignments;assignments designed;assignments goal;designed assignments;set assignments graduate;assignments graduate;build incrementally assignment;set assignments;designed assignments build;approximate search training;assignment;easily gradable students", "pdf_keywords": ""}, "399ab2a0eddf7a7abf776241d5c0a2c4cd5bf313": {"ta_keywords": "discriminative model speech;model speech recognition;estimate speech model;acoustic language model;speech signal searches;model speech;speech model represented;parameters speech model;estimate speech;speech model;speech recognition;speech model composed;speech recognition directly;speech acoustic lexicon;discriminative training approaches;optimizes parameters speech;discriminative training;parameters speech;better performance discriminative;language models;parameters acoustic language;performance discriminative training;speech acoustic;recognition decoder;performance discriminative;discriminative model;linear distributions decoding;acoustic lexicon language;network unlike discriminative;finite state transducer", "pdf_keywords": ""}, "4b9bc5b5985bbfbc860039b98f233f8a43ad9171": {"ta_keywords": "pde especially uncertainties;examples pdes engineering;pdes engineering;pdes engineering physics;numerical partial differential;equations pde especially;pde captured features;examples pdes;neural network parameterise;quantities derived pde;pdes;especially uncertainties modelled;numerical partial;pde especially;partial differential equations;coefficients variability physical;physics curse dimensionality;notable examples pdes;pde;neural network;input coefficients variability;modelled equations random;dimensionality commonly;using neural network;dimensionality commonly encountered;using neural;uncertainties modelled equations;uncertainties modelled;equations random coefficients;random coefficients based", "pdf_keywords": "layers relu nonlinearity;convolutional layers;network convolutional layers;inhomogeneous potential numerical;network convolutional;neural network;neural network section;neural network parameterizing;convolutional neural;convolutional neural network;elliptic equation nlse;convolutional layers relu;simple convolutional neural;solving pdes dimensions;potential numerical;relu nonlinearity use;relu nonlinearity;\ufb01rst architecture neural;pdes dimensions;inhomogeneous potential;odinger equation nlse;architecture neural;main network convolutional;architecture neural network;state energy nonlinear;nonlinear schr;energy nonlinear schr;neural;solving pdes;energy nonlinear"}, "302face5b5a0944cab13665a2d4e07ef3aaf5240": {"ta_keywords": "domain question answering;question answering task;question answering;question answering especially;answering ambiguous open;answering task involves;answering especially exploring;answering task;answering;ambigqa answering;answering especially;open domain qa;ambigqa answering ambiguous;domain qa benchmark;questions nq open;answer paired disambiguated;question answer pairs;answering ambiguous;weakly supervised;ambigqa new open;benefit weakly supervised;exploring new topics;domain qa;questions single unambiguous;incorporates nq open;questions present strong;weakly supervised learning;inherent open domain;open domain questions;questions nq", "pdf_keywords": "question answering sequence;question answering task;question answering;domain question answering;question disambiguation model;based question answering;answering task involves;nq open dataset;answering task;model question disambiguation;answering;answering sequence;disambiguation model;baseline models ambigqa;strong baseline models;answering sequence tosequence;questions nq open;question disambiguation;weakly supervised;answers open;learning incorporates nq;disambiguation model modi\ufb01cation;incorporates nq open;weakly supervised learning;partial supervision available;open dataset present;task construct ambignq;bene\ufb01t weakly supervised;supervision available nq;open dataset"}, "d86227948b6000e5d7ed63cf2054ad600b7994a0": {"ta_keywords": "sentiment analysis factoid;models natural language;learning compositionality inputs;simple deep neural;syntactically aware models;natural language processing;learning compositionality;outperforms models sentiment;models sentiment;question answering tasks;deep neural;question answering;computations deep;syntactically aware;models sentiment analysis;text classification;deep learning;deep unordered composition;sentiment analysis;deep learning models;text classification model;computations deep unordered;existing deep learning;simple deep;neural;factoid question answering;deep neural network;focus learning compositionality;composition rivals syntactic;expensive computations deep", "pdf_keywords": ""}, "8c4b187bdaf91bf068adfe005a0463c4f9c36387": {"ta_keywords": "deep architectures continued;deep architectures;powerful deep architectures;pixel wise losses;pixel wise loss;structures advent deep;deep learning;delineation refine predictions;iterative refinement boost;finding powerful deep;topological features linear;advent deep learning;deep learning current;trained binary;wise losses binary;topological features;classifier trained binary;features linear structures;trained binary cross;deep;habitual pixel wise;refine predictions;losses binary;loss term aware;predicted delineations;quality predicted delineations;iterative refinement;losses binary cross;refine predictions step;habitual pixel", "pdf_keywords": "neural network road;cracks natural images;network road segmentation;road segmentation 64;crack detection;road segmentation;recent detection segmentation;images neuronal;delineating neuronal boundaries;segmentation;segmentation methods 16;31 crack detection;natural images neuronal;roads cracks;including roads cracks;roads cracks natural;pixel labeling;segmentation 64;crack detection method;image patches net;based segmentation subsequent;segmentation methods;detection segmentation;segmentation subsequent;approach delineating neuronal;13 neural;pixel labeling using;images neuronal membranes;delineating neuronal;18 pixel labeling"}, "8147a495b9a933742f06458244f7c5df00767c4e": {"ta_keywords": "open information extraction;sentences open information;improving open information;open confidence modeling;supervised open systems;extracted assertions extraction;assertions extraction likelihood;information extraction iterative;assertions extraction;ranking extractions;extracting open;modeling ranking extractions;information extraction;recall extracted assertions;sentences open;extractions generated open;ranking extractions based;extracted different sentences;supervised open;extraction likelihood confidence;task extracting open;information extraction task;open information;extracted assertions;assertions natural language;open domain assertions;quality assertions extracted;assertions extracted;extracting open domain;extraction task extracting", "pdf_keywords": "open information extraction;open neural;neural models open;open neural network;formulation open neural;open domain assertions;assertions natural language;model assertion generation;sentences neural models;assertion generation;improving open information;extracting open;model rerank extractions;information extraction iterative;rerank extractions base;assertion generation generate;task extracting open;rerank extractions;rank aware learning;sentences neural;abstract open information;language sentences neural;extracting open domain;information extraction;improving open;extraction iterative rank;extractions base model;information extraction task;extraction task extracting;domain assertions natural"}, "43e8e371449aaef34c2f43ae90f2157fd5a617bd": {"ta_keywords": "pricing socially optimal;selfish agents pricing;socially optimal feedback;socially optimal cost;pricing socially;control nash equilibrium;agents pricing coordination;pricing mechanisms;pricing coordination;pricing coordination dynamic;feedback control nash;fairness weighted pricing;incurred pricing socially;use pricing mechanisms;nash equilibrium agents;nash equilibrium;control strategy selfish;pricing mechanisms means;socially optimal;coordination dynamic games;strategy selfish agents;agents pricing;equilibrium agents convex;control nash;selfish agents;achieve socially optimal;pricing reducing;weighted pricing;optimal feedback control;weighted pricing reducing", "pdf_keywords": ""}, "830a396a4a77567caad1c155dd3b22597314e9f3": {"ta_keywords": "algorithmic fairness institutional;algorithmic fairness;algorithmic fairness poor;fairness machine learning;fairness metric algorithmic;model algorithmic fairness;metric algorithmic fairness;fairness machine;fairness aware systems;profit fairness machine;outcomes fairness aware;fairness metric;based predefined fairness;fairness aware;predefined fairness;predefined fairness metric;fairness institutional logics;fairness institutional;non profit fairness;outcomes fairness;computational social choice;perspectives outcomes fairness;fairness poor;fairness;profit fairness;computational social;propose computational social;algorithmic;social technical systems;fairness poor match", "pdf_keywords": ""}, "5083d9e25113a09faeba7d56b7808e2f77b5c15e": {"ta_keywords": "hop templates neural;multi hop reasoning;symbolic knowledge base;large symbolic knowledge;hop reasoning using;multi hop templates;hop templates;hop reasoning present;hop reasoning;symbolic knowledge;knowledge base;templates neural;reasoning present efficient;using large symbolic;templates neural model;knowledge base kb;multi hop;hop;order multi hop;requiring multi hop;implementations second order;reasoning using large;large symbolic;construct second order;compositionally construct;compositionally construct second;alternative implementations;learning tasks requiring;reasoning using;used compositionally construct", "pdf_keywords": "hop templates neural;templates neural;usable neural component;trade offs neural;deep learning end;propose usable neural;usable neural;time deep learning;templates neural model;lstm;deep learning;learning end end;mixing lstm;learning end;time memory;neural;neural component;reasoning large symbolic;neural component used;rule learning;architectures scalable;memory;hop templates;architectures scalable implementation;neural ilp;memory appropriate implementation;multi hop templates;memory trade offs;offs neural ilp;time memory appropriate"}, "4b762c0344f14bb00d590f5666c27b3aac7b0a7d": {"ta_keywords": "models sentence completion;sentence completion challenge;sentence completion paper;neural language models;sentence completion;rnn language model;dependency recurrent neural;research sentence completion;challenge dependency rnn;language models sentence;recurrent neural language;model incorporating syntactic;language models;language model incorporating;dependency recurrent;recurrent neural network;completion challenge dependency;performance recurrent neural;dependency rnn;improve performance recurrent;incorporating syntactic dependencies;language model;rnn language;dependency rnn proposed;language modelling;performance recurrent;dependencies sentence effect;recurrent neural;network rnn language;syntactic dependencies sentence", "pdf_keywords": "syntactic dependencies rnn;neural language models;dependency rnn language;rnn language model;rnn incorporates syntactic;challenge dependency rnn;neural language model;model dependency rnn;dependencies rnn formulation;dependencies rnn;dependency rnn;dependency rnn proposed;recurrent neural network;rnn language;dependency rnn incorporates;results dependency rnn;language model learns;performance recurrent neural;network rnn language;learns recurrent neural;incorporating syntactic dependencies;model incorporating syntactic;neural network rnn;neural language;improve performance recurrent;learns recurrent;recurrent neural;sentence completion challenge;novel neural language;language models"}, "bcd45c86e1bcf8d1411eb6704c4c58d0831b5b4f": {"ta_keywords": "based multinomial distribution;multinomial distribution wide;words higher frequencies;models based multinomial;based multinomial;multinomial distribution;classification tasks;statistical models text;text treat words;frequencies occurrence sensible;classification;multinomial;frequencies occurrence;higher frequencies occurrence;statistical models;occurrence;models text;binomial distributions desirable;models based poisson;treat words higher;models text treat;words higher;occurrence sensible manner;present statistical models;binomial distributions;occurrence sensible;classification tasks classes;negative binomial distributions;distributions desirable;text", "pdf_keywords": ""}, "0ad8284dbae11901a725cc71318a165c08852278": {"ta_keywords": "approach voice conversion;model voice conversion;voice conversion;voice conversion addition;voice conversion using;voice conversion studies;integrate speaker gmm;utterances speaker model;content spoken speakers;speaker model voice;model parallel utterances;approach integrate speaker;density model speaker;integrate speaker;model voice;speaker gmm target;parallel utterances speaker;speaker gmm;speaker model proposed;gaussian mixture model;utterances speaker;spoken speakers;model speaker model;spoken speakers paper;model speaker;based gaussian mixture;gmm probabilistic densities;source target speakers;voice;gaussian mixture", "pdf_keywords": ""}, "b21b927c251c415b601b6d7f785a42cc5c292635": {"ta_keywords": "scientific information extraction;scientific knowledge graph;coreference clusters scientific;annotations tasks;relations coreference clusters;clusters scientific articles;includes annotations tasks;information scientific literature;coreference clusters;annotations tasks develop;sciie shared span;models scientific information;scientific literature;annotations;includes annotations;scientific articles;scierc dataset includes;scientific information;scierc dataset;information extraction;create scierc dataset;scientific literature introduce;scientific knowledge;dataset includes annotations;knowledge graph;sciie shared;relations coreference;entities relations coreference;construction scientific knowledge;information extraction using", "pdf_keywords": "relations coreference clusters;relations coreference resolution;entity relation extraction;coreference clusters scienti\ufb01c;coreference resolution sentences;coreference clusters;relation extraction;entities relations coreference;relations coreference;classify scienti\ufb01c entities;coreference resolution;relation extraction using;scienti\ufb01c entities relations;identifying entities relations;annotations tasks;information extractor sciie;scienti\ufb01c information extractor;includes annotations tasks;scienti\ufb01c entities;coreference;annotations tasks develop;information extractor;identifying entities;sciie shared span;better predicting span;scienti\ufb01c systems entity;shared span representations;entities relations;model identifying entities;includes annotations"}, "34fb3e21a63fb2987f7a87f88ecf49aea53cff36": {"ta_keywords": "cooperative persuasive dialogue;persuasive dialogue policies;learning cooperative persuasive;persuasive dialogue;cooperative persuasive;dialogue policies;dialogue policies using;policies using framing;persuasive;dialogue;learning cooperative;framing;using framing;cooperative;policies;policies using;learning;using", "pdf_keywords": ""}, "95cedaeb3178a4671703a05171a144e6b964a819": {"ta_keywords": "neural language models;distributions language models;language models;count based neural;weights distributions language;language models lms;neural language;based neural language;probabilities sequences words;probability distributions vocabulary;distributions vocabulary words;vocabulary words dynamically;words dynamically calculates;words dynamically;distributions vocabulary;based neural lms;distributions language;neural lms;mixture weights distributions;hybridizing count based;count based;generalizing hybridizing count;neural lms experiments;sequences words discrete;features count based;models lms statistical;hybridizing count;dynamically calculates mixture;lms statistical models;words discrete", "pdf_keywords": "neural language models;language models;language models lms;count based neural;language modeling;introduction language models;language modeling exist;abstract language models;neural language;based neural language;paradigms language modeling;language models graham;based gram models;based neural lms;gram models;neural lms;distributions vocabulary words;count based gram;gram models advantages;neural lms achieve;probabilities sequences words;neural lms experiments;vocabulary words dynamically;words dynamically calculates;models lms statistical;gram lms;lms statistical models;probability distributions vocabulary;generalizing hybridizing count;hybridizing count based"}, "a13d8400813743adb22ba0bd0570c49af2675a39": {"ta_keywords": "recording speech separation;continuous speech separation;speech separation;separation continuous speech;task speech separation;speech separation continuous;speech separation aiming;speech separation css;rnn long recording;block level separation;long recording speech;sentence level separation;separation interleaved intra;blocks perform separation;separation interleaved;level separation interleaved;separation performance;segment long recording;rnn dprnn architecture;perform separation independently;recording speech;partially overlapped recording;overlapped recording simple;overlapped recording;separation independently;perform separation;separating overlap free;long recording;separation;level separation models", "pdf_keywords": ""}, "54316d2861eb3d575a8c7d071f4cf7c2fc30be01": {"ta_keywords": "reconstruction adversarial performance;reconstruction adversarial;image reconstruction adversarial;adversarial performance end;adversarial performance;robustification pre trained;adversarial;trained robust;removing adversarial;adversarial manifold;trained trained robust;sense removing adversarial;removing adversarial manifold;adversarial attacks specifically;adversarial attacks;smoothing pre trained;certifiably robust classifiers;fidelity image reconstruction;adversarial manifold input;susceptible adversarial attacks;empirical robustification;susceptible adversarial;smoothing efficient inference;robust classifiers pre;empirical robustification pre;accuracy denoised smoothing;robustify pre trained;robust classifiers;denoised smoothing efficient;optimize fidelity image", "pdf_keywords": ""}, "73bbd0b53044e9f518a3596a3607521bbce12fc2": {"ta_keywords": "regularized segmentation losses;descent regularized segmentation;segmentation losses;segmentation losses work;regularized segmentation;gradient descent regularized;weakly supervised cnn;simplicity gradient descent;poorly gradient descent;shallow segmentation known;models shallow segmentation;supervised cnn segmentation;cnn segmentation;descent regularized;shallow segmentation;regularization models shallow;gradient descent;cnn segmentation demonstrate;training deeper;segmentation known global;optimization simplicity gradient;method training deeper;training deeper complex;context weakly supervised;optimization methods loss;supervised cnn;gradient descent gd;cnn;regularization;loss functions architectures", "pdf_keywords": ""}, "a5b1d1cab073cb746a990b37d42dc7b67763f881": {"ta_keywords": "learns representations nl;nl understanding tasks;natural language nl;nl questions structured;like semantic parsing;semantic parsing structured;lm jointly learns;representations nl sentences;language nl understanding;pretrained language models;semantic parsing;tables tabert pretraining;language models lms;models lms text;lms text based;textual tabular data;questions structured tabular;tabert pretrained lm;structured tables models;parsing structured;nl understanding;joint understanding textual;tasks like semantic;language nl;semi structured tables;form nl questions;jointly learns;nl text;nl sentences;parsing structured data", "pdf_keywords": "data semantic parsers;semantic parsing;semantic parsers;semantic parsing paradigms;utterances db tables;nl utterances db;different semantic parsing;tables vector utterance;semantic parsers using;parsers using tabert;utterances db;utterances labeled db;tabular data semantic;utterance token table;queries challenging weakly;parsing paradigms;contextual representations nl;tabert different semantic;challenging weakly supervised;jointly learns contextual;learns contextual;task speci\ufb01c parser;text sql dataset;representations nl utterances;data semantic;parsing paradigms classical;parsing;nl utterances labeled;parallel nl utterances;tabert built bert"}, "7e870eb8d580fb1b8b7a8f97d94d67555a225635": {"ta_keywords": "intelligent message addressing;expert search auto;corpus frequently email;addressing finding persons;message addressing finding;recipient contact intelligent;frequently email users;frequently email;intelligent auto completion;search auto completion;expert search;potential recipients message;specified recipients;email investigated combinations;previously specified recipients;recipients message composition;message addressing;message addressing problems;search auto;models expert search;email users suggesting;recipients message;recipients;addressing finding;email users subject;intended recipient contact;letters intended recipient;subject message addressing;recipient contact;intended recipient", "pdf_keywords": ""}, "267b94325028e0e2e6da1ae2cbe7f7a93284722e": {"ta_keywords": "search disambiguation email;disambiguation email using;disambiguation email;similarity metrics documents;email using graphs;using graphs reranking;graphs reranking;graph walk similarity;walk similarity measures;content social networks;similarity measures;extended similarity metrics;contextual search disambiguation;similarity metrics;graphs reranking schemes;similarity measures outperform;structural graph;based graph walk;walk similarity;search disambiguation;lazy graph walk;metrics documents objects;contextual search;extended similarity;email data content;social networks;graph walk interesting;lazy graph;structural graph paper;social networks timeline", "pdf_keywords": ""}, "a309ad4c4088843d230be1a85806960e633e1e46": {"ta_keywords": "data curation argues;data curation;changing world nlp;arguments data curation;world nlp community;nlp community currently;social biases annotation;nlp community;deep learning;biases annotation artifacts;biases annotation;training data alternative;world nlp;models learn;curation argues fundamentally;datasets deliver;deep learning models;development deep;models training data;curation argues;models training;learning models training;world changing data;training data;nlp;curation happening;curation happening changing;resources development deep;development deep learning;curation", "pdf_keywords": "data2 linguistic ethical;curating data2 linguistic;data curation argues;linguistics ai ethics;nlp understood deep;curating data2;data curation;interdisciplinary tension nlp;linguistic ethical perspectives;data2 linguistic;linguistic ethical;arguments data curation;ai ethics;arguments curating data2;nlp community;nlp community currently;computational linguistics ai;2021 abstract nlp;ai ethics position;linguistics ai;nlp;abstract nlp community;curating;abstract nlp;tension nlp;nlp understood;tension nlp understood;curation argues;computational linguistics;curation argues fundamentally"}, "1be28ce9a1145c2cf4f78e6c494a4c15397fbac3": {"ta_keywords": "knowledge summarization graph;biomedical knowledge graphs;knowledge graph summarization;summarization graph neural;sumgnn knowledge summarization;drug interaction prediction;subgraph summarization scheme;summarization graph;subgraph summarization;based subgraph summarization;knowledge graph;knowledge summarization;large biomedical knowledge;efficient knowledge graph;graph summarization;knowledge graphs;attention based subgraph;biomedical knowledge;external biomedical knowledge;provides interpretable prediction;datasets large biomedical;typed drug interaction;knowledge data integration;knowledge data;pharmacological effect prediction;prediction generated reasoning;biomedical knowledge significantly;prediction efficient knowledge;sumgnn provides interpretable;sumgnn knowledge", "pdf_keywords": "summarization graph neural;knowledge summarization graph;subgraph summarization scheme;subgraph summarization;sumgnn knowledge summarization;knowledge summarization module;based subgraph summarization;summarize subgraph information;summarization graph;summarize subgraph;module summarize subgraph;attention based subgraph;knowledge summarization;summarization module;summarization scheme generate;interpretability knowledge summarization;summarization module provide;summarization module summarize;subgraph extraction module;design knowledge summarization;summarization scheme;subgraph information;subgraph module;subgraph extraction;subgraph information graph;graph neural;sumgnn knowledge;subgraph multi;subgraph module ef\ufb01ciently;enabled subgraph extraction"}, "ce6143e24a455edc233f12933e9903426b963799": {"ta_keywords": "latent dirichlet allocation;statistical topic models;dirichlet allocation experimental;em latent dirichlet;dirichlet allocation;topic models;topic models latent;models latent dirichlet;latent dirichlet al;latent dirichlet;scalability parallelized variational;parallelized variational em;parallel implementations variational;summarize large document;em algorithm lda;parallelized variational;algorithm lda multiproces;large document collec;variational em algorithm;implementations variational em;algorithm lda;lda multiproces;lda multiproces sor;scalability parallelized;setting statistical topic;parallel implementations;speed scalability parallelized;variational em latent;document collections;implementations variational", "pdf_keywords": ""}, "51bf7a3aee6b1f61b902625f6badffedf200d31a": {"ta_keywords": "model gan learns;rewriting deep generative;deep generative;deep generative model;gan learns model;generative model gan;model gan;gan learns;rules encoded deep;encoded deep generative;generative models;rewriting deep;memory rewriting deep;generative model;generative;state art generative;associative memory rewriting;learns model rich;generative model derive;learns model;art generative models;gan;generative model paper;associative memory;manipulating layer deep;art generative;deep network;layer deep;linear associative memory;associative memory demonstrate", "pdf_keywords": "gan interactively rewriting;deep generative;interactively rewriting deep;deep generative model;rewriting deep generative;layers gan interactively;rules encoded deep;gan interactively;encoded deep generative;rewriting deep;rules generative;layers gan;encoded layers gan;rules generative model;generative models;generative model achieve;change rules generative;generative model;deep generator demonstrate;layer deep generator;generative;linear associative memory;convolutional layer deep;generative model layer;associative memory;deep generator;manipulating layer deep;associative memory demonstrate;rules encoded layers;deep network"}, "990c7726dd31723f97a364828d5191080fe7ec2d": {"ta_keywords": "topological quantum computing;chiral topological superconductor;topological superconductor;topological superconductor decomposed;universal topological quantum;topological quantum;superconductor decomposed fibonacci;quantum computing;chiral topological;mode chiral topological;shor integer factorization;superconductor;correlated majorana edge;superconductor decomposed;quantum computing application;strongly correlated majorana;quantum;correlated majorana;edge mode chiral;universal topological;majorana edge mode;anyon thermal operator;example universal topological;mode chiral;thermal operator anyon;factorization algorithm 120;integer factorization algorithm;fibonacci anyon thermal;majorana edge;topological", "pdf_keywords": "universal tqc quantum;tqc quantum circuit;tqc quantum;gates universal tqc;quantum gates universal;tqc consistent quantum;design universal tqc;universal tqc;universal tqc based;design quantum gates;quantum circuit models;universal tqc consistent;tqc based strongly;quantum circuit model;quantum gates;conventional quantum circuit;showed universal tqc;quantum circuit;tqc;tqc based;design quantum;tqc consistent;quantum models;quantum models combines;conventional quantum;quantum;tsc corresponding conformal;gates universal;article design quantum;consistent quantum models"}, "6fae71765a5e86dfef2f93bbe03c4a2e20f827b5": {"ta_keywords": "speech recognition asr;english speech recognition;speech recognition iwslt;speech recognition;core speech processing;automatic speech recognition;recognition asr;spoken language translation;speech processing;automatic speech;dnn acoustic models;english automatic speech;rescoring combination recognizer;recognition asr track;language models decoding;deep neural net;neural net dnn;speech processing technologies;decoding rescoring;asr systems;workshop spoken language;asr systems presented;decoding lattice rescoring;models decoding rescoring;net dnn acoustic;english automatic;recognizer output voting;language translation;recognizer output;recognizer", "pdf_keywords": ""}, "b9e6c65aacfe8ecc1b7833b47803672273a918ec": {"ta_keywords": "\u78ba\u7387\u7684\u30bf\u30b0\u4ed8\u4e0e\u30b3\u30fc\u30d1\u30b9\u304b\u3089\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb\u69cb\u7bc9 \u81ea\u7136\u8a00\u8a9e\u51e6\u7406 nl;\u78ba\u7387\u7684\u30bf\u30b0\u4ed8\u4e0e\u30b3\u30fc\u30d1\u30b9\u304b\u3089\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb\u69cb\u7bc9 \u81ea\u7136\u8a00\u8a9e\u51e6\u7406;\u78ba\u7387\u7684\u30bf\u30b0\u4ed8\u4e0e\u30b3\u30fc\u30d1\u30b9\u304b\u3089\u306e\u8a00\u8a9e\u30e2\u30c7\u30eb\u69cb\u7bc9;\u81ea\u7136\u8a00\u8a9e\u51e6\u7406 nl;\u81ea\u7136\u8a00\u8a9e\u51e6\u7406 nl vol;\u81ea\u7136\u8a00\u8a9e\u51e6\u7406;nl vol 2010;nl;nl vol;vol 2010 nl;2010 nl 196;nl 196;2010 nl;vol 2010;vol;2010;196", "pdf_keywords": ""}, "cf08bef866885edb8b001deb18e582eec94c51de": {"ta_keywords": "automatic speech summarization;spontaneous speech summarization;ted talk summarization;speech summarization open;speech summarization;speech summarization checking;talk summarization;method speech summarization;talk summarization experiments;automatic speech;human summarization;summarization checking semantic;summarization open domain;similarity human summarization;summarization use semantic;human summarization use;summarization checking;summarization experiments;summarization experiments results;ted talks challenges;domain ted talk;summarization open;problem automatic speech;domain ted talks;messages free talks;free talks addition;summarization;semantic acoustic features;ted talks;ted talk", "pdf_keywords": ""}, "b790c3e712c92065d596364af81a494adbc62c39": {"ta_keywords": "distributionally robust optimization;neural generative models;large scale generative;neural generative;generative models;trained minimize maximum;generative models develop;robust optimization;distributionally robust;generative;trained minimize;use neural generative;generative models characterize;model trained minimize;min max optimization;robust optimization dro;max optimization;scale generative models;learning models able;optimization large scale;gradient based optimization;models robust;distributions uncertainty set;constrained inner maximization;scale generative;loss distributions uncertainty;model selection heuristics;learning models;models robust comparable;based optimization large", "pdf_keywords": ""}, "43d5c00938bd2acb1aca8e81a7d220025eddbc23": {"ta_keywords": "historischer deutscher texte;normalisierung historischer deutscher;normalisierung historischer;die normalisierung historischer;deutscher texte;historischer deutscher;f\u00fcr die normalisierung;die normalisierung;guidelines f\u00fcr die;historischer;normalisierung;guidelines f\u00fcr;guidelines;deutscher;texte;f\u00fcr die;f\u00fcr;die", "pdf_keywords": ""}, "6695d3b92e7cd7f2359f698a09c7b3dc37996329": {"ta_keywords": "augmented multimodal pretraining;label augmented pretraining;multimodal pretraining;label augmented multimodal;augmented pretraining;representation learning pretraining;task label augmentation;pretraining task label;enrich vision language;learning pretraining;augmented multimodal;augmented pretraining model;visual language tasks;novel pretraining task;label augmentation;vision language pairs;pretraining task;auto generated labels;visual objects enrich;vision language;labels visual objects;learning pretraining increasing;pretraining universally;label augmentation second;quality vision language;visual language;lamp label augmented;pretraining;modal representation learning;label augmented", "pdf_keywords": "retrieval imagebert trained;label augmented pretraining;shot image retrieval;enrich vision language;labels pretraining;value labels pretraining;imagebert trained;imagebert trained lait;retrieval imagebert;visual objects enrich;labels pretraining effectiveness;image retrieval imagebert;labels visual objects;description corresponding image;vision language pairs;object label alignment;vision language;objects enrich vision;image retrieval;novel pretraining task;labels visual;visual objects;augmented pretraining;shot image;auto generated labels;zero shot image;augmented pretraining model;pretraining task;object label;label alignment task"}, "887d84c1310c6e71a0f89874ef9985b65a44c855": {"ta_keywords": "discriminative feature transforms;noisy speech recognition;acoustic feature space;speech recognition;feature transforms;discriminative feature;feature transforms demonstrate;summing transformed features;speech recognition community;feature transform;train feature transforms;feature compensation techniques;attracted speech recognition;acoustic feature;feature transforms using;transformed features weighted;transforms using discriminative;criterion discriminative feature;speech recognition experiment;dmmi criterion discriminative;typically acoustic feature;feature compensation;demonstrate noisy speech;noisy speech;feature compensation performed;dmmi achieves recognition;feature transform assigned;gmm feature transform;features weighted posterior;transforms demonstrate noisy", "pdf_keywords": ""}, "46bf4bece58764d22764acfd3d232b50fb7767f9": {"ta_keywords": "cnns classifying alzheimer;deep learning mri;alzheimer disease classification;classifying alzheimer;classifying alzheimer disease;hippocampal formation alzheimer;learning mri affirms;learning mri;alzheimer disease ad;formation alzheimer disease;ad structural neuroimaging;formation alzheimer;alzheimer disease;neuroimaging signatures finding;structural neuroimaging signatures;neuroimaging signatures;structural mri data;alzheimer;3d deep convolutional;hippocampal formation anatomical;deep convolutional neural;ad classification identify;classifiers deep;structural neuroimaging;classification identify ad;structural mri;3d deep;hippocampal formation;ad classification;longitudinal scans classification", "pdf_keywords": ""}, "6f173939f6defe3ebae8fb12f19349ba96b7b5c4": {"ta_keywords": "subsequence speaker correspondence;inter subsequence speaker;speakers subsequence;speaker correspondence;based diarization subsequence;subsequence speaker;speaker correspondence obtained;attractor based diarization;unsupervised clustering vectors;produce accurate diarization;subsequence wise diarization;accurate diarization;output speakers subsequence;diarization methods;accurate diarization results;based diarization;end diarization methods;produce diarization results;diarization subsequence;speakers subsequence limited;diarization;produce diarization;diarization results inter;diarization subsequence method;unsupervised clustering;clustering vectors;introduce unsupervised clustering;diarization results;diarization methods given;speakers recording", "pdf_keywords": "sequence acoustic features;speakers training data;speakers input sequence;speaker waveform mixture;input sequence acoustic;mixture speaker waveform;acoustic features;stacked transformer encoders;speaker waveform;sequence acoustic;number speakers training;acoustic features \ufb01rst;attractor based diarization;mixture speaker;attractor correspondence clustering;embeddings using stacked;split mixture speaker;speakers training;attractor calculation embedding;embedding extraction;speakers input;calculation embedding extraction;number speakers input;mixture rest speakers;embedding extraction based;embeddings short subsequences;encoders;encoders generalized;permutation invariant training;number speakers"}, "1fb88c130bedcd2e75fd205b70af2999c6a8c49d": {"ta_keywords": "malicious edges targeting;characterizing malicious edges;malicious edges;targeting graph neural;edges targeting graph;characterizing malicious;edges targeting;targeting graph;graph neural networks;graph neural;malicious;edges;targeting;graph;networks;neural;neural networks;characterizing", "pdf_keywords": ""}, "6ad56b1b776a2c448fc90c543b50756941e5a119": {"ta_keywords": "incentives estimating consumer;estimating consumer utility;consumer utility;consumer utility function;consumers revenue decoupling;utility company consumer;designing incentives estimating;incentives estimating;consumption patterns consumers;patterns consumers revenue;estimating consumer;utility companies motivations;algorithm designing incentives;consumers revenue;revenue decoupling demand;demand response programs;designing incentives;energy consumption patterns;consumer interaction principal;principal agent problem;decoupling demand;utility companies;consumption patterns;modifying energy consumption;consumers;decoupling demand response;principal agent;function utility companies;utility function utility;consumer interaction", "pdf_keywords": ""}, "bb6317bbd2c4a81e94cf3d7eb1b73da246a022db": {"ta_keywords": "neighbor language models;nearest neighbor language;predicting word nearest;memorization nearest neighbor;word nearest neighbor;neural language model;nn model nearest;memorization nearest;trained lm embedding;easier predicting word;nearest neighbors nn;language models results;learning similarity sequences;generalization memorization nearest;nearest neighbor search;model nearest neighbors;language modeling long;text easier predicting;word nearest;predicting word;trained neural language;nearest neighbor;language models;neighbors nn model;language model lm;suggest learning similarity;nearest neighbors;neighbor language;pre trained neural;lm neighbors", "pdf_keywords": "neighbor language models;memorization nearest neighbor;nearest neighbor language;language models similarly;new language modeling;existing language models;generalization memorization nearest;good learning representation;embeddings simple nearest;notion similarity transformer;effective domain adaptation;neural language model;language models;domain adaptation;language modeling;learning representation;memorization nearest;domain adaptation simply;representation learning;hypothesis representation learning;trained neural language;nearest neighbors knn;retaining effective similarity;word distribution nearest;neighbor language;embeddings;language model lm;language model;nearest neighbor scheme;nearest neighbor"}, "b31eb3428320342dfde042693ff2ca106dabed0d": {"ta_keywords": "learning abstractive summarization;abstractive summarization simcls;abstractive summarization;sequence learning framework;summarization simcls;sequence sequence learning;contrastive learning abstractive;framework abstractive summarization;sequence learning;contrastive learning;summarization;summarization simcls bridge;assisted contrastive learning;framework contrastive learning;formulating text generation;text generation;contrastive learning particularly;learning abstractive;cnn dailymail;learning objective evaluation;text generation reference;reference free evaluation;cnn dailymail dataset;rouge cnn dailymail;estimation assisted contrastive;assisted contrastive;cnn;learning objective;learning framework formulating;free evaluation", "pdf_keywords": "learning abstractive summarization;model abstractive summarization;abstractive summarization simcls;abstractive summarization achieves;abstractive summarization seq2seq;abstractive summarization;summarization achieves goal;contrastive learning abstractive;summarization simcls;approach abstractive summarization;framework abstractive summarization;summarization achieves;generate candidate summaries;summarization propose;text summarization;sequence learning framework;contrastive learning;summarization;sequence sequence learning;candidate summaries mle;2021b text summarization;summarization seq2seq model;text summarization propose;framework contrastive learning;summarization propose use;sequence learning;summaries mle;candidates contrastive learning;abstractive summarization yixin;formulating text generation"}, "5b1516c87818084dc5d195cc274e1ee8923210d2": {"ta_keywords": "bilingual word embeddings;lingual named entity;neural cross lingual;translations based bilingual;languages cross lingual;cross lingual named;cross lingual;cross lingual setting;lingual named;bilingual word;based bilingual word;lingual;entity recognition ner;named entity recognition;word embeddings;languages annotated;bilingual;transfer natural language;resource rich languages;based bilingual;entity recognition minimal;finds translations based;languages annotated resources;finds translations;translations based;languages cross;lingual setting;entity recognition;translations;lexical items languages", "pdf_keywords": "words shared embedding;translations words shared;lexical mapping languages;new lexical mapping;discrete word translations;embeddings different languages;lexical mapping approach;challenge lexical mapping;lexical mapping;word translations;perform lexical mapping;translations words;mapping \ufb01nd translations;mapping languages;lexical mapping \ufb01nd;\ufb01nd translations words;word translations looking;effectively perform lexical;translations looking nearest;translated data;translations;new lexical;lingual;propose new lexical;translations looking;lingual ner attempts;lingual ner;differences cross lingual;translated data speci\ufb01cally;challenge lexical"}, "1ce0664989e0b28ceea223cab68f885ed18c39c4": {"ta_keywords": "speech modeling propose;sampling approach speech;speech modeling;speaker clustering;speaker clustering using;based speaker clustering;approach speech modeling;gaussian mixture model;gibbs sampling based;speech dynamics;model speaker cluster;based gibbs sampling;gaussian based speaker;speaker cluster precisely;propose gibbs sampling;speech dynamics dynamics;dynamics speech observed;hierarchical mixture model;gibbs sampling;dynamics speech;mixture model based;based gaussian mixture;dynamics dynamics speech;segmental utterance wise;speaker cluster;scale mixture model;gaussian mixture;mixture model mixture;extension gaussian mixture;segmental utterance", "pdf_keywords": ""}, "2154bdb9ce841eb98b9fd13bf7bf0a42f11f89a6": {"ta_keywords": "protocol distributed optimization;distributed optimization strong;efficient decentralized training;sgd communication efficient;networks large;distributed optimization;networks large datasets;efficiency protocol distributed;decentralized training heterogeneous;iterative averaging protocol;speedup resnet;training deep;neural networks large;experiments speedup resnet;protocol distributed;communication efficient decentralized;training imagenet;training imagenet compared;decentralized training;imagenet compared competitive;guarantees running protocols;networks;imagenet;preemptible compute nodes;50 training imagenet;speed networking;efficient decentralized;averaging protocol;large datasets accelerated;averaging protocol exponentially", "pdf_keywords": "distributed training;training heterogeneous compute;known distributed training;protocol distributed optimization;distributed training utilize;sgd optimization;distributed training operate;local sgd optimization;distributed optimization;distributed optimization strong;centralized local sgd;sgd communication ef\ufb01cient;based distributed training;sgd communication;decentralized training heterogeneous;heterogeneous compute nodes;training deep;sgd optimization section;multiple compute nodes;training heterogeneous unreliable;neural networks large;networks large datasets;moshpit sgd communication;training deep neural;local sgd;deep learning;deep neural networks;federated learning;large datasets accelerated;deep learning enabled"}, "d9212b207e49a3aa6806fb2ddadb303b7b1d47a8": {"ta_keywords": "agents natural language;procedural knowledge agent;hierarchical procedural knowledge;command control hierarchical;command situated agents;task hierarchically;natural language instantiate;hierarchical modular networks;control situated agents;particular task hierarchically;knowledge agent command;paradigm hierarchical modular;representing hierarchical procedural;agent command control;task hierarchically splitting;procedural knowledge;hierarchical procedural;actions hierarchies procedures;hierarchically;level tasks;datasets nl instruction;modeling paradigm hierarchical;situated agents;hierarchical;natural language nl;nl command situated;higher level tasks;natural language;hierarchical modular;knowledge agent", "pdf_keywords": "agents natural language;task hierarchically;level programming language;hierarchical procedural knowledge;level programming;particular task hierarchically;high level programming;task hierarchically splitting;described natural language;leveraging hierarchical procedural;programming language like;higher level tasks;hierarchical modular networks;level tasks;control situated agents;language like python;programming language;accomplishing complex tasks;paradigm hierarchical modular;like python;hierarchically;situated agents accomplishing;modeling paradigm hierarchical;procedural knowledge;datasets nl instruction;like python work;hierarchical procedural;programming;tasks propose modeling;representing procedures programs"}, "8fcd012e8ed2ea8190163369c9f222178e70a19d": {"ta_keywords": "speech recognition asr;asr attention based;hmm asr systems;decoding combining attention;speech recognition;recognition asr based;recognition asr;hmm deep neural;model hmm deep;connectionist temporal classification;training decoding;architectures asr attention;conventional dnn hmm;automatic speech recognition;joint decoding linguistic;architectures training decoding;attention based ctc;asr attention;automatic speech;learning joint decoding;end asr effectively;model hmm;dnn hmm asr;deep network architecture;hybrid ctc attention;temporal classification ctc;comparable conventional dnn;combining attention based;large scale asr;end architectures asr", "pdf_keywords": ""}, "49418122bba375fa02907d38b0be80689f750b39": {"ta_keywords": "learned codes accurately;learned codes;learned codes highly;approach learned codes;learning encoding;codes learned codes;code machine learning;codes learned;codes accurately reconstruct;unavailable predictions neural;learning code machine;designing codes learned;learning code;redundant computation decoding;ones learning code;learning encoding decoding;machine learning approximate;image classifiers mnist;decoding function reconstructs;learning approximate;computations using coding;codes highly promising;classifiers mnist fashion;predictions neural;coding non linear;coded computation;classifiers mnist;encoding decoding functions;learning approximate non;reconstructions unavailable computation", "pdf_keywords": "unavailable predictions neuralnetwork;function learned codes;learned codes accurately;learning encoding;learned codes;predictions neuralnetwork;predictions neuralnetwork based;codes accurately reconstruct;neuralnetwork;approach learned codes;neuralnetwork based image;neuralnetwork based;learning approximate;predictions image classi\ufb01ers;class predictions image;learning code;learning code machine;erasure codes approximate;learning approximate non;code machine learning;learning encoding decoding;machine learning approximate;codes approximate;encoding decoding functions;linear function learned;predictions image;erasure codes;2018 learning code;image classi\ufb01ers mnist;codes approximate unavailable"}, "49f657d704a1b80ce3dba0d8a9e5479ec1d703d4": {"ta_keywords": "transformers structure speech;memory networks;short term memory;autoregressive network asr;term memory networks;speech recognition;machine translation training;deep transformers;memory networks large;recently deep transformers;translation training input;autoregressive transformers;non autoregressive transformers;asr network;structure speech recognition;network asr;speech recognition originally;autoregressive transformers structure;translation training;deep transformers start;input speech;train non autoregressive;outperformed kaldi nnet3;term memory;network asr network;machine translation;introduced machine translation;non autoregressive network;context input speech;asr network required", "pdf_keywords": "transformerbased automatic speech;speech recognition asr;speech recognition;automatic speech;automatic speech recognition;new decoding strategy;different decoding strategies;recognition asr;structure automatic speech;2010 named decoding;new decoding;recognition asr cmlm;support different decoding;decoding;different decoding;context input speech;named decoding;input speech;decoding strategies;tokens new decoding;decoder;decoding strategy proposed;training framework transformerbased;named decoding strategy;decoding strategies including;decoding strategy;decoding strategy easy;fed decoder;classical dependency parsing;nonautoregressive training framework"}, "a1321f4527559836509c27008329afaf11f8ea89": {"ta_keywords": "learns cognitive skills;learns cognitive;solving problems interleaved;agent learns cognitive;learning agent learns;learning agent;does solving problems;learning transfer study;problem solving experience;correction learning agent;orders suggests learning;learns;solving problems;agent learns;learning agent problem;cognitive skills;cognitive skills examples;study shows learning;learning;learning transfer;performance does solving;learning apply skill;skills examples problem;suggests learning;solving problems blocked;skills examples;implications learning transfer;examples problem solving;interleaved problem orders;shows learning", "pdf_keywords": ""}, "da46a0b5ddf0f4bf4caad9d29d6b4a93dd2eb2d2": {"ta_keywords": "bandit style problem;present bandit;bandit style;present bandit style;bandit;heuristic algorithms;modeling games preliminary;initial heuristic algorithms;specific problem agent;problem agent based;modeling games;heuristic algorithms compare;agent based modeling;problem agent;agent;heuristic;initial heuristic;based modeling games;games preliminary;provide initial heuristic;games preliminary work;agent based;algorithms;games;algorithms compare;algorithms compare baselines;based modeling;modeling;specific problem;preliminary", "pdf_keywords": ""}, "8ff54aa8045b1e30c348cf2ca42259c946cd7a9e": {"ta_keywords": "sequential question answering;parsing question answering;question answering task;question answering model;question answering;neural semantic parsing;learning sequential question;question answering focused;dynamic neural semantic;semantic parsing question;answering model;semantic parsing;answering model effectively;semantic parsing framework;structured learning sequential;answering task;parsing framework trained;neural semantic;answering task propose;search based neural;learning sequential;conversation humans solve;parsing;neural structured learning;answering focused long;neural structured;answering focused;parsing question;based neural structured;supervised reward guided", "pdf_keywords": ""}, "aaaff6b99684cb5b5e0a68e214bd8bbd4bf2e231": {"ta_keywords": "entity recognition systems;named entity recognition;entity recognition;biomedical named entity;fields syntactic parsing;syntactic parsing evaluation;random fields syntactic;parsing evaluation;data manually annotated;manually annotated;parsing;syntactic parsing;evaluation biomedical named;parsing evaluation assess;conditional random fields;dataset consists biomedical;based hidden markov;manually annotated material;hidden markov model;biomedical named;hidden markov;fields syntactic;annotated material;annotated material including;named entity;markov model based;evaluation biomedical;automatically generated data;random fields;annotated", "pdf_keywords": ""}, "956e096b1e8422c91989938b9508272b956d3070": {"ta_keywords": "graph reranking approach;graph reranking;probabilities graph reranking;walks directed graphs;typed graph walks;graph walks directed;random graph walks;improve graph walk;graph walks;graph walk performance;lazy random graph;reranking approach;graph walk;directed graphs entities;reranking approach uses;graph walks local;rank typed graph;reranking performance;reranking;walks directed;directed graphs;reranking performance usually;walk performance gradient;corpora reranking performance;learning rank typed;learning rank;corpora reranking;graphs entities;multiple corpora reranking;walks local global", "pdf_keywords": ""}, "a4a8e91995ae8c8b203dd857bdc0915facddeebe": {"ta_keywords": "crowdsourced labels;real crowdsourced labels;crowdsourced labels confirm;crowdsourced;labels worker quality;real crowdsourced;modeling labels worker;using real crowdsourced;crowd sourced data;supervised learning depends;noisy crowd sourced;supervised;imagenet simulated noisy;learning depends annotated;estimating worker quality;jointly modeling labels;labels worker;estimate worker quality;supervised learning;algorithm estimate worker;experiments conducted imagenet;imagenet;benefits supervised learning;crowd sourced;algorithm benefits supervised;quality noisy crowd;workers ms coco;annotated examples;annotated examples taken;better label examples", "pdf_keywords": "learning noisy crowd;learning noisy;algorithm learning noisy;learning noisy singly;supervised learning depends;models labels worker;modeling labels worker;supervised learning;supervised;labels worker quality;algorithm estimates worker;new supervised learning;noisy crowd sourced;learning depends annotated;new supervised;jointly modeling labels;2018 learning noisy;models learned algorithm;crowd sourced data;algorithm learning;estimating worker quality;supervised learning algorithm;new algorithm learning;labels worker;abstract supervised learning;presents new supervised;noisy crowd workers;estimate worker quality;abstract supervised;trained model algorithm"}, "ca3535dcdda9849350ad7c991a60660b22844f2f": {"ta_keywords": "decoder model speech;model speech translation;intermediates speech recognition;decomposable sequence tasks;model speech;speech recognition sub;sequence tasks;hidden intermediates speech;speech recognition;speech translation extracts;compositionality learn searchable;searchable hidden intermediates;searchable hidden representations;multi decoder model;decoder model;decomposed sub tasks;tasks hidden intermediates;speech translation;decoder;hidden intermediates improved;recognition sub task;intermediates speech;learn searchable hidden;multi decoder;translation extracts searchable;models decomposable sequence;framework multi decoder;hidden representations intermediate;sub tasks;hidden intermediates", "pdf_keywords": "decoder speech;decoder speech translation;multi decoder speech;speech translation intermediate;searchable intermediate representations;decoder model;transcript sequences asr;decoder;multi decoder;encoder decoder model;regressive encoder decoder;representations discrete transcript;speech translation bleu;searchable hidden intermediates;approach speech translation;encoder decoder;speech translation;sub net decoder;transcript sequences;decoder model decomposable;net decoder;searchable intermediate;englishfrench test sets;ef\ufb01cacy searchable intermediate;german englishfrench test;discrete transcript sequences;sequences asr sub;learn searchable hidden;sequences asr;translation intermediate stage"}, "d56244c6abf3141900386d6911dd9097697a346b": {"ta_keywords": "web page classifier;improving page classifier;page classifier;page classifier anchor;classifier anchor extraction;page classifier performance;classifier improving page;classifier anchor;words classifier improving;page structure hub;words classifier;classifier performance pages;improves simple web;bag words classifier;hub pages;learner categorization site;structure hub pages;category anchor wrappers;pages labeled using;words classifier reducing;pages labeled;categorization site;anchor extraction link;hub pages real;classifier improving;pages real world;classifier results;new unseen web;categorization site implies;average pages labeled", "pdf_keywords": ""}, "42605c1ee030721cb38a3c225992d63297a6ace0": {"ta_keywords": "language revitalization technologies;language documentation revitalization;language documentation conservation;workshop language technology;technology language documentation;practical language revitalization;language revitalization;language technology;social media language;documentation revitalization;language technology language;documentary linguists technologists;documentation revitalization areas;advances natural language;documentary linguists;corpora social media;technologies aid language;tools corpora social;documentation conservation;language learning bots;tools corpora;technology language;workshop language;language documentation;language community;language technology application;bring language community;members documentary linguists;processing language technology;linguists technologists discuss", "pdf_keywords": "languages arapaho cayuga;technologies languages arapaho;workshop language technology;language technology language;languages arapaho;language technology;language revitalization technologies;arapaho language video;technology language documentation;language documentation;arapaho language;practical language revitalization;language video;documentary linguists;language revitalization;technologies languages;workshop language;language video format;language technology application;database arapaho language;documentary linguists technologists;implemented technologies languages;language;language documentation conservation;language culture;languages;technology language;language documentation revitalization;language community;processing language technology"}, "c8d0e13de2eaa09a928eff36b99d63f494c2f5ec": {"ta_keywords": "python syntactic neural;syntax target programming;parsing natural language;semantic parsing;syntactic neural;like python syntactic;syntactic neural model;architecture powered grammar;semantic parsing paper;underlying syntax target;syntax prior knowledge;parsing;parsing natural;syntax target;code generation;purpose code generation;problem language generation;python syntactic;parsing paper propose;consider problem parsing;capture target syntax;code generation informed;target programming language;syntax prior;target syntax prior;language generation task;programming language like;powered grammar model;natural language;language generation", "pdf_keywords": "syntax driven neural;driven syntaxbased neural;syntaxbased neural;syntaxbased neural network;data driven syntaxbased;neural semantic parsing;syntax driven;syntax tree;syntax tree sequentially;abstract syntax tree;neural code generation;driven syntaxbased;semantic parsing model;propose syntax driven;syntax prior knowledge;syntaxbased;generates abstract syntax;sequence code generation;architecture powered grammar;proposes syntax driven;semantic parsing;powered grammar model;driven neural code;syntax prior;parsing model;seq2tree neural semantic;semantic parsing paper;grammar model explicitly;powered grammar;grammar model formalizes"}, "9c03d14520c897ca8536e165507f568d1980dabd": {"ta_keywords": "machine comprehension test;machine comprehension text;machine comprehension;comprehension test 2013;al machine comprehension;comprehension test;method machine comprehension;question types coreference;comprehension text;develop lexical matching;coreference resolution;lexical matching;comprehension test richardson;coreference resolution proposed;strong lexical matching;lexical matching method;comprehension text overarching;coreference;types coreference resolution;natural language processing;comprehension;processing strong lexical;develop lexical;lexical;language processing strong;language processing;types coreference;research natural language;strong lexical;natural language", "pdf_keywords": ""}, "2ea226a7fadde6a45f537c714e0832e83136f861": {"ta_keywords": "biomedical event extraction;structured prediction;structured prediction framework;based structured prediction;event extraction;biomedical event;prediction applied biomedical;approach biomedical event;classification tasks;event extraction using;structured prediction applied;classification tasks models;applied biomedical event;sensitive classification tasks;search based structured;score bionlp 2009;cost sensitive classification;bionlp 2009 shared;score bionlp;biomedical;bionlp 2009;extraction using search;performance joint inference;approach biomedical;prediction framework searn;tasks models learned;2009 shared task;sensitive classification;models learned jointly;points score bionlp", "pdf_keywords": ""}, "708f8c0eb5032edd6f31663a27febbb0529cbcf3": {"ta_keywords": "neural syntax learner;grounded neural syntax;learning syntactic;learning syntactic representations;syntax learner;syntax learner vg;visually grounded neural;approach learning syntactic;neural syntax;linguists model learns;syntactic representations;unsupervised parsing;syntactic representations structures;parse trees concreteness;various unsupervised parsing;visual grounding terms;unsupervised parsing approaches;reading paired captions;grounded neural;parsing;parsing approaches;guide parsing;use visual grounding;visual grounding;paired captions;learns looking natural;guide parsing text;parse trees;parse;parsing text", "pdf_keywords": "grounded neural syntax;neural syntax learner;syntax learner visually;neural syntax acquisition;jointly learns parse;visually grounded neural;visually grounded language;syntax learner vg;learns parse trees;visually grounded textual;grounded language learning;syntax learner;learns parse;learner visually grounded;grounded language structure;learning syntactic;grounded textual representations;syntax acquisition;learning syntactic structures;learning syntactic representations;neural syntax;learning produces parsing;grounded acquisition syntax;grounded neural;focus learning syntactic;grounded textual;parse trees visually;syntax natural language;nsl jointly learns;syntax acquisition haoyue"}, "06e36261b21af2943e464a562c92c09dac292a82": {"ta_keywords": "compilation augmenting decompiler;reverse compilation;attempts reverse compilation;decompiled variable retyper;compilation augmenting;dirty decompiled variable;reverse compilation transforming;types decompiler;compilation transforming binary;learned variable names;decompiler output learned;decompiled variable;names types decompiler;variable retyper;augmenting decompiler output;compilation;decompiler output automatically;meaningful variable names;variables comments abstractions;variable retyper novel;compilation transforming;generates meaningful variable;lost compilation augmenting;augmenting decompiler;abstractions lost compilation;decompiler output;types decompiler attempts;improving quality decompiler;present dirty decompiled;decompiler", "pdf_keywords": "names decompiled code;meaningful variable names;decompiled variable retyper;code model neural;types decompiled variables;dirty decompiled variable;types names decompiled;decompiled variables;decompiler variable retyper;dirty decompiler variable;decompiled code model;variable retyper opensource;names decompiled;variable names;generates meaningful variable;decompiled variables present;variable names types;predicting variable types;novel dataset code;decompiled code;meaningful variable types;decompiled variable;variable retyper novel;variable types names;variable retyper;decompiler variable;recovering meaningful variable;types decompiled;semantic types decompiled;recover syntactic"}, "54e7de06a97b4b6c41e185c0bee60c838a15265a": {"ta_keywords": "controllable speech modification;articulatory controllable speech;modify speech waveforms;speech waveforms manipulating;modified speech waveforms;mixture models articulatory;speech waveforms manipulated;generating modified speech;speech modification framework;speech modification;modify speech;speech modification using;enables modify speech;controllable speech;speech waveforms;manipulated articulatory parameters;manipulating unobserved articulatory;articulatory parameters production;articulatory parameters estimated;models articulatory controllable;unobserved articulatory parameters;modified speech;waveforms manipulated articulatory;articulatory manipulation method;statistical inversion production;articulatory manipulation;articulatory controllable;propose articulatory controllable;models articulatory;propose articulatory manipulation", "pdf_keywords": ""}, "7771aa7badc3375a31bfac8dc47755ff5d5c7780": {"ta_keywords": "word level distributions;morphological complexity;types morphological complexity;orthographic word types;word types different;languages typological characteristics;cross linguistic diversity;text entropy;linguistic diversity;word types;different types morphological;similar distributions languages;types morphological;languages typological;different languages typological;morphological;cross linguistic;subword word level;distributions orthographic word;distributions languages wide;linguistic;characteristics different writing;subword levels relatively;factors text entropy;word level;text entropy values;subword levels;range cross linguistic;linguistic diversity major;languages wide", "pdf_keywords": "morphological complexity types;different morphological complexity;word level distributions;morphological complexity measure;based morphological complexity;morphological complexity;varied subword tokenizatons;linguistic variation;languages typological characteristics;word types different;subword tokenizatons texts;word types;cross linguistic variation;orthographic word types;distributions languages quantify;entropy subword units;general linguistics;entropy subword;linguistics;languages quantify;quantify cross linguistic;corpus based morphological;subword tokenizatons;similar distributions languages;bpe text entropy;text entropy;typologically diverse languages;linguistic variation using;languages typological;discussion different morphological"}, "79c93274429d6355959f1e4374c2147bb81ea649": {"ta_keywords": "visual question answering;image question answering;encoder vision language;learning cross modality;vision language connections;modality encoder representations;cross modality encoder;connecting vision language;vision language reasoning;encoder cross modality;modality encoder vision;language encoder cross;encoder representations transformers;vision language semantics;modality encoder;challenging visual reasoning;language encoder;trained cross modality;encoder representations;cross modality matching;learn vision language;vision language;modality matching;image sentence pairs;visual reasoning task;representations transformers framework;task nlvr2 improve;encoder vision;semantics pre train;encoder language encoder", "pdf_keywords": "image question answering;visual question answering;vision language reasoning;modality encoder representations;learning cross modality;connecting vision language;vision language connections;vision language semantics;cross modality encoder;challenging visual reasoning;encoder cross modality;modality encoder propose;modality encoder;encoder representations transformers;image sentence pairs;vision language;visual reasoning dataset;cross modality matching;modality matching;language encoder;encoder representations;modality matching image;language encoder cross;representations transformers framework;question answering datasets;visual reasoning task;abstract vision language;language modeling masked;learn vision language;modality model adapting"}, "03e4f33c0ccc4cb8c7e1589158a5377cdf5241d2": {"ta_keywords": "ethical priorities ai;priorities ai systems;conditional preference networks;priorities ai;ethical priorities computer;preference networks cp;preferences ethical priorities;preference networks;subjective preferences ethical;subjective preference ethical;decisions ai;ensure decisions ai;rely subjective preferences;decisions ai aligned;set ethical priorities;preference ethical principles;agent subjective preference;compare subjective preferences;subjective preferences decisions;conditional qualitative preference;constrained ethical priorities;qualitative preference relations;ethical priorities;preferences ethical;preference ethical;conditional preference;decisions rely subjective;subjective preferences;preference relations used;preference relations", "pdf_keywords": ""}, "d5f22dbc8f4b9e99f62e6ecf886bc4b9a0372e4d": {"ta_keywords": "hierarchical exploratory em;corpus hierarchical exploratory;ontology text datasets;hierarchical exploratory;exploratory em algorithm;clueweb09 corpus hierarchical;challenges hierarchical classification;hierarchical classification;exploratory em;classifying entities;hierarchy seed;class hierarchy seed;exploratory em approach;datasets derived clueweb09;classes seed hierarchy;corpus hierarchical;extension exploratory em;seed class hierarchy;present hierarchical exploratory;hierarchy seed class;text datasets derived;classifying entities incomplete;discovered classes hierarchy;datasets class hierarchies;derived clueweb09 corpus;text datasets;seed hierarchy;entities classes seed;ontology text;entities incomplete ontology", "pdf_keywords": ""}, "680e61a17e27a1e8e121276c7ec53fc4fd40babb": {"ta_keywords": "language level uniformity;uniformity linguistic signal;language users utterances;measured linguistic;uniformity linguistic;linguistic acceptability;predictions language comprehension;linguistic signal lack;comprehension linguistic acceptability;linguistic signal;users utterances;language users maximizing;unclear uniformity linguistic;linguistic acceptability unclear;sentence language level;language production explored;language comprehension linguistic;reading time acceptability;measured linguistic unit;mean surprisal language;utterances;thereof measured linguistic;language comprehension;comprehension linguistic;users utterances structured;linguistic;makes predictions language;byproduct language users;utterances structured information;sentence language", "pdf_keywords": "information density uid;information density;information theoretical interpretation;information theoretically 1analysis;information theoretical;information rate;information theoretically;mean information;information rate better;mean information content;typical interpretation uid;reading time acceptability;measured information theoretically;uniform information density;language theory;explains linguistic acceptability;uid hypothesis;interpretation language users;uid natural language;information content measured;natural language theory;uid written language;language users maximizing;linguistic acceptability;language users prefer;hypothetical communication channel;theoretical interpretation language;linguistic;uid hypothesis using;explains linguistic"}, "34f8214cbaa0655794c2c9570898abf15649b079": {"ta_keywords": "dereverberation speech recognizer;recognition reverberant speech;speech reverberation time;recognition dereverberated speech;reverberant speech dereverberation;speech reverberation;performance automatic speech;reverberant speech reverberation;speech recognizer;method reverberant speech;speech dereverberation preprocessing;reverberant speech;speech dereverberation;speech recognition;compensation recognition reverberant;automatic speech;interconnection dereverberation speech;automatic speech recognition;speech recognizer paper;speech recognition severely;noise reverberation;reverberation time;compensation mllr adaptation;recognition reverberant;dereverberation speech;dereverberated speech;model variance adaptation;variance compensation recognition;adaptive training implemented;reverberation", "pdf_keywords": ""}, "3aba582b62d1abfcd95264e6c7b32aab4c9db4b8": {"ta_keywords": "selfexplain self explaining;explanations selfexplain;neural text classifiers;classifiers importantly explanations;importantly explanations selfexplain;self explaining model;text classifier predictions;self explaining;self explaining architecture;novel self explaining;concepts selfexplain self;concepts selfexplain;based concepts selfexplain;neural classifiers;text classifiers;explanations selfexplain sufficiency;existing neural classifiers;text classifiers importantly;explains text classifier;neural classifiers adding;explaining architecture neural;label introduce selfexplain;neural text;architecture neural text;predicted label introduce;classifier predictions;introduce selfexplain;model explains text;classifiers;predictions using phrase", "pdf_keywords": "neural text classi\ufb01cation;neural text classi\ufb01ers;explaining neural text;self explaining neural;selfexplain self explaining;interpretability layers neural;text classi\ufb01er predictions;existing neural classi\ufb01ers;layers neural text;neural text;architecture neural text;text classi\ufb01cation tasks;selfexplaining model explains;novel selfexplaining model;text classi\ufb01ers motivation;model explains text;selfexplain novel selfexplaining;selfexplaining model;in\ufb02uential concepts training;self explaining architecture;self explaining model;introduce selfexplain;phrase based concepts;explaining neural;neural classi\ufb01ers adding;self explaining architectures;neural classi\ufb01ers;explaining architecture neural;converting self explaining;generalization power neural"}, "e5a5888966be6b5f9c0e8a82facd604086a1ee4c": {"ta_keywords": "syntax bioner systems;syntax hmmbased systems;trained crf syntax;crf syntax bioner;syntax bioner;crf syntax hmmbased;tagging provided sentences;crf syntax performance;trained hmm based;syntax performance gains;hmm based crf;syntax hmmbased;speech tags;syntax performance;corpus;gene names tagging;performance paper corpus;score trained crf;syntactic parser;corpus described;corpus described section;morphological rule based;classifier improvement;gains crf syntax;speech tags grammatical;using annotation;parser;annotation;uses simple morphological;trained crf", "pdf_keywords": ""}, "73271677da83a3f55523148d1b43a0501f0a35dd": {"ta_keywords": "online learning periodic;online learning dynamics;zero sum games;learning dynamics cyclic;repeated game formulation;games online learning;game theory;game formulation;online regret dynamics;game theory von;learning dynamics;learning periodic zero;games time invariant;online learning behaviors;theorem online regret;sum games online;sum games time;sum games;robustness online learning;sum games general;game formulation realistic;learning periodic;online learning;result game theory;time invariant equilibrium;repeated game;equilibrium time;sum games admit;regret dynamics converge;essentially unique equilibrium", "pdf_keywords": "equilibrium online learning;games periodic zero;class games periodic;strategy spaces periodic;games study periodic;online learning dynamics;zero sum games;games periodic;sum matrix games;games periodic payoffs;sum games periodic;matrix games;games refer periodic;recurrence online learning;invariant equilibrium online;polymatrix games study;dynamics class games;games covered periodic;learning dynamics;polymatrix games;games poincar\u00e9 recurrence;matrix games network;equilibrium class games;sum polymatrix games;games network generalizations;time evolving games;sum bilinear games;learning behaviors zero;equilibrium online;unconstrained strategy spaces"}, "74fb2834c820d2297b08201cb72de1c1d3d27f54": {"ta_keywords": "privacy downstream speech;performance privacy;tradeoffs client privacy;offs performance privacy;privacy preservation;performance privacy complexity;privacy preservation sensitive;privacy;uploaded speech data;privacy complexity;privacy complexity approaches;fidelity reconstruction privacy;privacy downstream;client privacy;reconstruction privacy preservation;client privacy downstream;ensure uploaded speech;reconstruction privacy;privacy discuss;gender accent masking;privacy discuss unique;accent masking tasks;speech data;client privacy discuss;control biometric information;client control biometric;speech data mains;accent masking;sensitive personal attributes;define client privacy", "pdf_keywords": "privacy speech recognition;privacy speech data;client privacy speech;privacy approaches speech;privacy speech;privacy unique technical;privacy;client privacy algorithm;privacy metrics client;metrics client privacy;privacy metrics;client privacy approaches;achieving client privacy;client privacy;privacy approaches downstream;privacy approaches;privacy discuss;privacy algorithm needs;client privacy discuss;privacy algorithm;adversarial training;multiple privacy metrics;de\ufb01ne client privacy;speech data keeping;service client privacy;raw speech data;privacy unique;learning adversarial training;different client privacy;privacy discuss unique"}, "030d7d7ae48a9f81700b2c1f7cf835235777b8e7": {"ta_keywords": "question answering openqa;openqa retrieval natural;openqa retrieval;question answering;extracting answers passages;improves openqa retrieval;extracting answers;scalable neural retrieval;reader extracting answers;domain question answering;answering openqa generally;retrieval natural questions;retrieval model colbert;answering openqa;extractive openqa;neural retrieval model;large corpus reader;extractive openqa performance;retrieval model;retrieval;openqa colbert;corpus reader extracting;large corpus;retrieval natural;relevance guided supervision;neural retrieval;passages large corpus;natural questions squad;colbert openqa;colbert openqa greatly", "pdf_keywords": "effective openqa retrieval;retrieval model colbert;openqa retrieval;recent colbert retrieval;openqa retrieval proposed;colbert retrieval;scalable neural retrieval;retrieval proposed training;retriever modeling supervision;colbert retrieval model;retrieval model improve;retrievers propose relevance;relevance guided supervision;improve retrieval;openqa retrievers propose;improve retrieval quality;retrieval quality existing;retrieval proposed;retrieval;openqa retrievers;existing openqa retrievers;retrieval model;qa improve retrieval;retriever trained;modeling supervision openqa;retrieval quality;extractive openqa results;recent neural retrieval;neural retrieval model;extractive openqa"}, "7c655ef6f0de8c1a219cdb796c77f4ae3c389b82": {"ta_keywords": "aies conference;www aies conference;aies conference com;aaai acm conference;conference provided aaai;aies 18 held;ethics society aies;acm conference ai;society aies;aies 18;aies;conference ai ethics;www aies;aaai acm;acm conference;1st aaai acm;society aies 18;http www aies;ai ethics society;aaai 1st;held just aaai;conference ai;1st aaai;aaai;aaai order;aaai 1st aaai;18 held february;conference available;provided aaai 1st;paper conference", "pdf_keywords": ""}, "b2b0fbf9033f1c36bea8bb11c173f14378c60db9": {"ta_keywords": "speech s2s translation;speech speech s2s;s2s translation systems;speech s2s;s2s considering emphasis;emphasis expressed language;emphasis using speech;s2s translation;emphasis english japanese;translation systems;word level emphasis;emphasis english;level emphasis english;translation systems allow;speech features f0;expressed individual languages;emphasis using;emphasis expressed individual;emphasis expressed;level emphasis using;emphasis;considering emphasis;speech features;languages based estimation;s2s;expressed language languages;using speech;expressed language;level emphasis;individual languages", "pdf_keywords": ""}, "262c0e54370dfc03a7ad53d79930568d18dd448c": {"ta_keywords": "computation speed distributed;distributed matrix multiplication;coded computation speed;speed distributed matrix;matrix cached worker;computation speed;tail coded computation;coded computation;distributed matrix;speeding distributed;speeding distributed machine;runtime subtask exponential;users speeding distributed;matrix multiplication use;performance distributed;communication bottlenecks;matrix cached;matrix multiplication;multiplication use codes;communication bottlenecks little;coded algorithms;codes large scale;data matrix cached;large scale systems;performance distributed machine;affect performance distributed;speed distributed;matrix multiplication factor;computation;subtask exponential", "pdf_keywords": "novel coded computation;redundancy codes computation;coded computation;codes computation speci\ufb01cally;distributed algorithms introducing;coded computation framework;existing distributed algorithms;distributed algorithms;codes computation;distributed data processing;distributed data;communication computation;introducing redundancy codes;relevant communication computation;algorithms introducing redundancy;propose novel coded;communication computation phases;coded;redundancy codes;algorithms matrix multiplication;distributed;computation framework;existing distributed;novel coded;speed existing distributed;computation speci\ufb01cally;algorithms matrix;computation speci\ufb01cally identify;multiplication data shuf\ufb02ing;learning algorithms matrix"}, "14a058a1e41459a30327bb5fb480d51430b6a096": {"ta_keywords": "framework geneid ranking;geneid ranking relaxation;geneid ranking;geneid finding provides;geneid ranking extended;consider geneid ranking;curation process geneid;geneid finding studied;geneid finding task;search framework geneid;geneid finding;ranking relaxation geneid;ranked list genes;framework geneid;consider geneid;geneid;database identifier gene;document geneid finding;abstract consider geneid;process geneid finding;relaxation geneid finding;discussed document geneid;process geneid;list genes;document geneid;identifier gene;identifier gene discussed;graph search;finding provides ranked;biocreative challenge hirschman", "pdf_keywords": ""}, "a6d505a6e46c15ef0d213b9a4349ce2f852be894": {"ta_keywords": "positive examples mixture;negative classifier;negative classifier formally;positiveversus negative classifier;unlabeled examples positive;learning desired positive;versus negative classifier;positive examples unlabeled;negative classifier paper;mixture proportion estimation;positive negative classes;estimate learning desired;estimate learning;improving mixture proportion;negative classes;classifier;estimate accurate positiveversus;estimation pu learning;examples mixture;examples positive negative;negative classes hope;significantly improving mixture;examples positive;unlabeled examples;best bin estimation;improving mixture;examples mixture proportion;proportion estimator classifier1;classifier formally;examples unlabeled examples", "pdf_keywords": "best bin estimation;bin estimation bbe;bin estimation;examples bin bias;bin bias;bin bias depends;estimation bbe effective;binary classi\ufb01cation datasets;sample statistical guarantees;learning cvir objective;rates ii learning;techniques best bin;objective pu learning;estimation bbe;estimation bbe mpe;ii learning conditional;learning conditional;bbe pu learning;best bin;statistical guarantees;learning conditional value;datasets pu learning;examples training epoch;statistical guarantees achieving;nearly pure bin;examples bin;pure bin suf\ufb01cient;pure bin;learning cvir;examples training"}, "b08545e1281c1eb748e4474687eb61fd3b25d1a6": {"ta_keywords": "novelty lexical;word innovation types;class novelty lexical;word innovation;contextual prediction novelty;annotated class novelty;dataset novel words;novelty lexical derivation;novel words new;times word innovation;innovation types dataset;prediction novelty;compounding nytwit dataset;words published new;york times word;class novelty;novelty;words new;prediction novelty class;novel english words;novelty class;nytwit dataset novel;words new york;novel words;innovation types;state art nlp;compounding nytwit;nlp;lexical;types dataset nytwit", "pdf_keywords": "lexical enrichment;linguistic novelty class;analysis lexical enrichment;lexical enrichment serve;linguistically informed annotation;contexts linguistic novelty;words contextual information;annotated word;linguistic novelty;task classifying words;classifying words;novelty class annotations;oovs contexts linguistic;linguistically informed categories;context nyt corpus;linguistic analysis lexical;words contextual;novelty context nyt;novel words contextual;words categories based;english relative corpus;classifying words categories;nlp systems;data linguistic;contexts linguistic;corpus;dataset novel words;relative corpus;words categories;corpus articles"}, "43fae0a7af211d91557d115d2f82e3c46d8bf022": {"ta_keywords": "automatic evaluation nlg;language generation nlg;generation nlg;evaluation nlg particularly;natural language generation;generation nlg spans;evaluation nlg;compression summarization transduction;nlg tasks;nlg tasks need;different nlg tasks;nlg tasks including;nlg particularly challenging;information change nlg;dialog natural language;evaluating natural language;change nlg tasks;summarization style transfer;compression summarization;automatic alignment prediction;nlg particularly;different nlg;language generation complexity;change nlg;summarization transduction;nlg spans broad;dialog automatic alignment;aspects different nlg;language generation;nlg spans", "pdf_keywords": "perspective nlg evaluation;diverse nlg tasks;nlg evaluation;nlg tasks;key aspects nlg;characterizing diverse nlg;summarization style transfer;evaluation framework nlg;nlg tasks based;language generation tasks;unifying perspective nlg;nlg evaluation lens;aspects nlg tasks;diverse nlg;nlg tasks categorized;annotation datasets summarization;framework nlg tasks;nlg tasks leads;diverse tasks summarization;perspective nlg;summarization relevance consistency;aspects nlg;framework nlg;language generation;summarization relevance;generation tasks quality;including summarization relevance;tasks summarization style;tasks summarization;generation tasks"}, "b0894f5c914cd90cc3b3e16b15bec11efe317b14": {"ta_keywords": "promotions peer assessment;peer assessment tasks;peer assessment;peer assessment task;peer grading;various peer assessment;peer grading exams;including peer grading;assessment task competitive;competitive students graded;promotions peer;exams homeworks peer;hiring promotions peer;agents evaluate;peers output rankings;evaluations order improve;evaluations demonstrate strong;guarantees evaluate detection;experiment elicits strategic;grading;assessment tasks;homeworks peer review;students graded;grading exams;homeworks peer;incentivized misreport evaluations;tasks including peer;evaluate detection;alarm guarantees evaluate;evaluate subset peers", "pdf_keywords": "manipulations peer assessment;test detecting strategic;peer assessment;behavior peer assessment;detecting strategic behaviour;peer assessment peer;test detection strategic;assessment peer grading;peer assessment setup;peer grading;assessment peer;detecting strategic;strategic behavior peer;strategic manipulations peer;detection strategic manipulations;peer grading homeworks;experiment elicits strategic;rankings paper investigate;detection strategic;behavior peer;assessment setup rankings;manipulations peer;test detection;strategic behaviour subjects;test detecting;strategic behavior;elicits strategic behaviour;grading;grading homeworks;test answers"}, "f481d6dea08e348cecd5eb23a813d47373e62a94": {"ta_keywords": "programming natural language;programs natural language;natural language code;natural language computers;natural programming languages;natural language specifications;modeling natural language;automatic explanation programs;natural language;natural language interact;natural language elements;tutorial aimed nlp;language code modeling;code modeling natural;programming languages provide;use natural language;code language;explanation programs natural;communication collaborative programming;explanation programs;collaborative programming;programming languages;language automatic generation;programming natural;code language automatic;natural language focused;collaborative programming communities;programming communities;natural language allowing;language code language", "pdf_keywords": ""}, "c7424d651d60ef9f052e91bff18efd88782225a3": {"ta_keywords": "election breaking ties;winner election parallel;control election breaking;ties stage voting;election parallel universes;controlling result election;election parallel;control election;ties choose winners;hard control election;election breaking;deciding winner election;winners problem trivially;ties strategically problem;result election breaking;breaking ties strategically;problem deciding winner;chair election;break ties choose;election asked break;breaking ties prove;ties strategically;winners problem;choose winners problem;winner election;deciding winner;break ties;election;ties prove np;chair election asked", "pdf_keywords": "election breaking ties;control election breaking;ties stage voting;winner election parallel;controlling result election;election parallel universes;elections prove np;election parallel;multi round elections;control election;election breaking;hard control election;ties choose winners;winners problem trivially;round elections prove;result election breaking;rule chair election;deciding winner election;round elections;chair election;elections;election asked break;elections prove;problem deciding winner;breaking ties strategically;break ties ensure;winners problem;break ties choose;election;choose winners problem"}, "19a6e362840d3a2d27d0fa5509eaa4d4597a2859": {"ta_keywords": "adversarial perturbations sequence;preserving adversarial perturbations;adversarial perturbations;preserving adversarial;meaning preserving adversarial;adversarial;sequence sequence models;sequence models;perturbations sequence sequence;perturbations sequence;models;perturbations;sequence;sequence sequence;preserving;meaning preserving;meaning", "pdf_keywords": ""}, "b145a46718f293429054f0a9a4cdd2de94813b37": {"ta_keywords": "generation web search;web search;analy sis algorithms;link analy sis;successful link analy;search;web search setting;describes successful link;sis algorithms;search setting sites;link analy;sis algorithms state;algorithms;search setting;successful link;sites survey;generation web;algorithms state art;sites survey describes;sis;web;analy sis;sites;algorithms state;link;setting sites survey;setting sites;survey describes;state art;describes", "pdf_keywords": ""}, "5ea3c08614e9673a109f581cf114af488f3aa601": {"ta_keywords": "automatic embryo staging;embryo staging weakly;staging weakly supervised;embryo embryo staging;embryo staging;loss noting embryos;automatic embryo;method automatic embryo;dynamically decoded predictions;residual networks;weakly supervised region;classifier isolate embryo;noting embryos;supervised region selection;noting image embryo;supervised region;learning improve downstream;embryo;image embryo;noting embryos reaching;embryos;accuracy transition prediction;vanilla residual networks;embryo embryo;improve downstream classifier;downstream classifier;residual networks rival;decoded predictions;region proposal network;embryos reaching", "pdf_keywords": "predictions sequence level;automatic embryo staging;weakly supervised embryo;sequence morphokinetic stages;stage prediction;supervised embryo;method automatic embryo;predictions sequence;supervised embryo detection;process predictions sequence;accuracy transition prediction;decoding monotonic predictions;automatic embryo;transition prediction;embryo staging;stage prediction paper;sequence morphokinetic;corresponding embryo;observe embryo;monotonic predictions based;embryo detection motivating;videos extracted embryoscopetm;sequence level;embryo;predictions based \ufb01nding;relevance stage prediction;embryo detection;temporal context video;increasing sequence morphokinetic;vanilla residual networks"}, "6ab36d2577f7c9487b28b2bcdf236191ba901aad": {"ta_keywords": "task oriented dialogs;dialogs;dialogs experiments flonet;dialog;end learning flowchart;dialogs experiments;dialog mimics troubleshooting;learning flowchart grounded;dialog mimics;oriented dialogs;oriented dialogs experiments;dialogs tod;oriented dialogs tod;learning task oriented;end learning task;utterance flowchart explicit;technical challenges neural;grounding utterance flowchart;flowchart grounded task;tod dialog mimics;challenges neural tod;utterance flowchart;learning flowchart;dialogs tod dialog;flowchart explicit annotation;end end learning;test time dialogs;tod dialog;challenges neural;learning task", "pdf_keywords": "task oriented dialogs;task oriented dialog;learning task oriented;dialogs;dialog tod;dialogs tod;learning tod troubleshoots;dialog;dialog tod troubleshooting;dialog mimics troubleshooting;oriented dialogs tod;task endto;oriented dialog tod;oriented dialogs;tod dialog;dialogs tod dialog;novel task endto;grounded task oriented;task endto end;task oriented;end learning tod;learning task;oriented dialogs dinesh;tod dialog mimics;end learning flowchart;dialogs dinesh;dialog mimics;end learning \ufb02owchart;endto end learning;oriented dialog"}, "1b114486d67252ff83fc90d4a8607636045c54ce": {"ta_keywords": "language code retrieval;code retrieval;code summarization data;source code corpora;natural language code;code corpora;parallel natural language;retrieval code summarization;code summarization;code corpora stack;code retrieval code;nl code tasks;language source code;extracted snippets correspondence;natural language source;code synthesis natural;snippets correspondence features;like code synthesis;structure extracted snippets;code synthesis;source code;synthesis natural language;corpora stack overflow;nl code;learning parallel natural;snippets correspondence;extracted snippets;correlation nl code;learning parallel;corpora stack", "pdf_keywords": ""}, "518cb6d4247bdebf21e2811f296b0c7372602a0a": {"ta_keywords": "masked language models;pretraining masked language;masking allows mlm;masks token gram;masking tokens uniformly;bert pmi masking;masking tokens;performance masking tokens;jointly masks token;masked language;pretraining masked;jointly masks;mlms bert;masking principled masking;masking;masking principled;models mlms bert;principled masking;uniform masking allows;principled masking strategy;masks token;masking strategy;token gram;mlm minimize training;masking allows;uniform masking;language models mlms;mlms bert pmi;masking correlated;principled masking correlated", "pdf_keywords": "performance prior masking;collocations mask jointly;prior masking approaches;masking allows mlm;prior masking;collocations mask;masking approaches;masked subset text;masks token gram;masking reaches performance;mask jointly;masking;predicting masked subset;task predicting masked;masking spans;masking strategy;identify collocations mask;masking principled masking;predicting masked;jointly masks;approach masking spans;masking uses pointwise;jointly masks token;masking spans consistently;approach masking;masking strategy based;pled approach masking;principled masking strategy;masking principled;dubbed pmi masking"}, "aa9e0bf1e22563fca053578315b857688a0817cb": {"ta_keywords": "completion dialogues simulator;dialogues reinforcement learners;example dialogues reinforcement;dialogues simulator;dialogues reinforcement;dialogues simulator supports;dialogue systems;user simulator task;agents master simulator;simulator task completion;task oriented dialogue;simulator based corpus;task completion dialogues;oriented dialogue systems;dialogue systems obstacles;user simulator;movie seeking agents;reinforcement learners;dialogues;user simulator based;completion dialogues;simulator supports tasks;example dialogues;dialogue;simulator task;reinforcement learners typically;data user simulator;simulator designed movie;simulator;corpus example dialogues", "pdf_keywords": "dialogues user simulator;simulator operate dialog;task oriented dialogue;user action generation;user simulation task;simulation task completion;user simulation;dialogue systems;task completion dialogues;user simulator task;oriented dialogue systems;completion dialogues user;based user simulation;user simulation requires;simulator task completion;dialogue history user;literature user simulation;user simulation approach;dialog act;sequence natural language;user simulator;natural language generation;dialog act natural;completion dialogues;task completion dialogue;dialogue systems obstacles;level user simulator;2017 user simulator;dialogues user;user simulator operate"}, "6b4ca249b3b28d3fee65f69714440c08d42cee64": {"ta_keywords": "gan training convergent;convergence gan training;gans actually converge;local convergence gan;convergence gan;wasserstein gans wgan;training methods gans;continuous unregularized gan;unregularized gan training;results general gans;wasserstein gans;gan training;general gans prove;gan training absolutely;general gans;gans wgan gp;unregularized gan;gans wgan;methods gans;gans prove local;gans prove;high resolution generative;hand wasserstein gans;methods gans actually;gans;generative image models;gans actually;gan;resolution generative;generative image", "pdf_keywords": "gan training convergent;continuous unregularized gan;stabilize gan training;unregularized gan training;gans actually converge;gan optimization locally;gan optimization;stabilize gan;wasserstein gans wgan;training methods gans;based gan optimization;unregularized gan;gan training;gan training based;gan training training;wasserstein gans;results general gans;proposed stabilize gan;gans wgan gp;general gans prove;stability problems gan;gradient based gan;problems gan training;gans wgan;general gans;methods gans;gans prove local;gans prove;hand wasserstein gans;based gan"}, "993c184553c41ca9134f149a3eb71b5bfab298b5": {"ta_keywords": "collective threats hong;collective threats;narrative network information;campaigns particular coordinated;threats hong kong;threats hong;distinct narrative network;international reputation attacking;hong kong protestors;strength global crisis;groups accounts engaged;narrative network;narrative maneuvers;methodology disclosed chinese;kong protestors projecting;characterizing;broader digital campaign;digital campaigns;chinese international reputation;narrative maneuvers employing;campaign network analytics;campaigns;campaign network;accounts twitter;disclosed chinese;digital campaign network;digital campaign;disclosed chinese state;kong protestors;view modularity clustering", "pdf_keywords": ""}, "6916118de98cb5293425c8f74919395a003e6076": {"ta_keywords": "flipper text categorization;text categorization;methods text categorization;text categorization compare;classifying text;text categorization task;categorization compare propositional;classifying text pre;classify english textwith;task classifying text;ilp methods text;learning classify english;categorization compare;categorization;categorization task classifying;eeectiveness ilp methods;classify english;categorization task;categories learning;classifying;learning classify;categories learning classify;foil propositional counterparts;eeectiveness ilp;ilp methods;deened categories learning;evaluate eeectiveness ilp;english textwith;classify;compare propositional analogs", "pdf_keywords": ""}, "affdfafb0293b44412ec99ff39b114de5e83eb98": {"ta_keywords": "hydraulic fracturing processes;post hydraulic fracturing;hydraulic fracturing;unconventional tight reservoirs;fracturing processes unconventional;fracturing processes;tight reservoirs;reservoirs;fracturing;analyses post hydraulic;post hydraulic;depth analyses post;processes unconventional tight;depth analyses;hydraulic;processes unconventional;depth;analyses post;processes;analyses;unconventional tight;unconventional;post;tight", "pdf_keywords": ""}, "88347f9f12b50590f50aefce4cf71b3a3f0bd138": {"ta_keywords": "language grounding 3d;language instructions autonomous;instructions autonomous agents;language grounding;oriented language grounding;language grounding proposed;3d game engine;gated attention architectures;attention architectures task;instructions autonomous;autonomous agents need;attention architectures;grounding 3d environments;environment gated attention;natural language instruction;reinforcement imitation;learns policy execute;game engine simulate;reinforcement imitation learning;grounding 3d;execute natural language;end trainable neural;using gated attention;attention mechanism learns;trainable neural;standard reinforcement imitation;trainable neural architecture;language instruction input;task oriented language;learns policy", "pdf_keywords": "language grounding 3d;imitation learning architecture;learning imitation;attention mechanism multimodal;fusion representations verbal;gated attention learns;language grounding;3d environment reinforcement;oriented language grounding;imitation learning;learning imitation learning;mechanism gated attention;language grounding raw;interactions instruction image;reinforcement learning imitation;gated attention mechanism;instruction image representation;multimodal fusion representations;multimodal fusion;attention learns;gated attention;3d game engine;learns joint state;trainable neural;novel gated attention;representations verbal visual;grounding 3d environments;multimodal;learning architecture;language grounding rich"}, "5e10a61b34867c6e5b32ed7a1359bd47bbfb5e2d": {"ta_keywords": "explanation based learning;learning calledabductive explanation;explanations training;explanations training example;calledabductive explanation based;incorrect explanations constructed;explanations constructed;possible explanations training;applying explanation based;explanations constructed shown;based learning calledabductive;incorrect incorrect explanations;multiple inconsistent explanation;calledabductive explanation;inconsistent explanation problem;learning calledabductive;choose possible explanations;inconsistent explanation;incorrect explanations;learning ebl solves;explanation based;learning ebl imperfect;explanations;problem domain theories;explanation problem domain;theories themultiple inconsistent;imperfect theories;based learning ebl;based learning solution;imperfect theories themultiple", "pdf_keywords": ""}, "e3862b1ff18dbb6a421b9efd1c0db22e09644b6d": {"ta_keywords": "co2 fluxes wetland;wetland ecosystems european;fluxes wetland ecosystems;wetland ecosystems;ecosystems european russia;fluxes wetland;co2;co2 fluxes;ecosystems european;wetland;ecosystems;european russia;fluxes;russia;european", "pdf_keywords": ""}, "6f69fcacdf53a811ef18c5e9ac8ec58035dc43fc": {"ta_keywords": "network transducer rnn;transducer rnn;transducer rnn objective;training recurrent;training recurrent neural;neural network transducer;sequence transduction graph;generalizes rnn loss;recurrent neural network;sum training recurrent;speech recognition asr;recurrent neural;sequence transduction;generalizes rnn;ctc objective rnn;function generalizes rnn;rnn loss uses;rnn loss;objective rnn loss;transducer losses restricting;transducer losses;transducer based asr;standard rnn;speech recognition;recognition asr systems;automatic speech;graph based supervision;recognition asr;rnn;objective rnn", "pdf_keywords": "lattice training gtc;training lattices;lattice training;equivalent rnn based;gtct standard rnn;manipulate training lattices;equivalent rnn;generalizes rnn loss;outperforms standard rnn;gtc based asr;ctc like lattice;standard rnn ctc;based asr ctc;rnn ctc example;better optimization decoding;training lattices studying;compared standard rnn;like lattice training;generalizes rnn;function generalizes rnn;standard rnn;transducer based asr;optimization decoding;rnn ctc;standard rnn terms;asr ctc like;practical issue rnn;relative equivalent rnn;decoding procedure gtct;rnn terms"}, "31dc1e65d61a431964c75bf2eec167bcd9dca0fa": {"ta_keywords": "learning approximate control;approximate control rules;approximate control;learning approximate;rules high utility;control rules high;control rules;high utility;utility;control;learning;approximate;rules high;rules;high", "pdf_keywords": ""}, "86471bf927401bf88af83626797228c2bf10a282": {"ta_keywords": "causal attribution social;interpretation social attribution;attribution human behavior;decisions causal attribution;explanations proposed causal;causal attribution attribution;accurate attribution causality;attribution causality;attribution steps causal;causal attribution;attribution causality model;attribution social;attribution social attribution;steps causal attribution;social attribution;faithfulness accurate attribution;social attribution steps;explaining behavior;social attribution complete;attribution attribution human;behavior interpretation social;accurate attribution;faithful causal;attribution attribution;attribution;faithful causal chains;explanations proposed;faithfulness faithful causal;attribution human;human behavior interpretation", "pdf_keywords": "intelligence explanations propose;arti\ufb01cial intelligence explanations;intelligence explanations;explanations predict;explanations propose;explanations proposed causal;explanations proposed;explanations propose classi\ufb01cation;contrastive explanations rede\ufb01ne;contrastive explanations;characterize useful explanation;highlight explanations proposed;highlight explanations;using contrastive explanations;causal attribution social;highlights explanations predict;explaining behavior;social attribution interpretation;explanation argue social;explanations;argue social attribution;explaining behavior reformulate;explanations rede\ufb01ne faithfulness;accurate attribution causality;attribution interpretation method;explanations rede\ufb01ne;social attribution intent;attribution intent causal;aligned social attribution;causal attribution"}, "aa30949af5b59624224980e7d741ad8c084271ec": {"ta_keywords": "vaccine related tweets;vaccination including conspiracy;propagating misinformation vaccination;tweets posted covid;misinformation vaccination including;misinformation vaccination;discussed vaccinations posted;covid 19 outbreak;videos shared vaccine;vaccinations posted;covid 19 conversations;autism pandemic conspiracy;counteract misinformation spread;pandemic conspiracy theories;pandemic conspiracy;19 conversations covid;conversations covid 19;trust vaccination;misinformation spread;discussed vaccinations;claiming vaccines cause;impact trust vaccination;posted covid 19;19 outbreak public;covid 19 vaccine;conversations covid;vaccinations posted february;outbreak public;propagating misinformation;claiming vaccines", "pdf_keywords": ""}, "d35534f3f59631951011539da2fe83f2844ca245": {"ta_keywords": "training generative adversarial;adversarial networks jointly;spaces generative adversarial;generative adversarial networks;generative adversarial;training generative;learns latent codes;algorithm training generative;face verification;latent spaces generative;latent codes identities;latent codes generate;adversarial;face verification demonstrate;identity matched photographs;adversarial networks;generative;adversarial networks experiments;latent codes;networks jointly learns;codes generate diverse;generate diverse images;jointly learns latent;diverse images;jointly learns;diverse images subject;codes identities;pairs photorealistic;spaces generative;learns latent", "pdf_keywords": "semantically decomposed gans;decomposed generative adversarial;decomposed gans;semantically decomposed generative;training generative adversarial;training generative;algorithm training generative;sd gans trained;decomposed generative;generative adversarial networks;spaces generative adversarial;gans trained;generative adversarial;decomposed gans sd;gans sd gans;adversarial networks jointly;generative;demonstrate sd gans;latent spaces generative;gans new algorithm;gans sd;presented sd gans;gans;sd gans;adversarial networks chris;gans trained faces;sd gans encourage;gans encourage speci\ufb01ed;networks jointly learns;adversarial"}, "61a07d1e4eaa831152e253b96b91808ef3a184b4": {"ta_keywords": "crowdsourcing natural language;data annotation crowdsourcing;annotation crowdsourcing;annotation crowdsourcing shared;label collection crowdsourcing;crowdsourcing natural;crowdsourcing marketplaces present;crowdsourcing;crowdsourcing marketplaces make;labeling public crowdsourcing;collection crowdsourcing natural;crowdsourcing marketplaces;crowdsourcing shared;largest crowdsourcing;public crowdsourcing marketplaces;public crowdsourcing;project largest crowdsourcing;introduction data labeling;largest crowdsourcing marketplaces;collection crowdsourcing;language data annotation;data annotation;crowdsourcing shared leading;data labeling;natural language data;data labeling public;efficient natural language;annotation;labeling;natural language", "pdf_keywords": ""}, "aead4418733b998792deb9cbf198a834449e00d2": {"ta_keywords": "systematic generalization symbolic;neural sequence models;generalization symbolic;sequence models trained;generalization symbolic mathematics;systematic generalization;sequence models systematic;models systematic generalization;symbolic brittleness sequence;sequenceto sequence models;evaluating generalization takes;generalization develop;generalization carefully constructed;difficulty generalizing;neural sequence;evaluating generalization;sequence models;sequence models domain;generalization takes;highlights difficulty generalizing;generalization;generalizing;methodology evaluating generalization;generalization carefully;generalization takes advantage;brittleness sequence models;generalization develop methodology;distribution generalization;modeling learning;outof distribution generalization", "pdf_keywords": "generalization sequence models;neural sequence models;systematic generalization sequence;generalization sequence;sequenceto sequence models;sequence models assigning;conventional neural sequence;sequence models;neural sequence;systematic generalization;sequence models domain;generalization carefully constructed;evaluating generalization;evaluating generalization takes;achieving robustness compositionality;generalization introduction;generalization;compositionality distribution generalization;sequence models applied;generalization takes;adversarial problem sets;performance sequenceto sequence;unrealistic sequences;building adversarial;generalization required task;outof distribution generalization;generalizing;robustness compositionality outof;distribution generalization;probabilities unrealistic sequences"}, "9f7e317c6ef0bb15aacc9b19f0f0d00fee6c9a36": {"ta_keywords": "subsampled influence memorization;influence memorization values;influence memorization;memorization training data;memorization training;memorization values training;memorization values estimated;estimation influence training;fitting requires memorization;example memorization;memorization;requires memorization training;training example accuracy;deep learning algorithms;deep learning;test example memorization;example memorization values;memorization values;requires memorization;learning algorithms known;theory deep learning;training examples recent;training examples;example accuracy;influence training example;training data labels;learning algorithms;subsampled influence;learning;training data", "pdf_keywords": "neural networks memorize;memorize training examples;learning algorithms memorize;networks memorize training;test example memorization;memorize labels estimation;example memorization;training example accuracy;estimation memorization;memorization generalization;memorization generalization standard;memorization values training;resampling randomness training;algorithms memorize seemingly;example memorization values;estimation memorization in\ufb02uence;demonstrates accuracy learning;deep learning algorithms;ability memorize labels;networks memorize;memorization;bene\ufb01ts memorization generalization;deep learning;randomness training;memorize training;learning algorithm long;memorize seemingly;algorithms memorize;memorize labels;memorization values"}, "a1a8eeb64c0846070b10531061c18fed6d566f8c": {"ta_keywords": "strategic voting tie;voting tie breaking;complexity manipulation tie;ties matter complexity;vote ties;tied candidates random;vote ties matter;random vote ties;computing manipulating vote;voting tie;tied candidates;manipulation tie breaking;order tied candidates;manipulating vote;strategic voting;manipulation tie;breaking random vote;tie breaking random;candidates random vote;impact strategic voting;manipulating vote ask;complexity manipulation;rules computational complexity;relationship computational complexity;complexity computing manipulating;ties matter;complexity computing manipulation;computational complexity;random vote;candidates random", "pdf_keywords": ""}, "c4919feb50c514e32eb0f4131399180c6f9a0d7d": {"ta_keywords": "procurement cost function;allocation procurement costs;procurement cost functions;resource allocation procurement;online resource allocation;procurement costs design;allocation procurement;surrogate function procurement;posted pricing mechanism;polynomial procurement cost;allocation multiple customers;procurement costs;procurement cost;function procurement cost;cost total allocation;pricing mechanism;polynomial procurement;cost function objective;pricing mechanism algorithm;procurement cost total;optimal surrogate function;procurement cost analyze;increasing cost function;facing procurement cost;costs design method;optimal surrogate;competitive ratio online;focuses polynomial procurement;procurement;function procurement", "pdf_keywords": "surrogate function procurement;designing surrogate polynomial;optimal surrogate function;optimization design surrogate;polynomial procurement cost;quasiconvex optimization design;surrogate polynomial functions;quasiconvex optimization \ufb01nd;polynomial procurement;quasiconvex optimization;design surrogate function;exploit quasiconvex optimization;uses quasiconvex optimization;procurement cost function;procurement cost functions;focuses polynomial procurement;function procurement cost;optimal surrogate;cost functions particular;primal dual algorithm;surrogate polynomial;cost functions;optimization design;competitive ratio primal;uses optimal surrogate;\ufb01nd optimal design;ratio primal dual;optimization \ufb01nd;dual algorithm;dual algorithm \ufb01rst"}, "df56ccda14b5bc255a07fc061c50839e75563c5a": {"ta_keywords": "game parking traffic;routing game parking;parking traffic;parking related traffic;traffic selects parking;parking traffic selects;parking traffic impact;queue routing game;parking classical routing;game parking;traffic traffic;congestion route choices;traffic;queuing game model;congestion route;selects parking;routing game model;overall congestion route;parking zones;varying cost parking;queuing game;queue routing;traffic impact;routing game;new routing game;related traffic traffic;selects parking zone;traffic selects;parking related;parking", "pdf_keywords": ""}, "4218563e1fe927440e00bf0abe5cb1e037deaf71": {"ta_keywords": "confidence predicting accuracy;predicting accuracy;target domain accuracy;method learns threshold;learns threshold model;predicting target domain;learns threshold;methods predicting target;predict distribution performance;machine learning deployments;model confidence predicting;confidence predicting;domain accuracy;threshold model confidence;predicting target;data predict distribution;unlabeled data predict;thresholded confidence;methods predicting;general identifying accuracy;practical method learns;predicting accuracy fraction;predict distribution;thresholded confidence atc;mnist leveraging unlabeled;confidence exceeds threshold;method learns;source training target;leveraging unlabeled data;data predict", "pdf_keywords": "target domain accuracy;predicting target domain;con\ufb01dence predicting accuracy;machine learning deployments;methods predicting target;learns threshold model;method learns threshold;learns threshold;predict distribution performance;estimating target domain;predicting accuracy;predicting target;domain accuracy based;leveraging unlabeled;leveraging unlabeled data;source training target;accuracy based unlabeled;2022 leveraging unlabeled;domain accuracy;target test distributions;unlabeled data predict;training target;data unlabeled target;unlabeled target data;based unlabeled target;practical method learns;training target test;mismatches source training;learning deployments characterized;domain accuracy using"}, "2d6d26c118f43f3ab314d07f58c20df6e89a13af": {"ta_keywords": "particles vaccine;generation influenza virus;driven generation influenza;plasmid driven generation;influenza virus;efficient plasmid driven;like particles vaccine;plasmid driven;generation influenza;efficient plasmid;influenza virus like;vaccine;virus like particles;influenza;plasmid;particles;virus;virus like;like particles;driven generation;generation;driven;like;efficient", "pdf_keywords": ""}, "92a8f7f09f3705cb5a6009a42220a6f01ea084e8": {"ta_keywords": "actionable knowledge language;extracting actionable knowledge;actionable steps;actionable knowledge;existing demonstrations semantically;act interactive;actionable steps conducted;learning explicit step;demonstrations semantically;act interactive environments;set actionable steps;high level tasks;demonstrations semantically translates;used act interactive;extracting actionable;level tasks expressed;large language models;tasks expressed natural;language models;level tasks;language models llms;knowledge language models;plans admissible actions;learned large language;language models evaluation;actions;knowledge language;tasks mid level;learning explicit;level plans training", "pdf_keywords": "high level tasks;level tasks expressed;capable generating programs;level tasks;tasks mid level;level plans training;generating programs highly;generating programs;actionable steps;level plans;mid level plans;tasks expressed natural;level tasks mid;natural language make;learning explicit step;tasks expressed;expressed natural language;maintaining reasonable expressiveness;actionable steps prior;expressiveness measured program;pre trained lms;set actionable steps;step examples act;program;high level;natural language;plans training proposed;tasks;plans training;learning explicit"}, "59653e5cfa854a17c2ffcb86f2a454f27e12c716": {"ta_keywords": "machine translation search;diversity machine translation;neural machine translation;diversity human translations;machine translation;translation search strategies;bias translating gender;translating gender pronouns;translation search;human translations;machine translation nmt;human translations study;translating gender;generated translations;bias translating;known bias translating;agreement generated translations;translations examining;generated translations ground;real translations examining;translations study;generated real translations;translations;translation nmt systems;translations ground;metrics nlp researchers;gender pronouns;translations ground truth;translations examining cost;translation nmt", "pdf_keywords": "diversity machine translation;2020 decoding diversity;decoding diversity;pronouns battery diversity;decoding diversity machine;diversity diagnostics reveal;diversity metrics introduce;diversity diagnostics;machine translation nmt;diversity metrics;neural machine translation;diversity nmt outputs;diversity nmt;translation nmt systems;diversity diagnostics nmt;diversity machine;lack diversity nmt;machine translation;outputs relates translation;2020 decoding;agreement generated translations;generated translations;diversity bleu examine;diversity bleu diversity;translations examining;truth translations grams;diversity;translations grams;translation nmt;bleu diversity diagnostics"}, "ac41e0ef30b6f9ee4930ac85dc46a9b50a1963d2": {"ta_keywords": "crowdsourced labels;quality crowdsourced labels;training data crowdsourcing;crowdsourced labels adversely;voting incentives crowdsourcing;incentives crowdsourcing;incentives crowdsourcing growing;data crowdsourcing;quality crowdsourced;crowdsourcing important machine;crowdsourced;data crowdsourcing important;crowdsourcing;crowdsourcing important;crowdsourcing growing;experts incentives;crowdsourcing growing need;experts incentives workers;labeled training data;workers experts incentives;labeled training;approval voting incentives;labeled;voting incentives;need labeled training;labels;incentives;proper incentive;incentives workers;strictly proper incentive", "pdf_keywords": ""}, "76f02d20e02c6baf39fee8f115cd94e4ceacf32b": {"ta_keywords": "potential bias reviewers;bias reviewers;reviewers exhibit bias;bias reviewers recommendations;reviewers conferences;competent reviewers growing;reviewers negatively biased;burden reviewers conferences;reveals reviewers negatively;reviewers growing slower;analysis reveals reviewers;novice reviewers constitute;reviewers growing;novice reviewers;population novice reviewers;reveals reviewers;peer review;quality peer review;investigate reviewers;competent reviewers;investigate reviewers exhibit;reviewers receive;peer review pipeline;reviewers conferences started;reviewers negatively;reviewers exhibit;reviewer pool;reduce burden reviewers;number competent reviewers;reviewers", "pdf_keywords": ""}, "bf0beed35ea09aab56027d64c744098cc963fbde": {"ta_keywords": "dynamic cusum algorithm;change detection transient;detection transient dynamics;dynamic cumulative sum;transient phases algorithms;adaptive unknown transient;transient durations numerical;transient durations;unknown transient durations;considered dynamic cumulative;durations transient;dynamic cumulative;instantaneously series transient;durations transient phases;alarm durations transient;detect change quickly;detection transient;weighted dynamic cusum;quickest change detection;dynamic cusum;series transient;algorithms considered dynamic;series transient phases;cusum algorithm proposed;transient dynamics;cusum algorithm;sum cusum algorithm;transient dynamics studied;change detection;detect change", "pdf_keywords": "change detection qcd;qcd transient dynamics;detection qcd transient;change detection transient;qcd transient;detection transient dynamics;qcd transient post;problem qcd transient;quickest change detection;transient persistent;transient persistent phases;change detection;wd cusum algorithm;transient dynamics;detection transient;transient dynamics studied;transient post change;performance transient persistent;detection qcd;persistent distribution;change dynamics;change dynamics pre;dynamics studied change;qcd;transient dynamics theory;\ufb01nal persistent distribution;dynamics pre change;transient phases motivated;pre change distribution;studied change initial"}, "f1005edfa1fbc4ea0d9a90345388bda8a01e69ed": {"ta_keywords": "confluent vessel trees;forming vessel trees;reconstructing confluent vessel;vessel trees using;vessel trees;undirected tubular graphs;tubular graphs producing;tubular graphs limitations;tubular graphs;graph enforcing confluence;vessel trees enforce;curves forming vessel;discrete tubular graphs;arborescence directed graph;confluence simple flow;forming vessel;confluent vessel;minimum arborescence directed;capillary sub voxel;algorithm reconstructing confluent;confluence continuous;confluence continuous oriented;flow extrapolating arc;confluence;arborescence directed;using minimum arborescence;reconstruction accuracy bifurcations;confluence high order;enforcing confluence;minimum arborescence", "pdf_keywords": "con\ufb02uent vessel trees;tubular graphs minimizing;forming vessel trees;vessel trees using;vessel trees accurate;vessel trees;trees accurate bifurcations;tubular graphs;discrete tubular graphs;vessel trees enforce;tubular graphs con\ufb02uence;geodesic tubular graphs;tree structure bifurcation;arborescence directed graph;reconstructing con\ufb02uent vessel;curves forming vessel;minimum arborescence directed;using minimum arborescence;vasculature thousands bifurcations;minimum arborescence;forming vessel;accurate bifurcations;practical algorithm reconstructing;graphs minimizing;graph enforcing con\ufb02uence;structure bifurcation localization;trees using minimum;thousands bifurcations supervision;graphs;tree structure"}, "794b0a1e9719d809ebdf2ef87ff84c2039bfdd52": {"ta_keywords": "qp architecture wirelesshart;wirelesshart protocol stack;wirelesshart software protocol;design wirelesshart protocol;machines wirelesshart protocol;architecture wirelesshart;wirelesshart protocol mainly;wirelesshart protocol;architecture wirelesshart data;functions wirelesshart protocol;state machines wirelesshart;wirelesshart data service;wirelesshart software;design wirelesshart;machines wirelesshart;events paper wirelesshart;wirelesshart data;wirelesshart;functions wirelesshart;corresponding functions wirelesshart;protocol stack based;qp event driven;based qp architecture;protocol stack designed;software protocol stack;software protocol;qp architecture;protocol stack;paper wirelesshart software;based qp event", "pdf_keywords": ""}, "ae30f8fc5a969d2d14ae066db4cd07d86fadbf42": {"ta_keywords": "receptors disclosed recommendation;methionine5 enkephalin sulfoxides;opiate receptors disclosed;agonist activity opiate;activity opiate receptors;opiate receptors;methionine5 enkephalin;sulfones having agonist;activity opiate;opiate;methionine5;disclosed recommendation classification;enkephalin sulfoxides sulfones;recommendation classification;disclosed recommendation;enkephalin sulfoxides;receptors disclosed;sulfones;information recommendation;based information recommendation;agonist activity;sulfoxides sulfones;sulfoxides;recommendation classification using;having agonist activity;receptors;sulfoxides sulfones having;recommendation;sulfones having;agonist", "pdf_keywords": ""}, "ffe1416bcfde82f567dd280975bebcfeb4892298": {"ta_keywords": "end speech recognition;speech recognition connectionist;speech recognition asr;transformer slower rnn;speech recognition;neural networks rnn;networks rnn;slower rnn;recurrent neural networks;recognition asr tasks;automatic speech recognition;slower rnn regards;connectionist temporal classification;rnn best;automatic speech;training decoding;recognition connectionist temporal;recognition asr;networks rnn state;end automatic speech;rnn regards learning;neural network architecture;training transformer slower;training decoding methods;rnn state art;rnn best option;rnn;recognition connectionist;joint training decoding;temporal classification language", "pdf_keywords": ""}, "c50f98961c951fe3fbdb6f375beb28e40a6b0581": {"ta_keywords": "incentives reviewers participate;incentives reviewers;auction review slots;incentivizes participating reviewers;auction review;lack incentives reviewers;vcg auction review;demand reviewers;effortful reviews peer;demand reviewers large;pay reviewers based;pay reviewers;participating reviewers provide;participating reviewers;overwhelming demand reviewers;reviews peer;reviewers participate;reviewers participate expend;quality submissions revenue;reviews peer reviewed;submissions lack incentives;reviewers based quality;paper submission review;reviewers provide;ii pay reviewers;having paper reviewed;participate vcg auction;effortful reviews;review slots;paper reviewed", "pdf_keywords": "review information elicitation;peer review mechanism;predicting peers review;peer review machine;peer review;peer review information;mechanism incentivizes reviewers;review mechanism;stage peer review;review machine learning;incentivizes reviewers;incentivizes participating reviewers;review mechanism consisting;scienti\ufb01c peer review;incentivizes reviewers provide;participating reviewers provide;information elicitation literature;participating reviewers;effortful reviews;effortful reviews present;review machine;information elicitation;elicitation literature incentivizes;review information;peers review;honest effortful reviews;reviewers honest effortful;elicitation literature;review process;information elicitation veri\ufb01cation"}, "3febb2bed8865945e7fddc99efd791887bb7e14f": {"ta_keywords": "deep contextualized word;deep bidirectional language;contextualized word representations;contextualized word representation;polysemy word vectors;word vectors learned;word representations;contexts model polysemy;word representation models;deep contextualized;model polysemy word;deep bidirectional;type deep contextualized;bidirectional language model;word representation;polysemy word;linguistic contexts model;contextualized word;large text corpus;model polysemy;supervision signals deep;word vectors;language model bilm;linguistic contexts;bidirectional language;language model;signals deep contextualized;contextualized;text corpus;polysemy", "pdf_keywords": "contextualized word representations;nlp tasks representations;deep contextualized word;nlp tasks;contextualized word representation;word representations;challenging nlp;word representation models;broad range nlp;challenging nlp problems;deep contextualized;deep context dependent;sentiment analysis deep;range nlp tasks;textual entailment sentiment;context dependent representations;entailment sentiment analysis;nlp problems including;type deep contextualized;deep context;quality deep context;linguistic contexts model;word representation;question answering;nlp problems;question answering textual;nlp;entailment sentiment;linguistic contexts;including question answering"}, "aa2bd932a2ecb6e07c768bcf0dc119f0cd20f6e0": {"ta_keywords": "textual similarity measures;similarity measures matching;textual similarity;learning textual similarity;identifying approximately duplicate;approximately duplicate database;similarity measures;duplicate database records;similarity;duplicate database;matching identifying;combining learning textual;measures matching identifying;database records refer;information integration;essential information integration;records refer entity;authors compare;measures matching;approximately duplicate;matching;matching identifying approximately;authors compare methods;entity essential information;database records;refer entity;records refer;compare methods combining;database;textual", "pdf_keywords": ""}, "85a18aafcffdcc4eafcb9e5eda0abb8aa5cb8c3b": {"ta_keywords": "sdn big data;sdn utilize large;sdn big;sdn architecture;software defined networking;argue sdn big;networking sdn architecture;defined networking sdn;provided sdn hardware;sdn hardware;sdn architecture promises;sdn hardware industry;networking sdn;network architectural support;sdn utilize;particular sdn utilize;network architectural;ossified architecture internet;defined networking;sdn;argue sdn;hadoop provide ability;architecture internet;particular sdn;support provided sdn;architecture internet allowing;hadoop provide;platforms distributed storage;big data platforms;big data technological", "pdf_keywords": ""}, "d89f4534d1a87005cdf470ec5d8154998d5abdc7": {"ta_keywords": "parm uses neural;networks learn parity;learn parity models;parm improves;parm improves overall;queries single parity;prediction serving;object localization parm;parm uses;parity;learn parity;prediction serving extensive;parm;parity query performs;performs inference parity;parity models;approaches parm;using parity;parm encodes;query rates prediction;localization parm encodes;prediction serving systems;extensive evaluation parm;evaluation parm improves;single parity query;prediction serving incur;approaches parm reduces;maintaining median parm;failures prediction serving;inference parity", "pdf_keywords": "prediction serving;prediction serving systems;inference tasks parm;parm overcomes challenges;failures prediction serving;challenges prior coding;ability parm framework;introducing parity models;coding machine learning;slowdowns failures prediction;tasks parm overcomes;fast encoders;parm framework;introducing parity;prediction serv;simple fast encoders;prediction serv ing;techniques introducing parity;reconstruct unavailable predictions;parity models new;coding based resilience;fast encoders decoders;tasks parm;built parm;parm approach;parity models;parm overcomes;efficient resilience slowdowns;parm;source prediction serv"}, "e95a96dec775cc792b763f4eec13343c22e850e1": {"ta_keywords": "officers newsletter editors;activities officers newsletter;newsletter editors chief;newsletter editor chief;activity report acm;sigai acm org;newsletter editors;new newsletter editor;editors chief information;acm org;newsletter editor;report acm;report acm sigai;officers newsletter;editors chief;new information officer;chief information officer;editors scope acm;information officer;editors;acm sigai consists;activities officers;editor chief;information officer currently;editor chief additional;education activities officers;looking new newsletter;acm sigai;acm org thrilled;editor", "pdf_keywords": ""}, "49984bc327ef6952118c4b871eeef2a907f7a4ed": {"ta_keywords": "regularized learning games;classes regularized learning;convergence regularized learning;adversary maintaining faster;regularized learning;regularized learning algorithms;rates adversary maintaining;fast convergence regularized;rates adversary;class fast convergence;bias achieve faster;learning games;faster convergence rates;classes regularized;equilibria multiplayer;converges approximate optimum;game uses algorithm;convergence regularized;tilde rates adversary;algorithms form recency;adversary maintaining;adversary;faster convergence;natural classes regularized;learning algorithms form;correlated equilibria multiplayer;games black box;equilibria multiplayer normal;algorithms class fast;class individual regret", "pdf_keywords": "regularized learning games;hedge costminimization games;class regret algorithms;regret algorithms;costminimization games;convergence played robust;costminimization games worse;regret algorithms enjoy;convergence regularized learning;rapid convergence played;decoupled regret dynamics;fast convergence regularized;convergence played;played robust adversarial;learning games arxiv;adversarial opponents;regularized learning;learning games;classes regularized learning;adversarial opponents appendix;regularized learning algorithms;faster convergence rates;normal form games;hedge costminimization;regret dynamics;faster convergence;convergence regularized;convergence ef\ufb01ciency hedge;fast convergence;robust adversarial opponents"}, "c460fd4a0dc86bc518f9a8e982bc48faf1efb942": {"ta_keywords": "social platforms;social platforms today;online social platforms;like facebook twitter;time broadcast;broadcasts timelines;timeline information exchange;social platform users;broadcasts timelines primary;social platform;time broadcast scheduling;facebook twitter;timing broadcasts;facebook;parametrise social platform;audiences sparked startups;study time broadcast;attention received timing;twitter;broadcast scheduling;marketing gurus broadcasts;like facebook;popularity created increasingly;content maximise attention;timelines primary mechanism;timing frequency publishing;gurus broadcasts timelines;broadcasts capture attention;startups social marketing;services like facebook", "pdf_keywords": "rhythms microblogs;circadian rhythms microblogs;time consecutive tweets;time broadcast scheduling;rhythms microblogs studying;time broadcast;scheduling broadcasts;broadcasts network timelines;broadcast scheduling;broadcasts timelines;microblogs studying distribution;abstract broadcasts timelines;broadcasts timelines primary;network timelines;\ufb01rst time broadcast;microblogs;twitter;timing frequency publishing;existence bursty circadian;frequency publishing content;consecutive tweets;tweets;consecutive tweets sina;scheduling;scheduling broadcasts network;overload bursty circadian;facebook twitter;broadcast scheduling problem;information overload bursty;microblogs studying"}, "1a9c89cb2e57e06dadd4c2fab5fae1bfdbb3b6d5": {"ta_keywords": "preference networks pcp;conditional preference networks;cpnet aggregation;preference networks;probabilistic preference structure;probabilistic preference;voting cp nets;probabilistic conditional preference;perform cpnet aggregation;using probabilistic preference;nets using probabilistic;networks pcp nets;cp nets voting;cpnet aggregation evaluate;propose pcp net;pcp net based;pcp nets;nets voting cp;pcp nets provide;cp nets;pcp net;cp nets using;cp nets single;cpnet;collection cp nets;networks pcp;preference structure propose;perform cpnet;conditional preference;net based methods", "pdf_keywords": ""}, "f5a0c6593ba95d23c025608ce9280848da8b929f": {"ta_keywords": "gene mention task;corresponding gene mentions;results gene mention;sentences corresponding gene;gene mention;identify substrings sentences;gene mentions;gene mentions present;substrings sentences corresponding;systems identify substrings;results gene;substrings sentences;presented results gene;identify substrings;mention task biocreative;substrings;scoring submissions task;task biocreative ii;gene;corresponding gene;task biocreative;scoring submissions;results submissions score;results submissions;mention task;lowest scoring submissions;biocreative ii workshop;mentions present brief;submissions task;brief descriptions methods", "pdf_keywords": ""}, "92891a984b45df5fc764d81bf9bcd42e7e7ed1c7": {"ta_keywords": "network epidemic model;network epidemic;nash equilibrium;multi network epidemic;game nonlinear dynamics;nash equilibrium open;differential game nonlinear;local nash equilibrium;game nonlinear;epidemic model;equilibrium global;epidemic model spread;equilibrium;spread malware;equilibrium open loop;global equilibrium;induced equilibrium global;equilibrium global equilibrium;equilibrium open;model spread malware;epidemic;security multi network;differential game;desired local nash;designing quadratic pricing;network managers invest;loop differential game;quadratic pricing;induced equilibrium;spread malware abstract", "pdf_keywords": ""}, "3cc790174d138d7904189df997d5763f1793dedf": {"ta_keywords": "annotator agreement normalization;agreement normalization task;agreement normalization;inter annotator agreement;labels normalized wordforms;common annotation tasks;annotator agreement;common annotation;normalized wordforms;labels normalized;annotated word forms;annotation tasks;chancecorrected agreement measures;agreement measures;evaluating inter annotator;annotated word;inter annotator;annotations;differs common annotation;way annotated word;annotation;annotation tasks important;annotations match;class labels normalized;normalized;measure inter annotator;annotations match different;normalized wordforms open;normalization task task;normalization task", "pdf_keywords": ""}, "1d255aeabcb87929742280251007fd8c01bbe914": {"ta_keywords": "polyoxometalates cluster pmo11fe;oxidant demonstrated pmo11fe;recyclability pmo11fe;pmo11fe clusters functionalities;superior recyclability pmo11fe;cluster pmo11fe immobilized;pmo11fe immobilized nh2;recyclability pmo11fe sba;interactions pmo11fe clusters;pmo11fe clusters;composite pmo11fe;materials abbreviated pmo11fe;pmo11fe cluster supported;structure pmo11fe cluster;characterize composite pmo11fe;composite pmo11fe sba;iron substituted polyoxometalates;structure pmo11fe;pmo11fe immobilized;cyclooctene epoxidation;pmo11fe cluster;cluster pmo11fe;concluded pmo11fe cluster;polyoxometalates cluster;demonstrated pmo11fe;interactions pmo11fe;pmo11fe cluster kept;process cyclooctene epoxidation;demonstrated pmo11fe sba;cyclooctene epoxidation results", "pdf_keywords": ""}, "737f9a32d7f4007aa9526556c256ed4a182aec69": {"ta_keywords": "nash equilibrium robust;optimal nash equilibrium;socially optimal nash;nash equilibrium;equilibrium robust;linear quadratic game;optimal nash;quadratic game;equilibrium robust small;convex program;induce socially optimal;develop convex;socially optimal;game framed convex;develop convex conditions;convex conditions guarantee;equilibrium stable;equilibrium;quadratic game framed;convex;analysis develop convex;players feedback strategies;equilibrium stable respect;convex conditions;optimal;deviations players feedback;prices induce socially;convex program addition;perturbations linear quadratic;induced equilibrium stable", "pdf_keywords": ""}, "4aa72e4232ae809ea1a9fe142275da25ba930655": {"ta_keywords": "linear quadratic games;convergence nash equilibria;dynamics global nash;global nash equilibria;quadratic games satisfy;quadratic games;policy gradient algorithms;continuous global nash;convergence multi agent;nash equilibria continuous;multi agent learning;gradient avoid nash;local convergence nash;convergence nash;nash equilibria generate;global nash;avoid nash equilibria;solving reinforcement learning;coincide nash equilibrium;nash equilibria solving;quadratic games classic;reinforcement learning setting;policy gradient;nash equilibrium;agent learning;classic reinforcement learning;policy gradient avoid;reinforcement learning problems;nash equilibria;games satisfy conditions", "pdf_keywords": "single agent optimal;agent optimal control;gradient lq games;agent optimal;policy gradient guarantees;linear quadratic games;multi agent learning;policy gradient dynamics;optimization single agent;global convergence nash;policy gradient lq;policy gradient;convergence nash equilibria;game nash equilibrium;neighborhoods nash equilibrium;neighborhood nash equilibrium;quadratic games;gradient dynamics player;agent learning particular;agent learning;performing policy gradient;point policy gradient;nash equilibrium;using policy gradient;convergence nash;nash equilibria game;multi agent competitive;nash equilibrium point;gradient play;nash equilibrium paper"}, "b62ce3135ed6065863c0dec26037fd07c081abba": {"ta_keywords": "counterfactual target label;counterfactual target;counterfactually revised counterparts;language causality offers;language causality;changes language causality;natural language processing;natural language inference;counterfactually revised;natural language;counterfactual;language inference tasks;fail counterfactually revised;focus natural language;inference tasks classifiers;mentions genre models;spurious features mentions;accords counterfactual target;document accords counterfactual;classifiers;counterfactually;fail counterfactually;effects interestingly sentiment;sentiment analysis natural;sensitive spurious features;language inference;causality offers clarity;analysis natural language;sensitive spurious patterns;classifiers trained original", "pdf_keywords": "natural language processing;counterfactually manipulating documents;sentiment dataset;natural language;yelp dataset challenge;twitter sentiment dataset;counterfactually augmented data;sentiment;twitter sentiment;software twitter sentiment;sentiment dataset rosenthal;yelp dataset;yelp reviews;amazon reviews;focus natural language;models amazon reviews;better spurious associations;learning toolkit;released yelp dataset;notion spuriousness;data aggregated genres;language processing;reviews released yelp;statistical learning offers;2017 yelp reviews;distinction spurious;spurious associations yielding;spurious associations;yelp reviews released;counterfactually"}, "ca86a63362e51eea2e213ae2d3faed668ec1ad74": {"ta_keywords": "commonsense concept extraction;commonsense knowledge representation;commonsense expression extraction;commonsense knowledge;extraction semantic similarity;multi word commonsense;word commonsense expression;concept extraction semantic;word commonsense;extraction semantic;based approach commonsense;commonsense expression;approach commonsense;semantic similarity detection;semantic similarity;knowledge bases graph;commonsense concept;approach commonsense concept;concept extraction;commonsense;robust knowledge bases;similarity detection;knowledge bases;similarity detection solutions;knowledge representation reasoning;addition semantic similarity;expression extraction;knowledge bases capable;extraction unrestricted english;knowledge representation", "pdf_keywords": ""}, "a1da1d600acd506b80c8870d293a756c70791683": {"ta_keywords": "bilingual lexicon induction;bilingual lexicons distribution;aligned bilingual lexicons;bilingual lexicons larger;bilingual lexicon;bilingual lexicons;propose bilingual lexicon;lexicon induction semi;work bilingual lexicon;lexicons distribution matching;depended aligned bilingual;lexicon induction bli;limited aligned bilingual;lexicon induction;aligned bilingual;lexicons distribution;recent work bilingual;unaligned word embeddings;bilingual;embedding spaces empirically;word embeddings;induction semi supervision;propose bilingual;distant propose bilingual;work bilingual;semi supervised;assumption isometry embedding;embeddings novel hubness;word embeddings novel;semi supervised approach", "pdf_keywords": ""}, "f184908270fc934ab74438a0aaac7a43a5eae6d2": {"ta_keywords": "neural summarization models;neural summarization;art neural summarization;summaries modeling;summarization models;summarization models traditional;summaries modeling document;document summarization models;summarization models preserve;single document summarization;summarization present extensive;document summarization;document generating summary;document summarization relied;explicit sentence dependencies;analysis summaries modeling;sentence dependencies single;document summarization present;summarization relied;sentence dependencies;summarization relied modeling;summarization;graph sentences incorporate;summarization present;mention graph sentences;generating summary;sentences incorporate;latent sentence relations;summaries;structure document generating", "pdf_keywords": "summarization models augmenting;summarization framework trained;encoderdecoder summarization models;representations summarization models;document representations summarization;encoderdecoder summarization;standard encoderdecoder summarization;summarization models introduce;summarization models;abstractive summaries generating;document summarization models;generates abstractive summaries;structure induction summarization;explicit structure attention;representations summarization;summarization address challenges;structure attention module;human readable summaries;summarization task explicit;summarization models work;explicit attention module;readable summaries end;structured document representations;summarization address;document summarization;single document summarization;summaries generating;structure attention;aware document representations;attention module incorporates"}, "99ac83b990af1fc591db5b676300a7c002905dae": {"ta_keywords": "multi view learning;semi supervised learning;multi view datasets;hierarchical semi supervised;view learning;semi supervised;tackle semi supervised;multi view hierarchical;view hierarchical semi;views multi view;supervised learning optimal;multiple views multi;views multi;multi view;multiple views;supervised learning;view datasets paper;view datasets;views;extended multi view;number multi view;view hierarchical;data view;hierarchical class constraints;supervised;labels instances techniques;view;scores data view;learning optimal;hierarchical semi", "pdf_keywords": ""}, "3f90668994d6e5949a530dfc84a10b492ff35cfa": {"ta_keywords": "shallow semantic parsing;similar labeled sentences;task shallow semantic;similar sentences training;shallow semantic;parsing semantic labels;approach shallow semantic;sentences training;labeled sentences;parsing semantic;labeled sentences provide;grammatically similar labeled;semantic parsing;semantic parsing scientific;relation prediction sub;sentences training set;semantic parsing semantic;semantic structures corpus;semantic labels;structures corpus;structurally similar sentences;structures corpus appropriately;copied test sentences;semantic labels structurally;semantic structures;sub task shallow;similar labeled;prediction sub task;relation prediction;parsing", "pdf_keywords": ""}, "fce19dd512a82693ab9070049ed426179eca8856": {"ta_keywords": "collaboration web textual;mass collaboration web;analyze textual content;textual content analysis;mass collaboration wikis;comments mass collaboration;collaboration wikis web;nlp content;collaboration wikis;natural language processing;collaboration web;web textual;web textual content;analyze textual;nlp analyze artifacts;nlp content typically;nlp analyze;help nlp content;processing nlp analyze;processing nlp;utilizing natural language;efforts analyze textual;web forums debate;analyze web;analyze web based;resources help nlp;textual content;content analysis;textual content resources;debate platforms blog", "pdf_keywords": ""}, "0d22ce72a62419086fd4860a4671991846cd492b": {"ta_keywords": "lightweight block ciphers;aes suitable;block ciphers;aes;privacy internet things;aes suitable internet;block ciphers aid;standard aes suitable;advanced encryption standard;encryption standard aes;standard aes;ciphers aid securing;environments advanced encryption;advanced encryption;encryption standard;nsa developed simon;security agency nsa;encryption;ciphers;life privacy internet;nsa;suitable internet things;nsa developed;families lightweight block;ciphers aid;integrated life privacy;speck families lightweight;agency nsa;internet things;agency nsa developed", "pdf_keywords": ""}, "fb6ef2d6fbd1ea4905070077ab6c5b0108f2c38a": {"ta_keywords": "sarcasm detection twitter;sarcasm detection languages;sarcasm detection czech;research sarcasm detection;approach sarcasm detection;sarcasm detection;learning approach sarcasm;attempt sarcasm detection;czech twitter corpus;twitter corpus;detection twitter languages;twitter corpus consisting;research sarcasm;detection twitter;approach sarcasm;large czech twitter;czech twitter;twitter languages;detection czech language;labeled tweets provide;labeled tweets;twitter languages english;czech english datasets;sarcasm;manually labeled tweets;tweets provide community;czech evaluate classifiers;detection czech;features czech english;tweets", "pdf_keywords": ""}, "b8e49216e5b4a017342b0be5f6fbbd79e690a1c7": {"ta_keywords": "auctions deep learning;optimal auctions deep;auctions deep;auction modeled multilayer;network optimal auction;optimal auction;incentive compatible auction;optimal auctions;auction;near optimal auctions;optimal auction design;auction modeled;auction maximizes;auctions auctions;optimal auctions auctions;compatible auction maximizes;approach auction;auction design;unknown approach auction;auctions;tools deep learning;auction maximizes expected;approach auction modeled;pipelines optimal auctions;deep learning;multilayer neural;compatible auction;deep learning shaping;constrained learning;modeled multilayer neural", "pdf_keywords": "frame optimal auction;auction multi layer;optimal auction;progress optimal auction;introduction optimal auction;optimal auction design;auction design constrained;model auction;neural network architecture;auction design;auction;model auction multi;constrained learning;constrained learning problem;design constrained learning;auction design cornerstones;auction multi;auction design past;neural network;layer neural;multi layer neural;neural network frame;layer neural network;neural;constrain network architecture;standard machine learning;learning problem solved;learning problem;network frame optimal;machine learning pipelines"}, "f297e939212780637705eba8798c9a386befd771": {"ta_keywords": "improving pivot translation;pivot translation allows;pivot target translation;pivot translation remembering;pivot translation;translation remembering pivot;pivot language model;pivot phrases triangulation;translation language pairs;translation models;target translation models;pivot language;use pivot language;translation models source;translation language;high translation accuracy;pivot phrases;information source translation;translates combining source;phrases triangulation stage;target translation;translation time improving;translation accuracy;phrases triangulation;translation remembering;triangulation method translates;translates combining;torememberthe pivot phrases;source pivot;translation allows translation", "pdf_keywords": ""}, "8a902a848c3710290f04f2d59030f5670d3433f8": {"ta_keywords": "nlp models morphology;morphology predictive errors;morphological complexity language;increases morphological complexity;languages speculate morphology;morphology predictive;morphological complexity;morphologically simple languages;models morphology predictive;morphologically complex languages;morphology increases morphological;error analysis nlp;morphology discriminative morphologically;languages conjectures somewhat;true using morphological;morphology discriminative;using morphological;morphology;morphological;speculate morphology discriminative;morphologically simple;models morphology;discriminative morphologically simple;languages conjectures;analysis nlp models;using morphological features;morphological features does;morphological features;discriminative morphologically;morphology increases", "pdf_keywords": ""}, "ab48fb72541653f40523caa9fcaac9cb84bf3373": {"ta_keywords": "joint source separation;source separation;independent vector analysis;speaker automatic speech;source separation dereverberation;automatic speech;speech recognition;multi speaker automatic;automatic speech recognition;multi speaker;channel multi speaker;separation dereverberation based;independent vector;neural source model;based independent vector;frontend joint source;optimized jointly asr;speaker automatic;trained clean mixtures;module neural source;source steering algorithm;algorithm neural source;neural source;source steering;dereverberation based independent;joint source;speaker;separation dereverberation;iterative source steering;noisy data trained", "pdf_keywords": "speaker asr independent;multi speaker asr;multispeaker automatic speech;end multi speaker;speaker asr;channel multispeaker automatic;multi speaker;multispeaker automatic;channel multispeaker;automatic speech;joint source separation;multispeaker;multi channel multispeaker;asr independent vector;speech recognition;independent vector analysis;automatic speech recognition;source separation;neural beamforming frontends;source separation dereverberation;using neural beamforming;optimized jointly asr;neural beamforming;separation dereverberation based;speech recognition parameters;independent vector;speaker;frontend joint source;based independent vector;asr independent"}, "3c5d3bbb73aa0e3e969a25487a81b5b1f0c14044": {"ta_keywords": "knowledge graph;knowledge graph identification;information knowledge graph;probabilistic soft logic;knowledge graph propose;propose knowledge graph;structure knowledge graph;knowledge graph utilizing;knowledge graph scalability;inclusion knowledge graph;scale knowledge graph;reasoning structure knowledge;leveraging ontological information;facts inclusion knowledge;confidences leveraging ontological;large scale knowledge;jointly reasoning structure;soft logic;leveraging ontological;graph propose knowledge;soft logic psl;facts 80k ontological;80k ontological constraints;ontological information large;inference real world;use probabilistic soft;reasoning structure;psl recentlyintroduced probabilistic;joint inference;logic psl", "pdf_keywords": ""}, "5ede529879d162d2779d410a5775d3f6cd6be3f4": {"ta_keywords": "distributionally robust optimization;trained minimize maximum;neural generative models;distributionally robust;neural generative;robust optimization;generative models;large scale generative;trained minimize;model trained minimize;generative models develop;min max optimization;learning models able;robust optimization dro;set distributionally robust;use neural generative;max optimization;generative models characterize;distributions uncertainty set;loss distributions uncertainty;generative;uncertainty set distributionally;models robust;learning models;optimization large scale;gradient based optimization;distributions uncertainty;data distributions uncertainty;scale generative models;constrained inner maximization", "pdf_keywords": "distributionally robust optimization;distributionally robust;models robust;models robust comparable;robust optimization;player distributionally robust;combatting dataset bias;abstract distributionally robust;learning models able;robust optimization paul;models robust changes;robust optimization dro;learning models;robust accuracy;setting toxicity detection;combatting dataset;robust accuracy best;con\ufb01guration robust accuracy;realistic setting toxicity;models characterize worst;robust;data distributions uncertainty;machine learning models;applications combatting dataset;dataset bias;training machine learning;setting toxicity;yields models robust;toxicity detection;robust comparable"}, "c6b462aaca52d0325db3118d2779865915b266c3": {"ta_keywords": "rule induction large;complexity rule induction;rule learning induction;conquer rule learning;rule induction methods;runtime rule induction;induction large training;rule learning;rule induction;learning induction;learning induction methods;induction methods loss;new pruning techniques;pruning techniques;pruning techniques dramatically;induction methods scale;induction methods;improve runtime rule;pruning;complexity rule;new pruning;induction large;large training sets;learning problems;complexity;asymptotic complexity rule;induction;real world learning;form new pruning;study asymptotic complexity", "pdf_keywords": ""}, "dfb35ebe4fd754f59053d27c78f555bb5e7ccbff": {"ta_keywords": "estimation curvature regularization;structure estimation curvature;prior minimizing curvature;estimation curvature;minimizing curvature;curvature regularization;orientation estimation structure;vision require estimation;minimizing curvature center;valued orientation estimation;orientation estimation;surfaces applications vision;curvature regularization unlike;structure estimation;sub pixel localization;estimation structures boundary;pixel localization;curvature center lines;estimation structure estimation;coordinate descent;early vision framework;proposed early vision;block coordinate descent;curvature center;vision framework;early vision;optimization location detection;prior minimizing;pixel localization real;structures sub pixel", "pdf_keywords": "estimation curvature regularization;structure estimation curvature;estimation curvature;based curvature regularization;prior minimizing curvature;curvature regularization;curvature regularization propose;minimizing curvature;detection subpixel delineation;curvature regularization unlike;minimizing curvature centerlines;valued orientation estimation;vision require estimation;structures based curvature;quadratic curvature regularization;sub pixel localization;detection subpixel;orientation estimation;structure estimation;pixel localization;curvature regularization dmitrii;orientation estimation propose;estimation structures boundary;based curvature;subpixel delineation structures;algorithm detection subpixel;structures sub pixel;orientation estimation paper;centerlines surfaces optimization;surfaces optimization"}, "cd5a9a0061de6a6841c63e60281133207b2d6763": {"ta_keywords": "neural description model;interpretation learning phrases;description decoder;propose neural description;neural description;phrase natural language;learning phrases local;encoders description decoder;context encoders description;help interpretation learning;description decoder contrast;dictionaries definitions search;interpretation learning;learning phrases;phrases local global;natural language;definitions search;global contexts;meaning expressions;dictionaries definitions;describing;description model;definitions search documents;context consult dictionaries;global context;phrases local;contexts;global context help;oxford urban dictionaries;local global contexts", "pdf_keywords": "unfamiliar words phrases;phrases polysemous words;words phrases polysemous;learning unknown phrases;phrases polysemous;2018 polysemous words;polysemous words;slang emerging entities;phrases local;unknown phrases local;polysemous words novel;stuck unfamiliar words;words urban dictionary;polysemous words urban;unknown phrases;words phrases;unfamiliar words;phrases local global;descriptions unknown phrases;internet slang emerging;urban dictionaries;slang newlycreated wikipedia;oxford urban dictionaries;slang emerging;words oxford dictionary;2017 general words;general words;dictionary;general words oxford;wordnet"}, "89b8153a86708b411bd21357c5b6006142104fc9": {"ta_keywords": "spoken quote corpora;public speaking memorable;quote corpora ted;speaking memorable;speaking memorable quotes;quote corpora;share memorable spoken;ted public speaking;relevance corpora;corpora ted public;reveal public speeches;public speeches;public speeches retained;analysis memorable spoken;speeches;memorable spoken quote;relevance corpora required;corpora ted;speeches retained people;purpose relevance corpora;public speaking;speeches retained;public speaking achieve;spoken quote;corpora;memorable spoken;memorable quotes interesting;memorable quotes;quotes interesting useful;speaking achieve purpose", "pdf_keywords": ""}, "91ef95907dc637ad3c29ac3cc0e682b9c1985a37": {"ta_keywords": "simultaneous speech translation;speech translation;machine translation;machine translation methods;search optimal segmentation;performance machine translation;speech translation contrast;segmentation directly maximizes;optimal segmentation strategy;translation methods;optimal segmentation;segmentation strategy;learning segmentation strategies;segmentation strategies;segmentation strategies simultaneous;translation methods based;segmentation directly;segmentation;learning segmentation;finds segmentation;finds segmentation directly;algorithms learning segmentation;strategies simultaneous speech;method finds segmentation;automatic evaluation;simultaneous speech;segment input;speech;algorithm able segment;translation contrast previously", "pdf_keywords": ""}, "1f3c381eedfe8914b81e93070bfdb00cf86ac943": {"ta_keywords": "graph classification benchmarks;graph level representations;binary graph classification;graph classification;node graph classification;learning node graph;graphs compared supervised;structural views graphs;visual representation learning;views graphs;views graphs compared;reddit binary graph;node graph level;representation learning increasing;graph level;supervised learning node;learning node;representation learning;self supervised learning;visual representation;node graph;graphs;graphs compared;self supervised;binary graph;classification benchmarks linear;neighbors graph diffusion;neighbors graph;results self supervised;classification benchmarks", "pdf_keywords": "graph encodings views;learning graphs views;representation learning graphs;view representation learning;graphs views;contrastive representation learning;views graphs;visual representation learning;visual contrastive learning;graph level representations;graphs views does;improve contrastive representation;representation learning node;views graphs achieve;views augmentations contrastive;representation learning increasing;structural views graphs;graph encodings;unlike visual representation;learning graphs;contrasting graphgraph;contrastive learning increasing;graph encoders;contrastive learning;graph encoders maximizing;representation learning;compared contrasting graphgraph;node graph encodings;unlike visual contrastive;view representation"}, "4abdea830316d80ab0b29fb94ee0786216f6a1cd": {"ta_keywords": "phrase alignment extraction;alignment phrase extraction;joint phrase alignment;machine translation;phrase alignment;machine translation tasks;based machine translation;unaligned sentence pairs;phrase extraction;step word alignment;phrase based machine;alignment extraction;word alignment phrase;word alignment;inversion transduction grammars;model joint phrase;alignment extraction using;phrase extraction approach;memorizes phrases generated;translation tasks directly;formulation memorizes phrases;translation tasks;directly unaligned sentence;sentence pairs;transduction grammars itgs;memorizes phrases;transduction grammars;alignment phrase;phrases generated terminal;bayesian methods inversion", "pdf_keywords": ""}, "d6b3effdeb3d38ac9ee43c3b8292b0937a295c30": {"ta_keywords": "multitask learning pretraining;pretraining multitask learning;multitask learning auxiliary;conversational speech recognition;pretraining multitask;standard multitask training;multitask learning;multitask training;hierarchical multitask learning;multitask training works;speech recognition improved;multitask training higher;improved hierarchical multitask;encoder decoder speech;improves standard multitask;multitask approach improves;pretraining improves word;decoder speech recognition;speech recognition specifically;telephone conversational speech;speech recognition;improvements telephone conversational;decoder speech;multitask;relationship pretraining multitask;standard multitask;hierarchical multitask;deep encoder;performance improvements telephone;learning auxiliary tasks", "pdf_keywords": "multitask learning ctc;hierarchical multitask learning;speech recognition trained;ctc based speech;ctc model speech;multitask learning pretraining;multitask learning auxiliary;improved hierarchical multitask;multitask learning;encoder decoder speech;multitask learning better;decoder speech recognition;task multitask learning;multitask learning improve;speech recognition improved;standard multitask training;subword level ctc;multitask training;layers deep encoder;hierarchical multitask;layer phonelevel ctc;model speech recognition;learning ctc;conversational speech recognition;multitask learning context;learning ctc based;multitask training higher;phonetic recognition task;multitask training works;speech recognition"}, "3c78c6df5eb1695b6a399e346dde880af27d1016": {"ta_keywords": "question answering models;level question answering;adapting neural paragraph;question answering;answering models;neural paragraph level;paragraphs documents training;neural paragraph;paragraphs overall able;answering models case;paragraph level;results individual paragraphs;individual paragraphs overall;individual paragraphs;sample multiple paragraphs;document qa data;paragraphs overall;paragraph level question;paragraphs documents;paragraphs;models document qa;document qa;multiple paragraphs documents;documents given input;multiple paragraphs;adapting neural;normalization training;paragraph;answering;documents training", "pdf_keywords": "pipelined question answering;question answering systems;question answering;question answering propose;paragraphlevel qa model;general question answering;heuristic select paragraphs;single paragraph heuristically;answering systems;paragraphlevel qa;paragraph heuristically extracted;paragraph heuristically;passed paragraphlevel qa;select paragraphs train;answering systems single;paragraphs train test;paragraphs train;paragraph selection;multiple paragraph selection;paragraph con\ufb01dence scores;select paragraphs;paragraphlevel;produce accurate paragraph;systems single paragraph;paragraphs;multiple paragraph;single paragraph;method multiple paragraph;answering;document passed paragraphlevel"}, "7c3a2e953d2c07ff4f150865112e4ceec14090ea": {"ta_keywords": "electrolaryngeal speech enhancement;el speech enhancement;speech enhancement electrolarynx;approach electrolaryngeal speech;speech enhancement;speech enhancement evaluation;electrolaryngeal speech;electrolaryngeal el speech;excitation prediction electrolaryngeal;sounds enable laryngectomees;voiced prediction hybrid;enable laryngectomees produce;enable laryngectomees;prediction electrolaryngeal el;voiced prediction;laryngectomees produce el;parameters proficient laryngectomees;voice conversion;unvoiced voiced prediction;parameters voice conversion;prediction electrolaryngeal;voice conversion method;produce el speech;excitation feature prediction;excitation sounds enable;el speech sounds;removing micro prosody;proficient laryngectomees produce;statistical excitation prediction;enhancement electrolarynx device", "pdf_keywords": ""}, "73fe797b4f4f2d18784246bb74626426a8fe108e": {"ta_keywords": "pointer graph networks;graph neural networks;graphs graph neural;graph neural;graph networks;graph networks pgns;pointer graph;introduce pointer graph;networks gnns;networks gnns typically;gnns deep sets;graph connectivity tasks;latent graph structure;augment sets graphs;graph structure;pgns learn parallelisable;graph connectivity;learn parallelisable variants;networks;neural networks gnns;graph structure difficult;sets graphs additional;space possible graphs;unrestricted gnns deep;task gnn solving;networks pgns augment;networks pgns;graphs;deep sets;graphs additional inferred", "pdf_keywords": "pointer graph networks;graph networks pgns;graph networks;latent graph inference;propose pointer graph;graph inference;introduce pointer graph;pointer graph;graph connectivity tasks;graph inference demonstrate;graph connectivity;pgns learn parallelisable;gnns deep sets;augment sets graphs;networks pgns augment;dynamic graph connectivity;networks pgns;graph based models;challenging gnns pgn;learn parallelisable variants;unrestricted gnns deep;networks;known challenging gnns;latent graph;ef\ufb01cient latent graph;sets graphs additional;networks pgns framework;graphs additional;challenging gnns;pgns learn"}, "8b652c4d7a8d5836925ce0fe28a91dc661778524": {"ta_keywords": "neural language models;entities semantically comparable;nlp tasks;large neural language;comparative questions like;language models;results nlp tasks;semantically comparable;language models lms;completing comparative questions;world knowledge linguistic;semantically comparable similar;knowledge linguistic;comparative questions different;results nlp;authored comparative questions;lms like bert;comparative questions;nlp tasks unclear;comparative questions difficult;questions like country;neural language;neural lms ask;knowledge linguistic competence;linguistic;nlp;art results nlp;ai conference;answer reasonable questions;reasonable models trained", "pdf_keywords": "entities semantically comparable;large language models;semantically comparable;linguistic structures comparisons;language models complex;semantically comparable similar;entity focused language;language models;limited lexical phenomena;generalize lexical level;comparative questions like;semantic linguistic;limited lexical;world knowledge linguistic;complex semantic linguistic;models complex semantic;entities semantically;knowledge linguistic;lexical level;notion entities semantically;comparative questions different;humanauthored comparative questions;large language;comparative questions yields;lexical phenomena failing;generalize lexical;domain questions study;focused language model;semantic;\ufb01netuning limited lexical"}, "f53aa1d2676689c94429944f6a69431f96e05ae1": {"ta_keywords": "unsupervised topic models;topic models semi;topic models;topic indicative features;models semi supervised;supervision mixed membership;latent variable models;learning mixed membership;semi supervised learning;semi supervised;gibbs sampler;membership latent variable;gibbs sampler used;unsupervised topic;supervised learning mixed;mixed membership latent;topic indicative;modify gibbs sampler;supervised;entity clustering;entity clustering task;membership latent;approximate inference models;range unsupervised topic;supervision form labels;inference models;supervised learning;supervision mixed;results entity clustering;forms supervision mixed", "pdf_keywords": ""}, "57676e07d66b102f3335a5c538735ebff9076623": {"ta_keywords": "loop network motif;network motif;feedforward loop network;motif;loop network;feedforward loop;feedforward;network;loop", "pdf_keywords": ""}, "ba1823889a80c231966a0f24e57c6cf4a569ff8c": {"ta_keywords": "multimodal fake news;multimodal entity;diverse multimodal fake;detect diverse multimodal;entity enhanced multimodal;multimodal entity inconsistency;model multimodal entity;diverse multimodal;diverse multimodal clues;multimodal fake;multimodal fusion framework;capture multimodal clues;enhanced multimodal fusion;enhanced multimodal;multimodal fusion;multimodal clues things;fake news detection;multimodal clues innovatively;multimodal;news text images;multimodal clues;images model multimodal;issue multimodal fake;visual entities celebrities;model multimodal;news detection effectively;fuse diverse multimodal;effectively capture multimodal;improving fake news;news detection", "pdf_keywords": "fake news detection;improving fake news;multimodal fake news;fake news text;text fake news;news detection using;news detection things;detection using entity;news detection;fake news accordingly;fake news fuse;improving fake;clues detection extensive;correlations multimodal fake;diffusion text fake;diverse multimodal fake;fake news raising;clues detection;news text;issue multimodal fake;text fake;multimodal clues detection;multimodal fake;understand news related;fake news;detection using;news text images;recently fake news;news fuse;news fuse diverse"}, "499ada382b7ce8f1cbd890e8c21500d95e20f2fe": {"ta_keywords": "evaluation audio representations;holistic evaluation audio;audio representations;audio representations using;audio representation provides;scenarios audio embedding;audio embedding;audio embedding model;ates audio representations;audio representations remains;audio representation;purpose audio representation;audio representation perform;submitted audio embedding;audio embedding approach;evaluation audio;tasks scenarios audio;general purpose audio;scenarios audio;purpose audio;audio;use hear 2021;participant submitted audio;music hear 2021;submitted audio;ates audio;environmental sound music;evalu ates audio;speech environmental sound;use hear", "pdf_keywords": "audio representation holistic;evaluation audio representations;audio representations generalize;evaluates audio representations;holistic evaluation audio;audio representations;audio representations using;audio representations strong;insight audio representations;audio representations hear;audio representation models;audio representations greater;audio representation;representations hear challenge;\ufb02exible audio representations;purpose audio representation;submitted audio representation;evaluation audio;representations hear;learning applied audio;models hear holistic;greater insight audio;representations using benchmark;evaluates audio;2021 evaluates audio;insight audio;hear challenge created;participants submitted audio;representation holistic;audio"}, "aa2428e1c4ea6d6bb347cfa59beead8736e19c46": {"ta_keywords": "reasoning virtual knowledge;virtual knowledge base;knowledge base;hop reasoning virtual;virtual knowledge;reasoning virtual;multi hop reasoning;hop reasoning;knowledge;reasoning;multi hop;differentiable multi hop;hop;virtual;base;multi;differentiable;differentiable multi", "pdf_keywords": ""}, "95ee674a03ad23eaaf4837121fc8aea30d885088": {"ta_keywords": "structured preference representations;preference representations;compact preference representations;novel metric learning;distance structured preference;siamese networks learn;preference representations working;metric learning;siamese networks;preferences leverage deep;deep siamese networks;learning measure distance;use deep siamese;metric learning approach;structured preference;deep siamese;learn distance function;represent preferences;novel metric;deep neural;cpdist novel metric;neural networks learn;representations working preferences;represent preferences leverage;formalism represent preferences;networks learn kendal;sets objects preferences;compact preference;recently proposed metric;leverage deep neural", "pdf_keywords": "metric learning structured;preference representations cpmetric;cpmetric siamese network;structured preference representations;novel metric learning;learning structured preference;metric learning;preference representations;siamese network;model learn metric;cpmetric novel neural;structured preference representation;siamese network 11;learn metric;neural networks learn;representations cpmetric;representations cpmetric novel;network model learn;cpmetric siamese;problem metric learning;learn metric distance;metric learning approach;preference representation;present cpmetric siamese;learning structured;cpmetric novel metric;compact structured preference;deep neural;cp net formalism;networks learn"}, "683bbb665bdaea8688834e97559d63842242ee1f": {"ta_keywords": "combating deep reinforcement;deep reinforcement learning;deep reinforcement;reinforcement learning wild;curse reinforcement learning;intrinsic fear dqns;use deep reinforcement;reinforcement learning sisyphean;fear dqns solve;learn reward shaping;reinforcement learning;agent avoid catastrophic;reinforcement;fear dqns;reward shaping;dqns solve toy;combating deep;learn reward;reward shaping accelerates;reinforcement learning approach;atari games;improve atari games;environments improve atari;atari games seaquest;sisyphean curse reinforcement;learning wild;learning wild hope;improve atari;shaping accelerates learning;curse reinforcement", "pdf_keywords": ""}, "13d9d24ff2ba69de4cedcebd8f59371a5c1de7ed": {"ta_keywords": "word sense disambiguation;sense disambiguation;sense disambiguation experiments;individual context words;context words;identifying useful contextual;useful contextual;knowledge based word;disambiguation experiments using;contextual cues knowledge;based word sense;context words evaluated;context modeling;improvements context modeling;disambiguation;disambiguation experiments;word sense;contextual;usefulness individual context;useful contextual cues;based diverse lexico;diverse lexico;individual context;context modeling beating;contextual cues;context;diverse lexico statistical;simple word distance;word distance;lexico statistical syntactic", "pdf_keywords": ""}, "788aa828a194a6d6c4e5ab1d4b46fc5f987159b0": {"ta_keywords": "technology oil painting;oil painting creation;oil painting;painting creation era;painting creation;processing technology oil;painting;image processing technology;big data;digital image;digital image processing;era big data;application digital image;technology oil;image processing;processing technology;digital;influence application digital;data;application digital;oil;creation era big;processing;image;technology;era big;creation era;influence application;influence;application", "pdf_keywords": ""}, "3df97e8237c7d98c7343fc025eacbbc2b96a10ae": {"ta_keywords": "exosomes small extracellular;exosomes;release exosomes;exosomes small;capture release exosomes;extracellular vesicles secreted;vesicles secreted cells;extracellular vesicles;small extracellular vesicles;vesicles secreted;vesicles;hedgehog inspired immunomagnetic;immunomagnetic beads;inspired immunomagnetic beads;immunomagnetic beads high;small extracellular;immunomagnetic;extracellular;inspired immunomagnetic;secreted cells;secreted cells play;cells;cells play important;cells play;beads high efficient;beads;development hedgehog inspired;hedgehog inspired;occurrence development hedgehog;development hedgehog", "pdf_keywords": ""}, "a43d6fa0e96d56d0200e8d5e4407be8befc4e063": {"ta_keywords": "advertising social responsibility;fast food industry;responsibility moral myopia;fast food;food industry;social responsibility moral;advertising social;moral myopia;moral myopia overview;overview fast food;advertising;responsibility moral;social responsibility;moral;industry;responsibility;social;food;myopia;myopia overview;myopia overview fast;fast;overview fast;overview", "pdf_keywords": ""}, "b2fac3812885e3c8101cc729b6846f9108ac4d70": {"ta_keywords": "crowdsourcing use pairwise;bias pairwise comparisons;mle accuracy bias;accuracy bias pairwise;grading crowdsourcing;fairness represented bias;improves fairness represented;ai bot tournaments;improves fairness;peer grading crowdsourcing;effectiveness mle accuracy;grading crowdsourcing use;provably improves fairness;sports peer grading;bias pairwise;mle accuracy;accuracy bias;bias loss accuracy;bot tournaments sports;bot tournaments;effectiveness mle;crowdsourcing;bias maintaining minimax;estimators provably improves;bias;bias loss;represented bias loss;crowdsourcing use;items bots teams;bias maintaining", "pdf_keywords": "crowdsourcing use pairwise;btl model evaluate;grading crowdsourcing;peer grading crowdsourcing;grading crowdsourcing use;results btl model;luce btl model;results btl;search results btl;btl model widely;parameters estimated crowdsourcing;bias pairwise comparisons;luce btl;estimated crowdsourcing;btl model;btl model employed;crowdsourcing use;crowdsourcing;btl model assumes;sports peer grading;peer grading;crowdsourcing 27 understanding;students set btl;comparison data bradley;ai bot tournaments;btl;estimated crowdsourcing 27;pairwise comparisons jingyan;applications peer grading;pairwise comparisons"}, "8e56db786a685b4b9c7f1b750f60a81baebff0b5": {"ta_keywords": "enhanced speech noisy;intelligible speech noisy;speech nonaudible murmur;speech quality intelligibility;voiced speech effective;nonaudible murmur enhancement;speech noisy environments;silent speech communication;speech noisy;listens enhanced speech;effective silent speech;murmur enhancement;speech effective nam;speech quality;speak making audible;speech nonaudible;statistical voice conversion;improve speech quality;target speech nonaudible;generating intelligible speech;speech whispered voice;process voiced speech;speech generating intelligible;effective nam speech;target speech effective;voice nam whisper;voiced speech;enhanced speech;speech communication;speech effective", "pdf_keywords": ""}, "418349df9bf28e2b1290b758a4ebcf0d812c7288": {"ta_keywords": "classifying political blogs;political blogs blog;political blogs;blog network ranking;blogs blog network;blogs;blog network;blogs blog;seed blogs;algorithm classifying political;blog;using seed blogs;classifying political;semi supervised learning;new semi supervised;semi supervised;ranking predicted classes;classifying;network ranking predicted;predicted classes;classification;ranking predicted;achieve classification;learning algorithm classifying;algorithm classifying;network ranking;datasets achieve classification;achieve classification accuracy;classification accuracy;supervised learning", "pdf_keywords": ""}, "e00f0a9e184a9d2afd8bb344908ca25d8bdc9e04": {"ta_keywords": "natural language computer;language computer nlc;language nl processing;natural language nl;research natural language;english language programming;nl processing;natural language;implementation nlc;design natural language;implementation nlc completed;computer nlc;computer nlc extant;language nl;language programming;initial implementation nlc;nlc extant automatic;programming provides english;language computer;language programming environment;extant automatic programming;nl processing duke;automatic programming;automatic programming provides;nlc completed;provides english language;nlc;english language;programming provides;programming", "pdf_keywords": ""}, "692320cf5ae6980bc6b2b2d7bc48df961b545c22": {"ta_keywords": "speech enhancement video;channel speech enhancement;speech enhancement multiple;speech enhancement single;enhancement video conferencing;speech enhancement;speech quality targeting;multi channel speech;high speech quality;video conferencing challenge;speech quality;channel speech;conferencingspeech challenge far;conferencingspeech 2021 challenge;real video conferencing;conferencingspeech challenge;develop conferencingspeech challenge;video conferencing final;speakers recording facilities;video conferencing room;video conferencing;clean speech noise;speech noise datasets;enhancement single microphone;obtain high speech;conferencing challenge open;microphone;real speakers recording;conferencing challenge;room conferencingspeech 2021", "pdf_keywords": "speech enhancement multiple;channel speech enhancement;speech enhancement;enhanced speech low;speech enhancement methods;speech recorded microphone;speech enhancement problem;speech quality real;multi channel speech;intelligibility enhanced speech;speakers real conference;clean speech noise;microphone arrays video;speech quality;enhanced speech;channel speech;distributed microphone;real video conferencing;video conferencing room;speech noise datasets;distributed microphone arrays;speech recorded;video conferencing rooms;microphone arrays real;video conferencing scenarios;microphone arrays casual;subjective speech quality;video conferencing;conferencingspeech 2021 challenge;conferencingspeech challenge multi"}, "87d50fc84c71ed9860ed02b0149266b74c446c9c": {"ta_keywords": "regression hmm parameters;linear regression hmms;linear regression hmm;hmm parameters widely;regression hmms using;model hmm parameters;regression hmms;hmms using variational;regression hidden markov;regression hmm;hmm parameters;model hmm;hidden markov model;hmm parameters non;markov model hmm;hidden markov;speech processing;likelihood linear regression;marginalized log likelihood;linear regression hidden;log likelihood linear;parameters linear regression;variational techniques linear;especially speech processing;techniques linear regression;non parametric bayes;parametric bayes manner;markov model;likelihood linear;training time series", "pdf_keywords": ""}, "dc3adb99f682a11fe0507dcbc5dc2958199c5af1": {"ta_keywords": "optimal gene circuit;gene circuit design;gene circuit;optimal gene;circuit design;gene;circuit;optimal;design", "pdf_keywords": ""}, "48685f26b32d199e6a4d80f6c61e62cc9738e403": {"ta_keywords": "event extraction;task event extraction;domain adapted parsing;support vector machine;bionlp 2009 shared;rule based component;bionlp 2009;parsing;strong baselines bionlp;baselines bionlp 2009;vector machine classifiers;machine classifiers;classifiers;adapted parsing;adapted parsing paper;parsing paper presents;rule based approach;bionlp;parsing paper;machine classifiers achieve;classifiers achieve;rule based;baselines bionlp;component support vector;event;classifiers achieve performance;vector machine;implement rule based;support vector;reproducible baselines implement", "pdf_keywords": ""}, "e107beee5e84cd11d6460f7040676687a51a378b": {"ta_keywords": "reconstruction approaches variational;unrolled reconstruction operator;reconstruction network;reconstruction operators;reconstruction operator;reconstruction operators based;image reconstruction;distributions reconstruction;data driven reconstruction;output reconstruction network;reconstruction network solved;image reconstruction ray;example image reconstruction;end reconstruction operators;driven reconstruction approaches;reconstruction operator notably;driven reconstruction;initialized output reconstruction;output reconstruction;unrolled reconstruction;simultaneously unrolled reconstruction;variational framework iterative;ill posed inverse;reconstruction approaches;distributions reconstruction ground;posed inverse;reconstruction ray;reconstruction ray computed;distance distributions reconstruction;variational methods", "pdf_keywords": "supervised learned reconstruction;learned reconstruction approaches;trained reconstruction operator;learned reconstruction;approach ct reconstruction;ct reconstruction achieves;trained reconstruction;ct reconstruction;reconstruction operator;reconstruction operator corresponding;tomography ct;computed tomography;tomography;reconstruction achieves performance;computed tomography ct;adversarial regularization uar;tomography ct approach;reconstruction approaches;ray computed tomography;ct \ufb01ltered projection;ct approach outperforms;adversarial regularization;unrolled adversarial regularization;reconstruction approaches proposed;regularization uar approach;ct approach;reconstruction achieves;approaches ct \ufb01ltered;link trained reconstruction;regularization"}, "9f1d9dfb0b30d9fc5881d07b8e7f508815296c93": {"ta_keywords": "grammars domain adaptation;domain specific parser;parser generalizable way;statistical parsing concerns;statistical parsing;parser generalizable;domains parser;parsing concerns learning;specific parser;parser train parsing;specific parser train;parsing;multiple domains parser;parser;domains parser likely;parser likely;parser train;parsing model;parsing concerns;learning probabilistic syntactic;field statistical parsing;substitution grammars domain;syntactic models corpora;train parsing;probabilistic syntactic models;inferring syntactic structure;tree substitution grammars;parsing process inferring;parser likely identify;substitution grammars", "pdf_keywords": ""}, "51ec4e93d8ae4c62453fdb34c6866696da0527b1": {"ta_keywords": "news detection fake;multimedia fake news;fake news detection;detection fake news;detecting multimedia fake;visual content fake;content fake news;content fake;content important fake;multimedia fake;issues multimedia fake;detection fake;technology fake news;fake news attempts;multimedia technology fake;news detection social;news detection;news detection effectively;proliferation fake news;fake news increasing;important fake news;detection social media;fake news including;detecting multimedia;videos attract mislead;images videos attract;content images;fake news;news attempts utilize;technology fake", "pdf_keywords": "multimedia fake news;datasets fake news;multimedia datasets fake;fake news detection;features multimedia fake;multimedia fake;fake news analyze;content fake news;competitions multimedia fake;news detection research;concept fake news;datasets fake;issues multimedia fake;content fake;news detection finally;visual content fake;combat fake news;news detection follows;multimedia datasets;fake news including;news detection;news analyze;representative multimedia datasets;news detection problem;detection research appendix;relevant competitions multimedia;forensics features semantic;detection research;news analyze di\ufb00erent;context features multimedia"}, "a16cecbaf87d965e396e610f251f710a807b70ad": {"ta_keywords": "hearing impairment simulation;simulation hearing impairment;impairment simulation hearing;personalization hearing impairment;intelligibility hearing impaired;persons hearing impairment;adjusting hearing impairment;simulation hearing;hearing impaired vary;proposed hearing impairment;hearing impaired persons;perception hearing impaired;hearing impairment;similar hearing impaired;hearing impaired;hearing impairment level;hearing people auditory;characteristics hearing impaired;compare intelligibility hearing;hearing impaired normal;impaired persons hearing;people auditory perception;accuracy proposed hearing;intelligibility hearing;impairment simulation method;hearing persons similar;normal hearing people;normal hearing persons;based approximation auditory;adjusting hearing", "pdf_keywords": ""}, "f49065750931c1c3c9edaf7d2f4bc8ea1342450a": {"ta_keywords": "sequences neural autoregressive;neural autoregressive;neural autoregressive model;short sequences neural;autoregressive model degradation;regularization impacts;model degradation translation;tuning strength regularization;regularization model distribution;degradation translation quality;regularization model;distribution decoding performance;model distribution decoding;decoding performance;proposed regularization impacts;effect proposed regularization;sequences neural;proposed regularization model;regularization impacts outcome;strength regularization;distribution decoding;regularization;oversmoothing loss;proposed regularization;decoding;degradation translation;beam significantly lessens;translation quality;translation quality measured;model degradation", "pdf_keywords": "neural machine translation;regulariza tion translation;regularization use neural;neural autoregressive sequence;tion translation quality;regularization neural machine;regularization neural;oversmoothing neural autoregressive;proposed regularization neural;abstract neural autoregressive;machine translation;autoregressive sequence modeling;machine translation systems;autoregressive sequence models;translation quality;machine translation task;translation task testbed;sequence modeling ilia;neural autoregressive;translation systems;sequence models;tion translation;performance second regularization;regularization use;sequence modeling;machine translation using;regularization model distribution;translation task;regularization model;regulariza tion"}, "b0b1112b06898733faefc32f54940aa4e84bc383": {"ta_keywords": "translate paralinguistic information;speech corpora various;paralinguistic information;speech corpora;speech corpora used;emphasized speech corpora;paralinguistic information included;paralinguistic information focus;speech s2s translation;speech speech s2s;types paralinguistic information;usually translate paralinguistic;translate paralinguistic;speech s2s;various types paralinguistic;analysis japanese english;emphasis expressed languages;sentences conversation corpus;japanese english emphasized;conversation corpus;english emphasized speech;corpus;types paralinguistic;conversation corpus collected;analysis japanese;paralinguistic;sentence emotion speaker;speech speech;corpora various;corpus 2030 parallel", "pdf_keywords": ""}, "c55bc339122ad8cdba1ae74d1336be3d2f089699": {"ta_keywords": "stochastic convex optimization;convex optimization;stochastic primal dual;handle stochastic convex;convex smooth objective;convex optimization problems;distributed optimization;affine constraints spdstm;decentralized distributed optimization;spdstm stochastic primal;optimization methods spdstm;distributed optimization problem;stochastic dual;stochastic convex;use stochastic dual;primal dual approach;optimization problems affine;stochastic primal;constraints spdstm;proximal step convex;constraints spdstm rrma;consider stochastic convex;smooth strongly convex;methods spdstm stochastic;gradient sliding algorithm;optimization methods;smooth objective functions;dual problem primal;convex smooth;stochastic dual order", "pdf_keywords": "stochastic convex optimization;algorithms stochastic convex;optimal decentralized distributed;distributed algorithms stochastic;decentralized distributed algorithms;convex optimization problems;stochastic convex;convex optimization;convex optimization eduard;distributed algorithms;consider stochastic convex;optimal decentralized;primal dual approach;decentralized distributed;algorithms stochastic;2020 optimal decentralized;using primal dual;distributed;stochastics germany 5national;dual approach;primal dual;dual approach solve;optimization problems;optimization eduard;stochastics;methods using primal;optimization eduard gorbunov1;stochastics germany;stochastic;abstract consider stochastic"}, "6cf3bdcdee6236f9f04e7773e3601dbbb8fbc61e": {"ta_keywords": "entity recognition ner;named entity recognition;domain question answering;human annotated datasets;entity recognition;annotated datasets requir;rely human annotated;ner datasets;annotated datasets;generates ner datasets;recognition ner models;ner datasets 006;human annotated;resources domain sentences;recognition ner;question answering;resource 015 models;automatically generates ner;domain sentences 010;models popular ner;shot ner outperform;domain sentences;domain 016 dictionaries;ner models;ner outperform;ner models 001;sentences 010 labels;010 labels models;017 shot ner;ner outperform previous", "pdf_keywords": "entity recognition ner;entity recognition datasets;named entity recognition;automated ner dataset;ner dataset generation;entity recognition;models ner datasets;extracting named entities;open domain corpus;candidate entities phrases;domain corpus;ner task extracting;entities phrases evidence;domain question answering;entities phrases;ner datasets;phrase retrieval;ner dataset;ner datasets simple;phrase retrieval model;quality ner datasets;gener automated dataset;ner models generated;gener automated ner;domain corpus wikipedia;ner datasets conll;named entities;named entities speci\ufb01c;\ufb01ne grained entities;grained entities"}, "a0f00d5ea3727151b1c2fc8c407404f0c6641051": {"ta_keywords": "pcfg la parsers;parsers genre pcfg;robust parsers genre;robust parsers;phrase structure parser;parser robust;parser robust parsers;structure parser robust;parsers known;millions sentences parsed;parsers;parsers known achieve;parsers genre;performance parsing;parser;la parsers known;structure parser;parsing;la parsers;competitive performance parsing;sentences parsed;smoothing lexicon;sentences parsed failures;smoothing lexicon probabilities;causes parsing failure;parsing failure;ckylark pcfg la;ckylark pcfg;causes parsing;parses generated ckylark", "pdf_keywords": ""}, "3b7321832ba109cf47bfd13595c3b58acd4cb080": {"ta_keywords": "transnets review generation;transnets review;transnets;review generation;review;generation", "pdf_keywords": ""}, "3400b8bf1ffde3ef3d35dfcea893e6506427aa21": {"ta_keywords": "separation speech recognition;multi speaker speech;source separation speech;speaker speech recognition;single speech sequence;speech sequence unifying;recognition utterances multiple;utterances multiple speakers;end multi speaker;sequences single speech;speech recognition promising;speech sequence;recognition utterances;speech recognition utterances;speech recognition;multi speaker;sequence unifying source;single speech;separation speech;utterances multiple;speaker speech;multiple label sequences;unifying source separation;speech recognition functions;speakers recognized mixture;source separation;label sequences single;decode multiple label;growing multi speaker;directly decode multiple", "pdf_keywords": "separation speech recognition;speech sequence unifying;speech recognizer;single speech sequence;speech recognizer based;automatic speech;mapping speech mixture;multispeaker speech recognizer;speech recognition asr;sequences single speech;speech recognition;source separation speech;end multispeaker speech;mapping speech;recognize single utterance;automatic speech recognition;speech mixture multiple;speech mixture;learn mapping speech;speech sequence;multispeaker speech;speech recognition functions;speech signal transformation;conventional automatic speech;separation recognition modules;given speech signal;speech signal;single speech;multiple label sequences;label sequences achieving"}, "3ce0f00d6c949192107f1bd6a167c03e1fb7393a": {"ta_keywords": "deterministic dependency parsing;directional dependency parsing;dependency parsing algorithm;deterministic parsing algorithms;traditional deterministic parsing;dependency parsing;deterministic parsing;parser learns attachment;parser learns;parsing algorithms;parsing algorithms based;parsing;parsing algorithm;parser;parsing algorithm attempts;point parser learns;novel deterministic dependency;deterministic dependency;attachment point parser;directional dependency;point parser;arcs dependency structure;dependency structure;non directional dependency;framework traverse sentence;easiest arcs dependency;dependency structure non;learns attachment preferences;learns attachment;traverse sentence", "pdf_keywords": "dependency parsing algorithms;deterministic dependency parsing;dependency parsing algorithm;directional dependency parsing;dependency parsing;english dependency parsing;dependency parsing conll;category dependency parsing;non directional parsers;directional parsers;parsing algorithms;optimized parsers;parsing algorithms inspired;optimized parsers making;globally optimized parsers;directional parsers yields;parsers;parsing parsing algorithm;parsing algorithm;parsing;dependency parsing yoav;parsing parsing;parsers making good;\ufb01rst parsing parsing;parser;parsing conll 2007;parsers making;parsing algorithm attempts;easy \ufb01rst parsing;parsers yields"}, "b9f5115b0353c268999fcc2f49c4b8e03a223994": {"ta_keywords": "simulating outcomes interventions;simulate intervention outcomes;interventions predicting;diverse interventions predicting;method simulate intervention;simulate intervention;simulating outcomes;intervention outcomes prospectively;outcomes interventions;intervention outcomes;outcomes interventions using;interventions predicting long;based evolutionary causal;utilized predicting outcomes;outcomes diverse interventions;evolutionary causal matrices;predicting outcomes;outcomes interventions necessary;simulations based evolutionary;interventions;term outcomes interventions;interventions using;evolutionary causal;simulation program;simulation program based;multipurpose simulation program;multipurpose simulation;simulation model;diverse interventions;predicting outcomes diverse", "pdf_keywords": "simulation program python;simulate predicted outcomes;simulation program implemented;perform simulations;simulation program perform;simulation program;program perform simulations;developed simulation program;utilized predicting outcomes;users simulate predicted;multipurpose simulation program;enhanced simulation program;class structured simulation;simulations different datasets;multipurpose simulation;policy makers simulate;predicting outcomes;implementing computer simulation;simulate predicted;outcomes interventions using;structured simulation;data sets simulation;simulation model designed;simulation module;outcomes interventions;perform simulations different;simulation model;structured simulation module;computer simulation;simulations"}, "cc4cc594c7dd38482c46a2db440135b8f26ff54f": {"ta_keywords": "pt zn nanocubes;pt catalyst efficiently;pt catalyst;expensive pt catalyze;cut pt catalyst;pt catalyze;pt catalyze sluggish;skin pt78zn22 proposed;pt skin pt78zn22;membrane fuel cells;catalyze sluggish oxygen;high efficiency catalysis;o2 fuel cell;exchange membrane fuel;pt surface;pt surface urgent;pt78zn22 proposed;efficiency catalysis proton;oh pt surface;zn nanocubes high;pt zn;membrane fuel;pt78zn22 proposed high;h2 o2 fuel;pt78zn22 kb mass;catalysis proton exchange;zn nanocubes;pt bond reducing;fuel cells;catalyst efficiently", "pdf_keywords": ""}, "e2f015bbddd7bade7caca693e37f84c4cf70a7f5": {"ta_keywords": "negative matrix factorization;speech bases selection;matrix factorization based;matrix factorization;matrix factorization mnmf;speech recognition asr;speech bases;speech recognition;proposes speech bases;speech constructing bases;factorization based;factorization mnmf multi;automatic speech recognition;automatic speech;using automatic speech;factorization mnmf;factorization based spatial;target speech constructing;constructing bases phonemes;utterance improve performance;phonemes existing utterance;negative matrix;factorization;existing utterance improve;non negative matrix;recognition asr;optimization matrices;bases phonemes existing;speech constructing;recognition asr experiments", "pdf_keywords": ""}, "f8d7b263e8d663583cd22d5988c8ea4a49ed2840": {"ta_keywords": "entity relation extraction;entities extract relations;relation extraction;relation extraction aims;end relation extraction;extract relations simultaneously;extract relations;entities relations;relation extraction establish;entities relations fusing;representations entities relations;relations fusing entity;named entities extract;entities extract;entity relation;joint entity relation;contextual representations entities;features relation model;identify named entities;joint entity;relation model incorporating;relations fusing;relations simultaneously;relations;approach joint entity;named entities;representations entities;learning distinct contextual;input features relation;entity information", "pdf_keywords": "entity recognition relation;entity relation extraction;recognition relation extraction;relation extraction independently;relation extraction;encoders entity recognition;entity recognition;entities relations;representations entities relations;entities relations using;relations using entity;entities relations fusing;learns encoders entity;entity relation;relations fusing entity;contextual representations entities;relation extraction establish;features relation model;models refer entity;refer entity model;refer entity;entity model relation;trained independently relation;representations entities;relation model incorporating;input relation model;approach entity relation;relations using;relation model models;encoders entity"}, "249b7517a746b1389991e10fd618cad62e66c4df": {"ta_keywords": "synonym extraction parsed;extracting word synonyms;synonym extraction;synonyms corpus parsed;measures synonym extraction;word synonyms corpus;synonyms corpus;similarity measures synonym;word synonyms;graph based similarity;synonyms;similarity measures task;corpus parsed text;similarity measures;based similarity measures;corpus parsed;syntactic vector based;synonym;corpus;text constrained graph;similarity;task extracting word;extraction parsed text;based similarity;graph walk variant;parsed text constrained;extracting word;syntactic vector;extraction parsed;learn graph based", "pdf_keywords": ""}, "0718237a30408609554a0e2b90d35e37d54b1959": {"ta_keywords": "entropybased subword mining;subwordmine entropybased subword;word embeddings entropy;subwords fasttext embedding;subword mining;propose subwordmine entropybased;entropybased subword;subwordmine entropybased;entropy based subword;word embedding algorithms;based subword mining;subword mining application;word embeddings;word embedding;subword mining algorithm;known word embeddings;embedding vectors word;words subword structures;word embeddings traditionally;propose subwordmine;subwords fasttext;paper propose subwordmine;subword structures;subwordmine;mined subwords fasttext;embeddings traditionally word;words subword;utilizing mined subwords;traditionally word embedding;embeddings entropy based", "pdf_keywords": ""}, "771c1cb5fb161231e9aaa0a189caba672256a880": {"ta_keywords": "generated linguistically;words sequence morphemes;generating words;predicting words;knowledge predicting words;neural language models;morphological knowledge open;language models;predicting words using;words sequence characters;character sequence models;open vocabulary neural;generating word;generated linguistically na\u00efve;generating words sequence;directly generating word;word based models;using morphological knowledge;processes generating words;morphological knowledge;written morphological;models using morphological;generating word forms;word forms generating;vocabulary language model;vocabulary neural language;open vocabulary language;forms generating words;linguistic knowledge predicting;morphemes", "pdf_keywords": ""}, "7374494ee88608ef76f74b58a8f8c26ab06adfb9": {"ta_keywords": "clustering based diarization;overlapping speech frame;overlapping speech treating;overlapping speech;handle overlapping speech;based diarization methods;end diarization methods;diarization methods partition;diarization methods;diarization methods handle;based diarization;clusters number speakers;speech frame;diarization;end diarization;frame assigned speaker;end diarization method;end diarization model;speech frame assigned;based diarization compensate;diarization model;diarization method;speaker end;speech treating problem;use speaker end;end end diarization;diarization compensate;diarization model post;clustering;speaker end end", "pdf_keywords": "clusteringbased diarization using;clusteringbased diarization;method clusteringbased diarization;clustering based diarization;based diarization results;clusteringbased;based diarization;clustering;diarization results callhome;various types clustering;processing method clusteringbased;method clusteringbased;diarization results;dataset composed telephone;clustering based;datasets evaluations callhome;types clustering based;diarization;types clustering;diarization using;diarization model proposed;diarization model;callhome dataset composed;end diarization;diarization using end;callhome dataset;end diarization model;end end diarization;composed telephone conversations;telephone conversations"}, "6276bbe6cc56234d430725a31a27939eeec88149": {"ta_keywords": "document predicted quotability;bert based models;passage ranking;identification passage ranking;predicted quotability explore;quotability identification;predicted quotability;passage ranking problem;feature based bert;performance bert based;approach quotability identification;quotability identification given;based bert based;strong performance bert;task quotability identification;performance bert;quotability identification passage;based bert;bert based;quotability explore;quotability;quotable approach quotability;quotable approach;identify passages quotable;models rank passages;quotability explore problem;passages quotable approach;sequential sentence tagger;approach quotability;given document predicted", "pdf_keywords": "quoting patterns modeling;quotability features;quotable analyze;passages quotable analyze;quoting patterns;quotable analyze similarities;differences quoting patterns;quotability features identi\ufb01ed;passage ranking models;based passage ranking;identify passages quotable;quotability;quotability identi\ufb01cation;based models quotation;passage ranking;differences quoting;models equipped quotability;similarities differences quoting;models quotation;quoting labels large;quoting labels;quotability identi\ufb01cation given;passages quotable;computational linguistics source;quoting;quotable;linguistics source genres;source genres benchmark;computational linguistics;explore task quotability"}, "96d5e1f691397dfb51e8b818a21a2d11eee46a59": {"ta_keywords": "coded computation multicore;speed distributed computation;computation multicore setups;distributed computation coded;computation multicore;distributed computation;modern multicore processing;multicore processing;consider distributed computing;distributed computing;multicore processing architecture;worker computational times;exploiting modern multicore;multicore setups assuming;computation coded computation;coded computation;nodes equipped cores;multicore setups;worker computational;cores function;sparse linear codes;assuming worker computational;cores;distributed computing setup;modern multicore;multicore;coding theory;optimal runtime;faster uncoded schemes;computational times", "pdf_keywords": ""}, "75fe6c3ffdea2608794b4f21119c5a4dec07663a": {"ta_keywords": "sequence generation generative;neural machine translation;autoregressive sequence generation;generation generative flow;conditional sequence generation;machine translation nmt;sequence generation;autoregressive conditional sequence;generative flow specifically;translation nmt benchmark;machine translation;sequence seq2seq models;sequence generation using;generation generative;sequential latent variables;generative flow;generative flow elegant;sequential latent;non autoregressive sequence;autoregressive nmt models;models autoregressive generate;autoregressive sequence;autoregressive generate;generation using latent;density sequential latent;seq2seq models autoregressive;flowseq non autoregressive;autoregressive generate token;turn generative flow;autoregressive nmt", "pdf_keywords": "autoregressive sequence generation;sequence generation generative;neural machine translation;machine translation nmt;sequential latent variables;translation nmt benchmark;sequential latent;autoregressive conditional sequence;conditional sequence generation;non autoregressive generation;nonautoregressive nmt models;density sequential latent;generation generative flow;autoregressive generation;generation using latent;sequence generation;non autoregressive sequence;machine translation;autoregressive generation eq;neural sequence sequence;sequence seq2seq models;introduction neural sequence;generation generative;autoregressive sequence;models autoregressive generate;autoregressive generate;sequence generation using;nmt benchmark datasets;neural sequence;nonautoregressive nmt"}, "aa0b93501f79d57fe8542e72ccc8843ea50443c9": {"ta_keywords": "hmm seq2seq multilingual;seq2seq multilingual feature;seq2seq automatic speech;multilingual features;lingual seq2seq models;multi lingual seq2seq;multilingual feature;seq2seq multilingual;multilingual models;multi lingual training;multilingual feature techniques;multilingual features superior;training interestingly multilingual;features superior multilingual;multilingual models finding;interestingly multilingual features;superior multilingual models;speech recognition;lingual training stacked;lingual seq2seq;speech recognition asr;automatic speech;model hmm systems;automatic speech recognition;hmm systems;model hmm;hmm systems sequence;multi lingual;attention component training;ctc attention networks", "pdf_keywords": "sequence speech recognition;learning multilingual seq2seq;seq2seq automatic speech;hmm seq2seq multilingual;multilingual sequence sequence;multilingual features seq2seq;multilingual seq2seq model;multilingual sequence;seq2seq multilingual feature;speech recognition asr;lingual seq2seq models;sequence sequence speech;multilingual models;speech recognition;analysis multilingual sequence;multi lingual seq2seq;sequence speech;transfer learning multilingual;model hmm systems;seq2seq multilingual;multilingual seq2seq;superior multilingual models;multilingual models \ufb01nding;automatic speech;learning multilingual;automatic speech recognition;hmm systems sequence;multilingual features;speech recognition systems;model language transfer"}, "de971e50d70bc4d66f7debfab242942b0d1cae34": {"ta_keywords": "fusion speech summarization;speech summarization cascade;cascade speech summarization;ts speech summarization;speech summarization model;summarization performance attention;speech summarization generates;text summary speech;speech summarization;summarization cascade;summary speech achieved;summarization model robust;improve summarization performance;asr text summarization;fusion speech;attention based fusion;hypothesis fusion speech;summary speech;summarization model;combining automatic speech;summarization cascade approach;trained bert module;summarization performance;hypotheses attention based;representations transformers bert;summarization generates;automatic speech;bert ts speech;asr hypotheses attention;attention based multi", "pdf_keywords": "cascade speech summarization;perform speech summarization;speech summarization model;speech summarization experiments;speech summarization;propose cascade speech;trained bert module;cascade speech;summarization model robust;input bert based;asr input bert;bert based ts;attention based fusion;summarization model;summarization experiments;bert module;perform speech;input bert;voice throwing tips;expert voice throwing;bert based;summarization experiments how2;clip reference bertsum;word embedding vectors;asr hypotheses fusion;combine asr;combine asr hypotheses;trained bert;speech;attention based"}, "e0ab89821af308f51647bfe872f114d775fd8818": {"ta_keywords": "multilingual conversations medical;medical data multilingual;multilingual medical data;speech translation medical;data multilingual speech;multilingual speech recognition;multilingual medical;multilingual conversations;multilingual speech;medical situations multilingual;development multilingual medical;facilitate multilingual conversations;speech translation s2st;multilingual conversations reduce;translation medical domain;data multilingual;translate spoken utterances;speech speech translation;translation medical;speech translation;network based speech;chinese speech recognition;designed translate spoken;multilingual;medical data network;simulated medical conversation;situations multilingual conversations;medical conversation;medical conversation presented;speech recognition network", "pdf_keywords": ""}, "fba7ad8f63a42111b3618e51e3493ed70aafdcd0": {"ta_keywords": "estimating influences speakers;speaker depends word;word use speaker;influences speakers conversation;conversations people;conversations people tend;method conversations people;conversation data;speakers conversation;speakers conversation data;conversations;conversation data multiple;method conversations;word distribution propose;word distribution;influences speakers;conversation;speakers earlier word;general word distribution;proposed method conversations;use speaker depends;speaker depends;word use;assume word use;depends word use;estimating influences;propose probabilistic model;use speaker;speaker;model estimating influences", "pdf_keywords": ""}, "1ce96d8dbf69199ebd043de6cfa25d7e48b8ab03": {"ta_keywords": "causal effects linguistic;effects linguistic;effects linguistic properties;effects complaint politeness;noisy proxies linguistic;review sentiment;proxies linguistic properties;amazon review sentiment;review sentiment semi;sentiment semi simulated;proxies linguistic;linguistic;estimate causal effects;linguistic properties predictions;complaint politeness;complaint politeness bureaucratic;data estimate causal;politeness bureaucratic response;linguistic properties;effect amazon review;sentiment semi;bureaucratic response times;estimate causal;politeness bureaucratic;causal effects;predictions classifiers lexicons;sentiment;trained language model;linguistic properties method;classifiers lexicons", "pdf_keywords": "effects latent linguistic;causal effects linguistic;effects linguistic;latent linguistic;effects complaint politeness;effects linguistic properties;latent linguistic properties;amazon review sentiment;review sentiment;linguistic properties observational;review sentiment semi;complaint politeness bureaucratic;sentiment semi simulated;complaint politeness;researchers estimating causal;estimating causal effects;politeness bureaucratic response;causal effects latent;linguistic;effect amazon review;politeness bureaucratic;linguistic properties;sentiment semi;bureaucratic response times;estimating causal;nlp social;addressed setting nlp;algorithm estimating causal;investigating effects complaint;politeness"}, "c37ecbccecab1774b545a5a5804b575718218f7d": {"ta_keywords": "dnn bottleneck features;emotional speech recognition;cnn bottleneck features;features cnn bottleneck;speech recognition emotional;cnn dnn bottleneck;features emotional speech;bottleneck features cnn;recognition emotional speech;using dnn bottleneck;bottleneck features emotional;dnn bottleneck;improve emotional speech;features deep neural;cnn bottleneck;bottleneck features deep;features cnn;exploring cnn dnn;emotional speech degrades;using bottleneck features;model bottleneck features;emotional speech;bottleneck features;acoustic model bottleneck;using dnn;cnn dnn;speech degrades asr;features emotional;features deep;speech recognition", "pdf_keywords": ""}, "ae5a34c20fee705ad7094c93a711d5f724d535f0": {"ta_keywords": "fairness aware tensor;fairness tensor based;improving fairness tensor;tensor recommendation framework;fairness tensor;aware tensor recommendation;worsening fairness recommendations;tensor recommendation;fairness recommendations;fairness recommendations propose;factorization methods recommender;recommendation framework designed;recommendation framework;improved recommendation quality;traditional matrix factorization;matrix factorization methods;matrix factorization;sensitive latent factor;recommendation quality worsening;tensors achieve improved;fairness aware;aware tensor;novel fairness aware;tensor based;dramatically improving fairness;tensor based methods;recommendation quality;addressed tensors achieve;improving fairness;factorization methods", "pdf_keywords": "fairness aware tensor;tensor based recommendation;fairness implicit recommender;quality recommendation fairness;tensor recommendation framework;recommendation fairness effectiveness;recommendation fairness;aware tensor recommendation;tensor recommendation;recommendation framework designed;recommendation framework;factorization methods recommender;recommendation quality;traditional matrix factorization;comparing matrix tensor;recommendation quality recommendation;matrix tensor approaches;matrix factorization methods;aware tensor based;implicit recommender systems;matrix factorization;implicit recommender;aware tensor;fairness aware;recommender systems propose;enhance fairness implicit;novel fairness aware;tensor based methods;tensor based;matrix tensor"}, "ff187722c5b5462ac2066a737ae97650ffa177ed": {"ta_keywords": "students speech recording;automatic pronunciation assessment;recording classroom utterances;utterances student recorded;pronunciation assessment;speech recognition asr;performance automatic pronunciation;speech recognition;speech recording classroom;automatic speech recognition;pronunciation gop assessment;speech recording;use automatic speech;assessment utterances;assessment utterances just;automatic pronunciation;automatic speech;pronunciation assessment degraded;classroom utterances;technologies detect pronunciation;classroom utterances student;detect pronunciation errors;detect pronunciation;gop assessment utterances;students speech;individual students speech;pronunciation errors estimate;utterances student;language learning systems;recording classroom", "pdf_keywords": ""}, "0f61621206e363367db85b39e8e4325e425afcb4": {"ta_keywords": "singing voice conversion;convert singing voice;voice conversion;voice conversion based;voice preserving conversion;quality converted singing;converted singing voice;diffsvc statistical singing;voice conversion direct;statistical singing voice;singing voice preserving;convert singing;possible convert singing;conversion accuracy singer;speech quality converted;generated statistical singing;converted singing;quality voices generated;statistical singing;singing voice various;singing voice characteristics;singing voice degrades;voice preserving;natural singing voice;voices generated statistical;singer using vocoder;singing voice;improve quality voices;quality voices;voice characteristics source", "pdf_keywords": ""}, "94c3fd8eea08008cecd98f4aace024cf63954ead": {"ta_keywords": "filter malicious sensor;malicious sensor observations;attacks unknown sensor;malicious sensor detection;sensor detection secure;detection secure estimation;malicious sensor;secure remote estimation;problems malicious sensor;secure estimation;higher attack detection;attack detection;attack detection probability;secure estimation considered;detect injection attacks;detection secure;unknown sensor subset;estimation linear gaussian;filter malicious;attacks unknown;using sensor observations;sensor detection;sensor subset developed;sensor observations;sensor subset;observations multiple sensors;remote estimation;remote estimation linear;injection attacks unknown;detector detect injection", "pdf_keywords": "secure estimation;secure estimation presence;secure remote estimation;attack unknown sensor;attacks proposed detection;fdi attack detection;attack detection secure;attack safe sensor;improvement attack detection;attack anomaly estimates;attack detection offer;algorithm detects attack;detects attack anomaly;attack anomaly detection;attack detection;problem attack detection;detects attack;time secure estimation;detection secure;attack detection probability;detection estimates;algorithm fdi attack;fdi attacks proposed;anomaly detection estimates;detection estimates various;detection secure remote;attacks proposed;fdi attacks;attack fdi attack;improvement attack"}, "473021db54cbae9c4546597cd7e4b5d687a51c7f": {"ta_keywords": "training data crowdsourcing;voting incentives crowdsourcing;incentives crowdsourcing;data crowdsourcing vital;data crowdsourcing;workers crowdsourcing platform;workers crowdsourcing;crowdsourcing vital;rules workers crowdsourcing;crowdsourcing;workers crowdsourcing platforms;applications workers crowdsourcing;crowdsourcing platform;crowdsourcing platforms experts;crowdsourcing platforms;crowdsourcing vital tool;crowdsourcing platform typically;labeled training data;voting utilize expertise;labeled;labeled training;approval voting incentives;workers approval voting;choose label;expertise workers partial;expertise workers;experts making;need labeled training;introducing approval voting;workers partial knowledge", "pdf_keywords": "experts incentives;experts incentives workers;crowdsourced labels adversely;crowdsourced labels;quality crowdsourced labels;workers experts incentives;crowdsourced;quality crowdsourced;incentives;strictly proper incentive;proper incentive;proper incentive compatible;incentive;scale quality crowdsourced;incentives workers;voting utilize expertise;incentive compatible;incentive compatible compensation;deep learning demand;incentives workers aligned;workers partial knowledge;learning demand large;workers convey knowledge;guarantees optimality mechanism;expertise workers partial;guarantees optimality;learning demand;expertise workers;utilize expertise workers;partial knowledge"}, "042959b54176ad2c4f9d0966490ec407b6057527": {"ta_keywords": "federated learning framework;procedure federated learning;federated learning;design procedure federated;data protection oriented;protection oriented design;data protection;procedure federated;learning framework;protection oriented;federated;framework;oriented design procedure;protection;design procedure;oriented design;learning;data;design;procedure;oriented", "pdf_keywords": ""}, "90dd676184a796e3e5835c8e1f6a632985ce3e89": {"ta_keywords": "predicting embryo morphokinetics;embryo morphokinetics videos;embryo morphokinetics;morphokinetics videos;predicting embryo;morphokinetics videos late;morphokinetics;embryo;videos late fusion;late fusion nets;fusion nets;fusion nets dynamic;nets dynamic decoders;dynamic decoders;decoders;videos;fusion;predicting;nets dynamic;late fusion;dynamic;videos late;nets;late", "pdf_keywords": ""}, "80cb8981af401d9e4df0096626553c514d9e6600": {"ta_keywords": "y2ti2 xsnxo7 pyrochlore;mn4 y2ti2 xsnxo7;spectrum mn4 y2ti2;optical spectrum mn4;xsnxo7 pyrochlore solid;mn4 y2ti2;xsnxo7 pyrochlore;y2ti2 xsnxo7;pyrochlore solid solution;spectrum mn4;pyrochlore solid;mn4;y2ti2;pyrochlore;optical spectrum;xsnxo7;solid solution;solution line energy;optical;solid solution line;energy intensity;line energy intensity;solid;line energy;spectrum;energy;intensity;solution line;solution;line", "pdf_keywords": ""}, "12b12ea73652da56023e0e4776211e4f4301f339": {"ta_keywords": "argumentation mining web;annotated argumentation corpora;reliably annotated argumentation;argumentation mining;annotated argumentation;argumentation argumentation mining;argumentation corpora wide;argumentation corpora;annotation scheme argumentation;argumentation argumentation;argumentation theories;argumentation theory;argumentation mining function;expressing argumentation argumentation;fits argumentation theory;argument components annotated;argumentation theory applied;expressing argumentation;argumentation;argue annotation;argumentation theories actual;fits argumentation;scheme argumentation mining;argue annotation scheme;paper argue annotation;used expressing argumentation;annotation studies;gap argumentation theories;argument components;corpus properties annotation", "pdf_keywords": ""}, "77b919c4f4f37415d8f1019b1b04191d46de426c": {"ta_keywords": "walks pcrw similarity;recommendation retrieval;nodes recommendation retrieval;recommendation retrieval tasks;proximity queries labeled;recommendation retrieval problems;retrieval models;retrieval models based;walk based proximity;query nodes recommendation;experiments recommendation retrieval;random walks study;represented proximity queries;constrained random walks;random walks pcrw;random walk based;proximity queries;walks study;random walks;queries labeled directed;similarity defined learned;nodes recommendation;based proximity measures;retrieval;queries labeled;walks pcrw;used random walk;fast query;away query nodes;retrieval tasks", "pdf_keywords": ""}, "e862e5f9a17938f1817017b2730e10463d94fb54": {"ta_keywords": "isometric embedding spaces;non isometric embedding;isometric embedding;bliss non isometric;embedding spaces;embedding;non isometric;bliss;bliss non;isometric;spaces;non", "pdf_keywords": ""}, "2826ac3621fdd599303c97cb9e32f165521967b2": {"ta_keywords": "uncertainty prediction medical;better uncertainty classification;direct uncertainty prediction;trained uncertainty scores;uncertainty classification;trained uncertainty;medicine direct uncertainty;uncertainty prediction;models trained uncertainty;predict uncertainty score;high expert disagreements;expert disagreements;predict uncertainty;prediction medical second;machine learning medicine;disagreements human experts;expert disagreements particular;uncertainty score directly;model predict uncertainty;prediction medical;uncertainty prediction dup;uncertainty classification step;machine learning;machine learning models;direct uncertainty;better uncertainty;human experts ubiquitous;human experts;uncertainty scores;training classifier", "pdf_keywords": "trained uncertainty;trained uncertainty scores;models trained uncertainty;direct uncertainty prediction;predict label disagreement;uncertainty prediction;uncertainty prediction data;uncertainty prediction dup;better uncertainty classi\ufb01cation;high expert disagreement;predict uncertainty;predict uncertainty score;prediction dup training;expert disagreement;training data demonstrate;models trained;uncertainty classi\ufb01cation;expert disagreement work;machine learning;training data;model predict uncertainty;learning models trained;machine learning models;high expert disagreements;performing direct uncertainty;better uncertainty;ability predict label;uncertainty score directly;expert disagreements;learning models successfully"}, "a7f30bae9303825adbc333a8df8a03398dea5151": {"ta_keywords": "rules sentiment classification;logic rules sentiment;contextualized elmo embeddings;sentiment classification contrast;elmo embeddings;sentiment classification models;sentiment classification;rules sentiment;contextualized elmo;classification models syntactically;using contextualized elmo;visualizations demonstrate elmo;different sentiment classification;elmo ability implicitly;learn logic rules;elmo embeddings peters;instead logic rules;inputs like sentences;performance different sentiment;implicitly learn logic;elmo ability;elmo;demonstrate elmo ability;models syntactically complex;demonstrate elmo;syntactically complex inputs;logic rules yields;learn logic;revisiting importance encoding;syntactically complex", "pdf_keywords": "sentiment classi\ufb01cation complex;logic rules sentiment;sentiment classi\ufb01cation models;sentiment classi\ufb01cation systems;rules sentiment classi\ufb01cation;ambiguity sentiment classi\ufb01cation;sentiment ambiguity data;sentiment classi\ufb01cation task;improve sentiment classi\ufb01cation;sentiment classi\ufb01cation;improve sentiment;word embedding model;sentences ambiguous sentiment;contextualized word embeddings;sentiment ambiguity;contextualized word embedding;inherent sentiment ambiguity;word embeddings;sentences crowdsourced;rules sentiment;word embedding;vs negative sentences;sentences crowdsourced experiment;different sentiment classi\ufb01cation;sentiment classi\ufb01cation kalpesh;ambiguity sentiment;inherent ambiguity sentiment;sentiment classi\ufb01cation positive;ambiguous sentiment;negative sentences"}, "203da29a37a983c487ce75a894b0d70698077bf5": {"ta_keywords": "problematic news sources;encountering problematic news;problematic information facebook;problematic content news;spread problematic information;sources problematic information;detection problematic information;problematic content;problematic news;news media sources;media ecosystems attempts;content news media;problematic information contemporary;information facebook;problematic information;news media;disinformation recent electoral;disinformation;contemporary media ecosystems;fact checkers;content news;disinformation recent;media ecosystems;coordinated inauthentic behavior;information facebook using;news sources;news sources different;facebook;content bad actors;information contemporary media", "pdf_keywords": ""}, "7f588b1d2a5b199a19a4c3bad6bd5154c7355817": {"ta_keywords": "protein premodification imbs;immunomagnetic beads imbs;imbs certain proteins;performance immunomagnetic beads;enrichment performance immunomagnetic;imbs blood samples;blood proteins including;imbs blood;immunomagnetic beads;beads imbs blood;blood proteins;abundant blood proteins;environment blood proteins;performance immunomagnetic;promisingly human serum;protein corona generally;human serum albumin;protein corona camouflage;applied protein corona;immunomagnetic;serum albumin camouflaged;benefits circulating tumor;fetal bovine serum;immunoglobulin transferrin;properties applied protein;human serum;blood proteins endows;immunoglobulin transferrin separately;including human serum;protein corona", "pdf_keywords": ""}, "44aa9a79cfc9eef9ac3f861cfa58a172cb863bd2": {"ta_keywords": "watset", "pdf_keywords": ""}, "aeffb61024e5ccac5021ca0bf9d199d9196a0521": {"ta_keywords": "tolls population players;mdp congestion game;congestion game framework;modeling player congestion;congestion costs adaptive;congestion game;congestion games;congestion games application;process congestion games;enforce tolls population;player congestion costs;coupled congestion costs;enforcing tolls compute;congestion minimizing;toll value constraint;compute minimum toll;congestion costs;congestion minimizing impact;enforce tolls;congestion costs assume;population players stochastic;toll value ensuring;tolls compute;enforcing tolls;minimum toll value;satisfied enforcing tolls;player congestion;constraints population players;tolls compute minimum;repeatedly enforce tolls", "pdf_keywords": "constraints mdp congestion;tolled mdp congestion;mdp congestion game;mdp congestion games;congestion game mdp;tolling constraint satisfaction;tolling constraint;congestion costs adaptive;congestion game;congestion games;increasing congestion costs;congestion transportation networks;tolling algorithm;congestion games unknown;based tolling algorithm;congestion games application;reducing congestion transportation;congestion game unknown;unknown congestion costs;tolling algorithm enforces;congestion transportation;congestion costs;process congestion games;problem tolling constraint;game unknown congestion;class mdp congestion;mdp congestion;tolled game;congestion;oracle tolled game"}, "dc1d1f64503578d9c5d906da4556f631d4178b04": {"ta_keywords": "vehicle collision prediction;predict vehicle collisions;games predict vehicle;deep learning crash;collision prediction;cnn based;cnn;collision prediction algorithms;best cnn based;predict vehicle;learning crash crash;best cnn;danger collision predicted;collision predicted;learning crash;cnn architectures;large accident data;vehicle collisions;modern cnn architectures;video games predict;gta using accident;deep learning;observe best cnn;modern cnn;based modern cnn;accident data;games predict;developments deep learning;cnn based algorithm;collect large accident", "pdf_keywords": ""}, "de5057c1da9391269e926d4661d4558072db9f18": {"ta_keywords": "end speech recognition;train attention fusion;fusion based attention;attention fusion;speech recognition parallel;attention fusion module;speech recognition stage;speech recognition;recognition parallel encoders;stream model trained;stream level fusion;end automatic speech;universal feature extractor;ufe features pretrained;multi stream end;end end speech;end speech;automatic speech;automatic speech recognition;feature extractor ufe;extractor ufe encoder;features pretrained components;features pretrained;single stream;ufe encoder;train attention;attention mechanisms combine;recognition stage train;data joint training;parallel encoders aim", "pdf_keywords": "fusion wav;level fusion wav;improving asr performance;prediction fusion;fusion wav alignment;prediction fusion using;signal level fusion;train multi stream;training strategy mem;feature extractor trained;highlevel ufe features;level prediction fusion;universal feature extractor;substantially improving asr;improving asr;asr performance;mem array framework;word level fusion;level fusion models;fusion strategies;ufe features;encoders universal feature;extractor trained stage;fusion models utilized;multi stream end;fusion;fusion using rover;end asr stage;fusion using;multi encoder"}, "281605579936538ee92bc4b0baad1b83c683c076": {"ta_keywords": "parsing generation;parsing generation task;representation parsing generation;meaning representation parsing;graph dependency parse;representation parsing;train parse text;language generation amr;dependency parse tree;parsing;abstract meaning representation;dependency parse;parse text linearizer;generation task semeval;parse;parse tree;language generation;parse text;train parse;generation task;semeval 2017 task;content function words;parse tree added;similar dependency tree;task semeval 2017;function words;amr graph dependency;task semeval;dependency tree;text linearizer", "pdf_keywords": ""}, "4ae0c4a511697e960c477ea3e37b3e11bf3e0e02": {"ta_keywords": "convolutional networks penalizing;imagenet sketch;introduce imagenet sketch;imagenet classification;training robust convolutional;robust convolutional networks;imagenet sketch new;robust global representations;imagenet classification validation;convolutional networks;imagenet;matches imagenet classification;transfer introduce imagenet;gleaned local receptive;training robust;introduce imagenet;image data convolutional;local representations learned;learning robust global;convolutional neural;convolutional neural networks;robust convolutional;learning robust;local receptive;representations learned;images matches imagenet;sketch like images;local receptive fields;neural networks known;matches imagenet", "pdf_keywords": "benchmark domain adaptation;domain adaptation;domain adaptation tasks;adversarial regularization;wise adversarial regularization;training robust convolutional;convolutional networks penalizing;adversarial regularization par;patch wise adversarial;adversarial;training robust;robust convolutional networks;coerce convolutional neural;wise adversarial;local representations learned;learned earlier layers;convolutional networks;synthetic benchmark domain;representations learned;imagenet;robust convolutional;method training robust;confers improved generalization;learning scheme penalizes;introduce imagenet;convolutional neural;networks penalizing predictive;coerce convolutional;regularization par learning;convolutional neural networks"}, "ce4db7a32724e0abc8afe27f74d33e32e099b8e6": {"ta_keywords": "porcine fit1 gene;porcine fit1 promoter;porcine fit1 coding;promoter activity myogenesis;activated porcine fit1;activator porcine fit1;myogenic expression;modulating myogenic expression;porcine fit1;myod involved muscle;fit1 promoter activity;myogenic expression analysis;fit1 gene;upstream porcine fit1;fit1 gene define;fit1 gene interacting;activity myogenesis identified;myogenesis identified;activity myogenesis;region porcine fit1;fit1 gene human;role fit1 gene;sequence modulating myogenic;myogenesis;fit1 promoter;muscle development;involved muscle development;novel activator porcine;myogenesis cloned;muscle development gain", "pdf_keywords": ""}, "7506626f776f211afac2c2d1138aca0e0479e5c3": {"ta_keywords": "gans transform latent;images generated gan;networks gans transform;networks gans;generated gan precisely;generative adversarial networks;adversarial networks generative;networks generative adversarial;gans transform;generated gan;adversarial networks gans;gan precisely recover;generative adversarial;vectors generative adversarial;gan precisely;gans;stochastic clipping;latent vectors generative;gan formulation;networks generative;projecting images latent;stochastic clipping generally;original gan formulation;original gan;images latent space;gan formulation gives;images latent;adversarial networks;gan;thought original gan", "pdf_keywords": "gans transform latent;networks gans;networks gans transform;gan precisely recover;images generated gan;generated gan precisely;vectors generative adversarial;generative adversarial networks;generated gan;adversarial networks gans;gans transform;generative adversarial;gan precisely;gans;latent vectors generative;gan formulation;adversarial networks zachary;reconstruct latent vectors;original gan formulation;adversarial networks;recovery latent vectors;original gan;gan formulation gives;adversarial;abstract generative adversarial;gan;reconstruct latent;projecting images latent;precisely recover latent;images latent space"}, "712cd873d7370db280f4ceaaf000dc49f76b59fe": {"ta_keywords": "adversarial perturbations sequence;evaluation adversarial perturbations;allows adversarial perturbations;adversarial perturbations;adversarial attacks seq2seq;adversarial examples perturbations;adversarial perturbations meaning;attacks machine translation;evaluation adversarial;adversarial;adversarial attacks;adversarial examples;allows adversarial;attacks allows adversarial;evaluation framework adversarial;framework adversarial;framework adversarial attacks;account evaluation adversarial;robustness sequence sequence;robustness sequence;assessing robustness sequence;attacks seq2seq models;additional constraints attacks;constraints attacks;example untargeted attacks;constraints attacks allows;sequence seq2seq models;untargeted attacks;machine translation;untargeted attacks machine", "pdf_keywords": "performing untargeted adversarial;untargeted adversarial training;adversarial attacks seq2seq;adversarial robustness hurting;untargeted adversarial;training adversarial;adversarial inputs semantically;training adversarial examples;adversarial training;making adversarial;adversarial training adversarial;adversarial perturbations compared;terms adversarial robustness;attacks machine translation;terms adversarial;adversarial robustness;adversarial training meaning;evaluation framework adversarial;adversarial;robustness adversarial attacks;adversarial perturbations;model terms adversarial;existing methods adversarial;robustness adversarial;making adversarial inputs;adversarial examples;methods adversarial training;adversarial attacks;framework adversarial perturbations;bene\ufb01cial robustness adversarial"}, "1c8d9d5558dc43f3505fa37fc50247e3ce0d2f54": {"ta_keywords": "graph unobserved confounder;dataset confounder unobserved;confounded observational dataset;dataset confounder revealed;observational dataset confounder;confounder unobserved;unobserved confounder;deconfounded observational dataset;confounder unobserved alongside;dataset confounder;confounded data significantly;confounder revealed estimating;unobserved confounder average;selectively deconfounded data;confounded data;confounder average treatment;inference selectively deconfounded;confounding graph unobserved;inclusion confounded data;confounder revealed;deconfounded data;deconfounded data required;confounder;large confounded observational;confounded observational;confounder average;collect deconfounded data;deconfounded data finally;selectively deconfounded;deconfounded observational", "pdf_keywords": "dataset confounder unobserved;confounded observational dataset;observational dataset confounder;deconfounded observational dataset;confounder revealed estimating;dataset confounder revealed;confounder unobserved;confounded data achieves;dataset confounder;incorporating confounded data;confounder unobserved alongside;confounded data;causal inference estimate;large confounded observational;inference estimate causal;deconfounded data;confounder revealed;deconfounded data required;confounded observational;confounder;deconfounded observational;inclusion confounded data;confounded data signi\ufb01cantly;estimate causal;causal inference;small deconfounded observational;incorporating large confounded;incorporating confounded;problem causal inference;estimate causal e\ufb00ects"}, "4d86b32ea80e2d9df2283fac39892d6dbd87ea87": {"ta_keywords": "discriminative training methods;discriminative training;pattern recognition minimum;recognition minimum classification;minimum error classification;increased discriminative training;classifiers;prototype based classifiers;minimum classification;methods pattern recognition;pattern recognition;minimum classification error;methods svm;classification tasks derive;based classifiers;existing methods svm;class discriminant functions;classification tasks;training methods pattern;classification;discriminant functions develop;range classification;error classification;range classification tasks;recognition minimum;discriminant functions;general class discriminant;discriminative;classification various;wide range classification", "pdf_keywords": ""}, "8786ddc38ae0763e772337bf9331436252452918": {"ta_keywords": "detect fake news;news detect fake;fake news detection;fake news detectors;entity bias real;mitigating entity bias;entity bias;generalizes fake news;unintended entity bias;entity debiasing;entity bias cause;news detection models;entity debiasing framework;existing fake news;data fake news;news detectors;propose entity debiasing;entities news;entities news contents;news veracity;news detection aims;ability fake news;fake news future;news veracity separately;news detectors online;contents news veracity;predict news pieces;predict news;news detect;news detection", "pdf_keywords": "entity debiasing;entity debiasing framework;propose entity debiasing;mitigating entity bias;entity bias fake;entity bias;designed debiasing framework;debiasing framework based;entities news veracity;debiasing framework;fake news detection;fake news detectors;generalizes fake news;fake news detector;bias fake news;entity bias cause;designed debiasing;specifically designed debiasing;entities news;debiasing;future mitigating entity;effect entities news;news veracity explicitly;fake news increasingly;inference perform debiased;dissemination fake news;perform debiased prediction;news detection models;mitigating entity;perform debiased"}, "9d0e4e9c9343b85311b1adff145fdbdfb69486ff": {"ta_keywords": "telescope observation scheduling;hubble space telescope;observation scheduling;space telescope observation;systems tools hubble;space telescope;expert systems tools;telescope observation;tools hubble;telescope;tools hubble space;expert systems;hubble;hubble space;scheduling;systems tools;expert;tools;observation;systems;space", "pdf_keywords": ""}, "cc74ef901219dfd26efbbb8b7b87d1b7b7d38634": {"ta_keywords": "historical texts neural;normalization historical texts;normalization historical;texts neural;texts neural network;automatische normalisierung historischer;normalization;neural network models;historical texts;normalisierung historischer;normalisierung historischer sprachdaten;neuronaler encoder decoder;anwendung neuronaler encoder;models historische dokumente;automatische normalisierung;die automatische normalisierung;texts;decoder modelle;neural network;neuronaler encoder;network models historische;decoder;models historische;neural;encoder decoder modelle;historischer sprachdaten;decoder modelle fur;normalisierung;encoder decoder;anwendung neuronaler", "pdf_keywords": ""}, "cd06dfa789bfe491130ac7440e55d9d407396a43": {"ta_keywords": "acceleration strategy rcd;rcd importance sampling;rate rcd linear;conjugate descent methods;rcd linear rate;linear rate rcd;rcd based augmentation;spectral conjugate descent;rcd linear;descent methods;rate rcd;directions sampled rate;stochastic spectral conjugate;sampled rate method;rcd importance;acceleration strategy;rcd based;strategy rcd based;spectral conjugate directions;limitations rcd importance;extra directions sampled;directions sampled;stochastic spectral;type acceleration strategy;importance sampling;directions spectral conjugate;strategy rcd;rcd;method improves interpolates;rate method improves", "pdf_keywords": "randomized coordinate descent;coordinate descent rcd;conjugate descent methods;coordinate descent;spectral conjugate descent;descent methods;acceleration strategy rcd;descent methods arxiv;methods spectral conjugate;variants methods spectral;descent rcd;methods spectral;methods solving optimization;stochastic spectral conjugate;stochastic spectral;variants randomized coordinate;dimensions variants randomized;rcd linear;spectral conjugate directions;rate rcd linear;optimization;optimization problems big;descent rcd develop;rcd based augmentation;linear rate rcd;rcd linear rate;solving optimization;directions spectral conjugate;solving optimization problems;directions spectral"}, "aacaad6ab396e085799052b1a667c965d6465e32": {"ta_keywords": "protein emulsion;emulsion reduction protein;protein membrane oil;protein oil droplet;types protein emulsion;protein emulsifying activity;protein emulsifying;reduction protein emulsifying;protein emulsion degree;interaction protein oil;protein oil;membrane destroyed emulsion;membrane oil;membrane oil droplets;emulsifying activity emulsion;activity emulsion stability;emulsion nem blocking;emulsion reduction;emulsion droplet;activity emulsion;emulsion stability;destroyed emulsion droplet;emulsion nem;emulsion;content emulsion nem;increasing emulsion reduction;emulsion stability occurred;nem content emulsion;spi used emulsifiers;emulsion droplet size", "pdf_keywords": ""}, "d5810f15cfdd59da549ffa648c5a05d806d94eb7": {"ta_keywords": "fact checking platform;checking platform journalistic;automated fact checking;platform journalistic setting;fact checking essential;feedback journalists workflow;journalists workflow;fact checking;platform journalistic;textual evidence;using feedback journalists;verdict fact checking;evidence document collection;efforts combating misinformation;automated fact;feedback journalists;study fact checking;textual evidence document;journalistic setting integrated;present automated fact;combating misinformation;task journalism;journalistic setting;journalism importance highlighted;combating misinformation conduct;relevant textual evidence;essential task journalism;journalism;integrated collection news;journalistic", "pdf_keywords": "automated fact checking;factchecking platform journalistic;user study factchecking;factchecking platform;fact checking essential;fact checking news;study factchecking platform;information retrieval evaluation;information retrieval;fact checking;retrieval tasks;task journalism;factchecking;automated fact;misinformation automated fact;results retrieval tasks;study factchecking;retrieval evaluation;task journalism importance;retrieval tasks goals;journalists precision;abstract fact checking;journalists precision predictions;retrieved evidence sentences;retrieval results;evaluation retrieval;evaluation retrieval results;retrieval;essential task journalism;information extraction human"}, "a5148776955ef523de318a2fb45f8256e966b98e": {"ta_keywords": "distantly labeled data;nodes entity mentions;propagation lists labeling;label propagation lists;label propagation;label propagation graph;distantly labeled;using label propagation;entity mentions mentions;entity mentions;lists labeling approach;supervision information extraction;used distantly labeled;labeling approach;distant supervision information;lists labeling;improving distant supervision;labeled data;labeling;information extraction;graph nodes entity;information extraction using;labeling approach leads;list structures improving;extraction using label;labeled;noise using label;nodes entity;mentions mentions coupled;classifiers used distantly", "pdf_keywords": ""}, "05c2bb89a5c42ad7932420bb39df2e566df6e1ec": {"ta_keywords": "annotated data stochastic;annotation editor;annotation data;abstract annotation editor;annotated data;annotators create annotated;annotation data manually;annotators highly;create annotated;annotators;annotation;human annotators;human annotators create;annotated training data;create annotated data;annotators create;annotated training;abstract annotation;annotators highly timeconsuming;annotated;annotation editor recent;team annotators highly;manually team annotators;team annotators;java abstract annotation;aid human annotators;nlp natural language;processing natural language;lot annotated training;recent trends nlp", "pdf_keywords": ""}, "2127bea25859ba9c5997e2d15e17899a75ef6cb3": {"ta_keywords": "programs big code;programming big code;programming big;big code developing;big code term;programming challenges big;15472 programming big;programs big;statistical programming tools;challenges big code;code developing statistical;big code;programming tools;programming tools learn;seminar 15472 programming;programming;testing programs big;program outcomes dagstuhl;millions programs;important interesting programming;developing statistical tools;programming languages software;software engineering highly;programs open source;software engineering;code developing;interesting programming;big code order;kinds statistical programming;15472 programming", "pdf_keywords": ""}, "3e2bac2abfb5b33a43fe56db5a868e17e38c616a": {"ta_keywords": "teaching machine learning;teaching machine;classes master students;master students;students learn theoretical;students participate projects;master studies students;theoretical classes students;approach teaching machine;master studies;master students ural;classes master;students project work;projects;project topics experienced;learn theoretical;computing classes master;institutes russian academy;teaching;learning high performance;classes students;work result master;university institutes russian;students learn;participate projects;russian academy sciences;institutes russian;students;studies students;project topics", "pdf_keywords": ""}, "baf47cd0b471a9bb7b2230fec0b680fc9b3c4783": {"ta_keywords": "descriptions pragmatic listener;build pragmatic speaker;pragmatic inference improves;pragmatic inference aids;pragmatic models;pragmatic speaker uses;pragmatic speaker;previous pragmatic models;pragmatic listener;listener speaker models;pragmatic models use;explicit pragmatic inference;pragmatic inference;descriptions explicit pragmatic;descriptions pragmatic;human instructions speaker;pragmatic listener reasons;natural language instructions;pragmatics enabled models;instructions speaker models;sequential tasks pragmatics;models build pragmatic;pragmatics enabled;pragmatics;language generation interpretation;explicit pragmatic;listener simulate interpretation;listener speaker;candidate descriptions pragmatic;shows pragmatic inference", "pdf_keywords": "pragmatic inference improves;pragmatic inference models;reasoning human speaker;pragmatic reasoning abilities;instruction generation tasks;explicit pragmatic inference;explicit pragmatic reasoning;reasoning human listener;instruction generation incorporating;pragmatic inference;instruction generation;human instructions speaker;pragmatic reasoning;instructions speaker models;interpreting human instructions;language generation interpretation;models explicit pragmatic;models interpreting instructions;evaluation language generation;model pragmatics helps;shows pragmatic inference;following instruction generation;model pragmatics;language generation;interrelated tasks instruction;listener models interpreting;explicit pragmatic;accuracy listener models;human speaker improves;explicit model pragmatics"}, "00cc6deb3cf2c9281ddcf4875aad3ee14c92e52f": {"ta_keywords": "lingual named entity;translating entities matching;translating entities;machine translation cross;leveraging machine translation;named entity recognition;subsequently translating entities;recognition entity projection;entity recognition;translation cross lingual;entity recognition entity;projection machine translation;machine translation systems;entity recognition diverse;corpora named entity;large annotated corpora;translation systems;machine translation;lingual ner approach;cross lingual ner;annotated corpora;entity projection machine;methods cross lingual;translation systems subset;cross lingual named;annotated corpora named;entity projection methods;translation systems twice;lingual ner;shelf machine translation", "pdf_keywords": "translating entities matching;translating entities matches;mt translating entities;translating entities;subsequently translating entities;named entity recognition;languages entity projection;machine translation cross;leveraging machine translation;entity recognition;machine translation systems;entity recognition propose;projection machine translation;translation cross lingual;machine translation;translation systems;large annotated corpora;languages entity;corpora named entity;language shelf translation;results cross lingual;translation systems subset;entities matching;translation systems exist;entities matching entities;annotated corpora;cross lingual ner;annotated corpora named;entity projection machine;matching entities"}, "be360de73689dc4af56f7adcee7e38d7acfed1e1": {"ta_keywords": "fairness algorithm;stronger notion fairness;fairness algorithm presented;notion fairness algorithm;aggregate orderings;rankings smaller set;notion fairness;aggregate orderings particular;aggregate ordering;aggregating orderings;aggregate orderings maintains;just aggregate ordering;orderings rankings;set orderings rankings;orderings rankings smaller;presented aggregating orderings;certain properties fairness;game match race;set aggregate orderings;rankings smaller;match race available;fairness work;list aggregate orderings;fairness;match race;orderings;rankings;fairness work considers;properties fairness;orderings particular", "pdf_keywords": ""}, "0f5bb9ae0c060b349597c0b2582bf271a5a2156a": {"ta_keywords": "ccg supertagging parsing;supertagging parsing;supertagging parsing analyze;ccg supertagging;performance ccg supertagging;lstms pos tagging;tagging models encode;information encoded supertags;supertagging;encoded supertags model;tagging models;encoded supertags;supertags model outperforms;pos tagging models;supertags;supertags model;bidirectional lstms pos;bidirectional lstms;tagging;parsing;pos tagging;syntactic information encoded;long range syntactic;parsing analyze performance;compete bidirectional lstms;syntactic information;syntactic;parsing analyze;lstms pos;lstms", "pdf_keywords": ""}, "ce268e0942ce0d1f6942e4d7e7e2aa6464f1b577": {"ta_keywords": "handcrafted pronunciation lexicons;speech recognition asr;automatically learn pronunciation;native pronunciation lexicon;pronunciation learning;native pronunciations design;learn pronunciation lexicon;non native pronunciations;native pronunciation able;non native pronunciation;non native asr;data driven pronunciation;handcrafted pronunciation;pronunciation lexicon train;native asr;native asr utilizing;learn pronunciation;driven pronunciation learning;pronunciation learning zero;native pronunciation results;native pronunciation;native pronunciations;pronunciation lexicon iterative;native pronunciation variations;pronunciation lexicon;speech recognition;automatic speech recognition;speakers non native;pronunciations design new;pronunciation lexicons", "pdf_keywords": ""}, "cb53f9558bd13c853026f97dce3bbe3d989ca97d": {"ta_keywords": "game learning argumentation;argumentation fallacies german;learning argumentation fallacies;effective campaigns argumentation;campaigns argumentation;argumentation controversies culture;campaigns argumentation controversies;argumentation language;daily argumentation language;argumentation controversies;argumentation fallacies;daily argumentation;learning argumentation;deals daily argumentation;argumentation;argumentation language requires;controversies culture language;fallacies german;fallacies german context;argotario game learning;argotario game;controversies culture;game platform topic;creation effective campaigns;successful game platform;game learning;behavior game created;game platform;german context examine;users behavior game", "pdf_keywords": ""}, "57dd2bd5fb6677191f9b36b589c91bb171e217ff": {"ta_keywords": "queries based textual;web using queries;providing database like;queries based;database like;based textual similarity;using queries based;database like access;textual similarity;database;providing database;using queries;like access web;queries;web using;similarity;based textual;access web;access web using;web;textual;like access;access;based;using;like;providing", "pdf_keywords": ""}, "a4dd375c18709b1554249cc5cb88d8ba6acfea10": {"ta_keywords": "machine translation couldn;translation systems;use machine translation;machine translation;machine translation systems;machine translation long;forward machine translation;abstract machine translation;translation couldn starting;translation couldn;translation;translation long;translation long held;use machine;abstract machine;machine;real use machine;appearance computers;computers years reached;forward machine;steps forward machine;computers;appearance computers years;systems;able paper describes;computers years;enormous steps forward;able paper;steps forward;contributed enormous steps", "pdf_keywords": ""}, "a95400c70c4beb609c77cc500677b2f1ed852e8e": {"ta_keywords": "automated question generation;question generation improves;question generation;generating questions multiple;generating questions;approach automated question;automated question;paraphrase detection;resolution paraphrase detection;coreference resolution paraphrase;sentences coreference resolution;multiple sentences coreference;sentences coreference;answers using semantic;language learners generating;texts evaluation human;texts evaluation;evaluation human annotators;semantic understanding texts;assessment perspective generating;choice answers using;questions multiple choice;questions multiple;learners generating;steps multiple sentences;multiple choice answers;coreference resolution;questions utilize;perspective generating questions;learners generating multiple", "pdf_keywords": ""}, "63d99a61e798d7cb714f336a8d581ae2b75672ee": {"ta_keywords": "resource speech challenge;zero resource speech;speech challenge;phoneme discriminative representation;speech challenge 2021;phoneme discriminative;phonetic metric lexical;phonetic metric;35 phonetic metric;resource speech;lexical metric deep;contrastive predictive coding;pseudo labels supervised;improvement 35 phonetic;deep cluster train;phonetic;deep cluster;metric deep cluster;predictive coding;gain lexical metric;combines contrastive predictive;manner phoneme discriminative;syntactic metric achieved;supervised manner phoneme;contrastive predictive;lexical metric;cpc deep cluster;gain lexical;phoneme;35 phonetic", "pdf_keywords": "deep cluster zerospeech;resource speech challenge;cpc deep cluster;cluster zerospeech challenge;speech representation learning;speech challenge;speech challenge 2021;deep cluster method;zero resource speech;speech representation;deep cluster \ufb01rst;model deep cluster;deep cluster;metric deep cluster;coding cpc deep;network kmeans deep;deep cluster addition;cpc model deep;cluster zerospeech;conformer cpc deep;kmeans deep cluster;lexical metric deep;zerospeech challenge 2021;resource speech;cpc deep;zerospeech challenge;clustering outputs cpc;predictive coding cpc;phonetic metric lexical;contrastive predictive coding"}, "d4305b3bf233e5f192a5d17dde114b771b621d92": {"ta_keywords": "influence relation estimation;lexical entrainment conversation;influence relation;relation estimation based;based lexical entrainment;entrainment conversation;estimation based lexical;lexical entrainment;relation estimation;conversation;influence;based lexical;lexical;relation;entrainment;estimation based;estimation;based", "pdf_keywords": ""}, "de3c3eb590065a6d78ec8566161f8236ab2a7435": {"ta_keywords": "workshop asian translation;translation subtasks mixed;patent translation subtasks;paper translation subtasks;translation results submitted;translation wat2017 including;translation subtasks patent;translation results;asian translation wat2017;translation subtasks;subtasks patent translation;300 translation results;translation wat2017;subtasks 300 translation;scientific paper translation;patent translation;paper translation;asian translation;translation;workshop asian;4th workshop asian;300 translation;evaluated wat2017;submitted automatic evaluation;manually evaluated wat2017;tasks 4th workshop;evaluated wat2017 12;results shared tasks;automatic evaluation server;shared tasks", "pdf_keywords": ""}, "ffb562d3ac7d86b5c527863f5a3e72e1aa22a809": {"ta_keywords": "incentive mechanism prediction;information incentivize agents;crowdsourcing mechanism;cost prediction elicitation;joint incentive mechanism;incentive mechanism;optimal joint incentive;corresponds crowdsourcing mechanism;incentivize agents;incentivize agents different;heterogeneous agents private;agents private information;joint incentive;crowdsourcing mechanism employs;prediction elicitation;prediction minimal cost;cost prediction;rational agents private;incentive;crowdsourcing;agents private;design incentive mechanism;private information incentivize;corresponds crowdsourcing;elicit heterogeneous agents;joint design incentive;design incentive;prediction elicitation analysis;information incentivize;multiple agents", "pdf_keywords": ""}, "076b2ba158c35bd2941769864ce7455cf76ecd8e": {"ta_keywords": "herding peer review;peer review discussions;peer review;peer review process;herding peer;biases present peer;minimized peer review;reviewers senior decision;trial herding peer;understand reviewers senior;present peer review;reviewers senior;understand reviewers;peer review backbone;discussion case reviewers;cognitive biases;biases minimized peer;review discussions;reviewers;cognitive biases important;various cognitive biases;controlled trial herding;herding;reviewers form independent;responsible reviewing papers;reviewing;important understand biases;responsible reviewing;reviewers form;review process", "pdf_keywords": "machine learning conference;herding peer review;conference machine learning;peer review;abstract peer review;peer review discussions;review process icml;reviewers study;\ufb02agship machine learning;reviewing;discussion case reviewers;review process;peer review backbone;opinion outcome paper;reviewing papers;reviewers study following;tier machine learning;distributions reviewers;reviewers participate;responsible reviewing papers;trial herding peer;distributions reviewers participate;process international conference;learning conference;international conference machine;reviewers;review process international;responsible reviewing;review discussions;process responsible reviewing"}, "80b747af8d86541cf53198519c8fa51109eed4f9": {"ta_keywords": "augmentation effective unsupervised;unsupervised data augmentation;augmentation uda semisupervised;data augmentation effective;data augmentation furthermore;data augmentation;sequence labeling tasks;data augmentation uda;complex data augmentation;use clever augmentation;augmentation effective;clever augmentation;augmentation furthermore;produced data augmentation;augmentation;sequence labeling;augmentation uda;augmentation furthermore applying;benefits nlp;clever augmentation techniques;randomly substituted words;effective unsupervised data;method sequence labeling;effective unsupervised;unlabeled examples;gains unlabeled data;unlabeled examples corresponding;enforcing consistency predictions;observed unlabeled examples;words yields comparable", "pdf_keywords": "augmentation unlabeled data;sequence tagging datasets;datasets sequence tagging;naive augmentation unlabeled;uda semi supervised;unsupervised data augmentation;semi supervised learning;semi supervised;augmentation unlabeled;trained labeled unlabeled;data augmentation naive;proposed semi supervised;sequence tagging tasks;models trained labeled;extended sequence tagging;sequence tagging;semi supervised technique;data augmentation;domain data learning;suitable sequence tagging;based data augmentation;data augmentation techniques;data augmentation uda;sequence tagging introduce;augmentation techniques text;word label distributions;tagging datasets;learning objective unlabeled;trained labeled;augmentation naive"}, "b033400e9a80915a928f4603582e5e8bf7656a85": {"ta_keywords": "unimodal baselines multimodal;baselines multimodal;baselines multimodal domains;assessing performance multimodal;performance multimodal;performance multimodal techniques;unimodal baselines;multimodal;multimodal techniques demonstrate;multimodal techniques;baselines argue unimodal;modality performance visual;unimodal approaches better;single modality performance;strength unimodal baselines;baseline single modality;modality performance;multimodal domains;approaches better capture;performance visual navigation;unimodal approaches;visual navigation qa;multimodal domains make;visual;baselines;performance visual;visual navigation;baseline;unimodal;reflect dataset biases", "pdf_keywords": "benchmarks unimodal models;multimodal tasks outperform;models outperform multimodal;multimodal tasks;seemingly multimodal tasks;outperform multimodal counterparts;multimodal counterparts;benchmarks unimodal;multimodal counterparts ablate;outperform multimodal;unimodal models outperforming;baselines benchmarks unimodal;egocentric question answering;multimodal;unimodal models outperform;language seemingly multimodal;models unimodal models;models unimodal;deep architectures quick;seemingly multimodal;unimodal models;navigation question answering;unimodal models unimodal;navigation egocentric;biases deep architectures;2018 navigation egocentric;navigation qa benchmarks;crowdsourced language;question answering tasks;crowdsourced language descriptions"}, "c0099a15bd3251083c62ebd47c9705a16309b974": {"ta_keywords": "zero crossing image;crossing image representation;multiscale zero crossing;level early vision;crossing image;early vision;multiscale zero;stabilized multiscale zero;zero crossing;image processing tasks;stabilized multiscale;image representation;image processing;multiscale;representation image processing;image representation image;representation image;vision;crossing;level early;image;tasks level early;processing tasks level;processing tasks;processing;tasks level;representation;early;stabilized;zero", "pdf_keywords": ""}, "a16ae67070de155789a871cb27ecbf9eaa98b379": {"ta_keywords": "human evaluations text;evaluations text generated;nlg evaluation;nlg evaluation provide;untrained human evaluations;nlg researchers improving;evaluations play nlg;improving human evaluations;evaluations text;natural language generation;nlg researchers;models human evaluations;recommendations nlg researchers;authored text gpt2;provide recommendations nlg;play nlg evaluation;human evaluations typically;human evaluations;recommendations nlg;gpt3 authored text;language generation models;machine authored text;machine generated text;training evaluators better;text generated;generation models fluency;quickly training evaluators;nlg;evaluations typically;fluency improves evaluators", "pdf_keywords": "human evaluations text;untrained human evaluations;improving human evaluations;evaluations text generated;evaluators assess humanlikeness;evaluations play nlg;nlg evaluation provide;conclusion untrained evaluators;nlg evaluation;human evaluations;evaluations text;training evaluators better;human evaluation;human evaluation practices;current human evaluation;human evaluator \ufb01nds;quickly training evaluators;training evaluators;evaluator training methods;assess humanlikeness operationalized;evaluator training;human evaluator;nlg researchers improving;play nlg evaluation;non expert evaluators;untrained evaluators;improve evaluators;expert evaluators;annotated examples human;improve evaluators accuracy"}, "ece56ab633f11d1592a3d4f9386412d3f48fcf95": {"ta_keywords": "warrants comprehension;implicit warrants comprehension;argument reasoning comprehension;warrants comprehension does;argument reasoning;reconstructing warrants systematically;comprehension task argument;reconstructing warrants;reasoning comprehension task;task argument reasoning;comprehension task identification;methodology reconstructing warrants;warrant reconstruction;attention language models;reconstruction implicit warrants;reasoning comprehension;implicit warrants;arguments news comments;dataset warrants;authentic arguments news;implicit warrant;correct implicit warrant;automatic warrant reconstruction;warrant reconstruction paper;comprehension task;neural attention language;operationalize scalable crowdsourcing;challenging task argument;comprehension does require;attention language", "pdf_keywords": "argument reasoning comprehension;argument reasoning;crowdsourcing new task;arguments news comments;authentic arguments news;crowdsourcing new;task argument reasoning;reasoning comprehension task;reasoning comprehension;scalable crowdsourcing new;operationalize scalable crowdsourcing;computational linguistics human;comprehension task proposed;scalable crowdsourcing;means scalable crowdsourcing;arguments news;reconstruct implicit warrants;crowdsourcing;arguments;comprehension task identi\ufb01cation;challenging task argument;scalable crowdsourcing process;computational linguistics;linguistics human language;crowdsourcing process resulting;crowdsourcing process;comprehension task;steps argument reasoning;implicit warrants;implicit warrants realized"}, "2232808cf3161ca4c434126e35f47ee33c0c8219": {"ta_keywords": "explanations explanations teacher;explanations accuracy gains;evaluating explanations explanations;students crucially explanations;evaluating explanations;explanations accuracy;explanations teacher;explanations teacher aid;unstated evaluating explanations;explanations explanations;value explanations accuracy;explanations available student;quantify value explanations;explanations;explain predictions;features aims explanations;student model trained;crucially explanations available;crucially explanations;explain predictions highlighting;teacher model;explanations serve ought;explanations serve;simulate teacher model;trained simulate teacher;aims explanations serve;model trained;aims explanations;student model;predictions highlighting salient", "pdf_keywords": "explanations attention regularization;incorporate explanations attention;explanations attention;explanations accuracy gains;explanations explanations teacher;explanations accuracy;evaluating explanations explanations;evaluating explanations;explanations teacher;incorporate explanations;research explaining predictions;classi\ufb01cation question answering;explaining predictions;quality evaluating explanations;question answering;question answering tasks;explanations gold;explanations teacher aid;explanations explanations;explaining predictions highlighting;human provided explanations;explanations correlates;value explanations accuracy;attention regularization multitask;attention regularization;approaches incorporate explanations;question answering observe;multitask learning;student model learning;explanations"}, "228f2efe7b06b6db3b2c6c0a61d7b33daee1d641": {"ta_keywords": "sense disambiguation based;word sense disambiguation;unsupervised word sense;disambiguation word sense;sense disambiguation word;semantic resources russian;disambiguation based automatically;sense disambiguation;disambiguation based;semantic similarity;word sense corresponding;lexical semantic;disambiguation word;lexical semantic resources;mnogoznal unsupervised word;different lexical semantic;unsupervised word;resources russian sparse;semantic similarity given;russian sparse;disambiguation;sense target word;lexical;word sense;sentence synset;similar word sense;different lexical;target word architecture;evaluation different lexical;semantic", "pdf_keywords": ""}, "301352755a94d7524312b7c7f2fab7d3fd3d334d": {"ta_keywords": "uncertainty preference orderings;dominance queries pcp;optimality dominance queries;nets efficient dominance;preferences probabilistic;preferences probabilistic uncertainty;conditional preferences probabilistic;dominance queries;preference orderings;allowing uncertainty preference;nets multi agent;weighted cp nets;uncertainty pcp nets;cp nets aggregated;study optimality dominance;qualitative conditional preferences;optimality dominance;nets allowing uncertainty;efficient dominance procedure;preference orderings experimental;uncertainty preference;conditional preferences;nets aggregated;generalise cp nets;pcp nets formalism;multi agent;efficient dominance;nets aggregated single;aggregated single pcp;probabilistic uncertainty pcp", "pdf_keywords": ""}, "035595ebf6821031a543ee1c30386a6230fc7a41": {"ta_keywords": "online speaker diarization;speaker diarization algorithm;speaker diarization;speaker permutation information;speaker tracing buffer;frames representing speaker;speaker tracing;proposed speaker tracing;representing speaker permutation;online speaker;speaker permutation;assign speaker regions;speaker permutation problem;presents speaker permutation;speaker regions;representing speaker;online diarization;possibility assign speaker;assign speaker;novel online speaker;online diarization inherently;network online diarization;speaker regions incorrectly;diarization outputs buffer;buffered frames;speaker;buffered frames stacked;diarization algorithm based;proposed speaker;supervised self attention", "pdf_keywords": "online speaker diarization;end speaker diarization;speaker diarization;neural diarization speaker;diarization speaker tracing;speaker diarization proposed;speaker diarization sa;speaker diarization algorithm;diarization speaker;eend online speaker;end neural diarization;eend performing diarization;online speaker;speech processing;diarization proposed online;language speech processing;diarization sa eend;corpus spontaneous japanese;speaker recordings;speaker tracing;end speaker;novel online speaker;2center language speech;using speaker recordings;speech processing johns;neural diarization;diarization independently chunked;attention based end;end end speaker;speaker tracing buffer"}, "8328508dc12c295165f997e02d74d00a42971c01": {"ta_keywords": "semantic parsing context;parsing context;parsing context far;effective context modeling;parsing context received;world semantic parsing;13 context modeling;context modeling;context modeling methods;study semantic parsing;semantic parsing;context far effective;far effective context;frequent contextual;study context modeling;contextual;context modeling evaluate;effective context;challenging complex contextual;13 context;complex contextual;world semantic;context;contextual phenomena fine;real world semantic;frequent contextual phenomena;contextual phenomena;context far;summarize frequent contextual;exploratory study semantic", "pdf_keywords": ""}, "d36e39aedd802aea4be1ea303c70dc56e97dbc3c": {"ta_keywords": "summarization datasets qags;summaries evaluate qags;factual consistency summaries;summary factually consistent;generating usable factually;generated summaries;qags based intuition;consistency summaries evaluate;consistency summaries;answers summary factually;generated summary asking;qags based;evaluate qags;summarization models;source propose qags;factually consistent text;propose qags;abstractive summarization models;qags promising tool;generated computing qags;model generated summaries;evaluate qags collect;qags;generated summary;usable factually consistent;generated summaries cnn;input believe qags;believe qags;qags indicate;kags automatic evaluation", "pdf_keywords": "generated summaries summarization;detecting factual inconsistencies;model generated summaries;conditionally generated texts;detect factual inconsistencies;automatically detecting factual;generated summaries;conditional text generation;summarization datasets conclusion;inconsistencies abstractive summarization;summaries summarization datasets;detecting factual;factual inconsistencies conditionally;factual inconsistencies generated;inconsistencies conditionally generated;generated summary;measuring factual consistency;abstractive summarization;summarization datasets;generated summary finally;detect factual;identify factual inconsistencies;inconsistencies generated text;text generation;generated texts;model generated text;inconsistencies generated summary;summaries summarization;abstractive summarization work;factual consistency model"}, "6c6975750207f787c318627ff7cb63a649165a8d": {"ta_keywords": "learned display representation;integrating learned display;learned display;intelligent tutoring agent;tutoring agent;intelligent tutoring;algorithm intelligent tutoring;tutoring;using learned representation;learning using learned;skill learning using;using learned;grammar learner support;expert skill learning;grammar learner;learned representation;skill learning;learning using;free grammar learner;domains learned display;tutoring agent simstudent;display representation agent;learner support learning;human expert skill;learned representation effective;agent learning perceive;learner;display representation good;learning complex;learner support", "pdf_keywords": ""}, "1a671afdac8e7b759cf3b5ec7d03d485c76a989c": {"ta_keywords": "speech recognition asr;speech recognition tasks;speech recognition;automatic speech recognition;automatic speech;recognition asr framework;temporal classification ctc;different speech recognition;recognition asr;end automatic speech;connectionist temporal classification;mask prediction ctc;ctc model trained;mask ctc outperforms;prediction ctc inference;classification ctc;masked based ctc;prediction ctc;autoregressive output token;mask ctc;mask ctc model;outputs connectionist temporal;scenarios neural sequence;training mask prediction;present mask ctc;mask ctc novel;encoder;recognition tasks mask;temporal classification;sequence sequence models", "pdf_keywords": "autoregressive speech recognition;speech recognition asr;mask ctc decoding;asr mask ctc;ctc decoding;autoregressive asr mask;decoder model ctc;recognition asr framework;recognition asr;asr ctc mask;speech recognition tasks;speech recognition;automatic speech recognition;mask prediction ctc;end automatic speech;automatic speech;speech recognition 24;ctc decoding method;end asr ctc;different speech recognition;mask ctc outperforms;mask ctc outperformed;ctc mask predict;autoregressive speech;non autoregressive speech;speech processing;masked language model;mask ctc model;asr ctc;mask ctc"}, "38ff6cf441050a1db10df85ac0771ccc88dea748": {"ta_keywords": "efficient peer review;aggregation reviews guarantees;conference peer review;peer review systems;reviewer assignment aggregation;peer review empirically;peer review;setting conflicts reviewers;aggregation reviews;conflicts reviewers manipulate;conflicts reviewers;conflicts reviewers submissions;assignment aggregation reviews;peer review conference;peer review settings;authorship conflict graph;submissions conflicts reviewers;consider peer review;reviewers submissions conflicts;day peer review;conflict graph satisfies;review systems;efficient peer;conflicted reviewer work;reviewers manipulate;reviewer assignment;strategyproof efficient peer;manipulate reviews strategic;review conference;reviews guarantees strategyproofness", "pdf_keywords": "peer review algorithm;conference peer review;reviewer assignment aggregation;review algorithm;method conference peer;peer review;algorithm reviewer assignment;conference peer;review algorithm based;peer review \ufb01rst;peer review settings;reviewer assignment;appealing conference peer;authorship graph;unanimity authorship graph;algorithm reviewer;assignment aggregation guarantees;property authorship graph;behavior conference peer;assignment aggregation;analyze algorithm reviewer;authorship graph satis\ufb01es;authorship graph satis\ufb01ed;gnu linear programming;aggregation;review settings implementation;aggregation guarantees;peer;reviewer;method conference"}, "97ca917f66d60f5277651a74f233804b03cb5e3d": {"ta_keywords": "morphological segmentation russian;segmentation russian language;segmentation russian;morphological segmentation;morpheme boundaries;task morphological segmentation;morphological;morpheme boundaries beat;98 morpheme boundaries;russian language;morpheme;task morphological;segmentation;russian;98 morpheme;score 98 morpheme;convolutional neural networks;addresses task morphological;deep convolutional neural;non neural;convolutional neural;deep convolutional;existing non neural;non neural approaches;neural approaches present;neural networks;neural approaches;neural;convolutional;language", "pdf_keywords": ""}, "49989dc4d77b9df775b284ab7682ba76c080be12": {"ta_keywords": "classifiers hmms;classifiers hmms simply;markov models classifiers;hidden markov models;hidden markov;implement texture classification;unlike classifiers hmms;models hmms;classifying deformable objects;course hidden markov;texture classification;markov models hmms;texture classification algorithm;parts models classifying;noncausal hidden markov;models classifying deformable;classifying deformable;classified implement texture;articulated parts models;classifying;markov models;models hmms assume;object classified;classification;classifiers;models classifying;markov;classification algorithms studied;articulated parts;various classification algorithms", "pdf_keywords": ""}, "51d735419392dbe961c60bff7eee95388b8d6d3d": {"ta_keywords": "labeled grammar induction;unsupervised grammar induction;labeled grammar;bias labeled grammar;unlabeled dependency trees;induce labeled dependencies;unsupervised grammar;induction minimal supervision;grammar induction;induced word clusters;induce unlabeled dependency;grammar induction minimal;supervision frequent words;trees gold speechtagged;labeled dependencies automatically;grammar induction aims;automatically induced word;labeled dependencies;work unsupervised grammar;gold speechtagged text;clean linguistic classes;induce labeled;dependency trees gold;speechtagged text;clean linguistic;unlabeled dependency;dependency trees;gold speechtagged;induce unlabeled;required induce labeled", "pdf_keywords": ""}, "b26ca2bb882c2d3526fb4ac7f544fb87c39ded62": {"ta_keywords": "gradient matching pursuit;kernel gradient matching;kernel based classification;matching pursuit method;optimization kernel based;efficient optimization kernel;recognition developed kernel;kernel based probabilistic;kernel optimization;pursuit method approximates;matching pursuit;pursuit method approximation;kernel optimization incorporated;approximation technique kernel;scalable kernel optimization;optimization kernel;pursuit method introduces;technique kernel based;sequential pattern recognition;pursuit method;kernel based;pattern recognition;improved kernel gradient;propose improved kernel;high dimensional features;gradient matching;conventional kernel gradient;kernel gradient;pattern recognition developed;scalable kernel", "pdf_keywords": ""}, "2cd7c3ed5a06c461b259694376820dcfcfbe94a9": {"ta_keywords": "generative neural parsing;decode directly generative;neural parsing improving;neural parsing;inference generative neural;directly generative models;discriminative neural models;external parsers decoding;generative neural;parsers decoding tractable;directly generative;generative models feasible;effective inference generative;discriminative neural;parsers decoding;generative models;inference generative;generative;used discriminative neural;external parsers;models enables decode;parsing improving;decoding;output external parsers;parsers;search used discriminative;decoding tractable effective;neural models;beam search;decoding tractable", "pdf_keywords": "generative constituency parsers;generative neural parsing;generative parser achieves;generative parser;contained generative parser;neural parsing;search neural generative;decode directly generative;neural generative constituency;parsers lexical actions;constituency parsers lexical;constituency parsers;inference generative neural;parsers decoding tractable;neural generative;directly generative;generative neural;constituency parsing;generative neural models;external parsers decoding;constituency parsing related;directly generative models;generative constituency;abstract generative neural;results constituency parsing;inference generative;neural parsing mitchell;parsers decoding;effective inference generative;external parsers"}, "19a3af37df22c7c646cc99efad3af96cda6e80f0": {"ta_keywords": "multimodal translation task;multimodal machine translation;wmt17 multimodal translation;machine translation nmt;translation trained attentional;multimodal translation;neural machine translation;translation nmt model;naist wmt17 multimodal;translation nmt;machine translation;machine translation task;translation trained;multimodal nmt;multimodal nmt model;hiero translation trained;present multimodal nmt;wmt17 multimodal;networks help translation;attentional encoder;translation task;translation task language;language descriptions images;nict naist wmt17;hiero nmt decoding;attentional encoder decoder;nmt decoding;trained attentional encoder;nict naist wmt;multimodal machine", "pdf_keywords": ""}, "51546584aa394d159edcc08f2412ae30dd316f6c": {"ta_keywords": "understanding deep learning;understanding deep;effective prediction depth;prediction depth;inside deep models;deep learning;deep models;prediction depth extensive;deep learning employs;allows improve prediction;input effective prediction;improve prediction accuracy;faster networks learn;prediction depth given;improve prediction;networks learn easy;prediction accuracy existing;relationships prediction depth;deep;effective prediction;networks learn;prediction accuracy;work understanding deep;memorize early layers;early layers generalize;deep models showcase;difficulty making prediction;layers memorize early;prediction;making prediction", "pdf_keywords": "prediction depth;prediction depth training;prediction depth typically;prediction depth extensive;prediction depth related;e\ufb00ective prediction depth;prediction depth given;considering prediction depth;according prediction depth;shown prediction depth;uncertainty prediction adversarial;prediction adversarial input;depth training validation;understanding deep;adversarial input margin;depth training;depth typically learned;understanding deep learning;prediction adversarial;relationships prediction depth;adversarial input;deep;deep learning;depth;depth related accuracy;output margin learned;accuracy inputs strongly;deep learning employs;accuracy uncertainty prediction;input e\ufb00ective prediction"}, "ca73cc17ca69fa0807e566c22c7c1711da916281": {"ta_keywords": "nearest neighbor search;approximate nearest neighbor;similarity searching vast;nearest neighbor;spaces similarity searching;similarity searching;search high dimensional;structures approximate nearest;neighbor search;approximate nearest;searching vast range;data structures approximate;neighbor search results;metric spaces similarity;exact search suffer;proposed exact search;neighbor search high;dimensional spaces comparative;spaces similarity;exact search;nearest;similarity;especially non metric;searching vast;high dimensional spaces;dimensional spaces unfortunately;dimensionality applicable high;structures approximate;search high;searching", "pdf_keywords": ""}, "56501a3441c2074bbbbe31015d6d41c57d9d285b": {"ta_keywords": "paraphrastic sentence representations;paraphrastic sentence models;monolingual semantic similarity;lingual semantic similarity;semantic textual similarity;semantic similarity cross;semantic similarity;unsupervised semantic textual;semantic similarity bitext;textual similarity significantly;sentence representations variety;training paraphrastic;similarity cross lingual;training paraphrastic sentence;sentence representations;textual similarity;sentence models;unsupervised semantic;bert based models;state art paraphrastic;semantic textual;lingual semantic;monolingual semantic;cross lingual semantic;inference training;bases training paraphrastic;paraphrastic;suite monolingual semantic;models like sentencebert;work unsupervised semantic", "pdf_keywords": "embeddings language paraphrase;lingual semantic similarity;paraphrastic sentence embeddings;english semantic similarity;semantic similarity cross;semantic similarity tasks;similarity cross lingual;paraphrase bilingual parallel;learning inference paraphrastic;paraphrastic representations scale;semantic similarity;paraphrastic representations;paraphrase data;semantic similarity state;millions sentence pairs;cross lingual semantic;million sentence pairs;jwieting paraphrastic representations;unsupervised english semantic;semantic similarity bitext;language paraphrase bilingual;paraphrase bilingual;sentence embeddings language;bilingual parallel data;bitext paraphrase data;lingual semantic;inference paraphrastic;sentence embeddings;paraphrastic;inference paraphrastic sentence"}, "ce458be308f2c75edc53366272fa6e744fda7902": {"ta_keywords": "word sense disambiguation;semantic resources russian;unsupervised word sense;sense disambiguation semantic;disambiguation semantic similarity;disambiguation semantic;sense disambiguation;lexical semantic;lexical semantic resources;resources russian sparse;russian sparse;mnogoznal unsupervised word;word sense corresponding;semantic similarity;different lexical semantic;unsupervised word;russian sparse mode;word mnogoznal unsupervised;lexical;semantic similarity given;disambiguation;word sense;different lexical;semantic;similar word sense;resources russian;sense target word;evaluation different lexical;sense corresponding context;target word mnogoznal", "pdf_keywords": ""}, "9195186cf44876d0d1d03b87756c464b760a7f4e": {"ta_keywords": "speech translation track;offline speech translation;speech recognition translation;e2e speech translation;architecture audio segmentation;long context modeling;improved audio segmentation;audio segmentation;audio segmentation significantly;translation track;translation track architecture;end e2e speech;audio segmentation using;recognition translation tasks;speech translation;e2e speech;decoders speech recognition;offline speech;pyannote audio toolkit;data architecture audio;pyannote audio;recognition translation;segments long context;speech recognition;decoders speech;translation tasks unified;architecture audio;dedicated decoders speech;audio toolkit;translation tasks", "pdf_keywords": "speech translation track;speech translation;e2e speech translation;speech translation st;of\ufb02ine speech translation;long context modeling;translation track;ensembling better segmentation;better segmentation neural;decoder architecture segment;segmentation neural;german speech translation;end e2e speech;sequence level knowledge;speech translation year;labeling seqkd segmentation;segmentation neural network;better segmentation;segments long context;2020 multi decoder;knowledge distillation seqkd;e2e speech;context modeling;context modeling table;voice activity;seqkd segmentation investigated;seqkd multi decoder;algorithm long context;multi decoder architecture;voice activity vad"}, "4fd6488e38043d680c592170bf7f651c079d0e98": {"ta_keywords": "poisson process mobile;mean throughput mobile;tier poisson network;throughput mobile static;poisson network model;stochastic poisson;doubly stochastic poisson;throughput mobile;speed mobile users;network speed mobile;data outage mobile;poisson network;outage mobile users;mobile users handoff;downlink data rate;stochastic poisson process;outage mobile;handoff outage;heterogeneous tier network;average downlink data;handoff outage periods;poisson process static;optimize mean throughput;mean throughput;network model;users doubly stochastic;average throughput types;tier poisson;mobile moving users;calculate average downlink", "pdf_keywords": "heterogeneous cellular networks;mobility management heterogeneous;networks high mobility;management heterogeneous cellular;cell planning mobility;cellular networks;cellular networks arpan;mean throughput mobile;small cell networks;cell networks high;mobility management;users handoff heterogeneous;cell networks;handoff heterogeneous;planning mobility management;throughput mobile static;poisson process mobile;mobile users handoff;tier poisson network;high mobility users;data rate mobile;heterogeneous tier network;throughput mobile;static mobile users;cellular;base stations mobile;performance static mobile;data outage mobile;heterogeneous cellular;network model poisson"}, "4ab7b65e1a3b76eb3db064523c862f1325e04971": {"ta_keywords": "different speech recognizers;speech recognizers;speech recognition asr;speech recognition;speech recognition systems;automatic speech recognition;speech deficits studied;speech deficits;individual speech deficits;asr systems speech;speech people pd;speech recognizers attention;performance automatic speech;disease different speech;speakers parkinson disease;automatic speech;systems speakers parkinson;systems speech;systems speech people;recognition asr systems;speakers parkinson;recognition asr;tested speech;recognition systems speakers;tested speech 43;word error rate;parkinson disease pd;systems trained spanish;systems population parkinson;characters phonetic units", "pdf_keywords": ""}, "3f79b71b887d2ccb733926867a62f69902fcbdab": {"ta_keywords": "adaptive ontology mapping;adaptive ontology;similarity ontologies vector;generic adaptive ontology;structural similarity ontologies;similarity ontologies;ontology mapping;ontologies vector;ontologies vector space;ontology mapping approach;satisfy ontology;ontologies;ontology;ontology constraints;aggregation methods neural;satisfy ontology constraints;adaptiveaggregation outperforms aggregation;ontology constraints theexperimental;solution satisfy ontology;competition neuralnetwork;outperforms aggregation methods;activation competition neuralnetwork;aggregates themusing adaptive;oaei benchmark;linguistic structural similarity;performanceof similarity;measure harmony based;ranked systems benchmark;retrieval techniques arti\ufb01cial;outperforms aggregation", "pdf_keywords": ""}, "7954b31ce1f6ad935808b7cf62c34bc118d20a9a": {"ta_keywords": "patients formalize causal;formalize causal inference;heterogeneity decision;instance judges vary;regions heterogeneity decision;heterogeneity decision making;judges vary leniency;decisions faced context;inter decision maker;causal inference;judges vary;decision maker disagreement;instance judges;causal effect decision;decision individuals make;healthcare datasets;decision individuals;effect decision individuals;offenses doctors vary;inference problem seeking;formalize causal;vary leniency certain;contexts types cases;make different decisions;datasets recovering variation;decision maker;world healthcare datasets;healthcare datasets recovering;doctors vary;decision maker large", "pdf_keywords": "regions disagreement maximizing;datasets recovering variation;healthcare datasets;identify regions disagreement;healthcare datasets recovering;generalization;identifying regions heterogeneity;healthcare dataset;disagreement maximizing objective;heuristics choosing region;learning algorithm identifying;objective alternatingly optimizes;disagreement maximizing;iterative optimization;world healthcare datasets;algorithm identify regions;learning algorithm;regions variation;region generalizes;intuitive regions variation;recovering variation;world healthcare dataset;decision section iterative;identifying region heterogeneity;identifying regions;alternatingly optimizes;healthcare dataset con\ufb01rm;algorithm identifying region;introduce iterative optimization;training data"}, "4bf5084d21f681c09409bd890daa4bf1c4f9b691": {"ta_keywords": "platelet reactivity hpr;treatment platelet reactivity;platelet reactivity tests;tests antiplatelet drug;platelet reactivity;using platelet reactivity;hpr using platelet;antiplatelet drug administration;antiplatelet drug;high treatment platelet;reactivity tests antiplatelet;treatment platelet;periprocedural myocardial infarction;significance observed smoking;observed smoking;smoking cohort regarding;observed smoking cohort;tests antiplatelet;using platelet;smoking cohort;myocardial infarction;myocardial infarction study;platelet;antiplatelet;reactivity hpr;infarction study;infarction study begun;role periprocedural myocardial;smoking play significant;periprocedural myocardial", "pdf_keywords": ""}, "c3490ec9b8f695bed2187fb4a4164b1509389ca8": {"ta_keywords": "neural machine translation;machine translation;proceedings workshop neural;workshop neural machine;workshop neural;neural machine;neural;translation;proceedings workshop;workshop;machine;proceedings", "pdf_keywords": ""}, "7d94d4c6b2db490e08beabd2661df009f1a06d6c": {"ta_keywords": "wordnet;wordnet like thesaurus;wordnet like;create noun synsets;russian means crowdsourcing;open wordnet;noun synsets;open wordnet like;large open wordnet;thesaurus russian;like thesaurus russian;linguistic technical organizational;thesaurus;russnet project started;synsets project yarn;crowdsourcing stage project;thesaurus russian means;crowdsourcing;russnet project;project yarn russnet;crowdsourcing stage;means crowdsourcing;like thesaurus;synsets project;yarn russnet project;means crowdsourcing stage;assembling synsets project;linguistic;project create noun;describes linguistic technical", "pdf_keywords": ""}, "02a757548da783d43ffcfd4b60f2cbb0ac71a4bc": {"ta_keywords": "fairness elicitation fairness;subjective individual fairness;fairness eliciting enforcing;individual fairness constrained;fairness elicitation;fairness eliciting;notion individual fairness;subjective fairness using;fairness constraints elicited;study subjective fairness;subjective fairness;accuracy fairness eliciting;human subject fairness;subject fairness constraints;elicitation fairness indirectly;elicitation fairness;notions fairness report;individual fairness;fairness indirectly specified;fairness constraints;fairness using human;individual fairness proposed;subjective notions fairness;framework fairness elicitation;fairness constrained;fairness indirectly;fairness using;fairness report preliminary;subject fairness;fairness proposed", "pdf_keywords": "fairness constrained learning;elicited fairness constraints;fairness constraints elicited;subjects fairness constrained;subject fairness constraints;dataset using fairness;fairness constraints attributed;fairness constraints;accuracy elicited fairness;fairness constrained;human subjects fairness;using fairness constraints;human subject fairness;fairness experimental results;subjects fairness;fairness experimental;accuracy fairness experimental;using fairness;subject elicited fairness;fairness constraints prove;accuracy fairness;fairness violations \ufb01nd;elicited fairness;magnitude fairness violations;fairness violations;subject fairness;error fairness violations;bounds accuracy fairness;fairness terms;magnitude fairness"}, "f7247fefc9efb57ace33425a2981d6aba08da3b7": {"ta_keywords": "statistical dialogue management;statistical dialogue framework;dialogue management using;dialogue management;based statistical dialogue;pomdp dialogue evaluation;statistical dialogue;dialogue framework;method statistical dialogue;intention dependency graph;dialogue evaluation transition;dialogue evaluation;hierarchical graph intentions;function pomdp dialogue;pomdp dialogue;dialogue;directed intention dependency;markov decision process;using directed intention;graph intentions way;graph intentions;observable markov decision;markov decision;decision process pomdp;intention dependency;directed intention;policy function pomdp;learning policy function;process pomdp framework;dependency graph", "pdf_keywords": ""}, "23e42bc79f10234bdceef31441be39a2d9d2a9a0": {"ta_keywords": "neural logic programming;rules knowledge base;knowledge base reasoning;logical rules knowledge;learning order logical;neural logic;knowledge base;framework neural logic;knowledge base benchmark;rules knowledge;multiple knowledge base;logic programming combines;base reasoning empirically;logic programming;probabilistic order logical;order logical rules;logical rules;learning probabilistic order;logical rules end;learning probabilistic;parameter structure learning;learning parameters;base reasoning;problem learning probabilistic;requires learning parameters;differentiable model learning;neural controller learns;structure learning order;learning parameters continuous;structure learning", "pdf_keywords": ""}, "06064617f152f5032137204aec739c0c82dbb836": {"ta_keywords": "speech recognition asr;chime speech separation;speech separation recognition;distant microphone automatic;environment distant microphone;microphone automatic speech;distant microphone;speech recognition;chime challenge initiative;automatic speech recognition;recognition asr;reverberation second chime;chime challenge;automatic speech;2nd chime challenge;microphone;speech separation;microphone automatic;recognition asr remains;chime speech;separation recognition challenge;asr systems;performance asr;second chime speech;recognition challenge datasets;performance asr systems;separation recognition;asr remains challenging;reverberation;2nd chime", "pdf_keywords": ""}, "14047a24b23d9e392776229f9d40bee9f8243e4c": {"ta_keywords": "complexity dynamic sensor;sensor subset selection;selection stochastic approximation;sensor selection process;active sensor selection;sensor selection;sensor activation tracking;dynamic sensor activation;stochastic approximation learning;sampling sensor subset;sensor networks cyberphysical;gibbs sampling sensor;subset selection stochastic;sensor networks;sensor subset;dynamic sensor;energy efficient tracking;stochastic approximation;selection stochastic;parameter vector learned;tracking time varying;sensor activation;efficient tracking;active sensor;active sensors;active sensors key;sampling sensor;approximation learning;sensors key theoretical;locally optimal", "pdf_keywords": ""}, "0dd1b9ad5aeda250dc61f38cf7018e7a014e91c0": {"ta_keywords": "land use traffic;use traffic congestion;traffic congestion based;traffic congestion;congestion based gwr;based gwr model;use traffic;land use;congestion based;gwr model;impact land use;traffic;congestion;based gwr;study impact land;data driven;gwr;data driven study;impact land;land;data;driven study impact;driven;driven study;study impact;impact;model;based;use;study", "pdf_keywords": ""}, "a67face220a88b6b36f3343a6a017a3536562d5b": {"ta_keywords": "visual guessing games;guessing games supervised;spiel guessing games;successful guessing games;playing guessing games;learned visual guessing;scenario agent learns;guessing games prototypical;agent learns;guessing games;learning spiel guessing;guessing games work;guessing games later;benefit playing guessing;playing guessing;visual question answering;games supervised learning;guessing games novel;learning scenario agent;agent learns mimic;visual guessing;experience learning spiel;iterated experience learning;mimic successful guessing;games supervised;exploit playing guessing;neural representations learned;games prototypical instance;play iterated experience;representations learned visual", "pdf_keywords": "visual guessing games;abstract guessing games;learning guessing games;learned visual guessing;guessing games prototypical;guessing games;play guessing games;neural representations learned;games prototypical instance;learning guessing;representations learned visual;visual guessing;representations learned;abstract guessing;agent trained guesswhat;ways learning guessing;play guessing;guessing games data;play mechanism learning;representations agent trained;guessing games novel;learned representations;agents play guessing;games prototypical;learning interacting paradigm;learning interacting;neural representations;learned representations compguesswhat;prototypical instance learning;task learning spiel"}, "970383c0a41d7ae1ec4b8abaa3033778203377b9": {"ta_keywords": "question answering qa;question answering instance;factoid question answering;answering qa tasks;question answering;models synthetic corpus;virtual assistants answer;inputs question answering;automatic speech;assistants answer questions;recognition systems factoid;synthetic corpus;answering qa;000 noisy sentences;noise automatic speech;noisy sentences;recognition speech;speech recognition create;qa tasks mitigating;qa tasks;virtual assistants;human corpora quizbowl;automatic speech recognition;inputs machine translation;synthetic corpus 500;noisy sentences evaluate;character recognition speech;speech recognition;assistants answer;speech recognition systems", "pdf_keywords": "models synthetic corpus;neural qa systems;question answering;neural qa;noisy sentences;000 noisy sentences;downstream neural qa;answering qa tasks;noisy sentences evaluate;synthetic corpus;question answering qa;forced decoding neural;automatic speech;noise automatic speech;recurrent neural networks;decoding neural;decoding neural model;synthetic corpus 500;speech recognition;factoid question answering;structured recurrent;automatic speech recognition;human corpora quizbowl;structured recurrent neural;accuracy downstream neural;decoding unknown words;answering qa;recurrent neural;corpora quizbowl;speech recognition systems"}, "3193766c0439ff29a0a3d176628f8144d6e77231": {"ta_keywords": "sentiment analysis czech;supervised sentiment analysis;supervised sentiment;czech social media;sentiment analysis;reprint supervised sentiment;analysis czech social;analysis czech;czech social;sentiment;czech;social media;supervised;reprint supervised;social;analysis;media;reprint", "pdf_keywords": ""}, "b38ec68c8bab031138606a9b00e9d817be3e1d22": {"ta_keywords": "jointly modeling links;link modeling jointly;entity link modeling;topic models;link modeling;topic models improve;models topic models;modeling links text;links text entities;text entities linked;modeling links;entities linked;publications annotated proteins;annotated proteins;entities linked model;entity link;model datasets protein;category prediction proteins;entity entity link;pairs entities frequently;induced topics;linked model;prediction proteins perplexity;link text information;induced topics understand;text entities;inspecting induced topics;like analysis protein;datasets protein protein;modeling jointly modeling", "pdf_keywords": ""}, "3f256b31d446015d8cd0f9f3996009cdf2034c5e": {"ta_keywords": "monolithic multilingual asr;speech recognition asr;multilingual asr;multilingual asr language;language independent neural;speech recognition;training single multilingual;hybrid attention connectionist;asr language independent;recognition asr;joint language identification;monolithic multilingual;speech recognition augment;automatic speech recognition;identification speech recognition;attention connectionist temporal;build monolithic multilingual;single multilingual model;connectionist temporal classification;recognize speech automatically;language independent end;multilingual model;automatic speech;recognition asr significantly;language recognize speech;single multilingual;jointly identify language;language recognize;asr language;language identification speech", "pdf_keywords": ""}, "c56aced0f0c5cfebefadb530cb08d736c3ac5c05": {"ta_keywords": "code summaries retrieval;code generation summarization;code summary generation;developers code summary;code summaries;code code summaries;supplement code generation;code documentation software;code summaries written;source code code;code summary;code generation;datasets code generation;relevant code summaries;source code documentation;augmented code generation;generation summarization java;lot source code;generation summarization extends;summaries retrieval database;summaries retrieval;retrieval augmented code;source code;mimic developers code;developers code;code documentation;summarization java python;search relevant code;summary generation;generation summarization models", "pdf_keywords": "code summaries retrieval;code generation summarization;code summary generation;code summaries;supplement code generation;generation summarization java;generation summarization framework;code generation;datasets code generation;developers code summary;augmented code generation;generation summarization models;code summary;generation summarization;summary generation;relevant code summaries;code documentation software;summarization java python;summary generation behavior;retrieval augmented code;generation summarization md;summaries retrieval;summarization framework mimic;summaries retrieval database;summarization java;code documentation;summarization framework;source code documentation;code associated text;summarization models 2020"}, "a4ce6cd06bc73d81651f7888efa4337fd82a60f0": {"ta_keywords": "unknown word detection;words spoken dialog;unknown word perception;unknown words disturbs;spoken dialog;word detection;unknown words spoken;word perception significantly;words disturbs communication;spoken dialog systems;word perception;word detection based;related brain desynchronization;brain desynchronization responses;brain desynchronization;dialog systems;event related brain;dialog systems deals;words disturbs;brain waves time;unknown words;time unknown word;brain waves;detection based event;desynchronization responses;dialog;systems deals words;communication results detect;disturbs communication results;appearance unknown words", "pdf_keywords": ""}, "04b364d56995de2228cb1acfb320a935cbcf4440": {"ta_keywords": "weakly supervised segmentation;region weakly supervised;semantic segmentation expensive;low level segmentation;standard semantic segmentation;weakly supervised training;semantic segmentation;weakly supervised;supervised segmentation;quality weakly supervised;segmentation expensive;supervised segmentation allows;segmentation expensive requiring;level segmentation;segmentation;segmentation allows neural;regularized losses improving;segmentation allows;trust region weakly;level segmentation representing;regularized losses;segmentation representing;segmentation representing geometric;shown regularized losses;region approach1 regularized;regularized;supervised training;approach1 regularized losses;requiring pixel labeled;robust trust region", "pdf_keywords": "neural network optimization;hidden optimization;higher order optimization;regularized losses;regularized losses improving;hidden optimization subproblems;chain hidden optimization;approach1 regularized losses;optimization technique neural;subproblems allows neural;neural network training;gradients;optimization subproblems allows;higher order generalization;instead chain gradients;optimization subproblems;chain gradients;neural network;allows neural network;gradients derivatives composition;optimization use strong;regularized;gradients derivatives;network optimization;version kl divergence;segmentations motivate robust;order optimization;chain gradients derivatives;optimization;kl divergence"}, "fa774368fcf51cc0fa1bfda59b6a606e163c64b1": {"ta_keywords": "systems counting constraints;highly symmetrical counting;symmetrical counting problems;counting constraints;symmetrical counting;dimensional systems counting;counting constraints linear;systems counting;counting problems exploit;formulation counting constraints;problems exploit symmetry;inequalities control synthesis;exploit symmetry synthesize;correct controllers systems;exploit symmetry;control synthesis;symmetry synthesize provably;counting;symmetry synthesize;controllers systems;counting problems;formulation counting;provably correct controllers;constraints;subsequent formulation counting;control synthesis high;solution aggregate abstraction;high dimensional systems;controllers systems tens;linear inequalities control", "pdf_keywords": "symmetric counting constraints;systems counting constraints;control synthesis permutation;systems formulation counting;synthesis permutation symmetric;exploit symmetry synthesize;highly symmetrical counting;permutation symmetric systems;dimensional systems counting;counting constraints presented;counting constraints;states control synthesis;symmetric counting;formulation counting constraints;number states abstraction;systems counting;problems exploit symmetry;symmetrical counting problems;symmetry synthesize provably;synthesis permutation;counting constraints linear;symmetric systems;symmetrical counting;symmetry synthesize;symmetric systems subject;states abstraction;likewise symmetric counting;control synthesis;states abstraction \u03b7nnx;counting constraints petter"}, "9abf14d4f89bf6c297e1bbd637cd54e1a0335e71": {"ta_keywords": "compositor attribution clustering;attribution clustering pages;automatic compositor attribution;textual visual features;compositor attribution folio;attribution clustering;textual visual;describes textual visual;compositor attribution;textual;attribution folio;describes textual;clustering pages historical;visual details printed;distinguish compositors automatic;bibliographic;historical printed document;clustering pages;printed document;bibliographic task;printed document individual;details printed page;automatic compositor;pages historical printed;distinguish compositors;jointly describes textual;needed distinguish compositors;novel unsupervised;bibliographic task relies;type bibliographic", "pdf_keywords": "textual visual sources;attribution incorporates textual;incorporates textual visual;textual visual features;textual;bibliographers;compositor attribution incorporates;textual visual;manual judgements bibliographers;incorporates textual;describes textual visual;attribution incorporates;bibliographers accuracy;jointly describes textual;judgements bibliographers;describes textual;judgements bibliographers accuracy;attributions agree manual;visual sources;attributions;visual sources evidence;used bibliographers;compositor attribution;designed compositor attribution;traditionally used bibliographers;predicts attributions;images shakespeare folio;bibliographers hinman;bibliographers accuracy 87;predicts attributions agree"}, "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4": {"ta_keywords": "efficient prompt tuning;learning soft prompts;efficient prompt ensembling;language models perform;frozen language models;model soft prompts;prompt tuning remarkably;soft prompts learned;prompt tuning;soft prompts;gpt soft prompts;prompt tuning simple;enables efficient prompt;efficient prompt;soft prompts confers;prompt ensembling;explore prompt tuning;prompts learned backpropagation;language models;code model checkpoints;t5 prompt tuning;parameter efficient prompt;prompt tuning competitive;prompts learned;model checkpoints;discrete text prompts;prompts used gpt;model checkpoints reproduce;model tuning;text prompts", "pdf_keywords": "pretrained language models;frozen language models;language models downstream;frozen pretrained language;language models perform;autoregressive language models;trained large language;large language models;pretrained language;language models range;language models;ef\ufb01cient prompt ensembling;ef\ufb01cient prompt tuning;language models like;autoregressive language;prompt tuning;superglue benchmark task;learning soft prompts;adapting frozen pretrained;models downstream tasks;frozen language;model soft prompts;prompt tuning simple;unlike autoregressive language;prompt tuning competitive;model tuning;superglue benchmark;prompt ensembling;tasks popular superglue;large language"}, "1a3fcb1e2a416cbc79a011f1a1916aa53f7a2a09": {"ta_keywords": "dramatic action emotion;dramatic action verbs;using dramatic action;action emotion;action emotion using;describing body language;representation body postures;dramatic action theory;dramatic action;verbs describing body;postures stick figures;using dramatic;body language;describing body postures;emotional states describing;body language gained;describing body;study dramatic action;emotion using;called dramatic action;body postures stick;gained using dramatic;action verbs signal;role emotional states;emotion using systematic;postures stick;action verbs;postures effectiveness intent;body postures;emotion", "pdf_keywords": ""}, "e63c9eb5b623baad0a7805e839e5d9fabad37fce": {"ta_keywords": "generating detailed explanations;generate detailed explanations;worldtree explanation corpus;human readable explanations;question answering systems;question answering;automated question answering;explanation corpus;answers natural language;explanations require advancing;inference explanation regeneration;readable explanations answers;answering systems increasingly;retrieve answers natural;explanations standardized;explanations answers;reasoning performing achieved;explanations standardized elementary;explanation corpus detailed;hop reasoning performing;answering systems;inference information;readable explanations;hop inference information;inference information combination;questions ability generate;reasoning performing;baseline information retrieval;explanations multi;multi hop inference", "pdf_keywords": ""}, "ab8174a1f1810c1122f90649276a552d2eb1ccd4": {"ta_keywords": "level deep segmentation;deep segmentation;low level deep;losses optimization low;optimization low level;segmentation;losses optimization;order losses optimization;level deep;higher order losses;low level;optimization low;order losses;losses;optimization;low;deep;level;higher order;higher;order", "pdf_keywords": ""}, "7618c65685c98fa88526555ae3f62cd5645066ad": {"ta_keywords": "predicting relation entailment;relation extraction;answering relation extraction;relation extraction summarization;existing wikidata relation;relation relation entailment;relation entailment allows;relation entailment construct;relation entailment;wikidata relation hierarchy;wikidata relation;question answering relation;relation entail;relations words entities;task predicting relation;represent relations task;information represent relations;examine relation entailment;semantic understanding text;relations task paper;relation entailment existence;structured textual information;relation hierarchies enabling;relations words;relations meta relations;relations task;relation hierarchies;relations relations meta;predicting relation;relations meta", "pdf_keywords": ""}, "6e78e32481218e9391a88e6d0e30c0062ae71bec": {"ta_keywords": "embeddings speaker gestures;gesture style transfer;generates gestures speaking;gesture style speaker;gesture generation style;speaker gestures;gesture generation;gestures speaking;style transfer learn;approach gesture generation;gesture generation provides;performing gesture style;style content gestures;gestures gesturing styles;gestures speaking agent;unique gesture style;style transfer mix;gesture style;generates gestures;model generates gestures;study gesture generation;speaker gestures end;gesturing styles;gestures;gesturing styles input;style embeddings speaker;gestures gesturing;style transfer;called gesture style;style transfer multiple", "pdf_keywords": "gesture generation mix;speech gesture generation;audio gesture generation;gesture generation style;embeddings speaker gestures;gesture generation;transfer generated gestures;approach gesture generation;gesture generation conditioned;gesture generation provides;gesture style transfer;speaker gestures;generated gestures;gesture style audio;gesture style speaker;generated gestures new;style audio gesture;models conditioned gesture;study gesture generation;performing gesture style;speaker gestures end;conditioned gesture style;tasks gesture generation;drives speech gesture;speech gesture;gestures new style;audio gesture;unique gesture style;gesturing styles speaker;gesture style"}, "f3bca263a92b69c6da872a9a3268f260ba43f690": {"ta_keywords": "discriminative language models;models discriminative language;language model rnn;acoustic models discriminative;discriminative training acoustic;speech recognition asr;speech recognition task;speech recognition;continuous speech recognition;discriminative method recurrent;discriminative language;discriminative training;automatic speech recognition;vocabulary continuous speech;training acoustic models;recurrent neural network;training recurrent neural;gram language model;language models;discriminative training method;automatic speech;training recurrent;unlike discriminative training;neural network language;model rnn lm;language model;discounted training recurrent;proposes discriminative training;method rnn lm;language model effective", "pdf_keywords": ""}, "53880036fb85cc737103c480c613e1912c416010": {"ta_keywords": "wrapper learning systems;wrapper induction extracting;wrapper learning;extract job postings;structured wrapper induction;wrapper learning encoded;allows wrapper learning;wrapper learning used;structured wrapper;learning used extract;builders structured wrapper;bias wrapper learning;extracting information;extraction language;restricted extraction language;extracting information semi;strength wrapper learning;wrapper induction;induction extracting information;restricted extraction;extraction language implement;semi structured documents;extract job;structured documents;extracting;architecture allows wrapper;wrapper;job postings;learning systems easily;induction extracting", "pdf_keywords": ""}, "a2f4731258830c76af7e3bdb96c4488823219585": {"ta_keywords": "time frequency masking;frequency masking;frequency masking able;frequency masking reverberant;frequency masking method;frequency masking reveal;stereo input speech;blind source separation;masking reverberant environment;robust automatic speech;speech recognition asr;target speech interferences;input speech recognition;speech interferences effectively;masking reverberant;speech recognition;masking able separate;signal robustly noisy;robustly noisy reverberant;better binary mask;present noise robust;speech interferences;noise robust automatic;noise robust;masking method;underdetermined blind source;mask terms recognition;masking method separate;automatic speech recognition;mask better binary", "pdf_keywords": ""}, "341c72f55a89572915aa476db7f525c2e0b60eba": {"ta_keywords": "cluster similarity indices;analysis cluster similarity;similarity indices validate;cluster similarity;evaluate clustering algorithms;evaluate clustering;used evaluate clustering;similarity indices used;clustering algorithms choosing;similarity indices;clustering algorithms;analysis cluster;similarity measure;cluster;clustering;coefficient similarity measure;similarity measure satisfies;systematic analysis cluster;correlation coefficient similarity;validate validation measures;validation measures;indices validate validation;indices validate;similarity;validation measures paper;coefficient similarity;theoretically verify indices;verify indices;verify indices satisfy;indices used", "pdf_keywords": "cluster similarity indices;similarity indices validate;cluster similarity index;analysis cluster similarity;analyze similarity indices;similarity indices;cluster similarity;best cluster similarity;similarity index;similarity indices satisfy;formally analyze similarity;similarity index section;indices introduce notion;counting indices introduce;cluster;analyze similarity;indices introduce;similarity;systematic analysis cluster;indices validate;choosing best cluster;counting indices;discuss indices;indices validate validation;pair counting indices;analysis cluster;choice similarity index;validation measures;best cluster;indices"}, "143183584a8ebaad93490f4550295a9cb6cf9817": {"ta_keywords": "relational learning nlp;statistical relational learning;relational learning srl;relational learning;probabilistic logics;logic probabilistic;logic machine learning;logics applications nlp;probabilistic order logics;probabilistic logics applications;nlp scalable statistical;semantic parsing;natural language processing;learning nlp;order logic probabilistic;foundation probabilistic logics;classification semantic parsing;logic probabilistic order;scalable statistical relational;semantic parsing information;statistical relational;inference order logic;probabilistic inference tutorial;tasks statistical relational;processing nlp;text classification semantic;parsing information extraction;processing nlp tasks;nlp scalable;nlp tasks", "pdf_keywords": ""}, "fc78af26fd7644867af1abb8fbf2c37b47ad8257": {"ta_keywords": "lingual word embeddings;language english embedding;lingual embedding baselines;cross lingual embedding;lingual embedding;lexicon induction evaluation;english embedding;downstream lexicon induction;english embedding space;word embeddings severely;embeddings distant languages;lexicon induction;word embeddings;impact downstream lexicon;evaluation dictionaries english;lexicon induction zero;learning multilingual;downstream lexicon;majority lexicon induction;hub learning multilingual;embeddings severely anglocentric;learning multilingual setting;vast majority lexicon;dictionaries represented languages;language pairs;dictionaries english;language pairs include;embeddings severely;baselines extend language;include language pairs", "pdf_keywords": "lexicon induction performance;embeddings distant languages;lingual embedding baselines;lingual embedding;measure language similarity;cross lingual embedding;distant languages multilingual;language similarity;bilingual distant languages;downstream lexicon induction;induction performance bilingual;impact downstream lexicon;lexicon induction;trained word embeddings;word embeddings;distant languages;language similarity based;downstream lexicon;language pairs;lexicon induction zero;distant languages presents;language pairs using;a\ufb00ects lexicon induction;word embeddings azerbaijani;bilingual distant;distance measure language;extend language pairs;lingual;languages multilingual;languages varying morphological"}, "682660c7a014e806b924fdf1a2a3d999a9ac13cf": {"ta_keywords": "generation abstractive summarization;abstractive summarization;summarization using abstract;abstractive summarization using;representation summarization performance;neural language generation;meaning representation summarization;summarization performance;guidance improves summarization;language generation abstractive;abstract meaning representation;summarization performance later;representation summarization;improves summarization results;guided neural language;work abstractive summarization;summarization results;improves summarization;generation abstractive;summarization results 10;neural encoder decoder;summarization;neural language;summarization using;amr neural language;neural encoder;using abstract meaning;language generation;representation amr neural;parser", "pdf_keywords": "generation abstractive summarization;abstractive summarization progress;summarization using abstract;abstractive summarization;abstractive summarization using;summarization progress neural;based abstractive summarization;abstract meaning representation;neural language generation;guided nlg amr;language generation abstractive;summarization progress;guidance improves summarization;improves summarization results;graphs guided nlg;work abstractive summarization;annotated document summary;summarization results;guided nlg;improves summarization;guided neural language;amr neural language;nlg amr text;guided nlg mechanism;document summary amr;summarization using;guide nlg;neural encoder decoder;summarization using information;summarization results 10"}, "9d698e034d83eedc05237e629eaad1c0c4e5bbb9": {"ta_keywords": "learnable recursive clause;learnable recursive;learnable recursive programs;classes learnable recursive;consisting learnable recursive;recursive logic programs;free recursive logic;clause learnable;nonrecursive clause learnable;recursive programs constrained;clause learnable additional;recursive logic;determinate clause learnable;logic programs polynomial;clause learnable present;programs consisting learnable;recursive programs;recursive clause constant;free recursive;imply pac learnability;single ary recursive;pac learnability classes;clause programs consisting;clause programs;learnability classes;recursive constant depth;consisting learnable;recursive clause;learnable additional basecase;ary recursive", "pdf_keywords": "learnable recursive clause;recursive clause learnable;learnable recursive;learning recursive logic;recursive logic programs;free recursive logic;consisting learnable recursive;pac learning recursive;clause learnable;clause learnable additional;recursive clause class;determinate clause learnable;learning recursive;clause learnable largest;recursive clause constant;recursive logic;logic programs polynomial;programs consisting learnable;described clause programs;clause single recursive;queries largest learnable;clause programs consisting;single recursive clause;clause programs;recursive constant depth;free recursive;recursive literals;single ary recursive;closed recursive literals;non recursive clause"}, "bfe6d67ed1c9119f91774e62fe0f4f328830526e": {"ta_keywords": "neural conversation models;training neural conversation;adaptation neural conversation;conversation models;task learning speaker;learning speaker role;speaker role adaptation;conversation data model;persona based conversation;conversation models experiments;speaker specific conversation;conversation agent;speaker roles modeled;conversation agent challenging;conversation models leverages;based conversation agent;neural conversation;conversation data speakers;specific conversation data;conversation data;learning speaker;speaker roles;speaker role;leverages conversation data;traits speaking styles;speakers traits speaking;data pertaining speaker;speaker speaker roles;conversation;speaking styles", "pdf_keywords": "speaker conversational data;neural conversation models;conversational models;training neural conversation;istics conversational models;conversation models;conversational models using;individual speaker conversational;conversational data;speaker roles modeled;conversation models leverages;generating speaker role;conversation data speakers;speaker conversational;neural conversation;incorporate speaker role;conversation data;speaker role;data pertaining speaker;speaker role character;capture speaker roles;speaker roles expressive;character istics conversational;speaker roles;conversational data conclusion;multi task learning;leverages conversation data;speaker role speci\ufb01c;approach incorporate speaker;conversational"}, "ca879ec1c04b94de274954dfd09dddfde6cbb4f3": {"ta_keywords": "perceptual age singing;singing voice conversion;age statistical singing;statistical singing voice;singing voice age;voice conversion experimental;statistical voice conversion;age singing voice;voice conversion voice;voice timbre control;conversion voice quality;voice conversion;statistical singing;conversion voice;change singer perceptual;singer perceptual age;voice age;voice quality control;use statistical voice;age singing;statistical voice;voice age singer;singing voice;varieties voices singers;voice timbre varieties;voices singers produce;voice conversion svc;proposed voice timbre;age singer perceived;voice timbre", "pdf_keywords": ""}, "bd6c708a535af588d90025a0e6cf17407bf65434": {"ta_keywords": "evaluating model explanations;explanations improve ability;local explanations improve;explanations improve;model explanations;explanation control bert;attributing predictions features;deception detection models;techniques attributing predictions;attempts explain predictions;words model participants;deception detection;human understanding models;understanding models surprisingly;fake hotel reviews;interact deception detection;compared explanation control;reviews attempts explain;explanation control;bert based classifier;models trained;popular local explanations;predictions features deemed;attributing predictions;predictions machine learning;local explanations;explain predictions machine;trained distinguish genuine;models trained distinguish;participants interact deception", "pdf_keywords": "deception detection models;fake hotel reviews;deception detection task;deception detection;paradigm deception detection;models trained;interact deception detection;local explanations improve;learning models trained;trained distinguish genuine;hotel reviews bert;reviews bert based;hotel reviews genuine;bert model people;words model participants;explanations improve;crowdsourcing;remarkably explanation bert;models trained distinguish;conduct crowdsourcing;crowdsourcing study;explanations improve ability;imitate bert model;perform crowdsourcing;detection models trained;participants interact deception;model trained;linear model trained;reviews bert;detect hotel reviews"}, "bf7481685e63b85ef2586de3f6098f1a5fbe0e2d": {"ta_keywords": "spe sampler water;sampler organic contaminants;sampler water;spe cartridge water;water collected sampler;active sampler organic;sampling organic pollutants;sampling surface water;spe sampler;spe samplers;extraction spe cartridge;spe samplers deployed;sampler organic;situ sampling organic;op spe sampler;op spe samplers;extraction spe;sampler water continuously;sampling organic;phase extraction spe;sampler coupling osmotic;herbicides measured concentrations;cartridge water collected;analytes retained spe;organic contaminants approach;novel active sampler;river detect herbicides;extraction situ sampling;samplers deployed river;detect herbicides measured", "pdf_keywords": ""}, "72579f6ce4a413585445c4ef8c8c2fa63ea1b8bc": {"ta_keywords": "privacy deep neural;representations privacy deep;privacy deep;stochastic representations privacy;stochastic perturbations obfuscate;perturbations obfuscate private;representations privacy;privacy;obfuscate private;cloak reduces information;private data;perturbations obfuscate;dnn optimized known;obfuscate private data;private information;private information misused;deep neural;set private information;dnn optimized;use deep neural;deep neural inference;deep neural networks;dnns home automation;trained dnn optimized;neural networks dnns;private data sent;cloud receives inference;networks dnns home;cloud;learning stochastic representations", "pdf_keywords": "prediction privacy;preserving prediction privacy;prediction privacy arxiv;privacy computing;security privacy;privacy privacy protections;security privacy privacy;privacy privacy;privacy;security privacy computing;privacy protections;concepts security privacy;features;privacy computing methodologies;discovering essential features;privacy arxiv;features necessary;receive features fact;features preserving prediction;essential features;features fact;privacy protections usability;essential features preserving;features necessary target;receive features;privacy arxiv 2003;receiving machine learning;machine learning services;features equal discovering;usability security privacy"}, "80edd01d46228fac7ec0cd14aea1666253b28f4d": {"ta_keywords": "voter preferences computationally;voting environment;heuristics voter ignores;voter preferences;heuristics voter;agents manipulate vote;voting scenarios;look heuristics voter;voting scenarios people;information voter preferences;ignores information voting;information voting;voting environment affects;outcome voting way;agents vote choose;better outcome voting;outcome voting;approval voting scenarios;structure voting environment;information voting profiles;voting profiles;utility structure voting;manipulate vote;agents vote;voting scenarios missing;winner approval voting;world voting scenarios;vote choose alternative;real world voting;voting profiles makes", "pdf_keywords": "voting heuristics;voting heuristics computational;heuristics voter;keywords voting heuristics;heuristics voter ignores;voting environments;voting elections uncertainty;look heuristics voter;approval voting environments;approval voting scenarios;voting environments jaelle;voting environment;heuristics single winner;uncertain approval voting;winner approval voting;preferences collective decision;voting scenarios;elections uncertainty;votes behavioral;agents vote choose;computational social choice;utility structure voting;elections uncertainty paper;approval voting elections;approval voting;experiment people vote;structure voting environment;heuristics computational social;voting scenarios candidate;information voting"}, "1da3a9c194a01c0bff7b6ecda79db9d673810bee": {"ta_keywords": "outperform recurrent neural;outperform recurrent models;outperform recurrent;works outperform recurrent;transliteration morphological inflection;conversion transliteration morphological;transliteration morphological;does outperform recurrent;recurrent neural network;morphological inflection generation;recurrent models;sequence models batch;phoneme conversion transliteration;nlp tasks transformer;level nlp tasks;recurrent models using;recurrent neural;word level nlp;character level transduction;shown outperform recurrent;transduction tasks grapheme;level nlp;conversion transliteration;transliteration;text normalization;text normalization works;contrast recurrent sequence;historical text normalization;character level tasks;sequence sequence models", "pdf_keywords": "outperform recurrent neural;word level nlp;recurrent neural;outperform recurrent;level nlp tasks;universal morphological rein\ufb02ection;recurrent neural network;level nlp;task universal morphological;shown outperform recurrent;character level transduction;nlp tasks;models various word;universal morphological;sequence sequence models;morphological rein\ufb02ection;sequence models;recurrent;sequence models various;morphological rein\ufb02ection pages;various word level;nlp tasks proceedings;word level;morphological;conll sigmorphon 2017;nlp;transformer character level;conll sigmorphon 2018;transformer character;neural"}, "28e81f96eab94e99febcaaee00637825c8a3e664": {"ta_keywords": "iml interpretable machine;iml interpretable;interpretable machine learning;machine learning society;field iml interpretable;interpretable machine;building trust models;increasingly complex models;trust models performing;understand reasoning increasingly;reasoning increasingly complex;machine learning grew;trust models;reasoning increasingly;machine learning;model debugging generally;models;interpretable;iml;model debugging;increasingly complex;people inability understand;field iml;cases building trust;inability understand reasoning;human decision making;complex models;informing real human;understand reasoning;model", "pdf_keywords": ""}, "faf494d0aa25a17aa25930ffb4c750fa59c44849": {"ta_keywords": "tts speaker classification;learning speaker verification;dataset tts speaker;tts embeddings improved;unsupervised tts embeddings;tts speaker embedding;phonetic information tts;speaker embedding networks;representation learning speaker;speaker verification;speaker verification hypothesize;speaker classification loss;speaker embedding;transcripts kaldi phone;tts embeddings;speaker classification;asr transcripts kaldi;minimal phonetic information;using speaker classification;compared asr transcripts;asr transcripts;phonetic information;learning speaker;embeddings improved eer;transcripts kaldi;datasets manual transcripts;information tts decoder;manual transcripts;tts decoder;transcripts", "pdf_keywords": "speaker embedding tts;tts speaker embedding;representation learning speaker;learning speaker embedding;speaker embedding networks;trained speaker encoder;speaker embedding;computes representations speaker;learning speaker veri\ufb01cation;representations speaker veri\ufb01cation;embedding tts;speaker encoder computes;speaker encoder;representations speaker;learning speaker;phonetic information tts;speaker veri\ufb01cation task;embedding tts criterion;tts speaker;tacotron tts speaker;trained speaker;conclude learning speaker;improve representation learning;minimal phonetic information;tts decoder;information tts decoder;speaker veri\ufb01cation;speaker veri\ufb01cation jointly;embedding networks;train embeddings"}, "7efb1788b5e0fa3b4d9932722286ba1753b42f91": {"ta_keywords": "tasks language description;tasks convey semantics;taskspecific ontology schemata;semantics effectively task;tasks language;task oriented dialogue;taskspecific ontology;language description driven;contained taskspecific ontology;natural language descriptions;given tasks language;dialogue tod systems;tasks information conventionally;convey semantics effectively;description driven;tasks information;convey semantics;understanding task specifications;language descriptions;task specifications;semantics effectively;propose schemata;task oriented;task specifications higher;language description;ontology schemata;oriented dialogue tod;effectively task oriented;propose schemata modified;intents uniform tasks", "pdf_keywords": "task oriented dialog;task oriented dialogue;dialogue tod systems;dialogue state tracking;description driven task;dialog modeling;dialog tod conventionally;descriptiondriven dialogue state;oriented dialog modeling;oriented dialogue tod;descriptiondriven dialogue;oriented dialog tod;intents description driven;named descriptiondriven dialogue;dialog tod;intents language description;dialog modeling jeffrey;oriented dialog;language description driven;dialogue state;dialog;oriented dialogue;task oriented;information conversations completion;intents language;descriptions identi\ufb01cation active;driven task oriented;intents description;abstract task oriented;slots intents language"}, "9688671a573651955c26d710c12617de26715e78": {"ta_keywords": "regenerating msr codes;bandwidth exact repair;repair msr codes;bound repair bandwidth;regenerating codes possess;termed repair bandwidth;regenerating codes;interference alignment exact;msr codes proof;msr codes minimum;necessity interference alignment;termed miser code;code regenerating codes;regenerating codes require;mds code regenerating;msr codes 2k;exact interference alignment;interference alignment ia;exact repair msr;msr codes;interference alignment;repair bandwidth exact;network storage code;code regenerating;repair bandwidth;miser code achieves;storage regenerating msr;rate msr codes;mds codes 2k;miser code built", "pdf_keywords": "codes interference alignment;alignment regenerating codes;regenerating codes distributed;abstract regenerating codes;codes distributed storage;codes interference;reed solomon codes;mds regenerating codes;regenerating codes achieving;regenerating codes;explicit msr codes;msr codes interference;msr codes proof;codes distributed;interference alignment exact;mds codes 2k;bound repair bandwidth;solomon codes;interference alignment fact;repair msr codes;termed miser code;miser code built;miser code achieves;interference alignment ia;msr codes achieving;msr code conclusions;interference alignment regenerating;regenerating codes class;codes 2k termed;mds codes"}, "148efaba70165d9faef0dac28d5fa2538cfa662d": {"ta_keywords": "biases human ai;interplay cognitive biases;model cognitive biases;cognitive biases human;ai collaborative decision;account cognitive biases;cognitive biases provide;cognitive biases;ai accuracy people;cognitive biases confirmation;ai human decision;decision makers ai;bias implement time;ai assisted decision;human ai collaborative;ai collaborative;bias anchoring bias;collaborative performance ai;human ai accuracy;bias availability bias;biases confirmation bias;distorted cognitive biases;bias availability;effectiveness human ai;biases human;ai model predictions;anchoring bias availability;bias implement;collaborative decision;confirmation bias anchoring", "pdf_keywords": "ai collaborative decision;ai collaborative decisionmaking;biases human ai;human ai collaborative;ai collaborative accuracy;human ai collaboration;ai collaborative;ai collaboration assumptions;ai collaboration;human ai accuracy;optimal human ai;maximize human ai;time allocation policy;collaborative decision;human ai;collaborative decision making;cognitive biases human;collaborative decisionmaking;algorithm human decision;consider collaborative decision;interplay cognitive biases;bias variance ai;model cognitive biases;cognitive biases provide;ai accuracy maximize;human decision maker;collaborative accuracy;explainable ai;propose time allocation;collaborative decisionmaking setting"}, "a6219725a9ad2079536c091f02fda2d4da6d62ac": {"ta_keywords": "storage peer peer;erasure coding techniques;regenerating codes;storage peer;provided erasure coding;erasure coding;exact regenerating codes;regenerating codes introduced;regenerating codes additional;reliability distributed storage;regenerating code;regenerating codes certain;exact regenerating code;introduced regenerating codes;code exact regenerating;codes introduced regenerating;regenerating node failure;distributed storage;regenerating node;distributed storage systems;regenerating code derived;construction regenerating codes;failure minimum storage;systems minimizing storage;applications storage peer;storage systems minimizing;peer peer systems;storage systems;simultaneous node failures;property regenerating node", "pdf_keywords": ""}, "3c1001c04866647650216201feb54c927af3a05b": {"ta_keywords": "concept description language;description language learning;description language;antecedent description language;concept description;called concept description;predicates used programming;description language explicit;language called concept;knowledge including constraints;theories syntactically;describes learning;grammatically biased learning;language learning systems;including constraints predicates;langu concept learning;learning horn theories;large concept description;constraints predicates used;constraints predicates;concept learning;concept learning produces;theories theories syntactically;description language xed;explicit antecedent description;constrained language;language learning;knowledge including;learning learning horn;describes learning makes", "pdf_keywords": ""}, "60f0af1dbc2775a69f64e4351d969ac966659fb2": {"ta_keywords": "dictionaries automatic synset;synonymy graph sparseness;sparsity synonymy dictionaries;synonymy dictionaries automatic;automatic synset induction;synset induction methods;input synonymy graph;synonymy dictionaries;synset induction;quality extracted synsets;extracted synsets;performance synset induction;sparseness input dictionary;similar synset clusters;incompleteness input dictionaries;automatic synset;merging similar synset;synset clusters;graph sparseness input;similar synset;dictionaries automatic;extracted synsets evaluate;dictionaries methods;input dictionary substantially;synonymy graph;graph sparseness;synset;synsets;input dictionaries methods;dictionary substantially reduce", "pdf_keywords": "unsupervised synset induction;sparsity synonymy dictionaries;incompleteness synonymy dictionaries;synonymy graph sparseness;synonymy dictionaries run;synset induction methods;clustering synonymy graph;based synset induction;synonymy dictionaries;clustering synonymy;synset induction;performance synset induction;unsupervised synset;synset induction method;synonymy dictionaries paper;global clustering synonymy;graph based synset;datasets russian language;input synonymy graph;quality extracted synsets;extracted synsets;synonymy graph methods;sparsity incompleteness synonymy;sparseness input dictionary;incompleteness input dictionaries;dictionaries paper propose;dictionary substantially reduce;approaches datasets russian;input dictionary substantially;incompleteness synonymy"}, "22616702da06431668022c649a017af9b333c530": {"ta_keywords": "automated fact checking;fact checking task;journalism automated fact;fact checking research;research fact checking;fact checking;automated fact;survey automated fact;journalism automated;misinformation stimulated research;databases journalism automated;misinformation stimulated;natural language processing;checking research stemming;assessing truthfulness claim;misinformation;increased focus misinformation;truthfulness claim paper;use inconsistent terminology;representation databases journalism;databases journalism;natural language;focus misinformation;journalism;task assessing truthfulness;research automating task;research stemming;research automating;focus misinformation stimulated;paper survey automated", "pdf_keywords": "automated fact checking;fact checking task;approaches automated fact;research automated fact;fact checking research;fact checking work;automated fact;survey automated fact;research fact checking;fact checking;fact checking paper;natural language processing;nlp research automated;processing nlp;processing nlp related;nlp research;future nlp research;nlp related \ufb01elds;language processing nlp;nlp related;natural language;viewpoint natural language;language processing;knowledge representation;nlp;checking task formulations;future nlp;checking research stemming;knowledge representation databases;learning knowledge representation"}, "6b7f2f30840b0d72484784a15b3be670868a9f95": {"ta_keywords": "lingual syntactic transfer;parsing cross lingual;universal dependency treebanks;syntactic transfer unsupervised;tagging dependency parsing;dependency treebanks;cross lingual syntactic;dependency parsing cross;dependency treebanks use;source corpus transfer;improvement dependency parsing;interlingual latent embedding;dependency parsing;syntactic transfer;english source corpus;dependency parsing direct;lingual syntactic;dataset distant english;tagging dependency;transfer distant languages;lingual word embedding;corpus transfer wide;languages dataset distant;source corpus;corpus transfer;methods cross lingual;treebanks;treebanks use english;speech pos tagging;treebanks use", "pdf_keywords": "rely additional language;transfer distant languages;language category distant;language distances selection;target languages;methods cross lingual;target languages using;selected target languages;tagging dependency parsing;target languages study;dependency model language;taggers dependency parsers;model language category;languages study transfer;target languages training;parsing dependency model;language distances;linguistic features target;2019 extract linguistic;extract linguistic;dependency parsers trained;multiple source languages;additional language;languages training feature;prior dependency parsing;dependency parsing tasks;language category;features target languages;distant languages;syntactic analysis tasks"}, "eebc1811c55c2e5e8b3b78d0b0382ad50f22e32a": {"ta_keywords": "fact verification adversarial;adversarial fact verification;adversarial natural language;fact checking resources;identifying factual revisions;adversarial fact;10 adversarial fact;000 wikipedia revisions;verification adversarial natural;factual revisions;automatic edits factually;fact verification models;factual changes training;factual revisions providing;verification adversarial;edits factually consistent;wikipedia revisions;words evidence verifying;wikipedia revisions modify;natural language inference;fact leverage revisions;edits factually;factually consistent text;accuracy 10 adversarial;claim identifying factual;fact checking;language inference nli;adversarial natural;evidence verify claims;consistent text generation", "pdf_keywords": "fact checking resources;adversarial fact veri\ufb01cation;adversarial natural language;factual revisions wikipedia;adversarial fact;fact veri\ufb01cation adversarial;10 adversarial fact;textual sources evidence;identifying factual revisions;dataset based factual;natural language inference;000 wikipedia revisions;fact checking;based factual revisions;factual revisions;claims comparing textual;factual revisions providing;training evaluating fact;claim identifying factual;comparing textual sources;factually consistent text;automatic edits factually;fact leverage revisions;identifying factual;edits factually consistent;words evidence verifying;textual sources;edits factually;revisions wikipedia introduce;wikipedia revisions"}, "6dd1e4d97dbdb370a36c25f82a9a9baaa16c836c": {"ta_keywords": "coil ebola virus;ebola virus glycoprotein;ebola virus gp;coiled coil ebola;motif gp2 infectivity;peptides corresponding coiled;coil ebola;protein pseudotyped ebola;ebola virus;pseudotyped ebola virus;virus glycoprotein introduced;virus glycoprotein;entry ebola virus;coil motif gp2;gp2 infectivity mutant;ebola virus host;vesicular stomatitis virus;gp2 infectivity;motif gp2 examined;coiled coil motif;stomatitis virus;virus gp;peptides corresponding;hydrophobic residues coiled;virus lacking receptor;act efficient antiviral;glycoprotein introduced alanine;efficient antiviral;peptides;host cells peptides", "pdf_keywords": ""}, "7e122cc1a62e2f30951e14b91811896e1866dd7c": {"ta_keywords": "music transformer gans;gan symbolic music;sequence transformer gan;music generated;adversarial losses;gans used sequence;music generated approach;discriminative metric music;symbolic music generation;metric music generated;music generation;transformer gans;music generation using;adversarial;transformer gan symbolic;gans;gan symbolic;transformer gan;exploration adversarial losses;sequence generation quality;adversarial losses complement;music generation uses;generation using learned;transformer gans used;exploration adversarial;trained minimizing negative;music transformer;art music transformer;generation quality samples;using learned loss", "pdf_keywords": ""}, "c7af06170f3d81ab761873a4c1fe0af2736eb0a2": {"ta_keywords": "affective communication automatic;automatic prediction emotion;prediction emotion triggers;emotion human computer;natural conversation television;response affective interaction;prediction emotion;affective communication;conversation television talk;social affective communication;affective interaction;responses television talk;conversation television;process natural conversation;emotion emotional responses;emotional responses events;natural conversation;action response affective;emotion triggers responses;affective interaction paper;emotional responses;incorporate emotion human;social affective process;emotion triggers;television talk;talk shows utilize;analyze occurrences emotion;elicit emotional triggers;triggers responses television;talk shows knowledge", "pdf_keywords": ""}, "8dd3b88ac87372c9f4428029ac12288ff3405199": {"ta_keywords": "lipids nlr assessed;variability hdl ldl;lipoprotein cholesterol ldl;lipid levels nlr;hdl ldl risk;ldl risk factors;ldl hdl risk;lipoprotein cholesterol hdl;cholesterol ldl;variability ldl nlr;cholesterol hdl regression;variability hdl nlr;ldl risk;variability blood lipid;lipoprotein cholesterol;density lipoprotein cholesterol;cholesterol hdl;variability blood lipids;hdl risk factors;variability ldl;blood lipid levels;baseline ldl hdl;low density lipoprotein;blood lipids nlr;high density lipoprotein;density lipoprotein;cholesterol ldl 626;variability ldl subgroup;lipid levels;conclusion variability hdl", "pdf_keywords": ""}, "36b6abfb32ea56208a2858b558acbdd001c965e9": {"ta_keywords": "neural machine translation;translation generation;machine translation generation;machine translation;translation generation held;translation generation summarize;second workshop neural;workshop neural machine;workshop neural;neural machine;generation summarize;computational linguistics;linguistic structure domain;computational linguistics acl;linguistics acl 2018;linguistic structure;linguistic;translation;neural;structure domain adaptation;domain adaptation;note particular linguistic;particular linguistic structure;linguistics acl;linguistics;generation summarize research;particular linguistic;adaptation data augmentation;association computational linguistics;domain adaptation data", "pdf_keywords": "neural machine translation;machine translation nmt;machine translation generation;translation generation;machine translation;neural mt submissions;workshop neural machine;translation generation saw;second workshop neural;translation nmt participants;workshop neural;best practices neural;neural machine;translation nmt;ef\ufb01cient neural machine;neural mt;ef\ufb01ciency neural mt;neural network optimization;practices neural;task ef\ufb01cient neural;practices neural network;neural;neural network;tasked creating nmt;ef\ufb01ciency neural;translation;creating nmt systems;ef\ufb01cient neural;creating nmt;model distillation"}, "47b6023808002dfde031c17b34dcb1b522d3b326": {"ta_keywords": "toshiba rice cooker;rice cooker;new toshiba rice;toshiba rice;rice cooker proceedings;development new toshiba;cooker;toshiba;cooker proceedings;new toshiba;cooker proceedings 35th;design development new;rice;design development;design;annual conference jssd;development new;conference jssd;jssd;development;35th annual conference;annual conference;proceedings 35th annual;proceedings 35th;new;proceedings;conference;35th;35th annual;annual", "pdf_keywords": ""}, "743d1aae44a12fb37b743ec947fad41cba9831b8": {"ta_keywords": "text generation pragmatics;conditional text generation;generation pragmatics;text generation;language generation tasks;generation pragmatics imposed;methods text generation;language generation;pragmatic modeling;text generation using;pragmatic modeling methods;consider pragmatic modeling;techniques computational pragmatics;computational pragmatics;standard language generation;formulate language production;generate output text;language production;generation using techniques;language production game;computational pragmatics consider;pragmatics;models conditional text;generation tasks;modeling distractors approaches;consider pragmatic;pragmatics consider pragmatic;pragmatics consider;pragmatic;generation using", "pdf_keywords": "text generation pragmatics;language generation tasks;explicit pragmatic modeling;pragmatic modeling;structured meaning representations;generation pragmatics;language generation;referents structured descriptions;pragmatic modeling methods;consider pragmatic modeling;generating text explicit;speakers attempt referents;language generation problems;explicit modeling distractors;methods text generation;text generation;text explicit pragmatic;generation pragmatics imposed;structured descriptions lingustic;modeling distractors;pragmatic reasoning;generation tasks;referents structured;learned rational speech;use referents structured;generation tasks like;class language generation;pragmatic modeling procedure;distractors models;texts consider pragmatic"}, "ba5e3559a2d54bb0e8d7678c9905b4a77da63f71": {"ta_keywords": "incentivizing truthful feedback;incentivizes truthful responses;incentivize truthful behavior;evaluations incentivize truthful;rule incentivizing truthful;mechanism incentivizes truthful;truthful responses objective;incentivizing truthful;incentivize truthful;peer reward;peer reward inversely;matches peer reward;demonstrate robustness incentive;objective evaluations incentivize;agreement rule incentivizing;incentivizes truthful;responses objective evaluations;incentive;reward evaluation;robustness incentive;reward mechanism incentivizes;gets reward evaluation;truthful feedback online;incentive properties;evaluations incentivize;robustness incentive properties;truthful feedback;eliciting informative responses;reward evaluation answer;responses objective", "pdf_keywords": "incentive properties sra;incentive properties;distributional information agent;robustness incentive properties;agent responses;arbitrary mutual information;responses setting incentivizes;agent responses compute;leverages distributional information;demonstrate robustness incentive;information agent responses;incentivizes truthful behavior;robustness incentive;framework homogeneous responses;tasks homogeneous responses;responses setting mutual;distributional information;truthfulness arbitrary mutual;setting mutual information;mutual information;biases responses;agent homogenous responses;learned distributional information;evaluation agent homogenous;biases responses results;incentivizes truthfulness evaluation;responses compute payments;yield truthfulness arbitrary;incentive;distributional information obtained"}, "52824fb6eb5d3b55fb6634c77dc80f5826964464": {"ta_keywords": "extracting specifications software;extracting specifications;technique extracting specifications;inductive logic programming;specification recovery;specifications software using;used specification recovery;specifications software;specification recovery domain;logic programming methods;logic programming;inductive logic;inductive learning techniques;shelf inductive logic;programming methods;generating examples behavior;programming methods successfully;inductive learning;forming general description;successfully used specification;programming;technique instrumented code;generating examples;recovery domain methods;machine learning;cases generating examples;test cases generating;machine learning techniques;general description aspect;techniques used generalize", "pdf_keywords": ""}, "0cd693f1a1223f25e89c1f5efdedd7c3b7846691": {"ta_keywords": "traffic congestion;overall traffic congestion;traffic congestion probability;parking arterial traffic;city level traffic;proportion traffic;traffic parking data;traffic parking;traffic volume;arterial traffic;traffic;traffic addition;estimate proportion traffic;overall traffic;arterial traffic volume;travel time traffic;congestion average occupancy;congestion probability;queuing network;traffic vehicles depends;level traffic parking;traffic addition utilize;volumes traffic;traffic vehicles;controlling congestion;time traffic;proportion traffic searching;new queuing network;traffic volume data;congestion probability block", "pdf_keywords": "parking network;curbside parking network;model urban parking;network curbside parking;capacity queues tasks;parking network \ufb01nite;capacity queues context;capacity queues;parking considers;\ufb01nite capacity queues;urban parking considers;parking neighborhood;model curbside parking;parking considers spatial;searching parking neighborhood;urban parking;curbside parking;capacity queues demonstrating;parking;networks \ufb01nite capacity;queues tasks;drivers searching parking;searching parking;queue available server;queues;queue search;queue;queues context;queues tasks arrive;queue search service"}, "1f0524971c20a06d745ab784689eb8833435fde1": {"ta_keywords": "written factoid claims;evidence retrieved wikipedia;human written factoid;factoid claims supported;fact extraction verification;factoid claims;fact extraction;written factoid;results fact extraction;wikipedia present results;using evidence retrieved;wikipedia present;factoid;evidence retrieved;results fact;retrieved wikipedia present;using evidence;task challenged participants;verification fever shared;wikipedia;retrieved wikipedia;classify human written;refuted using evidence;task challenged;verification fever;evidence;fever shared task;challenged participants classify;results shared task;task summary systems", "pdf_keywords": "modeling sentence extraction;select wikipedia sentences;sentence extraction;fact extraction;extract sentences predict;wikipedia sentences providing;fact extraction veri\ufb01cation;extract sentences;sentence extraction veri\ufb01cation;fever fact extraction;memory extracted sentences;wikipedia sentences;sentences providing evidence;extracted sentences;high precision entailment;sentences wikipedia;information retrieval;relation extracted sentences;model fact extraction;sentences wikipedia article;entailment classi\ufb01er based;precision entailment;documents using lexical;identifying relevant documents;extracted sentences tune;approach select wikipedia;precision entailment classi\ufb01er;lexical syntactic features;fact checking veri\ufb01cation;task learning"}, "68731c68773b117250f04509103031109b222d27": {"ta_keywords": "sentence level extractions;information extraction global;extractions translatingbased objective;information extraction;extractions translatingbased;segmenting entity relation;entity relation phrases;extraction global structure;unify segmenting entity;segmenting entity;sentences based local;level extractions translatingbased;framework distant supervision;extraction global;individual sentences based;relation phrases individual;unify segmenting;phrases individual sentences;sentences based;translatingbased objective;corpora;real world corpora;systems information extraction;sentence level;phrases individual;individual sentences;quality sentence level;sentences;context measuring;extraction", "pdf_keywords": ""}, "fd8e176087335355ff5e81821a616d15ec8d3346": {"ta_keywords": "labels based indirect;synthesizing training labels;training labels based;unseen labels generalization;synthesize training labels;based indirect supervision;training labels;indirect supervision sources;weak indirect supervision;indirect supervision extend;label relations model;noisy supervision sources;label relations;labels generalization;indirect supervision;provided label relations;recent weak supervision;label spaces overcome;leverage indirect supervision;indirect supervision wis;automatically synthesizing training;labels based;plrm unseen labels;label spaces;output label spaces;training labels multiple;unseen labels;potentially noisy supervision;labels;supervision sources", "pdf_keywords": "weak indirect supervision;indirect supervision sources;based indirect supervision;annotation weak supervision;weak supervision sources;indirect supervision wis;indirect supervision;creating labeled training;labeled training sets;labels based indirect;supervision sources construction;supervision sources label;labeled training;weak supervision;leverage indirect supervision;supervision sources;synthesizing training labels;indirect supervision jieyu;training sets weak;sets weak indirect;training labels based;data annotation weak;training labels;synthesizes training labels;weak indirect;supervised;formulate weak indirect;label relations minimizing;creating training sets;creating labeled"}, "e34f9e9163b13de00707157feda6a8b853c5c82d": {"ta_keywords": "duplicates crowdsourced lexical;fuzzy duplicates crowdsourced;crowdsourced lexical resources;duplicates crowdsourced;crowdsourced lexical;collaboratively created lexical;fuzzy duplicates;eliminating fuzzy duplicates;created lexical resources;resource collaboratively created;crowdsourced;constructing resource collaboratively;resource collaboratively;lexical resources eventually;lexical resources trending;deduplication;collaboratively created;comparable expert based;lexical resources;created lexical;deduplication problem;deduplication problem fully;duplicates;solve deduplication problem;approach solve deduplication;quality comparable expert;lexical;expert participants;collaboratively;solve deduplication", "pdf_keywords": ""}, "e1d35deec12d18e53ca97a3cf4071526ad47968d": {"ta_keywords": "pretrained language models;language models lms;models acquisition linguistic;transformer language models;language models;large pretrained language;language models affect;ability learn linguistic;pretrained language;acquisition linguistic knowledge;learn linguistic;acquisition linguistic;linguistic knowledge modifications;attention configuration training;learn linguistic knowledge;linguistic knowledge;linguistic knowledge encoded;kinds linguistic knowledge;linguistic;examining kinds linguistic;language;self attention configuration;transformer language;models lms;knowledge encoded models;attention;ability learn;kinds linguistic;attention configuration;improvements subcategories detecting", "pdf_keywords": ""}, "e2ebf18e0b88752bd3ff905d2fba74213dcd2c51": {"ta_keywords": "prediction electrolarynx speaking;sounds generated electrolarynx;vibration control electrolarynx;prediction el speech;generated electrolarynx based;control electrolarynx based;electrolarynx based;prediction electrolarynx;electrolarynx based statistical;electrolarynx use prototype;electrolaryngeal el speech;electrolarynx speaking aid;contour prediction electrolarynx;speech real time;generate excitation sounds;electrolarynx speaking;laryngectomees produce electrolaryngeal;mechanical excitation sounds;physical electrolarynx;control electrolarynx;physical electrolarynx use;excitation sounds generated;generated electrolarynx;real time vibration;speech proposed method;sounds help laryngectomees;actual physical electrolarynx;speech yielded prototype;speech make possible;electrolarynx", "pdf_keywords": ""}, "595a79ca667258ca2a4f5e7775e95a0fb0a0f024": {"ta_keywords": "monotone games complexity;free gradient play;gradient play strongly;strongly monotone games;play strongly monotone;monotone games in\ufb02uential;games complexity target;gradient play;games complexity;free gradient;derivative free gradient;monotone games;unconstrained optimization;complexity target accuracy;rates derivative free;method unconstrained optimization;derivative free;unconstrained optimization 2018;optimization 2018;strongly monotone;shows derivative free;optimization;improved rates derivative;play strongly;games in\ufb02uential;complexity target;optimization 2018 shows;improved rates;games in\ufb02uential work;complexity", "pdf_keywords": "stochastic gradient play;monotone games complexity;free gradient play;strongly monotone game;gradient play strongly;strongly monotone games;gradient play;monotone game achieve;monotone game;play strongly monotone;algorithms games;games complexity;monotone games;monotone game tighter;games complexity d2;learning algorithms games;algorithms games continuous;smooth cost functions;monotone games dmitriy;game tighter analysis;unconstrained optimization;method stochastic gradient;stochastic gradient;unconstrained optimization arxiv;simply stochastic gradient;game de\ufb01ned cost;method unconstrained optimization;optimal rate;free gradient;gradient play slightly"}, "36f7827bc344f9c2198dcb29732c525c68dc637a": {"ta_keywords": "shapley allocations transport;allocations transport costs;shapley allocations;computationally tractable allocation;allocations transport;transport costs;proxies shapley allocations;approximating shapley;approximating shapley value;tractable allocation techniques;allocation techniques good;transport costs cost;tractable allocation;allocations;operationally transportation settings;transportation settings;allocation techniques;tours calculated scenarios;vehicle transport;single vehicle transport;allocation;vehicle transport setting;strategically operationally transportation;shapley value;transportation settings perform;calculating shapley;prove approximating shapley;shapley value tsg;synthetic euclidean games;euclidean games games", "pdf_keywords": "allocations transport costs;transport costs;transport costs haris;shapley allocations transport;allocations transport;transport costs arxiv;proxies shapley allocations;single vehicle transport;vehicle transport;shapley allocations;calculating cost serve;cost serve location;costs;vehicle transport setting;transport;tours calculated;allocations;tours calculated fast;games problems derived;transport setting;world tours calculated;calculating cost;australia treat transport;computationally tractable proxies;proxies good approximations;location single vehicle;problem calculating cost;cost;shapley value game;proxies shapley"}, "ead1e044d284f3deecd32c2d5cc89fe513195a0a": {"ta_keywords": "synonymy graph augmentation;input synonymy graph;synonymy graph;approach synonymy graph;synonymy graph paper;graph clustering comparing;quality graph clustering;graph clustering;graph augmentation;synonyms input;synonyms input synonymy;datasets russian language;synonyms;potential synonyms input;edges potential synonyms;synonymy relation;clustering comparing non;augmented input graph;clustering comparing;input synonymy;transitive edges;missing transitive edges;increase quality graph;synonymy relation implies;quality graph;input graph approach;approach datasets russian;input graph;synonymy;clustering", "pdf_keywords": ""}, "7e63be5285e6596fbbc6c56bc89f7b6fd8bbe8c5": {"ta_keywords": "trained model contamination;model explanations effective;explanations effective diagnosing;model explanations;diagnosing model contamination;model debugging;explaining model prediction;diagnosing model errors;explanations tools;primarily model predictions;identify mislabeled training;ineffective diagnosing model;explanations tools model;explanations effective;errors model debugging;diagnose mislabeled training;turning explanations tools;model debugging response;diagnosing model;deep network ineffective;effective diagnosing model;mislabeled training examples;model debugging complement;model trained;model contamination detect;explanation methods assess;model contamination;explanation methods proposed;tools model debugging;hoc model explanations", "pdf_keywords": "model debugging training;debugging training data;debugging training;model debugging;trained model contamination;diagnosing model errors;debugging tests model;errors model debugging;diagnosing model;tests model explanations;debugging tests;model contamination detect;diagnose mislabeled training;model contamination;model bugs stage;model bugs;debugging;debugging framework;model explanations;model trained;e\ufb00ective diagnosing model;test input model;mislabeled training examples;debugging framework standard;hoc model explanations;model trained model;learning pipeline explanation;diagnosing;trained model;correlation model bugs"}, "bd8922f8cc8284553dc9e6db529af309298451fe": {"ta_keywords": "pretraining using transcribed;text train attention;data transcribed speech;transcribed speech languages;transcribed speech depend;target text train;using transcribed speech;text train;text based augmentation;train attention decoder;transcribed speech nearby;transcribed speech;speech languages;speech languages test;using transcribed;amounts transcribed speech;based data transcribed;attention decoder;data transcribed;speech nearby languages;speech depend;test languages text;augmentation technique pretraining;attention decoder networks;train attention;improvement text based;languages text;small amounts transcribed;languages text based;decoder networks", "pdf_keywords": ""}, "25ddddbd0bd1cfebf1548b2ee91bb1bbd05fdff1": {"ta_keywords": "language instructions multimodal;training synthetic instructions;compositional tasks pretraining;challenges neural agents;instructions multimodal transformer;instructions multimodal;human instructions interaction;neural agents demonstrate;complex human instructions;agents demonstrate encoding;multimodal transformer encodes;language instructions dynamic;transformer encodes language;instructions dynamic environments;multimodal;neural agents;instructions interaction navigation;transformer improve training;compositional tasks;synthetic instructions improve;long sequence subtasks;episodic transformer improve;visual observations actions;multimodal transformer;encodes language inputs;natural language instructions;sequence subtasks;training synthetic;instructions dynamic;synthetic instructions intermediate", "pdf_keywords": "compositional tasks pretraining;highly compositional tasks;challenges neural agents;compositional tasks;task episodic transformer;long sequence subtasks;solve compositional tasks;training synthetic instructions;compositional tasks consisting;sequence subtasks;vision language navigation;complex human instructions;task episodic;challenges neural;neural agents demonstrate;transformer vision language;tasks pretraining;language instructions dynamic;significant challenges neural;sequence subtasks understanding;vision language;instructions dynamic environments;specify task episodic;neural agents;subtasks understanding complex;tasks pretraining joint;synthetic instructions improve;subtasks actions understanding;language navigation alexander;agents demonstrate encoding"}, "0805cb1b26577f08f84190445992f7f0584e4742": {"ta_keywords": "opera cmu;opera cmu usc;end information extraction;cmu usc isi;usc isi performs;usc isi;opera;information extraction;isi performs;media languages;information extraction multiple;isi;isi performs end;cmu;end end information;multiple media languages;end information;cmu usc;knowledge bases;builds knowledge bases;media languages english;knowledge bases domain;information;languages;results builds knowledge;languages english;reasoning answer questions;usc;builds knowledge;extraction", "pdf_keywords": ""}, "352ac73b7d92afa915c06026a4336927d550cec3": {"ta_keywords": "graph neural;graph neural network;novel graph neural;parameters gp gnns;hop relational reasoning;gnns transition matrices;gp gnns transition;hop relational;multi hop relational;gnns transition;hop reasoning mechanism;multi hop reasoning;novel graph;network generated parameters;neural network generated;relational reasoning;sentences inputs parameters;propose novel graph;relations multi hop;generated parameters gp;hop reasoning;datasets multi hop;gp gnns;graph;natural language;message passing procedure;discover accurate relations;neural network;parameters propagation module;sentences inputs", "pdf_keywords": "gnns solves relational;gnns process relational;relational reasoning unstructured;models graph neural;propose graph neural;parameters graph neural;natural language relational;graph neural;graph neural networks;novel graph neural;graph neural network;relational reasoning natural;language relational reasoning;reasoning natural languages;natural language parameters;relational reasoning indispensable;networks gnns;adapt graph neural;networks gnns effective;reasoning unstructured text;parameters gp gnns;relational message passing;relational reasoning;hop relational reasoning;models gp gnns;language relational;relation extraction;sentences enables gnns;relational reasoning task;networks gp gnns"}, "b53689b8c28353106f327f0981b108eb67816053": {"ta_keywords": "machine translation preprocessing;based syntactic preprocessing;syntactic preprocessing approaches;syntactic preprocessing syntax;syntactic preprocessing;translation preprocessing techniques;translation preprocessing;preprocessing syntax based;based machine translation;preprocessing syntax;syntax based smt;machine translation pbmt;machine translation;preprocessing method englishjapanese;rule based syntactic;syntax based machine;syntactic information linguistically;syntax based;techniques using syntactic;englishjapanese pbmt syntax;highly successful preprocessing;successful preprocessing;pbmt syntax based;phrase based machine;based syntactic;preprocessing;syntactic information;using syntactic information;using syntactic;preprocessing techniques using", "pdf_keywords": ""}, "21d45b4923ad165fbb6612e08d06f9d786f9b4cc": {"ta_keywords": "commonsense knowledge graphs;neural commonsense model;commonsense knowledge graph;neural commonsense;results neural commonsense;teacher model commonsense;model commonsense;train commonsense models;commonsense model surpasses;graphs train commonsense;model commonsense capabilities;commonsense knowledge;commonsense models;commonsense models altogether;commonsense model work;commonsense model;knowledge distillation;authored commonsense knowledge;distill knowledge;models author commonsense;distill knowledge symbolically;commonsense general language;author commonsense knowledge;knowledge graphs train;type commonsense model;distill aspect commonsense;general language models;knowledge distillation hinton;quality causal commonsense;difference distill knowledge", "pdf_keywords": "knowledge graphs commonsense;commonsense knowledge graphs;knowledge commonsense;models commonsense knowledge;symbolic knowledge distillation;authored knowledge commonsense;commonsense knowledge sources;language models commonsense;commonsense knowledge;author commonsense knowledge;graphs train commonsense;graphs commonsense leveraging;knowledge commonsense research;knowledge sources distill;knowledge distillation new;knowledge distillation;knowledge graphs train;commonsense leveraging;commonsense models overall;models author commonsense;automatic knowledge graphs;commonsense models;commonsense general language;train commonsense models;models commonsense;graphs commonsense;commonsense model;causal commonsense gpt;knowledge distillation offers;commonsense research symbolic"}, "53e0abebd9aef5915f72147d3674596a0051748c": {"ta_keywords": "personalized ai services;personalized ai;privacy data protection;privacy data;ai based services;context personalized ai;ai services ai;data protection;services ai;ai services;services ai based;evolving ai services;data resulting privacy;data protection context;personalization better support;protection context personalized;research privacy data;privacy;research privacy;personalization;ai services ii;personalization better;enabling enhanced personalization;edge research privacy;privacy implications;enhanced personalization;enhanced personalization better;user services;personalized;user data", "pdf_keywords": "data protection ai;protection ai services;data protection;protection ai;ai services;ai services survey;share data;advances artificial intelligence;data;artificial intelligence ai;ai;intelligence ai;megabytes new data;intelligence ai key;ai shaped today;ai key;new data;data second;intelligence ai shaped;process share data;artificial intelligence;user services;introduction artificial intelligence;services survey;share data 152;protection;data required new;new data second;ai shaped;ai key driver"}, "b6145cc19acfbec31373446a2dba210cc9b1eb7f": {"ta_keywords": "supervised relation extraction;distantly supervised relation;relation extraction jointly;relation extraction;relation extraction bootstrapping;relation extraction combine;extraction relation extraction;supervised relation;distantly supervised using;approaches distantly supervised;extraction jointly learning;bootstrapping distantly supervised;distantly supervised;performance distantly supervised;concept instance extraction;distantly labeled examples;instance extraction relation;small structured corpora;relation arguments distantly;extraction jointly;structured corpora;supervised using joint;extraction relation;jointly learning;distantly labeled;structured corpora sections;using joint learning;instance extraction;joint learning;extraction bootstrapping distantly", "pdf_keywords": "bootstrap learning;small structured corpus;classic bootstrap learning;structured corpus commonly;patterns large corpus;bootstrap learning scheme;text corpus;structured corpus;ordinary text corpus;large corpus;distant labeling methods;text corpus experimental;corpus used extract;corpus commonly available;earlier bootstrapping methods;large corpus used;bootstrapping methods;labeling methods;corpus commonly;distant labeling;corpus;bootstrapping;labeling methods based;corpus experimental results;bootstrapping methods combining;earlier bootstrapping;corpus used;labeling;extract instances iterative;corpus experimental"}, "181e1d4b08dc62237277a6a743576facd8c5e572": {"ta_keywords": "speaker number estimation;results speaker diarization;speaker diarization;voice activity detection;speaker diarization highly;speaker voice activity;estimate target speaker;overlapped speech target;speech target speaker;diarization applied speaker;highly overlapped speech;target speaker voice;overlapped speech;voice activity;speech target;speaker voice;libricss meeting corpus;meeting corpus;meeting corpus proposed;number speaker;applied speaker number;speaker number;known number speakers;target speaker;real conversations systems;conversations systems;promising results speaker;unknown number speaker;results speaker;corpus", "pdf_keywords": "long form recordings;meeting corpus;libricss meeting corpus;predict number speakers;meeting corpus proposed;corpus;form recordings;recordings;diarization methods;diarization methods including;corpus proposed;recordings unknown number;speakers recording consequently;number speakers recording;speakers recording;corpus proposed approach;recordings unknown;correspond speakers;form recordings unknown;estimating initial vectors;recording;numbers speakers approach;separate diarization;separate diarization model;different diarization methods;diarization model typically;using separate diarization;speakers furthermore fusion;diarization;speakers approach"}, "4236a5f650f5b7ced7512b5072a062b521220b31": {"ta_keywords": "predicting traffic;predicting traffic speed;traffic speed prediction;aim predicting traffic;semantic features traffic;features traffic speed;features traffic;transfer learning framework;transfer learning;traffic speed urban;propose transfer learning;transportation smart city;speed prediction;proposed kernel regression;traffic speed;traffic;intelligent transportation smart;kernel regression;intelligent transportation;applications intelligent transportation;speed prediction benefit;speed urban areas;temporal semantic features;historical data target;models proposed kernel;transportation smart;speed urban;recent supervised machine;areas recent supervised;kernel regression model", "pdf_keywords": ""}, "e25ce2a7b28699e1d57803ef977175937ce50923": {"ta_keywords": "annotation efficient corpus;pronunciation estimation introducing;based pronunciation estimation;pronunciation estimation;annotating particular words;word segmentation;efficient corpus;sentence based annotation;partial annotation efficient;efficient corpus construction;annotated words;corpus based techniques;utilize corpus based;annotation techniques word;word based pronunciation;annotation efficient;segmentation word based;utilize corpus;language processing;corpus based;based partial annotation;natural language processing;corpus;steps word segmentation;corpus construction;word segmentation word;partial annotation;based annotation techniques;number annotated words;annotation techniques", "pdf_keywords": ""}, "0e532d1489d7420cff7ff8aa211ded08e7d57fe9": {"ta_keywords": "strongly convex learning;online strongly convex;convex learning algorithms;convex learning;learning algorithms paper;batch learning occurs;learning algorithm presented;presented learning algorithm;learning algorithm;online learning;example learning algorithm;learning algorithm uses;updated online learning;learning algorithms;batch learning;algorithm learning algorithm;strongly convex;algorithm learning;learning algorithm learning;example learning;generalization ability online;online learning contrast;learning occurs sequence;algorithms paper discuss;learning;algorithms paper;learning occurs;algorithm high probability;contrast batch learning;algorithms", "pdf_keywords": ""}, "1109f787fc8d51feb3bae9bf6e1945dc4a1191e7": {"ta_keywords": "motivate explainable ai;explainable ai studies;improvements explanations ai;explanations ai;ai explains recommendations;explanations ai outperformed;accept ai recommendation;trust ai;trust ai help;tasks improves ai;ai team performance;improves ai explains;explainable ai;human ai team;ai recommendation;ai studies showing;showing human ai;ai recommendation regardless;improves ai;human ai;human centered ai;ai develop explanatory;higher human ai;ai studies;ai outperformed human;solo explanations increased;humans accept ai;appropriate trust ai;human ai working;ai accuracy", "pdf_keywords": "human ai collaboration;ai communities ai;ai collaboration;ai confidence communicate;ai communities;ai collaboration beneficial;communities ai;human centered ai;characterize human ai;trust ai help;human ai;communities ai communities;trust ai;showing ai confidence;ai confidence;hci ai communities;ai develop explanatory;confidence communicate explanations;ai communities highest;simply showing ai;ai;performance having ai;appropriate trust ai;having ai;showing ai;ai develop;ai help generate;communicate explanations increase;communicate explanations;human centered computing"}, "328a9fe143639810d6413c2cc901ec3afa6aa607": {"ta_keywords": "continuum model efficiently;continuum model upscaling;model upscaling molecular;discretized lps models;external loadings discretizations;loadings discretizations;upscaling molecular dynamics;loadings discretizations substantially;data driven peridynamic;surrogate md displacements;peridynamic continuum model;homogenized continuum model;displacements nonlocal models;discretized lps;posedness conditions discretized;md displacements nonlocal;conditions discretized lps;continuum model;discretizations substantially;md displacements;models including peridynamics;driven peridynamic continuum;md data optimal;model allow peridynamic;grained homogenized continuum;discretizations;model upscaling;model surrogate md;linear peridynamic solid;nonlocal models including", "pdf_keywords": ""}, "350e5f5a89cbb3a23442c9d0d3e59fc50d665dbb": {"ta_keywords": "greece electricity market;aspects greece electricity;greece electricity;design electricity market;electricity market;day ahead scheduling;electricity market based;costs resulting dispatching;scheduling;electricity market sketch;model greece day;scheduling das problem;ahead scheduling;units clearing prices;design electricity;scheduling das;ahead scheduling das;research design electricity;dispatching units clearing;greece day ahead;dispatching units;resulting dispatching units;minimum load costs;greece day;generating units load;load costs resulting;load costs;cost components energy;units load frequency;electricity", "pdf_keywords": ""}, "2226f5a13e3e9faac2e228e95175d3e612b52395": {"ta_keywords": "ai acm;acm sigai leadership;ai acm sigai;group ai acm;diversity acm sigai;org acm sigai;sigai acm org;increasing diversity acm;diversity acm;acm sigai consists;acm sigai recently;acm org;chennai acm sigai;internationalization acm sigai;acm sigai;group ai;acm org acm;new acm sigai;acm sigai chapters;org acm;ai conferences 2018;diversity officer;technology chennai acm;acm sigai newsletter;ai;special group ai;usa diversity officer;report acm;ai conferences;acm sigai started", "pdf_keywords": ""}, "c102ac8c779ee0a53dc8e4ee20b4088ac2c7e186": {"ta_keywords": "connectionist temporal classification;attention based encoder;trained lstm language;trained lstm;encoder jointly trained;attention based decoder;decoder encoder deep;speech recognition;encoder deep;classification ctc attention;encoder deep convolutional;temporal classification ctc;ctc attention based;lstm language;lstm language model;lstm;speech recognition asr;separately trained lstm;jointly trained attention;deep convolutional neural;predictions attention based;automatic speech recognition;encoder decoder network;neural network cnn;network cnn;encoder jointly;automatic speech;joint connectionist temporal;cnn based vgg;end automatic speech", "pdf_keywords": "speech recognition encoder;ctcattention decoding rnn;end speech recognition;encoder deep;encoder jointly trained;decoding rnn;attention based decoder;chinese japanese asr;decoding rnn lm;end asr linguistic;jointly trained attention;joint ctcattention decoding;recognition encoder;encoder deep convolutional;japanese asr systems;recognition encoder decoder;chinese speech end;ctc attention end;attention end end;mandarin chinese tasks;attention model;speech recognition;encoder jointly;encoder;encoder decoder connectionist;japanese asr;neural network cnn;decoder connectionist temporal;joint ctc attention;end asr achieves"}, "1144cc3e86b1cc4160aedddb085d7861d4b528dc": {"ta_keywords": "softmax rnn transducer;sampled softmax rnn;softmax rnn;sampled softmax optimize;extend sampled softmax;sampled softmax;softmax optimize memory;rnn transducer promising;rnn transducer;apply sampled softmax;rnn transducer advantages;memory consumption rnn;rnn transducer requires;consumption rnn transducer;softmax;softmax optimize;sampling vocabulary improve;losses sampling vocabulary;speech recognition extend;vocabulary training saves;rnn;ctc losses sampling;speech recognition;consumption rnn;automatic speech;sampling vocabulary;strong accuracy streaming;automatic speech recognition;vocabulary training;subset vocabulary training", "pdf_keywords": "sampled softmax auxiliary;transducer sampled softmax;sampled softmax greatly;sampled softmax optimize;rnntransducer sampled softmax;extend sampled softmax;softmax auxiliary ctc;sampled softmax;softmax optimize memory;speech recognition rnntransducer;extensions sampled softmax;experimentally sampled softmax;sampled softmax example;sampled softmax jaesong;softmax auxiliary;csjaps sampled softmax;sampled softmax gives;softmax greatly;end speech recognition;recognition rnntransducer sampled;sampling vocabulary improve;softmax;rnn transducer sampled;losses sampling vocabulary;softmax optimize;softmax example wise;recognition rnntransducer;training rnn transducer;softmax gives huge;rnn transducer promising"}, "69e9a040ef633c60533843442529cc68c5f12932": {"ta_keywords": "power iteration clustering;scalable graph clustering;graph clustering;graph clustering method;iteration clustering;simple scalable graph;iteration clustering pic;scalable graph;effective cluster;clustering;clustering method;clustering method called;cluster;clustering pic;turns effective cluster;effective cluster indicator;matrix data embedding;similarity matrix data;clustering pic pic;dimensional embedding dataset;cluster indicator;data embedding;power iteration normalized;wise similarity matrix;dimensional embedding;similarity matrix;cluster indicator consistently;spectral methods;graph;embedding dataset", "pdf_keywords": ""}, "3dc4580a154df87f3a56aa3d16b00c5a935ebe15": {"ta_keywords": "citation bias peer;citation reviewer;investigate citation bias;citation bias;peer review citations;bias peer review;study citation bias;does citation reviewer;test citation bias;citation reviewer work;cite prospective reviewers;citation bias actually;review citations;citing reviewer;citing reviewer work;review citations play;peer review analysis;reviewing study citation;size citing reviewer;citations;peer review;prospective reviewers;cite seeing reviewing;reviewer expertise;quality reviewer expertise;citations play important;reviewer expertise apply;score reviewer;evaluation scienti\ufb01c impact;investigate citation", "pdf_keywords": "citation bias peer;citation bias conducted;citation bias;studies citation bias;study citation bias;test citation bias;e\ufb00ect citation bias;citations;biases peer review;bias peer review;study citation;studies citation;citation;observational studies citation;reviewing study citation;citations play important;abstract citations;2022 abstract citations;cite;study test citation;citations play;biases peer;test citation;bias conducted;abstract citations play;cite seeing reviewing;bias peer;evaluation scienti\ufb01c impact;bias conducted \ufb02agship;2021 cite seeing"}, "3cfdec4f1664fcdc20fd5a6d3f86e7b40cf19f70": {"ta_keywords": "length neural encoder;control length encoder;length encoder decoder;output length neural;length encoder;sequence length neural;neural encoder decoder;neural encoder decoders;encoder decoder outputs;neural encoder;encoder decoder models;output sequence length;encoder decoder;encoder decoders;decoder outputs controlling;encoder;controlling output length;controlling output sequence;decoder outputs;length neural;decoder models;decoder models decoding;encoder decoders paper;sequence length;decoder models shown;sequence generation tasks;output length;sequence generation;decoders;decoder", "pdf_keywords": "length degrading summary;summarization task methods;summarization methods;quality summarization task;existing summarization methods;summary quality summarization;sequence length neural;summarization methods examined;length neural encoder;quality summarization;length decoding;compared existing summarization;desired length decoding;sequences decoding based;output sequences decoding;summarization task;summarization;degrading summary quality;output sequence length;sequences decoding;existing summarization;length output sequences;degrading summary;length decoding process;neural encoder decoder;outperform decoding based;outperform decoding;decoder models;generally outperform decoding;length neural"}, "7a070c558cdb9c525559d1ad48159551381750c9": {"ta_keywords": "knowledge gram machines;large search space;gram machines;large search;million sentences specifically;encoded knowledge learning;10 million sentences;million sentences;encode knowledge symbolically;gram machines tackle;organize knowledge gram;exponentially large search;gram machine ngm;symbolic meaning representations;gram machine;knowledge gram;knowledge symbolically generate;sequence models encode;encode knowledge;called gram machine;meaning representations indexed;efficiently ngm trained;models encode knowledge;encoded knowledge;search space;refine search space;questions encoded knowledge;gram;knowledge learning;approach called gram", "pdf_keywords": "answering questions large;large corpus text;trainable ef\ufb01ciently answering;gram machines scalable;questions large corpus;contains millions sentences;deep qa models;text understanding models;scales large corpus;millions sentences;large corpus;millions sentences method;end deep qa;gram machines;deep qa;neural network text;questions large;text symbolic representations;large corpus experimenting;natural language text;text understanding;ef\ufb01ciently answering questions;syntactic natural language;question complexity responding;content structured;symbolic representations memory;answering questions;memory;inference syntactic natural;natural language"}, "6516b800482100731f0eb348f678ad30799c839f": {"ta_keywords": "neologisms semantic;analysis neologisms semantic;distributional semantic;words born distributional;distributional semantics study;born distributional semantic;semantic analysis neologisms;distributional semantics paradigm;distributional semantics;neologisms semantic neighborhoods;distributional semantic analysis;words emerge language;new linguistic;semantic sparsity frequency;semantic sparsity;linguistic application distributional;factors semantic sparsity;diachronic corpora english;neologisms;growth rates semantic;analysis neologisms;words emerge;phenomenon neology;formalized distributional semantics;phenomenon neology process;new words emerge;linguistic;process new words;semantic neighbors;semantic", "pdf_keywords": "emergence neologisms area;emergence neologisms;word emergence;predictive word emergence;word emergence \ufb01nd;correlated emergence neologisms;neologisms area;words likely emerge;neologisms;distributional semantics paradigm;distributional semantics;neologisms area factors;words semantic area;neology assumes growing;semantic neighborhoods new;semantic neighborhood;using distributional semantics;words semantic;distributional semantics lenci;growing frequency words;semantic neighborhood sparsity;factors semantic neighborhood;semantic neighbors;frequency words semantic;formalized distributional semantics;semantic neighborhoods;linguistic;determining semantic neighborhoods;growth rates semantic;semantic area"}, "ac03cf22e2a831ab030ae33b5ddf5f9864917a17": {"ta_keywords": "growth attendance aaai;attendance aaai;attendance aaai years;recruiters students aaai;event 2018 aaai;students aaai acm;2018 aaai acm;attendance;growth attendance;students aaai;event recruiters students;steady growth attendance;popular event recruiters;fair 2018 retrospective;companies attended typically;participating job fair;aaai acm sigai;companies attended;acm sigai job;event recruiters;aaai acm;job fair year;2018 aaai;attended typically;companies students actively;aaai years seen;aaai years;fair year companies;year companies attended;sigai job fair", "pdf_keywords": ""}, "0a7f95adbaf0e46c93b5f82c74a26f5874c861ac": {"ta_keywords": "hybrid model ramp;model ramp metering;isolated traffic ramp;ramp metering robust;model based godunov;based godunov numerical;godunov based hybrid;model isolated traffic;godunov approximation based;godunov approximation;godunov numerical;ramp metering problem;godunov numerical scheme;traffic ramp metering;model ramp;analysis godunov approximation;based dynamics model;controllers robust uncertainties;dynamics model isolated;based godunov;approximation based dynamics;ramp metering;traffic ramp;godunov based;controllers robust;analysis godunov based;based hybrid model;isolated traffic;dynamics model;problem analysis godunov", "pdf_keywords": ""}, "c5e4eafd85949e6aac9d8e98d5e03b2acf444046": {"ta_keywords": "questions adversarially model;questions adversarially;adversarial datasets worse;compose questions adversarially;better adversarial datasets;adversarial datasets;adversarial data usually;perform better adversarial;trained adversarial data;adversarial data collection;models trained adversarial;trained adversarial;adversarially;adversarially model;model adversarial data;adversarial;better adversarial;adversarial data;model adversarial;brittle efficacy adversarial;efficacy adversarial data;efficacy adversarial;trained challenging datasets;challenging datasets rely;models trained challenging;adversarially model loop;question answering results;question answering;question answering assigning;collection question answering", "pdf_keywords": "analysis adversarial;analysis adversarial vs;qualitative analysis adversarial;adversarial data collection;adversarial data;adversarial vs;widely ef\ufb01cacy adversarial;adversarial;standard non adversarial;2021 abstract adversarial;non adversarial;adversarial vs standard;ef\ufb01cacy adversarial data;abstract adversarial data;ef\ufb01cacy adversarial;abstract adversarial;2018 question answering;adversarial setting;language inference nli;question answering results;non adversarial setting;nlp tasks natural;question answering;collection question answering;natural language inference;tasks natural language;adversarial setting model;diverse natural language;inference nli;question answering qa"}, "575ac3f36e9fddeb258e2f639e26a6a7ec35160a": {"ta_keywords": "intermediate parsing training;target language parsing;parsing semantic lu;parsing training;supervised parsing high;dependencies treebanks transformer;intermediate parsing;effect intermediate parsing;supervised parsing semantic;availing supervised parsing;language parsing explicit;language parsing;supervised parsing;parsing semantic;treebanks transformer;usefulness supervised parsing;treebanks transformer fine;transformer biaffine parsing;parsing high level;universal dependencies treebanks;spaces intermediate parsing;dependencies treebanks;parsing;intermediate parsing make;parsing explicit;parsing explicit formalized;parsing training ipt;high level semantic;self supervised language;semantic lu", "pdf_keywords": "downstream language understanding;languages downstream;downstream language;language understanding tasks;leveraging formalized syntactic;target languages downstream;downstream target languages;supervised syntactic parsing;languages downstream \ufb01ne;treebanks downstream;treebanks downstream target;neural language models;monolingual language transfer;2020 treebanks downstream;treebank downstream;language understanding;syntactic parsing bene\ufb01cial;parsing bene\ufb01cial language;syntactic parsing;semantic language understanding;supervised syntactic;syntactic parsing necessary;supervised parsing semantic;theart neural language;parsing semantic lu;parsing semantic;tasks monolingual language;language models;bene\ufb01cial language understanding;supervised parsing"}, "b437cc7c0ae672b188df078b5dd80f97e8dde978": {"ta_keywords": "unsupervised learning lexical;lexical units;machine translation unsupervised;lexical units used;translation unsupervised learning;lexical learning speech;lexical learning;question lexical units;learning lexical;models lexical learning;lexical information;learning lexical information;lexical information language;language processing;translation unsupervised;recognize speech translate;recognition machine translation;machine translation conventionally;languages human annotations;models lexical;lexical;machine translation;speech translate text;natural language processing;speech translate;presents models lexical;information language processing;language processing systems;question lexical;word obvious languages", "pdf_keywords": ""}, "69ba64b20d0a1849ef08d63c39bfafbaac909087": {"ta_keywords": "behavior constrained thompson;learns constrained policy;bandits learn ethical;agent learns constrained;constrained thompson sampling;online exploration exploitation;online agent learns;bandit setting provide;priorities online ai;online rewards guiding;bandit setting;bandits learn;online learning obeying;learn reward;online ai;multi armed bandits;constrained thompson;reward feedback ai;online ai systems;thompson sampling;multi armed bandit;agent learns;armed bandits learn;bandit;learns constrained;principles agent learns;agent learns set;agent uses constrained;armed bandit setting;online rewards", "pdf_keywords": ""}, "40bbd3046f1fa86a50e526b3848b4f2bd3a1d873": {"ta_keywords": "li o2 batteries;li o2 battery;designed soluble lithium;lithium salt;soluble lithium salt;ln based electrolyte;lithium salt preparing;batteries induced li;soluble lithium;capacity li o2;high capacity li;o2 batteries;protecting li anode;lithium;li anode;conventional electrolytes;o2 battery ln;li o2;soluble perfluorinated polyelectrolyte;perfluorinated polyelectrolyte;based electrolyte;based electrolyte achieves;perfluorinated polyelectrolyte safe;o2 batteries severe;o2 battery;flammable polyelectrolyte solution;flammable polyelectrolyte;li anode dendrite;electrolyte achieves;conventional electrolytes hinder", "pdf_keywords": ""}, "059f515bf53bcddeca031fd4a4071c911999a3c6": {"ta_keywords": "supervised apparel invariant;apparel invariant pedestrian;apparel invariant feature;learn apparel invariant;apparel simulation gan;invariant pedestrian representation;semi supervised apparel;pedestrian representation;gan synthesize cloth;pedestrian representation using;unsupervised apparel;apparel invariant;supervised apparel;invariant feature learning;unsupervised apparel simulation;wearing different clothes;different clothes;invariant pedestrian;images person wearing;clothes worth noting;cloth changing images;clothes;images person;identification reid performance;person identification reid;target cloth embedding;clothes propose unsupervised;cloth embedding;invariant feature representation;learn apparel", "pdf_keywords": "supervised apparel invariant;apparel invariant feature;apparel invariant pedestrian;learn apparel invariant;semi supervised apparel;models apparel invariant;discriminative feature embedding;apparel invariant;supervised apparel;learn discriminative feature;clothes learn apparel;clothes learn;discriminative feature;invariant feature learning;invariant pedestrian representation;feature embedding;different clothes learn;pedestrian representation using;pedestrian representation;learn apparel;baseline models apparel;invariant feature learned;invariant feature embeeding;wearing different clothes;feature embeeding;different clothes;models apparel;framework learn apparel;wearing similar cloth;apparel changed reid"}, "c96363c42bc8c465902c22b8c33c8704233f519e": {"ta_keywords": "languages code generation;benchmark code generation;code natural language;adapting code generation;code generation natural;languages code;generation code summarization;code pairs languages;generation new languages;natural programming languages;code generation;code generation systems;multiple natural languages;natural languages;language commands;code summarization applications;language challenge;programming languages code;generation natural language;code generation new;natural language commands;code generation code;language commands extending;code generation multiple;languages recent burgeoning;new languages;language challenge conala;challenges adapting code;multilingual dataset;theart code generation", "pdf_keywords": "multilingual code generation;languages code generation;task multilingual code;multilingual code;multilingual understanding code;multilingual encoder mconala;testbed languagecomprehensive approaches;pretrained multilingual encoder;models pretrained multilingual;multiple natural languages;nl code models;multilingual encoder;solid testbed languagecomprehensive;benchmark code generation;languages code;testbed languagecomprehensive;pretrained multilingual;converting natural language;language machine;task multilingual;natural languages;natural languages english;languagecomprehensive approaches;natural programming languages;models multilingual understanding;code synthesis;code generation;code generation multiple;multilingual;generation code summarization"}, "70a321f12a655e305781e2de0ca9617d96e462c3": {"ta_keywords": "data aggregators competition;aggregators competition users;aggregators competition;competition users data;aggregators game;aggregators game game;data aggregators;competition arises aggregators;estimation strategic data;central data aggregator;interactions data market;data aggregator;strategic data;statistical estimation strategic;data market;aggregators;strategic data sources;data aggregator design;data sources competitive;data market formulate;estimates aggregator;nash equilibrium social;multiple data aggregators;aggregator;reporting estimates aggregator;market formulate competition;arises aggregators game;aggregator design;nash equilibrium;competition users", "pdf_keywords": "incentives strategic data;competition data buyers;data market;handle competition data;data market demonstrate;buyers data market;strategic data sources;strategic data;data buyers;data buyers \ufb01rst;marketplace formulating game;data sources strategic;game buyers compete;incentives strategic;incentive mechanisms effectively;incentive mechanisms;formulating game buyers;competition data;issue incentives strategic;buyers issue incentives;game buyers;incentives;pricing mechanisms;designing pricing mechanisms;buyers data;model strategic data;issue incentives;design incentive mechanisms;compete designing pricing;incentive"}, "bee52c51cbd37d0e48c3ea5f71a08f177d2aff73": {"ta_keywords": "classification grapheme phoneme;binary classification grapheme;classification grapheme;adaptive regularization weight;grapheme phoneme conversion;regularization weight vectors;approach grapheme phoneme;online discriminative training;regularization weight;based adaptive regularization;discriminative training;noisy adaptive regularization;grapheme phoneme;adaptive regularization;multiclass classification evaluation;phoneme conversion based;discriminative training method;multiclass classification;binary classification;grapheme phoneme g2p;phoneme conversion;regularization;method multiclass classification;online discriminative;approach grapheme;problem binary classification;classification evaluation;training method multiclass;efficient training;classification", "pdf_keywords": ""}, "7129b62be18487db5e9602e353bb10a4c79a9b92": {"ta_keywords": "stripped executables diverse;engineering stripped executables;names stripped executables;available stripped executables;stripped executables;stripped executables contain;neural reverse engineering;executables contain debug;reverse engineering stripped;executables diverse assembly;engineering stripped binaries;stripped binaries using;executables;executables contain;stripped executables address;reverse engineering;debug information neural;stripped binaries;executables diverse;compiler;patterns arising compiler;procedure names stripped;compiler optimizations;assembly code patterns;problem reverse engineering;debug information;code patterns arising;contain debug information;arising compiler optimizations;executables address", "pdf_keywords": ""}, "f4cf4246f3882aa6337e9c05d5675a3b8463a32e": {"ta_keywords": "vision language tasks;embodied vision language;vision language task;grounded visual language;existing vision language;vision language;language existing vision;visual language understanding;visual language;mapping natural language;embodied vision;vision sequences actions;action learning realistic;language instructions egocentric;language tasks;alfred action learning;long compositional tasks;compositional tasks;recent embodied vision;instructions egocentric vision;action learning;language tasks performs;language task datasets;action space language;learning mapping natural;based recent embodied;learning realistic environments;instructions egocentric;egocentric vision sequences;learning mapping", "pdf_keywords": "vision language tasks;language driven robots;embodied vision language;vision language;grounded visual language;language instructions egocentric;visual language understanding;instructions egocentric vision;visual language;embodied vision;instructions egocentric;agents simulation robots;robots;recent embodied vision;map natural language;language driven;robots operating real;egocentric vision sequences;vision sequences actions;robots capable;simulation robots;language tasks;language tasks performs;robots capable navigation;driven robots;goal language driven;language understanding models;egocentric vision;based recent embodied;learning map natural"}, "8f11643b42976433fc3a2ec19feef486929527a1": {"ta_keywords": "argument mining;workshop argument mining;5th workshop argument;argument;workshop argument;proceedings 5th workshop;proceedings 5th;proceedings;5th workshop;workshop;mining;5th", "pdf_keywords": ""}, "d9e56aa9f69e18c9d37799b86b50d36709cbf711": {"ta_keywords": "babbitt alison bailey;bailey cathryn bailey;bailey cathryn;battersby nancy bauer;bailey celia bardwell;cathryn bailey;cathryn bailey celia;alison bailey cathryn;susan babbitt alison;bailey celia;babbitt alison;battersby nancy;susan babbitt;alison bailey;2010 ruth abbey;anderson barbara andolsen;rosalyn diprose;bailey;marilea bramer alison;barbara andolsen;nancy bauer;barbara andolsen barbara;babbitt;christine battersby nancy;susan dieleman david;joanne;michelle bastian;bastian christine battersby;anderson barbara;ruth abbey", "pdf_keywords": ""}, "59a228f48a83eb0905391f7e454fde0eeb6680ee": {"ta_keywords": "training phoneme lattices;lm automatic speech;speech recognition;speech recognition asr;phoneme lattices using;automatic speech recognition;automatic speech;learn language model;acoustic model linguistic;generate phoneme lattices;learning lexical;phoneme lattices;speech implementation;training phoneme;continuous speech able;language model lm;learning lexical units;phoneme lattices simultaneously;continuous speech implementation;simultaneously learning lexical;lattices using acoustic;finite state transducers;using continuous speech;lattices simultaneously learning;learned vocabulary expressive;gibbs sampling proposed;speech demonstrate lms;language model;perform training phoneme;gibbs sampling", "pdf_keywords": ""}, "8b7a8f9a27b8dc73a5b0b62ada14bbab047084fc": {"ta_keywords": "silent speech enhancement;implement speech enhancement;speech enhancement digital;speech enhancement based;silent speech interface;voice conversion silent;speech enhancement systems;conversion silent speech;voice conversion;speech enhancement;statistical voice conversion;voice conversion vc;vc silent speech;implement speech;speech interface;electrolaryngeal speech enhancement;enhancement electrolaryngeal speech;speech enhancement electrolaryngeal;conversion vc silent;conversion silent;audible speech;statistical voice;silent electrolaryngeal speech;silent speech;nonaudible murmur;voice electrolaryngeal speech;whispered voice electrolaryngeal;speech natural voice;focus nonaudible murmur;speech interface focus", "pdf_keywords": ""}, "4100256a125d7b56cac693a436bba2b00fae3fa3": {"ta_keywords": "automated audio captioning;audio captioning;audio captioning overcome;captioned audio samples;training incorporate audioset;captioned audio;incorporate audioset;pretrained audio neural;model audio samples;task automated audio;audiocaps datasets;audio neural;audio neural networks;incorporate audioset tags;audio embeddings;automated audio;audioset tags audio;audioset;clotho audiocaps datasets;tags audio embeddings;audioset tags;audio samples model;audio embeddings obtained;availability captioned audio;model audio;language descriptions acoustic;pretrained audio;audiocaps datasets test;train model audio;audiocaps", "pdf_keywords": ""}, "306c59458cebb35c2d520dd129f09d5c6cc2985f": {"ta_keywords": "erratum paraphrase database;erratum paraphrase;paraphrase database compositional;compositional paraphrase model;database compositional paraphrase;paraphrase database;compositional paraphrase;paraphrase model;paraphrase model correction;paraphrase;erratum;authors acknowledgements;compositional;correction list authors;database compositional;correction list;acknowledgements;list authors acknowledgements;authors;list authors;correction;model correction list;list;model;model correction;database", "pdf_keywords": "models paraphrase tasks;paraphrase models score;paraphrase models;parametric paraphrase models;paraphrase tasks;paraphrase tasks introduced;models short paraphrases;compositional models paraphrase;paraphrase pairs accurately;build parametric paraphrase;paraphrases \ufb01lling;paraphrase pairs;models paraphrase;score paraphrase pairs;models score paraphrase;short paraphrases \ufb01lling;word embeddings;paraphrases;paraphrases \ufb01lling gap;phrase pairs ppdb;word embeddings compositional;short paraphrases;parametric paraphrase;directly phrase pairs;paraphrase;score paraphrase;phrase pairs;gap nlp community;gap nlp;\ufb01lling gap nlp"}, "bba9b93ab8d9b98cd54001a5ba9673e513a35219": {"ta_keywords": "lstms modeling medical;effectiveness simple lstm;lstm;networks rnns particularly;recurrent neural networks;data recurrent neural;lstms recognize patterns;lstms modeling;networks rnns;simple lstm;short term memory;lstms;learning sequence data;memory lstm;term memory lstm;usefulness lstms;rnns particularly using;neural networks rnns;rnns;rnns particularly;establishing usefulness lstms;data recurrent;lstm network;lstms recognize;lstm network modeling;usefulness lstms modeling;recurrent neural;simple lstm network;ability lstms recognize;missing data recurrent", "pdf_keywords": ""}, "cc352ea39f820c3effc40ce09369cf59afe361df": {"ta_keywords": "cloud based health;health systems azure;electronic health health;electronic health;ict systems health;health health technology;cloud services;cloud computing environment;health technology;cloud computing;virtualized cloud;cloud based;virtualized cloud computing;health systems designed;developed cloud based;health systems;cloud end developed;implementation health systems;traditionally health systems;based health applications;migrate ict systems;health technology brought;systems azure cloud;telemedicine;cloud;architecture virtualized cloud;systems health sector;demonstrating cloud services;ict based systems;azure cloud", "pdf_keywords": "cloud based medical;cloud based telemedicine;cloud based health;health systems azure;electronic health;electronic health health;ict systems health;health applications deployments;cloud platform;applications cloud based;cloud services;practical applications cloud;health technology;health health technology;migrate ict systems;cloud computing environment;cloud computing;cloud computing applied;implementations health applications;migrating health applications;based telemedicine;cloud computing platform;cloud based;abstract electronic health;approach migrate ict;applications cloud;ict based systems;developed cloud based;cloud platform study;health systems designed"}, "589e651c69251ee20a89e075d015eb03b35cf17d": {"ta_keywords": "speech encoder;shared speech encoder;nar decoding e2e;speech translation st;speech encoder used;nar decoding;speech translation;competitive translation quality;translation quality compared;translation quality;deployment speech translation;decoders jointly trained;selecting better translation;translation st systems;encoder decoder architecture;maintaining competitive translation;fast inference speed;autoregressive ar decoders;text based translation;improving inference speed;decoding speed;encoder decoder;decoders jointly;decoding e2e st;inference speed maintaining;ar decoders jointly;encoder;trained shared speech;problem nar decoding;decoding e2e", "pdf_keywords": "speech encoder nar;nar decoding framework;nar decoding;nar decoding e2e;uni\ufb01ed nar decoding;nar decoder;shared speech encoder;speech encoder;encoder best nar;nar ar decoders;encoder nar;decoders shared speech;encoder nar ar;problem nar decoding;beam nar decoder;decoders jointly trained;autoregressive ar decoders;speech encoder orthros;ar decoders jointly;decoders shared encoder;decoders jointly;dual decoders;dual decoders shared;shared encoder best;decoder greatly improved;encoder best;best nar e2e;autoregressive nar;candidate ar decoder;autoregressive nar methods"}, "d5123ab81f511027cbe11dc92d99e116fd193158": {"ta_keywords": "sensing energy harvesting;energy harvesting;remote sensing energy;single energy harvesting;energy harvesting eh;harvesting source node;sensing energy;energy harvesting source;aoi energy harvesting;energy decides sample;channel state optimal;information remote sensing;state optimal source;harvesting source;markov decision process;harvesting eh source;node sampling policy;sensing setting considered;information aoi energy;time varying wireless;stored energy decides;sensing setting;available energy buffer;source node sampling;markov decision;optimal source node;harvesting;remote sensing;remote sensing setting;energy buffer instantaneous", "pdf_keywords": "aoi energy harvesting;sensing energy harvesting;energy harvesting;energy harvesting eh;single energy harvesting;scheduling transmit power;remote sensing energy;state optimal source;minimization time averaged;expected aoi energy;energy harvesting source;minimizing time averaged;aoi minimization wirelessly;transmit power selection;sampling transmission scheduling;considered aoi minimization;sensing energy;minimization wirelessly powered;harvesting source node;aoi minimization;aoi single energy;energy harvesting akanksha;aoi energy;minimization time;channel state optimal;aoi minimization problem;power selection;optimal source node;transmit energy formulate;available energy"}, "ddfd297531f56121b8383bd1eb2bb09189ab2e2b": {"ta_keywords": "emphasis transfer speech;transferring emphasis speech;language emphasis prediction;emphasis speech translation;transfer speech translation;emphasis translation task;speech translation systems;emphasis transfer;languages transferring emphasis;focus speech translation;model emphasis transfer;transferring emphasis;emphasis prediction;attentional neural network;emphasis prediction measure;target language emphasis;transfer speech;speech translation using;transfer linguistic content;hard attentional neural;translation systems transfer;speech translation;translation systems;emphasis speech;emphasis translation;term memory lstm;using hard attentional;attentional neural;emphasis information languages;transfer linguistic", "pdf_keywords": ""}, "f0a498014c4ef67c0b72ceb18d95e0d25087fd57": {"ta_keywords": "neural machine translation;translation systems neural;machine translation binary;bidirectional translation tasks;translation binary;machine translation;translation binary code;machine translation systems;translation tasks;translation tasks proposed;translation systems;10 improving decoding;japanese bidirectional translation;softmax;bidirectional translation;softmax reducing memory;neural machine;predicting binary code;improving decoding speed;improving decoding;binary code prediction;approach softmax;english japanese bidirectional;output layer neural;layer neural machine;binary code word;approach softmax reducing;softmax reducing;predicting binary;japanese bidirectional", "pdf_keywords": "neural machine translation;softmax binary codes;output words binary;softmax binary;words binary codes;binary code prediction;predicting binary code;using softmax binary;machine translation models;binary code word;codes viterbi decoding;binary codes model;viterbi decoding;machine translation;predicting binary;combining softmax binary;words binary;codes combining softmax;robustness binary code;binary codes;viterbi decoding viterbi;binary code;binary codes introducing;predict output words;translation models;convolutional error correcting;decoding viterbi;correcting codes viterbi;binary representation;binary codes method"}, "88e2beccbc89b3e3dd793e2502b17c1fa551151d": {"ta_keywords": "node repair bandwidth;data repair bandwidth;repair bandwidth;distributed storage;repair bandwidth explicit;distributed storage setting;point distributed storage;storage node repair;tradeoff storage node;repair bandwidth d\u03b2;storage node;codes minimum bandwidth;bandwidth point distributed;tradeoff storage;nodes additionally repair;bandwidth d\u03b2;regenerating codes minimum;reduced node stores;stored nodes network;node repair;distributed;minimum bandwidth;bandwidth d\u03b2 considerably;bandwidth explicit optimal;characterize tradeoff storage;stored nodes;storage;nodes downloading;storage setting consider;data repair", "pdf_keywords": ""}, "f4c8539bed600c9c652aba76a996b8188761d3fe": {"ta_keywords": "neural machine translation;machine translation grown;machine translation;vanilla nmt implementations;nmt implementations work;nmt implementations;nmt models addressed;nmt models;basic nmt;basic nmt models;translation grown rapidly;nmt;neural machine;weaknesses basic nmt;vanilla nmt;demonstrated language data;neural;gains vanilla nmt;language data scenarios;language data;baseline chosen neural;translation grown;chosen neural machine;models addressed new;effectiveness demonstrated language;shared task systems;translation;models addressed;algorithmic improvements lead;task systems reporting", "pdf_keywords": "effectiveness adam training;ensembling improving;model ensembling improving;ensembling improving bleu;adam training multiple;adam training;model ensembling;nmt systems empirically;improving vanilla nmt;ensembling combining techniques;encoding decoding ensembles;demonstrated effectiveness adam;strengthening nmt systems;model ensembling combining;effectiveness adam;techniques strengthening nmt;independent model ensembling;strengthening nmt;using adam;technique adam multiple;decoding ensembles;trained models;using adam multiple;ensembling;nmt systems;ensembling combining;nmt data;vanilla nmt data;learning rate annealing;pair encoding decoding"}, "3e3254bce9c321310d2e9825ed52b30da9879173": {"ta_keywords": "speech segment boa;utterance classification experiments;utterance classification;speaker adaptation;boa speech segments;applications utterance classification;speaker adaptation experiments;experiments speaker adaptation;word features decoder;networks speech segment;processing applications utterance;classification experiments speaker;wfst networks speech;speech segments;speech segment;arcs boa speech;based asr decoder;features decoder outputs;asr decoder;boa speech;features decoder;change decoding algorithms;applications utterance;overhead change decoding;networks speech;wfst based asr;combine asr decoder;asr decoder include;asr decoder consequently;proposed approach asr", "pdf_keywords": ""}, "285c50d98dab741a82649b1abcaca8273cb8f253": {"ta_keywords": "classification grapheme phoneme;binary classification grapheme;classification grapheme;grapheme phoneme conversion;regularization weight vectors;adaptive regularization weight;online discriminative training;discriminative training;conversion structured learning;regularization weight;noisy adaptive regularization;based adaptive regularization;adaptive regularization;discriminative training method;approach grapheme phoneme;phoneme conversion based;phoneme conversion;grapheme phoneme;regularization;grapheme phoneme g2p;structured learning;binary classification;online discriminative;multiclass classification evaluation;multiclass classification;structured learning problem;structured learning based;learning based margin;approach grapheme;phoneme g2p conversion", "pdf_keywords": ""}, "55bdc4ad158e272ccf796ae52b0ab7086a834352": {"ta_keywords": "automated tutoring;automated tutoring systems;affects automated tutoring;tutoring systems;instruction student model;discovers student models;tutoring;tutoring systems making;student modeling;student models;student models using;learning agent;automatically discovers student;student model;student modeling key;implications student modeling;student model model;systems making instructional;good student model;predict probability student;student behavior patterns;learning task difficulty;machine learning agent;learning agent simstudent;learning related problems;making instructional;making instructional decisions;discovers student;learning task;instruction student", "pdf_keywords": ""}, "c3177616ad35ef7850ea1e62da1fa3be36943e8b": {"ta_keywords": "dialog retrieval distributed;neural network paraphrase;dialog retrieval;based dialog retrieval;distributed word representations;network paraphrase identification;paraphrase identification example;paraphrase identification;word representations;retrieval distributed representations;word representations employ;query distributed word;example based dialog;pooling determine sentences;dialog pair database;vocabulary oov database;sentences arbitrary length;matching model dialog;handling vocabulary oov;dialog;employ recursive autoencoders;distributed word;recursive neural network;based dialog;recursive autoencoders;retrieval distributed;recursive neural;recursive autoencoders dynamic;network paraphrase;dialog pair", "pdf_keywords": ""}, "49a049dc85e2380dde80501a984878341dd8efdf": {"ta_keywords": "supervised learning speech;learning speech representations;representations speech audio;speech representations experiments;labeled data wav2vec;speech representations;learning speech;hour wav2vec outperforms;data wav2vec;simpler wav2vec framework;wav2vec outperforms;simpler wav2vec;speech audio;transcribed speech outperform;data hour wav2vec;hour wav2vec;representations speech;wav2vec;conceptually simpler wav2vec;wav2vec masks speech;speech outperform;wav2vec outperforms previous;wav2vec framework self;powerful representations speech;wav2vec framework;best semi supervised;speech outperform best;self supervised learning;tuning transcribed speech;semi supervised", "pdf_keywords": "representations speech audio;latent speech representations;learning speech representations;speech representations masks;speech representations;quantized speech representations;representations raw audio;speech representations arxiv;representations speech;task quantized speech;speech audio;powerful representations speech;encodes speech audio;speech representations ablate;speech representations 26;learning speech;encodes speech;supervised learning speech;masks latent representations;speech audio followed;approach encodes speech;latent speech;masked language modeling;resulting latent speech;representations model trained;contextualized representations model;latent representations raw;representations masks latent;learning representations raw;trained contrastive task"}, "5f1bbc96a22a630d3662b3fceb3160091e4bd814": {"ta_keywords": "voice activity detection;robust voice activity;noise robust voice;noise gaussian mixture;observed noisy speech;voice activity;gmms voice activity;noisy speech;robust voice;gaussian mixture model;normalizing gaussian weights;noise gmms voice;noisy speech signals;gaussian mixture;normalization noise robust;weight normalization noise;sequentially estimated noise;frame normalizing gaussian;based gaussian pruning;gaussian pruning weight;normalizing gaussian;noise gaussian;estimated noise gmms;gaussian weights;gaussian pruning;gmms voice;normalization noise;speech signals;gaussian weights remaining;speech signals composing", "pdf_keywords": ""}, "bf0105bdd5b0dfc09580697739fb84590d031d0b": {"ta_keywords": "cognitive tutor simstudent;cognitive tutor;cognitive tutor authoring;tutor simstudent machine;simulated student using;tutor simstudent;simulated student;algebra cognitive tutor;students cognitive model;simstudent genuine learning;generate cognitive model;human students cognitive;learns cognitive;build cognitive model;learns cognitive skills;students cognitive;agent learns cognitive;automatically generate cognitive;cognitive model generated;evaluating simulated student;block cognitive tutor;generate cognitive;experiments algebra cognitive;simstudent explained;cognitive skills demonstration;cognitive skills;simstudent machine learning;students applied simstudent;modeling real students;learning agent", "pdf_keywords": ""}, "8ec127925a8680928d546df7248963e772e07a5d": {"ta_keywords": "screening models employers;tests candidate employer;analyze optimal employer;optimal employer policy;policy assigning tests;assigning tests adaptively;efficient candidate screening;tests adaptively;optimal employer;tests implications fairness;tests adaptively based;candidates employers;job candidates employers;candidate screening;candidates employers rarely;employers rarely observe;tests subject candidates;tests candidate;candidate employer set;models employers;screening multiple tests;candidate screening multiple;employer set dynamic;policy employer sets;assigning tests;outcomes groups recruiting;observe underlying skill;estimate worker skill;screening models;groups recruiting job", "pdf_keywords": "tests hire policies;assigning tests adaptively;policy assigning tests;tests implications fairness;tests adaptively;testing candidates;testing candidates appear;adaptively allocated tests;tests adaptively based;continue testing candidates;tests candidate employer;screening multiple tests;employer decide test;tests hire;multiple tests;multiple tests implications;tests decision rule;assigning tests;candidate screening multiple;tests candidate;tests decision;tests subject candidates;tests recall dynamic;number tests hire;candidate screening;optimal employer policy;allocated tests;groups analyze optimal;tests recall;analyze optimal employer"}, "a34954d9e36ea6c57743f55124a6ae444b951c2c": {"ta_keywords": "explaining deep neural;activations training points;explain predictions deep;activation prediction;prediction representer point;activation prediction neural;training point learned;pre activation prediction;selection explaining deep;deep neural;point prediction representer;predictions deep neural;prediction representer;deeper understanding network;representer points training;point learned;training points negative;neural;predictions deep;explaining deep;neural network pointing;training points weights;point learned parameters;learned parameters network;importance training point;simply training point;training point influence;training points;neural networks;prediction neural", "pdf_keywords": "activation prediction neural;activation prediction;decompose prediction activation;weights corresponding representer;prediction activation;deep neural;prediction activation value;activation prediction values;learned parameters network;pre activation prediction;activations weights corresponding;deep neural networks;training point activations;generalized deep;linearly decompose prediction;activations weights;deep neural network;learned parameters;activations training points;say deep neural;generalized deep neural;point activations weights;prediction neural;learned parameter model;neural network linear;neural network makes;learned parameter;linear combination activations;representer theorem;understanding say deep"}, "ce4eadb324026191c075f1af876403a847329d5b": {"ta_keywords": "set valued features;nominal features;featurevector representation allows;valued features;dog nominal features;categorization;feature vectors;categorization problems;feature set strings;tree rule learning;valued features argue;rule learning;text categorization problems;feature vectors components;rule learning algorithms;categorization problems problems;value feature set;valued features particular;featurevector representation;text categorization;extended setvalued features;features argue decision;learning algorithms easily;decision tree;length feature vectors;value feature;setvalued features;extension featurevector representation;learning systems;feature vector", "pdf_keywords": ""}, "63bc09c11a792abfcbb2d9e2809aa67929f09262": {"ta_keywords": "learning hyponym hypernym;learning word subsumption;semantic relations hypernymy;word embeddings inducing;embeddings inducing relations;distributional semantics;inducing relations words;word embeddings;learning hyponym;semantic relations;projections russian language;popularisation distributional semantics;word subsumption projections;applying word embeddings;distributional semantics significant;embeddings inducing;semantics significant attention;relations hypernymy hyponymy;semantic;learning word;semantics;hypernymy hyponymy widely;framework semantic relations;subsumption projections russian;hyponym hypernym;hypernymy hyponymy;experiments russian language;relations hypernymy;hyponym hypernym projections;natural language processing", "pdf_keywords": ""}, "cfb1b39d1a6733f42cc5e8cfd60dc68cafa01d28": {"ta_keywords": "learning adequate multimodal;learning multi modal;lingual learning multi;multimodal representations;modal machine learning;adequate multimodal representations;multi lingual learning;deep learning language;adequate multimodal;lingual learning;multimodal;learning language;learning language statistics;natural language;coursework algorithms nlp;learning multi;algorithms nlp;natural language processing;area natural language;multi lingual;algorithms nlp introduction;language statistics directed;tasks multi lingual;machine translation deep;nlp introduction machine;nlp introduction;cross modal content;multi modal machine;modal content based;language processing", "pdf_keywords": ""}, "2b110fce160468eb179b6c43ea27e098757a56dd": {"ta_keywords": "controlled paraphrase networks;trained produce paraphrase;paraphrase networks;generate paraphrases;paraphrase networks scpns;generate paraphrases follow;syntactically controlled paraphrase;scpns generate paraphrases;uncontrolled paraphrase systems;adversarial example generation;generate adversarial examples;produce paraphrase;paraphrase networks given;paraphrase systems;paraphrase quality;controlled paraphrase;produce paraphrase sentence;baseline uncontrolled paraphrase;decreasing paraphrase quality;generate adversarial;paraphrase quality compared;generation syntactically controlled;example generation syntactically;uncontrolled paraphrase;use generate adversarial;paraphrases;paraphrases follow target;generation syntactically;adversarial examples;adversarial example", "pdf_keywords": "controlled paraphrase generation;uncontrolled paraphrase generation;paraphrase generation effective;paraphrase generation systems;paraphrase generation;generated paraphrases;parse produce paraphrase;syntactically controlled paraphrase;evaluations generated paraphrases;generated paraphrases follow;learning syntactically controlled;produce paraphrase;paraphrase generation given;generating adversarial examples;paraphrase combination automated;produces desired paraphrase;controlled paraphrase;learning syntactically;produce paraphrase sentence;existing uncontrolled paraphrase;generating adversarial;paraphrase quality;speci\ufb01cations paraphrase quality;way generating adversarial;decoder model syntactically;paraphrases;uncontrolled paraphrase;paraphrase combination;paraphrases follow target;model syntactically controlled"}, "cf0860ab99c63cb7cbd5317fca7cf1fe70e8fb63": {"ta_keywords": "corpus virtual knowledge;mentions entities corpus;entities corpus;traverses textual data;knowledge bases;entities corpus step;existing knowledge bases;virtual knowledge base;natural language inputs;traverses textual;starting natural language;knowledge base;corpus virtual;knowledge bases module;knowledge base kb;using corpus virtual;index contextual representations;contextual representations;drkit traverses textual;questions using corpus;textual data;consider task answering;textual data like;virtual knowledge;natural language;mentions entities;task answering;corpus;using corpus;task answering complex", "pdf_keywords": "corpus virtual knowledge;reasoning virtual knowledge;virtual knowledge base;virtual knowledge;entities corpus;entities corpus introduce;large text corpus;traverses textual data;corpus encoded query;knowledge base;consider task answering;task answering complex;corpus virtual;answering natural language;mentions entities corpus;using corpus virtual;knowledge base kb;task answering;questions using corpus;traverses textual;index contextual representations;metaqa dataset answering;reasoning virtual;text corpus;contextual representations;dataset answering;text corpus encoded;corpus;corpus introduce ef\ufb01cient;textual data"}, "024091a3c0223f27d6456b1a27db18fb08d41e5a": {"ta_keywords": "model machine translation;machine translation accuracy;vocabularies binarized neural;binarized neural network;machine translation;binarized neural;bnnjm learns binary;model nnlm word;language model nnlm;gains machine translation;translation accuracy;machine translation paper;learns binary;neural network language;learns binary classifier;translation accuracy problems;nnlm word source;neural network joint;bnnjm learns;nnjm bnnjm learns;nnlm word;network language model;vocabularies binarized;large vocabularies binarized;language model;context target words;binarized nnjm bnnjm;nnjm augments neural;contrastive estimation nce;binarized nnjm", "pdf_keywords": ""}, "629323c5b9f7c64afac9300212538e488569bd1e": {"ta_keywords": "embeddings ontology induction;knowledge synonym dictionaries;word embeddings ontology;synonym dictionaries establishing;lexical knowledge synonym;synonym dictionaries;dictionaries establishing semantic;ontology induction;presents ontology induction;ontology induction approach;structured lexical knowledge;dictionaries word embeddings;establishing semantic relations;lexical knowledge;embeddings ontology;semantic relations;establishing semantic;semantic relations structures;joining dictionaries word;semantic;word embeddings;extracts structured lexical;word embeddings projections;joining dictionaries;structured lexical;dictionaries establishing;knowledge synonym;dictionaries word;dictionaries;using word embeddings", "pdf_keywords": ""}, "33aa6c70eac0e4b7eb28d8386e5e4113fdd55203": {"ta_keywords": "question answering qa;question answering ntcir;automatic question answering;question answering;choice question answering;modular automatic question;answering qa ntcir;answering qa;automatic question;answers multiplechoice english;answering ntcir 11;assertions answer choice;exam;answering ntcir;history entrance exam;set answers multiplechoice;set answers;ntcir 11 qa;answer choice cmu;qa ntcir 11;testing set answers;answers multiplechoice;entrance exam;cmu uima;exam given context;qa ntcir;11 qalab evaluations;entrance exam given;cmu uima based;11 qa", "pdf_keywords": ""}, "2e820673ca861a9ece8d36f2b93793b5d2c7e1da": {"ta_keywords": "lightweight block ciphers;block ciphers;current cryptanalysis implementation;cryptanalysis implementation;cryptanalysis implementation results;block ciphers aid;advanced encryption standard;cryptanalysis;current cryptanalysis;ciphers aid securing;advanced encryption;encryption standard aes;ciphers;encryption standard;standard aes;standard aes suitable;ciphers aid;environments advanced encryption;aes suitable;aes;rationale current cryptanalysis;encryption;lightweight block;families lightweight block;speck families lightweight;aes suitable paper;security agency nsa;nsa developed simon;nsa;nsa developed", "pdf_keywords": ""}, "49d415cf593be38c6cd97a183dadc7d7b48bab72": {"ta_keywords": "firms evidence ai;growth firms ai;firms ai related;ai productivity labor;firms ai;data ai patenting;ai patenting census;ai patenting;impact ai productivity;innovating firms evidence;ai productivity;ai related innovations;collected innovating firms;innovations firm labor;faster employment growth;firms evidence;firms quantifying impact;labor firms industries;innovating firms;employment growth;growth firms;firms quantifying;labor firms;evidence ai related;quantifying impact ai;firm growth firms;firms industries;patenting census;associated firm growth;faster revenue growth", "pdf_keywords": ""}, "2225950d1d3e02bc0d88a0c78325d00e0122b576": {"ta_keywords": "deep learning separation;data speech separation;speech separation end;learning separation recognition;recognition overlapping speech;speech separation;speech separation methods;adapts separation recognition;methods speech separation;learning separation;overlapping speech signals;separation recognition;separation recognition overlapping;separation recognition components;overlapping speech;separation methods deep;speech signals joint;mixed data speech;jointly trained deep;separation end end;separation end;end deep network;simultaneous speech signals;end automatic speech;speech recognition;multiple simultaneous speech;simultaneous speech;speech recognition asr;separation;trained isolation connected", "pdf_keywords": ""}, "05fb5a180214bf092eeda30baf9f16fb6bd15727": {"ta_keywords": "speech based waveform;modified speech parameter;generate corrected speech;speech modified;speech modified speech;modifies speech parameter;generating corrected speech;native speech quality;speech quality corrected;speech furthermore waveform;speech significantly degrades;native speech based;speech parameter;using native speech;modifying durational patterns;modify durational patterns;speech parameter sequence;modified speech;modifying durational;modify durational;correcting durational patterns;analysis duration correction;speech based;speech quality;temporal warping analyzed;speech read japanese;corrected speech modified;quality corrected speech;time warping dtw;duration correction", "pdf_keywords": ""}, "649c1148439a4e875dab414ba67bf8c80350af4a": {"ta_keywords": "neural semantic parser;semantic parsing code;neural abstract syntax;abstract syntax parser;syntax parser semantic;semantic parser maps;parser semantic;parser semantic parsing;semantic parser;parsing code generation;semantic parsing;syntax parser;abstract syntax description;formal meaning representations;syntax description language;parser maps natural;based abstract syntax;parser;transition based neural;maps natural language;syntax description corresponding;abstract syntax;new abstract syntax;neural semantic;based neural semantic;parsing code;parsing;natural language nl;parser maps;syntax target mr", "pdf_keywords": "tranx semantic parsing;semantic parsing code;parser semantic;semantic parsing;semantic parsing procedure;semantic parsing atis;parser semantic parsing;syntax parser semantic;decoupling semantic parsing;abstract syntax parser;parsing code generation;syntax parser;parser;parsing;tranx semantic;knowledge guide parsing;grammars asn employs;speci\ufb01cities grammars asn;guide parsing;parsing atis;parsing procedure;guide parsing process;parsing atis geo;grammars asn;decoupling semantic;parsing code;parsing process;code generation django;construct asdl grammar;based abstract syntax"}, "86d84c1c9b0a500f930696ab27c83a4b30477560": {"ta_keywords": "paraphrastic sentence embeddings;learning paraphrastic;learning paraphrastic sentence;para phrase corpora;sentence embeddings directly;creating para phrase;paraphrastic;sentence embeddings;paraphrastic sentence;methodology learning paraphrastic;para phrase;phrase corpora;cross lingual tasks;phrase corpora resulting;lingual tasks outperforms;embeddings directly bitext;lingual tasks;corpora;embeddings directly;cross lingual;applied cross lingual;lingual;creating para;embeddings;corpora resulting model;corpora resulting;step creating para;para;state art baselines;sentence", "pdf_keywords": "monolingual similarity;bilingual sentence representations;lingual semantic textual;cross lingual semantic;large bilingual corpora;monolingual similarity devise;lingual semantic;fact monolingual similarity;semantic textual similarity;english paraphrastic representations;language pairs analyze;monolingual cross lingual;bilingual corpora;inducing paraphrase datasets;textual similarity;textual similarity sts;english datasets compared;language pairs;monolingual bilingual;cross lingual;2016 english datasets;corpora mining tasks;2in fact monolingual;lingual;sentence representations encode;state art monolingual;paraphrastic representations;manipulation large bilingual;parallel corpora;parallel corpora mining"}, "65c53ed3575e160eb1e7d0a516353ba52de7e7e5": {"ta_keywords": "auction bid leakage;leakage specific auction;leakage particular auction;leakage likely auctions;probability bid leakage;bid leakage detection;russian procurement auctions;bid leakage sample;bid leakage specific;bid leakage;bid leakage particular;procurement auctions;procurement auctions 2014;auction bid;specific auction bid;particular auction;corrupted bid leakage;bid leakage likely;specific auction;auction;auctions;winning bid;problem bid leakage;fall winning bid;particular auction tacit;auctions 2014;winning bid received;bidders;likely auctions;probability bid", "pdf_keywords": "russian procurement auctions;leakage higher auctions;procurement auctions 2014;procurement auctions;leaks opponents bids;stealed bid auctions;bid auctions detecting;auctions detecting bid;auction procurer leaks;auctions exposed;bid auctions;000 russian procurement;\ufb01nd auctions exposed;auctions detecting;auctions exposed bid;auctions analyze dataset;auctions analyze;russian procurement;bid auction;earlier auctions analyze;bid auction procurer;opponents bids;\ufb01nd auctions;sealed bid auction;auction;auctions;detecting bid;auctions 2014;detecting bid leakage;probability bid leakage"}, "a9e6222e71dd101d444b7192b3a0636c71edb0a4": {"ta_keywords": "mentions entities corpus;corpus virtual knowledge;contextual representations mentions;contextual representations;entities corpus;traverses textual data;index contextual representations;knowledge bases;mentions particular neural;starting natural language;natural language inputs;existing knowledge bases;contextual representation;mentions entities;consider task answering;contextual representation encoder;knowledge bases drkit;traverses textual;virtual knowledge base;task answering;knowledge base;natural language;textual data like;textual data;knowledge base kb;entities corpus consider;questions using corpus;corpus virtual;using corpus virtual;task answering complex", "pdf_keywords": ""}, "6536f36648d39f0f9f6105562f76704fcc0b19e8": {"ta_keywords": "persistent spatial semantic;tasks persistent spatial;spatial semantic representation;language robot actions;spatial semantic;tasks robotic agents;propose persistent spatial;robot actions long;language robot;persistent spatial;semantic representation high;agent performs hierarchical;performs hierarchical reasoning;robotic agents;natural language instruction;hierarchical reasoning effectively;robot actions;term tasks robotic;semantic representation;tasks persistent;tasks robotic;robot;representations natural language;level natural language;persistent representations natural;persistent representations;hierarchical reasoning;reasoning effectively execute;semantic representation method;gap language robot", "pdf_keywords": "spatial semantic representation;persistent spatial semantic;utilizing spatial semantic;spatial semantic;semantic representation hierarchical;update spatial semantic;descriptions actions 3d;representation world robot;mobile manipulation tasks;task descriptions actions;actions interactive 3d;manipulation tasks;representation hierarchical;manipulation actions interactive;learning map high;semantic representation world;hierarchical language;propose hierarchical language;spatial representation long;language conditioned spatial;world robot;manipulation tasks state;representation enables hierarchical;spatial representation;learning map;uses spatial representation;task propose hierarchical;mobile manipulation task;semantic representation enables;semantic representation"}, "2c0f2a03c3a427cc61359b5e2c31cfefe9850a31": {"ta_keywords": "extracting concept instance;domain information extraction;information extraction;clustering terms html;concept names clusters;extracting concept;information extraction method;pairs html corpus;html corpus;method extracting concept;concept instance pairs;concept names;names clusters using;terms html tables;assigning concept names;concept instance;terms concept instance;html corpus earlier;names clusters;terms html;approach clustering terms;clustering terms;extraction method extracting;instance pairs html;applied large corpus;novel approach clustering;extracting;large corpus;corpus earlier approaches;method extracting", "pdf_keywords": "html corpus clustering;term clusters;term clusters showed;term clusters merging;coordinate term clusters;corpus clustering;corpus clustering method;rich corpora method;extracting concept instance;agglomerative clustering;table rich corpora;triplets instances clustering;information extraction;html corpus;corpora method improved;pairs html corpus;clustering method outperforms;instances clustering;complexity clustering;instances clustering method;novel clustering method;agglomerative clustering algorithms;domain information extraction;means agglomerative clustering;extracting concept;information extraction technique;presented novel clustering;large corpora;novel clustering;apply large corpora"}, "ed2cc779c7eb0004bd6dd50538a2cafca092c94f": {"ta_keywords": "spelling normalization historical;normalization historical texts;normalization linguistic annotation;linguistic annotation historical;normalization pos tagging;spelling normalization;annotation historical language;automatic normalization linguistic;deals spelling normalization;normalization linguistic;tagging normalized data;tagging normalized;historical language data;historical german texts;pos tagging normalized;modern speech taggers;set historical german;normalization historical;linguistic annotation;german texts;historical german;german texts 15th;annotation historical;historical language;marks automatic normalization;automatic normalization;historical texts;best normalization pos;speech taggers;processing historical", "pdf_keywords": ""}, "5bcd9117899bc2c91db83532dcf587b9d8f8888b": {"ta_keywords": "concise edition casebook;case constitutional;raw case constitutional;case constitutional provision;text supreme court;supreme court;federal judiciary;constitutional;judiciary;supreme court opinions;judiciary separation;casebook;edition casebook compact;federal judiciary separation;edition casebook;casebook compact;constitutional provision;role federal judiciary;teachers freedom structure;casebook compact easy;judiciary separation powers;religion concise edition;teachers freedom;concise edition;federalism aspects;freedom structure course;allow teachers freedom;court opinions primary;court opinions;opinions primary documents", "pdf_keywords": ""}, "3d1cfefdbe40f7535ada772c260c192bb63bb9fe": {"ta_keywords": "scienti\ufb01c document similarity;scientific document similarity;document similarity model;document similarity;document similarity novel;grained aspects texts;vector models textual;relatedness provide textual;learning match aspects;textual supervision;texts multi vector;similarity model;textual descriptions cited;cited papers related;aspect matching fast;similarity model based;match aspects papers;descriptions cited papers;aspect matching;paper relatedness provide;grained scientific document;similarity novel;papers citations;similarity novel form;paper relatedness;aspects documents;source supervision sentences;similarity;sentences text papers;text papers cite", "pdf_keywords": "grained aspects texts;document similarity model;learning match aspects;vector models textual;textual supervision;scienti\ufb01c document similarity;documentlevel similarity novel;sentence level aspects;document similarity;documentlevel similarity;aggregates documentlevel similarity;grained scienti\ufb01c document;sentences text papers;model document similarity;aspect matching fast;aspect matching;source supervision sentences;contextualized sentence embeddings;models textual;vectors correspond sentence;document similarity makes;papers terms contextualized;sentence embeddings paper;methods aspect matching;aspects paper abstracts;textual supervision used;match aspects papers;models textual guidance;matches single aspects;correspond sentence level"}, "5e74d4e041a25e7752a596e2891975df5ba65aa2": {"ta_keywords": "microphones covariance prediction;beamforming improve speech;mask microphones covariance;microphone speech databases;microphone speech;multi microphone speech;microphones covariance;improve speech recognition;speech noise spatial;speech databases;channel mask prediction;single mask microphones;obtain speech noise;measures speech recognition;speech noise;mask microphones;speech recognition;speech recognition accuracies;improved mvdr beamforming;beamforming improve;perform beamforming improve;speech databases indicate;mask prediction networks;masks obtain speech;beamforming method performs;improve speech;mask prediction;quite speech recognition;beneficial perform beamforming;quality measures speech", "pdf_keywords": ""}, "a0ab4106dabd6bc067c7b3e4db06807e2c0f6036": {"ta_keywords": "pretrained language models;nlp tasks strong;large general corpus;general corpus tlm;corpus jointly optimizes;nlp tasks;corpus tlm;pretrained language;similar pretrained language;pretraining1 pretrained language;approach nlp tasks;learning framework tlm;corpus tlm uses;language models;general corpus;efficient learning framework;simple efficient learning;language modeling;language modeling objective;objective language modeling;large scale pretraining1;general corpus jointly;language models roberta;corpus;corpus jointly;approach nlp;task data large;tlm uses task;efficient learning;language models standard", "pdf_keywords": "corpus jointly optimizes;large general corpus;training corpus;general corpus tlm;corpus \ufb01netuning task;training language model;16 training corpus;corpus tlm;small subset corpus;objective language modeling;language modeling objective;learning task objective;pretraining \ufb01netuning approach;supervised language modeling;corpus tlm uses;pretraining simple ef\ufb01cient;language model entire;entire general corpus;task data large;data general corpus;approach encoder task;supervised language;corpus approach encoder;large scale pretraining;general corpus;general corpus using;learning task;self supervised language;encoder task objective;\ufb01netuning task data"}, "a4f2e6c38454c9e7b4068a456813d622b91f2663": {"ta_keywords": "speech diadochokinetic ddk;diadochokinetic ddk rates;speech diadochokinetic;reported speech diadochokinetic;diadochokinetic ddk;ddk rate measurement;ddk rate routine;ddk rate calculation;calculation ddk rate;ddk rate;ddk rates;ddk measurement;ddk rates published;ddk measurement described;diadochokinetic;rate routine clinical;problems ddk rate;calculation ddk;collection ddk rate;methodological problems ddk;protocol ddk measurement;clinical assessment methodological;routine clinical assessment;clinical assessment;rate routine;rate measurement;rate measurement identified;data collection ddk;exist calculation ddk;ddk", "pdf_keywords": ""}, "d408be961d0db8b97c0ca6b2fc7afd3c9dc914e7": {"ta_keywords": "mobility planning applications;mobility planning;integrates mobility providers;mobility platform mobilytics;modal mobility platform;available mobility options;mobility providers;mobility providers commuters;mobility platform;mobility options;available mobility;state art mobility;new mobility modes;integrate available mobility;mobilytics integrates mobility;mobility modes;multi modal mobility;mobility options localized;new mobility;providers commuters community;mobility;mobility modes new;transportation management platforms;trip planner monolithic;integration new mobility;microservices based modular;modal mobility;commuters community stakeholders;integrates mobility;providers commuters", "pdf_keywords": ""}, "4fe70c172cc38c2eb15103f0f1eac4e6766c60e6": {"ta_keywords": "voice activity detection;robust voice activity;noise robust voice;robust voice;voice activity;weight normalization noise;normalization noise robust;gaussian pruning weight;based gaussian pruning;normalization noise;pruning weight normalization;noise robust;gaussian pruning;voice;weight normalization;method based gaussian;activity detection;model estimation method;model estimation;normalization;based gaussian;pruning weight;wise model estimation;frame wise model;estimation method based;estimation method;gaussian;noise;robust;estimation", "pdf_keywords": ""}, "efe9fe804f34b18524708b18293508191bda78eb": {"ta_keywords": "static scheduling;redundant hardware severe;faults employing redundancy;permanent faults improves;faults improves reliability;consumption redundant hardware;static scheduling dynamic;improves reliability energy;preserving reliability tmr;tmr mitigated operations;dynamic task management;redundant hardware;decouples tasks cores;permanent faults employing;scheduling dynamic task;reliability tmr provides;combination static scheduling;tasks cores susceptible;transient permanent faults;consumption tmr mitigated;tasks cores;scheduling dynamic;reliability tmr;faults improves;power consumption redundant;executions program tmr;permanent faults;tmr mitigated;faults employing;introduce reactive tmr", "pdf_keywords": ""}, "395044a2e3f5624b2471fb28826e7dbb1009356e": {"ta_keywords": "pretrained sentence embeddings;sentence embeddings supervised;paraphrastic sentence embeddings;universal sentence embeddings;sentence embeddings;sentence embeddings based;supervision paraphrase database;paraphrase database;embeddings supervised tasks;annotated textual similarity;textual similarity datasets;short term memory;recurrent neural networks;embeddings based supervision;paraphrase database ganitkevitch;embeddings supervised;textual similarity;supervision paraphrase;term memory lstm;general purpose paraphrastic;based supervision paraphrase;memory lstm recurrent;lstm recurrent neural;annotated textual;pretrained sentence;paraphrastic sentence;paraphrastic;recurrent neural;embeddings;lstm recurrent", "pdf_keywords": "universal sentence embeddings;embedding sentences;sentence embeddings based;sentence embeddings propose;sentence embeddings;embedding sentences simple;paraphrastic sentence embeddings;baseline embedding sentences;embeddings improve;learned ppdb embeddings;embeddings improve performance;embeddings propose model;textual similarity datasets;embeddings propose;supervision paraphrase database;annotated textual similarity;ppdb embeddings improve;embeddings based supervision;textual similarity;compositional models encode;embeddings;create universal sentence;embeddings based;paraphrase database;representations learned models;arbitrary word sequences;universal paraphrastic;universal paraphrastic sentence;pairs paraphrase database;2016 universal paraphrastic"}, "14551d2bf2584bb1ea7ad69f9a64419bab82bb6e": {"ta_keywords": "sound event detection;sound event;sound event class;foreground sound events;sound events;sound events effectively;sound events essential;features audio data;optimized sound event;based sound event;local features audio;features audio;information foreground sound;foreground sound;characteristics sound events;background sound;sound local information;background sound local;audio data effectively;audio data;cnns global features;audio;information background sound;event detection;processing optimized sound;global features captured;utilizing semi supervised;learning data augmentation;uses semi supervised;cnns global", "pdf_keywords": ""}, "469ad889bd628e2cf46424f7097c4830719ec740": {"ta_keywords": "vowel space estimates;estimation talker vowel;talker vowel space;automatic vowel space;vowel space findings;vowel space area;vowel space using;expanded vowel space;talker vowel;vowel space representations;vowel space;vowel space expansion;larger sample vowels;range vowel space;sample vowels;relating automatic vowel;sparse expanded vowel;automatic vowel;f2 vowel space;addition talker vowel;intelligibility differences pronunciation;sample vowels point;vowels;vowels making;pitch range vowel;vowel;point vowels;estimates talker intelligibility;vowels point vowels;vowels point", "pdf_keywords": ""}, "d9944e13a38e5ca685985c9b5c050ec6d300e104": {"ta_keywords": "verbal communication training;communication training interactive;communication training;training interactive multimedia;non verbal communication;verbal communication;training interactive;interactive multimedia;interactive multimedia application;multimedia;non verbal;multimedia application;communication;verbal;interactive;training;application;non", "pdf_keywords": ""}, "ba602ea9aaecab5a3ad243211f110ae7db4cc66a": {"ta_keywords": "performative risk minimization;risk minimization performative;performative risk minimizers;minimization performative risk;risk minimization;repeated risk minimization;risk minimization additionally;minimizers performative risk;risk minimizers;risk minimization perturbed;deploying learning algorithms;prediction seek classify;strategic classification;strategic classification decision;performative prediction seek;risk minimizers motivating;performative prediction;work strategic classification;prediction seek;behavior deploying learning;classification decision dependent;classification;classification decision;mapping classifier;flows performative risk;classifier data distribution;performative risk;minimization performative;learning algorithms;learning algorithms explicitly", "pdf_keywords": ""}, "ef6a4d8bf248944ca1d0cfdc107d3bb107f57bff": {"ta_keywords": "\u30d1\u30e9\u30d5\u30ec\u30fc\u30ba\u3092\u8003\u616e\u3057\u305f\u6a5f\u68b0\u7ffb\u8a33\u306e\u8aa4\u308a\u7b87\u6240\u9078\u629e \u5fdc\u7528\u51e6\u7406 \u7b2c6\u56de\u96c6\u5408\u77e5\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0;\u30d1\u30e9\u30d5\u30ec\u30fc\u30ba\u3092\u8003\u616e\u3057\u305f\u6a5f\u68b0\u7ffb\u8a33\u306e\u8aa4\u308a\u7b87\u6240\u9078\u629e \u5fdc\u7528\u51e6\u7406;\u30d1\u30e9\u30d5\u30ec\u30fc\u30ba\u3092\u8003\u616e\u3057\u305f\u6a5f\u68b0\u7ffb\u8a33\u306e\u8aa4\u308a\u7b87\u6240\u9078\u629e;\u5fdc\u7528\u51e6\u7406 \u7b2c6\u56de\u96c6\u5408\u77e5\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0;\u7b2c6\u56de\u96c6\u5408\u77e5\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0;\u5fdc\u7528\u51e6\u7406", "pdf_keywords": ""}, "6c68866e6486923d2e8b999de57d450c9d4febab": {"ta_keywords": "machine translation nmt;neural machine translation;machine translation smt;translation pbmt neural;machine translation pbmt;statistical machine translation;machine translation phrase;machine translation;based machine translation;phrase based decoding;decoding cost nmt;translation nmt;phrase based smt;phrase based translation;translation smt phrase;pbmt neural machine;smt phrase based;phrase based machine;translation smt;translation phrase based;pbmt limited phrase;phrase based soft;pbmt neural;translation pbmt;nmt outputs search;smt phrase;cost nmt output;based translation rule;smt nmt;cost nmt", "pdf_keywords": ""}, "c2b22a18ea2ed444c6c1f5bb27ab55bda2b44567": {"ta_keywords": "emphasis transfer speech;models emphasis speech;emphasis speech translation;speech translation systems;model emphasis transfer;emphasis transfer;transfer speech translation;speech translation using;speech translation;emphasis speech;attentional neural network;hard attentional neural;speech translation paper;translation systems;traditional speech translation;transfer speech;attentional neural;networks traditional speech;model emphasis;paralinguistic information;hard attentional;neural network models;paralinguistic;paralinguistic information recent;conditional random fields;attentional;emphasis;translation systems oblivious;neural networks;models emphasis", "pdf_keywords": ""}, "b799d66c710dd82a1b925b9c31e55a0d2d99b624": {"ta_keywords": "people activities urban;hidden markov model;recovers urban dynamics;urban dynamics;sharing hidden markov;activities urban space;urban dynamics highly;activities urban;life mobility dataset;mobility dataset;rich urban dynamics;hidden markov;modeling people activities;urban dynamics different;mobility dataset results;real life mobility;urban states;semantics rich urban;dynamics human activities;urban states city;life mobility;urban space crucial;urban;markov model sshmm;extracts urban states;urban space;mobility;outperforms general hmm;recovers urban;people activities", "pdf_keywords": ""}, "0c39c0dc296a902e4a5eb85182209f7b9e6053b0": {"ta_keywords": "dynamic nns tensorflow;dynamic nn architectures;deep learning dl;nns tensorflow;tensorflow;dynamic nn;deep learning;frameworks dynamic nns;backpropagation scheduling;nns tensorflow fold;performs backpropagation scheduling;network architectures dynamic;recent deep learning;backpropagation scheduling execution;dataflow graph construction;dynamic nns;various dynamic nn;vertex centric programming;dynamic network;architectures dynamic ones;training various dynamic;dynamic models inefficient;models existing dataflow;nn architectures;dynamic dl models;architectures dynamic;dynamic network structure;dataflow graph;tensorflow fold dynet;tensorflow fold", "pdf_keywords": "dynamic nns tensorflow;deep architectures workhorse;nns deep architectures;deep learning dl;nns tensorflow;dynamic nn architectures;deep architectures;networks nns deep;tensorflow;deep learning;nns tensorflow fold;neural networks nns;dynamic nn;backpropagation scheduling;frameworks dynamic nns;performs backpropagation scheduling;backpropagation scheduling execution;static graph optimization;vertex function dynamic;introduction deep learning;various dynamic nn;dynamic nns implementation;dynamic nns;nn architectures;graph performs backpropagation;nns deep;cavs vertexcentric programming;tensorflow fold;vertexcentric programming interface;tensorflow fold dynet"}, "57026b2d45fa59c6326b5a1d2e27626403f083ba": {"ta_keywords": "artificial intelligence ethics;ai ethics;ethics artificial intelligence;integrate ai ethics;ai ethics general;intelligence ethics course;intelligence ethics;ethics artificial;ai educators;use ai educators;ethics general artificial;ai courses provide;ethics course instructors;moral ethical philosophical;ethical philosophical;issues ai courses;artificial intelligence course;ai courses;philosophical issues ai;moral ethical;ethics course;ethical;ethics;understand moral ethical;ethical philosophical issues;intelligence course teach;ethical philosophical impacts;students artificial intelligence;stand artificial intelligence;use ai", "pdf_keywords": "ai ethics;integrate ai ethics;ai ethics general;ai educators;term ai ais;use term ai;ai educators article;use ai;term ai;intelligence ethics;use ai educators;ai;ai ais refer;arti\ufb01cial intelligence ethics;ai ais;intelligence ethics course;ais designed;suggestions integrate ai;ais1 capable learning;integrate ai;autonomous decision maker;ais refer arti\ufb01cial;ethics;ais;arti\ufb01cial intelligence;resources use ai;discourse ais designed;ethics course;arti\ufb01cial intelligence course;ais designed built"}, "653add540adae12491fade7e18ec4e1e4288b4a7": {"ta_keywords": "advising support tool;advising support software;colleges advising support;course selection advising;selection advising career;selection advising;colleges advising;advising career paths;decision theoretic advising;theoretic advising support;advising career;advising support;advising;majors colleges advising;person advising;support tool undergraduates;theoretic advising;student advisor;replace person advising;student advisor relationship;course selection;advisor relationship particularly;concerning course selection;terms course planning;career paths;advisor;particularly terms course;augment student advisor;course planning;tool undergraduates", "pdf_keywords": "advising attitudes;advising attitudes needs;responses students advisors;automated advising;students advisors;automated advising support;advising;components automated advising;students decision making;based responses students;responses students;responses students engineering;computer programming course;students engineering computer;advising support;advisor student;programming course;students decision;college personal attention;students;engineering students;shown students decision;advisor student relationship;programming course \ufb01rst;understanding university college;non engineering students;prepare future courses;likely distrust advisor;computer programming;students situation"}, "6fe62b967376361d7cd55e1033ab968895841d67": {"ta_keywords": "deep learning text;fcm interpretable deep;interpretable deep learning;interpretable deep;focused concept miner;learning text mining;concepts text data;focusing concept discovery;recovered concept diversity;learning text;concept discovery;interpretability accuracy trade;recall unique concepts;text mining;compared blackbox classifiers;fcm custom neural;concept discovery study;concepts text;blackbox classifiers;custom neural;deep learning;concepts highly;interpretability accuracy;corpus level insights;concept diversity;fcm interpretable;classifiers;corpus level concepts;concept diversity need;concept correlational importance", "pdf_keywords": ""}, "29001ac04e61dfffb8e24ffd3e351ece12ce44af": {"ta_keywords": "speech enhancement source;mixture signal speech;estimate phase mask;signal speech enhancement;speech enhancement;independent speaker separation;phase mask;signal phasebook layers;estimation phase mixture;speaker separation matching;speaker separation;enhancement source separation;softmax activation mask;signal phasebook;phasebook layers;frequency representation mixture;phase estimation;phase reconstruction;source separation systems;phase estimation phase;domain signal phasebook;signal speech;phasebook layers use;phasebook;source separation;frequency masks;time frequency masks;channel speaker independent;additional phase reconstruction;phase mask suffering", "pdf_keywords": "independent speaker separation;speaker separation matching;speaker separation;complex mask representations;mask representations;corpus single channel;channel speaker independent;channel speaker;frequency masks;interval magnitude mask;frequency masks finally;time frequency masks;magnitude mask;complex mask;softmax activation magnitude;magnitude mask 12;independent speaker;introduction convex softmax;speaker independent;various complex mask;softmax;speaker independent speaker;mask based approaches;single channel speaker;softmax activation;theart mask based;mask representations started;convex softmax activation;softmax layer;mask based"}, "5dce0fd43a21825bebd8121fd0a28155d524c44c": {"ta_keywords": "\u30a2\u30e2\u30f3\u30ba\u3068\u73fe\u4ee3\u30a2\u30e1\u30ea\u30ab\u8a69", "pdf_keywords": ""}, "4b2d583e22f378f9104814d9f63cda411ddd5825": {"ta_keywords": "lexical sememe prediction;lingual sememe prediction;predict sememes words;lingual lexical sememe;sememe based linguistic;semantic space sememe;sememes multi lingual;cross lingual lexical;linguistic knowledge bases;sememes words languages;automatically predict sememes;lexical sememe;lingual lexical;languages sememe based;cross lingual sememe;predict sememes;lingual sememe;nlp tasks sememes;sememe prediction;linguistic knowledge;multi lingual words;sememes words;low dimensional semantic;space sememe prediction;sememe prediction aiming;languages sememe;lingual words;languages languages sememe;sememe prediction important;based linguistic knowledge", "pdf_keywords": ""}, "f1513d72cb5dd6d70541cce0da36b77467128d13": {"ta_keywords": "error rate training;certainty answer questions;rate training mert;exam pilot task;entrance exam;exam;nology entrance exam;entrance exam pilot;training mert;training mert train;rate training;2013 qa4mre;exam pilot;tech nology entrance;answer questions;qa4mre received score;mert train weights;threshold denes certainty;clef 2013 qa4mre;qa4mre;train weights model;weights model;2013 qa4mre core;answer questions paper;training;qa4mre core;science tech nology;task clef 2013;minimum error;weights model propose", "pdf_keywords": ""}, "838fbfd9066dbbac6c10059c5b183046fb1cd9d1": {"ta_keywords": "deep bayesian active;active learning disagreement;bayesian active learning;deep active learning;active learning natural;active learning al;active learning;active learning addressing;investigate active learning;language processing deep;study deep active;processing deep bayesian;deep active;processing deep;bayesian active;deep bayesian;learning natural language;data dependence deep;empirical study deep;settings bayesian active;deep learning natural;dependence deep learning;learning addressing;learning addressing multiple;natural language processing;deep learning;learning disagreement using;learning al mitigating;learning disagreement;learning", "pdf_keywords": "active nlp;active learning disagreement;deep active learning;active learning addressing;bayesian active learning;active learning experiments;entity recognition ner;active learning;active learning al;active nlp named;asiddhant active nlp;entity recognition;srl active learning;named entity recognition;study deep active;semantic role labeling;running active learning;tasks sentiment classi\ufb01cation;deep active;recognition ner semantic;empirical study deep;learning addressing;learning addressing multiple;learning disagreement using;ner semantic role;various annotation budgets;nlp named entity;bayesian active;learning data;deep learning data"}, "5693c74eb8ffde1490ba480fdc963f008243906a": {"ta_keywords": "crowd annotation framework;consolidation crowd annotations;crowd annotations;crowd annotation;based crowd annotation;crowd annotations real;suggesting annotations sampling;sequence tagging tasks;suggesting annotations;dynamically suggesting annotations;sequence tagging;rapid tagging recommendations;annotations sampling informative;annotations real time;data annotation framework;annotation framework;annotators real time;annotations sampling;data annotation;rapid tagging;annotation framework sequence;tagging recommendations powered;alpacatag sequence tagging;active intelligent recommendation;new annotations introduce;annotations introduce;sequence labeling tasks;tagging;framework sequence tagging;new annotations", "pdf_keywords": ""}, "a8239258abded4f08d1bf270c2e86662f4dc1760": {"ta_keywords": "learner errors arise;student learning complex;learning complex skill;learning computational model;learner errors;prior conceptual knowledge;computational model learner;investigate learning complex;model learning called;learning called;prior knowledge;learning complex;learning computational;model learner;investigate learning;weak prior knowledge;model learner errors;student learning;synthetic student modeling;process student learning;learning called simstudent;prior knowledge introduction;specific prior knowledge;prior knowledge oboru;learning complex problem;learning;student modeling;computational model learning;learner;conceptual knowledge", "pdf_keywords": ""}, "a182a8a0678857df5c513d52469fa707c32e69ec": {"ta_keywords": "translation rules based;statistical machine translation;translation rules;machine translation smt;machine translation;appropriate translation rules;space rule selection;based rule selection;rule selection model;machine translation contrast;syntax based statistical;rule selection;rules based sentence;rule selection continuous;rule selection csrs;translation smt choose;dependent rule selection;continuous space rule;syntax based smt;entropy based rule;space rule;syntax based;rule selection mers;selection model syntax;translation smt;words features csrs;context dependent rule;choose appropriate translation;model syntax based;translation contrast", "pdf_keywords": ""}, "4e3935ef7da6bcbb202ec7f8b285c313cadcd044": {"ta_keywords": "papers question answering;question answering systems;question answering;answering entire papers;answering systems;answering systems answer;models qa tasks;qa tasks;systems answer questions;natural language processing;answering questions underperforming;information seeking qa;nlp practitioners provide;answering questions;qa tasks perform;tasks perform answering;set nlp practitioners;nlp practitioner read;nlp practitioners;question written nlp;perform answering questions;written nlp practitioner;seeking qa dataset;points answering;points answering entire;facilitate questions answered;nlp practitioner;supporting evidence answers;written nlp;qa dataset designed", "pdf_keywords": "question answering search;question answering;datasets question answering;seeking question answering;answer large corpus;answering search;question answering qa;answering search reading;answering qa dataset;large corpus text;answering questions;large corpus;natural language processing;natural language query;generating answers best;comprehend natural language;answering questions entire;points answering;answer questions;answering;answering qa;evidence generating answers;generating answers;2017 models tasks;answers best model;language processing;language query extract;585 natural language;qa dataset academic;points answering questions"}, "85e148ac629b1b38556c5fe5f8d657f2eb01a701": {"ta_keywords": "mds queue;mds;queue", "pdf_keywords": ""}, "a997d6e253f08a3e589432c611d6d2a3097d7629": {"ta_keywords": "collaborative online research;online research tool;issues collaborative online;collaborative online;online research;research tool designed;usability studies;research tool;usability studies conducted;research students getting;collaborative;help young researchers;online reader open;usability;online reader;understand usability;issues collaborative;research students;young researchers understand;openreview apis;getting started research;undergraduate research students;understand usability functionality;prototype integration openreview;conducted understand usability;usability functionality;undergraduate research;integration openreview apis;goal usability studies;graduate undergraduate research", "pdf_keywords": ""}, "3ec37205c9201fc891ab51da200e361fdc34bfb3": {"ta_keywords": "embeddings reading comprehension;word embeddings reading;trained word embeddings;reading comprehension tasks;embeddings reading;study word embeddings;word embeddings;word embeddings representation;reading comprehension focus;deep learning architectures;comprehension tasks primarily;reading comprehension;comprehension tasks;embeddings representation vocabulary;learning architectures;deep learning;comprehension focus;representation vocabulary tokens;vocabulary tokens;vocabulary tokens test;embeddings;comprehension focus past;pre trained word;research reading comprehension;novel deep learning;representation vocabulary;reading;embeddings representation;comprehension;novel deep", "pdf_keywords": "embeddings reading comprehension;word embeddings reading;trained word embeddings;study word embeddings;embeddings reading;models reading comprehension;reading comprehension tasks;embeddings initializing word;word embeddings;neural models reading;word embeddings representation;trained embeddings initializing;pre trained embeddings;trained embeddings;comprehension tasks;initializing word vectors;comprehension tasks primarily;reading comprehension;embeddings representation outof;reading comprehension experiment;word vectors;representation outof vocabulary;embeddings initializing;outof vocabulary tokens;word vectors significant;embeddings;vocabulary tokens test;vocabulary tokens;deep learning architectures;deep learning"}, "cfad4dc5f1f7fcaf7ca318acf672ad92d47f8413": {"ta_keywords": "workforce imbalance;workforce imbalance result;hiring paradox companies;address workforce imbalance;inequality employment face;address inequality employment;employment face hiring;hiring paradox;inequality employment;solution hiring paradox;bias aware legal;imbalance result legal;failing address workforce;legal solution hiring;wells fargo labor;workforce;face hiring paradox;federal contract compliance;contract compliance programs;contract compliance;result legal conflict;legal conflict recent;legal conflict;solution hiring;fargo labor;fargo labor department;employment;compliance programs ofccp;imbalance;compliance programs", "pdf_keywords": "discrimination law biasaware;algorithmic approaches employment;ethical hiring goals;legal ethical hiring;ethical hiring;discrimination;mathematics anti discrimination;discrimination law;example hiring;allows example hiring;hiring goals;law biasaware;equal opportunity employment;approaches employment allowed;discriminatory practices;hiring goals face;illegal discriminatory practices;anti discrimination law;hiring committees base;approaches employment;discriminatory;hiring;banding evaluate candidates;law biasaware technique;illegal discriminatory;evaluate candidates;rankings candidate screening;arguably illegal discriminatory;hiring committees;example hiring committees"}, "f889723a4427e914e4e32547dfd0ca4996170180": {"ta_keywords": "converted speech prosody;generate converted speech;voice conversion challenge;converted speech;directly predict prosody;predict prosody linguistic;voice conversion;predict prosody;speech prosody modeling;based voice conversion;transcribe source speech;speech tts generate;representation target speaker;prosody modeling asr;text speech tts;speech naturalness conversion;speech tts;text prediction ttp;voice conversion vc;text speech;latest voice conversion;automatic speech;target text prediction;modeling prosody;prosody modeling;speech prosody;input text speech;use automatic speech;voice conversion address;transferring prosodic", "pdf_keywords": "converted speech text;voice conversion challenge;voice conversion;generate converted speech;based voice conversion;abstract voice conversion;prosody modeling asr;converted speech;voice conversion wen;text speech tts;voice conversion vc;latest voice conversion;speech tts generate;speech tts;input text speech;transcribe source speech;text speech;speech text based;introducing prosody modeling;mandarin speakers prosody;speech recognition asr;prosody modeling methods;tts based voice;speech text;use automatic speech;automatic speech;prosody modeling;spt ttp asr;examined prosody modeling;bene\ufb01cial mandarin speakers"}, "f4cca8ea79e26fa20a91c3d3b769c9f7b82a6207": {"ta_keywords": "microphones median plane;spherical array microphones;spherical microphone array;virtual spherical microphone;microphones median;spherical microphone;using microphones median;ambiguity 3d audio;3d audio;array microphones experimental;spectral notches median;microphone array;microphone array discussed;array microphones;pinna spectral notches;microphones experimental results;microphones experimental;measured spherical array;extraction pinna spectral;hrir using microphones;microphone array work;microphone;notches median plane;virtual spherical array;microphones;spectral notches;notches median;elevation ambiguity 3d;notches psn median;hrir measured spherical", "pdf_keywords": ""}, "8c38bffc058d558e7c734032ba63942865e05ae4": {"ta_keywords": "embeddings knowledge base;embeddings knowledge;knowledge base queries;query embedding;emph query embedding;query embedding qe;base queries emph;answer faithful embeddings;faithful embeddings knowledge;base queries;exactly logical queries;queries emph;knowledge base;answer queries;ideal knowledge base;queries emph query;deductive reasoning answers;logical queries kb;deductive reasoning;logical queries;knowledge base kb;queries represented;entities kb queries;embedding qe techniques;answer queries real;queries represented jointly;embedding qe;embeddings;kb queries represented;emph query", "pdf_keywords": ""}, "e4a6bc3ac385b8982bbbe0a2a5ac0c79101ec979": {"ta_keywords": "generality overgenerality;overgenerality;generality", "pdf_keywords": ""}, "9bbeb4f0e48032df19f9f6a08839da5d2e60e8eb": {"ta_keywords": "noisy speech recognition;speech recognition asr;discriminative methods reverberant;distant stereo microphones;reverberant noisy speech;training speaker adaptation;speaker adaptation;speech recognition;stereo microphones challenging;noisy automatic speech;discriminative training speaker;wer dnn discriminative;speaker adaptation combination;dnn discriminative training;microphones challenging;automatic speech recognition;environment speech applications;microphones challenging desirable;binary masking discriminative;home environment speech;dnn discriminative;reverberated noisy asr;automatic speech;methods reverberant noisy;microphones investigate effectiveness;second chime challenge;stereo microphones investigate;recognition asr;noisy speech;speech recognition using", "pdf_keywords": ""}, "8f963beca679cb1129df0a944c6de4b126e20fd5": {"ta_keywords": "language fusion;resourced language fusion;language fusion methods;term memory lstm;librispeech corpus transfer;mono lingual asr;memory lstm;lingual asr setup;corpus transfer;short term memory;corpus transfer learning;multilingual asr;lingual asr;memory lstm memory;lstm memory;term memory;lstm memory cell;multilingual asr mlasr;methods mono lingual;setup multilingual asr;librispeech corpus;lstm;transfer learning mlasr;seq2seq decoder;seq2seq decoder long;mono lingual;state seq2seq decoder;transfer baseline librispeech;setup librispeech corpus;low resourced language", "pdf_keywords": "sequence speech recognition;language model integration;sequence sequence speech;sequence speech;model rnnlm;neural machine translation;speech recognition jaejin;speech recognition;language model;model recurrent neural;integrating lm decoder;recurrent neural network;train seq2seq model;model rnnlm librispeech;lingual rnnlm nmt;nmt model rnnlm;lingual rnnlm;lm decoder;neural network lm;end neural machine;seq2seq model integrate;integration based memory;pre trained lm;rnnlm nmt model;machine translation nmt;nmt model recurrent;trained lm;mono lingual rnnlm;memory control sequence;model recurrent"}, "b2c47dd46bf7087b754aed45f06b6196cf2b1c28": {"ta_keywords": "imaging acute abdomen;acute abdomen;associated acute abdomen;acute abdomen clinico;abdomen clinico radiologic;acute abdomen providing;diagnostic imaging acute;imaging acute;abdomen clinico;abdomen;abdomen providing good;diagnostic imaging;abdomen providing;radiographic sonographic computed;computed tomography findings;radiographic sonographic;analysis radiographic sonographic;tomography findings;aspects diagnostic imaging;tomography findings diseases;sonographic computed;sonographic computed tomography;computed tomography;radiographic;analysis radiographic;diseases associated acute;clinico radiologic approach;clinico radiologic;diagnostic;tomography", "pdf_keywords": ""}, "46d87d4614d9353f1b7d527333073ef9109bfaea": {"ta_keywords": "rankings item labelings;label item ranking;item labelings superior;item ranking users;labelings superior accuracy;item ranking;users pick label;pick correct labels;ranking users based;label set candidates;user rankings item;variant hits algorithm;ranking users;user rankings;labelings superior;rankings item;pick label;ranking;labelings;item labelings;simple variant hits;likely label item;users consistent items;produces user rankings;hitsndiffs fast algorithm;pick label set;fast algorithm;labeling;item labeling;algorithmically various discrete", "pdf_keywords": ""}, "82ae0d4b41046ccedb435ece08a61f198cf77bb9": {"ta_keywords": "generating editing sentences;effectively manipulate text;text content manipulation;generate sentence accurately;generation text;editing sentences;generate sentence;game report corpus;editing sentences given;controlled generation text;manipulate text;manipulate text rewriting;content manipulation;aim generate sentence;text portions;generating editing;generation text high;text rewriting;text content;sentences given textual;report corpus;structured content;sentence kobe easily;given textual attributes;textual;deleting text portions;textual attributes sentiment;text;textual attributes;report corpus testbed", "pdf_keywords": "content style generation;hybrid attention copy;text generation style;attention copy;approach attention copy;text generation;stylistic control using;stylistic control;way stylistic control;data text generation;style generation;style generation study;sentences soft templates;style imitation;attention copy mechanism;hybrid attention;style imitation based;generation style imitation;content style;includes hybrid attention;stylistic;content \ufb01delity style;embodiment content style;style control \ufb02exible;new way stylistic;copy mechanism learns;existing sentences soft;learns weak supervisions;attention;style control"}, "bc1832e8b8d4e5edf987e1562b578bd9aa5e18a9": {"ta_keywords": "data selection chime3;mismatch condition training;speech recognizer;selection chime3 test;data augmentation;robustness speech recognizer;speech recognizer deployed;data augmentation simple;data selection sequence;acoustic conditions data;selection chime3;chime3 test set;proposed data selection;data selection;mismatched training test;condition training configurations;similarity acoustic conditions;conditions data selection;training test conditions;chime3 test;improve robustness speech;neural network mismatch;respect similarity acoustic;similarity acoustic;robustness speech;mismatched training;condition training;deployed mismatched training;selecting data respect;recognizer", "pdf_keywords": ""}, "e212f788c701370af02b138d2a61e180cddfb138": {"ta_keywords": "multi target translation;translating single source;simultaneously translating single;language multiple target;simultaneously translating;target translation;source language multiple;single source language;t2 disambiguate translations;disambiguate translations t1;translating single;method simultaneously translating;disambiguate translations;multiple target languages;grammars handle multiple;target translation expand;context free grammars;free grammars;language multiple;translations;translations t1 t2;free grammars handle;target languages t1;translation expand formalism;translation results specific;model t1 translations;target languages;weak language model;translation results;translations t1", "pdf_keywords": ""}, "c5dfc5fe7102fd8647edd1c9483aded82557e544": {"ta_keywords": "recourse summaries predictive;recourses interpretability explanations;recourses interpretability;actionable recourse summaries;generating recourse summaries;subpopulations generating recourse;global counterfactual explanations;correctness recourses interpretability;optimizes correctness recourses;recourses defined subpopulations;algorithms provide recourses;minimizing overall recourse;accurate summary recourses;recourses entire population;generating recourse;guarantees recourse correctness;recourse summaries;summary recourses;counterfactual explanations;counterfactual explanations provide;actionable recourse;summary recourses entire;recourse correctness;correctness recourses;interpretability explanations minimizing;recourses offers meaningful;provide recourses;recourses defined;guarantees recourse;optimality guarantees recourse", "pdf_keywords": "individualized recourse interpretable;recourses interpretability explanations;global counterfactual explanations;actionable recourse summaries;summaries actionable recourses;recourse interpretable;recourses interpretability;accurate summary recourses;counterfactual explanations provide;counterfactual explanations;recourse interpretable interactive;correctness recourses interpretability;actionable recourses;summary recourses;individualized recourse;summary recourses entire;actionable recourse;learn global counterfactual;recourse summaries;recourses entire population;recourses corresponding black;overview recourses;called actionable recourse;explanations provide interpretable;overview recourses corresponding;interpretability explanations minimizing;recourses;optimizes correctness recourses;algorithms provide recourses;minimizing overall recourse"}, "90ed32fa521b9e85f1c9efe356619814a2e79961": {"ta_keywords": "compiling prior knowledge;knowledge explicit basis;prior knowledge explicit;prior knowledge;knowledge explicit;compiling prior;explicit basis;prior;knowledge;basis;explicit;compiling", "pdf_keywords": ""}, "a75869d69cc86f501939c237ae4711aa2885f6a6": {"ta_keywords": "translation meta learning;low resource translation;resource translation meta;multilingual transfer learning;translation meta;neural machine translation;outperforms multilingual transfer;multilingual transfer;resource translation;multilingual high resource;machine translation;machine translation nmt;low resource languages;meta learning;high resource language;agnostic meta learning;proposed meta learning;meta learning strategy;resource language tasks;meta learning problem;tasks diverse languages;language tasks;meta learning algorithm;low resource neural;translation nmt proposed;significantly outperforms multilingual;diverse languages ro;transfer learning;outperforms multilingual;translation nmt", "pdf_keywords": "meta learning strategy;meta learning;proposed meta learning;meta learning approach;agnostic meta learning;multilingual transfer learning;propose meta learning;meta learning algorithm;neural machine translation;outperforms multilingual transfer;meta learned;machine translation nmt;outperforms multilingual translation;meta learned parameter;transfer learning;test meta learned;multilingual transfer;machine translation;transfer learning based;low resource neural;translation approach target;lowresource neural machine;nmt propose meta;model agnostic meta;translation nmt;multilingual nmt;multilingual translation approach;multilingual nmt propose;target language pairs;target language"}, "18e70ad07561cf09a2d7f0da992a0e87a5e5c0a8": {"ta_keywords": "topic tracking language;tracking language model;topic tracking;tracking language;model speech recognition;language model speech;language model;speech recognition;model speech;speech;language;topic;tracking;recognition;model", "pdf_keywords": ""}, "8cebfae7cd436241eb5c3442e687a913a75a5531": {"ta_keywords": "word sense induction;sense induction russian;sense induction disambiguation;disambiguation methods slavic;induction russian language;russian language participants;slavic language shares;methods slavic language;word sense;language shares features;features slavic languages;sense induction;slavic language;language participants;induction disambiguation methods;slavic languages;based sense embeddings;shares features slavic;language shares;sense embeddings;senses word bank;slavic languages rich;methods slavic;russian language;languages rich morphology;language participants asked;task word sense;disambiguation methods;languages explore;language russe 2018", "pdf_keywords": "sense induction slavic;sense induction russian;sense induction disambiguation;word sense induction;datasets word sense;disambiguation russian language;sense annotated contexts;induction slavic language;induction disambiguation russian;sense annotated datasets;induction russian language1;annotated contexts sense;disambiguation russian;computational linguistics;sense annotated;novel sense annotated;slavic language;russian language1;russian language1 panchenko;contexts sense inventories;language1 panchenko;induction slavic;slavic language second;sense induction;russian language;annotated contexts;russian language created;linguistics;word sense;computational linguistics intellectual"}, "c6bb04f3d8000b7e800f6359082de39548c7da79": {"ta_keywords": "adding locality information;locality information models;locality features contribute;text demonstrate locality;locality structure;utilizing structural locality;locality features improve;analysis locality features;locality information;contextual similarity metrics;locality features;adding locality;demonstrate locality features;approach adding locality;locality non parametric;used contextual similarity;locality structure paper;structural locality;locality;contextual similarity;topical clusters text;analysis locality;parametric language models;demonstrate locality;topical clusters;code repositories experiments;source code wikipedia;language models generate;language models;external source examples", "pdf_keywords": "utilizing structural locality;abstract structural locality;structural locality ubiquitous;incorporating structural locality;locality non parametric;differences structural locality;locality ubiquitous;structural locality;capturing structural locality;structural locality non;locality strictly nested;locality;locality affects lms;locality strictly;structural locality de\ufb01ne;structural locality affects;locality non;locality ubiquitous feature;parametric language models;levels structural locality;language domain empirically;non parametric language;locality de\ufb01ne;2017 treated locality;locality affects;language models;treated locality strictly;treated locality;language models generate;locality de\ufb01ne functions"}, "97846070369f66c3080a0803be58e96963dec581": {"ta_keywords": "website usage twitter;pandemic used twitter;usage twitter covid;twitter covid;conspiracy theory websites;clustering urls used;urls used covid;url usage study;usage patterns tweet;twitter covid 19;clusters url usage;view clustering urls;usage twitter;cluster websites urls;clustering urls;twitter;political biases information;used twitter;clusters url;main clusters url;urls based usage;news conspiracy theory;websites urls based;urls based;cluster websites;website usage;misinformation covid;include misinformation covid;url usage;news conspiracy", "pdf_keywords": ""}, "08f6819e66318cd49cddefd5d690a752d1098da7": {"ta_keywords": "conceptualize claims;argument mining popular;argument mining;argument argument mining;appear conceptualize claims;conceptualize claims quite;conceptualization claims;systems identify claims;divergent conceptualization claims;conceptualizations claim;identify claims;conceptualization claims different;claim claims;claims different datasets;different conceptualizations claim;claim practical applications;domain classification shared;cross domain classification;claims cross domain;conceptualizations claim practical;domain classification;research area nlp;identify claims cross;argument argument;claim practical;claims;claims cross;essence claim claims;claim claims central;nlp", "pdf_keywords": "datasets argument mining;argument mining datasets;existing argument mining;argument mining;argument mining popular;abstract argument mining;argument mining showing;conceptualizations claims datasets;showing conceptualizations claims;conceptualizations claims;useful claim identi\ufb01cation;argument including;claims datasets;claims fundamental;domain claim identi\ufb01cation;claims fundamental component;differences conceptualizations claims;argument existing;domain claim;concept claims;nlp propose computational;claims datasets differ;datasets model claims;argument existing argument;mining showing conceptualizations;cross domain claim;concept claims key;claims datasets shared;claim identi\ufb01cation heterogeneous;argument including systems"}, "9f73c3f86026c21d0e5e55c70462952c6ada1175": {"ta_keywords": "accelerating deep learning;dnns prioritizing examples;training deep;deep learning;approach selective backprop;deep learning focusing;accelerates training deep;training deep neural;networks dnns prioritizing;dnns prioritizing;selective backprop;importance sampling;iteration accelerating deep;selective backprop converges;deep neural;introduces selective backprop;deep neural networks;prioritizing examples high;selective backprop technique;prioritizing examples;selective backprop uses;faster standard sgd;backprop;shows selective backprop;training example forward;backprop technique accelerates;backprop converges;backprop converges target;backprop technique;high loss iteration", "pdf_keywords": "importance sampling;sampling techniques deep;deep learning measurements;importance sampling approach;training selective backprop;deep learning;learning rate;additional learning rate;evaluation selective backprop;practical effective sampling;backprop effect training;provided learning rate;art importance sampling;effective sampling;techniques deep learning;target accuracy;selective backprop stale;achieve target accuracy;training selective;training times target;learning rate schedules;learning measurements;target error rates;evaluate selective backprop;selective backprop;results additional learning;learning rate schedule;accuracies cifar10;effective sampling techniques;reduce training times"}, "dc984ea8be018a0244b40468d13f7b734ab55bac": {"ta_keywords": "neural machine translation;translation lexicons neural;machine translation nmt;probability word translation;translation lexicons efficiently;machine translation;discrete translation lexicons;translation lexicons;attention vector nmt;incorporating discrete translation;translation nmt;word lexical probabilities;lexicon probability word;lexicons neural machine;lexicon probability;calculate lexicon probability;lexical probabilities model;discrete translation;translation nmt makes;word translation candidate;lexical probabilities;systems discrete translation;efficiently encode translations;translations low;lexicons neural;word translation;encode translations low;lexicons efficiently encode;lexicons efficiently;source word lexical", "pdf_keywords": "lexical translation probabilities;translation probabilities predictive;transform lexical translation;word alignment methods;discrete translation lexicons;lexical probabilities model;word lexical probabilities;probability word translation;construct lexicon probabilities;probabilistic lexicons;probabilistic lexicons additional;lexicon probability;lexicon probabilities;translation lexicons;calculate lexicon probability;discrete probabilistic lexicons;lexical probabilities;lexicon probabilities using;attentional nmt models;translation probabilities;lexical translation;transform lexical;translation lexicons ef\ufb01ciently;traditional word alignment;lexicon probability word;attention vector nmt;alignment methods training;vectors attentional nmt;interpolation nmt probabilities;word alignment"}, "0533ccdc4840eed0fe1769b5e78da912631be609": {"ta_keywords": "solitons nonlinear optical;optical solitons;optical solitons formed;overview optical solitons;optical solitons overview;solitons nonlinear;solitons modulational instability;solitons overview optical;nonlinear fiber optics;elements optical solitons;solitons modulational;nonlinear optical;fiber optics bose;solitons;instability nonlinear fiber;research include solitons;existence solitons nonlinear;solitons formed;nonlinear optical bres;solitons overview;nonlinear fiber;include solitons modulational;nonlinear schr\u00e4odinger;optics bose einstein;introduction optical soiltons;optical soiltons applications;solitons formed balance;optics bose;modulational instability nonlinear;existence solitons", "pdf_keywords": ""}, "4b73f4956c31cd10994c73b21e2c38a60a68d03e": {"ta_keywords": "allocating papers referees;paper assignment problem;assignment problem fundamental;algorithms assignment problem;assignment problem sided;allocating papers;solving assignment problem;weighted averages assign;assignment problem;allocation;problem allocating papers;assign indivisible goods;assignment problem assignment;problem assignment problem;conference paper assignment;assignment problem using;algorithms assignment;papers referees conference;academic problem allocating;papers referees;constraints conference paper;reviewers objects papers;use order weighted;averages assign indivisible;preferences agents reviewers;averages assign;weighted averages owas;paper assignment;weighted averages;problem preferences agents", "pdf_keywords": "maximal assignment computationally;assignment computationally ef\ufb01cient;maximal assignment;assignment computationally;assignment polynomial time;allocation owa assignment;owa assignment polynomial;rank maximal assignment;assignment conclusions proposed;allocation owa;assignment polynomial;owa assignment conclusions;assignment owa assignment;egalitarian assignment owa;np hardness \ufb01nding;assignment owa;notion allocation owa;np hardness;algorithm \ufb01nding owa;assignment conclusions;assignment setting;\ufb01nding owa assignment;allocation;owa vectors assignment;assignment setting de\ufb01ne;utility maximizing rank;\ufb01nding egalitarian assignment;owa assignment using;owa assignment;egalitarian assignment"}, "b131cf78363993e4126b2562a156bd9d046c8bc4": {"ta_keywords": "tree based translation;pivot syntactic matching;pivot language words;pivot translation useful;subtrees pivot language;tree pivot syntactic;syntactic subtrees pivot;methods pivot translation;based translation models;approach pivot translation;triangulation pivot translation;pivot translation;pivot target translation;translation models combines;language distinguish pivot;distinguish pivot language;pivot translation used;translation models;pivot language distinguish;english tree pivot;target translation models;triangulation using syntactic;pivot syntactic;method translating languages;pivot language;nations parallel corpus;parallel corpus proposed;translating languages;syntactic matching methods;syntactic matching", "pdf_keywords": ""}, "18e5fb8cec55a75b288a499c57d77ede541dc049": {"ta_keywords": "question answering benchmarks;answering commonsense tasks;question answering commonsense;shot question answering;language models learn;question answering;commonsense tasks guided;commonsense tasks;language models training;commonsense question answering;tasks global knowledge;benchmarks individual knowledge;answering commonsense;accuracy commonsense;help language models;answering benchmarks;general semantic reasoning;pre trained neural;tasks learning;trained neural language;neural language modeling;answering benchmarks individual;structure task generating;pre training models;global knowledge graph;tasks learning utilize;task generating;specific tasks learning;knowledge graph better;perform general semantic", "pdf_keywords": ""}, "5b1bb1f6ed091dfd53adf7ebbcda2c48a3b67c2c": {"ta_keywords": "semantic frame induction;unsupervised semantic frame;task unsupervised semantic;unsupervised frame induction;word context embeddings;embeddings syntactical features;combining embeddings syntactical;embeddings role labeling;unsupervised semantic;embeddings syntactical;context embeddings;contextualized word embeddings;task unsupervised frame;semantic frame;context embeddings role;present semantic frame;induction using contextualized;frame induction using;word embeddings;frame induction;word embeddings approach;labeling combining embeddings;frame induction showed;frame induction qasem;verb clustering;verb clustering using;syntactical features;role labeling;role labeling combining;subtask semeval 2019", "pdf_keywords": "labeling verb roles;embeddings grouping verbs;verb clustering role;grouping verbs frame;embeddings disambiguate verb;semantic frame role;verb roles;unsupervised semantic frame;distributional word representations;word context embeddings;word representations embeddings;embeddings enhanced syntactical;unsupervised semantic;word representations;embeddings unsupervised task;verb clustering;verb roles 1hhmm;verbs frame type;verbs frame slots;frame role induction;grouping verbs;verbs frame;approach unsupervised semantic;semantic frame;trained embeddings disambiguate;context embeddings unsupervised;clustering arguments verbs;context embeddings;enhanced syntactical features;clustering role labelling"}, "384bf224d91a1691c9e6384201483121e2e7ddab": {"ta_keywords": "exact subspace clustering;subspace clustering;clustering subspace clustering;subspace clustering algorithmic;subspace clustering celebrated;limits subspace clustering;clustering subspace;subspace clustering subspace;clustering information theoretic;face clustering information;clustering information;face clustering;clustering celebrated problem;clustering;community recovery hypergraphs;segmentation face clustering;required reliable clustering;clustering algorithmic aspect;reliable clustering;hypergraphs characterize;clustering algorithmic;reliable clustering unknown;hypergraphs;clustering unknown;clustering celebrated;recovery hypergraphs characterize;recovery hypergraphs;segmentation;motion segmentation face;motion segmentation", "pdf_keywords": ""}, "e01aa6f8ce625469b6f161d7ab9e61a60ac33798": {"ta_keywords": "online streaming codes;streaming codes communicating;streaming codes;streaming codes need;rate streaming codes;streaming codes fact;setting streaming codes;messages streaming codes;latency streaming communication;streaming communication;offline rate streaming;streaming codes variable;streaming codes class;offline coding schemes;streaming communication settings;practice streaming codes;rate streaming;low latency streaming;codes communicating burst;latency streaming;online streaming;streaming;size messages streaming;streaming previously;setting streaming;messages streaming;offline coding;video streaming;live video streaming;streaming previously studied", "pdf_keywords": "streaming codes communicating;streaming codes;streaming codes fact;online streaming codes;codes communicating burst;communicating burst packet;channel packets rates;\ufb01rst channel packets;burst packet loss;packet loss channel;message size sequences;transmitted \ufb01rst channel;burst packet;channel packets;packets rates;communicating burst;message size sequence;of\ufb02ine coding scheme;packets rates rt;rt message size;online streaming;streaming;packet loss;of\ufb02ine coding;\ufb01rst channel;loss channel;introduce of\ufb02ine coding;loss channel introduce;packets;message size"}, "6b7004138ee2de5ec52e500cae4e65390e961e16": {"ta_keywords": "kernel clustering regularization;regularization kernel clustering;partitioning kernel clustering;kernel spectral clustering;bounds kernel clustering;kernel clustering;kernel clustering criteria;kernel clustering work;kernel cut algorithm;optimization kernel cut;cuts kernel spectral;data partitioning kernel;partitioning kernel;clustering meet regularization;clustering regularization;spectral clustering;regularization kernel;kernel cuts;regularization based segmentation;clustering regularization based;spectral clustering meet;kernel spectral bounds;cuts kernel;kernel cut;spectral bounds kernel;bound optimization kernel;explain regularization kernel;kernel cuts kernel;literature kernel cuts;kernel spectral", "pdf_keywords": ""}, "e0236106e51984e4ea6bbbd1fb5ce57abf3e4e5e": {"ta_keywords": "people images masks;wearing face mask;face mask;mask maintaining social;wearing mask;images masks;mask maintain social;images masks collected;face mask maintaining;mask maintaining;masks;wearing mask maintain;mask;person wearing mask;masks collected;distance reduces risk;using deep;wearing face;mask maintain;covid 19 pandemic;deep learning techniques;pandemic;using deep learning;governments pandemic;virus covid;deep learning;19 pandemic;techniques create safe;covid;covid 19", "pdf_keywords": ""}, "af92dd61340808f3008a84ae57803bb4aa57d03b": {"ta_keywords": "avatar dyadic conversations;dynamics avatar speech;avatar speech body;speech body pose;pose non verbal;predicting avatar pose;model dyadic conversational;intrapersonal dynamics avatar;forecasting personalized avatar;pose forecasting personalized;personalized avatar dyadic;dyadic residual attention;dynamics predicting avatar;predicting avatar;avatar speech;dyadic conversational data;visual pose forecasting;pose audio;attention model;pose audio participants;avatar pose;human operating avatar;body pose interlocutor;avatar pose non;audio body pose;attention generate sequences;pose conditioned audio;behaviours gestures facial;attention monadic dyadic;dyadic conversations creating", "pdf_keywords": "body pose conversation;pose conversation;pose conversation setting;predicting avatar pose;audio body pose;pose conditioned audio;pose audio;model dyadic conversational;pose audio participants;dyadic residual attention;body pose conditioned;dynamics predicting avatar;body pose interlocutor;generating body pose;body pose natural;consisting pose audio;pose natural;predicting avatar;attention model;attention generate sequences;pose interlocutor audio;sequences body pose;generated body pose;pose;attention generate;avatar pose;selective attention generate;human operating avatar;pose natural models;body pose"}, "5b1c0152bbb12ece2a8817c727e33e6d5c503065": {"ta_keywords": "distributed machine learning;speeding distributed machine;distributed machine;large scale computing;learning algorithms coding;distributed;speeding distributed;workers coded shuffling;codes data shuffling;coded shuffling;shuffling reduces communication;coded shuffling reduces;shuffling use codes;reduce communication bottlenecks;noise speeding distributed;algorithms coding theoretic;codes reduce communication;data shuffling;learning algorithms widely;optimal coded matrix;faster uncoded matrix;algorithms coding;uncoded shuffling;data shuffling use;coded matrix multiplication;communication bottlenecks;view distributed machine;algorithms widely run;learning using codes;scale computing", "pdf_keywords": ""}, "c395595cf7be23f7d90cbca98d8c7861ebfd884d": {"ta_keywords": "stable opinions annotator;comment toxicity task;annotator crowdsourced dataset;learning classification metrics;opinions annotator;learning classifiers human;facing machine learning;comment toxicity misinformation;annotator crowdsourced;classifiers human;multi annotator crowdsourced;opinions noise estimating;performance comment toxicity;tasks comment toxicity;comment toxicity;classifiers human facing;classifiers;classification metrics;machine learning tasks;opinions annotator paper;crowdsourced dataset;machine learning classifiers;learning classifiers;highly metrics roc;classification metrics values;stable opinions noise;metrics roc auc;metrics roc;today metrics roc;crowdsourced", "pdf_keywords": "disagreement deconvolution existing;disagreement deconvolution bringing;applying disagreement deconvolution;disagreement deconvolution;facing machine learning;learning classifiers human;comment toxicity misinformation;misinformation score highly;machine learning performance;classifiers human;comment toxicity task;deconvolution existing social;learning performance metrics;machine learning tasks;tasks comment toxicity;social computing datasets;bringing machine learning;classifiers human facing;performance comment toxicity;toxicity misinformation score;highly metrics roc;comment toxicity;classifiers;misinformation score;methodologies machine learning;machine learning classifiers;learning classifiers;metrics roc auc;abstract machine learning;metrics roc"}, "37074b2b9cebd89e4a92d20f41eec7360e11fe5a": {"ta_keywords": "streaming nar speech;nar speech recognition;speech recognition asr;speech recognition;online asr recognition;automatic speech recognition;end streaming nar;streaming nar;speech recognition combining;automatic speech;recognition low latency;asr recognition;recognition asr;asr recognition low;temporal classification maskpredict;improves online asr;speech processing;attention speech processing;nar speech;wait completion speech;utterance limits applications;completion speech utterance;based automatic speech;autoregressive nar modeling;recognition asr structure;blockwise streaming;mask ctc nar;connectionist temporal classification;autoregressive nar;inference input audio", "pdf_keywords": "streaming nar speech;nar speech recognition;e2e streaming nar;proposed streaming nar;end streaming nar;streaming nar;streaming nar model;ctc streaming decoding;nar speech;utterance latency decoding;speech recognition;streaming decoding;decoding low utterance;speech recognition combining;mask ctc streaming;mask ctc nar;ctc proposed streaming;streaming decoding low;end end streaming;context decoding outperforms;ctc streaming;e2e streaming;ctc nar compared;nar model;end streaming;novel e2e streaming;nar model achieves;low utterance latency;ctc nar;latency decoding"}, "bd6018632a360cb567da8e50e1717ff526503845": {"ta_keywords": "beam search stochastic;stochastic beam search;search stochastic;sequence generation tasks;sequence generation;generation tasks nlp;sequence models;search stochastic process;tasks nlp experiments;nlp experiments;model beam search;decoding strategy sequence;nlp experiments observe;stochastic beam;efficient estimators sbs;poisson stochastic beam;sample candidates replacement;beam search taking;beam search;sets sequence models;sequence models view;iteration sample candidates;improvements high entropy;search taking;turning beam search;sample candidates;tasks nlp;strategy sequence generation;conditional poisson sampling;estimators sample diverse", "pdf_keywords": "decoding algorithms sequence;decoding algorithm stochastic;algorithms sequence models;beam based decoding;expectations sequence models;sequence models solution;decoding algorithms;sequence models;sets sequence models;mainstay decoding algorithm;beam search;based decoding algorithms;modi\ufb01cation beam search;decoding algorithm;beam search table;sequence models derive;turning beam search;estimators expectations sequence;decoding;based decoding;mainstay decoding;build consistent estimators;algorithm stochastic;algorithms sequence;expectations sequence;estimator inclusion probabilities;diverse sets sequence;algorithm stochastic process;turn mainstay decoding;construct statistical estimators"}, "6494cd26511c076186673c9a636d21d1dfed8d5a": {"ta_keywords": "input speech enhancement;speech enhancement;features input speech;speech enhancement implicitly;speech enhancement essential;confirmed speech enhancement;technique improving asr;input teacher network;distant talking asr;training data enhanced;talking asr systems;noisy signals training;enhanced inputs teacher;enhanced features student;enhancement techniques training;features teacher model;signals training;improving asr;better enhanced inputs;noisy features input;beamformed features teacher;asr improvements;significant asr improvements;input speech;talking asr;better network teacher;features student teacher;model distant talking;mimic teacher network;improving asr performance", "pdf_keywords": ""}, "c8f9313ce8416a7be079935d1cbb637705f75182": {"ta_keywords": "quality translation individuality;translation individuality text;improvements translation dictionary;translation individuality;individuality using translation;translation dictionary language;vocabulary translation dictionary;translation dictionary method;construction translation dictionary;translation model;translation dictionary using;translation dictionary;using translation dictionary;individuality text speech;improvements quality translation;estimation translation model;transforming individuality spoken;translation dictionary limited;thesaurus gram statistics;individuality spoken language;translation model probabilities;vocabulary translation;individuality writer speaker;automatic construction translation;method vocabulary translation;dictionary language model;vocabulary words transformed;quality translation;model improvements translation;gram similarity", "pdf_keywords": ""}, "8c7628641450203b0aa959b5a69729ff906760ff": {"ta_keywords": "speaker diarization eend;speech activity detector;external speech activity;speaker diarization;approach speaker diarization;end neural diarization;results external speech;cascaded approach speaker;external speech;neural diarization eend;neural diarization;speech activity;estimated diarization;diarization eend;diarization eend methods;eend encoder decoder;eda eend encoder;decoder based attractor;decoder based attractors;estimated diarization results;eend encoder;introduce encoder;approach speaker;introduce encoder decoder;encoder decoder;encoder decoder based;encoder;end neural;end end neural;diarization eend method", "pdf_keywords": "speakerwise attractors calculated;speakerwise attractors;sequence speakerwise attractors;end speaker diarization;encoderdecoder based attractor;speaker diarization method;speakers using encoderdecoder;speaker diarization;attractors calculated embeddings;decoder based attractor;embeddings attractors;acoustic feature sequence;feature sequence speakerwise;input acoustic feature;attractor calculation;attractors calculated;attractor calculation module;based attractor calculation;end end speaker;end speaker;product embeddings attractors;numbers speakers using;calculated input acoustic;input acoustic;encoder decoder based;encoderdecoder based;based attractor;unknown numbers speakers;introduce encoder;attractor"}, "5aea95e1ae78a66474051a330ded374e199b658c": {"ta_keywords": "representation learning graphs;learning graphs;bioinformatics citation networks;learning graphs follow;citation networks demonstrate;architecture jumping knowledge;citation networks;follow neighborhood aggregation;networks;graphs follow neighborhood;neighborhood aggregation;networks demonstrate;jumping knowledge;graph structure analogous;structure aware representation;networks flexibly;networks demonstrate model;graph structure;knowledge jk networks;jumping knowledge jk;neighboring nodes;explore architecture jumping;nodes node representation;jk networks flexibly;node representation;node different neighborhood;social bioinformatics citation;jk networks;nodes;neighboring nodes node", "pdf_keywords": "graph attention networks;graph attention;graph convolutional networks;graphsage graph attention;convolutional networks graphsage;bioinformatics citation networks;like graph convolutional;citation networks;subgraph structures gcn;graph structure analogous;citation networks demonstrate;graph convolutional;models like graph;subgraph structures;networks consistently improves;networks graphsage;structure aware representations;graph structure;networks;networks graphsage graph;like graph;demonstrate subgraph structures;knowledge jk networks;networks consistently;subgraph;networks demonstrate model;structure aware representation;networks demonstrate;aware representations;graphsage"}, "564dec6eab6115ecd604f22738ce0b47777f6e17": {"ta_keywords": "modeling speech classification;robustly classified speech;word accuracies vbec;vbec total bayesian;speech recognition;speech classification;speech classification based;robust speech classification;accuracies vbec functions;continuous speech recognition;speech recognition experiments;acoustic modeling speech;bayesian framework speech;modeling speech;accuracies vbec;enables robust speech;vbec functions;speech recognition procedures;advantages vbec;functions vbec;framework speech recognition;robust speech;vbec functions vbec;vbec;vbec allows;vbec functions confirmed;classified speech totally;advantages vbec allows;acoustic models robustly;classified speech", "pdf_keywords": ""}, "8c5465eb110d0cab951ca6858a0d51ae759d2f9c": {"ta_keywords": "text classifier learns;learns words rationale;classifier learns words;text classifiers;text classifiers make;classifier learns;gradient based training;focus text classifiers;text classifier;input text classifier;text selection rate;binary selections gradient;selections gradient based;selections sparsity inducing;learns words;jointly training neural;training neural;short informative input;text selection;regularisation short informative;specified text selection;classifiers;binary latent masks;short selections sparsity;selections gradient;promote short selections;informative input text;short selections;neural;rate focus text", "pdf_keywords": "selections gradient based;gradient based training;binary selections gradient;selections gradient;selectors gradient estimation;text selection rate;discrete selectors gradient;estimation possible reinforce;jointly training neural;text selection;selectors gradient;gradient estimation;training neural;gradient estimation possible;training reinforce approach;rate selected input;prespeci\ufb01ed text selection;gradient based;rate selected;jointly training;selection rate;selection rate propose;latent model selects;training reinforce;neural network models;selected input text;based training reinforce;neural;possible reinforce instead;possible reinforce"}, "2660dbba723573266edb2a0a4929e6847ae83212": {"ta_keywords": "speaker adaptation rnn;adaptation rnn;adaptation rnn ts;vector speaker adaptation;speaker adaptation;speaker adaptation language;techniques rnn transducers;changes speaker adaptation;rnn transducers rnn;techniques rnn;rnn transducers;transducers rnn;adaptation language model;transducers rnn ts;rnn;adaptation language;architectural changes speaker;rnn ts;language model fusion;encoder prediction network;vector speaker;applicability vector speaker;rnn ts instrumental;conversational italian 900;encoder prediction;set techniques rnn;integration encoder prediction;prediction network vectors;prediction network;components training", "pdf_keywords": "prediction network embeddings;encoder prediction network;combine encoder prediction;language model fusion;external language model;encoder prediction;model regularization speaker;speaker adaptation;integration encoder prediction;regularization speaker adaptation;hours corpus;300 hours corpus;adaptation external language;language model;speaker adaptation external;network embeddings;encoder;embeddings using multiplicative;network embeddings using;hours corpus second;excellent recognition performance;corpus;prediction network;network combine encoder;regularization speaker;architecture character outputs;prediction network vectors;combine encoder;embeddings using;embeddings"}, "94f22d7a8b48784b3d8975616e20d8028a08162f": {"ta_keywords": "institute mathematics mechanics;russian academy sciences;goldstein institute mathematics;mathematics mechanics ural;sciences yekaterinburg russian;russian academy;mathematics mechanics;goldstein institute;branch russian academy;institute mathematics;mechanics ural;russian federation;mechanics;goldstein;yekaterinburg russian federation;ural branch russian;mechanics ural branch;branch russian;mathematics;academy sciences yekaterinburg;yekaterinburg russian;russian;sciences yekaterinburg;academy sciences;institute;sciences;ural branch;federation;academy;ural", "pdf_keywords": ""}, "d119cc4051ed1206a0dac963cd23a84acf77fea7": {"ta_keywords": "threshold calibrated forecaster;predicted accurately threshold;prediction decision makers;uncalibrated forecaster;calibrated forecaster;accurately threshold decisions;forecasts predict loss;calibrated forecaster procedure;cutoff predicted accurately;probabilistic forecasts predict;takes uncalibrated forecaster;decision loss prediction;decision threshold loss;loss predicted accurately;decision loss predicted;loss threshold decisions;decisions based forecasted;forecasts predict;prediction decision;forecaster input provably;loss prediction decision;accurate decision loss;rely probabilistic forecasts;outputs threshold calibrated;loss threshold decision;provably outputs threshold;estimate loss threshold;forecaster procedure allows;predicted accurately;predict loss", "pdf_keywords": "threshold calibrated forecaster;forecaster establish threshold;decision threshold loss;threshold calibration outperforms;threshold decision losses;loss estimation threshold;decision losses threshold;forecaster decision loss;threshold loss datasets;provably outputs threshold;losses threshold decision;threshold loss;estimation threshold decision;threshold loss functions;empirically threshold calibration;loss conclusion threshold;accurate decision loss;threshold decision threshold;threshold calibration minimizes;reliability gap threshold;decision threshold;losses threshold;loss datasets forecaster;outputs threshold calibrated;uncalibrated forecaster;achieving threshold calibration;threshold calibration necessary;forecaster theoretically guarantees;threshold calibration practical;calibrated forecaster theoretically"}, "6a9795853e5f39325deb0d916fe22d9e5a202a9f": {"ta_keywords": "printing milton areopagitica;pamphlet clandestine printers;milton areopagitica 1644;printing milton;pamphlets 1640s article;pamphlets 1640s;areopagitica london printers;100 pamphlets 1640s;abstract milton areopagitica;press pamphlet clandestine;milton areopagitica london;freedom press pamphlet;milton areopagitica;clandestine printers;pamphlets;clandestine printers successfully;press pamphlet;pamphlet clandestine;london printers matthew;areopagitica 1644;1644 significant texts;pa abstract milton;100 pamphlets;printers matthew;attributes printing milton;pamphlet;pieces 100 pamphlets;london printers;abstract milton;texts history freedom", "pdf_keywords": ""}, "5270b626feb66c8c363e93ba6608daae93c5003b": {"ta_keywords": "knowledge transformer;factual knowledge transformer;memorization generalization transformers;knowledge transformer models;memorization knowledge modification;transformer based language;memorization knowledge;facts memorize new;improving memorization;language models;knowledge modification;knowledge modifications;effective knowledge modifications;encoding factual knowledge;improving memorization generalization;memorize new;transformers forget specific;tasks improving memorization;facts memorize;memorization;knowledge modifications work;memorize new ones;transformer models ensuring;make transformers forget;updating stale knowledge;generalization transformers;language models shown;generalization transformers widely;memorization generalization;knowledge modification task", "pdf_keywords": "knowledge memorized transformers;factual knowledge transformer;knowledge transformer;knowledge transformer models;knowledge transformer model;transformer models ensuring;memorized transformers;memorized transformers section;transformer models;modify factual knowledge;factual knowledge benchmarked;modify knowledge memorized;knowledge modi\ufb01cation constrained;transformer model;transformer model desired;factual knowledge;way modify knowledge;unmodi\ufb01ed factual knowledge;knowledge benchmarked;modify knowledge;knowledge benchmarked approaches;speci\ufb01c factual knowledge;preserving model performance;transformer;formulate knowledge modi\ufb01cation;modifying implicit knowledge;knowledge memorized;modifying speci\ufb01c factual;formulate knowledge;modify factual"}, "7b99c51d562e33309a46601c846abbe72a65c6a4": {"ta_keywords": "gains nlp tasks;target english classification;nlp tasks abundance;nlp tasks;trained language models;sequence tagging tasks;transfer gains nlp;datasets intermediate training;task fine tuning;english classification;english classification multiple;efficient embedding;gains nlp;answering sequence tagging;target tasks;question answering;efficient embedding based;trained language;demonstrate efficient embedding;language models;pre trained language;question answering sequence;tagging tasks results;tagging tasks;target english;best datasets intermediate;language models infeasible;sequence tagging;fine tuning approaches;11 target english", "pdf_keywords": "transfer learning nlp;learning nlp multiple;learning nlp;transfer learning considerably;sequence tagging tasks;nlp multiple;nlp multiple different;transfer learning;intermediate transfer learning;aspect transfer learning;question answering;ef\ufb01cient embedding based;answering sequence tagging;based transfer learning;nlp;question answering sequence;tagging tasks;sequence tagging;ef\ufb01cient embedding;embedding based methods;target english classi\ufb01cation;11 target tasks;learning considerably;intermediate task data;datasets intermediate training;demonstrate ef\ufb01cient embedding;target tasks best;tasks intermediate transfer;learning considerably parameter;target tasks"}, "c47c8c2527bf2ca8339c342f44db2218a0cbcbbd": {"ta_keywords": "extraction knowledge base;knowledge graph construction;base knowledge graph;knowledge graph;information extraction knowledge;statistical relational learning;constructing knowledge bases;knowledge graph identification;knowledge graph introduce;knowledge base construction;desired knowledge base;knowledge bases;extraction knowledge;probabilistic soft logic;knowledge bases sources;results knowledge graph;knowledge base knowledge;relational learning;present knowledge graph;deriving knowledge text;knowledge base;information extraction;problem knowledge graph;relational learning framework;knowledge text using;statistical relational;solution knowledge graph;extractions information extraction;knowledge text;introduced statistical relational", "pdf_keywords": ""}, "4eb22b488052c430170139c492674aa05512f7bf": {"ta_keywords": "die shape optimization;shape forging obtained;forging optimization given;final forging optimization;shape forging;forging optimization;shaped forging process;shape shaped forging;shape optimization design;shape determined optimization;net shape forging;preform die shape;optimization design carried;shape preform die;shape forging deforming;shaped forging;forging process optimized;shape optimization;optimization given preform;optimization design;die final forging;optimization design variable;die shape determined;final forging used;forging process;determined optimization design;forging used;forging obtained;design variable optimiztion;energy forging process", "pdf_keywords": ""}, "f394c5101d7bfc3d8055f9391a83f7e2395dec4a": {"ta_keywords": "programs parallelized;programs parallelized using;parallel benchmark npb;performance decision trees;annotated openmp parallelization;parallel benchmark;nas parallel benchmark;sequential programs parallelized;benchmark npb code;parallelization;machines decision trees;openmp parallelization;parallelized using;collected program features;parallelized;comparable performance classification;learning algorithm versions;benchmark npb;openmp parallelization directives;decision trees comparable;parallelization produced;decision trees;performance classification;code collected runtime;parallelization directives;program features;boosting using adaboost;trees comparable performance;decision trees investigate;versions nas parallel", "pdf_keywords": ""}, "f41e6c832c9e0d5360b66ee7681d3b1ffd2d9c3d": {"ta_keywords": "learn hierarchical task;hierarchical tasks unified;hierarchical tasks;representation task structures;hierarchical task structure;tasks unified;hierarchical task learning;task learning sub;task learning language;task learning;benchmark task learning;learn hierarchical;hierarchical task;stands hierarchical tasks;90 hierarchical task;tasks unified transformers;task structures;decomposed task learning;task structures enables;task structure;tasks;task learning published;manner learn hierarchical;task structure alfred;architecture decomposed task;goal planning scene;planning scene navigation;planning scene;enables depth understanding;sub goal planning", "pdf_keywords": "learn hierarchical task;benchmark task learning;hierarchical task learning;hierarchical tasks;hierarchical task structure;task learning;task learning sub;hierarchical task;presents hierarchical task;learn hierarchical;hierarchical tasks uni\ufb01ed;decomposed task learning;approach learn hierarchical;task compositions;structure task compositions;task learning published;stands hierarchical tasks;learn hierarchical structure;tasks;tasks uni\ufb01ed transformers;task structure;benchmark task;task learning approach;manner learn hierarchical;architecture decomposed task;task compositions language;hierarchical structure task;task structure alfred;goal planning scene;planning scene"}, "c54ad6e29f3e516eecf0a72bd1f95b80e8617116": {"ta_keywords": "compressive phase retrieval;complexity compressive phase;general compressive phase;compressive phase;low complexity compressive;phasecode low complexity;phase uncertainty sparse;complexity compressive;phase retrieval problem;phase retrieval;approaching phasecode low;phasecode algorithm;approaching phasecode;phasecode algorithm introduced;propose variant phasecode;variant phasecode algorithm;phasecode;capacity approaching phasecode;phasecode low;sparse graph code;variant phasecode;uncertainty sparse complex;general compressive;sparse complex vector;tackle general compressive;compressive;sparse complex;recover global phase;sparse graph;degree sparse", "pdf_keywords": "phasecode based compressive;phasecode algorithm compressive;compressive phase retrieval;layer phase retrieval;phase retrieval layer;layer phasecode based;complexity compressive phase;based phase retrieval;phasecode based;phasecode low complexity;layer phasecode algorithm;layer phasecode;approach compressive phase;approaching phasecode;phasecode algorithm based;phasecode algorithm;phase retrieval;phasecode;compressive phase;propose variant phasecode;phase retrieval problem;phase retrieval inner;phasecode algorithm \ufb01rst;approaching phasecode low;phase retrieval section;different layer phasecode;variant phasecode algorithm;variant phasecode;address compressive phase;phasecode low"}, "044b502e5a00b5eeff1dd078ea03f491ca2c37bf": {"ta_keywords": "speech recognition;speech recognition asr;phoneme recognition task;acoustic language models;phoneme recognition;continuous phoneme recognition;modeling acoustic linguistic;automatic speech recognition;lecture transcription task;automatic speech;methods automatic speech;lecture transcription;discriminative training techniques;decoders baseline systems;discriminative training;computationally efficient decoding;efficient decoding;acoustic language;recognition asr;acoustic linguistic;systems trained discriminative;decoders baseline;trained discriminative training;decoding technique features;mit lecture transcription;transcription task;conventional acoustic language;possible decoding techniques;linguistic aspects recognizers;acoustic linguistic aspects", "pdf_keywords": ""}, "b9057dce43181a30aa3e0435c8ffc4c0b6f8f127": {"ta_keywords": "reasoning inference graphs;automatically generate inference;generate inference graphs;inference graphs automated;supporting inference graphs;argumentation supporting inference;machine reasoning;generate inference;inference graphs support;reasoning inference;inference graphs;defeasible inference task;human reasoning;inference graphs transfer;graphs defeasible inference;help human reasoning;human reasoning human;defeasible inference;reasoning mode reasoning;handcraft argumentation supporting;handcraft argumentation;defeasible reasoning;reasoning help human;supporting inference;reasoning human accuracy;literature handcraft argumentation;reasoning human;defeasible reasoning mode;machine reasoning help;reasoning conclusions overturned", "pdf_keywords": "inference graphs defeasible;inference graphs;inference generated graphs;graphs defeasible inference;using inference graphs;inference graph;inference graph defeasible;graph defeasible reasoning;defeasible inference generated;inference generated;knowledge defeasible reasoning;reasoning effectively construct;defeasible reasoning effectively;concept inference graph;crucial defeasible inference;defeasible reasoning cognitive;defeasible inference;inference scales usability;reasoning cognitive science;graph incorporates contextualizers;useful defeasible reasoning;defeasible reasoning discussion;reasoning cognitive;reasoning effectively;defeasible reasoning;defeasible inference scales;contextualizers mediators hypotheses;inference;idea using inference;defeasible task humans"}, "b143ee344fe3af4169bde8af8b682a2835dae4a4": {"ta_keywords": "task furnmove agents;agents learn collaborate;agent collaboration;multi agent collaboration;agent embodied tasks;multi agent embodied;decentralized action sampling;multi agent;agent collaboration research;agents achieve;expressive joint action;agent embodied;agents complete furnmove;agents work;agents learn;agent time task;centralized agent;agents complete;embodied tasks;autonomous agents learn;novel task furnmove;collaborate cordial;cordial agents achieve;centralized agent time;embodied tasks using;policies cordial agents;single agent;training agents complete;autonomous agents;learn collaborate cordial", "pdf_keywords": "task furnmove agents;agent task furnmove;agents furnmove gridworld;agent task;furniture moving task;multi agent task;planning self organizing;moving task furnmove;novel task furnmove;planning self;model planning self;planning;agents furnmove;agents complete furnmove;moving furniture;task furnmove;experience moving furniture;agents work;performance agents furnmove;multi agent;furnmove agents work;gridworld furnmove;moving task;furnmove gridworld;furnmove gridworld furnmove;actions toussaint learning;agents complete;agents;furnmove agents;furniture moving"}, "c3930cb34241a42e03ed02cbc83a3c87dddd60cc": {"ta_keywords": "generated stories quality;story generation reward;generated stories trained;automatically generated stories;generated stories;learn generate stories;metric story generation;story generation;generate stories;generate stories partial;function story generation;stories partial evaluation;stories quality;signals generated stories;stories quality signals;generate sentence continuation;stories trained scorer;generate sentence;task generate sentence;sentence continuation story;evaluation metric story;generation reward;generation reward function;stories trained;setting sentences story;quality automatically generated;learning learn generate;sentences story;continuation story study;story provided task", "pdf_keywords": ""}, "c9d7b1f9b13d6ea4ff45b908285cc65af959cc5b": {"ta_keywords": "grammar probability associated;probabilities word language;generated grammar probability;probability word language;grammar probability;assigning probability word;word probabilities;define word probabilities;measures abstract languages;languages generated grammar;word probabilities word;probabilities word;languages generated;probability word;word language considered;generated grammar;probability measures abstract;probabilistic;applied languages generated;word language conditions;language considered;probability associated;language conditions;assignment probabilistic;assigning probability;word language;language conditions assignment;probability measures;applying probability measures;languages", "pdf_keywords": ""}, "3050735eb35af3527276aa1952f79eb2483df3f0": {"ta_keywords": "conversation corpus;large conversation corpus;spontaneous dialogue data;conversation corpus new;training utterance representations;corpus spontaneous dialogue;utterance tagging;dialogue data;conversational understanding contextualized;conversational understanding task;dialogue data accurately;conversational understanding;unsupervised training utterance;task conversational understanding;utterance representations;training utterance;objective conversational understanding;speaker intentions emotions;utterance level analysis;resource utterance tagging;spontaneous dialogue;utterance representations large;analysis speaker intentions;core task conversational;speakers intentions emotions;task conversational;low resource utterance;objective conversational;process large conversation;dialogue", "pdf_keywords": ""}, "a556914c1b32372d47a36f2826cbe143ddae95ca": {"ta_keywords": "supervised taxonomy expansion;self supervised taxonomy;attachment prediction task;existing taxonomy expansion;node attachment prediction;taxonomy expansion model;taxonomy expansion;expansion taxonomies;taxonomy expansion taxonomies;taxonomy expansion 11;attachment prediction;expansion taxonomies important;study taxonomy expansion;attachment task learns;supervised taxonomy;supervision existing taxonomy;taxonomies new concept;methods taxonomy expansion;propose self supervised;self supervised;multiview training prediction;expand existing taxonomies;taxonomies new;new concept terms;prediction task anchor;task learns feature;taxonomy expansion problem;prediction study taxonomy;existing taxonomies;feature representations query", "pdf_keywords": ""}, "d85c0032d7bb0bd220eb2df8ba6d2130bc87e79e": {"ta_keywords": "semi supervised training;data joint speech;semi supervised;pseudolabel method eend;present semi supervised;diarization eend;neural diarization eend;supervised training technique;supervised training;diarization eend eend;iterative pseudolabel;pseudolabel;end neural diarization;pseudo labeling;pseudo labeling end;adaptation using labeled;neural diarization;propose iterative pseudolabel;eend requires labeled;pseudo label achieved;using pseudo labeling;labeled unlabeled data;supervised;labeled data;pseudo label;pseudolabel method;joint speech activities;speech activities speaker;relative diarization;iterative pseudolabel method", "pdf_keywords": "unlabeled data eend;adaptation pseudo labeling;supervised adaptation pseudo;semi supervised;semi supervised adaptation;adaptation using labeled;pseudolabel method eend;labeled unlabeled data;results semi supervised;supervised adaptation;iterative pseudo label;supervised;unlabeled data target;pseudo labeling explore;using labeled unlabeled;unlabeled data;using unlabeled data;labeled unlabeled;iterative pseudolabel;pseudo labeling;labeling explore iterative;propose iterative pseudolabel;pseudolabel;unlabeled data analyzed;using labeled;pseudo label achieved;labeling;model using unlabeled;pseudo label method;labeled"}, "571b4425498549c56c0828a824dc453ff6f482fc": {"ta_keywords": "mac protocols mind;mac protocols;design mac protocols;delay optimality throughput;hoc wireless networks;iot finally simulations;medium access control;emerging iot;iot;consider medium access;iot finally;scheduler design mac;protocols slotted;protocols slotted framework;protocols;protocols mind need;hoc wireless;delay optimality;wireless networks;things iot finally;medium access;emerging iot applications;control tdma protocols;ad hoc wireless;protocols mind;iot applications increasing;protocols achieve;macs propose ezmac;tdma protocols;adaptive macs propose", "pdf_keywords": ""}, "0823f2187eeed53be8fd452decf6ed9a6a6cd124": {"ta_keywords": "semantic parsing developments;semantic parsing;semantic parser;semantic parser document;semantic parsing corpus;semantic parser spatial;dialog act tagging;train semantic parser;spacebook referred semantic;referred semantic parser;creation semantic parsing;parsing corpus;language understandingcomponent spacebook;parsing corpus used;parser spatial personal;parsing;parser spatial;natural language understandingcomponent;parsing developments;parser;parsing developments occurred;parser document;act tagging;natural language;phase semantic parsing;prototype natural language;parser document preliminary;semantic;knowledge dialog act;tagging phase semantic", "pdf_keywords": ""}, "5de24203bf98ae7f4c514bc0bd2a310caa47a047": {"ta_keywords": "trains agents;bring trains agents;multi agent reinforcement;agent reinforcement learning;trains agents target;scheduling traffic virtually;agent reinforcement;multi agent;agents need coordinated;trips traffic networks;railway networks;scheduling trips traffic;challenging tasks agents;coordination hundreds agents;scheduling vehicles;scheduling traffic;agents real life;scheduling vehicles disruptions;networks scheduling vehicles;tasks agents need;modern railway networks;tasks agents;recently multi agent;railway network;agents need;traffic networks scheduling;scheduling trips;agents;like railway network;agents real", "pdf_keywords": "reinforcement learning rl;dynamic train scheduling;train scheduling;dynamic train;agent reinforcement learning;multi agent reinforcement;deep reinforcement learning;agent reinforcement;railway networks;train scheduling mohanty;reinforcement learning operations;reinforcement learning introduction;multi agent path;railway network;study dynamic train;reinforcement learning;modern railway networks;agent path;multi agent;railway;train;railway networks operated;vehicle scheduling;railways;deep reinforcement;\ufb01nding deep reinforcement;agent path \ufb01nding;setting like railway;learning rl;problem multi agent"}, "254d1b8cf247ae8b19e017f7ba758d670207ddda": {"ta_keywords": "asr speech processing;networks speech enhancement;speech recognition asr;performance discriminative beamforming;neural networks speech;speech recognition;speech enhancement recognition;speech processing;speech enhancement;discriminative beamforming;discriminative beamforming phase;speech processing systems;automatic speech recognition;predicts optimal beamforming;systems automatic speech;automatic speech;optimal beamforming;networks speech;beamforming phase aware;filtering optimal asr;function asr speech;beamforming;optimal beamforming parameters;recognition asr;optimal asr task;beamforming phase;optimal asr;introduce beamforming;asr speech;beamforming bf network", "pdf_keywords": ""}, "1f5a1e959147e989e12846a5bd1d20234ef667d7": {"ta_keywords": "anticoagulants management bleeding;oral anticoagulants determined;direct oral anticoagulants;oral anticoagulants;oral anticoagulants debated;plasma concentration anticoagulants;anticoagulants management;concentration anticoagulants management;oral anticoagulants appears;concentration anticoagulants;anticoagulants determined;anticoagulants debated results;anticoagulants;anticoagulants debated;management severe bleeding;management bleeding patients;oral anticoagulants creatinine;management bleeding;anticoagulants appears;anticoagulants determined 62;results bleeding gastrointestinal;severe bleeding patients;anticoagulants creatinine clearance;bleeding patients treated;outcomes severe bleeding;anticoagulants creatinine;results bleeding;hospitalized severe bleeding;severe bleeding events;bleeding gastrointestinal", "pdf_keywords": ""}, "148f055083666c72945eea79833a19494f5f57c0": {"ta_keywords": "sparsity synonymy dictionaries;synonymy dictionaries;dictionaries;sparsity synonymy;fighting sparsity synonymy;synonymy;sparsity;fighting sparsity;fighting", "pdf_keywords": ""}, "924ce584acc148be29ef905c228fda7fe552c0c2": {"ta_keywords": "probabilistic logics answering;challenge probabilistic logics;probabilistic logics;shared probabilistic logics;reasoning large knowledge;large knowledge bases;markov logic networks;probabilistic logics reasoning;large knowledge base;learning markov logic;inference learning large;logics reasoning large;learning large knowledge;logic networks;probabilistic language;efficient inference learning;computation personalized pagerank;logics answering queries;knowledge bases;knowledge bases kbs;algorithm efficient inference;efficient inference;logic networks allowing;learning kb inference;order probabilistic language;inference learning;constructing proofs logic;scheme based pagerank;probabilistic language called;logics answering", "pdf_keywords": "markov logic networks;learning kb inference;statistical relational learning;learning markov logic;faster learning markov;stochastic logic programs;parallelized weight learning;parallelize weight learning;learn weights mutually;logic networks;relational learning;faster learning;joint learning kb;relational learning solution;stochastic logic;kb inference;logic networks allowing;learning kb;kb inference leads;magnitude faster learning;inference noisy kb;challenges inference noisy;proveablycorrect approximate grounding;inference leads improvements;recursion joint learning;learn weights;extension stochastic logic;logic programs slp;learning scheme;learning markov"}, "8c4d1e81c277f71cd9e3c9a0af356203c7948dca": {"ta_keywords": "documentation transcription bottlenecks;el documentation transcription;documentation transcription;transcribers work improve;transcription bottleneck transcriber;systems novice transcribers;endangered language documentation;propose novice transcription;transcribers transcriber;asr endangered language;novice transcribers work;transcriber shortage;language documentation empirical;effective human transcribers;transcribers;novice transcription correction;transcribers transcriber shortage;transcribers work;reproducible asr community;transcription correction task;way reproducible asr;novice transcribers;novice transcription;human transcribers transcriber;transcriber;bottleneck transcriber shortage;human transcribers;reproducible asr;asr way reproducible;transcription bottlenecks", "pdf_keywords": "asr el documentation;task asr el;propose novice transcription;end asr efforts;systems novice transcribers;reproducible asr community;improve el documentation;way reproducible asr;systems eschews linguistic;transcribers work improve;eschews linguistic resources;novice transcription correction;asr way reproducible;hinders el documentation;transcription correction task;novice transcribers work;asr el;novice transcription;novice transcribers;transcribers;reproducible asr;transcription bottleneck transcriber;eschews linguistic;transcriber;transcribers work;asr community work;asr systems eschews;task asr;asr efforts;potential task asr"}, "c5ed3d1a2ce418610a6fc9b5520a4f845279969a": {"ta_keywords": "parm prediction serving;parity model neural;predictions using parity;models parm prediction;parm prediction;parity models parm;parity models decodes;parity models new;parity queries;parity models;inference parity queries;queries parity;parity;applicability parity models;performs inference parity;implement parity models;prediction serving;introduce parity models;using parity;using parity models;queries using parity;parm encodes;potential parity models;median parm encodes;implement parity;parity queries using;inference parity;applicability parity;models parm;maintaining median parm", "pdf_keywords": ""}, "657329c633709dd1ac34a30d57341b186b1a47c2": {"ta_keywords": "sparse attention routing;attention sparse routing;attention sparse;self attention sparse;attention routing;sparse attention patterns;sparse attention;sparse attention models;comparable sparse attention;dynamic sparse attention;attention routing transformers;based sparse attention;self attention layers;attention layers;attention models language;layers self attention;attention layers self;attention models;overall complexity attention;attention patterns;complexity attention n1;complexity attention;attention patterns avoid;attention;sparse routing;content based sparse;self attention;sparse routing module;attention n1;fewer self attention", "pdf_keywords": "comparable sparse attention;selfattention sparse routing;sparse routing module;routing transformer tensor\ufb02ow;sparse attention;sparse attention models;sparse routing;complexity attention n1;overall complexity attention;model routing transformer;transformer model trained;routing transformer model;complexity attention;attention layers additionally;layer routing transformer;sequence benchmarks wikitext;attention models language;code routing transformer;routing transformer endows;attention layers;self attention layers;19 imagenet;long sequence benchmarks;10 imagenet;pg 19 imagenet;sequence benchmarks;22 layer routing;routing transformer;imagenet;outperforms comparable sparse"}, "ba4a34680e09e77984624c95f5245d91b54373f6": {"ta_keywords": "multilingual models benchmark;cross lingual generalization;lingual generalization capabilities;evaluation multilingual encoders;nlp driven benchmarks;multilingual encoders xtreme;evaluating cross lingual;multilingual multi task;benchmarks limited english;transfer evaluation multilingual;lingual generalization;lingual transfer evaluation;multilingual models;increasing multilingual models;xtreme massively multilingual;generalization capabilities multilingual;multilingual encoders;introduce cross lingual;languages tasks;languages tasks broad;multilingual representations;cross lingual;evaluation multilingual;40 languages tasks;cross lingual transfer;sentence retrieval tasks;massively multilingual multi;learning models nlp;massively multilingual;capabilities multilingual representations", "pdf_keywords": "cross lingual generalization;multilingual encoders xtreme;transfer evaluation multilingual;cross lingual learning;evaluation multilingual encoders;evaluating cross lingual;lingual generalization capabilities;lingual transfer evaluation;lingual generalization translation;multilingual encoders;introduce cross lingual;lingual representation transfer;research cross lingual;cross lingual representation;purpose cross lingual;cross lingual transfer;cross lingual;generalization capabilities multilingual;multilingual representations;lingual generalization;evaluation multilingual;multilingual representation;transfer linguistic;languages tasks;lingual learning methods;lingual transfer;40 languages tasks;lingual learning;transfer linguistic knowledge;mt multilingual representation"}, "927ff874d3ed9307356d256c31b79a0624b3c9d5": {"ta_keywords": "fusion speech activity;fusion speech;speech diarization recognition;speech recognition asr;posterior fusion speech;speech recognition;combination automatic speech;speech activity detection;conversational speech diarization;speech diarization;automatic speech recognition;automatic speech;multi microphone conversational;hmm based overlap;score fusion diarization;acoustic model training;microphone conversational speech;distant multi microphone;reverberation overlapping speakers;speech activity;microphone conversational;model vb hmm;vb hmm based;guided source separation;multi microphone;source separation;source separation gss;fusion diarization;recognition asr;diarization recognition everyday", "pdf_keywords": "fusion speech activity;speech recognition asr;multi speaker asr;diarization speech recognition;speech diarization recognition;speech processing;speech recognition;combination automatic speech;fusion speech;speech recognition everyday;speech activity detection;speaker diarization;speech diarization;posterior fusion speech;speaker asr;automatic speech recognition;speaker asr chime;multi microphone conversational;speaker diarization speech;conversational speech diarization;speech processing human;conversational speaker diarization;reverberation overlapping speakers;microphone multi speaker;acoustic model training;automatic speech;hmm based overlap;diarization speech;multi speaker;language speech processing"}, "c4efaeccd7f0d900b1df95dadf51bad74264f613": {"ta_keywords": "profiles nash equilibrium;nash equilibrium profiles;preference profiles nash;ps rule strategic;rule allocation goods;strategising ps rule;manipulability ps rule;pure nash equilibrium;computation nash equilibrium;serial rule allocation;equilibrium profiles ps;nash equilibrium;nash equilibrium guaranteed;computation nash;functions pure nash;rule allocation;probabilistic serial rule;existence computation nash;nash equilibrium computed;agent manipulating ps;complexity agent manipulating;allocation goods;manipulating ps rule;ps rule examine;computational complexity agent;ps rule;strategising ps;strategic aspects probabilistic;equilibrium profiles;allocation goods secondly", "pdf_keywords": "allocation indivisible houses;sequential allocation indivisible;algorithm sequential allocation;response ps algorithm;sequential allocation;expected utility best;allocation indivisible;allocation;response algorithm sequential;ps algorithm;manipulability ps rule;expected utility;indivisible houses;demonstrate expected utility;indivisible houses proposed;computation nash equilibrium;existence computation nash;nash equilibrium guaranteed;response algorithm;pure nash equilibrium;ps algorithm section;computation nash;nash equilibrium;utility functions;equilibrium pro\ufb01les ps;agents objects utility;nash equilibrium pro\ufb01les;utility best response;pro\ufb01les ps rule;algorithm sequential"}, "605bae6c397e4829dde7ff7b8ddb84782ec6e607": {"ta_keywords": "understanding influenza viral;map influenza virus;influenza virus replication;influenza virus;influenza viruses;understanding influenza;map influenza;influenza viral;influenza viral infection;comprehensive map influenza;systematic understanding influenza;defend influenza viral;flumap comprehensive pathway;influenza;influenza viruses publicly;caused influenza viruses;influenza virus iav;virus interaction network;defend influenza;host virus interaction;disease caused influenza;flumap comprehensive;life cycle flumap;viral infection host;virus interaction;better defend influenza;using flumap;caused influenza;infection host cells;targets using flumap", "pdf_keywords": ""}, "a18b49fae647ae08711c2384611b3537485e8408": {"ta_keywords": "automatic speech translation;translators learning machine;speech translation systems;machine translation;learning machine translation;simultaneous interpreters learning;constructing speech translation;translation systems;translation data translators;translators learning;translation systems translate;data translators learning;speech translation;speech translation using;translation studies simultaneous;data translators;translation data;translation using simultaneous;studies simultaneous interpreters;translators;using translation data;simultaneous interpreters;simultaneous interpreter;simultaneous interpreters perform;automatic speech;interpreters learning;translate real time;data simultaneous interpreters;similar simultaneous interpreter;translation studies", "pdf_keywords": ""}, "417259d40d0d8b3ca7ebdcf811aa9f7814d5c0c5": {"ta_keywords": "model parameters saxophone;saxophone model using;parameters saxophone;parameters saxophone presented;fingering reed parameters;saxophone model;timbre saxophone model;reed parameters estimated;saxophone model couples;fingering reed model;fingering filter saxophone;filter saxophone model;dynamics timbre saxophone;measurement saxophone;saxophone configured possible;jointly estimating reed;derived measurement saxophone;saxophone;reed model parameters;measurement saxophone configured;saxophone presented problem;estimating reed;timbre saxophone;reed parameters;saxophone configured;filter saxophone;estimating reed source;reed source parameters;saxophone presented;source parameters fingering", "pdf_keywords": ""}, "4302e981e3ec118b68e0b3fcf1820b3f6ecfa988": {"ta_keywords": "theory argumentation quality;argumentation quality assessment;argumentation quality viewed;argumentation quality;argumentation theory practical;argumentation theory;theory argumentation;differently argumentation theory;based theory argumentation;arguments practice;argumentation;viewed differently argumentation;differently argumentation;represented theory argumentation;arguments practice correlate;arguments;comparisons arguments practice;comparisons arguments;quality assessment theory;quality phrased spontaneously;assessment theory vs;empirically observations quality;quality assessment;observations quality phrased;theory practical assessment;quality phrased;assessment theory;quality ratings;adequately represented theory;fact adequately represented", "pdf_keywords": ""}, "15251fa3a3bcf695bf153d0856886cab9a3145ea": {"ta_keywords": "cnn dailymail dataset;results cnn dailymail;cnn dailymail;techniques reranking;view text summarization;text summarization;text summarization summaries;summarization summaries combination;text summarization work;techniques reranking stacking;results cnn;summaries combination researchers;summarization summaries;reranking stacking approach;problem text summarization;summarization work;summarization;cnn;reranking;reranking stacking;refer techniques reranking;summaries combination;summarization work highlight;summaries;dailymail dataset;art results cnn;dailymail dataset 46;refactor provides;refactor;dailymail", "pdf_keywords": "refactoring neural summarization;neural summarization;summarization stage learning;performance text summarization;neural summarization yixin;text summarization stage;summarization stage;formulating text summarization;summarization systems apart;summarization systems;text summarization;refsum refactoring neural;summarization systems formulating;view text summarization;text summarization implications;text summarization summaries;modern text summarization;text summarization systems;summarization yixin liu;summarization summaries combination;summarization yixin;summarization implications future;problem text summarization;summarization summaries;summarization;summarization implications;improving performance text;refactoring neural;summaries combination new;summaries combination"}, "f9e3b7c6ca7d534694148bd0c7c37c1ef896a784": {"ta_keywords": "multilingual multispeaker asr;multilingual multispeaker;task multilingual multispeaker;multilingual multi speaker;multilingual asr systems;speech recognition asr;multispeaker asr using;automatic speech;speaker speech recognition;end multilingual asr;multispeaker asr;automatic speech recognition;speech recognition;multilingual asr;end automatic speech;speech recognition proposed;modeling multiple languages;end end multilingual;end multilingual;sequence acoustic features;multi speaker speech;end multilingual multi;recognition asr systems;recognition asr;codeswitching utterances;multilingual multi;multilingual;including codeswitching utterances;multispeaker;multiple languages", "pdf_keywords": ""}, "400e083a18ab94bbf45b0820693fb5035684dd7c": {"ta_keywords": "stochastic parsing hybrid;stochastic parsing;parsing hybrid semantic;parsing hybrid;meaning recognition spoken;utterances goal computer;parsing;spoken utterances goal;recognition spoken utterances;utterances goal;spoken utterances;utterances;hybrid semantic analysis;computation linguistic;hybrid semantic;semantic analysis;computation linguistic research;semantic analysis article;linguistic;recognition spoken;linguistic research;meaning recognition;semantic;problem meaning recognition;sentence article experiments;area computation linguistic;stochastic;goal computer algorithm;computer algorithm capable;algorithm capable", "pdf_keywords": ""}, "71a85e735a3686bef8cce3725ae5ba82e2cabb1b": {"ta_keywords": "ml pipeline underspecified;domains ml pipeline;practical ml pipelines;ml pipelines;training domain performance;training domain;predictors returned underspecified;ml pipeline;ml pipelines using;underspecified return predictors;based training domain;credibility modern machine;genomics predictors returned;performance training domain;learning ml models;challenges credibility;modern machine learning;underspecified pipelines;medical genomics predictors;returned underspecified pipelines;machine learning ml;clinical risk prediction;ml models exhibit;predictors behave;learning ml;pipeline underspecified;training deployment domains;challenges credibility modern;domain performance predictors;presents challenges credibility", "pdf_keywords": "prediction electronic health;genomics setting models;clinical risk prediction;adverse event prediction;models predicting;models early adverse;medical genomics setting;medical genomics;modern ml models;genomic features 000;deep learning;genomic features;world medical genomics;near optimal predictors;regularization schemes bias;ml models;genomics setting;predicting;event prediction electronic;predicting iop demographics;regression models predicting;regularization;prediction abilities;processing clinical risk;learning continuous risk;risk prediction;developing deep learning;event prediction;genomics;powerful prediction"}, "dd961bb9e2a70f3819a13b13402fe585ae384226": {"ta_keywords": "pure nash equilibrium;pure nash equilibria;nash equilibrium guaranteed;computing pure nash;nash equilibrium np;nash equilibrium;nash equilibrium conp;compute pure nash;nash equilibria;equilibria probabilistic serial;nash equilibrium yields;prove pure nash;profile pure nash;nash equilibria yield;equilibrium np hard;equilibria probabilistic;rule equilibria probabilistic;nash deviations;nash deviations ps;firstly nash deviations;equilibria yield social;equilibrium np;majority pure nash;equilibrium guaranteed exist;quality equilibria exist;pure nash;equilibrium guaranteed;equilibrium yields assignment;quality equilibria;rule firstly nash", "pdf_keywords": "pure nash equilibrium;nash equilibria computing;pure nash equilibria;equilibria computing nash;computing pure nash;nash equilibrium guaranteed;existence pure nash;nash equilibrium challenging;compute pure nash;computing nash equilibrium;nash equilibria;nash equilibrium;nash equilibrium np;nash equilibrium conp;prove pure nash;computing nash;nash equilibrium yields;nash equilibria yield;pro\ufb01le pure nash;equilibria computing;nash deviations;nash deviations ps;firstly nash deviations;equilibrium np hard;majority pure nash;equilibria exist ps;quality equilibria exist;equilibrium guaranteed exist;probabilistic serial ps;pure nash"}, "86d55c5a098689438ceb1d52bdd768da3b47f55f": {"ta_keywords": "optimal dynamic sensor;dynamic sensor subset;sensor subset selection;active sensor selection;sampling stochastic approximation;stochastic approximation learning;sensor activation tracking;active sensors fidelity;energy efficient tracking;sensor networks cyberphysical;dynamic sensor activation;sensor selection;sensor networks;stochastic approximation;sensors fidelity;sampling stochastic;sensor selection problem;subset selection tracking;sensor subset;efficient tracking;selection tracking time;dynamic sensor;optimal dynamic;sensors fidelity increases;time varying stochastic;gibbs sampling stochastic;examined optimal dynamic;tracking time varying;efficient tracking mechanisms;varying stochastic", "pdf_keywords": "decentralized tracking markov;selection decentralized tracking;decentralized tracking process;dynamic sensor subset;decentralized tracking;extension decentralized tracking;decentralized scheme markov;sensor networks cyberphysical;sensor subset selection;optimal dynamic sensor;algorithm sensor subset;tracking markov chain;centralized tracking iid;sensor subset;sensor activation tracking;tracking markov;subset selection decentralized;centralized tracking;sensor networks;subset selection tracking;dynamic sensor activation;scheme markov chain;scheme markov;varying stochastic process;dynamic sensor;examine centralized tracking;time varying stochastic;varying stochastic;tracking time varying;selection tracking time"}, "2c0ebf5479db7f76c1e15512676c16b9032343fb": {"ta_keywords": "gear shift control;automatic automotive transmission;gear shift;automotive transmission;shift control method;automatic automotive;shift control;method automatic automotive;transmission;shift;automotive;gear;automatic;control method automatic;control method;method automatic;control;method", "pdf_keywords": ""}, "0d360a1256ccdfca58cf98d12243df8407fd442d": {"ta_keywords": "attacks pretrained models;weights injected vulnerabilities;attacks pretrained;tuning enabling attacker;attacks pre trained;weights models pre;untrusted pre trained;pre trained models;pre trained weights;weights pose security;vulnerabilities expose backdoors;embedding surgery attacks;models pre trained;attacker manipulate model;poisoning attacks pretrained;download weights models;pretrained models;backdoors fine tuning;expose backdoors;injected vulnerabilities;weights models;trained weights injected;trained models users;expose backdoors fine;attacks pre;users download weights;enabling attacker manipulate;enabling attacker;trained weights;trained models", "pdf_keywords": "weights injected vulnerabilities;vulnerabilities applying regularization;publicly distributed weights;backdoors computer systems;vulnerabilities expose backdoors;tuning enabling attacker;malware backdoors;expose backdoors;preventing vulnerabilities;weights pose security;untrusted software;attacks pre trained;detecting preventing vulnerabilities;pose security threat;backdoors computer;detection attack widely;expose backdoors \ufb01ne;injected vulnerabilities;introduction malware backdoors;security threat;preventing vulnerabilities applying;threat fundamental computer;potential introduction malware;vulnerabilities expose;security threat fundamental;attack widely applicable;untrusted software online;pre trained weights;embedding surgery attacks;running untrusted software"}, "c14254fd285706e549d0dcc57ae74680164c9afc": {"ta_keywords": "inverse reinforcement learning;sensitive inverse reinforcement;inverse reinforcement;problem inverse reinforcement;modeling passengers decisions;risk sensitivity reinforcement;risk sensitive inverse;sensitivity reinforcement learning;reinforcement learning markov;learning markov decision;reinforcement learning framework;mdp modeling passengers;reinforcement learning;passengers decisions;markov decision processes;reinforcement learning gradient;passengers decisions regarding;agent risk;markov decision;agent risk sensitive;sensitivity reinforcement;modeling passengers;learning markov;reinforcement;decision processes agent;model risk sensitivity;risk sensitivity;risk;model risk;economics neuroscience risk", "pdf_keywords": "inverse reinforcement learning;inverse risk sensitive;inverse risk;based inverse reinforcement;inverse reinforcement;problem inverse reinforcement;learning markov decision;risk sensitive reinforcement;reinforcement learning markov;sensitive reinforcement learning;rewards markov decision;2017 inverse risk;rewards markov;employs convex risk;sharing passengers decisions;markov decision processes;probabilities rewards markov;reinforcement learning;reinforcement learning algorithm;learning markov;convex risk;passengers decisions;risk sensitive agent;agent risk;markov decision process;passengers decisions given;convex risk metrics;gradient based inverse;agent risk sensitive;derive risk"}, "e10dba1d4a56a81429d6ec4c9b7bdc15ea75474b": {"ta_keywords": "filter malicious sensor;malicious sensor observations;detection secure estimation;attack unknown sensor;secure estimation proposed;algorithm secure estimation;malicious sensor;secure remote estimation;secure estimation;attack detection secure;developed attack detection;attack detection;detect injection attack;secure estimation false;data injection attack;injection attack cyber;detection secure;estimation linear time;remote estimation linear;unknown sensor subset;filter malicious;attack cyber physical;subset developed attack;estimation linear;using sensor observations;attack cyber;sensor observations;estimation scheme;attack unknown;gaussian process observations", "pdf_keywords": ""}, "5403fd71810d098e572d9bd0f9ec10e96d6b6336": {"ta_keywords": "graph signal processing;graph symmetrization methods;formulated markov decision;markov decision process;graph symmetrization;network transmission control;wireless standard algorithms;graphs employed subspace;linear programming;markov decision;point network transmission;decision process mdp;formulated markov;networks large state;graph developed gsp;method applied mdp;iteration linear programming;graph signal;point point network;point network;probability transition graph;applied mdp problems;process mdp;problem formulated markov;classical dynamic programming;mdp problems;network transmission;linear programming employed;reconstruction optimal policy;mdp main challenge", "pdf_keywords": ""}, "967b2d10b8b378f1da43fd4d9107826e540e1112": {"ta_keywords": "animations natural language;embedding language pose;joint language pose;language grounded pose;learns joint embedding;language pose objective;grounded pose forecasting;language pose;corpus 3d pose;pose jl2p learns;pose data human;joint embedding language;pose forecasting;human annotated sentences;planning joint embedding;pose;human animation;pose forecasting evaluate;embedding space learned;language2pose natural language;pose data;language pose jl2p;pose objective;embedding language;data human annotated;multimodal;joint embedding;3d pose data;learns joint;generating animations natural", "pdf_keywords": "joint language pose;pose data humanannotated;embedding language pose;language pose learn;learns joint embedding;language grounded pose;corpus 3d pose;joint embedding language;language pose generate;learn joint embedding;language pose;grounded pose forecasting;joint multimodal;human annotated sentences;pose data human;pose forecasting;pose learn joint;space language pose;joint multimodal space;pose learn;multimodal space language;integrates language pose;uses joint multimodal;data humanannotated sentences;pose;humanannotated sentences;data human annotated;pose generate animation;human annotated;joint language"}, "0bdf1f3b79f4df5d5e11af1ea00379e1461e22fa": {"ta_keywords": "pdp generalization;model automated dependency;automated dependency plots;proposed pdp generalization;selection interesting pdps;instance specific pdps;pdp generalization multiple;dependence plots pdp;applications machine learning;model automated;generalization multiple use;automated dependency;generalization;machine learning;machine learning necessary;features checking;example raw feature;pdp including instance;specific pdps;interesting pdps extend;usefulness proposed pdp;dependency plots;formalize method automating;partial dependence plots;method automating selection;interesting pdps;validate various qualitative;validate model automated;model demonstrate usefulness;raw feature spaces", "pdf_keywords": ""}, "d95973f0f0d86b758154e9a5f3d7434430d7856c": {"ta_keywords": "gradient free optimization;gradient free method;euclidean proximal operator;accelerated gradient free;free optimization methods;proximal operator;propose accelerated gradient;accelerated gradient;value accelerated gradient;gradient free;free optimization;proximal operator associated;optimization methods non;euclidean proximal;non euclidean proximal;optimization methods;norm obtain estimates;gradient;method low noise;operator associated norm;estimates rate convergence;convergence method low;function value accelerated;norm obtain;optimization;rate convergence method;associated norm obtain;norm;method non euclidean;free method", "pdf_keywords": ""}, "194c5644c49e9e1b87990439fae05c98ba8b4fbb": {"ta_keywords": "annotating materials synthesis;corpus annotating materials;annotating materials;text make corpus;procedural text corpus;semantic structure materials;synthesis extraction models;scientific information extraction;science procedural text;synthesis extraction;annotating scientific text;materials science literature;understanding materials synthesis;text corpus;synthesis procedures annotated;unstructured natural language;scientific understanding materials;text corpus annotating;natural language text;millions materials synthesis;corpus annotating;materials science procedural;evaluation synthesis extraction;understanding materials;semantics synthesis sentences;materials synthesis enable;automated synthesis;scientific text shallow;semantics synthesis;language text make", "pdf_keywords": "synthesis sentences annotate;materials syntheses text;semantics synthesis;annotated semantic structure;semantics synthesis sentences;synthesis extraction models;annotated semantic;semantic structure domain;extract shallow semantic;semantic structures make;procedures annotated semantic;text shallow semantic;express semantics synthesis;models automatic extraction;annotate step synthesis;semantic structures;shallow semantic structures;synthesis extraction;shallow semantic structure;annotating scienti\ufb01c text;supervised entity tagging;syntheses text;structured frame semantic;semantic structure;annotated domain experts;scienti\ufb01c information extraction;domain experts materials;syntheses text work;dataset synthesis;frame semantic representation"}, "03b68259f9e70d2007d40e5331c9ff31f2bb46b9": {"ta_keywords": "proposes activity recognition;activity modeling;acceleration based activity;activity recognition method;activity recognition;adapts activity models;activity models;based activity modeling;activity models end;user activities using;user activities;training sensor data;activity modeling paper;activities using labeled;activities using;sensor data training;training data;automatically adapts activity;acceleration sensor data;training data using;user recognition;obtained user recognition;adapts activity;appropriate training data;activity;user physical characteristics;data training data;data training;training sensor;training data obtained", "pdf_keywords": ""}, "e11d6a031d5f85f372b0fda3ab62ca4ce2d89f2c": {"ta_keywords": "design socrates rule;rule based expert;expert optimizes combinational;rule generation;rule based optimizing;socrates rule based;rules rule generation;rule generation module;optimizes combinational logic;design socrates;based expert optimizes;expert optimizes;optimizing combinational logic;knowledge base rule;socrates rule;based optimizing combinational;rule based;rules inserts knowledge;speed design socrates;optimizes combinational;combinational logic specific;automatically encodes rules;combinational logic;optimizing combinational;based optimizing;base rule based;encodes rules;optimizes;rules rule;logic specific", "pdf_keywords": ""}, "90fbeb4c871d3916c2b428645a1e1482f05826e1": {"ta_keywords": "reviewer module caption;encode review decode;encode review;decode reviewer module;review decode;model encode review;review decode reviewer;reviewer module;reviewer module improve;decode reviewer;caption generation;module reviewer module;module caption generation;reviewer module performs;module reviewer;attention mechanism decoder;review steps attention;attention mechanism encoder;novel module reviewer;reviewer module generic;encoder decoder learning;module caption;caption;reviewer;decoder learning;review;framework reviewer module;module improve encoder;learning framework reviewer;input attention", "pdf_keywords": "captioning introduction encoder;captioning encoder;captioning encoder decoder;captioning using cnns;code captioning encoder;rnns encoders;cnns rnns;cnns rnns encoders;networks rnns;rnns encoders respectively;captioning source code;sequence learning recently;image captioning source;sequence sequence learning;captioning source;code captioning;source code captioning;sequence learning;captioning;image captioning;using cnns rnns;tasks image captioning;networks rnns applications;recurrent neural networks;code captioning using;neural networks rnns;code captioning introduction;captioning using;rnns;encoder decoders tasks"}, "ddd74358d7e11535ee77e2c323dd662d115a0f20": {"ta_keywords": "natural language robot;learning robot policy;language robot instruction;trained augmented reality;objects training shot;robot instruction;learning robot;objects training;introduce shot language;object locations instructed;mapping natural language;trained augmented;language robot;conditioned object grounding;object grounding mapping;robot policy;shot language conditioned;shot language;robot instruction following;objects prior;training shot object;method trained augmented;natural language instructions;shot object grounding;problem learning robot;robot policy follow;language conditioned object;observes objects training;objects present learned;objects prior approach", "pdf_keywords": "conditioned object grounding;trained augmented reality;conditioned shot segmentation;language conditioned shot;learned map groundings;introduce shot language;object centric learned;trained augmented;object mentions instructions;object mentions;shot language conditioned;shot language;object mentions observations;natural language object;shot segmentation;object grounding;ground natural language;method trained augmented;grounding method trained;visual language;language conditioned object;combines visual language;mentions instructions language;map groundings object;shot segmentation model;augmented reality data;learned map;exemplars identify objects;visual language modalities;learning map"}, "97943a5dee3c6e36d01a6099acb9ec360ad0ee19": {"ta_keywords": "portmanteaus word formation;end trainable language;creation portmanteaus word;trainable language;embedding models portmanteau;portmanteau generation;neural sequence sequence;trainable language independent;use additional phonetic;additional phonetic information;portmanteau generation end;word formation;portmanteaus word;portmanteau creation portmanteaus;character embedding models;neural sequence;models portmanteau creation;additional phonetic;character level neural;phonetic information propose;creation portmanteaus;character embedding;models portmanteau;phonetic information;portmanteau creation;level neural sequence;phonetic;words combine new;word formation phenomenon;task portmanteau generation", "pdf_keywords": "words generate portmanteaus;portmanteaus word formation;neural model portmanteau;words generate;input words generate;embedding models portmanteau;model vocabulary words;abstract portmanteaus word;word formation;propose neural s2s;model predict portmanteaus;word formation phenomenon;proposed predicting portmanteaus;portmanteaus word;predicting portmanteaus;language model vocabulary;features portmanteaus exhaustively;language model;character embedding models;predict portmanteaus;incorporate language model;trainable language;portmanteaus exhaustively generate;predicting portmanteaus given;neural s2s model;predict portmanteaus given;input words;words speci\ufb01cally making;character embedding;neural s2s"}, "30f86d38f0660af5ea2e16d996434c72eee8c5ee": {"ta_keywords": "speech recognition asr;setup speech recognition;end speech processing;speech recognition;automatic speech;recognition speech processing;automatic speech recognition;speech processing;learning engine espnet;neural network toolkits;asr toolkits;asr toolkit;source asr toolkits;speech recognition speech;open source asr;recognition speech;speech processing named;asr toolkits experimental;speech processing experiments;deep learning engine;end automatic speech;setup speech;kaldi asr toolkit;recognition asr;espnet open source;engine espnet;recognition asr adopts;processing named espnet;deep learning;asr benchmarks", "pdf_keywords": "speech processing espnet;speech processing toolkit;end speech processing;asr speech processing;speech processing;toend speech processing;platform asr speech;speech processing named;terms speech recognition;end asr techniques;speech recognition;processing espnet;speech recognition open;processing espnet fully;neural network toolkits;processing named espnet;espnet fully utilizes;toolkit named espnet;end platform asr;end toend speech;end end speech;processing toolkit;espnet end end;end speech;training recognition asr;recognition asr pipeline;espnet end;platform asr;end end asr;multilingual asr experiments"}, "36bca9d41de386fce5dce06999a45a802a7c4f41": {"ta_keywords": "preference networks cp;conditional preference networks;preference networks;cp net algorithms;preferences method computationally;cp nets equiprobable;cp nets uniformly;generate cp nets;random conditional preference;networks cp nets;acyclic cp nets;formalism modeling preferences;properties cp nets;modeling preferences;modeling preferences method;conditional preference;cp nets commonly;cp nets;net algorithms;nets equiprobable manner;cp net;networks cp;nets uniformly random;cp nets performance;nets equiprobable;nets performance cp;nets uniformly;dependency graph;indegree dependency graph;preferences method", "pdf_keywords": ""}, "0c5bfa2d4bb351a479073cb358c3ae6f7ecf0476": {"ta_keywords": "nlp annotation modules;nlp tools;cogcomp nlp annotation;corpora nlp community;corpora annotations data;corpora annotations;development nlp applications;processing nlp;nlp annotation;nlp tools augment;annotation modules integrated;nlp applications;reading corpora annotations;annotators python;challenges provide corpus;corpora nlp;popular corpora nlp;implementing natural language;processing nlp requires;language processing nlp;nlp applications providing;development nlp;easily use annotators;annotators python interface;annotations data structures;annotation modules;nlp community module;corpus reader module;natural language processing;shelf nlp tools", "pdf_keywords": ""}, "a2aa642db090b3aa28a44ccbc3c51fdb0be8335b": {"ta_keywords": "neural constituency parsers;generalization neural parsers;neural parsers substantially;parsers neural parsers;treebanks constituency parsing;neural parsers;constituency parsers neural;learn neural parsers;improvement domain treebanks;neural parsers zero;representations neural parsers;neural parsers obtain;neural parsers benefit;domain treebanks finally;parsers neural;domain treebanks;benchmark treebanks constituency;parsers substantially improves;constituency parsers;parsers zero shot;treebanks finally;results benchmark treebanks;benchmark treebanks;constituency parsing;treebanks constituency;parsers benefit structured;treebanks;treebanks finally despite;parsers substantially;parsers", "pdf_keywords": "generalization neural parsers;neural constituency parsers;neural parsers better;neural parsers;non neural parsers;parsers neural models;neural parsers neural;abstract neural parsers;neural parsers obtain;treebanks constituency parsing;parsers neural;benchmark treebanks constituency;neural parsers zero;parsers non neural;constituency parsers;state art parsing;benchmark treebanks;results benchmark treebanks;treebanks order parser;parser structured;constituency parsing;parsers better;parsers;treebanks;parser structured tree;treebanks constituency;constituency parsers non;english web treebanks;parsers obtain;encoder parser structured"}, "de5834305ea419c25b17f0c8d27bad6a5feb311a": {"ta_keywords": "chess commentary dataset;language descriptions chess;descriptions chess games;chess commentary;descriptions chess;generate commentary;generating natural language;scale chess commentary;natural language generation;commented given chess;commentary frequently depend;natural language descriptions;generate commentary individual;commentary individual moves;commentary dataset propose;language generation data;methods generate commentary;commentary dataset;commentary texts;commentary;chess games;commentary frequently;truth commentary texts;truth commentary;chess game highlight;pragmatic aspects game;language generation;commentary texts terms;ground truth commentary;language descriptions", "pdf_keywords": ""}, "47234fca1b14666d72bc5df0e2d911ff7cdea688": {"ta_keywords": "clustering hsc hypergraph;clustering hsc;spectral clustering hsc;nodes weights hyperedges;nodes analysis reveals;sum edge weights;hsc hypergraph;hsc hypergraph spectral;hypergraph spectral clustering;spectral clustering weighted;nodes sum edge;algorithms random hypergraph;partition sum edge;weights hyperedges;edge weights explained;clustering weighted;clustering weighted stochastic;weights hyperedges respectively;nodes weights;nodes sum;nodes analysis;hypergraph model weighted;hidden partition sum;formula number nodes;edge weights;modeled nodes weights;fraction nodes sum;edge weights inline;inline formula hsclr;nodes", "pdf_keywords": "hypergraph spectral clustering;spectral clustering weighted;clustering hsc hypergraph;spectral clustering local;case hypergraph spectral;spectral clustering celebrated;spectral clustering;hypergraph spectral;clustering weighted stochastic;abstract spectral clustering;spectral clustering hsc;hsc hypergraph spectral;clustering weighted;setting hypergraph spectral;clustering local re\ufb01nement;hypergraph;sum edge weights;barrier case hypergraph;case hypergraph;binary edge weight;partition sum edge;hsc hypergraph;nodes analysis reveals;clustering local;clustering based hsclr;edge weights explained;algorithm subspace clustering;weighted stochastic block;edge weights;clustering celebrated algorithm"}, "9a41111cf881b052555985bd8cf304ef9fc4f6d5": {"ta_keywords": "corpus coupling constraints;large corpus coupling;corpus coupling;augmenting large corpus;small structured corpus;structured corpus improve;structured corpus;corpus improve performance;large corpus;exist corpora augmenting;corpora augmenting;document structure corpora;corpora augmenting large;corpus improve;corpora sections;corpus;structure corpora;corpora;structure corpora sections;corpora sections identified;sections exist corpora;labels example coupling;coupling items;exist corpora;distant identifying coupling;example coupling items;identifying coupling constraints;coupling items list;coupling constraints;coupling", "pdf_keywords": "structured corpus nps;corpus nps edges;nps target corpus;corpus nps;target corpus;small structured corpus;labeled nps training;structured corpus;distantlysupervised test corpus;labeled nps;pseudo labeled nps;training data nps;corpus small structured;test corpus;corpus;structured sections diebolds;potential instance labels;nps training data;target corpus small;sections diebolds distant;classifier graph mrw;diebolds distant bootstrapping;associated distant identifying;test corpus does;training data svm;svm classifier graph;classifier;classifier graph;sections diebolds;labeled"}, "c8a95217cde1bc893b230297250918818aa01dd7": {"ta_keywords": "backpack mobile mapping;mapping framework backpack;mobile mapping degenerate;mobile mapping;systematic mapping framework;backpack mobile;framework backpack mobile;mapping degenerate environments;mapping framework;systematic mapping;framework backpack;backpack;mapping;mapping degenerate;degenerate environments;environments;mobile;systematic;framework;degenerate", "pdf_keywords": ""}, "71cdf94d13cc6c497dcc2dcb20893fe64cfaf62e": {"ta_keywords": "continuations text generation;text generation model;text generation;generation desired topical;topics predicting;guide text generation;text generation desired;candidate topics predicting;topically controllable language;language generation;interactive writing assistants;language generation current;controllable language generation;interactive writing;word clusters;current interactive writing;based language models;word clusters possible;topics predicting centers;language models aid;language models;chosen topics address;chosen topics;set candidate topics;upcoming topics user;topically controllable;adheres chosen topics;topics;plausible continuations text;upcoming topics", "pdf_keywords": "generates topics;generates topics related;indicate generates topics;generation userchosen topics;text generation model;topics automated evaluation;topics automated;text generation userchosen;candidate topics predicting;text generation;chosen topics automated;topics predicting;generates sentences;generates sentences \ufb02uent;guides text generation;continuations text generation;set candidate topics;topic options generated;interactive writing framework;topics guides text;prompt generates sentences;propose interactive writing;set topics;interactive writing;chosen topics;candidate topics;candidate upcoming topics;provides set topics;upcoming topics user;userchosen topics"}, "edb49aa423afc210facec998277923c4b75e4648": {"ta_keywords": "phase transitions zncr2se4;spinel zncr2se4 studied;transitions zncr2se4;zncr2se4 studied;antiferromagnetic phase transitions;structural antiferromagnetic phase;antiferromagnetic phase transition;spinel zncr2se4;cr3 ions crse4;ions crse4 chains;structural antiferromagnetic;correlation structural antiferromagnetic;type spinel zncr2se4;antiferromagnetic phase;ions crse4;zncr2se4 studied function;occurs antiferromagnetic phase;zncr2se4;magnetic interaction cr3;structural phase transition;crse4 chains;occurs antiferromagnetic;crse4;crse4 chains 110;antiferromagnetic;cr3 ions;neutron diffraction structural;interaction cr3 ions;structural phase;diffraction structural phase", "pdf_keywords": ""}, "4e749b2e0728044af44d50a708fc99d49359ea0b": {"ta_keywords": "character level transduction;sequence models attention;encode structural linguistic;script language russian;models unsupervised character;structural linguistic;languages;unsupervised character level;structural linguistic knowledge;attention analyze distributions;related languages;models attention analyze;state models unsupervised;sequence sequence models;script language;neural finite state;unsupervised character;error analysis neural;closely related languages;languages serbian;sequence models;linguistic knowledge;native script language;level transduction problems;models attention;analysis neural finite;related languages serbian;transduction problems;language;unsupervised tasks testbeds", "pdf_keywords": "transduction tasks transliteration;language sequence transduction;tasks transliteration grapheme;tasks transliteration;romanization decipherment;romanization decipherment related;transliteration grapheme;transliteration;language sequence;language translation study;transduction tasks translating;informal romanization decipherment;tasks informal romanization;sequence models decoding;sequence transduction tasks;transliteration grapheme phoneme;romanization;decipherment related language;decipherment informal romanization;language translation;natural language sequence;related language translation;translation study performance;tasks translating closely;converting informally romanized;parameterization re\ufb02ects linguistic;script language russian;informal romanization;languages written different;translation study"}, "9700940262cd5e797ab81eee464c3b3a16295cba": {"ta_keywords": "adaptation speech enhancement;speech enhancement static;speech enhancement pre;speech enhancement;derived speech enhancement;interconnection speech enhancement;capabilities speech enhancement;speech enhancement increase;robustness speech recognizers;increase robustness speech;processor speech recognizer;speech recognizer;model adaptation speech;recognition dereverberated speech;enhancement static adaptation;speech recognizers;speech dereverberation preprocessing;efficient interconnection speech;speech recognizer research;method speech dereverberation;speech recognizers known;robustness speech;pre processor speech;adaptation speech;speech recognition performs;automatic speech recognition;speech recognition;automatic speech;dynamic capabilities speech;known automatic speech", "pdf_keywords": ""}, "0d2a1c0724743de0cb74463466b075598ba36c45": {"ta_keywords": "medical equipment alarm;alarms medical equipment;alarm medical devices;alarms medical;problems alarm medical;equipment alarm related;alarm medical;equipment alarm;status alarms medical;alarm related;alarms;related problems alarm;alarm related problems;status alarms;alarm;problems alarm;present status alarms;medical devices;medical equipment;health labour sciences;labour sciences;labour sciences research;grants study present;medical;equipment;grants study;purpose health labour;sciences research grants;devices;research grants study", "pdf_keywords": ""}, "f01f4808263ecfa221f856c34d3420166dbf5930": {"ta_keywords": "confusion status detector;representing driver behavior;confusion status detection;driver using classifier;confusion level driver;features representing driver;annotated driver;classifier trained multimodal;car navigation;lstm based;representing driver;driver confusion status;road driving traffic;driver behavior traffic;road driving;trained multimodal sensor;lstm;classifier trained;estimating confusion level;driving traffic;detection using recurrent;recurrent neural networks;classifiers trained;collected road driving;memory lstm based;status detector car;manually annotated driver;term memory lstm;traffic conditions driver;trained multimodal", "pdf_keywords": ""}, "ff7b5379641875be7357766af0b1e2bd55c74cc8": {"ta_keywords": "dramatically simplifying retrieval;information retrieval;information retrieval accomplished;simplifying retrieval;transformer information corpus;information corpus encoded;corpus encoded parameters;simplifying retrieval process;retrieval;differentiable search index;learns text text;learns text;demonstrate information retrieval;models corpus sizes;models corpus;interplay models corpus;information corpus;retrieval accomplished;search index;retrieval accomplished single;corpus encoded;variations documents identifiers;model answers queries;search index dsi;documents identifiers represented;text model;directly relevant docids;string queries directly;introduce differentiable search;retrieval process", "pdf_keywords": "indexing retrieval tasks;document representation indexing;document retrieval task;document retrieval;indexing retrieval;novel indexing retrieval;information retrieval;dramatically simplifying retrieval;retrieval tasks encode;representation indexing retrieval;retrieval tasks;simplifying retrieval process;simplifying retrieval;studied document retrieval;documents docids explored;retrieval task;retrieval;sequence seq2seq learning;information retrieval requires;terms docids;indexing training;represent documents docids;novel indexing;search uni\ufb01ed manner;indexing training strategies;learns text;document representation;retrieval requires series;directly relevant docids;terms docids completely"}, "3cd4ae1cac866f853bb3276d215cff18df371b67": {"ta_keywords": "noisy speech recognition;speech recognition discriminative;robust speech recognition;speech recognition chime;noise robust speech;microphone speech recognition;robust speech;recognition chime challenge;discriminative methods noise;asr techniques discriminative;difficult microphone speech;speech recognition;reverberated noisy speech;microphone speech;discriminative feature transform;discriminative feature transformation;speech recognition task;recognition chime;augmented discriminative feature;challenge difficult microphone;discriminative training;techniques discriminative training;discriminative language modeling;discriminative feature;noisy speech;chime challenge benchmark;features discriminative feature;feature transform efficient;features discriminative;decoding asr", "pdf_keywords": ""}, "ceef266c59698999c9283a0cda852d8bc1ce27ea": {"ta_keywords": "fine tuned embedding;isotropy embedding space;isotropy embedding;embedding space changes;tuning pretrained language;pretrained language models;tuned embedding space;extent isotropy embedding;embedding space making;tuned embedding;directions embedding space;pretrained language;embedding space;elongated directions embedding;affect geometry embedding;embedding space contrast;embedding;directions embedding;geometry embedding space;tasks demonstrate isotropy;language models;isotropy desirable geometrical;result isotropy enhancements;isotropy enhancements;existing isotropy enhancement;geometry embedding;language models usually;isotropy enhancement;embedding space case;trained cwrs", "pdf_keywords": "tuning embedding space;tuning embedding;increased isotropy embedding;\ufb01ne tuning embedding;embedding space changes;tuned embedding space;isotropy embedding space;tuning pretrained language;isotropy embedding;pretrained language models;tuned embedding;embedding space;trained language models;\ufb01ne tuned embedding;embedding space making;extent isotropy embedding;embedding space contrast;embedding space base;embedding;pretrained language;directions embedding space;affect geometry embedding;language models;geometry embedding space;elongated directions embedding;embedding space case;trained language;pre trained language;directions embedding;language models usually"}, "b719fc66b173f8e9e0624317bb00abf10a4d5606": {"ta_keywords": "basketball game video;segmentation time video;segmentation video shot;histogram video shot;video shot basketball;segmentation video;detection video shot;video image;detection video;image frames histogram;histogram video;quantization level video;segmentation algorithm video;temporal segmentation video;dimension video image;based histogram video;detection rate video;edge detection video;boundary detection video;algorithm video shot;frames histogram;analysis video shot;game video;game analysis video;video shot calculate;video combined dimensional;video image color;video dimensional vector;video dimensional;reduce dimension video", "pdf_keywords": ""}, "18ef33a6e040b49ba475e586202932cecbafba0d": {"ta_keywords": "event influence generation;question answering;generate event influences;influence generation using;reference relevance generations;question answering wiqa;relevance generations;language models generate;influence generation;performance question answering;event influences generated;influence distance reasoning;generation using pre;eigen event influence;relevance generations furthermore;models generate event;trained language models;reasoning eigen event;generation using;generate event;event influence;pre trained language;language models;influences generated eigen;eigen event;event influences;reasoning chain;influences generated;generated eigen improve;generations furthermore event", "pdf_keywords": "event in\ufb02uence generation;reasoning events tracking;generation event in\ufb02uences;language models generate;generate event in\ufb02uences;models generate event;reasoning events;events tracking in\ufb02uences;abstract reasoning events;reference relevance generations;generation event;events distance reasoning;trained language models;context relationship events;in\ufb02uence generation using;pre trained language;language models aman;relevance generations;generation using pre;language models;in\ufb02uence generation;relevance generations summary;generate event;in\ufb02uence generation work;eigen event;background reasoning;generation using;eigen event in\ufb02uence;used generation event;based background reasoning"}, "e5acad5bba23a8c3a9f7cd24f7694ab10357ebc7": {"ta_keywords": "speech separation recognition;dereverberation beamforming speech;beamforming speech recognition;speech dereverberation separation;speaker speech separation;separation recognition singlechannel;speech separation;beamforming speech;reverberant multi speaker;multi speaker reverberant;speaker reverberant;speech recognition improved;speech recognition;end dereverberation beamforming;multichannel multi speaker;speech dereverberation;dereverberation beamforming;recognition singlechannel multichannel;decent speech dereverberation;dereverberation separation performance;multi speaker speech;speaker condition trained;voice activity detection;recognition singlechannel;speaker reverberant condition;multi speaker;wsj1 2mix corpus;reverberant multi;separation recognition;voice activity", "pdf_keywords": "beamforming speech recognition;speech separation recognition;separation speech recognition;dereverberation beamforming speech;speech dereverberation separation;speaker speech separation;speech enhancement performance;beamforming speech;speech separation;speech recognition improved;multi speaker reverberant;decent speech enhancement;reverberant multi speaker;speech recognition;speaker condition optimized;speaker reverberant;denoising separation speech;performance reverberant multi;speech enhancement;performance reverberant;speech dereverberation;multi speaker speech;enhancement performance reverberant;speaker reverberant condition;training dereverberation beamforming;results decent speech;multichannel multi speaker;decent speech dereverberation;separation recognition singlechannel;speaker condition trained"}, "4b9b7240ef9b6bc442044684ed5646ef02897d87": {"ta_keywords": "advising planning domain;planning domain;mdp planning research;planning competition domain;mdp planning;advising planning;planning competition;limits mdp planning;advising domain planning;domain planning competition;planning research;academic advising planning;planning;domain planning;academic advising domain;planning research paper;actions academic advising;advising domain;propose academic advising;concurrent actions academic;academic advising;competition domain exhibits;advising;mdp;propose academic;pushing limits mdp;competition domain;actions academic;paper propose academic;limits mdp", "pdf_keywords": ""}, "c3aa698b562e91f78a042b938ffce1877b6e859c": {"ta_keywords": "committees awful german;awful german language;semantics nominal compounds;germanet natural language;nominal compounds germanet;semantics nominal;organising committees awful;nominal;awful german;language processing;natural language processing;german language;committees awful;nominal compounds;cope semantics nominal;natural language;semantics;germanet natural;german language cope;germanet;program organising committees;organising committees;language cope semantics;language;german;committees;compounds germanet;compounds germanet natural;cope semantics;language cope", "pdf_keywords": ""}, "728a6850882a0d8ef5551949cc2baee1e1667cd8": {"ta_keywords": "bribery voting combinatorial;optimal bribery schemes;bribery schemes voting;optimal bribery;finding optimal bribery;easy bribery voting;bribery voting;bribery schemes;bribery problem;bribery problem easy;easy bribery;nets cases bribery;voting combinatorial domains;problem easy bribery;bribery;voting combinatorial;cases bribery problem;cases bribery;schemes voting;schemes voting domains;voting domains;voting domains candidate;complexity finding optimal;candidate set cartesian;combinatorial domains easy;computational complexity finding;computational complexity;agents preferences represented;complexity finding;combinatorial domains", "pdf_keywords": ""}, "df8ae2068d17d969db6ab2d27108776e99413975": {"ta_keywords": "natural language inference;language inference nli;improving natural language;question answering;inference nli fundamental;knowledge solve nli;inference nli;natural language processing;inference using external;natural language;language inference;external knowledge solve;language inference using;nlp applications;external knowledge science;questions domain nli;fundamental natural language;nlp applications including;nli problem science;search question answering;external knowledge improve;problem natural language;techniques text graph;processing nlp;including semantic;harness external knowledge;processing nlp applications;nlp;using external knowledge;textual information", "pdf_keywords": "knowledge source conceptnet;sources wordnet dbpedia;wordnet dbpedia;wordnet dbpedia present;external knowledge source;sources wordnet;source conceptnet;dbpedia;wordnet;2017 dbpedia;external knowledge improve;conceptnet;dbpedia present;knowledge solve nli;predict entailment relationships;source conceptnet evaluate;harness external knowledge;external knowledge solve;dbpedia present results;sources including wordnet;knowledge provided external;external knowledge;performance sources wordnet;using external knowledge;hypotheses graph text;knowledge source;conceptnet speer;techniques text graph;predict entailment;2017 dbpedia auer"}, "a7abd783de8d21d640e41d31ec89f2c1caec4e42": {"ta_keywords": "word sense disambiguation;sense disambiguation presented;sense disambiguation wsd;sense disambiguation;word disambiguation texts;disambiguation wsd knowledge;sense representations disambiguation;word disambiguation;disambiguation presented tool;interpretable word sense;word sense inventories;interpretable knowledge free;disambiguation texts;knowledge free interpretable;free interpretable word;disambiguation wsd;disambiguation texts makes;word sense;interface word disambiguation;unsupervised knowledge free;disambiguation presented;representing word senses;disambiguation results;disambiguation results providing;disambiguation;providing interpretable word;sense predictions human;interpretable knowledge based;predictions human readable;predictions unsupervised knowledge", "pdf_keywords": "thesaurus wsd model;sense disambiguation applications;api words disambiguation;word sense disambiguation;sense representations disambiguation;thesaurus wsd;sense disambiguation;embeddings lexemes synsets;interpretable wsd;wsd models restful;senses hypernyms extraction;sense representations labeling;wsd models scala;hypernyms extraction sense;applications word sense;word sense inventories;api words;wsd models;distributional thesaurus wsd;examples wsd web;learn embeddings lexemes;wsd api scala;wsd web interface;disambiguation applications word;word disambiguation texts;restful api words;word sense induction;hypernyms extraction;interface interpretable wsd;wsd web"}, "58961f0ea3291ddab697fbe5be999a0793b0efaf": {"ta_keywords": "overhead muc codes;muc codes existence;codes existence muc;existence muc codes;storage overhead muc;muc codes;access sets analysis;codes existence;bound storage overhead;lower bound storage;muc codes meeting;access sets achievable;codes certain subsets;bound update cost;named access sets;bound storage;access sets;lower bound update;bound randomized construction;required access sets;update cost storage;bound update;storage overhead tailoring;storage overhead;overhead muc;storage overhead finally;access sets required;codes certain;lower bound randomized;cost storage overhead", "pdf_keywords": ""}, "f75e691daae9133941c9a083e319b39bd837d456": {"ta_keywords": "entity alignment joint;joint knowledge embeddings;entity alignment improve;improvements entity alignment;entities wikipedia links;entity alignment;knowledge embeddings;knowledge embeddings specifically;knowledge embeddings existing;entities wikipedia;alignment joint knowledge;approach entity alignment;knowledge graph completion;aligned entities;joint semantic space;alignment improve knowledge;semantic distance joint;align entities;information entities wikipedia;encodes entities relations;improve knowledge graph;joint semantic;align entities according;entities relations;set aligned entities;distance joint semantic;entities according semantic;jointly encodes entities;entities relations various;knowledge graph", "pdf_keywords": ""}, "80a085a79ac6cee94f21d21ab8ca302458c4e131": {"ta_keywords": "privacy partial dnn;shredder learning noise;dnn intact shredder;accuracy shredder learning;shredder learning;dnns shows shredder;accuracy shredder learns;inference accuracy shredder;dnn inference acceptably;shredder learns additive;shredder enables inference;provide dnn inference;keeping dnn intact;data network;cloud;high accuracy shredder;communicated data cloud;data compromising cloud;provider keeping dnn;accuracy shredder;data cloud;keeping dnn;shredder learns;provide dnn;dnn inference;compromising cloud;network learns;shredder;partial dnn inference;cloud service", "pdf_keywords": "shredder learning noise;inference privacy arxiv;protect inference privacy;inference privacy;inference edge device;shredder learning;learning noise;learns additive noise;compute heavy inference;learning noise distributions;deep neural;noise devise shredder;distributions protect inference;protect inference;inference edge;network learns additive;network learns;heavy inference;balance accuracy privacy;accuracy privacy;trained network learns;noise distributions protect;heavy inference address;dnns showed shredder;deep neural applications;trained network;variety deep neural;disjoint offline learning;distribution noise devise;cloud"}, "464b47a6a395fa1338e230254965cf5f669e715c": {"ta_keywords": "neural machine translation;machine translation polysynthetic;translation polysynthetic languages;machine translation;resource machine translation;machine translation techniques;translation polysynthetic;translation techniques;polysynthetic languages;translation techniques developed;polysynthetic languages interestingly;fit polysynthetic languages;polysynthetic languages end;phrase based model;neural machine;character trigrams;languages interestingly;low resource machine;babbling baseline using;random babbling baseline;languages interestingly ii;using character trigrams;range neural machine;languages;character trigrams evaluated;babbling baseline;polysynthetic;difficult low resource;standard phrase based;resource machine", "pdf_keywords": ""}, "ca57443fcb87f03267fccee162a4924c56062c6f": {"ta_keywords": "computer based training\u306b\u3088\u308b\u975e\u8a00\u8a9e\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3\u30b9\u30ad\u30eb\u306e\u6539\u5584\u306b\u95a2\u3059\u308b\u691c\u8a0e;based training\u306b\u3088\u308b\u975e\u8a00\u8a9e\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3\u30b9\u30ad\u30eb\u306e\u6539\u5584\u306b\u95a2\u3059\u308b\u691c\u8a0e;training\u306b\u3088\u308b\u975e\u8a00\u8a9e\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3\u30b9\u30ad\u30eb\u306e\u6539\u5584\u306b\u95a2\u3059\u308b\u691c\u8a0e;computer based;computer;based", "pdf_keywords": ""}, "d3304b926cfcd91110bd5ba01db21d26ce5fca2d": {"ta_keywords": "paraphrastic sentence embeddings;learning paraphrastic;learning paraphrastic sentence;paraphrases translation bilingual;data learning paraphrastic;generate sentential paraphrases;sentential paraphrases translation;neural machine translation;sentence embeddings translated;paraphrase pairs ability;paraphrases translation;sentential paraphrases;english paraphrase pairs;paraphrase pairs;machine translation generate;machine translation;al learning paraphrastic;paraphrase pairs advantage;sentence embeddings process;sentence embeddings;paraphrastic sentence;paraphrases;paraphrastic;general purpose paraphrastic;embeddings translated;evaluate paraphrase pairs;embeddings translated bitext;bilingual sentence pairs;machine translation output;translation generate sentential", "pdf_keywords": "produce paraphrastic embeddings;effective paraphrase generation;paraphrastic embeddings par;paraphrastic embeddings;paraphrastic sentence embeddings;generated paraphrase corpus;paraphrase generation technique;paraphrase generation;improve generated paraphrase;learning paraphrastic;learning paraphrastic sentence;pretrained embeddings sentences;neural machine translation;written paraphrase datasets;data learning paraphrastic;paraphrase datasets;paraphrase pairs ability;paraphrase corpus;paraphrase corpus explored;pretrained word embeddings;paraphrase datasets signi\ufb01cantly;generated paraphrase;paraphrase pairs;produce paraphrastic;embeddings sentences;sentence embeddings;human written paraphrase;translating effective paraphrase;word embeddings;sentences evaluate paraphrase"}, "305a1251a68fb16835876d8c99de498472c0cd8f": {"ta_keywords": "coded computation emerging;coded computation;known coded computation;coded computation scheme;coded computation rederives;code coded computation;locality codes;locality codes using;design coded computation;coded computation lens;computation lens locality;computational locality;locality called computational;called computational locality;view coded computation;functions viewpoint locality;computational locality locality;code function computed;lens locality codes;best known coded;defined code coded;known coded;code coded;locality locality;locality based;local decoding scheme;coding theory;coded;locality;defined code function", "pdf_keywords": ""}, "3332dc72fbe3907e45e8a500c6a1202ad5092c0f": {"ta_keywords": "source separation deep;separation deep learning;trained speaker mixtures;deep clustering;speaker mixtures;spectrogram input mixtures;called deep clustering;spectral clustering embeddings;speaker mixtures improve;deep clustering test;mixtures multiple speakers;separation deep;source separation;model trained speaker;speaker independent model;spectral clustering;trained speaker;clustering embeddings;single channel mixtures;spectrogram;mixtures held speakers;analogue spectral clustering;channel mixtures;deep network;clustering embeddings form;channel mixtures multiple;deep learning;mixtures improve signal;speaker independent;train deep network", "pdf_keywords": "spectrogram speech separation;model speech separation;speech separation experiments;speech separation methods;channel speech separation;speech separation task;applied speech separation;separation deep learning;acoustic source separation;speech separation;speech separation including;source separation deep;regions spectrogram speech;separate speech signal;spectrogram speech;segmentation separation;speech signal mixture;embeddings segmentation separation;separate speech using;speaker independent model;auditory scene analysis;method separate speech;separate speech;source separation;separation deep;frequency regions spectrogram;segmentation separation arxiv;11 deep clustering;spectrogram regions;spectrogram regions dominated"}, "6e7cfed8815cce163efac9d17b1109849c050c6b": {"ta_keywords": "page independent heuristics;extracting data web;learning page independent;heuristics extracting data;data web pages;heuristics extracting;learning page;independent heuristics extracting;web pages;page independent;extracting data;pages;extracting;data web;page;independent heuristics;web;heuristics;data;learning;independent", "pdf_keywords": ""}, "7038b181f776e9cd587d4d61cb68692fdac8ec26": {"ta_keywords": "signalized traffic flow;signalized traffic systems;control signalized traffic;signalized traffic;traffic signal control;congestion koopman operator;signalized traffic propose;applications signalized traffic;cascading congestion koopman;determine traffic signal;traffic signal;infer traffic signal;traffic systems;problems signalized traffic;traffic systems demonstrate;traffic flow;determine traffic;traffic flow data;cascading congestion;traffic flow networks;congestion koopman;status determine traffic;timing directly traffic;directly traffic flow;operator applications signalized;koopman operator applications;infer traffic;method infer traffic;traffic;modes koopman operator", "pdf_keywords": "operator applications signalized;signalized traf\ufb01c systems;control signalized traf\ufb01c;koopman operator applications;signalized traf\ufb01c \ufb02ow;modes koopman operator;koopman operator theory;signalized traf\ufb01c;traf\ufb01c signal control;proposes koopman operator;koopman operator;algorithm dynamical mode;dynamical mode decomposition;applications signalized traf\ufb01c;control signalized;analysis control signalized;oscillatory modes koopman;cascading congestion;signal control parameters;koopman operator approximated;interactions network signalized;determine traf\ufb01c signal;operator applications;cascading congestion demonstrate;infer traf\ufb01c signal;network signalized;signalized traf\ufb01c \ufb01rst;traf\ufb01c signal;signal control;mode decomposition dmd"}, "1e58c9d1153d2f25d94b3a12b785bd7abe43bd1c": {"ta_keywords": "semi supervised;semi supervised learning;classification change points;activity recognition datasets;used semi supervised;learned autoencoder;semi supervised classification;human activity recognition;sequences train neural;activity recognition;leveraging change points;deep learning;sequences train;learned autoencoder obtain;neural network learn;change points labeled;learning contexts unsupervised;class changes leveraging;representations classification change;change point detection;learn improved representations;recognition datasets;dissimilar sequences train;supervised;large datasets sequences;classification change;datasets sequences;supervised learning contexts;improved representations classification;classification setting deep", "pdf_keywords": "semi supervised sequential;change point detection;semi supervised;unsupervised change point;semi supervised classi\ufb01cation;supervised sequential classi\ufb01cation;detection leveraging change;leveraging change points;classi\ufb01cation change points;activity recognition datasets;used semi supervised;human activity recognition;activity recognition;change points labeled;supervised sequential;unsupervised change;supervised classi\ufb01cation;learned autoencoder;classi\ufb01cation using change;supervised;recognition datasets;neural network learn;supervised classi\ufb01cation setting;unsupervised manner change;sequences train neural;learned autoencoder obtain;sequences train;point detection leveraging;segmenting unlabeled sequence;improved representations classi\ufb01cation"}, "5bad092098ba7400e19468a06cb8b238c43b7637": {"ta_keywords": "william douglas inquiry;douglas inquiry;douglas inquiry state;state individual freedom;william douglas;individual freedom;freedom;douglas;inquiry state individual;inquiry state;state individual;inquiry;william;individual;state", "pdf_keywords": ""}, "c81bb5ff79e8c7f65a3e28b7ba52d90deaa32fde": {"ta_keywords": "code switching arabic;switching arabic languages;code switching social;arabic wikipedia capture;arabic languages predictor;switching arabic;languages predictor social;collaborative editing code;influence collaborative editing;wikipedia editor success;wikipedia capture language;arabic languages;arabic wikipedia;code switching positively;task arabic wikipedia;code switching;wikipedia editor;collaborative editing;editing code switching;language pages;directly related arabic;capture language choice;associated wikipedia editor;code switching online;borrowing technical language;arabic speaking regions;effect code switching;languages predictor;arabic;languages", "pdf_keywords": ""}, "f26f17ec49f2593bcc926051394871480a80c0c2": {"ta_keywords": "bilingual word embedding;lingual word embedding;monolingual embedding;monolingual embedding spaces;density matching bilingual;expresses monolingual embedding;word embedding generally;vectors languages density;languages density matching;word embedding;matching bilingual word;embedding vectors languages;lingual word;matching bilingual;languages density;language pairs word;language pairs;embedding spaces probability;difficult language pairs;approaches cross lingual;bilingual word;cross lingual word;lingual;word embedding argue;cross lingual;embedding generally based;expresses monolingual;embedding generally;bilingual;monolingual", "pdf_keywords": "bilingual word embedding;densities monolingual embedding;lingual word embedding;lingual word embeddings;monolingual embedding;vector monolingual embedding;monolingual embedding space;density matching bilingual;monolingual embedding spaces;matching bilingual word;expresses monolingual embedding;lingual word similarity;bilingual lexicon;learning bilingual word;matching bilingual;word embedding generally;bilingual lexicon induction;set bilingual lexicon;densities monolingual;word embedding;probability densities monolingual;word embeddings;word embedding mappings;data set bilingual;embeddings represent words;bilingual word;vector monolingual;lingual word;approaches cross lingual;cross lingual word"}, "28028458d75bf9281200389a880741eb6d06a3a4": {"ta_keywords": "recovering software specifications;reconstruct abstract datalog;inductive logic programming;specifications inductive logic;abstract datalog specifications;software specifications inductive;extract specifications modules;specification recovery;logic programming methods;abstract datalog;extract specifications;used specification recovery;specification recovery specifically;logic programming;datalog specifications;datalog specifications certain;datalog;operation recovering software;recovering software;grendel2 extract specifications;database software examples;specifications inductive;specifications modules test;specifications modules;inductive logic;reconstruct abstract;used reconstruct abstract;type database software;shelf inductive logic;software examples operation", "pdf_keywords": ""}, "436380dd75d8ff3f2debb29913bd2fe8dde0b684": {"ta_keywords": "source separation factorize;separation factorize matrix;separation speech mixtures;separation factorize;methods source separation;source separation conventional;separation conventional nmf;source separation results;phase source separation;improvement source separation;matrix factorization approach;factorize matrix spectral;source separation;effective separation speech;matrix factorization;speech mixtures magnitude;complex matrix factorization;separation process spectral;results effective separation;matrix factorization problem;conventional nmf methods;factorize matrix;phase incorporated decomposition;factorization approach;phase included decomposition;nmf methods;separation speech;speech mixtures;effective separation;nmf methods source", "pdf_keywords": "source separation factorize;separation factorize matrix;separation factorize;factorize matrix spectral;channel source separation;phase source separation;single channel separation;methods source separation;matrix factorization approach;complex factorization method;complex matrix factorization;channel separation;matrix factorization;source separation proposed;generalized nmf method;matrix factorization jointly;conventional nmf methods;spectrum incorporated decomposition;source separation;separation proposed algorithm;making complex factorization;complex factorization;channel separation domain;factorization method method;source separation instead;factorization method;factorize matrix;nmf method applications;factorization approach;nmf methods"}, "6992f54509c139455c3cffa9b0e4ae5c19ebff82": {"ta_keywords": "asr multilingual conversations;multilingual conversations medical;multilingual asr;based multilingual asr;multilingual asr multilingual;asr multilingual;multilingual conversations;network based multilingual;conversations medical domain;multilingual;conversations medical;based multilingual;asr;medical domain;conversations;medical;network based;network;domain;based", "pdf_keywords": ""}, "30c6be4c7f549a2ec7328d24ecc0a54fbf90d41c": {"ta_keywords": "policies wireless networks;optimal control policies;control policies wireless;determine optimal policies;optimal policies;policy iteration;policies wireless;iteration policy iteration;iteration policy;optimal control;wireless networks numerical;value iteration policy;markov decision processes;policy iteration employed;develop optimal control;markov decision;policies moderate complexity;performance markov decision;policy structure;control policies;optimal policies moderate;wireless communication networks;loss performance markov;wireless networks;optimal;networks numerical;policy structure exploited;determine optimal;faster convergence rate;communication networks", "pdf_keywords": ""}, "4da018847a0f44378e6a1ded93fee672a3c7c370": {"ta_keywords": "speech recognition asr;new training decoding;training decoding;speech recognition;recognition asr;training decoding methods;recognition asr desired;enhance encoder network;automatic speech recognition;encoder;mask predict connectionist;encoder network;automatic speech;encoder network architecture;inference speed recognition;connectionist temporal classification;end asr based;predict connectionist temporal;fast inference speed;temporal classification ctc;asr based mask;capable fast inference;asr tasks proposed;predict connectionist;enhance encoder;tokens inference boost;different asr tasks;end end asr;asr based;inference boost performance", "pdf_keywords": "new training decoding;training decoding;training decoding methods;speech translation experimental;enhance encoder network;encoder;end speech translation;encoder network;mask ctc end;tokens inference boost;encoder network architecture;inference boost performance;decoding methods introducing;mask ctc;mask ctc achieving;performance mask ctc;decoding;enhance encoder;speech translation;improve mask ctc;inference boost;decoding methods;improved mask ctc;different asr tasks;propose enhance encoder;mask ctc \ufb01rst;asr tasks proposed;application mask ctc;end speech;end end speech"}, "abaf39dc9d1b156ddf387230611f5102378d052c": {"ta_keywords": "unsupervised ocr correction;attention unsupervised ocr;ocr post correction;ocr correction;unsupervised ocr;texts large corpora;observed textual variants;ocr correction design;decoding corpora;evidence decoding corpora;approach ocr post;textual variants bootstrapping;decoding corpora historical;novel approach ocr;approach ocr;ocr post;large corpora source;noisily observed textual;observed textual;exploits repeated texts;textual variants;ocr;training correction;ways training correction;large corpora;evidence decoding;source evidence decoding;repeated texts large;human annotation training;repeated texts", "pdf_keywords": "unsupervised ocr correction;ocr transcripts;ocr post correction;ocr correction;unsupervised ocr post;onebest ocr transcripts;manually transcribed texts;attention unsupervised ocr;unsupervised ocr;transcribed texts;propose unsupervised ocr;ocr transcripts 1384;ocr output sources;texts onebest ocr;best ocr output;transcribed texts onebest;approach ocr post;national library trove;manually transcribed;approach ocr;novel approach ocr;ocr post;decoding rdd newspaper;experiments best ocr;2017 resulting ocr;digitized text;texts large corpora;ocr output;best ocr;newspaper dataset"}, "63bfe58735f44b0af24da3c2cb6b1651b001b83c": {"ta_keywords": "coding distributed secret;distributed secret sharing;algorithm secret sharing;secret sharing networks;distributed secret;threshold secret sharing;secure network coding;secret sharing;secret sharing low;communication complexity algorithm;network coding distributed;problem secret sharing;secret sharing dealer;sharing networks satisfy;communication complexity;secret sharing important;secure multiparty computation;consider problem secret;algorithm secret;network coding;sharing low communication;communication complexity superlinear;coding distributed;protocols secure multiparty;condition secure network;shares secret;sharing networks;condition communication complexity;shares secret participant;network coding problem", "pdf_keywords": ""}, "5d69380565aa258bfa54005c9ba05e30675be227": {"ta_keywords": "classification tasks;semantic drift seeded;hierarchical classification tasks;semi supervised classification;datasets derived clueweb09;exploratory learning methods;supervised classification tasks;classification task experimented;builds exploratory learning;exploratory learning;derived clueweb09 corpus;classification;hierarchical semi supervised;exploratory learning 12;hierarchical classification;supervised classification;semantic drift;methods hierarchical classification;flat classification task;classes cause semantic;semi supervised;classification task;al exploratory learning;flat classification;supervised;classification tasks presence;learning methods hierarchical;outperforms existing exploratory;limited flat classification;semantic", "pdf_keywords": ""}, "62d6ccd01c2e022a385add5e689b4561b0fbfd88": {"ta_keywords": "speaker diarization rpnsd;rpnsd speaker diarization;speaker diarization region;diarization rpnsd speaker;based speaker diarization;speaker diarization;speaker diarization method;novel speaker diarization;speech segment proposals;proposals compute speaker;overlapped speech segment;speaker diarization important;speech segment;compute speaker embeddings;overlapped speech method;speaker embeddings;compute speaker;network based speaker;rpnsd speaker;generates overlapped speech;overlapped speech;speaker embeddings time;speech method;speech applications;deal overlapped speech;speech method neural;diarization rpnsd;segment proposals;step speech applications;speech applications aims", "pdf_keywords": "segment proposals speaker;speech segment proposals;speaker diarization rpnsd;proposals speaker embeddings;speaker diarization;based speaker diarization;novel speaker diarization;speaker diarization method;segment boundaries speaker;proposals compute speaker;boundaries speaker embeddings;predicts speech segment;speaker embeddings;speaker embeddings jointly;compute speaker embeddings;overlapped speech segment;speech segment;segment proposals;proposals speaker;compute speaker;network based speaker;segment proposals compute;speaker embeddings time;predicts speech;model predicts speech;diarization datasets;boundaries speaker;diarization datasets reveal;faster cnn 27;faster cnn"}, "9ba545841b837fa077579290e252eb00351ebeb0": {"ta_keywords": "convex distributed learning;distributed learning compression;distributed learning;distributed learning heterogeneous;federated learning;learning compression;communication compression strategy;communication compression;based compression gradient;important federated learning;novel communication compression;compression gradient;distributed order methods;biased gradient estimator;communication complexity bounds;federated learning develop;faster non convex;communication efficient method;compression strategy;compression gradient differences;strategy based compression;convex distributed;communication complexity;new communication efficient;communication efficient;gradient estimator;learning heterogeneous;compression strategy based;non convex distributed;biased gradient", "pdf_keywords": "federated learning;critical federated learning;convex optimization;non convex optimization;based compression gradient;important federated learning;federated learning marina;compression gradient;local loss functions;convex optimization problems;loss functions;deep learning;training deep learning;communication compression strategy;training deep;communication compression;deep learning models;novel communication compression;compression gradient differences;compression strategy;methods non convex;learning models;strategy based compression;loss functions owned;compression strategy based;case local loss;especially training deep;optimization;local loss;learning"}, "ea1f61270480a8dec54ec571c0e6ce116d096241": {"ta_keywords": "hmm speech recognition;sound event detection;polyphonic sound event;neural networks hmm;networks hmm speech;speech recognition proposed;speech recognition;segment sound event;hmm convolutional;hmm convolutional bidirectional;networks hmm;sound event;cblstm hmm convolutional;hybrid polyphonic sound;memory hidden markov;estimate hmm;model hybrid polyphonic;sound event post;hybrid polyphonic;hidden markov;detecting segment sound;estimate hmm state;hidden markov model;polyphonic sound;hmm state output;recurrent neural network;polyphonic;propose polyphonic sound;short term memory;propose polyphonic", "pdf_keywords": ""}, "598321d9c3eb5c035b449e19e539b6fa04b3802a": {"ta_keywords": "billion crawled urls;billion crawled;general web crawl;large web corpus;crawled;web crawl;crawl date billion;web corpus;corpus 10 billion;date billion crawled;web corpus 10;crawled urls;large web;tokens licensed creativecommons;billion tokens licensed;billion tokens;10 billion tokens;commoncrawl largest publicly;languages extracted commoncrawl;crawl;general web;licensed creativecommons license;50 languages extracted;creativecommons license;web crawl date;licensed creativecommons;corpus;crawl date;tokens licensed;extracted commoncrawl", "pdf_keywords": ""}, "8f1f43408baf1ccb0ec3e7985592326c83ee276d": {"ta_keywords": "chat oriented dialog;movie twitter scripts;statistical machine translation;building chat oriented;approaches building chat;machine translation;building chat;machine translation smt;syntactic semantic similarity;utilize movie twitter;twitter scripts;twitter scripts performance;semantic similarity tf;movie twitter;chat oriented;dialog;metrics syntactic semantic;semantic similarity retrieval;semantic similarity;similarity evaluation metrics;similarity retrieval response;translation smt approaches;similarity tf idf;dialog investigates combined;similarity retrieval tf;phrase based smt;similarity evaluation;retrieval response generation;similarity retrieval;oriented dialog investigates", "pdf_keywords": ""}, "07a9f47885cae97efb7b4aa109392128532433da": {"ta_keywords": "hard coded attention;attention heads encoder;coding cross attention;attention connects decoder;learned cross attention;coded attention variant;coded attention;decoder encoder significantly;encoder significantly;achieving high translation;language pairs bleu;high translation quality;attention variant learned;learned self attention;cross attention;attention head hard;attention variant;cross attention head;encoder;self attention heads;cross attention connects;important self attention;encoder decoder;attention;decoder encoder;encoder decoder fixed;translation quality push;minimally impacts bleu;attention head;impacts bleu scores", "pdf_keywords": "attention crucial translation;improving multiheaded attention;achieving high translation;crucial translation quality;attention allows decoder;multiheaded attention;hard coded attention;high translation quality;attention hard coded;neural machine translation;decoder self attention;compared cross attention;translation quality compared;translation quality;attention head hard;attention neural;cross attention hard;attention neural machine;attention mechanisms hard;multi headed attention;machine translation;attention head;gaussian attention;coded attention concretely;coded attention;attention crucial;gaussian attention neural;coded gaussian attention;multiheaded attention draw;cross attention"}, "8afbc4188be9e9452ce1fe868ebe217179d36793": {"ta_keywords": "speech spectra;speech spectra represented;noisy speech spectra;models noisy speech;sparse representation;sparse representation obtained;problems sparse representation;speech noise sources;speech exemplars noise;individual speech noise;sparsest possible linear;spectrum used recognition;sparse combination basis;noisy speech;finding sparsest possible;clean speech exemplars;finding sparsest;obtained finding sparsest;speech noise;linear typically sparse;clean noisy spectra;matrix factorisation nmf;speech exemplars;spectra represented linear;sparsest possible;describing individual speech;noise sources reconstruction;approach problems sparse;sparsest;compositional models noisy", "pdf_keywords": ""}, "3426fadf73a5ce418486e640b26b3d2470d932b5": {"ta_keywords": "multilingual adversarial speech;multilingual adversarial;massively multilingual adversarial;adversarial speech recognition;adaptation multilingual end;adaptation multilingual;target pretraining languages;pretraining languages dimensions;adversarial speech;trained 100 languages;pretraining languages;languages dimensions phonetics;end speech recognition;speech recognition models;speech recognition;massively multilingual;multilingual;report adaptation multilingual;dimensions phonetics phonology;multilingual end;phonetics phonology language;100 languages findings;phonetics phonology;orthography massively multilingual;100 languages;dimensions phonetics;phonetics;languages dimensions;languages findings;similarity target pretraining", "pdf_keywords": "pretraining multilingual asr;pretraining languages target;pretraining multilingual;explore pretraining multilingual;pretraining languages diverse;multilingual asr models;language adversarial pretraining;multilingual speech dataset;pretraining languages;pretraining objectives multilingual;pick pretraining languages;multilingual asr model;particularly pretraining languages;similarity pretraining languages;language target speakers;languages target speakers;multilingual speech;languages used pretraining;multilingual asr;models trained languages;phoneme language adversarial;target language;adapted target language;wilderness multilingual speech;trained languages similar;trained languages;language target;multilingual seed model;asr model training;models using speech"}, "8de431e0e62653711136836642af38179731c2f0": {"ta_keywords": "secure erasure codes;erasure codes distributed;erasure coded distributed;codes distributed storage;coded distributed storage;secure erasure;theoretically secure erasure;erasure codes;erasure coded;coding schemes repair;operations erasure coded;coded distributed;codes distributed;distributed storage systems;ensure security data;distributed storage;security data presence;bounds storage network;distributed storage potentially;lower bounds storage;security risk repair;bounds storage;proposed codes optimal;storage potentially expose;storage systems;security data;storage network;repair operations erasure;coding schemes;eavesdroppers active adversaries", "pdf_keywords": "secure erasure codes;erasure codes distributed;codes distributed storage;secure erasure;ef\ufb01ciency codes security;theoretically secure erasure;erasure codes;erasure codes repair;presents erasure codes;codes security;bounds storage network;security data presence;distributed storage;distributed storage systems;ensure security data;lower bounds storage;explicit codes distributed;operations distributed storage;reliability ef\ufb01ciency codes;distributed storage nihar;codes repair algorithms;bounds storage;data passive eavesdroppers;codes security active;security data passive;codes distributed;distributed storage msr;ef\ufb01ciency codes;security data;storage systems"}, "fdf1aec2da3597010c31138159574b1016019f73": {"ta_keywords": "computational social choice;research computational social;computational social;preference reasoning uses;preference reasoning areas;preference reasoning;social choice preference;driven research computational;group decision making;choice preference reasoning;research computational;social choice;voting resource allocation;choice preference;empirical computer scientists;computational;data driven;voting resource;group decision;preference;establish data driven;data driven application;driven research;computer scientists toolkit;decision making argue;development empirical computer;data building human;topics group decision;data building;reasoning uses tools", "pdf_keywords": ""}, "538deb39d57bef62833c492a56c796a2bafa340f": {"ta_keywords": "parsing active learning;machines active learning;vector machines active;active learning using;active learning;instances active learning;support vector machines;active learning provide;active learning support;learning support vector;shallow parsing active;entity recognition;parsing active;entity recognition treated;named entity recognition;vector machines;support vector;use support vector;learning using linear;shallow parsing;function kernels observed;function kernels;learning using;kernels observed;kernels observed distribution;tackled shallow parsing;basis function kernels;kernels;learning support;machines active", "pdf_keywords": ""}, "63e7e3b16e03da62a2c535ac9cfccfa3ae48b292": {"ta_keywords": "human ai team;ai team human;accurate ai team;ai team;ai team mate;training ai systems;ai systems human;ai systems recommend;human ai;directly optimizing team;ai recommendation;training ai;optimizing team performance;type human ai;propose training ai;ai advised decision;optimizing team;accept ai recommendation;sacrifice ai accuracy;team human overseer;ai accuracy;ai advised;accurate ai;argue accurate ai;making ai;ai;ai systems;team utility;decision making ai;teammate example predictable", "pdf_keywords": "human ai teaming;ai teaming human;human ai teams;ai collaboration optimize;ai teaming;human ai collaboration;ai teams;directly optimized team;collaboration optimize team;modeling teamwork training;teaming human;optimize team;ai optimization;optimized team performance;work ai optimization;ai collaboration;teaming human overseer;optimized team;optimize team performance;ai teams contrast;teamwork training improvements;human ai;ai systems trained;ai optimization problems;accuracy ai;type human ai;making human ai;ai recommendation;team performance experiments;datasets accuracy ai"}, "b2baf9e053c32abfb3c8658b9bc6d6790ae671cb": {"ta_keywords": "reading using eye;eye tracking features;reading using svms;unknown word detection;word detection;using eye tracking;gaze based unknown;eye gaze based;eye tracking;detect unknown words;gaze duration word;native language reading;gaze based;eye movement features;natural reading using;words natural reading;word detection non;eye gaze;language reading using;support vector machines;svms random forests;svm random forests;novel eye movement;gaze duration;language reading;eye movement;approach utilizes gaze;utilizes gaze duration;natural reading;gaze", "pdf_keywords": ""}, "15931520cce546bbf19b4cebeb4161c4debeabe7": {"ta_keywords": "behavior single winner;decisions using voting;modeling voters multi;outcome voting;winner approval voting;employing voting;modeling voters;voting scenarios;favors modeling voters;outcome voting way;behaviors multi winner;using voting scenarios;better outcome voting;employing voting rules;voters multi winner;agent manipulate ballot;multi winner approval;multiple winners scenarios;voting scenarios varying;voting av agent;examine voting behavior;votes choosing candidates;elections employing voting;voting behavior;voting behavior single;winners scenarios agent;winners scenarios;candidates wish winners;approval voting scenarios;tallying votes choosing", "pdf_keywords": "behavior single winner;winner approval voting;heuristic threshold approval;behaviors multi winner;multi winner approval;accuracy heuristics model;voting scenarios;utility heuristic threshold;heuristics accuracy heuristics;voting scenarios varying;setting single winner;approval voting scenarios;heuristic threshold;winner approval;voting behavior single;effectiveness heuristics accuracy;heuristics model;heuristics accuracy;attainability utility heuristic;voting behavior;examine voting behavior;accuracy heuristics;effectiveness heuristics;multi winner;winner multi winner;threshold approval voting;utility heuristic;obtained mechanical turk;heuristics model realworld;examine effectiveness heuristics"}, "931cbd9d689e9fd6bd91f4e8e1dbdd7fbb6df9de": {"ta_keywords": "speech recognition end;recognize speech mixtures;target speech recognition;separation systems speakerbeam;inserting speech separation;speech separation mechanism;speech separation;alternative speech separation;e2e automatic speech;speech recognition;speech recognition promising;speech enhancement diarization;speech separation mitigate;combine speakerbeam e2e;recognition results speaker;automatic speech recognition;speech recognition asr;sequence speech features;results speaker mixture;speech features sequence;speech mixtures inserting;speech recognition results;speech mixtures;channel target speech;speakerbeam e2e asr;speaker mixture;speech enhancement;automatic speech;extended recognize speech;training target speech", "pdf_keywords": ""}, "a445adf335aa5212f929f67c1ca56a62c221b43a": {"ta_keywords": "discovering unknown unknowns;discovering unknown;training data predictive;unknowns given predictive;unknown unknowns intelligently;strategy discovering unknown;unknowns intelligently guide;informed discovery;problem informed discovery;identifying unknown unknowns;discovery unknown unknowns;unknowns intelligently;informed discovery unknown;predictive models deployed;identify unknown unknowns;strategy discovering;data predictive;discovering;data predictive models;training data cases;unknown unknowns occur;unknowns occur systematic;confidence identifying unknown;predictive models;unknown unknowns;intelligently guide discovery;systematic biases training;predictive;predictive model utilizes;predictive model unknown", "pdf_keywords": "discovering unknown unknowns;discovering unknown;problem informed discovery;discover unknown unknowns;unknowns given predictive;partitions based feature;informed discovery;exploit strategy discovering;strategy discovering unknown;discovering;unknown unknowns intelligently;strategy discovering;identify unknown unknowns;discovery unknown unknowns;unknowns partitions based;unknowns discovered querying;components discovery;predictive model deployed;discovered querying;unknowns intelligently guide;discovered querying oracle;informed discovery unknown;constituent components discovery;unknowns intelligently;instances similar feature;unknown unknowns different;wild formulate optimization;discover unknown;instances chosen algorithmic;intelligently guide discovery"}, "947750c717a5fbd17fc52758322d1ca201c4c6bc": {"ta_keywords": "mining nlp disaster;entity recognition tweet;nlp disaster;nlp disaster paper;tweet classification safety;information mining nlp;tweet classification;efforts nlp researchers;named entity recognition;mining nlp;nlp researchers;efforts nlp;nlp researchers create;recognition tweet classification;entity recognition;describes efforts nlp;safety information mining;recognition tweet;safety people disaster;nlp;stricken area twitter;disaster paper describes;word segmentation;classification safety information;twitter;systems word segmentation;information mining;word segmentation named;people disaster stricken;segmentation named entity", "pdf_keywords": ""}, "c45a23e7c565169c5a55898683aceac458c116bb": {"ta_keywords": "speech separation recognition;chime speech separation;model speech enhancement;rnns model speech;memory lstm enhancement;lstm enhancement network;lstm enhancement;speech recognition;decoding speech recognition;speech enhancement language;speech recognition systems;speech separation;speaker adaptive training;model speech;dnn based acoustic;speech enhancement;recognition challenge chime;term memory lstm;data augmentation speaker;decoding speech;rnn language models;lstm;augmentation speaker adaptive;chime benchmark;challenge chime combining;chime speech;3rd chime speech;separation recognition challenge;speaker adaptive;memory lstm", "pdf_keywords": ""}, "2b0aa68ef2c1773642ca91627a4fc03f536cc5fc": {"ta_keywords": "statelens", "pdf_keywords": ""}, "f762ce106b37728df1126375981a02a589e0497c": {"ta_keywords": "stochastic gradient descent;randomized coordinate descent;sgd methods unified;iterates stochastic gradients;sgd methods;stochastic gradients;stochastic gradients framework;batch sampling quantization;sgd randomized coordinate;gradient descent;proximal stochastic gradient;sgd randomized;stochastic gradient;quantized methods;non quantized methods;reduction importance sampling;sampling quantization;mini batch sampling;importance sampling mini;coordinate descent;variants proximal stochastic;tt sgd methods;quantized methods paper;sampling mini batch;proximal stochastic;sampling quantization coordinate;gradient descent tt;sampling mini;tt sgd randomized;variance reduction", "pdf_keywords": "reduced sgd methods;sgd variance reduction;stochastic gradient descent;sgd methods;randomized coordinate descent;sgd methods \ufb01rst;variance reduced sgd;gradient descent sgd;theory sgd variance;quantization coordinate descent;theory sgd randomized;sgd randomized coordinate;sgd variance;sgd randomized;non quantized methods;descent sgd;proximal stochastic gradient;quantized methods;quantized methods uni\ufb01ed;reduced sgd;variants proximal stochastic;variance reduction sampling;gradient descent;stochastic gradient;theory sgd;reduction sampling quantization;variance reduction;uni\ufb01ed theory sgd;descent rcd methods;descent sgd far"}, "ec084dac14a069da2e924ff7f3d5d2fb75b9b39a": {"ta_keywords": "sentiment sufficient dimension;rich sentiment information;multinomial inverse regression;sentiment information;sentiment variables;sentiment information particular;regression phrase counts;multinomial logistic regression;sentiment variables research;representations rich sentiment;dimensional response text;multinomial logistic;reduction text data;gamma lasso;sentiment sufficient;multinomial inverse;logistic regression phrase;literature nonconcave likelihood;dimension reduction text;connected sentiment variables;phrase counts;phrase counts document;developed multinomial logistic;dimensional document representations;multinomial distribution logistic;likelihood penalization;gamma lasso scheme;lasso;document representations rich;dimensional logistic regression", "pdf_keywords": "multinomial inverse regression;inverse regression text;regression text analysis;rich sentiment information;2013 multinomial inverse;multinomial logistic regression;regression text;multinomial inverse;multinomial logistic;regression phrase counts;sentiment information;sentiment variables;developed multinomial logistic;multinomial distribution logistic;sentiment variables research;sentiment information arxiv;inverse regression introduced;representations rich sentiment;aug 2013 multinomial;logistic regression phrase;dimension document representations;multinomial;connected sentiment variables;2013 multinomial;phrase counts document;inverse regression;phrase counts;regression phrase;modeling novel estimation;developed multinomial"}, "bc251481aa5566b1e86a8dbd0417cdf858205e3b": {"ta_keywords": "patterns fake news;fake news detection;fact based fake;fake news posts;fake news researchers;defend fake news;news posts claim;fact based models;based fake news;posts claim fact;fact based methods;fact based;veracity considering patterns;news detection model;news posts;fake news;claim fact based;news detection;learning pattern fact;model preference learning;pattern fact based;based fake;news researchers;posts claim;claim veracity considering;shared patterns fake;claim veracity;making pattern fact;patterns fake;preference learning", "pdf_keywords": "fake news detection;fact based fake;fact based models;news detection model;aware fake news;defend fake news;based fake news;fake news researchers;model preference learning;learning patternand fact;preference aware fake;news detection;fact based;news detection framework;pattern fact based;patternand fact based;preference learning;fake news;making pattern fact;detection model preference;preference learning arxiv;based fake;preferences pattern fact;based patterns facts;models based patterns;aware fake;modeling preference differences;patterns facts end;news researchers;modeling preference"}, "dc2f6f092fa04e334dfe2e8592b6d597e00b97ca": {"ta_keywords": "helpful extraction hypernyms;semantic class hypernyms;extraction hypernyms;aware semantic classes;sense aware semantic;automatically extracted hypernyms;extraction hypernyms furthermore;hypernymy extraction terms;semantic classes helpful;hypernymy extraction;distributionally induced semantic;extracted hypernyms;labeling semantic class;semantic classes;induced semantic classes;quality hypernymy extraction;semantic classes filtering;distributional semantics using;labeling semantic;distributional semantics;extracted hypernyms using;noisy hypernymy relations;using distributional semantics;aware semantic;recall denoising hypernyms;hypernyms performed labeling;semantic classes using;semantic class;domain taxonomy induction;performed labeling semantic", "pdf_keywords": "distributional semantics graph;semantic class hypernyms;aware semantic classes;semantics graph clustering;sense aware semantic;induced semantic classes;distributional semantics using;domain taxonomy induction;distributional semantics;semantic classes;using distributional semantics;proposed semantic classes;aware semantic;noisy hypernymy relations;text denoising hypernyms;semantics graph;labeling semantic class;noisy hypernymy databases;labeling semantic;taxonomy induction text;induced semantic;semantic class;semantic classes using;taxonomy induction;using induced semantic;semantic classes \ufb01ltering;semantic classes used;hypernyms performed labeling;semantic;hypernymy databases extracted"}, "02a83a01d6236149e4ead01e202b2453f9590e9e": {"ta_keywords": "group fairness incompatibilities;group fairness notions;group fairness;group fairness class;including group fairness;fairness objectives elements;fairness class fairness;fairness objectives;group fairness fact;fairness confusion tensor;optimize accuracy fairness;accuracy fairness objectives;fairness incompatibilities;diagnostic group fairness;fairness incompatibilities propose;majority group fairness;class fairness notions;group fairness trade;class fairness;offs group fairness;fairness notions expressed;fairness notions;fairness class;fairness trade offs;fairness confusion;accuracy fairness;fairness notions measure;notions expressed fairness;expressed fairness confusion;expressed fairness", "pdf_keywords": ""}, "c263507db2c15a8b2e3c955bda7b3c29a1ebd106": {"ta_keywords": "wild intent detection;intent detection systems;intent detection;intent detection wild;detection wild intent;bert based classifier;chatbots facilitates penalising;nlu platforms bert;platforms bert based;received chatbots facilitates;chatbots facilitates;chatbots;wild intent;platforms bert;queries received chatbots;intent;evaluate nlu platforms;bert based;bar intent detection;datasets crowdsourced datasets;received chatbots;crowdsourced datasets;datasets crowdsourced;unintended patterns training;nlu platforms;complexities imbalanced datasets;training data hint3;intent unintended correlations;bert;existing datasets crowdsourced", "pdf_keywords": "analyzed intent detection;intent detection systems;intent detection new;intent detection;intent detection wild;abstract intent detection;intents;analyzed intent;intent;bar intent detection;diverse set intents;abstract intent;intent unintended;intents single;set intents;paper analyzed intent;meanings intents;bar intent;nlu platforms bert;platforms bert based;2020 abstract intent;chat bots;nlu platforms dialog\ufb02ow1;evaluate nlu platforms;perceived meanings intents;perception intent unintended;intent unintended correlations;platforms bert;chat bots period;varying perception intent"}, "9bd170248355047067f05349d57110cc8e4de5cf": {"ta_keywords": "billion crawled urls;billion crawled;general web crawl;large web corpus;crawled;web crawl;crawl date billion;web corpus;corpus 10 billion;date billion crawled;web corpus 10;crawled urls;large web;tokens licensed creativecommons;billion tokens licensed;billion tokens;10 billion tokens;commoncrawl largest publicly;languages extracted commoncrawl;crawl;general web;licensed creativecommons license;50 languages extracted;creativecommons license;web crawl date;licensed creativecommons;corpus;crawl date;tokens licensed;extracted commoncrawl", "pdf_keywords": ""}, "9952fe8cbd09e4fc89dc7d76595d138e36c7d7b5": {"ta_keywords": "evaluation transferability bert;transferability bert based;transferability bert;ranking models english;bert based neural;compared transfer learning;neural ranking models;models english datasets;compare transfer learning;high annotation costs;neural ranking;training pseudo labels;queries high annotation;transfer learning;transfer learning training;annotated queries produce;annotated queries;based neural ranking;english datasets furthermore;bert based;english datasets;systematic evaluation transferability;learning training pseudo;number annotated queries;annotation costs;ranking models;high annotation;annotation costs making;bert;enables shot evaluation", "pdf_keywords": "transferability bert based;bertbased neural ranking;evaluation transferability bertbased;bert based ranking;study transferability bert;transferability bert;transferability bertbased neural;labeling bert based;labeling bert;transferability bertbased;pseudo labeling bert;ranking models english;neural ranking models;evaluation transfer learning;bertbased neural;retrieval models ranking;neural ranking;compare transfer learning;ranking models compare;models ranking carry;models ranking;transfer learning pseudo;ranking models;systematic evaluation transferability;based ranking models;bert based;ranking models iurii;transfer learning;evaluation transferability;bertbased"}, "1f76ee8472ec3a41511540f62e4676317df14ea5": {"ta_keywords": "singing voice conversion;convert singing voice;perceptual age singing;statistical voice conversion;possible convert singing;singing voice age;singing voice timbre;convert singing;age control singing;voice conversion;voice conversion technique;age singing voice;control singing voice;voice timbre control;statistical voice;use statistical voice;voice timbre arbitrary;singing voice;varieties voices singers;singer regression;age singing;voice timbre;voice timbre varieties;voices singers produce;target singer regression;voice age singer;voice age;control singing;singer regression approaches;conversion svc singers", "pdf_keywords": ""}, "5a5fb155b5fc518389a7fe67b55271e143ad695d": {"ta_keywords": "community recovery hypergraphs;community recovery graphs;hypergraphs studied community;hypergraphs data clustering;recovery hypergraphs studied;recovery hypergraphs data;recovery graphs;recovery graphs extensively;recovery graphs popular;recovery hypergraphs;hypergraphs data;hypergraphs studied;hypergraphs;limit community recovery;graphs extensively studied;studied community recovery;censored block model;generalized censored block;nodes hyperedge;nodes hyperedge corrupted;noise community recovery;clustering;hyperedge corrupted bernoulli;limit community;community recovery;graphs extensively;theoretic limit community;bernoulli noise community;problem community recovery;clustering core", "pdf_keywords": "community recovery hypergraphs;hypergraphs generalized censored;recovery hypergraphs generalized;recovery hypergraphs;community parity measurement;nodes community parity;reveals nodes community;censored block models;network clustering;hypergraphs generalized;hypergraphs;reconstruct communities;nodes community;community parity;reconstruct communities considered;face clustering;network clustering motion;face clustering protein;required reconstruct communities;clustering protein complex;abstract community recovery;community recovery;communities;generalized censored block;recovery hypergraphs kwangjun;community recovery central;clustering;clustering protein;communities considered models;nodes characterize"}, "17c7c92db1f4ace842f9db6b44bfce264308b628": {"ta_keywords": "labeled parsing deep;phrase vectors deep;parsing deep;parsing deep inside;learned phrase vectors;unsupervised labeled parsing;outside recursive autoencoders;recursive autoencoders diora;recursive autoencoders;latent code learning;labeled parsing;recursive autoencoders work;phrases unsupervised labeled;human annotations improving;autoencoders;autoencoders diora;autoencoders work effectively;autoencoders diora additionally;annotations improving;annotations improving previous;improve model labeling;labels using learned;vectors deep;autoencoders work;phrase vectors;model labeling accuracy;speech tags;code learning training;outperforms elmo bert;vectors deep inside", "pdf_keywords": ""}, "753fd6952c9f06f3bbd46e37129acc3f7a984896": {"ta_keywords": "retrieve relevant passages;retrieve passages textual;retriever relevant passages;text generation systems;generating informative utterances;textual knowledge corpus;text generation;generating informative;wikipedia providing passages;passages textual knowledge;corpus wikipedia providing;textual knowledge;additional context generator;generator underperform retriever;retrieve passages;knowledge corpus;like generating informative;retriever generator underperform;retriever generator;context generator;knowledge corpus wikipedia;retriever retrieve passages;retrieve relevant;generation tasks like;context generator model;generation tasks;tasks like generating;standard retriever generator;retriever generator maximizing;additional guide retriever", "pdf_keywords": "thoroughly evaluate retrievalaugmented;informative conversations retriever;retriever generator retrieval;evaluate retrievalaugmented systems;generator retrieval augmented;conversations retriever achieves;evaluate retrievalaugmented;generator retrieval;retrievalaugmented;retrievalaugmented systems;words retrieved passage;supervision retriever generator;rare words retrieved;generation task free;retrieval;retrievalaugmented systems going;generation considering retrieved;conversations retriever;retrieve relevant passages;words retrieved;retrieval augmented;retriever improve supervision;generation tasks;improve supervision retriever;nlgen dataset thoroughly;dataset informative conversations;retrieved passages generator;guide retriever improve;retrieved passages;generation task"}, "7fa3a5318ac45b2fd93a0130f0ceba9995ffa3c0": {"ta_keywords": "slang languages cross;computing cross cultural;languages cross cultural;lingual natural language;useful machine translation;common cross lingual;entities finding similar;similarities social media;machine translation applications;slang languages;languages cross;social media mining;terms slang languages;cultural differences named;cross lingual;machine translation;translation applications;translation applications research;cross lingual natural;mining cross cultural;differences named entities;finding similar;cultural differences similarities;cross cultural differences;finding similar terms;natural language;similar terms slang;differences named;cross cultural;differences similarities common", "pdf_keywords": ""}, "8306e4a566e2b1279d5d67b40facc8e1e345c4e3": {"ta_keywords": "bidirectional compression ef21;contractive compression operators;compression ef21;contractive compression;construction markov compressor;communication compression strategies;compression strategies;markov compressor;bidirectional compression;enhanced communication compression;compressed rate gradient;communication compression;compression operators;compression strategies based;gradients compressed rate;application contractive compression;markov compressor induced;gradient descent regime;momentum bidirectional compression;regime gradients compressed;convergence distributed gradient;compression operators work;nonconvex regime gradients;rate gradient descent;compression;ef smooth nonconvex;gradient descent;distributed gradient based;convergence rates best;gradients compressed", "pdf_keywords": "known rate ef;rate ef smooth;ef21 bells whistles;gradient descent regime;ef21;2021 ef21;ef smooth nonconvex;ef smooth;ef relies strong;rate ef;theory ef relies;theory ef;existing theory ef;ef relies;pessimistic convergence rates;ef;regime gradients compressed;oct 2021 ef21;nonconvex regime gradients;rate gradient descent;gradient descent;convergence rates best;gradients provides pessimistic;ef21 bells;2021 ef21 bells;gradients compressed rate;regime gradients;compressed rate gradient;smooth nonconvex regime;convergence rates"}, "bad5d2d6d1f3282ebbcb602a6f3a5dd9488fd713": {"ta_keywords": "language phoneme recognition;phone recognition datasets;phoneme recognition;phoneme recognition fine;multilingual recognition;fine tuning phonetic;recognition based phonetic;phonetic annotation;language phoneme;phone recognition;based phonetic annotation;curate phone recognition;phoneme recognition using;phonetic annotation incorporates;tuning phonetic representations;tuning phonetic;appear language phoneme;multilingual recognition based;phonemes appear language;focus phoneme recognition;method multilingual recognition;endangered tangkhulic language;phonetic representations;recognition datasets;dataset endangered tangkhulic;phone set phonemes;1000 utterances;language variety spoken;recognition fine tuning;based phonetic", "pdf_keywords": "phoneme recognition;phone recognition datasets;multilingual recognition;phone recognition new;recognition based phonetic;phone recognition;phonetic annotation;phoneme recognition using;reasonable phone recognition;recognition new language;based phonetic annotation;curate phone recognition;phonetic annotation incorporates;method multilingual recognition;multilingual recognition based;phonemes appear language;phone representations quickly;focus phoneme recognition;spoken kenya uganda;phone representations;universal phone representations;phone recognizer;allosaurus method multilingual;spoken kenya;endangered tangkhulic language;phone recognizer based;model phone recognizer;phone set phonemes;phonetic;burman language variety"}, "c64843e51f24773c895511ba9befa8a9bc4924a9": {"ta_keywords": "perceptual age singing;age singing voices;age singing voice;voice conversion perceptual;singing voices prosodic;age singing;age spectral prosodic;perception singer age;singing voice conversion;acoustic features singing;features singing voice;age singer speech;voices prosodic features;perceived age singer;processing statistical voice;statistical voice conversion;singer speech analysis;singing voices;statistical voice;singing voices corresponds;voice conversion;features singing;voice conversion based;singing voice voice;singing voice;voice voice conversion;singer age;varieties voices singers;singing voice singers;voices prosodic", "pdf_keywords": ""}, "8ef0ca924ae88ac2bb2803c49589722b52efc5b4": {"ta_keywords": "corpus social affective;affective interaction corpus;corpora social affective;emotion discourse structure;emotion discourse;terms emotion discourse;corpus terms emotion;conversations real emotion;affective interactions english;social affective interactions;social affective interaction;interactions english indonesian;presents corpus social;analysis social affective;interaction corpus;interaction corpus english;emotionally rich corpora;real emotion occurrences;affective interactions;affective interaction;corpora containing social;corpus social;corpus english indonesian;emotion occurrences;social affective;containing natural conversations;social affective aspects;corpora social;containing social interactions;various television talk", "pdf_keywords": ""}, "192fc995631e3443bb7f291a971089bd06e61017": {"ta_keywords": "annotating science questions;human annotation science;annotation science;annotating science;ai2 reasoning challenge;introduces ai2 reasoning;interface annotating science;annotation science question;human annotation;interface human annotation;ai2 reasoning;annotating;annotation;knowledge reasoning types;classification new questions;knowledge reasoning;reasoning challenge arc;reasoning types;reasoning challenge;interface annotating;types knowledge reasoning;introduces ai2;challenge arc associated;ai2;challenge set interface;respective knowledge reasoning;challenge arc;work introduces ai2;question answer pairs;knowledge reasoning required", "pdf_keywords": ""}, "39ffb5e9f2f36df42ef8ea010499e484c913e79e": {"ta_keywords": "approach spelling normalization;spelling normalization;spelling normalization combines;employ spelling normalization;spelling normalization map;taggers used normalized;normalization methods previously;normalization map historical;map historical wordforms;data normalization methods;normalization methods;training data normalization;data normalization;normalized;normalization;normalization algorithms;historical wordforms modern;used normalized;normalization combines;normalized data;historical wordforms;normalized data common;different normalization algorithms;used normalized data;combines different normalization;different normalization;normalization algorithms evaluates;texts historical german;normalization map;normalization combines different", "pdf_keywords": ""}, "26f427d2b27828f2893e95344342570699e9c589": {"ta_keywords": "achieve code conversions;code conversions;encoding resource overhead;code conversion;encoded code maintaining;allow code conversions;convertible codes optimal;code conversions resource;code conversions significantly;code data encoded;convertible codes broad;code conversion process;code conversion particular;encoded code data;data encoded code;maintaining desired decodability;codes broad range;code pairs;accessed code conversion;data traditional codes;code rate encoded;codes optimal;notion code conversion;code data;introduce convertible codes;code pairs allow;mds convertible codes;approach encoding resource;codes optimal parameter;mds decodability constraint", "pdf_keywords": ""}, "5c333f11431d1f0d04ced62b712c8d05ebac0891": {"ta_keywords": "model speech enhancement;speech enhancement experiments;speech enhancement;speech enhancement algorithm;novel speech enhancement;speech signal diffusion;probabilistic model speech;observed noisy speech;noisy speech;noises estimated speech;model speech;noisy speech signal;estimated speech signal;advances diffusion probabilistic;estimated speech;diffusion probabilistic models;conditional diffusion probabilistic;diffusion probabilistic;gaussian real noises;diffusion probabilistic model;real noises estimated;formulation diffusion probabilistic;datasets noise characteristics;noises estimated;enhancement algorithm incorporates;generative models;speech signal work;named conditional diffusion;noise characteristics unseen;speech signal", "pdf_keywords": "generative speech enhancement;speech enhancement diffusion;generative speech;speech enhancement;world speech enhancement;compared generative speech;speech enhancement approaches;speech enhancement problems;speech enhancement showed;25 speech enhancement;enhancement diffusion probabilistic;terms speech enhancement;probabilistic model generative;noises estimated speech;audio synthesis;generative model deep;generative models;enhancement diffusion;model generative;generative models investigate;generative model;generative models shown;estimated speech;gaussian real noises;audio synthesis 23;model generative model;noise statistics;generative;estimated speech signal;noise statistics real"}, "c82854b8d4c715da141d34c73bf9bda67adf307c": {"ta_keywords": "modeling polarizing topics;polarization news topics;topic polarization frequently;analyzing political blogs;sentiment polarity prediction;topic polarization;polarizing topics;political blogs;polarity based topics;polarizing topics different;political blogs posts;sentiment polarity comments;level topic polarization;assign sentiment polarity;sentiment polarity;responses news topics;polarization news;using sentiment polarity;accuracy sentiment polarity;observed using sentiment;assign sentiment;polarity comments serve;polarity comments;sentiment polarity comment;models predict polarity;communities jointly predict;topics blog content;community responses news;based topics blog;predict polarity based", "pdf_keywords": ""}, "30a30781c66c758e8e59cdb00c368f3add99768b": {"ta_keywords": "speaker recognition augmentation;training unsupervised speaker;unsupervised speaker representations;augmentation adversarial training;unsupervised speaker recognition;network discriminative speaker;voxceleb voices datasets;unsupervised speaker;augmentation adversarial;voices datasets;speaker representations;discriminative speaker information;propose augmentation adversarial;humans augmentation adversarial;encourage utterance embeddings;adversarial training unsupervised;similar utterance embeddings;utterance embeddings similar;embeddings similar utterance;utterance embeddings;speaker information invariant;discriminative speaker;speaker recognition;works unsupervised speaker;speaker representations based;utterance embeddings dissimilar;voices datasets significant;recognition augmentation;augmentation simulates acoustic;adversarial training", "pdf_keywords": "augmentation methods speech;augmentation adversarial training;augmentation adversarial;propose augmentation adversarial;2020 augmentation adversarial;recognition models speaker;self supervised speaker;supervised speaker;network discriminative speaker;augmentation data augmentation;adversarial training;speaker recognition;supervised speaker recognition;speaker recognition models;discriminative speaker information;augmentation simulates acoustic;robust speaker recognition;train robust speaker;speaker information invariant;adversarial training strategy;adversarial training self;data augmentation;models speaker labels;speaker recognition jaesung;discriminative speaker;augmentation data;augmentation methods;network invariant augmentation;information invariant augmentation;exploit popular augmentation"}, "84370f2fa3ac21ed3c6a30144fbdb377157b8853": {"ta_keywords": "preferences probabilistic uncertainty;preference reasoning tasks;preferences agents answering;conditional preferences probabilistic;preferences probabilistic;key preference reasoning;preference reasoning;preferences agents;conditional preferences;reasoning pcp nets;represents preferences agents;model conditional preferences;agents pcp nets;agents answering dominance;nets multi agent;preferences expressed;queries reasoning pcp;probabilistic uncertainty;nets using probabilistic;probabilistic uncertainty reconcile;using probabilistic uncertainty;qualitative preferences;qualitative preferences expressed;probabilistic uncertainty study;agents answering;dominance queries reasoning;conflicting qualitative preferences;reasoning pcp;answering dominance queries;multi agent context", "pdf_keywords": ""}, "d66110a315f5f216b42d99cfafec31e8e30a03ea": {"ta_keywords": "neural uncertainty estimation;target speaker extraction;rnn transducer;equipped rnn transducer;estimation target speaker;speaker extraction;neural uncertainty;extraction equipped rnn;investigation neural uncertainty;speaker extraction equipped;uncertainty estimation target;uncertainty estimation;target speaker;transducer;speaker;estimation target;equipped rnn;rnn;uncertainty;neural;estimation;investigation neural;extraction;extraction equipped;target;equipped;investigation", "pdf_keywords": ""}, "05d1e21f5f0c8209cc125f2e9ccd3a62d6479114": {"ta_keywords": "document similarity metrics;similarity pairs documents;documents document similarity;document similarity;similarity metrics used;hoc document retrieval;assessing similarity pairs;methods assessing similarity;document retrieval;assessing similarity;similarity metrics;information retrieval;publication information retrieval;similarity pairs;similarity metrics central;document retrieval text;similarity;retrieval text;retrieval text classification;pairs documents;retrieval large study;information retrieval large;retrieval;determine bibliography records;record linkage;central record linkage;pairs documents document;area similarity metrics;bibliography records;records different hospitals", "pdf_keywords": ""}, "f9409302c4d8201481fe65675bc6f0fa32e01df7": {"ta_keywords": "protection individual;protection;individual", "pdf_keywords": ""}, "59d3f6a14e20efdf54216188e227e58a351237e5": {"ta_keywords": "gan fingerprint fake;fingerprint gan generated;fingerprint gan;disentangle fingerprint gan;disentangle gan fingerprint;analysis gan fingerprint;properties gan fingerprint;gan fingerprint disentangling;fingerprint gan architecture;gan fingerprint providing;gan fingerprint;dominate fingerprint gan;propose gan fingerprint;gan fingerprint factors;gan generated images;fingerprint fake image;disentangle gan;gan generated;representation fake image;fake image attribution;learning disentangle gan;analysis gan;gan architecture experiments;comprehensive analysis gan;fingerprint fake;fingerprint disentangling network;gan architecture;fake image detection;binary fake image;properties gan", "pdf_keywords": "gan fingerprint disentangling;disentangle \ufb01ngerprint gan;\ufb01ngerprint gan generated;gan generated images;images generated gan;gan fingerprint;\ufb01ngerprint gan;propose gan fingerprint;gan \ufb01ngerprints qualitatively;extract gan \ufb01ngerprints;gan \ufb01ngerprints;distinct different gans;\ufb01ngerprint gan architecture;generated gan;properties gan \ufb01ngerprints;gan generated;successfully extract gan;gan \ufb01ngerprints common;generated gan distinct;analysis gan;analysis gan \ufb01ngerprint;analyze gan;extract gan;properties gan \ufb01ngerprint;gan \ufb01ngerprint;qualitatively analyze gan;different gans apply;different gans;gan \ufb01ngerprint providing;analyze gan architecture"}, "207c64b36fbd6accf7067366a251d071e8dd03a7": {"ta_keywords": "flora mt vardoussia;vardoussia neighbouring mountains;vascular flora mt;mt vardoussia neighbouring;mt vardoussia comprises;mountainous character flora;mt vardoussia;flora mt;vascular flora;vardoussia neighbouring;flora examination floristic;floristic affinity mt;vardoussia comprises;flora;vardoussia;vardoussia comprises 1114;flora examination;affinity mt vardoussia;examination floristic affinity;mountains iti timfristos;examination floristic;floristic affinity;character flora examination;character flora;hemicryptophytes;neighbouring mountains iti;predominance hemicryptophytes;floristic;timfristos killini erimanthos;neighbouring mountains", "pdf_keywords": ""}, "4f6d64eec6eaa38177ae45ad6315cf25d1535294": {"ta_keywords": "encode conversation history;answer embedding method;answer embedding;dialog act prediction;conversation task dialog;conversation task;conversation history position;history answer embedding;handling conversation history;encode conversation;essential conversation task;prediction essential conversation;answer prediction;method encode conversation;answer prediction essential;conversation history;conversation history advantage;information using bert;leverage conversation history;conversation history understand;mtl answer prediction;task dialog act;dialog act;task dialog;using bert;dialog;handling conversation;learning mtl answer;current question bert;based helpful answering", "pdf_keywords": "dialog act prediction;encode conversation history;conversation task dialog;answer span prediction;prediction dialog act;selection conversation histories;conversation task;conversation history position;prediction dialog;span prediction dialog;handling conversation history;conversation histories;prediction essential conversation;answer embedding;answer embedding method;answer prediction essential;conversation histories work;answer prediction;essential conversation task;learn answer span;conversation history advantage;conversation history;history answer embedding;multi task learning;task dialog act;encode conversation;dialog act;history attention mechanism;mtl answer prediction;task dialog"}, "e8135016ff3bd33ace936e50247fd650fcc58a7a": {"ta_keywords": "neural machine translation;translation based attentional;machine translation nmt;translation lexicons improve;machine translation naist;translation nmt models;machine translation;asian translation based;workshop asian translation;translation lexicons;translation nmt;discrete translation lexicons;translation naist cmu;english translation track;translation track;translation track 2016;lexicons minimum risk;cmu submission japanese;risk training neural;asian translation;translation based;lexicons improve probability;submission japanese english;translation naist;attentional neural machine;score lexicons minimum;japanese english;translation;discrete translation;japanese english translation", "pdf_keywords": "neural translation model;translation model attentional;machine translation model;neural machine translation;baseline translation model;neural translation;translation model baseline;translation model;model improve translation;estimates neural translation;machine translation;model baseline translation;improve translation accuracy;translation lexicons minimum;translation lexicons improve;translation accuracy;baseline translation;translation task workshop;translation task;translation lexicons;translation accuracy 2016;paper translation task;discrete translation lexicons;paper translation;incorporating discrete translation;improve translation;workshop asian translation;scienti\ufb01c paper translation;discrete translation;nmt systems japanese"}, "7847419becbc04596b79f804f844cf9719e875ea": {"ta_keywords": "learning demonstrations;unannotated demonstrations parsing;approach learning demonstrations;parsing demonstrations sequences;learning demonstrations matching;parsing demonstrations;demonstrations parsing demonstrations;demonstrations parsing;planning latent language;hierarchical policies demonstrations;demonstrations sequences named;learning hierarchical policies;natural language commands;demonstrations sequences;ground language action;action sequences goals;unannotated demonstrations;skills autonomous;primarily unannotated demonstrations;level subtask descriptions;policies demonstrations using;reusable skills autonomous;skill induction planning;level instruction;high level instruction;skills plan generating;induction planning latent;high level subtasks;model action sequences;low level actions", "pdf_keywords": "demonstrations learning yields;unannotated demonstrations parsing;demonstrations learning;10 demonstrations learning;parsing demonstrations sequences;planning latent language;policy natural language;learning hierarchical policies;parsing demonstrations;hierarchical policies demonstrations;demonstrations parsing demonstrations;natural language plan;demonstrations parsing;actions trained models;ground language action;induction planning latent;actions trained;level actions trained;natural language commands;sparse natural language;unannotated demonstrations;policies demonstrations using;primarily unannotated demonstrations;skill induction planning;goals natural language;action sequences goals;hierarchical policy natural;language plan;language action;language action formulate"}, "ad734a3f530a6c338af6bf2bf678e5af05477c1a": {"ta_keywords": "complexity secret sharing;algorithm secret sharing;threshold secret sharing;secret sharing general;distributed algorithm secret;secret sharing network;secret sharing;communication complexity secret;generalization cryptographic protocols;problem secret sharing;secret sharing dealer;cryptographic protocols large;efficient generalization cryptographic;complexity secret;protocols secure multiparty;secret sharing important;secure multiparty computation;algorithm secret;sharing general networks;generalization cryptographic;communication complexity algorithm;bounds communication complexity;communication complexity super;byzantine agreement protocols;shamir threshold secret;threshold secret;sharing dealer;shares secret;satisfying propagating dealer;protocols secure", "pdf_keywords": ""}, "11154216ca898590e7b2f0339587e3378c2c646c": {"ta_keywords": "quantifying driving confidence;drivers driving confidence;driving confidence proposed;confidence psychology driving;psychology driving confidence;driving confidence warning;driving warning information;driving confidence psychology;psychology driving warning;driving confidence;driving confidence guide;important driving psychology;driving warning;driving psychology;degree driving confidence;driving psychology driving;psychology driving;driving behavior;driving confidence increase;quantifying driving;affect drivers driving;indicators important driving;based driving behavior;driving behavior data;drivers warning information;method quantifying driving;drivers driving;psychological characteristics drivers;drivers warning;drivers degree driving", "pdf_keywords": ""}, "87951cea6573eed827986371a35025e478d3c184": {"ta_keywords": "stochastic extragradient seg;stochastic extragradient;literature stochastic extragradient;max optimization variational;optimization variational;sampling stochastic gradients;optimization variational inequalities;stochastic gradients mini;stochastic gradients;extragradient seg method;extragradient seg;mini batching convergence;gradients mini batching;batching convergence guarantees;sum variational inequalities;variational inequalities problems;sum variational;variational inequalities possibly;variational inequalities;extragradient;batching convergence;finite sum variational;variational;min max optimization;including sampling stochastic;seg lipschitzness monotonicity;convergence guarantees monotone;sample seg lipschitzness;max optimization;sampling stochastic", "pdf_keywords": "convergence properties seg;batching convergence guarantees;mini batching convergence;convergence guarantees monotone;variational inequalities;sum variational inequalities;sampling stochastic gradients;variational inequalities possibly;convergence guarantees;batching convergence;stochastic gradients;convergence guarantees rely;stochastic gradients mini;variants seg outperform;sum variational;state art convergence;variants seg;guarantees monotone;\ufb01nite sum variational;new variants seg;guarantees monotone \ufb01nite;seg outperform;seg;art convergence guarantees;variational;convergence properties;sampling stochastic;including sampling stochastic;regarding convergence properties;seg open"}, "dfa7bdea128b899d348ed32a84a7ccb1da4340e4": {"ta_keywords": "dialog response retrieval;robust dialog systems;short term memory;term memory lstm;dialog systems;generate dialog response;generate dialog;retrieval response generation;building robust dialog;dialog response based;based dialog baselines;memory lstm neural;dialog systems using;generation retrieval tasks;robust dialog;retrieve generate dialog;dialog baselines employing;generation retrieval;term memory neural;lstm decoder generate;memory lstm;example based dialog;dialog response;response generation;memory neural;lstm decoder;dialog dialog response;memory neural network;lstm neural;based dialog dialog", "pdf_keywords": ""}, "3675958405f3ad1633d565efa36b4eb3004bcf59": {"ta_keywords": "streaming live streams;live video streaming;live streaming upload;traditional live streaming;live streams;live streams presence;live streaming;social live streams;video streaming;streaming live;streaming upload solution;streaming;stream real time;streaming upload;live streaming live;vantage live streaming;live streams watched;live stream real;real time quality;video streaming slvs;bandwidth social live;optimizing video upload;youtube live twitch;stream;streaming slvs applications;higher quality video;delays viewers;live twitch;facebook live youtube;live youtube", "pdf_keywords": ""}, "0ca2a7465fe88f1f4912b8dd7b4b0db69a268b0b": {"ta_keywords": "lattice language models;neural lattice language;english neural lattice;language modeling tasks;language models;language modeling;multiple language modeling;language models utilize;new language modeling;language models models;language model;language modeling paradigm;lattice language;seamlessly incorporate linguistic;sentence marginalize lattice;granularities neural lattice;english neural;baseline chinese model;intuitions including polysemy;neural lattice;incorporate linguistic intuitions;modeling tasks english;multiword lexical items;multiword lexical;linguistic intuitions including;lexical items language;linguistic intuitions;including polysemy;embeddings able improve;polysemous embeddings", "pdf_keywords": "lattice language models;neural lattice language;neural lattice languages;lattice language model;lattice languages models;lstm language modeling;lattice language;lattice languages;neural lattice lms;seamlessly neural lattice;new language modeling;language models;instantiations neural lattice;neural lattice;lstm language;language modeling;languages models;granularities neural lattice;language models instantiations;languages models aiming;language modeling paradigm;language model;language modeling used;models instantiations neural;language modeling framework;standard lstm language;demonstrated neural lattice;embedding words;language models qualitatively;lattice lms"}, "4786e10003655be97feee21b9d9894a88a62885f": {"ta_keywords": "transfer nlp models;multilingual transfer nlp;massively multilingual transfer;transfer nlp;entity recognition;multilingual transfer;target language massively;named entity recognition;models source languages;entity recognition 41;nlp models source;nlp models;supervision target language;target language;transfer based unsupervised;unsupervised truth inference;target language evaluating;resource target language;languages techniques effective;ensembling unsupervised;standard ensembling unsupervised;source languages;language massively multilingual;41 languages techniques;based unsupervised truth;source languages applied;unsupervised truth;massively multilingual;ensembling unsupervised method;language massively", "pdf_keywords": "multilingual transfer models;multilingual transfer outperforms;unsupervised transfer supervised;target language bayesian;new multilingual transfer;multilingual transfer;ideas multilingual transfer;shot multilingual transfer;multilingual transfer inspired;truth inference crowd;transfer supervised;dataset target language;low resource supervised;labelling target language;inference crowd sourcing;reasoning noisy annotations;language speci\ufb01c transfer;supervised transfer;source english transfer;annotations outperforms competitive;language bayesian;data target language;english transfer model;transfer supervised transfer;showed multilingual transfer;based unsupervised transfer;zero shot multilingual;annotations outperforms;annotating data target;target language"}, "4a06bfa86cbccdf5e55dcec3505cdc97b8edb288": {"ta_keywords": "categorial grammars generalized;combinatory categorial grammars;grammars generalized composition;normal form parsing;categorial grammars;grammars generalized;form parsing combinatory;parsing combinatory categorial;grammatical type raising;parsing combinatory;grammatical type;composition type raising;generalized composition type;form parsing;grammars;type raising normal;deal grammatical type;combinatory categorial;parsing;type raising;generalized composition;composition type;raising normal form;account generalized composition;generalized composition bounded;normal form;composition bounded degree;extension deal grammatical;combinatory;composition", "pdf_keywords": ""}, "622f980030f766e5eb3989f36eea4459ccc948bf": {"ta_keywords": "adaptation techniques speech;incremental adaptation;novel incremental adaptation;proposed incremental adaptation;incremental adaptation techniques;parameters incremental adaptation;incremental adaptation involves;incremental adaptation framework;temporal changes speaker;direct adaptation approaches;adaptation transformation parameters;changes speaker speaking;adaptation approaches;acoustic models quickly;indirect adaptation transformation;adaptation techniques;adaptation transformation;time variant acoustic;direct adaptation;adjusting acoustic models;changes speaker;adaptation framework;acoustic characteristics temporal;indirect adaptation;adaptation approaches based;adaptation framework based;interpretation adaptation approaches;approaches indirect adaptation;quick stable adaptation;adaptation approaches reveal", "pdf_keywords": ""}, "ae06bc1e8e67c27b89329ebcfe61b71625d853f6": {"ta_keywords": "explainability techniques human;explanations perform best;gradient based explanations;explainability techniques text;assigned explainability techniques;explanations perform;based explanations perform;diverse explainability techniques;reviewed explainability techniques;explainability techniques;assigned explainability;explainability techniques downstream;diverse explainability;scores assigned explainability;explainability;based explanations;study explainability techniques;text classification tasks;study explainability;classification tasks neural;text classification overall;reviewed explainability;human annotations salient;classification tasks;diagnostic study explainability;properties reviewed explainability;text classification;annotations salient input;downstream text classification;annotations salient", "pdf_keywords": "explainability techniques human;explainability techniques text;assigned explainability techniques;explainability techniques;attributions explainability techniques;explainability methods;evaluation explainability techniques;diverse explainability techniques;explainability techniques different;study explainability techniques;assigned explainability;evaluation explainability;explainability methods evaluate;study explainability;human annotations salient;explainability;study attributions explainability;diverse explainability;scores assigned explainability;explainability techniques downstream;attributions explainability;annotations salient input;annotations salient;diagnostic study explainability;complexity compare saliency;classi\ufb01cation tasks neural;properties explainability methods;properties explainability;diagnostic properties explainability;compare saliency"}, "43896ea7d488100d135645fbb4be6e7eb2e7f4e2": {"ta_keywords": "attribute representation;feature vector representation;vector representation allows;set valued features;valued features;feature vectors;attribute representation example;nominal features;feature vectors components;representation allows value;infinite attribute representation;feature set strings;dog nominal features;length feature vectors;set valued feature;valued feature;vector representation;vector representation closely;traditional feature vector;valued features particular;categorization problems;categorization;valued feature color;represented set valued;value feature set;representations;examples represented;categorization problems problems;naturally represented set;feature vector", "pdf_keywords": ""}, "adb80a4190fdb6a019d57f18ae072ca93f494b7e": {"ta_keywords": "noisy speech recognition;discriminative training low;discriminative training model;discriminative training improved;dnn acoustic models;reverberated speech recognition;speech recognition;speech recognition effective;sequence discriminative training;discriminative training;rank deep neural;evaluation noisy speech;neural network acoustic;speech recognition sequence;speech recognition task;effective noisy speech;perform discriminative training;low rank deep;reduce dnn model;improved performance dnn;combination discriminative training;recognition sequence discriminative;rank sequence discriminative;network dnn acoustic;rank model deep;training low rank;model perform discriminative;acoustic models outperform;noisy speech;low rank model", "pdf_keywords": ""}, "f3762141fd64bee8d09e55ad4c83057cd4e002d4": {"ta_keywords": "coalition utility learning;correlation utility learning;correlation coalition utility;game correlation coalition;leveraging correlations learning;correlations learning utilities;estimate correlations agents;coalition jointly optimize;leveraging correlations;correlation coalition;correlations learning;utility functions coalition;utility learning;estimate correlations players;coalition utility;approaches leveraging correlations;competing game correlation;functions coalition agents;game correlation;schemes coalition utility;utility learning method;coalition agents;agents coalition jointly;coalitions agents;correlation utility;coalitions agents positively;correlated correlation utility;cooperative agents competing;correlations agents;coalition agents coalition", "pdf_keywords": ""}, "388d41b99c9c0867301f345c65877a2796225ead": {"ta_keywords": "target speaker asr;speech recognition asr;speaker asr accuracy;speaker asr baseline;speaker asr;target speaker utterances;transcribes target speaker;interference speaker asr;speaker automatic speech;function target speaker;sample target speaker;automatic speech;speech recognition;automatic speech recognition;novel auxiliary loss;proposed auxiliary loss;auxiliary loss;asr accuracy training;target speaker baseline;auxiliary loss function;interference speakers speech;asr interference speakers;target speaker automatic;recognition asr built;multiple speakers speech;speaker utterances monaural;speaker baseline achieved;target speaker;speaker baseline;recognition asr", "pdf_keywords": "target speaker utterances;target speaker asr;speaker speech recognition;function target speaker;transcribes target speaker;target speaker speech;speech recognition asr;speaker asr baseline;speaker automatic speech;sample target speaker;speaker asr;loss target speaker;speech recognition naoyuki;speaker loss target;speech recognition;automatic speech;automatic speech recognition;target speaker automatic;auxiliary interference speaker;target speaker;target speaker \ufb01rst;multiple speakers speech;speaker utterances;speaker utterances monaural;novel auxiliary loss;interference speaker loss;strong target speaker;speakers speech;speaker speech;auxiliary loss"}, "7a8f8109e65ed9a6048859681a825eb5655e5dd2": {"ta_keywords": "word embeddings training;trained word embeddings;sentence embeddings;modern sentence embeddings;word embeddings;sentence embeddings gain;sentence representations;encoders sentence classification;sentence representations pre;sentence embeddings solid;embeddings gain random;aim sentence embeddings;computing sentence representations;embeddings training;embeddings training using;random encoders sentence;embeddings;sentence classification;representations pre trained;exploring random encoders;random encoders;embeddings gain;encoders sentence;embeddings solid;training using random;pre trained word;random parameterizations aim;random parameterizations;using random parameterizations;encoders", "pdf_keywords": "sentence representations pretrained;trained word embeddings;word embeddings training;pretrained word embeddings;produce sentence embeddings;encoders sentence classification;sentence embeddings;sentence representations;embeddings pre trained;word embeddings;computing sentence representations;sentence embeddings pre;representations pretrained word;sentence representations pre;embeddings requiring training;word embeddings requiring;embeddings training;embeddings training using;computed sentence representations;random embedding;random encoders sentence;sentence classification;representations pretrained;representations pre trained;initialized recurrent networks;exploring random encoders;embeddings;random embedding projections;embeddings pre;recurrent networks"}, "03058f9a39d37a8bee635969eed227d59bbc8152": {"ta_keywords": "stochastic differential equations;solution gradient memory;stochastic differential;method stochastic differential;stochastic differential equation;gradient memory;derive stochastic differential;gradient memory efficient;memory computation gradients;stochastic dynamics;fit stochastic dynamics;stochastic dynamics defined;generalize method stochastic;derive stochastic;specifically derive stochastic;stochastic;method stochastic;fit stochastic;adaptive solvers adjoint;computation gradients;scalably computes gradients;method fit stochastic;adaptive solvers;dynamics defined neural;neural networks achieving;defined neural networks;computation gradients high;solution gradient;high order adaptive;solvers adjoint sensitivity", "pdf_keywords": "gradientbased stochastic variational;variational inference sde;stochastic variational inference;training latent sdes;approach gradientbased stochastic;gradientbased stochastic;marginalizing latent sde;backward stratonovich sde;combined gradientbased stochastic;latent sde models;latent sde motion;inference sde models;stochastic variational;inference sde;stratonovich sde;latent sde;stochastic adjoint framework;latent sdes;stratonovich sde dynamics;variational inference;adjoint method stochastic;latent sdes combine;variational inference scheme;networks latent sde;sde models arbitrary;adjoint approach gradientbased;learn dynamics parameterized;stochastic adjoint;sde models;sde motion"}, "4eb62ee328ceac9976c72bca65570d73ca0e8b64": {"ta_keywords": "behavioral stable marriage;stable marriage problems;marriage problems;stable marriage;marriage;behavioral stable;behavioral;stable;problems", "pdf_keywords": ""}, "30a6a5614727017e7d7981f87df57d17713501a0": {"ta_keywords": "combinatorial bandits incentivizing;combinatorial bandits;bandit framework matching;environment combinatorial bandits;bandit framework;bandits incentivizing agents;bandits incentivizing;algorithm ucb bandits;greedy matching paradigm;armed bandit framework;matching incentives;bandits;matching incentives users;greedy matching;demand bike sharing;incentives users preferences;multi armed bandit;bandits iii mixing;theoretical bounds regret;framework matching incentives;ucb bandits;bandits iii;bandit;demand bike;greedy;bounds regret;incentives users;ucb bandits iii;bounds regret demonstrate;bike sharing", "pdf_keywords": "bandit algorithm matching;propose bandit algorithm;bandit framework matching;combinatorial bandits incentivizing;combinatorial bandits;bandit strategy theory;bandit algorithm;developed bandit algorithm;bandit framework;bandit strategy;bandits incentivizing agents;propose bandit;bandits incentivizing;armed bandit framework;algorithm ucb bandits;algorithm matching incentives;times propose bandit;armed bandit strategy;matching incentives users;incentives users preferences;multi armed bandit;bandits;personalized incentives recommendations;bandit;matching incentives;ideas greedy matching;bandits iii;greedy matching paradigm;incentives recommendations;ucb bandits"}, "357ff26120dd220d7132f8083697d54b007ef260": {"ta_keywords": "speech processing universal;speech processing tasks;model self supervised;lightweight prediction heads;wide range speech;supervised learning ssl;lightweight prediction;self supervised learning;range speech processing;superb tasks learning;self supervised;universal performance benchmark;learning task specialized;specialized lightweight prediction;representation learned ssl;speech processing;introduce speech processing;benchmark performance shared;task specialized lightweight;extracting representation learned;tasks learning;processing universal performance;promising ssl representations;benchmark superb paradigm;ssl representations competitive;nlp computer vision;performance benchmark superb;representation learned;performance shared model;ssl representations", "pdf_keywords": ""}, "9f24b8f93ed00a1592e02fdb0edf5ebf0d8752ff": {"ta_keywords": "assignment papers reviewers;fairness peerreview algorithm;peer review statistical;accuracy peer review;reviewers conference peer;maximize review quality;peer review;conference peer review;guarantees fairness peerreview;automated assignment papers;peer review process;fairness peerreview;accuracy peer;assignment algorithms overcomes;peerreview algorithm;experiments peer review;various assignment algorithms;papers reviewers;maximize review;objective maximize review;accuracy fairness objective;assignment algorithms;automated assignment;statistical accuracy fairness;papers reviewers conference;fairness statistical accuracy;near optimally fair;fairness objective maximize;review quality disadvantaged;optimally fair finally", "pdf_keywords": ""}, "6a4deeb40aed8a4d56c8d9401c94b6c7a769e8c3": {"ta_keywords": "known information retrieval;information retrieval;information retrieval task;retrieval task document;graded relevance;graded relevance scores;retrieval;retrieval task;query documents;papers analogous query;relevance scores;query document envision;query scientific paper;query documents drawn;scores high annotation;50 query documents;expert annotated test;relevance scores high;high annotation;annotated test collection;retrieve relevant documents;analogous query scientific;high annotation agreement;relevant documents large;query document;document envision models;search query goal;documents large collection;retrieve scientific papers;research articles faceted", "pdf_keywords": "dataset faceted qbe;faceted qbe task;evaluation faceted qbe;known information retrieval;articles faceted query;information retrieval;faceted query;retrieval;faceted query example;faceted qbe;information retrieval task;retrieval task document;articles faceted;case faceted qbe;retrieval task;qbe task introduce;qbe task;research articles faceted;collection evaluation faceted;evaluation faceted;faceted qbe problem;qbe applied scienti\ufb01c;faceted qbe applied;aspects dataset faceted;expert annotated test;dataset faceted;retrieve relevant documents;search query goal;relevant documents large;annotated test collection"}, "356ea9b29101ec6974a7a97b62266b0e7e58d6bf": {"ta_keywords": "shinji watanabe librispeech_asr_train_asr_conformer_raw_bpe_batch_bins30000000_accum_grad3_optim_conflr0;watanabe librispeech_asr_train_asr_conformer_raw_bpe_batch_bins30000000_accum_grad3_optim_conflr0;watanabe librispeech_asr_train_asr_conformer_raw_bpe_batch_bins30000000_accum_grad3_optim_conflr0 001_sp_valid;librispeech_asr_train_asr_conformer_raw_bpe_batch_bins30000000_accum_grad3_optim_conflr0;librispeech_asr_train_asr_conformer_raw_bpe_batch_bins30000000_accum_grad3_optim_conflr0 001_sp_valid;librispeech_asr_train_asr_conformer_raw_bpe_batch_bins30000000_accum_grad3_optim_conflr0 001_sp_valid acc;espnet2 pretrained model;espnet2 pretrained;pretrained model shinji;espnet2;pretrained model;fs 16k lang;pretrained;16k lang;fs 16k;16k lang en;001_sp_valid;model shinji watanabe;model shinji;ave fs 16k;001_sp_valid acc ave;16k;001_sp_valid acc;model;fs;en;acc;shinji watanabe;lang;shinji", "pdf_keywords": ""}, "9e0d3161b13481418b7e85e3a691d23d67cf1e68": {"ta_keywords": "images leak privacy;privacy leaking image;sharing images social;leaking image detection;regions graph convolutional;images social media;regionaware graph convolutional;identify images leak;graph convolutional network;images social;privacy leaking;graph convolutional;convolutional network drag;automatically identify images;sharing images;privacy leakage;leak privacy;practice sharing images;dataset privacy leaking;convolutional network;images leak;identify images;leaking image;privacy;dataset privacy;image detection 10;dynamic regionaware graph;issue privacy leaking;issue privacy leakage;largest dataset privacy", "pdf_keywords": "region aware gcn;regions graph convolutional;regionaware graph convolutional;graph gcn based;aware gcn privacy;aware gcn;dynamic regionaware graph;region aware;leaking image detection;graph convolutional network;gcn privacy;gcn privacy leaking;privacy leaking image;gcn based spatially;graph gcn;dynamic region aware;proposed region aware;graph convolutional;sharing images social;convolutional network drag;object detectors dynamically;initialize graph gcn;regionaware graph;gcn based;image detection guang;propose dynamic regionaware;convolutional network;network drag;object detectors;regions models correlation"}, "a3c9d1c5e403f35e5694778b86832f0f9a7d87e6": {"ta_keywords": "utterance gmm search;fast finding speech;search large speech;acoustically similar query;finding speech model;large speech data;speech model acoustically;speech models;speech model;finding speech;similarity search;similarity search large;speech data;fast similarity search;model acoustically similar;speech models conventional;speech model set;similarity search available;speech data set;set speech models;acoustically similar;evaluations utterance gmm;utterance gmm;gmm search;gmm search tasks;large set speech;model acoustically;fast similarity;gaussian mixture model;large speech", "pdf_keywords": ""}, "77910e51a40d17157fc798325d06edfa6cff18d6": {"ta_keywords": "nl code generation;language code generation;natural language code;code generation open;code generation testbed;code generation automatically;code generation aims;domain code generation;code generation;generate code general;generate code;knowledge nl code;programming language api;aims generate code;writing code explore;training natural language;score code generation;code explore effectiveness;python natural language;code explore;stackoverflow programming language;language python natural;api documentation evaluations;web writing code;language api documentation;language nl intents;natural language nl;nl code pairs;stackoverflow programming;language api", "pdf_keywords": "nl code generation;natural language code;language code generation;code generation models;python natural language;language nl intents;code generation;generate code general;programming language api;nl intents code;language python natural;code generation automatically;code generation aims;training natural language;code generation tranx;domain code generation;code generation frank;knowledge nl code;intents code snippets;code generation testbed;nl intents approach;generate code;state art syntax;natural language nl;method code generation;existing api documentation;code snippets;language api documentation;aims generate code;programming language python"}, "948b68677c4f3bcbb1bae7f1d4e1fd5a103f03d4": {"ta_keywords": "optimize speech enhancement;speech enhancement quality;data speech enhancement;speech enhancement automatic;speech enhancement;speech enhancement realized;speech enhancement systems;improves speech enhancement;enhancement automatic speech;based speech enhancement;speech enhancement using;estimation speech distortion;improves speech;optimize speech;speech recognition asr;noisy data speech;objective improves speech;incorporating estimation speech;end speech recognition;speech recognition objectives;jointly optimize speech;estimation speech;data speech;speech recognition;automatic speech;automatic speech recognition;speech distortion;speech distortion factor;enhancement systems denoise;learning based speech", "pdf_keywords": ""}, "5edaab1fa078a5c468e3fb26d267ca49be32e70e": {"ta_keywords": "preference elicitation aggregation;preference elicitation;framework preference elicitation;elicitation aggregation;elicitation aggregation plackett;information criteria efficient;aggregation plackett luce;luce model features;elicitation questions order;cost effective elicitation;criteria efficient;effective elicitation questions;questions given budget;luce model;queries randomly asking;criteria efficient arrive;effective elicitation;elicitation questions;aggregation plackett;budget framework iteratively;randomly asking questions;plackett luce model;designed information criteria;aggregation;preference;propose cost effective;fewer queries;criteria evaluate expected;expected information gain;information criteria", "pdf_keywords": "preference elicitation aggregation;preference elicitation framework;effective preference elicitation;preference elicitation;elicitation aggregation framework;amazon mechanical turk;ranking models;ranking models information;elicitation aggregation;framework preference elicitation;rules ranking models;voting rules ranking;elicitation aggregation plackett;elicitation aggregation arxiv;elicitation questions compare;rules ranking;estimate cost answering;mechanical turk;mechanical turk use;ranking;randomized voting rules;turk use estimate;elicitation framework;cost effective preference;cost answering;aggregation plackett luce;aggregation framework predict;elicitation framework accommodates;luce model features;cost answering different"}, "98caf4eb79208cf4bbfe20bde37bc1b6ded6d6de": {"ta_keywords": "named entity recognition;entity recognition english;entity recognition models;entity recognition;neural named entity;lingual entity linking;method soft gazetteers;entity recognition address;gazetteers lists entities;data soft gazetteers;english knowledge bases;soft gazetteers incorporates;soft gazetteers;entity linking;wikipedia neural named;bases wikipedia neural;soft gazetteers low;models use gazetteers;named entity;wikipedia neural;english data soft;gazetteers low resource;cross lingual entity;lingual entity;knowledge bases wikipedia;models cross lingual;traditional named entity;recognition english data;information english knowledge;gazetteers incorporates ubiquitously", "pdf_keywords": "soft gazetteer features;entity recognition ner;soft gazetteer methods;named entity recognition;lingual entity linking;method soft gazetteers;entity recognition models;entity recognition;proposed soft gazetteer;soft gazetteers incorporates;soft gazetteers;soft gazetteer;gazetteer features;english knowledge bases;lingual entity;gazetteer methods;features based lexical;language processing tasks;low resource languages;cross lingual entity;resource languages experiments;2009 soft gazetteer;neural named entity;used linguistic features;features lowresource ner;models cross lingual;information english knowledge;gazetteer methods experiment;lowresource languages kinyarwanda;gazetteers incorporates ubiquitously"}, "55b61befce42280c3d57331121c7d349dd8be4cf": {"ta_keywords": "evaluations speech translation;evaluation speech translation;speech translation results;delay evaluation speech;simultaneous speech translation;speech translation technology;evaluation simultaneous speech;user evaluations speech;speech translation;evaluations speech;evaluation speech;speech translation somewhat;speech translation beginning;delay inherent speech;inherent speech translation;better simultaneous speech;sentence boundaries speed;accuracy delay evaluation;translation technology attempts;text speech;translation technology;delay evaluation;translation results;presented text speech;translation results different;translation results helps;reducing delay results;simultaneous speech;considering accuracy delay;accuracy delay using", "pdf_keywords": ""}, "a949ba38194ad43c86925acec6705b434d5a920f": {"ta_keywords": "entity bias fake;bias fake news;fake news detection;mitigating entity bias;entity bias;bias fake;fake news;bias;future mitigating entity;news detection;mitigating entity;news;generalizing future mitigating;fake;entity;future mitigating;generalizing future;generalizing;detection;mitigating;future", "pdf_keywords": ""}, "22b6e88a2f234fc5646f6239f9040a776e841a97": {"ta_keywords": "inducing bilingual lexicons;bilingual lexicon corpus;induction bilingual lexicon;bilingual lexical entries;bilingual lexicon;bilingual lexicons;monolingual bilingual lexical;bilingual lexical;bilingual lexicons small;transcriptions monolingual bilingual;improvements inducing bilingual;inducing bilingual;induction bilingual;investigate induction bilingual;lexicon corpus phonemic;phonemic transcriptions monolingual;monolingual bilingual;transcriptions monolingual;corpus phonemic transcriptions;bilingual;high precision corpora;corpus phonemic;aligned english translations;corpora;lexical entries learnt;lexicon corpus;phonemic transcriptions sentence;corpus;transcriptions sentence aligned;precision corpora", "pdf_keywords": ""}, "9abd13caa32b1a90e32462a884a512f8666e80cc": {"ta_keywords": "independent semantic parsing;dependent semantic parsing;natural language queries;language queries contextual;semantic parsing propose;semantic parsing;queries contextual;queries contextual information;follow query analysis;context independent semantic;challenging task parser;dependent natural language;context dependent semantic;perform follow query;parsing;follow query;semantic parsing proven;task parser independent;parser independent;independent semantic;approach follow query;followup dataset;language queries;followup dataset star;nearly context dependent;parsing propose;dependent semantic;task parser;context dependent natural;parser", "pdf_keywords": "star sqa dataset;sqa dataset annotations;sqa dataset promising;sqa dataset finally;sqa dataset;star sqa;extensibility star sqa;recombination process extend;redesign recombination process;extension sqa dataset;redesign recombination;recombination process;recombination;promising redesign recombination;process introduce phases;extend star sqa;process extend star;sqa;dataset annotations;phase process introduce;dataset finally demonstrate;dataset promising redesign;demonstrate star extensibility;explore extensibility star;phase process methodology;introduce phases;dataset promising;designed phase process;phase process;dataset finally"}, "3c8853d4ae3ad2633c47e840a48951d62b64a5b4": {"ta_keywords": "view graph learning;multi view graph;graph learning adaptive;graph learning mvgl;graph learning;multi view learning;view graph;view learning;graph sparsification label;extraction graph sparsification;sparse graph;graph sparsification;constructs sparse graph;sparse graph accordingly;factor extraction graph;semi supervised;propagation semi supervised;label propagation unified;view learning baselines;classification multi view;label propagation efficient;label propagation semi;adaptive label propagation;semi supervised classification;label propagation;learning adaptive label;latent factor extraction;factors multi view;paradigms semi supervised;multi view data", "pdf_keywords": ""}, "df29486c04eafd004f2f0816e84c798783802cdf": {"ta_keywords": "transliteration cross lingual;transliteration related languages;transliterating transfer language;transliterating transfer;transliterating;transliteration;transliteration related;alleviate issue transliteration;use transliteration related;use transliteration;transliteration cross;cases transliterating transfer;issue transliteration;explore use transliteration;lingual morphological inflection;issue transliteration cross;cross lingual morphological;transfer language data;languages grapheme phoneme;converting languages shared;converting languages;lingual morphological;like international phonetic;cases transliterating;finding cases transliterating;related languages grapheme;languages grapheme;language data;grapheme phoneme conversion;international phonetic", "pdf_keywords": ""}, "298a68153859303ee70b3ef1525ee9c7031e32f5": {"ta_keywords": "chat dialogue systems;dataset persona chat;persona chat;persona chat demonstrate;dialogue generation mutual;dialogue generation;chit chat dialogue;chat dialogue;dialogue systems;impress dialogue generation;mutual persona perception;dialogue systems majority;human like responses;automatic metrics human;persona perception;generation mutual persona;chat demonstrate effectiveness;consistency chit chat;chit chat;dialogue;dataset persona;persona;chat demonstrate;mutual persona;mimicking human;p\u02c62 bot transmitter;mimicking human like;propose p\u02c62 bot;bot transmitter;metrics human evaluations", "pdf_keywords": "chat dialogue systems;chit chat dialogue;chatbots;chat dialogue;building personalized chatbots;persona chat demonstrate;persona chat;dialogue systems;dialogue generation experiments;personalized chatbots;dialogue generation mutual;chatbots deliver engaging;personalized chatbots deliver;dialogue generation;mimicking human;personalized dialogue generation;personalized dialogue;dataset persona chat;mimicking human like;human like responses;modeling understanding interlocutors;consistency chit chat;dialogue systems majority;chatbots deliver;quality personalized dialogue;chit chat;conversations gain user;dialogue;mutual persona perception;impress dialogue generation"}, "251a80dd4126fed3d6ae64f00dc24479f0ba5662": {"ta_keywords": "tutorials;23 tutorials hosted;tutorials hosted web;tutorials hosted;summarizes 23 tutorials;23 tutorials;tutorials 14;style tutorials;lecture style tutorials;style tutorials 14;hands tutorials;tutorials 14 hands;14 hands tutorials;web conference 2021;web conference;web;2021 lecture;2021 lecture style;hosted web conference;conference 2021 lecture;hosted web;lecture style;lecture;summarizes;summarizes 23;conference 2021;hosted;report summarizes;report summarizes 23;conference", "pdf_keywords": ""}, "46dab5eb9c11bd49893e2dafa7d1b720a0aa2b3d": {"ta_keywords": "reading comprehension tasks;like reading comprehension;comprehension tasks achieving;tag prediction task;tasks like reading;combines word level;paragraphs reading comprehension;comprehension tasks;performance reading comprehension;combine word level;reading comprehension present;reading comprehension;questions paragraphs reading;reading comprehension demonstrate;character level representations;tag prediction;paragraphs reading;word level character;word level;like reading;concatenation scalar weighting;dynamically combine word;improve performance reading;grained gating modeling;fine grained gating;media tag prediction;reading;grained gating;representations using concatenation;idea fine grained", "pdf_keywords": ""}, "e92677eb974a2814d57de54e2c3733cbd92e2c00": {"ta_keywords": "coding distributed computing;hierarchical coding distributed;coding distributed;systems coding distributed;distributed computing;hierarchical coding scheme;proposed hierarchical coding;propose hierarchical coding;distributed computing considering;distributed computing supports;hierarchical coding;computation time hierarchical;distributed computing systems;world distributed computing;computing systems coding;hierarchical computational;decoding cost computing;time hierarchical coding;consider hierarchical computational;hierarchical coding shown;low latency computation;hierarchical computational structure;systems coding;coding scheme;latency computation relieving;coding;work propose hierarchical;cost computing time;simple master worker;cost computing", "pdf_keywords": "hierarchical coding scheme;proposed hierarchical coding;propose hierarchical coding;parallel decoding;ef\ufb01cient parallel decoding;hierarchical coding;distributed computing tree;parallel decoding reducing;hierarchical coding shown;coding scheme;reducing decoding costs;decoding cost computing;mds code groups;coding scheme model;mds code group;decoding reducing decoding;distributed computing;reducing decoding;considering decoding;coding scheme employs;outer mds code;considering decoding cost;model distributed computing;decoding cost expected;analyze decoding cost;decoding cost;code groups;decoding costs;mds code;coding"}, "0e9e334e2647307f8fa7f9937d93f3ca9095e351": {"ta_keywords": "monotone lipschitz extragradient;lipschitz extragradient method;cocoercivity extragradient method;lipschitz extragradient;monotone variational inequalities;convergence monotone variational;operators optimistic gradient;optimistic gradient method;gradient method hamiltonian;variational inequalities connections;monotone lipschitz vip;method hamiltonian gradient;variational inequalities problems;variational inequalities;hamiltonian gradient method;saddle point variational;cocoercivity update operators;point variational inequalities;cocoercivity extragradient;extragradient method;optimistic gradient;operator monotone lipschitz;monotone variational;extragradient method korpelevich;extragradient method iterate;lipschitzness jacobian operator;hamiltonian gradient;lipschitz vip additional;relies lipschitzness jacobian;monotone lipschitz", "pdf_keywords": "extragradient method;abstract extragradient method;extragradient method korpelevich;variational inequalities problems;extragradient method iterate;saddle point variational;variational inequalities connections;point variational inequalities;monotone variational inequalities;variational inequalities;convergence monotone variational;abstract extragradient;methods solving saddle;extragradient;inequalities connections cocoercivity;monotone variational;point variational;cocoercivity arxiv;chair abstract extragradient;solving saddle point;inequalities problems vip;variational;iterate convergence monotone;method iterate convergence;method korpelevich 1976;cocoercivity arxiv 2110;connections cocoercivity arxiv;solving saddle;method korpelevich;inequalities connections"}, "75ba422d90c488b1388345865e0525208331bb3d": {"ta_keywords": "private learning nlp;differentially private learning;privacy guarantees differentiallyprivate;private learning;differentially private;strategies differentially private;better privacy preserving;privacy preserving;privacy regime;privacy preserving strategies;different privacy preserving;task privacy regime;stricter privacy guarantees;privacy regime requires;pattern task privacy;know stricter privacy;privacy guarantees;privacy;stricter privacy;task privacy;different privacy;better privacy;analysis different privacy;private approaches solving;private approaches;non private approaches;private;non private;usually better privacy;stochastic gradient descent", "pdf_keywords": "private learning nlp;preserving privacy training;privacy preserving learning;differentially private learning;differentially private training;privacy training;privacy training modern;private learning;differential privacy;different privacy preserving;better privacy preserving;preserving privacy;privacy guarantees differentiallyprivate;privacy preserving;primer differential privacy;task privacy regime;differential privacy explanation;abstract preserving privacy;privacy preserving strategies;explored differentially private;privacy regime;pattern task privacy;differentially private;poses privacy preserving;strategies differentially private;analysis different privacy;privacy regime requires;task privacy;privacy;different privacy"}, "9d3e33875ec39001e72313fb919f66242ee97880": {"ta_keywords": "discovery linguistic units;words language orthography;discovery linguistic;linguistic units subwords;discovery raw speech;language orthography;orthographic transcriptions images;surrounding discovery linguistic;orthographic transcriptions;replacement orthographic transcriptions;transcriptions images translated;linguistic units;language orthography study;subwords words language;units subwords words;transcriptions images;unsupervised discovery;words language;unsupervised discovery raw;language help unsupervised;subwords words;units subwords;transcriptions;orthography;help unsupervised discovery;orthographic;linguistic;raw speech;subwords;images translated text", "pdf_keywords": "unit discovery speech;discovery raw speech;discovery linguistic units;discovery speech synthesis;units speech2image;units speech2image speech2translation;discovery speech;speech summarization;speech2image;symbolic units speech2image;speech summarization going;speech2image speech2translation;improve speech summarization;discovery linguistic;raw speech arxiv;speech2translation;transcriptions images translated;machine translation concentrated;words language orthography;speech2translation cross language;linguistic units subwords;speech2image speech2translation cross;retrieval machine translation;language orthography;language semi supervised;translation concentrated tasks;orthographic transcriptions images;language adaptation asr;speech synthesis;raw speech"}, "7d7469e059c6890c24d42931c697df835329f26a": {"ta_keywords": "estimating noise mixture;noise mixture model;method noise mixture;noise model estimation;mixture model noise;noise mixture;model noise suppression;noise suppression;noise suppression proposed;estimating noise;based noise suppression;noise model robust;way estimating noise;estimation noise noise;estimation method noise;mmse estimation noise;estimate noise;estimation noise;mmse estimate noise;vts based noise;noise suppression usually;noise noise model;estimate noise iterating;noise model;gaussian distribution noise;distribution noise model;signal model noise;noise iterating mmse;robust estimation method;based noise", "pdf_keywords": ""}, "f7f6160d4e9e3bf7f36bacbc9f15e916a6f226de": {"ta_keywords": "multichannel speech enhancement;enhancement speech recognition;speech enhancement objectives;speech enhancement;speech enhancement work;adequate speech enhancement;does speech enhancement;gains speech enhancement;instead speech enhancement;speech enhancement noise;speech enhancement ability;speech enhancement component;speech enhancement speech;end speech recognition;enhancement speech;question speech enhancement;evaluation speech quality;speech recognition architecture;multichannel speech;components multichannel speech;channel clean speech;speech recognition single;training speech recognition;clean speech data;speech recognition asr;speech quality;enhancement noise suppression;speech recognition;speech recognition component;speech quality findings", "pdf_keywords": ""}, "99c87e16c56b8a113124779734951f11bd662d5d": {"ta_keywords": "occupants vote desired;occupants vote;vote desired lighting;occupants utility maximizers;energy efficient behavior;using nash equilibrium;utility maximizers;behavior social game;behavior building occupants;nash equilibrium;encouraging energy efficient;utility maximizers utility;energy consumption building;continuous game characterize;maximizers utility functions;occupants non cooperative;lighting level win;social game designed;nash equilibrium concept;game defined estimated;maximizers utility;continuous game;agents continuous game;building occupants aim;game characterize play;using occupant voting;social game;occupants utility;estimated utility functions;vote maximum", "pdf_keywords": "game building energy;abstract social game;utility learning simulation;energy consumption building;continuous game characterize;building occupants vote;occupants vote desired;energy e\ufb03ciency utility;energy e\ufb03cient behavior;social game building;behavior building occupants;social game designed;consumption building occupants;encouraging energy;energy consumption;continuous game;utility maximizers;building energy;occupants vote;occupants utility maximizers;vote desired lighting;consumption building;social game;using nash equilibrium;behavior model occupants;utility maximizers utility;utility learning;designed encouraging energy;nash equilibrium;maximizers utility functions"}, "4697ef43450f173e12b1e22b77e976dc56fdf5fe": {"ta_keywords": "attack compressive sampling;existing compressive sensing;compressive sensing;attack compressive;novel compressive sensing;compressive sensing based;compressive sampling;compressive sampling matching;type attack compressive;sampling matching pursuit;modified basis pursuit;domain existing compressive;basis pursuit using;matching pursuit cosamp;novel compressive;compressive;proposes novel compressive;matching pursuit;basis pursuit;existing compressive;pursuit using;sensing based defence;adaptive defence cad;defence cad algorithm;white box attacks;pursuit;pursuit using novel;constraint norm attack;pursuit cosamp recover;box attacks mnist", "pdf_keywords": "bandit techniques adversarial;adversaries gradient attacks;gradient attacks norm;attacks gradient based;adversaries gradient;norm attacks gradient;attacks gradient;gradient attacks;adversarial videos;detecting adversarial videos;gradient based attacks;adversarial;detecting adversarial;adversarial videos contributions;adversarial image classi\ufb01cation;classifying detecting adversarial;gradient attacks paper;adversarial image;exploitation decision compressive;techniques adversarial;l2 adversaries gradient;box gradient attacks;techniques adversarial image;attacks norm attacks;bandit techniques;attacks norm;recovery algorithms compressive;algorithms compressive;norm attacks;decision compressive sensing"}, "ce3d6673b7eebdd0198940316d18e383e9597c9a": {"ta_keywords": "predictors contextual bandits;learning contextual bandits;bandits bound achieved;contextual bandits;bandits bound;bandits help loss;results optimal regret;contextual bandits help;optimal regret mathcal;optimal regret;contextual bandits surprising;minimax regret mathcal;armed bandits bound;improve minimax regret;minimax regret;multi armed bandits;bandits surprising results;bandits;bandits help;regret mathcal min;bandits surprising;sqrt learning rounds;armed bandits;optimal;loss predictors contextual;better bound;surprising results optimal;regret mathcal;regret mathcal sqrt;better bound mathcal", "pdf_keywords": "predictors contextual bandit;contextual bandit learning;predictors contextual bandits;contextual bandits algorithms;bandit learning;contextual bandit;bandit learning action;contextual bandits;bandits algorithms;bandits algorithms analysis;regret underestimator estimating;optimal regret underestimator;optimal regret known;leveraging loss predictors;technique optimal regret;optimal regret;setting optimal regret;leveraging loss;loss predictors contextual;regret known computationally;12 leveraging loss;bandit;bandits;regret underestimator;learning action remapping;referential scheme learning;stochastic setting optimal;loss predictors;predictors contextual;predictors stochastic setting"}, "29b3a609f2b5cb10cffb80a6aaf96413a4a9998e": {"ta_keywords": "efficient lossless compression;lossless compression using;compression uniform coder;lossless compression;enabling efficient compression;flows efficient lossless;neural compression algorithms;compression ratios iflow;efficient compression;lossless compression uniform;compression using normalizing;efficient compression fortunately;state art compression;neural compression;socalled neural compression;compression ratios quicker;discuss lossless compression;compression algorithms significantly;codecs terms compression;achieving high compression;efficient lossless;compression;compression using;compression algorithms;compression uniform;invertible flow transformations;transform;modular scale transform;high compression ratios;coder propose modular", "pdf_keywords": "neural compression codec;based neural compression;lossless compression;enabling ef\ufb01cient compression;compression codec;ef\ufb01cient compression;suitable lossless compression;neural compression;ef\ufb01cient compression furthermore;accelerate coding;modular scale transform;accelerate coding time;practical coding speed;invertible \ufb02ow transformations;iflow numerically invertible;compression codec bridges;coding speed;transform;compression;coding speed \ufb01rst;invertible \ufb02ow based;scale transform mst;uniform base conversion;compression furthermore techniques;scale transform;\ufb02ow transformations based;numerically invertible \ufb02ow;estimators practical coding;used accelerate coding;practical coding"}, "24ee54c8d5a01197e015d40be4277cfbb727394f": {"ta_keywords": "bikes planning bus;planning bus routes;plan bus routes;bus routes sharing;bus routes;planning bus;bus routes consideration;design bus routes;plan city transportation;driven plan bus;sharing bikes planning;routes consideration sharingbikes;plan bus;bus route suitable;routes sharing bike;designed bus route;bus route;extend bus routes;sharing bike lanes;bikes planning;sharing bikes area;bike lanes;bus routes current;bus;consideration sharingbikes extensive;city transportation systems;city transportation;route suitable passengers;consider sharing bikes;consideration sharingbikes", "pdf_keywords": ""}, "ea5cfce90444b17b36da07840b2f0cafb54ab0a7": {"ta_keywords": "automatic deception detection;deception detection;deception detection attempts;automatic deception;detect deception;attempt detect deception;deception asking questions;detect deception perform;deceptive conversational partner;deception asking;deceptive conversational;deception perform actions;signs deception asking;features deception;work automatic deception;deception;learn signs deception;features deception salient;telltale signs deception;catch potential liar;deception perform;unveil deceptive conversational;deception salient;signs deception;deception salient significant;make features deception;dialogue systems asks;potential liar paper;potential liar;action envisioning dialogue", "pdf_keywords": ""}, "e3a1b2a19356dc685d78630ae2a8852ad6c86200": {"ta_keywords": "proximity aware ranking;retrieval enterprise search;relevance features;combination relevance features;relevance features goal;moderately loaded retrieval;aware ranking functions;enterprise search;enterprise search engine;indexing word pairs;indexing close word;learned indexing close;loaded retrieval enterprise;aware ranking;retrieval enterprise;evaluate relevance modification;evaluate relevance;retrieval;relevance modification;pruning proximity information;loaded retrieval;indexing close;ranking functions;indexing word;lessons learned indexing;ranking functions use;functions evaluate relevance;learned indexing;search engine;relevance modification classic", "pdf_keywords": ""}, "5e24aa9fdf5466e96d314dfcde973fccec02995d": {"ta_keywords": "online feature selection;batch learners;feature selection learn;batch learners single;data online learning;batch feature selection;batch feature;learning competitive batch;feature selection finally;used batch feature;perform feature selection;online feature;feature selection;online learning methods;pass online learning;selection learn;feature;passes training data;winnow algorithm naturally;massive data streams;feature selection methods;advantages line learning;data streams;balanced winnow algorithm;online learning;perform feature;selection learn concepts;data streams essential;learning performance;training line learner", "pdf_keywords": ""}, "e2854bf66ed86a5dc74183bae5fde18e65699833": {"ta_keywords": "sound event detection;classification acoustic scenes;acoustic scenes events;polyphonic sound event;detection classification acoustic;speech recognition;classification acoustic;acoustic scenes;polyphonic tasks;monophonic polyphonic tasks;sound event;approaches monophonic polyphonic;speech recognition multi;polyphonic tasks finally;neural network hmm;field speech recognition;method polyphonic sound;approaches monophonic;conventional approaches monophonic;network hmm achieved;monophonic polyphonic;polyphonic;new method polyphonic;event detection;method polyphonic;polyphonic sound;hidden markov;memory hidden markov;hidden markov model;event detection based", "pdf_keywords": ""}, "21c9c624bc328686cef4bb1f80a786a5027d8886": {"ta_keywords": "scientific information extraction;information extraction sciie;extraction tasks citationie;information extraction tasks;leveraging citation graph;leveraging citation;citation graph scientific;citationie leveraging;information extraction;citationie leveraging citation;information extraction despite;information scientific documents;tasks citationie leveraging;information extraction state;content citation graph;considered extracting document;extraction sciie consider;sciie consider extraction;structure content citation;raw scientific text;language scientific documents;extracting document;scientific text;extracting document level;extraction sciie;graph scientific information;scientific documents potential;scientific documents;extraction state art;improve literature search", "pdf_keywords": "citation graph information;information citation graph;scienti\ufb01c information extraction;textual content citation;content citation graph;citation graph helps;information extraction tasks;citation graph model;information extraction;incorporating citation graph;relation extraction;relation extraction provides;information citation;citation graph;structure content citation;classi\ufb01cation relation extraction;content citation;ways incorporating citation;motivate information citation;language scienti\ufb01c documents;information neural sciie;identifying key entities;document known salient;salient entity classi\ufb01cation;graph information neural;relation extraction table;scienti\ufb01c documents;structure textual content;textual content;table relation extraction"}, "50c651e9f94f9d4927a726af0ef44818179d87da": {"ta_keywords": "semantic parsing;semantic parsing evaluations;semantic parsing machine;semantic parser semantic;parser semantic parsing;parsing machine translation;semantic parser;parser semantic;utterance semantic parsing;semantic parsing problem;semantic parsing benefit;adapted semantic parser;baseline semantic parsing;research semantic parsing;straightforward machine translation;parsing evaluations;parsing;machine translation components;parsing evaluations suggest;machine translation;parsing machine;machine translation task;structured meaning representation;parser;standard machine translation;utterance semantic;machine translation results;advances machine translation;parsing benefit;parsing benefit advances", "pdf_keywords": ""}, "cd0702deabaa8b7ccfba077f89dcc24e48ae1d47": {"ta_keywords": "retrieval methods subtopic;evaluating subtopic retrieval;subtopic retrieval using;subtopic retrieval generalizes;methods subtopic retrieval;subtopic retrieval;relevance ranking;performing subtopic retrieval;baseline relevance ranking;relevance ranking data;subtopic retrieval problem;problem subtopic retrieval;likelihood relevance ranking;retrieval methods;document ranking;retrieval problem subtopic;traditional retrieval problem;non traditional retrieval;traditional retrieval methods;retrieval problem utility;retrieval generalizes;ranking dependent documents;documents ranking;relevance mmr ranking;outperform baseline relevance;utility document ranking;dependent documents ranking;relevance ranking shown;document ranking dependent;retrieval using statistical", "pdf_keywords": ""}, "c1d0e73ec3aaf7ffdcbe41835d649d638cbc2f2d": {"ta_keywords": "confidently accelerating inference;accelerating inference large;accelerated inference confident;accelerating inference;inference confident adaptive;consistent accelerated inference;confident adaptive transformers;accelerated inference;inference large expensive;confident adaptive;classifier consistent accelerated;nlp method trains;inference confident;prediction heads intermediate;inference large;computational efficiency guaranteeing;processing nlp;prediction heads;increase computational efficiency;natural language processing;adaptive;approximate computational;cats confident adaptive;adaptive transformers work;adaptive transformers;additional prediction heads;meta consistency classifier;nlp;adaptive transformers simultaneously;language processing nlp", "pdf_keywords": "accelerating inference;con\ufb01dent adaptive transformers;accelerated inference;consistent accelerated inference;prediction faster;trained intermediate transformer;con\ufb01dently accelerating inference;accelerating inference large;adaptive transformers;adaptive transformers cats;prediction faster amortized;meta predictors accurately;accelerated inference con\ufb01dent;accurately assess prediction;conformal prediction;consistently improving computational;predictors accurately;compute alongside transformer;conformal prediction approach;new conformal prediction;inference con\ufb01dent adaptive;adaptive transformers tal;prediction approach calibrating;prediction simple early;calibrating con\ufb01dence meta;intermediate transformer representation;transformers cats framework;transformer representation;predictors accurately assess;transformer representation likely"}, "77cd3ae8a0b9ef6865d5324a4d62280e6f7a1053": {"ta_keywords": "gan augmentation trained;chime datasets augmentation;cycle gan augmentation;gan augmentation;augmentation trained;augmentations chime task;datasets augmentation;data augmentation;augmentation trained map;datasets augmentation method;speech recognition e2e;data augmentation using;asr models trained;combine augmentations chime;augmentations chime;augmentation methods;augmentation methods thier;augmentation provided pretrained;use augmentation methods;label augmentation;pretrained asr;automatic speech;augmentation using;recognition e2e asr;use augmentation;label augmentation provided;e2e asr robust;cycleconsistent generative adversarial;provided pretrained asr;pretrained asr module", "pdf_keywords": "gan augmentation trained;chime datasets augmentation;gan augmentation;cycle gan augmentation;augmentation trained;augmentations chime task;augmentation methods specaugment;data augmentation methods;augmentation trained map;augmentation methods;combine augmentations chime;augmentations chime;datasets augmentation method;datasets augmentation;augmentation methods thier;use augmentation methods;data augmentation;data augmentation using;augmentation using;augmentation provided pretrained;use augmentation;label augmentation;investigated data augmentation;combination augmentation methods;augmentation method;label augmentation provided;augmentation method individually;augmentation;augmentation using text;specaugment combine augmentations"}, "3b3a5a9c7352b74e8377ce3182ea646b0bed5b4c": {"ta_keywords": "reranking neural semantic;semantic parser reranking;neural semantic parsing;neural semantic parser;semantic parsers achieved;parser reranking best;parser reranking;parsing semantic;semantic parsing;semantic parsers;parsing semantic parsing;semantic parsing semantic;semantic parser;existing neural semantic;based semantic parsers;semantic parsing considers;neural semantic;reranking neural;parsers achieved;parsers achieved impressive;natural language nl;transducing natural language;nl utterances machine;parsers;parsing;parser;utterances machine;parsing considers task;natural language;nl utterances", "pdf_keywords": ""}, "89d15c9de3608157ff746af7368556149b50e037": {"ta_keywords": "words language modeling;language models improve;language modeling;driven language model;atomic semantic;language models;headline generation;atomic semantic units;enables language models;language modeling named;experiments language modeling;necessarily atomic semantic;language models work;language modeling downstream;headline generation demonstrate;robustness language models;sememes minimum semantic;semantic units language;language modeling methods;language model;units language modeling;language models paper;semantics words language;words atomic language;sememe distinct semantic;language model sdlm;sememe level semantics;semantics words;sememe driven language;implicit semantics words", "pdf_keywords": "language modeling sparse;words language modeling;language modeling;language modeling task;language model;language modeling named;driven language model;sememesense word generation;language modeling dataset;language modeling methods;predict word;sememe driven language;modeling named sememe;chinese language modeling;abstract language modeling;sememes minimum semantic;words order predict;sdlm language modeling;sequential patterns words;word generation;predict word design;order predict word;sememes capturing syntactic;word generation process;language model sdlm;sequence learning;sequence sequence learning;words language;sparse product sememe;novel sememesense word"}, "9dbd86f089c2132dc46d316750d9786d60d5d720": {"ta_keywords": "web based annotation;annotation tool historical;manual annotation historical;based annotation tool;annotation tool;annotation historical non;annotation historical;manual annotation;standard language data;annotation;based annotation;annotation process;annotation tool manual;language data;language data cora;tool manual annotation;language data allows;token boundaries annotation;boundaries annotation process;boundaries annotation;standard language;non standard language;data cora web;cora web based;historical non standard;language;tool historical non;present cora web;data cora;data allows editing", "pdf_keywords": ""}, "6f1ca0249eafa36a5762ac53f6ba2a4ee2133456": {"ta_keywords": "transcribed audio collected;transcribed audio;speech recognition training;speech recognition corpus;hours transcribed audio;transcription quality gigaspeech;english speech recognition;000 hours transcribed;quality transcription;domain asr corpus;asr corpus;speech recognition;asr corpus 10;quality transcription dev;suitable speech recognition;recognition corpus 10;low quality transcription;transcription dev;segments suitable speech;transcription quality;transcription dev test;transcribed;transcription;corpus;corpus 10 000;audio suitable supervised;high transcription quality;recognition corpus;hours transcribed;corpus 10", "pdf_keywords": "speech recognition toolkits;speech recognition training;transcription espnet baseline;quality transcription espnet;speech recognition;transcription espnet;suitable speech recognition;popular speech recognition;quality transcription;alignment segmentation pipeline;low quality transcription;segments suitable speech;transcription;forced alignment segmentation;recognition toolkits athena;recognition toolkits;convolutional neural networks;recognition training;alignment segmentation;recognition training \ufb01lter;segmentation pipeline;convolutional neural;segmentation pipeline proposed;neural networks long;toolkits athena espnet;sentence segments;espnet baseline system3;espnet baseline;transformers 17 baseline;speech"}, "22b7a7c9faa8f340520ae1418c9cf8d960aaeec0": {"ta_keywords": "symbolic reasoning systems;symbolic knowledge bases;neuro symbolic knowledge;symbolic knowledge based;intractability symbolic reasoning;neural symbolic;knowledge based reasoning;symbolic logics answering;emulate symbolic logics;symbolic logics;reasoning systems;symbolic reasoning;neural symbolic methods;symbolic knowledge;questions neuro symbolic;reasoning systems based;neuro symbolic;symbolic reasoning useful;knowledge bases;logics answering;dataflow query language;logics answering natural;gap neural symbolic;reasoning based set;logics computationally;based dataflow query;reasoning based;based reasoning based;np neural;based reasoning", "pdf_keywords": ""}, "eeec05fc11b2e0b40b3b0800bc50930e240cafeb": {"ta_keywords": "wikipedia corpus;using wikipedia corpus;wikipedia corpus suggest;prerequisite structure corpus;textual information sources;textual information;corpus semantic;corpus semantic impact;analyzing textual features;structure corpus semantic;textual features;structure corpus;corpus suggest;analyzing textual;domain textual information;corpus;corpus suggest task;pages analyzing textual;textual;results using wikipedia;pages containing corpora;containing corpora;using wikipedia;corpora;wikipedia;prerequisite structure generalizing;textual features pages;information sources;domain textual;information sources means", "pdf_keywords": ""}, "525b7f73744f5650391be4678d6d51ddaf23ed72": {"ta_keywords": "semiparametric estimation nonlinear;semiparametric estimation;estimation nonlinear errors;semi parametric estimation;nonlinear structural errors;2001 semiparametric estimation;estimation nonlinear structural;nonlinear errors invariables;nonlinear errors variables;consistent estimation nonlinear;parametric estimation nonlinear;nonlinear errors;estimation nonlinear;structural errors variables;errors invariables models;semiparametric;robust consistent estimation;econometrics;structural errors;based semi parametric;consistent estimation;errors variables models;2001 semiparametric;semi parametric;econometrics 110;parametric estimation;econometrics 110 26;nonlinear structural;errors variables model;models journal econometrics", "pdf_keywords": ""}, "d338bcd1e34a8259e123465203b05c5bf21aa12a": {"ta_keywords": "japanese speaker framework;model japanese speaker;speaker individuality japanese;japanese speaker using;acoustic model japanese;japanese erj voices;english speech synthesized;individual japanese speakers;speech preserving speaker;speech synthesized erj;based speech synthesis;individuality japanese speaker;english speech preserving;speech synthesis;preserving speaker individuality;japanese speaker;erj voices based;english acoustic model;prosody erj voices;model speaker dependent;speech synthesized;speech preserving;speaker framework;japanese english synthesize;captures speaker individuality;japanese speakers;speaker framework using;develop erj acoustic;preserving speaker;erj acoustic model", "pdf_keywords": ""}, "095bc69eddbf73fabf58a929d2be9a99c1b533a6": {"ta_keywords": "preference reasoning techniques;preference reasoning establishing;preference reasoning;recommendation systems kidney;preferences online resource;library preferences online;preflib library preferences;recommendation systems;applicability preference reasoning;computational social choice;preferences online;library preferences;preflib library;preferences higher;preferences higher introduce;www preflib org;introduce preflib library;kidney exchanges preferences;facets preference reasoning;preferences;preference;computational social;www preflib;http www preflib;library;data based simple;high quality data;data based;library invite community;preflib org", "pdf_keywords": ""}, "85099e075880a4844f3de77006a80c73daf99a4c": {"ta_keywords": "syntax german implementation;german based simplenlg;syntax german;describes simplenlg german;simplenlg german;features syntax german;based simplenlg;describes simplenlg;simplenlg german surface;paper describes simplenlg;syntax;simplenlg;realisation engine german;word order phenomena;word order;german implementation;grammatical coverage demonstrated;based simplenlg gatt;features syntax;2009 grammatical coverage;german surface realisation;realisation engine;grammatical coverage;focus word order;simplenlg gatt;surface realisation engine;simplenlg gatt reiter;reiter 2009 grammatical;german surface;engine german", "pdf_keywords": ""}, "f7ce4c7ec30c846cc122393deee98f1eacd24049": {"ta_keywords": "dialogue state tracking;dialogue state tracker;dialogue state;short term memory;term memory lstm;lstm;utterances dialogue;utterances separated vectors;receives utterances dialogue;memory lstm neural;utterances dialogue participants;construct lstm;outputs dialogue state;lstm neural networks;memory lstm;lstm neural;dialogue state current;dialogue participants input;lstm network;memory neural networks;dialogue;networks construct lstm;networks input utterances;outputs dialogue;dialogue participants;tracking using long;propose dialogue state;tracker based long;state tracking using;construct lstm network", "pdf_keywords": ""}, "bd24b47165407a8b2d32016645ca71f7c9213636": {"ta_keywords": "classify email speech;learning classify email;useful classify email;classify email;classify email according;email speech acts;email speech;typical email speech;classify email fashion;describing typical email;topical text classification;text classification;text classification learning;email according intent;text classification certain;existing text classification;acts useful classify;learning classify;speech acts useful;email fashion class;classification;useful classify;classify;acts demonstrate categorization;email according;speech acts;classification learning;categories messages;certain categories messages;typical email", "pdf_keywords": ""}, "b54d49cdf57abf7f7e7bcc0e946f450fd807f829": {"ta_keywords": "inverse reinforcement learning;inference inverse reinforcement;inverse reinforcement;reinforcement learning irl;problem inverse reinforcement;constraint inference inverse;reinforcement learning;estimating reward;irl markov decision;reinforcement learning reformulate;estimating reward function;maximum entropy irl;agent attempting maximize;likelihood expert agent;markov decision processes;action feature constraints;expert agent policy;irl markov;entropy irl framework;reward function seek;inference inverse;expert agent demonstrations;given knowledge mdp;agent policy demonstrated;based maximum entropy;reinforcement;learning irl;estimate state action;represented simple reward;simple reward", "pdf_keywords": "inference inverse reinforcement;inverse reinforcement learning;inverse reinforcement;constraint inference inverse;problem inverse reinforcement;reinforcement learning irl;reinforcement learning dexter;estimating reward function;estimating reward;reinforcement learning;reward function seek;irl markov decision;action feature constraints;constraint inference;inference inverse;maximum entropy irl;environment nominal reward;likelihood constraint inference;markov decision processes;reinforcement;focus estimating reward;estimate state action;inferring constraints;represented simple reward;entropy irl framework;markov decision;agent policy demonstrated;simple reward;constraints environment motivate;irl markov"}, "1a2410823486613e327892f05b38d3070f2d712c": {"ta_keywords": "efficient sampling networks;sampling networks;efficient sampling;sampling;networks;efficient", "pdf_keywords": ""}, "3bb1e24eb3429f807397833105d1e137d9927767": {"ta_keywords": "active sequence labeling;named entity recognition;sequence labeling;sequence labeling methods;entity recognition;sequence labeling sequence;entity recognition event;labeling sequence mixup;sequence labeling method;leveraging human annotations;labeling sequence;extra labeled sequences;labeling method seqmix;labeled sequences;augmenting active sequence;human annotations;detection tasks seqmix;tasks seqmix improve;labeled sequences iteration;generating extra labeled;named entity;sequences plausible seqmix;labeling;annotations;improve label efficiency;samples seqmix augmenting;labeling methods;experiments named entity;sequence mixup judges;sequences token level", "pdf_keywords": "facilitate sequence labeling;sequence labeling framework;sequence labeling baselines;active sequence labeling;sequence labeling;sequence labeling task;named entity recognition;sequence labeling method;labeled sequences queried;labeling task seqmix;entity recognition;seqmix data augmentation;labeling framework seqmix;tasks propose seqmix;tasks seqmix improve;sequences labels based;entity recognition event;detection tasks seqmix;labeled sequences;augmented labeled sequences;sub sequences labels;sequences labels;propose seqmix data;pseudo labeled sequences;tasks seqmix;task seqmix;seqmix capable generating;sequences queried samples;labeling task;seqmix data"}, "17c9a0f1a287c08bb2c1c1df47fa51ce1e428c4e": {"ta_keywords": "microphone speech recognition;multi microphone speech;speech recognition integrating;microphone speech;speech recognition;recognition integrating beamforming;beamforming robust feature;multi microphone;integrating beamforming robust;extraction advanced dnn;microphone;beamforming robust;advanced dnn rnn;integrating beamforming;dnn rnn backend;beamforming;advanced dnn;robust feature extraction;feature extraction advanced;dnn rnn;recognition integrating;rnn backend;rnn;feature extraction;speech;robust feature;recognition;dnn;extraction advanced;robust", "pdf_keywords": ""}, "a064010cf6fe594b2506a8fecd16dc0040211daa": {"ta_keywords": "neural machine translation;nmt decoder multilingual;decoder multilingual;multilingual encoding;decoder multilingual data;machine translation nmt;low resource languages;multilingual encoding method;translation nmt low;machine translation;purpose multilingual encoding;high resource language;multilingual data beneficial;multilingual data;efficient character gram;models translate lrl;lrl target language;nmt models translate;resource languages lrl;gram based embedding;languages lrl effective;multilingual;soft decoupled encoding;ones translate lrls;target language ones;translation nmt;character gram based;translate lrl target;models translate;translate lrl", "pdf_keywords": "transfer multilingual neural;multilingual transfer target;multilingual neural machine;lexical transfer multilingual;multilingual neural;multilingual encoding;multilingual transfer;embedding target words;neural machine translation;target word embedding;purpose multilingual encoding;transfer multilingual;embedding method multilingual;limiting multilingual transfer;multilingual encoding method;machine translation luyu;target lexical transfer;high resource language;limiting multilingual;improving target lexical;multilingual;low resource languages;general purpose multilingual;machine translation;word embedding;lexical transfer;gram based embedding;resource languages lrl;machine translation nmt;languages lrl effective"}, "2fbb75d7947808698f1554e4d400ec5ecb5ef998": {"ta_keywords": "reading comprehension dataset;challenging reading comprehension;choice reading comprehension;answer selection helps;answer selection task;reading comprehension;answer selection;multiple choice readingcomprehension;multiple choice reading;adapted answer selection;choice readingcomprehension long;dataset answer selection;attention multiple choice;comprehension dataset;choice readingcomprehension;choice reading;readingcomprehension long;comprehension dataset answer;readingcomprehension long documents;challenging reading;span prediction;attention multiple;span prediction models;performing span prediction;span prediction model;readingcomprehension;narrativeqa challenging reading;reading;long summaries;performance long summaries", "pdf_keywords": "reading comprehension models;reading comprehension model;improving answer selection;answer selection helps;answer selection task;reading comprehension dataset;answer selection aids;answer selection model;current reading comprehension;models answer selection;challenging reading comprehension;comprehension models;adapted answer selection;answer selection long;answer selection;dataset answer selection;performance reading comprehension;reading comprehension;comprehension model parallel;comprehension models answer;applying reading comprehension;comprehension dataset;predictions portions documents;comprehension model;document normalizing scores;span prediction;long documents weighted;performance long summaries;long summaries;reading comprehension rc"}, "c7c93601b52b1bcc68ec1f8b2c77c54f1b358ab9": {"ta_keywords": "pairwise comparison models;testing pairwise comparison;bound sample complexity;pairwise comparison data;test pairwise comparison;pairwise comparison;sample testing pairwise;comparison models;sample complexity;crowdsourcing long;instance crowdsourcing long;comparison models wst;comparison data;sample complexity required;sample test pairwise;comparisons;testing pairwise;theoretic lower bounds;ratings converted comparisons;question comparison data;comparisons paper;range pairwise comparison;crowdsourcing long standing;comparison data provided;bound sample;samples instance crowdsourcing;upper bound sample;comparison data prove;converted comparisons;test pairwise", "pdf_keywords": ""}, "e59adee86b666ad76164b3446cfee5068a15e5c9": {"ta_keywords": "abft nn inference;nn inference exploits;abft adaptive;neural networks nns;approach abft nn;efficient abft;abft nn;abft scheme nn;adaptive approach abft;abft adaptive arithmetic;guided abft adaptive;nn layer neural;efficient abft scheme;abft schemes best;selects efficient abft;approaches abft;nn layers;current approaches abft;abft reduces execution;networks nns increasingly;spacecraft investigate abft;nns increasingly;abft;nn inference;networks nns;approach abft;overhead 09 nns;nns;nn layer;scheme nn layer", "pdf_keywords": "nn inference gpus;network inference gpus;inference gpus;inference gpus sc;fault tolerance nns;gpu hardware arithmetic;nn inference exploits;gpus lead nns;execution nn inference;gpus;hardware arithmetic intensity;fault tolerance nn;fault tolerance abft;gpu hardware;inference gpus lead;abft nn inference;tolerance nn inference;gpu;redundant execution nn;trends gpu hardware;nn inference intensityguided;fault tolerance neural;nn inference algorithm;gpus sc;trends gpu;arithmetic intensity nn;tolerance nns section;tolerance nns;nn inference;hardware arithmetic"}, "e6fa88f4af68aa7be4ae91940892eee52571997c": {"ta_keywords": "shot object detection;detecting rare objects;crucial shot object;classes crucial shot;detectors rare classes;shot object;simple shot object;object detection task;object detection;detection detecting rare;outperforms meta learning;simple shot;meta learning;object detection detecting;rare objects examples;detectors rare;objects examples emerging;rare objects;meta learning methods;detecting rare;existing detectors rare;frustratingly simple shot;crucial shot;new benchmarks;detectors;benchmarks doubles accuracy;detection task fine;detection;current benchmarks;detection task", "pdf_keywords": "neural networks shot;siamese neural;siamese neural networks;learning 2015 benchmarks;meta learning;meta learning methods;koch siamese neural;previous meta learning;shot object detection;recognition revise evaluation;outperformed koch siamese;dataset models improved;new benchmarks;new benchmarks models;learning methods large;benchmarks based datasets;siamese;large margin recognition;machine learning 2015;recognition revise;benchmarks revised evaluation;benchmarks models;benchmarks revised;arts lvis dataset;early lexical learning;benchmarks;shot object;networks shot image;conference machine learning;2015 benchmarks"}, "db0a3ce9f315f650fe5220101c5677778de39fee": {"ta_keywords": "machine translation reordering;parser machine translation;discriminative parser optimize;translation reordering using;translation reordering;optimize machine translation;discriminative parser machine;learning discriminative parser;reordering treating parser;inducing discriminative parser;discriminative parser;parallel text inducing;machine translation;translation reordering treating;parser optimize;parser optimize machine;aligned parallel text;text inducing discriminative;parser derivation tree;parser machine;accuracy factored parse;parallel text;parser;parser derivation;learning discriminative;factored parse tree;parse;inducing discriminative;treating parser derivation;parse tree", "pdf_keywords": ""}, "06f4de06fc37576e1e381cd76e375d57852047b9": {"ta_keywords": "neural machine translation;translation multi task;machine translation multi;machine translation nmt;machine translation;translation nmt achieves;translation multi;text improving robustness;improving robustness neural;shared task french;robustness neural machine;trained clean text;translation nmt;robustness neural;text improving;clean text improving;robustness shared task;text performance known;multi task learning;task french english;french english dataset;translation;task french;text performance;transformer trained clean;text typos grammatical;english dataset bleu;task learning submission;domain text performance;text typos", "pdf_keywords": ""}, "cd3595f65519e4af6bcd073790ac32acdafadf55": {"ta_keywords": "domain adapting appearance;shot image generation;adapting appearance target;adapting appearance;source domain adapting;pretraining human faces;weights adaptation;source domain pretraining;image generation;domain adapting;image generation seeks;appearance target shot;domain pretraining human;diversity source domain;domain pretraining;adaptation;shot image;target shot image;adapting;weights adaptation order;changes weights adaptation;adaptation order best;adapt pretrained model;pretrained model;domain adapt pretrained;domain available training;pretrained model introducing;adaptation order;appearance target;source dataset fitting", "pdf_keywords": "shot image generation;pretrain generative;image generation;pretrain generative model;image generation continuous;generative model source;\ufb01rst pretrain generative;generative;training algorithm generate;weights adaptation;generation continuous learning;generative model;domain real faces;generating high quality;shot image;real faces lot;real faces;dataset \ufb01tting target;source dataset \ufb01tting;faces lot data;adaptation;learning;weights adaptation order;changes weights adaptation;continuous learning;adaptation order best;faces lot;training algorithm;source dataset;faces"}, "6887537de3655a25c75bf4d0833f51e72331bdad": {"ta_keywords": "signal extended audio;signal enhanced audio;audio signal enhanced;enhanced audio signal;extended audio signal;converts noisy audio;audio signal processed;noisy signal extended;extended audio;detecting noisy audio;audio signal environment;audio signal process;converting noisy signal;audio signal;noisy audio signal;enhanced audio;audio signal detecting;signal processed extension;signal detecting noisy;noisy audio;converts noisy;method converting noisy;detecting noisy;signal extended;noisy signal;signal enhanced;converting noisy;audio;extension network network;extension network", "pdf_keywords": ""}, "a8fc183c089bd596ccc48b3d666f8814e1b41e55": {"ta_keywords": "large generative code;code infilling synthesis;generative code model;generative code;program synthesis;program synthesis left;program synthesis benchmarks;perform program synthesis;trained generate code;standard program synthesis;generation editing infilling;generation editing;large generative;generative model code;generate code;code infilling bidirectional;generate code files;right generation editing;inference comment generation;generative;infilling synthesis;allowing code infilling;code infilling;incoder unified generative;model large generative;incoder trained generate;incoder generative;synthesis benchmarks;unified generative;trained generate", "pdf_keywords": "generation editing in\ufb01lling;program synthesis editing;generation editing;conditioned code editing;synthesis editing qualitative;right generation editing;code in\ufb01lling editing;interactive code editing;code editing;in\ufb01lling editing tasks;in\ufb01lling editing;code editing interactive;program synthesis;editing qualitative;editing qualitative examples;comment generation;docstring generation;generation metadata conditioning;editing in\ufb01lling;generative;editing tasks introduce;perform program synthesis;bidirectional translation technical;training generative;model program synthesis;incoder uni\ufb01ed generative;synthesis editing;program synthesis left;editing interactive code;translation technical"}, "9a43dda4b01dde5d513c431564098e4d8794a7a5": {"ta_keywords": "sentiment datasets;large sentiment datasets;document level sentiment;word cluster features;sentiment analysis;level sentiment analysis;sentiment datasets czech;new semi supervised;sentiment analysis test;semi supervised;method large sentiment;level sentiment;semi supervised method;word cluster;large sentiment;adding word cluster;movie reviews;sentiment;clusters words;movie reviews outperform;english movie reviews;clusters words represented;cluster features;exploit clusters words;movie product reviews;cluster features article;semantic spaces;reviews english movie;product reviews;reviews outperform", "pdf_keywords": ""}, "2f201c77e7ccdf1f37115e16accac3486a65c03d": {"ta_keywords": "pruning robust adversarial;adversarial defense stochastic;adversarial defense;strategy adversarial defense;robust adversarial defense;adversarial defense neural;strategy adversarial;guard adversarial;adversarial;guard adversarial examples;vulnerable adversarial;adversarial examples;robust adversarial;mixed strategy adversarial;vulnerable adversarial examples;adversarial examples inspiration;adversarial examples experiments;known vulnerable adversarial;defense neural networks;activation pruning robust;defense stochastic activation;game adversary;stochastic activation pruning;confers robustness attacks;robustness attacks;game adversary model;robustness attacks increasing;defense neural;adversary;adversary model", "pdf_keywords": "adversarial defense;robust adversarial defense;adversarial defense guneet;sap effective adversarial;pruning robust adversarial;sap adversarial training;strategy adversarial defense;effective adversarial;training adversarial;adversarial training;sap adversarial;combining sap adversarial;adversarial defense nayebi;adversarial training yields;vulnerable adversarial;robust adversarial;training adversarial examples;adversarial;effective adversarial examples;adversarial training method;adversarial setting;bene\ufb01ts showed adversarial;vulnerable adversarial examples;adversarial examples light;showed adversarial;method training adversarial;strategy adversarial;adversarial examples;showed adversarial setting;adversarial examples notably"}, "136235d2a3dc4f1c995eaf977aec9c42114da850": {"ta_keywords": "task morphological reinflection;morphological reinflection generalization;crosslingual variation morphosyntactic;reinflection generalization languages;morphological reinflection focuses;typological diversity crosslingual;morphological reinflection;morphosyntactic features;shared task morphological;crosslingual variation;task morphological;language families;languages achieving;generalization languages;diversity crosslingual variation;languages;languages achieving 90;13 language families;morphological;language families resourced;variation morphosyntactic features;languages addition;morphosyntactic;reinflection focuses typological;diversity crosslingual;performance majority languages;majority languages achieving;majority languages;morphosyntactic features transformer;generalization languages addition", "pdf_keywords": ""}, "8a09c90f6e9a3f6c3b172e5059c7af47f528f66b": {"ta_keywords": "reinforcement artistic typography;artistic typography;artistic typography approach;thematic reinforcement artistic;text visually appealing;memorable semantic reinforcement;reinforcement artistic;semantic reinforcement;typography approach;text visually;make text visually;typography;semantic reinforcement called;visually resemble letters;appealing memorable semantic;thematic reinforcement;semantic reinforcement use;typography approach make;letters adding creative;visually appealing memorable;approach semantic reinforcement;make text;theme word;text;artistic;reinforcement use visual;word theme;represent letters;represent letters cliparts;treat thematic reinforcement", "pdf_keywords": "reinforcement artistic typography;artistic typography treat;thematic reinforcement artistic;artistic typography;artistic typography present;typography treat generate;reinforcement artistic;reinforcement text replacement;semantic reinforcement;memorable semantic reinforcement;artistic typography introduction;semantic reinforcement use;semantic reinforcement called;semantic reinforcement text;artistic typography purva;typography present computational;thematic reinforcement;treats semantic reinforcement;approach semantic reinforcement;reinforcement text;based word typography;typography given word;word typography;typography;typography treat;theme based word;text visually appealing;treat thematic reinforcement;approach thematic reinforcement;word typography given"}, "1022696090666eab5c82ebc07d63c0de2fca2521": {"ta_keywords": "identifiers soft joins;joins based similarity;soft joins based;web joins;web joins generalize;wide web joins;soft joins;joins generalize text;joins based;similarity textual identifiers;inductive classification tasks;classification tasks whirl;perform soft joins;soft joins extend;text classification;joins generalize;textual identifiers soft;inductive classification;joins;similarity based reasoning;inductive classification systems;text classification using;generalize text classification;classification using whirl;classification tasks;based similarity textual;classification tasks using;relational databases;joins extend;joins extend traditional", "pdf_keywords": ""}, "71124b00b873e85aa55b07100cd5b492e5b1d73d": {"ta_keywords": "integrity message authentication;distributed storage integrity;integrity protection scheme;secure distributed storage;storage integrity protection;integrity protection realized;integrity protection;proposing integrity protection;secure distributed;based universal2 hash;theoretically secure distributed;storage integrity;data integrity certification;achieve data integrity;encryption secret sharing;data integrity;quantum key distribution;data integrity message;universal2 hash;key distribution qkd;sharing powerful security;message authentication;storage using quantum;integrity check data;trusted calculator computes;using quantum key;secret sharing;verification implement scheme;integrity certification data;data owner secret", "pdf_keywords": "security distributed storage;verifiable secret sharing;storage authentication;authentication data integrity;storage authentication data;transmission storage authentication;secret sharing protocol;sharing protocol promising;theoretical security distributed;based verifiable secret;secure data transmission;verifiable secret;security distributed;data integrity party;secret sharing scheme;secure data;theoretically secure data;ensures data integrity;secret sharing;sharing scheme ensures;information theoretical security;demonstration secure data;sharing protocol;authentication data;distributed data storage;distributed storage information;protocol promising solution;theoretical security;qkd links secret;data integrity"}, "eb7a64195ef4a268f79fa6740f128387f2696c65": {"ta_keywords": "inverse reinforcement;inverse reinforcement learning;reward environment learn;uses inverse reinforcement;maximize reward environment;agents maximize reward;reinforcement learning policy;constraints demonstrations reinforcement;policy contextual bandit;demonstrations reinforcement learning;reward maximizing constrained;reinforcement learning learn;reward environment;environment reward based;actions reward maximizing;demonstrations reinforcement;learning learn maximize;reward maximizing;environment reward;reinforcement;rewards teaching ai;learning policy orchestration;based environment reward;learn maximize environmental;reinforcement learning;maximize environmental rewards;maximize reward;orchestration contextual bandit;learn maximize;contextual bandit based", "pdf_keywords": ""}, "6cddfbed35c46937588bd9d6b846ca2855953cea": {"ta_keywords": "speech translation lattices;neural lattice sequence;inputs word lattices;lattice sequence models;neural lattice;lattice posterior scores;translation lattices;consume word lattices;word lattices allows;lattice posterior;stream neural lattice;translation lattices report;attentional encoder decoder;attentional encoder;lattices used encoder;word lattices;input neural sequence;encoder attentional;hypothesis lattice posterior;neural sequence sequence;latticelstm able;neural sequence;latticelstm;encoder attentional encoder;used encoder attentional;lattice sequence;word lattices used;integrate lattice posterior;bias term attention;2015 latticelstm", "pdf_keywords": "speech translation lattices;sequential encoder attentional;lattice encoder conditioned;generalization sequential encoder;lattice encoder;attentional encoder decoder;sequential encoder pre;translation lattices;attentional encoder;encoder attentional;lattice encoder strict;lattice posterior scores;translation lattices report;sequential encoder;fact lattice encoder;encoder attentional encoder;lattice scores crucial;lattice encoder replaces;units lattice encoder;derive lattice encoder;encoder pre training;high lattice scores;training sequential data;hypothesize lattice scores;outputs bias attention;lattice scores;pre training sequential;encoder conditioned predecessor;improvements baselines translate;integrating lattice scores"}, "58a2e825884bc86e650fffafb86a2833117852c5": {"ta_keywords": "ranking tuning pre;tuned ranked ptms;tuning heterogeneous ptms;models rank ptms;novel bayesian tuning;tuning pre trained;hubs ranking tuning;ranking tuning;bayesian tuning tuning;ptm selection;ptm tuned ranked;tuning multiple ptms;bayesian tuning;pre trained models;ptm fine tuned;tuning homogeneous ptms;ranked ptms;ranked ptms proposed;rank ptms;rank ptms model;ptm selection popularity;pre trained model;model hubs ranking;best ranked ptm;ranked ptm;ptms proposed tuning;fine tune ptm;tune ptm;pick ptm;usually pick ptm", "pdf_keywords": "model hubs ranking;models rank ptms;ranking tuning pre;hubs ranking tuning;ptm hubs ranking;pre trained models;trained models rank;ranking tuning;rank ptms model;exploiting model hubs;rank ptms;paradigm ranking tuning;models rank;tuning pre trained;ptms model hub;trained models;trained models new;trained models conference;exploiting ptm hubs;hubs ranking;exploit ptm hub;ptms tasks;ptms tuning;trained models paper;ptms tuning su\ufb03ciently;ranking;ptm hub;ptm hubs;new paradigm ranking;paradigm exploiting ptm"}, "e6cec3044688f1701b4b72b4b2189f215abc3759": {"ta_keywords": "crowdsourced evaluation tasks;eliciting truthful responses;crowdsourced evaluation;truthful responses agents;massively crowdsourced evaluation;crowdsourcing types evaluation;challenge faced crowdsourcing;effort eliciting truthful;incentivizing truthful behavior;crowdsourced;crowdsourcing;crowdsourcing types;incentivizing truthful;agreement mechanisms;faced crowdsourcing;evaluation tasks mechanisms;faced crowdsourcing types;eliciting truthful;serums massively crowdsourced;reward mechanisms settings;agreement mechanisms despite;responses agents;arbitrary incentivizing effort;massively crowdsourced;beliefs arbitrary incentivizing;new reward mechanisms;output agreement mechanisms;reward mechanisms;incentivizing effort eliciting;reward", "pdf_keywords": ""}, "549df5fc83c382cbdf633dc782fa67bf2f983f2c": {"ta_keywords": "differentially private sgd;gradient descent random;random mixtures sgdrm;stochastic gradient descent;activations differentially private;descent random mixtures;differentially private;propose stochastic gradient;private sgd;threats sgdrm converges;mixtures sgdrm simple;mixtures sgdrm;stochastic gradient;gradient descent;sgdrm converges globally;sgdrm simple;breach threats sgdrm;sgd;deep neural;random mixtures;threats sgdrm;deep neural networks;networks linear activations;sgdrm simple way;sgdrm converges;descent random;protecting data;sgdrm;private sgd andom;way protecting data", "pdf_keywords": ""}, "f61886d138497431cbeaa7bb73051bfb7a745026": {"ta_keywords": "captions linguistic supervision;subjects objects captions;images captions linguistic;objects captions;image level supervision;supervision scalable crowdsourced;linguistic supervision scalable;captions relations;captions weak prior;captions linguistic;objects captions weaker;captions relations individual;supervision visual scene;human annotated subjects;leveraging phrasal sequential;linguistic supervision;leveraging phrasal;annotated subjects objects;scene graph generation;crowdsourced triplets;human annotated;triplets nouns captions;list human annotated;provided captions relations;context subjects objects;crowdsourced triplets given;categorical supervision;weak supervision visual;scalable crowdsourced triplets;multimodal data", "pdf_keywords": "captions supervision;using captions supervision;supervision captions;captions supervision captions;supervision captions use;image level supervision;captions use crowdsourced;context using captions;leveraging phrasal;captions use;using captions;leveraging phrasal sequential;accompanying images web;captions;diverse sources multimodal;accompanying images;text accompanying images;impact leveraging phrasal;multimodal data web;multimodal data;leverage instance image;video descriptions instructional;multimodal;descriptions instructional videos;instructional videos;sources multimodal;context techniques improve;sources multimodal data;imagecaption;images web"}, "275aaa20ba853c40a461f224eefbf06730bf03a9": {"ta_keywords": "robust hessian power;hessian power method;gradient descent methods;perturbed gradient descent;nonconvex optimization;gradient descent;implementing robust hessian;robust hessian;hessian power;topic nonconvex optimization;gradient based algorithm;compared perturbed gradient;hessian;algorithm smooth function;iterations algorithm polynomially;perturbed gradient;propose simple gradient;simple gradient based;descent methods;algorithm smooth;algorithm polynomially better;method using gradients;gradient based;gradients negative curvature;based algorithm smooth;achieve polynomial speedup;polynomial speedup log;simple gradient;gradient;using gradients negative", "pdf_keywords": "introduction nonconvex optimization;nonconvex loss functions;nonconvex optimization introduction;gradient descent methods;gradient descent;nonconvex optimization;perturbed gradient descent;optimization introduction nonconvex;simple gradient descent;robust hessian power;gradient descent based;topic nonconvex optimization;models nonconvex loss;hessian power method;implementing robust hessian;nonconvex optimization central;robust hessian;hessian power;hessian;escaping saddle points;introduction nonconvex;escape saddle points;nonconvex loss;models nonconvex;descent methods paper;formulated models nonconvex;descent methods;compared perturbed gradient;gradient based algorithm;perturbed gradient"}, "e6924d247b56980260e4c68dbc51b947409e4764": {"ta_keywords": "local sgd unified;local sgd;sgd unified theory;sgd unified;sgd;theory new efficient;new efficient methods;efficient methods;supported kaust baseline;efficient methods work;kaust baseline research;unified theory new;unified theory;kaust baseline;supported kaust;arik research gorbunov;richt arik research;work supported kaust;research gorbunov partially;efficient;local;kaust;supported ministry science;arik research;methods work supported;new efficient;higher education russian;gorbunov partially supported;theory;methods", "pdf_keywords": "local sgd methods;sgd methods convex;sgd methods;analyzing local sgd;local sgd uni\ufb01ed;local sgd;sgd uni\ufb01ed theory;sgd;sgd uni\ufb01ed;distributed federated training;methods convex strongly;federated training supervised;methods convex;learning models;strongly convex regimes;convex regimes distributed;strongly convex;convex strongly convex;convex strongly;supervised machine learning;machine learning models;training supervised;federated training;supervised;regimes distributed federated;convex regimes;training supervised machine;distributed federated;new e\ufb03cient methods;machine learning"}, "a1fc0041ef89ed5371317c8e2cc5effa8f38ae48": {"ta_keywords": "bayesian network structure;reliable causal discovery;network structure learning;causal discovery;causal discovery improved;graphs reliable causal;research bayesian network;bayesian network;super structure estimation;large graphs reliable;structure learning;large graphs;structure estimation;network structure;structure learning focuses;search local clusters;nodes;scale large graphs;graphs reliable;structure estimation method;neighbors hops superstructure;hundreds nodes;assumption exact search;hundreds nodes high;discovery improved exact;discovery improved;local clusters;super structure;reliable causal;exact search methods", "pdf_keywords": "causal discovery linear;reliable causal discovery;causal discovery;assumptions causal discovery;causal discovery procedure;discovery linear structural;super structure estimation;search linear gaussian;estimated super structure;structure estimation;methods linear gaussian;discovery linear;constraint estimated super;structure estimation method;exact search linear;common assumptions causal;reliable causal;linear gaussian;assumptions causal;linear gaussian setting;scalability exact search;search linear;linear structural equation;linear structural;structural equation model;support inverse covariance;causal;structural equation;rise reliable causal;covariance matrix requires"}, "0a227a21172f7344ad911aeefc40ae4ec82d7cac": {"ta_keywords": "metaphor annotated datasets;metaphor using nlp;metaphor identification workshop;metaphor annotated;lingual metaphor identification;metaphor identification;modelling metaphor using;useful metaphor identification;modelling metaphor;creation metaphor annotated;metaphor identification cross;lingual metaphor;metaphor using;approaches figurative language;features useful metaphor;existing annotated corpora;figurative language;annotated corpora;computational approaches figurative;annotated corpora used;figurative language naacl;metaphor;linguistic creativity naacl;lexical semantics;approaches linguistic creativity;linguistic creativity;cross lingual metaphor;annotated datasets;nlp methods;computational lexical semantics", "pdf_keywords": ""}, "178f424d0f156cbf5b35eb241fc00b27a0a3808b": {"ta_keywords": "speech recognition asr;asr using lstm;speech enhancement se;speech recognition combination;robust speech recognition;speech recognition;lstm based se;performance speech enhancement;combination speech enhancement;hybrid lstm;asr sequence training;memory lstm;speech enhancement recognition;noise robust speech;task hybrid lstm;architecture lstm;term memory lstm;speech enhancement;hybrid lstm network;architecture lstm based;effective modeling speech;robust speech;using lstm;automatic speech recognition;modeling speech achieved;lstm networks;lstm;recognition combination speech;modeling speech;lstm based", "pdf_keywords": ""}, "317ed59456d76b500a7eb63b181df9e8b795976b": {"ta_keywords": "parking compare nash;queueing game;based queueing game;observe queuing game;framework urban parking;queuing game;queuing game framework;parking urban centers;parking urban;urban parking compare;urban parking;model parking urban;queues overlay game;parking compare;parking;queueing;nash induced welfare;based queueing;maximizers consider games;queues network;queues;queue;circling nash solution;nash solution;decide based queueing;queueing game enter;queuing;nash solution suboptimal;parallel queues;observe queuing", "pdf_keywords": "parking parallel queues;queues impose game;queueing game;based queueing game;queue \ufb02ow network;queues network;congestion develop simulation;traf\ufb01c congestion;congestion welfare;wait time congestion;time congestion welfare;based queueing;congestion welfare drivers1;queueing;occupancy congestion relationship;congestion;occupancy congestion;overall traf\ufb01c congestion;traf\ufb01c congestion develop;queues impose;parking resources;queue;queues;parallel queues;nash socially optimal;congestion relationship;queue \ufb02ow;game nash equilibrium;parallel queues impose;nash equilibrium"}, "56823e326f2515f73662b176054fbee0895e0c44": {"ta_keywords": "forms navigate assistant;web forms;navigate assistant;request learning navigate;navigate web forms;assistant navigation;helps user navigate;handle request learning;web forms familiar;update request specialist;learning navigate web;request learning;assistant helps user;navigate assistant deployed;forms navigate;thousands forms navigate;navigate web;accomplish update request;user navigate;requests users;tasks users various;request specialist expertise;forms familiar tasks;update request www;complex tasks users;familiar tasks users;building assistant navigation;request specialist;web forms large;expertise handle request", "pdf_keywords": ""}, "7891ec1d8ba2abf238326dc6e8862cc4431a6f5c": {"ta_keywords": "heuristic placing relay;relay placement minimize;relay placement;network random lattice;lattice path markov;wireless network random;optimal impromptu deployment;deployment multi hop;convex hop costs;path optimal impromptu;link scheduling consider;random lattice path;costs number relays;wireless network deployment;sequential placement policies;link scheduling;optimal sequential placement;placing relay distance;path optimal;multi hop wireless;relay distance previously;network random;problem relay placement;consider problem relay;relay distance;hop wireless network;link link scheduling;network deployment operative;placing relay;placed relay", "pdf_keywords": ""}, "cdf5eb63e9c2434073e811aba50ae80ede9d15f6": {"ta_keywords": "focused retrieval web;collection focused retrieval;retrieval passage retrieval;retrieval focused retrieval;focused retrieval passage;retrieval web;retrieval web document;focused retrieval;focused retrieval focused;relevance using crowdsourcing;announcement focused retrieval;passage retrieval;passage retrieval important;retrieval focused;web document corpus;documents highly ranked;retrieval passage;retrieval;document corpus category;document corpus;question answering systems;based collection focused;retrieval important;relevance using;highly ranked query;judged relevance using;question answering;crowdsourcing new collection;ranked query highly;collection focused", "pdf_keywords": ""}, "1afe82d34c182d43cbcc365d26e704058aa32351": {"ta_keywords": "corpus voice conversion;voice conversion;integration based voice;based voice conversion;voice conversion mivc;parallel corpus voice;density model speaker;voice conversion vc;corpus voice;model speaker model;model speaker;gaussian mixture model;speaker model;based gaussian mixture;combines parameter generation;voice;speaker model mitigate;speaker model second;mixture model gmm;gaussian mixture;mixture model;parameter generation algorithm;speaker;requirement parallel corpus;parameter generation;combines parameter;paper combines parameter;parallel corpus;proposes parameter generation;probabilistic integration joint", "pdf_keywords": ""}, "2c871df72c52b58f05447fcb3afc838168d94505": {"ta_keywords": "knowledge neurons pretrained;knowledge neurons knowledge;knowledge neurons;concept knowledge neurons;activation knowledge neurons;fact activation knowledge;factual knowledge stored;cloze task bert;knowledge stored pretrained;task bert;recalling factual knowledge;neurons knowledge neurons;neurons knowledge;knowledge neurons positively;factual knowledge presented;activation knowledge;task bert large;pretrained language models;factual knowledge;neurons express fact;express fact activation;concept knowledge;knowledge stored;knowledge;knowledge presented training;bert large scale;knowledge attribution;fact activation;pretrained language;knowledge presented", "pdf_keywords": "knowledge neurons pretrained;knowledge neurons;analysis knowledge neurons;knowledge pretrained transformers;knowledge neurons feed;utilize knowledge neurons;analyze knowledge neurons;activation knowledge neurons;knowledge neurons update;knowledge neurons based;examine knowledge neurons;knowledge neurons positively;knowledge pretrained;knowledge \ufb01nd activation;pretrained language models;transformers examine knowledge;\ufb01nd activation knowledge;recalling factual knowledge;activation knowledge;knowledge expressed;neurons express fact;neurons pretrained transformers;factual knowledge presented;knowledge expressed implicit;task knowledge expressed;pretrained language;erase knowledge pretrained;knowledge expression;factual knowledge \ufb01nd;factual knowledge"}, "26c2aad87810418b09e0f5b80352dd4d2536afe3": {"ta_keywords": "automated social skills;ease talking ability;communication skills human;talking ability;skill speech language;talking ability talk;human social skills;social skill speech;user speech language;developing dialogue;user speech;skills effect computer;sst developing dialogue;talk compared interaction;ease talking;recognizes user speech;social communication skills;ability talk compared;skills human;language features human;skill speech;computer based training;social skills training;social interaction;human human interaction;human interaction hhi;human interaction;social skills;social interaction acquire;use interaction avatar", "pdf_keywords": ""}, "07cedc7899497f2f4ee6f4736e03b78accb47b74": {"ta_keywords": "semi supervised learning;semi supervised classification;intuitive semi supervised;semi supervised;relational neighbor classifier;learning network data;accuracy semi supervised;learning labeled;learning labeled unlabeled;labeled training data;supervised learning network;baseline semi supervised;supervised learning ssl;neighbor classifier;supervised classification network;classification network data;learning network;required learning labeled;supervised classification;reduce labeled training;supervised learning;supervised learning method;classification network;classification accuracy semi;labeled data;goal semi supervised;relational neighbor;neighbor classifier wvrn;benchmark datasets labels;authoritative instances training", "pdf_keywords": ""}, "d4d26ccbf1e64e725b5bffc08ab28a72e271facb": {"ta_keywords": "dependent text sql;pragmatic chinese dataset;text sql xdts;queries annotated;sql queries annotated;queries easy;queries annotated 280;940 questions sql;chinese dataset xdts;text sql;queries easy cross;database context dependent;coherent question sequences;sql queries easy;easy sql queries;sql xdts;chinese dataset;context dependent text;cross database context;easy sql;database context;questions context independent;easy cross database;queries consists;annotated 280 databases;context independent questions;questions context;question sequences indicating;challenges xdts;major challenges xdts", "pdf_keywords": ""}, "26d5c7ad2778c77a1b8734dceb34fe38a1179e2f": {"ta_keywords": "generate malicious app;malicious applications app;malicious apps;novel malicious application;malicious application detection;applications malicious app;malicious app space;malicious apps space;malicious app;situation malicious apps;malicious applications;app space malwares;various malicious applications;possible malicious applications;applications malicious;malicious application;malicious applications malicious;real time malicious;application detection model;model directly malwares;application detection;time malicious application;application detection situation;propose novel malicious;malwares detection various;malwares limitation application;malwares detection;novel malicious;model generate malicious;detection various malicious", "pdf_keywords": ""}, "9045bf2a9c1e2b9621c69c57f991d10880e91f18": {"ta_keywords": "computational hardness;infer computational hardness;algorithms computational hardness;computational hardness variant;sparse pca;hardness host statistical;computational hardness host;statistical problems sparse;problems sparse pca;polynomial time algorithms;submatrix detection;computational efficiency existence;computational efficiency;clique problem studied;hardness variant planted;planted clique problem;equating computational efficiency;algorithms;algorithms computational;infer computational;variant planted clique;sparse pca br13a;clique problem used;planted clique;interesting computational;submatrix detection mw15;clique problem;pca br13a submatrix;hardness variant;host statistical problems", "pdf_keywords": "computational hardness;computational hardness host;infer computational hardness;computational hardness variant;hardness host statistical;algorithms computational hardness;transfer computational hardness;hardness variant planted;examples sparse pca;submatrix detection;sparse pca;polynomial time algorithms;variant planted clique;clique problem statistical;infer computational;clique problem studied;abstract planted clique;statistical problems sparse;planted clique problem;leakage planted clique;interesting computational;planted clique;computational phenomena associated;hardness host;submatrix detection mw15;host statistical problems;hardness variant;statistical problems robust;used infer computational;computational e\ufb03ciency space"}, "24a2f68cf81ba3ee55e7a87d0770374ab8e99858": {"ta_keywords": "learnability recursive logic;recursive logic programs;learnable equivalence queries;programs learnable equivalence;recursive programs learnable;logic programs learning;learnability recursive;free recursive logic;eecient learnability recursive;pac learnability recursive;learnability function free;learnable equivalence;logic programs models;recursive logic;logic programs;learnability function;logic programs particular;analyze learnability function;learning boolean functions;recursive programs;simulation pac learnability;programs learnable;class recursive programs;hard learning boolean;learnability;learning boolean;analyze learnability;eecient learnability;programs learning;programs models polynomial", "pdf_keywords": ""}, "5ed4b17a4b7932619f0969e1f5acae76e90f7bdd": {"ta_keywords": "neural semantic parsers;recursive semantic parsing;recparser recursive semantic;utterances nested sql;utterance predicting sql;novel recursive semantic;semantic parsing;semantic parsers;sql query generation;semantic parsing framework;query generation;semantic parsers usually;predicting sql queries;sql query layer;query generation problems;recursive semantic;nested sql queries;query generation problem;layer neural semantic;parsing framework;recparser generate nested;complicated utterances nested;parsers;parsing;parsers usually;neural semantic;text sql task;parse long complicated;utterances nested;generate nested sql", "pdf_keywords": ""}, "feb403bb5a064ab68b2db655b80a7417f7cfc9f3": {"ta_keywords": "relational learning;proposed relational learning;novel relational learning;relational learning method;relation tree based;relational learning approach;features modeling relational;relation tree;modeling relational data;modeling relational;features relational;features relational data;datasets proposed relational;called relation tree;model markov networks;markov networks;relational mns;relational data achieves;relational data;markov networks mns;relational data empirical;novel relational;relational;class relational mns;pairwise features relational;propose novel relational;relational mns rmns;proposed relational;restricted class relational;tree based rmn", "pdf_keywords": ""}, "2fb44f1317bc51a1e011a5a44d817ad9104e29e8": {"ta_keywords": "privacy applications nlp;unsubstantiated differential privacy;differential privacy;differential privacy applications;privacy provides formal;private auto encoder;differentially private;tight privacy guarantees;differential privacy provides;utterances privatized easily;formal approach privacy;differentially private auto;privacy guarantees;dimension utterances privatized;providing tight privacy;utterances privatized;privacy guarantees intention;adept differentially private;tight privacy;point differential privacy;privacy individuals;differentially private rendering;privacy applications;privacy provides;privacy;approach privacy individuals;applications nlp rely;private mechanism;approach privacy;encoder text rewriting", "pdf_keywords": "privacy meets nlp;differential privacy;differential privacy meets;concepts differential privacy;differential privacy dp;neighborhood differential privacy;utterances project latent;privacy dp present;privacy dp;privacy;utterances;privacy meets;language processing emnlp;utterances project;space input utterances;input utterances;input utterances project;meets nlp devil;language processing;natural language processing;lm atis;meets nlp;nlp devil;methods natural language;latent space;latent space explore;natural language;nlp;train lm atis;nlp devil ivan"}, "b3848d32f7294ec708627897833c4097eb4d8778": {"ta_keywords": "language models dialog;fine tuning annotated;safety factual grounding;models dialog applications;neural language models;models specialized dialog;tuning annotated;crowdworker annotated data;tuning annotated data;improving model safety;crowdworker annotated;language models;models dialog;small crowdworker annotated;challenges safety factual;safety factual;words public dialog;annotated data;model safety;improvements safety factual;dialog data;language models specialized;quantify safety using;safety using metric;public dialog data;lamda language models;transformerbased neural language;annotated;factual grounding;quantify safety", "pdf_keywords": "crowdworker annotated data;crowdworker annotated;speci\ufb01c helpfulness useful;dialog modeling;small crowdworker annotated;dialog modeling finally;recommendations analyze helpfulness;analyze helpfulness;retrieval tool dialog;speci\ufb01c helpfulness;tool dialog modeling;candidate responses using;models specialized dialog;analyze helpfulness role;tuning annotated data;tuning annotated;dialog examples collected;utterances match agent;candidate responses;annotated data model;helpfulness useful;consistency agent utterances;helpfulness role;helpfulness;helpfulness role consistency;\ufb01ltering candidate responses;dialog examples;use information retrieval;dialog data;useful correct responses"}, "17c5e16d16585a01fbfd90ff39f6799952675b21": {"ta_keywords": "factorization conversational bilingual;bilingual speech recognition;monolingual code switch;factorized final bilingual;conversational bilingual;conversational bilingual speech;bilingual speech;monolingual asr conditional;switched monolingual asr;bilingual output code;bilingual output;code switched monolingual;final bilingual output;comprise bilingual speech;monolingual code;monolingual asr;switched monolingual;tasks comprise bilingual;bilingual speech encompasses;bilingual;monolingual sub tasks;utterances purely monolingual;recognition defining monolingual;final bilingual;likelihoods monolingual code;given monolingual information;comprise bilingual;monolingual information joint;monolingual information;given monolingual", "pdf_keywords": "conditional factorization bilingual;factorization bilingual task;factorization bilingual;model bilingual mandarin;proposed model bilingual;model bilingual;bilingual asr;bilingual speech recognition;formulation bilingual asr;speech recognition monolingual;bilingual mandarin;monolingual asr conditional;bilingual asr problem;bilingual task;monolingual code switch;bilingual speech;bilingual mandarin english;bilingual task logically;recognition monolingual code;formulation bilingual;monolingual code switched;propose formulation bilingual;monolingual asr;recognition monolingual;bilingual;monolingual code;joint model monolingual;monolingual cs asr;comprise bilingual speech;tasks comprise bilingual"}, "c204d40384d39c59cd7249bde4cd8615972acaac": {"ta_keywords": "machine translation robustness;robustness machine translation;translation robustness;translation robustness cover;current machine translation;machine translation mt;machine translation systems;translation systems;task machine translation;machine translation;translation systems ability;translation mt findings;robustness cover language;task improving robustness;translation mt;2020 shared task;improving robustness machine;cover language pairs;translation;shared task improving;robustness machine;cover language;improving robustness;language pairs;domain diversity;wmt 2020 shared;domain diversity non;including domain diversity;language pairs english;pairs english", "pdf_keywords": ""}, "024aa0b78e2a29d07533ee1c6e3b2e875ae45618": {"ta_keywords": "speaker depends word;estimating influences speakers;word use speaker;multiple people conversations;influences speakers conversation;people conversations;people conversations people;conversations people tend;conversations people;conversation data;speakers conversation;speakers conversation data;conversation data multiple;conversations;word distribution;word distribution propose;influences speakers;conversation;general word distribution;speakers earlier word;use speaker depends;trust experiments;speaker depends;trust experiments meeting;word use;assume word use;depends word use;estimating influences;level trust experiments;speaker", "pdf_keywords": ""}, "ffc211476f2e40e79466ffc198c919a97da3bb76": {"ta_keywords": "multi agent reinforcement;value policy learning;deep multi agent;agent reinforcement learning;policy learning guarantees;policy learning;agent reinforcement;multi agent;reinforcement learning;based multi agent;agent values allows;learning joint action;reinforcement;reinforcement learning methods;micromanagement tasks qmix;learning best strategy;agent values;learning guarantees;starcraft ii micromanagement;action value policy;agents centralised;learning guarantees consistency;qmix challenging;maximisation joint action;exploit centralised learning;agents;policies evaluate qmix;agents coordinate behaviour;reinforcement learning time;value monotonic agent", "pdf_keywords": "value policy learning;multi agent reinforcement;agent reinforcement learning;deep multi agent;policy learning;learning decentralised policies;micromanagement tasks starcraft;policy learning guarantees;agent reinforcement;starcraft ii micromanagement;policies solution qmix;micromanagement tasks qmix;multi agent rl;multi agent;multi agent methods;based multi agent;qmix allows learning;reinforcement learning;train decentralised policies;learning methods qmix;agent values allows;reinforcement learning methods;tasks starcraft;action value policy;agent action value;decentralised unit micromanagement;qmix deep multi;starcraft ii qmix;tasks starcraft ii;qmix deep"}, "a6b431df3b3d40c98d8d623cab559a9cddd41662": {"ta_keywords": "dialog neural models;schema attention model;dialog neural;shot generalizability dialog;generalizability dialog neural;attention model;schema attention;introduce schema attention;representations star corpus;dialog systems;generalizability dialog;challenge dialog research;attention model sam;star corpus;attention;schema representations star;dialog research;dialog systems unseen;dialog;task specific dialog;dialog policies training;star corpus sam;adapt dialog systems;challenge dialog;major challenge dialog;unseen tasks;implicitly memorize task;improved schema representations;specific dialog;models implicitly memorize", "pdf_keywords": "schema attention model;schema attention;natural language responses;introduce schema attention;introduces schema attention;challenge dialog research;zero shot dialog;dialog systems;shot dialog;attention model;shot dialog shikib;shot transfer dialog;dialog research;attention;major challenge dialog;language responses;dialog;dialog systems unseen;schema representations star;adapt dialog systems;challenge dialog;representations star corpus;improved schema representations;dialog policy mapping;\ufb02exibly adapt dialog;corpus introduce schema;attention model sam;response generation;2020 improved schema;unseen tasks domains"}, "62763dbdd47f144c73663b6c6b5d95caeb318e43": {"ta_keywords": "noisy matrix completion;matrix completion;matrix completion goal;approximated low rank;low rank approximated;rank matrix restrictions;matrix low rank;low rank matrix;permutation rank matrices;noisy matrix;permutation rank matrix;rank matrix paper;rank approximated low;low rank models;richer permutation rank;permutation rank model;rank matrices;low permutation rank;low rank assumptions;rank approximated;noisy completion propose;rank matrices structural;problem noisy matrix;rank models;rank matrix;noisy completion;permutation rank low;matrix entries partially;approaches underdetermined inverse;properties noisy completion", "pdf_keywords": "permutation rank decomposition;permutation rank polytope;richer permutation rank;low permutation rank;approximations permutation rank;permutation rank;permutation rank matrices;algorithms permutationrank;permutation rank model;computable algorithms permutationrank;permutation rank setting;uniqueness permutation rank;algorithms permutationrank setting;constituent permutation rank;permutationrank;rank matrices algorithms;permutationrank setting estimating;permutationrank setting;rank polytope;rank decomposition characterizing;rank polytope \ufb01rst;rank decomposition;optimal low rank;convex approximations permutation;richer permutation;rank matrices;using richer permutation;approximations permutation;singular valuethresholding algorithm;characterizing uniqueness permutation"}, "bd49e66af9755e6138967eba6aeb37d8190d2b4f": {"ta_keywords": "explanations relation extraction;relation extraction tasks;biases natural language;relation extraction;text use bert;natural language explanations;natural language;pairs spouses text;bert baseline;use bert;extracting pairs spouses;spouses text use;spouses text;language explanations relation;inductive bias married;multinli interpret explanations;use bert fine;honeymoons task extracting;bias married couples;explanation guided representations;matches bert baseline;language explanations;bias married;bert;producing explanation guided;inductive biases natural;bert fine tuned;interpret explanations;bert fine;interpret explanations respect", "pdf_keywords": "natural language explanations;models natural language;language explanations allow;language explanations ability;natural language inference;language explanations;use explanations programmatically;semantic parsing bert;explanations programmatically;relation extraction tasks;explanations ability incorporate;existing natural language;explanations programmatically evaluated;biases natural language;language explanations figure;explanations ability;language inference datasets;explanations constrained use;relation extraction;fact models trained;new explanation groups;explanations constrained;bert patterns semantic;parsing bert;natural language;ofconcept relation extraction;language inference;semantic parsing;use explanations;explanations allow"}, "5e51edfcef2b28594c63cce97c08752dfd438af0": {"ta_keywords": "structured online discriminative;discriminative models grapheme;online discriminative learning;weighted learning grapheme;online discriminative;weighted learning structured;confidence weighted learning;discriminative learning;learning grapheme phoneme;learning structured soft;discriminative learning methods;discriminative models;grapheme phoneme conversion;learning structured learning;phoneme conversion methods;phoneme conversion;structured learning;learning structured;learning grapheme;generative discriminative models;models grapheme phoneme;conventional generative discriminative;weighted learning;structured learning structured;weighted learning extends;class confidence weighted;generative discriminative;margin confidence weighted;robustness overfitting regularization;soft margin confidence", "pdf_keywords": ""}, "eebfece29b7a5c2202f1ec53ef49d6fdb75ce0ea": {"ta_keywords": "time neuronal computations;neuronal computations;temporal functions learned;neuronal computations depend;learned synaptic weights;inputs presynaptic neurons;filtered time neuronal;neurons integrated;learned online backpropagation;learned synaptic;parameters learned synaptic;neuron;neural computations;presynaptic neurons integrated;neuron recover parameters;time neuronal;simulated neural;neurons;teacher neuron;learning neural computations;neural computations ungraded;computations depend synaptic;neuronal;online backpropagation time;learning simulated neural;presynaptic neurons;synaptic weights;neurons integrated cellular;online learning neural;online backpropagation", "pdf_keywords": "neurons learn synaptic;temporal dynamics neural;neurons learn;temporal functions learned;neural computations learned;dynamics neural computations;learned online backpropagation;online backpropagation time;backpropagation time;neural computation;neurons process modifying;neurons process;neuron;neuron threshold;neuron depend;dynamics neural;neuron depend values;learning arti\ufb01cial neurons;learn synaptic connectivity;neural computations;neurons leaky resonate;temporally sparse feedback;backpropagation time relying;synaptic weights intrinsic;\ufb01re neurons learn;learn synaptic;synaptic weights;intrinsic properties neuron;arti\ufb01cial neurons process;online backpropagation"}, "e31efa7295e5d6681607ed8ef9c45300d64227aa": {"ta_keywords": "agent manipulate vote;manipulation multi winner;agent vote candidates;voting behavior multi;winner approval voting;agent vote;behavior multi winner;votes using simulations;agents use heuristics;generally manipulate vote;approval voting scenarios;multi winner approval;outcome voting way;voting scenarios;outcome voting;voting behavior;voting scenarios complete;better outcome voting;voting av agent;manipulate vote;votes choosing candidates;candidates receiving votes;votes choosing;manipulate vote obtain;heuristics situations agents;examine voting behavior;approval voting;heuristics strategize;manipulate vote achieve;winner approval", "pdf_keywords": "heuristics multi winner;voting experimental;approval voting experimental;decisions using voting;strategies voting;voting experimental study;approval voting scenarios;strategies voting truthfully;winner approval voting;voting behavior multi;behavior multi winner;alternative strategies voting;effectiveness heuristics;voting scenarios;voting scenarios complete;multi winner approval;effectiveness heuristics multi;effectiveness heuristics situations;identified optimal heuristic;voting behavior;optimal heuristic best;demonstrate effectiveness heuristics;heuristics multi;heuristics;heuristics situations;approval voting;collective decisions using;examine voting behavior;use effectiveness heuristics;optimal heuristic"}, "99c4007b1f6cb905788479db7fc886168f05e57c": {"ta_keywords": "recurrent dnn architecture;improvements dnn systems;recurrent dnns efficient;proposed recurrent dnn;proposed recurrent dnns;speech recognition asr;dnn systems achieving;dnn systems;improvements dnn;speech recognition;recurrent dnns;wer improvements dnn;dnn architecture hybrid;chime challenge track;recurrent dnn;tasks new backpropagation;networks dnns;dnn architecture;recurrent deep neural;dnns efficient effective;neural networks dnns;networks dnns robust;speaker adaptive training;deep neural networks;chime challenge data;robust automatic speech;dnns efficient;dnns robust automatic;conventional feedforward dnn;dnns robust", "pdf_keywords": ""}, "c783e1fb3ce8514f981925ee590c00884660ee4e": {"ta_keywords": "causal masked multimodal;causally masked languageimage;causally masked generative;masked generative models;masked languageimage models;context generating masked;causal masked language;masked generative;bidirectional context generating;masked multimodal;generative models trained;masked language models;masked multimodal model;masked document contexts;structured multimodal outputs;rich structured multimodal;enabling generative;context generating;models enabling generative;text image tokens;generative models;structured multimodal;masked languageimage;causal masked;generative;enabling generative modeling;languageimage models;masking cm3 causal;languageimage models large;cm3 causal masked", "pdf_keywords": "causal masked multimodal;masked generative models;causally masked generative;generative modeling bidirectional;generative models trained;models enabling generative;generative models;context control generative;masked multimodal model;masked generative;generative modeling;masked enabling generative;enabling generative;enabling generative modeling;multimodal model;modal multimodal tasks;causal masked language;masked language models;generative;multimodal model internet;masked multimodal;multimodal tasks;multimodal;trained large corpus;large corpus structured;modal multimodal;hybrid causal masked;cm3 causal masked;modeling bidirectional context;generative mask in\ufb01lling"}, "80b92f762e116d4513da27792822897ca3915247": {"ta_keywords": "privacy preserving graph;privacy preserving 010;strong privacy guar021;privacy preserving;differential privacy 009;private gradient based;differential privacy;guages differential privacy;privacy guar021 antees;strong privacy;privacy guar021;privacy leaks 007;privacy 009;improves baseline privacy;015 optimizers nlp;privacy bounds 019;baseline privacy bounds;private gradient;privacy leaks;differentially private gradient;privacy bounds;privacy;provides strong privacy;edges prone privacy;differentially private;baseline privacy;privacy 009 dp;networks text classification;prone privacy leaks;013 differentially private", "pdf_keywords": "privacy preserving graph;private stochastic gradient;privacy preserving models;private gradient based;privacy preserving;private gradient;classi\ufb01cation citation networks;differentially private stochastic;strong privacy;differentially private gradient;strong privacy guarantees;adapting differentially private;citation networks;classi\ufb01cation social networks;achieves strong privacy;privacy guarantees performance;citation networks reddit;convolutional networks text;networks text classi\ufb01cation;private stochastic;strict privacy guarantees;privacy;privacy guarantees;graph convolutional networks;calling privacy preserving;stochastic gradient descent;differentially private;reddit datasets;preserving graph convolutional;\ufb01ve nlp datasets"}, "3d5b51fc30ffacdcc8424618555accb36756ccc9": {"ta_keywords": "algorithm stochastic points;free algorithm stochastic;randomized derivative free;unconstrained minimization;random search direction;parameter random search;randomized derivative;algorithm stochastic;novel randomized derivative;consider unconstrained minimization;unconstrained minimization problem;stepsize parameter random;stochastic points stp;free algorithm;search direction iteration;analyze iteration complexity;stochastic points;derivative free algorithm;direction iteration stp;objective function points;iteration complexity;stepsize selection schemes;minimization problem smooth;novel randomized;random search;minimization;points stp method;direction iteration;iteration stp;randomized", "pdf_keywords": "approach nonconvex functions;nonconvex functions;nonconvex functions studied;derivative free algorithm;unconstrained minimization;algorithm stochastic points;randomized derivative free;performances approach nonconvex;free algorithm stochastic;approach nonconvex;novel randomized derivative;randomized derivative;unconstrained minimization problem;minimization problem smooth;consider unconstrained minimization;free algorithm;algorithm stochastic;numerical performances approach;stochastic points stp;minimization;stochastic points;method coordinate search;coordinate search method;points stp method;function evaluations performance;numerical performances;theoretical numerical performances;stepsize selection schemes;analyze iteration complexity;nonconvex"}, "845aad7b99f48526fe003c775836091521624471": {"ta_keywords": "lexicography projects wiktionary;predicting words week;predicting word week;predicting word;predicting words;week russian wiktionary;collaborative lexicography;collaborative lexicography projects;words week russian;phenomena collaborative lexicography;projects wiktionary;articles russian wiktionary;lexicography projects;crowdsourcing phenomena collaborative;crowdsourcing phenomena;week articles russian;attention predicting words;focuses predicting word;word week articles;word week;crowdsourcing;lexicography;wikipedia expert;wiktionary;fuzzy nature crowdsourcing;words week;classification task words;projects wiktionary strong;russian wiktionary;wikipedia expert built", "pdf_keywords": ""}, "a3cd9c4f8fa52c5e23885c2f82931d7e0f7d4b45": {"ta_keywords": "2d barcode;symbology 2d barcode;barcode symbology 2d;carried 2d barcode;dispensing data drug;2d barcode capability;barcode symbols database;employing dimensional barcode;using dimensional barcode;limits 2d barcode;2d barcode hold;barcode symbology;dimensional barcode;dimensional barcode symbology;barcode followings 2d;dimensional barcode symbols;followings 2d barcode;using linear barcode;linear barcode;barcode capability;barcode symbols;barcode;2d barcode high;barcode capability superior;linear barcode followings;barcode followings;barcode high density;barcode hold;barcode hold necessary;data drug", "pdf_keywords": ""}, "697e6eecb0e77ba56c685bb99b221d959739d13b": {"ta_keywords": "geo tagging images;automatic geo tagging;geo tagging;automatic image geotagging;image geotagging;geo tagging consider;image geotagging implemented;geotagging;geotagging implemented;geotagging implemented approach;task geo tagging;tags automatic geo;tagging images determining;tagging images;tags lda based;flickr comparing various;tagging;location models providing;location models;textual tags lda;user tag model;latent dirichlet allocation;tags lda;providing location based;dirichlet allocation lda;flickr comparing;model user tags;query photo city;images flickr comparing;tag model", "pdf_keywords": ""}, "e42b3ead5ff04adfa95c87e0180561f0c3ba4af4": {"ta_keywords": "learning policies control;constraints policy network;learned control policies;reinforcement learning policies;learning policies;learns convex combination;learned policy output;learned control;learns convex;reinforcement learning challenging;algorithm learns convex;exploration learned control;state action constraints;policy network;finding reinforcement learning;learned policy;policy network architecture;vertex networks;safety constraints policy;actions constrained;action constraints;guarantees safety exploration;constraints policy;domains reinforcement learning;reinforcement learning;policies control systems;variables actions constrained;safety exploration learned;control policies;termed vertex networks", "pdf_keywords": "vertex policy network;constraints policy network;novel vertex policy;vertex policy;vertex networks;proposed vertex policy;policy network architecture;policy network;learned control policies;termed vertex networks;constraints network;geometry constraints network;policy network framework;vertex networks vns;constraints network architecture;vertex calculation neural;safety region vertex;safe layer leveraging;safety constraints policy;learns convex combination;policy network encodes;guarantees safety exploration;learns convex;learned control;incorporating safety constraints;safety constraints;safety exploration learned;networks vns guarantees;networks;control policies"}, "e54a4e49917eb3da18c2f239be70a68fbd3274c3": {"ta_keywords": "peer review packages;documentation peer review;analysis peer reviews;review packages;contributed packages community;organize peer review;documentation peer;peer review;packages reviewed;peer reviews;peer reviews ii;reviews ii documentation;review packages concerted;reviewers tend report;packages community;editors ropensci reviewers;contributed packages;documentation debt prevalent;package authors editors;user contributed packages;reported authors package;analysis peer;packages concerted effort;reviewers tend;package method collected;package authors;td documentation peer;packages reviewed approved;reviewers;ropensci reviewers", "pdf_keywords": "oo scienti\ufb01c software;proposals oo programming;technical debt peer;oo programming;scienti\ufb01c software;debt peer review;oo programming objective;scienti\ufb01c software multi;technical debt td;investigate td documentation;software multi paradigm;debt td metaphor;non object oriented;programming language;technical debt;context technical debt;object oriented;td documentation peer;taxonomy td proposed;documentation packages ropensci;data science;paradigm programming language;object oriented oo;multi paradigm programming;documentation peer;debt peer;programming;documentation peer review;peer review packages;debt td"}, "59d225fcb08ce66935e0285a9936ee158c4fdb97": {"ta_keywords": "answers entailment trees;explaining answers entailment;entailment trees explaining;textual question answering;answers entailment;entailment trees work;entailment trees;approach generate explanations;explanations form entailment;trees explaining answers;multistep entailment trees;multipremise entailment steps;question answering;entailments baselines;generate explanations;tree multipremise entailment;entailment steps;entailmentbank dataset;entailments baselines offering;skill created entailmentbank;created entailmentbank dataset;entailmentbank;created entailmentbank;form entailment trees;entailment steps facts;entailment trees tree;entailments;question answering qa;generate explanations form;multistep entailments baselines", "pdf_keywords": "entailment tree explanations;entailment trees entailmentbank;entailment based explanation;authoring entailment trees;questions trained entailmentbank;entailment trees presented;trees entailmentbank;trees created entailmentbank;readable entailment tree;automatically generating entailment;human readable entailment;trained entailmentbank;entailment trees qa;generating entailment tree;entailment tree;entailment trees;entailment trees created;entailment based;trained entailmentbank extend;trees entailmentbank \ufb01rst;multistep entailment trees;explanations multistep entailment;support entailment based;authoring entailment;skill created entailmentbank;sentence entailment ways;entailmentbank \ufb01rst dataset;generating entailment;entailmentbank;readable entailment"}, "deedb9b61a01d686b28e6034770fccc142e77fab": {"ta_keywords": "nlp tasks predictors;nlp experiment;nlp model perform;nlp tasks;score nlp experiment;plausible judgments nlp;predictions unseen languages;experimenting different nlp;nlp research computationally;nlp experiment given;different nlp tasks;evaluation score nlp;judgments nlp model;nlp research;nlp model;processing nlp research;judgments nlp;models predict evaluation;nlp;language processing nlp;processing nlp;domains natural language;natural language processing;different nlp;produce meaningful predictions;score nlp;predict evaluation;meaningful predictions unseen;plausible predictions experimental;predictions experimental", "pdf_keywords": "characterizing nlp experiment;effectiveness robustness nlperf;nlp experiment;characterizing nlp;score nlp experiment;nlp experiment task;processing nlp extraordinarily;evaluation score nlp;process characterizing nlp;nlp extraordinarily vast;processing nlp;language processing nlp;robustness nlperf comparing;nlp experiment given;robustness nlperf;nlp;nlperf comparing;natural language processing;nlperf;nlp extraordinarily;models predict evaluation;score nlp;natural language;nlperf comparing multiple;predict evaluation;language processing;adequately predict performance;predict performance new;predict performance;models predict"}, "4cfbd97a5b42695697f70a9f28ee29711f6ca433": {"ta_keywords": "features adversarial attacked;features adversarial;similar features adversarial;environment adversarial;deep learning driven;adversarial attacks unintentional;aware novelty detection;changes environment adversarial;adversarial attacked images;adversarial;novelty detection visual;saliency map predicted;environment adversarial attacks;saliency provide learning;real world driving;detect situations trained;deep learning autonomous;network saliency;trustworthy prediction demonstrate;adversarial attacks;trustworthy prediction;adversarial attacked;datasets driving scenarios;make trustworthy prediction;learning driven safety;novelty detection;visual based deep;learning autonomous systems;learning autonomous;task aware novelty", "pdf_keywords": ""}, "10e88416035a8a3cbef0e65f8967df650abd0a00": {"ta_keywords": "disambiguation unsupervised word;word sense disambiguation;sense disambiguation unsupervised;unsupervised word sense;semantic resources russian;sense disambiguation resourced;sense disambiguation;disambiguation unsupervised;disambiguation resourced languages;word sense corresponding;semantic similarity;resources russian sparse;lexical semantic resources;lexical semantic;different lexical semantic;unsupervised word;russian sparse;disambiguation;semantic similarity given;disambiguation resourced;watasense unsupervised word;sense target word;resources russian;word sense;similar word sense;lexical;different lexical;russian sparse mode;evaluation different lexical;target word architecture", "pdf_keywords": "word sense disambiguation;sense disambiguation resourced;disambiguation parameterized word;sense disambiguation parameterized;sense disambiguation;disambiguation resourced languages;unsupervised word sense;disambiguation resourced;parameterized word sense;disambiguation;disambiguation parameterized;sense disambiguation conclusion;word sense inventory;semantic similarity;unsupervised word;watasense unsupervised word;source unsupervised word;semantic;word sense;sense target word;disambiguation conclusion paper;semantic similarity given;parameterized word;disambiguation conclusion;sense input word;resourced languages;resourced languages arxiv;respect semantic similarity;sense inventories russian;different sense inventories"}, "4fffa5245d3972077c83614c2a08a47cb578631e": {"ta_keywords": "self supervised speech;supervised speech representation;supervised speech;supervised approaches speech;speech representation learning;approaches speech representation;speech representation;bert like prediction;prediction hidden units;acoustic language model;hidden unit bert;self supervised;self supervised approaches;target labels bert;learning masked prediction;approach self supervised;combined acoustic language;representation learning masked;labels bert like;units input utterance;labels bert;unit bert;learning masked;prediction loss masked;acoustic language;lexicon input sound;learn combined acoustic;input utterance lexicon;utterance lexicon input;representation learning challenged", "pdf_keywords": "self supervised speech;prediction loss speech;supervised speech representation;speech features predict;speech representation learning;supervised speech;loss speech sequences;speech representation;hidden unit bert;approaches speech representation;bert like prediction;prediction hidden units;supervised approaches speech;speech sequences represent;speech sequences;speech features;representation learning masked;training concretely bert;unit bert;continuous speech features;learning masked prediction;masked continuous speech;loss speech;concretely bert model;bert model;learning masked;target labels bert;labels bert like;bert like training;labels bert"}, "520e82c0f35a14ecf78b93de3673bb8b2a3212fc": {"ta_keywords": "timeline extraction goal;timeline extraction;timeline extraction using;timeline timeline extraction;timeline task distantly;2015 timeline task;entity involved timeline;timelines documents;timeline task;heuristically aligning timelines;task distantly supervised;involved timeline timeline;timelines;involved timeline;produce timeline timeline;temporal expressions entities;aligning timelines documents;timeline;propose distantly supervised;timeline timeline;distantly supervised approach;using distant supervision;produce timeline;distantly supervised;aligning timelines;distant supervision joint;semeval 2015 timeline;supervision joint inference;2015 timeline;events temporal expressions", "pdf_keywords": ""}, "0115d5d37f7cdc7b8d2147c0bb348e714432e899": {"ta_keywords": "channel speech enhancement;speech enhancement se;use speech enhancement;speech enhancement;identification noisy speech;speech video domain;single channel speech;speech enhancement step;speech video;noisy speech video;audio preprocessing language;telephone speech noisy;channel speech;models telephone speech;unaltered noisy speech;audio domain adapting;noisy speech systems;speech noisy speech;improvement language identification;speech systems domain;telephone audio;speech noisy;speech systems;noisy speech;language identification noisy;telephone speech;language recognition evaluation;performance telephone audio;language recognition;audio preprocessing", "pdf_keywords": ""}, "cc2c3df6b09166c54e670d347bfe26dae236ac73": {"ta_keywords": "automatic knowledge base;knowledge base construction;multiview semi supervised;knowledge base;semi supervised learning;automatic knowledge;semi supervised;supervised learning incomplete;view exploratory learning;multi view exploratory;abstract problem multiview;learning incomplete;exploratory learning akbc;supervised learning;learning incomplete class;exploratory learning;single abstract problem;argue automatic knowledge;hierarchy multi view;supervised;multi view;instances single abstract;multiview semi;problem multiview semi;single abstract;separately viewed instances;learning akbc;knowledge;view exploratory;incomplete class hierarchy", "pdf_keywords": ""}, "f7979c6690562c5f8bf700e3fd184c4d1df0a54c": {"ta_keywords": "lingual entity linking;cross lingual entity;lexical resources bridge;lingual entity;transfer cross lingual;bilingual lexical resources;languages cross lingual;improves entity linking;entity linking accuracy;target languages resources;languages transfer;resource languages transfer;heavily bilingual lexical;source target languages;resource languages cross;transferring languages;target language;bilingual lexical;low resource languages;transferring languages use;entity linking;cross lingual;different target language;target languages;languages transfer total;mention source language;languages resources scarce;languages cross;target language experiments;lingual", "pdf_keywords": "neural entity linking;improves entity linking;entity linking models;entity linking accuracy;transfer neural entity;entity linking;introduction entity linking;neural model linking;entity linking el;linking entities;based entity linking;entity linking leverages;neural entity;model linking entities;bilingual lexicon hrl;pivot language train;entities hrl english;linking entities hrl;wikipedia freebase downstream;level neural entity;models using bilingual;low resource language;low resource languages;transfer language lowresource;document understanding entity;bilingual lexicon;language zero shot;small lexical data;base wikipedia freebase;linking models"}, "3b0a1a10d8f7496226635c5c3b8475fcd10d890d": {"ta_keywords": "scheduling redundant requests;performance redundant requests;latency serving requests;redundancy requests optimal;having redundancy requests;redundancy requests;redundant requests achieves;sending redundant requests;redundant requests;systems latency serving;optimal redundant requesting;redundant requests request;latency performance redundant;latency serving;redundant requests primary;scheduling redundant;maximally scheduling redundant;redundant requests help;data systems latency;redundant requests diverse;serving requests;redundant requesting;distributed storage;serving requests potentially;faster execution request;distributed storage systems;designing optimal redundant;reduced sending redundant;requests achieves optimal;requests way distributed", "pdf_keywords": ""}, "f826381aea632791b6007e427a9587c11b239b6a": {"ta_keywords": "dialog policy learning;efficient exploration dialog;policy learning deep;exploration dialog policy;policy learning;greedy boltzmann exploration;exploration dialog;dialogue tasks;task oriented dialogue;learning deep bbq;based thompson sampling;harvest dialogue tasks;boltzmann exploration;dialogue tasks make;replay buffer experiences;rewards sparse action;dialogue systems;actions efficient exploration;thompson sampling;networks replay;bbq networks replay;dialogue systems primary;replay buffer spiking;deep bbq networks;spiking replay buffer;spiking replay;spiking rewards sparse;tasks make learning;greedy exploration;greedy exploration inefficient", "pdf_keywords": ""}, "f07a326e21395f025a87b2d77cac7e8ca502f002": {"ta_keywords": "entailment medical domain;task textual inference;question entailment medical;entailment medical;textual inference;language understanding models;knowledge data augmentation;domain knowledge;incorporating domain knowledge;domain knowledge data;question entailment;domains medicine work;entailment;acl bionlp 2019;textual inference question;domains medicine;medical domain based;medical domain;specialized domains medicine;inference question entailment;task textual;specialized medical domain;language understanding;shared task textual;acl bionlp;textual;knowledge data;models specialized medical;domain based prior;inference", "pdf_keywords": "entailment medical domain;entailment using contextualized;natural language inference;language inference dataset;task textual inference;entailment tailored medical;question entailment tailored;question entailment using;approach textual inference;inference dataset medical;textual inference;language understanding models;entailment using;entailment medical;question entailment medical;language inference;2019 textual inference;natural language understanding;entailment tailored;question entailment;language understanding model;entailment;dataset medical domain;domain knowledge specialized;learning approach textual;mednli natural language;textual inference question;domain knowledge data;knowledge data augmentation;language understanding"}, "d95aafa571e9cb6795cc28ecf257ead123664e3c": {"ta_keywords": "mrf segmentation energies;clustering regularization models;clustering regularization;standard pairwise clustering;segmentation energies;mrf segmentation;segmentation energies benefit;using mrf segmentation;common pairwise clustering;common regularization energies;regularization models;pairwise clustering;regularization models widely;pairwise clustering criteria;regularization energies;regularization;standard clustering;normalized cut;common regularization;regularization functionals existing;spectral bound formulations;regularization functionals;mrf constraints;combining common regularization;like normalized cut;segmentation;standard clustering applications;corresponding optimization;corresponding optimization propose;kernel spectral", "pdf_keywords": "image segmentation restoration;segmentation restoration;new segmentation model;label image segmentation;image segmentation clustering;image segmentation;vision image segmentation;new segmentation;segmentation model;segmentation;segmentation clustering ef\ufb01cient;biomedical image analysis;propose new segmentation;segmentation restoration registration;segmentation model combining;vision biomedical image;segmentation results;potent segmentation results;computer vision biomedical;segmentation clustering;potent segmentation;regularization energies;segmentation results basic;common regularization energies;object recognition photo;biomedical image;image analysis arxiv;image analysis;ef\ufb01cient bound optimization;editing potent segmentation"}, "250e4a8f5155f1f9f60b2dee3e8da8024338db4d": {"ta_keywords": "dirichlet distribution sentiment;distribution sentiment labels;sentiment labels;document level sentiment;sentiment target movie;sentiment analysis;level sentiment analysis;movie review datasets;sentiment analysis rely;sentiment target;distribution sentiment;sentiment labels use;context sentiment target;entropy classifier;level sentiment;global context sentiment;maximum entropy classifier;dirichlet;context sentiment;review datasets outperforms;sentiment;classification measure;consistency dirichlet distribution;entropy classifier gain;classification;classification measure absolute;english movie review;review datasets;consistency dirichlet;increases classification measure", "pdf_keywords": ""}, "8eda71ecad19cdef6092e76276eba48312ec7063": {"ta_keywords": "tokens dense retrieval;topic representations specifically;topic representations;level topic representations;dense retrieval;dense retrieval dr;document query encoders;discrete representations analyze;retrieval;models regard representations;based discrete representations;representations analyze;representations specifically;discrete representations;specifically discretize embeddings;input tokens dense;representations analyze attribution;retrieval little known;representations specifically discretize;query encoders;embeddings;retrieval little;representations learned dr;representations learned;discretize embeddings;embeddings output;tokens dense;representations;embeddings output document;attribution input tokens", "pdf_keywords": "embeddings analyze dr;repmot discretize embeddings;embeddings analyze;encoded dr models;specifically discretize embeddings;discretize embeddings analyze;models based repmot;dr models mixture;embeddings;high level representations;dr models based;models mixture topics;analyze dr models;repmot attribution techniques;embeddings output document;discretize embeddings;analysis dr models;representations encoded dr;based repmot attribution;embeddings output;level representations analysis;document query encoders;dr models;investigate dr models;level representations;encoders regard representations;discretize embeddings output;representations analysis results;attribution techniques;representations analysis"}, "e0c66240239263f16159eef166a391d3939ae2d5": {"ta_keywords": "reading comprehension;does reading comprehension;questions passages predict;reading comprehension require;question passage models;squad cbt cnn;predict corresponding answers;comprehension require critical;reading;reading does reading;comprehension;benchmarks remain unanswered;cbt cnn;does reading;passages predict;comprehension require;unanswered presumably model;reading does;finding question passage;cbt cnn did;passages predict corresponding;cnn;difficulty popular benchmarks;popular benchmarks despite;needed accurate prediction;popular benchmarks remain;leaderboard;investigation popular benchmarks;popular benchmarks;accurate prediction", "pdf_keywords": "reading comprehension examples;reading comprehension;comprehension examples consist;questions passages predict;benchmarks remain unanswered;comprehension examples;question passage information;address reading comprehension;benchmarks divyansh kaushik;popular benchmarks remain;predict corresponding answers;investigation popular benchmarks;dif\ufb01culty popular benchmarks;popular benchmarks;popular benchmarks divyansh;tasks analyzing performance;comprehension;benchmarks;benchmarks divyansh;answer tuples popular;tasks analyzing;benchmarks remain;reading;kaushik language technologies;corresponding answers;models proposed tasks;examples consist question;questions passages;proposed tasks analyzing;language technologies"}, "3105b5863d4597058bf51aeda40db53394075784": {"ta_keywords": "bribery manipulation tournaments;complexity bribery manipulation;complexity bribery;bribery manipulation;manipulation tournaments uncertain;tournaments uncertain information;manipulation tournaments;bribery;tournaments uncertain;tournaments;complexity;uncertain information;manipulation;information;uncertain", "pdf_keywords": ""}, "446efa0bcf3528b51332a12495cb56784dd8bad3": {"ta_keywords": "graphs transferable representations;embeddings graphs trained;transferable representations learned;representations learned graphs;deep transfer;embeddings graphs;learned relational graphs;deep transfer learning;relational graphs transferable;transferable representations;transferred different embeddings;embeddings task;different embeddings graphs;embeddings language pretrained;vectors task transferable;learned graphs;graphs trained including;graphs trained;transfer learning;modern deep transfer;latent relational graphs;graphs transferable;learned graphs generic;embeddings;representations learned;learned relational;embedding free;tasks word embeddings;embeddings task specific;relational graphs capture", "pdf_keywords": "relational graph learning;latent graph learning;graph learning;graph learning propose;unsupervised relational graph;graph learning called;unsupervised latent graph;graph learning achieve;graph learning orthogonal;latent graphs transfer;graph representations;hierarchical graph representations;graphs transfer network;graphs features hierarchical;extract graph structures;decoupling graphs features;graphs features;latent relational graph;graphs transfer;graph structures downstream;prediction unsupervised relational;features hierarchical graph;network extract graph;output latent graphs;generic graphs decoupling;unsupervised relational;latent graphs;graph representations sparsity;graph structures;graphs decoupling graphs"}, "549dae68d04eefad88885c64a4d946205e524b79": {"ta_keywords": "document clustering sentiment;document embedding;document classification;vectors document embedding;document clustering;document representations;word embeddings;geometry word embeddings;based document representations;specifically document clustering;clustering sentiment;clustering sentiment classification;document embedding stable;help document classification;document representations traditional;embeddings;topology text data;word embeddings help;topology based document;sentiment classification;embedding;sentiment classification does;representations traditional nlp;basis topic sentiment;text data analysis;traditional nlp tasks;traditional nlp;nlp tasks specifically;embeddings help document;nlp tasks", "pdf_keywords": "document clustering sentiment;sentiment classi\ufb01cation persistent;clustering sentiment;clustering sentiment classi\ufb01cation;embeddings text classi\ufb01cation;word embeddings;document representations;sentiment classi\ufb01cation tasks;document embedding;geometry word embeddings;embeddings;vectors document embedding;document embedding stable;based document representations;embeddings text;word embeddings help;document representations traditional;representations traditional nlp;sentiment classi\ufb01cation;document clustering;speci\ufb01cally document clustering;embedding;utility embeddings text;text representation;embedding model mapping;contribute document clustering;persistence diagrams text;embedding model;utility embeddings;topology based document"}, "35c710f5fdacc71a675832f6beaa2dbfe301d0ce": {"ta_keywords": "example based dialog;dialog systems;dialog systems creating;based dialog;alternatives random selection;construction dialog systems;based dialog popular;dialog;domains complexity utterances;random selection domains;sampling strategies selecting;strategies selecting inputs;uncertainty sampling strategies;propose uncertainty sampling;random selection;dialog popular;dialog popular option;responses selected inputs;option construction dialog;alternatives random;sampling strategies;complexity utterances;strategies random selection;selection strategy simulation;complexity utterances low;random selection strategy;utterances;uncertainty sampling;human annotators create;selection domains complexity", "pdf_keywords": ""}, "25efc17ba82ba4af29f2e03868de74e1ea66d025": {"ta_keywords": "multilingual text video;multilingual multimodal embeddings;multimodal embeddings multilingual;multilingual instructional video;embeddings multilingual multimodal;learns contextual multilingual;multilingual multimodal pre;contextual multilingual multimodal;introduce multilingual multimodal;multilingual multimodal;multimodal pre training;embeddings multilingual;multilingual annotations;multilingual text image;furthermore multilingual annotations;multilingual annotations available;text video search;multimodal embeddings;learns contextual;new multilingual instructional;multilingual instructional;improves video search;instructional video dataset;specifically focus multilingual;contextual multilingual;vision language models;multimodal pre;vatex multilingual text;multilingual text;video search non", "pdf_keywords": "multilingual multimodal pretraining;multimodal pre training;multilingual instructional video;multilingual text video;multilingual multimodal embeddings;multilingual multimodal pre;multilingual multimodal representations;contextual multilingual multimodal;multilingual text tovideo;contextualized multilingual multimodal;learns contextualized multilingual;multilingual multimodal;multimodal pretraining;introduce multilingual multimodal;presented multilingual multimodal;learning contextual multilingual;instructional video dataset;multimodal embeddings;multimodal pre;multimodal representations introduce;multilingual instructional;new multilingual instructional;multimodal representations;learns contextualized;multimodal embeddings conclusion;contextual multilingual;multihowto100m pre training;vision language models;contextualized multilingual;text video model"}, "e60b88313fad52c1ef8dd02b482785651d09ad66": {"ta_keywords": "size depth networks;depth networks uniform;depth networks poly;depth separation neural;depth neural networks;size depth neural;depth networks;depth neural;separation neural networks;neural networks exponentially;mathbb depth separation;depth depth networks;networks poly bounded;depth separation;bounded weights approximate;separation depth;networks exponentially bounded;weights establishes separation;separation depth depth;networks uniform;poly bounded weights;establishes separation depth;weights approximate approximated;networks exponentially;networks uniform distribution;separation neural;poly size depth;functions form approximated;weights approximate;bounded weights", "pdf_keywords": "sized depth networks;versus depth networks;depth networks approximated;depth network;depth networks;size depth network;depth networks ask;power depth networks;depth networks focus;expressive power depth;expressive power neural;separation deep networks;networks approximated;networks approximated exponential;exponential size depth;separation deep;deep networks 11;particular separation deep;power neural networks;neural networks studied;sized depth;depth;deep networks;depth versus depth;poly sized depth;neural networks;size depth;power neural;power depth;10 expressive power"}, "a1c5af2a531c64f1c06e806d7986cd878ec3c33a": {"ta_keywords": "explanations data ml;score explanations data;explanations data;model score explanations;generate model explanations;model fraud analysts;explainers improve accuracy;new explainable ai;explainable ai;ml model fraud;model explanations;information assessment explanations;explainable ai xai;score explanations conclusion;model explanations having;assessment explanations chosen;explainers improve;score explanations;explanations case conducted;assessment explanations;model fraud;explanations having specific;explanations case;interpretability study;explanations chosen;fraud analysts research;human interpretability study;human interpretability;robustness human interpretability;explanations conclusion highlights", "pdf_keywords": "model fraud analysts;encompassing fraud analysts;fraud analysts ml;hoc explainability methods;score explanations data;task encompassing fraud;explanations data ml;explanations data;fraud analysts stages;encompassing fraud;compare explanations utility;fraud detection task;explanations utility;assess compare explanations;ml model fraud;explainability methods;explainers improve accuracy;fraud analysts;score explanations propose;fraud detection;explanations utility different;fraud analysts following;explainability methods lime;model fraud;provided fraud analysts;explanations propose xai;post hoc explainability;real world fraud;hoc explanation methods;model score explanations"}, "4fa4e39ade763085a75146392b997b7d4da49725": {"ta_keywords": "weakly supervised text;grained contextualized corpus;contextualized weak supervision;text classification contextualized;create contextualized corpus;fine grained contextualized;grained contextualized;contextualized corpus;classification contextualized weak;weak supervision text;text classification weakly;classification contextualized;word create contextualized;supervision text classification;supervised text;contextualized corpus utilized;contextualized corpus paper;providing contextualized weak;contextualized;contextualized weak;using contextualized weak;providing contextualized;contextualized representations word;seed word information;create contextualized;using contextualized;word occurrences seed;supervised text classification;seed words iterative;weakly supervised", "pdf_keywords": "contextualized seed words;contextualized weak supervision;keywords contextualized;indicative keywords contextualized;keywords contextualized corpus;weak supervision text;create contextualized corpus;contextualized corpus;word create contextualized;supervision text classi\ufb01cation;corpus leading contextualized;contextualized corpus design;contextualized seed;interpretations based contextualized;leading contextualized seed;contextualized corpus leading;contextualized representations word;seed word information;userprovided seed words;contextualized;label indicative keywords;providing contextualized weak;word occurrences seed;providing contextualized;words discriminative highly;introduce contextualized weak;create contextualized;contextualized weak;supervision text;supervision train text"}, "462e36e5e296900c80dcd36173340f9c29e36c80": {"ta_keywords": "sharedstate triphone hmms;sst hmm based;phonetic decision tree;triphone hmms sst;triphone hmms;design sst hmms;sst hmm using;hmm based variational;sst hmms practical;hmms higher recognition;hmms practical bayesian;structure sst hmm;appropriate phonetic decision;method appropriate phonetic;derived sst hmm;phonetic decision;independent word recognition;hmm using criterion;sst hmms;hmms sst hmms;hmm based;constructing sharedstate triphone;speaker independent;word recognition;sst hmms higher;word recognition order;phonetic;based variational bayesian;experiments speaker independent;triphone", "pdf_keywords": ""}, "f05741b65a1d644f2fae4c654dae315a7451ee85": {"ta_keywords": "text network exploration;exploration text network;heterogeneous topic web;text networks hyperlinked;heterogeneous web topics;text networks;investigate text network;citation networks;proliferation text networks;web topics;text network associating;network exploration text;topic web quantified;topic web;exhibit heterogeneous topic;academic citation networks;relationships heterogeneous topic;text links proposed;heterogeneous topic;topicatlas exhibit heterogeneous;text network;networks hyperlinked webpages;new text network;documents represented edges;exploration heterogeneous web;text network text;web topics allows;text links;text network refers;named topicatlas", "pdf_keywords": "text network exploration;heterogeneous topic web;heterogeneous web topics;web topics;navigation heterogeneous topic;investigate text network;topic web;topic web assist;topic web aan;web topics allows;construct heterogeneous topic;topic web quanti\ufb01ed;topicatlas prototype text;text network associating;exhibit heterogeneous topic;prototype text network;heterogeneous topic;develop topicatlas;text links proposed;topicatlas prototype;topicatlas exhibit heterogeneous;text network;topic web demonstrate;topicatlas prototype demo;users exploration;relationships heterogeneous topic;task text network;topicatlas;develop topicatlas prototype;network exploration"}, "fe54832083f65eade8e2847627d330a24df22488": {"ta_keywords": "multi channel eeg;channel eeg observed;channel eeg;eeg observed signal;channel eeg signal;eeg signal;removing noise event;eeg observed;eeg signal according;noise removal single;noise removal;noise event related;noise event;removing noise;background noise removal;eeg;separated multiple signals;covariance matrices frequency;potentials erps recorded;multi channel wiener;observed signal separated;grouped covariance;related potentials erps;erps recorded multi;signal separated multiple;event related potentials;erps recorded;grouped covariance matrices;channel wiener filter;signal separated", "pdf_keywords": ""}, "d7a7ebd1565c3795bc2bcdec4334d42a65ad17c5": {"ta_keywords": "text generation pretrained;plms text generation;models text generation;text generation;text generation survey;text generation resurgence;neural generation models;generated text preliminaries;generated text;neural generation;language models plms;text generation paper;strategies text generation;generation pretrained language;plms text;pretrained language models;language models text;architectures plms text;topic plms text;field neural generation;models text;models plms summarize;generation models;language models;plms summarize;text;text preliminaries;achieved topic plms;properties generated text;generation models especially", "pdf_keywords": "plms text generation;text generation survey;text generation;provide text generation;text generation tasks;different text generation;text generation researchers;used text generation;strategies text generation;text generation section;text generation summarize;plms text;generation summarize;architectures plms text;plms used text;generation summarize important;topic plms text;generation tasks;generation researchers synthesis;generation tasks sec;generation survey aims;generation section preliminaries;achieved topic plms;generation survey;tuning strategies text;generation section;text;provide text;generation;generation researchers"}, "a2221b03211408ac2db0559b9a54c1d72b5f560c": {"ta_keywords": "self supervised music;music acoustic representation;genre classification auto;supervised music acoustic;music acoustic data;supervised music;acoustic representation learning;music apply masked;unlabeled music acoustic;unlabeled music;genre classification;self supervised learning;bert musicoder;new self supervised;music genre classification;massive unlabeled music;acoustic representation;self supervised;bert musicoder builds;training continuous acoustic;models music;acoustic data;success bert musicoder;approach named musicoder;acoustic frame domain;data effectiveness musicoder;acoustic data finetune;music acoustic;transformers results musicoder;art models music", "pdf_keywords": "predict music genres;statistics predict music;predict music;genres musicoder model;musicoder model compared;musicoder model;model music genre;music genre classi\ufb01cation;model music;music genres musicoder;jamendo musicoder outperforms;theme recognition music;musicoder outperforms;dataset music;recognition music;genres musicoder;dataset music autotagging;music genres;gtzan dataset music;recognition music using;music features;using jamendo musicoder;music using jamendo;musicoder universal music;music autotagging;genre classi\ufb01cation gtzan;music genre;theart model music;musicoder outperforms state;level music features"}, "74e9053d6f44f4507bd40bbea999ee65f0cbefb2": {"ta_keywords": "adversarial examples confidence;nlp word importance;interpreting neural models;neural model predictions;way interpret neural;interpretation methods nlp;interpret neural model;adversarial examples;interpret neural;words appear nonsensical;heatmap visualization words;model predictions highlight;predictions high confidence;interpreting neural;models make predictions;word importance determined;models remaining words;predictions highlight important;determined important interpretation;nlp word;adversarial;nlp;neural models trained;model predictions;methods nlp word;word importance;difficulties interpreting neural;prediction label models;predictions highlight;neural models", "pdf_keywords": "neural models explain;visual question answering;way interpret neural;interpret neural;interpret neural model;interpretation pathologies neural;neural model predictions;neural models overcon\ufb01dence;overcon\ufb01dence neural models;models make interpretations;neural models trained;neural models make;reading comprehension;neural models;2016 reading comprehension;reading comprehension snli;question answering explain;question answering;models trained;neural;make interpretations dif\ufb01cult;model predictions highlight;interpretations dif\ufb01cult;fundamental dif\ufb01culties interpretation;comprehension;make interpretations;issues neural models;predictions highlight important;model predictions;neural model"}, "01a21d74fb7414404851872f23cdca42243ab6a8": {"ta_keywords": "trained reid model;transfer learning;used transfer learning;progressive transfer learning;dukemtmc reid datasets;reid datasets;pre trained feature;batch related convolutional;reid datasets based;trained feature;trained feature extraction;transfer learning approach;transfer learning ptl;person identification reid;mini batch training;trained reid;batch training;pre trained reid;reid model performance;batch training challenging;improve reid model;instead training model;identification reid applications;approach person identification;identification reid;person identification;model progressive transfer;cells pre trained;training model;reid model progressive", "pdf_keywords": "batch related convolutional;batchrelated convolutional cell;propose novel cnn;progressive transfer learning;novel cnn;transfer learning;used transfer learning;novel cnn building;pre trained feature;transfer learning ptl;batchrelated convolutional;cnn;trained feature;transfer learning approach;trained feature extraction;convolutional cell;trained reid model;related convolutional cell;called batchrelated convolutional;cells pre trained;cnn building block;convolutional cell bconv;cnn building;transfer learning zhengxu;jointly optimizing bconv;dukemtmc reid datasets;instead training model;person identi\ufb01cation reid;related convolutional;tuning pre trained"}, "c2dd1c332f65fea3a66f4a982428f31ce1a9dc70": {"ta_keywords": "retrieval web based;information retrieval web;retrieval web;databases ranked retrieval;information retrieval;deductive databases ranked;web based information;ranked retrieval methods;deductive databases;queries integrate information;retrieval methods;retrieval methods information;style deductive databases;like queries structured;ranked retrieval;allows queries integrate;queries structured;search engines;methods information retrieval;database representation;common database representation;like queries;database like queries;whirl allows queries;information multiple web;information sources easily;text database like;retrieval;database like;text database", "pdf_keywords": ""}, "eca07d2b351d81719b33c913a87c63d6930ee7f5": {"ta_keywords": "resources debating technologies;debating technologies;existing resources debating;resources debating;debating;existing resources;resources;technologies;existing", "pdf_keywords": ""}, "3a95fab610d5ff49fbdb7a4d8760b02c51df0013": {"ta_keywords": "anonymizing clinical notes;embeddings improve privacy;privacy clinical notes;secure clinical texts;improve privacy clinical;anonymizing clinical;technique anonymizing clinical;improve privacy;sensitive information unachievable;privacy clinical;word embeddings improve;removal sensitive information;anonymization;sensitive information;differential privacy;sensitive data;differential privacy sought;including sensitive data;word embeddings;privacy technique anonymizing;anonymizing;anonymization technique;sensitive data family;privacy;inspired differential privacy;technique anonymizing;introduce privacy;using word embeddings;notes guarantees private;word vectors embedding", "pdf_keywords": "word embeddings;word embeddings correlation;corpus create embeddings;novel anonymization;sensitive information text;masking sensitive information;human annotated word;nlp classification tasks;introduced novel anonymization;anonymization;ml nlp classification;embedding space semantic;novel anonymization technique;embeddings correlation proximity;specialized corpus create;nlp classification;sentiment classification noise;classification binary sentiment;properties word embeddings;annotated word pair;sentiment classification;downstream natural language;annotated word;sensitive information;effect anonymization;specialized corpus;anonymization technique;binary sentiment classification;effect anonymization technique;having specialized corpus"}, "76b95833fd0e242896d231abdea8dc01a167c7a6": {"ta_keywords": "process prior drift;estimating drift functions;prior drift function;unobserved latent dynamics;approximate gaussian process;estimating drift;process inference drift;prior drift;drift function stochastic;approach estimating drift;drift functions systems;inference drift function;drift function state;dynamics observations approximate;systems stochastic differential;gaussian process inference;drift functions;stochastic differential equations;observations approximate gaussian;stochastic differential;gaussian process prior;drift function;inference drift;unobserved latent;using gaussian process;systems stochastic;latent dynamics observations;gaussian process;stochastic;deal unobserved latent", "pdf_keywords": "approximate gaussian process;sparse gaussian process;gaussian process inference;process prior drift;gaussian process regression;gaussian process prior;variational em approximation;prior drift function;process inference drift;inference drift stochastic;unobserved latent dynamics;estimating drift functions;approximated sparse gp;sparse gaussian;drift facilitated sparse;using gaussian process;em approximation minimizing;gaussian process;posterior states approximated;approximate gaussian;sparse observations state;facilitated sparse gaussian;prior drift;drift stochastic;approximated sparse;drift stochastic differential;estimation drift;estimation drift facilitated;optimized variational em;estimating drift"}, "a660429b77e932af1c1d7d3f0554f4b17c044082": {"ta_keywords": "clusters terrorist groups;terrorist groups related;islamist jihadist groups;latent clusters terrorist;groups participating terrorist;jihadist groups;clusters terrorist;terrorist groups;includes terrorist groups;terrorist groups limitations;network includes terrorist;jihadist groups hold;behaviors islamist jihadist;approach latent clusters;latent clusters;groups related information;complex networks;data terrorist attacks;terrorist attacks;common behaviors islamist;participating terrorist actions;algorithm cluster formation;islamist jihadist;terrorist attacks occurred;networks approach latent;terrorist actions investigating;behaviors islamist;complex networks approach;reveals groups belonging;includes terrorist", "pdf_keywords": "terror group clustering;clusters terrorist groups;clustering ideology baseline;group clustering ideology;latent clusters terrorist;clustering ideology;ideological intra cluster;clusters terrorist;cluster assignment terrorist;groups participating terrorist;ideology leads cluster;terrorist groups behavioral;terror groups;based terror group;terror group;population terror groups;assignment terrorist groups;approach latent clusters;group clustering;terrorist groups;terror groups active;terrorist groups expanding;algorithm cluster formation;latent clusters;detecting latent clusters;groups behavioral ideological;clustering;cluster formation;ideology novel algorithm;cluster association"}, "8512718bafa447f9b433da9e809215dfc28b6b28": {"ta_keywords": "nlp performance prediction;reliable nlp performance;grained performance prediction;nlp performance;grained reliable nlp;performance prediction task;reliable nlp;performance prediction;performance prediction methods;grained performance measures;performance measures accuracy;analysis performance prediction;performance predictors;performance predictors holistic;task estimating performance;nlp tasks;estimating performance performing;performance prediction necessity;estimating performance;examine performance predictors;nlp tasks demonstrate;prediction task estimating;performance measures;types nlp tasks;languages tasks models;datasets languages tasks;prediction task;models examine performance;fine grained performance;examine performance", "pdf_keywords": "performance prediction nlp;prediction nlp tasks;prediction nlp;reliability performance prediction;improving performance prediction;performance prediction models;performance prediction methods;performance prediction;grained performance prediction;performance prediction model;typical nlp tasks;analysis performance prediction;nlp tasks make;nlp tasks;reliable nlppp;performance prediction necessity;introduction performance prediction;models typical nlp;nlp tasks holistic;performance prediction p2;nlp tasks demonstrate;different performance prediction;machine learning performance;learning performance based;predict machine learning;types nlp tasks;understand reliability performance;nlppp introduction performance;learning performance;predict machine"}, "84702b091af8842b6bbe457e5435c343a9824693": {"ta_keywords": "learning energy disaggregation;energy disaggregation;utility learning energy;learning energy;modification utility learning;utility learning;behavior modification utility;disaggregation;behavior modification;energy;utility;modification utility;behavior;learning;modification", "pdf_keywords": ""}, "54e7209e692ca4f5c85f0e68df34040b3cfa8bad": {"ta_keywords": "matrix coded computation;sparsity encoded matrix;encoded matrix coded;coded computation;tradeoff sparsity encoded;encoded matrix;coded computation explore;matrix coded;encoding scheme achieve;encoding scheme;encoded matrix number;sparsity encoded;new encoding scheme;study sparsity encoded;matrix number processors;propose new encoding;encoding;sparsity existing schemes;coded;upper bound sparsity;short dot scheme;new tradeoff sparsity;bound sparsity;strictly larger sparsity;tradeoff sparsity;encoded;new encoding;mds matrix propose;limits sparsity;limits sparsity level", "pdf_keywords": ""}, "1acbfc7d3e245bd3146e9e24eae7550aa2d03482": {"ta_keywords": "deep networks bn;batch normalization markov;understanding batch normalization;activation matrices neural;batch normalization;pre activation matrices;normalization markov chain;normalization markov;training performance networks;standard neural network;matrices neural;deep networks;matrices neural network;specifically deep networks;speed training empirical;standard neural;activation matrices;neural network architectures;performance networks;optimization speed training;neural network theoretical;networks bn;rank pre activation;theoretical understanding batch;neural network;normalization;networks equipped bn;experiments standard neural;neural;networks bn exhibit", "pdf_keywords": "rank bound deep;generalizes relu nets;rank robustness generalizes;rank stability crucial;rank robustness;initialization deep networks;hidden matrices rank;deep linear networks;demonstrate rank robustness;robustness generalizes relu;matrices rank preserves;rank preservation;computationally cheap rank;deep networks spectral;relu nets;deep linear;random initialization deep;rank stability;matrices rank;cheap rank preservation;bound deep linear;con\ufb01rm rank stability;relu nets finally;deep neural architectures;rank preservation operator;empirically demonstrate rank;rank width network;deep neural;deep networks;rank bound"}, "796f29cee975603c7a1469df1eb21ed5142ecff5": {"ta_keywords": "literary evidence retrieval;retrieving evidence literary;78k literary quotations;evidence literary claims;literary quotations surrounding;excerpt literary analysis;literary claims solving;literary evidence;task literary evidence;literary quotations;literary linguistic;literary claims;retrieve quoted passage;evidence literary;complex literary linguistic;literary linguistic phenomena;literary analysis;semantic similarity matching;quotations work relic;literary analysis surrounding;novel form quotations;understanding complex literary;lexical semantic similarity;excerpt literary;literature;evidence retrieval;quotations surrounding;quotations surrounding critical;quoted passage set;given excerpt literary", "pdf_keywords": "literary evidence retrieval;dataset literary evidence;collecting dataset literary;retrieving evidence literary;dataset literary;passage retrieval;dense passage retrieval;scholarly excerpts literary;embedding scholarly claims;quotations collecting largescale;passage retrieval guu;excerpts literary analysis;text literary claims;literary claims relic;scholarly excerpts;literary claims analysis;excerpts literary;evidence literary claims;literary claims quotations;evidence retrieval;excerpt literary analysis;78k scholarly excerpts;quotations collecting;evidence retrieval models;literary evidence;evidence retrieval collect;78k literary quotations;large scale retrieval;literary quotations pretrained;retrieve quoted passage"}, "d10e410765699a75628a1437b93f0d0fc3dc0aa6": {"ta_keywords": "supervised classification graph;semi supervised classification;semi supervised learning;classification graph data;classification graph;semi supervised;labeled instances training;performing semi supervised;unlabeled instances graph;accurate semi supervised;pagerank;labeled seed instances;pagerank style;learning propagating labels;labeled instances;pagerank style method;propose new pagerank;new pagerank style;new pagerank;labels labeled seed;fully supervised algorithms;supervised learning propagating;propagating labels labeled;supervised classification;algorithms require labeled;labeled seed;instances graph;supervised;instances graph using;supervised learning", "pdf_keywords": ""}, "48aa33ad92566cb60ef348ffa438e4712f618b03": {"ta_keywords": "reflectance depth measurements;transillumination reflectance depth;occlusal transillumination reflectance;reflectance depth;reflectance measurements swir;swir reflectance occlusal;infrared swir reflectance;depths measured microct;reflectance occlusal transillumination;depths measured swir;occlusal surface significant;reflectance occlusal;swir reflectance;reflectance measurements;measured swir occlusal;occlusal transillumination images;depth swir images;tooth proximal occlusal;images lesions tooth;viewed occlusal surface;estimated depth swir;depth using microct;lesion penetration depth;polarization reflectance measurements;transillumination reflectance;cross polarization reflectance;compared depths measured;depths measured;swir images measured;swir occlusal transillumination", "pdf_keywords": ""}, "3fb78bee6cb39588a1a4cbb4e0abce5e362aa130": {"ta_keywords": "bounds adversarial bandits;adversarial bandits;improved regret bounds;adversarial bandits words;regret bounds obtained;regret bounds;dependent bounds adversarial;provides regret bounds;regret bounds terms;bounds adversarial;optimal regret;optimistic mirror descent;optimal regret kt;case optimal regret;bandits;bandits words;mirror descent;sequence completely adversarial;adversarial;bounds obtained loss;variants optimistic mirror;bandits words classical;easier loss sequences;completely adversarial;improved regret;optimistic mirror;sequences provides regret;particular variants optimistic;obtain worst;optimal", "pdf_keywords": ""}, "e6ffeb4b9d808d6c9b8d388a7cbb431ac96bf194": {"ta_keywords": "aspirin dosing thrombotic;dosing thrombotic outcomes;dosing thrombotic;thrombotic outcomes patients;thrombotic outcomes;impact aspirin dosing;aspirin dosing;patients hvad;outcomes patients hvad;impact aspirin;thrombotic;aspirin;hvad;outcomes patients;dosing;patients;outcomes;impact", "pdf_keywords": ""}, "99053e3a708fc27709c9dab33110dc98b187c158": {"ta_keywords": "reasoning financial data;deep questions financial;finance knowledge;questions financial data;finance knowledge complex;financial data dataset;financial data;large corpus financial;financial data aiming;numerical reasoning financial;reasoning financial;acquiring finance knowledge;financial experts;written financial experts;financial experts results;finqa question answering;questions financial;corpus financial documents;financial documents;financial reports;corpus financial;finance;answering pairs financial;question answering;financials;humans acquiring finance;answering deep questions;knowledge finqa dataset;financial statements;question answering pairs", "pdf_keywords": "annotate gold reasoning;question answering pairs;gold reasoning programs;finqa question answering;answering deep questions;question answering;numerical reasoning knowledge;gold reasoning;answering deep;representations annotate gold;reasoning knowledge facilitate;answering pairs financial;heterogeneous representations annotate;reasoning knowledge;reasoning programs;annotate gold;automate analysis large;reasoning understanding heterogeneous;focus answering deep;deep questions;large scale dataset;deep questions \ufb01nancial;\ufb01nancial documents dataset;complex numerical reasoning;answering;reasoning understanding;numerical reasoning understanding;answering pairs;analysis large corpus;questions \ufb01nancial data"}, "0acbdcac9edf74cc2c1e98bd59e301c9300977d0": {"ta_keywords": "crowd annotation framework;consolidation crowd annotations;crowd annotations;crowd annotation;based crowd annotation;crowd annotations real;suggesting annotations sampling;sequence tagging tasks;suggesting annotations;sequence tagging;dynamically suggesting annotations;annotations sampling informative;rapid tagging recommendations;annotations real time;annotation framework;data annotation framework;annotators real time;annotations sampling;annotation framework sequence;data annotation;rapid tagging;alpacatag sequence tagging;sequence labeling tasks;new annotations introduce;annotations introduce;tagging recommendations powered;framework sequence tagging;active intelligent recommendation;annotations;new annotations", "pdf_keywords": ""}, "ee2e171d6a897ee5d0b0bde2d5f2548b52d3a840": {"ta_keywords": "teach quiz simstudent;test tutored simstudent;tutored simstudent;students meta cognitive;tutored simstudent meta;tutoring teachable agent;students learn cognitive;effective tutoring;skills effective tutoring;meta cognitive skills;simstudent meta cognitive;equations tutoring teachable;tutoring;learn cognitive skills;tutoring teachable;cognitive skills;tutoring classroom study;learn cognitive;cognitive help;meta cognitive help;cognitive skills effective;tutoring classroom;students learned solve;cognitive help problems;cognitive help results;cognitive help conducted;effective tutoring classroom;post test tutored;test tutored;provide meta cognitive", "pdf_keywords": ""}, "9633928f72cda45d102fb6740291d47137d0a5ca": {"ta_keywords": "attacks federated learning;attack federated learning;backdoor attacks training;training phase malicious;backdoor attacks federated;tune pruning attack;pruning attack success;pruning attack;federated pruning;mitigate backdoor attacks;mitigating backdoor attacks;federated pruning method;federated learning;server federated learning;designed federated pruning;federated learning systems;attacks training;training server federated;federated learning specifically;attacks training phase;backdoor samples training;validation dataset attackers;dataset attackers;attacks aggregation;attacks federated;backdoor attacks;mitigate backdoor;federated learning work;attacks aggregation stage;mitigating backdoor", "pdf_keywords": "attacks federated learning;attack federated learning;federated learning systems;federated learning;backdoor attacks federated;federated learning simplify;federated learning 14;called federated learning;federated learning chen;attacks federated;mitigate backdoor attacks;introduce federated pruning;mitigating backdoor attacks;inputs federated pruning;federated pruning methods;attack federated;federated pruning;clients attack federated;novel federated pruning;backdoor samples training;federated pruning process;federated pruning method;backdoor attacks;mitigate backdoor;mitigating backdoor;malicious data;method mitigate backdoor;abstract malicious clients;inputs federated;malicious data including"}, "7731e3dec97c48498b585408d44615346ade144a": {"ta_keywords": "characterizing language variation;language variation trends;social group language;language variation;glossaries draw sociolinguistic;language variation internet;variation senses words;group language deviates;reddit communities specificity;sociolinguistic;draw sociolinguistic theories;sociolinguistic theories;connect language variation;variation internet social;communities specificity different;sociolinguistic theories connect;language deviates;communities specificity;community unique word;characterizing language;draw sociolinguistic;different sense clusters;bert characterize variation;sense clusters community;reddit communities;group language;words used groups;internet social groups;language deviates norm;trends community behavior", "pdf_keywords": "communities distinctive language;distinctiveness community language;community language;language variation trends;language variation;varieties drawn sociolinguistics;glossaries draw sociolinguistic;language tend;scoring subreddits comparing;distinctive language tend;sociolinguistics;community language fraction;connect language variation;attributes online english;measuring usage sense;sociolinguistics literature showing;attribute scoring subreddits;sociolinguistic;comparing subreddits;draw sociolinguistic theories;reddit communities;sociolinguistics literature;language tend mediumsized;community behavior examine;usage sense variation;subreddits comparing subreddits;reddit communities investigate;drawn sociolinguistics;attributes subreddits community;drawn sociolinguistics literature"}, "b116e5044fe047fc48307795af1f3e11b3a9401c": {"ta_keywords": "statistical machine translation;machine translation;machine translation terms;machine translation using;translation using variational;word alignments statistical;alignments statistical machine;computes word alignments;variational bayes giza;variational bayes improves;word alignments;moses machine translation;using variational bayes;translation using;bayesian technique variational;variational bayes;translation terms;bayes giza widely;alignments statistical;bayes giza;software computes word;em algorithm;translation terms bleu;giza using variational;technique variational bayes;bayes improves performance;bayesian approaches;bayes improves;parameters apply bayesian;translation", "pdf_keywords": ""}, "4b18303edf701e41a288da36f8f1ba129da67eb7": {"ta_keywords": "zero shot learning;shot learning;shot learning approach;domain adaptation;domain adaptation methods;shot learning provide;casting domain adaptation;approach zero shot;zero shot;weights layer learned;datasets embarrassingly simple;layer learned;features attributes classes;real datasets;learning approach implemented;datasets;learning;learning approach;datasets approach able;datasets approach;approaches standard datasets;datasets embarrassingly;learning bound generalisation;attributes classes linear;provide learning bound;real datasets approach;attributes classes;layer learned given;classes linear layers;learning bound", "pdf_keywords": "zero shot learning;shot learning;shot learning consists;shot learning approach;approach zero shot;zsl datasets approach;abstract zero shot;zsl datasets;zero shot;collection zsl datasets;learning approach implemented;learning approach;approaches standard datasets;learning recognise;datasets;provide learning bound;layer learned;shot learning bernardino;learning;datasets approach;learning recognise new;weights layer learned;paper zero shot;features attributes classes;attributes classes;learned given environment;datasets discussion;learning bound generalisation;attributes classes linear;learning consists"}, "b29dd2c50da0dc4589eafac58007f6be7e13c501": {"ta_keywords": "room layout estimation;indoor scene understanding;geometry indoor environments;helpful indoor scene;model enclosing room;objects helpful indoor;room layout;indoor scene;3d geometry indoor;indoor environments;geometry indoor;doors pictures objects;pictures objects beds;room cabinets;room box frames;layout estimation;modeling geometry location;objects beds;indoor environments focus;doors pictures;modeling;modeling geometry;helpful indoor;enclosing room;location specific objects;scene understanding;performance room layout;objects beds tables;relative dimensions locations;center room cabinets", "pdf_keywords": ""}, "873b83326ad1f98549beb85bdb130a40a61e1f9b": {"ta_keywords": "information alternative scoring;choice tasks;multiple choice tasks;choice tasks simply;answer competition strings;ranking string probability;forms compete probability;task large language;tasks simply conditioning;language models;probability introduce domain;alternative scoring;large language models;answer competition;compete probability mass;mutual information alternative;alternative scoring function;mutual information;form competition simply;language models shown;surface forms compete;form competition;perform multiple choice;compete probability;lowers probability;highest probability introduce;string probability;multiple choice options;correct answer competition;competition strings", "pdf_keywords": "pmidc consistently outperforms;pmidc scoring;use pmidc scoring;analyze datasets pmidc;datasets pmidc;models used selection;datasets pmidc does;pmidc pmidc consistently;pmidc scoring given;pmidc consistently;probability scoring methods;pmidc does worse;extensive experiments pmidc;experiments pmidc consistently;outperforms previous scoring;broad use pmidc;avg comparison pmidc;comparison pmidc pmidc;probability scoring;comparison pmidc;generative models;generative models used;selection tasks extensive;experiments pmidc;scoring methods;scoring functions wide;previous scoring functions;better raw probabilities;scoring given model;use pmidc"}, "9fe579e54712ba82c4f1c93e46409613f592df16": {"ta_keywords": "vector matrix formulas;matrix formulas;vector matrix;vector;matrix;formulas", "pdf_keywords": ""}, "178d51c35c03e3ccaae2409c32a3c2001cefe7eb": {"ta_keywords": "incremental estimation;incremental estimation algorithm;incremental model adaptation;derive incremental estimation;incremental model;new incremental model;model adaptation approach;kalman filter algorithm;model adaptation;kalman filter;filter algorithm posterior;based posterior distributions;derive incremental;incremental;posterior refinement modeled;algorithm posterior distributions;estimation algorithm based;new incremental;process posterior refinement;estimation algorithm;posterior distributions process;evolutions posterior distributions;solution kalman filter;posterior distributions model;distributions process posterior;adaptation approach based;propose new incremental;bayesian approaches algorithm;adaptation approach;bayesian approaches", "pdf_keywords": ""}, "415d4231cab5ddee73e2ed536d033d5c31f24b4a": {"ta_keywords": "bootstrapping biomedical ontologies;information extraction biomedical;entity recognition biomedical;extraction biomedical text;open information extraction;biomedical ontologies scientific;learning terms biomedical;biomedical ontologies;named entity recognition;web bootstrapping biomedical;information extraction;ontology seeds automatically;biomedical text based;ontologies scientific text;entity recognition;recognition biomedical entities;ontology seeds;bootstrapping biomedical;initial ontology seeds;biomedical text;seeds ontology;extraction web text;extraction web;seeds ontology category;biomedical entities using;semi supervised bootstrapping;designed extraction web;ontologies scientific;biomedical entities;learning terms", "pdf_keywords": ""}, "8163c4010fc103343518d49db5974577593972f6": {"ta_keywords": "dialogue based deception;deception detection;based deception detection;detect deception;deception asking questions;attempt detect deception;deceptive conversational partner;detect deception perform;deceptive conversational;deception asking;deception perform actions;deception;signs deception asking;features deception;deception perform;catch potential liar;unveil deceptive conversational;telltale signs deception;dialogue systems asks;features deception salient;based deception;deception salient;deception detection hand;potential liar paper;signs deception;make features deception;envisioning dialogue systems;dialogue systems;potential liar;envisioning dialogue", "pdf_keywords": ""}, "fa5c7406d09af3f06a3a7ead49975e3ee90ed584": {"ta_keywords": "shared autonomy explains;share autonomy human;shared autonomy;agents share autonomy;autonomy explains efficiency;autonomy explains;share autonomy;autonomy human;robotic agents share;humanrobot language;viewing humanrobot language;autonomy;communicating robot;lens shared autonomy;autonomy human leverage;humanrobot language lens;robotic agents;robot;viewing humanrobot;humanrobot;robotic;communicating robot taxing;task extra knowledge;cooperative explicit;human domain knowledge;efficiency versus cognitive;make deciding cooperative;plan efficiency user;lost communicating robot;leverage human domain", "pdf_keywords": "robots language communication;humans robots language;robots language;task robot;interaction humans robots;task robot butler;language robots;incorporating language robots;humanrobot language;autonomy manipulation objects;language robots lens;robot butler knowledge;human robot need;robot construct task;viewing humanrobot language;humanrobot language lens;robot need;shared autonomy manipulation;butler knowledge robotic;humans robots;robot need handle;object destination robot;knowledge robotic manipulation;task language;shared autonomy explains;robotic manipulation capabilities;robot construct;knowledge robotic;loads human robot;robot"}, "9527352b925f9fa36c40966ed755afd22301b0aa": {"ta_keywords": "preferences ethical principles;ethical complies constraints;important preferences ethical;preferences ethical;decisions preferences;preferences decision maker;preferences decision makers;decision based preferences;bad decisions preferences;decision ethical complies;complies constraints priorities;decisions preferences important;ethical moral norms;ethical principles derived;constraints priorities;qualitatively model preferences;cp net formalism;decision ethical;constraints decision context;ethical principles;preferences optimization criteria;preferences outcomes decisions;decision makers properties;specific ethical principles;subjective preferences decision;moral norms based;constraints priorities given;model preferences outcomes;ethical principles conflict;based constraints decision", "pdf_keywords": ""}, "74c80622b91894efbe4ae9ce1428e4d699b05516": {"ta_keywords": "distributed convex optimization;dual stochastic gradient;dual stochastic oracle;stochastic gradient oracle;methods distributed convex;dual stochastic;primal dual stochastic;distributed convex;additionally dual stochastic;stochastic gradient;stochastic oracle;stochastic oracle propose;gradient oracle methods;convex optimization;convex optimization problems;oracle methods distributed;gradient oracle;method rate convergence;convergence terms duality;methods distributed;optimization problems networks;duality gap probability;stochastic;bound distance iteration;iteration sequence optimal;rate convergence;convex;probability large deviations;distance iteration sequence;rate convergence terms", "pdf_keywords": ""}, "c00e4564ea054c14c83cb564af6c37e47c8ab367": {"ta_keywords": "sensor subset tracking;estimation restricting sensor;tracking markov chain;active tracking markov;observations markov chain;sensor sampling;sensor sampling comparable;active sensor subset;tracking markov;sensor usage order;sensor usage;chain subset sensors;complete sensor observation;making observations markov;uniform sensor sampling;sensor subset;selection active sensor;sensors gather observations;restricting sensor usage;observations remote estimation;active sensor;observations markov;subset sensors;activating sensors gather;sensor observation;total sensors available;remote estimation;total sensors;activating sensors;tracking discrete time", "pdf_keywords": "active tracking markov;centralized tracking markov;tracking markov chain;tracking markov;state estimation achieve;observations markov chain;complexity active sensor;state estimation;state markov chain;centralized active tracking;making observations markov;markov chain subset;state markov;sensor subset tracking;tracking discrete time;like state estimation;optimal sensor subset;observations markov;active sensor subset;\ufb01nite state markov;active sensor selection;markov chain unknown;algorithm centralized tracking;markov chain;state estimation conclusions;subset tracking discrete;selection active sensor;usage state estimation;centralized tracking;activating optimal sensor"}, "98ef0db84e62aef969629264c9de1f4d0013f3b9": {"ta_keywords": "diverse nlu tasks;multi task learning;learned multiple tasks;transfer learning;task composition transfer;transfer learning empirically;nlu tasks effectively;knowledge multiple tasks;task learning;nlu tasks;composition transfer learning;learn task;learn task specific;task learning methods;adapters encapsulate task;tuning multi task;tasks effectively combines;forgetting difficulties dataset;multi task;knowledge composition classifier;information adapterfusion;task composition;learned multiple;stage learn task;specific information adapterfusion;multiple tasks;multiple tasks sequential;diverse nlu;exploit representations learned;difficulties dataset balancing", "pdf_keywords": "learned multiple tasks;diverse nlu tasks;knowledge multiple tasks;nlu tasks;separate knowledge composition;adapters separate knowledge;knowledge composition;nlu tasks \ufb01nd;catastrophic forgetting balancing;learned multiple;issues catastrophic forgetting;multiple tasks avoiding;multiple tasks;extraction knowledge composition;catastrophic forgetting;knowledge composition step;representations learned multiple;knowledge multiple;forgetting balancing different;separate knowledge;separating stages knowledge;balancing different tasks;different tasks empirically;tasks empirically;knowledge composition classi\ufb01er;share knowledge multiple;forgetting balancing;stages knowledge extraction;exploit representations learned;diverse nlu"}, "43953a051b6518f32fc37734cfc49942baeac5a1": {"ta_keywords": "speech spectral variation;spectral variation utterances;spectral parameters utterances;speaker spectral variation;statistical voice conversion;conversion statistical voice;sample converted speech;speech spectral;converted speech speech;speaker spectral parameter;input speech spectral;converted speech;intra speaker spectral;parameter variation utterances;voice conversion;speaker spectral;statistical voice;prediction speaker utters;metrics intra speaker;speech sample;utterances vary distance;voice conversion technologies;speech speech sample;parameters utterances vary;variation utterances;utterances vary;spectral conversion statistical;utters sentence spectral;speaker imitating prosody;parameters utterances", "pdf_keywords": ""}, "4077c1986f32817801b3082ce8dde514424f71a1": {"ta_keywords": "crowdsourcing synset cleansing;thesaurus created crowdsourcing;quality synsets thesaurus;synsets thesaurus created;synsets thesaurus;expert annotators thesaurus;crowdsourcing synset;workflow crowdsourcing synset;thesaurus created collaboratively;annotators thesaurus crucial;annotators thesaurus;novel workflow crowdsourcing;workflow crowdsourcing;thesaurus crucial;thesaurus created;thesaurus crucial resource;expert annotators;non expert annotators;crowdsourcing;thesaurus;created crowdsourcing;using russian thesaurus;russian thesaurus created;crowdsourcing showing;improve synset quality;russian thesaurus;synset cleansing;crowdsourcing showing does;collaboratively non expert;quality synsets", "pdf_keywords": ""}, "2344cca985dd4e2e2519838b2353b5c295e73036": {"ta_keywords": "answering questions arc;knowledge reasoning types;questions arc dataset;ai2 reasoning challenge;knowledge reasoning context;classification knowledge reasoning;reasoning context arc;definitions knowledge reasoning;questions arc;introduces ai2 reasoning;types necessary answering;reasoning types;knowledge reasoning;reasoning challenge arc;reasoning types necessary;necessary answering questions;arc dataset 2018;types knowledge reasoning;present arc corpus;arc corpus;naive information retrieval;associated arc dataset;arc dataset additionally;ai2 reasoning;questions easy challenge;arc dataset;reasoning challenge;challenge arc associated;reasoning context;context arc dataset", "pdf_keywords": "annotations questions returned;annotations questions;annotators label knowledge;set annotations questions;knowledge reasoning arc;reasoning type labels;knowledge reasoning types;label knowledge reasoning;improve knowledge reasoning;reasoning types improve;annotation interface;knowledge reasoning type;novel annotation interface;annotations;annotators;annotation instructions knowledge;systems following annotation;types knowledge reasoning;reasoning type categories;annotation;annotation interface de\ufb01ne;reasoning types;annotate;make annotation intuitive;reasoning arc dataset;annotation intuitive accurate;knowledge reasoning;dataset annotate;annotation intuitive;annotation instructions"}, "05fb1eea6381ccd21bde53495c7707546aa234c7": {"ta_keywords": "named entity recognition;entity recognition shared;entity recognition social;entity recognition;short term memory;comprehensive word representations;entity social media;memory bilstm neural;term memory bilstm;recognition social media;emerging named entity;recognizing emerging named;word representations;term memory;word representations multi;named entity social;social media messages;bilstm neural;multi channel neural;named entity;channel neural;channel bilstm crf;channel neural architecture;bidirectional long short;bilstm neural network;memory bilstm;entity social;random fields crf;channel bilstm;text", "pdf_keywords": ""}, "3132a18a441ab6067066e4d4d85608b058c9ed33": {"ta_keywords": "change point detection;autoencoders time invariant;detection time series;subtle changes autocorrelation;changes time series;changes autocorrelation statistics;changes autocorrelation;autoencoders time;using autoencoders time;autoencoders learn;autoencoders;autoencoder based;used autoencoders learn;detection alarms;data using autoencoders;false detection alarms;autoencoder;indicate change points;detection time;abrupt property changes;employ autoencoder based;autoencoder based methodology;identify subtle changes;using autoencoders;deep learning;used autoencoders;detection alarms use;detection cpd;employ autoencoder;changes time", "pdf_keywords": "change point detection;detection time series;autoencoders time invariant;autoencoders time;propose new autoencoder;using autoencoders time;subtle changes autocorrelation;new autoencoder based;detection time;changes time series;autoencoders;multiview learning;data using autoencoders;new autoencoder;autoencoder based;point detection time;changes autocorrelation;autoencoder;deep learning;using autoencoders;changes autocorrelation statistics;time invariant representation;\ufb01eld multiview learning;deep learning techniques;detection alarms;point detection cpd;multiview learning 52;time invariant;autocorrelation statistics signal;time domain"}, "91e605a125f64207a242693d0dc1c862080f6c27": {"ta_keywords": "translation model phoneme;phoneme lattices translations;learning lexicon translation;manual phonemic transcription;automatic phoneme recognition;automatic transcription word;improve automatic phoneme;lexicon translation model;learning phoneme lattices;translations improve automatic;translations speech transcribed;useful bilingual lexical;bilingual lexical entries;phoneme recognition;phoneme recognition learning;automatic phoneme;learning phoneme;lattices translations speech;bilingual lexical;lexicon manual phonemic;phonemic transcription;automatic transcription;prior lexicon translation;transcription word level;manual automatic transcription;phonemic transcription possible;instead learning phoneme;recognition learning lexicon;translations improve;translations speech", "pdf_keywords": ""}, "b953a582cc79c33054b295c20c1201e8d5bd8243": {"ta_keywords": "automated tutoring systems;automated tutoring;tutoring systems discovering;intelligent tutoring systems;student models clustering;intelligent tutoring;instruction intelligent tutoring;discovering student models;affects automated tutoring;tutoring systems;systems discovering student;tutoring;tutoring systems making;discovering student;finds student models;learning related problems;student models;student models using;automated approach knowledge;student behavior patterns;approach finds student;student model built;student model;learning task difficulty;automaticallygenerated problem content;content learning;models clustering;systems making instructional;learning related;good student model", "pdf_keywords": ""}, "d9d0d908e3f652ee350f4919d4c2ab972ada1ca4": {"ta_keywords": "coreference dataset entertains;coreference dataset annotation;new coreference dataset;existing coreference data;text data coreferences;coreference dataset;coreference core nlp;coreference data;challenges computers coreference;data coreferences;develop new coreference;new coreference;coreferences;existing coreference;coreferences named entities;coreference data lack;computers coreference;coreferences named;coreference;computers coreference core;coreference core;core nlp;data coreferences named;wheels coreference dataset;source existing coreference;references quiz bowl;annotation framework;denser references quiz;truly solve coreference;annotation", "pdf_keywords": ""}, "f18ec4e0bce2e4d847954c9692959d88ba8a9b66": {"ta_keywords": "secure estimation v2x;vehicle v2x communications;secure remote estimation;secure estimation;malicious compromised vehicles;ensure security sensing;estimation v2x networks;estimation error secure;vehicles communicated wireless;compromised vehicles goal;error secure estimation;security sensing;security sensing systems;systems v2x networks;cooperative intelligent transport;compromised vehicles;packet drop attacks;observations enabling secure;v2x communications;remote estimation gauss;v2x communications essential;sensing systems v2x;v2x networks injection;individual vehicles communicated;enabling secure estimates;vehicles communicated;malicious observations enabling;filtering malicious observations;v2x networks;networks injection packet", "pdf_keywords": ""}, "5e27712db641bc8f16c510292f7fd5440acd563d": {"ta_keywords": "stacked graphical learning;learner collective classification;collective classification;collective classification classes;collective classification widely;collective classification usually;used collective classification;expensive collective classification;graphical learning meta;instances stacked graphical;graphical learning efficient;base learner collective;propose stacked graphical;related instances stacked;graphical learning;stacked graphical;classification relational;classification relational datasets;meta learning scheme;instances stacked;learner collective;used classification relational;meta learning;learning meta learning;related instances predicting;base learner;learning scheme;relational datasets;classes predicted simultaneously;predictions related instances", "pdf_keywords": ""}, "73e868f74376814a4c08eca6ce043fe7c7aefeed": {"ta_keywords": "personalized pagerank similarity;pagerank similarity;pagerank similarity vectors;personalized pagerank measure;personalized pagerank;walks graphical models;personalized pagerank widely;fields personalized pagerank;pagerank;pagerank measure roughly;pagerank measure;using personalized pagerank;pagerank widely;models graph walks;tree structured markov;pagerank widely used;graph walks;draft graph walks;nodes based graphwalks;set personalized pagerank;graph walks graphical;similarity measure graph;probabilities tree structured;marginal probabilities tree;structured markov random;based graphwalks;walks graphical;graphical models graph;connection inference tree;similarity vertices graph", "pdf_keywords": ""}, "f6160c3196288b9e435dc6f86024f56e6b5ab722": {"ta_keywords": "fair utilitarian allocations;utilitarian maximal allocations;computing fair utilitarian;computing allocations fair;utilitarian allocations indivisible;fair allocations compute;utilitarian allocations;allocations fair maximize;fair maximize utilitarian;allocations indivisible goods;fair allocations;allocations fair;allocations indivisible;computational problems utilitarian;computing fair;welfare computing fair;maximal allocations;maximal allocations decide;prop1 fair allocations;fair utilitarian;fairness concepts envy;problems utilitarian maximal;indivisible goods problem;tractable fairness concepts;complexity computing allocations;computing allocations;maximize utilitarian;maximizes utilitarian;maximizes utilitarian welfare;tractable fairness", "pdf_keywords": ""}, "579095d50eab27ace24a1ea0e97af2f70191dc7c": {"ta_keywords": "subcellular location image;information text images;figures associating information;matching sub figures;combination text images;text images journal;information text image;stacked graphical model;articles stacked graphical;images journal articles;text images;images figures associating;labels sub figures;stacked graphical;text images figures;text image;sub figures labels;images journal;figures labels sentences;image finder;location image finder;figures associating;location image;sub figures;match labels sub;sub figures sentences;associating information;image finder extracts;subcellular;model associating information", "pdf_keywords": ""}, "0c61265a4325df4b97389f92e5e4f5df412f8e97": {"ta_keywords": "localization partial discharge;discharge power transformer;electrical joint localization;location power transformer;discharge location power;joint localization method;partial discharge location;localization method partial;partial discharge power;method partial discharge;acoustic electrical joint;localization method;operation power transformers;nonlinear localization;socp acoustic electrical;dimensional nonlinear localization;power transformers;ultrasonic signal;accurate localization partial;power transformer consideration;joint localization;acoustic electrical;discharge location;power transformer;order convex optimization;electrical joint method;power transformer considering;nonlinear localization equations;ultrasonic;localization accuracy efficient", "pdf_keywords": ""}, "46a2960e409c39901c1efd07a6adfc5f26e22ee8": {"ta_keywords": "email leaks;techniques popular email;information leaks suggestions;email leak;popular email client;responded data mining;costly mistake email;cases email leaks;email clients;information leaks;email leaks 47;mistake email leak;additions email clients;techniques information leaks;email leak accidentally;email client;thunderbird report;unintended recipient widespread;popular email;mozilla thunderbird report;email client mozilla;thunderbird report users;provided data mining;users responded data;thunderbird;thunderbird conclude;mozilla thunderbird conclude;client mozilla thunderbird;mozilla thunderbird;permanent additions email", "pdf_keywords": ""}, "d6a7d2e9f2caf3e8eb615580f4ee8329ff9a271d": {"ta_keywords": "queue available parking;parking bulk traffic;curbside parking network;capacity queues drivers;cruising curbside parking;parking advantageously modeled;available parking space;queue available space;capacity queues;network interdependent queues;parking network;finite capacity queues;parking comparing rate;parking space;modeling curbside parking;occupancy approximated parking;queues drivers;available parking;curbside parking advantageously;curbside parking;interdependent queues;approximated parking;curbside parking comparing;approximated parking transaction;parking bulk;parking comparing;adjacent queue;interdependent queues furthermore;queues drivers arrive;traffic", "pdf_keywords": ""}, "4d01d6b445077ad0f1c9d85af93f9ed9239f3c33": {"ta_keywords": "ofspeech tagging historical;tagging historical data;tagging historical;tagging accuracy manuscript;data tagging accuracy;tagging accuracy;training data tagging;method ofspeech tagging;ofspeech tagging;data tagging;tagging;corpora historical german;modern german corpora;tagger trained modern;corpora historical;german corpora;different corpora historical;texts different corpora;tagger;tagger trained;used preprocess texts;preprocess texts;spelling normalization;applying pos tagger;historical german;spelling normalization used;data evaluates texts;pos tagger trained;corpora;historical german 15th", "pdf_keywords": ""}, "189e6bb7523733c4e524214b9e6ae92d4ed50dac": {"ta_keywords": "learning sequence tagging;tagging hierarchical recurrent;neural sequence taggers;sequence tagging tasks;sequence tagging hierarchical;sequence taggers;sequence tagging;microblogs transfer learning;sequence taggers source;recurrent networks improvements;tagging microblogs transfer;hierarchical recurrent networks;taggers source task;recurrent networks domains;deep hierarchical recurrent;transfer learning sequence;different sequence tagging;tagging tasks;tagging penn treebank;recurrent networks;pos tagging microblogs;annotations pos tagging;transfer learning deep;tagging microblogs;tagging tasks paper;hierarchical recurrent;learning neural sequence;tagging hierarchical;problem transfer learning;transfer learning neural", "pdf_keywords": "neural sequence taggers;sequence taggers;sequence tagging;taggers source task;approach sequence tagging;transfer learning neural;sequence taggers source;learning neural sequence;problem transfer learning;transfer learning;present transfer learning;deep hierarchical recurrent;sequence tagging exploits;tagging microblogs;transfer learning approach;task plentiful annotations;annotations pos tagging;pos tagging microblogs;deep neural;microblogs present transfer;recurrent neural network;deep hierarchical;tagging penn treebank;tagging microblogs present;based deep hierarchical;tagging exploits generality;hierarchical recurrent neural;develop transfer learning;target task labels;lingual transfer"}, "58b628792d3eb22a034a871ed3cf373afe591928": {"ta_keywords": "codes hadoop hdfs;hadoop hdfs;hadoop hdfs compare;codes modified hdfs;reliability distributed storage;distributed storage systems;hdfs;codes hadoop;new codes hadoop;hdfs compare;hdfs implementation;distributed storage;modified hdfs implementation;erasure codes efficiently;hdfs compare currently;codes efficiently repairable;disk repair network;modified hdfs;storage systems;hdfs module;storage systems large;erasure codes;hdfs module uses;erasure codes used;solomon codes overhead;recently erasure codes;high storage efficiency;storage overhead replicated;family erasure codes;storage compared reed", "pdf_keywords": "codes hadoop hdfs;hadoop hdfs;novel erasure codes;hadoop hdfs compare;erasure codes big;new erasure codes;distributed data reliability;abstract distributed storage;hdfs compare;distributed storage;hdfs;erasure codes ef\ufb01ciently;distributed storage systems;erasure codes;codes hadoop;hdfs compare currently;new codes hadoop;codes ef\ufb01ciently repairable;challenges distributed data;storage systems large;storage systems;family erasure codes;codes big data;hdfs module;reed solomon codes;data reliability;codes achieve achievability;storage;hdfs module uses;distributed data"}, "58174f5bb9f9815b52a99fa03ec42f2b44f2d550": {"ta_keywords": "set instance extraction;production semantic lexicons;semantic lexicons large;semantic lexicons;instance extraction;instance extraction using;language independent set;language dependent hyponym;lexicons;automatic set instance;finding instances set;lexicons large corpus;semantic class;production semantic;lexicons large;web based set;finding instances;instances set given;named asia automatic;instances set;problem production semantic;semantic class input;takes semantic class;automatic set;semantic;asia automatic set;automatically outputs instances;extraction using web;based set expansion;language independence", "pdf_keywords": ""}, "2bafaffe45ba66685c87e2d0a598222a9a68ae13": {"ta_keywords": "russian linguistic resources;russian language processing;community russian linguistic;russian linguistic;resources russian language;catalogue linguistic resources;russian language;tools resources russian;catalogue linguistic;linguistic resources presented;catalogue community russian;linguistic resources assist;linguistic resources;specialized catalogue linguistic;resources russian;language processing;community russian;russian;linguistic;language processing significant;language;survey nlpub catalogue;nlpub catalogue community;nlpub catalogue;survey nlpub;gathering representation approaches;data gathering representation;representation approaches described;resources presented compared;representation approaches", "pdf_keywords": ""}, "108ec3512cdf2e89ba3067f5b10eaa4a96df9347": {"ta_keywords": "speaker adaptation;speaker adaptation experiments;transfer vectors acoustic;transfer vector adaptation;vectors acoustic modeling;accurate transfer vectors;speech recognition;vector adaptation;speech recognition efficient;vectors acoustic;cft transfer vectors;transfer vectors;transfer vectors compared;representation transfer vectors;continuous speech recognition;adaptation particular bayesian;conventional transfer vector;transfer vector;cft transfer vector;acoustic modeling;transfer vectors small;acoustic modeling using;training cft transfer;transfer vector decomposed;reformulate adaptation scheme;directional statistics cft;reformulate adaptation;paper reformulate adaptation;vector adaptation particular;adaptation scheme coarse", "pdf_keywords": ""}, "b575d272036740e03fcf67d64db969557843f629": {"ta_keywords": "representations tweets learning;space representations tweets;tweets learning;representations tweets;tweets learning complex;composition model tweet2vec;model tweet2vec;text social media;tweet2vec;annotated hashtags;user annotated hashtags;annotated hashtags associated;character sequences informal;tweet2vec finds vector;hashtags associated posts;tweet2vec finds;model tweet2vec finds;characters commonplace posts;traditional nlp;text social;hashtags;traditional nlp approaches;tweets;hashtags associated;character composition model;nlp approaches;nlp;sequences informal language;nlp approaches fail;predicting user annotated", "pdf_keywords": "learning tweet representations;tweet representations;tweet representations work;tweet2vec character based;tweet embedding;learning tweet;network learning tweet;tweet2vec character;combined tweet embedding;type tweet2vec character;presented tweet2vec character;tweet2vec composing;word type tweet2vec;tweet2vec composing constituent;learning distributed representations;tweet2vec character level;type tweet2vec;tweet2vec;tweet embedding conclusion;bdhingra tweet2vec composing;hashtags traditional neural;presented tweet2vec;representations social media;language models nnlms;encoder social media;distributed representations;neural network language;based distributed representations;network language models;linearly combined tweet"}, "d34712c217046ccf8063efe083fbb1e6cbfc0340": {"ta_keywords": "replication based cdns;large cdns replicate;coding cdn architecture;erasure coding cdn;content delivery networks;cdn redundancy server;cdns replicate;cdn architecture;delivery networks cdns;cdn architecture use;internet cdn redundancy;cdn redundancy;networks cdns;large cdns;cdns replicate objects;ef\ufb01cient content delivery;networks cdns deliver;write load servers;based cdns c2dn;based cdns;redundancy server;content delivery;used large cdns;servers state art;coding cdn;replication;cdns deliver world;servers cluster;cdns;internet cdn", "pdf_keywords": ""}, "f8e580fcf34ee6da50989bbde685634018cbe224": {"ta_keywords": "dialog state tracking;track dialog state;utterance tracker;track dialog;dstc5 utterance tracker;human dialog;human human dialog;dialog state human;challenge dstc5 utterance;advanced dialog state;utterance tracker emits;dialog state;attention based tracker;dstc5 track dialog;5th dialog state;advanced dialog;dialog;present advanced dialog;state tracking challenge;dstc5 utterance;history dialog;trackers elaborated english;tracking challenge dstc5;considering history dialog;5th dialog;task dstc5 track;decoder architecture attention;compared rule trackers;designed 5th dialog;rule based trackers", "pdf_keywords": ""}, "8a6d2e134b3b2df6291af8e36e126ae55d50649c": {"ta_keywords": "recurrent networks paraphrastic;paraphrastic sentence embeddings;networks paraphrastic sentence;sentence embeddings;recurrent averaging network;networks paraphrastic;sentence embeddings revisiting;revisiting recurrent networks;recurrent networks underperform;lstm recurrent networks;recurrent networks;recurrent architecture;sentence embeddings analyze;recurrent architecture gated;new recurrent architecture;architecture gated recurrent;general purpose paraphrastic;relations lstm recurrent;paraphrastic;inspired averaging lstms;paraphrastic sentence;lstm recurrent;dependency relations lstm;recurrent averaging;embeddings revisiting;gated recurrent averaging;averaging lstms;averaging lstms outperforming;purpose paraphrastic sentence;purpose paraphrastic", "pdf_keywords": "recurrent averaging network;recurrent architecture semantic;sentence embeddings transfer;recurrent architecture;new recurrent architecture;architecture gated recurrent;averaging lstms introduce;outperforms averaging lstm;learning paraphrastic;averaging lstm;paraphrastic sentence embeddings;recurrent neural network;sentence embeddings;recurrent methods learning;learning paraphrastic sentence;averaging lstm transfer;gated recurrent averaging;averaging lstms;performance averaging lstms;recurrent averaging;modeling sentential compositionality;avg recurrent methods;recurrent neural;lstm transfer supervised;embeddings transfer supervised;performance learning paraphrastic;recurrent methods recurrent;promising new recurrent;lstms improve;recurrent methods"}, "3911b13a61a3a57674cc8c70c760f545de8aeea2": {"ta_keywords": "honest responses agents;eliciting honest responses;reward response evaluation;evaluation matches peer;agent gets reward;reward response;gets reward response;eliciting honest;large scale evaluations;incentive;evaluations mechanism;reward;mechanism strong incentive;responses agents;strong incentive properties;evaluations mechanism simple;incentive properties;grading online;propose new reward;gets reward;new reward mechanism;strong incentive;truth serum;courses eliciting honest;evaluations product;incentive properties applicable;reward mechanism;evaluations product service;intuitive output agreement;scale evaluations product", "pdf_keywords": ""}, "f335e2256b31d9458c10c61e60bb8bed9dcaf1d9": {"ta_keywords": "learning navigate visual;vision language navigation;learning navigate;learning vision language;vision language;view learning vision;view learning;learning vision;multi view learning;navigate visual;language navigation;navigate visual environment;task natural language;navigation;navigate;following natural language;natural language instructions;language instructions challenging;language ambiguity improve;visual environment;visual environment following;instructions different views;learning;language instructions highly;natural language;training paradigm learn;visual;generalization recent room;view;paradigm learn", "pdf_keywords": "instructions approach learns;learns effectively;learns effectively limited;vision language navigation;approach learns effectively;approach learns;agreement instruction variants;learns;instruction variants sharing;instruction variants;multiple instruction;single instruction;language ambiguity improve;environments single instruction;language navigation;vision language;training paradigm learn;improves generalization unseen;models vision language;improves generalization;mutual agreement instruction;multiple instruction used;training paradigm;novel training paradigm;language navigation allowing;training data generalizes;spl improves generalization;effectively limited training;room r2r benchmark;new training paradigm"}, "5f23482a8c06ca1ae3e4577e3fdd9213884dac85": {"ta_keywords": "complexity probabilistic lobbying;probabilistic lobbying;lobbying;complexity probabilistic;probabilistic;complexity", "pdf_keywords": ""}, "1e638d235a512cc76d00713639259540342c6fbe": {"ta_keywords": "relation extraction classification;relation classification task;semantic relation extraction;relation extraction task;relation extraction;relation extraction model;relation classification;end relation extraction;second relation classification;senerio relation extraction;semantic relation;task semantic relation;concept candidate embeddings;extraction classification scientific;extraction task subtask;classification task subtask;candidate embeddings paper;embeddings paper describes;candidate embeddings;shared task semantic;classification scientific papers;extraction classification;task semantic;semeval 2018 task;relation;classification task;encoding attention;end relation;semantic;subtask senerio relation", "pdf_keywords": ""}, "c07fdc95bbf533f8709f8e39c069c1e22b73a7dc": {"ta_keywords": "soluble perfluorinated polyelectrolyte;perfluorinated polyelectrolyte;perfluorinated polyelectrolyte safe;li batteries;performance li batteries;polyelectrolyte safe;polyelectrolyte safe high;polyelectrolyte;soluble perfluorinated;perfluorinated;batteries;high performance li;li;performance li;soluble;safe high;safe;safe high performance;high;high performance;performance", "pdf_keywords": ""}, "26c65dad79da20aa67df21a6c10e509a964f0841": {"ta_keywords": "english entity linking;entity linking;entity linking el;reference knowledge base;entity linking integrates;entity mentions correspond;entries reference knowledge;entity mentions;kbp english entity;cluster entity mentions;tasks entity linking;english entity;validation follows entailment;knowledge base;reference knowledge;knowledge base slot;entity;wikifier additional functionality;mentions correspond entries;correspond entries reference;validation sfv tasks;entailment formulation evaluates;wikifier;entailment formulation;wikifier additional;cluster entity;illinois wikifier additional;linking;illinois wikifier;mentions correspond", "pdf_keywords": ""}, "d4d25eaa373087ac80810d79afff863ef1bae3c3": {"ta_keywords": "seat activity wheelchair;movement validating wheelchair;wheelchair seat activity;seat activity tracker;activity wheelchair users;activity wheelchair;validating wheelchair seat;validating wheelchair;seat movement validating;wheelchair users results;seat movement scores;seat movements validated;characterizing seat activity;wheelchair cushion;wisat characterizing seat;movements methods wisat;wheelchair seat;accuracy characterizing seat;seat activity;wheelchair users;shifts seat movements;seat movements methods;weight shifts seat;wheelchair;seat activity terms;shifts seat movement;seat movements;wisat sensor;methods wisat sensor;beneath wheelchair cushion", "pdf_keywords": ""}, "a4cd428d196bf041c22592216f15246b98b91915": {"ta_keywords": "bayesian matching equilibrium;matching equilibrium public;bayesian matching;matching equilibrium;equilibrium public good;uniqueness bayesian matching;equilibrium public;public good economy;existence uniqueness bayesian;uniqueness bayesian;good economy;economy;matching;bayesian;equilibrium;public good;uniqueness;public;existence uniqueness;existence;good", "pdf_keywords": ""}, "8a94106364576f0aa79dccfb30f0536514408249": {"ta_keywords": "rank learning pairwise;rank learning;pairs rank learning;rank learning learning;robust rank learning;ranking datasets optimization;feature based ranking;based ranking functions;dierent ranking datasets;ranking datasets;baseline rankers;baseline ranker;ranking;robust rank;approach robust rank;based ranking;ranking functions;pairs rank;ranking functions fundamental;linear baseline ranker;learning pairwise preferences;dierent ranking;baseline ranker used;rank;based pairwise preference;rankers;pairwise preference framework;outlying pairs rank;ranker;ranker used input", "pdf_keywords": ""}, "abcaec70b463ed925c29180437ed581c971952cf": {"ta_keywords": "dialogue example databases;dialogue modeling;users spoken dialogue;systems dialogue modeling;improving dialogue quality;based dialogue modeling;dialogue modeling important;example based dialogue;dialogue modeling ebdm;spoken dialogue systems;response candidates improves;dialogue systems dialogue;improving dialogue;dialogue systems;dialogue quality experimental;select response utterances;multi response candidates;response utterances examples;response utterances;use plural responses;dialogue quality;multiple response candidates;response plural appropriate;plural responses properly;appropriate response plural;dialogue example;plural appropriate responses;appropriate response candidates;plural responses;response plural", "pdf_keywords": ""}, "f46a3a5dc70a70292175e6c7ad505b8206cb070c": {"ta_keywords": "speech recognition asr;chime speech separation;speech separation recognition;speech recognition;vocabulary moving talker;chime challenge initiative;microphone automatic speech;automatic speech recognition;automatic speech;outcomes distant microphone;distant microphone automatic;vocabulary stationary talker;speech separation;chime challenge;recognition asr;2nd chime challenge;distant microphone;chime speech;talker medium vocabulary;microphone;asr remains challenging;recognition asr remains;microphone automatic;asr systems;stationary talker;talker;recognition challenge overview;tracks small vocabulary;performance asr;moving talker", "pdf_keywords": ""}, "1756376bf7cf0d0a7bec881d663b57907a361ecf": {"ta_keywords": "incremental editing structured;modeling tree edits;editing structured;edits incremental tree;incremental editing;model incremental editing;tree edits;tree edits directly;editing structured data;novel edit encoder;proposed edit encoder;learning represent edits;edit encoder learning;learning structural edits;abstract syntax trees;structural edits incremental;data entire editing;syntax trees;editing sequential;syntax trees computer;proposed models editing;approaches generate edited;model single editing;generate edited;models editing processes;structural edits;represent edits;represent edits imitation;generate edited program;editing process formulated", "pdf_keywords": "incremental editing structured;modeling tree edits;editing structured;proposed edit encoder;proposed tree editor;edits incremental tree;incremental editing;novel edit encoder;editing structured data;learning represent edits;tree edits;tree edits directly;model incremental editing;learning structural edits;tree editor;edit encoder learning;improved program editing;structural edits incremental;code edit datasets;represent edits performed;represent edits;structural edits;program editing;tree editor source;new edit encoder;generate edited;represent edits imitation;program editing accuracy;encoder editor;edit encoder editor"}, "5eba3e525056cac6112cf0b13b62d86ba66661d9": {"ta_keywords": "active learning al;tags active learning;active learning;training instances annotating;minimize annotation cost;annotating;experimentation aforementioned languages;minimize annotation;samples minimize annotation;annotation;annotating instances;learning al;selects examples closely;view training analysis;uncertain representative training;instances annotating;typologically diverse languages;predictions current heuristics;annotation cost;representative training instances;useful training samples;diverse languages;aforementioned languages;annotation cost present;training analysis;select useful training;learning al uses;closely follow oracle;training instances;annotating instances reduce", "pdf_keywords": "learning speech tagging;languages confusionreducing strategy;diverse languages confusionreducing;languages confusionreducing;speech tagging;tags empirically;typologically diverse languages;output tags empirically;speech tagging aditi;tagging works;tagging;active learning al;tagging works reducing;learning al;transfer related languages;diverse languages;tags empirically demonstrate;learning al uses;confusion active learning;related languages;resource pos tagging;pos tagging works;pos tagging;languages;cross lingual;confusion output tags;lingual;1language technologies;reducing confusion active;active learning speech"}, "84bc74d875e748aa0f11ac0c5e3000b16484b053": {"ta_keywords": "resilience systems adversarial;systems adversarial evaluation;adversarial;adversarial evaluation present;adversarial evaluation;systems adversarial;simple adversarial;shared task fever2;robust fact checking;neural networks submissions;adversarial attacks;simple adversarial attacks;adversarial attacks systems;fact checking models;task fever2;task fever2 explores;fever2 explores resilience;building robust fact;collection simple adversarial;deep neural;fever shared task;models understand language;natural language inference;language inference task;fact checking;language inference;verification shared task;fact extraction verification;use deep neural;participants use deep", "pdf_keywords": "shared task adversarial;task adversarial;task adversarial attacks;shared task fever2;fever2 shared task;systems adversarial evaluation;adversarial attacks fact;systems adversarial;adversarial;task fever2;resilience systems adversarial;adversarial evaluation;simple adversarial;task fever2 explores;adversarial evaluation 28;fever shared task;fever2 shared;adversarial attacks;simple adversarial attacks;adversarial attacks systems;neural semantic matching;fever2 explores resilience;fever2;decomposable attention model;shared task attack;fever shared;attacks fact extraction;collection simple adversarial;task attack;decomposable attention"}, "ecab8208e5182d4b3b0d6183928e816301d2366d": {"ta_keywords": "net regularization sparse;efficiently training linear;elastic net regularization;regularization sparse;stochastic gradient descent;net regularization;regression classifier elastic;classifier elastic net;regularization sparse linear;decreasing learning rates;gradient descent sgd;efficient elastic net;regularization;classifier elastic;efficiently training;learning rates;gradient descent;efficient elastic;logistic regression classifier;training linear;fixed decreasing learning;descent sgd forward;descent sgd;needed efficient elastic;learning rates derive;sparse linear models;stochastic updates;net reg ularization;features lazily;bag words dataset", "pdf_keywords": "net regularization sparse;elastic net regularization;regularization learning rate;ef\ufb01cient training sparse;net regularization learning;regularization sparse;training sparse linear;training sparse;regularization sparse linear;net regularization paper;net regularization;regularization learning;elastic net regularizers;fast training linear;net regularization 2000;regularization paper;regularization;sparse linear models;models elastic net;net regularizers;regularization 2000 times;regularization paper builds;regularization 2000;training linear models;net regularizers con\ufb01rm;training linear;ef\ufb01cient elastic net;sparse linear;algorithms fast training;learning rate"}, "02cc92287c6614b6a2aa982007471f16b3450013": {"ta_keywords": "natural language actions;language actions plans;sequences natural language;natural language generation;utterances action sequences;mappings natural language;language actions goals;learning planning;planning policies learning;learning planning policies;natural language;learning mappings action;natural language utterances;language learning planning;grounded language;communication grounded language;language actions;actions plans;language generation;grounded language acquisition;sequences understanding learning;actions plans introduce;action sequences natural;policies learning mappings;language utterances action;policies learning;action sequences understanding;learning mappings natural;planning;utterances action", "pdf_keywords": ""}, "ddc502b6c0d08fefe5b77639e4737cd8c7bce25c": {"ta_keywords": "structure html documents;structure html;html documents improves;web pages structure;html documents;html;wrapper web;wrapper web page;wrapper extraction program;information structure recognition;types structure html;maintaining wrapper web;wrapper extraction;pages structure;structure recognition methods;coded wrapper extraction;pages structure ranked;structure recognition;textual similarity;textual similarity developed;information retrieval;meaningful structure used;examples maintaining wrapper;wrapper examples maintaining;meaningful structure;web pages;notion textual similarity;purpose methods recognizing;learn wrapper examples;similarity developed information", "pdf_keywords": ""}, "ab42ad9698386cc15a30a8c7885fa82b260f537b": {"ta_keywords": "mixture models parking;parking demand quantified;parking demand data;models parking demand;similar parking demand;parking demand;curbside parking demand;parking demand propose;parking performance based;demand quantified spatial;gaussian mixture models;curbside parking;parking;zones similar parking;parking performance;models parking;search parking performance;cruising search parking;gaussian mixture model;properties curbside parking;search parking;develop gaussian mixture;similar parking;gaussian mixture;attention gaussian mixture;mixture models;congestion caused drivers;pricing policies specifically;mixture model based;pricing policies", "pdf_keywords": ""}, "3abcd0ffc54c3a16c9dc5e5d3ea59eaa43070127": {"ta_keywords": "designing machine learning;machine learning algorithm;machine learning algorithms;machine learning propose;algorithms simplifies safe;learning algorithm design;standard machine learning;machine learning approaches;using machine learning;machine learning;learning algorithms ubiquitous;learning algorithms;learning algorithm;999 machine learning;application machine learning;user designer algorithm;algorithm design burden;designer intelligent machines;user designer intelligent;algorithm design framework;algorithms ubiquitous;learning algorithms simplifies;algorithm computer;algorithm computer scientist;algorithm standard machine;designer algorithm;algorithm design;user algorithm computer;user algorithm;algorithms", "pdf_keywords": ""}, "37241cdc693b9c2daf49557f18c1ad6a15247239": {"ta_keywords": "document image binarization;document binarization;present document binarization;document binarization scheme;image binarization;state art binarization;binarization;color document images;niblack thresholding method;thresholding;art binarization;color document image;binarizing;binarization scheme;niblack thresholding;thresholding method present;thresholding method;popular niblack thresholding;binarizing range;consistently binarizing;binarization scheme intended;based segmentation applied;art binarization solutions;consistently binarizing range;based segmentation;binarization solutions;segmentation;algorithm based segmentation;document images;document images proposed", "pdf_keywords": ""}, "f66a17836380c0c79c1b42a9219cf8fde6524287": {"ta_keywords": "software questions corpus;question answering;questions corpus;modern question answering;question answering qa;answering qa systems;questions corpus tagged;answering cloze style;entity labeled corpus;knowledge form corpus;text semi structured;answering cloze;corpora sources answers;corpus entity;corpus entity labeled;corpus tagged posts;semi structured knowledge;text corpora;text corpora sources;qa using text;entity labeled documents;unstructured text corpora;answering qa;rnn language model;software questions using;corpora sources;knowledge bases;corpus tagged;rnn language;knowledge bases kbs", "pdf_keywords": ""}, "a817785f0100f3fadc5c1203974d151d5b093310": {"ta_keywords": "java based parallel;implement parallel programs;virtual machines programs;communication performance java;performance java based;parallel programs;parallel virtual machines;based parallel virtual;pvm libraries javapvm;libraries parallel virtual;java language;parallel virtual machine;performance java;pvm implemented code;parallel programs multiple;java based;virtual machines;libraries javapvm java;libraries javapvm;javapvm java;java;javapvm;libraries parallel;implement parallel;implemented code java;pvm implemented;javapvm java code;java code performs;parallel virtual;java language ers", "pdf_keywords": ""}, "9336a2ff833d0b4bc914e2282ad04e19d27bc2be": {"ta_keywords": "substation grounding;substation grounding impedance;220kv substation grounding;grounding grid 220kv;grounding grid parameters;grounding grid;comparison grounding grid;grounding grid affect;copper grounding grid;grid 220kv substation;injected grounding grid;grounding grid calculated;grounding grid cause;connected ground grid;ground grid;grounding impedance ground;ground grid locally;grounding impedance;grid cause ground;220kv substation;points grounding grid;impedance ground potential;substation;ground potential difference;equipment comparison grounding;steel intelligent substation;substation shell secondary;ground potential;aiming grounding grid;substation shell", "pdf_keywords": ""}, "4d5f9a0aba65ba6294c543ba5e6108e6d690f133": {"ta_keywords": "supervised domain adaptation;extracting protein mentions;domain adaptation;labeled protein mentions;domain data abstracts;semi supervised domain;limited annotated data;protein mentions academic;supplement limited annotated;annotated data;supervised domain;annotated data available;protein mentions target;abstracts labeled protein;biology source domain;protein mentions;wholly unlabeled captions;limited annotated;semi supervised;data abstracts labeled;labeled protein;mentions academic publications;labeled examples desired;researchers labeled examples;data abstracts;abstracts labeled;labeled data related;features semi supervised;unlabeled captions;source domains seemingly", "pdf_keywords": ""}, "c69da8266e2f3f67febf22b8f2bf91623346d283": {"ta_keywords": "vaccination prevalent tweets;tweets related vaccination;vaccine related tweets;prevalent tweets covid;tweets posted covid;websites shared vaccine;tweets covid;websites shared tweets;tweeted websites;tweeted websites constituting;tweets covid 19;dissemination health information;20 tweeted websites;prevalent tweets;vaccination including links;screening million tweets;misinformation sharing websites;topic vaccination prevalent;conversations identify tweets;conversations twitter;shared tweets;19 conversations twitter;tweets posted;tweets related;websites common communication;tweets;tweets objective;identify tweets related;twitter;tweets objective informed", "pdf_keywords": ""}, "cd9bfa6266cab4bf4b04c82746a5b650f83b57e4": {"ta_keywords": "non convex optimization;algorithms non convex;convex optimization;stochastic randomized gradient;convex problems minimization;efficiently global minimizer;guarantees optimization algorithms;randomized gradient schemes;optimal stochastic randomized;non convex problems;guarantees optimization;global minimizer exploiting;randomized gradient;convex optimization setting;deal non convexity;minimizer exploiting;optimal stochastic;general non convex;convex optimization application;optimization algorithms non;performance guarantees optimization;non convexity;minimizer exploiting structure;quasi convex functions;neural networks optimization;analysis optimal stochastic;global minimizer;non convex;minimization alpha weakly;quasi convex", "pdf_keywords": ""}, "d385d8563192569b229bde762fcd4d57ce2b3ee2": {"ta_keywords": "automatically generating definitions;generating definitions technical;learn definitions software;definitions software entities;definitions software;specifically learn definitions;definitions technical terms;learn definitions;text technical domain;domain specific terms;word occurrence ontological;technical terms reading;definitions technical;language model based;entities large corpus;knowledge domain ask;software entities;definitions train language;language model incorporate;knowledge domain;definition generation way;ontological category information;comings language model;generating definitions;terms reading text;occurrence ontological;language model;text technical;ontological;domain specifically learn", "pdf_keywords": ""}, "63d7e40da7f0d37308b8e97fca4a14a26a6b52ea": {"ta_keywords": "data quality ai;incentivizing data excellence;data practices high;data excellence;data cascades high;incentivizing data;ai ml practices;ai models increasingly;undervalue data quality;quality ai models;evidence data cascades;practices undervalue data;data quality;data practices;usa data quality;quality ai;designing incentivizing data;data cascades;ai resulting safer;high stakes ai;evidence data;ai heightened;stakes ai interviews;empirical evidence data;undervalue data;ai heightened downstream;report data practices;data quality carries;ai;data", "pdf_keywords": "incentivizing data excellence;data practices challenges;data practices;data practices high;data excellence;incentivizing data;designing incentivizing data;study data practices;citizen ai;ai practitioners;ai models increasingly;ai practitioners india;ai;data;ai resulting safer;data work;challenges 53 ai;stakes ai interviews;citizen ai resulting;high stakes ai;report data practices;health conservation;qualitative study data;like health conservation;data excellence class;ai models;class citizen ai;ai interviews;data work data;ai interviews 53"}, "7707af52b3e19bfd3fc07c2be5aed044e5d7953a": {"ta_keywords": "trained persistent homology;homology based loss;road networks neuronal;topological loss;networks neuronal;topological loss functions;networks trained persistent;images neuronal;persistent homology based;deep networks segment;non topological loss;networks neuronal processes;networks trained;persistent homology;reconstructions road networks;use persistent homology;neuronal processes microscopy;images neuronal processes;preserve connectivity;deep networks delineate;training deep;aerial images neuronal;topology data;homology based;persistent homology ph;networks segment;neuronal processes preserve;networks;preserve connectivity originals;delineate road networks", "pdf_keywords": "trained persistent homology;homology train deep;homology improved locality;persistent homology train;persistent homology improved;persistent homology based;homology based loss;homology train;using persistent homology;use persistent homology;introduce persistent homology;persistent homology;persistent homology application;homology based;homology improved;deep networks delineate;persistent homology ph;improved locality;homology application characterizing;improved locality information;homology;deep networks segment;homology application;locality information effective;networks trained persistent;training deep;networks delineate curvilinear;networks neuronal;locality information;images neuronal"}, "f74ccbc8988b7f0b847c480d4e8bea3082f4f931": {"ta_keywords": "efficient deep reinforcement;deep reinforcement learning;deep reinforcement;gan simulated;conditional gan simulated;gan simulated state;generative adversarial tree;trained generative;trained generative model;monte carlo tree;reinforcement learning drl;conditional gan;reinforcement learning;predictor conditional gan;adversarial tree search;policy atari game;using trained generative;carlo tree search;generative model tree;sample efficient deep;adversarial tree;generative adversarial;gan;policy atari;best policy atari;generative model;train reward predictor;atari game;atari game pong;generative", "pdf_keywords": ""}, "3f5b7fcb6fc50ba80318ab959f3d63253cd0ef6b": {"ta_keywords": "acoustic event detection;events classification conventional;events classification;interdependence events classification;acoustic event;event detection aed;detection aed classifier;proposes acoustic event;aed classifier chains;aed classifier;event detection;multiple binary classifiers;classifier chains new;classification conventional aed;neural network baseline;binary classifiers;classifier chains;new classifier based;classifier based probabilistic;chains new classifier;classifiers linear layer;binary classifiers linear;real recording dataset;new classifier;classifier;recording dataset proposed;recurrent neural network;classifiers;detection aed;classifier based", "pdf_keywords": "acoustic event detection;rule acoustic event;acoustic event;event detection aed;event detection classi\ufb01er;proposes acoustic event;detection aed classi\ufb01er;event detection;binary detection event;detection classi\ufb01er chains;detection classi\ufb01er;detection event proposed;detection event;iterative binary detection;detection aed;aed classi\ufb01er;rule acoustic;events classi\ufb01cation;proposed aed classi\ufb01er;chain rule acoustic;events classi\ufb01cation conventional;binary detection;interdependence events classi\ufb01cation;aed classi\ufb01er chains;convolutional recurrent neural;convolutional recurrent;gated recurrent unit;multiple binary classi\ufb01ers;gated recurrent;recurrent neural network"}, "e6beab7c192d7fb04c8bfb0886464fd719cd3421": {"ta_keywords": "confidence predicting accuracy;predicting accuracy;target domain accuracy;method learns threshold;learns threshold model;learns threshold;domain accuracy;general identifying accuracy;model confidence predicting;predicting target domain;machine learning deployments;threshold model confidence;methods predicting target;thresholded confidence;predicting accuracy fraction;confidence predicting;practical method learns;domain accuracy using;identifying accuracy just;methods predicting;method learns;confidence exceeds threshold;accuracy using labeled;identifying accuracy;thresholded confidence atc;predicting target;accuracy;machine learning;model confidence;accuracy fraction unlabeled", "pdf_keywords": ""}, "d25c4bf23b4b951f2417e4a8a44574c99608e9d7": {"ta_keywords": "sinusoidal wave wavelets;wave wavelets;seismic time frequency;wavelet transform;frequency analysis seismic;wavelet transform cwt;frequency feature seismic;wavelets;linear chirplet transform;robust seismic time;time frequency analysis;feature seismic signals;chirplet transform;continuous wavelet transform;robust time frequency;wavelet;time frequency feature;chirplet transform lct;time frequency representation;wavelets paper introduces;seismic signals algorithm;seismic signals;seismic time;chirplet transform glct;analysis seismic data;linear chirplet;time frequency spectrum;chirplet atom time;wave wavelets paper;seismic data using", "pdf_keywords": ""}, "4a160efbe80c38cd5eb2f92c7c095b49b113397d": {"ta_keywords": "code generation retrieval;generation code retrieval;code retrieval;code retrieval functionality;generated code developer;generation retrieval developer;better code generation;code retrieval vice;retrieval developer assistants;code generation models;prefer code generation;code generation;python programming tasks;code developer;based code generation;code developer written;developer written code;developer productivity accuracy;developers prefer code;retrieval developer;learning based code;assistants demonstrates developers;developer productivity;improve developer productivity;developer assistants demonstrates;demonstrates developers;developer assistants;code generation code;14 python programming;develop plugin pycharm", "pdf_keywords": "ide developer assistants;ide code generation;generation code retrieval;code generation;ide developer;retrieval ide developer;pycharm ide implements;code generation natural;pycharm ide;python programming tasks;natural language code;plugin pycharm ide;code generation code;great software development;code retrieval;inside pycharm ide;code retrieval functionality;developer assistants;python programming;hybrid code generation;software development;code nl2code generation;generation retrieval ide;pycharm ide asking;software development involves;generation natural language;programming tasks;programming;nl2code generation retrieval;develop plugin pycharm"}, "e7ce1b01d2928514710bba044ac2af758c975d99": {"ta_keywords": "multicommodity selfish routing;selfish routing game;selfish routing;equilibrium quality multicommodity;commodity routing networks;congestion costs multiplicative;multi commodity routing;routing game;uncertainty multi commodity;quality multicommodity selfish;network costs;routing game types;commodity routing;favorable network conditions;estimates congestion costs;equilibrium quality optimism;network costs significantly;equilibrium quality;congestion costs;multicommodity selfish;estimates congestion;favorable network;routing networks;uncertainty leads favorable;type estimates congestion;network conditions;perception network costs;uncertainty user type;routing;costs multiplicative", "pdf_keywords": "transportation networks consider;transportation networks parking;networks parking routing;commodity routing game;transportation networks;urban transportation networks;favorable network conditions;parking routing users;networks parking;realistic urban network;parking routing;constraining network;routing game;routing game studied;urban network topologies;constraining network topology;commodity routing;parking urban transportation;social cost equilibrium;multi commodity routing;network conditions;networks long uncertainty;cost equilibrium;networks consider realistic;network conditions speci\ufb01cally;favorable network;congestion;networks consider;speci\ufb01cally sli networks;tackling congestion"}, "caf40157a7a1d72ae3a6946169c992d8c973b743": {"ta_keywords": "gingival graft coronally;free gingival graft;gingival inflammation areas;gingival graft;gingival inflammation;recession gingival inflammation;practice periodontics clinicians;attached gingiva;periodontics clinicians faced;esthetics presence gingival;coverage free gingival;periodontium providing therapy;periodontics clinicians;current practice periodontics;gingiva defined;attached gingiva defined;free gingival;gingiva defined mucogingival;gingival recession gingival;practice periodontics;recession gingival;gingival recession;gingiva;periodontics;gingival;band attached gingiva;present periodontium;periodontium;problems present periodontium;periodontium providing", "pdf_keywords": ""}, "1b57ffe73ae95f339015c174ec574b59f99ea553": {"ta_keywords": "guarantee cloak optimizations;cloak optimizations reduce;cloak optimizations;reduces mutual information;receiving machine learning;learning services cloud;machine learning services;cloak reduces mutual;based perturbation maximization;representations sent cloak;perturbation maximization;necessary target prediction;mutual information;data sifted representations;theoretically guarantee cloak;cloak greatly;bound mutual information;results cloak;features theoretically guarantee;mutual information input;guarantee cloak;target prediction;services cloud;sifted representations;cloud;cloak;results cloak reduces;input sifted representations;mutual information mi;learning services", "pdf_keywords": ""}, "c6048cd0b1368be0e62633ef723f9d691323102c": {"ta_keywords": "mixture model speaker;model speaker clustering;modeling speech gmm;scale mixture models;mixture models;speaker modeling speech;speaker clustering;gaussian mixture model;mixture model multi;speaker modeling;mixture model gmm;modeling speech;noise mixture gmms;scale mixture model;speech gmm;intra speaker dynamics;mixture model;mixture models assume;speaker clustering noisy;speaker dynamics derived;inter speaker dynamics;speaker dynamics;multi scale mixture;mixture gmms mogmms;mixture gmms;gaussian mixture;mixture represented gaussian;represented gaussian mixture;noise mixture;gibbs sampling based", "pdf_keywords": ""}, "40e292d16168fcb8ac87c20682b827ad17a999dd": {"ta_keywords": "app usage representation;personalized mobile apps;app usage;spatio temporal app;temporal app usage;semantic aware spatio;mobile apps;apps;usage representation graph;app;proliferation personalized mobile;temporal app;personalized mobile;aware spatio temporal;aware spatio;apps poses;graph convolutional;mobile apps poses;representation graph convolutional;graph convolutional network;semantic aware;mobile;usage representation;apps poses pressing;spatio temporal;improvement semantic aware;experience improvement semantic;user experience;user experience improvement;need user experience", "pdf_keywords": ""}, "6332d5bb0e6af89471ffc6157e3816c029b3ae83": {"ta_keywords": "parameters durability pemfc;durability pemfc;pemfc;operating parameters durability;dynamic load cycle;load cycle operating;parameters durability;load cycle;durability;cycle operating parameters;dynamic load;influence dynamic load;cycle operating;operating parameters;cycle;dynamic;load;experimental study;study influence dynamic;influence dynamic;parameters;operating;experimental study influence;experimental;study influence;study;influence", "pdf_keywords": ""}, "8b3c0dd95167d4d63161038493a691ee5cdc76b3": {"ta_keywords": "monolingual sentence matching;sentence matching text;similarity sentences limitation;similarity sentences;sentence matching;matching text simplification;model similarity sentences;aligning monolingual sentence;text simplification experiments;parallel corpora;performance aligning monolingual;parallel corpora model;matching text;aligning monolingual;text simplification;available parallel corpora;similarity;corpora;corpora model;corpora model trained;model similarity;simplification experiments rescoring;structure model similarity;sentences;monolingual sentence;introduce convolutional neural;sentences limitation;sentences limitation available;convolutional neural network;high performance aligning", "pdf_keywords": "convolution word embeddings;trained word embeddings;trained word vectors;similarity sentences convolutional;sentence level embedding;convolutional latent semantic;word embeddings;sentences convolutional latent;semantic vector representations;word embeddings obtained;sentences convolutional;semantic similarity scores;semantic vector representation;neural language model;structural semantic similarity;generating latent semantic;word embeddings \ufb01ne;rich contextual semantic;semantic similarity;wordnet structural semantic;latent semantic vector;dimensional semantic vector;word vectors;similarity sentences;semantic vector;low dimensional semantic;model similarity sentences;unsupervised neural language;contextual semantic;sentence use convolutional"}, "e4de1009eb7b3524bf7d19bdcebced80035a47cf": {"ta_keywords": "neighbor discovery iot;neighbor discovery protocol;asynchronous neighbor discovery;noncoherent neighbor discovery;discovery iot;asynchronous noncoherent neighbor;neighbor discovery;iot using sparse;asynchronous neighbor;based asynchronous neighbor;sparse graph codes;discovery iot using;neighbors1 network nodes;neighbors1 network;things iot asynchronous;iot asynchronous noncoherent;active neighbors1 network;noncoherent neighbor;discovery protocol;iot asynchronous;graph codes proposed;discovery protocol internet;active neighbors1;network nodes codeword;neighbor;neighbors1;set active neighbors1;graph codes;protocol internet things;sparse graph", "pdf_keywords": ""}, "dbb159b288930c6be32c2d5b91373ca1e341e633": {"ta_keywords": "speech feature enhancement;stereo based speech;noisy speech feature;based speech feature;speech separation recognition;enhancement using dictionary;speech feature;speech feature representation;noisy speech recognition;chime speech separation;based feature enhancement;feature enhancement;dictionary learning;feature enhancement using;dictionary learning instead;dictionary learning effectiveness;speech recognition;alternative noisy speech;stereo based feature;vocabulary noisy speech;using dictionary learning;feature representation stereo;speech separation;speech recognition task;sparse weight vectors;noisy speech;separation recognition;separation recognition challenge;chime speech;representation stereo based", "pdf_keywords": ""}, "2f3ec666ba50c6a9ce74abad6a5127ea38a05bca": {"ta_keywords": "erasure code distributed;efficient erasure codes;erasure codes efficient;erasure codes;traditional erasure codes;erasure codes available;code distributed storage;erasure codes used;erasure code;node repair erasure;repair erasure code;repair erasure decoding;efficient erasure;node failures codes;repair erasure;erasure decoding;variety efficient erasure;distributed storage;enable traditional erasure;node repair;codes used retaining;distributed storage given;repair error detection;storage space repair;nodes repair;erasure decoding constituent;minimization storage;failures codes perform;coding schemes;simultaneous minimization storage", "pdf_keywords": "erasure code distributed;repair erasure decoding;node repair erasure;erasure codes ef\ufb01cient;erasure codes;code distributed storage;abstract erasure codes;erasure code;reduced erasure decoding;erasure decoding;repair erasure code;node repair;node repair reduced;erasure performance decoding;reconstruction node repair;decoding algorithms codes;codes decoding algorithms;erasure decoding constituent;repair reduced erasure;replacement node;data reconstruction node;problem node repair;codes decoding;distributed storage;code distributed;node repair error;process node repair;codes twin code;enabling node repair;repair erasure"}, "8f2182846d5d4cfbc216b5e4c00411e021dc4776": {"ta_keywords": "recurrent neural networks;lstm;short term memory;networks rnns particularly;lstms recognize patterns;learning sequence data;measurements recurrent neural;networks rnns;clinical measurements recurrent;rnns particularly;rnns;term memory lstm;lstms;memory lstm;rnns particularly using;recurrent neural;sequence data patient;neural networks rnns;lstms recognize;models learning sequence;classification diagnoses training;learning sequence;classify 128 diagnoses;time series clinical;ability lstms recognize;diagnoses training model;sequence step trained;measurements recurrent;classification diagnoses;lstm hidden", "pdf_keywords": "learning diagnose lstm;recurrent neural networks;diagnose lstm recurrent;lstms recognize patterns;effectiveness simple lstm;lstm recurrent neural;diagnose lstm;learning diagnose;lstm;lstm recurrent;recurrent neural;classify 128 diagnoses;ability lstms recognize;time series clinical;simple lstm;modeling clinical data;lstms recognize;2017 learning diagnose;lstms;clinical medical data;diagnoses training model;clinical data;lstm network modeling;evaluate ability lstms;lstm network;diagnoses training;neural networks;classi\ufb01cation diagnoses training;ability lstms;medical data"}, "bcffee102a99f726ddfe765906babb01b8226269": {"ta_keywords": "mutation tcr therapy;ras mutation tcr;targeting ras mutation;mutation tcr;tcr therapy;ras mutation;mutation;tcr;1499 targeting ras;targeting ras;ras;abstract 1499 targeting;therapy;1499 targeting;targeting;abstract 1499;1499;abstract", "pdf_keywords": ""}, "322ef476e90a487c8f9797bece7799b69af9e5c1": {"ta_keywords": "coded matrix multiplication;coded computation scheme;coded matrix;propose coded computation;new coded computation;coded computation;dimensional coded matrix;coded computation involving;product coded matrix;ldpc codes achieve;node coded computation;study coded computation;codes computing;coded computation framework;large matrix multiplication;codes computing product;high dimensional coded;mds codes computing;separable mds codes;multiplication matrices large;ldpc codes;codes achieve linear;compute node coded;coded scheme achieves;decoding complexity allowing;mds coded scheme;coded scheme;decoding complexity;matrix multiplication propose;matrix multiplication high", "pdf_keywords": ""}, "822395760906f4940df68aa33925b6bf9123bac2": {"ta_keywords": "competitions popular tool;data science community;data data science;data science;tool data science;competitions;question arises nips;competitions popular;nips;data science problem;popular tool data;arises nips;platforms like codalab;open source platforms;data;data data;arises nips add;nips add rich;source platforms like;source platforms;science community solve;platforms;spur new research;tool data;platforms like;like codalab connect;codalab connect people;open source;people data;nips add", "pdf_keywords": ""}, "24d28783f6061bd1e91fb60417ac8b3646305a49": {"ta_keywords": "automatically sequencing component;information extraction systems;information extraction;information components;sequencing component;automatically sequencing;control automatically sequencing;blackboard communication components;sequencing component level;like classification extraction;scale information extraction;large scale information;learned components;components large scale;information components flexibly;tasks like classification;containing learned components;extraction systems described;extraction feature computation;actions components infrastructure;architecture large scale;component level tasks;extraction systems;components declarative control;classification extraction;components infrastructure;components infrastructure components;blackboard communication;share information components;declarative control automatically", "pdf_keywords": ""}, "c8648d04f52e49167d1a4443a4830709cf3331ff": {"ta_keywords": "stackelberg strategies security;stackelberg strategies games;optimal stackelberg strategies;computing game theoretic;stackelberg strategies;strategy called stackelberg;algorithms computing game;algorithms assume defender;game theoretic solutions;computing optimal stackelberg;allocation games;security resource allocation;optimal stackelberg;resource allocation games;compute optimal stackelberg;strategies security;computing game;allocation games paper;game theoretic;defender security;strategies games;strategies security resource;complexity computing optimal;defender security personnel;assume defender security;games showing polynomial;real world security;allocation;airport algorithms;computing optimal", "pdf_keywords": "solving stackelberg games;stackelberg strategies games;optimal stackelberg strategies;defender polynomial time;stackelberg strategy guaranteed;optimal stackelberg strategy;stackelberg games;stackelberg strategies;stackelberg games pure;stackelberg strategy;complexity solving stackelberg;strategy defender polynomial;games studied optimal;large strategy spaces;studied optimal stackelberg;optimal stackelberg;strategies correspond allocations;games showing polynomial;nash equilibrium cumbersome;guaranteed nash equilibrium;computational perspective nash;exponentially large strategy;polynomial time cases;compute optimal stackelberg;games pure strategies;strategy spaces;programs polynomial size;equilibrium player games;equilibrium strategy defender;linear programs polynomial"}, "651468a69da74dab716cebbd179a5cbb8e672c14": {"ta_keywords": "learning suboptimal demonstrations;demonstrations rarely optimal;self imitation learning;imitation learning sil;imitation learning;learn demonstrations noisy;learn demonstrations;silfd learn demonstrations;demonstrations training;demonstrations training additional;demonstrations naturally;suboptimal demonstrations naturally;achieved reinforcement learning;reinforcement learning rl;influence demonstrations training;extend self imitation;suboptimal demonstrations;reinforcement learning;reinforcement;demonstrations noisy;environments sparse rewards;self imitation;quality demonstrations rarely;demonstrations naturally benefits;sophisticated exploration;demonstrations noisy far;control influence demonstrations;achieved reinforcement;demonstrations rarely;imitation", "pdf_keywords": "learning demonstrations silfd;learning demonstrations;imitation learning demonstrations;learn demonstrations noisy;silfd learn demonstrations;learn demonstrations;imitation learning;noisy suboptimal demonstrations;self imitation learning;scheduling in\ufb02uence demonstrations;demonstrations training additional;suboptimal demonstrations explore;demonstrations training;noisy demonstrations;in\ufb02uence demonstrations training;suboptimal noisy demonstrations;empirically silfd learn;expert demonstrations;expert demonstrations alleviating;environments especially demonstrations;demonstrations noisy;demonstrations explore properties;demonstrations silfd;demonstrations highly suboptimal;noisy demonstrations experimental;demonstrations explore;suboptimal demonstrations;demonstrations noisy far;demonstrations highly;especially demonstrations highly"}, "1cb7015c0a8015c65844876459809ecac917ec02": {"ta_keywords": "multi channel acoustic;channel acoustic model;speech recognition single;channel acoustic;input multi channel;multi channel trained;trained multi channel;talker speech recognition;channel trained multi;multi channel input;speech enhancement se;acoustic model single;speech enhancement;single channel multi;single multi channel;single channel trained;multi talker speech;multi talker;channel multi;multi channel;tal speech enhancement;model single channel;channel multi channel;channel input;speech recognition;acoustic modeling;single channel;acoustic model;distant multi talker;acoustic modeling distant", "pdf_keywords": ""}, "899055ad2f0863cf1931c41f04da8b1dd7382607": {"ta_keywords": "hba1c lipid variability;lipid variability;consistent lipid variability;variability lipid profiles;lipid variability represented;variability lipid;lipid profiles risk;visit variability lipid;lipid variability methodswe;lipid profiles patients;hba1c variability ldl;lipoprotein cholesterol ldl;lipoprotein cholesterol;lipoprotein cholesterol non;risk factors cardiovascular;coronary intervention pci;density lipoprotein cholesterol;variability ldl;coronary intervention retrospective;hba1c lipid;cholesterol ldl;variability ldl non;lipid profiles;low density lipoprotein;triglyceride tg significantly;cholesterol ldl non;cholesterol non hdl;relationship hba1c lipid;hba1c variability;factor variability ldl", "pdf_keywords": ""}, "173c73077a421680f12576524e85dff4b890c17e": {"ta_keywords": "wasserstein distance sample;projected wasserstein distance;wasserstein distance;wasserstein distance present;distance projected distributions;kernel projected wasserstein;projected wasserstein;projected distributions sample;test kernel projected;projected distributions;distance sample;sample test kernel;wasserstein;distance sample test;distance projected;quantification empirical estimates;kernel projected;empirical estimates;develop kernel projected;uncertainty quantification empirical;asymptotic uncertainty quantification;statistics machine learning;computing distance;computing distance function;non asymptotic uncertainty;maximizes distance projected;test kernel;asymptotic uncertainty;distributions sample;algorithms computing distance", "pdf_keywords": "projected wasserstein distance;wasserstein distance sample;kernel projected wasserstein;test kernel projected;distance projected distributions;wasserstein distance;wasserstein distance jie;distance distributions projection;projected wasserstein;wasserstein kpw distance;projected wasserstein kpw;based kernel projected;sample test kernel;develop kernel projected;distributions projection;kernel projected;projected distributions;distributions projection work;projected distributions numerical;based reproducing kernel;test kernel;statistic aiming highdimensional;distance distributions;reproducing kernel hilbert;probability distance distributions;nonlinear dimensionality reduction;distance projected;maximizes distance projected;high dimensional data;reproducing kernel"}, "4ae15dbb068cc962b39dca07d87b22fe5dcd5f6a": {"ta_keywords": "utility privacy tradeoff;grid operations privacy;utility privacy;quantifying utility privacy;privacy tradeoff smart;control consumer privacy;tradeoff smart grid;control monitoring privacy;privacy tradeoff;privacy consumers quantifying;smart grid;smart grid operations;consumer privacy;operations privacy consumers;privacy consumers;privacy metric assumes;monitoring privacy;privacy metric;operations privacy;privacy;monitoring privacy metric;infer private parameter;private parameter;load control consumer;private parameter independent;adversary model provides;ability infer private;adversary model;infer private;consumers quantifying utility", "pdf_keywords": "model user energy;privacy different sampling;model energy consumption;inferential privacy;privacy metric inferential;metric inferential privacy;formal de\ufb01nition privacy;privacy exploits uncertainty;user energy consumption;quantify privacy;sampling policies;tradeoff smart grid;inferential privacy exploits;sampling policies analyze;privacy;model predictive control;smart grid;privacy metric;energy consumption patterns;privacy allows quantify;de\ufb01nition privacy;new privacy metric;smart grid operations;new privacy;user energy;different sampling policies;quantify privacy different;de\ufb01nition privacy allows;introduce new privacy;allows quantify privacy"}, "513552a56668279d6cd0857a4399fe8a63d92145": {"ta_keywords": "\u6587\u8108\u3092\u8003\u616e\u3057\u305f\u78ba\u7387\u7684\u30e2\u30c7\u30eb\u306b\u3088\u308b\u8a71\u3057\u8a00\u8449\u306e\u6574\u5f62 \u97f3\u58f0\u8a00\u8a9e\u60c5\u5831\u51e6\u7406 slp;\u6587\u8108\u3092\u8003\u616e\u3057\u305f\u78ba\u7387\u7684\u30e2\u30c7\u30eb\u306b\u3088\u308b\u8a71\u3057\u8a00\u8449\u306e\u6574\u5f62 \u97f3\u58f0\u8a00\u8a9e\u60c5\u5831\u51e6\u7406;\u6587\u8108\u3092\u8003\u616e\u3057\u305f\u78ba\u7387\u7684\u30e2\u30c7\u30eb\u306b\u3088\u308b\u8a71\u3057\u8a00\u8449\u306e\u6574\u5f62;\u97f3\u58f0\u8a00\u8a9e\u60c5\u5831\u51e6\u7406 slp vol;\u97f3\u58f0\u8a00\u8a9e\u60c5\u5831\u51e6\u7406 slp;\u97f3\u58f0\u8a00\u8a9e\u60c5\u5831\u51e6\u7406;slp vol 2009;slp;slp 79;vol 2009 slp;2009 slp;slp vol;2009 slp 79;vol 2009;79;vol;2009", "pdf_keywords": ""}, "8113f05360c6483e52b3e261fc9efce671e0aaa6": {"ta_keywords": "separation speaker diarization;speech separation diarization;speech separation speaker;integration speech separation;speech separation;overlapping speech diarization;speaker diarization automatic;dealing speech separation;multi speaker meetings;meeting transcription automatic;speaker speech recognition;speaker meetings;speaker diarization;speaker meetings description;recognition multi speaker;diarization automatic speech;separation speaker;separation diarization recognition;speech diarization asr;overlapping speech;speech recognition asr;speech diarization;multi speaker speech;automatic speech;speech recognition unsegmented;problem overlapping speech;meeting transcription;automatic speech recognition;speech recognition;analysis multi speaker", "pdf_keywords": "utterances integrates pipeline;trained separation diarization;single speaker utterances;speaker utterances integrates;separation diarization recognition;speaker utterances;diarization recognition components;diarization recognition;utterances integrates;recognition components;asr model trained;asr best separation;utterances;performs separation diarization;diarization recognition order;state art asr;independently trained separation;separation diarization;trained separation;overlapping asr best;recognition components order;combines independently trained;best achieves speaker;chime pipeline;non overlapping asr;recognition order best;speaker attributed wer;achieves speaker;speaker;recognition"}, "29437d98b9e6f45bef7029f3ce1237b8b284464f": {"ta_keywords": "text generation focused;sentences soft templates;text generation;hybrid attention copy;data text generation;improving content fidelity;control writing styles;imitate writing style;existing sentences soft;way stylistic control;attention copy;stylistic control;sentence automatic adaptions;stylistic control using;content fidelity style;writing style;hybrid attention;learns imitate writing;learns weak supervisions;writing styles;content fidelity lacking;sentences soft;stylistic;writing style given;exemplar sentence automatic;includes hybrid attention;content fidelity;control writing;new way stylistic;way stylistic", "pdf_keywords": ""}, "d63edef60d674408819bb015b64b7f42470e151b": {"ta_keywords": "protein binding pocket;molecular generative model;design molecular generative;binding pocket novo;novo molecule design;protein binding;pocket novo molecule;model composition protein;molecules model composition;design molecular;protein binding sites;ligand normal rnn;molecules model;molecule design molecular;composition protein binding;3d information protein;information protein binding;novo molecule;like molecules model;molecule design;structural information protein;ray bound ligand;molecular generative;atoms composing binding;bound ligand;protein environment information;binding pocket descriptor;model conditioned 3d;3d structural information;rnn model", "pdf_keywords": "protein binding pocket;molecular generative model;design molecular generative;novo molecule design;design molecular;abstract novo molecule;molecule design molecular;molecules novo molecule;novo molecule;model composition protein;molecules novo;protein binding;molecule design;molecular generative;protein binding sites;composition protein binding;ligand normal rnn;cn abstract novo;like molecules novo;3d information protein;information protein binding;composition binding pocket;structural information protein;atoms composing binding;binding pocket novel;abstract novo;rnn model;pocket conditional rnn;protein environment information;pocket novel generative"}, "1ddf9d306ae27113f55ea3d4eee12c8441235656": {"ta_keywords": "workshop asian translation;paper translation subtasks;patent translation subtasks;asian translation wat2016;translation results;translation results submitted;translation subtasks patent;translation subtasks;translation subtasks newswire;translation wat2016 including;500 translation results;subtasks patent translation;scientific paper translation;asian translation;paper translation;subtasks 500 translation;patent translation;translation wat2016;3rd workshop asian;translation;workshop asian;500 translation;submitted automatic evaluation;automatic evaluation server;automatic evaluation;mixed domain subtasks;evaluation server;tasks 3rd workshop;overview 3rd workshop;evaluation", "pdf_keywords": ""}, "b03feec6f5b898484fdfdc3cd12f084afbe77036": {"ta_keywords": "rank learning pairwise;pairs rank learning;pairwise preference ranking;rank learning;feature based ranking;preference ranking;based ranking functions;outliers pairwise preference;preference ranking disadvantage;learning pairwise preferences;ranking functions based;pairs rank;ranking;ranking functions;based ranking;pairwise preference framework;based pairwise preference;learning pairwise;pairwise preference;outlying pairs rank;baseline ranker;ranking disadvantage;ranking disadvantage process;linear baseline ranker;meta learning algorithm;labeled document pairs;document pairs algorithm;pairwise preferences introduce;suppressing outliers pairwise;noisy relevance judgment", "pdf_keywords": ""}, "f388c2be45e4415fcb59cf43a3b29463cf7e7940": {"ta_keywords": "fact checking assessment;fact checked journalists;fact checking;journalists verifying claims;fact checking relates;statements fact checked;discuss fact checking;manually journalists verifying;journalists verifying;task fact checking;using statements fact;natural language processing;checked journalists;statements fact;checking assessment truthfulness;fact checked;manually journalists;claims public figures;performed manually journalists;assessment truthfulness claim;natural language;verifying claims public;assess truthfulness;mainstream natural language;need assess truthfulness;checked journalists available;verifying claims;truthfulness claim;assess truthfulness increasing;journalists", "pdf_keywords": ""}, "9352dfd127dcfce8013eb350e0229cc72b9bd203": {"ta_keywords": "convolutional block attention;module attention weights;attention weights;attention module;augmented attention saa;augmented attention;attention augmented wgan;fully convolutional network;encoder decoder network;attention module cbam;self augmented attention;encoder decoder subnet;module attention;attention saa module;block attention module;layer feature maps;attention augmented;saa module attention;convolutional network;self attention augmented;attention weights produced;layer features;attention;scale features reconstruct;layer feature;wise feature representation;improved super resolution;block attention;convolutional network fcn;multi scale features", "pdf_keywords": ""}, "4104d632d1cff0c9314cde344e2b1da06e662c5b": {"ta_keywords": "unpaired speech text;sequence automatic speech;speech text data;using unpaired speech;speech text sequence;asr text speech;speech recognition asr;leverage unpaired speech;automatic speech;text speech;semi supervised sequence;unpaired speech;speech text;supervised sequence sequence;automatic speech recognition;speech recognition;data quantity speech;unsupervised semi supervised;speech tts models;speech text modalities;semi supervised training;text speech tts;asr using unpaired;recognition asr models;quantity speech text;semi supervised;recognition asr;supervised sequence;sequence sequence automatic;speech tts", "pdf_keywords": ""}, "7e43dad7fbae3a7db47adc6b89c76acbd2fb225f": {"ta_keywords": "hierarchical knowledge base;domain hierarchical knowledge;procedures based wikihow;hierarchical knowledge;data hierarchical knowledge;knowledge base;discovering hierarchies procedures;hierarchical knowledge critical;details discovering hierarchies;web data hierarchical;procedures inherently hierarchical;structured web;hierarchies procedures;structured web data;procedure details discovering;discovering hierarchies;semi structured web;hierarchical;hierarchies procedures semi;domain hierarchical;details discovering;open domain hierarchical;instructional articles documenting;data hierarchical;procedures semi structured;knowledge base kb;knowledge critical reasoning;articles documenting steps;inherently hierarchical;reasoning complex procedures", "pdf_keywords": "hierarchical knowledge base;instructional video retrieval;construct hierarchical knowledge;hierarchical knowledge;propose search rerank;domain hierarchical knowledge;search rerank;retrieving instructional videos;procedures based wikihow;tasks retrieving instructional;knowledge base;tasks instructional video;knowledge base procedures;search rerank algorithm;rerank algorithm effectively;effectively construct hierarchical;video retrieval introduction;instructional articles documenting;rerank algorithm;instructional articles;rerank;hierarchical;based wikihow work;retrieving instructional;topics wikihow large;110k instructional articles;instructional videos paper;tasks instructional;video retrieval;wikihow work"}, "15a6c3d32ae1daefba3c4b40146de8efdf16ec8d": {"ta_keywords": "bayesian speech language;bayesian speech;speech language processing;language processing introduction;speech language;language processing;bayesian;processing introduction;speech;language;processing;introduction", "pdf_keywords": ""}, "950c2c041db52c416e49fb0945078f6463c501b8": {"ta_keywords": "utility learning energy;utility learning disaggregation;learning energy disaggregation;estimating consumer utility;disaggregated consumption;iterative control incentive;incentives estimating consumer;disaggregated consumption data;utility learning;data disaggregated consumption;incentive design estimation;energy disaggregation;consumption data disaggregated;designing incentives estimating;incentive design utility;consumer utility function;estimation utility learning;energy consumption patterns;control incentive design;control incentive;consumers revenue decoupling;consumption data utility;modifying energy consumption;incentives estimating;design utility learning;energy disaggregation present;revenue decoupling demand;consumer utility;consumption patterns consumers;based aggregate consumption", "pdf_keywords": "incentives consumers estimating;consumers estimating utility;incentive design estimation;iterative control incentive;incentives consumers;estimate consumer utility;utility learning;consumer utility function;estimating utility functions;energy consumption consumers;estimation utility learning;control incentive;design incentives consumers;estimating utility;consumers model utility;control incentive design;incentives induce desired;incentive design;function choosing incentive;consumer utility;consumer aggregated power;incentives behavior;utility learning disaggregation;desired consumer behavior;function design incentives;incentives behavior modi\ufb01cation;design incentives behavior;utility functions propose;consumers estimating;choosing incentive"}, "f637d061704579531a8b8e03ef6e8331ba117490": {"ta_keywords": "risk estimator adaptivity;risk estimator oracle;oracle estimator propose;estimator adaptivity index;compares risk estimator;relative oracle estimator;bern adaptive estimators;estimator adaptivity;estimator achieve adaptivity;index compares risk;adaptive estimators;adaptive estimators bernoulli;oracle estimator;compares risk;risk estimator;estimator oracle appropriate;estimator adapts;stochastic transitivity introduce;estimator oracle;models oracle knows;models oracle;strong stochastic transitivity;introduce adaptivity index;stochastic transitivity;sub models oracle;adaptivity index compares;achieve adaptivity index;probabilities pairwise comparisons;case risk estimator;computationally efficient estimator", "pdf_keywords": ""}, "7626f73c3b013b5b7bf293c1cc22d2835b6579b3": {"ta_keywords": "based speech synthesis;speech synthesis;speech synthesis using;hmm based speech;improvements hmm based;improvements hmm;based speech;hmm based;rich context models;synthesis using rich;context models;synthesis using;synthesis;speech;using rich context;rich context;models;context;improvements;based;using rich;hmm;using;rich", "pdf_keywords": ""}, "bf0b66e0e328df1df42b075422c8fecdd95736c0": {"ta_keywords": "teachable peer learner;tutor learning;tutor learning students;tutored problem solving;learning tutored problem;cognitive tutor;learning tutored;simstudent teachable peer;learning agent simstudent;teachable peer;peer learner;agent simstudent teachable;peer learning environment;peer learner allows;learning agent;peer learning;comparing cognitive tutor;compared learning tutored;tutor;solving classroom;learning teaching simstudent;artificial peer learning;influence tutor learning;simstudent teachable;tutored problem;learner allows student;learning students;problem solving classroom;learn teaching simstudent;teaching simstudent", "pdf_keywords": ""}, "4ffca5d623950e2396089e7fc1621b4a477436cb": {"ta_keywords": "sentences soft templates;text generation focused;text generation;hybrid attention copy;control writing styles;data text generation;improving content fidelity;existing sentences soft;imitate writing style;way stylistic control;stylistic control;attention copy;stylistic control using;writing style;styles sentence structures;content fidelity style;hybrid attention;sentence automatic adaptions;learns weak supervisions;learns imitate writing;sentences soft;writing styles;stylistic;writing style given;content fidelity lacking;control writing;includes hybrid attention;writing styles sentence;exemplar sentence automatic;way stylistic", "pdf_keywords": ""}, "97e033d79b6aebab1927ab9232afa8268e198481": {"ta_keywords": "codes distributed caching;distributed caching;distributed caching based;caching based video;based distributed caching;distributed caching architected;scalable vod codes;vod codes distributed;caching based;cache communication;cache communication aspects;desirable tradeoffs cache;cache content placement;cache users topology;codes distributed;caching;tradeoffs cache;cache content;content placement cache;cache cache communication;cache;placement cache;tradeoffs cache user;vod based distributed;placement cache users;based video demand;user cache;user cache cache;caching architected;video demand", "pdf_keywords": ""}, "f5ca46585818771e64ee9449c930748fbee35cba": {"ta_keywords": "explainable nlp models;structured explanations;language feedback explanations;interactive refines explanations;natural language feedback;form structured explanations;structured explanations happens;feedback natural language;nlp models reasoning;models reasoning tasks;class explainable nlp;feedback explanations;corrected explanation structures;reasoning tasks;refines explanations;explainable nlp;feedback explanations simply;reasoning tasks support;explanation structures;explanations happens supporting;nlp models;language feedback;refines explanations given;explanations given reasoning;reasoning domains introduce;accuracy defeasible reasoning;defeasible reasoning domains;defeasible reasoning;natural language;explanations simply appending", "pdf_keywords": "defeasible reasoning generated;reasoning generated;generating explanation structure;speci\ufb01c semantic parsing;semantic parsing;semantic parsing schema;graphs defeasible reasoning;task defeasible inference;interactive re\ufb01nes explanations;feedback speci\ufb01c semantic;produced defeasible reasoning;inference defeasible inference;defeasible inference;reasoning using human;feedback natural language;defeasible reasoning using;parsing schema specialized;speci\ufb01c semantic;inference defeasible;defeasible reasoning;defeasible inference defeasible;language approaches feedback;capable generating explanation;reasoning task getting;parsing schema;semantic;parsing;defeasible inference rudinger;reasoning task;inference"}, "97db55b196cf0c768644a392a7e6c79d1c65207e": {"ta_keywords": "speech enhancement frame;speech enhancement overlapped;reverberant speech enhancement;neural speech enhancement;online speech enhancement;speech enhancement;speech enhancement task;speech enhancement systems;enhancement frame deep;overlapped frame prediction;frame online speech;online neural speech;frame deep neural;enhancement overlapped frame;enhancement frame;frame online neural;istft improving frame;stft istft improving;reverberant speech;enhancement overlapped;noisy reverberant speech;frame prediction algorithmic;frame prediction;frame prediction technique;enhancement;frames necessary overlap;improving frame online;neural speech;istft improving;predicting current frame", "pdf_keywords": "speech enhancement frame;speech enhancement overlapped;online speech enhancement;neural speech enhancement;speech enhancement;speech enhancement systems;overlapped frame prediction;enhancement frame deep;frame online speech;online neural speech;frame prediction zhong;enhancement overlapped frame;enhancement frame;frame online neural;frame deep neural;frame prediction technique;frame prediction;improving frame online;enhancement overlapped;stft istft addition;add instead predicting;frames necessary overlap;neural speech;predicting current frame;prediction technique deep;improving frame;overlapped frame;enhancement;network dnn predicts;proposes overlapped frame"}, "45ce9fce4a4eea9f72688885182aee0c84786fab": {"ta_keywords": "translate musical domains;domains musical transcriptions;domain wavenet autoencoder;domains musical;musical domains;musical transcriptions;samples domains musical;musical domains seen;wavenet autoencoder;method translating music;translating music musical;wavenet autoencoder shared;translating music;domain independent encoder;domain wavenet;musical instruments genres;instruments genres;allows translate musical;translate musical;multi domain wavenet;music musical instruments;autoencoder;musical instruments;autoencoder shared encoder;autoencoder shared;instruments genres styles;encoder allows translate;wavenet;music;independent encoder allows", "pdf_keywords": "music translation network;wavenet autoencoder;domain wavenet autoencoder;method translating music;translating music;wavenet autoencoder shared;learning domain audio;instrumental music untrained;universal music translation;translating music musical;audio mozart;music translation;music untrained;music untrained humans;translation network;wavenet;recurrent networks 17;sequence recurrent networks;audio mozart symphony;orchestra audio;instruments genres;beethoven supervised learning;instrumental music;domain wavenet;based wavenet;convert audio mozart;recurrent networks;creation instrumental music;autoencoder;musical instruments genres"}, "782a50a48ba5d32839631254285d989bfadfd193": {"ta_keywords": "grained entity typing;entity representations large;entity representations human;fine entity typing;interpretable entity representations;entity representations;grained entity types;entity typing;creating entity representations;entity typing model;fine grained entity;entity typing data;language processing entities;large scale typing;type interpretable entity;grained entity;processing entities text;improve performance embeddings;entities text typically;typing model trained;interpretable entity;natural language processing;entities text;typing model;confidence typing model;entity types;entity types indicating;representations human readable;fine entity;representations using fine", "pdf_keywords": "interpretable entity representations;entity representations human;entity representations large;interpretability embeddings debug;entity representations;interpretability embeddings;entity representations simultaneously;interpretable entity;supervised entity typing;embeddings debug downstream;leverage interpretability embeddings;embeddings debug;creating interpretable entity;2020 interpretable entity;tasks entity representations;entity typing models;language processing entities;set interpretable entity;readable finally embeddings;entity typing;entity typing dataset;creating interpretable;based entity typing;representations human readable;typing models trained;trained supervised entity;processing entities text;entities text typically;entities;entities text"}, "d26683135c70d7b2a61ce5f70fb49b4fa22cf9c4": {"ta_keywords": "sampling arbitrary ranking;arbitrary ranking;arbitrary ranking distributions;ranking distributions conditional;ranking distributions;pairwise comparisons alternatives;ordinal rankings;pairwise evidence computationally;arbitrary pairwise comparisons;ranking;rankings develop algorithms;sampling arbitrary;allows sampling arbitrary;ordinal rankings develop;model pairwise evidence;rankings;pairwise comparison data;existing learning inference;pairwise evidence;blocks ordinal rankings;comparisons alternatives represent;pairwise comparisons;approximate samplers;sampling;comparisons alternatives;comparison data experiments;bounds pairwise evidence;approximate samplers exact;allows sampling;inference methods impose", "pdf_keywords": "sampling learning mallows;learning mallows mixtures;learning mallows mixture;sampling learning;likelihood learning mallows;learning mallows models;e\ufb00ective sampling learning;data sampling mallows;mallows models pairwise;algorithms learning mallows;models pairwise preference;mallows models mixtures;algorithm learning mallows;pairwise preference data;sampling mallows model;approximate samplers;pairwise evidence computationally;comparison data sampling;approximate samplers exact;sampling mallows;mallows model pairwise;leverages sampling algorithms;sample mallows mixtures;sampling algorithms provide;conditioned incomplete preferences;learning mallows;sampling algorithms;pairwise preference;sample mallows;data sampling"}, "205d67dfe0112df846bc4b221fa2665b0434d441": {"ta_keywords": "workshop asian translation;asian translation wat2015;asian translation;translation wat2015;2nd workshop asian;workshop asian;translation;asian;proceedings 2nd workshop;2nd workshop;wat2015;workshop;proceedings 2nd;proceedings;2nd", "pdf_keywords": ""}, "7df95dceaba3f4fb45e2b9de29caf7fbce20e25c": {"ta_keywords": "greeb v0;greeb;v0", "pdf_keywords": ""}, "83a2582b94aeaaa97b2f52af8d827d28dc4690bf": {"ta_keywords": "speech affects fluency;fluency mel frequency;fluency measures;fluency mel;fluency;measures effectiveness speech;affects fluency mel;fluency measures effectiveness;effectiveness speech;pauses speech affects;silent pauses speech;affects fluency;effectiveness speech delivering;pauses speech;speech basic means;speech recognition mfcc;speech affects;means communication;speech recognition;communicating person speech;stuttered speech recognition;silent pauses;speech basic;speech;speech delivering information;recognition mfcc means;removal silent pauses;silent pauses repetitions;pauses stuttered speech;speech delivering", "pdf_keywords": ""}, "930445d9cda71d6ff857e69aa5bb4b1bef7d31e5": {"ta_keywords": "dynamic regret bounds;bandit reinforcement learning;regret bounds optimal;bounds discounted reinforcement;discounted reinforcement learning;bound reinforcement learning;bandit reinforcement;confidence bound reinforcement;regret bounds;regret bounds discounted;regret bounds parameter;reinforcement learning drifting;low dynamic regret;cases bandit reinforcement;bound reinforcement;dynamic regret;reinforcement learning confidence;discounted reinforcement;enjoy dynamic regret;bounds discounted;reinforcement learning;reinforcement learning texttt;bandit;emph dynamic regret;learning drifting;various cases bandit;learning drifting non;learning confidence widening;non stationary policy;cases bandit", "pdf_keywords": "non stationary bandit;online learning reinforcement;stationary bandit rl;stationary online learning;stationary bandit;stochastic online learning;conventional optimistic exploration;optimistic exploration;optimistic exploration techniques;low dynamic regret;dynamic regret rl;bandit rl piecewise;stationary stochastic online;reinforcement learning notably;dynamic regret;mdps conventional optimistic;non stationary online;reinforcement learning;non stationary mdps;stochastic online;learning reinforcement;online learning;learning reinforcement learning;stationary mdps;online learning including;stationary online;bandit rl;non stationary stochastic;optimism face uncertainty;stationary mdps conventional"}, "d3231772937a2182b2377d028417245c49868dd1": {"ta_keywords": "neural machine translation;neural sequence models;search minimum translation;machine translation nmt;machine translation;bias shorter translations;minimum translation length;entire wmt15 english;minimum translation;translation length;shorter translations;score translation revealing;translation nmt constraining;neural models properly;best score translation;wmt15 english german;50 sentences model;wmt15 english;neural sequence;translation revealing massive;failure neural models;translation nmt;model errors neural;sequence models;neural models;translations;massive failure neural;translation revealing;score translation;inference procedure neural", "pdf_keywords": "decoding algorithm nmt;neural sequence models;exact decoding;nmt scores;propose exact decoding;nmt search;decoding;monotonicity nmt scores;nmt scores conditional;exact decoding algorithm;translation evidence nmt;inference procedure neural;nmt search errors;sequence models;model prefers translation;best model scores;entire wmt15 english;model scores;algorithm nmt;adequacy nmt search;algorithm nmt exploits;decoding algorithm;model adequacy nmt;model scores transformer;model scores consistency;neural sequence;nmt exploits monotonicity;search depth;wmt15 english german;evidence nmt failure"}, "6b2b5d3d9a2ca4bc4fbd81551a62370be2fbff1b": {"ta_keywords": "scaling laws trained;limited scaling behavior;scaling regimes exhibit;explaining scaling laws;exhibit scaling regimes;limited scaling;scaling laws;scaling regimes controlled;scaling laws identify;total scaling regimes;scaling behavior;connects scaling laws;scaling regimes;explaining scaling;related scaling regimes;framework explaining scaling;scaling behavior dataset;scaling;regimes exhibit scaling;scaling regimes respect;related scaling;dataset model size;size total scaling;resolution limited scaling;exhibit scaling;large random feature;random feature pretrained;explains connects scaling;total scaling;trained neural networks", "pdf_keywords": "deep networks largely;deep network representations;learned deep network;deep models standard;learned deep;deep networks;versatility learned deep;invariance dataset scaling;networks largely learning;deep models;dataset scaling;scaling regions neural;limited scaling behavior;used deep networks;deep network;dataset scaling exponent;largely learning properties;architectures datasets empirically;regions deep models;scaling behavior dataset;limited scaling;network representations;exhibit scaling regimes;feature pretrained models;connects scaling laws;random feature pretrained;scaling behavior;scaling regimes exhibit;scaling exponent superclassing;exhibit scaling"}, "4e1b16fd719354b0a9e92075be66c85d4b95082c": {"ta_keywords": "information neural representations;relevant information neural;neural representations;information neural;task relevant information;similarity task relevant;density similarity task;similarity task;neural;measuring density similarity;task relevant;density similarity;similarity;representations;task;measuring density;density;measuring;relevant information;information;relevant", "pdf_keywords": ""}, "713844009469478141671c53a3b73cd12caf9df0": {"ta_keywords": "\u540c\u6642\u97f3\u58f0\u7ffb\u8a33\u306b\u304a\u3051\u308b\u7ffb\u8a33\u7cbe\u5ea6\u3068\u9045\u5ef6\u6642\u9593\u3092\u540c\u6642\u306b\u8003\u616e\u3057\u305f\u8a55\u4fa1\u5c3a\u5ea6 \u7ffb\u8a33\u8a55\u4fa1 \u7b2c6\u56de\u96c6\u5408\u77e5\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0;\u540c\u6642\u97f3\u58f0\u7ffb\u8a33\u306b\u304a\u3051\u308b\u7ffb\u8a33\u7cbe\u5ea6\u3068\u9045\u5ef6\u6642\u9593\u3092\u540c\u6642\u306b\u8003\u616e\u3057\u305f\u8a55\u4fa1\u5c3a\u5ea6 \u7ffb\u8a33\u8a55\u4fa1;\u540c\u6642\u97f3\u58f0\u7ffb\u8a33\u306b\u304a\u3051\u308b\u7ffb\u8a33\u7cbe\u5ea6\u3068\u9045\u5ef6\u6642\u9593\u3092\u540c\u6642\u306b\u8003\u616e\u3057\u305f\u8a55\u4fa1\u5c3a\u5ea6;\u7ffb\u8a33\u8a55\u4fa1 \u7b2c6\u56de\u96c6\u5408\u77e5\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0;\u7b2c6\u56de\u96c6\u5408\u77e5\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0;\u7ffb\u8a33\u8a55\u4fa1", "pdf_keywords": ""}, "810420af4fa5f3ed932724aea5f7b66d3bd592b2": {"ta_keywords": "query web learning;learning query web;web learning methods;web learning;search engine;www search engine;learning query;queries www search;www filled resource;world wide web;links known documents;collect links known;machine learning;ripper rule learning;learning methods ripper;query web;use machine learning;problem learning query;queries www;machine learning methods;using machine learning;learning methods address;wide web www;search;known documents specific;web;web www;learning methods;www browser batch;filled resource", "pdf_keywords": ""}, "e5e74d312679eae8f2a2943e16f2efebcb5cc50f": {"ta_keywords": "wind speed changes;surface wind speed;wind speed;40 outperform reanalysis;speed changes china;near surface wind;outperform reanalysis;surface wind;cra 40 outperform;outperform reanalysis products;does cra 40;reanalysis;reanalysis products evaluating;reanalysis products;wind;cra 40;does cra;changes china;speed changes;cra;40 outperform;evaluating near surface;evaluating near;speed;products evaluating near;china;near surface;outperform;40;evaluating", "pdf_keywords": ""}, "e35357ac461a669fe7e4b877ee1fad0dfda26303": {"ta_keywords": "style transfer languages;style transfer task;crowdsource formality annotations;transfer languages style;style labelled corpus;inference style extraction;style transfer using;style transfer;controllable style transfer;paraphrases shot controllable;sentence target style;style labelled corpora;extraction style transfer;style transfer new;sentiment transfer;modeling stylistic;sentences inference style;style extraction;paraphrases shot;difference paraphrases shot;shot style transfer;annotations 4000 sentence;annotations;style transfer low;setting style transfer;tasks sentiment transfer;modeling stylistic difference;formality annotations;method modeling stylistic;transfer tasks sentiment", "pdf_keywords": "transfer natural language;style transfer task;style preserving semantics;attribute transfer tasks;tasks sentiment transfer;crowdsource formality annotations;language generation task;sentiment transfer;transfer tasks sentiment;gender neutralization text;natural language generation;abstract style transfer;style examples inference;sentence target style;controllable style transfer;style transfer natural;annotations 4000 sentence;attribute transfer;sentiment transfer simpli\ufb01cation;formality annotations;style transfer;introduction style transfer;automatic evaluations;language generation;annotations;natural language;10 style examples;style transfer systems;preserving semantics;formality annotations 4000"}, "e97c5b206c1f308b821917bc2f584b5f1faad547": {"ta_keywords": "induced ranking estimators;estimators rely ranking;ranking estimators;ranking estimators flexible;scores arbitrary adversarially;induced ranking popular;adversarially chosen miscalibrations;induced ranking;scores induced ranking;consistent induced ranking;rely ranking specifically;bias scores;rely ranking;cardinal scores arbitrary;consider cardinal scores;biases bias scores;information cardinal scores;scores arbitrary;cardinal scores induced;ranking specifically consider;ranking;bias scores approach;testing ranking;ranking specifically;possible estimators rely;crowdsourcing;assumptions miscalibration widely;miscalibration widely;ranking popular approach;cardinal scores", "pdf_keywords": "estimators rely ranking;testing ranking;testing ranking compelling;ranking theorem;ranking theorem canonical;rely ranking theorem;testing ranking design;superiority cardinal data;ranking section estimators;ranking;ranking based algorithms;rely ranking;applications testing ranking;testing section ranking;ranking based;improve ranking based;improve ranking;concept testing ranking;ranking compelling;ranking design estimators;ordinal data testing;ranking design;plug improve ranking;ranking compelling feature;superiority cardinal;ranking section;cardinal data ordinal;section ranking;predicts relative ordering;concept superiority cardinal"}, "2d3fcbaf28e650471b942f221c5fa3c178b1b72a": {"ta_keywords": "accelerated directional search;directional search method;convex unconstraint optimization;directional search derivative;directional search;unconstraint optimization;norm accelerated directional;propose accelerated directional;accelerated directional;convex unconstraint;unconstraint optimization problem;advance norm solution;derivative free methods;search derivative free;optimization;free methods;consider convex unconstraint;methods non euclidian;norm mirr step;nesterov method;method non euclidian;step norm mirr;optimization problem mathbb;search method;euclidian prox structure;search derivative;free methods non;search method non;make nesterov method;norm solution close", "pdf_keywords": ""}, "b345057638e60eee581fea6c7110a98e3b9ebe61": {"ta_keywords": "trends semantic scan;topics text streams;emerging topics text;paper semantic scan;semantic scan;semantic scan alternative;topic modeling;semantic scan provides;compare semantic scan;trends semantic;identifies anomalous text;contrastive topic modeling;topic modeling online;topics text;business trends semantic;anomalous text patterns;better event detection;massive text streams;tasks semantic scan;semantic scan does;business trend detection;speedup paper semantic;trend detection task;scanning identify emerging;emerging topics;event detection;identify emerging events;reviews early detection;semantic scan integrates;text streams tasks", "pdf_keywords": "event semantic scan;contrastive topic modeling;spatial event detection;contrastive topic model;topic modeling online;topic model robust;topic modeling;emerging event semantic;spatiotemporal free text;event detection;topic model;event pattern detection;semantic scan;presented semantic scan;semantic scan alternative;event detection integrating;compare semantic scan;incorporated spatial event;identify emerging events;event semantic;topics;detecting anomalous patterns;novel contrastive topic;contrastive topic;semantic scan integrates;scanning identify emerging;topics time;detecting anomalous;paper presented semantic;topics time online"}, "2038086c604f1f8841d086cd5cc6052e546ffc24": {"ta_keywords": "shinji watanabe spgispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe10000_valid;watanabe spgispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe10000_valid;watanabe spgispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe10000_valid acc;spgispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe10000_valid acc ave;spgispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe10000_valid;spgispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_bpe10000_valid acc;espnet2 pretrained model;espnet2 pretrained;pretrained model shinji;espnet2;fs 16k lang;pretrained model;pretrained;model shinji watanabe;16k lang;fs 16k;model shinji;16k lang en;ave fs 16k;fs;shinji watanabe;shinji;model;en;16k;lang en;lang;watanabe;ave fs;acc ave fs", "pdf_keywords": ""}, "9850d2b41c6c5be039649d6422306121b760169d": {"ta_keywords": "oblivious updates distributed;oblivious update algorithms;optimal oblivious updates;communication oblivious updates;oblivious updates generic;oblivious updates;oblivious update;codes oblivious update;updates distributed storage;bounds communication oblivious;optimal oblivious;mds optimal oblivious;codes update protocols;explicit codes oblivious;updates distributed;distributed storage networks;codes oblivious;communication oblivious;updated nodes requiring;lower bounds communication;distributed storage;update protocols;storage networks;update algorithms;nodes need update;distributed computer systems;update protocols stale;updated nodes;data updated nodes;truly distributed computer", "pdf_keywords": ""}, "71bcdfe5b6be3a0d08ce4bde45acdfd0f738e2f7": {"ta_keywords": "inverse reinforcement learning;based inverse reinforcement;inverse reinforcement;problem inverse reinforcement;modeling passengers decisions;reinforcement learning markov;learning markov decision;risk sensitivity reinforcement;sensitivity reinforcement learning;reinforcement learning framework;reinforcement learning algorithm;mdp modeling passengers;reinforcement learning;passengers decisions;markov decision processes;passengers decisions regarding;learning markov;markov decision;modeling passengers;models human decision;gradient based inverse;agent risksensitive;sensitivity reinforcement;reinforcement;risksensitive particular model;learning algorithm minimizes;decision processes agent;agent risksensitive particular;model risk sensitivity;risk", "pdf_keywords": ""}, "1843c91e9692484b574ef40961f1d0443a56ddf4": {"ta_keywords": "incentivizing objective feedback;simple incentive mechanism;incentive mechanism obtaining;propose simple incentive;incentive mechanism;feedback online platforms;truthful equilibrium;agents truthful equilibrium;simple incentive;incentive;bayes nash equilibrium;payoff agents truthful;nash equilibrium;objective feedback online;eliciting informative feedback;objective feedback platforms;feedback platforms;feedback absence verifiability;informative feedback;nash equilibrium game;obtaining objective feedback;objective feedback;equilibrium game induced;informative feedback absence;truthful equilibrium close;rule incentivizing objective;commerce platforms eliciting;agreement rule incentivizing;equilibrium game;agents truthful", "pdf_keywords": ""}, "4c0a915b9389e6489753a968085ee12833131d0a": {"ta_keywords": "challenges peer review;problems peer review;peer review;peer review urgent;practice peer review;peer review consequences;importance peer review;unfairness peer review;peer review particularly;peer review process;peer review cornerstone;author peer review;addressing problems peer;peer review outline;peer review tutorial;challenges peer;key challenges peer;author peer;problems peer;scholarly research kdd;academic practice peer;noted incompetent review;review consequences;unfairness peer;scholarly;scholarly research;bias unfairness peer;importance peer;incompetent review;review cornerstone academic", "pdf_keywords": ""}, "df53aabeca68a8c0076f7e110f2cc7df7d010e7a": {"ta_keywords": "multi source localization;localization source splitting;talker speech recognition;multi talker speech;source localization;multi talker;speech recognition;source localization source;localization source;effectiveness multi talker;source splitting;talker speech;source splitting effectiveness;localization;multi source;talker;based multi source;learning based multi;deep learning;deep learning based;speech;splitting effectiveness multi;splitting;splitting effectiveness;based multi;recognition;learning;learning based;multi;deep", "pdf_keywords": "single speech source;multi source localization;speech source input;source localization classify;aid speech recognition;joint optimization speech;optimization speech recognition;speech recognition;speech source;source localization;optimization speech;proposed single speech;localization task models;speech recognition proposed;directional asr learn;arrival doa speakers;single speech;speech recognition devised;loss function localization;localization task;multi talker asr;localization classify;deep learning;audio mixture;learn predict doa;doa speakers simultaneously;doa speakers;simultaneously audio;simultaneously audio mixture;multi talker"}, "b03c7ff961822183bab66b2e594415e585d3fd09": {"ta_keywords": "multi head attention;parallel attention;parallel attention head;multiple attention;attention heads;large percentage attention;attention head;percentage attention heads;attention mechanisms parallel;multi headed attention;head attention;multiple attention mechanisms;mechanisms parallel attention;neural models focus;attention;attention examine greedy;percentage attention;attention head potentially;apply multiple attention;attention mechanisms;models bert finally;models bert;attention powerful;mt models bert;headed attention;nlp models transformer;attention heads removed;head attention examine;allowing neural models;attention powerful ubiquitous", "pdf_keywords": "decoder attention layers;decoder attention;attention layers reliant;attention layers;encoder decoder attention;self attention layers;attention layers provided;translation models encoder;multi head attention1;machine translation models;translation models;trained transformer models;head attention1;attention;neural networks decomposes;trained transformer;training neural;representations labels compression;models encoder decoder;machine translation;head attention1 examine;training neural networks;neural;according training neural;models encoder;pruning models potential;self attention;training progresses suggesting;heads increases training;headedness self attention"}, "315432474166ff7abbc6351e8ff07fcccbd68458": {"ta_keywords": "misinformation websites shared;misinformation network news;quality misinformation websites;news sources connecting;quality misinformation network;network news information;quality misinformation sources;misinformation sources high;health news sources;misinformation network;web health news;misinformation terms tweets;misinformation websites;news information network;news sources common;sources connecting community;urls high quality;urls shared twitter;covid 19 twitter;quality urls shared;connections news sources;quality urls extensively;health information network;higher rate tweets;low quality urls;quality urls;misinformation sources;news sources;quality health information;urls low quality", "pdf_keywords": "covid 19 twitter;sources shared twitter;urls shared twitter;quality url sharing;url sharing covid;quality urls shared;news sources shared;twitter conversation news;shared twitter;twitter conversation;19 twitter conversation;twitter;low quality urls;links low quality;users discuss covid;quality urls;information sources coronavirus;19 twitter streams;low quality url;twitter users discuss;links high quality;quality url;information sources shared;information sites shared;quality information sources;low quality information;covid 19 misinformation;quality health information;twitter streams;rate links high"}, "92acaf505a9c738e56ed70759e8d0062f3c520d6": {"ta_keywords": "audio segmentation asr;neural network audio;speech recognition asr;concatenate audio segmentation;realizes audio segmentation;segmentation audio;audio segmentation;segmentation audio deal;network audio segmentation;audio segmentation non;long recording neural;models segmentation asr;recording neural end;segmentation asr introduce;segmentation asr;issue segmentation audio;segmentation asr model;automatic speech;recognition asr systems;automatic speech recognition;speech recognition;recognition asr;asr single neural;input long recording;recording neural;propose concatenate audio;practical automatic speech;concatenate audio;autoregressive asr;network audio", "pdf_keywords": "audio segmentation nonautoregressive;concatenate audio segmentation;realizes audio segmentation;models segmentation asr;audio segmentation;combining audio segmentation;audio segmentation non;combination audio segmentation;segmentation nonautoregressive asr;recording audio recognition;segmentation asr;segmentation asr introduce;audio recognition;long recording audio;asr streaming;nonautoregressive asr streaming;asr streaming long;propose concatenate audio;streaming long recording;asr single neural;concatenate audio;long recording;recording audio;combining audio;autoregressive asr;autoregressive asr single;non autoregressive asr;autoregressive asr realize;audio;proposed combining audio"}, "f75d05e759447c2aedb7097728f29f9a520d9bc1": {"ta_keywords": "range language models;long range language;transformer language models;longer sequences models;sentence level prediction;language models;language models including;long range context;language models actually;language models generally;range language;trained short truncated;tokens models improves;longer sequences;sequence lm benchmark;long sequence lm;range transformer language;context improve predictions;sentence level;long sequence;language models process;truncated input sequences;transformer language;sequences 8k tokens;long range;range context improve;process longer sequences;sequences models;context previous 2k;tokens long range", "pdf_keywords": "range language models;long range language;transformer language models;tokens models improves;longrange transformer language;token sequencelevel improvements;longsequence lm benchmark;long range context;language models;sequencelevel improvements long;language models including;language models actually;range language;tokens subword clusters;language models using;sequences 8k tokens;sequencelevel improvements;context token sequencelevel;tokens models;tokens away models;longsequence lm;context 2k tokens;2k tokens models;grained analysis longrange;range transformer language;transformer language;token sequencelevel;sentence level prediction;context previous 2k;range context 2k"}, "1dfa71ecab0c25c5fdd6b2df83a41e944ffa5d58": {"ta_keywords": "based multinomial distribution;multinomial distribution wide;words higher frequencies;models based multinomial;based multinomial;multinomial distribution;classification tasks;statistical models text;text treat words;frequencies occurrence sensible;classification;multinomial;frequencies occurrence;higher frequencies occurrence;statistical models;occurrence;models text;binomial distributions desirable;models based poisson;treat words higher;models text treat;words higher;occurrence sensible manner;present statistical models;binomial distributions;occurrence sensible;classification tasks classes;negative binomial distributions;distributions desirable;text", "pdf_keywords": ""}, "0c47eb31b2dd76d8dc986173a1d3f00da1c9c74d": {"ta_keywords": "benchmark domain adaptation;models nlms learn;language models nlms;non parametric nlms;nlms learn predictive;nearest neighbors language;domain adaptation datasets;neighbors language model;neural language models;parametric nlms practical;models nlms;domain adaptation;parametric nlms;language models;nlms learn;benchmark domain;memorizing training datapoints;inference speed retaining;parametric neural language;explicitly memorizing training;nlms practical applications;learn predictive distributions;adaptation datasets methods;learn explicitly memorizing;wikitext 103 benchmark;adaptation datasets;nlms practical;language model;neighbors language;datastore allows learn", "pdf_keywords": "benchmark domain adaptation;nearest neighbor language;neighbor language models;free domain adaptation;adaptive retrieval;nearest neighbors language;language modeling benchmark;prune unnecessary retrieval;adaptive retrieval explore;retrieval operations adaptive;domain adaptation;neighbors language model;domain adaptation datasets;inference speed retaining;unnecessary retrieval operations;operations adaptive retrieval;benchmark domain;learned automatically prune;unnecessary retrieval;nearest neighbor;ef\ufb01cient nearest neighbor;domain adaptation setting;retrieval explore;retaining comparable performance;wikitext 103 benchmark;103 benchmark domain;retrieval operations;retrieval;neighbors language;datastore pruning based"}, "83cbe142d445a521aefa11acbd184e176085e7c7": {"ta_keywords": "trusting suspicious voters;trusting voters suspicious;suspicious voters model;trusting voter anomalous;biased communications trusting;trusting voters partisan;audience trusting voters;trusting voters;voters negative information;biased sources behavior;updates suspicious voters;voters suspicious;suspicious voters types;trusting voter;voter anomalous updates;voters suspicious voters;voters trusting voters;behavior observed voters;suspicious voters information;suspicious voters;voters trusting;biased sources;voters model;unbiased sources biased;suspicious voter opposite;information sources unbiased;sources biased sources;observed voters;biased communications;observed voters negative", "pdf_keywords": "trusting suspicious voters;voters negative information;trusting voters suspicious;trusting voters behave;trusting voters;voters behave intuitively;behavior observed voters;opinion candidate reasoning;observed voters negative;easy trusting voters;suspicious voters distinct;trusting voters trusting;multiple agents biased;voters suspicious;suspicious voters types;biased communications trusting;agents biased communications;negative information candidate;suspicious voters;voters suspicious voters;voters trusting;voters trusting voters;candidate reasoning;observed voters;candidate reasoning new;bayesian networks;generalize bayesian networks;voters negative;agents biased;voters types information"}, "6a79ff7465d8249d9c8a50fa5f2e0a3e308b436d": {"ta_keywords": "domain adaptation retrieval;domain adaptation;unsupervised domain adaptation;labeling unsupervised domain;domain adaptation dense;adaptation dense retrieval;domain adaptation method;generative pseudo labeling;scenario domain adaptation;domain specialized datasets;domain robust training;adaptation retrieval tasks;dense retrieval approaches;novel unsupervised domain;pseudo labeling unsupervised;query generator pseudo;dense retrieval approach;results dense retrieval;adaptation retrieval;dense retrieval;unsupervised domain;retrieval approaches overcome;retrieval approaches;generator pseudo labeling;retrieval tasks yield;domain specialized;representative domain specialized;gpl generative pseudo;dense retrieval investigate;target domain robust", "pdf_keywords": "unsupervised domain adaptation;domain adaptation;labeling unsupervised domain;generative pseudo labeling;domain adaptation dense;results representative domainspecialized;domainspecialized datasets;representative domainspecialized datasets;novel unsupervised domain;adaptation dense retrieval;domain adaptation technique;domain adaptation method;dense retrieval approaches;domainspecialized datasets \ufb01nd;dense retrieval approach;dense retrieval models;pseudo labeling unsupervised;abstract dense retrieval;query generator pseudo;gpl generative pseudo;unsupervised domain;dense retrieval;domainspecialized;retrieval approaches;technique dense retrieval;generative pseudo;retrieval approaches overcome;method dense retrieval;representative domainspecialized;encoder gpl generative"}, "3e3f55cb25b919c4e8158195fd3ce2f23cfa7723": {"ta_keywords": "language bias vqa;reduce language bias;mitigate language bias;bias vqa counterfactual;capture language bias;language bias direct;counterfactual inference framework;rely language bias;language bias;language bias sensitive;language bias recent;novel counterfactual inference;language bias shortcut;counterfactual inference;vqa counterfactual;look language bias;language prior inference;proposed counterfactual inference;counterfactual vqa;bias sensitive vqa;vqa counterfactual vqa;language bias subtracting;counterfactual vqa cause;language motivated causal;performance language bias;bias vqa;answers reduce language;bias direct causal;bias recent debiasing;mitigate language", "pdf_keywords": "reduce language bias;language bias vqa;counterfactual inference framework;language bias direct;capture language bias;language bias;formulate language bias;novel counterfactual inference;counterfactual inference;language bias sensitive;proposed counterfactual inference;rely language bias;data counterfactual vqa;counterfactual reasoning causal;language bias subtracting;counterfactual vqa;counterfactual reasoning;performance language bias;answers mitigate bias;bias direct causal;data counterfactual;counterfactual vqa cause;augmented data counterfactual;language bias shortcut;motivated counterfactual reasoning;effect motivated counterfactual;demonstrate proposed counterfactual;proposed novel counterfactual;propose novel counterfactual;counterfactual"}, "7b29f45df975ed1e4c3864b6ab4483f11086aa76": {"ta_keywords": "machine translation embeddings;translation embeddings surprisingly;translation embeddings;neural machine translation;trained word embeddings;word embeddings useful;word embeddings;embeddings surprisingly effective;machine translation;embeddings useful neural;machine translation nmt;word embeddings proven;performance natural language;embeddings surprisingly;embeddings useful;translation nmt systems;parallel corpora;pre trained word;embeddings;scale parallel corpora;embeddings proven;language analysis tasks;embeddings proven invaluable;translation nmt;performance neural;parallel corpora obtained;performance neural machine;corpora;useful neural machine;useful neural", "pdf_keywords": "pretrained word embeddings;embeddings help multilingual;bilingual systems embeddings;trained word embeddings;pre trained embeddings;word embeddings nmt;word embeddings useful;languages pre trained;word embeddings;trained embeddings;compared bilingual systems;target languages pre;embeddings surprisingly effective;word embeddings proven;embeddings useful neural;bilingual systems;trained embeddings help;neural machine translation;training affected language;embeddings useful;pretrained word;considering pretrained word;target languages;compared bilingual;languages pre;systems compared bilingual;multilingual systems compared;trained embeddings q5;embeddings surprisingly;multilingual systems"}, "7a684045afae2ccf40338ff07b8fa429bad93a57": {"ta_keywords": "large web corpora;web corpus;web corpus 10;pages web corpus;web size corpus;billion crawled;billion crawled urls;corpus 10 billion;web corpora;corpus free;commoncrawl corpus;crawled;corpus free license;web corpora containing;size corpus free;process commoncrawl corpus;million pages web;general web crawl;c4corpus multilingual web;web crawl;large web;date billion crawled;commoncrawl corpus 2000;corpus;corpus 2000 cpu;highly scalable hadoop;size corpus;crawl date billion;multilingual web;corpora containing documents", "pdf_keywords": ""}, "fc5d79301a0876201c95954a764ec374b8eb236e": {"ta_keywords": "sentences domain adaptation;parallel domain corpus;domain adaptation neural;domain adaptation;neural machine translation;target sentences domain;translation monolingual domain;domain parallel sentences;pseudo domain corpus;domain target sentences;translation lexicon induction;strong translation baselines;machine translation lexicon;domain corpus;extract domain lexicon;trained domain nmt;monolingual domain target;domain corpus specifically;parallel sentences improving;domain lexicon;domain corpus performing;domain lexicon construct;pre trained domain;machine translation;sentences domain;domains pairwise adaptation;translation baselines;trained domain;translation lexicon;translation baselines remedy", "pdf_keywords": "parallel domain corpus;domain parallel corpus;domain adaptation lexicon;domain source corpus;corpus translation domain;domain target corpus;parallel corpus domain;domain unaligned corpus;pseudoparallel corpus translation;corpus copying monolingual;domain corpus word;pseudo domain corpus;domain corpus;translation domain monolingual;adaptation lexicon;translation monolingual domain;domain target sentences;adaptation lexicon induction;parallel corpus;unaligned corpus giza;domain corpus analysis;creation pseudoparallel corpus;target corpus pseudo;parallel indomain corpus;monolingual domain target;domain corpus performing;unaligned corpus;extract domain lexicon;domain monolingual target;pseudoparallel corpus"}, "75abecb4568366d89e89c3c9d39574b9c1c028a5": {"ta_keywords": "curators navigate article;resultspaperbrowser nlp powered;curators enhanced navigational;nlp powered interface;nlp tasks;nlp biomedical text;certain nlp tasks;processing nlp biomedical;nlp powered;paperbrowser navigational;resultspaperbrowser nlp;nlp tasks named;processing nlp;navigational functionalities paperbrowser;text highlighting task;nlp biomedical;paperbrowser navigational functionalities;improves navigational;named entity recognition;performance certain nlp;natural language processing;curators navigate;entity recognition anaphora;enhanced navigational utility;text technology facilitate;events improves navigational;provide curators enhanced;ways highlight text;language processing nlp;curators enhanced", "pdf_keywords": ""}, "ec9367ab933a142124eecd3232fe2d933d93a144": {"ta_keywords": "efficient navigation language;efficient navigation;navigation language pre;navigation language;navigation;training stochastic sampling;pre training stochastic;training stochastic;stochastic sampling;language pre training;language pre;sampling;stochastic;pre training;language;training;efficient;pre", "pdf_keywords": ""}, "51c2321244b0a489970e1b52c59b049fdcc5cd46": {"ta_keywords": "automatic translation evaluation;translation evaluation metrics;lingual question answering;machine translation evaluation;machine translations;automatic translation;manual machine translations;manual automatic translation;machine translations perform;translation evaluation;machine translation;question answering knowledge;answer questions language;translations perform clqa;translation results;cross lingual qa;question answering;answering knowledge bases;question answering qa;bases question answering;translations perform;answering qa systems;lingual qa clqa;metrics cross lingual;investigation machine translation;questions language based;lingual qa;translations;questions language;source cross lingual", "pdf_keywords": ""}, "bb6c2a64ecb6e4c9f3f5720d53cca76a2c37505d": {"ta_keywords": "language grounding embodiment;language grounding;foundations language grounding;linguistic communication;natural language;linguistic;grounding embodiment;contextual foundations language;text corpora deeply;trained text;makes utterances meaningful;natural language processing;social nature language;models trained text;linguistic communication relies;language processing models;contextual social;utterances;relate language;grounding embodiment social;facilitates successful linguistic;contextual;contextual information;makes utterances;language processing;utterances meaningful posit;trained text today;utterances meaningful;relate language physical;language physical world", "pdf_keywords": "multimodal comprehension cooking;language multimodal;comprehension cooking;multimodal comprehension;communication language multimodal;comprehension cooking recipes;dataset multimodal comprehension;language multimodal phenomenon;multimodal;language learning;challenge dataset multimodal;dataset multimodal;communication language;recipeqa challenge dataset;text corpora;representation learning;implications language learning;corpora;representation learning approaches;large text corpora;recipes;cooking recipes;cooking;language;deeper questions communication;multimodal phenomenon implications;recipeqa challenge;trained large text;multimodal phenomenon;language learning processing"}, "6b98bef930182a848c027dece1bfb58ca706449d": {"ta_keywords": "pronunciation assisted sub;sub word extraction;characters sub words;sub word modeling;sub word segmentation;sequence characters sub;speech recognition;end speech recognition;characters sub;word segmentation;speech recognition output;character based baseline;directly sequence characters;word extraction method;speech recognition systems;word extraction;pronunciation information;leverages pronunciation information;character sequence frequencies;extraction consider character;assisted sub word;pronunciation information word;word modeling;word segmentation lead;sub words;pronunciation assisted;erroneous speech recognition;word modeling pasm;sequence characters;sub word", "pdf_keywords": "end speech recognition;pronunciation assisted sub;sub word extraction;sub word modeling;sub word segmentation;speech recognition pronunciation;recognition pronunciation assisted;characters sub words;language speech processing;speech recognition output;speech processing;acoustic language models;speech recognition;recognition pronunciation;sequence characters sub;separate pronunciation;word extraction;speech recognition systems;word segmentation;word extraction method;word modeling;separate pronunciation acoustic;word modeling pasm;word modeling hainan;language models;pronunciation information;word segmentation lead;assisted sub word;sub words;erroneous speech recognition"}, "8d7db1b1290e5d6f802e9f1075ef197cb55d754f": {"ta_keywords": "speech recognition iwslt;english speech recognition;speech recognition;janus recognition toolkit;recognition iwslt 2012;iwslt 2012 evaluation;recognition iwslt;contrastive english speech;built janus recognition;recognition toolkit developed;recognition toolkit;janus recognition;iwslt ted task;naist contrastive english;employ single decoding;single decoding;2012 evaluation campaign;asr track iwslt;kit naist contrastive;iwslt 2012;single decoding fully;speech;evaluation campaign;decoding;describes kit naist;contrastive english;track iwslt ted;english speech;iwslt ted;kit naist", "pdf_keywords": ""}, "acbf4f9a4457cf2884e6018e4653519beef2833a": {"ta_keywords": "pentamer gpcmv infection;complex pentamer glycoproteins;gp131 gpcmv pentamer;pentamer glycoproteins;pentamer glycoproteins required;cytomegalovirus homologue pentameric;pentamer gpcmv;gpcmv infection;gpcmv pentamer;gpcmv pentamer components;function pentamer gpcmv;potent vaccine antigen;guinea pig cytomegalovirus;pig cytomegalovirus homologue;gpcmv infection recent;pentamer components infection;pig cytomegalovirus;cells potent vaccine;gp131 gpcmv;cytomegalovirus cmv;vaccine antigen;vaccines;human cmv infection;cytomegalovirus cmv infection;cmv infection;mutations gp131 guinea;mutations infection cell;cmv infection major;cmv infection endothelial;congenital cytomegalovirus cmv", "pdf_keywords": ""}, "790d3503fa95ec32f04c280bd9a52fef6bf1e874": {"ta_keywords": "models traffic flow;finite differencing methods;differencing methods macroscopic;differencing schemes macroscopic;finite differencing schemes;macroscopic models traffic;traffic flow;finite difference schemes;microscopic models traffic;upwind forward differencing;finite differencing;various finite difference;models traffic;compare finite differencing;differencing schemes;differencing methods;difference schemes including;finite difference;traffic flow given;difference schemes;forward differencing;certain finite differencing;forward differencing lax;methods macroscopic models;schemes macroscopic models;differencing lax friedrichs;traffic;macroscopic models;differencing lax;macroscopic models better", "pdf_keywords": ""}, "0fcfa0ef253a81c103854e1dc123d90e7310a0e1": {"ta_keywords": "private deep learning;differentially private deep;differential privacy;using differential privacy;differential privacy dp;privacy dp sgd;advances differentially private;differential privacy speci\ufb01cally;differentially private;application differential privacy;private deep;privacy dp;privacy speci\ufb01cally dp;privacy;sgd terms fairness;sgd algorithm disparate;dp sgd algorithm;speci\ufb01cally dp sgd;dp sgd vs;ones dp sgd;privacy speci\ufb01cally;training deep learning;dp sgd;deep learning;deep learning models;sgd vs pate;private;training deep;deep learning demonstrated;sgd", "pdf_keywords": "private deep learning;fairness dp sgd;better fairness privacy;differentially private deep;fairness privacy;fairness privacy trade;differential privacy;privacy dp sgd;differential privacy dp;sgd terms fairness;using differential privacy;fairness implications dp;advances differentially private;differential privacy speci\ufb01cally;fairness dp;studying fairness;application differential privacy;achieving better fairness;studying fairness implications;differentially private;terms fairness dp;private deep;privacy dp;privacy trade offs;aim studying fairness;privacy speci\ufb01cally dp;better fairness;training deep learning;sgd algorithm disparate;privacy"}, "634bbe75c34b82e664f1e9f083314b5bdb6ba187": {"ta_keywords": "signals collecting eeg;electroencephalogram eeg event;eeg data intentionally;analyze electroencephalogram eeg;electroencephalogram eeg;collecting eeg data;analyze electroencephalogram;electroencephalogram;eeg event related;eeg event;evaluation eeg ocular;eeg data;collecting eeg;eeg ocular artifact;attempting analyze electroencephalogram;eeg ocular;evaluation eeg;wiener filters based;wiener filter based;eeg;ocular artifact removal;wiener filters;channel wiener filters;filters based probabilistic;separates observed signal;filter based probabilistic;observed signal event;data contamination ocular;artifacts eye blinks;wiener filter", "pdf_keywords": ""}, "79a6f290cfe8652575e7bb65cfed519bca8f3bd3": {"ta_keywords": "information extraction world;information extraction;world wide web;extraction world;extraction world wide;extraction;wide web;web;information;world wide;world;wide", "pdf_keywords": ""}, "9cda754187545c3cc8f9e9f134c08707269d0fae": {"ta_keywords": "speech pre training;speech text pre;unified encoder speech;modeling speech text;unifying speech text;text pre training;language pre training;incorporating speech text;speechstew asr tasks;modeling speech;speech text data;speech pre;modality pre trained;encoder speech;speech text joint;speech text;language modeling speech;function speech pre;speech translation;covost speech translation;encoder speech language;speech language modeling;pre trained models;quality covost speech;data pre training;objective unlabeled speech;unifying speech;step unifying speech;speech translation bleu;unsupervised language pre", "pdf_keywords": "multimodal pre trained;language model slam;speech text pre;modeling speech text;supervised approaches speech;unified encoder speech;text pre training;modeling speech;joint speech language;speech text joint;text speech understanding;text speech;language modeling speech;speech text downstream;contextualized representations speech;speech encoder;multimodal pre;speech understanding;speech text;approach text speech;speech encoder text;truly multimodal pre;pre trained models;speech language modeling;text encoder multimodal;encoder speech;representations speech text;associated multimodal pre;speech text simultaneously;encoder speech language"}, "b50d03ecd9f2055b32451e3c04138a0da07b0f69": {"ta_keywords": "meeting recognition;time meeting recognition;real time meeting;meeting recognition understanding;participant speaking;meeting analyzer;activity participant speaking;automatically speaking online;meeting analyzer monitoring;monitoring conversations;monitoring conversations ongoing;recognize automatically speaking;captures utterances face;time meeting analyzer;online manner meeting;continuously captures utterances;speech recognition activity;poses speaker using;captures utterances;ongoing group meeting;conversations ongoing group;speaking online;group meeting;face poses speaker;camera parallel speech;poses speaker;individual speaker;separated individual speaker;microphone;analyzer monitoring conversations", "pdf_keywords": ""}, "2bd54adb3b5588281396a4b5dae7db09496b2c61": {"ta_keywords": "stanford squad russian;squad russian language;squad russian;analog stanford squad;sberquad;stanford squad;sberquad large scale;sberquad large;russian language;russian;analog stanford;squad;stanford;russian language valuable;scale analog stanford;language;language valuable resource;language valuable;experimental;presented scientific;analysis baseline experimental;scientific;large scale;thorough analysis baseline;description thorough analysis;description thorough;baseline experimental;presented scientific community;analysis baseline;experimental results", "pdf_keywords": "sberquad russian reading;russian reading comprehension;sberquad russian;analysis russian reading;comprehension dataset sberquad;russian reading;analysis russian;russian language valuable;russian language;reading comprehension dataset;squad russian language;question answering;comprehension dataset;question answering qa;results sberquad;sberquad large scale;2020 abstract sberquad;russian;sberquad;comprehension dataset description;answering qa challenge;qa challenge substantial;russia 2sberbank moscow;sberquad created 2017;russia announced question;russia 2sberbank;announced question answering;petersburg russia 2sberbank;abstract sberquad;research multilingual qa"}, "5e657bc8097c12649d027ca3c16ff7d37df1354d": {"ta_keywords": "multilingual machine translation;balancing training multilingual;training multilingual machine;training multilingual neural;multilingual neural machine;languages balancing training;neural machine translation;training multilingual;machine translation;translate multiple languages;machine translation mt;multilingual neural;multilingual machine;languages training data;languages balancing;test languages balancing;multilingual;translation mt models;languages training;translate multiple;sets languages training;languages faced imbalanced;training sets languages;models translate multiple;translation mt;languages optimized;models translate;translation;multiple languages;multiple languages faced", "pdf_keywords": "balancing training multilingual;multilingual model training;training multilingual machine;language balancing training;multilingual machine translation;target language balancing;training multilingual neural;monolingual training objective;objective multilingual training;model trained translate;training multilingual;multilingual neural machine;trained translate;trained translate single;multilingual training preliminaries;multilingual training;abstract training multilingual;multilingual models;language balancing;neural machine translation;multilingual model;multilingual machine;objectives multilingual models;monolingual training;machine translation;languages training data;data multilingual model;multilingual neural;learns language scorer;translation mt models"}, "302ae0d991d62dee82b63530b487a50469810af4": {"ta_keywords": "interpretable spatial operations;mapping natural language;spatial operations rich;interpretable spatial;complex spatial pragmatic;spatial actions 3d;complex spatial actions;spatial actions;3d spatial operations;spatial pragmatic interpretations;spatial pragmatic;instructions complex spatial;inventory interpretable spatial;spatial operations;natural language descriptions;3d spatial;spatial;natural language instructions;propose new neural;actions 3d;rich natural language;complex 3d spatial;natural language;language instructions complex;descriptions require complex;complex spatial;language descriptions;neural architecture achieves;spatial operations figure;neural", "pdf_keywords": "learning interpretable spatial;learning spatial operations;complex spatial language;interpretable spatial operations;spatial language;interpretable spatial;3d spatial operations;complex spatial actions;spatial actions 3d;learning spatial;spatial operations rich;complex spatial pragmatic;spatial actions;spatial language previously;spatial operations;spatial pragmatic interpretations;3d spatial;instructions complex spatial;interpretable neural;introduce interpretable neural;learning interpretable;model learning spatial;operations rich 3d;inventory interpretable spatial;mapping natural language;world learning interpretable;complex 3d spatial;spatial pragmatic;actions 3d;learning interpretable operators"}, "a73d83e50b5687455336a2adce32a069c77ba163": {"ta_keywords": "\u7d71\u8a08\u7684f_0\u8f2a\u90ed\u4e88\u6e2c\u306b\u57fa\u3065\u304f\u4eba\u5de5\u5589\u982d\u306e\u5b9f\u6642\u9593\u632f\u52d5\u5236\u5fa1 powered nict;\u7d71\u8a08\u7684f_0\u8f2a\u90ed\u4e88\u6e2c\u306b\u57fa\u3065\u304f\u4eba\u5de5\u5589\u982d\u306e\u5b9f\u6642\u9593\u632f\u52d5\u5236\u5fa1 powered;\u7d71\u8a08\u7684f_0\u8f2a\u90ed\u4e88\u6e2c\u306b\u57fa\u3065\u304f\u4eba\u5de5\u5589\u982d\u306e\u5b9f\u6642\u9593\u632f\u52d5\u5236\u5fa1;powered nict;nict;powered", "pdf_keywords": ""}, "13608821aa3b369526221182dfbd3a8842549652": {"ta_keywords": "relay placement optimal;wireless relay placement;relay placement;placement relays;deploy wireless relay;deployment relay nodes;optimal power allocation;deployment relay;relay channel;relay nodes;wireless relay;placement relays line;problem placement relays;multi relay channel;relay nodes structure;relay channel study;relay channel model;multi relay;formulas multi relay;allocation given placement;power allocation;relay;placement optimal sequential;using multi relay;placement optimal;cost markov decision;power allocation given;relays line exponentially;problem deployment relay;total cost markov", "pdf_keywords": "relay channel optimal;channel optimal relay;optimal relay placement;relay placement optimal;networks relay placement;optimal relay;relay nodes allocation;wireless relay placement;relay channel;wireless networks relay;relay channel models;relay placement;networks relay;deploy wireless relay;multi relay channel;relay channel study;relay channel model;placement relay;deployment relay;placement relay nodes;wireless relay;relay channel 10;deployment relay nodes;relay placement impromptu;channel optimal;relays constraint;relay placement relax;relay nodes study;relay nodes;number relays constraint"}, "81e684d01bbfb1f4143bb2ffea36cc4791f0530c": {"ta_keywords": "speech enhancement preprocessing;discriminative training acoustic;channel speech enhancement;training acoustic models;multi channel speech;asr techniques discriminative;reverberation time estimation;multiple asr systems;speech enhancement;acoustic models including;acoustic models;asr techniques;reverberation;method reverberation;multiple asr;asr systems;single asr use;asr techniques preprocessing;state art asr;acoustic environments single;training acoustic;channel speech;asr systems different;using multiple asr;reverberation time;asr use combination;dereverberation method reverberation;environments single asr;single asr;method reverberation time", "pdf_keywords": ""}, "14dddd1d8cb2e8c5f4e9998fef84e715cb321ac9": {"ta_keywords": "trust artificial;formalizing trust artificial;formalizing trust;human trust ai;trustworthiness sociology concepts;trust ai;technology formalizing trust;ai evaluate trust;notion trustworthiness sociology;trustworthiness sociology;interpersonal trust trust;trust artificial intelligence;trustworthiness;interpersonal trust;trust implicit;warranted trust intrinsic;human trust;trust intrinsic;trust people defined;ai model trust;trust trust;formalization trustworthiness;cognitive mechanism trust;trust user ai;mechanism trust;notion trustworthiness;trustworthiness detaches notion;trust manifested warranted;mechanism trust promote;trust trust people", "pdf_keywords": "formalizing trust artificial;trust artificial intelligence;trust ai;formalizing trust;formalization trustworthiness;trust artificial;ai model trust;human trust ai;trust user ai;formalization contractual trust;formalization trustworthiness detaches;notion trustworthiness;abstract trust;trust ai alon;trust distrust;trust implicit;trust trust;trust implicit explicit;trustworthiness;trust distrust finally;notion trustworthiness sociology;detaches notion trustworthiness;au abstract trust;model trust implicit;model trust;trustworthiness detaches notion;trust commitment cases;contractual trust trust;trust;abstract trust central"}, "941e42ee75fc2bf07078bcfbd14bdf9ca7fe99ff": {"ta_keywords": "training language models;improve language models;language models improved;monolingual language models;training monolingual language;learning cross lingual;models textual training;lingual word embeddings;language models textual;textual training data;language models results;language models support;training language;textual training;language models;bilingual lexicons;lexicons improve language;training monolingual;languages bilingual lexicons;word embeddings;important training language;linguistics textual data;word embeddings preliminary;bilingual lexicons available;languages language models;documentary linguistics textual;language highlights challenges;documentary linguistics;task documentary linguistics;language highlights", "pdf_keywords": ""}, "981152cb3f1e11c9cee6af2275b57ef79c621934": {"ta_keywords": "diversity text generation;diversity generated samples;improved diversity text;sampling outperform beam;nucleus sampling outperform;outperform beam search;sampling nucleus sampling;heuristic sampling;nucleus sampling;propose heuristic sampling;sampling nucleus;distribution enhances diversity;text generation;heuristic sampling method;diversity text;search based decoding;generated samples achieving;sampling outperform;text generation propose;sampling method inspired;increase diversity generated;diversity generated;generated samples;beam search based;human text explore;tedious repetitive candidates;sampling;inspired inverse probability;resemblance human text;decoding", "pdf_keywords": "diversity neural text;neural text generation;text generation inverse;improving diversity neural;diversity neural;text generation;neural text;abstract neural text;text generation suffers;permutation predicted distribution;intelligence music information;diversity generated samples;2021 abstract neural;trained language model;diversity generated;generation inverse probability;increase diversity generated;intelligence music;distribution enhances diversity;permutation predicted;inverse probability weighting;arti\ufb01cial intelligence music;generation inverse;reasonable permutation predicted;abstract neural;neural;resemblance human text;repetition proposed algorithm;music arti\ufb01cial intelligence;language model"}, "1ea337ac24503d9da8dd9bbf98aac0bfd5920834": {"ta_keywords": "transcripts speech;creating clean transcripts;speaking style transformation;transcripts speech recognition;faithful transcripts speech;clean transcripts;transcripts asr;transcripts propose model;machine translation perform;clean transcripts human;transforming faithful transcripts;clean transcripts propose;statistical machine translation;faithful transcripts asr;transcripts propose;results clean transcripts;transcripts human;transcripts;transcripts asr results;machine translation;faithful transcripts;speaking style;insertion punctuation correction;speech recognition results;model faithful transcripts;punctuation correction colloquial;label speaking style;speech;speech recognition;words transformations", "pdf_keywords": ""}, "b8b5b95a0471e0553a0e6cd5086f384cf0f4d4d8": {"ta_keywords": "estimated emphasis sequence;emphasis sequence transcription;speech lr hsmms;emphasis sequence target;language emphasis sequence;synthesizes emphasized speech;speech emotion emphasis;translate emphasis information;emphasis translation module;emphasis using acoustic;target language emphasis;speech synthesis;emphasis levels words;model translate emphasis;translates estimated emphasis;estimated emphasis;finally speech synthesis;speech results;emphasis levels;acoustic features speech;speech synthesis module;emphasis sequence using;features emphasis levels;features speech results;translated emphasis sequence;accurately translate emphasis;speech tags;language emphasis;s2st systems linguistic;emphasis sequence", "pdf_keywords": ""}, "0f2ea810c16275dc74e880296e20dbd83b1bae1c": {"ta_keywords": "attention ga reader;model gated attention;implementing gated attention;attention mechanism based;recurrent neural network;recurrent neural;gated attention;gated attention ga;attention;novel attention mechanism;attention mechanism;answer selection;novel attention;accurate answer selection;architecture novel attention;attention ga;answering cloze style;answer selection ga;query embedding;task cnn;neural network document;query embedding intermediate;states recurrent neural;query specific representations;cnn daily mail;reader build query;reader;interactions query embedding;answering;representations tokens document", "pdf_keywords": "attention ga reader1;attention visualization ga;ga reader trained;gated attention ga;ga reader features;model gated attention;attention ga;using gated attention;visualization ga reader;ga reader;reader trained;attention visualization;layer wise attention;gated attention information;gated attention;reader features;attention;attention information;ga reader1;wise attention visualization;attention information \ufb01lters;architecture novel attention;ga reader1 integrates;reader trained wdw;novel attention mechanism;novel attention;attention mechanism based;reader;attention mechanism;neural network document"}, "e58edbeb41f3d2d24832e6e3abb94baac754e3f7": {"ta_keywords": "evaluation text summarization;text summarization assessing;standard evaluation summarization;summarization assessing;evaluation summarization;evaluation summarization papers;summarization assessing reliability;text summarization;text summarization field;method text summarization;summarization field progressed;summary level evaluation;evaluating evaluation text;tasks text summarization;summarization field;summarization;summarization papers;evaluation text;automated evaluation metrics;summarization papers paper;evaluation metrics stand;text generation tasks;evaluation method text;evaluation metrics;text generation;automatic metrics using;automated evaluation;manual evaluation;automatic metrics;metrics using scoring", "pdf_keywords": "metaevaluation summarization metrics;evaluating summarization metrics;summarization metrics;summarization metrics updates;summarization metrics including;text summarization assessing;summarization assessing;meta evaluating summarization;evaluation text summarization;summarization systems cnn;evaluating summarization;metaevaluation summarization;summarization systems;summarization assessing reliability;systematic metaevaluation summarization;summarization community;summarization community shared;tasks text summarization;abstractive summarization systems;text summarization;bene\ufb01t summarization community;summary level evaluation;text summarization manik;potential bene\ufb01t summarization;task machine translation;summarization;extractive abstractive summarization;method text summarization;translation systems metrics;summarization manik"}, "0180c56bfbfb21243f8605e4c6f6aab2779d3ef0": {"ta_keywords": "user explanation leveraging;natural language explanations;language explanations optimal;explanation generation markov;explanation generation;natural language argumentation;policy based explanation;explanation leveraging existing;generate salient explanations;trust user explanation;explanations end user;language argumentation interface;interface explanation generation;explanation leveraging;argumentation interface;explanations optimal action;language explanations;natural language;explanations optimal;rely natural language;generation markov decision;argumentation interface explanation;explanations order build;language explanations order;based explanation ported;generation markov;language argumentation;explanations order;user explanation;salient explanations", "pdf_keywords": ""}, "197fcdfe05d0892ee7b4a98ef6fa74dfbcd14b48": {"ta_keywords": "crowdsourcing guarantee convex;functions crowdsourcing guarantee;computation crowdsourcing;functions crowdsourcing;human computation crowdsourcing;crowdsourcing guarantee;computation crowdsourcing involves;convex inference axioms;objective functions crowdsourcing;inference human computation;guarantee convex inference;crowdsourcing;crowdsourcing involves joint;convex inference;ensure convexity inference;crowdsourcing involves;inference human;inference axioms unfortunately;function inference human;convexity inference;inference axioms;inference ground;machines svms generally;machines svms;human computation;requirement model spammers;inference ground truth;support vector machines;convex optimization;literature convex optimization", "pdf_keywords": "crowdsourcing guarantee convex;functions crowdsourcing guarantee;models crowdsourcing satisfy;crowdsourcing guarantee;crowdsourcing satisfy;known models crowdsourcing;models crowdsourcing;functions crowdsourcing;crowdsourcing satisfy proposed;objective functions crowdsourcing;crowdsourcing;convexity human computation;human computation axiomatic;convex inference axiomatic;inference axioms unfortunately;guarantee convex inference;inference axioms;human computation guarantee;explicit modelling spammers;inference axiomatic;requirement model spammers;modelling spammers;modelling spammers possible;ensure convexity inference;function inference axioms;spammers construct reasonable;models human computation;convexity objective inference;inference axiomatic approach;convexity human"}, "18c00a9b1e6fde799ec5100cf0b1f37c306d061f": {"ta_keywords": "information retrieval;approach web search;web search;traditional information retrieval;information retrieval assumed;web queries;web search traditional;search traditional information;web queries posed;interoperable multimedia catalog;retrieval;multimedia catalog;relevant information extracting;finding relevant information;retrieval assumed queries;information extracting;multimedia catalog electronic;catalog electronic commerce;centric approach web;approach web;pages integrating information;data centric approach;search;queries posed topic;statistical learning methods;statistical learning;data centric;information extracting data;extracting data pages;catalog", "pdf_keywords": ""}, "5c283474bbb4838160410e24d33ce89ebaf32c07": {"ta_keywords": "method speaker clustering;speaker clustering clustering;speaker clustering;speaker clustering experiment;suitable speaker clustering;bayesian models speech;gaussian mixture model;bayesian method speaker;clustering accuracy markov;gaussian mixture;scale gaussian mixture;models speech;models speech processing;clustering clustering accuracy;clustering accuracy;speech processing;mixture model;compared variational bayesian;variational bayesian method;mixture model suitable;variational bayesian;method speaker;clustering accuracy decreased;clustering;methods terms clustering;terms clustering accuracy;clustering clustering;fully bayesian models;multi scale gaussian;model suitable speaker", "pdf_keywords": ""}, "2acc25a01a7ab7cd6b1a75d534ad29ea7d26f92d": {"ta_keywords": "retrieval subtopic retrieval;evaluating subtopic retrieval;baseline relevance ranking;relevance ranking;subtopic retrieval using;relevance ranking data;subtopic retrieval generalizes;subtopic retrieval;performing subtopic retrieval;likelihood relevance ranking;retrieval subtopic;document ranking;relevance mmr ranking;subtopic retrieval problem;retrieval methods;problem subtopic retrieval;relevance ranking shown;outperform baseline relevance;retrieval methods present;subtopic retrieval subtopic;ranking dependent documents;utility document ranking;traditional retrieval problem;retrieval generalizes;documents ranking;retrieval problem subtopic;non traditional retrieval;dependent documents ranking;traditional retrieval methods;document ranking dependent", "pdf_keywords": ""}, "0d3baef146655c5727452ccc0dd680d21d92ae4e": {"ta_keywords": "propose hierarchical gaussian;proposed hierarchical gaussian;hierarchical gaussian process;individuals consumer behaviors;hierarchical gaussian;consumer behaviors;introduce gaussian process;gaussian process approach;consumer behaviors vary;consumer behaviors marketing;behaviors marketing variables;different individuals consumer;gaussian process method;individuals consumer;distributions flexible models;relationship consumer behaviors;gaussian process;gaussian process deal;introduce gaussian;behaviors marketing;similarities purchasing behaviors;purchasing behaviors;capture relationship consumer;approach store level;prior distributions flexible;marketing variables automatically;marketing variables;consumer;process approach store;flexible models", "pdf_keywords": ""}, "0735fb79bf34698c1df4461a05ed51c232c412e4": {"ta_keywords": "transformer encoder attention;transformers recurrent neural;recurrent neural networks;trained models transformers;transformers recurrent;encoder attention;like transformers recurrent;encode task transformer;transformer trained;learned transformer;transformer transformer trained;sequence processing language;encoder attention feed;processing language rasp;programming language;transformer trained mimic;processing language;learned transformer transformer;conceivably learned transformer;recurrent neural;transformer encoder;language rasp;encode task;thinking like transformers;transformers;neural networks direct;sequence processing;layers attention heads;model transformer encoder;necessary encode task", "pdf_keywords": "transformer encoder attention;neural transformer;transformer trained task;realised neural transformer;learned transformer;transformer trained;transformer transformer trained;transformer encoder;learned transformer transformer;transformers tasks;processing language rasp;conceivably learned transformer;solution transformer trained;rasp language;transformers tasks \ufb01nding;program realised neural;implemented transformer train;language rasp;transformer trained mimic;neural transformer figure;written rasp language;operations implemented transformer;transformer encoder form;components transformer encoder;model transformer encoder;implemented transformer;train transformers tasks;transformer;transformers;transformer architecture"}, "6fea118a29d78340ae26c465ff06e80e55efbe3b": {"ta_keywords": "reading question answering;question answering model;question answering models;question answering;answering models;context question answering;answering model;incremental reading question;question answering present;approaches incremental reading;answering models reason;incremental reading;allow incremental reading;passage incrementally naive;answering model jointly;poorly incremental reading;reading question;incremental reading restriction;answering;passage incrementally;language models;incremental reading loss;language models model;incrementally naive;naive approaches incremental;incrementally naive approaches;unidirectional language models;incrementally;incremental;art question answering", "pdf_keywords": "comprehends text incrementally;question answering models;models text comprehension;incremental reading;comprehension question answering;incremental reading context;question answering;model question answering;question answering consumes;incremental models text;allow incremental reading;answering models;learn text incrementally;text incrementally;problem incremental reading;question answering qa;text incrementally introduce;text incrementally primary;text comprehension;text incrementally inspired;reads comprehends text;passage incrementally;text comprehension current;incrementally introduce incremental;reads comprehends;introduce incremental;consumes text incrementally;introduce incremental models;model reads comprehends;incrementally"}, "92731a953ad063eab1bc90dc541fb956f147a6ba": {"ta_keywords": "preferences making dining;personal preference driven;preference driven;food preferences;preference based software;preference handling used;preference handling;reasoning restaurant software;preference based;restaurant software seminal;food preferences motivating;potential preference based;preference driven presentation;restaurant software;used food preferences;preferences motivating examples;preference representation;preference representation reasoning;reasoning restaurant;based software restaurants;preferences motivating;dining experience enjoyable;software restaurants;preference;dining experience;making dining experience;software restaurants believe;preferences making;potential preference;use preferences making", "pdf_keywords": ""}, "0791fe161d947d1e4d3af279b261155b88bc9ddf": {"ta_keywords": "optimized clinical prediction;clinical prediction tasks;benchmark mortality prediction;clinical prediction;efficient data augmentation;predictive accuracy ensembling;ensembling predictions based;mortality prediction task;accuracy ensembling predictions;prediction tasks;ensembling predictions;mortality prediction;data augmentation;networks optimized clinical;benchmark mortality;clinical time series;postulated temporal clustering;prediction tasks postulate;improving predictive accuracy;sequences multivariate clinical;map benchmark mortality;multivariate clinical time;ensemble;invariant temporal clustering;temporal clustering;multiresolution ensemble mre;prediction task;propose multiresolution ensemble;improving predictive;multiresolution ensemble", "pdf_keywords": "optimized clinical prediction;clinical prediction tasks;healthcare time series;clinical time series;temporally coarsen downsample;postulated temporal clustering;irregular healthcare time;clinical prediction;2019 temporal clustering;invariant temporal clustering;temporal clustering;predictive accuracy ensembling;temporal clustering invariance;irregular time series;irregularly paced timestamps;prediction tasks;accuracy ensembling predictions;ensembling predictions based;time series grouping;multivariate clinical time;prediction tasks propose;paced timestamps;networks optimized clinical;ensembling predictions;paced timestamps postulate;data augmentation;improving predictive accuracy;sequences multivariate clinical;regularize deep neural;clustering invariance regularize"}, "8d35230fec724398bed3f5939e9fa6a94f55a785": {"ta_keywords": "learning differential privacy;privacy machine learning;private machine learning;differentially private algorithms;differential privacy privacy;differential privacy;private datasets;differential privacy machine;data private datasets;learned differentially privately;differentially private;data privacy;large differentially private;data privacy preserved;private algorithms;data private;differentially private machine;functions differentially private;non differentially private;privacy privacy;privacy preserving;protect patient privacy;privacy privacy preserving;differentially privately;privacy;private datasets number;private algorithms example;privacy preserved;privacy preserving machine;private algorithms objective", "pdf_keywords": "private machine learning;private datasets;learning di\ufb00erential privacy;data private datasets;private algorithms;di\ufb00erentially private algorithms;data private;privacy privacy;privacy preserving;privacy;private datasets number;privacy preserving machine;private algorithms paper;privacy objective machine;di\ufb00erential privacy privacy;privacy privacy preserving;privacy objective;di\ufb00erential privacy objective;di\ufb00erential privacy;learned differentially privately;preserving machine learning;public data;private machine;private;privately;missing data private;machine learning algorithms;functions di\ufb00erentially private;public data deal;learning algorithms achieved"}, "6eae6230ae277b6915706ec05241c8db6b9fab86": {"ta_keywords": "similarity search library;computation cheaper distance;similarity search;new similarity search;distance computations performed;distance computations;realistic benchmarks;number distance computations;benchmarks;realistic benchmarks end;parallelization looking;producing realistic benchmarks;similarity;parallelization looking best;benchmarks end;present new similarity;cheaper distance function;distance function approximates;new similarity;parallelization;degree parallelization looking;cheaper distance;algorithms pursue;computation cheaper;high degree parallelization;algorithms;costs computation cheaper;distance function;distance;performance methods measured", "pdf_keywords": ""}, "8872e32284467fcbeadd1edd2f11aff077de4ccf": {"ta_keywords": "existing rule learning;proposed rule learning;rule learning algorithm;rule learning systems;rule learning;algorithm ripperk;algorithm ripperk competitive;resulting algorithm ripperk;learning algorithm irep;learning algorithm;large noisy datasets;noisy datasets;learning systems computationally;algorithm irep large;existing rule;benchmark problems;benchmark problems irep;ripperk;ripperk competitive c4;5rules existing rule;benchmark;modiications resulting algorithm;datasets;algorithm irep;ripperk competitive;algorithm;computationally expensive;learning systems;resulting algorithm;learning", "pdf_keywords": ""}, "47442ea4c28d631a9d46a9c23454684b834e49ea": {"ta_keywords": "coreference trigger word;coreference abstract words;coreference trigger;resolving coreference trigger;coreference;approach resolving coreference;unsupervised event coreference;event coreference;resolving coreference;event coreference abstract;coreference abstract;semantics corpus;distributional semantics corpus;semantics corpus unsupervised;corpus unsupervised event;dataset biomedical language;essential distributional semantics;biomedical language processing;corpus vocabulary;corpus vocabulary captures;distributional semantics;language processing;corpus;trigger word refers;biomedical language;word refers;corpus unsupervised;original corpus vocabulary;trigger word;language processing 25", "pdf_keywords": ""}, "df873bde0b44e543634d109a7a8b1ba7dfaa8187": {"ta_keywords": "learns latent ontological;hierarchy latent semantic;latent semantic relationships;existing knowledge bases;knowledge bases;topic models infer;models topic models;topic models;latent ontological structure;concept hierarchy latent;latent concept hierarchy;model corpus web;components learned ontology;modeling text latent;jointly modeling text;learned ontology approach;semantic relationships;learned ontology;semantic relationships entities;knowledge bases kbs;latent ontological;latent semantic;model corpus;facts corpus;corpus web documents;identifies facts corpus;concept hierarchy;corpus web;ontological structure;modeling text", "pdf_keywords": ""}, "22dd93fe1a0b8e9cb83eaff6e2ecca0cd6693294": {"ta_keywords": "voice activity detection;voice activity;integrates voice activity;speech recognition integrated;speech recognition online;detecting speech segments;recognition online speech;cue detecting speech;speech recognition;detecting speech;end automatic speech;audio recordings threshold;ctc based voice;automatic speech;based voice activity;automatic speech recognition;online speech interface;recordings threshold;ctc pre softmax;speech segments;speech interface;speech segments simple;recordings threshold value;speech region;long audio recordings;voice;long audio;speech interface transcribing;integrates voice;activity detection vad", "pdf_keywords": "voice activity detectors;voice activity detection;speech recognition end;recognition voice activity;recognition online speech;end voice activity;speech recognition online;speech recognition integrated;online speech recognition;voice activity;speech recognition;speech recognition voice;integrates voice activity;speech recognition propose;automatic speech;terms speech recognition;online speech interface;end automatic speech;automatic speech recognition;recognition voice;based voice activity;speech interface;ctc based voice;conventional voice activity;speech interface transcribing;end end voice;end voice;ctc attention architectures;integrates voice;conclusion automatic speech"}, "9d332ad27bfce66ee725b413aa07bd93c355efdf": {"ta_keywords": "natural language augmentation;language augmentation framework;nl augmenter framework;language augmentation;nl augmenter using;nlp enhancing;present nl augmenter;processing nlp enhancing;trained nl augmenter;natural language tasks;nl augmenter;participatory pythonbased natural;language augmentation demonstrate;pythonbased natural language;natural language models;nl augmenter new;models natural language;augmenter new participatory;augmentation framework;augmentation framework supports;augmenter framework task;augmenter framework;tasks data augmentation;data augmentation;nlp enhancing diversity;natural language processing;augmenter;new participatory pythonbased;data augmentation important;participatory pythonbased", "pdf_keywords": "natural language augmentation;language augmentation framework;nlp enhancing;language augmentation;nl augmenter using;processing nlp enhancing;introduction data augmentation;present nl augmenter;nl augmenter new;pythonbased natural language;data augmentation;nl augmenter;natural language models;augmentation framework;models natural language;augmentation framework supports;natural language tasks;nlp enhancing diversity;augmenter new participatory;natural language processing;participatory pythonbased natural;processing nlp;e\ufb03cacy nl augmenter;language processing nlp;augmenter;variety natural language;augmenter using tranformations;data augmentation act;augmenter using;augmenter new"}, "8057a5e7bcb0be7059a6e632124bc861b533c794": {"ta_keywords": "language model rnn;pooling speech data;data pooling speech;short term memory;rnn lstm;language model lstm;rnn lstm lms;term memory recurrent;memory recurrent neural;speech data;recurrent neural network;75 rnn lstm;speech data channels;sequential discriminative training;dnn acoustic model;lstm;term memory;lstm lms;model rnn;discriminative training;model lstm;recurrent neural;pooling speech;model rnn lm;discriminative training produce;neural network language;memory recurrent;lstm lm;lstm lms used;experiments dnn acoustic", "pdf_keywords": ""}, "88167f36dced91c279162d68af7225f2b4e2091c": {"ta_keywords": "masked language models;natural language downstream;language downstream tasks;language downstream;language models glue;language models corpora;transferred natural language;pre trained unstructured;language models;pre training structured;models pre trained;corpora certain features;masked language;based masked language;tune language models;training structured data;models corpora;trained unstructured data;trained unstructured;pre training data;training structured;corpora;training data contributes;models corpora certain;pre trained;structured data does;models glue benchmarks;language;downstream tasks;fine tune language", "pdf_keywords": "language downstream tasks;natural language downstream;downstream natural language;pre training language;language downstream;training language model;models pre trained;trained human language;training language;pre training structured;pre trained unstructured;language tasks performance;language model mlm;language model human;language aids downstream;embeddings used pretraining;model human language;pre training data;language tasks;pre trained human;natural language tasks;human language results;performance pre trained;language model;human language data;transferred natural language;trained nonenglish language;mlm pre training;pre trained;pre trained nonenglish"}, "f752bf6f8c1502b8cb58aa1483ef598f9fc0d44c": {"ta_keywords": "cp nets ranking;counting cp nets;generating cp nets;generate cp nets;nodes cp nets;cp nets uniformly;nets ranking computing;arbitrary cp net;nets ranking;generating cp net;cp nets given;nets uniformly random;algorithms counting cp;cp nets;number nodes cp;nodes generating cp;rank arbitrary cp;nets uniformly;net given rank;nodes cp;cp nets encode;random generate cp;nets given number;cp net;cp net given;number nodes generating;computing rank arbitrary;nodes generating;number nodes;algorithms counting", "pdf_keywords": ""}, "5ebe542ee1a7eab7aad8e36ed53dbdd7ebd98c8d": {"ta_keywords": "capsule networks semantic;scene parsing understanding;scene parsing;encapsulation auto encoders;scene understanding;scene understanding core;semantic segmentation review;deep neural;objects involved capsule;semantic segmentation;capsule networks;networks semantic segmentation;using deep neural;convolutional neural networks;deep neural network;convolutional neural;involved capsule networks;representation given scene;statement scene parsing;auto encoders;importance scene understanding;capsule;scene efficient;involved capsule;core computer vision;usually convolutional neural;segmentation;concept encapsulation auto;alternative convolutional neural;using deep", "pdf_keywords": ""}, "73484141ca58d9714ac592e3667de416322b51eb": {"ta_keywords": "crossing representation wavelet;wavelet transform;wavelet transform practical;processing wavelet transform;representation wavelet transform;signal processing wavelet;representation wavelet;wavelet transform domain;processing wavelet;multiresolution analysis proposed;wavelet;signal reconstruction;multiresolution analysis;domain signal reconstruction;method multiresolution analysis;signal zero crossing;multiresolution;reconstructed image;signal reconstruction paper;practical method multiresolution;zero crossing representation;method multiresolution;reconstruct original signal;original signal zero;original signal method;image method proposed;reconstructed image good;actual image method;transform domain signal;image method", "pdf_keywords": ""}, "29263fa3632951be0ca617988d7c9ce651e74393": {"ta_keywords": "multilingual translation models;bilingual models encoders;multilingual training multilingual;training multilingual;multilingual training;multilingual translation;training multilingual training;translation models;compare bilingual models;bilingual models;initialized multilingual training;translation models work;machine translation model;multilingual training bene\ufb01cial;multilingual training tribute;decoders initialized multilingual;translation model;light multilingual translation;decoder machine translation;multilingual;varieties multilingual training;multilingual settings learning;multilingual training essen;different multilingual;machine translation;translation model different;machine translation mt;translation mt systems;multilingual settings;light multilingual", "pdf_keywords": "multilingual translation models;train multilingual models;multilingual models;learning multilingual training;multilingual machine translation;learning multilingual;multilingual training;abstract multilingual training;bilingual models encoders;multilingual training analysis;multilingual models using;multilingual training essential;compare bilingual models;bilingual models;train multilingual;multilingual training bene\ufb01cial;multilingual machine;multilingual translation;initialized multilingual training;multilingual training contribute;translation models;multilingual;decoders initialized multilingual;varieties multilingual training;translation models work;different multilingual;machine translation ting;breaking multilingual machine;multilingual settings learning;\ufb01rst train multilingual"}, "8b1be80cc1fabcd9ccea76d9a8830e2b07e71f0c": {"ta_keywords": "raspberries guy beer;beer playing imitation;imitation game deep;fruit vegetable beer;raspberries;review fruit;hallucinate review fruit;programmed know words;guy beer review;beer review remarkable;beer review;imitation game;fruit;raspberries guy;vegetable beer;fruit vegetable;playing imitation game;bouquet raspberries guy;playing imitation;recurrent neural network;recurrent neural;imitation;programmed;review remarkable;game deep learning;beer playing;vegetable beer playing;beer;bouquet raspberries;review fruit vegetable", "pdf_keywords": ""}, "76b36a059c0d8d66a1bf910de32b34dba19482fa": {"ta_keywords": "decoder predicts endpoint;streaming encoder decoder;decoding streaming encoder;synchronous decoding streaming;streaming encoder;synchronous decoding;decoder asr evaluations;blockwise synchronous decoding;block synchronous decoding;encoder decoder asr;synchronous decoding algorithm;endpoint prediction compute;decoding streaming;decoder automatic speech;encoder decoder automatic;speech recognition asr;decoder asr;endpoint prediction;encoder decoder;encoder;endpoint prediction endpoint;encoder features;decoder predicts;efficiency proposed decoding;recognition asr systems;tokens emitted encoder;emitted encoder features;prediction endpoint;proposed decoding;inference encoder decoder", "pdf_keywords": "streaming encoder decoder;decoding streaming encoder;synchronous decoding streaming;synchronous decoding;decoder asr;synchronous decoding algorithm;encoder decoder asr;blockwise synchronous decoding;streaming encoder;block synchronous decoding;decoding streaming;decoder predicts endpoint;encoder decoder automatic;speech recognition asr;decoder automatic speech;decoding algorithm enc;encoder decoder;recognition asr systems;decoder automatic;decoding algorithm hybrid;decoder asr emiru;encoder;decoding;decoder;recognition asr;decoder predicts;decoding algorithm;encoder features;decoding algorithm does;asr systems"}, "caabc3d0c5ece9d44fb2216a347362d4609934c1": {"ta_keywords": "large language models;code natural language;programming languages large;language models;language models lms;programming languages targeted;various programming languages;large language;language modeling;lingual corpus code;12 programming languages;mainly natural language;programming languages;natural language modeling;open source models;languages large language;languages targeted;results programming languages;languages large;code synthesizing;languages single machine;programming languages single;large open source;synthesizing code natural;languages targeted mainly;codeparrot various programming;synthesizing code;open source model;code synthesizing code;language descriptions", "pdf_keywords": "large language models;evaluation large language;language models code;source language model;large language;language models;polycoder trained github;code natural language;abstract large language;language model code;model code trained;code trained;lingual corpus code;language model;12 programming languages;language models lms;code synthesizing;popular programming languages;source language;programming languages;different programming languages;polycoder trained;language descriptions release;open source language;code synthesizing code;gpt architecture trained;code trained exclusively;synthesizing code natural;synthesizing code;languages single machine"}, "3a8129e6fe3ad9bc3a51e44da32424e38612e4cc": {"ta_keywords": "inference tensorlog efficient;inference tensorlog;compilation inference tensorlog;called tensorlog reasoning;tensorlog reasoning uses;tensorlog reasoning;probabilistic deductive database;tensorlog efficient;tensorlog clause logical;tensorlog efficient compilation;tensorlog clause;knowledge deep gradient;tensorlog;large knowledge bases;deductive database;database called tensorlog;differentiable tensorlog clause;probabilistic deductive;perform belief propagation;knowledge bases;called tensorlog;belief propagation;differentiable tensorlog;recursively perform inference;inference linear database;deductive database called;belief propagation bp;problem probabilistic deductive;logical theories containing;knowledge deep", "pdf_keywords": "probabilistic deductive database;differentiable deductive database;called tensorlog reasoning;tensorlog differentiable deductive;tensorlog reasoning uses;deductive database;probabilistic logic programming;stochastic logic programs;tensorlog reasoning performed;tensorlog reasoning;knowledge deep gradient;deductive database called;deductive database william;probabilistic deductive;probabilistic logic;database called tensorlog;deductive database prddb;formally tensorlog subsumes;described probabilistic deductive;formally tensorlog;prior probabilistic logic;tensorlog subsumes;motivated probabilistic deductive;process formally tensorlog;stochastic logic;variants stochastic logic;tensorlog;problem probabilistic deductive;logic programming models;process probabilistic deductive"}, "891fd2690a21f29b2ab54ee2249261d93c8cbc5c": {"ta_keywords": "self correction crowdsourcing;crowdsourcing worker answers;learning tasks crowdsourcing;correction crowdsourcing;tasks crowdsourcing;crowdsourcing worker;crowdsourced;crowdsourcing;crowdsourced labels;crowdsourced labels suffer;fast obtain crowdsourced;obtain crowdsourced;tasks crowdsourcing popular;setting crowdsourcing;setting crowdsourcing worker;obtain crowdsourced labels;stage setting crowdsourcing;crowdsourcing popular;crowdsourcing popular means;answering randomly stage;errors crowd sourcing;inadvertent errors crowd;answering randomly;refrain answering randomly;noisy reference answer;crowd sourcing;crowd sourcing workers;machine learning tasks;worker answers questions;answers questions", "pdf_keywords": ""}, "0ce6db2fb8c691ff8a89bd01f379ce92b1d248d0": {"ta_keywords": "topical retrieval classification;topical classification;non topical classification;informative words frequent;frequent terms text;text models contagion;topical classification soft;terms text models;contagion classification;contagion classification soft;text models;topical retrieval;models contagion classification;probabilistic models contagion;appropriate topical retrieval;retrieval classification;bayesian methods frequent;words frequent paper;frequent terms;contagion statistic introduce;informative words;sentiment author informative;retrieval classification tasks;words frequent;introduce probabilistic models;models contagion statistic;clustering based poisson;introduce probabilistic;contagion statistic;classification soft clustering", "pdf_keywords": ""}, "6994b9860248aea10f8b8bac74e87afd3fcdc842": {"ta_keywords": "sets named entities;benchmark sets languages;google sets terms;web set expansion;expansion named entities;superior google sets;expanding sets named;sets languages;google sets;set expansion named;named entities;set expansion refers;set expansion;benchmark sets;method expanding sets;sets named;named entities using;36 benchmark sets;sets terms;sets languages showing;partial set objects;expanding sets;sets terms mean;language independent set;sets;entities;entities using web;independent set expansion;web set;set objects", "pdf_keywords": ""}, "49f9afa4d0405019d01b55529ce4167380acc103": {"ta_keywords": "controllable speech modification;speech production mapping;input speech waveform;mapping articulatory speech;speech modification capable;speech modification based;articulatory controllable speech;articulatory speech production;speech waveform;developed speech modification;performing speech articulatory;speech articulatory inversion;degradation synthetic speech;synthetic speech;filter input speech;speech modification;speech articulatory;vocoderbased waveform generation;articulatory speech;controllable speech;speech caused modeling;waveform modification using;speech waveform time;waveform modification;speech production;synthetic speech caused;vocoderbased waveform;waveform generation framework;articulatory inversion mapping;direct waveform modification", "pdf_keywords": ""}, "e1bb329621de73d08c47beae9b5439a1c244eb1a": {"ta_keywords": "augmentations novelty detection;novelty detection;suited novelty detection;novelty detection identifying;various novelty detection;novelty detection scenarios;novelty detection designing;contrastive learning visual;representation suited novelty;augmentations novelty;contrastive learning;learning visual representations;conventional contrastive learning;learning visual;contrastive learning methods;shifted augmentations novelty;attempts learning representation;instances conventional contrastive;learning representation;training scheme contrasts;various novelty;novelty;visual representations;success contrastive learning;learning representation suited;contrasting shifted instances;detection identifying;method various novelty;image benchmark datasets;visual representations end", "pdf_keywords": "novelty detection contrastive;novelty detection;novelty detection identifying;various novelty detection;abstract novelty detection;novelty detection scenarios;csi novelty detection;detection contrastive learning;contrastive learning distributionally;detection contrastive;new detection score;contrastive learning scheme;existing contrastive learning;contrastively learned;detection identifying;learning distributionally shifted;contrastive learning;contrastively learned representation;utilizes contrastively learned;detection score;new detection;propose new detection;representation detecting oods;proper detection score;abstract novelty;learning distributionally;novelty;various novelty;discriminative representation detecting;detection"}, "931a103258c96a1230dc5c7e38a1cd0b095b9d62": {"ta_keywords": "learning language model;learn language model;speech phoneme lattice;gram language model;language model directly;language model;language model noisy;lexical acquisition techniques;language model text;language model construction;continuous speech phoneme;language model prior;learning word;large vocabulary speech;processing lexical acquisition;robustly learn language;model prior linguistic;language model able;speech phoneme;lexical acquisition;asr phoneme;approach language model;lattice processing lexical;vocabulary speech;learning language;phoneme lattice;gram language;reduce asr phoneme;prior linguistic knowledge;phoneme lattice created", "pdf_keywords": ""}, "a5690b0a514a7cbc913871e41e54c9ad4f6362db": {"ta_keywords": "evaluated submissions human;translations task provides;automatic evaluation;sourced translations task;judgment automatic evaluation;translations task;automatic evaluation bleu;evaluated submissions;task evaluated submissions;submissions human judgment;challenges task evaluated;professionally sourced translations;sourced translations;language pairs english;translations;evaluated blind test;japanese submitted systems;pairs english;evaluated blind;language pairs;systems evaluated blind;testbed representing challenges;human judgment automatic;language;noisy comments;focus language;focus language pairs;english japanese submitted;submitted systems evaluated;evaluation", "pdf_keywords": "machine translation robustness;robustness machine translation;translation robustness;translation robustness xian;machine translation mt;machine translation;task machine translation;translation mt task;translations new task;sourced translations;translations;professionally sourced translations;texts translations;sourced translations new;translations new;translation mt;texts translations english;translations english;translation directions;translation directions eng;media texts translations;task improving robustness;improve models robustness;translations english eng;improving robustness machine;translation;models robustness noisy;robustness machine;models robustness;improving robustness"}, "6aecc93c2d61da073b70dec19795172ca1ff3405": {"ta_keywords": "counterfactual invariance spurious;counterfactual invariant predictors;introduce counterfactual invariance;counterfactual invariance outof;implications counterfactual invariance;counterfactual invariance;counterfactual;similarly counterfactual invariance;learning approximately counterfactual;introduce counterfactual;counterfactual invariance depend;counterfactual invariant;tests introduce counterfactual;cause label counterfactual;counterfactual invariance formalization;predictors access counterfactual;similarly counterfactual;label counterfactual invariance;counterfactual invariance implies;connect counterfactual invariance;counterfactual examples informally;counterfactual examples;predictions connect counterfactual;implications counterfactual;spurious correlations stress;label counterfactual;informally spurious correlation;invariance spurious correlations;approximately counterfactual;spurious correlations", "pdf_keywords": "counterfactual invariant predictors;counterfactual invariance formalization;introduce counterfactual invariance;2021 counterfactual invariance;counterfactual invariance robust;counterfactual invariance spurious;predictors access counterfactual;learning approximately counterfactual;counterfactual invariance;counterfactual invariance domain;counterfactual invariant;clarifying counterfactual invariance;tools causal inference;counterfactual invariance distributional;connect counterfactual invariance;introduce counterfactual;relationship counterfactual invariance;causal inference synthetic;causal inference formalize;counterfactual invariance buys;counterfactual;counterfactual examples introduce;2021 counterfactual;examples introduce counterfactual;counterfactual examples;aim clarifying counterfactual;correlations pass stress;clarifying counterfactual;approximately counterfactual invariant;causal inference"}, "dfd4beb1ecf70b07eb4a52e6ae58f3357e66f478": {"ta_keywords": "invariant representation learning;representation learning irl;representation learning;speech recognition;matched representations layer;matched representations;advances speech recognition;learning irl training;invariant representation;speech recognition current;representations;representation propose invariant;coerce matched representations;propose invariant representation;learning irl;recognition current models;irl training iteration;example sample noisy;recognition;representations layer;sample noisy counterpart;domain noise;training example;demonstrated librispeech dataset;training iteration;rapid advances speech;noisy counterpart apply;learning;sample noisy;training example sample", "pdf_keywords": "baselines invariantrepresentation learners;representations data augmentation;invariantrepresentation learners;inductive bias regularization;data augmentation models;augmentation improves generalization;model trained noise;data augmentation improves;invariantrepresentation learners irls;bias regularization;outperform wellknown regularization;augmentation models clean;noise invariant representations;trained noise augmented;bias regularization terms;data augmentation;representations penalizing differences;regularization tactics;models trained;domain noise improve;robust baselines invariantrepresentation;augmentation models;noise improve convergence;model trained;regularization tactics like;robust domain noise;generic data augmentation;wellknown regularization tactics;noise augmented data;models trained irl"}, "a427334e296b6be27c3a9c7d6b942d6468e487b8": {"ta_keywords": "sequential deployment wireless;relay networks deployment;deployment wireless sensors;deployment wireless sensor;radio propagation;wireless relay networks;wireless sensors relays;impromptu deployment wireless;model radio propagation;optimal sequential deployment;sensor networks situations;radio propagation extract;deployment wireless;computation optimal deployment;wireless relay;deployment connected wireless;efficient deployment algorithms;measurement based deployment;wireless sensor networks;wireless relay network;networks deployment agent;relay networks;deployment algorithms;networks deployment;relays line forest;optimal deployment;multihop wireless relay;sensor networks;relay locations;network computation optimal", "pdf_keywords": ""}, "823956ee7b994735f3605f426a71e7f85d86f1d4": {"ta_keywords": "semantic frame induction;unsupervised semantic frame;unsupervised semantic;perform unsupervised semantic;semantic frame;verb class clustering;frame induction;frame induction using;web scale corpus;framenet derived dataset;corpus perform unsupervised;task framenet derived;frame induction replicable;framenet derived;corpus;task framenet;scale corpus;framenet;triclustering use dependency;induction using triclustering;approach triframes;based approach triframes;class clustering task;semantic;results task framenet;verb class;triclustering;approach triframes shows;scale corpus perform;triframes", "pdf_keywords": "triclustering algorithms unsupervised;frame induction algorithms;unsupervised frame induction;clustering biclustering;graph based triclustering;frame induction propose;evaluations frame induction;standard clustering biclustering;triclustering algorithms;algorithms unsupervised frame;dataset based framenet;frame induction task;algorithms tested triclustering;frame induction enabling;apply triclustering algorithms;frame induction;new approach triclustering;triclustering methods baselines;framenet derived dataset;triclustering task;simultaneously clustering;clustering biclustering cheng;simultaneously clustering objects;triclustering algorithm;based triclustering algorithm;approach triclustering achieving;verb class clustering;evaluation frame induction;triclustering algorithm yields;approach triclustering"}, "6eb5029dabd60eb47fddebb5919c613d399fddc6": {"ta_keywords": "discriminant analysis speech;speech recognizer linear;speech recognition objective;speech recognizer;discriminant analysis lda;speech recognition;analysis speech recognition;linear discriminant analysis;recognizer linear discriminant;based sequential discriminative;information linear discriminant;sequential discriminative criterion;sequential lda slda;discriminant analysis;various discriminative feature;errors speech recognizer;training lda;discriminative feature transformation;discriminative feature;sequential discriminative;lda slda based;sequential lda;mmi training lda;linear discriminant;improve discriminability maximizing;function proposed lda;discriminative criterion;discriminative criterion computed;lda slda;training lda does", "pdf_keywords": ""}, "edca37b2004861513c54e7e97b64d4e00e72003f": {"ta_keywords": "learning indeterminate clauses;learning learning logic;learning logic;learning logic programs;formally problem learning;learning learning indeterminate;learning indeterminate;clauses logarithmic depth;depth determinate clauses;logarithmic depth learnable;dnf determinate clauses;clause focus generalizations;problem learning single;clauses used practical;single horn clause;logic programs;logic programs examples;determinate clauses logarithmic;learning dnf determinate;clauses indeterminate variables;horn clause;variables equivalent learning;determinate clauses;generalizations language constant;clauses indeterminate;problem learning;clauses logarithmic;determinate clauses used;practical learning;practical learning systems", "pdf_keywords": ""}, "49775f20431c4a605c5dcc7111c9fe785bf00c62": {"ta_keywords": "policy gradient markov;optimal policy gradient;optimality policy gradient;markov coherent risk;optimizing coherent risk;convergence optimality policy;policy gradient;optimize coherent risk;optimal policy;risk aversion reinforcement;optimality policy;policy gradient pg;risk difficult markov;coherent risk functionals;gradient markov;aversion reinforcement learning;gradient markov coherent;globally optimal policy;coherent risk mcr;suboptimality learned policy;learned policy characterizing;risk functionals;reinforcement learning;coherent risk present;markov decision processes;reinforcement learning emerging;learned policy;coherent risk;risk aversion;model risk aversion", "pdf_keywords": "policy gradient markov;optimality policy gradient;markov coherent risk;convergence optimality policy;sampling learn policies;optimize coherent risk;coherent risk functionals;aversion reinforcement learning;pg learn risk;risk aversion reinforcement;optimality policy;policy gradient;learn risk;importance sampling learn;gradient markov coherent;reinforcement learning;gradient markov;reinforcement learning emerging;risk functionals;conditional valueat risk;policies exploding gradient;learn risk sensitive;risk functionals class;model risk aversion;risk sensitive policies;learn policies;risk aversion;importance sampling;coherent risk;learn policies exploding"}, "4350ce87dd3ec067f1e583ad415f71ef4ba6075e": {"ta_keywords": "dependency parser japanese;japanese dependency parsing;training dependency parsers;dependency parsers trained;dependency parsing;based dependency parser;parser japanese trained;dependency parsers;partially annotated corpora;dependency parser;dependency parsing approach;parsers partially annotated;dependency parsers partially;parser japanese;annotated corpora;trained partially annotated;parsers trained;annotated corpora allowing;annotated corpora use;parsers trained fully;trained fully annotated;word based dependency;japanese dependency;fully annotated data;partially annotated;parsers partially;parsing;art dependency parsers;parsing approach allows;parsers", "pdf_keywords": ""}, "598e2d69d3573adae1f0e3bbe54d10c43f48e0b0": {"ta_keywords": "stochastic gradient;proximal stochastic gradient;stochastic gradient method;stochastic optimization;optimization distributional drift;stochastic optimization distributional;efficiency proximal stochastic;guarantees stochastic algorithms;stochastic algorithms;stochastic algorithms iterate;schedule stochastic optimization;convergence guarantees stochastic;proximal stochastic;guarantees stochastic;stochastic dynamics;stochastic dynamics depend;gradient noise time;stochastic;minimizing convex function;optimization error gradient;tracking efficiency proximal;stepdecay schedule stochastic;minimizing convex;convex function evolving;possibly stochastic dynamics;schedule stochastic;optimization distributional;distributional drift;algorithms iterate averaging;distributional drift provide", "pdf_keywords": "proximal stochastic gradient;stochastic gradient;stochastic gradient method;error proximal stochastic;stochastic optimization;proximal stochastic;e\ufb03ciency proximal stochastic;optimization distributional drift;convergence guarantees stochastic;guarantees stochastic algorithms;probability stochastic optimization;stochastic algorithms;stochastic optimization distributional;guarantees stochastic;stochastic algorithms iterate;learning rate noise;32 proximal stochastic;noise variance gradient;stochastic;stochastic dynamics;variance gradient oracle;gradient method time;stochastic dynamics depend;gradient oracle;distributional drift;minimizing convex function;variance gradient;unknown possibly stochastic;tracking error proximal;minimizing convex"}, "22f93927c487e0e0e0d2844489423bcb5d21b45c": {"ta_keywords": "discrete optimization;optimization;discrete", "pdf_keywords": ""}, "19803adec3b97fb2e3c8097f17bf33fabf311795": {"ta_keywords": "supervision contrastive regularized;weak supervision contrastive;trained language models;trained language model;weak supervision labeled;pre trained language;supervised fine tuning;supervision contrastive;contrastive self training;lms weak supervision;supervision fine tuning;generated weak supervision;performance fully supervised;fully supervised fine;trained language;fully supervised;regularized self training;using weak supervision;model weak supervision;language models lms;pre trained lms;language model weak;tuning pre trained;nlp tasks;nlp tasks require;supervised fine;self training framework;contrastive regularized self;weak supervision;trained lms", "pdf_keywords": "supervision contrastive regularized;trained language models;pretrained language model;tuning pretrained language;weak supervision contrastive;language models weak;trained language model;models weak supervision;pre trained language;language model weak;model weak supervision;lms weak supervision;supervision contrastive;pretrained language;supervision underpinned contrastive;language models lms;trained models weak;\ufb01netuning pre trained;language models;underpinned contrastive regularization;trained lms weak;trained language;pre trained models;performance fully supervised;weak supervision;propose contrastive regularized;weak supervision underpinned;regularized self training;nlp tasks;pre trained lms"}, "53dc99155c52979e311a403571f1b1d57ff73b48": {"ta_keywords": "tissue modeling;biological tissue modeling;tissue modeling aims;model biological tissues;tissues digital image;biological tissues digital;workflow biological tissue;learning model material;microstructure properties learned;neural operator learning;tissues digital;operator learning;material response modeled;displacement tracking;predict displacement field;biological tissues;biological tissue;predict displacement;operator learning model;physics guided neural;guided neural operator;tissues;tissue;learned implicitly data;operator learning approach;displacement field material;displacement field based;model material response;image correlation measurements;knowledges material microstructure", "pdf_keywords": "biological tissue modeling;tissue modeling;tissue modeling aims;model biological tissues;tissue response learning;modeling biological materials;material modeling tasks;driven material modeling;tissues digital image;modeling biological;predict displacement \ufb01eld;material modeling;predict displacement;dic displacement tracking;material modeling using;displacement \ufb01eld based;work\ufb02ow biological tissue;biological tissues digital;physics constraints neural;model biological;displacement tracking;method modeling biological;model porcine heart;physics guided neural;dic displacement;fourier neural;work\ufb02ow model porcine;data driven material;displacement tracking measurements;physics constraints data"}, "6bca949b7ce69d6a43120d75e65f43d4c5a80ed4": {"ta_keywords": "incentive design;incentive design core;state art incentive;incentive design clear;incentive;research incentive design;art incentive design;communities economics control;research incentive;area research incentive;intelligent infrastructure commerce;economics control;core communities economics;economics control theory;communities economics;art incentive;resource constraints;infrastructure commerce;economics;intelligent infrastructure;self interested parties;designing mechanisms;environments resource constraints;machine learning;infrastructure;designing mechanisms help;humans operations domains;exogenous uncertainties dynamic;interested parties avoiding;infrastructure commerce led", "pdf_keywords": ""}, "0ee468b9b709a2610c4b574d67218e7960350224": {"ta_keywords": "augmentation text based;neural machine translation;efficient data augmentation;data augmentation text;augmentation text;data augmentation strategy;experiments translation datasets;data augmentation policy;augmentation algorithm neural;data augmentation;augmentation strategy nmt;machine translation nmt;machine translation;simple data augmentation;augmentation strategy;randomly replacing words;alternatives word dropout;augmentation policy;data augmentation algorithm;augmentation policy desirable;subsumes existing augmentation;existing augmentation;augmentation;translation datasets;methods data augmentation;translation nmt experiments;replacing words source;replacing words;existing augmentation schemes;augmentation algorithm", "pdf_keywords": "neural machine translation;augmentation text based;machine translation nmt;augmentation strategy nmt;experiments translation datasets;data augmentation text;data augmentation strategy;augmentation text;augmentation algorithm neural;translation nmt experiments;augmentation techniques nmt;machine translation;randomly replacing words;data augmentation policy;data augmentation techniques;data augmentation;knowledge data augmentation;augmentation technique nmt;simple data augmentation;nmt experiments translation;replacing words source;replacing words;augmentation strategy;ef\ufb01cient data augmentation;data augmentation technique;seek data augmentation;alternatives word dropout;subsumes existing augmentation;augmentation;data augmentation algorithm"}, "4b22503d6da9ff3222d94106cc7425ea4fea43af": {"ta_keywords": "dependency parsing weibo;parsing weibo efficient;parsing weibo;sentence dependency parsing;parsers trained;big challenge parsers;dependency parsing;formulate dependency parsing;parsing;task nlp widely;parsers;nlp widely;challenge parsers trained;extraction question answering;task nlp;dependency parsing problem;weibo efficient probabilistic;challenge parsers;core task nlp;parsers trained traditional;token sentence dependency;nlp widely used;nlp;parsing problem small;question answering machine;sentence dependency;efficient probabilistic logic;dependency parsing core;parsing problem;newswire corpora", "pdf_keywords": ""}, "1fa69608666e66452df56b1f71282def7ac16035": {"ta_keywords": "theoretical voting paradoxes;voting paradoxes;paradoxes associated voting;voting paradoxes orders;choice manipulation bribery;voting means preference;theoretical voting;test theoretical voting;computational social choice;preference aggregation;social choice manipulation;preference information;preference aggregation work;structured preference information;choice manipulation;associated voting;structured preference;associated voting occur;results social choice;information structured preference;manipulation bribery work;means preference aggregation;manipulation bribery;voting occur rarely;voting occur;preference information new;voting;social choice subfield;bribery work;voting means", "pdf_keywords": ""}, "2f7c03f0d3c6f51728e925a874c49a25559cc6b3": {"ta_keywords": "compositional question answering;documents answer compositional;long documents answering;question answering;question answering long;documents answering complex;retrieval compositional;dochopper answer compositional;documents answering;multihop retrieval compositional;answering long documents;retrieval method dochopper;retrieval compositional question;answers contrast retrieval;answer compositional questions;retrieval based;questions long documents;long documents answer;retrieval;predicting answers;answer compositional;answering complex questions;answering long;evidence predicting answers;sentence embedding document;reading long documents;retrieval method;document mixes retrieved;representation dochopper outperforms;end multihop retrieval", "pdf_keywords": ""}, "c13c400d1f481863d57ec265d296b0a08ec77876": {"ta_keywords": "speech recognition asr;audio segmentation non;realize audio segmentation;audio segmentation;automatic speech;speech recognition;automatic speech recognition;non autoregressive decoding;connectionist temporal classification;autoregressive decoding;autoregressive decoding using;recognition asr systems;practical automatic speech;recognition asr;segmentation non autoregressive;temporal classification ctc;autoregressive decoding generates;audio;realize audio;method realize audio;decoding;temporal classification;ctc decode length;baseline ctc decode;asr systems;segment insertion based;decoding using;ctc decode;classification ctc loss;decode length token", "pdf_keywords": ""}, "0a2ba7b1c05062d2bb7cd35e218fe08d6ea29488": {"ta_keywords": "representing email meeting;meeting email related;email meeting information;graph documents meeting;meeting email;email meeting;meeting information joint;meeting information;framework representing email;data email meeting;representing email;email meeting assistant;framework meeting email;meeting descriptions connected;email related tasks;email related;graph walks framework;information joint graph;timeline structural graph;graph documents;meeting descriptions;meeting;documents meeting;assistant using graph;social networks timeline;networks timeline structural;meeting assistant using;structural graph;meeting assistant;string graph documents", "pdf_keywords": ""}, "fd54706252a094d592feadf53a0a3ffed4af9295": {"ta_keywords": "speech recognition technology;speech recognition;speech recognition real;reference chime challenge;field speech recognition;chime challenge;chime challenge targets;talking tablet device;3rd chime challenge;automatic speech recognition;microphone;chime challenge series;modeling reference chime;talking tablet;microphone array;speech recognition paper;person talking tablet;processing dnn acoustic;processing automatic speech;performance automatic speech;dnn acoustic modeling;automatic speech;channel microphone;channel microphone array;far field speech;reference chime;recognition technology;field speech;recognition technology promoting;microphone array paper", "pdf_keywords": ""}, "142407d3cb61067e88d385f95ae238c74b19d554": {"ta_keywords": "gold data text;corpus;fbdata xlsx gold;file fbdata;fbdata;excel file fbdata;000 facebook posts;file fbdata xlsx;gold data;manually annotated sentiment;archive;xlsx gold data;corpus consisting;archive contains data;gold posts txt;data text files;annotated sentiment;corpus consisting 10;text files posts;facebook posts;files posts gold;fbdata xlsx;facebook posts manually;statistics excel file;annotated sentiment 587;data text;000 facebook;10 000 facebook;posts manually annotated;manually annotated", "pdf_keywords": ""}, "75983c55a489d526427fe399ce2670376168a2f0": {"ta_keywords": "paraphrase database japanese;japanese paraphrase resources;extract paraphrases similar;domain paraphrase database;paraphrase database;japanese japanese paraphrase;paraphrases similar;generate paraphrases;extract paraphrases;extract generate paraphrases;generate paraphrases building;japanese paraphrase;paraphrases similar quality;mismatch extract paraphrases;paraphrases building free;paraphrase resources cover;paraphrase resources;paraphrases building;paraphrases;statistical machine translation;domain paraphrase;parallel corpora alignment;general domain paraphrase;corpora alignment techniques;parallel corpora;machine translation;phrase based statistical;machine translation extract;paraphrase;corpora alignment", "pdf_keywords": ""}, "5bc188b4ab7b27649236fad6a686b2cfe6368219": {"ta_keywords": "document topic modeling;topic modeling presented;topic modeling;composition topic modeling;topic modeling technique;discovering abstract topics;mining paper commonsense;topics occur collection;opinion mining paper;paper commonsense knowledge;abstract topics occur;text auto categorization;topics occur;commonsense knowledge based;document topic;topics;abstract topics;opinion mining;categorization opinion mining;average agglomerative clustering;commonsense knowledge;particular word distributions;word distributions;word distributions making;agglomerative clustering;algorithm effective texts;word occurrence particular;knowledge based algorithm;paper commonsense;clustering", "pdf_keywords": ""}, "d8d49cc56b303d6ed0e821f8593e2f7acd1b4fb4": {"ta_keywords": "supervised sound event;sound event detection;semi supervised sound;dcase2020 task4 sound;supervised sound;audio feature sequence;task4 sound event;sound event;audio feature;event detection separation;context information audio;information audio feature;task4 sound;event detection technical;transformer semi supervised;dcase2020 task4;audio;detection separation domestic;event detection;semi supervised;information audio;convolution networks efficiently;detection separation;convolution networks;dcase2020;event based macro;supervised;feature sequence convolution;submission dcase2020 task4;detection technical", "pdf_keywords": ""}, "a61ef7be5b5c9fbc6654f7c17fa595976652416b": {"ta_keywords": "online organ matching;organ matching;australia matching donations;kidney transplants;authority australia matching;transplants performed australia;matching donations deceased;australia matching;matching donations;kidney transplants performed;online organ;matching;data provided organ;mechanisms perform matching;donations deceased patients;perform matching;transplants performed;mechanisms online organ;85 kidney transplants;organ tissue authority;matching compare;transplants;tissue authority australia;perform matching compare;provided organ tissue;matching compare new;provided organ;currently consideration organ;australia mechanisms online;consideration organ tissue", "pdf_keywords": ""}, "ff1a1e39a94b9ca31e6013d12bc2d27f7a31567c": {"ta_keywords": "head attention models;head attention model;end speech recognition;multi head decoder;multiple decoders attention;multi head attention;attention based encoder;head decoder end;head decoder;decoders attention;decoders attention integrates;attention models capture;attention models;attention model;head attention;contexts attention based;speech recognition extension;speech recognition;attention integrates outputs;end end speech;decoder end end;different attention functions;attention based;end speech;attention functions;attention functions used;capture different speech;contexts attention;multiple decoders;decoder end", "pdf_keywords": "end speech recognition;speech recognition end;head attention models;head attention model;multi head decoder;head decoder end;multihead decoder end;multi head attention;multihead decoder;speech recognition extension;head decoder;attention models capture;attention based encoder;end end attention;called multihead decoder;attention models;speech recognition tomoki;recognition end end;end end speech;attention model;speech recognition;end speech;decoder end end;terms speech recognition;end attention dynamical;contexts attention based;head attention;capture different speech;attention model experimental;decoder end"}, "3b4b5e72a2f84d079d0d1d825309c2f6ded76539": {"ta_keywords": "speaker adaptation;parameters speaker adaptation;speaker adaptation experiments;transfer vector estimation;training transfer vectors;improve speech recognition;recognition performance adaptation;accurate transfer vectors;transfer vectors transfer;speech recognition;vector estimation gaussian;transfer vectors;adaptation focus transfer;vectors transfer;transfer vector;vectors transfer vector;speech recognition performance;adaptation data;focus transfer vector;map adaptation;transfer vectors small;vector estimation;improve speech;adaptation data compared;parameters speaker;transfer vector decomposed;conventional map adaptation;class estimation direction;adaptation experiments proposals;map adaptation focus", "pdf_keywords": ""}, "77e5c4fa595466aa51d29327a60f9d4af4436876": {"ta_keywords": "learns incrementally;learns incrementally ia;learning algorithm incremental;explanation based learning;knowledge intensive inductive;ebl learns incrementally;inductive learning;inductive learning algorithm;concept learner;intensive inductive learning;concept learner ia;cover based learning;learnability finally;learning longer easily;learnability;learnability finally compared;based learning algorithm;bounded learnability finally;learning algorithm called;alternative learning algorithm;bounded learnability;mistake bounded learnability;learning algorithm ebl;learns;based learning ebl;learning longer;based learning;learning algorithm;learning;ebl learns", "pdf_keywords": ""}, "3c0d4dc4237934e37467f4ede3af859bcb140abf": {"ta_keywords": "context crowd counting;token attention;person count image;token attention module;crowd dataset;count image regression;global context crowd;crowd counting specifically;crowd dataset method;attention module;propose token attention;image regression token;crowd counting;channel wise attention;attention module tam;context crowd;person count;jhu crowd dataset;features channel wise;count image;attention informed context;predict total person;features channel;total person count;attention;image regression;tokens corresponding image;crowd;features global information;classification add context", "pdf_keywords": "supervised crowd counting;point supervised crowd;supervised crowd;crowd counting using;crowd counting;count crowd;count crowd best;total count crowd;counting using vision;features global receptive;token attention;token attention module;regression token module;module token attention;crowd best knowledge;learning context token;crowd;crowd best;point supervised;attention module;global receptive;regression total count;token using regression;supervised;regression token;global receptive \ufb01elds;using vision transformers;vision transformers;attention module tam;investigate point supervised"}, "9f49ed155d7575d181d16dd5bc92b754cae0bea9": {"ta_keywords": "blind subspace identification;minimization blind subspace;subspace identification;subspace methods blind;norm minimization blind;blind subspace;methods blind identification;subspace identification n2bsid;minimization blind;known subspace;subspace methods;nuclear norm minimization;blind identification multiple;identification problem rank;known subspace able;identification multiple input;output linear systems;rank constrained optimization;blind identification;rank constrained;identification feasible;norm minimization;problem rank constrained;lie known subspace;linear systems;extension subspace methods;output nuclear norm;identification problem;subspace;subspace able", "pdf_keywords": ""}, "91a2d496553cfee2b66906f704b8e3d081e2d1bf": {"ta_keywords": "augment logic programs;inductive logic programming;logic programs variation;logic programming ilp;logic programs;counting clauses augment;learn counting clauses;flip learn counting;logic programming;augment logic;learn counting;clauses augment logic;programming ilp methods;programming ilp;counting clauses;description logics;clauses augment;programs variation number;represented calling tree;used description logics;methods predicting fault;evaluate inductive logic;description logics signiicantly;inductive logic;predicting fault density;programs variation;predicting fault;fault density classes;calling tree labeled;faults errors", "pdf_keywords": ""}, "2bbb33ab8124e5078ec39e821a25c24c20a31b9b": {"ta_keywords": "crowdsourced audio;organized crowdsourced audio;crowdsourced audio transcription;crowdsourcing technology users;make crowdsourcing technology;helping crowdsourcing transition;crowdsourcing technology;helping crowdsourcing;make crowdsourcing;crowdsourcing transition;crowdsourcing;workshop crowd science;crowd science workshop;organized crowdsourced;crowdsourced;crowdsourcing transition art;face make crowdsourcing;year organized crowdsourced;second workshop crowd;goal helping crowdsourcing;workshop crowd;participants world workshop;crowd science organized;audio transcription shared;crowd science;2021 edition workshop;world workshop features;world workshop;editions crowd science;workshop features invited", "pdf_keywords": ""}, "0e96925c57b3325e7e37c1964b518e9276024cbf": {"ta_keywords": "natural language processing;language processing emnlp;methods natural language;2016 conference empirical;language processing;empirical methods;conference empirical methods;natural language;empirical methods natural;empirical;conference empirical;emnlp 2016;processing emnlp;processing emnlp 2016;emnlp 2016 austin;emnlp;language;processing;methods natural;2016 conference;proceedings 2016 conference;2016 austin texas;methods;2016 austin;november 2016;conference;usa november 2016;texas usa november;proceedings 2016;2016", "pdf_keywords": ""}, "7c2ff8fa0d24ed712e4bc2dbdb370a1cd62c965b": {"ta_keywords": "automated discrimination heart;discrimination heart disease;heart disease using;discrimination heart;automated discrimination;heart disease;disease using artificial;automated;using artificial;artificial;discrimination;heart;disease using;disease;using", "pdf_keywords": ""}, "3d846cb01f6a975554035d2210b578ca61344b22": {"ta_keywords": "semi supervised learning;distantly supervised entity;graph embeddings;semi supervised;based graph embeddings;present semi supervised;learned embeddings;entity classification improved;supervised entity extraction;entity classification;supervised entity;determined learned embeddings;classification distantly supervised;instances train embedding;graph instances train;embedding instance jointly;extraction entity classification;text classification distantly;learned embeddings input;distantly supervised;train embedding instance;inductive variant embeddings;variant embeddings;supervised learning framework;entity extraction;embeddings;classification distantly;supervised learning;train embedding;embeddings input feature", "pdf_keywords": "semi supervised learning;graph based semisupervised;semi supervised;generalize graph embeddings;graph embeddings novel;previous semi supervised;semisupervised learning framework;graph embeddings;graph context prediction;semisupervised learning;neighbors embeddings transductively;labels neighbors embeddings;neighbors embeddings;predicting labels neighbors;graph laplacian regularization;based semisupervised learning;training classi\ufb01cation graph;context graph best;supervised learning;supervised learning approaches;embeddings transductively inductively;embeddings novel instances;classi\ufb01cation graph context;graph context;planetoid predicting labels;supervised;instance jointly trained;instance context graph;embedding instance jointly;laplacian regularization propose"}, "b6de9d0ca42a03967287aa7abfd59479e086a35a": {"ta_keywords": "gloss finding knowledge;automatic gloss finding;supervision available ontological;hierarchical semi supervised;finding knowledge base;available ontological constraints;knowledge base;using ontological constraints;gloss finding;propose glofin hierarchical;available ontological;knowledge base using;ontological constraints;semi supervised;knowledge glofin task;ontological constraints demonstrate;glofin hierarchical;automatic gloss;ontological constraints facilitate;semi supervised learning;glofin hierarchical semi;best knowledge glofin;knowledge glofin;base using ontological;supervised;hierarchical semi;using ontological;ontological;hierarchical;finding knowledge", "pdf_keywords": ""}, "d39478dd8d825bbd6c963d6a5ef2cee6857f6c21": {"ta_keywords": "argumentation computationally addressing;argumentation computationally;computational argumentation;deal argumentation computationally;computational argumentation journey;argumentation multifaceted communication;computationally addressing argument;argumentation journey;addressing argument quality;context computational argumentation;argument quality;attempts deal argumentation;argumentation multifaceted;argumentation arguments;argumentation journey semantics;argument quality understanding;argumentation;instead argumentation;argument reasoning dealing;deal argumentation;argument reasoning;instead argumentation multifaceted;world instead argumentation;view argumentation arguments;view argumentation;argue online;argumentation arguments logical;understanding argument reasoning;arguments;classical view argumentation", "pdf_keywords": ""}, "358d7d6333d3edd530e37efd8004cb9da8cfd5d4": {"ta_keywords": "structured procedural knowledge;procedural knowledge extraction;instructional cooking videos;knowledge extraction cooking;procedural knowledge extracted;extraction cooking videos;knowledge extracted cooking;extracted cooking videos;cooking videos;cooking videos propose;cooking videos 15;structured knowledge;sentence level annotations;semantic role labeling;benchmark structured procedural;procedural knowledge;instructional cooking;annotations benchmark structured;action procedure structured;structured knowledge form;structured procedural;labeling visual action;annotated open vocabulary;cooking videos watching;clip sentence level;instructional videos;learn procedures;produce interpretable structured;interpretable structured knowledge;knowledge extracted", "pdf_keywords": "multimodal comprehension cooking;vocabulary narrative videos;dataset multimodal comprehension;multimodal comprehension;extracted cooking videos;instructional cooking videos;comprehension cooking recipes;cooking videos;narrative videos;structured representation multimodal;challenge dataset multimodal;comprehension cooking;cooking videos 15;recipeqa challenge dataset;cooking videos best;extracting procedural knowledge;knowledge extracted cooking;open vocabulary narrative;transcripts recipeqa challenge;structured procedural knowledge;vocabulary narrative;multimodal information contained;procedural knowledge extracted;dataset extracting procedural;narrative videos noisy;dataset multimodal;representation multimodal information;sentence level annotations;multimodal information;annotated open vocabulary"}, "adf726bdcdddacee1c70d911b8f84b6a16841a32": {"ta_keywords": "summarizing customer reviews;extracting prominent review;customer reviews products;reviews products service;textual reviews extra;extracting prominent aspects;prominent review aspects;reviews extra extracting;review aspects customer;reviews products;textual reviews;type textual reviews;review aspects;aspects customer feedback;customer reviews;prominent aspect terms;prominent review;review aspects costly;analyzing summarizing customer;summarizing customer;extracts prominent aspect;reviews extra;aspect terms phrases;aspects given product;analyzing summarizing;customer feedback;number prominent review;systems analyzing summarizing;customer feedback proposed;prominent aspects", "pdf_keywords": ""}, "0dff2b00fd6e8e7b5f3c0707b0e51e3628988420": {"ta_keywords": "privacy aware text;privacy aware translation;semantics fluency adversarial;sensitive information publishing;task based adversarial;based adversarial;adversarial;aware text rewriting;based adversarial training;adversarial training;publishing data biased;sensitive information;adversarial training approximate;fluency adversarial;adversarial training method;data biased decisions;fluency adversarial training;training approximate fairness;leak sensitive information;task explore privacy;way protect data;protect data;propose new privacy;decisions based text;new privacy aware;new privacy;biased decisions automatic;text rewriting task;data biased;privacy aware", "pdf_keywords": "privacyaware translation methods;privacy aware text;privacyaware translation;explore privacyaware translation;sensitive information fairness;adversarial training;adversarial training approximate;based adversarial training;adversarial models optimized;training approximate fairness;adversarial;new privacy aware;adversarial models;privacy risk loss;task explore privacyaware;relevance adversarial models;privacyaware;task based adversarial;relevance adversarial;based adversarial;privacy aware;information fairness;new privacy;information fairness risk;reconstruction loss privacy;explore privacyaware;loss privacy;propose new privacy;leakage sensitive information;privacy"}, "694c0c5a4d0176e29bb85e1b9ca8ea84075fbbbb": {"ta_keywords": "hmms rnns architectural;bridging hmms rnns;rnns learn hidden;softmaxes use;hmms rnns learn;softmaxes;commonality hmms rnns;rnns learn;placement softmaxes;rnns architectural;rnns architectural transformations;speech induction;placement softmaxes use;rnns;observation placement softmaxes;hidden representations sequential;algorithm hmms;hmms rnns;networks bridging hmms;algorithm hmms special;softmaxes use non;learn hidden representations;parts speech induction;bridging hmms;neural networks;welch algorithm hmms;modeling parts speech;hidden representations;representations sequential data;expressivity interpretability model", "pdf_keywords": ""}, "194a1e5f9af0ea00b22def879d90b926187fbb64": {"ta_keywords": "shinji watanabe open_li52_asr_train_asr_raw_bpe7000_valid;watanabe open_li52_asr_train_asr_raw_bpe7000_valid;watanabe open_li52_asr_train_asr_raw_bpe7000_valid acc;open_li52_asr_train_asr_raw_bpe7000_valid;espnet2 pretrained model;open_li52_asr_train_asr_raw_bpe7000_valid acc ave;open_li52_asr_train_asr_raw_bpe7000_valid acc;espnet2 pretrained;pretrained model shinji;espnet2;pretrained model;fs 16k lang;pretrained;16k lang noinfo;model shinji watanabe;16k lang;model shinji;fs 16k;shinji watanabe;16k;ave fs 16k;model;shinji;noinfo;watanabe;lang noinfo;fs;lang;acc;ave fs", "pdf_keywords": ""}, "c02da00857c33fa39b115c0eb6c655ff6cf96878": {"ta_keywords": "workshop asian translation;asian translation;2nd workshop asian;workshop asian;translation;asian;overview 2nd workshop;2nd workshop;workshop;overview;overview 2nd;2nd", "pdf_keywords": ""}, "ca201db9980e49647feedf39eb30b19f074bf68a": {"ta_keywords": "end neural transcription;end speech recognition;transcription fully formatted;transcribed financial audio;speech text stt;transcription fully;speech text;transcription;neural transcription fully;end end speech;transcribed financial;000 hours transcribed;end speech;transcribed;neural transcription;text stt machine;endto end neural;english speech text;end neural;speech recognition propose;speech recognition;learning task acoustic;text stt;stt machine learning;fully formatted end;hours transcribed financial;financial audio fully;formatted end;hours transcribed;absent transcription", "pdf_keywords": "end transcription english;end transcription;transcription fully formatted;end end transcription;trained corpus;end neural transcription;models trained corpus;transcription english;trained corpus 000;transcription fully;professionally transcribed;formatted speech recognition;fully formatted speech;transcribed earnings calls;transcription;professionally transcribed earnings;transcribed earnings;transcription english conclusions;predict complete english;neural transcription fully;models spgispeech corpus;corpus free;transcribed;corpus free noncommercial;corpus;spgispeech corpus;neural transcription;formatted speech;corpus uniquely suited;hours professionally transcribed"}, "48530f3d6425f2f150f07ccdd61ba951951a0a7d": {"ta_keywords": "universal machine translation;bilingual models massively;massively multilingual dataset;massively multilingual model;models massively multilingual;translation fine tuning;individual bilingual models;neural machine translation;machine translation experiments;bilingual models;adapting new languages;massively multilingual nmt;103 languages adaptation;languages adaptation;machine translation;massively multilingual;translation experiments;multilingual model;multilingual dataset;translation experiments domain;machine translation fine;translation nmt models;machine translation nmt;languages adaptation approach;multilingual model language;tasks domain adaptation;ii massively multilingual;multilingual dataset 103;multilingual nmt;domain adaptation", "pdf_keywords": "adaptation massively multilingual;universal machine translation;massively multilingual translation;domain adaptation multilingual;neural machine translation;adaptation multilingual nmt;adaptation multilingual;universal translation model;bilingual models massively;massively multilingual model;models massively multilingual;adapting new languages;multilingual translation;translation model languages;translation model;massively multilingual dataset;translation nmt models;machine translation;massively multilingual nmt;machine translation nmt;train massively multilingual;languages adaptation;individual bilingual models;bilingual models;domain adaptation massively;103 languages adaptation;universal translation;scalable adaptation neural;multilingual model;multilingual translation make"}, "e3a85c5defe60f1f394fc4e7245fc071a249cf5b": {"ta_keywords": "behavioral models occupants;building occupants control;model occupants behavior;learn behavioral models;occupants behavior competitive;models occupants building;occupants behavior;occupants control management;occupants control;building occupants energy;occupants energy efficient;generative model occupants;learning parameters behavioral;occupants building;integrate building occupants;building occupants;learn behavioral;occupants building sustainable;efficient learn behavioral;behavioral models;behavioral model;incentivize building occupants;occupants energy;models occupants;social game experiments;energy efficient learn;estimation stopping time;model occupants;framework social game;building sustainable automation", "pdf_keywords": ""}, "c99e050b83360e5cbeee8fd2957aaab5b31aa638": {"ta_keywords": "memory language models;measure memory language;language models empirically;language models;rates memory language;memory language;language models including;language models use;generative sequence model;models including lstms;entropy rates memory;lstms transformers;lstms transformers emph;lstms;discrepancies generative sequence;including lstms;including lstms transformers;measure memory;entropy rates generations;term discrepancies generative;models empirically state;memory;sequence model;generative sequence;model calibration entropy;miscalibrated entropy rates;calibration entropy rates;emph miscalibrated entropy;generations drift dramatically;models use prediction", "pdf_keywords": "memory language models;neural language models;language models;language models observed;language models including;measure memory language;memory language;language models use;neural language;models including lstms;generative sequence model;models use prediction;use prediction;prediction end present;use prediction end;empirically state art;sequence model;entropy rates generations;memory;models;liquidity guarantee;plan timing;state art neural;state art language;temporary liquidity;plan timing completion;market solar energy;including lstms transformers;large degradations entropy;term discrepancies generative"}, "7ad913d1c6eddbdad1ab4571ab91f00f055ab735": {"ta_keywords": "fast recurrence attention;attention speech recognition;recurrence attention speech;speech recognition asr;recognition asr attention;machine translation tasks;parallelized regular rnn;attention better parallelized;modeling machine translation;speech recognition;recurrence attention;translation tasks improved;sru asr tasks;automatic speech;asr attention;asr attention mechanism;recognition asr;recurrence attention mechanism;sequence transduction tasks;attention mechanism sru;attention speech;translation tasks;automatic speech recognition;machine translation;sequence modeling achieves;language modeling machine;including automatic speech;regular rnn;speech recognition work;asr tasks", "pdf_keywords": "speech recognition asr;speech outperforming conformer;sru asr tasks;attention speech recognition;speech outperforming;recurrence attention speech;speech recognition;recognition asr attention;form speech outperforming;recognition asr;fast recurrence attention;asr benchmarks study;asr attention mechanism;asr benchmarks;asr attention;librispeech benchmark sru;applying sru asr;benchmark sru;multiple asr benchmarks;benchmark sru model;speech inputs;automatic speech;sru asr;automatic speech recognition;asr tasks;asr tasks comparing;speech recognition jing;long form speech;including automatic speech;attention speech"}, "40947612162cc4644f9489721ec1ca94fe7e765c": {"ta_keywords": "russian semantic relatedness;russian semantic;thesaurus russian benchmarks;computing semantic relatedness;task russian semantic;semantic relatedness extensively;purpose russian semantic;semantic relatedness list;semantic relatedness;thesaurus russian;lexical databases evaluation;semantic relatedness attracted;distributional thesaurus russian;evaluations thesaurus;evaluations thesaurus including;word similarity;semantic relatedness complementing;computing semantic;multiple evaluations thesaurus;generated lexical databases;lexical databases;word word similarity;language resources associative;datasets generated lexical;semantic relation;thesaurus including;terms semantic relation;terms semantic;semantic;thesaurus", "pdf_keywords": "russian semantic relatedness;thesaurus russian language;russian semantic;russian distributional thesaurus;distributional thesaurus russian;thesaurus russian;semantic relatedness measures;purpose russian semantic;evaluating semantic relatedness;semantic relatedness systems;thesaurus russian order;training semantic relatedness;semantic relatedness list;language resources russian;semantic relatedness;resources russian language;build distributional thesaurus;word collection russian;distributional thesaurus used;distributional thesaurus present;distributional thesaurus;thesaurus;thesaurus used;training evaluating semantic;open distributional thesaurus;russian language table;training semantic;russian texts extracted;thesaurus present;evaluating semantic"}, "69379f55de081938ae9d8b91ef549542ed78f5f0": {"ta_keywords": "review speaker diarization;speaker diarization;speaker diarization recent;review speaker;diarization recent advances;speaker;advances deep learning;diarization;diarization recent;deep learning;recent advances deep;advances deep;learning;deep;review;recent advances;advances;recent", "pdf_keywords": "speaker diarization task;speaker diarization;speaker diarization systems;abstract speaker diarization;discuss speaker diarization;speaker diarization recent;review speaker diarization;detection speech;speech recognition;integrated speech recognition;classes correspond speaker;detection speech activity;robust detection speech;speaker identity;integrated speech;correspond speaker identity;correspond speaker;speech activity;speech recognition applications;speech communication;speaker identity short;spoken language processing;conference spoken language;systems integrated speech;international speech communication;speaker;diarization recent advances;diarization systems integrated;deep learning;speech activity presence"}, "d74c5b5ed8eb467dc7f313b70a08880fcd74c39d": {"ta_keywords": "technology acceptance model;technology acceptance;user assessment electronic;acceptance model tam;assessment electronic filing;findings user assessment;acceptance model;electronic filing;electronic filing payment;web questionnaire survey;user assessment;using web questionnaire;survey tax conducted;extension technology acceptance;tax survey;web questionnaire;national tax survey;assessment electronic;users use information;success residents credibility;survey tax;tax survey tax;factors tam success;using web;users use;evaluation residents local;tam success residents;useful evaluation residents;acceptance;filing payment national", "pdf_keywords": ""}, "625764f8e3e1334ffbfe5b3139e555499e6df4d5": {"ta_keywords": "nlp requests;processing nlp requests;nlp requests information;requests using corpus;website update requests;site update requests;automatically natural language;requests update information;update requests;natural language website;update requests using;requests information studied;update requests semi;understanding requests update;requests update;process natural language;web site update;natural language processing;language website update;requests information;requests using;understanding requests;learning understand web;website update;processing nlp;natural language;requests;site update;requests semi automatically;using corpus generated", "pdf_keywords": ""}, "b5241fcbfbf30f6fd8ff1ae19d947dd2ca23244f": {"ta_keywords": "automatic event extraction;emotion provoking events;event extraction;aggregating events web;discovery aggregation events;acquiring aggregating events;effectiveness automatic event;aggregation events provoke;aggregation events;dictionary events survey;automatically acquired events;aggregating events;experiences emotion provoking;events web data;events web;automatic event;provoke particular emotion;automatically acquiring aggregating;dictionary events;emotion person experiences;experiences emotion;emotion provoking;concerned discovery aggregation;constructed dictionary events;discovery aggregation;person experiences emotion;events;provoking events;events survey;particular emotion", "pdf_keywords": ""}, "21c39ce886dc38dd2006ea25d6bd1eff4cdba0b8": {"ta_keywords": "political blogs models;political blogs;comment prediction;latent dirichlet allocation;online political blogs;predict blog;comment prediction task;predict blog users;novel comment prediction;blogs models;blogs models jointly;blogs;discussions online political;used predict blog;dirichlet allocation;blog community verbal;latent dirichlet;blog community;blog users;blog;comments given post;contents blog community;extend latent dirichlet;comments extend latent;online political;users leave comments;reactions post comments;model discussions online;contents blog;discussions online", "pdf_keywords": ""}, "51321a60f5ec2c80253394ef86e8b5fcc768f52a": {"ta_keywords": "clustering matching identifier;matching identifier names;identifier names scalable;entities integrating databases;integrating databases;techniques clustering matching;clustering matching;learning match cluster;integrating databases web;matching identifier;data integration;data integration determining;information extraction;sets data integration;using information extraction;information extraction methods;databases web;identifier names;match cluster;entities integrating;match cluster large;databases;objects different databases;determining sets identifiers;databases web obtained;names scalable;identifiers;names scalable adaptive;similarities textual names;identifiers refer", "pdf_keywords": ""}, "6954a6bb9d6f3e365b26b694c963ae1d62a03444": {"ta_keywords": "long text modeling;text modeling performance;context representations extensive;attention methods transformer;better long text;contexts transform token;use additive attention;additive attention methods;long text;based additive attention;additive attention;context representations;global context representations;text modeling;transform token representation;contexts transform;global contexts transform;powerful model text;additive attention mechanism;token representation based;text understanding;sequence length transformer;model text understanding;fastformer efficient;propose fastformer efficient;inefficient long sequences;attention methods;transform token;token representation;fastformer efficient existing", "pdf_keywords": "additive attention fastformer;fastformer additive attention;attention fastformer;attention fastformer \ufb01rst;context aware attention;additive attention models;vector additive attention;use additive attention;attention models;additive attention need;memory cost inference;vector attention;vector attention keys;based additive attention;additive attention;uses additive attention;additive attention achieve;accuracy memory;effective context modeling;query vector attention;inference speed;speed comparison fastformer;attention mechanism summarize;key attention values;cost inference speed;attention query matrix;aware attention values;attention values elementwise;summarize input attention;attention query"}, "03006aefccdd0c5c6736ab11ed574d02ba1cc086": {"ta_keywords": "neural machine translation;resource machine translation;machine translation nmt;machine translation;supervision data augmentation;target language sentences;augmentation methods translation;nmt models language;translation nmt;divergent languages handling;translation nmt standard;monolingual data;especially syntactically divergent;language sentences ordered;models language pairs;alleviate issues translation;use monolingual data;monolingual data help;syntactic divergence low;syntactically divergent languages;handling syntactic divergence;translation make possible;target language;especially syntactically;nmt standard benchmarks;language pairs;additional source training;benchmarks limited parallel;data augmentation;language sentences", "pdf_keywords": "sentences data augmentation;resource machine translation;machine translation;monolingual target sentences;monolingual target data;sourceordered target sentences;target sentences data;pseudoparallel sentences nmt;machine translation arti\ufb01cially;sentences nmt language;nmt language pairs;ordered target sentences;target sentences create;low resource syntactically;target sentences;create pseudoparallel sentences;monolingual target;reorder monolingual target;divergent language pairs;language pairs tackle;syntactically divergent language;en translations approach;translations approach achieves;divergent syntactic structures;translations approach;leverage monolingual target;sentences create source;approach divergent language;reorder monolingual;sentences data"}, "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269": {"ta_keywords": "synthesizing programs docstrings;gpt language model;codex gpt language;gpt language;correctness synthesizing programs;programs docstrings model;introduce codex gpt;programs docstrings;synthesizing programs;codex gpt;study python code;code github study;evaluating large language;docstrings describing;large language models;docstrings;github study python;github copilot furthermore;python code writing;github copilot;code writing;code writing capabilities;models trained code;docstrings model solves;docstrings model;language models trained;code github;trained code;difficulty docstrings describing;codex powers github", "pdf_keywords": "powerful code generation;synthesizing programs docstrings;code generation;correctness synthesizing programs;code github models;code generation technologies;trained code generate;natural language docstrings;language docstrings humaneval;evaluating large language;language models produce;programs docstrings model;models trained code;powerful code;language models trained;trained code task;trained code;producing docstrings code;large language models;synthesizing programs;language docstrings;code task producing;programs docstrings;docstrings code bodies;code generate;codex powers github;language models;code bodies natural;code bodies;tuning gpt code"}, "b5a667bf189a0cfda22bac702d97b601ae6adb6f": {"ta_keywords": "optimistic gradient descent;free optimistic gradient;gradient descent ascent;adversarial distribution shifts;robust adversarial distribution;optimistic gradient;adversarial distribution;gradient free optimistic;learning problems gradient;simultaneously robust adversarial;gradient descent;robust adversarial;distributionally robust decision;proposed approach learns;gradient information readily;approach learns models;learns models;algorithms convex minimization;decision dependent learning;descent ascent algorithm;adversarial;ascent algorithm;learns models simultaneously;gradient information;approach learns;dependent learning problems;problems gradient information;dependent learning;robust decision dependent;convex minimization", "pdf_keywords": "optimistic gradient descent;free optimistic gradient;gradient descent ascent;optimistic gradient;gradient free optimistic;robustness strategically adversarially;convex concave minmax;algorithms convex minimization;max optimization emerging;min max optimization;gradient descent;risk minimization;strategically adversarially;robust adversarial distribution;strategically adversarially generated;concave minmax problems;concave minmax;adversarial distribution;dependent risk minimization;adversarial distribution shifts;convex minimization;descent ascent algorithm;simultaneously robust adversarial;convex minimization problems;max optimization;ascent algorithm;robust adversarial;ascent algorithm solving;minmax problems applications;algorithms convex"}, "26cc9e13a7a76e3cf5f9885d08cdafabd6fbd7ec": {"ta_keywords": "tournament solution sets;tournaments theoretical results;tournaments theoretical;reasoning tournaments;tournament solution;sets reasoning tournaments;study tournaments fundamental;tournaments fundamental comsoc;tournaments fundamental;reasoning tournaments theoretical;tournaments using;tennis study tournaments;study tournaments;tournaments;experiments tournaments using;computational social choice;published tournament solution;tournaments using real;experiments tournaments;data soccer tennis;tournament;soccer tennis study;published tournament;results published tournament;end experiments tournaments;computational social;tennis study;soccer tennis;tennis;data soccer", "pdf_keywords": "realistic tournament data;generate realistic tournament;tournament data empirical;realistic tournament;tournament easily solvable;tournaments using real;tournament easily;real world tournaments;tournament model;win tournament easily;world tournaments;tournament data;random cr tournament;cr tournament model;understanding tournaments;understanding tournaments using;tournaments end experiments;tournament;synthetic tournaments;tournaments;contributions understanding tournaments;generate synthetic tournaments;experiments tournaments;win tournament;tournaments using;tournament model does;team win tournament;world tournaments nicholas;computational social choice;data soccer tennis"}, "ac46562e61cfef6213a915bbb80d1a1a2901542a": {"ta_keywords": "paper recommendation study;recommending conference paper;recommendation study combining;consider recommendation service;multiple information sources;technical paper recommendation;recommendation service researchers;paper recommendation;recommending conference;recommendation study;recommendation service;paper recommendation ask;problem recommending conference;information sources;instance consider recommendation;information sources evaluate;information sources growing;recommending;paper submissions reviewing;consider recommendation;conference paper submissions;problem recommending;online data sources;recommendation ask extended;pointers journal papers;conference reviewing;data sources;reviewing committee data;98 conference reviewing;recommendation ask", "pdf_keywords": "information retrieval collaborative;retrieval collaborative approach;based information retrieval;information retrieval;retrieval collaborative;recommending conference paper;conjunctive queries outperforms;recommendation study combining;relating recommendation process;recommendation process;approach recommendation algorithm;recommendation algorithm;recommending conference;relating recommendation;recommendation process use;paper recommendation study;queries outperforms approaches;recommendation study;recommendation algorithm using;collaborative approach recommendation;questions relating recommendation;problem recommending conference;ways formulating queries;approach recommendation;queries using content;technical paper recommendation;recommending;multiple information sources;conjunctive queries;queries outperforms"}, "559fdae33f0b7733b80a7dbcb902c79598a0d26e": {"ta_keywords": "treatment effect estimation;associated treatments transformers;treatment effect estimators;transformers treatment effect;effect estimators transtee;estimating causal effects;treatments transformers;transformers treatment;transformers strong treatment;effect estimation;treatments transformers strong;aimed estimating causal;experiments transformers treatment;effect estimators;estimating causal;inductive biases effective;demonstrate inductive biases;estimators transtee demonstrate;inductive biases;estimators transtee;effect estimation tee;aimed estimating;transformers;vision experiments transformers;research aimed estimating;experiments transformers;estimating;transformers emerged;treatment effect;estimation problems datasets", "pdf_keywords": "propensity score modeling;propensity score modelling;methods estimate propensity;propensity score network;propensity score model;model propensity score;estimate propensity score;propose propensity score;propensity score including;outcome model propensity;propensity score;algorithm propensity score;estimate propensity;training algorithm propensity;model propensity;propensity;bias results transtee;overcoming selection bias;bias matching weighting;impacts selection bias;causal inference transtee;propose propensity;selection bias results;algorithm propensity;discussions propensity score;bias matching;bias results;inference transtee directly;settings propose propensity;2011 targeted regularization"}, "bf63276c90a803fe0d069ce0a3a4a8236e756363": {"ta_keywords": "comic book narratives;narratives conveyed stylized;artwork dialogue comic;comics demonstrates text;inferences panels comic;tasks suggesting comics;predict narrative character;suggesting comics;depth analysis comics;comic book panels;visual narrative;stylized artwork dialogue;narrative character centric;visual narrative combination;artwork dialogue;comics contains fundamental;analysis comics;dialogue comic;plot visual narrative;suggesting comics contains;narratives conveyed;dialogue comic book;tell comic book;comics;panels comic;narrative character;comics contains;analysis comics demonstrates;panels comic book;models predict narrative", "pdf_keywords": "text image comics;image comics understanding;dataset comics;construct dataset comics;tasks suggesting comics;multimodality contextual;dataset comics consists;comics understanding;image text models;comic book narratives;multimodality contextual understanding;inferences panels comic;image comics;visual question answering;predict narrative character;impact multimodality contextual;multimodal models;context visual storytelling;visual narrative combination;comics consists;comics understanding construct;suggesting comics;multimodality;narrative character centric;visual narrative;multimodal models combine;storytelling 25 text;importance multimodal models;abstract visual narrative;comics consists million"}, "83165cf62e62a013c2bad61c98120ccb9a0087ae": {"ta_keywords": "bias peer review;fairness bias peer;peer review sociotechnical;peer review peer;peer review;bias peer;improve peer review;review peer;ii peer review;review peer review;tutorial fairness bias;bias unfairness thewebconf;fairness bias;peer review backbone;bias unfairness;challenges pertaining bias;2020 tutorial fairness;tutorial fairness;pertaining bias unfairness;review sociotechnical intelligent;improve peer;review sociotechnical;need improve peer;unfairness thewebconf tutorial;unfairness thewebconf;sociotechnical intelligent;peer;ii peer;bias;sociotechnical intelligent systems", "pdf_keywords": ""}, "e8deeebc7ff6315115f01fd70a343d62db202888": {"ta_keywords": "specific lexical resource;lexical resource;domain specific lexical;lexical resource useful;specific glossaries sophisticated;species match crowdsourcing;glossaries sophisticated;domain specific glossaries;glossaries sophisticated general;glossaries terms;glossaries;specific lexical;glossaries terms provided;genus species match;specific glossaries;match crowdsourcing workflow;individual terms resources;match crowdsourcing;crowdsourcing workflow matching;terms resources represented;lexical;representing hyponymic hypernymic;hyponymic hypernymic relations;terms resources;crowdsourcing workflow;species match;performance natural language;synsets representing hyponymic;natural language processing;genus species", "pdf_keywords": ""}, "b778a7c4001898a1c3888577154d747522f16db4": {"ta_keywords": "adversarial loss functions;adversarial losses;adversarial loss;different adversarial losses;adversarial loss divergence;functions adversarial loss;adversarial losses propose;adversarial losses framework;understanding adversarial losses;adversarial losses combining;valid adversarial loss;adversarial losses decoupling;component functions adversarial;set adversarial losses;deeper understanding adversarial;functions adversarial;understanding adversarial;adversarial;different adversarial;adversarial networks;discriminative adversarial;functions valid adversarial;discriminative adversarial networks;valid adversarial;compare different adversarial;based discriminative adversarial;set adversarial;extensive set adversarial;functions loss functions;loss functions", "pdf_keywords": "adversarial losses discriminative;losses discriminative adversarial;adversarial loss functions;adversarial loss divergence;different adversarial losses;adversarial losses propose;adversarial losses;various adversarial loss;adversarial loss;dantest adversarial losses;functions adversarial loss;understanding adversarial losses;adversarial training objectives;adversarial losses combining;adversarial training;component functions adversarial;set adversarial losses;designing adversarial training;adversarial losses decoupling;discriminative adversarial;deeper understanding adversarial;various adversarial;discriminative adversarial networks;different adversarial;discriminative adversarial network;dantest adversarial;functions adversarial;adversarial;adversarial networks dans;understanding adversarial"}, "0453bab552e83f19dd6ba12061949f128fa9b045": {"ta_keywords": "multiclass semi supervised;semi supervised multiclass;supervised multiclass;supervised multiclass learning;semi supervised learning;known semi supervised;supervised learning ssl;multiclass learning;semi supervised;multiclass learning methods;classes exploratory learning;multiclass semi;supervised learning;learning methods robust;classes exploratory ssl;classes exploratory;classes present data;supervised;provided classes exploratory;exploratory learning;multiclass;data known labeled;learning ssl;classes;known labeled examples;learning methods;classes seed examples;learning ssl case;classes seed;labeled examples", "pdf_keywords": "multiclass learning methods;supervised multiclass learning;supervised multiclass;multiclass learning;semi supervised multiclass;methods semi supervised;semi supervised multinomial;supervised multinomial naive;learning methods robust;learning methods;supervised multinomial;classi\ufb01ers mixtures multinomials;models classi\ufb01ers mixtures;known semi supervised;exploratory learning algorithm;naive bayes seeded;classes exploratory ssl;alternative exploratory learning;classes seed examples;multinomial naive bayes;semi supervised;classes exploratory;exploratory learning;gibbs sampling crpgibbs;seed examples classes;bayes seeded;classi\ufb01ers mixtures;classes seed;supervised;classes"}, "b36dc8db9930a785edd55ca30328ace2896523e6": {"ta_keywords": "shared semantic corpora;semantic corpora;semantic annotation corpus;annotated corpus atis;semantic annotation gather;semantic corpora result;annotation corpus;semantic annotation;annotation corpus allows;shared semantic;annotated corpus;annotation tools semantics;annotation gather teams;standardized annotated corpus;cooperation standardized annotated;atis corpora;problematic semantic annotation;annotation gather;standardized shared semantic;annotation;corpus allows share;annotation tools;corpus atis corpus;corpora;atis corpus;trends semantic annotation;standardized annotated;atis corpora set;corpus atis;semantic", "pdf_keywords": ""}, "87eece8d39d1e25ba87550be8b01af32738cbf2c": {"ta_keywords": "speaker parallel attention;talker speech recognition;speech single talker;attention modules speaker;multi talker asr;speech recognition asr;progress single talker;talker automatic speech;recognize overlapped speech;single talker speech;overlapped speech single;monaural multi talker;multi talker;talker asr architecture;single talker end;speech recognition;multi talker architecture;single talker automatic;talker asr structure;multi talker single;extended multi talker;end multi talker;talker architecture permutation;multiple attention modules;talker single talker;single talker;attention module multiple;multi speaker model;talker asr;overlapped speech", "pdf_keywords": ""}, "e2ece7ea0924b4f95f65587973118bea9a44a3d2": {"ta_keywords": "grounded entities corpus;entities corpus;relation discovery;relation discovery end;entities corpus statistics;methods relation discovery;relationships software entities;relationships entities software;term relationships software;similarity measure java;term relationships entities;entities human labeling;refer java classes;classes grounded discovery;entities suggesting coupling;relationships software;measure java classes;relationships entities;entities software;information grounded entities;refer java;distribution contexts classes;term relationships;software combine corpus;software entities;develop similarity measure;software entities human;named entities suggesting;corpus;contexts classes", "pdf_keywords": "grounded relation discovery;interesting code taxonomy;relation discovery;code taxonomy;entities representing java;relation discovery domain;code based information;text entities underlying;representing java classes;approach grounded discovery;representing java;described java;idea grounded relation;classes described java;relationships text entities;grounding extract information;code based grounding;described java class;term relationships;discovery domain software;java classes additionally;term relationships text;sources code based;entities underlying;common usage patterns;grounded relation;namespace hierarchies;namespace hierarchies using;code based distributional;software code based"}, "bbc7e533e5bfb388af1afd85bfb7ba17330cae76": {"ta_keywords": "shore power supply;cascade bridge inverter;voltage shore power;high voltage shore;bridge inverter current;bridge inverter;converter chain shore;dc bias transformer;output voltage converter;transformer dc;compensate dc current;bias transformer dc;compensation suppression dc;voltage converter;voltage shore;chain shore power;inverter current eliminated;power supply proposed;power supply fundamental;voltage converter chain;transformer dc component;effective high voltage;inverter current;high cascade bridge;power supply given;affect output voltage;transformer analyzed;platform high voltage;bias phenomenon transformer;power supply", "pdf_keywords": ""}, "92cee1e209f2d9a311416b0d9fd8a49b0fbe7df2": {"ta_keywords": "symbolic learning methods;learns filter documents;symbolic learning;retraining learned information;learned information filters;text categorization;learning methods diverse;improving generalization performance;learned classifiers;training classifier learned;classifier learned;learns filter;generalization performance collaborative;conclude symbolic learning;used text categorization;learning methods direct;learning methods;classifiers;categorization;transferring retraining learned;feature selection;different learning methods;training classifier;learned classifiers adapted;holds learning methods;generalization performance;improving generalization;classifier learned classifiers;classifiers adapted;classifiers adapted new", "pdf_keywords": ""}, "956aa64b0d5f5802b98bf551d5bab8993b114fd0": {"ta_keywords": "tfidf similarity;similarity metrics good;tfidf similarity appropriate;similarity function tfidf;modification tfidf similarity;good similarity functions;similarity metrics;sensitive similarity metrics;context sensitive similarity;widely used similarity;similarity metric;function tfidf similarity;metrics good similarity;similarity functions crucial;sensitive similarity;similarity metric called;similarity;similarity functions;used similarity function;used similarity;good similarity;cx idf;similarity function;duplicate free data;large data collections;similarity appropriate certain;idf shares tfidf;similarity appropriate;datasets large data;free data integration", "pdf_keywords": ""}, "f52f7964febd6d6d72aa23505b50d33e1d4ce0aa": {"ta_keywords": "weakly supervised learning;weakly supervised;labeling rules data;interactive weakly supervised;novel labeling rules;model weakly supervised;labeling rules;rule discovery boosting;supervised learning wsl;labeling rule set;based rule discovery;labeling rule;boosting interactive weakly;discovering novel labeling;rules data improve;weak labels;rule discovery;weak labels strengthen;label scarcity nlp;scarcity nlp tasks;quality labeling rule;supervised;supervised learning;nlp tasks manually;labeling;novel labeling;nlp tasks;rules data;discovery boosting interactive;scarcity nlp", "pdf_keywords": "supervised weakly supervised;weakly supervised;weakly supervised learning;supervised weakly;rule discovery boosting;interactive weakly supervised;weakly supervised interactive;method supervised weakly;boosting interactive weakly;abstract weakly supervised;weaklysupervised baselines rulelevel;novel labeling rules;labeling rules data;discovering novel labeling;labeling rules;rule annotation comprehensive;weaklysupervised baselines;state art weaklysupervised;supervised learning wsl;based rule discovery;labeling rule set;rulelevel annotation helps;discovery boosting interactive;rule discovery;labeling rule;rule annotation;baselines rulelevel annotation;rulelevel annotation;rule discovery framework;rules data improve"}, "60f3e69e4f18e8e8e7dcc4ba66c1e216b49ad982": {"ta_keywords": "common sense knowledge;common sense game;collect common sense;engine common sense;common sense;knowledge acquisition gecka;common sense consists;gecka game engine;sense knowledge acquisition;sense knowledge;engine commonsense knowledge;sense game designers;game engine commonsense;commonsense knowledge acquisition;language understanding gecka;sense game;game engine common;understanding gecka game;live commonsense knowledge;commonsense knowledge;commonsense knowledge representation;knowledge acquisition;multi purpose knowledge;task game engine;purpose knowledge;game engine;gecka game;understanding gecka;engine commonsense;acquisition gecka aims", "pdf_keywords": ""}, "09b87b6e7bfbf66d355574d292586595e0185d6e": {"ta_keywords": "languages features known;linguistic probing typological;typological knowledge bases;languages features;probing typological features;automatically populate typological;languages annotations features;typological knowledge;typological kbs;populate typological kbs;typological features;typological features correlate;languages annotations;information linguistic properties;linguistic properties;sense languages annotations;linguistic properties world;lingual transfer learning;typological kbs focus;populate typological;populated sense languages;linguistic probing;transfer learning linguistic;values languages features;probing typological;languages;task typological knowledge;learning linguistic probing;information linguistic;adoption typological kbs", "pdf_keywords": "predicting typological;concerned predicting typological;predicting typological features;prediction typological features;prediction typological;world languages conclusions;typological features world;typological knowledge bases;task prediction typological;typological features;typological knowledge;abstract typological knowledge;typological features wals;2020 abstract typological;languages conclusions;typological features johannes;language structures wals;languages conclusions paper;linguistic properties world;world languages;linguistic properties;abstract typological;typological;information linguistic properties;linguistic;language structures;languages;information linguistic;properties world languages;languages small languages"}, "5b16d138bf16762d43b55b6e21d9b0b61021180e": {"ta_keywords": "transcription networks;transcription;networks", "pdf_keywords": ""}, "b15ea460c77a4ee8aa159a30ab0331deedfcf392": {"ta_keywords": "training large sparse;token expert allocation;layer large language;expert allocation linear;large language models;expert allocation;assignment experts base;layers optimal assignment;optimal assignment expert;experts base layer;layers simplifying training;large sparse models;language models greatly;assignment expert receives;capacity sparse layers;sparse layers optimal;training inference;simplifying training large;assignment scheme improves;sparse layers;training inference routing;sparse models;assignment experts;language models;efficiency training inference;assignment expert;large sparse;specialized expert modules;expert modules;token specialized expert", "pdf_keywords": "sparse experts base;assignment sparse experts;sparse experts;expert based sparsity;sparsity language models;tokens experts training;assignment sparse;experts training;learn expert specialization;experts base layer;learn expert;complexity learn expert;expert specialization using;experts training use;experts training introduce;tokens multiple experts;sparse models;expert specialization;experts base;language models;proposed sparse models;simple sparse;previously proposed sparse;sparse;expert based;expert;proposed sparse;introduced simple sparse;sparse base layer;experts"}, "bbb7eb10c45cabaee6e427242fce7180c0217ef1": {"ta_keywords": "learning programs languages;programming languages;syntax tree;predicting variable names;programs languages;abstract syntax tree;programming languages present;tasks programming languages;different programming languages;represent program using;programs languages javascript;word2vec based learning;learning programs represent;learning programs;programming languages allows;represent program;represent programs way;syntax tree ast;representation learning programs;variable names;idea represent program;programming;different programming;java python representation;challenge learning programs;program using paths;learning programs main;paths abstract syntax;languages javascript;represent programs", "pdf_keywords": ""}, "cbf941fef87830efa4de98455cfe943917909b66": {"ta_keywords": "superlinear convergence bfgs;hessian approximation present;hessian approximation;convergence bfgs method;methods convex broyden;hessian approximation trace;moment superlinear convergence;superlinear convergence;inverse hessian approximation;superlinear convergence classical;newton methods convex;quasi newton methods;local superlinear convergence;convergence bfgs;determinant hessian approximation;methods convex;hessian;bfgs methods obtain;trace inverse hessian;bfgs methods;strong convexity parameter;inverse hessian;analysis local superlinear;dfp bfgs methods;bfgs method;logarithm determinant hessian;classical quasi newton;bfgs method depends;convexity parameter lipschitz;newton methods", "pdf_keywords": "method superlinear convergence;superlinear convergence rate;superlinear convergence standard;superlinear convergence;quasi newton methods;quasi newton algorithms;rates superlinear convergence;gradient method superlinear;hessian newton method;convergence classical gradient;newton algorithms smooth;study convex broyden;method superlinear;newton method approximation;classical gradient method;exact hessian newton;asymptotic rates superlinear;rules convex broyden;hessian newton;convex broyden;convex broyden class;classical quasi newton;standard quasi newton;smooth unconstrained optimization;quasi newton scheme;convergence rate form;rate linear convergence;exact hessian;newton methods;approximation updated iterations"}, "d8551a4b49aa547ad8884ba9f545480860fcadd1": {"ta_keywords": "fourier neural operators;material modeling network;implicit fourier neural;deep implicit fourier;fourier neural operator;fourier neural;learn material models;ifno learning deep;deep neural operator;deep neural;predict material response;learning deep;material response modeled;material models directly;material models;neural operators;material modeling;deep networks work;heterogeneous material modeling;neural operator architecture;fft accelerated learning;predict material;learning techniques deep;learned solution operators;deep networks;ifno learning;response modeled learning;learning deep implicit;operator ifno learning;neural operator", "pdf_keywords": "learn material models;material models;material models directly;material modeling;fourier neural operators;heterogeneous material modeling;deep implicit fourier;fourier neural operator;material modeling huaiqian;implicit fourier neural;fourier neural;fft accelerated learning;modeling mechanical;learned solution operators;predicting displacement \ufb01elds;modeling mechanical responses;learning deep;based continuum mechanics;accelerated learning;modeling based continuum;models predicting displacement;deep neural operator;network discretized integral;displacement \ufb01elds model;fast fourier transformation;constitutive modeling;predicting displacement;deep neural;neural operators;learning deep implicit"}, "cee25a535ec7165eae38f498a391050077ad9f65": {"ta_keywords": "speaker clustering sampling;speaker clustering using;speaker clustering;based speaker clustering;clustering using utterance;sampling based speaker;utterance oriented dirichlet;dirichlet process mixture;clustering sampling;clustering sampling based;utterance oriented speaker;mixture model evaluation;large amounts utterances;oriented speaker model;model based speaker;infinite mixture model;speaker model present;process mixture model;mixture model;mixture model applied;speaker model;hierarchical agglomerative clustering;carlo incorporated utterance;dirichlet process;using utterance oriented;agglomerative clustering;agglomerative clustering especially;speakers purpose framework;clustering especially large;data infinite mixture", "pdf_keywords": ""}, "7668b23aadf43bebe5e2d3abf37938b44bd16200": {"ta_keywords": "unified multimodal reasoning;visual question answering;multimodal reasoning models;multimodal reasoning;multimodal qa;multimodal qa existing;unified multimodal;knowledge images text;multihop multimodal qa;knowledge images;language groundable visual;multimodal;reason knowledge images;images text challenge;create unified multimodal;webqa multihop multimodal;knowledge richer visual;question answering;richer visual online;question answering vqa;webqa challenging;representation learning knowledge;webqa challenging new;models answer questions;text challenge community;knowledge aggregation language;language generation webqa;learning knowledge aggregation;introduce webqa challenging;visual online world", "pdf_keywords": "tanabata festival hiratsuka;tanabata festival masskruege;oktoberfest tanabata festival;festival syonan hiratsukatanabata;tanabata festival;tanabata festivals;tanabata festivals held;tanabata festivapl;austria tanabata festival;okt tanabata festival;scale tanabata festivals;festival hiratsuka festival;hiratsuka festival syonan;tanabata festival hmoisrtaotfsdueckeam;pageant tanabata festival;festival hiratsuka;large colorful festival;festival hiratsuka japan;lights pageant tanabata;oktoberfest tanabata;hiratsuka festival;transported tanabata festivapl;sendai tanabata festival;colorful festival;hiratsukatanabata decorated lights;rtasatusruitk wtanhaibcatha festivalsctaarlinghyt;colorful festival strength;festival syonan decorated;wtanhaibcatha festivalsctaarlinghyt olaustisngeethraoucghtlaaarsngtaelbsetatatiannfatebhsattieavalbf;wtanhaibcatha festivalsctaarlinghyt"}, "c688e187cede868e35fc1b53913e0fbbe6e38ea0": {"ta_keywords": "structured prediction task;structured prediction;algorithms structured prediction;structured prediction paradigm;based structured prediction;structured prediction algorithm;complex structured prediction;framed structured prediction;prediction task biomedical;learning algorithms structured;biomedical event extraction;imitation learning algorithms;structured prediction daume;search based structured;imitation learning;imitation learning paradigm;event extraction;investigation imitation learning;prediction task;task biomedical event;algorithms structured;paradigm algorithms learn;learning paradigm algorithms;task biomedical;algorithms learn;prediction paradigm;structured;natural language processing;algorithms investigation imitation;algorithms learn expert", "pdf_keywords": ""}, "2448e63a7bb626d09001fe37e60befdb2919f6e6": {"ta_keywords": "commonsense information extraction;graph based commonsense;commonsense knowledge determining;information extraction graph;commonsense knowledge useful;commonsense knowledge;based commonsense information;knowledge sparse markov;information extraction;sense knowledge web;commonsense information;readable knowledge bases;information extraction unlike;graph based markov;extraction graph representation;knowledge sparse;knowledge web;commonsense facts world;knowledge web scale;factual knowledge sparse;machine readable knowledge;extract common sense;knowledge bases;extraction graph;commonsense facts;basic commonsense facts;based commonsense;common sense knowledge;nature commonsense knowledge;markov chains robust", "pdf_keywords": ""}, "cee96ee69adacfdeb648c230d2c9b01011724724": {"ta_keywords": "overlapping speech flexible;neural diarization handling;end neural diarization;handling overlapping speech;overlapping speech;speech flexible numbers;diarization handling;speech flexible;diarization handling overlapping;neural diarization;flexible numbers speakers;streaming end;diarization;numbers speakers;streaming end end;online streaming end;end end neural;streaming;end neural;online streaming;speech;speakers;neural;handling overlapping;flexible numbers;overlapping;end end;numbers;end;flexible", "pdf_keywords": ""}, "007371feab4af758b74580c43e74827b3500c67e": {"ta_keywords": "distributed video demand;distributed vod content;demand optimized distributed;vod content distribution;video demand streaming;optimized distributed;optimized distributed video;demand streaming;distributed vod;distributed video;demand streaming theory;framework distributed vod;streaming theory design;fractional storage architecture;propose simple distributed;video demand;simple distributed;vod content;content distribution;distributed;fractional storage;streaming theory;simple fractional storage;content distribution problem;convex content placement;user demand network;streaming;distributed solution;bandwidth node;highly distributed implementation", "pdf_keywords": ""}, "2a0cb1a1e78b77fe9981e4935410cf3ea900e370": {"ta_keywords": "speech recognition untranscribed;building speech recognition;speech recognition;recognition untranscribed data;building speech;recognition untranscribed;speech;untranscribed;untranscribed data report;jhu workshop 2016;untranscribed data;report jhu workshop;jhu workshop;workshop 2016 luk\u00e1\u0161;data report jhu;recognition;workshop 2016;rott jan honza;daichi mochihashi takahiro;honza \u010dernock\u00fd editor;jan honza;najim dehak jan;khudanpur najim dehak;report jhu;sanjeev khudanpur najim;honza \u010dernock\u00fd;honza;ond\u0159ej \u0107\u0131fka yibo;jan honza \u010dernock\u00fd;sanjeev khudanpur", "pdf_keywords": ""}, "d60b4594fb0404329d9ebf6fd88702ca3479e904": {"ta_keywords": "extracting abbreviation definitions;hmm matching abbreviations;based abbreviation extractor;matching abbreviations;alignment based abbreviation;algorithm extracting abbreviation;extracting abbreviation;matching abbreviations definitions;abbreviations definitions results;abbreviation extractor;abbreviation extractor naturally;abbreviations abbreviations;abbreviations;abbreviations definitions;abbreviation definitions;abbreviation definitions biomedical;alignment hmm matching;abbreviations abbreviations chemical;based alignment hmm;biomedical text model;specific types abbreviations;types abbreviations abbreviations;definitions biomedical text;abbreviations chemical formulas;abbreviations chemical;abbreviation;biomedical text;types abbreviations;recall standard data;comparable alignment based", "pdf_keywords": ""}, "99546b4d1f2547095bb15eec36e03f64b74a78d4": {"ta_keywords": "community feedback affects;users newcomers;feedback newer users;community feedback;community feedback newer;users newcomers examine;finally community feedback;susceptibility community feedback;new users prone;past community feedback;topics users new;new users wallstreetbets;analysis new users;term users newcomers;wsb community increase;community feedback report;new users;users new;newcomers;message topics;newcomers examine;caused wsb community;users prone;users new users;users wallstreetbets;wsb community;users wallstreetbets gamestop;community increase;regular new users;feedback", "pdf_keywords": ""}, "5e3d1bece9dd2356fd2b31312bd62c8f7126882d": {"ta_keywords": "energy disaggregation utility;energy disaggregation;disaggregation utility privacy;utility privacy tradeoff;utility privacy;privacy tradeoff;disaggregation utility;disaggregation;privacy;energy;utility;tradeoff", "pdf_keywords": ""}, "b990331a5394f3642a1fd1791d70bfa2d85d9d1d": {"ta_keywords": "vaccines related tweets;tweets discussed vaccination;tweets posted covid;tweeted web domains;include tweets discussed;including tweeted web;screened million tweets;websites shared vaccines;twitter topics;tweets discussed;include tweets;tweeted youtube videos;related tweets;content including tweeted;conversations twitter topics;twitter topics dynamics;tweeted domain;conversations include tweets;related tweets posted;19 conversations twitter;million tweets posted;domains urls tweets;tweeted web;urls tweets;tweets posted;tweets;conversations twitter;million tweets;topics dynamics tweeted;tweeted youtube", "pdf_keywords": ""}, "8b98f7ff3bb1b199db85fc219a5c27b355adf1be": {"ta_keywords": "osseous crown lengthening;technique osseous crown;crown lengthening procedure;treatment osseous crown;alternative osseous crown;flap osseous crown;crown lengthening typically;crown lengthening;crown lengthening needed;osseous crown;laser enables clinician;crown lengthening negating;erbium laser enables;present technique osseous;erbium laser;conventional treatment osseous;flap surgery procedure;surgery procedure;invasive alternative osseous;flap surgery;involves flap surgery;surgery procedure frequently;treatment osseous;tooth structure placement;technique osseous;lengthening procedure;surgery;complications interfere aesthetic;closed flap osseous;aesthetic outcome infection", "pdf_keywords": ""}, "a604ad4654f31d325b888806e276123a704cb5c8": {"ta_keywords": "support vector machine;increase classification robustness;classification robustness conventional;classification robustness;minimum error classification;margin maximization support;classifiers;geometric margin maximization;minimum classification error;minimum classification;support vector;based classifiers;prototype based classifiers;maximization support vector;margin general class;classification tasks derive;vector machine svm;wide range classification;svm;range classification;value minimum classification;error classification;margin maximization;classification;geometric margin general;based classifiers increase;classifiers increase classification;class discriminant functions;classification error mce;machine svm", "pdf_keywords": ""}, "652e3c774da47c0c8788111ec886a00d3b8fc637": {"ta_keywords": "deformation tumour boundary;deformations tumour resected;brain deformations tumour;estimating tissue deformation;tissue deformation tumour;compute brain deformations;deformations tumour;tumour cavity surface;tumour cavities surfaces;deformation tumour;tissue deformation;points tumour cavity;tumour boundary;deformed surfaces ventricles;tumour boundary resection;computed deformed surfaces;surfaces ventricles tumour;ventricles tumour cavities;forces tumour surrounding;tumour cavity;tumour resected loading;ventricle surface;tumour cavities;fluid skull calculate;points ventricle surface;operative image computed;computed deformed;brain deformations;intra operative image;tumour surrounding", "pdf_keywords": ""}, "d0ea87ce3bcd86428d379fd478c365c64f870200": {"ta_keywords": "dialogue sgd dataset;schema guided dialogue;guided dialogue sgd;robustness dialogue systems;measuring robustness dialogue;dialogue systems linguistic;dialogue state tracking;schemas evaluate dialogue;task oriented dialogue;dialogue sgd;dialogue systems;guided dialogue systems;robustness dialogue;dialogue systems furthermore;guided dialogue;dialogue state;improve schema robustness;evaluate dialogue state;dialogue;dialogue research;linguistic variations schemas;oriented dialogue;evaluate dialogue;oriented dialogue research;robust generalization schema;measuring schema sensitivity;generalization schema guided;schema sensitivity sgd;schemas zero shot;schema robustness", "pdf_keywords": "schema guided dialogue;dialogue models schema;schemaguided dialogue state;guided dialogue models;dialogue state tracking;schemaguided dialogue;topperforming dialogue state;dialogue models;topperforming dialogue;evaluate topperforming dialogue;guided dialogue systems;guided dialogue state;schema robustness sgd;schema guideddialogue;guided dialogue;dialogue systems;task oriented dialogue;dstc8 schema guideddialogue;guided dialogue extended;measuring schema sensitivity;improving schema robustness;script schemaguided dialogue;improve schema robustness;novel schema sensitivity;robust schema variations;robustness schema guided;dialogue state;schema sensitivity metric;dialogue systems harrison;schema guideddialogue figure"}, "429b65937d4922578a81e1f0ef5aeab7361ae36b": {"ta_keywords": "bash commands nl2bash;nl2bash corpus semantic;commands nl2bash;parser natural language;nl2bash nl2bash corpus;commands nl2bash nl2bash;sentences bash commands;nl2bash corpus;semantic parser;corpus semantic parser;semantic parsing;semantic parser natural;semantic parsing methods;parser;data semantic parsing;parsing;bash commands;scripting;nl2bash nl2bash;natural language interface;scripting simply;commands;parser natural;specific scripting simply;scripting simply stating;specific scripting;nl2bash;natural language;sentences bash;parsing methods", "pdf_keywords": "parser natural language;semantic parsing benchmarks;existing semantic parsing;nl2bash corpus semantic;semantic parser natural;semantic parsing;semantic parser;corpus semantic parser;operating nl2bash corpus;nl2bash corpus;semantic parsing methods;data semantic parsing;dataset nl2bash challenging;commands nl2bash dataset;commands nl2bash introducing;parsing methods novel;parsing;parser natural;natural language interface;nl2bash dataset introduce;sentences bash commands;bash commands nl2bash;parser;commands nl2bash;new dataset nl2bash;nl2bash challenging;domain natural language;corpus semantic;natural language;target meaning representations"}, "ba45a346690f3c5b6f8c371b5c6cf1d7cce5619d": {"ta_keywords": "constrained rank minimization;rank minimization;rank minimization problem;identifying arx model;arx model output;abstract blind identification;constrained rank;blind identification known;blind identification;task identifying arx;relaxed convex formulation;known linear subspace;arx model;identifying arx;solution abstract blind;convex formulation;convex formulation approximate;posed problem assumptions;lies known linear;known linear;relaxed convex;minimization;minimization problem present;minimization problem;present relaxed convex;arx;model output measurements;phrase constrained rank;problem posed assume;measurements phrase constrained", "pdf_keywords": "estimating arx model;estimating arx;arx model output;arx model identi\ufb01cation;method arx model;identifying arx model;problem estimating arx;arx model solely;arx model;task identifying arx;method arx;model output measurements;novel method arx;identifying arx;identi\ufb01cation output measurements;measurement noises constraints;arx;products known optimization;model solely outputs;convex optimization;model identi\ufb01cation output;known optimization;known optimization referred;model identi\ufb01cation;form convex optimization;model output;output measurements;complicated problem estimating;convex optimization problem;output measurements paper"}, "65c2a39f1579a947926ac5746888445ea4afdf6e": {"ta_keywords": "unsupervised grammar induction;grammar induction;free grammar cfg;context free grammar;grammar induction focus;grammar cfg based;methods grammar induction;grammar induction benefit;unsupervised grammar;free grammar;modeling lexical dependencies;grammar cfg;lexical dependencies neural;dependencies neural lexicalized;grammar induction contrasts;unsuitable unsupervised grammar;neural lexicalized pcfgs;lexical dependencies;syntactic formalisms lexicalized;lexical dependencies previous;return lexical dependencies;syntactic formalisms;formalisms lexicalized pcfgs;disparate syntactic formalisms;grammar;modeling lexical;dependencies neural;dependencies return lexical;lexicalized pcfgs;current methods grammar", "pdf_keywords": "lexicalized constituency parsing;dependency parsing constituency;unsupervised dependency parsing;phrasestructure dependency structure;parsing constituency parsing;constituency parsing;lexical dependencies provide;neural models lexicalized;lexical dependencies;dependency structure grammar;constituency parsing compound;models lexicalized pcfgs;dependency parsing;scaffolding dependencies lexical;dependencies lexical;lexicalized constituency;parsing constituency;pcfg neural parameterization;propose neural pcfgs;parameterization method lexicalized;neural parameterization;dependencies lexical dependencies;uni\ufb01ed phrasestructure dependency;phrasestructure dependency;grammar propose neural;method lexicalized pcfgs;neural parameterization extends;lexicalized pcfgs unsupervised;lexicalized pcfgs;neural pcfgs"}, "562f33611cdc0d8ed6609aa09f153e6238d5409e": {"ta_keywords": "missing data sequences;modeling missing data;missing data sequential;trained missingness patterns;models trained missingness;missing data improving;missing data observations;trained missingness;data sequences rnns;model missing data;predictive results rnns;rnns improved classification;sequences rnns improved;missing data;rnns;clinical time series;missingness patterns;modeling missing;cope missing data;sequences rnns;classification clinical time;results rnns learn;rnns improved;rnns learn;results rnns;missingness;classification diagnoses;missingness patterns showing;rnns make effective;functions missing data", "pdf_keywords": "modeling missing data;missing data clinical;missing data sequential;modeling missing;missing data indicators;machine learning healthcare;missing data;patterns missing values;clinical time series;representative lstm models;time series rnns;predictive power rnns;missing values;cope missing data;lstm models;zeroimputation missing data;missing values underutilized;lstm zeroimputation missing;data clinical time;rnns logistic;imputation plus missing;series rnns;rnns;representative lstm;models trained imputed;rnns unlike linear;measurements imputation plus;proposes patterns missing;rnns logistic regression;scores representative lstm"}, "a36f7d5d8f724168e534925edff97b3680e545c9": {"ta_keywords": "tensor contraction layers;deep nets;tensor contraction layer;tensor contractions neural;activation tensors;tensor contraction performance;dimensionality activation tensors;deep nets demonstrate;incorporate tensor contractions;activation tensors applied;cifar100 imagenet datasets;contractions neural network;alexnet vgg tensor;imagenet;imagenet datasets;parsimonious deep nets;use tensor contractions;tensor contractions;imagenet datasets studying;apply activation tensors;propose tensor contraction;neural network layers;tensor contraction;activation tensors number;trainable neural;tensors;trainable neural network;tensor contractions end;reduce dimensionality activation;networks alexnet", "pdf_keywords": "activation tensors;tensor contraction activation;tensor contraction layer;activation tensor;contraction activation tensor;dependencies activation tensors;tensor contractions neural;activation tensors produced;apply activation tensors;order activation tensors;incorporate tensor contractions;recognition network;layer performs tensor;performs tensor contraction;use tensor contractions;tensor contraction performance;tensor contractions;activation tensor yield;activation tensors paper;image recognition network;tensor contraction;propose tensor contraction;contractions neural network;performs tensor;tensors;tensor contractions end;incorporate tensor;recognition network posit;tensor;neural network layers"}, "c2ff76c75acc777e005360e9d4c4d928d95c0432": {"ta_keywords": "codes distributed storage;regenerating codes distributed;codes regenerating codes;optimum codes regenerating;codes distributed;regenerating codes;codes regenerating;regenerating codes including;overview regenerating codes;codes possess reconstruction;distributed storage;distributed storage networks;failure redundant storage;storage networks;reed solomon codes;storage data distributed;solomon codes possess;storage nodes;solomon codes;redundant storage data;redundant storage;minimization repair bandwidth;reliability reed solomon;storage systems minimization;storage systems;data dispersed storage;optimum codes;individual storage nodes;reliability simple replication;simple replication schemes", "pdf_keywords": ""}, "6c3b8e65dc45cb62172f9425dcff4c48055d47eb": {"ta_keywords": "food social media;language food geographic;food related posts;food geographic;language food social;language food;related posts twitter;twitter demonstrate latent;food geographic locale;language food tasks;posts twitter;textual features;mirrored language food;twitter;connections language food;food related;food social;food tasks language;analyze textual features;power language food;social media;corpus million food;million food related;social media lastly;language based models;time query visualization;visualization tools geo;textual features greatest;query visualization dataset;geographic locale community", "pdf_keywords": "food related tweets;visualizations language food;annotating querying tweets;food related posts;language food geographic;using tweets predict;food geographic;food geographical;querying tweets;food geographical temporal;language food geographical;food geographic locale;tweets source predictive;tweets predict latent;tweets predict;related tweets;related tweets using;tweets;querying tweets source;using tweets;related posts twitter;tweets using tweets;textual features predictive;twitter demonstrate latent;food related;tweets source;twitter;posts twitter;language food;visualizations language"}, "5e00596fa946670d894b1bdaeff5a98e3867ef13": {"ta_keywords": "vision language pretraining;image captioning tasks;captioning tasks;captioning tasks 10;vision language benchmarks;visual language model;generative vision language;accuracy image captioning;representations vision language;language pretraining;language pretraining vlp;image captioning;captioning;modeling visual textual;visual textual representations;image captions regional;vision language;textual representations vision;visual language;clean image captions;simple visual language;captions regional;image captions;expensive annotations;impressive performance multimodal;captions;discriminative generative vision;minimalist pretraining framework;expensive annotations including;captions regional labels", "pdf_keywords": "vision language pretraining;language model pretraining;representations vision language;language pretraining;visual language model;vision language;language pretraining vlp;pretraining objectives results;present minimalist pretraining;model pretraining weak;pretraining objectives;effectiveness pretraining objectives;visual question answering;minimalist pretraining;multimodal downstream tasks;pretraining weak supervision;minimalist pretraining framework;textual representations vision;visual language;effectiveness pretraining;model pretraining;modeling visual textual;visual textual representations;performance multimodal;pretraining;performance multimodal downstream;pretraining framework;coco evaluations;pretraining weak;visual textual"}, "96bb4b49f69419c31857e928969fcaa137e15060": {"ta_keywords": "question answering;question answering task;models answer generation;answer generation;based question answering;classify question answerable;review based qa;answer generation propose;comprehension models synthesizing;question review;answerable unanswerable based;selecting relevant reviews;reading comprehension models;questions answered based;knowledgeable customer answer;models synthesizing answer;reviews propose task;comprehension models;customer answer question;unanswerable based;question review observing;given question review;review observing questions;unanswerable based available;marking question answerable;questions answered;product reviews;reviews given question;amazonqa review based;answering", "pdf_keywords": "question answering data;question answering;build question answering;question answering analysis;question review dataset;product review dataset;community question answering;question answering qa;answering analysis large;comprising question answering;neural qa models;models answerable questions;comprehension models synthesizing;reading comprehension models;answerable questions;building answerability;extracting review snippets;reviews question answerable;selecting relevant reviews;answering qa product;information retrieval;answers 14m reviews;training qa models;review dataset;based neural qa;comprehension models;neural qa;combines information retrieval;building answerability classi\ufb01er;answering analysis"}, "2873f78efd7adcb118a70f8ea3ca7fa1501e320a": {"ta_keywords": "shot relation classification;relation classification;relation classification models;relation classification research;relation choice fewrel;fewrel dataset;shot relation;challenging shot relation;build fewrel dataset;fewrel dataset adding;fewrel challenging task;challenges construct fewrel;domain adaptation nota;domain nota relation;relation;fewrel challenging;fewrel challenging shot;construct fewrel;domain adaptation;nota relation choice;nota relation;aspects shot relation;techniques domain adaptation;present fewrel challenging;art shot relation;relation choice;fewrel;classification models struggle;present fewrel;fewrel build fewrel", "pdf_keywords": "shot domain adaptation;domain adaptation shot;techniques domain adaptation;domain adaptation;domain adaptation methods;challenging shot relation;shot relation classi\ufb01cation;shot domain;shot relation;used domain adaptation;domain adaptation nota;ignores shot domain;knowledge base biomedical;biomedical domain;biomedical domain setting;biomedical literature umls;dataset fewrel propose;work shot domain;set biomedical domain;based dataset fewrel;shot models commonly;new dataset extensive;fewrel dataset;shot models;test set biomedical;fewrel dataset carry;fewrel ignores shot;fewrel challenging shot;adaptation shot da;adaptation shot theabove"}, "3f311aee9d25b0284d21274cfc8706d6f0277f87": {"ta_keywords": "deep quantization neural;quantization neural networks;deep quantization;quantizing weights activations;networks dnns wide;approach deep quantization;weights activations dnn;activations dnn layers;quantization neural;dnn layers;heterogeneous quantization network;quantization network quantizing;quantization network;dnns hyperparameter optimization;advances dnns hyperparameter;networks dnns;network quantizing layer;neural networks dnns;bit weights activations;advances dnns;quantization entire network;deep neural networks;activations dnn;quantizing layer;dnns wide range;quantizing weights;quantizes networks;network quantizing;performing heterogeneous quantization;quantizing layer different", "pdf_keywords": "deep reinforcement learning;deep reinforcement;releq formulate reinforcement;end deep reinforcement;explore quantization hyperparameter;reinforcement learning rl;formulate reinforcement learning;quantization hyperparameter;policy optimization;quantization hyperparameter space;deep networks alexnet;explore quantization;tuning parametric reward;learning framework releq;proximal policy optimization;quantization levels layers;deep networks;reinforcement learning;40 explore quantization;discovering quantization levels;discovery quantization levels;automated discovery quantization;discovery quantization;discovering quantization;quantization;reward formulation shaping;2018 quantization;solution quantization large;quantization levels end;policy optimization 40"}, "bcd6cd7bdd661bd86c58b7251ae4633a6ba9979e": {"ta_keywords": "automate conference reviewing;recommending papers mining;conference reviewing;reviewers rate keywords;conference reviewing process;papers mining web;papers mining;people recommending papers;conference paper submissions;recommending papers;assigning conference paper;technical paper recommendation;submissions suitable reviewers;paper recommendation cases;paper recommendation;attempts automate conference;suitable reviewers viewed;automate conference;reviewers rate;conference paper;requires reviewers rate;rate keywords;suitable reviewers;actual reviewing preferences;reviewing;mining web;people recommending;task requires reviewers;assigning conference;paper submissions", "pdf_keywords": ""}, "705794a57cca12c2e58b2d77ac32bd4f92ed31ab": {"ta_keywords": "russian using crowdsourcing;thesaurus russian using;readable thesaurus russian;thesaurus russian;machine readable thesaurus;using crowdsourcing;crowdsourcing;thesaurus;readable thesaurus;russnet project aims;russnet project;yarn russnet project;yarn russnet;russian using;2013 yarn russnet;russnet;user study;russian;user study conducted;yarn;end 2013 yarn;2013 yarn;machine readable;open machine readable;open machine;readable;creating;describes project;pilot user study;conducted end 2013", "pdf_keywords": ""}, "fd306df2809c7acc19dd1994e8ecb11caa33290d": {"ta_keywords": "communication environment behaviors;communication environment;personal adaptive communication;adaptive communication environment;spatial personal adaptive;adaptive communication;spatial personal;environment behaviors;communication;environment behaviors objects;personal adaptive;spatial;behaviors;behaviors objects;environment;behaviors objects operations;adaptive;operations knowledge;objects operations knowledge;knowledge;objects;objects operations;operations;personal", "pdf_keywords": ""}, "99e56ebc2f3739dfca93d5a92ebc1e6e2a3050d2": {"ta_keywords": "learning peer grading;peer grading;practice peer grading;peer grading observed;peergrading students grade;assessment moocs empirical;peer grading employed;grading;moocs student evaluation;assessment moocs;grading systems;grading systems provides;peergrading students;grading technology;graders automatically;students grade;grading observed;moocs empirical;student evaluation;auto grading;grade number graders;students grade number;employing peergrading students;grading technology feasible;students setting automated;online courses moocs;learning peer;number graders automatically;moocs empirical theoretical;research assessment moocs", "pdf_keywords": ""}, "65b226f71faaac9b8a4d63445c85601a16635464": {"ta_keywords": "gradient descent sgd;gradient descent gd;stochastic gradient descent;gradient descent;practice gradient descent;sgd truly efficient;performance methods convex;descent sgd workhorses;sgd avoid saddle;gd stochastic gradient;methods convex optimization;descent gd stochastic;nonconvex optimization;descent sgd;nonconvex optimization gap;convex optimization;involved nonconvex optimization;convex optimization problems;stochastic gradient;gd sgd avoid;learning involved nonconvex;polylogarithmic algorithms converge;sgd workhorses;gd sgd;sgd avoid;optimization gap arisen;gd sgd truly;sgd workhorses large;sgd;scale machine learning", "pdf_keywords": "nonconvex optimization;class nonconvex optimization;nonconvex optimization problems;dimensionality non stochastic;stochastic gradient general;guarantees solving nonconvex;stochastic gradient;gradient general stochastic;lipschitz stochastic gradient;learning signal;stationary points global;non stochastic setting;non stochastic;points global minima;second order stationary;solving nonconvex problem;stochastic setting overhead;broad class nonconvex;dependence polylogarithmic overheads;solving nonconvex;second order stationarity;wide class nonconvex;learning signal processing;global minima;machine learning signal;stationary points;order stationary points;general stochastic setting;polylogarithmic overheads;optimization"}, "c6bb9e4a9eaa0f0f8309597af2cefe03bd3f1bb5": {"ta_keywords": "known chaotic dynamical;known chaotic;granularity chaotic systems;length granularity chaotic;chaotic dynamical;generative nature chaos;granularity chaotic;chaotic systems;distributions chaotic systems;chaotic dynamical systems;categorize diverse dynamics;chaotic systems repeatedly;chaotic;distributions chaotic;surrogate transfer learning;information underlying attractor;chaotic systems pose;training benchmarking symbolic;nature chaos;time series databases;chaos;fractal;diverse dynamics;nature chaos like;symbolic regression algorithms;benchmarking symbolic regression;diverse dynamics present;probability distributions chaotic;dynamics present collection;131 known chaotic", "pdf_keywords": "chaotic systems;distributions chaotic systems;chaotic systems repeatedly;distributions chaotic;generative nature chaos;strange attractors;probability distributions chaotic;attractors;chaos like probability;dynamical delay model;attractor arising;chaos interpretable;underlying attractor;chaotic;information underlying attractor;attractor;strange attractors underscores;attractor arises;attractor arising dynamical;geometry strange attractors;nature chaos like;nature chaos;scroll attractor arising;chaos interpretable benchmark;delay model;dynamical delay;delay model implemented;chaos;attractor arises merging;arising dynamical delay"}, "ede108538033ae00d1667685afbd488380020613": {"ta_keywords": "variants antibodies including;respiratory syndrome coronavirus;variants antibodies;antiviral drugs different;syndrome coronavirus sars;sensitivities variants antibodies;antibodies antiviral drugs;syndrome coronavirus;antiviral drugs sars;coronavirus;antibodies antiviral;variants assay revealed;coronavirus sars;ba variants assay;drugs sars cov;coronavirus sars cov;variants assay;variants concern alpha;antiviral drugs;neutralizing antibodies antiviral;subvariant ba variants;sars cov omicron;antiviral;infection assay quantified;antibodies including;efficacies neutralizing antibodies;ba compared variants;compared variants concern;cov omicron subvariants;antibodies", "pdf_keywords": ""}, "f9f862f48599526147bbb110ba986ff6872ef4b0": {"ta_keywords": "uncertainty indoor trajectories;movement uncertainty indoor;modelling movement uncertainty;indoor trajectories mobile;indoor trajectories;trajectories mobile sensing;movement uncertainty;uncertainty indoor;lstm based approach;mobile sensing;mobile sensing data;lstm based;modelling movement;trajectories mobile;lstm;sensing data;approach modelling movement;trajectories;uncertainty;sensing;movement;indoor;modelling;mobile;approach modelling;based approach modelling;data;based;based approach;approach", "pdf_keywords": ""}, "be0c64252a2c3071236d88feeab47d06ef6e0fb7": {"ta_keywords": "recipient recommendation systems;recipient recommendation;useful email clients;corpus recipient recommendation;classification task email;email corpus;accurate useful email;enron email corpus;systems suggesting recipients;classification based reranking;work recipient recommendation;email systems;useful email;easily implemented email;email corpus recipient;corporate email collection;recipient recommendation real;email server propose;email systems enhanced;suggesting recipients;recommendation systems;email clients;email collection;investigate email systems;recommendation systems suggesting;suggesting recipients message;based reranking scheme;scale corporate email;study recipient recommendation;email addressed multiple", "pdf_keywords": ""}, "b661520bf0061b7d96ccf12016e351dd3a6ee780": {"ta_keywords": "weighting deep learning;importance weighting deep;weighting l2 regularization;importance weighting l2;importance weighting;deep networks practical;impacts parameterized deep;importance weighting characterized;realistic deep networks;deep learning;parameterized deep neural;parameterized deep;deep networks;deep linear networks;separable data deep;deep neural;deep neural networks;deep learning present;networks optimized sgd;sgd learn weight;importance weighting impacts;optimized sgd learn;l2 regularization produce;weighting deep;l2 regularization batch;impact importance weighting;data deep;effect importance weighting;l2 regularization;tweak l2 regularization", "pdf_keywords": "weighting deep learning;importance weighting deep;neural networks weighting;importance weighting diminishes;importance weighting;deep learning;deep neural;deep networks;deep neural networks;overparameterized deep neural;impacts overparameterized deep;loss function deep;deep networks fails;gradient descent sgd;convolutional networks trained;networks trained images;effects importance weighting;networks trained;overparameterized deep;importance weighting wellcharacterized;attention transformer;attention transformer based;training tasks architectures;deep learning variety;transfer learning;attention;effect importance weighting;training effect importance;tuned transfer learning;function deep networks"}, "2cd2df06d488565063e0600ff840d293be2eaf31": {"ta_keywords": "game regret matching;playing game regret;game regret;regret matching;regret matching procedure;adaptive procedure playing;play probabilities;play probabilities proportional;proportional measures regret;measures regret;correlated equilibria game;equilibria game;current play probabilities;procedure playing game;regret;play converge;measures regret having;playing game;play;matching procedure players;guarantees probability empirical;adaptive;strategies past shown;strategies past;empirical distributions play;play converge set;guarantees probability;procedure playing;game;adaptive procedure guarantees", "pdf_keywords": "game regret matching;adaptive procedure playing;correlated equilibria game;nash equilibria mathematically;set nash equilibria;adaptive procedure guarantees;equilibria game;game regret;nash equilibria;playing game regret;adaptive;proportional measures regret;regret matching;regret matching procedure;play probabilities proportional;adaptive procedures expected;play probabilities;adaptive procedure;play converge;adaptive procedures;converging correlated equilibria;simple adaptive procedures;play converge set;simple adaptive;simple adaptive procedure;guarantees probability empirical;shown adaptive procedure;correlated equilibria convex;measures regret;current play probabilities"}, "372657f609f5a95b378a1aad7b08deb9b9b510c0": {"ta_keywords": "better policy optimization;policy optimization;policy optimization potential;reinforcement learning framework;continuous control benchmark;model based reinforcement;reinforcement learning;control benchmark tasks;learn dynamics model;control benchmark;reinforcement learning methods;model adaptation minimize;model adaptation;simulated data real;learning framework ampo;based reinforcement learning;learn dynamics;real simulated data;estimation better policy;unsupervised model adaptation;simulated real data;generate simulated data;methods learn dynamics;model real data;optimization potential distribution;distributions real simulated;model estimation better;real data sampled;simulated data;simulated data begin", "pdf_keywords": "policy optimization unsupervised;better policy optimization;policy optimization;policy optimization preliminaries;policy optimization introducing;based policy optimization;unsupervised model adaptation;reinforcement learning framework;model adaptation minimize;introducing model adaptation;learning framework ampo;model based policy;model adaptation;ampo adaptation augmented;framework ampo adaptation;model based reinforcement;reinforcement learning methods;adaptation augmented model;ampo adaptation;reinforcement learning concepts;reinforcement learning;learn dynamics model;estimation better policy;model adaptation procedure;optimization unsupervised model;model adaptation jian;setup reinforcement learning;ampo introduces unsupervised;adaptation minimize;adaptation augmented"}, "a0511f02a867bf19e2fa01e6cbd3663f4bd1b953": {"ta_keywords": "memory causal relationships;causal relationships michael;causal relationships;memory causal;creating memory causal;causal;book;relationships michael pazzani;relationships;book review;michael pazzani;memory;book review creating;pazzani;relationships michael;creating memory;review creating memory;creating;review creating;michael;review", "pdf_keywords": ""}, "1ae1850bcfa3c31d7bc828cc33f7dd3926cee26f": {"ta_keywords": "entity linking discovery;entity discovery linking;linking discovery;discovery linking english;discovery linking;automated entity discovery;entity linking;linking discovery systems;entity discovery;tale entity linking;population entity discovery;discovery linking edl;measuring similarity graphs;linking english language;linking english;knowledge base;automated entity;similarity graphs;strategies measuring similarity;measuring similarity;knowledge base population;tac knowledge base;linking;relations tale entity;probabilistic logics;performing automated entity;linking edl;discovery systems;linking edl track;probabilistic logics complex", "pdf_keywords": ""}, "30cf652bd33049aaf111a5f84eb262a87c045bdb": {"ta_keywords": "fact checkers detecting;sentences verified ranked;research fact checkers;human fact checkers;text similarity stance;document sentences verified;fact checkers;modeling text similarity;fact checkers journalists;list document sentences;text similarity;detecting previously fact;similarity stance taking;annotated dataset task;document sentences;sentences verified;manually annotated;manually annotated dataset;annotated dataset;reranked list document;veracity retrieved;assisting human fact;fact checked claims;similarity stance;new manually annotated;importance modeling text;taking account veracity;retrieved previously fact;previously fact checked;corresponding evidence", "pdf_keywords": "fact checking effort;fact checkers detecting;manual fact checking;human fact checkers;manually annotated sentence;fact checkers;fact checking;annotated dataset task;debates 054 sentences;detecting previously fact;annotated sentence;fact checked claims;claim pairs analysis;consisting seven debates;manually annotated;assisting human fact;consists seven debates;manually annotated dataset;annotated dataset;dataset task formulation;text similarity stance;taking account veracity;veracity retrieved;similarity stance taking;seven debates;new manually annotated;claims match;debates;dataset task propose;veri\ufb01ed claims match"}, "31412f9b23511e212895305927d9ccddb445bcbc": {"ta_keywords": "parameters voice timbre;voice timbre control;statistical voice timbre;control voice timbre;voice timbre controllability;parameters statistical voice;design voice timbre;multiple voice timbre;control parameters voice;parameters voice;annotation voice timbre;voice timbre;converted voice timbre;corresponding multiple voice;statistical voice;timbre control parameters;controlling converted voice;voice timbre axes;described voice timbre;speakers voice timbre;control voice;acoustic basis vectors;multiple voice;parameters acoustic basis;using converted voices;voice timbre investigate;voices natural voices;timbre control multiple;improving converted voice;independences voice timbre", "pdf_keywords": ""}, "633ee881c594cface387557359ef13613d8eaef0": {"ta_keywords": "random assignment mechanisms;egalitarian welfare random;assignment mechanisms random;efficiency envy freeness;welfare random assignment;ordinality envy freeness;optimal egalitarian;optimal egalitarian value;approximate optimal egalitarian;tradeoffs efficiency envy;achievable egalitarian value;welfare random;efficiency envy;assignment mechanisms;ordinality envy;assignment mechanisms agents;achievable egalitarian;truthfulness achievable egalitarian;assignment mechanisms approximate;egalitarian value oev;like ordinality envy;egalitarian welfare;prominent random assignment;random assignment;envy freeness;envy freeness truthfulness;different random assignment;consider egalitarian welfare;egalitarian value;mechanisms approximate optimal", "pdf_keywords": ""}, "1ccd031f28dccfb226f6c0c588c93a97a50bf95f": {"ta_keywords": "benchmark code generation;automatic code generation;code generation performance;evaluating code generation;code generation;assess code generation;code generation difficult;code generation increases;models improve programming;code generation despite;improve programming broadly;improve programming;learn code;generate satisfactory python;benchmark code;language specification generate;large language models;programming broadly;automatic code;language models github;beginning learn code;apps benchmark code;learning models write;models github training;significance automatic code;difficult assess code;arbitrary natural language;language models;code fine tune;assess code", "pdf_keywords": "benchmark code generation;apps benchmark code;upstream program synthesis;code generation;copyrighted work apps;introduce apps benchmark;introduced apps benchmark;benchmark code;000 python programming;apps benchmark;program synthesis advancements;program synthesis;apps benchmark provide;apps benchmark 10;programming problems apps;code generation evaluate;generative models benchmark;fair use copyrighted;copyright fair use;language models github;copyright fair;generation evaluate gpt;problems apps benchmark;models github training;infringement copyright fair;gpt 175b apps;challenge introduce apps;models benchmark overall;10 000 python;python programming"}, "ac5e7f9bbc5d46bebc4ec5616aba9d014a6d237f": {"ta_keywords": "science fiction reviews;undergraduate computer science;fiction reviews used;research literature students;research reviews;computer science curriculum;fiction reviews;undergraduate computer;navigate research literature;research field learn;students exposed research;research topic stirs;science fiction;computer science;learn navigate research;literature students;literature students learn;gateway research reviews;science curriculum;science curriculum generally;critically compare technical;navigate research;technical papers topic;technical papers;chosen science fiction;tv science fiction;movie tv science;tools students;compare technical papers;science fiction book", "pdf_keywords": ""}, "615b823d1fc9548ce384f1bb4f544445175e8537": {"ta_keywords": "pea starch alcohol;properties pea starch;starch alcohol solution;starch alcohol;gelatinization pasting rheological;pea starch;rheological properties pea;gelatinization pasting;starch;gelatinization;alcohol solution;pasting rheological properties;pasting rheological;rheological properties;properties pea;alcohol;rheological;pea;pasting;solution;properties", "pdf_keywords": ""}, "556a4a0b5fcda4d9f9fad637f2655aeb1b1a00b2": {"ta_keywords": "translation paralinguistic information;translation sensitive paralinguistic;paralinguistic information input;translation task paralinguistic;task paralinguistic information;paralinguistic information;paralinguistic information evaluate;translation paralinguistic;sensitive paralinguistic information;method translation paralinguistic;task paralinguistic;paralinguistic information different;paralinguistic features;possible paralinguistic features;speech speech translation;paralinguistic features handle;output speech information;paralinguistic;speech translation;different possible paralinguistic;sensitive paralinguistic;speech translation sensitive;possible paralinguistic;features input speech;output speech;speech information;speech output;information input speech;input speech;speech output speech", "pdf_keywords": ""}, "d6fc0fcf0764065f6e58c57ca850abfdd918504b": {"ta_keywords": "certainty answer questions;log linear scoring;answer question document;answer questions paper;linear scoring model;scoring model;answer questions;error rate training;linear scoring;score according criterion;2013 qa4mre features;questions paper;question document;qa4mre features;rate training mert;questions paper describes;input candidate answer;defines certainty answer;assign score according;question document uses;assign score;answer question;2013 qa4mre;rate training;intersentence features;score according;features;uses assign score;clef 2013 qa4mre;criterion core log", "pdf_keywords": ""}, "7de4a82edf68b69a9c007fe8e840edf4ade1171c": {"ta_keywords": "kinetics ficin arginine;ficin enzymic action;ficin sulphydryl enzyme;ficin arginine derivatives;ficin enzymic;kinetics ficin;attributes ficin enzymic;ficin arginine;recently kinetics ficin;conclusively ficin sulphydryl;ficin sulphydryl;conclusively ficin;ficin;showed conclusively ficin;sulphydryl enzyme characterization;arginine derivatives studied2;sulphydryl enzyme;acetylation free arnino;enzymic action;attributes ficin;general attributes ficin;enzyme characterization;arginine derivatives;enzymic action thoroughly;enzyme characterization properties;effect acetylation free;effect acetylation;properties effect acetylation;acetylation;enzyme", "pdf_keywords": ""}, "2cae732250b59f9e2238626d8d7e0064b97de3c9": {"ta_keywords": "crossing representation wavelet;feature signal reconstruction;signal reconstruction;representation wavelet transform;wavelet transform;signal reconstruction stabilized;representation wavelet;algorithm signal reconstruction;signal reconstruction problem;wavelet transform domain;zero crossing representation;wavelet;image representation early;reconstruction stabilized zero;stabilized zero crossing;extension image representation;image representation;minimum norm optimization;zero crossing;based edge intensity;iterative algorithm signal;edge intensity reduce;operation based edge;reconstruction problem reduces;representation early vision;norm optimization;edge intensity;reconstruction problem;feature signal;algorithm signal", "pdf_keywords": ""}, "616c15dd765c36c21efc75c7ed52e5af81c21053": {"ta_keywords": "proposals artificial intelligence;ai technologies public;funding proposals artificial;ai technologies;ai;acm sigai invites;sigai invites funding;acm sigai;intelligence ai activities;artificial intelligence ai;ai activities;proposals artificial;ai activities strong;funding proposals;invites funding proposals;intelligence ai;working ai technologies;artificial intelligence;practitioners working ai;acm;outreach component;outreach component students;working ai;outreach;invites funding;proposals;funding;strong outreach component;sigai invites;strong outreach", "pdf_keywords": ""}, "bf9b069242f0af129c2aad8430a52454b008c327": {"ta_keywords": "stochastic gradient langevin;learning rate sgd;stochastic gradient descent;descent stochastic gradient;gradient descent stochastic;rate stochastic gradient;gradient langevin dynamics;gradient langevin;learning rate stochastic;stochastic gradient;convergence rate learning;gradient descent sgd;learning rate decay;descent stochastic;sgd contrasting gradient;learning rate linear;learning rate dependent;time formulation sgd;stochastic differential;decay nonconvex optimization;contrasting gradient descent;formulation sgd;rate learning;langevin dynamics;learning rate tends;surrogate sgd paper;surrogate sgd;gradient descent;langevin dynamics based;descent sgd", "pdf_keywords": "stochastic gradient langevin;gradient descent stochastic;descent stochastic gradient;learning rate sgd;stochastic nonconvex optimization;convergence rate learning;stochastic gradient;gradient langevin;broadly stochastic nonconvex;descent stochastic;gradient langevin dynamics;sgd contrasting gradient;stochastic nonconvex;learning rate linear;contrasting gradient descent;learning rate dependent;time formulation sgd;formulation sgd;rate learning;learning rate tends;learning rates;gradient descent;learning rates schr;rate learning rate;zero learning rate;learning rate;surrogate sgd;rate convergence;nonconvex optimization;nonconvex optimization strikingly"}, "250f8f71f7cff972a70482229ca9053b356217cd": {"ta_keywords": "online voice activity;online unsupervised voice;unsupervised voice activity;voice activity detection;online variational bayes;method online voice;online voice;voice activity;use online variational;online variational;variational bayes framework;unsupervised voice;variational bayes method;comparison variational bayes;activity detection vad;bayes framework online;variational bayes;bayes method online;online model comparison;online parallel noise;provided variational bayes;vad using online;context using online;online unsupervised;model comparison variational;using online model;detection vad unsupervised;online model;models estimated online;framework online unsupervised", "pdf_keywords": ""}, "3d2ceea5dea234ae9a20f8e1c9e558735757e90e": {"ta_keywords": "multilingual acoustic models;phone recognition multilingual;recognition multilingual allophone;language dependent phoneme;language independent phones;multilingual acoustic;language independent phone;phone language dependent;independent phone language;language corresponding phones;multilingual allophone multilingual;allophone multilingual asr;multilingual allophone;speech recognition languages;universal phone recognition;phoneme distributions universal;phone language;language dependent recognizers;allophone multilingual;multilingual asr experiments;recognition multilingual;recognition languages world;phone recognition;phone recognizer combined;universal phone recognizer;phoneme distributions;phonemes sounds support;speech recognition;phone recognizer;recognition languages", "pdf_keywords": "multilingual acoustic modeling;multilingual model allophone;multilingual acoustic;accuracy multilingual acoustic;phonology multilingual model;language dependent phoneme;multilingual recognition;recognition based phonetic;phonetic annotation;based phonetic annotation;phonetic annotation tackle;multilingual recognition based;phonology multilingual;acoustic modeling phoneme;independent phone language;phonemic transcriptions allophone;phoneme distribution language;language independent phone;phone language dependent;recognition universal speech;phone language;allophone automatic recognition;method multilingual recognition;asr encoder allophone;transcriptions allophone;transcriptions allophone list;phoneticians;modeling phoneme;knowledge phonology multilingual;modeling phoneme error"}, "8234049255a0e03fc745457de456634d1aab214b": {"ta_keywords": "web page classification;web pages learning;pages learning discovering;understanding web pages;pages learning;discovering structure web;structure web pages;page classification entity;page classification;machines understand pages;extraction web pages;understanding web;structure web;structure web page;extraction web;understand pages;format understanding web;ways structure web;entity extraction web;web pages;learning discovering structure;discovering structure;entities mentioned web;wrapper learning;web pages survey;page techniques;hyperlinks pages;hyperlinks;classification entities;recognizing using structure", "pdf_keywords": ""}, "e11b4750e288785134f042c144f057a11dc0180a": {"ta_keywords": "flexible representative democracy;interactive democracy;hybrid representative democracy;representative democracy introduction;interactive democracy model;representative democracy;using direct democracy;democracy introduction binary;democracy majority voting;representative flexible representative;direct democracy;representative democracy rd;direct democracy majority;introduce flexible representative;flexible representative;representative democracy frd;various democratic systems;set elected representatives;democratic systems using;democracy introduction;majority voting participation;democratic systems;rd direct democracy;voting participation ideal;literature interactive democracy;democracy model;versus representative flexible;representative flexible;democracy;voting participation", "pdf_keywords": "flexible representative democracy;democracy proxy voting;limitations representative democracy;voting representing voters;voting representing;representative democracy proxy;democracy majority voting;representatives hard;proxy voting representing;hybrid representative democracy;variants proxy voting;scale representative democracy;2019 flexible representative;democracy introduction binary;representing voters;voting ideal;majority voting ideal;set elected representatives;representative democracy;set representatives hard;introduce flexible representative;outperforms representative democracy;symmetric issues electing;representative democracy introduction;representative democracy uses;outcomes voter participation;representative democracy provide;majority voting;election rules selecting;proxy voting yield"}, "b8b813111c411ae61881ab9cd25707d9de6444ec": {"ta_keywords": "key value attention;attention disentangling search;multi head attention;generalization compositional attention;value attention;retrieval easily implemented;compositional attention;value attention backbone;value attention blocks;scaling search retrieval;key interactions retrieval;retrieval composes;retrieval composes dynamic;compositional attention disentangling;disentangling search retrieval;called compositional attention;head attention;compositional attention replaces;attention;search retrieval composes;attention heads;disentangles search retrieval;standard attention heads;attention blocks;attention heads network;variants attention mechanism;attention mechanism;variants attention;head attention allows;attention mechanism called", "pdf_keywords": "attention disentangling search;attention variety tasks;multi head attention;compositional attention;head attention variety;compositional attention leads;compositional attention disentangling;attention variety;head attention;key value attention;retrieval composes dynamic;retrieval composes;attention heads;value attention;scaling search retrieval;disentangling search retrieval;attention;search retrieval composes;head attention allows;value attention backbone;disentangles search retrieval;standard attention heads;attention mechanisms;attention heads network;2022 compositional attention;retrieval;attention disentangling;demonstrate compositional attention;attention leads dynamic;search retrieval sarthak"}, "a469f2ec3ab15f20f06d95aea1839b1263d3385e": {"ta_keywords": "random assignment mechanisms;assignment mechanisms random;efficiency envy freeness;ordinality envy freeness;optimal egalitarian;welfare aspects random;optimal egalitarian value;approximate optimal egalitarian;tradeoffs efficiency envy;achievable egalitarian value;efficiency envy;ordinality envy;assignment mechanisms;truthfulness achievable egalitarian;achievable egalitarian;assignment mechanisms agents;egalitarian welfare;assignment mechanisms approximate;like ordinality envy;egalitarian welfare aspects;envy freeness;envy freeness truthfulness;random assignment;prominent random assignment;consider egalitarian welfare;different random assignment;egalitarian value;mechanisms random;mechanisms approximate optimal;serial dictatorship probabilistic", "pdf_keywords": "tradeoffs ef\ufb01ciency envy;random assignment mechanisms;ef\ufb01ciency envy freeness;equilibria assignment mechanisms;fairness ef\ufb01ciency randomized;tradeoffs fairness ef\ufb01ciency;assignment mechanisms random;ef\ufb01ciency envy;randomized mechanisms assignment;explore tradeoffs fairness;ef\ufb01ciency randomized mechanisms;fairness ef\ufb01ciency;tradeoffs fairness;welfare nash equilibria;randomized mechanisms;assignment mechanisms;ef\ufb01ciency randomized;utilitarian welfare nash;analyzing tradeoffs ef\ufb01ciency;envy freeness;nash equilibria assignment;random assignment;prominent random assignment;utility functions distributions;utility functions;tradeoffs ef\ufb01ciency;envy freeness truthfulness;probabilistic serial mechanism;results utilitarian welfare;mechanisms assignment problem"}, "18f4ec53a4221a97e1482f091f41a23f3d873cf2": {"ta_keywords": "evidence annotations strong;supervised evidence extraction;evidence annotations;semi supervised evidence;combine evidence annotations;practice evidence annotations;evidence extraction;supervised evidence;evidence annotations available;evidence extraction practice;labels weak supervision;weakly semi supervised;annotations strong;strong semi supervision;annotations strong semi;supervision abundant document;semi supervised;task evidence extraction;predictions supporting evidence;annotations;annotations available minority;weak supervision task;semi supervision;supporting evidence;semi supervision abundant;supervision task evidence;annotations available;supporting evidence human;prediction tasks stakeholders;combine evidence", "pdf_keywords": "evidence annotations strong;evidence sequence labeling;evidence extraction predicted;evidence annotations;labels weak supervision;combine evidence annotations;generating evidence label;learning extract evidence;sequence labeling tasks;evidence extraction;annotations strong;evidence annotations \ufb01nd;annotations strong semi;text classi\ufb01cation evidence;semi weak supervision;evidence extraction methods;feature evidence annotations;conditioning evidence extraction;extraction predicted label;evidence label especially;strong semi supervision;task evidence extraction;weak supervision task;annotations;evidence label;baselines annotations present;baselines annotations;sequence labeling;annotations present methods;label classify extract"}, "c4536a5c7f47bfc48df202ba882002531248f955": {"ta_keywords": "sounds generated electrolarynx;generated electrolarynx based;electrolarynx automatic fundamental;statistical prediction electrolarynx;enhanced electrolarynx automatic;electrolarynx based statistical;electrolarynx automatic;electrolarynx based;prediction electrolarynx;electrolarynx type speaking;electrolaryngeal el speech;laryngectomees produce electrolaryngeal;performance enhanced electrolarynx;generate excitation sounds;prediction electrolarynx type;generated electrolarynx;physical electrolarynx;enhanced electrolarynx;frequency sounds generated;actual physical electrolarynx;electrolarynx evaluate performance;speech real time;electrolarynx;sounds help laryngectomees;excitation sounds proposed;frequency control based;sounds generated;automatic fundamental frequency;physical electrolarynx evaluate;electrolarynx type", "pdf_keywords": ""}, "f43ae70242aea3dbb80b7c3b5474356e9ee9079b": {"ta_keywords": "meaning representation amr;representation amr;natural language represents;language represents meaning;representation amr popular;meaning representation;represents meaning sentence;amr popular formalism;amr;sentence semantic graph;meaning sentence semantic;semantic;semantic graph;represents meaning;language represents;natural language;sentence semantic;multilingual natural language;language processing;natural language processing;formalism natural language;representation;represents;computational typology multilingual;multilingual natural;multilingual;language;amr popular;typology multilingual natural;typology multilingual", "pdf_keywords": ""}, "72c9663494827b2e87ad5a65a6ff7e769eb15a57": {"ta_keywords": "visual storytelling dataset;visual storytelling;experiments visual storytelling;storytelling dataset;storytelling dataset vist;high quality story;storytelling;quality story human;quality story;story paper examine;topically coherent story;makes good story;relevance coherence expressiveness;story paper;coherent story end;human evaluation;automatic human evaluation;reco rl reward;story human;story human eye;human evaluation demonstrate;rl reward;story end;coherent story;visual;rl reward functions;reward;story end propose;coherence expressiveness;evaluation demonstrate reco", "pdf_keywords": "story generation quality;generation visual storytelling;optimizes story generation;visual storytelling dataset;visual storytelling task;story generation;visual storytelling;approach visual storytelling;storytelling directly optimizes;visual storytelling directly;experiments visual storytelling;quality text generation;text generation visual;storytelling dataset;storytelling dataset vist;optimizes story;directly optimizes story;storytelling task;generation visual;text generation;storytelling directly;storytelling;assess quality text;quality text;automatic human evaluation;expressiveness experiments visual;generation quality;human evaluation;policy gradient training;human evaluation demonstrate"}, "15bb07d0996ece844de8cae24d3dc15972e6841a": {"ta_keywords": "summarization datasets study;know summarization datasets;summarization datasets;popular summarization datasets;summarization datasets despite;summarization systems trained;summarization complexity datasets;state art summarization;summarization models popular;summarization models;summarization systems;samples popular summarization;know summarization;summarization complexity;popular summarization;web know summarization;art summarization systems;noise summarization complexity;art summarization models;summarization;data noise summarization;noise summarization;summaries receive low;summaries receive;summaries;automatic metrics like;automatic metrics;art summarization;popular metrics report;key insights datasets", "pdf_keywords": "summarization datasets;summarization models reveals;popular summarization datasets;summarization datasets conclusion;summarization models popular;summarization models;27 summarization models;samples popular summarization;evaluation popular summarization;state art summarization;summarization datasets gigaword;popular summarization;summarization;art summarization models;27 summarization;analysis 27 summarization;metric performance heavily;popular metrics report;automatic metrics;art summarization;automatic metrics like;metric performance;datasets conclusion study;popular metrics;metrics report;reveals metric performance;models popular metrics;reliability automatic metrics;metrics like;key insights datasets"}, "674833d48a77ef009f751a66988590592dd5d996": {"ta_keywords": "\u5206\u6563\u5171\u6709\u30d5\u30eb\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30e2\u30c7\u30eb\u306b\u3088\u308bhmm\u97f3\u58f0\u5408\u6210\u306e\u6539\u5584 \u4e00\u822c\u30bb\u30c3\u30b7\u30e7\u30f3 \u798f\u7949\u3068\u97f3\u58f0\u51e6\u7406;\u5206\u6563\u5171\u6709\u30d5\u30eb\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30e2\u30c7\u30eb\u306b\u3088\u308bhmm\u97f3\u58f0\u5408\u6210\u306e\u6539\u5584 \u4e00\u822c\u30bb\u30c3\u30b7\u30e7\u30f3;\u5206\u6563\u5171\u6709\u30d5\u30eb\u30b3\u30f3\u30c6\u30ad\u30b9\u30c8\u30e2\u30c7\u30eb\u306b\u3088\u308bhmm\u97f3\u58f0\u5408\u6210\u306e\u6539\u5584;\u4e00\u822c\u30bb\u30c3\u30b7\u30e7\u30f3 \u798f\u7949\u3068\u97f3\u58f0\u51e6\u7406;\u4e00\u822c\u30bb\u30c3\u30b7\u30e7\u30f3 \u798f\u7949\u3068\u97f3\u58f0\u51e6\u7406 \u4e00\u822c;\u798f\u7949\u3068\u97f3\u58f0\u51e6\u7406 \u4e00\u822c;\u798f\u7949\u3068\u97f3\u58f0\u51e6\u7406;\u4e00\u822c\u30bb\u30c3\u30b7\u30e7\u30f3;\u4e00\u822c", "pdf_keywords": ""}, "49ee2270f3265ee27b36e05e130be79e05d5ba29": {"ta_keywords": "textbook knowledge case;learning textbook knowledge;textbook knowledge;problem learning texts;natural language understanding;learning texts including;learning texts;problem learning textbooks;problems natural language;learning textbooks learning;texts including omissions;learning textbook;textbooks learning;knowledge case;natural language;language understanding;addresses problem learning;manually translated logical;omissions inconsistencies clarified;learning textbooks;knowledge case study;textbooks learning textbook;language understanding consider;textbook;problem learning;omissions inconsistencies;including omissions inconsistencies;translated logical theory;learning problem;logical theory", "pdf_keywords": ""}, "68ea2572584068befd441dccf461f3444ff14f4a": {"ta_keywords": "learning agent physical;like learning agent;simstudent cognitive robot;game simstudent robot;educational robot;learning agent;cognitive robot;agent able learn;creating educational robot;robot able learn;simstudent robot;cognitive robot calliope5kp;simstudent robot able;intelligent agent;learning agent integrate;building intelligent agent;educational robot embedding;toe game simstudent;agent models student;create physical agent;agent physical world;embedding learning agent;human like learning;skill knowledge interacting;robot calliope5kp;learn skill knowledge;robot;integrate simstudent cognitive;physical agent;learn skill", "pdf_keywords": ""}, "4cfc7d3c6a61f6db48b1f3c75235592c1609a54f": {"ta_keywords": "transcription interface;assisted transcription quality;transcription quality iterative;scratch transcription interface;automatic transcription quality;computer assisted transcription;quality speech transcription;transcription quality;assisted transcription;automatic transcription;superior automatic transcription;transcription quality insufficient;speech transcription;automatic transcription segment;transcription reduced costs;iterative scratch transcription;assisted transcription promises;parts automatic transcription;transcription;speech transcription reduced;transcription promises high;scratch transcription;iterative user interfaces;transcription promises;transcription segment;quality iterative;quality iterative user;iterative interfaces;high quality speech;transcription segment contained", "pdf_keywords": ""}, "e2ac96254d7e9ec0dde882e3a09797d00f26220f": {"ta_keywords": "machine learning;machine learning proceedings;icml 2006;learning;conference icml 2006;international conference icml;icml;conference icml;icml 2006 pittsburgh;machine;international conference;usa;usa june;proceedings international conference;international;conference;june;2006;usa june 25;pennsylvania usa;pennsylvania usa june;2006 pittsburgh pennsylvania;pennsylvania;29 2006;learning proceedings;june 25;learning proceedings international;june 25 29;2006 pittsburgh;pittsburgh pennsylvania usa", "pdf_keywords": ""}, "757acf616a38422c7186952e1075a28fed1a07c0": {"ta_keywords": "xylazole inhibits cgmp;analgesic effects xyl;xyl inhibited cgmp;anesthetic mechanism xyl;xyl veterinary anesthetic;xylazole inhibits;xyl related inhibition;xylazole xyl veterinary;xyl using fetal;suggested xyl inhibited;xyl inhibited;fetal rat nerve;cells treated xyl;xylazole xyl;vitro background xylazole;anesthetic analgesic effects;inhibits cgmp pathway;cgmp pathway fetal;functionally similar xylazine;xylazine;anesthetic analgesic;xylazole;rat nerve cells;xylazine variations amino;inhibited cgmp signaling;inhibits cgmp;inhibition cgmp signaling;signaling pathway nerve;effects xyl related;mechanism xyl using", "pdf_keywords": ""}, "fc912e9af47bf10428396b687b2bfb1e5832fcb1": {"ta_keywords": "speech recognition asr;ctc based speech;speech recognition;automatic speech recognition;loss regularization ctc;connectionist temporal classification;temporal classification ctc;asr based connectionist;function automatic speech;automatic speech;recognition asr based;recognition asr;ctc encoder;cer aishell corpus;regularization ctc based;efficient auxiliary loss;based speech recognition;regularization ctc;ctc encoder network;classification ctc;layer ctc encoder;intermediate loss regularization;aishell corpus;aishell corpus respectively;intermediate ctc loss;based connectionist temporal;loss regularization;regularizes ctc training;auxiliary loss function;wer wsj corpus", "pdf_keywords": "speech recognition asr;speech recognition;automatic speech recognition;speech recognition jaesong;ctc based speech;loss regularization ctc;automatic speech;function automatic speech;recognition asr;intermediate loss regularization;regularization ctc based;recognition asr based;based speech recognition;regularization ctc;asr based connectionist;cer aishell corpus;loss regularization;ef\ufb01cient auxiliary loss;ctc loss stochastic;autoregressive decoder;auxiliary loss;aishell corpus;loss function automatic;auxiliary loss function;aishell corpus respectively;aishell autoregressive decoder;decoder external language;based autoregressive decoder;ctc encoder;wer wsj corpus"}, "f3132572bb3870dbe99b2d1c01ce17fa38783a2f": {"ta_keywords": "robust speech processing;robust speech;uncertainty propagation session;uncertainty uncertainty propagation;speech processing;uncertainty propagation;speech processing using;observation uncertainty uncertainty;using observation uncertainty;observation uncertainty;uncertainty uncertainty;uncertainty;robust;speech;processing using observation;propagation session paper;propagation session;using observation;processing using;processing;session paper overview;propagation;session paper;observation;session;paper overview;overview;paper;using", "pdf_keywords": ""}, "213e471bacff5c0852943988fcb955797f1e591f": {"ta_keywords": "metrics machine translation;existing reference translations;machine translation increasingly;reference translations;machine translation;reference translations counteracts;translations counteracts bias;quality automatic metrics;collect references compare;concentrating translationese language;references exhibit poor;alternative multi reference;diversity concentrating translationese;finding typical references;translations;translation increasingly;references compare;concentrating translationese;translation increasingly called;paraphrasing task linguists;translationese language develop;automated evaluation reporting;typical references exhibit;translations counteracts;translationese language;collect references;multi reference bleu;multi reference;language develop paraphrasing;multi reference formulation", "pdf_keywords": "metrics machine translation;underestimated references translationese;machine translation increasingly;machine translation;quality automatic metrics;references translationese artifacts;references translationese;automated evaluation reporting;english german evaluation;human evaluation submissions;automatic metrics using;value automated evaluation;automatic metrics machine;evaluation submissions wmt;automatic metrics;human evaluation variety;german evaluation;automated evaluation;evaluation reporting;translation increasingly;human evaluation;translation ape augmented;translation increasingly called;translationese artifacts;references compare;correlation human evaluation;collect references compare;demonstrate paraphrased references;tends underestimated references;paraphrased references correlate"}, "77568c594470f9aa029f92774e2c12ab0451d9bb": {"ta_keywords": "language models;knowledge test distribution;likelihood mle training;topics news reviews;distributionally robust optimization;language models generally;training text;mixture topics sufficient;mixture topics;training distribution;topics news;range topics news;test distribution propose;mle training;paper training text;new distributionally robust;case mixture topics;test distributions;distributionally robust;topics sufficient overlap;training text outside;reviews paper training;overlap training distribution;maximum likelihood mle;news reviews;likelihood mle;test distribution degrade;topics;maximum likelihood;mle training remedy", "pdf_keywords": "language models;robust language modeling;distributionally robust language;language models generally;language modeling;abstract language models;language modeling fails;language modeling yonatan;robust language;approach language modeling;likelihood training;robust mle subpopulation;maximum likelihood training;robust mle;likelihood mle training;cvar robust mle;topic cvar robust;based distributionally robust;likelihood training large;mle subpopulation;corpus propose;new distributionally robust;training text;modeling fails challenges;distributionally robust;sentences corpus propose;sentences corpus;corpus;distributionally robust optimization;common sentences corpus"}, "89c64fd60ca58f4753a818cd0923f5041b51a807": {"ta_keywords": "placing relays walks;multi connectivity;relays walks;routing multiple;relays walks order;relay network sensor;relay network;routing multiple available;wireless relay network;achieving connected multihop;alternate routing multiple;place relay based;connected wireless relay;network cost objective;relay based link;placing relays;connected multihop network;disadvantages multi connectivity;multi connectivity validate;place relay;network connecting sensor;propose network cost;placed relays;wireless relay;multihop network;routing;additive deployed relays;network sensor;network cost;example placing relays", "pdf_keywords": ""}, "612d577534dbbf546405d4036d912666523a8164": {"ta_keywords": "learnable description logics;known description logics;known learnable description;learnable description;learnability restricted;description logics;logics known description;learnability restricted order;instead learnability restricted;description logics called;description logic;simple description logic;learnability order representations;description logics subsets;learnable sublanguage;learnability;description logic summarize;learnability order;languages learnable sublanguage;type languages learnable;analyze learnability;learning concepts expressed;instead learnability;polynomial learnability;learnable sublanguage appears;terminological logics kl;terminological logics;languages learnable;polynomial learnability order;logics called terminological", "pdf_keywords": ""}, "0d516b476559485e04290e859ca59101c0a91ae1": {"ta_keywords": "relay exploration switching;relay exploration;obstacles millimeter wave;local relay selection;relay selection presence;relay selection;exploration switching;frequency relay exploration;dynamic obstacles millimeter;additional exploration time;markov decision process;blockage obstacles millimeter;exploration switching average;d2d communication seek;average exploration time;observable markov decision;markov decision;exploration time;selecting given relay;local relay;obstacles millimeter;uncertainty d2d link;locally local relay;exploration time units;exploration time average;select new relay;relay;tradeoff average exploration;decision process pomdp;relay locally", "pdf_keywords": "obstacles millimeter wave;relay exploration switching;relay exploration;relay selection presence;local relay selection;relay selection;frequency relay exploration;blockage obstacles millimeter;dynamic obstacles millimeter;selecting given relay;relay locally extensive;millimeter wave mmwave;obstacles millimeter;exploration switching;local relay;millimeter wave d2d;additional exploration time;exploration switching average;select new relay;millimeter wave;average exploration time;blockage obstacles;given relay locally;relay locally;wave mmwave;relay;given relay;wave mmwave device;exploration time average;exploration time"}, "99fe5475ab28fa7ad4bce51d7b294b3f40caad4d": {"ta_keywords": "concept learning web;extracting features concept;automatically extracting features;features concept learning;concept learning;extracting features;learning web;automatically extracting;features concept;extracting;learning;concept;features;automatically;web", "pdf_keywords": ""}, "31392ad8722d9c66181b621936e2013199e02edc": {"ta_keywords": "language understanding large;100m words learn;ability encode linguistic;improvements language understanding;nlu tasks lms;language model knowledge;tuning nlu tasks;lms learn large;lms learn;reliably encode syntactic;models pretrained 1m;pretraining learn data;transformer lms learn;model knowledge probing;learn reliably encode;nlu tasks explore;large pretrained models;knowledge probing;encode linguistic features;unsupervised language model;encode linguistic;language understanding;language understanding likely;words learn reliably;judgments unsupervised language;encode syntactic semantic;pretraining learn;models pretrained;pretrained models;knowledge skills transformer", "pdf_keywords": "probing linguistic features;probing linguistic;words pretraining data;learnable 100m words;billions words pretraining;tuning nlu tasks;pretrained 100m words;linguistic features learnable;look probing linguistic;words pretraining;pretrained 30b words;measures linguistic ability;nlu tasks lms;words data nlu;linguistic knowledge roberta;human language understanding;100m words data;100m words;nlu tasks;nlu task performance;linguistic ability;measures linguistic;100m 1b words;linguistic knowledge;linguistic features;data nlu task;linguistic features explain;features learnable 100m;words data;acquire billions words"}, "6c82727731955a2332a0cc38ec56b35a971061eb": {"ta_keywords": "translation toolkit xnmt;machine translation toolkit;translation parsing xnmt;neural machine translation;toolkit xnmt;translation toolkit;toolkit xnmt distin;tasked machine translation;parsing xnmt extensible;machine translation;machine translation speech;parsing xnmt;source nmt toolkits;nmt toolkits;machine translation parsing;xnmt extensible neural;tasks machine translation;translation parsing;xnmt extensible;open source nmt;extensible neural machine;translation speech recognition;design xnmt;describes xnmt extensible;describes xnmt;paper describes xnmt;neural machine;xnmt;xnmt distin;nmt toolkits focus", "pdf_keywords": "neural machine translation;machine translation toolkit;xnmt nmt toolkit;machine translation speech;machine translation task;tasked machine translation;tasks machine translation;parsing machine translation;machine translation;machine translation parsing;inference training nmt;nmt toolkit extensibility;translation speech recognition;translation toolkit;model parsing machine;xnmt nmt;nmt toolkit;task mt parsing;translation toolkit toolkit;translation parsing;introduced xnmt nmt;design xnmt;translation task;sequence model parsing;model parsing;describes xnmt;xnmt extensible neural;parsing machine;mt parsing;training nmt"}, "7e406537f52528527d10872d1807ad974599b13a": {"ta_keywords": "neural generation translation;neural machine translation;generation translation summarize;workshop neural generation;generation translation;neural generation;machine translation;machine translation nmt;level generation translation;generate summaries structured;generation translation held;systems generate summaries;workshop neural;generation translation dgt;document level generation;generate summaries;fourth workshop neural;translation summarize;creation possible translations;translation summarize research;translation nmt participants;tasks efficient neural;summaries structured;translation nmt;efficient neural machine;language staple task;efficient neural;neural machine;translations;neural", "pdf_keywords": ""}, "5babe5334c6867db13fa7e6943f64059c7cba6ce": {"ta_keywords": "information representation language;word based information;based information representation;information representation;whirl word based;word based;representation language;whirl word;based information;language;word;representation;information;whirl;based", "pdf_keywords": ""}, "c0cce8955bf10b21753161ffaa1978a7c8b78a16": {"ta_keywords": "audible murmur enhancement;murmur enhancement based;using nam microphone;nam microphone air;nam microphone;murmur enhancement;microphone non audible;called nam microphone;statistical voice conversion;microphone called nam;microphone air;voice conversion vc;voice conversion;microphone;air conductive microphone;microphones noisy environments;microphone non;microphone air conductive;microphone called;body conductive microphone;non audible murmur;microphones noisy;microphones;body conductive microphones;audible murmur nam;speech communication quality;conductive microphones noisy;statistical voice;whispered voice detected;conductive microphone", "pdf_keywords": ""}, "6027ef3b4e5585b45db0b9d333956425d3972351": {"ta_keywords": "commonsense reasoning benchmarks;commonsense reasoning opencsr;making commonsense reasoning;current commonsense reasoning;commonsense reasoning research;task answering commonsense;commonsense reasoning;popular commonsense reasoning;answering commonsense;open ended commonsense;commonsense knowledge answer;use commonsense knowledge;commonsense knowledge;ended commonsense reasoning;answering commonsense question;resource corpus commonsense;models use commonsense;step making commonsense;use commonsense;reasoning benchmarks collect;commonsense facts;sourcing current commonsense;opencsr task answering;corpus commonsense facts;adapt popular commonsense;commonsense;reasoning benchmarks;current commonsense;making commonsense;corpus commonsense", "pdf_keywords": "targeting commonsense reasoning;current commonsense reasoning;commonsense reasoning research;commonsense reasoning novel;commonsense reasoning;reasoning knowledge corpus;commonsense knowledge;use commonsense knowledge;commonsense reasoning yuchen;ended commonsense reasoning;open ended commonsense;datasets targeting commonsense;commonsense knowledge answer;reasoning novel crowdsourced;models use commonsense;targeting commonsense;current commonsense;use commonsense;knowledge corpus facts;knowledge corpus;abstract current commonsense;commonsense;reasoning knowledge facts;reasoning knowledge;reasoning method traverses;traverses corpus;ended commonsense;hop reasoning knowledge;traverses corpus hypergraph;reasoning novel"}, "a38e0f993e4805ba8a9beae4c275c91ffcec01df": {"ta_keywords": "models program synthesis;programs natural language;program synthesis;language models program;program synthesis general;programming languages benchmarks;programming languages;large language models;generation large language;models synthesize code;languages benchmarks designed;python programs natural;programming;programming tasks;languages benchmarks;synthesize short python;purpose programming languages;language models;general purpose programming;short python programs;programming tasks designed;python programs;natural language descriptions;language descriptions;synthesize code;short python;synthesize code complex;974 programming tasks;purpose programming;large language", "pdf_keywords": "neural code completion;code completion;vulnerabilities neural code;models synthesize code;transformer language models;synthesis short programs;code completion finally;language models performs;synthesize code;large transformer language;vulnerabilities neural;synthesize code complex;autocomplete poisoning vulnerabilities;poisoning vulnerabilities neural;programming languages;neural code;transformer language;language models;programming languages mathqa;programs written general;programming;autocomplete poisoning;program execution;short programs;models synthesize;short programs written;general purpose programming;purpose programming languages;programs written;languages mathqa python"}, "4f4da6fdb9496b0295764b2db11381dd390de02d": {"ta_keywords": "correction speech transcripts;cost models transcription;speech transcripts framework;recognized speech transcripts;cost sensitive correction;sensitive correction speech;transcripts framework;manual correction automatically;automatically recognized speech;speech transcripts;transcripts framework trains;sensitive manual correction;correction speech;transcription;transcripts;models transcription;cost sensitive manual;manual correction;correction automatically recognized;models transcription process;sensitive correction;modeling cost sensitive;require transcriber;supervise cost sensitive;transcriber;transcription process;correction automatically;does require transcriber;segmentation segments supervise;transcription process does", "pdf_keywords": ""}, "c581686edbd7227e9eb4a0841cce16728ca27369": {"ta_keywords": "multimodal comprehension cooking;comprehension cooking recipes;comprehension cooking;multimodal comprehension;dataset multimodal comprehension;reasoning cooking recipes;recipes understanding reasoning;reasoning cooking;understanding reasoning cooking;recipes understanding;recipeqa dataset multimodal;knowledge recipeqa challenge;cooking recipes understanding;instructional recipes multiple;instructional recipes;comprehension reasoning tasks;recipeqa challenge;20k instructional recipes;recipeqa challenge dataset;knowledge recipeqa;procedural knowledge recipeqa;recipes;recipes multiple;cooking recipes;cooking recipes fruitful;introduce recipeqa dataset;cooking recipes work;cooking;recipeqa dataset;introduce recipeqa", "pdf_keywords": "multimodal comprehension cooking;comprehension cooking;comprehension cooking recipes;multimodal comprehension;dataset multimodal comprehension;understanding reasoning cooking;reasoning cooking recipes;reasoning cooking;knowledge recipeqa challenge;comprehension reasoning tasks;machine comprehension systems;machine comprehension;recipeqa dataset multimodal;procedural knowledge recipeqa;comprehension systems;recipeqa challenge dataset;recipeqa challenge;knowledge recipeqa;instructional recipes multiple;evaluating machine comprehension;instructional recipes;20k instructional recipes;cooking recipes computer;comprehension reasoning;reasoning tasks;joint understanding images;cooking recipes fruitful;cooking recipes;introduce recipeqa dataset;sense procedural knowledge"}, "ad48174ccbff6259a7d3cb0d0985e5aefa314b84": {"ta_keywords": "level machine translation;modeling machine translation;machine translation mt;character level modeling;machine translation able;machine translation;translation quality;translation able robustness;character level systems;character level machine;noise translation quality;translation quality does;subword systems virtually;subword systems;comparable subword systems;noise translation;use character level;character level;source noise translation;literature character level;translation able;level modeling machine;translation mt;art character level;decoding;systems comparable subword;level machine;level systems comparable;people use character;level modeling", "pdf_keywords": "level machine translation;characterlevel natural language;language processing characterlevel;character level modeling;modeling machine translation;machine translation mt;character level machine;characterlevel mt systems;processing characterlevel mt;processing characterlevel;characterlevel natural;machine translation;characterlevel;modeling innovations characterlevel;innovations characterlevel natural;innovations characterlevel;characterlevel mt;translation mt empirically;use character level;character level mt;character level;subword based counterparts;level modeling machine;language processing;speech processing lmu;subword based;level machine;level modeling;people use character;speech processing"}, "1e3e2b03e28f48bb4d48154992cd6b62969c643e": {"ta_keywords": "protein sequence database;protein sequence;value protein sequence;annotated sequence entries;000 annotated sequence;swiss prot curated;annotated sequence;sequence database strives;description function protein;sequence database;prot curated;prot curated added;80 000 annotated;000 annotated;value protein;protein domain structure;protein domain;high level annotations;protein;entries 500 species;function protein;swiss prot;added value protein;swiss prot near;sequence entries 500;annotations;annotations description;annotated;500 species;sequence entries", "pdf_keywords": ""}, "7b51209e7d9cbedc18b6ab202e6fcdabaebbb088": {"ta_keywords": "discriminative language modeling;learned word features;embeddings learned word;neural network discriminative;language modeling;word features;network discriminative language;language modeling structured;modeling structured classification;product feature representation;feature representation;discriminative language;embeddings learned;structured classification;word features extracted;parameterized neural network;problem embeddings learned;function parameterized neural;parameterized neural;feature representation used;classification problem embeddings;continuous space model;neural network;embeddings;network discriminative;log linear models;dot product feature;structured classification problem;convolutional layers log;model continuous space", "pdf_keywords": ""}, "795aca47df94300fa6bfd464e6873aef56c7f3ae": {"ta_keywords": "extraction synsets semantic;multilingual babelnet semantic;babelnet semantic network;babelnet semantic;semantic relations babelnet;synsets semantic relations;recurrent extraction tasks;babelnet tool extracts;babelnet extract;synsets semantic;lexical semantic resources;addressing recurrent extraction;semantic resources wordnet;large lexical semantic;effective extraction synsets;babelnet tool;samples large lexical;multilingual babelnet;recurrent extraction;extraction synsets;relations babelnet tool;natural language processing;present babelnet extract;resources wordnet wiktionary;wordnet;synsets form semantic;word senses synsets;experiments natural language;omegawiki evaluation training;extracts individual word", "pdf_keywords": ""}, "4cd66273298128dfb5be290e891870085ecfc455": {"ta_keywords": "end speech recognition;speech recognition end;ctc attention decoding;attention decoding end;speech recognition asr;speech recognition;asr attention based;ctc attention architecture;dnn hmm systems;automatic speech recognition;end automatic speech;attention decoding;automatic speech;hybrid ctc attention;end end speech;end speech;recognition end end;conventional dnn hmm;attention architecture;connectionist temporal classification;architectures asr attention;temporal classification ctc;recognition asr;end end asr;end architectures asr;attention architecture effectively;hmm systems;recognition end;joint ctc attention;decoding end", "pdf_keywords": ""}, "7099d5a4b2d4ed47905071fc23aff08580401e42": {"ta_keywords": "networks rnn;neural networks rnn;state speech recognition;networks rnn layers;rnn layers models;recurrent neural networks;echo state speech;untrained echo state;speech recognition;echo state network;speech recognition asr;rnn layers;rnn conformer models;automatic speech recognition;models inspired echo;automatic speech;focuses rnn conformer;initialized untrained echo;rnn conformer;training asr models;rnn;inspired echo state;untrained echo;recurrent neural;recognition asr models;focuses rnn;speech recognition study;propose automatic speech;subset recurrent neural;neural networks", "pdf_keywords": "speech recognition rnn;rnn layers models;recognition rnn;neural networks rnn;networks rnn layers;networks rnn;recognition rnn traditionally;rnn layers rnn;rnn layers;rnn echo state;layers rnn;rnn conformer models;rnn echo;network recurrent input;recurrent neural networks;speech models excellent;layers rnn conformer;speech recognition asr;networks rnn contrast;representations acoustic inputs;state network recurrent;type rnn echo;speech models;properly trained encoders;rnn conformer;network recurrent;state speech recognition;subset rnn layers;rnn contrast randomizing;rnn"}, "af460a6b3ecaddd4015b34255564c366ecfef802": {"ta_keywords": "teach computer ethics;ethics science fiction;computer ethics science;moral imagination teach;computer ethics;imagination teach computer;moral imagination;science fiction;ethics science;science fiction particular;teach computer;capacity moral imagination;fiction;imagination teach;moral;ethics;fiction particular;fiction particular offers;cultivate capacity moral;students way cultivate;teach;science;students way;capacity moral;offers students way;students;offers students;imagination;particular offers students;computer", "pdf_keywords": ""}, "5861dbfcb253ca02067dd182d42b7d567433c834": {"ta_keywords": "frontdoor estimators;applies confounders observed;confounders mediators observed;confounders mediators;confounders observed;confounders observed frontdoor;backdoor frontdoor estimators;scenario confounders mediators;causal model derive;estimators example backdoor;formula applies confounders;applies confounders;gaussian causal model;confounders;linear gaussian causal;causal graph calculus;gaussian causal;identified scenario confounders;estimators valid calculus;causal;causal effect derive;causal model;scenario confounders;mediator transmits causal;given causal;transmits causal effect;transmits causal;causal effect;observed frontdoor formula;parameters given causal", "pdf_keywords": "causal inference econometrics;frontdoor estimators;frontdoor estimators model;estimators combine confounders;frontdoor estimators improvement;backdoor frontdoor estimators;causal inference;study causal inference;inference econometrics explore;inference econometrics;analysis linear causal;confounders mediators observed;confounders mediators exhibit;linear causal model;confounders mediators;linear causal;combine confounders mediators;linear gaussian causal;scenario confounders mediators;gaussian causal model;causal model;causal;gaussian causal;econometrics explore semiparametric;causal model demonstrate;study causal;estimators model assumptions;confounders;identi\ufb01ed scenario confounders;scenario confounders"}, "0f655f0e1937ad19b038952e2df69e30d447aac8": {"ta_keywords": "training models missingness;missing data sequential;models missingness patterns;missingness linear models;missingness patterns;models recurrent neural;models recurrent;models missingness;missingness patterns diseases;recurrent neural networks;classification diagnoses;missingness;missing data;linear models recurrent;missingness linear;imputation;clinical time series;recurrent neural;classification diagnoses given;multilabel classification diagnoses;indicators missingness;binary indicators missingness;series observations training;imputation achieve;cope missing data;handled imputation;typically handled imputation;indicators missingness linear;handled imputation achieve;imputation achieve superior", "pdf_keywords": ""}, "4857e0e3d720b87b4523a6435cc166bcb7ae328a": {"ta_keywords": "neural machine translation;machine translation generation;translation generation;machine translation;2nd workshop neural;workshop neural machine;workshop neural;neural machine;neural;translation;proceedings 2nd workshop;2nd workshop;generation;workshop;machine;proceedings 2nd;2nd;proceedings", "pdf_keywords": ""}, "8ff620f704a4151fd7abba1db792463fbd32bfe5": {"ta_keywords": "trained abstractive summarizer;summarization low resource;document summarization low;long document summarization;summarization low;abstractive summarizer bart;abstractive summarizer;document summarization;summarizer bart achieves;summarizing long;summarizer bart;summarizer;summarizing long legal;setting summarizing long;document summary pairs;document summary;salient sentences tend;setting summarizing;identified salient sentences;salient sentences source;identifying salient sentences;documents identifying salient;available document summary;summarizing;summarization;salient sentences;resource setting summarizing;salience detection baselines;long document;pretrained language models", "pdf_keywords": "document summariza training;long document summariza;long document summarization;summariza training data;summariza training;pretrained abstractive summarizer;summarization task low;document summariza;document summarization task;document summarization;abstractive summarizer summary;summary source extractive;extractive summary compressed;identifying salient sentences;document summary pairs;source sentence saliency;salient sentences source;abstractive summarizer bart;salient sentences;abstractive summarizer;summarizer summary;summarizing long;documents identifying salient;score source sentences;resource long document;extractive summary;sentence saliency;document summary;summarizer;bart abstractive summarizer"}, "baf34ac4080a365a7cec30b6877fa1a018eb31cf": {"ta_keywords": "multi answer retrieval;answer retrieval combined;answer retrieval;answering improved retrieval;answer retrieval explored;answer generation models;answer generation;larger answer generation;downstream question answering;question answering improved;multi answer datasets;question answering;passage ranking diverse;retrieval combined;multiple distinct answers;distinct answers;answer datasets;improved retrieval;answering improved;retrieved passages models;answer coverage;diverse multi answer;retrieval combined downstream;retrieval explored;passage ranking;modeling retrieved passages;answering;retrieval;retrieval explored problem;better answer coverage", "pdf_keywords": "multi answer retrieval;answer retrieval;answer retrieval propose;passage retrieval model;joint passage retrieval;passage retrieval;retrieval recall downstream;retrieval recall;retrieval model;retrieval propose jpr;retrieval propose;retrieval model multi;retrieval;algorithms retrieval recall;selects sequence passages;jointly score passages;passages jointly score;decoder reranker;passage references modeling;decoder reranker autoregressively;decoding algorithms retrieval;encoder decoder reranker;retrieved passages;retrieval model integrates;selected passages jointly;score passages jpr;retrieved passages jpr;previously retrieved passages;recall downstream qa;generates passage references"}, "f4465442a9b850a2c5b71a63fff0d24396b15f2c": {"ta_keywords": "gradient descent ascent;ascent finite timescale;descent ascent finite;descent ascent;analysis gradient descent;gradient descent;ascent finite;convergence analysis gradient;ascent;finite timescale separation;local convergence analysis;analysis gradient;local convergence;descent;timescale separation;gradient;finite timescale;convergence analysis;convergence;timescale;local;analysis;separation;finite", "pdf_keywords": ""}, "7fb1262d4484732c8f7295fa5fb5e6ed6eabb6a0": {"ta_keywords": "energy market models;energy market simulations;zonal energy markets;energy markets;electricity markets;energy market;energy markets exclusion;performing energy market;based energy market;reserve electricity markets;modeling cooptimized energy;leads energy market;market simulation provided;formulation market simulation;market simulation;energy reserve electricity;market simulations;cooptimized energy reserve;modeling formulation market;energy ancillary services;market simulations optimized;locational marginal pricing;reserve electricity;markets detailed transmission;market models;cooptimized energy;market models widely;ancillary services markets;pricing lmp based;simulations optimized energy", "pdf_keywords": ""}, "9f208842f70503e8b71fd4c34ba682dcd0ea4788": {"ta_keywords": "incentives principal agent;adaptive incentive design;design incentives principal;priori adaptive incentive;adaptive incentive;simultaneously designing incentives;incentives principal;adaptively design incentives;designing incentives;incentive design;designing incentives change;principal agent problems;agents decision making;utility functions incentives;principal agent;agents utility functions;agent problems principal;design incentives;learn agents decision;incentives;incentives change response;incentives change;incentives offered;agents utility;functions incentives offered;functions incentives;particular principal objective;agents decision;incentives offered develop;decision makers agents", "pdf_keywords": "adaptive incentive design;adaptive incentive;method adaptive incentive;learning agents decision;learning step incentive;incentive design planner;learn agents decision;learning agents;algorithm learning agents;incentive mapping;incentive design;step incentive design;simultaneously designing incentives;agents decision making;incentive design step;designing incentives;designing incentives change;step incentive;agents utility functions;process updating incentives;utility learning;incentive mapping evaluated;incentives provide convergence;value incentive mapping;utility learning step;learn agents;agents utility;incentives change response;agents decision;updating incentives"}, "f136a0fdc2065485c83396ae41d431395de51af4": {"ta_keywords": "reviewers large conferences;recruiting reviewers population;recruiting reviewers;machine learning conference;ai conferences challenged;leading ai conferences;procedure recruiting reviewers;ai conferences;qualified reviewers growing;qualified reviewers;qualified reviewers large;pool qualified reviewers;conference peer review;scarcity qualified reviewers;population icml reviewers;reviewers population;small set reviewers;reviewers growing;identifies best submissions;reviewers;set reviewers;novice reviewer experiment;icml reviewers;reviewers population typically;reviewers large;conferences ii guiding;conference peer;large conferences;reviewers growing slower;learning conference recruit", "pdf_keywords": "reviewers large conferences;committee meta reviewers;enhancement reviewer pool;performance new reviewers;conference peer review;reviewer pool results;reviewer pool;reviewers evaluated;groups reviewers evaluated;meta reviewers;meta reviewers rate;quali\ufb01ed reviewers large;reviewers evaluated results;groups reviewers;novice reviewer experiment;traditional reviewer pool;reviewers invited;reviewers;invited reviewers;pool reviews evaluated;reviewers large;meta reviewers conclude;enhancement reviewer;icml reviewers experiment;reviewer experiment;reviewers rate;reviewers experiment;reviewers avoid;new reviewers;population icml reviewers"}, "b24e2c3983c3207b1c7124c48d691cf459a3197b": {"ta_keywords": "approximate bayesian inferences;bayesian inference methods;bayesian inferences based;bayesian machine learning;practical bayesian inference;processing approximate bayesian;bayesian inferences;bayesian inference;approximate bayesian;bayesian machine;use practical bayesian;practical bayesian;hidden markov models;apply bayesian machine;learn apply bayesian;markov models gaussian;topic models applications;language processing approximate;gaussian mixture models;latent topic models;markov models;topic models;speech recognition;including automatic speech;speech language processing;bayesian;models gram models;speech recognition speaker;gram models;gram models latent", "pdf_keywords": ""}, "0c0d9ecde0efead75e15353ac6c179c4fc22bdda": {"ta_keywords": "local nash equilibria;equilibria continuous games;nash equilibria characterization;nash equilibria continuous;characterization local nash;differential nash equilibria;nash equilibria isolated;equilibria characterization local;characterizing local nash;convex strategy spaces;constitute local nash;nash equilibria;local nash;continuous games;guaranteeing differential nash;equilibria characterization;continuous games provide;continuous games infinite;games infinite dimensional;strategy spaces;differential nash;non convex strategy;strategies constitute local;equilibria continuous;equilibria isolated equilibria;equilibria structurally stable;strategy spaces provide;isolated equilibria;equilibria structurally;equilibria isolated", "pdf_keywords": "equilibria continuous games;nash equilibria continuous;differential nash equilibria;local nash equilibria;nash equilibria isolated;games non convex;degenerate differential nash;nash equilibria structurally;guaranteeing differential nash;convex strategy spaces;computation differential nash;differential nash;nash equilibria;continuous games;games inverse modeling;continuous games non;strategy spaces non;non convex strategy;strategy spaces;equilibria isolated equilibria;games inverse;equilibria structurally stable;compute local nash;equilibria continuous;equilibrium behavior;isolated equilibria;competitive environments;local nash;equilibrium behavior property;behavior competitive environments"}, "e8b026b36d8be73ed428f7e4e55c26b27c34a544": {"ta_keywords": "bifunctional electrochemical sensor;electroactive analytes;simultaneous determination electroactive;electrochemical sensor;determination electroactive;non electroactive analytes;electroactive analytes universal;electrochemical sensor simultaneous;determination electroactive non;therapeutic drug monitoring;bifunctional electrochemical;electrochemical;drug monitoring;electroactive non electroactive;electroactive;sensor simultaneous determination;electroactive non;non electroactive;analytes;analytes universal effective;analytes universal;sensor;sensor simultaneous;simultaneous determination;therapeutic drug;serving therapeutic drug;drug;monitoring;platform serving therapeutic;determination", "pdf_keywords": ""}, "f20d7185c47ce55cdcd9b839ef6fce595baba029": {"ta_keywords": "conference speech computer;speech computer;conference speech;speech;international conference speech;speech computer volume;international conference;15th international conference;computer;conference;proceedings;proceedings 15th;computer volume 8113;proceedings 15th international;volume 8113;computer volume;volume;international;8113;15th international;15th", "pdf_keywords": ""}, "3ebed41fa35e5902b692a3e380c7c9a035c04426": {"ta_keywords": "ai ethics;ai ethics using;teaching ai ethics;modern ai research;enable ai researchers;implications modern ai;ai research far;ai researchers;ai researchers help;modern ai;ai research;impacts ai teaching;enable ai;ai teaching ai;teaching ai;ai teaching;ai;impacts ai;potential impacts ai;fiction advanced control;ethics using science;using science fiction;science fiction advanced;science fiction appropriate;tool enable ai;use science fiction;fiction advanced;fiction appropriate tool;machines modern military;ethics using", "pdf_keywords": ""}, "6b02fe6e0f6b2120a08e098513511e15a05f9073": {"ta_keywords": "detecting dataset shift;dataset shifts explore;interestingly dataset shifts;quantifying shift malignancy;dataset shift identifying;dataset shift hope;dataset shifts;dataset shift;helpful characterizing shifts;characterizing shifts qualitatively;shifts qualitatively determining;characterizing shifts;shift quantifying;shift malignancy;malignancy focus datasets;methods detecting dataset;quantifying shift;shift quantifying shift;shifts qualitatively;harmful interestingly dataset;detecting dataset;classifiers dimensionality reduction;typify shift quantifying;datasets;shifts explore sample;shift identifying;datasets various;classifiers;dataset;datasets various perturbations", "pdf_keywords": "quantifying shift malignancy;detecting dataset shift;dataset shift identifying;interestingly dataset shifts;dataset shifts explore;dataset shifts;helpful characterizing shifts;shift malignancy results;characterizing shifts;characterizing shifts qualitatively;shift quantifying;shift type malignancy;shift quantifying shift;shifts qualitatively determining;shift malignancy;quantifying shift;dataset shift;shifts qualitatively;shift identifying;characterizing distribution shift;shifts underlying;detection soft predictions;typify shift quantifying;comparable shift detection;shift detection;characterization shift;predictions domain discriminating;malignancy focus datasets;shift detection soft;characterization shift type"}, "3483d04a89dd69afd7b1393eadd8e8e4c5376d59": {"ta_keywords": "conceptual clustering;unsupervised conceptual clustering;conceptual clustering algorithm;clustering enhance classification;classification presented based;using hierarchical clustering;disambiguation;cobweb unsupervised conceptual;classification presented;classification;based cobweb unsupervised;unsupervised conceptual;corpus collection articles;clustering;proposed improved classification;classification accuracy;disambiguation built;hierarchical clustering;corpus collection;clustering algorithm using;improved classification;improved classification accuracy;enhance classification accuracy;classification accuracy tested;corpus;new approach classification;articles greek financial;approach classification presented;enhance classification;brown corpus collection", "pdf_keywords": ""}, "b1a8c6de4fbfe485c8f1c7723404467b72788ff2": {"ta_keywords": "deep learning healthcare;machine learning healthcare;learning healthcare assumptions;rnns multivariate clinical;learning healthcare conditions;breakthroughs deep learning;learning healthcare;medical decisionmaking requires;medical decisionmaking;machine learnings impressive;strict machine learning;standard machine learning;pioneering rnns multivariate;deep learning;assumptions medical decisionmaking;rnns;pioneering rnns;healthcare assumptions;learning healthcare group;healthcare assumptions medical;work pioneering rnns;rnns multivariate;healthcare;healthcare conditions shift;machine learnings;healthcare conditions;clinical time series;breakthroughs deep;recent breakthroughs deep;setup machine learnings", "pdf_keywords": ""}, "b6b76f529d273a35180d0dc65912db1538539067": {"ta_keywords": "document categorization;document classification concerned;document categorization aims;supervised document classification;document classification;text metadata semantic;embed text metadata;text metadata;conventional supervised document;topic label document;supervised document;signals document categorization;metadata semantic;metadata serve compelling;categorization needs performed;topic label;metadata semantic space;leveraged categorization framework;tags guided generative;categorization needs;categorization framework label;categorization framework;categorization;metadata;leveraged categorization;annotated data;annotated data based;topic indicators leveraged;compelling topic indicators;labeled training samples", "pdf_keywords": "text metadata semantic;embed text metadata;metadata semantic;categorization text metadata;framework metacat embedding;metacat embedding;metadata semantic space;text metadata;metacat embedding module;metadata;embedding module meta;text metadata yu;baselines minimally supervised;novel framework metacat;supervised categorization text;minimally supervised;semantic;semantic space encode;embedding methods significant;metadata yu;effectiveness metacat;model embed text;metacat;minimally supervised categorization;embedding module generation;supervised;generative model embed;metacat competitive baselines;hin embedding methods;document categorization"}, "ac713aebdcc06f15f8ea61e1140bb360341fdf27": {"ta_keywords": "model extraction bert;language processing adversary;attacker extract model;extraction bert based;extraction bert;processing adversary query;bert based apis;language model bert;adversary query;extraction natural language;learning methods nlp;nlp tasks;answering 2019 adversary;nlp tasks including;model bert;model extraction diverse;natural language inference;model extraction natural;attacker extract;adversary query access;queries model extraction;pretrained language model;apis fact attacker;question answering;methods nlp;question answering 2019;model extraction;problem model extraction;adversary victim model;processing adversary", "pdf_keywords": "language model attacker;model extraction bert;model extracted adversary;extraction bert based;extraction bert;language processing adversary;model attacker;bert model extracted;model extraction natural;extraction attacker interesting;bert based apis;model attacker architecture;extraction attacker;pretrained bert model;model extraction;bert model;processing adversary query;extracted adversary;problem model extraction;bert pretrained language;process extraction attacker;extraction natural language;pretrained language models;language models;extracted adversary does;model extraction paper;victim model attempts;model attempts;non bert pretrained;processing adversary"}, "3671dabbfd2e854060e1e382bad96b6bb00fcb46": {"ta_keywords": "noise dictionary adaptation;noise robust speech;speech recognition exemplar;supervised noise dictionary;robust speech recognition;noise robust asr;robust speech;noise semi supervised;speech exemplars noise;noise dictionary;semi supervised noise;enhancing speech features;exemplar based noise;speech exemplars fixed;keeping speech exemplars;noise exemplars;supervised noise;speech exemplars;noise robust;based noise robust;exemplars noise exemplars;speech features;dictionary adaptation exemplar;noise exemplars target;performance noise robust;adapt noise exemplars;noise exemplars built;speech recognition;noise keeping speech;exemplars target noise", "pdf_keywords": ""}, "6bea71fa6deb19c67e9586428f8f240e789fb3df": {"ta_keywords": "linear stochastic bandit;linear stochastic bandits;stochastic bandit;stochastic bandit problem;stochastic bandits;bandit problem linear;stochastic bandits modification;bandit problem studied;bandit problem;bandit problem particular;armed bandit problem;multi armed bandit;regret bound logarithmic;improves regret bound;bandits modification improves;bandits modification;regret improved algorithms;bandit;armed bandit;probability constant regret;bandits;algorithms linear stochastic;stochastic multi armed;regret bound;constant regret improved;algorithm linear stochastic;algorithms stochastic;auer ucb algorithm;algorithms stochastic multi;performance algorithms stochastic", "pdf_keywords": "linear stochastic bandit;stochastic bandit problems;stochastic bandit problem;stochastic bandit;various stochastic bandit;bandit problem studied;bandit problems;bandit problems particular;bandit problem;bandit;probability constant regret;martingales allows improve;vector valued martingales;valued martingales allows;valued martingales;novel tail inequality;martingales allows;algorithms various stochastic;martingales;algorithm linear stochastic;linear stochastic;tail inequality vector;stochastic;auer ucb algorithm;ucb algorithm auer;constant regret;various stochastic;empirical performance algorithms;tail inequality;achieves high probability"}, "2f0221142db900e75bd9c54fa153fb770a72f672": {"ta_keywords": "synset assembly;synset assembly interface;describes synset assembly;russian using crowdsourcing;describes synset;synset;crowdsourcing;paper describes synset;yarn russnet project;assembly interface developed;using crowdsourcing;assembly interface;thesaurus russian using;assembly;open thesaurus russian;thesaurus russian;russnet project;results yarn russnet;russnet project started;yarn russnet;thesaurus;large open thesaurus;interface developed project;open thesaurus;results yarn;yarn;interface developed;experimental results yarn;russian using;interface", "pdf_keywords": ""}, "7dce2877758b0103d1f7a454c184dc641e123359": {"ta_keywords": "querying annotation graphs;indexing querying annotation;querying annotation;queries included linguistically;document retrieval;document retrieval task;evaluated document retrieval;unstructured queries;unstructured queries di;outperformed unstructured queries;question answering;factoid question answering;query language;annotation compression;query language implementing;implementing annotation compression;sentences possible retrieve;retrieval task purpose;querying;corpus;extending query language;retrieval;indexing querying;question answering qa;nlp pipelines;annotation compression extending;generated nlp;annotation graphs;corpus sentences;answering qa queries", "pdf_keywords": ""}, "309b2c75dcdafea19a053876e56cef9747d428fb": {"ta_keywords": "attentional models lattice;lattice models training;neural lattice models;attention sequence modeling;model speech translation;neural lattice;previous neural lattice;attentional models;lattice models;models lattice inputs;models handle lattices;self attentional models;model lattice inputs;speech translation task;attention sequence;incorporate lattice;translation task outperforms;lattice inputs;recurrent neural networks;lattice inputs achieved;lattices efficient;incorporate lattice structure;inference self attentional;self attention sequence;lattice inputs extend;lattice structure model;model lattice;handle lattices;models lattice;lattice scores", "pdf_keywords": "lattice positional embeddings;sequential encoders latticelstm;lattice self attentional;positional embeddings lattice;lattice positional encodings;encoders latticelstm encoder;lattice models training;neural lattice models;latticelstm encoder;latticelstm encoder recently;encoders latticelstm;use lattice positional;lattice positional;embeddings lattice structures;embeddings lattice;attentional sequential encoders;previous neural lattice;speech translation benchmarks;model speech translation;attentional encoder decoder;neural lattice;attentional encoder propose;attentional encoder;encoder component attentional;lattice models;component attentional encoder;attentional sequential;positioning ordering lattice;adapting positional embeddings;positional embeddings model"}, "975551547fef77605fb85a551bbd7523b77746b7": {"ta_keywords": "enrichment topic modeling;repository classification github;repository classification;classification github;classification github important;keyword enrichment topic;embedding keyword enrichment;data repository classification;keyword enrichment;topic modeling;topic based search;topic label functionality;github repositories labels;propose higitclass framework;topic label;topic modeling pseudo;unstructured data repository;github repository collections;higitclass framework comprising;repositories labels impeding;challenges propose higitclass;repositories labels;enrichment topic;higitclass framework;github repositories;massive number repositories;topic based analysis;search topic based;majority github repositories;propose higitclass", "pdf_keywords": "enrichment topic modeling;keyword enrichment topic;embedding keyword enrichment;keyword enrichment;repositories machine learning;repository classi\ufb01cation recognition;propose higitclass framework;topic modeling;higitclass framework comprising;challenges propose higitclass;higitclass framework;topic modeling pseudo;higitclass;propose higitclass;datasets github repositories;higitclass framework consists;con\ufb01rm higitclass;unstructured data repository;design higitclass framework;collect datasets github;datasets github;enrichment topic;higitclass superior existing;design higitclass;existing weakly supervised;learning bioinformatics research;higitclass superior;embedding keyword;github repository collections;document generation"}, "46ed42e4318e1363a0ec3dde195422cdfecf2017": {"ta_keywords": "phrase bert embeddings;phrase embeddings bert;paraphrase generation model;bert embeddings easily;phrase embeddings;embeddings bert;improved phrase embeddings;bert embeddings;embeddings bert application;phrase representations;powerful phrase embeddings;paraphrases automatically generated;paraphrase generation;diverse phrasal paraphrases;bert application corpus;relatedness phrase bert;phrasal paraphrases automatically;paraphrases automatically;study phrase bert;using paraphrase generation;bert improved phrase;phrase bert relies;neural topic model;phrase representations derived;approach phrase bert;phrase based neural;phrase bert improved;phrasal paraphrases;phrase bert;generated using paraphrase", "pdf_keywords": "phrase embeddings bert;bert phrase embeddings;diverse phrasal paraphrases;improved phrase embeddings;phrase embeddings;powerful phrase embeddings;phrase level paraphrases;diverse phrasal paraphrase;paraphrases phrases incontext;phrase representations;diverse phrase level;lexically diverse phrasal;phrasal paraphrases automatically;embeddings bert;phrase embeddings approach;phrase bert induces;phrasal paraphrase pairs;paraphrase generation model;bert application corpus;paraphrases automatically generated;phrase bert consistently;level paraphrases phrases;paraphrase pairs additionally;phrasal paraphrases;paraphrase generation;bert improved phrase;phrasal semantic relatedness;embeddings bert application;develop phrase bert;abstract phrase representations"}, "d4af2654f97c09741aba9f0da9ace7bc84b9a63f": {"ta_keywords": "time bayesian network;time bayesian networks;bayesian network ectbn;bayesian network;bayesian networks;continuous time bayesian;time bayesian;bayesian networks application;modeling progression poverty;model paths poverty;novel event driven;poverty clients citylink;structure learning;event driven;structure learning based;paths poverty clients;causal dynamics evolution;event driven continuous;procedure structure learning;event occurrences interventions;influence event occurrences;bayesian;networks;causal dynamics;ectbn representation model;poverty clients;influenced occurrences events;progression poverty;learning based bic;progression poverty integrated", "pdf_keywords": ""}, "73a6e4574de038878be1bbb5985400998e420a5b": {"ta_keywords": "strategyproofing peer assessment;form peer assessment;strategyproofing peer;peer assessment;peer assessment peer;review peer assessment;peer assessment employees;strategyproofness compromise assignment;assessment peer;assessment peer grading;peer grading assignments;price strategyproofing peer;peer grading;compromise assignment quality;assignment evaluators submissions;provide dishonest evaluations;individuals assigning evaluate;peer review peer;evaluating provide dishonest;evaluators submissions maximizes;dishonest evaluations;assignment quality;review peer;competition submissions evaluating;dishonest evaluations increase;peer review;form peer;strategyproofness compromise;submissions evaluating;assignment evaluators", "pdf_keywords": ""}, "78438b61afc2c9123c28ca4d6b58e598462ae9be": {"ta_keywords": "source domain adaptation;domain adaptation clustering;domain adaptation;domain specific adversarial;adversarial task specific;adversarial training task;optimization adversarial task;clustering embedded adversarial;adversarial task;adversarial training analysis;specific adversarial training;domain adaptation utilizing;adversarial training recently;adversarial training process;domain adaptation method;adversarial training;embedded adversarial training;adaptation clustering embedded;min optimization adversarial;target domain diverse;specific adversarial;optimization adversarial;domain diverse multi;embedded adversarial;disturb adversarial training;adaptation clustering;refines discriminative target;adversarial;discriminative target representations;domain diverse", "pdf_keywords": ""}, "1f0446dddd192e94f3930a3a449bd89796f4200f": {"ta_keywords": "transforms acoustic features;transformation matrices feature;dnn adaptation decoding;transformation matrix feature;feature transformation matrix;adaptation decoding;feature space adaptation;multiple transformation matrices;dnn adaptation;transformation matrices gmm;transforms acoustic;acoustic features adapted;applied dnn adaptation;transformation matrices based;transformation matrices;single transformation matrix;adaptation applied dnn;feature transformation;adaptation decoding share;estimates multiple transformation;single feature transformation;space adaptation weighted;transformation matrix;transformation matrix frame;adaptation applied deep;speech recognition;matrices based regression;regression fmllr transforms;multiple transformation;matrices feature space", "pdf_keywords": ""}, "2226560f94c1e90d6900d4674b649cc5522b78cc": {"ta_keywords": "translation models multilingual;attentional translation models;self attentional translation;multilingual self attentional;sharing single translation;models multilingual neural;single translation model;multilingual neural machine;translation models;attentional translation;bilingually trained models;multilingual neural;translation model;neural machine translation;single translation;models multilingual;multilingual self;improvements translation accuracy;translation model multiple;methods multilingual self;translation accuracy;sharing methods multilingual;machine translation;translation accuracy work;multilingual;performance gains bilingually;substantial improvements translation;bilingually trained;gains bilingually trained;translation shown sharing", "pdf_keywords": "multilingual machine translation;multilingual neural machine;attentional translation models;sharing single translation;multilingual neural;multilingual self attentional;neural machine translation;self attentional translation;single translation model;bilingually trained models;abstract multilingual neural;multilingual machine;translation models;task multilingual machine;attentional translation;sharing methods multilingual;task multilingual;multilingual;multilingual_nmt shared encoder;translation model;machine translation;multilingual self;translation model multiple;single translation;translation models devendra;methods multilingual self;language decoder shared;translation using self;machine translation using;multilingual_nmt shared"}, "04b91791225a4f86b0715b41c6f56c00c197d810": {"ta_keywords": "optimal convertible codes;conversion bandwidth optimal;bandwidth optimal convertible;constructions conversion bandwidth;convertible codes derived;bound conversion bandwidth;convertible codes derive;mds convertible codes;limits conversion bandwidth;resource optimize conversions;conversion bandwidth present;conversion bandwidth;code conversion;convertible codes focused;codes derived bound;convertible codes;nodes accessed conversion;optimize conversions present;conversion bandwidth significantly;optimize conversions;conversion bandwidth bandwidth;access cost conversion;variable capacity edges;bandwidth optimal existing;accessed conversion;codes derive lower;codes derived;lower bound conversion;subclass convertible codes;code conversion problem", "pdf_keywords": "convertible codes bandwidth;codes bandwidth optimal;optimal convertible codes;bandwidth optimal convertible;accessoptimal convertible codes;bandwidth optimal conversion;mds convertible codes;codes bandwidth;reducing conversion bandwidth;convertible codes achieve;conversion bandwidth;conversion bandwidth default;bandwidth cost conversion;savings conversion bandwidth;bound bandwidth optimal;conversion bandwidth ri;convertible codes;lower bounds bandwidth;access optimal convertible;bandwidth optimal;lower bound bandwidth;bandwidth optimal ri;mds codes;bounds bandwidth cost;code conversion;accessoptimal convertible;optimal conversion;bounds bandwidth;variable capacity edges;convertible codes matches"}, "43fe2d8781473360eeaae7a3284169a303200846": {"ta_keywords": "news stance classification;news stance detection;detection fake news;stance classification task;fake news challenge;stance classification;fake news stance;news challenge;news stance;news challenge models;stance detection using;stance detection;board fake news;stacked ensemble classifiers;fake news hotly;news hotly debated;stacked ensemble;using stacked ensemble;2017 fake news;ensemble classifiers;classifiers used stacking;news hotly;classifiers;topic journalism;classification accuracy;challenge models detection;stacking ensemble method;debated topic journalism;stance;ensemble classifiers used", "pdf_keywords": ""}, "d9b89de5c2a39479768c6e32f13ac3e816635cc1": {"ta_keywords": "word lattices transcription;translation model trained;learning translation model;outperforming translation model;learning lexical translation;translation models;computer aided translation;lexical translation parameters;learning translation;model word lattices;aided translation;translation models used;outperforming translation;translation model;directly word lattices;translation model word;additionally outperforming translation;word lattices consistent;translation parameters directly;aided translation propose;word lattices;model learning lexical;lattices transcription;translation parameters;lattices transcription sought;lattices consistent word;lexical translation;improve automatic speech;automatic speech recognition;speech recognition", "pdf_keywords": ""}, "44e24aabd05bef8cb45646486f1a24b7caecee45": {"ta_keywords": "language model rnnlm;multilingual seq2seq model;model rnnlm seq2seq;monolingual models babel;multilingual sequence sequence;rnnlm seq2seq model;multilingual seq2seq;transfer learning language;multilingual model;multilingual sequence;build multilingual seq2seq;learning language modeling;models babel languages;model rnnlm;sequence speech recognition;approach incorporating rnnlm;alignments multilingual sequence;language modeling;seq2seq model decoding;monolingual models;incorporating rnnlm;neural network language;languages using transfer;rnnlm seq2seq;recurrent neural network;language model;approach multilingual model;10 babel languages;multilingual model shows;babel languages", "pdf_keywords": "language model rnnlm;multilingual seq2seq model;transferring multilingual model;multilingual training approaches;multilingual model;multilingual model target;language model boosting;model rnnlm seq2seq;multilingual seq2seq;multilingual training;character based rnnlm;rnnlm seq2seq model;monolingual models babel;prior multilingual seq2seq;model rnnlm;transferring multilingual;monolingual models;build multilingual seq2seq;approach multilingual model;explore multilingual training;improving prior multilingual;learning approach multilingual;language model;neural network language;hybrid dnn rnn;seq2seq model decoding;multilingual model shows;incorporate existing multilingual;existing multilingual;rnnlm seq2seq"}, "8b468872cf915c98ff46a2bea4d2a34112b7b0b0": {"ta_keywords": "relational classification retrieval;rules learning relational;learning relational features;relational classification;longer relational rules;relational features backward;relational rules;relational rules including;relational features;learning relational;path ranking algorithm;relational rules class;rules learning;algorithms searching features;ranking algorithm pra;walks efficiently discover;path ranking;scalability path ranking;address relational classification;space relational rules;searching features;faster algorithms searching;longer relational;ranking algorithm;types rules learning;including longer relational;classification retrieval;algorithms searching;larger space relational;classification retrieval tasks", "pdf_keywords": ""}, "d8704a63517868475b3af7ec25eaa2fb2a44362b": {"ta_keywords": "loss cnn segmentation;crf loss cnn;cnn loss minimization;cnn loss;loss cnn;dominate cnn loss;cnn segmentation variants;grid crf loss;cnn segmentation;grid crf losses;loss minimization;segmentation variants gradient;loss adm;powerful loss functions;loss adm splitting;crf loss;gradient descent;loss functions practically;crf losses;vision powerful loss;optimization grid crf;loss minimization computer;gradient descent gd;loss functions;gd dominate cnn;improves optimization grid;cnn;segmentation variants;variants gradient descent;crf losses yielding", "pdf_keywords": "losses deep segmentation;regularized segmentation losses;loss minimization deep;shallow segmentation loss;losses deep learning;minimization deep cnn;segmentation loss minimization;regularized losses deep;descent regularized segmentation;deep cnn segmentation;minimization deep;segmentation losses;segmentation loss;deep cnn;deep segmentation;models losses deep;weakly supervised cnn;deep segmentation compare;training deeper;segmentation losses dmitrii;supervised cnn segmentation;training deeper complex;regularized segmentation;cnn segmentation demonstrate;gradient descent regularized;regularization models losses;cnn segmentation;cnn segmentation work;losses deep;gd regularized losses"}, "6dd6d4dfc3cf9ff41aad7e903cf1294de2ac5629": {"ta_keywords": "assessment automated vehicles;vehicle safety assessments;validation automated driving;automated vehicle safety;safety assessment automated;automated driving systems;inform automated driving;driving systems safety;safety assessment defining;automated driving;automated driving developers;vehicle safety;safety assessment;safety assessments;safety assessments applicable;vehicles verification validation;develop automated vehicle;automated vehicles verification;based safety assessment;automated vehicles;parameterize scenarios extracted;introduction automated vehicles;automated vehicles market;scenario based safety;automated vehicle;driving systems;highways sakura initiative;systems safety assessment;japanese highways;deceleration scenarios extracted", "pdf_keywords": ""}, "1288d6570085a28518a9f3495e77dbb75899421c": {"ta_keywords": "active annotation;applied active annotation;active annotation named;annotated data;introduces semi supervised;active annotation main;semi supervised learning;entity recognition biomedical;annotation named entity;active learning framework;active learning;seed annotated data;semi supervised;annotated data needed;named entity recognition;annotation;entity recognition;material active annotation;human annotator;annotate;annotate imperfectly data;annotator applied active;human annotator applied;initially annotate;annotation named;supervised learning framework;annotate imperfectly;corrected human annotator;annotator;popular active learning", "pdf_keywords": ""}, "163a67b5b0371035fa6e0f88b36ba97a32e735bc": {"ta_keywords": "convertible codes optimal;achieve code conversions;convertible codes broad;code conversions;allow code conversions;encoded code maintaining;code conversion;mds decodability constraint;maintaining desired decodability;mds convertible codes;code conversions significantly;code conversions resource;linearity mds decodability;code data encoded;introduce convertible codes;codes broad range;code conversion process;codes optimal parameter;codes optimal;decodability properties maximum;encoded code data;code conversion particular;data encoded code;desired decodability;desired decodability properties;convertible codes;mds decodability;decodability constraint;accessed code conversion;constructions optimal mds", "pdf_keywords": "achieve code conversions;code data encoded;code conversions;allow code conversions;code conversion;convertible codes optimal;convertible codes broad;introduce convertible codes;code conversions resource;code conversion process;data encoded nf;notion code conversion;code conversions signi\ufb01cantly;mds convertible codes;converting data encoded;concept code conversion;approach encoding;maintaining desired decodability;convertible codes;code data;data encoded;encoded nf;formalize notion code;codes broad range;desired decodability properties;codes optimal parameter;desired decodability;codes optimal;decodability properties maximum;formalizes concept code"}, "027c5e44164a2ee3543ecdff73cd4d7888a42a90": {"ta_keywords": "learning value alignment;systems preferences ai;ai systems preferences;preferences ai;value alignment preference;flexible value alignment;orderings actions ai;decision making machines;preferences ethical systems;metric learning value;value alignment;actions ai systems;actions ai;ethical systems formalism;ai;alignment preference central;ai systems;preferences ai expect;ethical systems;metric distance preferences;learning value;metric learning;value alignment paper;user preferences ethical;consistent preferences decisions;preferences decisions adhere;recommendations consistent preferences;decisions recommendations consistent;consistent preferences;alignment preference", "pdf_keywords": ""}, "c17ccb7f0372ec98b7e070b0f70518f28516ecd5": {"ta_keywords": "markov processes book;theory markov processes;processes book lectures;chapters stochastic processes;markov processes;correlation theory markov;stochastic processes theory;theory markov;markov decision processes;markov chain monte;equilibrium concept markov;processes book;stochastic processes;carlo markov decision;concept markov chain;carlo markov;standard chapters stochastic;processes theory correlation;concept markov;processes theory;monte carlo markov;chapters stochastic;markov decision;markov chain;markov;khinchin ergodic theorem;stochastic;birkhoff khinchin ergodic;ergodic theorem macrosystem;theory correlation theory", "pdf_keywords": ""}, "23e03cd57b5d75993545127f3fecf99d25021583": {"ta_keywords": "phenotypes tumor embeddings;representation tumor embedding;predicting multiple cancer;representation phenotype prediction;cancer phenotypes based;tumor status representation;profiled cancer genome;tumor embeddings;cancer genome;genes degs tumors;tumor embedding;cancer genome atlas;cancer phenotypes;tumor embeddings shown;response cancer cell;tumor model instantiate;cancer cell;tumor model;multiple cancer phenotypes;phenotypes tumor;tumors profiled cancer;distinguishes cancer drivers;sgas tumor model;profiled cancer;cancer drivers;deep neural;phenotype prediction including;model 468 tumors;multiple phenotypes tumor;drug response cancer", "pdf_keywords": "tumor embedding representations;gene embedding;gene embeddings;gene embeddings precisely;gene embedding tumor;powerful gene embedding;embedding tumor embedding;representation phenotype prediction;project tumor embeddings;cancer genome atlas;tumor embeddings;regulatory networks cancer;embedding representations highly;tumor embedding;genes degs tumors;cancer genome;networks cancer propose;tumor embeddings shown;embedding representations;representation gene regulatory;gene regulatory networks;tumor status representation;representation gene;networks cancer;network model encoder;knowledge representation gene;pro\ufb01led cancer genome;expressed genes degs;embedding tumor;response cancer cell"}, "1d3539a8d94bd3ab78993d7cc584efc06ed0e460": {"ta_keywords": "benchmarking feature attribution;benchmarking popular explainability;feature attribution algorithms;explainable machine learning;feature attribution methods;feature attribution;explaining model predictions;popular explainability techniques;model explainability;popular explainability;attribution algorithms;model predictions increasingly;attribution algorithms machine;different feature attribution;research explainable machine;explainability techniques;research model explainability;tools explaining model;explainability techniques evaluation;benchmarking feature;model explainability given;model predictions;library benchmarking feature;attribution methods;attribution methods lime;synthetic benchmarks;machine learning models;attribution;synthetic benchmarks scientific;attribution methods remains", "pdf_keywords": "benchmarking feature attribution;benchmarking popular explainability;benchmarking popular explainers;benchmarking feature;library benchmarking feature;empirical evaluation metrics;feature attribution algorithms;popular explainability techniques;metrics data intensive;datasets library benchmarking;library benchmarking popular;feature attribution methods;library benchmarking;explainability techniques evaluation;benchmarking;evaluation metrics data;popular explainability;feature attribution;challenging evaluations ideally;explainability techniques;datasets synthetic datasets;shapley values metrics;synthetic datasets library;real world datasets;prohibitive realworld datasets;attribution algorithms showcase;benchmarking popular;synthetic datasets;suite synthetic datasets;realworld datasets synthetic"}, "8a14b3a9e642f4ca7fad4df997fc1941bdcfb935": {"ta_keywords": "statistical models speech;models speech language;speech language processing;models speech;statistical models;speech language;language processing;statistical;speech;models;language;processing", "pdf_keywords": ""}, "ffecb8b8b415149f5351b64a2dbb1a1fa64219f0": {"ta_keywords": "end speech translation;speech translation;speech translation st;generalization multilingual training;machine translation mt;translation st speech;multilingual end;multilingual training;multilingual end end;machine translation;utterances source languages;asr machine translation;multilingual models;data multilingual models;languages directly translated;multilingual training evaluated;useful automatic speech;available data multilingual;automatic speech;generalization multilingual;desired target languages;effectiveness multilingual end;multilingual;multilingual models shown;framework multilingual end;data multilingual;target languages;problem generalization multilingual;automatic speech recognition;target languages universal", "pdf_keywords": ""}, "9c78481004b7dbb601b83cc081ec23c02e6f5270": {"ta_keywords": "stable equilibria learning;linearized game dynamics;equilibria learning dynamics;learning dynamics continuous;dynamics continuous games;game stability gradient;equilibria learning;learning dynamics necessarily;learning dynamics;continuous games scalar;stable nash robust;equilibria stable nash;corresponding game stability;scalar games equilibria;linearized game;gradients player continuous;continuous games;games equilibria stable;stable nash;gradient learning dynamics;player continuous games;differential nash equilibria;game dynamics;stability gradient learning;game stability;equilibria corresponding game;nash robust;nash equilibria corresponding;nash robust variations;games scalar action", "pdf_keywords": "stable equilibria learning;stability nash optimality;equilibria learning dynamics;local stability nash;equilibria stable nash;stability nash;equilibria learning;differential nash equilibria;stable nash robust;learning dynamics;stable nash;learning dynamics continuous;nash equilibria corresponding;learning dynamics necessarily;gradient learning dynamics;dynamics continuous games;equilibria continuous games;continuous games scalar;nash optimality;stability gradient learning;differential nash;nash robust;nash equilibria;nash robust variations;nash optimality \ufb01xed;set differential nash;equilibria corresponding game;games \ufb01nd equilibria;continuous games;stable equilibria"}, "8b231737e0048a400527d89aa56c712e8b9bc690": {"ta_keywords": "speech translation generalization;end speech translation;speech translation;generalization multilingual training;speech translation st;translation generalization multilingual;sequence architecture multilingual;translation st speech;machine translation mt;multilingual training;utterances source languages;machine translation;multilingual end;multilingual models;asr machine translation;multilingual end end;useful automatic speech;data multilingual end;automatic speech;multilingual training evaluated;languages directly translated;available data multilingual;architecture multilingual models;translation generalization;multilingual;multilingual models shown;effectiveness multilingual end;generalization multilingual;desired target languages;automatic speech recognition", "pdf_keywords": "generalization multilingual e2e;multilingual end;end speech translation;multilingual end end;generalization multilingual training;con\ufb01rm multilingual end;multilingual e2e;perform multilingual e2e;utterances source languages;effectiveness multilingual end;multilingual e2e st;outperform bilingual ones;speech translation st;22 generalization multilingual;desired target languages;generalization multilingual;speech translation;outperform bilingual;languages directly translated;perform multilingual;framework multilingual end;evaluate generalization multilingual;multilingual training;2center language speech;multilingual;translation st speech;languages universal sequence;target languages;multilingual training evaluated;target languages universal"}, "da20ab7724335eb48bcd0e9be30f0ac4b6a464c6": {"ta_keywords": "novelty detection network;novelty detection;detection novel scenarios;detection network saliency;network saliency visual;novel scenarios vision;detect situations trained;saliency visual;detection network;network saliency;scenarios vision;visual based deep;detection novel;saliency visual based;safety critical autonomous;learning driven safety;scenarios vision based;trained prediction;trustworthy prediction;saliency;able detect situations;detection;situations trained model;detect situations;trained model;make trustworthy prediction;learned trained prediction;deep learning;new image similarity;deep learning machine", "pdf_keywords": "novel scenarios vision;detection novel scenarios;real world driving;scenarios vision;scenarios vision based;visual saliency preprocessing;autonomous driving;visual saliency;autonomous driving sys;navigation self driving;saliency preprocessing;detect novel scenarios;method visual saliency;self driving;approach driving datasets;driving datasets;driving datasets real;visual navigation self;approach employs autoencoder;vision based autonomous;trained prediction;approach driving;visual navigation;world driving;saliency preprocessing novel;saliency;based autonomous driving;autonomous systems leveraging;learned trained prediction;autoencoder based"}, "e54e0d9eaa922cefb1c69e105979399fd34497b1": {"ta_keywords": "fairness aware learning;learning fair classifiers;achieve group fairness;group fairness;fair classifiers;accuracy fairness improved;datasets accuracy fairness;fair classifiers partially;applicable fairness aware;fairness aware;readily applicable fairness;algorithmic group fair;group fairness perform;target labels fair;accuracy fairness;recently fairness aware;fair pg learning;labels fair;labels fair pg;fairness improved;pg learning fair;group fair;applicable fairness;learning fair;group fair ness;roup labels fair;fairness perform;fairness;annotated group labels;demographic group labels", "pdf_keywords": "learning fair classifiers;fairness aware learning;fair classifiers;fair classifiers partially;accuracies fairness metrics;fairness metrics;fairness metrics jointly;fairness aware;recently fairness aware;state art fairness;accuracies fairness;fairness aware inprocessing;target accuracies fairness;abstract recently fairness;learning fair;art fairness aware;fairness;empirically benchmark datasets;recently fairness;celeba experiments scalability;fully annotated demographic;benchmark datasets;classifiers partially;art fairness;classifiers;annotated demographic group;classifiers partially annotated;demographic group labels;annotated demographic;empirically benchmark"}, "dcb28c8ba94434eb8a06e81eb55bfdbc343d2340": {"ta_keywords": "sentence relation extraction;relation extraction;relation extraction generally;ary relation extraction;relation extraction relations;relation extraction methods;extraction relations involve;extraction relations;cross sentence relation;spans document subrelation;sentence relation;neural architecture document;document subrelation hierarchy;document subrelation;involve entity mentions;relations involve entity;far apart document;labels distant supervision;spans consecutive sentences;document multiscale modeling;machine reading;biomedical machine reading;relations involve;apart document;entity mentions far;small text spans;various text spans;multiscale neural architecture;text spans;machine reading approach", "pdf_keywords": "relation extraction multiscale;sentence relation extraction;relation extraction;relation extraction generally;relation extraction relations;neural architecture documentlevel;relation extraction methods;extraction relations involve;extraction relations;ary relation extraction;cross sentence relation;information extraction;documentlevel ary relation;sentence relation;extraction multiscale representation;involve entity mentions;entity mentions far;labels distant supervision;information extraction methods;abstract information extraction;spans document subrelation;spans consecutive sentences;document multiscale modeling;binary relations;relations involve entity;single sentences paper;focus binary relations;multiscale neural architecture;various text spans;binary relations expressed"}, "900b785dbbea7ccd5846eafb14c6715f76fe5e00": {"ta_keywords": "malware using deep;infer devices compromised;devices compromised;learning identify security;deep learning identify;devices compromised extension;malware;devices enterprise networks;identify security;uses deep learning;detection unseen threats;malware using;identify security risks;deep learning;analyzes times malware;using deep learning;families malware;guest wireless networks;infer devices;dynamic anomaly detection;times malware;times malware using;anomaly detection;multiple families malware;deep learning model;demonstrate deep learning;guest wireless;security;security risks;security challenge", "pdf_keywords": ""}, "2a81081c987da2bb8184b8e9a884cf6a73712ee8": {"ta_keywords": "\u968e\u5c64\u7684\u30d5\u30ec\u30fc\u30ba\u30d9\u30fc\u30b9\u7ffb\u8a33\u306b\u304a\u3051\u308b\u30d4\u30dc\u30c3\u30c8\u7ffb\u8a33\u624b\u6cd5\u306e\u5fdc\u7528 \u5fdc\u7528\u51e6\u7406 \u7b2c6\u56de\u96c6\u5408\u77e5\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0;\u968e\u5c64\u7684\u30d5\u30ec\u30fc\u30ba\u30d9\u30fc\u30b9\u7ffb\u8a33\u306b\u304a\u3051\u308b\u30d4\u30dc\u30c3\u30c8\u7ffb\u8a33\u624b\u6cd5\u306e\u5fdc\u7528 \u5fdc\u7528\u51e6\u7406;\u968e\u5c64\u7684\u30d5\u30ec\u30fc\u30ba\u30d9\u30fc\u30b9\u7ffb\u8a33\u306b\u304a\u3051\u308b\u30d4\u30dc\u30c3\u30c8\u7ffb\u8a33\u624b\u6cd5\u306e\u5fdc\u7528;\u5fdc\u7528\u51e6\u7406 \u7b2c6\u56de\u96c6\u5408\u77e5\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0;\u7b2c6\u56de\u96c6\u5408\u77e5\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0;\u5fdc\u7528\u51e6\u7406", "pdf_keywords": ""}, "126be977c03d732fbef2381565a41b957d41a2cc": {"ta_keywords": "language understanding deep;nlp deep learning;nlp deep;learning architectures nlp;advances nlp deep;architectures nlp tasks;architectures nlp;nlp tasks;nlp tasks require;understanding deep;high level linguistic;models understand language;understanding deep learning;advances nlp;discourse level language;deep learning mohit;recent advances nlp;narrative modeling;level language understanding;high level discourse;processing nlp thesis;building representations language;processing nlp;context question answering;deep learning architectures;contextual language;sentence level computers;nlp thesis;deep learning;contextual representations", "pdf_keywords": ""}, "aae8d332c3ff9d081ae36967d3d7b5f394b51bcc": {"ta_keywords": "predictions self training;teacherstudent stackelberg game;supervised learning tasks;supervised;learning tasks;weakly supervised learning;semi supervised;supervised weakly;prediction performance teacher;weakly supervised;differentiable self training;semi supervised weakly;pseudo labels game;makes predictions self;semiand weakly supervised;weakly supervised classification;labels game leader;supervised weakly supervised;entity recognition tasks;supervised learning;student makes predictions;classification named entity;various semi supervised;margins self training;pseudo labels student;entity recognition;predictions self;supervised classification named;labels game;named entity recognition", "pdf_keywords": "self training framework;differentiable selftraining framework;supervised text classi\ufb01cation;framework self training;selftraining framework;semi supervised text;supervised text;differentiable self training;self training stackelberg;self training methods;training framework drift;entity recognition tasks;entity recognition;selftraining framework drift;self training approach;semi supervised classi\ufb01cation;propose differentiable selftraining;named entity recognition;training framework;differentiable selftraining;semi supervised;supervised classi\ufb01cation;training methods framework;supervised;supervised classi\ufb01cation graphs;self training set;existing self training;effective semi supervised;self training;weakly semi supervised"}, "2270b8628fd8ca67ae39d277f45bc3c38ac63d5f": {"ta_keywords": "tensorflow implement efficient;mesh tensorflow language;distributed tensor computations;tensorflow language;tensors operations batch;tensorflow;use mesh tensorflow;introduce mesh tensorflow;large models memory;efficient data parallel;data parallelism;mesh tensorflow;dimension mesh tensorflow;mesh tensorflow implement;tensorflow language specifying;splitting data parallelism;tensorflow implement;distributed tensor;data parallel model;model batch splitting;language modeling benchmark;dimensional mesh processors;model parallel version;distributed deep neural;sizes mesh tensorflow;data parallelism dominant;parallel model parallel;distributed deep;batch dimension mesh;data parallel", "pdf_keywords": "mesh tensorflow language;tensorflow language;distributed tensor computations;introducing mesh tensorflow;mesh tensorflow implement;computations mesh tensorflow;introduce mesh tensorflow;use mesh tensorflow;mesh tensorflow;tensorflow language specifying;tensorflow;tensorflow implement;language modeling benchmark;distributed tensor;mesh tensorflow graph;tensorflow implement ef\ufb01cient;tensorflow graph compiles;tensor computations mesh;tensorflow graph;class distributed tensor;model parallel version;tensor computations;tensor computations using;parallel model parallel;data parallel model;language modeling;model parallel;billion word language;word language modeling;translation task billion"}, "e2bd274c8dd2a3b2a0a6f5d8a29baee07df34eb9": {"ta_keywords": "string machine translation;efficient accurate translation;machine translation mt;machine translation;inferior machine translation;t2s translation theoretically;string t2s translation;tree string t2s;t2s translation;translation mt methods;tree string machine;parsing alignment search;systems including parsing;phrase based hierarchical;hierarchical phrase based;systems tree string;parsing alignment;including parsing alignment;string t2s;accurate translation previous;based hierarchical phrase;tree string;translation theoretically holds;translation theoretically;translation previous reports;hierarchical phrase;phrase based mt;systems tree;accurate translation;translation mt", "pdf_keywords": ""}, "98b7d5611c0a128f45db100cc796b981573adcc5": {"ta_keywords": "information retrieval introduction;retrieval introduction;information retrieval;learning information retrieval;retrieval;machine learning information;machine learning;issue machine learning;learning information;special issue machine;special issue;introduction;information;issue machine;special;learning;machine;issue", "pdf_keywords": ""}, "bcd4c46e4d75ddedb6138cfd77600c6d964a9aa8": {"ta_keywords": "pretrained code models;code models natural;code translation execution;language tocode tasks;natural language tocode;code translation;generative models code;incorporate program semantics;natural language code;models code pretrained;tocode tasks natural;code models;program semantics execution;pretrained code;language code translation;semantics execution results;program semantics;program selection improves;semantics execution;performance pretrained code;code pretrained;language tocode;code pretrained large;translation execution;marginalizing program implementations;tocode tasks;programs generated candidate;tasks natural language;generative;models natural language", "pdf_keywords": "pretrained code models;program selection improves;pretrained language code;language code models;risk decoding mbrexec;executed code training;algorithm pretrained language;quality generated programs;code models datasets;performance pretrained code;generated programs;program selection;code models natural;generated programs especially;language tocode tasks;marginalizing program implementations;pretrained code;programs generated candidate;code models;execution accuracy evaluation;mbrexec program selection;code training;output programs generated;programming languages select;execution based inference;analyzed mbrexec execution;semantics execution;representative programming languages;language tocode;output programs"}, "8688169ad5701e726968e293ff7dc53d76dd8007": {"ta_keywords": "ml papers suffer;papers suffer flaws;ml papers;flaws mislead public;flaws mislead;research practice;research research practice;suffer flaws mislead;research research;future research research;papers suffer;research;mislead public;papers;future research;suffer flaws;flaws;ml;stymie future research;mislead public stymie;mislead;public stymie future;public;future;practice;stymie future;public stymie;suffer;stymie", "pdf_keywords": ""}, "4f78624defde3b60551cfeb37e3943b267ea704a": {"ta_keywords": "distributed learning compressed;differences distributed learning;distributed learning method;distributed learning;new distributed learning;learning compressed gradient;learning compressed;compressed gradient differences;large machine learning;gradient differences distributed;compression gradient differences;compression gradient;compressed gradient;models requires distributed;distributed computing approach;gradient differences methods;able learn gradients;distributed computing;learn gradients;issues compression gradient;model updates bottleneck;propose new distributed;quantization updates;compression sparsification quantization;new distributed;learn gradients renders;based compression;methods based compression;updates bottleneck;based compression sparsification", "pdf_keywords": "distributed learning compressed;distributed learning method;distributed learning;new distributed learning;learning compressed gradient;learning compressed;large machine learning;compressed gradient differences;models requires distributed;compressed gradient;differences block quantization;compression gradient;compression gradient differences;distributed computing approach;block quantization;distributed computing;quantization;learning method;block quantization operator;propose new distributed;abstract training large;new distributed;learning models;quantization operator introduced;learning method diana;requires distributed computing;issues compression gradient;distributed;model updates bottleneck;quantization operator"}, "dbdefb498b619912a726fec7c85533594a1c6a1b": {"ta_keywords": "adversarially robust;gans adversarially robust;minimax optimization smooth;gans adversarially;adversarially robust models;smooth algorithmic adversaries;networks gans adversarially;adversary max;adversary max player;minimax optimization;algorithmic adversaries;adversarially;iterations minimax optimization;nonconcave maximization np;adversarial;minimax optimization minx;maximization np hard;adversarial networks gans;considers minimax optimization;adversarial networks;maximization algorithm guaranteed;algorithmic adversaries framework;optimization smooth algorithmic;algorithm min player;maximization np;minx maxy challenging;optimization minx maxy;optimization smooth;computable optimality;minimax", "pdf_keywords": "minimax optimization smooth;minimax optimization minx;adversarial training max;minimax optimization;adversary max player;minimax optimization problems;optimization minx maxy;algorithm min player;considers minimax optimization;adversary max;nonconcave minimax optimization;optimization minx;minx maxy challenging;2021 minimax optimization;considers minimax;minimax;nonconvex nonconcave minimax;deployed adversary max;adversarial training problems;nonconcave minimax;problems adversarial;adam optimization;adam optimization method;player instead maximization;problems adversarial training;paper considers minimax;smooth algorithmic adversaries;optimization smooth algorithmic;algorithmic adversaries;using adam optimization"}, "a3ce3004a0eade48a3ae652dbf5c04a60c2416aa": {"ta_keywords": "personalized dialogue generation;personalized dialogues anonymization;traits dialogue generation;persona aware dialogue;personalized dialogue;aware dialogue generation;personalized dialogues;deliver personalized dialogues;dialogue generation models;study personalized dialogue;dialogue generation;addressed dialogue generation;dialogues anonymization;dialogues anonymization schemes;dialogue generation researches;dialogue generation propose;personality traits dialogue;dialogue generation deliver;dialogue dataset;aware dialogue;turn dialogue dataset;dialogue dataset containing;traits dialogue;attention persona aware;construct personaldialog;personaldialog;firstly construct personaldialog;persona aware;captured addressed dialogue;personaldialog large scale", "pdf_keywords": "traits dialogue generation;personalized dialogue generation;persona information decoding;persona representation speaker;dialogue generation models;personality traits dialogue;aware personalized dialogue;personalized dialogue;dialogue generation diversified;personalized dialogues;deliver personalized dialogues;traits dialogue;dialogue generation;present personalized dialogue;persona aware personalized;dialogue generation process;dialogues propose persona;dialogue generation deliver;personalized dialogues propose;representation persona aware;persona aware attention;conditioned persona representation;persona representation;obtain persona representation;incorporate persona information;persona representation persona;capture address personality;decoding process persona;persona aware bias;persona information"}, "54a13bcc9613dcaa76fb25fbe96572f376cfcca9": {"ta_keywords": "adaptive learning rates;gradients adafactor adaptive;adafactor adaptive learning;learning rates sublinear;optimizer secondly adaptive;learning rates;rates sublinear memory;methods rmsprop adam;stochastic optimization methods;stochastic optimization;machine translation;adaptive learning;german machine translation;adafactor adaptive;sublinear memory;neural network weight;rmsprop adam adadelta;machine translation task;adam adadelta parameter;sublinear memory cost;proposed stochastic optimization;past gradients adafactor;secondly adaptive methods;rmsprop adam;adaptive methods;gradients adafactor;auxiliary storage optimizer;adaptive;adadelta parameter updates;translation task using", "pdf_keywords": "learning rate adam;gradient accumulator training;adafactor adaptive learning;scale machine translation;adaptive learning rates;rates sublinear memory;adafactor adaptive;sublinear memory;learning rates sublinear;popular machine translation;machine translation;stabilizing adaptive gradient;adaptive gradient;adaptive gradient methods;cause adafactor adaptive;finally learning rate;learning rates;learning rate;adam using sublinear;gradient accumulator;accumulators secondly adaptive;machine translation task;rate adam denotes;empirically using adam;sublinear memory cost;translation task known;accumulator slow;rate adam;translation task demonstrated;memory"}, "957e3ec3c722f5cb382fe8ac54fc846ee772a95f": {"ta_keywords": "algorithms adversarial bandits;adversarial bandits;combinatorial semi bandit;regret bounds improving;semi bandit problem;dependent regret bounds;bandit problem generally;bandit problem examples;bandit problem;semi bandit;regret bounds;feedback regret bound;armed bandit problem;multi armed bandit;regret bound simultaneously;logarithmic regret exists;loss logarithmic regret;regret bound depending;arm regret bound;bandit;logarithmic regret;regret bound;bandits;data dependent regret;include regret bound;adversarial multi armed;adaptive algorithms adversarial;armed bandit;partial feedback regret;regret exists arm", "pdf_keywords": "combinatorial semi bandit;generic bandit algorithm;algorithms adversarial bandits;adversarial bandits;bandit problem semi;semi bandit problem;semi bandit feedback;problem semi bandit;general semi bandit;bandit algorithm;consider combinatorial bandit;combinatorial bandit;bandit algorithm general;combinatorial bandit problem;adversarial bandits chen;semi bandit;semi bandit setting;bandit problem generally;bandit problem;novel generic bandit;bandit feedback;generic bandit;bandit problem work;bandit feedback subsumes;bandit setting formally;armed bandit problem;multi armed bandit;bandit setting;bandit;bandits"}, "c4ce6aca9aed41d57d588674484932e0c2cd3547": {"ta_keywords": "prominent pubmed search;pubmed search study;scientific search covid;pubmed search;natural language scientific;covid 19 literature;interdisciplinary scientific search;supporting interdisciplinary scientific;relations papers covid;scientific literature;papers covid 19;search covid 19;useful knowledge experiments;papers covid;scientific papers;scientific literature challenging;scientific search;covid 19 pandemic;annotate dataset mechanisms;search covid;relevance breadth annotate;extract mechanism relations;language scientific papers;knowledge base;interdisciplinary scientific;scientific papers developing;mechanism relations papers;pandemic spawned diverse;dataset mechanisms schema;outperforming prominent pubmed", "pdf_keywords": "mechanisms anotated covid;mechanisms anotated;19 annotated schema;covid 19 annotated;extract mechanism relations;annotate dataset mechanisms;mechanisms covid 19;mechanisms schema train;mechanism relations covid;mechanisms based schema;mechanisms schema;annotated schema;dataset mechanisms schema;mechanism knowledge base;seeing breakthroughs ai;19 papers annotated;anotated covid 19;mechanism relations papers;papers annotated;mechanisms covid;extract information scienti\ufb01c;mechanic dataset papers;papers annotated dataset;mechanic mechanisms anotated;search mechanism relations;relations covid 19;dataset mechanisms;papers related covid;related covid 19;mechanism knowledge"}, "73b22457a2f52a834d73d73a76b4124c1cb326be": {"ta_keywords": "gradient algorithms player;multiplayer performative prediction;performative prediction multiplayer;prediction multiplayer performative;player performative prediction;prediction multiplayer;algorithms \ufb01nding nash;player alternates learning;adaptive gradient algorithms;methods adaptive gradient;\ufb01nding nash equilibria;decision dependent games;adaptive gradient;stochastic gradient;dependent games equilibria;nash equilibria game;stochastic gradient method;repeated stochastic gradient;game theoretic;dependent games;strong monotonicity game;new game theoretic;monotonicity game;\ufb01nding nash;games equilibria;gradient algorithms;free methods adaptive;monotonicity game use;performative prediction learning;nash equilibria", "pdf_keywords": "multiplayer performative prediction;gradient algorithms player;player performative prediction;algorithms \ufb01nding nash;player alternates learning;decision dependent games;\ufb01nding nash equilibria;nash equilibria game;adaptive gradient algorithms;performative prediction learning;game theoretic;methods adaptive gradient;algorithms player;new game theoretic;\ufb01nding nash;algorithms player alternates;learning decision dependent;dependent games;adaptive gradient;nash equilibria;learning decision;performative prediction;multi player performative;stochastic gradient;game theoretic framework;stochastic gradient method;strong monotonicity game;multi player;formulates new game;called multi player"}, "6eb974721719056ba8dc74a898c64ae1d081e0ae": {"ta_keywords": "usefulness diagnostic curves;black box models;behavior diagnostic curves;diagnostic curves;diagnostic curve input;diagnostic curves black;diagnostic curve;univariate diagnostic curve;black box model;validate various qualitative;black box;curves black box;diagnostic curves multiple;univariate diagnostic;sample behavior diagnostic;demonstrate usefulness diagnostic;features checking undesirable;machine learning necessary;features checking;behavior diagnostic;usefulness diagnostic;properties black box;models;validating;framework approximately validating;finding univariate diagnostic;metrics test accuracy;applications machine learning;machine learning;box models", "pdf_keywords": "automated dependence plots;partial dependence plots;dependence plots;dependence plots de\ufb01ning;dependence plots model;usefulness automated dependence;unexpectedness dependence plots;automated dependence;directional dependence plots;dependence plots pdp;data partial dependence;resulting automated dependence;dependence plots adp;dependence plots adps;plots model response;plots widely used;plots widely;plots model;model selection bias;visual textual data;including model selection;generative model address;generative model;generative model demonstrate;exploring latent space;plots;visual tool;selection bias detection;interesting pdp plots;exploring latent"}, "4e2c41466c8246af0a563ea36fbe80c896bbab2c": {"ta_keywords": "neural machine translation;word sense disambiguation;handling homographs neural;homographs neural machine;translation based context;sense disambiguation;sense disambiguation literature;translation systems;machine translation nmt;homographs neural;machine translation;machine translation systems;word context aware;translation systems difficult;context aware word;differentiate word sense;homographs words different;homographs words;translation nmt systems;word embeddings;aware word embeddings;sentential context hypothesize;word sense;word sense feeding;global sentential context;translation based;sentential context;input word context;inspired word sense;word context", "pdf_keywords": "word translation accuracy;translation accuracy baseline;translation systems;neural machine translation;translation accuracy;machine translation systems;examining word translation;machine translation;dif\ufb01culty machine translation;translation systems dif\ufb01cult;word handling homographs;homographs neural machine;homographs words different;translation based;select correct translation;handling homographs neural;language pairs outperformed;correct translation based;translation based context;senses word handling;homographs words;homographs neural;neubig language technologies;model different language;language pairs;abstract homographs words;accuracy baseline nmt;language technologies;word translation;word handling"}, "e9c52a3fac934919eca036909cc18d909db0d467": {"ta_keywords": "upscaling molecular dynamics;model upscaling molecular;molecular dynamics;continuum model upscaling;upscaling molecular;peridynamic continuum model;driven peridynamic continuum;continuum model;data driven peridynamic;peridynamic continuum;model upscaling;molecular;driven peridynamic;dynamics;peridynamic;continuum;upscaling;data driven;model;data;driven", "pdf_keywords": "md displacements integrands;continuum model upscaling;model upscaling molecular;surrogate md displacements;md displacements;upscaling molecular dynamics;peridynamic continuum model;external loadings discretizations;loadings discretizations;data driven peridynamic;homogenized continuum model;loadings discretizations substantially;models including peridynamics;displacements integrands;continuum model;driven peridynamic continuum;linear peridynamic solid;model upscaling;peridynamic solid lps;displacements integrands operators;discretizations substantially;upscaling molecular;including peridynamics use;including peridynamics;md data optimal;grained homogenized continuum;nonlocal models including;peridynamic continuum;nonlocal models;discretizations"}, "52ec4713343083e69b87e36a7a12c7b5898e2780": {"ta_keywords": "multichannel speech enhancement;features speech enhanced;speaker adaptation multichannel;end speech recognition;multichannel speech;enhancement speech recognition;speech enhancement;speech recognition asr;speech recognition recent;speaker adaptation established;speaker adaptation;speech enhanced;effectiveness speaker adaptation;speech recognition;speech enhancement speech;noisy speech features;speech enhanced pathway;speech features;enhancement speech;corpora speaker adaptation;shown multichannel speech;unprocessed noisy speech;automatic speech recognition;speech recognition functions;dnn hmm hybrid;end multichannel asr;adaptation multichannel end;automatic speech;end automatic speech;multichannel asr", "pdf_keywords": ""}, "4d1a14352ffb526a1fa0e1cd90e2484e188cddc0": {"ta_keywords": "training dialogue systems;dialogue data interaction;creating dialogue data;dialogue user simulator;training dialogue;dialogue data;domain dialogues;dialogue systems;dialogues equips agents;data interaction dialogue;creating dialogue;source domain dialogues;domain dialogues equips;new dialogue scenarios;possibility creating dialogue;incorporate new dialogue;dialogue scenarios self;agents transfer learning;interaction dialogue;difficulties training dialogue;dialogue systems lack;dialogues;dialogue scenarios;dialogue;learning structured reward;dialogue user;dialogues equips;reinforcement learning structured;interaction dialogue user;agents converse", "pdf_keywords": "training dialogue systems;transferable dialogue systems;dialogue user simulator;developing dialogue systems;transferable dialogue;developing dialogue;dialogue data interaction;domain dialogues;creating dialogue data;training dialogue;dialogues equips agents;dialogue systems;dialogue data;domain dialogues equips;source domain dialogues;possibility creating dialogue;creating dialogue;data interaction dialogue;dialogues;dialogue systems lack;dialogue systems user;dialogue systems performs;interaction dialogue;agents transfer learning;dialogue user;dif\ufb01culties training dialogue;dialogues equips;dialogue;interaction dialogue user;work developing dialogue"}, "6d00b1024298e5b64ee873028385f7bb4396b05d": {"ta_keywords": "compositional generalization neural;compositional generalization learning;compositional generalization ability;comprehensive compositional generalization;compositional generalization;generalization neural sequence;limited compositional generalization;recombination compositional generalization;generalization neural;semantic parsing tasks;compositional generalization benchmarks;generalization ability semantic;generalization learning algebraic;semantic parsing task;semantic parsing;model semantic parsing;syntactic algebra semantic;parsing tasks;syntactic algebra;learning algebraic recombination;latent syntax interpreter;latent syntactic algebra;ability semantic parsing;generalization learning;semantic algebra encouraging;algebraic recombination compositional;parsing;neural sequence models;learn algebraic recombination;comprehensive compositional", "pdf_keywords": "recombination compositional generalization;compositional generalization;compositional generalization requires;compositional generalization algebraic;exhibits compositional generalization;recombining structured expressions;learn algebraic recombination;structured expressions recursive;syntactic algebra;algebraic recombination compositional;syntactic algebra semantic;semantic algebra encouraging;novel expressions dynamically;compositional generalization key;latent syntactic algebra;expressions recursive manner;semantic parsing;semantic parsing task;intelligence exhibits compositional;parsing;algebra semantic;structured expressions;recombination compositional;algebra semantic algebra;model semantic parsing;semantic algebra;expressions recursive;generalization algebraic;parsing task homomorphism;expressions dynamically recombining"}, "6ccac8a95bc77549b98d045db6d5e0de3d356ba4": {"ta_keywords": "bert models exploring;bert based contextualized;lexical translation models;existing bert models;bert models;layer bert based;lexical translation model;bert based ranking;query document embeddings;contextualized embeddings;model effective bert;neural lexical translation;effective bert based;translation models;model layer bert;layer bert;models information retrieval;based contextualized embeddings;document embeddings;translation model ibm;utility lexical translation;english text retrieval;model english text;contextualized embeddings does;translation models information;retrieval interpretability;context free neural;retrieval interpretability effectiveness;free contextualized query;text retrieval", "pdf_keywords": "bert models;bert based contextualized;existing bert models;bert models knowledge;lexical translation model;layer bert based;model layer bert;translation model ibm;free contextualized embedding;contextualized embeddings;context free neural;model english text;contextualized embedding network;contextualized embedding;layer bert;faster bert;contextualized embeddings does;utility lexical translation;based contextualized embeddings;translation model;interpretable neural model;ibm model english;adding interpretable neural;interpretable neural;model english;context free contextualized;length existing bert;faster bert gpu;free contextualized;bert based"}, "69c515a62403fcc19125d3a6dd8e878aa5cde604": {"ta_keywords": "incomplete utterance restoration;utterance restoration;utterance restoration inspired;generation sequence labeling;large conversation data;dialogue systems;turn incomplete utterance;challenge frequent coreference;sequence labeling text;conversation data;dialogue;sequence labeling;autoregression generation sequence;dialogue systems open;conversation data development;autoregression generation;inspired autoregression generation;frequent coreference;restoration inspired autoregression;frequent coreference information;incomplete utterance;semi autoregressive generator;large conversation;labeling text editing;text editing;autoregressive generator;coreference information;autoregressive generator sarg;conversation;text editing propose", "pdf_keywords": "autoregression text generation;model utterance restoration;generation sequence labeling;text generation sequence;incomplete utterance restoration;text generation;sequence labeling autoregressive;utterance restoration task;model utterance;utterance restoration boosting;generation suitble utterance;corpus development deep;utterance restoration;suitble utterance restoration;labeling autoregressive generation;sequence labeling;sequence labeling text;autoregression text;appropriate model utterance;dialogue systems;turn incomplete utterance;utterance restoration experimental;single turn corpus;inspired autoregression text;challenge frequent coreference;turn corpus;abstract dialogue systems;dialogue systems open;autoregressive generation suitble;turn corpus development"}, "d2f327736c9b68f68ad64d0b1cefed9b4dd83313": {"ta_keywords": "imitation learning structured;imitation learning language;generating natural language;structured prediction learns;generation nlg task;learning structured prediction;language generation nlg;manually constructed linguistic;structured prediction task;natural language generation;learning language generation;nlg task generating;structured prediction;imitation learning;learning structured;use imitation learning;prediction learns incremental;language generation;generation nlg;constructed linguistic;learns incremental;learns incremental model;prediction learns;space structured prediction;language generation unaligned;nlg task;data phrase templates;templates natural language;training data phrase;natural language", "pdf_keywords": ""}, "e37fb85e8869d464bae8eeebf4cd9321ec8c70ad": {"ta_keywords": "interpretable classifiers;classification view interpretability;accuracy interpretability restriction;restriction interpretable classifiers;enforcing interpretability;enforcing interpretability model;interpretability performing empirical;accuracy interpretability;implications enforcing interpretability;interpretability restriction interpretable;enforcing interpretability using;interpretability restriction;interpretable classifiers does;interpretability propose;enforcing interpretability performing;interpretability;act enforcing interpretability;empirical risk minimization;interpretability using known;trade accuracy interpretability;interpretability constraint;interpretable hypotheses;interpretability using;interpretability propose model;interpretability model;define interpretability propose;binary classification view;emph enforcing interpretability;statistical implications enforcing;define interpretability", "pdf_keywords": "interpretability constraint learning;seeking interpretability constraints;enforcing interpretability statistical;interpretability constraints;interpretability performing empirical;enforcing interpretability;enforcing interpretability constraint;enforcing interpretability model;interpretability constraint;concrete enforcing interpretability;enforcing interpretability performing;interpretability propose;seeking interpretability;interpretability;interpretability statistical;accuracy interpretability;offs accuracy interpretability;increase seeking interpretability;act enforcing interpretability;define interpretability propose;interpretability model;interpretability statistical impacts;define interpretability;interpretability propose model;interpretability performing;accuracy interpretability preprint;interpretable hypotheses;interpretability preprint;interpretable;set interpretable hypotheses"}, "8b0b2b69657076fc1ce7cce75a6d69e3e5ba2d63": {"ta_keywords": "framework electrodeposited nickel;organic framework electrodeposited;electrodeposited nickel foam;metal organic framework;supercapacitors;high performance supercapacitors;framework electrodeposited;electrodeposited nickel;performance supercapacitors;nickel foam binder;nickel foam;based metal organic;binder free electrode;cobalt based metal;metal organic;cobalt based;foam binder free;electrodeposited;free electrode;electrode high performance;foam binder;organic framework;free electrode high;electrode;cobalt;foam;electrode high;nickel;framework;based metal", "pdf_keywords": ""}, "6a173e22819480b891306eac65fd44be010dfca8": {"ta_keywords": "h1n1 virus influenza;inflammation macaque lungs;inflammation ineffective ph1n1;virus induced inflammation;influenza virus induced;pandemic influenza virus;influenza virus;virus influenza;seasonal influenza virus;defend influenza virus;signaling inflammation;pandemic h1n1 virus;signaling inflammation ineffective;induced inflammation macaque;pandemic influenza;mitigating viral pathogenesis;inflammation macaque;ph1n1 virus infection;seasonal influenza;ph1n1 virus;cell signaling inflammation;2009 pandemic influenza;influenza virus isolate;mitigating viral;ineffective ph1n1 virus;influenza;virus influenza california;defend influenza;viral pathogenesis;relative seasonal influenza", "pdf_keywords": ""}, "1d2a2b14ef14eeaf89169f738f7634cdc685c785": {"ta_keywords": "whirl query language;similarity predicate term;similarity constraints query;built similarity predicate;similarity predicate;query language;relational databases;contains built similarity;built similarity;relational database;whirl query;similarity constraints;relational databases statistical;satisfies similarity constraints;sql;databases statistical ir;whirl manipulates relational;databases;satisfies similarity;relational;similarity;properties relational databases;database;predicate term vectors;relation contains document;databases statistical;sql contains;language extension sql;sql contains built;predicate term", "pdf_keywords": ""}, "c968e8dc442102b38b134b1afadc7cc78fc5b5fb": {"ta_keywords": "named entity recognition;entity recognition ner;entity recognition;evaluation named entity;named entity;models natural language;task interpretable multi;ner task interpretable;natural language processing;interpretable evaluation named;interpretable multi dataset;interpretable evaluation;holistic metrics accuracy;language processing tasks;recognition ner task;entity;multi dataset evaluation;task interpretable;recognition ner;methodology interpretable evaluation;interpretable multi;dataset evaluation;natural language;language processing;ner task;dataset evaluation named;metrics accuracy bleu;holistic metrics;diverse datasets;metrics accuracy", "pdf_keywords": "named entity recognition;evaluation named entity;entity recognition ner;entity recognition;interpretable evaluation named;interpretable evaluation;interpreteval methodologically evaluation;entity score making;entity score;methodology interpretable evaluation;usually entity score;accuracy usually entity;named entity;score assessing accuracy;holistic score assessing;methodologically evaluation framework;evaluation named;score making analysis;interpreteval;evaluation framework;datasets interplay identifying;score assessing;interpreteval methodologically;evaluation;entity;evaluation comparison;evaluation comparison multiple;current evaluation methodology;neulab interpreteval methodologically;methodologically evaluation"}, "7a56aba1a4d4020c4933319588b9ed2b34d51125": {"ta_keywords": "secure computation;ml algorithms secure;data secure computation;algorithms secure;algorithms secure multi;secure computation machine;mpc protocol malicious;machine learning spdz;mpc area cryptography;party computation mpc;implementation mpc protocol;mpc protocols;theoretical mpc protocols;multi party computation;cryptography enables computation;mpc protocol;secure multi party;computation machine learning;protocol malicious security;computation sensitive data;computation mpc;mpc protocols scale;secure multi;implementation mpc;data secure;cryptography;cryptography enables;guarantees theoretical mpc;malicious security;provides implementation mpc", "pdf_keywords": "secure computation;secure computation machine;security theoretical mpc;machine learning spdz;privacy guarantees project;computation sensitive data;computation machine learning;cryptography enables computation;privacy guarantees;maintaining privacy guarantees;computation sensitive;enables computation sensitive;semi honest mpc;multi party computation;cryptography enables;malicious security;mpc area cryptography;privacy;party computation mpc;mpc protocol malicious;secure multi party;cryptography;honest mpc techniques;secure;maintaining privacy;using semi honest;secure multi;theoretical mpc protocols;providing stronger security;protocol malicious security"}, "e02f79b710cdcaa9135b835fad964f6f2c78b1a7": {"ta_keywords": "explicit tokenization vocabulary;tokenization vocabulary pre;training efficient tokenization;tokenization vocabulary;tokenization free encoder;recent tokenization approaches;tokenization step canine;explicit tokenization;sequences explicit tokenization;efficient tokenization free;canine neural encoder;efficient tokenization;representation recent tokenization;pipelined nlp;tokenization approaches;pipelined nlp systems;tokenization;abstract pipelined nlp;tokenizers;encoder language representation;tokenization approaches based;tokenizers techniques equally;require explicit tokenization;tokenizers techniques;language representation recent;directly character sequences;tokenization free;language representation;manually engineered tokenizers;explicit tokenization step", "pdf_keywords": "tokenization free deep;language modeling character;masked language modeling;tokenizationfree vocabulary;pre trained tokenization;trained tokenization;trained tokenization free;trained encoder language;uses tokenizationfree vocabulary;tokenizationfree vocabulary free;canine designed linguistic;encoder language understanding;pre trained encoder;deep encoder;free deep encoder;encoder language;model performs tokenization;tokenizers;morphologically rich languages;trained encoder;deep encoder ef\ufb01cient;modeling character;masked language;tokenization;tokenization free;linguistic properties morphologically;language modeling;embedding text character;morphology canine;tokenization input avoiding"}, "4572ded23106285cbd8ebbe6c3b354973ac06ff7": {"ta_keywords": "emotion indonesian conversational;analysis emotion indonesian;emotion indonesian;indonesian conversational speech;recognition analysis emotion;indonesian conversational;analysis emotion;speech \u97f3\u58f0 \u7b2c16\u56de\u97f3\u58f0\u8a00\u8a9e\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0;conversational speech \u97f3\u58f0;conversational speech;emotion;speech;\u97f3\u58f0 \u7b2c16\u56de\u97f3\u58f0\u8a00\u8a9e\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0;indonesian;speech \u97f3\u58f0;\u7b2c16\u56de\u97f3\u58f0\u8a00\u8a9e\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0;conversational;recognition analysis;recognition;analysis;\u97f3\u58f0", "pdf_keywords": ""}, "e214d2a6399925ce60fa5ce90c0374127a32b47e": {"ta_keywords": "impromptu deployment wireless;deployment wireless relay;deployment wireless sensor;approaches impromptu deployment;impromptu deployment deployment;deployment wireless;based impromptu deployment;relay deploys;impromptu deployment;algorithms converge deployment;deploys relays locations;sensor networks;knowledge radio propagation;wireless path;wireless sensor networks;deploys relays;wireless relay;relays locations aim;sequential decision algorithms;converge deployment progresses;networks field deployment;deployment agent forward;radio propagation;wireless relay network;algorithms explore forward;deployment progresses;multihop wireless path;previous relay deploys;relays locations;relay deploys relays", "pdf_keywords": "deployment wireless relay;impromptu deployment wireless;wireless relay networks;relay networks;approaches deployment wireless;sensor networks;wireless relay;deployment wireless sensor;deployment propose learning;deployment wireless;networks \ufb01eld deployment;wireless relay network;wireless sensor networks;knowledge radio propagation;stochastic approximation;algorithms converge deployment;based stochastic approximation;relay networks using;converge deployment progresses;relay;relay network;sequential decision algorithms;impromptu deployment deployment;radio propagation;stochastic approximation asymptotically;converge deployment;impromptu deployment;approaches impromptu deployment;based impromptu deployment;sensor networks conclusion"}, "7ee55c115470e1b86e552c5594e2e4258b4ccefb": {"ta_keywords": "sites oxygen reduction;fe nx active;active sites oxygen;hosted fe nx;sites oxygen;oxygen reduction reaction;oxygen reduction;edge hosted fe;fe nx;nx active sites;nx active;hosted fe;configuration engineering edge;engineering edge hosted;optimizing configuration;oxygen;optimizing configuration engineering;fe;nx;reduction reaction;engineering edge;edge hosted;configuration engineering;active sites;configuration;optimizing;reduction;active;edge;hosted", "pdf_keywords": ""}, "a7766d4c41df235764dfaa9971ce861f6120ac27": {"ta_keywords": "latency meeting recognition;meeting recognition;meeting analyzer;real time meeting;meeting recognition understanding;group meetings;analyzer group meetings;videos meeting;time meeting analyzer;meeting analyzer group;meetings;recognizes speaks online;meetings low latency;low latency meeting;meeting;latency meeting;meeting browser;meeting browser present;information captured microphone;meeting situation;group meetings low;microphone;distant microphones demo;distant microphones;time meeting;demo videos meeting;captured microphone array;captured microphone;using distant microphones;meetings low", "pdf_keywords": ""}, "252ef125a8874fe8face4540f87f2e000275cc96": {"ta_keywords": "communities terrorist groups;networks communities terrorist;terrorist communities testing;latent terrorist communities;terrorist communities;communities terrorist;terrorism research;measure cluster perpetrators;detecting latent terrorist;cluster perpetrators different;terrorism research highlight;cluster perpetrators;terrorist groups;task terrorism research;terrorist groups active;similarity measure cluster;clustering;similarity based clustering;partite networks communities;terrorism;clustering algorithm;similarity based;key task terrorism;networks communities;clustering algorithm multi;similarity similarity measure;similarity similarity;clustering algorithm designed;task terrorism;innovative clustering algorithm", "pdf_keywords": ""}, "803c7fdd6e01e1ff8cd43297f4e052078409456d": {"ta_keywords": "emerges social inferences;social inferences dyadic;inferences dyadic interactions;strangers influence communicative;bayesian theory coordination;social inferences;continual hierarchical adaptation;communicative context conventions;social conventions stable;interaction partner gradual;coordination;context conventions eventually;theory coordination;coordination convention formation;hierarchical adaptation inference;inference chai hierarchical;communicative context;communication simply;introduce continual hierarchical;continual hierarchical;communication;repeated interaction partner;correspond beliefs intentions;communicative;influence communicative context;words acquire new;hierarchical adaptation;conventions stable priors;observations language;strangers influence", "pdf_keywords": "bayesian theory coordination;convention formation hierarchical;coordination convention formation;theory coordination;communicative context conventions;context conventions eventually;coordination;human generalization behavior;human generalization;strangers in\ufb02uence communicative;theory coordination convention;ad hoc coordination;coordination convention;conventions eventually form;generalization behavior;hierarchical bayesian inference;inference hierarchical bayesian;in\ufb02uence communicative context;convention formation;hoc coordination;generalization behavior natural;formation hierarchical bayesian;captured human generalization;inference hierarchical;communicative context;hierarchical bayesian theory;coordination convention aims;language communication;ground strangers in\ufb02uence;natural language communication"}, "c6488f0c62ee4a4d48d0fbf8e8185655226294c1": {"ta_keywords": "attack energy detector;attack detection;attacker minimize;attack detection given;attack design;attack detection probability;different attack detection;attack design problem;ump attack energy;manipulation attack cma;formulate attack design;attack cma reconfigurable;attacker minimize data;controller manipulation attack;constraint attack detection;attack detection models;attacker potential manipulate;snr moments attack;goal attacker minimize;based attack detection;powerful ump attack;attack energy;attack obtained analytically;attack cma;manipulation attack;ump attack;attack called controller;formulate attack;attack obtained;ris assisted communication", "pdf_keywords": "manipulation attack ump;attack ump energy;controller manipulation attack;attack ump;transmitter receiver attacker;manipulation attack recon\ufb01gurable;manipulation attack cma;snr moments attack;manipulation attack;attack called controller;attack recon\ufb01gurable intelligent;attack design;receiver attacker;attack recon\ufb01gurable;receiver attacker potential;attack detection;attack design problem;attacker potential manipulate;ump energy detector;attacker minimize;attack detection probability;aided wireless communication;manipulate ris controller;moments attack obtained;attack obtained analytically;attack cma recon\ufb01gurable;attack obtained;ris assisted communication;formulate attack design;fading block snr"}, "2e4cdd36d77b9d814638fc2cd6c703535cb1d2f7": {"ta_keywords": "prompt tuning nlg;tuning nlg tasks;pretraining corpus tuning;frequently nlg tasks;frozen pretrained language;tuning nlg;pretrained language model;linguistically different pretraining;nlg tasks;better prompt tuning;nlg tasks demonstrate;corpus tuning;plm prompt tuning;nlg tasks unfamiliar;pretraining corpus;prompt tuning;prompt tuning fine;seven nlg tasks;frequently nlg;pretrained language;different pretraining corpus;tuning continuous prompts;prompt tuning takes;corpus tuning continuous;tasks input tuning;language model plm;prompts frozen pretrained;occur frequently nlg;consistently better prompt;input tuning adapting", "pdf_keywords": "prompt tuning nlg;pretraining corpus tuning;better prompt tuning;frozen pretrained language;prompt tuning;tuning nlg tasks;linguistically different pretraining;prompt tuning takes;pretrained language model;prompt tuning proposed;tuning continuous prompts;pretrained language;plm prompt tuning;different pretraining corpus;pretraining corpus;prompts input representations;tuning nlg;consistently better prompt;corpus tuning;prompts frozen pretrained;better soft prompts;input unfamiliarity essential;tuning alleviate unfamiliarity;indispensable input tuning;input unfamiliarity;prompts indispensable input;development prompt tuning;corpus tuning continuous;propose input tuning;soft prompts"}, "4c2d9136c579a0393d4f50bbbbc6f8dab43c38e9": {"ta_keywords": "blind estimation shape;evaluation blind estimation;estimation source priors;blind estimation;source priors weighted;priors weighted prediction;trained backpropagation;trained backpropagation outputs;backpropagation;network trained backpropagation;shape parameter priors;speech recognition;backpropagation outputs;gaussian cgg prior;speech recognition asr;gaussian sources depending;gaussian sources;prior shape parameter;backpropagation outputs network;super gaussian sources;source priors;generalized gaussian cgg;generalized gaussian;estimation shape parameter;priors weighted;estimation shape;shape parameter estimator;prior shape;weighted prediction;differentiable neural network", "pdf_keywords": ""}, "6f49026ff623c64ce6de81fd04cf6e1ffe7dd6d9": {"ta_keywords": "gans learn generate;adversarial networks binary;networks gans learn;networks gans;learn generate music;gans learn;binary neurons polyphonic;generate music;study convolutional gan;networks binary neurons;convolutional gan;neurons polyphonic music;convolutional generative adversarial;generative adversarial networks;convolutional gan model;deep convolutional generative;generative adversarial;convolutional generative;using binary neurons;generative;represent music binary;music binary valued;polyphonic music generation;matrices convolutional generative;gan model;binary valued piano;music generation existing;gans;refiner network trained;adversarial networks gans", "pdf_keywords": "gans learn generate;binary neurons polyphonic;learn generate music;generate music;polyphonic music generation;neurons polyphonic music;creates binaryvalued piano;study convolutional gan;adversarial networks binary;binaryvalued piano rolls;represent music binary;convolutional gan;convolutional gan model;binaryvalued piano;networks gans learn;music binary valued;novel convolutional gan;binary valued piano;networks gans;music binary;gan based model;gans learn;convolutional gan based;deep convolutional generative;convolutional generative;music generation;gan model;convolutional generative adversarial;generative;polyphonic music"}, "57e7be6b404abfd7a56a73c0ff9bccc5b27ad7ae": {"ta_keywords": "translation domain aware;machine translation domain;unsupervised domain adaptation;domain adaptation neural;domain adaptation;neural machine translation;machine translation models;translation domain;translation models;infeasible translation models;translation models control;translation models relies;domain adaptation strategies;machine translation;domain copied monolingual;models domain aware;sentences desired domain;monolingual translated data;auxiliary language modeling;domain specific representations;makes infeasible translation;training model domain;embeddings learned auxiliary;domain shift;domain aware feature;copied monolingual translated;model domain copied;language modeling task;embeddings learned;adaptation neural machine", "pdf_keywords": "unsupervised domain adaptation;translation domain aware;machine translation domain;domain adaptation disentangling;domain adaptation neural;neural machine translation;domain adaptation;domain translation results;domain translation;machine translation adapts;machine translation models;output domain translation;domainaware feature embedding;translation domain;domain adaptation technique;translation models;translation models relies;machine translation nmt;machine translation;feature embedding dafe;embedding dafe performs;translation improve performance;translation adapts;embedding dafe;adaptation disentangling representations;unsupervised domain;performs unsupervised domain;models domain aware;effective unsupervised domain;domain aware feature"}, "2b3ab7e9c66bffc7af9e4413036e7bba686a7734": {"ta_keywords": "potential bias reviewers;bias reviewers;reviewers exhibit bias;bias reviewers recommendations;reviewers negatively biased;burden reviewers conferences;reviewers conferences;competent reviewers growing;reveals reviewers negatively;analysis reveals reviewers;reviewers growing slower;reviewers growing;novice reviewers constitute;reveals reviewers;novice reviewers;population novice reviewers;investigate reviewers;investigate reviewers exhibit;quality peer review;reviewers conferences started;reviewers negatively;peer review;competent reviewers;reviewers receive;reviewers exhibit;reduce burden reviewers;number competent reviewers;reviewers;peer review pipeline;reviewer pool", "pdf_keywords": "novice reviewers bias;reviewers bias resubmissions;reviewers bias;reviewers exhibit bias;prejudice novice reviewers;competent reviewers growing;novice reviewers tend;observe novice reviewers;novice reviewers constitute;novice reviewers;population novice reviewers;conference peer review;reviewers negatively biased;competent reviewers;peer review;reviewers master junior;reviewers tend underrate;reviewers tend;analysis reveals reviewers;reviewers growing;reviewers growing slower;reviewer pool;quality peer review;investigate reviewers;reviewers;investigate reviewers exhibit;reviewers master;peer review pipeline;reviewers exhibit;number competent reviewers"}, "536ce077f08886b5834b639da25068d877c98b2c": {"ta_keywords": "lasers dentistry xxiv;lasers dentistry;lasers;dentistry xxiv;dentistry;xxiv", "pdf_keywords": ""}, "b54efe01969adaa1c623331d5791897a4dd9f886": {"ta_keywords": "fruit fly genomics;information extraction flybase;fly genomics;genomics incorporation flybase;fly genomics incorporation;extraction flybase;extraction flybase curation;adaptive information extraction;papers fruit fly;information extraction designed;information extraction;genomics;interactive information extraction;fruit fly;associated pdf extraction;curation flybase;pdf extraction;curation flybase employs;flybase curation;pdf extraction tool;curation papers fruit;flybase curation flybase;genomics incorporation;flybase;papers fruit;flybase employs;incorporation flybase bootstrapping;seed based;paper curatable information;flybase bootstrapping", "pdf_keywords": ""}, "0e8da301b098f96fed39c5aa8e2194f678690a16": {"ta_keywords": "secret share dissemination;share dissemination communication;algorithm secret share;deterministic secret share;share dissemination;distributed deterministic secret;disseminating shares secret;share dissemination network;dissemination network;shares secret;shares secret dealer;secret share;network coding;communicate respective shares;secret dealer participants;dissemination communication;network coding problem;dissemination communication efficient;protocols assume dealer;network interestingly solution;communication efficient distributed;provide algorithm secret;algorithm secret;problem disseminating shares;deterministic secret;nodal eavesdropping;efficient distributed;distributed;dealer direct communication;respective shares participants", "pdf_keywords": ""}, "73569460b023f9ac1fe5a1876c3401460d2fc15d": {"ta_keywords": "code pre trained;training code data;code clone detection;trained models code;code clone;code data natural;exploit semantic preserving;natural language pretrained;classification code clone;source code understanding;pre training code;models learn semantic;training code;language pretrained model;source code;code data;code search;code related tasks;pre trained models;semantic preserving transformation;semantic preserving;tasks source code;trained models learn;learn semantic;existing code pre;clone detection code;detection code search;exploit semantic;language pretrained;learn semantic features", "pdf_keywords": "code pre trained;code semantic knowledge;trained code data;extracting code semantic;pre trained code;trained models code;code semantic;code clone detection;trained code;code clone;code search;code model;pre trained language;classification code clone;trained models codebert;language pre trained;detection code search;trained language model;natural language pre;architecture pre trained;pre trained models;complementing pre trained;code model architecture;code related tasks;code data;trained language;pre trained model;code search paper;source code;results natural language"}, "23f1d4b46bc7c8f357a5a89144d5d32af7be13a5": {"ta_keywords": "news summarization;focusing news summarization;news summarization little;models focusing news;training models content;generation strategies learnt;content selection generation;model learns;summarization little understood;improving factuality improving;summarization;study model learns;summarization little;generation models focusing;tokens learnt quickly;copy behavior learnt;learnt iterations;learns;approaches modifying training;models content selection;generation models;model learns different;learnt quickly;learnt;pre training models;learnt quickly simple;strategies learnt iterations;goals improving factuality;models content;training models", "pdf_keywords": "summarization models trained;summarization models abstractiveness;abstractive summarization models;factuality summarization models;train summarization models;models focusing summarization;summarization models;target summarization goals;summarization models entire;summarization goals;optimize target summarization;summarization goals like;target summarization;improve factuality summarization;train summarization;abstractive summarization;summarization finally insights;factuality summarization;components abstractive summarization;focusing summarization finally;focusing summarization;summarization;summarization finally;cnndm mediasum factuality;improvements abstractiveness cnndm;section train summarization;like factuality abstractiveness;substantially improve factuality;factuality abstractiveness copy;improve factuality"}, "9389af659f14239319186dff1cef49e8ece742c8": {"ta_keywords": "graph ml ogb;improve graph ml;learning graphs ogb;machine learning graphs;graph learning tasks;graph learning;large scale graph;learning graphs;graphs ogb;core graph learning;graph data expressive;graph ml;graphs ogb lsc;scale graph ml;graph ml scale;prediction graph regression;link prediction graph;efforts improve graph;prediction graph;graph regression;graph regression node;ogb large scale;scale graph data;learning ml large;graphs;large scale challenge;graph data;improve graph;challenge machine learning;core graph", "pdf_keywords": "expressive graph ml;massive datasets expressive;graph ml models;improve graph ml;graph ml challenge;large scale graph;scaling expressive graph;expressive graph;graph ml furthermore;massive modern datasets;graph ml;scale graph ml;massive datasets;graph ml scale;datasets expressive models;models massive datasets;datasets expressive;graph ml present;state art graph;ml challenge ogb;art graph ml;ogb large scale;efforts improve graph;large scale challenge;ml models massive;simple scalable baselines;challenge ogb large;scalable baselines;scaling expressive;expressive models"}, "c68349ba142be731d6f3339d894764921c69b774": {"ta_keywords": "twitter predict obesity;twitter predict;social media driven;automatically twitter;media twitter predict;like quiz data;semi automatically twitter;twitter;quiz data;twitter doing achieved;social media twitter;social media identify;automatically twitter doing;individuals took quiz;like quiz;using social media;build social media;predict obesity;media twitter;train public health;game like quiz;predict obesity risk;use social media;diabetes mellitus t2dm;using social;twitter doing;social media;use social;training data individual;detection individuals risk", "pdf_keywords": "obesity detection mitigates;algorithm obesity detection;obesity detection;trained social media;social media data;social media driven;social media identify;trained random forest;algorithm obesity;automatically twitter;semi automatically twitter;random forest;collection algorithm obesity;public twitter statuses;diabetes mellitus t2dm;twitter statuses;bootstrapped machine learning;detection individuals risk;twitter;twitter built random;training data;overweight communities particular;automatically twitter built;overweight communities;twitter strategy;public twitter;classi\ufb01cation overweight communities;questions acquired twitter;random forest classi\ufb01er;early detection individuals"}, "6665e03447f989c9bdb3432d93e89b516b9d18a7": {"ta_keywords": "effective rule induction;rule induction;fast effective rule;effective rule;induction;fast effective;rule;effective;fast", "pdf_keywords": ""}, "e92de0c4ef62a84201fac284eb66c37330b5fe1c": {"ta_keywords": "automated bug repair;generate fix code;helpful fixing bugs;helpful correcting bugs;fixing bugs;fixing bugs occasions;bug repair;future bugs learning;bugs learning generate;correcting bugs;automated bug;bug prone code;code past fixes;generate fix;bugs learning;neural machine translation;bug repair paper;corrective patch generation;patch generation using;patch generation;patches using neural;generate corrective patches;bugs occasions fix;fix code;fixes neural;fixes propose fixes;fixes neural sequence;given bug prone;able generate fix;future bugs", "pdf_keywords": "neural machine translation;ratchet generate syntactically;corrective patch generation;bug prone code;correction code past;patch generation using;generation using neural;patch generation;code past \ufb01xes;ratchet corrective patch;neural sequence sequence;machine translation;post correction code;correction code;ratchet corrective;propose ratchet corrective;ratchet generate;sequence model ratchet;\ufb01xes neural sequence;prone code query;generate syntactically;neural sequence;prone code;generate \ufb01x code;generate syntactically valid;code past;ratchet able generate;\ufb01ndings ratchet generate;query \ufb01ndings ratchet;neural machine"}, "90848c88f56fcd421ac3cfd2c87d3e61211103ea": {"ta_keywords": "text categorization relational;categorization relational learning;categorization relational;text categorization;relational learning;categorization;relational;learning;text", "pdf_keywords": ""}, "051a85bd1384767ea5882dcefa98aee5664aa2cf": {"ta_keywords": "deep architectures speech;deep architectures;novel deep architectures;deep network generalizing;deep network;networks speech processing;deep neural networks;networks speech;novel networks speech;discriminative training;architectures speech processing;layers deep network;architectures speech;straightforward discriminative training;speech processing frameworks;neural networks tremendously;deep neural;learning novel deep;deterministic deep neural;discriminative training relatively;speech processing including;methods deep neural;speech processing;inference iterations layers;neural networks;nonnegative matrix factorization;inference straightforward discriminative;matrix factorization;neural networks constructed;associated inference algorithm", "pdf_keywords": ""}, "6fde1c63b1a353cf539d319341ae9396000660ed": {"ta_keywords": "automatic pronunciation proficiency;pronunciation evaluation systems;pronunciation proficiency estimation;automatic pronunciation evaluation;based pronunciation scoring;robustness automatic pronunciation;technique automatic pronunciation;pronunciation scoring;automatic pronunciation;pronunciation evaluation;used noise learners;pronunciation proficiency;noise learners recorded;proficiency estimation noisy;noise learners;pronunciation scoring baseline;students microphone;estimation noisy classroom;students microphone noise;noise sources speech;gop based pronunciation;using noise reduction;surrounding students microphone;pronunciation;based pronunciation;proficiency estimation;noise reduction;splice reading speech;learners recorded;splice noise reduction", "pdf_keywords": ""}, "e03d9684a19c8f8e29ee97b347d4f1e280a88e44": {"ta_keywords": "gestures language grounding;language gestures crossmodal;gestures crossmodal grounding;spoken language gestures;gestures spoken language;gestures crossmodal;language gestures;approaches gestures language;gestures language;contrastive learning grounding;learning grounding spoken;gestures spoken;spoken language gesture;similarities gesture space;language gesture;gesture space;gestures;language example gesture;prior approaches gestures;example gesture;grounding spoken language;language gesture approach;crossmodal clustered contrastive;approaches gestures;timed gestures spoken;gesture;language grounding;gesture semi;learning grounding;clustered contrastive learning", "pdf_keywords": ""}, "d31b5b60e1b3af84cd977da8db0ed4faeb79e7f7": {"ta_keywords": "text style transfer;style transfer;perform style transfer;style extraction;text style extraction;style transfer present;style transfer label;shot text style;extract style;style extraction tunable;style recast transfers;extract style vector;strong pretrained text;style labeled training;requiring style labeled;text style;pretrained text text;style vector text;style vector space;style labeled;pretrained text;style adjacent sentences;approaches requiring style;requiring style;encoding facets style;shot text;decoder perform style;model extract style;style vector;targeted restyling vector", "pdf_keywords": "text style transfer;inducing text style;text style representations;style transfer targeted;style transfer;style transfer model;noisy translation training;translation training illustrate;decoder style transfer;style transfer use;translation training;pretrained text;pretrained text text;inducing text;style representations;representation textual style;reframe style transfer;text text models;textual style;effectiveness noisy translation;shot text style;large pretrained text;style representations reframe;text models;textual style extracted;text models like;shot style transfer;style transfer competitive;style dialect emotiveness;style dialect"}, "5812e30eb4756aeaf0b013a65b98f8f8aa0f8315": {"ta_keywords": "smt systems englishgerman;interpolated language models;language models interpolated;hierarchical phrase based;german english systems;net language models;language models;english systems;english systems based;models interpolated language;systems englishgerman german;language models compound;systems englishgerman;based phrasebased pre;neural net language;ntt naist smt;risk combination smt;splitting german english;phrase based phrasebased;interpolated language;recurrent neural net;phrasebased pre ordering;smt systems forest;string hierarchical phrase;iwslt 2013 evaluation;naist smt;2013 evaluation campaign;hierarchical phrase;word splitting german;word splitting", "pdf_keywords": ""}, "cf2a953dc82115d34de51737fef46bf3ff4cd5a6": {"ta_keywords": "chime speech separation;speech separation recognition;speech separation;chime speech;2nd chime speech;overview 2nd chime;separation recognition challenge;separation recognition;2nd chime;chime;separation;speech;recognition challenge;recognition;overview 2nd;overview;challenge;2nd", "pdf_keywords": ""}, "c44addf352f25f28f69ca9f9422c0e463783206f": {"ta_keywords": "synonym extraction framework;word synonym extraction;synonym extraction;word similarity measure;parsed text corpus;text corpus;text corpus instance;text corpora;word similarity;similarity measures parsed;syntactic relations graph;dependency parsed text;corpus;walk based similarity;corpus instance labeled;parsed text empirical;term extraction;specific word similarity;text empirical evaluation;words weighted directed;text empirical;measures parsed text;corpus instance;nodes represent words;represent words weighted;size text corpora;information graph walk;based similarity measures;term extraction general;words weighted", "pdf_keywords": ""}, "9c16dcbcdfe6991f5d448543e6f4cbdf37149883": {"ta_keywords": "multimodal model learn;does multimodal model;does multimodal;multimodal model;multimodal;multimodally additive;empirical multimodally additive;modal interactions improve;modal feature interactions;cross modal interactions;multimodally additive function;empirical multimodally;task does multimodal;cross modal feature;learn cross modal;modal interactions results;multimodally;predictions cross modal;tool empirical multimodally;modal interactions;cross modal;modal interactions eliminated;modal interactions seven;modal feature;isolating cross modal;feature interactions harder;interactions improve performance;additive unimodal structure;additive unimodal;interactions outperform expressive", "pdf_keywords": "multimodal classi\ufb01cation models;analyzing multimodal classi\ufb01cation;multimodal models;baselines multimodal models;analyzing multimodal;multimodal model learn;multimodal models consider;multimodal classi\ufb01cation tasks;multimodal model;does multimodal model;performing multimodal model;multimodal classi\ufb01cation;diagnostic analyzing multimodal;multimodal model recommendations;critically empirical multimodally;empirical multimodally additive;unimodal baselines multimodal;multimodal;does multimodal;work multimodal classi\ufb01cation;performing multimodal;best performing multimodal;baselines multimodal;propose empirical multimodally;empirical multimodally;multimodally additive projection;work multimodal;multimodally additive;empirical multimodally additive1;future work multimodal"}, "335bf6f23ccdae43e45a7c12f33bc4f3488e3762": {"ta_keywords": "neural machine translation;multi source translation;trained multilingual corpora;machine translation;incomplete corpora multilingual;machine translation nmt;corpora multilingual translations;multilingual corpora complete;translation single source;translate incomplete corpora;machine translation rife;incomplete corpora training;practice multilingual corpora;translation nmt learn;corpora multilingual corpora;source translation single;methods trained multilingual;multilingual corpora;corpora multilingual;language sentence training;trained multilingual;learn translate incomplete;multilingual corpora multilingual;multilingual translations;multilingual corpora include;missing source translations;source translations;incomplete corpora;parts source translations;nmt learn translate", "pdf_keywords": ""}, "82459c972cc1e439c759010acf7ddce1a89b66e0": {"ta_keywords": "fuzzy graph clustering;graph unsupervised semantic;synonymy graph unsupervised;global graph clustering;graph clustering widely;watset meta algorithm;graph clustering;distributional thesaurus watset;induction synonymy graph;unsupervised semantic class;unsupervised semantic;thesaurus watset;thesaurus watset local;graph clustering applications;meta algorithm fuzzy;unsupervised semantic frame;semantic frame induction;semantic class induction;clustering widely;sense frame induction;graph unsupervised;watset meta;synonymy graph;analysis watset meta;unsupervised synset induction;clustering;algorithm fuzzy graph;fuzzy graph;synset induction synonymy;triples unsupervised semantic", "pdf_keywords": "fuzzy graph clustering;global graph clustering;graphs linguistic;graphs linguistic data;graph clustering widely;globally cluster disambiguated;graph clustering;semantic classes graph;distributionally related words;cluster disambiguated symbols;known distributional thesaurus;processing graphs linguistic;cluster disambiguated;graph clustering applications;distributional thesaurus;clustering widely;present new clustering;words known distributional;distributional thesaurus dt;linguistic data;semantic classes;clustering;new clustering;clustering widely applicable;watset meta algorithm;globally cluster;subsequently globally cluster;thesaurus;classes graph distributionally;semantic"}, "684821e2459c7fc3ef8a2ec8102678af3613a962": {"ta_keywords": "speech understanding performance;trained speech representations;representation learning speech;speech representations superb;pre trained speech;speech tasks;downstream speech applications;trained speech;speech tasks limited;learning speech processing;multiple speech tasks;speech representations;downstream speech;learning speech;superb speech understanding;speech applications results;speech understanding;self supervised learning;introduce speech understanding;range downstream speech;available self supervised;speech processing community;self supervised models;self supervised;speech processing;representations superb speech;speech applications;specialized prediction heads;using self supervised;specialized prediction", "pdf_keywords": ""}, "2467b2daea0398709d7ea57d084cc1f00f9d168f": {"ta_keywords": "conditional speaker features;estimated conditional speaker;classification ctc encoder;speaker inferred using;speaker features;temporal classification ctc;input mixture speech;speaker features adopt;connectionist temporal classification;speaker inferred;output speaker inferred;speech previously estimated;ctc encoder;variable numbers speakers;conditional speaker;specifically output speaker;outperforms nar models;latency achieving wers;speaker;mixture speech;classification ctc;ctc encoder used;mixed speakers;latency obtaining wers;mixture speech previously;number mixed speakers;model outperforms nar;nar connectionist temporal;numbers speakers model;nar connectionist", "pdf_keywords": "nar multi speaker;multi speaker asr;conditional speaker chain;speaker chain based;speaker asr;models conditional speaker;speaker asr improved;speaker chain module;speaker chain condchain;speaker asr study;compare conditional speaker;proposed conditional speaker;speaker chain;conditional speaker;includes conditional speaker;nar models conditional;model nar multi;multi speaker;consists conditional speaker;nar models;chain model nar;ar nar models;model nar;nar multi;ctc based encoders;chain based transformer;conformer ctc encoders;speaker;method nar multi;ctc encoders"}, "a1588ac6d582d30742f998464500bb5ead125dc6": {"ta_keywords": "regretnet new neural;deep learning regret;regretnet;controlling revenue regret;auctions attention;revenue regret trade;learning regret based;modifications regretnet;revenue regret;learning regret;auctions attention regarding;regretnet new;independent modifications regretnet;modifications regretnet new;er auctions attention;bidding truthfully optimal;benefit bidding;benefit bidding truthfully;incentive;regret trade;bidding;regret trade varying;neural;alternative loss;participants benefit bidding;auctions;quantify incentive;regret;relax quantify incentive;attention regarding loss", "pdf_keywords": "regretnet new neural;deep learning regret;regretnet;auctions attention;regretnet recent breakthrough;maximizing auctions;auctions attention dmitry;revenue maximizing auctions;maximizing auctions propose;er auctions attention;regretformer alternative loss;regretnet new;learning regret based;modifications regretnet;regretnet recent;auctions;learning regret;usa regretnet;benefit bidding;optimal er auctions;er auctions;regretformer;regretformer alternative;benefit bidding truthfully;bidding;regretformer consistently outperforms;auctions propose;modifications regretnet new;independent modifications regretnet;experiments regretformer"}, "1c682dca13e47e6e1ee3c8db54af631a8e5e5792": {"ta_keywords": "mdp exploiting spectral;graph signal processing;spectral decomposition;algorithms direct spectral;spectral decomposition outer;direct spectral decomposition;exploiting spectral;computing optimal policy;probability transition matrices;transition matrices ptm;proposed computing optimal;numerical results wireless;spectral techniques appropriate;spectral techniques;policy iteration optimal;definite psd matrix;signal processing methods;cost mdp;iteration optimal policy;exploiting spectral properties;computing optimal;graph signal;psd matrix;matrices ptm;spectral properties probability;use spectral;discounted cost mdp;use spectral techniques;matrix generated ptm;signal processing", "pdf_keywords": ""}, "48e32ba9a891f36183a26f35316e8906d14d83c0": {"ta_keywords": "crowdsourcing computational quality;crowdsourcing computational;control algorithms crowdsourcing;purpose crowdsourcing computational;parameterizing crowdsourcing;algorithms crowdsourcing;parameterizing crowdsourcing process;algorithms crowdsourcing including;crowdsourcing including uncertainty;crowdsourcing;crowdsourcing including;crowdsourcing process;crowdsourcing process workers;allow parameterizing crowdsourcing;measures crowd consensus;purpose crowdsourcing;quality control toolkit;general purpose crowdsourcing;crowd consensus methods;crowd consensus;python computational quality;computational quality control;crowd kit;evaluation toolkit datasets;benchmarking computational quality;evaluation toolkit;quality control algorithms;major annotation tasks;toolkit python;annotation tasks", "pdf_keywords": "crowdsourcing computational quality;crowdsourcing computational;purpose crowdsourcing computational;crowdsourcing;crux crowdsourcing;control crux crowdsourcing;major annotation tasks;general purpose crowdsourcing;annotation tasks;annotation tasks categorical;purpose crowdsourcing;quality control toolkit;crux crowdsourcing paper;crowdsourcing paper;categorical annotation latent;tasks categorical annotation;major annotation;evaluation toolkit datasets;annotation latent;categorical annotation;annotation latent label;computational quality control;evaluation toolkit;crowdsourcing paper demonstrate;methods major annotation;extensive evaluation toolkit;annotation;benchmarking computational quality;toolkit datasets;toolkit"}, "d4756d1a7b81f53e71f939ab387cad5f0a4a13b7": {"ta_keywords": "memory lstm polyphonic;lstm polyphonic;lstm polyphonic sound;duration controlled lstm;controlled lstm polyphonic;duration sound event;activity detection speech;sound event detection;sound event activity;lstm recurrent;lstm;segments sound event;voice activity detection;term memory lstm;lstm recurrent neural;memory lstm;bidirectional lstm recurrent;sound event insertion;polyphonic sound event;short term memory;voice activity;duration sound;detection speech;sound activity detection;recurrent neural network;bidirectional lstm;sound activity;controlled lstm;speech recognition;sequence sequence detection", "pdf_keywords": ""}, "19418493b1f9c82809fe4584af427b8807b8ae2d": {"ta_keywords": "purpose relation classifier;relation classifier;sentences locatednear relation;automatically extract relationship;000 sentences annotated;extract relationship sentence;entity pairs;sentence level classifier;sentences annotated mentioned;sentences annotated;relationship sentence level;mentioned entity pair;vision natural language;annotated mentioned entity;relation describes typically;relation describes;entity pair locatednear;natural language understanding;extract relationship;natural language;sentences locatednear;entity pair;relation;containing 000 sentences;pair locatednear relation;locatednear relation describes;relationship sentence;located objects;language understanding machine;number sentences locatednear", "pdf_keywords": "objects textual corpora;relation textual corpora;locatednear relation extraction;textual corpora;relation extraction;textual corpora additionally;sentence level relation;automatically extract relationship;relation extraction systems;objects human annotated;objects textual;extract relationship sentence;large corpus;pairs large corpus;sentences describing scene;000 sentences describing;extract commonsense locatednear;textual corpora propose;physical objects textual;relation textual;locatednear relation textual;entity pairs;human annotated;corpus;novel tasks extracting;objects located scene;tasks 000 sentences;human annotated scores;commonsense locatednear relation;000 sentences"}, "298ddceada580c46e40e2a0323c0e3b16ed5f3c9": {"ta_keywords": "incongruity detection asr;outputs based eeg;eeg signals;incongruity detection;detection asr outputs;based eeg signals;detection asr;eeg;based eeg;asr outputs based;asr outputs;incongruity;asr;signals;detection;outputs based;outputs;based", "pdf_keywords": ""}, "562fbb5d706d46f3e250429ac48e6acd2bf18cb1": {"ta_keywords": "enhancement speech separation;speech enhancement separation;speech separation systems;end speech enhancement;downstream speech recognition;speech separation;speech enhancement;speech enhancement speech;speech recognition module;development speech enhancement;enhancement speech;downstream speech;rich automatic speech;speech recognition;enhancement separation toolkit;automatic speech;automatic speech recognition;recognition module espnet;optional downstream speech;separation toolkit;separation toolkit designed;speech recognition related;denoising source separation;datasets espnet se;datasets espnet;benchmark datasets espnet;source separation;toolkit designed asr;enhancement separation;processing single channel", "pdf_keywords": "speech processing toolkit;speech separation enhancement;source speech separation;speech enhancement separation;source speech processing;audio source separation;speech separation;open source speech;speech data;speech enhancement;speech recognition integration;speech data including;audio source;speech processing;different speech data;speech recognition;source separation library;separation enhancement library;audio;asteroid audio source;art speech enhancement;separation library;source speech;especially speech recognition;source toolkit espnet;espnet toolkit;source separation;11 asteroid audio;toolkit espnet toolkit;asteroid audio"}, "2dbe78aa516cc911a71ff333a35a5ce0b1a49640": {"ta_keywords": "speech recognition asr;asr automatic speech;recognition asr models;automatic speech;asr models;automatic speech recognition;recognition asr;speech recognition;multi mode asr;streaming baselines trained;asr propose stochastic;asr automatic;higher accuracy latency;latency budget flexible;trained different latency;accuracy latency budget;mode asr propose;speech information;asr models make;asr model;fulfill various latency;mode asr;asr model fulfill;inference larger latency;latency based;latency budgets multi;latency requirements;latency requirements inference;mode asr model;various latency requirements", "pdf_keywords": "practical streaming asr;streaming asr;baseline streaming models;streaming models trained;streaming asr pursuit;streaming models;context mode streaming;streaming baselines trained;competitive streaming baselines;baseline streaming;streaming context;mode asr training;mode streaming;streaming mode;streaming baselines;streaming;streaming con\ufb01guration;streaming mode speci\ufb01ed;modes streaming context;18 baseline streaming;samples streaming con\ufb01guration;practical streaming;modes streaming;multi mode asr;different modes streaming;mode streaming mode;asr training method;streaming con\ufb01guration iteration;dual mode asr;streaming context 18"}, "0dc379de3a613110c5fdc9c0361372c1114ee18d": {"ta_keywords": "kev extracted beam;kev sup beams;conversion ion source;ion source built;beam transport lebt;alamos neutron;alamos neutron scattering;energy beam transport;low energy beam;lansce proton storage;beam transport;conversion ion;pop ion source;ion source new;proton storage ring;beam emittance 80;ion source operated;proton storage;sup injector development;ion source;sup beams enable;los alamos neutron;surface conversion ion;energy beam;80 kev extracted;psr pop ion;lansce proton;ma design beam;sup beams;80 kev sup", "pdf_keywords": ""}, "c6c6b4d328381a530e933c208bb43db2a7fa93c8": {"ta_keywords": "semilocal domains altitude;semilocal domains;domains altitude;semilocal;altitude;domains", "pdf_keywords": ""}, "7225c2a42990f850f692f8d82e7f3bfaf312145c": {"ta_keywords": "neural machine translation;learning framework nmt;curriculum learning neural;performance recurrent neural;neural networks slow;curriculum learning;nmt reduces training;performance recurrent;propose curriculum learning;large neural networks;use large neural;sampling training;improve training time;uniformly sampling training;machine translation;state art nmt;curriculum learning framework;filtering training;sampling training examples;filtering training samples;time performance recurrent;based curriculum learning;reduces training time;training samples manner;training examples;decrease training time;training samples;learning rate schedules;tricks specialized learning;specialized learning rate", "pdf_keywords": "training nmt systems;curriculum learning neural;learning framework nmt;curriculum learning training;framework training nmt;curriculum learning;neural machine translation;training nmt;large neural networks;propose curriculum learning;neural networks slow;use large neural;proposed curriculum learning;curriculum learning framework;nmt reduces training;curriculum learning method;training neural machine;specialized learning rate;curriculum learning approach;based curriculum learning;heuristics large batch;proposing curriculum learning;specialized learning;learning training;reduces training time;large neural;tricks specialized learning;learning rate schedules;training neural;learning training framework"}, "4a76869cda286efb20eb78cc6adb13daab37a0d1": {"ta_keywords": "simulations fokker planck;particle fokker planck;fokker planck equations;fokker planck solutions;interacting particle fokker;simulations comparable particle;stochastic simulations comparable;particle density approximated;evolution fokker planck;stochastic simulations;direct stochastic simulations;particle fokker;simulations fokker;planck equations extensively;fokker planck;based simulations fokker;particle based simulations;logarithm particle density;planck equations;particle density;planck solutions;planck equations low;density functions interactions;time evolution fokker;stochastic systems level;comparable particle number;planck solutions terms;stochastic;stochastic systems;particle number", "pdf_keywords": "simulations fokker planck;stochastic simulations particle;fokker planck equations;fokker planck solutions;evolution fokker planck;simulations fokker;stochastic simulations;simulations particle number;based simulations fokker;simulations particle;particle based simulations;compared stochastic simulations;fokker planck;time evolution fokker;simulating time evolution;planck equations;planck solutions;planck equations low;\ufb02uctuations compared stochastic;cumulant trajectories;reliable particle based;approach simulating time;planck solutions terms;small particle numbers;leibler kl divergence;distances distribution cumulants;conservative model systems;evolution fokker;simulating time;computational approach simulating"}, "674f892caa52fa400109defa1773a10088918124": {"ta_keywords": "model predictive control;predictive control mpc;horizon optimal control;predictive control;finite horizon optimal;predictive control instead;primal dual methods;primal dual method;optimal control;augmented lagrangian iteration;minimizing augmented lagrangian;convex iteration;horizon optimal;optimal control problem;integral projected gradient;gradient method mpc;increasing primal dual;mpc require minimizing;primal dual;function convex iteration;lagrangian iteration;method model predictive;projected gradient method;method mpc underlying;input constraints method;lagrangian iteration method;control mpc;state input constraints;convex iteration number;model predictive", "pdf_keywords": ""}, "cc549a11d277d86f6228443cb16c231c9bda6c96": {"ta_keywords": "improving emphasis estimation;emphasis modeling based;utterances modeling emphasis;emphasis estimation measure;clustering emphasis contextual;emphasis estimation;emphasis modeling;level emphasis modeling;clustering emphasis;hybrid improving emphasis;combines emphasis contextual;focus utterances modeling;improving emphasis;modeling emphasis active;word level emphasis;emphasis contextual;modeling emphasis;adaptive training emphasis;hmm state clustering;state clustering emphasis;focus utterances;modeling based hmm;utterances modeling;emphasis contextual factor;state clustering adaptive;combines emphasis;clustering adaptive training;indicating word emphasized;speech conveys focus;emphasis using state", "pdf_keywords": ""}, "c1546da843be7ea3e0adfb85b69a0b08d41c7159": {"ta_keywords": "dissertation template \u043f\u0440\u0438\u043c\u0435\u0440\u044b;russian phd latex;latex dissertation template;template \u043f\u0440\u0438\u043c\u0435\u0440\u044b \u043a\u043e\u043c\u043f\u0438\u043b\u044f\u0446\u0438\u0438;phd latex dissertation;template \u043f\u0440\u0438\u043c\u0435\u0440\u044b;latex dissertation;dissertation template;russian phd;phd latex;dissertation;\u043a\u043e\u043c\u043f\u0438\u043b\u044f\u0446\u0438\u0438 \u0448\u0430\u0431\u043b\u043e\u043d\u0430 \u0440\u0430\u0437\u043d\u044b\u0445;\u043a\u043e\u043c\u043f\u0438\u043b\u044f\u0446\u0438\u0438 \u0448\u0430\u0431\u043b\u043e\u043d\u0430;\u043f\u0440\u0438\u043c\u0435\u0440\u044b \u043a\u043e\u043c\u043f\u0438\u043b\u044f\u0446\u0438\u0438 \u0448\u0430\u0431\u043b\u043e\u043d\u0430;\u043f\u0440\u0438\u043c\u0435\u0440\u044b \u043a\u043e\u043c\u043f\u0438\u043b\u044f\u0446\u0438\u0438;\u043a\u043e\u043c\u043f\u0438\u043b\u044f\u0446\u0438\u0438;\u0448\u0430\u0431\u043b\u043e\u043d\u0430 \u0440\u0430\u0437\u043d\u044b\u0445 \u0440\u0435\u0436\u0438\u043c\u0430\u0445;\u043f\u0440\u0438\u043c\u0435\u0440\u044b;\u0440\u0430\u0437\u043d\u044b\u0445;phd;template;\u0448\u0430\u0431\u043b\u043e\u043d\u0430 \u0440\u0430\u0437\u043d\u044b\u0445;\u0440\u0430\u0437\u043d\u044b\u0445 \u0440\u0435\u0436\u0438\u043c\u0430\u0445;\u0440\u0435\u0436\u0438\u043c\u0430\u0445;russian;latex;\u0448\u0430\u0431\u043b\u043e\u043d\u0430", "pdf_keywords": ""}, "5b3ca06a7673e2bf372d5f89afb15ae1eb714075": {"ta_keywords": "time bayesian network;bayesian network ectbn;bayesian network;structure learning;structure learning based;model paths poverty;time bayesian;ectbn representation model;continuous time bayesian;causal dynamics evolution;procedure structure learning;novel event driven;poverty clients citylink;greedy search;event driven;paths poverty clients;learning based bic;network ectbn representation;graphical structure;causal dynamics;propose greedy search;model situations state;influence event occurrences;event driven continuous;model situations;greedy search procedure;event occurrences interventions;poverty clients;influenced occurrences events;bayesian", "pdf_keywords": ""}, "594827fdb2047bc7be4ea2f0d2364f46d187247e": {"ta_keywords": "japanese speech corpus;corpus japanese speech;youtube speech recognition;jtubespeech corpus japanese;jtubespeech corpus;corpus youtube;corpus japanese;corpus youtube videos;construction corpus youtube;speech corpus;subtitles speech recognition;established jtubespeech corpus;speech collected youtube;japanese speech collected;collected youtube speech;videos subtitles speech;largesize speech corpora;corpus called jtubespeech;youtube speech;japanese asr benchmark;new japanese speech;japanese speech;speech recognition speaker;corpus;data japanese asv;speech corpus called;recognition speaker verification;subtitles speech;speech corpora open;speech corpora", "pdf_keywords": "japanese speech corpus;speech corpus jtubespeech;corpus japanese speech;asv jtubespeech corpus;jtubespeech corpus japanese;youtube speech recognition;jtubespeech corpus;speech corpus;corpus japanese;propose speech corpus;speech corpus asr;construct speech corpus;construct japanese speech;japanese speech collected;speech corpus construction;corpus jtubespeech;speech recognition speaker;corpus jtubespeech using;construction corpus youtube;corpus youtube;speech corpus called;automatic speech;largesize speech corpora;speech recognition asr;corpus called jtubespeech;new japanese speech;japanese speech;subtitles speech recognition;speech collected youtube;speech recognition"}, "539631a828bf0badd20d2241784b4e06c223250e": {"ta_keywords": "mixture model speaker;model speaker clustering;modeling speech gmm;scale mixture models;mixture models;speaker modeling speech;speaker clustering;gaussian mixture model;mixture model multi;speaker modeling;mixture model gmm;modeling speech;noise mixture gmms;scale mixture model;speech gmm;intra speaker dynamics;mixture model;mixture models assume;speaker clustering noisy;speaker dynamics derived;inter speaker dynamics;speaker dynamics;multi scale mixture;mixture gmms mogmms;mixture gmms;gaussian mixture;mixture represented gaussian;represented gaussian mixture;noise mixture;gibbs sampling based", "pdf_keywords": ""}, "d5181375d242ed181bcde0d682a3c7ec4c4c6102": {"ta_keywords": "cognitive skills autistic;skills position autism;skills autistic conditions;skills autistic;verbal cognitive skills;autism spectrum;cognitive skills members;position autism spectrum;autism spectrum members;non verbal cognitive;cognitive skills;position autism;skills communication;social skills communication;autism;cognitive skills position;social skills;skills communication greater;autistic conditions;autistic conditions analysis;verbal cognitive;enhance non verbal;trouble social skills;non verbal;learning results examined;pre post learning;autistic;examined effects training;training help enhance;skills members", "pdf_keywords": ""}, "562fe9b2f5e7ede128dd9a93edc3971c5e0a2394": {"ta_keywords": "artificial dialog acts;dialog acts word;dialog acts;artificial dialog;generate artificial dialog;dialog act effect;word generative model;interlocutors polylogue cache;dialog act;dialog acts improve;polylogue cache model;polylogue cache;usage dialog acts;dialog acts guarantees;model dialog act;word generative;effect dialog acts;accuracy word generative;influence interlocutors polylogue;word use polylogue;improve word prediction;dialog;word prediction;improve word generative;dialog act information;words influence model;word prediction accuracy;knowledge following dialog;interlocutors polylogue;influence model dialog", "pdf_keywords": ""}, "05710169c48ac1ffe6af514cc10e72d025023343": {"ta_keywords": "learning nonlocal models;driven learning nonlocal;nonlocal models;learning nonlocal;nonlocal models high;data driven learning;driven learning;nonlocal;models high fidelity;simulations constitutive laws;fidelity simulations constitutive;data driven;simulations constitutive;fidelity simulations;models;high fidelity simulations;learning;simulations;models high;constitutive laws;data;high fidelity;driven;constitutive;laws;fidelity;high", "pdf_keywords": ""}, "14a6452d6d026a3f384e425add6ab68f8e65037f": {"ta_keywords": "data labelling crowdsourcing;labelling crowdsourcing;data collection crowdsourcing;labelling crowdsourcing shared;crowdsourcing aggregation;labelling public crowdsourcing;collection crowdsourcing;crowdsourcing marketplaces present;crowdsourcing aggregation incremental;crowdsourcing marketplaces;collection crowdsourcing aggregation;public crowdsourcing marketplaces;crowdsourcing;efficient data labelling;largest crowdsourcing marketplaces;crowdsourcing marketplaces practice;largest crowdsourcing;crowdsourcing shared;public crowdsourcing;introduction data labelling;efficient label collection;crowdsourcing shared leading;data labelling;efficiently collect labelled;collect labelled data;data labelling public;efficient label;label collection project;labelling;label collection", "pdf_keywords": ""}, "ea1ba6f5e5852e38ace7bbae4e4f60ffeeabe5b1": {"ta_keywords": "sustained microfracturing produced;formation hydraulic fractures;sustained microfracturing;microfracturing produced formation;microfracturing produced;classes sustained microfracturing;hydraulic fractures;microfracturing;produced formation hydraulic;formation hydraulic;fractures;produced formation;hydraulic;formation;classes sustained;sustained;produced;classes", "pdf_keywords": ""}, "70a2a554829f2cebb9fa89829994444fa1ec5a7b": {"ta_keywords": "voting cp nets;consensus distance cp;consensus distance;sequential plurality voting;cp nets formalism;reach consensus distance;distance rationalizability sequential;plurality voting cp;preferences reach collective;distance cp nets;conditional qualitative preferences;qualitative preferences able;reach consensus;aggregate preferences;cp nets;rationalizability sequential plurality;notion distance cp;aggregate preferences reach;tractable approximations distance;features cp nets;voting cp;distance distance rationalizability;cp nets relate;distance preference individuals;nets formalism compactly;distance rationalizability;cp nets multi;know aggregate preferences;induced cp nets;nets formalism", "pdf_keywords": ""}, "39c5740304b5f4072f92e4e012a4b57e7bc2e817": {"ta_keywords": "separation speech enhancement;channel speech separation;speech separation studies;speech enhancement;speech separation;speech enhancement techniques;speech separation speech;single channel speech;speaker verification based;separation speech;speaker verification;evaluate systems speaker;systems speaker verification;channel speech;metrics signal quality;evaluation single channel;systems speaker;diversity evaluation metrics;signal quality;majority deep learning;speaker;evaluation metrics;deep learning;separation studies;diversity evaluation;based single channel;ground truth waveform;enhancement;separation;single channel", "pdf_keywords": ""}, "470fd4faf9b8499e8bf21c5d143145305d07fe83": {"ta_keywords": "geocoded tweets linked;entity linking tweets;linking tweets space;non geocoded tweets;tweets space time;geocoded tweets;time geocoded tweets;tweets space;linking tweets;tweets linked common;tweets close space;sparse tweets;tweets linked;geocoded tweets overly;sparse tweets use;linking tweets close;collective entity linking;overly sparse tweets;proximity collective entity;temporal proximity collective;tweets close;tweets overly sparse;tweets use;tweets;entity linking;tweets overly;linked common mentions;spatio temporal proximity;entities mentioned spatio;proximity collective", "pdf_keywords": ""}, "5667f934c5bc008d0464878729eed34cbf7ec1df": {"ta_keywords": "relation extraction;relation extraction experimental;modeling semi supervised;semi supervised learning;learning constraints unlabeled;supervised learning constraints;semi supervised;constraints unlabeled data;natural semisupervised learning;task relation extraction;learning constraints;semisupervised learning;constraints unlabeled;tasks natural semisupervised;semisupervised learning heuristics;natural semisupervised;walks graph classifiers;supervised learning;unlabeled data;supervised classification tasks;classification tasks natural;supervised classification;graph classifiers;graph classifiers propose;classification tasks;supervised;traditional supervised classification;semisupervised;results modeled constraints;classification", "pdf_keywords": ""}, "2f1743d1a1be46452ab90691ead8bf916ffd912b": {"ta_keywords": "probabilistic enhancement eeg;enhancement eeg component;enhancement eeg;probabilistic enhancement;eeg component;eeg component using;prior distribution correlations;eeg;distribution correlations channels;correlations channels;component using prior;prior distribution;enhancement;distribution correlations;using prior distribution;using prior;probabilistic;prior;correlations;channels;component;distribution;component using;using", "pdf_keywords": ""}, "402ab2adcf9da95e6aad9884b1ec53271f39cd32": {"ta_keywords": "inverse kalman filter;adversary kalman filter;formulations inverse kalman;inverse kalman;inverse filtering bayesian;inverse filtering;inverse extended kalman;estimating adversary kalman;research inverse filtering;inverse filters;proposed inverse filters;adversary kalman;filter proposing inverse;inverse filters using;inverse filtering address;extended kalman filter;context inverse filtering;kalman filter;nonlinearity forward inverse;forward inverse state;kalman filter ekf;kalman filter ikf;kalman filter tracked;advances counter adversarial;formulations inverse;estimating adversary;inverse state space;counter adversarial systems;adversarial systems;counter adversarial", "pdf_keywords": "stochastic \ufb01ltering inverse;inverse extended kalman;inverse stochastic \ufb01ltering;extended kalman \ufb01lter;kalman \ufb01lter;adversarial denoting inverse;studied inverse \ufb01ltering;\ufb01ltering inverse;inverse \ufb01ltering;forward inverse state;inverse stochastic;\ufb01ltering inverse cognition;proposed inverse \ufb01lters;inverse \ufb01ltering problem;inverse \ufb01lter;denoting inverse \ufb01lter;inverse \ufb01lters using;kalman \ufb01lter ekf;inverse state space;nonlinearity forward inverse;inverse state;inverse \ufb01lters;inverse \ufb01lter step;\ufb01lter proposing inverse;focus inverse stochastic;extended kalman;inverse cognition applications;inverse cognition;inverse \ufb01ltering address;forward inverse"}, "f82ae0a87cae2f3a43d4c0289d0cdf7ca57461d0": {"ta_keywords": "\u6587\u732e\u60c5\u5831 global \u79d1\u5b66\u6280\u8853\u7dcf\u5408\u30ea\u30f3\u30af\u30bb\u30f3\u30bf\u30fc;global \u79d1\u5b66\u6280\u8853\u7dcf\u5408\u30ea\u30f3\u30af\u30bb\u30f3\u30bf\u30fc;\u30cf\u30fc\u30c9\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u7528\u3044\u305f\u6ce8\u610f\u578b\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u3088\u308b\u97f3\u58f0\u7ffb\u8a33 \u6587\u732e\u60c5\u5831 global;\u79d1\u5b66\u6280\u8853\u7dcf\u5408\u30ea\u30f3\u30af\u30bb\u30f3\u30bf\u30fc;\u30cf\u30fc\u30c9\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u7528\u3044\u305f\u6ce8\u610f\u578b\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u3088\u308b\u97f3\u58f0\u7ffb\u8a33 \u6587\u732e\u60c5\u5831;\u30cf\u30fc\u30c9\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u7528\u3044\u305f\u6ce8\u610f\u578b\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u3088\u308b\u97f3\u58f0\u7ffb\u8a33;\u6587\u732e\u60c5\u5831;\u6587\u732e\u60c5\u5831 global;global", "pdf_keywords": ""}, "217074971e3dfdbfab3a8c3819cd7953ae666da4": {"ta_keywords": "biomedical text mining;text mining deep;automatic biomedical text;lstm text mining;text mining training;text mining building;text mining answers;text mining;text mining flexibility;database deep reinforcement;articles knowledge automatic;efficiency biomedical text;learning massive knowledge;biomedical text;mining deep network;reliability text mining;knowledge automatic;mining answers;mining deep;mining answers chal;scientific literature biological;pubmed selecting articles;reliability biomedical text;massive knowledge collected;database deep;association database deep;knowledge discovered automatic;intelligent reader automatically;knowledge automatic human;behaviors querying pubmed", "pdf_keywords": ""}, "1b759204e7c13f0e4af9fe00b052af4456ac3669": {"ta_keywords": "skipping reinforcement learning;games reinforcement learning;reinforcement learning rl;games reinforcement;reinforcement learning;console games reinforcement;skipping reinforcement;frame skipping reinforcement;reinforcement;reinforcement learning clear;parameter states atari;atari console games;benefit atari;atari domain images;benefit atari console;atari;atari domain;states atari domain;atari console;states atari;sequential decision;indicate benefit atari;learning smaller task;sequential decision making;steps ignoring state;frame skip parameter;frame skip;skip parameter states;decision making agents;practice sequential decision", "pdf_keywords": "learning defensive play;action repetition allows;control action repetition;action repetition;demonstration learning defensive;action repetition focus;learning defensive;action sequences;parameter states atari;game prediction;explored game prediction;game prediction setting;loop action sequences;focus action repetition;demonstration learning;frame skipping actionrepetition;action sequences regular;action repetition common;learning succeed defense;defensive play soccer;atari;control action;length sequences action;defensive play;successful demonstration learning;repetition focus action;skipping actionrepetition rl;repetition allows td;states atari;sequences action"}, "c1125fa33a239ef4fd3378ccd46b2a0a0cf79a15": {"ta_keywords": "erasure coded storage;coded storage;storage overheads codes;storage fault tolerance;coded storage reduces;erasure coded data;data reconstruction erasure;io erasure codes;reconstruction erasure coded;lower storage overheads;data additional storage;data centers hitchhiker;new erasure coded;methods lower storage;erasure codes;storage;storage overheads;coded data centers;bandwidth disk io;lower storage;storage reduces network;erasure coded;disk io erasure;disk io;data replication;storage reduces;disk io reconstruction;traffic disk io;erasure codes reed;data replication methods", "pdf_keywords": ""}, "ac8d33e4c0a45e227a47353f3f26fbb231482dc1": {"ta_keywords": "language models temporal;models temporal knowledge;temporal data trained;models trained temporal;aware language models;time aware language;temporal knowledge bases;time models trained;temporal knowledge;trained temporal context;trained temporal;language models lms;facts training time;language models;temporal data;models temporal;temporal context efficiently;knowledge changes time;aware language;lms trained snapshots;periods language models;time aware;models lms trained;data time aware;temporal data time;memorization seen facts;lms factual knowledge;trained snapshots;temporal context;improves memorization", "pdf_keywords": "acquisition temporal knowledge;temporal data trained;temporal knowledge;temporal language model;pretraining temporally scoped;temporal language;temporally scoped knowledge;short temporal language;temporally scoped pretraining;scoped pretraining temporally;temporal shift knowledge;temporal knowledge conduct;temporal data summary;pretraining temporally;temporal data;acquisition temporal;new dataset temporally;knowledge changes time;time sensitive knowledge;scoped knowledge probes;knowledge probes;dataset temporally;dataset temporally scoped;temporally scoped;analysis temporal;knowledge probes propose;wide range temporal;temporal;improvements offered temporally;offered temporally scoped"}, "9b1057e1f6eb17abf3962d6cd2f49468d27b94c6": {"ta_keywords": "emphasis pause prediction;translation emphasis pause;pauses emphasizing speech;emphasis pause;pause prediction speech;machine translation emphasis;speech translation systems;pauses emphasizing;emphasis prediction;emphasis prediction measure;translation integrating pause;subjects emphasis prediction;pause prediction;translation emphasis active;improving translation emphasis;importance pauses emphasizing;pause prediction model;speech translation;integrating pause prediction;words prosodic emphasis;speech speech translation;emphasis translation;prosodic emphasis vital;translation emphasis;emphasis target language;emphasis cross lingual;proposed emphasis translation;emphasizing speech;prosodic emphasis;importance pauses", "pdf_keywords": ""}, "c985600f0aa223ddc76a2ea628f1fa23504dcbcd": {"ta_keywords": "target speech extraction;speech enhancement separation;target speech recognition;speech extraction module;extraction module speech;speech extraction;speech extraction specific;guided target speech;end speech recognition;speech extraction using;location anchor speech;speech recognition module;speech recognition asr;speech recognition objectives;module speech recognition;speech recognition;source separation auxiliary;terms speech enhancement;anchor speech input;speech enhancement;optimize source separation;promising target speech;automatic speech;automatic speech recognition;target speech;speech recognition results;end automatic speech;source separation;source separation systems;separation auxiliary information", "pdf_keywords": ""}, "0e141942fa265142f41a2a26eb17b6005d3af29e": {"ta_keywords": "linguistic diversity inclusion;linguistic diversity;representation nlp conferences;diversity inclusion nlp;languages world represented;evolving language technologies;languages world;inclusion nlp world;resources representation nlp;nlp conferences understand;languages resources representation;language technologies;nlp world;nlp conferences;languages;rapidly evolving language;nlp world small;different languages;7000 languages world;inclusion nlp;fate linguistic diversity;language technologies applications;disparity languages especially;linguistic;language;languages especially terms;evolving language;disparity languages;language agnostic status;representation nlp", "pdf_keywords": "representation nlp conferences;languages world broadly;nlp conferences understand;language disparity taxonomical;resource rich languages;languages resources representation;nlp conferences;languages suf\ufb01ciently resource;resources representation nlp;neglected classes languages;languages world;resources languages;languages world represented;disparity languages especially;languages;languages especially terms;conferences embeddings;disparity languages;conferences embeddings analysis;nlp technology;existing languages;classes languages;resource poor languages;language disparity;rapidly evolving language;evolving language technologies;languages class;\ufb01ndings language disparity;languages suf\ufb01ciently;languages resources"}, "683e201783bf76ab99791a02e3763fd3ab8dad96": {"ta_keywords": "entity recognition deep;named entity recognition;learning named entity;entity recognition;deep active learning;data deep active;deep learning active;named entity;active learning named;data deep;training data deep;learning named;manually labeling data;labeling data available;labeling data;active learning outperform;manually labeling;recognition deep;deep learning;result deep learning;deep active;deep learning employed;active learning;learning active learning;recognition deep neural;labeling;art named entity;deep neural networks;deep neural;entity", "pdf_keywords": "lstm architecture ner;lstm tag decoder;deep active learning;cnn lstm;lightweight deep neural;encoder convolutional word;dnn architectures ner;cnn lstm architecture;cnn cnn lstm;cnn lstm model;convolutional word level;learning sequence tagging;word encoders;memory lstm tag;term memory lstm;use deep active;lstm tag;algorithms lightweight deep;word level encoder;deep active;active learning sequence;character word encoders;ner consisting convolutional;short term memory;deep neural;propose new cnn;convolutional character word;lstm;memory lstm;word encoders long"}, "e318e554098224c9475dfc80765cbbb82fa4a409": {"ta_keywords": "ai collaboration pipeline;human ai collaboration;ai collaboration;ai ml conferences;conference artificial intelligence;collaboration pipeline peer;pipeline peer review;peer review;conference artificial;submissions leading ai;21 peer review;peer review mitigate;collaboration pipeline;aaai conference artificial;mechanism science dissemination;human ai;peer review backbone;science dissemination decades;science dissemination;science progresses fair;ai;artificial intelligence aaai;ensure science progresses;automated;collaboration;despite peer review;ml conferences challenged;pipeline peer;peer review primary;automated tools", "pdf_keywords": ""}, "8c9033f976e7787dde9af5ba952d7f9ac9c34496": {"ta_keywords": "xstream outlier detection;feature evolving streams;evolving data streams;xstream outlier;feature evolving stream;stream feature evolving;noise xstream outlier;ensemble outlier detector;evolving streams;outlier detection feature;based ensemble outlier;evolving streams exists;overhead evolving streams;ensemble outlier;evolving stream;data streams;stream feature;evolving streams studied;outlier detection;outlier detector;detection feature evolving;outlier detector called;evolving stream experiments;streams;streams exists;outlier;updates stream;updates stream progresses;feature evolving data;algorithm measures outlierness", "pdf_keywords": ""}, "36cbc3c24429ba3b69def38e6e64b41485b0a023": {"ta_keywords": "billion crawled urls;billion crawled;general web crawl;large web corpus;crawled;web crawl;crawl date billion;web corpus;corpus 10 billion;date billion crawled;web corpus 10;crawled urls;large web;tokens licensed creativecommons;billion tokens licensed;billion tokens;10 billion tokens;commoncrawl largest publicly;languages extracted commoncrawl;crawl;general web;licensed creativecommons license;50 languages extracted;creativecommons license;web crawl date;licensed creativecommons;corpus;crawl date;tokens licensed;extracted commoncrawl", "pdf_keywords": ""}, "119c33321fc0e1db837ce293f1b65cc26c1cc34e": {"ta_keywords": "sentence matching detecting;key sentences patterns;key sentence matching;sentences explanations experiments;claim key sentences;sentence matching;capture key sentences;key sentences explanations;semantics sentence templates;article reranking memory;predict article fact;sentences patterns;key sentences;providing lexical information;sentences explanations;key sentences selected;using key sentences;detecting previously fact;information article reranking;event lexical semantic;select key sentences;sentence templates;sentences selected event;events providing lexical;lexical information semantics;article fact checks;information semantics;sentences patterns paper;sentences;sentence templates introduce", "pdf_keywords": "detection fact checking;claim detection fact;factchecked claim detection;fact checking articles;article reranking memory;sentence matching detecting;detecting previously fact;sources article reranking;key sentence matching;fact checking;article reranking;propose novel reranker;sentence matching;claim detection;novel reranker;matching rank fc;reranking;reranking candidates event;novel reranker mtm;reranking memory;rank fc articles;dataset factchecked claim;factchecked claim;chinese dataset factchecked;matching rank;reranking memory enhanced;reranker;fact checked claims;event lexical semantic;reranker mtm memoryenhanced"}, "708dcd8456426cd609c89a86344e0007c04c80bf": {"ta_keywords": "multilingual factual knowledge;multilingual benchmark cloze;factr multilingual;multilingual benchmark;factr multilingual factual;located factr multilingual;languages assess factual;factual knowledge retrieval;multilingual factual;create multilingual benchmark;benchmark languages assess;queried languages;knowledge retrieval lms;retrieval pretrained language;multilingual lms;written queried languages;effectiveness benchmark languages;english language models;ability multilingual lms;benchmark languages;multilingual lms access;language models;language models lms;languages assess;language models propose;multilingual;diverse languages properly;pretrained language models;diverse languages;typologically diverse languages", "pdf_keywords": "crosslingual factual retrieval;multilingual benchmark probing;new multilingual benchmark;multilingual benchmark;multilingual benchmark cloze;multilinguality factual knowledge;lms crosslingual factual;knowledge lms crosslingual;create multilingual benchmark;multilinguality factual;factual retrieval benchmark;crosslingual factual;intersection multilinguality factual;multilingual lms;effectiveness benchmark languages;benchmark languages;multilingual lms access;ability multilingual lms;factual knowledge retrieval;multilinguality;retrieval benchmark factr;multilingual multi token;new multilingual;examine intersection multilinguality;knowledge retrieval lms;comparing contrasting languages;lms crosslingual;multilingual;multilingual multi;contrasting languages lms"}, "db0c587111cfed85dcea413e385b17881e6e0cbb": {"ta_keywords": "matrix adaptation evolution;improve speech recognition;short term memory;recurrent neural network;matrix adaptation;term memory lstm;parameter tuning neural;speech recognition performance;speech recognition;tuning neural network;evolution strategy cma;memory lstm recurrent;tuning neural;recurrent neural;structure discovery parameter;covariance matrix adaptation;adaptation evolution strategy;lstm recurrent neural;language models known;neural network language;memory lstm;language models;discovery parameter tuning;language model;improve speech;lstm recurrent;term memory;structure discovery;hyper parameter optimization;lstm", "pdf_keywords": ""}, "3b30422b372040ad19a713b35006c21808287720": {"ta_keywords": "erasure codes storage;optimal erasure codes;bandwidth erasure codes;codes storage network;codes storage;erasure codes;minimum storage regeneration;storage reliability;storage reliability rs;optimal erasure;tolerance distributed storage;storage reliability network;optimality respect storage;erasure codes reed;respect storage reliability;erasure codes called;distributed storage;optimal respect storage;storage systems;distributed storage systems;storage network;called minimum storage;bandwidth erasure;minimum storage;storage regeneration;storage;storage regeneration msr;jointly optimal erasure;respect storage;class erasure codes", "pdf_keywords": ""}, "6fa7de6f3ce3a599de6fab273a0d43939e176e9d": {"ta_keywords": "method navigation agent;navigation agent achieves;human assisted navigation;assisted navigation;navigation agent;assisted navigation problem;learned method navigation;learning assistance requesting;learning assistance;autonomous behavior agent;fully autonomous behavior;simulated human assisted;useful information humans;learning request rich;policy learned method;autonomous behavior;navigation;agent determine request;useful information assistant;enables agent determine;aided assistance requesting;request contextually useful;agent achieves;aided assistance;behavior agent;assistant recursively;policy assistant recursively;method navigation;learning request;assistant", "pdf_keywords": "human assisted navigation;assistance communication;agent elicit information;assistance communication speaker;learning assistance requesting;human assistance communication;elicit information humans;teaching agent elicit;learning assistance;assisted navigation;useful information assistant;pomdp based interactive;assisted navigation problem;simulated human assisted;general interactive;interactive;assistant;assistant recursively;teaching agent;information humans;human assistance;based interactive;present general interactive;information humans improve;framework allows agents;human assisted;request contextually useful;challenges learning assistance;policy assistant recursively;enables agent determine"}, "48e4ba2d04bd98843d5aab6e227b29584d63f7b6": {"ta_keywords": "crowdsourcing cooperation linguistic;crowdsourcing taxonomies cooperation;crowdsourcing example linguistic;crowdsourcing taxonomies;crowdsourcing cooperation;involved crowdsourcing cooperation;resource created crowdsourcing;russian linguistic resource;survey crowdsourcing taxonomies;cooperation linguistic resources;created crowdsourcing;people involved crowdsourcing;linguistic resources populated;survey crowdsourcing;involved crowdsourcing;crowdsourcing;crowdsourcing evidence;taxonomies cooperation linguistic;linguistic resource created;genres crowdsourcing evidence;crowdsourcing gamification;existent genres crowdsourcing;genres crowdsourcing;crowdsourcing example;popular russian linguistic;russian linguistic;created crowdsourcing example;crowdsourcing gamification motivated;crowdsourcing evidence efficiency;approaches crowdsourcing", "pdf_keywords": "crowdsourcing cooperation linguistic;crowdsourcing taxonomies cooperation;crowdsourced linguistic;activity crowdsourced linguistic;crowdsourced linguistic resources;crowdsourcing cooperation;crowdsourcing taxonomies;resource created crowdsourcing;2017 crowdsourcing cooperation;cooperation linguistic resources;taxonomies cooperation linguistic;gami\ufb01cation crowdsourcing cooperation;russian linguistic resource;activity crowdsourced;crowdsourcing use team;user activity crowdsourced;survey crowdsourcing taxonomies;crowdsourcing provides;crowdsourcing use;created crowdsourcing;crowdsourcing evidence;created crowdsourcing use;crowdsourced;crowdsourcing provides evidence;presents survey crowdsourcing;crowdsourcing;crowdsourcing evidence e\ufb03ciency;survey crowdsourcing;cooperation linguistic;linguistic resource created"}, "43a87867fe6bf4eb920f97fc753be4b727308923": {"ta_keywords": "efficient transfer learning;text classification benchmarks;transfer learning methods;transfer learning;parameters pretrained model;tuning parameters tasks;pretrained models;parameters pretrained;efficient fine tuning;classification benchmarks utilize;machine translation;classification benchmarks;machine translation text;new parameter efficient;pretrained model;parameter efficient transfer;finetune parameters pretrained;fine tuning methods;text classification;fine tuning parameters;pretrained models define;states pretrained models;translation text summarization;learning methods fine;parameter efficient;studies machine translation;benchmarks utilize unified;benchmarks utilize;parameters tasks;learning methods", "pdf_keywords": "experiments nlp benchmarks;nlp benchmarks;pretrained language models;nlp benchmarks covering;language models downstream;machine translation;large pretrained language;machine translation mt;transfer learning junxian;nlp experiments nlp;experiments nlp;efficient transfer learning;language models;learning paradigm nlp;nlp experiments;paradigm nlp experiments;summarization machine translation;benchmarks covering text;transfer learning;transfer learning methods;pretrained language;tasks facto learning;models downstream tasks;nlp;learning junxian;paradigm nlp;general language understanding;ef\ufb01cient transfer learning;parameter efficient transfer;mt text classi\ufb01cation"}, "3e24375a1810375183d47ceadc7418e94533ba5f": {"ta_keywords": "online scheduling algorithms;online scheduling;online allocation perishable;novel online scheduling;fair online allocation;online allocation;scheduling algorithms;mechanisms online allocation;scheduling;scheduling algorithms compare;allocation perishable resources;vehicle charging agents;allocation perishable goods;electric vehicle charging;allocation perishable;vehicle charging consider;charging consider mechanisms;vehicle charging;charging agents;charging agents arrive;fair online;perishable goods application;charging consider;goods application electric;perishable resources energy;electric vehicle;application electric vehicle;online offline;perishable goods;perishable resources", "pdf_keywords": ""}, "b63b698d177ba2861fe97d23763d66324bb1236a": {"ta_keywords": "virus cell culture;influenza virus undergo;mice influenza virus;virus cell;cell culture virus;growth mice influenza;influenza virus;mice influenza;virus undergo;viral replication;promotes efficient viral;viruses defective m2;culture virus replicated;type virus cell;m2 protein transmembrane;replication m2 transmembrane;culture virus;efficient viral;generate viruses;efficient viral replication;influenza;m2 ion channel;generate viruses defective;viral;virus undergo multiple;mutant lacking transmembrane;viruses;mutant containing m2;viruses defective;virus replicated reasonably", "pdf_keywords": ""}, "dcd39e2eb27d17c369f3bf7a5a7a2a30bb9201c8": {"ta_keywords": "training language models;trained language models;pre trained language;training data semantics;pre training language;pre training corpora;datasets pre training;pre training data;transferability pre trained;corpora pre training;language models;trained language;training corpora;training corpora pre;training language;language models lms;trained downstream tasks;pre trained lm;sequence transferability pre;semantics make pre;pre training downstream;language models study;distribution pre training;constructed datasets pre;dependencies tokens sequence;semantics control characteristics;semantics control;datasets pre;explicit dependencies tokens;implicit dependencies tokens", "pdf_keywords": "natural language downstream;language downstream tasks;transferability natural language;human language downstream;training language models;trained language models;language downstream;trained natural language;language gain transferability;transferability pre trained;pre trained language;dependencies sequences transferability;token dependencies sequences;trained downstream tasks;mlms english downstream;dependencies tokens sequences;language models lms;trained language;language models;dependencies tokens sequence;training data downstream;pre training language;model token dependencies;training language;implicit dependencies tokens;natural language gain;explicit dependencies tokens;sequences transferability pre;token dependencies;dependencies tokens"}, "c6af1ad95917badd7bc65b303a40f54950360279": {"ta_keywords": "statistical dialog managers;statistical dialog manager;statistical dialog approaches;dialog manager statistical;rule based dialog;statistical dialog;practice statistical dialog;manager statistical dialog;dialog manager based;dialog approaches regularization;conventional statistical dialog;based dialog manager;dialog managers;dialog managers difficult;dialog approaches;dialog manager practice;dialog manager;dialog managers potentially;dialog scenario;dialog manager cost;automobile dialog scenario;based dialog;function statistical dialog;dialog;task automobile dialog;automobile dialog;dialog scenario demonstrate;dialog manager enable;manager based bayes;incorporating rule based", "pdf_keywords": ""}, "595306f993993e44e2c2f674367103f44df03d9b": {"ta_keywords": "low resource translation;supervised translation baselines;resource machine translation;compared supervised translation;techniques improve translation;improve translation quality;supervised translation;unsupervised machine translation;challenges machine translation;machine translation framework;machine translation;translation quality;machine translation terms;translation baselines;resource translation specifically;resource translation;improve translation;translation baselines second;target monolingual data;language low resource;machine translation using;low resource language;translation framework;monolingual data;translation quality bleu;translation terms;monolingual data pivoting;augmentation low resource;high resource language;low resource words", "pdf_keywords": "resource machine translation;low resource translation;translation low resource;machine translation terms;resource translation;machine translation uses;machine translation;challenges machine translation;low resource languages;target monolingual data;augmentation low resource;monolingual data;abstract translation low;monolingual data pivots;translation terms;related highresource language;highresource language hrl;induced bilingual dictionary;resource languages;resource languages lrls;generalized data augmentation;translation low;highresource language;translation uses target;convert high resource;bilingual dictionary;data augmentation;data augmentation low;translation uses;high resource data"}, "5d6f87e31d806a77d22e344106d0310be3342259": {"ta_keywords": "randomized reviewer assignments;reviewer assignment optimally;randomized reviewer;peer review randomized;randomized algorithm reviewer;probability assignment reviewer;optimally solve reviewer;similarities reviewer assignment;reviewer assignment;review randomized reviewer;manipulation peer review;reviewer assignments;algorithm reviewer assignment;reviewer anonymization;reviewer assignment code;assignment reviewer paper;solve reviewer assignment;chance malicious reviewer;reviewer assignment problem;assignment reviewer;conference peer review;review randomized;peer review reviewers;malicious reviewer;reviewer assignments conceptual;malicious reviewer gets;review reviewers maliciously;reviewers maliciously;iii reviewer anonymization;reviewer gets assigned", "pdf_keywords": "reviewer assignment optimally;randomized algorithm reviewer;probability assignment reviewer;algorithm reviewer assignment;optimally solve reviewer;reviewer assignment;assignment reviewer paper;similarity number reviewers;reviewer assignment problem;solve reviewer assignment;assignment reviewer;need assigned reviewers;probability paper reviewer;assigned reviewers;reviewer deanonymization;reviewer gets assigned;peer review;chance malicious reviewer;importance peer review;malicious reviewer;reviewing reviewer deanonymization;peer review process;similarity empirical evaluations;malicious reviewer gets;optimal similarity empirical;algorithm reviewer;challenges peer review;reviewer paper;paper reviewer;peer review academic"}, "68d0b245e9754de9f36cba305e4ce50ff868cb6a": {"ta_keywords": "combinatory categorial grammars;combinatory categorial grammar;grammar induction combinatory;grammar induction algorithm;categorial grammars;robust grammar induction;categorial grammar;induction combinatory categorial;categorial grammar ccg;based grammar induction;categorial grammars additionally;grammar induction;em based grammar;algorithm combinatory categorial;unsupervised parsing;robust grammar;unsupervised parsing ccgs;combinatory categorial;grammar ccg achieves;remains robust parsing;simple robust grammar;robust parsing;robust parsing longer;induction algorithm combinatory;grammar ccg;grammar remains robust;induction combinatory;approaches grammar;grammars;parsing longer sentences", "pdf_keywords": ""}, "7cc74ffa1215321712d4a830bb9dee19d9f0fb47": {"ta_keywords": "question answering grounding;language representations grounding;generalization question answering;representations grounding structured;question answering models;improves compositional generalization;improve compositional generalization;compositional generalization language;answering models struggle;answering models;question answering;decoding improves compositional;grounding structured predictions;generalization language representations;benchmark compositional generalization;structured predictions attention;grounded graph decoding;representations grounding;challenging benchmark compositional;compositional generalization;answering grounding enables;question answering current;answering grounding;baselines compositional freebase;predicting structured;significantly improving generalization;compositional generalization question;compositional freebase questions;language representations;structured predictions", "pdf_keywords": "query graph decoder;generalization question answering;syntactic semantic compositions;query natural language;syntactic composition attention;language representations grounding;semantic composition;model semantic composition;semantic compositions;question answering model;aware syntactic composition;grounded graph decoding;question answering;encoder aware syntactic;queries sequence encoder;answering model semantics;representations grounding structured;compositional generalization language;improve compositional generalization;semantic composition using;queries notably grounded;embeddings model semantic;conjunctive query graph;benchmark compositional generalization;generalization language representations;structured predictions attention;conjunctive queries sequence;compositionally generalizable predictions;association syntactic semantic;composition attention"}, "ee3ce47b79917974d30b6eeaaeeba99f1b1a5c59": {"ta_keywords": "end asr espnet;auxiliary loss decoding;performance decoding;implemented systems espnet;streaming task;asr espnet architecture;loss decoding strategies;streaming task experiments;asr espnet;decoding strategies additionally;espnet architecture auxiliary;systems streaming task;loss decoding;multiple decoding strategies;systems espnet;regards performance decoding;decoding strategies;performance decoding speed;systems espnet regards;espnet regards performance;multi task learning;task learning;powerful systems streaming;streaming;multiple decoding;espnet;decoding;systems streaming;espnet architecture;decoding speed enabling", "pdf_keywords": "espnet speech recognition;decoding models espnet;extension espnet speech;espnet speech;speech recognition toolkit;trained language model;performance decoding;implemented systems espnet;real time decoding;streaming task;systems espnet;espnet regards performance;models espnet;speech recognition;extension espnet;decoding external language;performance decoding speed;introduced extension espnet;decoding models;streaming task work;pre trained language;external language model;espnet;trained language;decoder decoding external;regards performance decoding;transducer models additionally;decoder;toolkit dedicated transducer;decoding speed enabling"}, "5f96e3e00b36c5eeebff09a1bf4c804bd4ce4620": {"ta_keywords": "belief estimator attack;attack detection;malicious sensors;quickest attack detection;attack detection problem;attack remote state;estimator attack;remote state estimation;remote estimator quickest;potentially malicious sensors;linear attack scheme;estimator attack turns;remote estimator;observations remote estimator;estimator quickest detection;data injection attack;attack remote;known linear attack;malicious sensors start;linear attack;injection attack remote;attack scheme;minimize expected detection;attack turns optimal;probability belief estimator;attack scheme posed;false alarm constraint;belief estimator;state estimation considered;state estimation", "pdf_keywords": "attack remote estimation;attack scheme bayesian;detecting linear attack;quickest attack detection;attack detection;attack known matrix;estimator quickest attack;attack detection problem;geometric prior attack;known linear attack;prior attack initiation;linear attack scheme;belief estimator attack;detection fdi attack;linear attack known;attack remote state;prior attack;remote estimation bayesian;remote estimator quickest;estimator attack;linear attack;remote state estimation;attack remote;quickest attack;attack initiation instant;attack scheme;attack initiation;injection attack remote;data injection attack;bayesian detection"}, "64a106b707586345a055aa22c3356c4dc3d01877": {"ta_keywords": "connectivity event timing;connectivity event;network dynamics;network connectivity event;germany network dynamics;event timing patterns;inferring network connectivity;network dynamics institute;timing patterns;event timing;network dynamics max;connectivity;inferring network;computational neuroscience;computational neuroscience bccn;timing patterns jose;network connectivity;planck institute dynamics;center computational neuroscience;01 inferring network;chair network dynamics;dynamics self organization;network;physics complex systems;dresden germany network;germany network;dynamics institute theoretical;neuroscience bccn;planck institute physics;neuroscience", "pdf_keywords": ""}, "d5a95567e079685322cd485033d334284c4b0a62": {"ta_keywords": "reversible polymerization controlled;reversibility controlled polymerization;reversible polymerization systems;develop reversible polymerization;reversible polymerization approaches;developments reversible polymerization;reversible polymerization;utility reversible polymerization;polymerization controlled polymerization;controlled polymerization systems;established controlled polymerization;controlled polymerization;polymerization controlled;polymerization systems;polymerization systems recent;controlled polymerization growing;polymerization systems offer;polymerization approaches established;polymerization systems field;field controlled polymerization;polymerization systems lack;polymerization approaches;polymerization;polymerization growing;polymerization growing evolving;facilitating polymer;develop reversible;vinyl polymers;discussed reversibility controlled;reversibility controlled", "pdf_keywords": ""}, "f16cf130ae75d1ea1ad3b926f605adef41af4af1": {"ta_keywords": "conservation based uncertainty;uncertainty propagation dynamic;considers uncertainty propagation;based uncertainty propagation;theory uncertainty propagation;uncertainty propagation;uncertainty propagation initial;uncertainty propagation using;understanding uncertainty propagation;methods uncertainty propagation;uncertainty propagation terms;method uncertainty propagation;uncertainty propagation results;based uncertainty;theory uncertainty;considers uncertainty;generalized theory uncertainty;existing methods uncertainty;probability structure conservation;methods uncertainty;formulate understanding uncertainty;uncertainty present;understanding uncertainty;based method uncertainty;uncertainty;method uncertainty;presented considers uncertainty;propagation dynamic systems;propagation using conservation;uncertainty present everyday", "pdf_keywords": ""}, "b13e9d23983273c0c67b91ae70c55d4c3f745b8b": {"ta_keywords": "simultaneous translation agent;neural machine translation;translation agent learns;simultaneous translation;translation agent;translating real time;machine translation nmt;simultaneous translation outputs;machine translation;machine translation trade;learning translate;time simultaneous translation;translation methods learning;translate real time;conventional machine translation;learning translate real;framework simultaneous translation;translation words input;machine translation methods;translation outputs translation;translation methods;outputs translation words;translation nmt framework;translate interaction pre;translation nmt;methods learning translate;translation outputs;nmt environment translating;outputs translation;translate interaction", "pdf_keywords": "simultaneous machine translation;machine translation;translation directions proposed;machine translation based;neural simultaneous machine;translation directions;formulating translation;translation based;formulating translation interleaved;perform neural simultaneous;nmt read write;en translation directions;translation interleaved sequence;based formulating translation;neural simultaneous;translation based devise;perform neural;translation interleaved;learning perform neural;nmt read;translation;en translation;nmt based algorithm;german en translation;compared nmt based;neural;improvements compared nmt;actions read write;nmt;segmentation"}, "ed1c17451a23471afde91c109ecadc6aab8b2ba6": {"ta_keywords": "multimodal disinformation detection;disinformation detection;disinformation detection taking;survey multimodal disinformation;disinformation detection recent;multimodal disinformation;tackle disinformation detection;disinformation detection covering;disinformation online;disinformation online different;art multimodal disinformation;misinformation disinformation online;disinformation;propaganda misinformation disinformation;news propaganda misinformation;tackle disinformation;text survey multimodal;misinformation disinformation;modalities text images;news propaganda;modalities factuality harmfulness;fake news propaganda;propaganda misinformation;need tackle disinformation;multimodal;textual content;proliferation fake news;survey multimodal;state art multimodal;videos gained popularity", "pdf_keywords": "multimodal disinformation detection;research multimodal disinformation;multimodal disinformation;survey multimodal disinformation;art multimodal disinformation;multimodal factuality prediction;multimodal factuality;multimodality interact harmfulness;disinformation detection;harmful content detection;multimodality;disinformation detection based;disinformation detection firoj;solutions multimodal factuality;image speech video;multimodal;research multimodal;disinformation detection covering;image speech;survey research multimodal;information survey multimodal;factual image speech;figure envision multimodality;envision multimodality;speech video;survey multimodal;modalities text images;speech video network;harmful factual image;state art multimodal"}, "8c838c7631a7408d5ea5801c9360213782665c9c": {"ta_keywords": "shinji watanabe laborotv_asr_train_asr_conformer2_latest33_raw_char_sp_valid;espnet2 pretrained model;espnet2 pretrained;watanabe laborotv_asr_train_asr_conformer2_latest33_raw_char_sp_valid;watanabe laborotv_asr_train_asr_conformer2_latest33_raw_char_sp_valid acc;laborotv_asr_train_asr_conformer2_latest33_raw_char_sp_valid;laborotv_asr_train_asr_conformer2_latest33_raw_char_sp_valid acc ave;laborotv_asr_train_asr_conformer2_latest33_raw_char_sp_valid acc;pretrained model shinji;espnet2;16k lang jp;fs 16k lang;model shinji watanabe;pretrained model;pretrained;16k lang;model shinji;jp;fs 16k;shinji watanabe;lang jp;model;shinji;16k;ave fs 16k;watanabe;fs;lang;acc;ave fs", "pdf_keywords": ""}, "db1dafd0c356491cbbf53338b9984de324e7239c": {"ta_keywords": "bilingual lexicon induction;bilingual lexicons distribution;aligned bilingual lexicons;bilingual lexicons larger;bilingual lexicon;bilingual lexicons;technique bilingual lexicon;propose bilingual lexicon;work bilingual lexicon;lexicon induction semi;spaces propose bilingual;depended aligned bilingual;limited aligned bilingual;lexicons distribution matching;aligned bilingual;filtering technique bilingual;lexicon induction bli;lexicon induction;recent work bilingual;technique bilingual;bilingual;unaligned word embeddings;embedding spaces empirically;word embeddings;propose bilingual;lexicons distribution;work bilingual;induction semi supervision;non isometric embedding;supervision non isometric", "pdf_keywords": "bilingual lexicon induction;aligned bilingual lexicons;bilingual lexicon;bilingual lexicons larger;framework bilingual lexicon;bilingual lexicons;propose bilingual lexicon;lexicon induction semi;limited aligned bilingual;embedding alignment unsupervised;distant language pairs;supervised embedding alignment;aligned bilingual;induction semi supervision;instantiations semi supervised;semi supervised;propose framework bilingual;language pairs;framework bilingual;language pairs datasets;unaligned word embeddings;word embeddings;translation loss constraint;optimizes supervised embedding;bilingual;language pairs particular;bliss semi supervised;semi supervised framework;lexicon induction;semi supervised approach"}, "9bb9b23823b45ba7521d872bb3e970ede4aafb8a": {"ta_keywords": "joint speaker diarization;speaker diarization;speaker diarization speech;diarization speech recognition;diarization speech;speech recognition;joint speaker;region proposal networks;speech recognition based;speaker;recognition based region;diarization;region proposal;based region proposal;speech;proposal networks;recognition based;recognition;networks;region;based region;proposal;joint;based", "pdf_keywords": ""}, "41675d91ad815f64b0df382c0944247811a62cc9": {"ta_keywords": "translation phrase table;statistical machine translation;list bilingual phrases;phrase alignment extraction;joint phrase alignment;machine translation phrase;alignment phrase extraction;bilingual phrases;machine translation;learn phrase table;phrase alignment;phrase table parallel;phrase table;machine translation systems;table parallel corpus;phrases level granularity;corpus sentences aligned;phrase extraction;based machine translation;word alignment phrase;alignment extraction statistical;reducing phrase table;parallel corpus sentences;phrases granularities;phrase extraction approach;modeled phrases level;phrase based machine;phrases granularities included;parallel corpus;translation systems", "pdf_keywords": ""}, "1fc6290fa3e6784501ca67cbef33a6a8edcbdb9e": {"ta_keywords": "measures wasserstein distance;empirical measures wasserstein;wasserstein distance probability;mu wasserstein distance;wasserstein distance;distance probability measures;wasserstein distance order;measures wasserstein;probability measures metric;wasserstein distance work;convergence empirical measures;quickly empirical measure;empirical measures;empirical measure;closeness applications statistics;measure closeness applications;approaches mu wasserstein;measure closeness;empirical measure obtained;measures metric;probability measures;measures metric space;space measure closeness;measures;convergence empirical;measures exhibit;mu wasserstein;learning sharp asymptotic;wasserstein;metric space measure", "pdf_keywords": "measures wasserstein distance;empirical measures wasserstein;wasserstein distance probability;convergence empirical measures;spaces wasserstein distance;wasserstein distance;approaches wasserstein distance;wasserstein distance jonathan;distance probability measures;wasserstein distance order;measures wasserstein;probability measures metric;metric spaces wasserstein;quickly empirical measure;empirical measures;samples approaches wasserstein;convergence general measures;closeness applications statistics;empirical measure;measures metric;convergence empirical;measure closeness applications;measures metric space;space measure closeness;empirical measure obtained;measure closeness;general measures;spaces wasserstein;rates convergence empirical;measures"}, "f5b6819e05e087d2bbae3ddb6d58d2ab4e1d7ca2": {"ta_keywords": "inverse reinforcement;inverse reinforcement learning;reward environment learn;uses inverse reinforcement;maximize reward environment;agents maximize reward;reinforcement learning policy;constraints demonstrations reinforcement;policy contextual bandit;reward maximizing constrained;demonstrations reinforcement learning;reward environment;environment reward based;reinforcement learning learn;actions reward maximizing;demonstrations reinforcement;reward maximizing;learning learn maximize;reinforcement;environment reward;rewards teaching ai;based environment reward;learn maximize environmental;learning policy orchestration;reinforcement learning;maximize environmental rewards;maximize reward;orchestration contextual bandit;contextual bandit based;learn maximize", "pdf_keywords": ""}, "bfb13c6889626e833bf449fdb361d186467919af": {"ta_keywords": "experiments addressing sentiment;appeal feature feedback;incorporating feature feedback;benefits feature feedback;feedback auxiliary annotations;benefit supplemental annotations;feature feedback;feature feedback delivered;feature feedback methods;exploiting feature feedback;supplemental annotations lessening;supplemental annotations;addressing sentiment;analysis feature feedback;annotations provided training;addressing sentiment analysis;domain evaluations inspired;sentiment analysis feature;auxiliary annotations;feature feedback auxiliary;counterfactually augmented data;appeal feature;auxiliary annotations provided;annotations lessening sensitivity;domain evaluation attempts;annotations;feedback;intuitive appeal feature;feedback methods;feedback delivered significant", "pdf_keywords": "feature feedback nlp;domain datasets sentiment;feedback nlp methods;models addressing sentiment;feedback nlp;datasets sentiment;incorporating feature feedback;trained feature feedback;datasets sentiment analysis;feature feedback;sentiment analysis dataset;feature feedback generalize;feedback \ufb01nd sentiment;nlp methods datasets;feature feedback imdb;addressing sentiment;rely feature feedback;addressing sentiment analysis;supplementary annotations incorporated;\ufb01netuned feature feedback;supplementary annotations;sentiment analysis models;feedback imdb data;feedback generalize better;feature feedback paper;feedback imdb;feedback generalize;feature feedback \ufb01nd;analysis dataset nli;sentiment analysis compare"}, "df9949abc06cb4f0f4c0ac1eb7ce0bc62ed5ec02": {"ta_keywords": "annotated phoneme sequences;sentence phoneme estimation;phoneme sequence estimation;corpus word sequence;sequences word phoneme;approach phoneme sequence;annotated phoneme;partially annotated corpus;phoneme sequences realize;word segmentation input;list annotated phoneme;phoneme sequences;phoneme sequence;phoneme sequence estimator;word segmentation;sequence estimation sentences;word sequence list;annotated corpus;annotated corpus word;sequence list annotated;sentences words annotated;input sentence phoneme;word segmenter;tasks word segmentation;gram model based;pointwise word segmenter;pointwise phoneme sequence;phoneme estimation word;sentence phoneme;word segmenter pointwise", "pdf_keywords": ""}, "5331a846c854c3ecedf9ecf3ea516cb6dcaba4c8": {"ta_keywords": "evaluation saliency methods;based evaluation saliency;saliency methods currently;existing saliency methods;evaluation saliency;saliency methods;saliency methods controlling;saliency methods especially;saliency methods popular;existing saliency;saliency;limitations existing saliency;adoption saliency methods;synthetic evaluation tasks;evaluations smerf reveal;synthetic evaluation framework;development adoption saliency;model predictive reasoning;smerf synthetic evaluation;adoption saliency;predictive reasoning identifying;identifying important pixels;predictive reasoning;truth based evaluation;feature attribution tools;evaluations smerf;synthetic evaluation;feature attribution;input image development;evaluation framework smerf", "pdf_keywords": "saliency methods capture;saliency methods capturing;saliency methods;leading saliency methods;evaluation saliency methods;evaluation leading saliency;saliency methods ability;saliency methods controlling;evaluation saliency;performance saliency methods;saliency;based evaluation saliency;saliency methods smerf;bound saliency methods;evaluating performance saliency;leading saliency;saliency methods practitioners;saliency methods stylized;view performance saliency;performance saliency;bound saliency;upper bound saliency;degree saliency methods;modes leading saliency;degree saliency;image finally contextualize;perspective aware;model reasoning tasks;study degree saliency;natural images leverage"}, "807600ef43073cd9c59d4208ee710e90cf14efa8": {"ta_keywords": "sparse retrieval models;benchmark information retrieval;neural information retrieval;retrieval models;retrieval models computationally;sparse retrieval;information retrieval models;dense sparse retrieval;existing retrieval systems;retrieval ir models;information retrieval existing;text retrieval tasks;retrieval systems;existing retrieval;retrieval systems including;diverse text retrieval;retrieval existing neural;information retrieval;retrieval tasks;retrieval systems contributes;retrieval existing;retrieval;understand existing retrieval;text retrieval;evaluation information retrieval;information retrieval ir;retrieval tasks domains;theart retrieval systems;retrieval ir;heterogeneous evaluation benchmark", "pdf_keywords": "retrieval ir models;information retrieval ir;18 retrieval datasets;information retrieval models;information retrieval;retrieval models;retrieval ir;retrieval datasets;neural information retrieval;retrieval datasets comparison;retrieval dbpedia;retrieval systems including;diverse text retrieval;retrieval trec news;retrieval models nandan;retrieval systems;evaluation information retrieval;text retrieval tasks;retrieval dbpedia wiki;news retrieval;news retrieval trec;text retrieval;retrieval tasks;benchmarking ir comprising;datasets news retrieval;retrieval;comprising 18 retrieval;retrieval trec;benchmarking ir;retrieval tasks domains"}, "3a40cdd82f0706cda6c247e586d5054abeab4e1f": {"ta_keywords": "list question answering;improve question answering;answer list entities;question answering qa;question answering;question answering given;list answers better;extend candidate answers;list answers;answer list;new list answers;automatic set expansion;expected answer list;list produced qa;set expansion list;answering qa;set expansion se;expansion list;extended list;se algorithms textual;produce extended list;extended list including;answering qa expected;list including additional;generate new list;answers produced qa;qa generate;candidate answers produced;expansion list question;list including", "pdf_keywords": ""}, "2c3d02ce8780cc6648caf4ee996d9628c6388751": {"ta_keywords": "conditional entropy crowdsourcing;entropy crowdsourcing;crowdsourcing data labeling;crowdsourced labels principle;crowdsourced labels;entropy crowdsourcing paper;noisy crowdsourced labels;crowdsourcing datasets binary;truth noisy crowdsourced;crowdsourcing data;minimax conditional entropy;crowdsourcing datasets;increasing crowdsourcing data;real crowdsourcing datasets;noisy crowdsourced;probabilistic labeling;unique probabilistic labeling;probabilistic labeling model;crowdsourcing;real crowdsourcing;increasing crowdsourcing;variety real crowdsourcing;crowdsourced;rapidly increasing crowdsourcing;crowdsourcing paper propose;crowdsourcing paper;propose minimax conditional;regularized minimax conditional;conditional entropy;minimax conditional", "pdf_keywords": "conditional entropy crowdsourcing;crowdsourcing data labeling;entropy crowdsourcing;crowdsourced labels;crowdsourced labels principle;entropy crowdsourcing dengyong;noisy crowdsourced labels;minimax conditional entropy;crowdsourcing datasets;crowdsourcing data;increasing crowdsourcing data;crowdsourcing datasets binary;real crowdsourcing datasets;truth noisy crowdsourced;probabilistic labels;probabilistic labeling;unique probabilistic labeling;probabilistic labeling model;generating probabilistic labels;crowdsourcing dengyong zhou;increasing crowdsourcing;variety real crowdsourcing;crowdsourcing dengyong;noisy crowdsourced;labeling model jointly;real crowdsourcing;rapidly increasing crowdsourcing;crowdsourcing;crowdsourced;regularized minimax conditional"}, "ce5ff42d629e67a84731b3c62b57b47fc7f2b20d": {"ta_keywords": "e2e automatic speech;streaming e2e asr;speech recognition asr;performance alternative recurrent;training streaming transformer;encoder introducing;chunkwise attention;recurrent neural networks;alternative recurrent neural;attention network;speech recognition;self attention network;encoder;transformer encoder introducing;transformer self attention;encoder introducing context;training streaming;task proposed streaming;aishell mandarin tasks;automatic speech;mandarin tasks librispeech;mandarin tasks;recognition asr systems;entire streaming e2e;monotonic chunkwise attention;global linguistic channel;linguistic channel speaker;streaming e2e;transformer encoder;encode local acoustic", "pdf_keywords": "speech recognition asr;performance alternative recurrent;recurrent neural networks;alternative recurrent neural;e2e automatic speech;speech recognition;recognition asr systems;recognition asr;transformer asr blockwise;transformer self attention;chunkwise attention;streaming transformer asr;attention network;automatic speech;neural networks end;self attention network;automatic speech recognition;monotonic chunkwise attention;recurrent neural;alternative recurrent;chunkwise attention mocha;transformer asr evaluations;transformer asr;e2e transformer asr;synchronous beam search;blockwise processing encoder;compute self attention;beam search;encoder;aishell mandarin librispeech"}, "945d4addf8e94487f6199af71dc15a298791c1b4": {"ta_keywords": "markovian energy arrival;markov decision process;channel markovian energy;policy time markovian;markovian channel markovian;markov decision;channel markovian;markovian energy;time markovian channel;state energy harvesting;markovian channel;horizon markov decision;energy arrival processes;channel optimal source;mdp learning algorithm;channel optimal;energy arrival;infinite horizon markov;probing channel optimal;time markovian;channel state energy;energy harvesting;seeks learn optimal;node sampling policy;single energy harvesting;learn optimal;harvesting source node;markovian;process available energy;energy harvesting characteristics", "pdf_keywords": "opportunistic sampling energy;horizon markov decision;markovian energy arrival;sampling energy harvesting;sampling transmission scheduling;expected aoi minimization;considered aoi minimization;aoi minimization wirelessly;channel markovian energy;minimization time averaged;minimization opportunistic sampling;scheduling transmit power;markovian energy;transmit power selection;minimization formulated mdp;aoi minimization;markov decision process;in\ufb01nite horizon markov;minimization time;abstract minimization time;markovian channel markovian;analytically case markovian;markov decision;horizon markov;aoi energy harvesting;sampling energy;process derive optimal;channel markovian;aoi minimization formulated;aoi minimization problem"}, "27df24c537b2d3c2a769d917adf92a6a059c5917": {"ta_keywords": "solvers predicting multiscale;multiscale simulations deeponet;multiscale modeling mechanics;fast multiscale modeling;multiscale modeling machine;predicting multiscale systems;multiscale modeling;multiscale modeling effective;multiscale simulations;problems multiscale modeling;idea multiscale modeling;simulations deeponet inference;predicting multiscale;cost multiscale simulations;solver learning underlying;simulations deeponet;fine solver learning;multiscale systems;computational cost multiscale;fast multiscale;operators fast multiscale;solver learning;solver lower fidelity;multiscale systems new;modeling mechanics;elements deep neural;idea multiscale;neural operators fast;deeponet inference cost;multiscale", "pdf_keywords": "multiscale modeling mechanics;multiscale modeling machine;fast multiscale modeling;multiscale modeling;abstract multiscale modeling;machine learning multiscale;multiscale modeling e\ufb00ective;idea multiscale modeling;learning multiscale;framework multiscale modeling;multiscale simulations deeponet;multiscale simulations;modeling mechanics;learning multiscale active;operators fast multiscale;fast multiscale;computational cost multiscale;coupling framework multiscale;modeling mechanics problems;idea multiscale;cost multiscale simulations;multiscale;modeling machine;elements deep neural;element methods fem;abstract multiscale;pa abstract multiscale;finite elements deep;multiphysics systems;explore idea multiscale"}, "46ef61536a01578e79b6d4e35e803a914afeb629": {"ta_keywords": "mn4 line emission;emission intensity tunability;mn4 line;intensity tunability solids;line emission;line emission intensity;emission intensity;emission;mn4;tunability solids;intensity tunability;tunability;solids;line;intensity", "pdf_keywords": ""}, "65ee083ce61576955d76b36819bf3ac271335597": {"ta_keywords": "codes distributed storage;regenerating codes distributed;codes minimum storage;storage peer peer;storage peer;distributed storage explicit;distributed storage;relevant storage peer;regenerating codes possessing;distributed storage systems;reliability distributed storage;regenerating codes minimum;codes regenerating codes;codes distributed;storage repair bandwidth;regenerating codes regenerating;regenerating codes;erasure coding techniques;codes regenerating;systems erasure coding;exact regenerating codes;erasure coding;regenerating codes provided;systems minimizing storage;storage explicit construction;storage systems minimizing;bandwidth point storage;minimum storage point;storage systems;storage explicit", "pdf_keywords": "erasure coding techniques;erasure coding;exact regenerating codes;abstract erasure coding;regenerating codes introduced;regenerating codes;regenerating codes mbr;proposed code construction;codes mbr point;linear exact regenerating;code construction algorithm;reliability distributed storage;codes mbr;storage systems minimizing;storage systems;code construction;systems minimizing storage;distributed storage systems;proposed code;coding techniques;coding techniques used;distributed storage;minimizing storage;codes introduced;storage;minimizing storage overhead;exact regenerating;storage overhead section;coding;notion exact regenerating"}, "092ee3a32b6cd951da971124a24872c7cccf3a9f": {"ta_keywords": "transductive transfer learning;unsupervised transductive transfer;transfer learning labeled;transfer learning information;transfer learning;transfer learning protein;problem transfer learning;transductive transfer;methods transductive transfer;transfer learning previous;learning protein extraction;unsupervised transductive;inductive transductive approaches;learning protein;case unsupervised transductive;transductive approaches;transductive approaches adapt;transductive;learning labeled data;learning labeled;inductive transductive;study methods transductive;methods transductive;art inductive transductive;models problem transfer;learning information;learning information gained;domain available training;protein extraction;labeled data target", "pdf_keywords": ""}, "fce10a1a9727cbda33d44b62409e303f1009417a": {"ta_keywords": "neural network grammars;network grammars rnng;grammars rnng;grammars rnng recently;grammars learn syntax;recurrent neural;network grammars learn;recurrent neural network;grammars learn;modeling parsing;language modeling parsing;modeling parsing performance;model attention;language modeling;inspection recurrent neural;model latent attention;model attention mechanism;learn syntax state;network grammars;augmenting model attention;learn syntax;grammars;phrasal representation model;probablistic generative modeling;generative modeling;parsing performance attention;generative;attention mechanism ga;generative modeling family;probablistic generative", "pdf_keywords": "neural network grammars;network grammars rnngs;grammars rnngs;network grammars learn;distributions recurrent neural;attention mechanism composition;grammars learn syntax;grammars learn;variant gated attention;recurrent neural;network grammars;rnngs test linguistic;novel gated attention;recurrent neural network;learn syntax ablation;model attention mechanism;grammars;model syntactic derivations;syntactic derivations sentences;gated attention;rnng incorporate interpretability;phrasal representation role;model attention;syntactic derivations;probe recurrent neural;attention mechanism ga;gated attention mechanism;distributions recurrent;rnng composition function;probability distributions recurrent"}, "49af035c598901fbf766da2cfb040cca7336a8ac": {"ta_keywords": "exploration semantic parsers;abstract meaning representation;learning abstract meaning;meaning representation parsing;interpretations imitation learning;semantic parsers;semantic parsers map;exploration semantic;representation parsing targets;representation parsing;imitation learning abstract;exploration imitation learning;ambiguous interpretations imitation;steps imitation learning;parsing highly beneficial;interpretations imitation;meaning representations abstract;parsing targets exploration;map natural language;imitation learning;amr parsing highly;parsing;parsers map natural;amr parsing;imitation learning areas;representations abstract syntactic;targeted exploration semantic;parsers;exploration imitation;targeted exploration imitation", "pdf_keywords": ""}, "d47ad0a606bedf41dcea614bfa7b7494879c7ba0": {"ta_keywords": "domain procedural text;procedural text;changes procedural text;just procedural text;text entities;procedural text arbitrary;tracking state changes;vocabulary dataset tracking;text entities involved;state changes procedural;dataset tracking state;predicted open vocabulary;open vocabulary dataset;text describing;domain procedural;tracking state;procedural text input;dataset tracking entities;open domain procedural;changes procedural;new task formulation;text arbitrary;vocabulary example text;text arbitrary domains;attribute state;provide text entities;vocabulary dataset;example text describing;procedural;open vocabulary example", "pdf_keywords": "dataset tracking entities;dataset track entities;crowdsourced annotated dataset;annotated dataset;vetted crowdsourced annotated;dataset tracking;mechanical turk annotators;dataset tracking state;dataset track;annotated dataset openpi;track entities;crowdsourced annotated;tracking entities;text end crowdsourced;annotators;turk annotators;\ufb01rst dataset tracking;vetted crowdsourced;\ufb01rst dataset track;tracking state changes;domain procedural text;human vetted crowdsourced;training dataset task;tracking entities open;crowdsourcing task;annotated;amazon mechanical turk;track entities open;dataset task;crowdsourced"}, "188928df74f9ce00bd1b58686db93ac8cdd07275": {"ta_keywords": "speech faster beam;vectorization hypotheses speech;batch multiple speech;hypotheses speech faster;traverse multiple utterances;speech faster;faster beam search;speech recognition;utterances line recognition;multiple speech utterances;multiple speech;decoder based speech;speech recognition achieved;multiple utterances;beam search encoder;speech utterances;vectorizing multiple hypotheses;beam search algorithm;beam search;attention based encoder;hypotheses speech;speech utterances line;rnnlm ctc batch;search process vectorizing;beam search accelerates;multiple utterances paper;right beam search;search algorithm vectoring;utterances line;program attention based", "pdf_keywords": ""}, "2f369845ae7191196d65310210db2485feb3aa86": {"ta_keywords": "regularization weights grapheme;g2p conversion training;adaptive regularization weights;regularization weights narow;weights grapheme phoneme;grapheme phoneme conversion;adaptive regularization;regularization weights;narrow adaptive regularization;phoneme conversion;weights grapheme;learning algorithm speech;speech recognition;conversion training method;weights narow online;algorithm speech recognition;robust g2p conversion;regularization;speech recognition field;new g2p conversion;g2p conversion;narow online learning;weights narow;grapheme phoneme;g2p conversion increasing;conversion training;algorithm speech;online learning algorithm;learning rate updated;robust g2p", "pdf_keywords": ""}, "bd2f3822801a7e2f933d06c261b8783764d8ce18": {"ta_keywords": "ood samples adversarial;samples adversarial;adversarial adv samples;samples adversarial adv;datasets adv attacks;adversarial;adversarial adv;text classi\ufb01cation model;features hidden representations;adv samples emerge;hidden representations;using hidden representations;classi\ufb01cation model performance;id ood datasets;types anomalies ood;adv attacks;samples emerge deeper;distinguishing id ood;results distinguishing id;ood datasets adv;hidden representations output;anomalies ood adv;adv attacks proposed;samples using hidden;types anomalies;exceptional results distinguishing;statistically distribution ood;hurting text classi\ufb01cation;datasets adv;results distinguishing", "pdf_keywords": "features hidden representations;similarity score based;based hidden features;adv datasets based;similarity score;ood adv datasets;adv datasets;hidden features;adv ones based;features separate adv;anomalies ood adv;similarity;different id samples;adv id ones;compare types anomalies;characterize ood adv;using hidden representations;id samples;hidden features separate;samples id adv;aspect hidden representations;effective staged detection;id ones model;types anomalies ood;hidden representations;hidden representations observe;input features;id adv ones;adv samples;detection method separate"}, "576860f910ea8fde366deb03c910ab30cd776966": {"ta_keywords": "continuous speech separation;speech separation using;separation using speaker;speech separation;using speaker inventory;speaker inventory;speaker inventory long;inventory long recording;long recording;continuous speech;separation using;using speaker;recording;speaker;separation;speech;inventory long;inventory;long;continuous;using", "pdf_keywords": ""}, "95a35473fd1936927dd4a53fe0a5d2d6762d99b3": {"ta_keywords": "musical performance hierarchical;hierarchical model musical;neural audio synthesis;midi;detailed control musical;introduce midi;articulation midi;audio synthesis detailed;control musical performance;audio synthesis;expressive performance timbre;notes performance synthesis;dynamics articulation midi;midi ddsp detailed;parameters infer musical;midi ddsp hierarchical;realistic neural audio;articulation midi ddsp;introduce midi ddsp;control musical;model musical instruments;work introduce midi;notes synthesis;midi ddsp;neural audio;infer musical notes;infer musical;model musical;generate realistic audio;musical notes", "pdf_keywords": "generative model musical;generative model music;realistic audio synthesis;neural audio synthesis;audio synthesis detailed;audio synthesis;hierarchical model musical;music modeling;hierarchical music modeling;model musical performance;midi;notes performance synthesis;model music;proposed midi;note performance synthesis;introduce midi;audio synthesis 13;midi ddsp hierarchical;expressive performance timbre;proposed midi ddsp;ddsp hierarchical music;model music notes;performance synthesis train;introduce midi ddsp;hierarchical music;generation audio;realistic neural audio;model musical instruments;midi ddsp;parameters infer musical"}, "55a3b36fd21dbbe9384ab3ba1bcf901235d95f47": {"ta_keywords": "unsupervised question decomposition;questions decompositions cumbersome;questions decompositions;labeling questions decompositions;decomposition question answering;question decomposition;question answering;improve question answering;question answering qa;capable answering labeling;answering qa decomposing;question decomposition question;decomposing hard questions;unsupervised sequence transduction;produce sub questions;answering labeling;answering labeling questions;questions matching;systems capable answering;automatically learns decompose;answering qa;answers recomposition model;qa decomposing;decompositions cumbersome unsupervised;cumbersome unsupervised approach;answer sub questions;questions existing qa;simpler sub questions;sub questions unsupervised;qa decomposing hard", "pdf_keywords": "supervised question decompositions;answers question decomposition;question decomposition supervised;question decompositions;question decompositions using;question decomposition;effectiveness parallel corpus;decompose question sub;questions matching;machine translation;answers recomposition model;automatically learns decompose;approach machine translation;recompose sub answers;qa answers;parallel corpus;model resulting answers;parallel corpus mining;proposed qa answers;unsupervised sequence transduction;qa recompose sub;qa recompose;fasttext pseudo decompositions;best qa;answers recomposition;questions matching utility;resulting answers;resulting answers recomposition;decomposition methods qa;automatically learns"}, "e153713b0423b4bae325340b2211e704effd5252": {"ta_keywords": "augment logic programs;inductive logic programming;logic programs;logic programs variation;logic programming ilp;logic programming;logical representation software;flipper learn counting;logics significantly improve;description logics significantly;predicting fault density;learn counting;programming ilp methods;learn counting clauses;predicting fault;logics significantly;augment logic;known inductive logic;programming ilp;predict fault density;description logics;abstract logical representation;directly abstract logical;task predicting fault;programming;inductive logic;logical representation;algorithms known inductive;counting clauses augment;programs variation", "pdf_keywords": ""}, "e718ffe247a61a77b45953a7e8a5b86a45ed579f": {"ta_keywords": "training dependency parsers;dependency parsers trained;dependency parser trained;japanese dependency parsing;dependency parsing;dependency parsers;dependency parser;dependency parsing approach;parsers partially annotated;dependency parsers partially;partially annotated corpora;mst dependency parser;annotated corpora;parsers trained;parser trained;parsers trained fully;parser trained partially;annotated corpora allowing;trained partially annotated;trained fully annotated;dependency tree sentence;annotated corpora experiments;dependency tree;parsers;fully annotated data;parsing;parsing approach allows;parsers partially;parser;art dependency parsers", "pdf_keywords": ""}, "84e50df18c284b985d287b462c63c20186cc5da1": {"ta_keywords": "cognitive tutors programming;building cognitive tutors;cognitive tutors;authoring cognitive tutors;tool cognitive tutors;cognitive tutors educators;tutors educators ai;cognitive tutors evaluation;tutors programming demonstration;educators ai programmers;tutors programming;programmers building cognitive;task taught machine;representing task taught;educators ai;tutors evaluation example;authoring tool cognitive;ai programmers;authoring cognitive;tutors evaluation;simulated student;facilitate authoring cognitive;task taught;technique programming demonstration;build cognitive model;tutors;learning agent called;learning agent;taught machine;called simulated student", "pdf_keywords": ""}, "efada589efdb0adf3aa9dc2b6cb6979a50658276": {"ta_keywords": "journalists estimation veracity;journalism project rumour;fact checking presenting;rumour debunking associated;project rumour debunking;rumour debunking;journalism project;news articles collected;debunking associated article;headline agreement claim;labelled journalists estimation;context fact checking;fact checking;labelled journalists;examine headline;debunking associated;headline stance;news articles;collected labelled journalists;article headline stance;associated news articles;features examine headline;examine headline agreement;headline labelled;journalism;estimation veracity;digital journalism project;natural language processing;headline stance respect;stance observing claim", "pdf_keywords": ""}, "3c6407554fb4ee599f42501cf5cba8fcefa88783": {"ta_keywords": "paired data speech;speech recognition asr;data speech;unsupervised data speech;speech encoder;automatic speech;loss based speech;utterances transcriptions;automatic speech recognition;data speech utterances;end automatic speech;speech utterances transcriptions;speech tts asr;transformation text speech;speech recognition;utterances transcriptions approaches;text speech;unpaired data training;data speech example;audio data transcriptions;based speech encoder;raw speech signal;speech encoder state;text speech tts;recognition asr models;recognition asr;data transcriptions;instead raw speech;speech tts;data transcriptions solve", "pdf_keywords": "speech recognition asr;audioonly data transcriptions;automatic speech;automatic speech recognition;speech recognition;end automatic speech;attention based asr;recognition asr models;data transcriptions;recognition asr;data transcriptions work;cycle consistency training;transcriptions;consistency loss training;transcriptions work;training reduced word;transcriptions work propose;results librispeech corpus;audioonly data;hours audioonly data;asr models;combined asr;language model;asr models using;text encoder;encoder tte model;using audio data;consistency training reduced;finally combined asr;librispeech corpus"}, "b568c562fcfad8d7a943a9ea63aca36c487b6d7d": {"ta_keywords": "counterfactual target label;counterfactual target;language causality;language causality offers;counterfactually revised counterparts;patterns language causality;counterfactual;counterfactually revised;causality offers clarity;natural language inference;counterfactually;fail counterfactually;fail counterfactually revised;accords counterfactual target;natural language processing;spurious features mentions;sensitive spurious features;unnecessary changes classifiers;document accords counterfactual;indirect causal effects;language inference tasks;sensitive spurious patterns;data fail counterfactually;spurious patterns language;signal interestingly sentiment;natural language;models sensitive spurious;spurious associations;inference tasks classifiers;cause direct indirect", "pdf_keywords": ""}, "afa9364ec48e38d19099cfc22ac9cb679c4baa39": {"ta_keywords": "biases interact multimodal;intermodality associations biases;exhibits gender biases;multimodal language models;analyzed biases vision;biases vision pre;gender biases;attention paid biases;text based bias;investigate multimodal language;biases vision;gender biases preferring;vision language models;biases learned models;biases learned;multimodal language;biases interact;analyzed biases;bias;biases;associations biases learned;intermodality associations;trained language models;biases preferring;bias analysis;trained vision language;investigate multimodal;vision language;intraand intermodality associations;multimodal", "pdf_keywords": "gender agent language;gender biases text;bias visual linguistic;measuring gender biases;gender information language;sources gender bias;gender biases;models in\ufb02uenced gender;bias de\ufb01ne gender;gender bias;stereotypically gendered entities;gender bias de\ufb01ne;gender bias undesirable;biases text language;de\ufb01ne gender bias;visual linguistic models;sources gender;language models multimodal;linguistic models;text based bias;gender information;language models;predictions visual linguistic;multimodal language models;intermodality associations biases;expressed stereotypically gendered;modality biases expressed;gendered entities;in\ufb02uenced gender information;cross modality biases"}, "8b5071a38718194063cf17ca446ba8d9f4907a18": {"ta_keywords": "words models deepening;models natural language;outperforms models sentiment;syntactically aware models;natural language processing;simple deep neural;deep neural;learning compositionality inputs;models deepening network;deep learning models;question answering;deep learning;words models;existing deep learning;models sentiment;sentiment analysis factoid;syntactically aware;question answering tasks;learning compositionality;models sentiment analysis;deepening network;simple deep;models deepening;dropout;bag words models;deep neural network;focus learning compositionality;novel variant dropout;neural;model syntactically ignorant", "pdf_keywords": ""}, "4240d8e1e5c2ef82d62ba9d7bb323c357c718c1c": {"ta_keywords": "topological layer based;topological layer;efficient topological layer;novel topological layer;topological features input;topological layer general;exploit underlying topological;layer based persistent;underlying topological features;underlying topological;persistent homology arbitrary;persistent homology;topological features;persistence landscapes efficiently;inputs general persistent;based persistence landscapes;pllay efficient topological;based persistent landscapes;persistence landscapes;general persistent homology;layer general deep;layers improve learnability;persistent landscapes;pllay novel topological;efficient topological;information topological features;layer robust;novel topological;persistent landscapes provide;layer inputs", "pdf_keywords": ""}, "1678eccf0f3895dbea6dfac44fc9d4f86de15ff6": {"ta_keywords": "emotion online conversation;predict response emotion;conversation recognized emotion;affective conversation recognized;predict person emotional;response emotion trigger;predicting eliciting emotion;eliciting emotion online;social affective conversation;affective conversation;conversational partner emotional;person emotional reaction;emotion trigger;emotion human interaction;affective communication;emotion based speaker;response emotion;reaction conversational partner;emotional reaction social;social affective communication;emotion online;recognized emotion based;affective communication concerned;reaction social affective;eliciting emotion;emotion trigger paper;emotion human;person emotional;reaction conversational;having reaction conversational", "pdf_keywords": ""}, "751816df0027c0ae6c337ba392a5447bef86ca77": {"ta_keywords": "oligothiophenes transfer learning;state energy oligothiophenes;energy oligothiophenes tddft;oligothiophenes tddft calculations;energy oligothiophenes;phase oligothiophenes transfer;energies poly hexylthiopnene;state energies poly;p3ht single crystal;oligothiophenes transfer;oligomers polymers ml;gas phase oligothiophenes;energies poly;oligomers polymers;conjugated oligomers polymers;phase oligothiophenes;excited state energies;oligothiophenes tddft;chemistry modeling optoelectronic;excited state energy;poly hexylthiopnene p3ht;transfer learning;transfer learning predicted;learning predicted excited;polymers ml;transfer learning models;oligothiophenes;state energies;polymers;conjugated oligomers", "pdf_keywords": ""}, "a4577911d247e472772e2101d21aeaf8f46053cc": {"ta_keywords": "semantic parsing ambiguous;ambiguity semantic parsing;string semantic parsing;parsing ambiguous;semantic parsing;parsing ambiguous input;semantic parsing sp;scfg semantic parsing;parsing given natural;parsing;context free grammars;grammars scfg semantic;tri synchronous grammars;multi synchronous grammars;semantic parsing mr;free grammars scfg;synchronous grammars generate;target string semantic;parsing sp;synchronous grammars;free grammars;string semantic;grammars generate multiple;problem parsing;sp problem parsing;ambiguity semantic;synchronous grammars concrete;grammars scfg;parsing given;grammars concrete approach", "pdf_keywords": ""}, "fdb3969b654ab01be1807bbf84707a80e6283a52": {"ta_keywords": "learning synthesis information;synthesis information extracted;synthesis procedures texts;learning synthesis;synthesis information;supervised learning synthesis;science synthesis procedures;synthesis procedures computational;automatically extracting structured;extracting structures expert;science synthesis;materials science synthesis;structures expert annotated;inorganic compounds automatically;procedures computational synthesis;structured representations synthesis;model procedural text;approaches extracting structures;synthesis procedures;computational synthesis planning;representations synthesis procedures;computational synthesis;synthesis procedures readily;synthesis planning;compounds automatically;syntheses inorganic compounds;extracting structured;syntheses inorganic;analogous synthesis planning;synthesis planning approaches", "pdf_keywords": "models entity extraction;entity extraction;extracting structured representations;relation extraction;entity extraction baseline;dataset entity extraction;models word embedding;extracted scienti\ufb01c entities;ary relation extraction;automatically extracting structured;unsupervised approaches extracting;entity extraction evaluate;word embedding features;extracting structured;relation extraction 37;generative model extract;model extract structures;structured neural network;word embedding;structured representations;extracting structures expert;regression token representations;model procedural text;approaches extracting structures;structured neural;model extract;structured representations synthesis;extracting structures;jointly models entities;extract structures"}, "19f727b7a42a21bc3f99536e8368029f4b9b8e14": {"ta_keywords": "noun images annotated;lexical resource pictures;mapping noun images;noun images;foreign language lexical;annotating foreign language;senses foreign language;images annotated;language lexical;language represents things;nouns natural languages;language represents;language lexical resource;bilingual dictionary;lexical resource usage;images annotated collection;mapping noun;lexical resource;usage bilingual dictionary;word senses foreign;languages meanings;bilingual dictionary conducted;study mapping noun;lexical;foreign language;natural languages meanings;resource pictures;natural languages;tagbag annotating foreign;annotated collection word", "pdf_keywords": ""}, "97ef5081aa4e2984c16ea78b862266e4852c7faf": {"ta_keywords": "email tagging folders;task email tagging;activity management email;task identify email;email tagging;activity represented folder;tasks email including;email messages related;tasks email;evaluate task email;task email;tasks predicting;identify email messages;novel tasks predicting;activity management;email messages;messages relevant folder;folder novel task;activity centered tasks;centered tasks email;real world email;email data;tasks predicting future;ongoing activity represented;facilitate activity management;task finding messages;email including novel;tagging folders;identify email;email including", "pdf_keywords": ""}, "af034b0e893a0a24e41cdb54afb35d4250407f50": {"ta_keywords": "international speech communication;conference international speech;speech communication;speech communication association;international speech;speech;communication association portland;communication association;communication;interspeech 2012;interspeech;interspeech 2012 13th;annual conference international;13th annual conference;conference;conference international;annual conference;association portland oregon;association portland;oregon;portland oregon;portland oregon usa;international;portland;oregon usa;oregon usa september;2012;2012 13th annual;13 2012;2012 13th", "pdf_keywords": ""}, "7657b56d2ac9269b32e8bcbe2a20f99ea17afe09": {"ta_keywords": "modification singing voice;singing voice age;age singing voice;voice characteristics singing;singing voice conversion;waveform modification singing;statistical voice conversion;characteristics singing voice;perceived age singing;statistical voice timbre;measures voice characteristics;use statistical voice;controlling voice timbre;voice age;voice age singer;voice timbre control;modification singing;proposed statistical voice;voice conversion;age singing;statistical voice;voice characteristics;voice timbre singers;age singer perceived;voice conversion technique;singing voice singers;controlling voice;varieties voice timbre;characteristics singing;voice singers", "pdf_keywords": ""}, "869d53277b0ec5e47a30b874aeb157df88649ea0": {"ta_keywords": "\u6587\u8108\u3092\u8003\u616e\u3057\u305f\u78ba\u7387\u7684\u30e2\u30c7\u30eb\u306b\u3088\u308b\u8a71\u3057\u8a00\u8449\u306e\u6574\u5f62 \u97f3\u58f0\u691c\u7d22 \u8981\u7d04;\u6587\u8108\u3092\u8003\u616e\u3057\u305f\u78ba\u7387\u7684\u30e2\u30c7\u30eb\u306b\u3088\u308b\u8a71\u3057\u8a00\u8449\u306e\u6574\u5f62 \u97f3\u58f0\u691c\u7d22;\u6587\u8108\u3092\u8003\u616e\u3057\u305f\u78ba\u7387\u7684\u30e2\u30c7\u30eb\u306b\u3088\u308b\u8a71\u3057\u8a00\u8449\u306e\u6574\u5f62;\u97f3\u58f0\u691c\u7d22 \u8981\u7d04 \u7b2c11\u56de\u97f3\u58f0\u8a00\u8a9e\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0;\u8981\u7d04 \u7b2c11\u56de\u97f3\u58f0\u8a00\u8a9e\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0;\u7b2c11\u56de\u97f3\u58f0\u8a00\u8a9e\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0;\u8981\u7d04;\u97f3\u58f0\u691c\u7d22;\u97f3\u58f0\u691c\u7d22 \u8981\u7d04", "pdf_keywords": ""}, "bff4d630cbea6a90b149b28caff5489c1a4ccaad": {"ta_keywords": "language model estimation;stochastically tagged corpus;tagged corpus;estimation stochastically tagged;corpus;language model;model estimation stochastically;stochastically tagged;estimation stochastically;model estimation;language;tagged;stochastically;estimation;model", "pdf_keywords": ""}, "7e0342b304ca8ce564a664eb17e85358b07488fe": {"ta_keywords": "adaptation noise mixture;joint speaker adaptation;speaker adaptation noise;estimation variation speaker;unsupervised joint speaker;noise mixture model;noise suppression unsupervised;speaker adaptation;method noise mixture;approach noise suppression;achieves speaker adaptation;noise mixture;mixture model estimation;based noise suppression;noise suppression including;estimation accurate noise;noise suppression;model based noise;estimation method noise;noise model crucial;noise suppression deal;accurate noise model;adaptation noise;variation speaker characteristics;joint speaker;mixture model vts;simultaneously achieves speaker;noise model;speaker characteristics crucial;variation speaker", "pdf_keywords": ""}, "fcdac45272543b4f8b8eaa59d66044d1b7018494": {"ta_keywords": "trained translation model;neural machine translation;pre trained translation;trained translation;forward translation model;final translation models;translation models;translation model pseudoparallel;train forward translation;translation quality pseudo;translation models lower;translation model;machine translation;forward translation;translation model validation;machine translation nmt;better translation quality;translation quality;translation model method;translation nmt generating;improve performance neural;translation nmt;performance neural;meta learning algorithm;meta learning;performance neural machine;data pre trained;generating pseudo parallel;pseudo parallel data;method meta learning", "pdf_keywords": "neural machine translation;trained translation model;pre trained translation;trained translation;machine translation nmt;translation tasks performance;performance translation tasks;machine translation;translation nmt generating;translation model;translation tasks;meta translation;performance translation;2021 meta translation;multilingual translation;translation nmt;14 multilingual translation;translation nmt delivers;multilingual translation setting;translation model paper;translation setting;improve performance neural;meta translation hieu;baselines introduction neural;nmt setting multilingual;performance neural;performance neural machine;abstract translation;edu abstract translation;translation setting method"}, "e8f42dd98d7f546036fa4a1109c3fe3dd98f9647": {"ta_keywords": "argumentation related tasks;natural language argument;argument reasoning comprehension;argument component identification;argument comprehension requires;argument comprehension;argument summarization focus;task argument comprehension;arguments news comments;argument reasoning;abstractive argument summarization;detection argument component;argument summarization;stance detection argument;instances suitable argumentation;argumentation related;focus argument reasoning;sense argument reasoning;argumentation;authentic arguments news;suitable argumentation related;argument component;task argument reasoning;arguments news;identification abstractive argument;suitable argumentation;attention language models;reasoning comprehension task;language argument;language argument reason", "pdf_keywords": ""}, "4358335263622fe189cf95c613f4d6fdcb67fbea": {"ta_keywords": "\u97f3\u58f0\u5165\u529b\u306b\u57fa\u3065\u304f\u97fb\u5f8b\u5236\u5fa1\u6a5f\u80fd\u3092\u6709\u3059\u308bhmm\u97f3\u58f0\u5408\u6210\u30b7\u30b9\u30c6\u30e0 \u30dd\u30b9\u30bf\u30fc \u30c7\u30e2\u30bb\u30c3\u30b7\u30e7\u30f3;\u97f3\u58f0\u5165\u529b\u306b\u57fa\u3065\u304f\u97fb\u5f8b\u5236\u5fa1\u6a5f\u80fd\u3092\u6709\u3059\u308bhmm\u97f3\u58f0\u5408\u6210\u30b7\u30b9\u30c6\u30e0 \u30dd\u30b9\u30bf\u30fc;\u97f3\u58f0\u5165\u529b\u306b\u57fa\u3065\u304f\u97fb\u5f8b\u5236\u5fa1\u6a5f\u80fd\u3092\u6709\u3059\u308bhmm\u97f3\u58f0\u5408\u6210\u30b7\u30b9\u30c6\u30e0;\u30c7\u30e2\u30bb\u30c3\u30b7\u30e7\u30f3 \u7b2c16\u56de\u97f3\u58f0\u8a00\u8a9e\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0;\u7b2c16\u56de\u97f3\u58f0\u8a00\u8a9e\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0;\u30dd\u30b9\u30bf\u30fc \u30c7\u30e2\u30bb\u30c3\u30b7\u30e7\u30f3 \u7b2c16\u56de\u97f3\u58f0\u8a00\u8a9e\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0;\u30dd\u30b9\u30bf\u30fc \u30c7\u30e2\u30bb\u30c3\u30b7\u30e7\u30f3;\u30c7\u30e2\u30bb\u30c3\u30b7\u30e7\u30f3;\u30dd\u30b9\u30bf\u30fc", "pdf_keywords": ""}, "03fff40cff6ac531e340f6ffb376e34609770846": {"ta_keywords": "coordinated twitter accounts;twitter accounts examining;networks coordinated twitter;coordinated twitter;twitter accounts;accounts likely coordinated;coordinated campaigns used;coordinated campaigns;retweets temporal patterns;coordinated coordinated campaigns;influence campaigns;manipulate social media;campaigns used influence;twitter;influence campaigns diverse;detect networks coordinated;sequences retweets temporal;social media platforms;hashtag sequences retweets;sequences retweets;traces shared accounts;networks coordinated;retweets temporal;social media;retweets;groups accounts likely;uncover groups accounts;likely coordinated coordinated;influence manipulate social;accounts examining identities", "pdf_keywords": "coordinated twitter accounts;twitter accounts examining;networks coordinated twitter;coordinated twitter;coordinated campaigns used;manipulate social media;twitter accounts;uncovering coordinated networks;retweets temporal patterns;coordinated campaigns;coordinated networks social;accounts likely coordinated;social media methods;abstract coordinated campaigns;networks social media;twitter;hashtag sequences retweets;sequences retweets temporal;networks social;sequences retweets;detect networks coordinated;studies in\ufb02uence campaigns;campaigns used in\ufb02uence;coordination information warfare;social media;retweets temporal;uncovering coordinated;networks coordinated;in\ufb02uence campaigns;1observatory social media"}, "eb07ff030df4c3dc20e85d89c2e0d0cc730918a0": {"ta_keywords": "separation using speaker;recording speaker inventory;speech separation using;continuous speech separation;known speech separation;speech separation;using speaker inventory;speech separation received;speaker inventory;long recording speaker;speaker inventory fully;separating participating speakers;facilitate speech separation;speaker inventory long;recording speaker;speaker inventory ssusi;reverberant long recording;multi talker recording;improve separation performance;extracting target speech;talker recording;speaker signals leveraging;talker recording work;leveraging additional speaker;model long recordings;speaker voice snippet;target speaker voice;long recordings;significantly improve separation;long recording", "pdf_keywords": "speaker clustering;using speaker clustering;recording speaker inventory;constructs speaker inventory;speaker clustering methods;speaker inventory;informed speaker inventory;speaker inventory mixed;speaker inventory fully;build speaker inventory;speaker inventory work;inventory long speech;long speech mixture;speaker inventory long;speaker embeddings long;speaker embeddings;recordings performed clustering;extracted speaker embeddings;cssusi constructs speaker;embeddings long recordings;long recording speaker;recording speaker;speech mixture using;cssusi extracted speaker;speech mixture;speaker enrollments using;long recordings;self informed speaker;multi talker conversations;enrollments using speaker"}, "4c66979a31fa4be5b5814fdb5cb8411572d61da8": {"ta_keywords": "normalization language data;normalization language;deals normalization language;language data;deals normalization;normalization;language data early;luther bible weighted;aware rewrite rules;versions luther bible;paper deals normalization;bible weighted;bible weighted according;rewrite rules;luther bible;high german evaluation;context aware rewrite;rewrite rules apply;rules apply sequences;aligned versions luther;sequences characters paper;sequences characters;apply sequences characters;new high german;versions luther;german evaluation;aware rewrite;matches clearly outperforms;bible;language", "pdf_keywords": ""}, "729260566c7fdf689bb04eaaecef59d40da93ef7": {"ta_keywords": "attack deep neural;detection deception attack;image classifier attacker;dnn malicious deception;vulnerability dnn malicious;dnn based classifier;classifier attacker;deception attack deep;malicious deception attacks;vulnerability dnn;images dnn based;classification autonomous cyber;neural network dnn;classifier attacker model;dnn based image;deception attacks;modified images dnn;network dnn based;deception attacks proposed;dnn malicious;detection deception;images dnn;dnn based;significant dnn based;network dnn;deception attack;shown vulnerability dnn;deep neural network;attack deep;image classifier", "pdf_keywords": "high attack detection;attacks implemented gaussian;attack detection;attack detection probability;image classi\ufb01er attacker;white box attacks;detection reduce computational;adversarial;regression based detector;based detection reduce;detects adversary;detection probability popular;detects adversary perturbing;facilitates detection modi\ufb01ed;perturbation based detection;detection probabilities;detection modi\ufb01ed images;facilitates detection;detection probability;attacks implemented;detection;box attacks implemented;detection reduce;images dnn based;adversarial examples;attacker model proposed;based detection;detection modi\ufb01ed;probability high attack;attacker model"}, "ede8ba65c4db10d357d9c3bf8e75b092f536fc84": {"ta_keywords": "vision language navigation;augmentation pragmatic reasoning;guided natural language;natural language instructions;instruction follower;instruction machine learning;data augmentation pragmatic;augmentation implement pragmatic;baseline instruction follower;vision language;perceptual context experiments;approach vision language;augmentation pragmatic;model navigation guided;language navigation;instruction followers;navigation guided;navigation guided natural;enable learning reasoning;learning reasoning;instructions data augmentation;pragmatic reasoning panoramic;driven data augmentation;reasoning panoramic action;instruction followers use;perceptual context;approach speaker driven;natural language;language navigation addresses;context experiments", "pdf_keywords": "vision language navigation;challenge vision language;vision language;submission vision language;approach vision language;participated vision language;model vision language;language navigation challenge;language navigation task;language navigation challenge3;action sequences instruction;instruction submission vision;language navigation substantially;language navigation;action sequences instructions;instructions action sequences;navigation task r2r;online challenge vision;instruction follower;augmentation pragmatic reasoning;instruction generation speaker;augmentation implement pragmatic;speaker predicts navigation;data augmentation pragmatic;baseline instruction follower;mapping action sequences;approach speaker driven;mapping instructions action;language navigation addresses;sequences instruction generation"}, "d5634a21b3727258822b78f5c5ababf7261a5c79": {"ta_keywords": "speech enhancement separation;learning speech enhancement;methods speech enhancement;noise speech separation;speech enhancement;apply speech enhancement;speech enhancement suppresses;tasks speech enhancement;ssl enhancement separation;learning ssl enhancement;speech separation;speech separation extracts;supervised learning speech;experimental results voicebank;voicebank;supervised learning ssl;ssl enhancement;learning based enhancement;self supervised learning;enhancement separation downstream;enhancement separation methods;results voicebank;based enhancement separation;speech interfering;enhancement separation limited;enhancement separation;target speech interfering;speech interfering speakers;enhancement separation furthermore;self supervised", "pdf_keywords": "ssl speech enhancement;ssl models speech;models speech enhancement;speech enhancement separation;ssl speech;31 speech enhancement;speech enhancement;experimental results voicebank;ssl paradigm speech;speech processing universal;apply speech enhancement;upstream models speech;tasks speech processing;investigate ssl speech;methods speech enhancement;speech processing;voicebank;toolkit 31 speech;models speech;results voicebank;speech related tasks;voicebank demand;ssl representations;results voicebank demand;upstream methods speech;ssl upstream models;existing ssl models;voicebank demand libri2mix;ssl representations consistently;ssl models"}, "3813627f7fec57aa4c15b791e36912f470273bb1": {"ta_keywords": "clusters twitter hashtags;clustering analyze hashtags;topical clusters hashtags;hashtag usage twitter;clusters hashtags;characterizing communities hashtag;twitter hashtags;twitter hashtags propose;trends hashtag usage;analyze hashtags analysis;analyze hashtags;hashtags analysis;hashtags analysis greater;clusters twitter;trends hashtag;communities hashtag usage;communities hashtag;hashtags particular topical;clusters hashtags shift;temporal trends hashtag;users interact hashtags;hashtags propose;hashtag usage characterizing;analysis social media;usage twitter;twitter 2020 covid;hashtags;interact hashtags;quality clusters twitter;hashtags propose novel", "pdf_keywords": ""}, "3e8420d1bebb3f93d285da2de801d2e43b290880": {"ta_keywords": "neural sequence labeling;sequence labeling models;entity recognition ner;sequence labeling self;labeling models large;tagging dialog systems;sequence labeling;nlp tasks;labeling self training;multilingual ner datasets;named entity recognition;challenge neural sequence;sequence labeling important;nlp tasks named;tagging dialog;slot tagging dialog;human annotation;entity recognition;processing nlp tasks;cost human annotation;datasets slot tagging;slot tagging datasets;tagging datasets task;labeling models;labeling self;ner slot tagging;shot neural sequence;human annotation privacy;challenge neural;training shot neural", "pdf_keywords": "neural sequence taggers;neural sequence labeling;trained language model;sequence labeling models;self training framework;sequence taggers;tagging datasets task;labeled training;pre trained language;training meta learning;trained language;sequence taggers labels;labeled training samples;task oriented dialog;slot tagging datasets;tagging task oriented;tagging task;datasets task oriented;slot tagging task;self training meta;number labeled training;tagging datasets;components self training;labeling models;sequence labeling;validation shot learning;training neural sequence;training framework;ner slot tagging;dialog systems"}, "e51bca890c004c43b25c5a5e7aa968fe70ec2668": {"ta_keywords": "stacked graphical learning;stacked graphical models;stacked graphical model;meta learning scheme;traditional machine learning;relational datasets hyperlinked;graphical learning;graphical learning recent;algorithm stacked graphical;meta learning;machine learning methods;relational datasets;describes algorithm stacked;relation template stacked;algorithm stacked;stacked graphical;graphical models;predictions related instances;learning methods;machine learning;learning scheme;datasets hyperlinked web;called stacked graphical;learning scheme called;instance features predictions;graphical models demonstrated;reality relational datasets;related instances;base learner;template stacked graphical", "pdf_keywords": ""}, "622e05f5d3dd430644288d5048f6050f37947de7": {"ta_keywords": "related domain adaptation;domain adaptation;domain adaptation exploiting;learning named entity;domains genres tasks;named entity recognition;entity recognition prior;transfer learning named;supervised transfer learning;subproblem domain adaptation;transfer learning information;transfer learning;entity recognition;problem transfer learning;hierarchy transfer learning;entity recognition motivated;domain adaptation model;adaptation related domain;information domains genres;supervised transfer;trained source domain;feature hierarchy transfer;useful information domains;domains genres;genre adaptation;structure supervised transfer;closely related domains;inter genre adaptation;named entity;genre adaptation related", "pdf_keywords": ""}, "146b84bdd9b9078f40a2df9b7ded26416771f740": {"ta_keywords": "inverse reinforcement learning;based inverse reinforcement;inverse reinforcement;problem inverse reinforcement;risk sensitive reinforcement;learning markov decision;sensitive reinforcement learning;reinforcement learning markov;gradient based inverse;reinforcement learning;reinforcement learning algorithm;markov decision processes;markov decision;risk;learning markov;agent risk;minimize loss;minimize loss function;gradient loss function;loss function defined;risk metrics models;risk metrics;agent risk sensitive;models human decision;coherent risk metrics;loss function respect;gradient loss;loss function;making agent risk;risk sensitive", "pdf_keywords": ""}, "77899bac8f463b7a77c0c282748e989d419386e7": {"ta_keywords": "semi supervised learning;difficult relation extraction;supervised learning ssl;relation extraction;relation extraction task;learning agreement constraints;strategies semi supervised;semi supervised;supervised learning agreement;ensembles semi supervised;regularization constraints learners;supervised learning;constraints learners;domain specific heuristics;learning agreement;constraints learners used;supervised;regularization constraints;training novel domain;ssl heuristics;individual ssl heuristics;heuristics automatically combined;ssl heuristics multiple;bayesian optimization;bayesian optimization methods;specify ensembles semi;learning ssl;multiple heuristics automatically;using bayesian optimization;agreement constraints", "pdf_keywords": "supervised learning declaratively;semi supervised learning;learner declaratively constrained;semi supervised;declaratively constrained entropy;learning declaratively speci\ufb01ed;dce learner declaratively;supervised learning ssl;constrained entropy uses;learning declaratively;speci\ufb01ed entropy constraints;constrained entropy;strategies semi supervised;entropy constraints;declaratively constrained;entropy uses constrained;supervised learning;entropy constraints arxiv;constrained speci\ufb01cation language;supervised;dif\ufb01cult relation extraction;relation extraction;dce learner;cohen machine learning;\ufb02exible underlying learning;sun machine learning;uses constrained speci\ufb01cation;declaratively speci\ufb01ed entropy;relation extraction task;constrained speci\ufb01cation"}, "1a53e7446274016f737236bdd48e3ff05d966384": {"ta_keywords": "nl code mining;code mining wide;code mining;language code retrieval;code natural language;code retrieval;natural language code;learning aligned code;code summarization data;quality code snippets;retrieval code summarization;data natural language;snippets correspondence features;language nl code;extracted snippets correspondence;code retrieval code;classifier language testing;training classifier language;code summarization;code snippets;annotate data learning;code using neural;code synthesis natural;natural language pairs;synthesis natural language;structure extracted snippets;code synthesis;classifier language;snippets correspondence;natural language nl", "pdf_keywords": "code natural language;extracted snippets correspondence;snippets correspondence features;natural language pairs;machine translation;snippet structural features;structure extracted snippets;code snippet correspondence;machine translation method;snippets correspondence;combines snippet structural;code pairs posts;model machine translation;snippet correspondence;extracted snippets;nl code pairs;natural language query;snippet structural;extracting aligned code;extracting aligned nl;code using neural;natural language;snippets;language pairs website;annotated examples using;code pairs;language query conclusions;aligned nl code;snippet correspondence original;annotated examples"}, "d0e9c5cb669dec908a38eab4315cbf101bc4b0a0": {"ta_keywords": "language models trained;neural language models;language models data;language models experiments;language models;use language models;machine translation;language models viable;experiments machine translation;spanish neural language;language models makes;select similar sentences;trained small domain;general domain corpora;use neural language;neural language;selection using neural;words neural language;data selection improvements;general domain text;using neural language;language pairs;domain corpora;russian spanish neural;small domain text;corpora;spanish neural;train small domain;adaptation data selection;evaluation language pairs", "pdf_keywords": ""}, "308eb6751a3a1da0f64f291366c8ee27f84b3f16": {"ta_keywords": "dual dfa learning;dfa learning;dfa learning problem;learning problem hardness;dual dfa;learning order representations;demonstration learning;demonstration learning order;programming demonstration learning;learning problem;hardness results programming;learning order;dfa;learning;order representations;representations extended abstract;representations;problem hardness results;order representations extended;representations extended;programming demonstration;problem hardness;results programming demonstration;dual;hardness results;programming;extended abstract;hardness;abstract;results programming", "pdf_keywords": ""}, "2550fafc0cbd8bbf7aadd864ac569596d33db038": {"ta_keywords": "grounding missing nlp;nlp tasks;nlp usage;nlp community;nlp community seen;facilitate interaction language;missing nlp tasks;implicitly capture nlp;questions aspects grounding;capture nlp;nlp usage differs;grounding facilitate;interaction language;formally defines grounding;interaction language technologies;nlp;grounding facilitate interaction;textual modality investigate;nlp tasks contrast;language technologies;missing nlp;recent grounding facilitate;capture nlp usage;defines grounding;non textual modality;cognitive science formally;textual modality;aspects grounding;data non textual;non textual", "pdf_keywords": "grounding language;dialog discourse patterns;contributing grounding language;multilingual datasets tasks;grounding language different;dialog discourse;discourse patterns knowledge;multilinguality contributing linguistic;discourse patterns;natural language processing;language processing;linguistic diversity;multilingual datasets;types multilinguality contributing;trends multilingual datasets;knowledge grounding;contributing linguistic diversity;media dialog discourse;language different trends;data types multilinguality;linguistic;analysis trends grounding;annotations trends;augmenting annotations trends;natural language;multilinguality;types multilinguality;multilinguality contributing;linguistic diversity interview;methods natural language"}, "7ce80c7df1774e4483b32a813d54a8ff35dd0163": {"ta_keywords": "dynamics stackelberg games;nash stackelberg equilibrium;guaranteed stackelberg equilibrium;descent stackelberg equilibria;converges stackelberg equilibria;stackelberg equilibria;stackelberg games;dynamics stackelberg equilibria;stackelberg equilibrium concepts;stackelberg equilibria zero;stackelberg games stable;learning dynamics stackelberg;stackelberg equilibrium;stackelberg gradient dynamics;stackelberg equilibrium zero;gradient dynamics stackelberg;stackelberg equilibria given;gradient descent stackelberg;point guaranteed stackelberg;critical points stackelberg;strategy stable critical;dynamics stackelberg;zero sum games;provably converges stackelberg;converges stackelberg;stackelberg gradient;connections nash stackelberg;points stackelberg gradient;consider hierarchical game;hierarchical game", "pdf_keywords": "stackelberg games generative;gans using stackelberg;stackelberg learning dynamics;learning dynamics stackelberg;gradient descent stackelberg;gradient dynamics stackelberg;dynamics stackelberg games;stackelberg learning;stackelberg gradient dynamics;bene\ufb01ts training gans;stackelberg games;gradient descent dynamics;stackelberg gradient;descent stackelberg equilibria;games generative adversarial;stackelberg game;stackelberg game establish;games generative;stackelberg equilibria opti;using stackelberg learning;training gans;learning dynamics;convergence learning dynamics;dynamics stackelberg equilibria;structure stackelberg game;stackelberg equilibria;nash stackelberg equilibrium;gans empirical bene\ufb01ts;mization landscape gans;stackelberg equilibrium concepts"}, "203636315f7c9526189d88c541bedf623d63ea7c": {"ta_keywords": "factoid question answering;question answering qa;question answering;answering qa task;question answers ambiguous;answers ambiguous questions;answering qa;focuses factoid questions;ambiguous question answers;factoid questions ambiguous;answer summaries questions;qa tasks eli5;qa tasks;summaries questions ambiguous;ambiguous questions synthesize;factoid questions;answers ambiguous;qa task focuses;questions synthesize factual;qa task;answers depending interpretation;question answers;task focuses factoid;summary resolves ambiguity;progress factoid;reliable evaluation metrics;synthesize factual information;strong progress factoid;factoid;metric human judgments", "pdf_keywords": "factoid question answering;question answering qa;qa dataset factoid;questions asqa factoid;question answering;factoid qa;factoid questions asqa;answering qa;human evaluations;asqa factoid questions;factoid questions meet;factoid questions;aspect factoid qa;factoid qa \ufb02uency;human evaluations iii;form question answering;automated evaluation metric;answering qa conclusion;evaluations constructing long;correctness aspect factoid;dataset factoid questions;automated evaluation;interface human evaluations;novel automated evaluation;asqa factoid;question answering work;long form qa;progress factoid;evaluation procedures asqa;agreement automated metric"}, "018bf5da2ba1f1901e98f72c7eedbf6b91967192": {"ta_keywords": "differential privacy bert;privacy preserving bert;privacy preserving nlu;privacy bert;privacy adaptive lm;language understanding privacy;pretrained lms privacy;privacy bert fine;lms privacy preservation;propose privacy adaptive;privacy adaptive;differential privacy;lms privacy;level privacy preservation;retaining level privacy;privacy preserving;understanding privacy preserving;explorations privacy preserving;privacy preservation;privacy variant;privacy utility;quantify level privacy;privacy variant local;understanding privacy;future explorations privacy;privacy preservation provide;level privacy;level privacy protection;privacy protection;d\u03c7 privacy variant", "pdf_keywords": "differential privacy bert;privacy preserving bert;privacy bert finetuning;privacy adaptive lm;privacy adaptive pretraining;privacy bert;privacy adaptive;propose privacy adaptive;language understanding privacy;method privacy adaptive;level privacy preservation;privacy protection natural;retaining level privacy;differential privacy;privacy preserving;privacy preservation;privacy variant local;privacy variant;abstract privacy preservation;applying privacy variant;quantify level privacy;understanding privacy preserving;privacy utility;preserving bert;protection natural language;privacy preservation provide;level privacy;level privacy protection;local differential privacy;bert pretrained denoising"}, "a6a7374c5ddac1446ceab9d7cbe5a3305238d0ee": {"ta_keywords": "conversation dialog corpora;conversation corpora;trigram conversation turn;turn trigram conversation;conversation corpora contains;conversation corpora created;trigram conversation;filter conversations;end conversation corpora;dialog corpora;talking extract conversations;work conversation corpora;dialogue systems;extract conversations;natural conversation templates;conversations speakers example;dialog corpora television;conversation dialog;filter conversations speakers;dialogue systems require;conversation performed speakers;conversation templates;conversation turns performed;extract conversations television;represent conversation turns;conversation templates examples;conversation turn;based dialogue systems;conversation turn allow;conversations speakers", "pdf_keywords": ""}, "963c4c34f1292f64a6e9fc04428fc7a0893b8ef3": {"ta_keywords": "enhanced residual attention;residual deep attention;residual attention group;deep attention;attention group;attention processing module;deep attention mechanism;channel attention processing;spatial channel attention;residual attention;attention processing;attention;attention mechanism adaptive;sizes convolution kernels;channel attention;propose residual deep;feature fusion magnify;convolution kernels extract;convolution kernels;attention group erag;residual deep;feature fusion;sizes convolution;visual quality combination;rescales hierarchical features;attention mechanism;different sizes convolution;raan stronger feature;image super;convolution", "pdf_keywords": ""}, "53feb3b34425ea95c259e8d0693edd490d6b470f": {"ta_keywords": "words elicited erp;processing mistaken words;erp correct words;mismatch feelings semantically;mistaken words elicited;semantically mistaken words;sentences measuring erp;erp kind brain;feelings mistaken words;feelings semantically mistaken;auditory stimuli japanese;mistaken words presented;mistaken words visual;words visual auditory;stimuli japanese;mistaken words japanese;words presented auditory;auditory stimuli version;semantic processing mistaken;reflects mismatch feelings;words semantic violation;erp experiments;presented auditory stimuli;visual auditory stimuli;auditory stimuli;category mistaken words;erp experiments conducted;analysis mismatch feelings;visual stimuli;semantic violation words", "pdf_keywords": ""}, "a792d5a1e9a6a53edd8cbc00e387bc07c54e423c": {"ta_keywords": "eliciting truthful responses;truthful responses agents;bayesian truth serum;eliciting truthful;crowdsourced evaluation tasks;problem eliciting truthful;simple truth serums;massively crowdsourced evaluation;crowdsourced evaluation;serums massively crowdsourced;bayesian truth;agents beliefs arbitrary;truth serums;allow agents beliefs;responses agents;crowdsourced;truth serum;method bayesian truth;truth serums massively;massively crowdsourced;agents beliefs;peer prediction;elicitation agents;peer prediction method;truth serum quite;elicitation agents furthermore;pioneered peer prediction;truthful responses;elicitation;eliciting", "pdf_keywords": ""}, "10efdde1ae3a9d359ac1aae0bd5ef7bfd68810dd": {"ta_keywords": "mlm multilingual representation;multilingual representation learning;language modeling mlm;pre trained multilingual;mlm multilingual;multilingual language models;limitation mlm multilingual;multilingual pre training;trained multilingual;trained multilingual language;masked language modeling;multilingual representation;predict language;language learning objective;key language learning;predict language specific;predicting masked words;representations dict mlm;disincentivizes learning language;model predict language;rely masked language;language learning;representation learning pre;language models;goal multilingual pre;nlp tasks especially;gains natural language;dict mlm works;language specific token;contextualized representations dict", "pdf_keywords": "mlm multilingual representation;mlm improved multilingual;multilingual representation learning;trained multilingual model;mlm multilingual;crosslingual sentence retrieval;limitation mlm multilingual;lingual word alignment;predict cross lingual;trained multilingual;multilingual pre training;pre trained multilingual;multilingual representation;bilingual dictionaries demonstrate;monolingual corpora;multilingual model;entailment xnli crosslingual;improved multilingual pre;bilingual dictionaries;using bilingual dictionaries;improved multilingual;concatenated monolingual corpora;cross lingual synonym;facilitate cross lingual;goal multilingual pre;cross lingual;multilingual;cross lingual word;training dict mlm;goal multilingual"}, "ef1d93b03c20b2f488b66e8e2c24fceb2105d58f": {"ta_keywords": "language metaphor detection;metaphor detection cross;metaphor detection;metaphoric expressions languages;language metaphor;identify metaphoric expressions;metaphorically using lexical;model identify metaphoric;metaphoric expressions;identify metaphoric;lingual model transfer;work language metaphor;bilingual dictionary model;metaphor;cross lingual model;detection cross lingual;literally metaphorically using;literally metaphorically;lingual model;metaphorically using;metaphoric;bilingual dictionary;lexical semantic;metaphorically;using lexical semantic;meant literally metaphorically;lexical semantic features;lingual;using english resources;cross lingual", "pdf_keywords": ""}, "3993788eb252f5eb7fc19e9f98357a72f9f0476d": {"ta_keywords": "mixed membership stochastic;mixed membership graphs;membership network models;membership stochastic block;membership block models;membership graphs;membership stochastic;membership graphs small;regularization mixed membership;classical network models;mixed membership network;observations mixed membership;network models;distribution latent roles;aggregates mixed membership;graph different latent;network models using;membership network;roles entropic regularization;noisy copies aggregates;stochastic block model;penalizing models aggregates;mixed membership block;network models actual;assigned nodes noisy;mixed membership controlled;nodes usually;nodes usually taking;latent roles;models aggregates", "pdf_keywords": ""}, "71c7104eaed93497824cf197949c77e7d6cb36d3": {"ta_keywords": "domain question answering;retrieval knowledge bases;learning retrieve reasoning;question answering iterative;corpus knowledge base;retrieve reasoning heterogeneous;question answering;iterative retrieval knowledge;answers drawn corpus;answering iterative retrieval;retrieval knowledge;corpus knowledge;knowledge bases text;reasoning heterogeneous information;knowledge bases;best answer pullnet;retrieve reasoning;drawn corpus knowledge;knowledge base;learning retrieve;question answering qa;answer pullnet;reasoning heterogeneous;knowledge base kb;answer pullnet open;framework learning retrieve;answering iterative;iterative retrieval;corpus supplemented large;retrieval", "pdf_keywords": "corpus reasoning heterogeneous;reasoning heterogeneous information;corpus reasoning;learning retrieve corpus;kb corpus reasoning;best answer pullnet;answer pullnet integrated;learning retrieve;reasoning heterogeneous;answer pullnet;supplemented information extracted;answers using corpus;reasoning heterogeneous \ufb01nd;information extracted;learning retrieve kb;conclusions pullnet;retrieve corpus;conclusions pullnet novel;pullnet integrated framework;deriving answers using;framework learning retrieve;heterogeneous information;information extracted text;kb corpus;combining heterogeneous information;pullnet;incomplete supplemented information;using corpus;pullnet novel;corpus"}, "911536dc3dfbbbf2bb8d71181b31e0aa7920b9f6": {"ta_keywords": "expert strategies algorithm;algorithm expert solutions;experts distributed prediction;expert solutions aggregated;expert solutions;experts algorithms;adaptive algorithm expert;experts algorithms capable;mixing expert solutions;expert strategies;experts distributed;combination experts distributed;games financial markets;strategies algorithm constructed;algorithm expert;strategies algorithm;methods experts algorithms;expert;expert solutions help;predictions suffering losses;distributed prediction;adaptive algorithm;strategies games financial;exponentially weight expert;financial markets suggested;algorithm presented strategies;losses inaccuracy solutions;aggregated sustained losses;financial markets;experts", "pdf_keywords": ""}, "798e45ea830884be36c3f526d3b169eaba95f989": {"ta_keywords": "zero sum games;continuous zerosum games;local nash equilibria;nash equilibria zero;zerosum games;zerosum games differential;nash equilibria generically;nash equilibria isolated;generic local nash;strict local nash;nash equilibria generic;sum continuous games;differential nash equilibria;particular local nash;nash equilibria continuous;games differential nash;equilibria continuous zerosum;sum games structurally;adversarial learning;local nash;sum games;equilibria zero sum;adversarial;points local nash;adversarial network;adversarial network approaches;nash equilibria;differential nash;equilibria generic local;games machine learning", "pdf_keywords": "local nash equilibria;generic local nash;nash equilibria nondegenerate;strict local nash;differential nash equilibria;local optimality players;nondegenerate differential nash;nash equilibria continuous;nash equilibria isolated;re\ufb01nement local nash;continuous zerosum games;nash equilibria generic;nash equilibria zero;equilibria generic local;local nash;differential nash structurally;equilibria continuous zerosum;games local nash;optimality players generic;nash structurally stable;differential nash;local optimality;nash equilibria re\ufb01nement;nash equilibria de\ufb01ned;characterize local optimality;zero sum games;nash equilibria;zerosum games;equilibria re\ufb01nement local;sum continuous games"}, "8a0a8568acf2b95c9cb471e28ee6b25c5e4fe186": {"ta_keywords": "knowledge representations senticnet;sentics conceptual affective;semantics sentics conceptual;semantic affective resource;available semantic affective;semantics sentics;semantic affective;senticnet models nuanced;nuanced semantics sentics;representations senticnet;conceptual affective information;senticnet;senticnet models;senticnet makes use;senticnet makes;techniques senticnet makes;representations senticnet publicly;affective resource;sense knowledge representations;sentics conceptual;affective information;concept level sentiment;conceptual affective;techniques senticnet;affective information associated;affective resource concept;sentiment analysis;level sentiment analysis;common sense knowledge;reduction techniques senticnet", "pdf_keywords": ""}, "e75c388b60cf447be7148be25feeee3e10d12cf4": {"ta_keywords": "interpolation extrapolation fundamental;interpolate training;interpolation happens tasks;dataset interpolation surely;notion interpolation extrapolation;interpolation extrapolation definition;correctly interpolate training;interpolate training data;extrapolation fundamental;extrapolation definition;interpolation surely happens;extrapolation fundamental various;100 dataset interpolation;dataset interpolation;interpolation extrapolation;interpolation happens;generalization performances learning;interpolation surely;current interpolation extrapolation;notion interpolation;learning high dimension;interpolation;extrapolation;extrapolation definition indicator;dimension amounts extrapolation;learning function approximation;mis conception interpolation;happens notion interpolation;extrapolation second;ability correctly interpolate", "pdf_keywords": "interpolation extrapolation fundamental;interpolation extrapolation notions;dimension amounts extrapolation;notion interpolation extrapolation;extrapolation fundamental;extrapolation fundamental various;extrapolation notions;extrapolation notions hard;extrapolation indicators generalization;intrinsic dimension data;interpolation extrapolation;learning high dimension;data interpolation surely;interpolation extrapolation indicators;interpolation surely occurs;interpolation surely;maintain interpolation new;notion interpolation;interpolation new samples;maintain interpolation;extrapolation;use interpolation extrapolation;real data interpolation;interpolation new;underlying intrinsic dimension;extrapolation randall;\ufb01elds deep learning;abstract notion interpolation;data interpolation;intrinsic dimension"}, "42c3c50b8e368ee2e1b52d010b6c53b3d732770c": {"ta_keywords": "sentiment analysis spoken;speech sentiment analysis;texts speech sentiment;learn sentiment information;speech sentiment approach;sentiment annotation available;human sentiment annotation;sentiment annotation;transcripts based sentiment;sentiment annotation boost;learn sentiment;speech sentiment;end speech sentiment;data human sentiment;sentimentspecific information speech;sentiment information;sentiment information written;models learn sentiment;trained language models;speech dataset training;sentiments text representation;automatic speech;sentiment analysis;understanding sentiment;semi supervised training;annotation boost;trained language model;understanding sentiment proposed;employing automatic speech;sentiment analysis separately", "pdf_keywords": "human sentiment annotation;annotation speech dataset;sentiment annotation;sentiment annotation boost;sentiment annotation available;e2e speech sentiment;speech sentiment analysis;learning speech sentiments;human annotation speech;transcripts based sentiment;annotation speech;speech sentiment approach;end speech sentiment;speech sentiment;data human sentiment;semi supervised training;annotation boost performance;annotation boost;speech dataset training;trained language model;semi supervised;automatic speech;employing automatic speech;based semi supervised;unlabeled speech dataset;large unlabeled speech;text speech;speech dataset investigate;sentiment analysis separately;pipeline e2e speech"}, "5dcbdb9bf80575953b5d21f378d8139f0a44168b": {"ta_keywords": "recognition emotion using;automatic recognition emotion;spoken dialogue recognition;dialogue recognition;dialogue recognition analysis;identifying user emotion;recognition emotion;human spoken dialogue;natural spoken dialogue;emotion triggered speakers;user emotion investigating;emotion using;analysis regarding emotion;user emotion;emotion triggers human;triggers human spoken;support vector machine;spoken dialogue;dialogue current situation;spoken dialogue essential;emotion investigating;svm present analysis;emotion occurred emotion;vector machine svm;emotion using support;situated dialogue current;investigating reason emotion;emotion investigating reason;dialogue;regarding emotion triggers", "pdf_keywords": ""}, "bc632f81dab322ac610a8d11463cc1bba6130eda": {"ta_keywords": "historical text normalization;text normalization datasets;multi task learning;text normalization relies;text normalization;normalization datasets languages;task learning architectures;languages training data;tasks historical text;task learning;improvements languages training;autoencoding grapheme phoneme;languages using autoencoding;task learning configurations;lemmatization auxiliary tasks;task learning lead;autoencoding grapheme;using autoencoding grapheme;normalization datasets;normalization relies;improvements training data;auxiliary tasks historical;significant improvements languages;languages training;63 multi task;training data abundant;multi task;normalization;autoencoding;using autoencoding", "pdf_keywords": "historical text normalization;text normalization relies;text normalization datasets;text normalization marcel;normalization datasets languages;text normalization;improvements languages training;zero shot learning;language zero shot;multi task learning;task learning;shot learning historical;learning historical text;languages training data;normalization relies;languages using autoencoding;normalization relies small;shot learning outperforms;normalization marcel;shot learning;improvements training data;lemmatization auxiliary tasks;normalization;improvements languages;normalization datasets;minimal improvements training;task learning con\ufb01gurations;autoencoding;autoencoding grapheme;using autoencoding grapheme"}, "13b674bb3078623608045a18570b47f6e49a8358": {"ta_keywords": "visual question answering;paintings text descriptions;paintings descriptions annotate;descriptions annotate paintings;descriptions simple embedding;identifying paintings descriptions;identifying paintings text;text image coreferences;image coreferences;image coreferences achieves;descriptions annotate;paintings text;annotate paintings;descriptions visual;paintings descriptions;understanding descriptions visual;matching question answering;art results task;layout identifying paintings;question answering;textual description;identifying paintings;annotate paintings contour;descriptions visual themes;identify artistic;text descriptions;ontology associate image;text descriptions simple;question answering systems;identify artistic work", "pdf_keywords": ""}, "4b9795493a937b9034be9c26afab23f6dc751f62": {"ta_keywords": "retrieval automaton;retrieval automaton approximates;retrieval based language;retomaton retrieval automaton;datastore search based;automaton approximates datastore;retrieval based;datastore search;approximates datastore search;automaton built datastore;costly datastore search;search based;language models;weighted finite automaton;retrieval;based language models;datastore search performed;automaton unsupervised retomaton;language models lm;search performed frequently;computationally costly datastore;automaton built;language model;finite automaton built;search;language model lm;automaton unsupervised;automaton;costly datastore;automaton approximates", "pdf_keywords": "cheaper traversal automaton;empirically traversing automaton;automaton datastore allows;\ufb01nite automaton datastore;automaton datastore;automaton built datastore;datastore allows approximating;traversing automaton inference;neighbors automaton states;similar neighbors automaton;automaton allows;searches cheaper traversal;automaton allows sporadic;traversal automaton;traversing automaton;neighbors automaton;weighted \ufb01nite automaton;states automaton allows;automaton states keeping;traversal automaton time;approximates knn search;empirically traversing;\ufb01nite automaton built;automaton states;datastore entries clustering;automaton built;automaton inference time;automaton wfa;automaton inference;states automaton"}, "a7b6802f20c399615dbac161678cd6a6d2df5a97": {"ta_keywords": "quality control crowdsourcing;crowdsourcing;spatial crowdsourcing;combined spatial crowdsourcing;crowdsourcing describing;spatial crowdsourcing enables;control crowdsourcing describing;control crowdsourcing;crowdsourcing enables keeping;crowdsourcing enables;crowdsourcing describing relevant;impactful human loop;crowd kit overall;crowd kit;human loop pipelines;human opinions gather;loop subjective human;aggregation quality estimation;ai human loop;demonstrate crowd kit;use human loop;crowd kit open;ai human;opinions gather training;datasets tools discuss;human loop subjective;aggregation quality;human opinions;rank models online;deploying human loop", "pdf_keywords": ""}, "7df6aa19f50c8ec5f12d58e0685ed5c6e9a08bb2": {"ta_keywords": "reviews inputs recommender;generating reviews model;generating reviews;generate plausible reviews;using reviews inputs;like product reviews;reviews inputs;rankings existing reviews;focus generating reviews;reviews model;reviews model output;reviews recommend existing;existing reviews recommend;reviews estimate nuanced;using reviews;efficiently model text;reviews provide valuable;product reviews;recommend existing products;recommender focus generating;inputs recommender;plausible reviews estimate;provide personalized rankings;reviews estimate;personalized rankings;personalized rankings existing;reviews recommend;existing reviews;natural language approaches;recommendation focus learning", "pdf_keywords": ""}, "a3e3a9d878999c7038c275e75f5cd8a232aa4999": {"ta_keywords": "benchmark semantic generative;semantic generative capabilities;models transfer learning;task diversity superb;speech processing universal;sg enhanced speech;pre trained models;evaluating semantic generative;transfer learning;limited task supervision;generative capabilities pre;semantic generative;benchmark semantic;generative capabilities;enhanced speech;transfer learning proven;representations learned;trained models;task diversity;robustness representations learned;representations learned pre;generative capabilities lack;performance benchmark semantic;capabilities pre trained;task diversity difficulty;task supervision;task supervision effective;models increasing task;increasing task diversity;learned pre trained", "pdf_keywords": "sg enhanced speech;speech model pre;model representation speech;speech processing universal;representation speech model;conversion speech separation;voice conversion speech;enhanced speech processing;speech translation domain;tasks speech translation;voice conversion;enhanced speech;speech model;asr voice conversion;speech processing;conversion speech;tasks include speech;domain asr voice;speech enhancement;include speech translation;speech enhancement evaluate;benchmark semantic generative;separation speech enhancement;speech enhancement emphasis;speech translation;tasks speech;speech separation;new tasks speech;semantic generative capabilities;asr voice"}, "b5002aa334f8d0c0e1a4dedad79580e10a928c30": {"ta_keywords": "extractors ssl models;sf ssl representations;ssl training;ssl training domain;ssl training set;ssl models automatic;ssl representations;quality ssl representations;ssl representations depends;sf extractors ssl;translation quality ssl;extractors ssl;relatedness ssl training;resource speech recognition;baseline ssl models;ssl models;ssl representations combining;ssl models effective;mismatch ssl training;models automatic speech;ssl models conventional;speech recognition translation;contribution ssl models;speech recognition asr;speech recognition;automatic speech recognition;resource speech tasks;quality ssl;recognition asr speech;learnable sf extractors", "pdf_keywords": "supervised learning ssl;learning ssl models;ssl training domain;resource speech recognition;ssl representations;ssl representations quality;ssl training set;ssl representations depends;representations quality ssl;ssl training;relatedness ssl training;sf ssl representations;learning ssl;models speech tasks;models automatic speech;quality ssl representations;speech recognition translation;ssl models automatic;baseline ssl models;ssl models;mismatch ssl training;speech recognition asr;speech recognition;low resource speech;models speech;recognition asr speech;automatic speech recognition;based models speech;contribution ssl models;ssl models conventional"}, "8809d0732f6147d4ad9218c8f9b20227c837a746": {"ta_keywords": "speech processing toolkit;applications automatic speech;automatic speech;end speech processing;developments espnet toolkit;text speech;speech processing;speech processing applications;speech translations;automatic speech recognition;speech recognition;espnet toolkit;espnet toolkit boosted;text speech tts;recent developments espnet;speech recognition asr;available corpora tasks;asr speech translations;speech separation ss;speech separation;recognition asr speech;developments espnet end;ss text speech;developments espnet;corpora tasks;st speech separation;speech tts;speech translations st;speech tts experiments;corpora tasks pre", "pdf_keywords": "speech recognition asr;end speech processing;speech separation ss;recognition asr speech;speech separation;speech processing;st speech separation;automatic speech;speech recognition;automatic speech recognition;including automatic speech;model various speech;various speech applications;text speech;speech applications large;speech processing applications;corpus tts corpora;corpus tts;speech tts;asr speech translation;text speech tts;speech applications;corpus ss corpus;speech translation st;ss corpus tts;corpus;ss text speech;asr speech;speech translation;asr corpora"}, "d8252e24b6036ca895800b547698ab44d09ae350": {"ta_keywords": "personal information management;management personal information;improve personal information;machine learning used;learning information management;ways machine learning;information management;information management promising;machine learning;information management tools;machine learning information;machine learning techniques;learning information;personal information email;personal information;learning techniques effectively;track task;evidence machine learning;information email messages;management personal;learning used;searches require awareness;management tools;learning used improve;directions machine learning;email messages;searches;losing track task;searches notably searches;information email", "pdf_keywords": ""}, "f91c24b0dc56a6b377e99e046d7540e5bb7aa46e": {"ta_keywords": "student information management;cs information management;information management based;information management module;student cs management;information management;students information security;informatization college student;information module accounting;information management ims;management based web;informatization college;information management lays;student information;cs management;reform informatization college;information structure framework;college student information;students information;information module;college management traditional;introduction information technology;vocational college management;information technology;information technology automation;student cs;student curriculum module;basic information module;college management;management module", "pdf_keywords": ""}, "a6a7724763d8adba466519489b0b9d209e7f2d15": {"ta_keywords": "text generation;evaluate generated texts;evaluation generated text;involve text generation;evaluating generated text;machine translation summarization;text text generation;generated texts;machine translation;summarization dialog involve;translation summarization dialog;translation summarization;generated text reference;generated texts actually;text generation major;summarization dialog;text generation code;evaluation text;sequence sequence models;generated text;text generation problem;trained sequence sequence;nlp applications machine;generated text text;applications machine translation;scores generated text;generated text better;conceptualize evaluation generated;nlp applications;pre trained sequence", "pdf_keywords": "generated generating textual;generating textual;text generation;evaluation generated text;generating textual inputs;text generation task;text text generation;generated text;generated text text;generated text better;text generation problem;generated text reference;convert generated text;generation task empirically;generation task;trained sequence sequence;textual inputs outputs;pre trained sequence;scores generated text;evaluation generated;trained convert generated;directly evaluating text;sequence sequence models;conceptualize evaluation generated;evaluating text;trained sequence;evaluation text;textual inputs;generating;generated generating"}, "e3f1a9c3d87e9828cdeb08ba90a260c69e974a75": {"ta_keywords": "dance generation audio;dnns generating dance;music challenging dnns;music dnns require;music input dnn;large dance sequences;dance sequences;beat music dnns;networks basic dance;dance sequence;large dance sequence;dance sequence uses;music dnns;generated motion beat;generates basic dance;dance piece music;sequences mapping music;generating dance;generating dance piece;deep recurrent neural;music motion beat;mapping music motion;dance sequences ii;basic dance generation;supervised deep recurrent;music motion;input dnn needs;deep recurrent method;dnns generating;timing music motion", "pdf_keywords": "dance generation audio;neural networks dance;large dance sequences;dance sequences;dance generation tasks;deep recurrent neural;dance sequence;dance sequence solo;generates basic dance;new dance sequence;deep recurrent model;networks dance generation;supervised deep recurrent;generating basic dance;sequence solo dancer;generate new dance;deep recurrent method;dance synchronized music;basic dance generation;basic dance synchronized;dance sequences ii;recurrent neural networks;deep recurrent;dancer generative model;recurrent neural;rhythm;generation basic dance;basic dance;baseline dancer generative;recurrent model generating"}, "78d57a1ecd724c5f8b1534372969d5b35daa6d4b": {"ta_keywords": "models constituency parsing;improving neural parsing;constituency parsing achieve;constituency parsing;neural parsing;neural parsing disentangling;neural models constituency;base parsers decoding;outputs base parsers;base parsers;parsers decoding straightforward;parsers decoding;parsers;parsing disentangling model;parsing;proposed generative neural;generative neural;generative neural models;parsing disentangling;direct search generative;parsing achieve state;generative models difficult;search generative;search generative models;proposed generative;generative;generative models;models constituency;parsing achieve;decoding straightforward", "pdf_keywords": "generative parsers parses;modeling generative parser;generative parsers;directly generative parser;generative neural parsing;generative parsers rg;grammar generative parser;generative parser;neural models parsers;generative parser lm;generative parser added;generative parser rg;neural parsing models;rd generative parsers;parser rd generative;rnng discriminative parser;effects generative parsers;parsing models recurrent;parser rnng discriminative;grammar generative;network grammar generative;models parsers investigate;models parsers;neural parsing;base parser rnng;parsing models;parser rnng;parsers parses;neural network grammar;language modeling generative"}, "b6c6e06b4bc68349845b30e01e01d7603f468547": {"ta_keywords": "optimal relay location;formulas optimal relay;optimal relay;optimal placement relay;relay location optimal;optimal capacity relay;allocation source relay;placement relay nodes;optimal power allocation;relay node placement;relay channel exponential;placement relays;relay channel;placement relay;relay nodes;multi relay channel;capacity relay node;location optimal power;placement relays line;power allocation;relay channel study;source relay channel;capacity relay;power allocation given;formulas multi relay;relays line exponentially;placement multi hop;node relay;multi relay;relay location", "pdf_keywords": "optimal relay locations;optimal relay location;optimal relay;optimal capacity relay;formulas optimal relay;relay location optimal;allocation source relay;relay channel exponential;provides optimal relay;relays derive optimization;optimal power allocation;relay channel;constraint source relays;source relay channel;relay node placement;placement relay nodes;source relays derive;relay nodes;relays line exponentially;capacity relay;capacity relay node;power allocation;channel1 multiple relay;relays derive;allocation transmission powers;location optimal power;power allocation given;relay channel1 multiple;placement relays;multiple relay"}, "7729fbebff327bebb9292dc1c51c51dd55390954": {"ta_keywords": "tensors verbs compositional;rank tensors verbs;verb order tensor;tensors sentence similarity;tensors verbs;tensors transitive verb;verbs compositional distributional;compositional distributional semantics;distributional semantics;sentence similarity verb;rank tensors sentence;verbs compositional;sentence similarity;low rank tensors;tensors sentence;rank tensors transitive;tensors transitive;tensors low rank;transitive verb construction;tensors require orders;verb order;rank tensors require;rank tensors;tensors require;unconstrained rank tensors;construction verb order;tensors;similarity verb disam;verb construction;tensors low", "pdf_keywords": ""}, "f3271e61dc0507183ee399393129d7888c2f82b9": {"ta_keywords": "automatically predicted quality;reference based evaluation;automatic quality estimation;quality estimate evaluation;supervised quality estimation;predict overall quality;quality output language;predicted quality scores;quality estimation approaches;fully automatic quality;evaluating quality output;machine translation speech;quality estimation;overall quality estimate;evaluating quality;quality scores;automatic quality;supervised quality;annotation effort;test corpus;annotated scores;manually annotated scores;estimates quality reliably;keeping annotation effort;domains quality standards;annotation effort low;machine translation;lightly supervised quality;predicted quality;estimates quality", "pdf_keywords": ""}, "68aa7c7b65365c3303d5024b1273408fb435d178": {"ta_keywords": "entrainment affects dialogue;dialogue acts entrainment;entrainment changes dialogue;entrainment dialogue;effect entrainment dialogue;entrainment dialogue acts;dialogue acts lexical;dialogue systems;affects dialogue;dialogue analyzing effect;entrainment lexical level;affects dialogue abstract;dialogue systems like;dialogue abstract structural;entrainment lexical;dialogue analyzing;dialogue;changes dialogue analyzing;dialogue abstract;dialogue acts;changes dialogue;guidelines dialogue systems;acts entrainment lexical;choice given dialogue;given dialogue acts;given dialogue;entrain users;entrainment changes;entrain users similar;entrainment", "pdf_keywords": ""}, "b0ddd849c5ae0004678fa483908c06d87894f3ab": {"ta_keywords": "model speech recognition;constructing ss hmms;hidden markov model;ss hmm based;speech recognition;criterion ss hmm;hmm based variational;hmms practical bayesian;model ss hmm;isolated word recognition;hidden markov;acoustic model speech;ss hmm structure;model speech;hmm structure practical;speech recognition paper;state hidden markov;ss hmms practical;ss hmm widely;markov model ss;word recognition;hmm based;hmm structure;bayesian model selection;based variational bayesian;markov model;markov model structure;variational bayesian approach;bayesian criterion;word recognition demonstrate", "pdf_keywords": ""}, "b0d644277933988c00b22d8ae012512fe498ad62": {"ta_keywords": "word sense disambiguation;sense disambiguation;trained word embedding;sense disambiguation wsd;word sense inventory;linguistic knowledge representations;disambiguation context inherent;disambiguation context;word embeddings;word embedding;fasttext word embeddings;word embedding model;disambiguation word;disambiguation word senses;fledged word sense;approaches word sense;disambiguation wsd disambiguation;wsd disambiguation word;using word embeddings;used disambiguation context;linguistic knowledge;disambiguation;trained fasttext word;sense disambiguation 158;used disambiguation;quality linguistic knowledge;word sense;wsd disambiguation;unsupervised knowledge free;disambiguation wsd", "pdf_keywords": "sense disambiguation design;word sense disambiguation;sense disambiguation;word sense induction;word sense inventory;trained word embedding;unsupervised word sense;trained word vectors;fasttext word embeddings;word embeddings;word embedding model;word embedding;disambiguation context use;disambiguation context;disambiguation design;sense word context;trained fasttext word;used disambiguation context;\ufb02edged word sense;disambiguation;pre trained fasttext;word context create;word vectors;sense particular context;trained fasttext;used disambiguation;create massively multilingual;sense induction;word sense;single sense word"}, "ebc64974e9e0021984a0158b3c04b60327730a88": {"ta_keywords": "neural semantic parsing;semantic parsing framework;base question answering;semantic parsing;retrack neural semantic;question answering;question answering kbqa;knowledge base;neural semantic;parsing framework;parsing framework large;parsing;scale knowledge base;knowledge base question;webquestionssp benchmark;typical webquestionssp benchmark;webquestionssp benchmark interact;checker retrack neural;semantic;transducer generate logical;answering kbqa retrack;large scale knowledge;checker improve transduction;retriever transducer checker;retrieve relevant;answering kbqa;generate logical form;generate logical;typical webquestionssp;performance typical webquestionssp", "pdf_keywords": ""}, "f0cd4de3cdf547dcdcc6995dca9ab3f65955b324": {"ta_keywords": "recurrent highway networks;lstm recurrent highway;layer recurrent models;multi layer recurrent;recurrent models limited;lstm recurrent;recurrent models;grid lstm recurrent;recurrent units lru;layer recurrent;lattice recurrent units;recurrent unit improving;lattice recurrent unit;lattice recurrent;state lattice recurrent;recurrent units;lstm;recurrent unit;learning deep;recurrent highway;grid lstm;deep multi;called lattice recurrent;learning deep multi;accurate language models;language models;deep multi layer;recurrent;sequence modeling;new lru models", "pdf_keywords": "layer recurrent models;recurrent models limited;lattice recurrent units;multi layer recurrent;recurrent models;recurrent neural networks;lattice recurrent unit;recurrent neural;lattice recurrent;recurrent units lru;layer recurrent;abstract recurrent neural;depth abstract recurrent;recurrent unit improving;recurrent units;called lattice recurrent;recurrent unit;state lru models;sequence modeling;abstract recurrent;modeling sequences;new lru models;lru models;modeling sequences introduce;lru models computational;models called lattice;learning deep;recurrent;lru model;lru models achieve"}, "485b3f77b9913e151e7ca897d99497e70e7f30d1": {"ta_keywords": "neural machine translation;machine translation;granularity subword units;subword units hyperparameter;machine translation nmt;translate using subword;smaller general subwords;machine translation paper;segmentation granularity neural;subword units;subword units allow;using subword units;pass granularity subword;granularity neural machine;optimizing segmentation granularity;training optimizing segmentation;subwords adding larger;tuned language task;granularity subword;improvements rare words;general subwords;general subwords adding;translation nmt;subwords;translation nmt standard;words incrementally;subwords adding;standard translate using;translation paper propose;standard translate", "pdf_keywords": "tune subword granularity;subword granularity;granularity automatically training;subword granularity single;smaller frequent subwords;segmentation granularity automatically;subwords online training;frequent subwords;frequent subwords online;tune segmentation granularity;search segmentation granularities;tunes subword granularity;subwords;embeddings component subwords;subwords shown pytorch;segmentation granularities;segmentation granularity;subwords size;granularity single training;subwords online;tune subword;embeddings new vocabulary;subwords size single;segmentation granularities essentially;subword;component subwords;granularities essentially pretraining;improves translation accuracy;component subwords size;embeddings compressing trained"}, "1e4e2aceed87febcc643f1473507c9535ba5c19a": {"ta_keywords": "speech translation st;slu speech translation;speech recognition asr;target language translations;regularization streaming slu;attention simultaneous translation;speech translation;speech processing tasks;optimized target language;cross lingual encoding;ctc translation output;speech recognition;speech processing;streaming slu task;st task streaming;ctc translation;language understanding slu;lingual encoding;translation st achieving;automatic speech;loss regularization streaming;language translations;addition ctc translation;success speech processing;regularization streaming;automatic speech recognition;language translations experimental;simultaneous translation;simultaneous translation time;task streaming baselines", "pdf_keywords": "regularization streaming slu;speech recognition asr;streaming slu task;st task streaming;loss regularization streaming;step streaming slu;task streaming baselines;attention simultaneous translation;regularization streaming;streaming slu;streaming slu simultaneous;speech recognition;streaming baselines proposed;gain streaming slu;accuracy gain streaming;task streaming;blockwise streaming transformer;recognition asr;streaming baselines;ctc translation output;recognition asr based;optimized target language;using blockwise streaming;ctc attention simultaneous;cross lingual encoding;asr based intermediate;automatic speech recognition;translations addition ctc;streaming transformer;streaming transformer achieves"}, "6e24bcfcdb31afbb313a13c1c84cb779ceb17500": {"ta_keywords": "decision learning prospect;decision learning;sequential decision;learning prospect theory;sequential decision maker;decision problem reference;point sequential decision;learning prospect;prospect theory value;influences decision learning;prospect theory;person given decision;decision maker;given decision problem;decision problem repeatedly;expectation maximization algorithm;chooses action maximize;loss influences decision;expectation maximization;hidden markov model;person chooses action;action maximize value;decision problem;using expectation maximization;markov model;person determines outcomes;hidden markov;time given decision;constructing hidden markov;repeatedly person chooses", "pdf_keywords": ""}, "0d20360c5d533760d97d7ce19b78d4791a5173cb": {"ta_keywords": "predicting terrorist targets;terrorist target prediction;algorithm predicting terrorist;predicting terrorist;networks terrorist target;terrorist meta network;dynamics terrorist attacks;complex networks terrorist;networks terrorist;terrorist organizations dynamics;analyzing terrorist organizations;terrorist targets complex;terrorist attacks creation;terrorist attacks;analyzing terrorist;methods analyzing terrorist;terrorist meta;network terror;targets complex networks;heterogeneous dynamics terrorist;dynamics terrorist;terrorist targets;network terror period;network based inference;use terrorist meta;meta network terror;terrorist target;terrorist organizations;inference algorithm predicting;complex networks", "pdf_keywords": ""}, "5d9fe38b750f59b4ef0a2b58fde0f60d4317c7ad": {"ta_keywords": "attributes multi domain;multi domain learning;metadata attributes multi;learn metadata attributes;multiple metadata attributes;datasets multiple metadata;single metadata attribute;domain learning techniques;domains attribute;best domains attribute;attributes multi;domain learning;metadata attributes;techniques multi attribute;multi attribute;simultaneously learn metadata;metadata attributes divide;domains attribute impact;metadata attribute;multi attribute setting;multiple metadata;single metadata;learning techniques multi;learn metadata;domain learning assumes;attribute impact classification;single attribute;data domains;multi domain;attributes", "pdf_keywords": ""}, "242c35b91fe1d7aedab9d1da7652aad2219d4784": {"ta_keywords": "speech translation task;model deeper speech;how2 speech translation;speech translation e2e;espnet how2 speech;deeper speech encoder;speech translation;outperform rnn models;speech translation iwslt;end speech translation;speech encoder;rnn models tasks;outperform rnn;rnn models;translation task iwslt2019;speech encoder confirm;translation task;deeper speech;consistently outperform rnn;rnn based models;how2 speech;submissions how2 speech;nmt model deeper;knowledge distillation nmt;translation iwslt 2019;rnn based;training knowledge distillation;rnn;tasks corpora espnet;corpora espnet how2", "pdf_keywords": ""}, "924e43c4de98743d2e7c14c241b03b2109325b90": {"ta_keywords": "collapsed gibbs sampling;gibbs sampling;gibbs sampling multiple;speech tagging;metropolishastings rejection sampling;parallel sampling methods;parallel sampling;speech tagging word;parallel run metropolishastings;previous parallel sampling;tagging word segmentation;distributing collapsed gibbs;sampling multiple processors;metropolishastings;results speech tagging;distributing sampling;distributing sampling block;word segmentation;run metropolishastings;word segmentation tasks;blocks distributing sampling;sampling;proposed blocked sampling;sampling dividing training;sampling block;metropolishastings rejection;blocked sampling;uses blocked sampling;sampling methods;sampling block multiple", "pdf_keywords": ""}, "030fade3049e0847702393dde3100ecc41a5e86a": {"ta_keywords": "hill climbing search;climbing search;learning tasks;probability hill climbing;intractable practical learning;nding global optimum;high probability hill;local optimum arbitrarily;optimal;global optimum;optimum arbitrarily high;nd locally optimal;optimum arbitrarily;locally optimal;learning systems;local optimum;global optimum intractable;learning tasks involve;hill climbing;probability hill;climbing search settings;hill climbing complicated;practical learning;learning;close local optimum;practical learning systems;optimum;hill climbing nd;forms hill climbing;performance elements seeking", "pdf_keywords": ""}, "bea54062d105b9fe3250ce3569cf817e54772894": {"ta_keywords": "historical treebanks training;treebanks training data;treebanks training;modern historical treebanks;historical treebanks;neural sequence labeling;treebanks;outperforms sequence labeling;unlexicalized parser outperforms;sequence labeling tool;sequence labeling;historical syntax based;sequence labeling approach;parser outperforms sequence;probabilistic parser created;probabilistic parser;parser outperforms;tool probabilistic parser;historical syntax;unlexicalized parser;historical corpora;parser;labeling tool probabilistic;different historical corpora;theories historical syntax;data neural sequence;corpora;parser created;annotated data;syntax based", "pdf_keywords": "historical treebanks training;phrase recognition historical;historical corpora using;historical treebanks;corpus modern;treebanks training data;present paper corpus;modern historical treebanks;treebanks training;paper corpus modern;historical corpora;corpus;paper corpus;department linguistics fakult\u00e4t;annotated phrases present;annotated phrases;phrase recognition;automatic phrase recognition;corpus modern non;historical syntax based;different historical corpora;linguistics fakult\u00e4t;based sequence labeling;neural sequence labeling;treebanks;german text archive;sets annotated phrases;berkeley parser outperforms;ortmann department linguistics;parser outperforms neural"}, "38705aa9e8ce6412d89c5b2beb9379b1013b33c2": {"ta_keywords": "deep nets nonparametric;semiparametric inference demonstrate;semiparametric inference;semiparametric inference results;estimation rates semiparametric;rates semiparametric inference;deep learning monte;effectiveness deep learning;neural nets rates;valid semiparametric inference;convergence deep feedforward;deep neural;rates convergence deep;use semiparametric inference;networks use semiparametric;deep feedforward;nets nonparametric regression;deep nets;deep neural networks;deep learning;deep feedforward neural;bounds deep nets;neural networks estimation;size deep neural;convergence deep;neural nets;study deep neural;feedforward neural nets;causal effects semiparametric;nets nonparametric", "pdf_keywords": "convergence deep feedforward;deep feedforward;neural networks nonparametric;bounds deep neural;deep learning monte;e\ufb00ectiveness deep learning;deep feedforward neural;deep neural;neural nets estimation;deep neural networks;deep nets general;rates convergence deep;deep nets;bounds deep nets;convergence deep;deep learning;novel bounds deep;feedforward neural nets;neural nets;nets estimation rates;deep relu networks;nonasymptotic bounds deep;feedforward neural networks;neural networks speaking;networks nonparametric;semiparametric inference results;estimation rates semiparametric;semiparametric inference;feedforward neural;relu networks"}, "4dd85ae17a5fd0bce09ffef0455b6e827d7e1e2b": {"ta_keywords": "attack distributed cyber;linear attack learnt;attack distributed;parameters linear attack;linear attacks;linear attack;nodes kalman consensus;injection attack distributed;distributed cyber physical;kalman consensus filtering;attack detection;constraint attack detection;attack detection probability;kalman consensus;distributed cyber;data injection attack;nodes kalman;constraint attack;attack learnt;class linear attacks;respecting constraint attack;attacker steer estimates;online stochastic gradient;neighbouring nodes kalman;attack learnt line;observations agent nodes;sensor observations agent;consensus filtering;attacks;estimates agent nodes", "pdf_keywords": "attack networked vehicular;optimal deception attack;linear attack distributed;networked vehicular cyber;deception attack networked;optimal linear attack;proposed attack scheme;deception attack attacker;attack distributed cyber;attack scheme;attack distributed;vehicular cyber physical;attack scheme conclusions;injection attack distributed;vehicular cyber;linear attack;attacks integrity deception;integrity deception attack;deception attack;fdi attacks;attack networked;attacks integrity;fdi attacks integrity;data injection attack;ef\ufb01cacy proposed attack;attack numerical results;injection fdi attacks;attack numerical;attack scheme learnt;denial service attack"}, "5143ebd23322fe805bed2667fcfb70920c105f7f": {"ta_keywords": "demonstration cognitive modeling;better demonstration cognitive;cognitive tutor computerized;demonstration cognitive;cognitive modeling demonstration;model cognitive tutor;cognitive tutor;tutor computerized;computerized tutor;computerized tutor teaches;tutor teaches human;tutor computerized tutor;demonstrations involve implicit;characteristics effective demonstrations;cognitive skills observing;studies expressive demonstrations;cognitive skills;students cognitive skills;learned cognitive;tutor teaches;cognitive model simulated;characterizes better demonstration;simulated student;modeling demonstration;expressive demonstrations;human students cognitive;learning agent learns;learning agent;agent learns;learned cognitive skills", "pdf_keywords": ""}, "cfbe9183f2fe2847f7a3c811f6309a2cab3f85cf": {"ta_keywords": "pretraining extractive summarization;scientific summarization dataset;summarization scientific documents;extractive summarization scientific;summarization datasets report;summarization scientific;existing summarization datasets;summarization dataset scitldr;released scientific summarization;summarization datasets;pretraining bert based;summarization dataset;scientific summarization;pretraining bert;influence pretraining bert;bert based extractive;pretraining corpus;pretraining corpus changing;domain pretraining corpus;leverages existing summarization;extractive summarization;based extractive summarization;existing summarization;pretraining extractive;effect pretraining extractive;summarization;bert based;scitldr effect pretraining;analyze intermediate pretraining;corpus changing", "pdf_keywords": ""}, "20086d6a9fab6081f300e08d3f952cb9b16e6de8": {"ta_keywords": "approximate search algorithms;approximate search;analysis approximate search;shortest retrieval;shortest retrieval times;provide shortest retrieval;require huge indices;indexing deletion neighborhoods;linear indices great;algorithms involve indexing;residual dictionary strings;search algorithms;indexing;involve indexing deletion;indexing deletion;search algorithms involve;huge indices;stores residual strings;indexed using perfect;huge indices sizes;involve indexing;data indexed;directly data indexed;indices great;fastest benchmarks present;fastest benchmarks;retrieval;indexed using;known benchmarks;indexed", "pdf_keywords": ""}, "693f5d55e0561099944f5e00e301bf26db0b972d": {"ta_keywords": "stochastic semantic analysis;utterance transform semantic;statistical semantic analysis;statistical semantic;spoken language understanding;stochastic semantic;developing stochastic semantic;task spoken language;human computer dialogue;input acoustic utterance;methods statistical semantic;computer dialogue systems;acoustic utterance;dialogue systems task;semantic representation;semantic representation thesis;semantic analysis;dialogue systems;utterance transform;acoustic utterance transform;spoken language;semantic analysis represent;systems task spoken;language understanding;utterance;semantic;computer dialogue;transform semantic representation;semantic analysis annotation;semantic analysis comparison", "pdf_keywords": ""}, "939a149f156425b83e48ea72e9e09a55ea33b8d7": {"ta_keywords": "signal reconstruction;complete signal reconstruction;algorithm signal reconstruction;signal reconstruction problem;signal reconstruction formulated;domain signal reconstruction;signal reconstruction variants;feature signal reconstruction;involved signal reconstruction;signal reconstruction cope;signal reconstruction large;representation wavelet transform;wavelet transform;crossing representation wavelet;wavelet transform domain;representation wavelet;far signal reconstruction;fractional sampling;wavelet;reconstruction large computational;fractional sampling interval;reconstruction variants representation;case fractional sampling;reconstruction problem reduces;reconstruction problem;iterative algorithm signal;reconstruction variants;complete signal;zero crossing representation;transform domain signal", "pdf_keywords": ""}, "0fbb90b8fe1d02a4f0f616df9a09ec42eace53bd": {"ta_keywords": "voice activity detection;unsupervised method voice;method voice activity;voice activity;online unsupervised classification;activity detection vad;bayes framework voice;classifier online;framework voice activity;online unsupervised method;unsupervised classification model;unsupervised classification;activity detection new;activity detection;detection new online;method voice;based variational bayes;recording condition online;variational bayes framework;reliability classifier online;vad algorithms especially;new online unsupervised;online expectation maximization;classifier;online unsupervised;variational bayes vb;vad algorithms;decision level statistical;classification model comparison;especially remote recording", "pdf_keywords": ""}, "e21633b0b5e55dce56bc07e919c6d12ecf8cef0c": {"ta_keywords": "clauses pac learnable;clauses constant locality;locality pac learnable;inductive logic programming;constant locality pac;clauses constant depth;learnable obvious syntactic;locality strictly expressive;learnable language nonrecursive;determinate clauses pac;generalizations language pac;pac learnable obvious;nonrecursive clauses constant;clauses pac;practical inductive logic;locality present formal;learn determinate clauses;pac learnable language;logic programming;constant locality strictly;logic programs;logic programming systems;constant locality;language pac learnable;depth determinate clauses;efficiently learn determinate;nonrecursive determinate clauses;language nonrecursive clauses;clauses constant;restriction logic programs", "pdf_keywords": ""}, "c305e3314c0853b14911f704c68b04cfc9ea7aa1": {"ta_keywords": "hindi dependency treebank;hindi treebank conforming;hindi treebank;crosslingual dependency parsing;converted hindi treebank;universal dependencies hindi;dependencies hindi;dependency treebank universal;hindi dependency;dependencies hindi dependency;dependency treebank;dependency parsing;treebank universal dependencies;dependency parsing paper;treebank conforming international;techniques crosslingual dependency;treebank universal;automatically converted hindi;treebank conforming;present parsing experiments;treebank;crosslingual dependency;converted hindi;annotation schemes present;multilingual language technology;hindi;evaluation cross lingual;hindi produce automatically;differences annotation schemes;parsing", "pdf_keywords": "hindi dependency treebank;hindi treebank conforming;hindi treebank;converted hindi treebank;hindi universal dependency;hindi dependency;10th linguistic annotation;linguistic annotation;dependencies ud hindi;ud hindi dependency;linguistic annotation workshop;dependency treebank;pos tags hindi;treebank conforming international;tags hindi;dependency treebank hdtb;hindi universal;indian languages conversion;indian languages;treebank conforming;automatically converted hindi;annotation schemes present;tags hindi map;followed hindi universal;hindi extend;treebank;annotation schemes;differences annotation schemes;hindi map universal;hindi extend ud"}, "652579315d767331d8e05ea46489e6bd081ef48a": {"ta_keywords": "peer evaluation moocs;ordinal peer evaluation;peer evaluation;evaluation moocs argue;peer evaluation answers;evaluation moocs;moocs argue ordinal;students evaluate;ordinal peer;evaluation students;evaluation feedback peer;feedback peer evaluation;feedback evaluation students;set students evaluate;evaluation students severe;case ordinal peer;ordinal approach significantly;argue ordinal approach;argue ordinal;ordinal;ordinal approach;students evaluate case;feedback peer;moocs highly;students;experts available moocs;evaluation answers;classrooms critical;peer;moocs highly successful", "pdf_keywords": ""}, "b293e4659e20815bcf0b6d31ce46b8bd9437c1fa": {"ta_keywords": "privacy machine learning;machine learning private;learning aided privacy;learning based privacy;private machine learning;learning private machine;protection machine learning;privacy protection machine;privacy preservation;privacy machine;privacy preservation problems;state art privacy;preservation privacy machine;problem privacy preservation;data privacy;data privacy protection;privacy protection;study privacy preservation;aided privacy protection;problem privacy;privacy;privacy problems machine;privacy attack;aided privacy;preservation privacy;analysis area privacy;privacy attack corresponding;privacy preservation context;learning private;privacy protection iii", "pdf_keywords": "learning meets privacy;privacy preservation;problem privacy preservation;data privacy;data privacy protection;privacy protection machine;traditional data privacy;privacy preservation context;protection machine learning;privacy protection;privacy;meets privacy survey;privacy survey;problem privacy;meets privacy;privacy survey outlook;emerged machine learning;machine learning meets;machine learning act;machine learning quite;note problem privacy;2020 machine learning;machine learning;protection machine;preservation context;preservation context machine;context machine learning;preservation;traditional data;protection"}, "8d17543c20f23b6a40bec9334d50e9c15a08c1c4": {"ta_keywords": "domain question answering;question answering;question answering qa;question answering using;end deep neural;large text corpus;corpus open domain;deep neural;questions training data;text corpus open;entity linked text;knowledge bases text;fusion knowledge bases;answering qa evolving;answering qa;end end deep;answering using early;knowledge bases;answering;questions training;corpus open;text corpus;answering using;end deep;linked text;neural;deep;deep neural networks;suite benchmark tasks;completeness graft net", "pdf_keywords": "facts text networks;text networks;graph representation learning;net graphs relations;natural language graft;subgraph containing text;net extracting answers;graph constructed text;language graft net;graphs relations facts;novel graph convolution;graphs relations;text networks speci\ufb01cally;graph convolution based;graft net graphs;subgraph containing;net classifying nodes;advances graph representation;relations construct suite;relations facts text;graphs kb facts;net classifying;subgraph consisting kb;net graphs;graph convolution;natural language;graph representation;classifying nodes subgraph;nodes subgraph;kb entities relations"}, "1808b64aec21863489f0fe66f250890a3ac2b843": {"ta_keywords": "privacy preserving statistical;random noise secure;privacy preserving;databases privacy;implementation privacy preserving;noise secure malicious;distributed implementation privacy;databases privacy obtained;verifiable secret sharing;noise secure;shares random noise;privacy obtained perturbing;distributed random noise;implementation privacy;secret sharing;generating shares random;efficient distributed;provide efficient distributed;random noise purpose;secret sharing needed;shares random;shares unbiased coins;privacy obtained;secure malicious participants;privacy;protocols generating shares;verifiable secret;generation gaussian noise;unbiased coins;distributing shares unbiased", "pdf_keywords": "privacy preserving statistical;distributed implementation privacy;privacy preserving;secure evaluation sums;poisson noise shares;implementation privacy preserving;developed distributed algorithms;distributed algorithms;noise shares;power noisy sums;noisy sums primitive;noisy sums;secret sharings;binomial poisson noise;number secret sharings;secret sharings needed;implementation privacy;sharings needed generating;coins purpose noise;distributed;distributed implementation;simplicity secure evaluation;distributed algorithms generation;noise shares inspired;noise generation;simplicity secure;algorithms generation binomial;noise generation create;secure evaluation;create distributed"}, "c3a662b864673d8cc7469051419ab8819926d4b0": {"ta_keywords": "essential bert multilinguality;bert multilingual based;multilingual bert;bert multilinguality;necessary bert multilingual;bert multilingual;shown multilingual bert;multilingual bert mbert;bert models trained;languages necessary bert;small bert models;bert linguistic properties;bert models;bert linguistic;properties bert linguistic;multilingual representations enables;quality multilingual representations;multilingual representations;architectural properties bert;essential bert;elements essential bert;multilingual pretraining;necessary bert;experiment multilingual pretraining;setup small bert;high quality multilingual;multilingual pretraining setup;unsupervised embedding alignment;bert;properties bert", "pdf_keywords": "bert multilingual overall;necessary bert multilingual;bert multilingual;bert linguistic properties;languages necessary bert;bert linguistic;properties bert linguistic;small bert models;bert models;bert models trained;multilinguality experiments;multilinguality experiments xnli;architectural properties bert;setup small bert;multilingual pretraining;unsupervised embedding alignment;experiment multilingual pretraining;elements multilinguality experiments;multilingual overall;necessary bert;multilingual overall identify;multilinguality;embeddings shared;elements in\ufb02uence multilinguality;multilinguality allow fast;multilingual pretraining setup;multilingual;vecmap unsupervised embedding;properties bert;in\ufb02uence multilinguality"}, "b1d24e8e08435b7c52335485a0d635abf9bc604c": {"ta_keywords": "verification textual sources;fact extraction verification;verification textual;dataset verification textual;fact extraction;textual sources fever;verified knowledge sentence;oracles claims classified;oracles claims;refuted notenoughinfo annotators;sentences extracted wikipedia;wikipedia subsequently verified;annotators recorded sentence;designed oracles claims;textual sources;altering sentences extracted;fever fact extraction;labeling claim accompanied;labeling claim;notenoughinfo annotators achieving;sources fever fact;textual;achieve labeling claim;sentences extracted;notenoughinfo annotators;annotators;annotators achieving;extraction verification consists;extraction verification;generated altering sentences", "pdf_keywords": "fact extraction veri\ufb01cation;fact extraction;dataset fact extraction;evidence retrieval;fever fact extraction;recall evidence retrieval;dataset veri\ufb01cation textual;evidence retrieval order;textual sources fever;retrieval textual entailment;veri\ufb01cation textual sources;information retrieval textual;textual entailment components;annotation consistency developed;textual sources;annotation consistency;retrieval textual;sources fever fact;textual entailment;dataset fact;veri\ufb01cation textual;sentences forming evidence;dataset claim veri\ufb01cation;information retrieval;textual;evidence documents \ufb01nally;evidence documents;ensure annotation consistency;comprises information retrieval;annotation"}, "4a4bc9f6c5ec76b0d501b641d3092aceb2e083bd": {"ta_keywords": "capture preference knowledge;preference knowledge;restaurant customizable diner;preference knowledge expert;customizable diner;food preferences involving;preference handling;preference handling software;customizable diner profiles;restaurant customizable;food preferences;diner profiles;preference handling community;integration restaurant customizable;diner profiles composed;preferences examples;preferences examples quite;meal serve canonical;capture preference;preferences involving;packages available restauranteurs;composed preferences examples;preferences involving overall;preference;fivestars food preferences;examples preference handling;regular customer order;available restauranteurs;field preference handling;meal serve", "pdf_keywords": ""}, "d5eeaac5c5e524ad05d9b1f3f3f41aece082955a": {"ta_keywords": "speech recognition variational;posteriors sparse training;clustering speech recognition;bayesian posteriors sparse;estimation clustering speech;bayesian framework speech;clustering speech;recognition variational bayesian;sparse training;posteriors sparse;speech recognition vb;speech recognition vbec;sparse training data;data speech recognition;speech recognition;bayesian predictive classification;training data speech;variational bayesian estimation;bpc speech recognition;data speech;classification using variational;framework speech recognition;data sparseness;bayesian estimation clustering;problem speech recognition;variational bayesian posteriors;distribution bayesian predictive;speech recognition effects;sparse data;variational bayesian", "pdf_keywords": ""}, "04d96a75b4383240cb15fb729b29f5775219d724": {"ta_keywords": "speech recognition asr;deep learning library;neural automatic speech;recognition asr toolkit;neural machine translation;machine translation toolkit;fast parallelized decoder;faster decoding;speech recognition;parallelized decoder implemented;toolkit based deep;asr toolkit based;parallelized decoder;11x faster decoding;deep learning;faster decoding similar;recognition asr;asr toolkit;automatic speech recognition;translation toolkit;language model fusion;automatic speech;end neural automatic;machine translation;decoder implemented espresso;learning library pytorch;state art asr;asr performance wsj;popular neural machine;asr performance", "pdf_keywords": "machine translation toolkit;neural machine translation;end asr toolkit;asr toolkit;asr toolkit built;benchmark asr;training decoding quick;fairseq extensible neural;translation toolkit;machine translation;language model fusion;learning framework pytorch;training decoding;parallelized decoder;fully parallelized decoder;benchmark asr data;translation toolkit software;parallelized decoder end;distributed training decoding;decoding step pretrained;deep learning;deep learning framework;extensible neural machine;end end asr;end asr;variety benchmark asr;speech nlp;language model provides;parallelization distributed training;end asr espresso"}, "312b12dd6aa558b92df3ddd9b1057aa80a0ad718": {"ta_keywords": "relation classification systems;relation classification;shot relation classification;relation classification exploiting;task relation classification;relation classification pose;exploiting relation descriptions;textual entailment datasets;relation descriptions;textual entailment models;relation descriptions ii;zero shot relation;textual entailment;entailment datasets;entailment datasets enhance;performance relation classification;classification exploiting relation;zero shot classification;existing textual entailment;available textual entailment;task textual entailment;textual entailment experiments;shot relation;entailment models;relation;entailment models iii;shot classification;textual;entailment;shot classification performance", "pdf_keywords": ""}, "5885625fac055f4f8f47b0d6b5c026c8806896f0": {"ta_keywords": "theory analyze chunking;analyze chunking;distribution free learning;chunking;free learning theory;free learning;learning theory;using distribution free;learning theory analyze;distribution free;learning;distribution;using distribution;theory;analyze;theory analyze;free;using", "pdf_keywords": ""}, "098076a2c90e42c81b843bf339446427c2ff02ed": {"ta_keywords": "influence functions deep;deeper networks estimates;influence functions neural;influence functions approximate;accurate shallow networks;models influence functions;shallow networks;improved influence estimation;networks estimates erroneous;influence estimates fairly;influence estimates;networks estimates;influence estimates vary;shallow networks deeper;influence estimation methods;general influence functions;particular influence estimates;accuracy influence estimates;influence estimation;influence functions;networks deeper;influence functions defined;network models trained;networks deeper networks;deeper networks;influence estimates iii;approximate effect training;functions deep learning;influence function implemented;influence function", "pdf_keywords": "deep learning extensive;deep learning fragile;estimates complex deep;complex deep architectures;deep architectures experiments;deep architectures;deep learning;functions deep learning;complex deep;learning extensive;functions deep;in\ufb02uence functions deep;approximate effect training;learning interpretability uncertainty;imagenet;deep;influence functions deep;learning interpretability;architecture stochastic approximation;learning extensive experimental;machine learning interpretability;complex models datasets;interpretability uncertainty estimation;predictions wide;architectures experiments datasets;imagenet architectures;accuracy in\ufb02uence estimates;learning fragile general;growing complex models;100 imagenet architectures"}, "446f1eaec22a90574670491073cd5b03bfa1e273": {"ta_keywords": "learn supervised models;supervised models;annotating data;supervised baseline;text claims population;statistical properties countries;supervised baseline approach;claims statistical properties;distantly supervised baseline;annotating data property;statistical claims;supervised models develop;simple statistical claims;numerical information text;statistical claims properties;instead annotating data;extracting numerical information;supervised;properties countries freebase;knowledge base raw;annotating;knowledge base;simple claims statistical;countries freebase approach;claims statistical;using knowledge base;claims population inflation;countries freebase;information text;text claims", "pdf_keywords": ""}, "76468db928e18f97dadbd25c04c80ebd491fec9b": {"ta_keywords": "7f1 manifold pyrochlore;metal cations eu3;cations eu3 asymmetry;manifold pyrochlore zircon;pyrochlore zircon compounds;transition metal cations;nd0 transition metal;cations eu3;7f1 crystal field;7f1 crystal;eu3 asymmetry ratio;pyrochlore zircon;eu3 asymmetry;manifold pyrochlore;crystal field splitting;5d0 7f1 crystal;zircon compounds;transition metal;splitting 7f1 manifold;7f1 manifold;metal cations;splitting 7f1;influence nd0 transition;eu3;crystal field;nd0 transition;ratio 5d0 7f2;7f2 5d0 7f1;field splitting 7f1;7f1", "pdf_keywords": ""}, "aaf7e94e1a2f8891e5c5f4d11d4f135a1687bb0a": {"ta_keywords": "displacement locking arms;spring locking arms;locking position assembled;axial displacement locking;spring aperture locking;spring locking;displacement locking;locking arms cooperate;aperture arms;arms cooperate panel;aperture locking position;pair spring locking;locking positions;position assembled;locking arms;interfere locking arm;inwardly center assembly;locking arm;spring aperture;locking position;center assembly;ends spring aperture;depressed locking positions;arms cooperate;position assembled tabs;locking arm free;assembled tabs;locking positions pair;center assembly tabs;assembly tabs", "pdf_keywords": ""}, "c589c4ec7247980f38a6bd22f215fea8028a0f66": {"ta_keywords": "methods interpretable nlp;interpretable nlp explain;interpretable nlp;datasets interpretable nlp;interpretable nlp understand;annotation results speci\ufb01c;annotation quality;explanation methods interpretable;annotations conduct;annotation quality highly;annotation results;annotations conduct comprehensive;nlp explain model;results reveal annotation;annotations;annotation process careful;annotation results obtained;using annotations conduct;annotation process;details annotation process;annotation;nlp explain;provide certain annotations;reveal annotation quality;details annotation;annotations instructions speci\ufb01cally;using annotations;annotations instructions;certain annotations instructions;certain annotations", "pdf_keywords": "nlp details crowdsourced;crowdsourced annotation process;details crowdsourced annotation;crowdsourced annotation;details crowdsourced;experiments using crowdsourced;annotation quality;annotation quality highly;crowdsourced;annotation process largely;using crowdsourced;crowdsourced websites;using crowdsourced websites;crowdsourced websites widely;results reveal annotation;annotation results;reveal annotation quality;annotation process eraser;annotations;annotation process;online crowd sourced;crowd sourced platforms;annotation results obtained;interpretable nlp details;annotation;datasets interpretable nlp;benchmarks interpretable nlp;certain annotations;nlp details;provide certain annotations"}, "ecc520794da34d2b141235002c70b06c999bda73": {"ta_keywords": "sense embeddings evaluations;interpretable sense representations;existing sense embeddings;sense embeddings;evaluation sense embeddings;sense embeddings existing;existing sense representations;discovering interpretable sense;embeddings existing sense;sense representations;sense representations actually;sense representations coherent;uncover sense representations;attention sense induction;language sense inventory;sense induction optimized;sense representations uniformly;sense induction;coherence evaluation sense;discovering interpretable;inspecting language sense;evaluations uncover sense;optimized discovering interpretable;interpretable sense;existing sense;coherent existing sense;language sense;attention sense;embeddings evaluations;embeddings evaluations uncover", "pdf_keywords": ""}, "c12e46d7d0bb9fa062bce0549f2a6a9de00758d5": {"ta_keywords": "sound event detection;bert natural language;sound event;transformer weakly supervised;convolutional recurrent neural;sequence weak label;novel sound event;recurrent neural network;self attention modules;convolutional recurrent;attention modules;weakly supervised learning;weak label prediction;based convolutional recurrent;recurrent neural;weakly supervised;dcase2019 task4 baseline;event detection sed;input feature sequence;outperforms dcase2019 task4;natural language processing;attention modules allowing;event detection;encoder;attention;feature sequence;token input sequence;bert natural;success bert natural;incorporates self attention", "pdf_keywords": ""}, "f2fb4a931580c4f1c5bdb47ebc80c801b422cd1a": {"ta_keywords": "language using neuroevolution;grounding language;grounds language actions;grounding language using;task evolving neural;logic grounding language;robot computer understand;language actions;neuroevolution discover grounding;artificial intelligence;grounds language;evolving neural;neural network understand;computer understand natural;ability robot computer;agent perform tasks;control logic grounding;using neuroevolution;artificial intelligence little;neuroevolution;logic grounding;grounding actions;neural;language actions necessity;evolving neural network;neuroevolution discover;robot computer;natural language instructions;understand natural language;ability robot", "pdf_keywords": ""}, "57b972ebe314cfe8e57fd6b9f9239123eb70e979": {"ta_keywords": "encoders connectionist temporal;temporal classification cnn;connectionist temporal classification;rnns encoder ctc;networks rnns;encoder ctc cnns;rnns encoder;instead rnns encoder;convolutional encoders connectionist;training time decoding;networks rnns compare;neural networks rnns;recurrent neural networks;temporal classification ctc;ctc cnns;decode connectionist temporal;performance lstms;performance cnn;deep convolutional neural;study convolutional encoders;rnns;ctc cnns lack;encoders connectionist;lstms matching faster;lstms;convolutional encoders;effectively instead rnns;speech recognition;rnns compare performance;performance cnn based", "pdf_keywords": "speech recognition connectionist;connectionist temporal classification;encoders connectionist temporal;networks rnns;rnns encoder ctc;recognition connectionist temporal;convolutional encoders connectionist;rnns encoder;instead rnns encoder;conversational speech recognition;neural networks rnns;recurrent neural networks;cnns encoders ctc;networks rnns explore;rnns explore deep;speech recognition;rnns;training time decoding;recognition connectionist;convolutional ctc models;neural networks long;encoders connectionist;cnns encoders;encoders ctc models;deep convolutional neural;recurrent neural;convolutional architectures trained;instead rnns;effectively instead rnns;speech recognition typically"}, "6f79cbe893ae46a6f97617d14656ab57c26e6faf": {"ta_keywords": "directional automatic speech;speaker speech recognition;multi speaker data;speaker data;speaker data end;explicit speaker locations;speech recognition source;speech recognition asr;models source speaker;speech recognition;automatic speech recognition;source speaker locations;asr localization separation;automatic speech;speaker locations;multi speaker speech;speaker locations advantages;recognition source localization;speaker locations improves;field multi speaker;source speaker;asr localization;localization separation recognition;e2e multi speaker;source localization;multi speaker;microphone array;provides explicit speaker;directional asr;recognition asr", "pdf_keywords": "speech recognition asr;directional automatic speech;speech recognition source;speaker speech recognition;speech recognition;\ufb01eld speech recognition;asr localization separation;automatic speech recognition;automatic speech;models source speaker;speech processing;speaker locations functionalities;multispeaker data end;multi speaker speech;directional asr asr;multispeaker data;source speaker locations;source speaker;asr localization;\ufb01eld multispeaker data;language speech processing;directional asr;technique directional asr;recognition asr;far \ufb01eld multispeaker;multispeaker;functionalities asr localization;recognition source localization;localization mimo speech;localization separation recognition"}, "5cb74e269c57263d475734a66d34e4d2d2f9e1ac": {"ta_keywords": "vibration characteristics truss;truss core panels;additive manufacturing lam;laser additive manufacturing;characteristics truss core;characteristics truss;based laser additive;panels based laser;constructed using lam;structures particular truss;modal test vibration;particular truss core;test vibration characteristics;truss core;panels base laser;vibration characteristics;test vibration;laser additive;finite element method;manufacturing lam;manufacturing lam subject;particular truss;additive manufacturing;lam compared modal;base laser additive;study finite element;core panels base;vibration;core panels;truss", "pdf_keywords": ""}, "cefd3993db4d065b95ab8f105452fb728c02b60e": {"ta_keywords": "review generation;automate scientific reviewing;good review generation;generated reviews;human written reviews;written reviews generated;generated reviews tend;reviews generated text;review generation potential;reviews generated;results generated reviews;reviews scientific papers;nlp models generate;scientific reviewing;scientific reviewing discussing;defining good review;peer reviews scientific;written reviews;nlp models;reviewing;peer reviews;reviewed scientific publications;review paper;review paper laborious;processing nlp models;peer reviewed scientific;reviews tend;processing nlp;papers machine learning;pass peer reviews", "pdf_keywords": "automate scienti\ufb01c reviewing;review generation;good review generation;nlp models generate;review generation potential;nlp models;scienti\ufb01c papers review;2015 automate scienti\ufb01c;papers review;papers review paper;reviews scienti\ufb01c papers;papers machine learning;processing nlp models;processing nlp;scienti\ufb01c reviewing;peer reviews scienti\ufb01c;automate scienti\ufb01c;scienti\ufb01c reviewing discussing;review paper;reviewing;language processing nlp;peer reviews;natural language processing;pass peer reviews;review paper tl;reviewing discussing;reviews scienti\ufb01c;question automate scienti\ufb01c;nlp;dataset papers machine"}, "f491a5f09ee01436d772a6cff25f22d700d8c9c0": {"ta_keywords": "power distribution networks;power flow analysis;distributed generator dg;distributed generator;distribution network solution;approach distribution network;power flow;challenges distributed generator;electricity renewable energy;method power flow;support renewable generation;voltage support renewable;way power distribution;distribution network;power distribution;electricity renewable;reactive power support;fast power flow;distribution networks dns;renewable generation;sector electricity renewable;distribution networks;generator dg;generator dg characteristics;renewable energy;augmented nodal analysis;renewable generation storage;nodal analysis approach;sector electricity;reactive powers dgs", "pdf_keywords": ""}, "72213e24713264da816f43a42d606f115998fe7b": {"ta_keywords": "stacked graphical learning;learning online stacked;stacked learning online;stacked learning;online stacked graphical;window stacked learning;graphical learning nearly;graphical learning text;learning text mining;algorithm stacked graphical;idealized stacked graphical;stacked graphical;online stacked;graphical learning combining;applications stacked graphical;graphical learning;algorithm stacked;online learning algorithm;graphical learning real;learning text;graphical learning applied;text mining;learner classify instances;intuition stacked graphical;graphical learning studied;graphical learning save;version stacked graphical;stacked;idealized stacked;learned greedy", "pdf_keywords": ""}, "c04c865c8b33ce0251c9f37d0cccf2b3b1e4fd34": {"ta_keywords": "learning causal state;learn policies reinforcement;policies reinforcement learning;observable navigation tasks;partially observable navigation;efficiently learn policies;policies reinforcement;state representations rnns;causal state representations;reinforcement learning;learning causal;partially observable markov;observable navigation;agnostic state abstractions;approximate causal states;partially observable environments;state abstractions;reinforcement learning problems;causal states optimally;extracts causal state;observable markov decision;methods learning causal;state representations partially;state abstractions used;actions observations partially;learn policies;learned task agnostic;observable markov;observations partially observable;rnns", "pdf_keywords": "learning causal state;learning state;learning state representations;causal state representations;learning causal;step prediction pomdps;causal states reconstruction;prediction pomdps articulated;22 learning causal;prediction pomdps;causal state representation;algorithm learning causal;approximate causal states;causal 22 learning;observations partiallyobservable markov;state representations partially;mechanics causal inference;states causal feature;partiallyobservable markov;pomdp connections causal;algorithm approximate causal;actions observations partiallyobservable;state representations;partiallyobservable markov decision;approach learning latent;approach learning state;learning latent representations;decision processes pomdp;causal states coarsest;principled approach learning"}, "8504a5eb4638aeb2f61f8b7f93440b9e495b443b": {"ta_keywords": "centralized tracking stochastic;selection centralized tracking;tracking stochastic;sensor activation centralized;dynamic sensor subset;dynamic sensor activation;tracking stochastic process;centralized tracking;activation centralized tracking;sensor subset selection;sensor selection;active sensor selection;energy efficient tracking;sensor networks cyberphysical;stochastic approximation learning;sensor selection numerical;dynamic sensor;sensor networks;sampling stochastic approximation;sampling stochastic;efficient tracking;gibbs sampling stochastic;sensor subset;tracking proposed methods;tracking proposed;algorithm tracking proposed;stochastic approximation;cases dynamic sensor;sensor activation;efficient tracking mechanisms", "pdf_keywords": ""}, "8e7d063c681557c94382ff3da6415d3720fe11a7": {"ta_keywords": "twitter stance detection;stance detection bidirectional;stance detection corpus;stance detection task;neutral stance detection;stance detection;twitter stance;task twitter stance;representation tweet dependent;labelled tweets test;text target hillary;tweet dependent target;tweets test target;representation tweet;labelled tweets;tweets test;tweet target;negative neutral stance;detection corpus achieving;encoding tweet target;2016 task twitter;detection bidirectional conditional;detection corpus;neutral stance;automatically labelled tweets;classifying attitude expressed;expressed text target;attitude expressed text;builds representation tweet;conditional lstm encoding", "pdf_keywords": "twitter stance detection;stance detection twitter;tweet stance detection;stance detection corpus;2016 stance detection;lstm encodes tweet;tweet stance;stance detection task;task tweet stance;representation tweet dependent;twitter stance;stance detection previously;detection twitter task;training word embeddings;semeval stance detection;present stance detection;task twitter stance;detection task tweet;representation tweet;stance detection methods;stance detection;example tweet realdonaldtrump;detection twitter;novel stance detection;detection twitter dataset;stance target donald;term memory lstm;positive stance target;labelled tweets;word embeddings"}, "8568f6eda2e4cb7921fe175ab44b2f5ecbb2b870": {"ta_keywords": "cause reading difficulty;transpositions harder misspellings;reading difficulty;words cause reading;readers unimpaired comprehension;reading difficulty presence;misspelling error rates;surprisal human readers;harder misspellings contain;harder misspellings;human readers unimpaired;reading difficulty correct;words difficultto predict;naturally occurring misspelling;increased reading times;word based surprisal;human readers;difficulty correct words;model reading difficulty;occurring misspelling;misspellings contain unexpected;misspelling;increased reading;misspellings;words contain errors;character based surprisal;unimpaired comprehension;misspellings contain;readers unimpaired;eye tracking study", "pdf_keywords": ""}, "ae77189921ffade5ee4c4d4a0e93e879d7280b80": {"ta_keywords": "forecasting gestures poses;forecast avatar pose;pose forecasting gestures;predict avatar pose;forecasting body pose;multimodal forecasting body;grounded pose forecasting;pose forecasting;dialogue grounded pose;accurately forecast avatar;forecasting gestures;virtual avatars dialogue;gestures virtual avatars;model multimodal forecasting;multimodal forecasting;forecast avatar;poses speaker;avatar pose;gestures poses speaker;body pose gestures;gestures poses;predict avatar;better predict avatar;avatars dialogue;pose gestures;body pose;poses speaker linked;pose gestures virtual;pose;poses", "pdf_keywords": ""}, "93e012cbf8e29aacb9654313250a81d53bbcbdf2": {"ta_keywords": "email leak detection;detect email leaks;email leaks method;email leaks;associated email leaks;email leaks begin;email leak;cases email leaks;email leaks paper;leak recipients learn;email leaks 82;leak recipients;problem email leak;simulated leak recipients;detect email;leak detection presents;real email examples;patterns associated email;recipients outliers;easily implemented email;leak detection;unintended recipients outliers;able detect email;email examples;email examples enron;implemented email;implemented email client;email client;email server;email client changes", "pdf_keywords": ""}, "df99459a75328393a9a989498db46ec445335724": {"ta_keywords": "peer review data;review data privacy;reviewers authors privacy;release peer review;privacy preserving release;peer review unavailability;review unavailability peer;data released privacy;unavailability peer review;data privacy preserving;improving peer review;sensitivity peer review;review data release;peer review;privacy preserving manner;privacy preserving;data retaining privacy;data pertaining reviews;protecting identities reviewers;authors privacy utility;review data;released privacy mechanism;data privacy;conference peer review;reviews available public;privacy guarantees identifying;review data distributions;retaining privacy guarantees;review data terms;tradeoff peer review", "pdf_keywords": "peer review data;release peer review;privacy preserving release;privacy preserving dissemination;data released privacy;privacy preserving;peer review;data retaining privacy;improving privacy utility;released privacy mechanism;conference peer review;reviewers paper protected;reviews available public;literature privacy preserving;data pertaining reviews;improving privacy;algorithm improving privacy;retaining privacy guarantees;retaining privacy;privacy utility;preserving dissemination data;review data ensuring;ensuring identities reviewers;review data;released privacy;review data distributions;privacy guarantees;privacy mechanism;privacy;privacy mechanism manner"}, "c4bc2f7e04e02107aa6eaa0c811c3c046efbbc14": {"ta_keywords": "programming demonstration learning;free prolog;demonstration learning straightline;free prolog clauses;learning description logic;demonstration learning;learning problems;learning problem examples;finite automata;prolog clauses ground;natural learning problems;learning problems including;hardness natural learning;subconcepts learning arity;function free prolog;learning straightline programs;simple learning;deterministic finite automata;prolog;learning problem concepts;finite automata dfas;learning arity determinate;problems including learning;hardness results learning;hardness formal problems;analyze simple learning;ground clauses hardness;learning arity;automata;including learning description", "pdf_keywords": ""}, "69b184f62c97513b03deed96a1443f79b34af0d7": {"ta_keywords": "dishonest reviewers adversarially;robustness dishonest reviewers;reviewers adversarially influence;dishonest reviewers collude;reviewers adversarially;bidding assign reviewers;reviewers bid papers;adversarially influence paper;dishonest reviewers;assign reviewers papers;reviewers bid;reviewers collude;bid manipulation attacks;suggests reviewers bid;reviewers papers;reviewers collude knowledge;assign reviewers;robust bid manipulation;reviewers papers paper;adversarially influence;provides robustness dishonest;rely paper bidding;door dishonest reviewers;paper reviewing robust;influence paper reviewing;reviewing robust bid;efficacy bid manipulation;paper bidding enables;adversarially;manipulation attacks jeopardize", "pdf_keywords": "bid manipulation attacks;robust bid manipulation;manipulation attacks;adversaries reviewer pool;assignment robust bid;paper assignment robust;robustness dishonest reviewers;manipulation attacks realistic;reviewing robust bid;assignment robust;adversaries reviewer;manipulation attacks ment;multiple adversaries collude;bid manipulation;manipulation attacks settings;adversaries collude;box adversaries reviewer;robust bid;attacks realistic paper;dishonest reviewers collude;provides robustness dishonest;paper reviewing robust;adversaries collude depth;ef\ufb01cacy bid manipulation;attacks ment robust;reviewers collude;dishonest reviewers;robustness dishonest;reviewers collude knowledge;reviewing robust"}, "b21fa4f31c4813444e50259dfbe2c56660161174": {"ta_keywords": "segments active learning;learning machine translation;machine translation;active learning;active learning machine;syntactic non redundant;selecting syntactic;redundant segments active;selecting syntactic non;segments active;syntactic;non redundant;translation;non redundant segments;active;redundant segments;syntactic non;redundant;selecting;segments;learning machine;learning;non;machine", "pdf_keywords": ""}, "89fd287f7eacc7a40d0216ba3b919812da658b94": {"ta_keywords": "deep inference latent;lfads models;temporal super resolution;inference latent dynamics;deep inference;selective backpropagation time;optimizing model hyperparameters;backpropagation time mentioned;model hyperparameters;backpropagation time;lfads models benefit;pbt deep inference;selective backpropagation;spatio temporal super;latent dynamics spatio;temporal super;latent dynamics;backpropagation;regularization strategy coordinated;framework combines regularization;using selective backpropagation;autolfads;model hyperparameters population;regularization strategy;regularization;training pbt deep;lfads;inference latent;combines regularization strategy;combines regularization", "pdf_keywords": "learning latent dynamics;trained sparse;models trained sparse;trained sparse data;cortex models trained;latent dynamics captured;neural dynamics;latent dynamics data;relevant neural dynamics;sequential autoencoders demonstrate;data novel neural;electrophysiology data macaque;neural dynamics 80;model sparse datasets;modeling neural;neural population dynamics;modeling neural population;real electrophysiology data;electrophysiology data;autoencoders demonstrate;data model sparse;sequential autoencoders;space modeling neural;features latent dynamics;learning deep;neural population activity;sparse datasets;training data novel;selective backpropagation time;autoencoders"}, "c5950fa3ee124cf2dcb8783db6f582f49170fb45": {"ta_keywords": "generalization gap training;data guarantee generalization;guarantee generalization;label noise generalization;generalization machine learning;guarantee generalization approach;bound generalization gap;typically bound generalization;certify generalization labeled;deep learning early;guarantees overparameterized models;guarantees overparameterized;empirical risk minimization;generalization labeled;leveraging unlabeled data;validate empirically holdout;bound generalization;generalization gap;certify generalization;leveraging unlabeled;noise generalization;vacuous guarantees overparameterized;classifiers achieve low;generalization labeled holdout;intuitive assumption classifiers;empirically holdout data;generalization;assess generalization machine;unlabeled data guarantee;generalization machine", "pdf_keywords": "label noise generalization;generalization bounds empirically;generalization guarantees track;nlp tasks bound;vision nlp tasks;generalization labeled;generalization guarantees;computer vision nlp;certify generalization labeled;deep learning early;vacuous generalization guarantees;bounds empirically canonical;vision nlp;vision natural language;deep learning;data produce generalization;noise generalization;produce generalization;training bound;produce generalization bounds;generalization bounds;training bound true;empirically canonical;leverages unlabeled data;certify generalization;random label;benchmark datasets;non vacuous generalization;nlp tasks;vacuous generalization"}, "a7822238f5db7d62731eaeabf9725a65f4edf893": {"ta_keywords": "xl language modeling;language modeling longer;language modeling;transformer xl language;longer term dependency;xl language;term dependency;modeling longer term;language;modeling;transformer xl;dependency;modeling longer;transformer;longer term;term;xl;longer", "pdf_keywords": ""}, "9cdf512f273083efa1ea01f7b31daa97a7bbe884": {"ta_keywords": "coded computation schemes;existing coded computation;coded computation emerged;coded computation;coded computation frameworks;codes coded computation;coded computation leverage;coded computation multivariate;coded computation nonlinear;fundamental coded computation;design coded computation;locality codes design;model coded computation;coded computation lens;workers coded computation;results coded computation;locality codes introduce;locality codes;present coded computation;studied locality codes;functions computational locality;computation schemes;codes introduce generalized;computation lens locality;computational locality equivalent;computational locality;bound existing coded;denoted computational locality;locality denoted computational;computational locality building", "pdf_keywords": "coded computation schemes;coded computation enables;coded computation;coded computation admits;computation schemes locality;coded computation model;workers coded computation;coded computation section;coded computation leverage;locality codes design;design coded computation;present coded computation;locality codes;studied locality codes;model coded computation;computational locality required;computational locality;computation schemes exploit;multivariate polynomial computations;computation schemes;schemes locality based;polynomial computations compared;adaptively exploit locality;polynomial computations;equivalence computational locality;computation schemes multivariate;proposed locality based;workers coded;codes design coded;coded"}, "6aeb477e5f0882f8363a3a8e5e6f83962d91edc6": {"ta_keywords": "accelerated future learning;future learning feature;future learning learning;future learning;rapidly prior learning;learning better learning;future learning measured;better learning strategies;learning learning proceeds;better learning strategy;learning learning;better learning;prior learning;learning techniques feature;knowledge better learning;learning mechanisms yield;learning better;future learning use;learning feature;future learning little;speed learning;prior learning considered;learning proceeds effectively;speed learning process;future learning paper;learning outcomes;knowledge learning better;learning mechanisms;learning;learning strategies", "pdf_keywords": ""}, "39ab4b9cdeb4a71b3e25fe5339962654c7c9ff8c": {"ta_keywords": "fake news spreader;news spreader prediction;false information spreaders;network fake news;credibility features node;news spreader;trust credibility features;predict false information;identify nodes spread;based graph neural;exist social networks;information spreaders;credibility features;nodes spread;information spreaders accuracy;spreader prediction;spreader prediction efficient;network fake;trust credibility;social networks;graph neural network;neural network fake;social networks competing;nodes spread path;networks competing influence;graph neural;fact checking exist;endorse false information;predict false;real world twitter", "pdf_keywords": "credibility based graph;credibility features node;social network predict;news spreader prediction;fake news spreader;trust credibility features;network fake news;trust credibility based;people credibility interpersonal;based graph neural;social network;trust credibility;graph neural network;credibility interpersonal trust;false information spreaders;people credibility;spread false information;credibility interpersonal;credibility based;graph neural;credibility features;propose graph neural;news spreader;network predict node;trust features social;features social network;aggregate trust credibility;integrates people credibility;attention based graph;node likely spread"}, "00799dceb9e7209bb9d71b38fa5b49483e886978": {"ta_keywords": "dynamic nns tensorflow;training dynamic nn;dynamic nn architectures;deep learning dl;nns tensorflow;inference dynamic nns;dynamic nn;dynamic network architectures;recent deep learning;dynamic neural;training inference dynamic;dynamic neural network;frameworks dynamic nns;deep learning;dynamic nns;tensorflow;nns tensorflow fold;runtime overcomes bottlenecks;handling dynamic network;dynamic network;nns recent deep;moving dynamic neural;dynamic nn static;nn architectures;network nn architectures;dataflow graph construction;speedup training dynamic;neural network nn;vertex function dynamic;nn static vertex", "pdf_keywords": ""}, "ab57c70c14b82d07c40c75fecaac98b0e2dc0510": {"ta_keywords": "supervised record linkage;record linkage methods;model record linkage;supervised record;linkage paper hierarchical;fully supervised record;referent records known;record linkage;matching referent records;hierarchical graphical model;referent records;record linkage paper;latent variables hierarchical;linkage methods;supervised;unsupervised setting hierarchical;names record linkage;matching referent;framework record linkage;graphical model record;record linkage problem;hierarchical model task;label latent variables;linkage problem unsupervised;hierarchical model;label latent;record linkage experimental;noisily label latent;hierarchical graphical;hierarchical", "pdf_keywords": "hierarchical graphical model;latent variables hierarchical;hierarchical graphical;supervised record linkage;hierarchical latent;hierarchical latent variable;described hierarchical latent;labels latent variables;paper hierarchical graphical;hierarchical model;variables hierarchical model;label latent variables;supervised;model paper hierarchical;proposed hierarchical;hierarchical;hierarchical model note;variables hierarchical;\ufb01tting graphical model;constraints graphical model;labels latent;supervised record;latent variable graphical;noisily label latent;described hierarchical;note proposed hierarchical;variable graphical model;graphical model framework;noisy labels latent;proposed hierarchical model"}, "6413e6a4f68be0ea6aed0082b205147d9f893699": {"ta_keywords": "morphological inflection generation;inflection generation character;models inflection generation;inflection generation task;inflection generation;morphological inflection;transformation morphological inflection;inflection generation using;solving morphological inflection;linguistic transformation morphological;generation character sequence;generating inflected;sequence sequence learning;sequence learning;problem inflection generation;variant neural encoder;neural encoder decoder;generation using character;generating inflected form;models inflection;neural encoder;character sequence sequence;character sequence;morphologically rich languages;using character sequence;sequence learning problem;linguistic transformation;inflection;sequence learning evaluate;particular linguistic transformation", "pdf_keywords": "sequence transducer morphological;sequence learning manaal;morphological in\ufb02ection generation;sequence sequence learning;sequence learning;language model trained;generation character sequence;neural network sequence;sequence sequence transducer;sequence string transducer;language model;character language model;in\ufb02ection generation character;linguistic transformation model;sequence transducer;character sequence sequence;abstract morphological in\ufb02ection;sequence learning problem;model trained vocabulary;linguistic transformation;character sequence;generation based neural;particular linguistic transformation;integrating character language;transducer morphological in\ufb02ection;using character sequence;trained vocabulary language;morphological in\ufb02ection;sequence sequence string;sequence string"}, "8dd85c3a3700d0d282ddbd4dff5e238f24c00676": {"ta_keywords": "gaussian mixture modelling;speech spectral envelope;bayesian modelling speech;framework gaussian mixture;mixture modelling histogram;modelling speech spectrum;mixture model histogram;modelling speech spectral;gaussian mixture;mixture gaussians mog;mixture gaussians;gaussians mixture;modelling speech;method modelling speech;mixture modelling;using mixture gaussians;speech spectrum using;speech spectral;spectrum using mixture;fewer gaussians mixture;speech spectrum;gaussians mixture resulting;mixture model;envelope using mixture;spectral envelope using;model histogram;ml variational bayesian;model histogram data;envelope maximum likelihood;modelling histogram", "pdf_keywords": ""}, "74dccd379776bbb50b352c19b8caf2a7896d58ee": {"ta_keywords": "constraints power optimisation;variables operating constraints;constraints application sensitivity;operating constraints power;systems set constraints;operating constraints application;operating constraints;sensitivity based coherency;operating constraints dictate;constraints application;constraints power;identifying coherent constrained;coherent constrained variables;constrained variables;power optimisation problems;constrained variables included;coherency analysis method;large power optimisation;optimisation problems deal;constraints;power optimisation;set constraints large;coherent constrained;based coherency analysis;set constraints;failed operating constraints;coherency analysis large;constraints large;constraints dictate control;coherency analysis", "pdf_keywords": ""}, "6a3cc30d5d6342d912851deb4362b8c47fa5ede3": {"ta_keywords": "debiasing nlu models;biases nlu models;nlu tasks datasets;debiasing nlu;biases nlu;utilizing biases knowing;tendency debiasing nlu;models exploit biases;utilizing biases;nlu models exploit;certain biases nlu;self debiasing framework;models unknown biases;biases specifically targeting;targeting certain biases;exploit biases;unknown biases;debiasing framework;introducing self debiasing;improvement challenge datasets;debiasing methods;biases specifically;mainly utilizing biases;nlu tasks;biases knowing;bias known;bias known priori;debiasing framework prevents;mitigating tendency debiasing;bias", "pdf_keywords": "leveraging biased features;leveraging biased;potentially biased examples;models debiased using;biased features;utilizing biases knowing;models debiased;identifying potentially biased;improvement challenge datasets;utilizing biases;biased features super\ufb01cial;bias results better;robustness introduction neural;bias results;self debiasing framework;biased examples;nlu leveraging biased;biased examples allows;mainly utilizing biases;compared models debiased;debiasing methods effective;insight synthetic dataset;biased examples furthermore;introducing self debiasing;debiasing framework;debiasing methods;debiasing framework prevents;biases knowing;bias;biases"}, "51f6654b9d5925002ccaa5cd339b4377b96719ce": {"ta_keywords": "workstation users clustering;users clustering;users clustering email;designed clustering emails;clustering emails user;clustering emails;clustering email;automatically discovering;inferring ongoing activities;interested automatically discovering;emails user activity;cluster knowledge user;user activities automatically;ongoing activities workstation;knowledge user activities;activities automatically inferred;user activities;cluster knowledge;clustering;unsupervised clustering methods;activities automatically;designed clustering;unsupervised clustering;folder meeting;information discovered cluster;clustering methods designed;methods designed clustering;user activity;according activity automatically;activity automatically", "pdf_keywords": ""}, "f4906089f0720c83e57e4a46ae75283df4d67e5a": {"ta_keywords": "science peer selection;funding peer evaluation;peer evaluation increasingly;peer evaluation;peer selection;mechanisms peer selection;funding peer;proposals funding peer;peer review;choices desire crowdsourced;peer review evaluation;peer selection properties;peer selection problems;mechanisms peer;crowdsourced approach participants;desire crowdsourced;participants making selection;science peer;10 mechanisms peer;peer;desire crowdsourced approach;crowdsourced;scientific funding bodies;scientific funding;process scientific funding;funding bodies selected;evaluation increasingly popular;crowdsourced approach;subset proposals funding;grading moocs athenian", "pdf_keywords": ""}, "05f8dd59d4184d38e240bdea4d58e424b8cd055c": {"ta_keywords": "reputation persuasion constructing;reputation persuasion;effect reputation persuasion;debates argumentation platform;persuasion deliberation;persuasion deliberation individuals;persuasion platform average;reputation moderated characteristics;persuasion platform;causal effect reputation;argumentation platform;persuasion constructing;persuasive power reputation;successful persuasion deliberation;successful persuasion platform;million debates argumentation;reputation heuristic information;probability successful persuasion;panel million debates;shaping opinions drive;behavior impact reputation;persuasion;debate competition;debate competition controlling;impact reputation;effect reputation;reputation deliberation online;constructing instrument reputation;debates argumentation;reputation measure", "pdf_keywords": "reputation persuasion constructing;persuasion reputation;persuasion reputation serves;reputation persuasion;effect reputation persuasion;model persuasion reputation;endogenous opinion selection;large online argumentation;persuasion quantify;causal effect reputation;online argumentation;challenger reputation respect;challenger reputation;persuasion constructing;effect challenger reputation;persuasion quantify persuasive;debates argumentation platform;constructing instrument reputation;persuasion;opinion selection;million debates argumentation;successful persuasion quantify;online argumentation platform;argumentation platform;text challenger response;persuasion constructing instrument;panel million debates;debates argumentation;debate competition;challenger response"}, "59f41c5024a238ae8843f3dd67692961ecc63e75": {"ta_keywords": "phoneme recognition spoken;phoneme recognition;development dnn systems;networks dnns constructed;recognition spoken digit;dnn systems;dnn structure;dnn structure automatically;networks dnns;appropriate dnn structure;neural networks dnns;graph dnn structure;dnn systems skilled;spoken digit detection;dnn structure directed;experiments phoneme recognition;discovering appropriate dnn;dnn structure represented;deep neural networks;development dnn;parametrizes dnn structure;acyclic graph dnn;automatically deep neural;graph dnn;recognition spoken;speech processing;understanding expertise dnns;deep neural;spoken digit;limits development dnn", "pdf_keywords": ""}, "f9e32b30fd9ad50cce12ffb753c7be88100b6dc2": {"ta_keywords": "crowd labeling optimal;model crowd labeling;crowd labeled data;crowd labeling;model crowd labeled;crowd labeled;denoising crowd labeled;labeling optimal estimation;aggregating denoising crowd;crowdsourcing;model crowd;based model crowd;crowdsourcing platforms;massive datasets permutation;crowdsourcing platforms massive;labeling optimal;advent crowdsourcing;crowd;denoising crowd;optimal estimation;advent crowdsourcing platforms;computationally efficient estimators;efficient estimators wan;labeled data gained;datasets permutation based;efficient estimators;labeled data;datasets permutation;significance advent crowdsourcing;estimators wan", "pdf_keywords": "model crowdsourced labeling;estimation noisy crowdsourced;crowdsourced labeling;crowdsourced labeling considerably;noisy crowdsourced labels;crowdsourced labels;model crowdsourced;noisy crowdsourced;crowd labeled data;crowdsourced labels 39;based model crowdsourced;crowdsourced;model crowd labeled;objective labeling tasks;crowd labeled;labeling tasks;objective labeling;permutationbased model crowd;labeling tasks involving;problem estimation noisy;labeling;labeling considerably general;estimation noisy;problem estimation;labeling considerably;labeled;estimation;labeled data;estimators design computationally;model crowd"}, "15ac2d8629ca9241ea558eb2b816272d82447ac7": {"ta_keywords": "challenges crowdsourcing learning;crowdsourcing learning;crowdsourcing learning form;crowdsourcing multiplicative mechanisms;tradeoff crowdsourcing;variance tradeoff crowdsourcing;tradeoff crowdsourcing data;crowdsourcing multiplicative;crowdsourcing data;crowdsourcing;crowdsourcing data collected;challenges crowdsourcing;critical challenges crowdsourcing;interfaces crowdsourcing multiplicative;interfaces crowdsourcing;common interfaces crowdsourcing;people challenge estimation;challenge incentivizing people;incentive mechanisms;incentive mechanisms elicit;challenge incentivizing;challenge estimation various;challenge estimation;incentive;learning estimation;designing incentive mechanisms;win statistical bias;estimators automatically adapt;designing incentive;statistical bias", "pdf_keywords": ""}, "18ddcd250bbbe716a0616412ea329a8343f60542": {"ta_keywords": "voting incentives crowdsourcing;incentives crowdsourcing;incentives crowdsourcing supplementary;incentives crowdsourcing let;crowdsourcing let p1;crowdsourcing supplementary;crowdsourcing supplementary material;approval voting incentives;crowdsourcing;crowdsourcing let;voting incentives;material approval voting;incentives;correct probability p1;approval voting;p1 probability according;probability p1;supplementary material approval;let p1 probability;p1 probability;probability according belief;probability according;option correct probability;probability;worker believes option;correct probability;belief worker option;material approval;correct worker believes;voting", "pdf_keywords": ""}, "c18700ed4ef07dd85ba8bceab3b9584c6e6af49c": {"ta_keywords": "billion crawled urls;billion crawled;general web crawl;large web corpus;crawled;web crawl;crawl date billion;web corpus;corpus 10 billion;date billion crawled;web corpus 10;crawled urls;large web;tokens licensed creativecommons;billion tokens licensed;billion tokens;10 billion tokens;commoncrawl largest publicly;languages extracted commoncrawl;crawl;general web;licensed creativecommons license;50 languages extracted;creativecommons license;web crawl date;licensed creativecommons;corpus;crawl date;tokens licensed;extracted commoncrawl", "pdf_keywords": ""}, "af4e11436268cf68505f1caee5d6f7ff0df9c99a": {"ta_keywords": "nlp models causal;research causality nlp;causality nlp;causality nlp remains;causal inference language;causal inference improve;estimating causal effects;challenge estimating causal;causal inference computational;estimating causal;inference computational linguistics;inference natural language;overview causal inference;learn causal relationships;causal inference natural;causal inference;fairness interpretability nlp;models causal inference;learn causal;uses causal inference;social sciences causality;causal relationships;nlp models;overview causal;unified overview causal;processing research causality;computational linguistics;models causal;causality;causal", "pdf_keywords": "fairness interpretability nlp;causality nlp;causal formalization fairness;causality nlp distinct;fairness interpretability;formalization fairness bias;causal inference improve;text using causal;intersection causality nlp;causal effects text;causal inference computational;causal conclusions text;robustness fairness interpretability;fairness bias problems;fairness bias;uses causal inference;develop causal formalization;ideas causal inference;discussion causal;causal inference used;nlp models trustworthy;formalization fairness;causal formalization;using causal formalisms;inference computational linguistics;causal inference;discussion causal approaches;nlp models;estimating causal;causal formalisms make"}, "f8b32c2edcd7ef098ce40b7fd2e68448ac818191": {"ta_keywords": "eye tracking features;svm novel eye;text using eye;using eye tracking;word detection;unknown word detection;reading using eye;eye tracking;gaze duration word;detect unknown words;native language reading;eye movement features;support vector machines;word detection non;using eye gaze;native language text;personalized unknown word;vector machines svm;machines svm novel;language reading using;eye gaze;detection personalized;machines svm;svm;utilizes gaze duration;detection personalized unknown;svm novel;gaze duration;vector machines;perform detection personalized", "pdf_keywords": ""}, "11e4346e60ac76ad018231a851fbbdb2112044d2": {"ta_keywords": "phrase level veracity;phrase veracity;fact verification benchmark;phrase veracity valued;veracity phrases;level veracity phrases;phrase veracity paper;claim phrase veracity;interpretable fact verification;veracity phrases serves;verify veracity;public fact verification;statement verify veracity;claim phrase level;natural language statement;approach interpretable fact;verify veracity large;textual knowledge source;veracity large scale;faithful accurate interpretability;fact verification;textual knowledge;veracity paper propose;accurate interpretability;verification claim phrase;large scale textual;scale textual knowledge;phrases serves explanations;interpretability easy;natural language", "pdf_keywords": "regularized reasoning interpretable;regularized reasoning;logic regularized reasoning;phrasal veracity learning;veracity learning;modeling regularized logical;regularized logical rules;veracity learning mrc;supervise phrasal veracity;phrase veracity valued;regularized logical;verify veracity large;logic regularized;phrase veracity;veracity large scale;scale textual knowledge;textual knowledge;textual knowledge source;reasoning interpretable fact;predict veracity;phrasal veracity;verify veracity;large scale textual;knowledge source like;interpretable fact veri\ufb01cation;approach interpretable fact;veracity large;predict veracity claim;knowledge source;reasoning interpretable"}, "bcbac71ac64cd6a6aaae41e37ebe960f508ab741": {"ta_keywords": "knowledge massive language;neural language model;massive language models;neural knowledge massive;adaptable interpretable neural;factual information memorized;interpretable neural memory;language models core;subsymbolic neural knowledge;neural memory symbolic;factual information subsymbolic;memory symbolic knowledge;language models;language model;symbolically interpretable factual;interpretable factual information;neural language;experts adaptable interpretable;interpretable neural;facts experts adaptable;training manipulating symbolic;neural knowledge;knowledge massive;commonsense factual information;interpretable factual;adaptable interpretable;information subsymbolic neural;memorized training corpora;symbolic knowledge;core modern nlp", "pdf_keywords": "external fact memory;fact memory neural;memory neural language;fact memory;symbolically bound memories;training manipulating symbolic;memory neural;interfacing neural language;subsymbolic neural knowledge;neural language model;symbolically bound memory;bound memories retraining;neural language;manipulating symbolic representations;memory propose incorporate;memory propose;neural knowledge;facts manipulating symbolically;language model interpretable;factual information subsymbolic;symbolically interpretable factual;model interpretable symbolically;language model modifying;memories retraining;develop neural language;interpretable factual information;memories retraining parameters;neural knowledge conclusion;bound memory propose;bound memories"}, "1ca247158522991ad54cccaac6c6938576a8bd26": {"ta_keywords": "ranking traditional ir;ir achieved mrr;document ranking leaderboard;ir rivals neural;marco document ranking;document ranking;transformer model ranking;ranking leaderboard 2020;ranking;model ranking traditional;ranking leaderboard;bert based models;model ranking;achieved mrr 100;ranking traditional;inferior bert based;ir achieved;06 inferior bert;traditional ir achieved;mrr 100;leaderboard 2020;achieved mrr;leaderboard 2020 12;neural models ms;leaderboard;inferior bert;ir;mrr;mrr 100 equal;bert based", "pdf_keywords": "ir rivals neural;evaluation trec nist;ranking leaderboard 2020;document ranking leaderboard;marco document ranking;document ranking;ranking;outperforms tuned bm25;ranking leaderboard leonid;ranking leaderboard;leaderboard 2020 12;leaderboard 2020;2019 2020 queries;trec nist data;2021 traditional ir;trec nist;ir achieved mrr;2020 queries;evaluation trec;rivals neural models;retrieval toolkit;rivals neural;according evaluation trec;flexneuart retrieval toolkit;nist data achieves;2020 queries respectively;traditional ir rivals;ir rivals;leaderboard;nist data"}, "2eef9173946078c402596b9b080b6878db00b8ac": {"ta_keywords": "food twitter community;food twitter;twitter community data;language food twitter;social media data;detecting obesity risk;twitter social media;twitter social;detecting obesity;t2dm twitter social;twitter community;twitter;detection t2dm twitter;t2dm twitter;methods detecting obesity;social media early;diabetes mellitus t2dm;using social media;obesity;social media;community data;obesity risk;t2dm language food;obesity risk factor;type ii diabetes;challenges using social;diabetes mellitus;using social;diabetes;community data peculiarities", "pdf_keywords": ""}, "38bd034e6a0589bf1132d3e8c79818b271377290": {"ta_keywords": "margin based weightedclassi\ufb01cation;margin based mmi;margin mmi mpe;margin mmi;weightedclassi\ufb01cation error modeled;weighted discriminative training;use margin mmi;error weighted discriminative;svms margin context;uni\ufb01cation margin based;loss arange margin;generalized error weighted;margin based;leveraging bene\ufb01ts margin;based weightedclassi\ufb01cation error;mmi mpe trainingbased;bene\ufb01ts margin use;term margin based;\ufb01eld discriminative training;mpe mmi broader;margin context;margin values support;loss differencing mmi;based weightedclassi\ufb01cation;discriminative training;concept margin space;weighted discriminative;error weighted;svms margin;concept margin", "pdf_keywords": ""}, "3429a6b440fb6f71990bbeda9d097d709634a913": {"ta_keywords": "parser selftraining parsing;parser self training;parser selftraining;selftraining parsing;tree parser selftraining;selftraining parsing errors;translation parse tree;improve parser accuracy;parser output training;improving accuracy parser;translation parse;accuracy parser self;syntaxbased machine translation;parser self;training uses parser;using translation parse;improve parser;automatically generated parse;parser accuracy;machine translation syntax;accuracy parser;accuracy parsing;self training syntax;method improve parser;parse tree parser;parser;accuracy parsing greatly;generated parse trees;generated parse;translation syntax based", "pdf_keywords": ""}, "9ce09b03f056253252f3e8c0c65d86a27117a0ac": {"ta_keywords": "test time adaptation;adaptation entropy minimization;adaptation supervised;adaptation supervised model;unlabeled test data;adaptation entropy;time adaptation supervised;confronts unlabeled test;time adaptation entropy;labeled training data;unlabeled test;labeled training;minimization approach adaptation;test time;measured entropy predictions;fully test time;training data fully;training data;supervised model confronts;adaptation model confidence;objective measured entropy;supervised;test data;propose entropy minimization;time adaptation;data fully test;entropy minimization;entropy minimization propose;help labeled training;entropy predictions faced", "pdf_keywords": "test data adaptation;test time adaptation;adaptation entropy minimization;entropy adaptation;test entropy minimization;adversarial domain adaptation;supervised domain adaptation;entropy adaptation objective;domain adaptation;adaptation adversarial domain;examine entropy adaptation;data adaptation adversarial;adapt test entropy;compare domain adaptation;test time entropy;domain adaptation uda;domain adaptation self;measured entropy predictions;adaptation entropy;data adaptation;adaptation adversarial;test time normalization;test time training;entropy predictions examine;test entropy;predictions test;predictions test data;time adaptation entropy;model predictions test;predictions examine entropy"}, "2db020e3398c06e3a22f12d8caffe76b0d9d1dda": {"ta_keywords": "question answering benchmarks;answering commonsense tasks;shot question answering;question answering commonsense;question answering;commonsense tasks addition;language models learn;commonsense tasks;commonsense question answering;language models training;semantic reasoning recent;answering commonsense;tasks global knowledge;help language models;answering benchmarks;tasks learning;neural language modeling;trained neural language;general semantic reasoning;pre trained neural;accuracy commonsense;benchmarks individual knowledge;neural language;answering benchmarks individual;questions help language;structure task generating;tasks learning utilize;specific tasks learning;perform general semantic;pre training models", "pdf_keywords": "question answering benchmarks;shot question answering;question answering;answering commonsense tasks;question answering commonsense;question answering arxiv;shot evaluation commonsense;commonsense question answering;answering benchmarks;answering benchmarks paper;commonsense tasks;language models training;answering commonsense;accuracy commonsense;evaluation commonsense;question generation techniques;question generation;commonsense tasks vary;neural language modeling;sources question generation;general semantic reasoning;language models;semantic reasoning contrast;knowledge driven;tasks learning;perform general semantic;neural language;trained neural language;knowledge driven data;semantic reasoning"}, "68dca6ee694f22e2af66b56e60fdfa74041242e6": {"ta_keywords": "shinji watanabe spgispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_unnorm_bpe5000_valid;watanabe spgispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_unnorm_bpe5000_valid;watanabe spgispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_unnorm_bpe5000_valid acc;spgispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_unnorm_bpe5000_valid acc ave;spgispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_unnorm_bpe5000_valid;spgispeech_asr_train_asr_conformer6_n_fft512_hop_length256_raw_en_unnorm_bpe5000_valid acc;espnet2 pretrained model;espnet2 pretrained;espnet2;pretrained model shinji;fs 16k lang;pretrained model;16k lang en_unnorm;pretrained;model shinji watanabe;16k lang;fs 16k;model shinji;en_unnorm;fs;ave fs 16k;shinji watanabe;lang en_unnorm;model;shinji;16k;watanabe;lang;ave fs;acc", "pdf_keywords": ""}, "a0f8733dd84608b3cad97904624f8bfdc2d2fcbf": {"ta_keywords": "manage medical equipment;medical equipment;manage medical;special feature special;feature special isuue;special feature;feature special;isuue manage medical;equipment;special isuue manage;medical;feature;special isuue;aim special feature;special;manage;isuue manage;aim special;isuue;aim", "pdf_keywords": ""}, "890317710697a9e41d0d9961d99986c4d865393f": {"ta_keywords": "app usage representations;app usage graph;app usage prediction;app usage representation;app usage learning;semantic aware app;aware app usage;semantic aware spatio;semantic aware representations;usage representation graph;learned representations app;temporal app usage;usage learning semantic;representations app usage;aware app;spatio temporal app;apps location time;app usage;graph regarding app;semantic aware;usage prediction task;usage prediction;app time;aware representations;learning semantic aware;aware spatio temporal;apps;locations apps;usage graph;time locations apps", "pdf_keywords": ""}, "b6c4a96e09b9f11e7c70e7f1fbe3f3971b92762d": {"ta_keywords": "text generation fudge;text generation;controlled text generation;model generating text;generating text;language generation formality;discriminators generation fudge;generating text distribution;language generation;text distribution fudge;generation fudge learns;poetry topic control;control language generation;fudge learns attribute;machine translation;generation formality;learns attribute predictor;discriminators generation;learns attribute;tasks fudge models;future discriminators generation;generation formality change;text distribution;machine translation observe;generation fudge;model generating;generating;fudge models terms;existing model generating;topic control language", "pdf_keywords": "fudge composing multiple;poetry topic control;modularity fudge composing;fudge composing;fudge easily compose;attributes additionally fudge;easily compose predictors;modularity fudge;completion topic control;fudge tasks;completion individual words;tasks fudge achieves;topic control language;fudge tasks couplet;language generation formality;tasks fudge;topic control tasks;topic control informal;control language generation;compose predictors;sentence constraints;fantasy bag words;formal machine translation;demonstrate modularity fudge;additionally fudge easily;attribute constraints;words topic bag;completion poetry;poetry couplet completion;tasks evaluate fudge"}, "97e8430fe01394ea9a49fd841d9aecdfc294a796": {"ta_keywords": "optimization fuzzy;classification myocardial heart;classification myocardial;method optimization fuzzy;optimization fuzzy reasoning;aided diagnosis myocardial;fuzzy reasoning genetic;parameters fuzzy rules;gas classification myocardial;optimizing parameters fuzzy;fuzzy rules using;diagnosis myocardial heart;application discrimination myocardial;heart disease ultrasonic;fuzzy rules;algorithms gas classification;parameters fuzzy;discrimination myocardial heart;diagnosis myocardial;heart disease optimal;computer aided diagnosis;genetic algorithms;fuzzy reasoning;genetic algorithms application;discrimination myocardial;gaussian distributed membership;using genetic algorithms;disease ultrasonic images;genetic algorithms gas;fuzzy", "pdf_keywords": ""}, "33206a493e3519d27df968e98eb7fe6af14ef985": {"ta_keywords": "memory causal relationships;creating memory causal;memory causal;causal relationships;occam incremental learning;causal relationships michael;constructs causal;events constructs causal;constructs causal model;causal;causally;causally connected events;causal model;explanation based learning;causal model processes;sequences causally;similaritybased learning creating;learning creating memory;incremental learning;learning methods like;causally connected;sequences causally connected;examples sequences causally;model human learning;incremental learning inputs;learning;human learning;knowledge intensive;ai machine learning;learning methods used", "pdf_keywords": ""}, "89440a4cb27d17ced5d54bbe0f81f3477bf16404": {"ta_keywords": "entropy discrimination automatic;relative entropy discrimination;speech recognition;automatic speech recognition;entropy discrimination;discrimination automatic speech;minimum relative entropy;automatic speech;relative entropy;kernel machine derived;discrimination automatic;kernel machine;recognition;kernel;entropy;machine derived minimum;speech;derived minimum relative;minimum relative;machine derived;discrimination;derived minimum;relative;machine;minimum;automatic;derived", "pdf_keywords": ""}, "27dafb0b6076e050c31ede8d3e0184ef3592b364": {"ta_keywords": "metal organic framework;electrodeposition based metal;framework superior capacitive;based metal organic;metal oriented electrodeposition;organic framework superior;electrodeposition based;oriented electrodeposition based;metal organic;organic framework;electrodeposition;oriented electrodeposition;superior capacitive performance;capacitive performance;superior capacitive;capacitive;efficient metal oriented;framework;framework superior;efficient metal;based metal;metal oriented;organic;metal;based;superior;efficient;oriented;performance", "pdf_keywords": ""}, "591ebe6dccb388d041623840db29a5e58824b4b0": {"ta_keywords": "preference orders uniformly;underlying preference orders;preference reasoning literature;preference reasoning;statements underlying preference;preference orders;preferences represented;preference tables;preferences represented cp;underlying preference;random cp net;nets experiments empirical;choice preference reasoning;conditional preference tables;preference tables expected;generating cp nets;cp nets experiments;preference variables;complete conditional preference;algorithms generating cp;generation preferences represented;conditional preference;preference variables extend;conjecture cp nets;uniformly random cp;maximum expected flipping;provably generate graph;number preference variables;graph underlying cp;choice preference", "pdf_keywords": ""}, "7f1a6c67d03de88b898271d52dd2e51907d5b615": {"ta_keywords": "tasks natural language;spanning dependency parsing;natural language processing;tasks predicting syntax;semantic role labeling;dependency parsing;semantics relation extraction;role labeling semantics;dependency parsing syntax;predicting syntax semantics;span relation representations;natural language analysis;generalizing natural language;language analysis span;labeling spans relations;relation extraction;spans single task;parsing syntax semantic;relation extraction information;natural language;parsing;relations spans single;predicting syntax;relations spans;spans relations spans;relation representations;syntax semantic role;spans relations;role labeling;semantics information content", "pdf_keywords": "trained contextualized representations;stronger contextualized representation;evaluating natural language;contextualized representations;pre trained contextualized;contextualized representation models;contextualized representations like;trained contextualized;contextualized representation;language analysis datasets;tasks glad benchmark;natural language analysis;stronger contextualized;general language analysis;language analysis capability;provides holistic testbed;testbed evaluating natural;natural language;holistic testbed evaluating;glad benchmark datasets;spans predicts;language analysis;extracts spans predicts;language analysis contrast;benchmark provides holistic;bertlarge achieves best;representation models provide;spans predicts labels;predicts relations spans;contextualized"}, "a9c0ffc760f65ccaa99a08fc66b31653fd4a5bd7": {"ta_keywords": "organ donation india;organ donation;success organ donation;donation organ;donation organ donation;review organ donation;organ donation donation;organ donation organ;organ donation extremely;donation program organ;program organ donation;organ transplantation;transplantation organ;organ donation scooping;lead organ donation;organ donation program;transplantation organ transplantation;donation india nurses;pledging organ donation;need transplantation organ;donation biological;donation biological tissue;organ transplantation rapid;donation donation biological;people pledging organ;recipient need transplantation;pledging organ;transplantation;success organ;review organ", "pdf_keywords": ""}, "a03675379685d88c727bc985a323cc71d06f2514": {"ta_keywords": "unsupervised dependency parsing;unsupervised learning syntactic;learns discrete syntactic;learning syntactic structure;learning syntactic;dependency parsing difficult;dependency parsing;dependency parsing gold;induction unsupervised dependency;discrete syntactic structure;parsing difficult training;syntactic structure invertible;structured generative;discrete syntactic;tree structured priors;word representations unsupervised;network structured generative;structured generative prior;unsupervised dependency;jointly learns discrete;annotation unsupervised learning;parsing difficult;speech pos induction;parsing gold pos;syntactic structure;parsing;generative models;parsing gold;pos annotation unsupervised;generative models discrete", "pdf_keywords": "learning latent syntactic;learns discrete syntactic;latent syntactic representation;generates latent syntactic;prior syntax model;trained word embeddings;latent syntactic structure;syntactic representation;network structured generative;structured generative;word representations unsupervised;syntactic dependencies;word embeddings;word embeddings projecting;latent syntactic;structured prior syntax;projection pretrained embeddings;pretrained embeddings;sequence latent embedding;discrete syntactic structure;structured generative prior;markov structure neural;syntactic representation approach;latent embedding;word representations;jointly learns discrete;new latent embedding;syntactic dependencies dependency;embeddings projecting latent;dependency parse discrete"}, "7f817600b612aab6039dfba576ae8e8e7460d8f1": {"ta_keywords": "learning pronunciation dictionary;supervised learning pronunciation;learning pronunciation;acquire word pronunciations;pronunciation dictionary using;learn pronunciation dictionary;bayesian learning dirichlet;learning dirichlet processes;initial pronunciation dictionary;automatic speech;speech recognition;speech recognition systems;automatic speech recognition;learning dirichlet;pronunciation dictionary containing;pronunciations phone transcripts;learn pronunciation;acoustic textual data;pronunciation dictionary;area learn pronunciation;word pronunciations phone;performance automatic speech;bayesian learning;pronunciation dictionary disjoint;based bayesian learning;sampled acoustic textual;recognize words;humans recognize words;pronunciations phone;disjoint phonemic transcripts", "pdf_keywords": ""}, "60a9438c24847a949419e0350a61fc2a330e4a09": {"ta_keywords": "biases kernel clustering;kernel clustering limitations;kernel clustering criteria;kernel clustering;kernel clustering directly;understanding kernel clustering;density biases kernel;power kernel clustering;kernels findings;bias kernel means;class kernels findings;kernels findings suggest;kernel methods;clustering criteria density;biases kernel;kernel methods popular;isolation bias kernel;theoretical understanding kernel;bias kernel;clustering limitations principled;clustering generality discriminating;popular clustering generality;density biases;clustering generality;density biases proposed;clustering limitations;discriminating power kernel;methods popular clustering;illustrate density biases;kernel means", "pdf_keywords": "kernel clustering criteria;kernel clustering limitations;clustering criteria density;kernel clustering;breiman kernel clustering;approximate optimization clustering;clustering limitations principled;understanding kernel clustering;optimization clustering criteria;clustering limitations;clustering criteria;propose density equalization;kernel means approximates;clustering criteria 68;density equalization;optimization clustering;density equalization principle;clustering;criteria density biases;density biases proposed;density biases;density biases theoretically;continuous generalization gini;means approximates;bias similar discrete;generalization gini criterion;approximates continuous generalization;conditions kernel means;kernel means;mode isolation bias"}, "11db042ed2264f3ea1b8f20151adf725ec3461e8": {"ta_keywords": "error surfaces neural;convergence loss;convergence loss does;convex error surfaces;surfaces neural networks;apparent convergence loss;weights converge point;networks deep learning;trivial neural network;converge point weight;weights converge;networks deep;neural networks deep;deep learning;suggest weights converge;deep learning researchers;neural networks;error surfaces locally;trivial neural;surfaces neural;weights arriving critical;network error surfaces;non convex error;neural;partial training;neural network error;training paths;initialization partial training;error surfaces globally;local minima", "pdf_keywords": "convex trivial neural;convex error surfaces;surfaces neural networks;error surfaces neural;trivial neural network;neural networks appears;non convex error;trivial neural;layer convolutional neural;initialization partial training;neural network error;surfaces neural;network error surfaces;convergence loss;convex error;training standard layer;neural;neural networks;apparent convergence loss;partial training;neural network;networks;nonconvex breaking symmetry;weights arriving critical;training paths weight;training paths;convergence loss does;neural network 819557;error surfaces locally;mnist dataset lecun"}, "201b79be15b6b01e62a82b29ac4d30d3e6a11799": {"ta_keywords": "neural beamformer;rnn based encoder;speech recognition single;network neural beamformer;encoder decoder speech;2mix corpus transformer;neural beamformer multi;decoder speech recognition;corpus transformer;corpus transformer based;speaker speech recognition;speech recognition;decoder speech;fully recurrent neural;speech recognition model;neural network rnn;recurrent neural network;network rnn;network rnn based;rnn based end;recognition single channel;attention component;self attention component;wsj1 2mix corpus;recurrent neural;attention component restricted;recognition model transformer;rnn;rnn based;2mix corpus", "pdf_keywords": "speech recognition module;encoder decoder speech;speech recognition single;decoder network speech;speaker speech recognition;decoder speech recognition;end multispeaker asr;speech recognition model;speech recognition;multispeaker asr;end end multispeaker;end multispeaker;recognition module transformers;neural beamforming module;decoder speech;network speech recognition;lstms encoder;rnn based encoder;multispeaker asr single;end multi speaker;lstms encoder decoder;network speech;recognition model transformer;lstms;multi speaker speech;transformer models endto;network neural beamforming;asr single channel;replace lstms encoder;multispeaker"}, "62606fbb3aa3ffb17c5427b3652c18a81425cd65": {"ta_keywords": "training named entity;named entity recognizer;data annotated training;annotated training data;learning extract gene;entity recognizer;names weakly labeled;gene protein extractor;entity recognizer ner;annotated training;named entity;significant annotated training;gene protein names;protein names weakly;extract gene protein;data annotated;learning extract;data learning extract;generate significant annotated;sources data annotated;extract gene;protein names;weakly labeled text;training data automatically;create training data;training data;protein extractor;protein extractor using;training data learning;labeled text", "pdf_keywords": ""}, "99848c6424556bce427d621e89b6d05dac131910": {"ta_keywords": "crowdsourcing annotation yandex;microtask based crowdsourcing;crowdsourcing annotation;annotation yandex;lexical relations wisdom;passed crowdsourcing annotation;annotation yandex toloka;language using microtask;russian language;yandex toloka microtask;russian language using;crowdsourcing interestingly workers;intelligence task hit;human intelligence task;crowdsourcing;layout human intelligence;dataset kind russian;lexical relations;crowdsourcing interestingly;lexical;intelligence task;pairs passed crowdsourcing;kind russian language;tongue russian;relations wisdom crowd;pair annotated;human intelligence;annotators;language;annotated seven different", "pdf_keywords": ""}, "7a1bcf3c84607f7aeb0601658845ca2083059f43": {"ta_keywords": "semi supervised;semi supervised learning;proposed semi supervised;dataset semi supervised;learning vision language;visual language tasks;time semi supervised;vision language tasks;nlvr2 dataset semi;supervised learning vision;supervised;visual language;supervised learning;vision language;learning vision;leveraging large unlabelled;modal framework lxmert;nlvr2 dataset;supervised learning strategy;tasks using mixmatch;labeled data leveraging;mixmatch;mixmatch limited;dataset semi;performance nlvr2 dataset;mixmatch recently;multi modal framework;modal framework;reformulate mixmatch recently;using mixmatch", "pdf_keywords": ""}, "ef09dd6f5615e2b937d3f9dd555c2daafb4c4f4b": {"ta_keywords": "java based parallel;virtual machines java;machines java language;implement parallel programs;parallel programs;performance java based;machines java;communication performance java;parallel virtual machines;libraries parallel virtual;pvm libraries javapvm;based parallel virtual;java language;parallel programs multiple;performance java;parallel virtual machine;libraries parallel;pvm implemented code;libraries javapvm;different computer architectures;libraries javapvm java;virtual machines;passing libraries parallel;java based;javapvm java;common application programming;pvm libraries;javapvm;java;java code performs", "pdf_keywords": ""}, "aa0d4f7cfa13758a02d248fd607547f045306519": {"ta_keywords": "person recognizers email;extracting personal names;personal names email;names email applying;names email;recognizers email;named entity recognition;entity recognition informal;email applying named;recognizers email email;personal names;person recognizers;recognition informal text;entity recognition;email specific structural;documents extracting personal;extracting personal;recognition informal;performance person recognizers;applying named;structural features recall;features recall enhancing;named entity;recall enhancing method;recognizers;applying named entity;features recall;email applying;names;recall enhancing", "pdf_keywords": ""}, "36db0616e59c8ac5e9ba8ded820ef6c969f068c1": {"ta_keywords": "art time southern;art time;time southern california;southern california guide;contemporary art;california guide;southern california;documentation contemporary art;art;time southern;california guide documentation;california;southern;contemporary;documentation contemporary;time;guide documentation contemporary;guide;documentation;guide documentation", "pdf_keywords": ""}, "78f1eef6d79a129f59b977a5037f5fc9cc7fda90": {"ta_keywords": "improved peer review;improve peer review;peer review;peer review processes;peer review particularly;applications peer review;toolkit improved peer;peer review faces;review processes algorithms;improved peer;improve peer;need improve peer;review processes;peer;objectives fairness accuracy;algorithmic toolkit;achieving objectives fairness;algorithmic toolkit improved;submissions especially ml;fairness accuracy robustness;fairness accuracy;review particularly;developing algorithmic toolkit;objectives fairness;review;applications peer;variety applications peer;submissions especially;algorithmic;developing algorithmic", "pdf_keywords": ""}, "afdae523d420278670c30f45c015cc5860a0de22": {"ta_keywords": "improves convergence adaptive;adaptive gradient methods;gradient methods converge;adaptive gradient;networks adaptive gradient;converge faster parameterization;convergence adaptive methods;neural networks adaptive;adagrad robust;generalizes better sgd;convergence adaptive;adaptive methods tasks;gradient methods;improves convergence;interpolation converges minimizer;consistently improves convergence;momentum variants adam;methods converge faster;classification deep;faster parameterization;converge minimizer rate;better sgd;empirical results adagrad;deep neural networks;adagrad robust violation;converges minimizer optimal;adaptive methods;networks adaptive;deep neural;sgd hand adagrad", "pdf_keywords": "generalization adaptive gradient;practice adaptive gradient;adaptive gradient methods;convergence optimizers;superior convergence optimizers;convergence generalization adaptive;gradient methods converge;adaptive gradient;convergence optimizers using;networks adaptive gradient;gradient methods tasks;converge faster parameterization;generalization adaptive;gradient methods focus;optimizers;optimizers using sls;convex loss functions;stochastic line search;gradient methods;classi\ufb01cation deep networks;iteration adagrad;performance adagrad;size iteration adagrad;improve convergence generalization;deep networks adaptive;smooth convex loss;faster parameterization;optimizers using;methods converge faster;convex loss"}, "656aedc681975c3c97b1764466832de537358150": {"ta_keywords": "unsupervised adaptation dnn;adaptation dnn;adaptation dnn hybrid;acoustic model adaptation;speech recognition asr;hybrid automatic speech;speech recognition;input deep neural;adaptation encoder;based adaptation encoder;adaptation encoder decoder;automatic speech recognition;dnn hybrid automatic;neural network dnn;recognition asr systems;auxiliary features instead;automatic speech;representing speakers vectors;features representing speakers;sequence summary network;network dnn effective;deep neural network;dnn hybrid;recognition asr;network dnn;compute auxiliary features;auxiliary feature extraction;vectors input deep;e2e models asr;deep neural", "pdf_keywords": ""}, "2ccfa631708b78130b1ea1b8ae3c2b688caf3938": {"ta_keywords": "scheduling surgeries challenging;duration scheduling surgeries;scheduling surgeries;estimate duration surgery;schedule surgeries efficiently;able schedule surgeries;schedule surgeries;uncertainty duration scheduling;surgery case durations;surgeries efficiently allowing;duration surgery surgery;using surgery records;estimate parameters surgery;surgeries challenging task;scheduling techniques estimating;surgeries efficiently;surgery records;heteroscedastic predictions optimally;duration surgery;duration scheduling;scheduling;effective scheduling;surgery surgery specific;surgeries challenging;scheduling strategies;surgery specific notion;additionally heteroscedastic predictions;nuanced effective scheduling;surgery surgery;booking investigate neural", "pdf_keywords": "predicting surgery duration;surgery duration neural;predicting surgery;scheduling surgeries challenging;duration neural heteroscedastic;2017 predicting surgery;scheduling surgeries;machine learning healthcare;surgery duration;surgery case durations;neural heteroscedastic regression;abstract scheduling surgeries;heteroscedastic predictions optimally;neural heteroscedastic;additionally heteroscedastic predictions;heteroscedastic predictions;surgeries challenging task;estimate parameters surgery;surgery records;using surgery records;duration neural;neural regression algorithms;booking investigate neural;booking minutes especially;scheduling;booking minutes;neural regression;department anesthesiology3;minutes scheduling;anesthesiology3 university california"}, "58777f0af009a225e315b7240db20ba545207702": {"ta_keywords": "computational complexity elections;complexity elections;complexity computational reasoning;complexity elections worst;complexity theory autonomous;computational reasoning uncertain;computational complexity;seeks insight complexity;complexity manip;complexity computational;insight complexity computational;complexity;uncertainty artificial intelligence;agents uncertainty;insight complexity;reasoning uncertain information;autonomous agents uncertainty;agents uncertainty artificial;complexity theory;computational reasoning;areas complexity;literature computational complexity;worst case complexity;uncertain information rich;case complexity manip;areas complexity theory;computational;uncertainty artificial;reasoning uncertain;complexity manip comsoc", "pdf_keywords": ""}, "309fb4d4d0946ac746f352c13cd3be4e2cd86dae": {"ta_keywords": "bayesian approaches acoustic;acoustic modeling speech;modeling speech recognition;approaches acoustic modeling;bayesian approaches widely;modeling speech;inference bayesian approaches;bayesian approaches;acoustic modeling;applications bayesian approaches;problems bayesian approaches;speech recognition;bayesian approaches involves;inference bayesian;applications bayesian;speech recognition problems;practical speech recognition;speech processing;bayesian;recognition related speech;recognition problems bayesian;problems bayesian;speech processing applications;focuses applications bayesian;acoustic modeling review;speech recognition related;related speech processing;approaches acoustic;statistics machine learning;approaches maximum likelihood", "pdf_keywords": ""}, "ce17dab00ddd2c86da508fc0502247f9b18a570f": {"ta_keywords": "winner approval voting;compute best vote;ballots agents rule;multi winner approval;voting satisfaction approval;voting satisfaction;approval ballots agents;approval voting satisfaction;prominent voting rules;proportional approval voting;ballots agents;voting rules;satisfaction approval voting;approval voting;agents compute best;reweighted approval voting;multiple winners rules;compute representative winning;winner approval;voting particular examine;winners rules proportional;approval voting particular;voting rules use;voting reweighted approval;voting particular;approval ballots;use approval ballots;ballots;agents computational;approval voting reweighted", "pdf_keywords": "voting np hard;agents compute best;approval voting np;complexity computing winner;compute best vote;ballots agents;ballots agents \ufb01rst;agents closed computational;computational complexity;voting np;approval ballots agents;complexity computing best;agents compute;studied computational complexity;computing winner pav;computational complexity computing;examine computational complexity;agents \ufb01rst computing;complexity computing;agent agents compute;proportional approval voting;complexity;np hard agent;closed computational complexity;computing winner;computing winner proportional;voting rules;approval voting;winner proportional approval;computational"}, "1d1bbed89882ac1001b915ee73199a919aa0d13c": {"ta_keywords": "language modelling conditioning;language model inductively;language obtain prior;language modelling;language model;sample training languages;neural language model;explore language modelling;languages use prior;training languages approximated;vocabulary language modelling;training languages;sample languages;open vocabulary language;language modelling construct;universal linguistic knowledge;neural language;construct neural language;languages;prior held languages;language obtain;supervision held languages;linguistic knowledge harness;human language obtain;learning human language;language specific information;explore language;universal linguistic;languages world features;diverse sample languages", "pdf_keywords": "learned prior language;prior language;prior language infer;training languages laplace;typologically diverse languages;universal linguistic knowledge;languages zero shot;generalizing new languages;universal linguistic;diverse training languages;prior typological features;universal prior typological;training languages;imbued universal linguistic;universal prior neural;language infer;characters learned prior;new languages;multilingual learning sample;language infer distribution;language modeling sample;multilingual learning;language modeling;languages;diverse languages;joint multilingual learning;learned prior;level language modeling;knowledge uninformative priors;linguistic knowledge uninformative"}, "7b7f8fb08262fce3da64c09788fd4b595408e4e6": {"ta_keywords": "based speech synthesis;speech synthesis;mudulation spectrum hmm;hmm based speech;spectrum hmm based;postfilter based mudulation;spectrum hmm;based speech;based mudulation spectrum;mudulation spectrum;hmm based;postfilter based;based mudulation;synthesis;speech;postfilter;mudulation;spectrum;based;hmm", "pdf_keywords": ""}, "d940e0192a6cc1ddd6288239b77b06e50f042114": {"ta_keywords": "pretrained speech representations;supervised pretraining speech;pretraining speech data;applications pretrained speech;end speech recognition;pretraining speech;pretrained speech;eral pretrained speech;self supervised pretraining;speech representations;speech recognition e2e;speech representations advanced;self supervised pretrained;supervised pretrained representations;speech data;speech data achieved;automatic speech recognition;automatic speech;speech recognition;pretrained representations end;supervised pretraining;speech signal learned;speech representations present;pretrained representations;representation speech signal;supervised pretrained;end automatic speech;speech recognition select;pretrained representations various;fidelity representation speech", "pdf_keywords": "pretrained speech representations;e2e speech processing;speech recognition e2e;applications pretrained speech;speech processing toolkit;pretrained speech;speech representations advanced;automatic speech;select pretrained speech;speech representations;speech recognition;automatic speech recognition;end automatic speech;e2e speech;speech representations present;recognition e2e asr;speech processing;pretrained sslrs achieve;used e2e speech;using pretrained sslrs;pretrained sslrs;pretrained representations wsj;e2e asr models;pretrained representations;asr e2e asr;e2e asr;corpora e2e asr;end asr e2e;asr systems using;asr models"}, "d5924c8cdef6270a955ba82c2b07a8282d869744": {"ta_keywords": "gesture predictions subword;language speech gestures;speech gestures;gesture predictions;speech gestures context;language freeform gestures;gesture generation;second gesture predictions;distributions text gestures;text gestures inherently;freeform gestures substantiate;approaches gesture generation;gestures left learning;text gestures;gestures inherently;gestures;freeform gestures;multimodal multiscale attention;gestures substantiate;gesture;gestures context;language acoustic cues;alignment language acoustic;approaches gesture;multiscale attention;gestures substantiate effectiveness;spoken language freeform;gestures left;gestures inherently skewed;second gesture", "pdf_keywords": ""}, "b82e9b84cb639f6bb061c8a43b97986ecfec00ea": {"ta_keywords": "queries fast similarity;similarity queries fast;fast similarity queries;dimensional representation entities;similarity queries semi;similarity queries;large partite graphs;pic embeddings;proposed pic embeddings;embeddings represent large;datasets web approach;datasets web;representation entities;partite graphs;pic embeddings represent;embeddings;structured data web;partite graphs using;embeddings represent;queries semi structured;low dimensional representation;entities different datasets;enabling fast similarity;fast similarity;queries fast;representation entities different;semi structured data;low dimensional;different datasets web;entities", "pdf_keywords": ""}, "ee33d61522fd70fa4e6470decbdac6c17f8b4fdb": {"ta_keywords": "attractors speech embedding;neural speaker diarization;attractors multiplied speech;speaker diarization;attractors speech;end speaker diarization;speaker diarization unknown;speech embedding;number attractors speech;multiplied speech embedding;speaker diarization sa;der speech embedding;speech embedding sequence;end neural speaker;decoder based attractor;decoder based attractors;speakers encoder;speaker subset callhome;neural speaker;speakers encoder decoder;number speakers encoder;number speaker activities;speaker subset;speaker activities unknown;speaker activities;der speaker subset;based attractor calculation;attractor calculation;speaker condition method;produce number speaker", "pdf_keywords": "attractors speech embedding;attractors multiplied speech;speech embedding sequence;neural speaker diarization;speech embedding;end neural speaker;attractors speech;end speaker diarization;number attractors speech;diarization speech mixtures;multiplied speech embedding;attractors sequence embeddings;eda speech embedding;speaker diarization;decoder based attractor;speaker diarization speech;neural speaker;speech mixtures;speaker diarization sa;calculate attractors sequence;diarization speech;speech mixtures \ufb02exible;end speaker;attractors sequence;end end speaker;calculate attractors;\ufb02exible number speakers;number speaker activities;eda calculate attractors;generated multiple attractors"}, "d6d2003d112e3d9d93edd4920436fe2fe879eb87": {"ta_keywords": "clustering big text;clustering big;big text datasets;effective clustering;effective clustering methods;elegant effective clustering;fast method clustering;method clustering big;clustering methods;clustering;text datasets;text datasets long;wise similarities data;similarities data;similarity matrix;scale text datasets;large scale text;clustering methods exploits;method clustering;similarities data points;datasets long eluded;similarity;big text;similarity matrix state;operating similarity matrix;datasets long;wise operating similarity;pair wise similarities;operating similarity;similarities", "pdf_keywords": ""}, "3b8b6f27a5df5dc2c231d0fa1e471887e4583466": {"ta_keywords": "curated citation networks;citation networks curated;metadata citation networks;knowing genes author;citation networks improve;citation networks;citations help predict;curated citation;identifying genes academic;using curated citation;related metadata citation;coauthors citations;genes author likely;genes author previously;genes author;coauthors citations help;metadata citation;new genes author;link prediction using;link prediction;genes academic biomedical;academic biomedical publications;genes academic;citations;based link prediction;extraction link prediction;link prediction technique;networks curated databases;networks curated;networks improve gene", "pdf_keywords": ""}, "4bfc185dcc67b3eddfa059fc5446f4df844a0728": {"ta_keywords": "\u56fd\u4f1a\u97f3\u58f0\u8a8d\u8b58\u30b7\u30b9\u30c6\u30e0\u306e\u97f3\u97ff \u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u534a\u81ea\u52d5\u66f4\u65b0 \u8a00\u8a9e\u7406\u89e3\u3068\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3;\u56fd\u4f1a\u97f3\u58f0\u8a8d\u8b58\u30b7\u30b9\u30c6\u30e0\u306e\u97f3\u97ff \u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u534a\u81ea\u52d5\u66f4\u65b0;\u56fd\u4f1a\u97f3\u58f0\u8a8d\u8b58\u30b7\u30b9\u30c6\u30e0\u306e\u97f3\u97ff;\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u534a\u81ea\u52d5\u66f4\u65b0 \u8a00\u8a9e\u7406\u89e3\u3068\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3;\u8a00\u8a9e\u7406\u89e3\u3068\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3;\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u534a\u81ea\u52d5\u66f4\u65b0", "pdf_keywords": ""}, "9635e3c008f7bfa80638ade7134a8fb0ef1b37e1": {"ta_keywords": "language model marginal;language model training;pretrained language model;language models;language models evaluated;model best tokenisation;language model best;language model performance;instead language models;samples evaluate language;language model;evaluation language model;evaluate language model;marginal likelihood tokenisations;likelihood tokenisations;best tokenisation marginal;tokenisation robustness suggest;likelihood tokenisations discuss;evaluate pretrained language;regard tokenisation robustness;tokenisation robustness;tokenisation marginal perplexities;best tokenisation;results language model;pretrained language;particularly regard tokenisation;tokenisation marginal;evaluate language;likelihood tokenisations link;tokeniser uncertainty", "pdf_keywords": "predictive poorly tokeniser;language models besttokenisation;tokenising word types;tokenising word;tokens evaluate language;neural language models;loss language models;single tokenisation estimate;suggesting tokeniser entropy;tokenisation estimate;models results tokenising;likelihood tokenisations;tokenising;samples single tokenisation;tokenisations;models typically tokenise;language model marginal;single tokenisation;compared independently tokenising;tokenisation estimate marginal;tokenisation;marginal likelihood tokenisations;suggesting tokeniser;independently tokenising;tokeniser entropy especially;language models results;poorly tokeniser;multiple tokenisations;tokenised;tokeniser entropy"}, "3f2821dd40c12da560c89b9dad7f95cd4ad9354f": {"ta_keywords": "storage clusters imposes;overwhelms storage infrastructure;storage clusters;roadblock diskadaptive redundancy;scale storage clusters;diskadaptive redundancy transition;storage clusters disk;clusters disk adaptive;clusters io load;disks production clusters;storage infrastructure;large scale storage;disk adaptive redundancy;diskadaptive redundancy;attacks storage clusters;overwhelms storage;roadblock diskadaptive;overload roadblock diskadaptive;clusters transition io;clusters io;clusters disk;needing cluster io;redundancy transition io;cluster io;cluster io continuously;traces millions disks;storage infrastructure termed;millions disks production;world clusters io;100 cluster io", "pdf_keywords": "disks production clusters;pacemaker hadoop distributed;pacemaker hdfs implemented;implementation pacemaker hdfs;pacemaker hdfs evaluation;clusters transition io;roadblock diskadaptive redundancy;pacemaker hdfs;hadoop distributed \ufb01le;diskadaptive redundancy transition;hadoop distributed;needing cluster io;distributed \ufb01le hdfs;roadblock diskadaptive;hdfs implemented;pacemaker hadoop;prototype pacemaker hadoop;using cluster io;cluster io;hdfs evaluation;overload roadblock diskadaptive;disks production systems;integration pacemaker hdfs;hdfs;analyzes disk deployment;millions disks production;scale production clusters;cluster io continuously;diskadaptive redundancy;distributed storage"}, "ff6ddbd7ba59e0fd4a74748942083391d6e9a666": {"ta_keywords": "end information extraction;opera performs endto;information extraction;isi opera;usc isi opera;information extraction multiple;isi opera performs;extracted information;endto end information;extracted information performs;opera;end information;opera performs;containing extracted information;knowledge bases;knowledge bases containing;produces knowledge bases;extraction multiple media;performs endto;results english;information performs;performs endto end;endto end;endto;extraction multiple;ukrainian produces knowledge;extraction;results english russian;information;media integrates results", "pdf_keywords": ""}, "9fe09ca520cb7ce106e65b39c455777d18ec6efe": {"ta_keywords": "textual taxonomies predicting;taxonomy nodes arborist;curated taxonomies enhance;curated taxonomies;nodes arborist learns;evaluation curated taxonomy;curated taxonomy;curated taxonomy pinterest;arborist learns latent;taxonomies predicting;15 curated taxonomies;embeddings taxonomy nodes;textual taxonomies;taxonomies enhance performance;infer nodes taxonomic;datasets demonstrate arborist;expand textual taxonomies;recall 15 curated;taxonomies predicting parents;extensive evaluation curated;nodes measure taxonomic;arborist learns;edge semantics embeddings;nodes taxonomic;taxonomy nodes;taxonomies enhance;nodes taxonomic roles;semantics embeddings taxonomy;embeddings taxonomy;taxonomic relatedness node", "pdf_keywords": ""}, "ef4166a7fb2c40ca87b0ebb253e8ba1e80c09fd7": {"ta_keywords": "chime speech separation;speech separation recognition;speech separation;discriminative feature transformation;feature transformation reverberated;augmented discriminative feature;asr techniques discriminative;training feature transformation;augmented feature transformation;discriminative training feature;reverberated noisy speech;feature transforms;separation recognition challenge;techniques discriminative training;discriminative feature;chime speech;feature transforms highly;features discriminative feature;features discriminative;discriminative training;speech home noises;separation recognition;propose augmented discriminative;discriminative training various;feature transformation;reverberation;feature transformation provides;various feature transformation;discriminative training model;arbitrary features discriminative", "pdf_keywords": ""}, "5db0fa82c322bb7d9f60109294d088ff139eebf3": {"ta_keywords": "denoising images generative;manifold learned gan;gan manifold recovering;networks gans transform;gans transform low;images generative adversarial;learned gan;networks gans;nearest point gan;gans transform;gan manifold;generative adversarial networks;projection denoising images;generative adversarial;adversarial networks gans;denoising images;denoising quality best;denoising quality;vector denoise image;learned gan contain;denoise image;latent vector denoise;point gan manifold;images generative;gans;gan manifold approximately;vector denoise;projection denoising;denoise corrupted images;correction projection denoising", "pdf_keywords": "gans denoising deblurring;gans denoising;vector recovery gan;use gans denoising;latent vectors dcgan;denoising images generative;gan signi\ufb01cantly denoise;recovery gan;gans transform lowdimensional;vectors dcgan;trained gan;generated trained gan;networks gans transform;networks gans;gans transform;vectors dcgan used;generative adversarial networks;images generative adversarial;recovery gan practice;use gans;generative adversarial;gans;adversarial networks gans;dcgan used image;adversarial networks subarna;trained gan signi\ufb01cantly;focus use gans;images generative;gan;denoise deblur images"}, "1d05e91b6d94f06439b2b41291a8dcc3d8064149": {"ta_keywords": "approach color clustering;color clustering;color clustering using;clustering using kernel;clustering methods image;kernel means approach;image grid regularization;wise feature clustering;feature clustering methods;grid regularization;approach segmentation benefits;using kernel means;bandwidth kernel means;feature clustering;formulation kernel means;means approach segmentation;kernel means energy;segmentation benefits;grid regularization using;segmentation;clustering methods;kernel bandwidth kernel;fitting techniques segmentation;bound formulation kernel;segmentation benefits higher;clustering;techniques segmentation;color model fitting;kernel means;bandwidth kernel", "pdf_keywords": ""}, "da1f22dd6d834e031eb733d2b70320f34ef9458f": {"ta_keywords": "pairwise comparisons arises;pairwise comparisons;models pairwise comparison;peer grading bounds;comparisons sharp minimax;pairwise comparison data;competitions peer grading;estimation pairwise comparisons;comparison graph induced;comparison graph;pairwise comparison;ordinal models pairwise;pairwise comparisons sharp;compared class models;parametric ordinal models;quality score;estimating quality score;comparisons;comparison data;grading bounds depend;grading bounds;quality score vector;peer grading;form pairwise comparisons;laplacian comparison graph;comparisons arises domains;compared class;ordinal models;competitions peer;topology comparison graph", "pdf_keywords": "comparisons inference ranking;pairwise comparisons arises;inference ranking topology;models pairwise comparison;pairwise comparisons inference;ranking topology;ordinal models pairwise;estimation pairwise comparisons;pairwise comparisons;keywords pairwise comparisons;inference ranking;comparisons sharp minimax;ranking;parametric ordinal models;ranking topology crowdsourcing;pairwise comparison data;comparisons guiding choice;pairwise comparison;pairwise comparisons sharp;competitions peer grading;selection comparisons;compared class models;comparisons guiding;guiding selection comparisons;comparisons arises domains;ordinal models;comparisons inference;models pairwise;selection comparisons guiding;sporting competitions peer"}, "b07b124852f897823490db0a04ea6e411bb77f00": {"ta_keywords": "classifiers maximize f1;multilabel classification optimal;multilabel classification special;thresholding classifiers maximize;optimal thresholding classifiers;multilabel classification;classification optimal thresholding;classifiers maximize;classification optimal;used multilabel classification;thresholding classifiers;binary classifier;classification special;success binary classifier;binary classification;probabilities optimal threshold;classifier;classification special case;mean precision recall;classifiers;context multilabel classification;classification context multilabel;binary classification context;classification;maximizing f1 measures;measures used multilabel;optimal threshold;binary classifier class;optimal thresholding;classifier outputs", "pdf_keywords": ""}, "b6502b61bf8f0332c6caa30198cff3619a9790aa": {"ta_keywords": "performance redundant requests;latency performance redundant;redundancy request optimal;sending redundant requests;costs redundant requests;redundant requests;redundant requests reduce;optimal redundant requesting;redundant requests primary;having redundancy request;redundant requests help;redundancy request;redundant requests diverse;performance redundant;latency performance;distributed storage;empirically evaluate latency;designing optimal redundant;redundant requesting;latency mechanism;replicas data serve;evaluate latency performance;instance distributed storage;latency mechanism trades;request increase delay;service times memoryless;optimal redundant;reduce average latency;average latency mechanism;sending redundant", "pdf_keywords": "consider centralized queueing;centralized queueing;distributed storage;instance distributed storage;distributed storage storing;centralized queueing section;replicas data serve;queueing;redundant requests;queueing section requests;storing multiple replicas;cluster multiple processors;replicas data;multiple replicas;multiple replicas data;servers store;multiple servers store;servers store requested;redundant requests help;queueing section;distributed;request multiple servers;\ufb02exibility serve requests;servers;cluster;serve requests;storage storing multiple;storage;performed compute cluster;multiple servers"}, "b72c5236dacf2b958ebcf427d17a100bc54af504": {"ta_keywords": "mandarin speech recognition;networks rnns end;networks rnns;speech recognition datasets;neural networks rnns;e2e automatic speech;speech recognition asr;speech recognition;asr contextual block;attention network;automatic speech recognition;recurrent neural networks;self attention network;transformer asr contextual;contextual block processing;rnns;automatic speech;encoder introducing context;performance alternative recurrent;alternative recurrent neural;proposed contextual block;contextual block;encode local acoustic;encoder introducing;global linguistic channel;transformer self attention;block helps encode;rnns end end;recognition asr systems;acoustic information global", "pdf_keywords": "transformer asr contextual;speech recognition asr;speech recognition datasets;mandarin speech recognition;networks rnns;speech recognition;networks rnns end;transformer encoder introducing;neural networks rnns;e2e automatic speech;asr contextual block;encoder introducing context;performance alternative recurrent;transformer self attention;recurrent neural networks;alternative recurrent neural;transformer encoder;encoder introducing;automatic speech recognition;automatic speech;recognition asr systems;attention network;contextual block processing;context inheritance train;recognition asr;encoder;method transformer encoder;rnns;self attention network;proposed contextual block"}, "6dfc2ff03534a4325d06c6f88c3144831996629b": {"ta_keywords": "visual commonsense reasoning;cognition commonsense reasoning;task visual commonsense;recognition cognition networks;reasoning engine recognition;commonsense reasoning glance;commonsense reasoning world;cognition commonsense;commonsense reasoning;visual commonsense;inferences grounding contextualization;engine recognition cognition;layered inferences grounding;grounding contextualization reasoning;new reasoning engine;recognition cognition;cognition networks r2c;layered inferences;contextualization reasoning;image effortlessly imagine;necessary layered inferences;challenging question image;reasoning engine;reasoning glance image;cognition networks;question image machine;reasoning world;image effortlessly;cognition level understanding;grounding contextualization", "pdf_keywords": "recognition cognition networks;recognition cognitionlevel image;recognition level understanding;cognitionlevel image understanding;reasoning engine recognition;recognition cognitionlevel;recognition cognition;scenes cognition level;reasoning present adversarial;engine recognition cognition;movie scenes cognition;scenes cognition;recognition level;model recognition cognition;cognition networks r2c;cognition level understanding;cognitionlevel image;image understanding requires;layered inferences grounding;image understanding;cognition networks;inferences grounding contextualization;cognitive commonsense understanding;recognition;grounding contextualization reasoning;gap recognition cognitionlevel;new reasoning engine;challenge computer vision;present adversarial matching;cognitive commonsense"}, "df43f6ff7c66d39240235af3052be55222bef80d": {"ta_keywords": "mixture models;mixture mixture models;method speaker clustering;mixture distributions gibbs;mixture mixture distributions;nested gibbs sampling;speaker clustering;mixture distributions;sampling method mixture;novel gibbs sampling;sampling develop mixture;distributions gibbs sampling;gibbs sampling;mixture distributions higher;mixture mixture model;gibbs sampling develop;structure based mixture;mixture model represent;gibbs sampling possible;mixture model;sampling based variational;components mixture model;speaker clustering problem;mixture model deterministic;gibbs sampling method;based mixture mixture;model components mixture;elemental mixture distributions;method mixture mixture;develop mixture mixture", "pdf_keywords": ""}, "964293c1cb1b619eb9b474381d2ba60cf44fcc2d": {"ta_keywords": "mapping distribution facility;using facility map;facility map database;facility map;develop mapping distribution;facilities development mapping;maps managing distribution;distribution facility management;company tepc maps;design distribution facility;distribution facility construction;mapping distribution line;managing distribution facilities;mapping described tokyo;tepc maps managing;maps design;maps managing;mapping distribution;distribution facilities essential;distribution line maps;distribution facility;develop mapping;maps tepc;distribution facilities;line maps tepc;tepc maps;maps design jobs;jobs using mapping;maps tepc making;using mapping", "pdf_keywords": ""}, "d389d8c2e15f9e9269c17fe6f960f70559eee840": {"ta_keywords": "parsimonious morpheme segmentation;unsupervised morpheme segmentation;morpheme segmentation parsimonious;morpheme segmentation;morphemes enrich word;morphemes enrich;morphmine morphemes enrich;unsupervised morpheme;segmentation parsimonious morpheme;morpheme segmentation application;utilizing morphmine morphemes;morphemes level;embeddings infrequent words;morphmine unsupervised morpheme;morphmine segments words;morphemes;morphmine morphemes;morphemes level hierarchy;enrich word embeddings;morphemes morphmine applies;morphemes morphmine;verified morphemes morphmine;hierarchically segment words;enriching word embeddings;human verified morphemes;word embeddings consistently;word embeddings experiments;morpheme;word embeddings;word embedding", "pdf_keywords": "morpheme segmentation entropy;unsupervised morpheme segmentation;framing morpheme segmentation;morphemes improves language;morpheme segmentation finally;morpheme segmentation;parsimonious morpheme segmentation;morphmine morphemes enrich;utilizing morphmine morphemes;morphemes improves;morphemes enrich word;morphemes enrich;high quality morphemes;morpheme segmentation application;unsupervised morpheme;word morphemes;segment word morphemes;quality morphemes improves;morphemes;morphmine unsupervised morpheme;morphmine morphemes;quality morphemes;framing morpheme;quality candidate morpheme;enrich word embeddings;enriching word embeddings;candidate morpheme set;morphemes paper propose;morpheme;methodology framing morpheme"}, "3d1318bc66d534eefac7c665fd7cc891fba27b87": {"ta_keywords": "\u5c0f\u7279\u96c6 2020\u5e74 \u8a00\u8449\u306e\u58c1\u3092\u8d85\u3048\u308b\u97f3\u58f0\u7ffb\u8a33;2020\u5e74 \u8a00\u8449\u306e\u58c1\u3092\u8d85\u3048\u308b\u97f3\u58f0\u7ffb\u8a33 \u65b0\u3057\u3044\u6280\u8853\u3068\u7814\u7a76\u306e\u53ef\u80fd\u6027;2020\u5e74 \u8a00\u8449\u306e\u58c1\u3092\u8d85\u3048\u308b\u97f3\u58f0\u7ffb\u8a33;\u6a5f\u68b0\u7ffb\u8a33\u6280\u8853 \u5c0f\u7279\u96c6 2020\u5e74;\u8a00\u8449\u306e\u58c1\u3092\u8d85\u3048\u308b\u97f3\u58f0\u7ffb\u8a33 \u65b0\u3057\u3044\u6280\u8853\u3068\u7814\u7a76\u306e\u53ef\u80fd\u6027;\u5c0f\u7279\u96c6 2020\u5e74;\u65b0\u3057\u3044\u6280\u8853\u3068\u7814\u7a76\u306e\u53ef\u80fd\u6027;\u8a00\u8449\u306e\u58c1\u3092\u8d85\u3048\u308b\u97f3\u58f0\u7ffb\u8a33;\u6a5f\u68b0\u7ffb\u8a33\u6280\u8853 \u5c0f\u7279\u96c6;\u5c0f\u7279\u96c6;\u6a5f\u68b0\u7ffb\u8a33\u6280\u8853;2020\u5e74", "pdf_keywords": ""}, "33972d9e9a102f9388e5850d8aed3d1aefc9d2e5": {"ta_keywords": "argumentation meets games;argumentation quality;argumentation quality investigated;argumentation scholars nlp;dealing fallacious argumentation;fallacious argumentation calls;fallacious argumentation;argumentation scholars;researchers focus argumentation;computational argumentation meets;argumentation meets;computational argumentation;argumentation ability;argumentation calls;focus argumentation quality;argumentative discourse deceptive;critical thinking argumentation;argumentation;thinking argumentation;importance argumentation scholars;argumentative discourse;argumentation ability spot;thinking argumentation ability;fallacious arguments omnipresent;argotario computational argumentation;focus argumentation;fallacious arguments;argumentative;fallacies fallacious arguments;arguments omnipresent argumentative", "pdf_keywords": "argumentation meets games;argotario computational argumentation;computational argumentation meets;argumentation ability;computational argumentation;fallacious argumentation calls;dealing fallacious argumentation;argumentation theory research;fallacious argumentation;argumentation meets;argumentation ability spot;argumentation theory;argumentation calls;focuses fallacies argumentative;thinking argumentation ability;fallacies argumentative;argumentation;critical thinking argumentation;fallacies argumentative discourse;criteria computational argumentation;thinking argumentation;topic argumentation;argumentation calls scalable;topic argumentation theory;fallacy recognition;fallacy recognition task;gami\ufb01cation fallacy recognition;annotation games;annotation games methodology;resources dealing fallacious"}, "04138da3bac26f83a9d57152118d4cd5cc8c717d": {"ta_keywords": "inter entity similarity;measure similarity entities;entity similarity;similarity entities directly;similarity entities;walk based similarity;entity similarity learning;based similarity measure;walk based search;similarity measure;similarity learning techniques;similarity learning;queries using adaptive;measure similarity;similarity measure thesis;graph walk based;based similarity;elicit measure similarity;similarity;adaptive graph walk;queries possible correspond;persons related email;person disambiguation addressed;arbitrary queries;graphs heterogeneous sense;threading person disambiguation;graph walks;disambiguation addressed;like alias finding;disambiguation addressed uniformly", "pdf_keywords": ""}, "395aae6e7a79e5760457ca38e868acc970016230": {"ta_keywords": "table reasoning datasets;relational tables web;sparse attention transformer;uses sparse attention;sparse attention way;20 relational tables;tables mate;relational tables;table architecture;attention transformer architecture;web tables;tables web;sparse attention;table reasoning;structure web tables;2008 large tables;tables;large tables;contain large tables;reasoning datasets;large tables mate;tables web 20;tables mate uses;attention transformer;columns table architecture;tables present challenge;large tables present;web tables work;presents sparse attention;accelerators 20 relational", "pdf_keywords": "reasoning tabular textual;reasoning tabular;tabular textual;structure web tables;tabfact sqa;tabular textual data;tables;tabfact sqa hybridqa;web tables;tabular data improve;tabular;hop reasoning tabular;large scale qa;state art tabfact;training inference longer;table structure;table structure allow;table;qa tasks like;tabfact;web tables ii;tables ii introduce;structure web;textual data;inference longer sequences;qa tasks;tabular data;tables ii;scale qa tasks;model structure web"}, "e07c2e66dab7b61091bb8a4ad132bf279c233027": {"ta_keywords": "citations topic modeling;modeling text citations;citation prediction;citation prediction task;best citation prediction;topic modeling;similarity contents cited;topic modeling framework;citing documents pairwise;citeseer data models;cited citing documents;text citations;topical similarity contents;link prediction;link prediction task;citations;topical similarity;text citations topic;citations topic;citing documents;citation link pair;cited citing;notion topical similarity;contents cited citing;contents cited;citing;subset citeseer data;joint modeling text;citation link;cited", "pdf_keywords": ""}, "aeb4478619461ca25592e6d692f3591ec8c4091b": {"ta_keywords": "generating semantic collisions;generate semantic collisions;semantic collisions texts;study semantic collisions;semantic collisions;summarization vulnerable semantic;collisions texts semantically;texts semantically unrelated;semantic collisions evade;including paraphrase identification;vulnerable semantic collisions;approaches generating semantic;similar nlp models;generating semantic;semantic collisions demonstrate;semantic collisions example;generate semantic;similar nlp;paraphrase identification;analyzing meaning similarity;collisions texts;extractive summarization vulnerable;judged similar nlp;nlp models generate;summarization vulnerable;texts including paraphrase;nlp models;models generate semantic;texts semantically;including paraphrase", "pdf_keywords": "vulnerabilities nlp models;vulnerabilities nlp;vulnerabilities nlp applications;class vulnerabilities nlp;models paraphrase identi\ufb01cation;document sentence retrieval;nlp models;nlp models analyzing;nlp applications semantic;nlp applications;analyzing meaning similarity;paraphrase identi\ufb01cation document;applications semantic collisions;semantically similar;semantic collisions;semantic collisions input;generate semantic collision;sentence retrieval;sentence retrieval extractive;nlp;models paraphrase;semantic collision;similarity texts;paraphrase identi\ufb01cation;semantic collision unrelated;meaning similarity texts;text judged semantically;unrelated text judged;generate semantic;retrieval extractive summarization"}, "3a3fb140890dbba93290e358af700f9a5c8bcc7a": {"ta_keywords": "question answering;knowledge intensive ai;questions gram machines;contains millions sentences;answer questions gram;deep qa models;domain question answering;large corpus;answering qa learning;millions sentences;scales large corpus;question answering qa;tasks deep neural;gram machines;intensive ai tasks;deep qa;end deep qa;tasks deep;limited knowledge intensive;knowledge intensive;deep neural;millions sentences apply;gram machines prohibitive;natural language processing;representative tasks deep;question complexity responding;ai tasks open;gram machine;organize knowledge answer;gram machine ngm", "pdf_keywords": ""}, "2f780a18d44f4e3c5c4c74d4060b8dfd542a778d": {"ta_keywords": "sports commentator bias;racial bias football;investigating sports commentator;commentator bias;commentator bias large;bias football;examining racial bias;racial bias;sports broadcasters;transcripts american football;football broadcasts sports;broadcasts sports;american football broadcasts;broadcasts sports broadcasters;investigating sports;bias football perform;sports commentator;player narratives subjective;football broadcasts;sports broadcasters inject;broadcast transcripts american;team player narratives;broadcasters inject drama;subjective analyses anecdotes;play commentary;bias large corpus;broadcast transcripts;bias;researchers examining racial;player narratives", "pdf_keywords": "racial bias sports;bias sports commentary;racial bias football;sports commentator bias;investigating sports commentator;bias sports;analysis racial bias;commentator bias;commentator bias large;bias football;examining racial bias;studies commentator sentiment;abstract sports broadcasters;racial bias;sports commentary;bias football perform;transcripts american football;investigating sports;sports broadcasters;sports commentary major;american football broadcasts;commentator sentiment;sports broadcasters inject;sports commentator;football broadcasts;analysis racial;commentator sentiment naming;player narratives subjective;broadcasts nfl national;game broadcasts"}, "34fc6da7a88433478fd976fd0b9de3cf7134e652": {"ta_keywords": "commucation training multimodal;automated commucation training;training multimodal information;training multimodal;commucation training;automated commucation;multimodal information;multimodal;commucation;training;automated;information", "pdf_keywords": ""}, "df1d89f4ca9c20e2c6703cdbf26a62f2b50ac71c": {"ta_keywords": "dynamics stackelberg games;stackelberg games continuous;dynamics stackelberg equilibria;guaranteed stackelberg equilibrium;nash stackelberg equilibrium;stackelberg equilibria;descent stackelberg equilibria;stackelberg games;stackelberg equilibrium concepts;learning dynamics stackelberg;stackelberg equilibrium;stackelberg equilibria zero;stackelberg equilibrium zero;games dynamics converge;stackelberg gradient dynamics;gradient dynamics stackelberg;dynamics stackelberg;sum games dynamics;learning dynamics stable;point guaranteed stackelberg;gradient descent stackelberg;critical points stackelberg;learning dynamics;hierarchical order play;games dynamics;connections nash stackelberg;convergence learning dynamics;guaranteed stackelberg;zero sum games;games continuous action", "pdf_keywords": ""}, "02980e5ba847282b683a85e7a8862c6c1b6e0d94": {"ta_keywords": "phenyl amine aqueous;aqueous organic solvent;organic solvent;amine aqueous organic;solute solvent interaction;methyl phenyl amine;solvent interaction di;solute solvent;solvent interaction;phenyl amine;study solute solvent;solvent;amine aqueous;di methyl phenyl;interaction di methyl;methyl phenyl;di methyl;aqueous organic;amine;solute;phenyl;study solute;methyl;interaction di;aqueous;organic;di;interaction;study", "pdf_keywords": ""}, "73aa33fd469b171d50c452c5e3fe0e9e03520520": {"ta_keywords": "english asr systems;speech text stt;asr track evaluation;speech text;decoding stages;decoding stages systems;asr systems;asr systems iwslt;english speech text;text stt systems;iwslt ted asr;decoding;ted asr track;combined confusion network;english asr;asr track;confusion network;track evaluation outputs;ted asr;evaluation outputs subsystems;naist english asr;systems iwslt evaluation;evaluation outputs;confusion network combination;asr;describes english speech;text stt;2012 iwslt ted;iwslt evaluation;stt systems 2012", "pdf_keywords": ""}, "c330ec2047d019d98233abb59d13b3256c662cc7": {"ta_keywords": "bounding counterfactual queries;arbitrary structural causal;counterfactual distributions arbitrary;counterfactual queries arbitrary;causal diagram unobserved;counterfactuals polynomial programming;bounding counterfactuals;bounds counterfactual query;problem bounding counterfactuals;counterfactual distributions;counterfactual queries;counterfactual query paper;structural causal;bounding counterfactual;causal model scm;causal model;optimal bounds counterfactual;structural causal model;problem bounding counterfactual;causal;counterfactual query;scms causal;causal diagram develop;causal diagram;bounds counterfactual;scms causal diagram;family scms causal;bounding counterfactuals polynomial;counterfactuals;arbitrary combination observational", "pdf_keywords": "counterfactual distributions arbitrary;identi\ufb01cation counterfactual distributions;arbitrary structural causal;target counterfactual probabilities;bounding counterfactual probabilities;counterfactual probabilities arbitrary;counterfactual distributions;represent counterfactual distributions;counterfactual distributions concerns;bounds target counterfactual;counterfactual distributions \ufb01nite;causal diagram unobserved;experiments counterfactual distributions;counterfactual probabilities;bounding counterfactual;concerns bounding counterfactual;structural causal;data provided causal;target counterfactual;counterfactual probabilities \ufb01nite;structural causal model;observed variables causal;partial identi\ufb01cation counterfactual;causal model scm;causal model;provided causal;identi\ufb01cation counterfactual;causal;observations experiments counterfactual;causal diagram encoding"}, "0a3caecce668731efe7abf37720793eed1fb951a": {"ta_keywords": "twitter user political;politically oriented tweets;classifier twitter user;classifier twitter;tweets accurate classifier;accurate classifier twitter;classifier politically oriented;classifier politically;political affiliation sender;information emotion social;message sentiment associated;political information;oriented tweets;motivated reasoners information;message sentiment;oriented tweets accurate;receiving political information;twitter;tweets;community information sharing;receiver message sentiment;political information important;user political affiliation;twitter user;sharing influenced political;sentiment associated;tweets accurate;reasoners information;sentiment associated message;information sharing influenced", "pdf_keywords": ""}, "5bca90a331417402f5018f552e1a62656dd7fc5b": {"ta_keywords": "labeled graph observation;communities features labeled;edges observed graph;recovering communities features;features labeled graph;observed graph;observed graph generated;graph observation;symmetric stochastic block;labeled graph;corresponding community feature;stochastic block;stochastic block model;communities features;recovering communities;assume edges observed;problem recovering communities;community feature;edges observed;graph generated symmetric;symmetric stochastic;community feature characterize;generated symmetric stochastic;communities;information theoretic;graph generated;partially observed version;corresponding community;graph;assume edges", "pdf_keywords": ""}, "5166c0e04d77ac0f7969c49c0f8f18129a114198": {"ta_keywords": "clinical gait analysis;measurement clinical gait;gait analysis;clinical gait;motion measurement clinical;automated motion measurement;motion measurement;gait;automated motion;measurement clinical;motion;measurement;automated;clinical;analysis", "pdf_keywords": ""}, "205ff5dae21ca44c15d3b7d7a9febb7d84b47bc4": {"ta_keywords": "massively multilingual models;multilingual seed models;neural machine translation;multilingual models;multilingual models explicit;starting massively multilingual;massively multilingual seed;machine translation systems;similar language regularization;translation systems;massively multilingual;demonstrate massively multilingual;language regularization jointly;translation systems new;language regularization;multilingual seed;machine translation;high resourced language;language regularization method;languages lrls effectively;low resourced languages;adapting neural machine;multilingual;languages lrls;resourced languages lrls;language prevent fitting;adapting neural;similar language;regularization jointly train;resourced language", "pdf_keywords": "languages multilingual modeling;neural machine translation;multilingual modeling;machine translation new;translation new languages;multilingual modeling methods;similar language regularization;language regularizing similar;regularizing similar languages;language regularization slr;translation systems new;language regularization;language regularizing;multilingual training single;multilingual training;translation systems;resource language regularizing;low resourced languages;machine translation systems;machine translation;similar languages multilingual;regularization slr training;low resource language;second similar languages;similar languages;adapting neural machine;languages lrls effectively;new languages;new source language;translation new"}, "488b1849dd81e63aae2cd327564077ae123c0369": {"ta_keywords": "sgd absolute compression;absolute compression operators;methods absolute compression;absolute compression arbitrary;compression operators;methods biased compression;compression error compensation;compression arbitrary sampling;class absolute compression;lsvrg absolute compression;communication compression powerful;absolute compression strongly;communication compression;compression strongly convex;compressors ec sgd;absolute compression;error compensated sgd;compression powerful approach;biased compression;optimal compressors ec;compression arbitrary;absolute compression error;compression strongly;problems communication compression;biased compression error;compression operators showing;compensated sgd ec;optimal compressors;compression;ec sgd absolute", "pdf_keywords": "sgd absolute compression;distributed optimization methods;absolute compression operators;methods absolute compression;distributed optimization;compression strongly convex;abstract distributed optimization;methods biased compression;compression operators;class absolute compression;absolute compression arbitrary;communication compression powerful;absolute compression strongly;communication compression;absolute compression;compression powerful approach;optimal compressors ec;biased compression;compression arbitrary sampling;lsvrg absolute compression;compressors ec sgd;optimal compressors;absolute compression error;compression strongly;compression error compensation;compression arbitrary;error compensated sgd;distributed methods absolute;contains optimal compressors;biased compression error"}, "4cd92a56dca741190e453b4229eb9851abf6944c": {"ta_keywords": "clipping stochastic gradients;accelerated stochastic gradient;convex stochastic optimization;stochastic gradient descent;noise stochastic optimization;stochastic optimization heavy;sgd clipping stochastic;new accelerated stochastic;stochastic gradients extend;noise stochastic gradients;stochastic optimization;variant accelerated stochastic;accelerated stochastic;stochastic gradients;stochastic gradient;theory stochastic optimization;smooth convex stochastic;clipping stochastic;convex stochastic;noise accelerated gradient;accelerated stochastic order;stochastic gradients derive;gradient descent sgd;method strongly convex;distributed noise stochastic;descent sgd clipping;tailed noise stochastic;sstm smooth convex;noise stochastic;accelerated gradient clipping", "pdf_keywords": "convex stochastic optimization;stochastic optimization heavy;noise stochastic gradients;theory stochastic optimization;new accelerated stochastic;stochastic optimization;accelerated stochastic \ufb01rst;accelerated stochastic;noise accelerated gradient;stochastic gradients;convex stochastic;smooth convex stochastic;stochastic gradients derive;distributed noise stochastic;tailed noise accelerated;noise stochastic;optimization heavy tailed;tailed distributed noise;noise accelerated;sstm smooth convex;stochastic \ufb01rst order;stochastic \ufb01rst;theory stochastic;accelerated gradient clipping;stochastic;distributed noise;optimization heavy;accelerated gradient;clipped sstm smooth;tailed noise"}, "40848b41ed8c9c255ecd8a920006877691b52d03": {"ta_keywords": "challenges datasets shifts;benchmark wild distribution;wild distribution shifts;datasets shifts;retrofitting distribution shifts;datasets shifts did;range machine learning;benchmark wild;unifying datasets;challenges datasets;distribution shifts generally;mapping distribution shifts;distribution shifts;wilds benchmark wild;datasets reflect distribution;wilds benchmark;present wilds benchmark;world distribution shifts;associated challenges datasets;distribution shifts work;datasets typically training;diverse data modalities;drop unifying datasets;shifts spanning diverse;distribution shifts arising;shifts encountered wild;diverse data;datasets evaluation metrics;distribution shifts spanning;datasets", "pdf_keywords": ""}, "2e492af839e971d05592df1c76d4878908e1d4c0": {"ta_keywords": "factor graph grammars;grammars factor graphs;graph grammars fggs;graph grammars factor;factor graphs general;replacement graph grammars;graph grammars;factor graphs;graphs factor graph;sets factor graphs;factor graphs factor;graphs factor;factor graph;single factor graph;factor diagrams;graphs generalization;factor graph amenable;case factor diagrams;graphs generalization variable;grammars fggs;grammars fggs short;graphs general;graphical models case;hyperedge replacement graph;sets graphs generalization;replacement graph;graphical models;dynamic graphical models;finite sets graphs;factor diagrams sum", "pdf_keywords": "graph grammars fggs;factor graph grammars;replacement graph grammars;graph grammars;graph grammars hrgs;grammars fggs;optimizing inference fggs;factor graphs;grammars fggs short;inference fggs enumerating;hrgs factor graphs;perform inference fggs;inference fggs;graphs factor graph;fggs short expressive;graphs factor;exact tractable inference;factor graphs factor;tractable inference;hyperedge replacement graph;replacement graph;factor graph;variable elimination fggs;elimination fggs;short expressive solve;fggs enumerating possibly;tractable inference situations;inference fggs example;techniques optimizing inference;fggs enumerating"}, "c66b59394f99d639b277a54ad357d20de30285bd": {"ta_keywords": "text images biomedical;literature image finder;images biomedical literature;annotated text images;structured literature image;literature human labeling;text mining image;information text images;text images information;text images;literature image;figures biomedical literature;free annotated text;topic modeling provide;annotated text;images biomedical;human labeling;biomedical literature;latent topic modeling;information figures biomedical;mining image;learning select images;select images label;image finder extracting;biomedical literature slif;topic modeling;active learning;figures biomedical;select images;image finder", "pdf_keywords": ""}, "8fc728b71f9e92f91455f957f10c7e496cbe4772": {"ta_keywords": "enhancing entity typing;entity typing language;entity typing model;labels entity typing;entity typing;propose entity typing;entity mention;language model enhancement;types entity mention;typing language model;entity mention specific;semantic types entity;entity typing aims;labels entity;context sentences labels;language model significantly;typing model information;dependent labels entity;typing model;information language model;entity;language model;utilizes language model;semantic types;classify semantic types;typing language;sentences labels;types entity;sentences labels automatically;model information language", "pdf_keywords": ""}, "208e5c187e81f63024ece8e2003dbaef094703cb": {"ta_keywords": "minimizing repair bandwidth;repair bandwidth distributed;bandwidth distributed storage;repair bandwidth;distributed storage;bandwidth distributed;minimizing repair;storage;bandwidth;distributed;repair;minimizing", "pdf_keywords": ""}, "7dadf1e4f6f7a6966d5f691c3707fe221038528b": {"ta_keywords": "fair allocations indivisible;maximizing fair allocations;computing allocations fair;utilitarianmaximal allocations;problems utilitarianmaximal allocations;allocations fair maximize;fair allocations compute;fair maximize utilitarian;allocations indivisible goods;utilitarianmaximal allocations decide;fair allocations;allocations fair;welfare maximizing fair;computational problems utilitarianmaximal;allocations indivisible;maximizing fair;computing welfare maximizing;prop1 fair allocations;indivisible goods problems;fairness concepts envy;maximizes utilitarian welfare;maximizes utilitarian;allocations compute maximizes;maximize utilitarian;compute maximizes utilitarian;tractable fairness concepts;computing allocations;maximize utilitarian social;computing welfare;problems utilitarianmaximal", "pdf_keywords": "allocations indivisible goods;allocating indivisible goods;computing allocations fair;maximizing fair allocations;fair allocations indivisible;indivisible goods agents;allocations fair maximize;allocations indivisible;goods agents problems;fair allocations;allocations fair;computing welfare maximizing;computing allocations;algorithms allocating indivisible;welfare sum agents;allocations;fair maximize utilitarian;welfare maximizing fair;indivisible goods;complexity computing allocations;maximizing fair;allocating indivisible;sum agents utilities;computing welfare;algorithms allocating;goods agents;welfare maximizing;maximize utilitarian;agents problems;agents problems strongly"}, "346081161bdc8f18e2a4c4af7f51d35452b5cb01": {"ta_keywords": "strategyqa question answering;creative questions crowdsourcing;questions crowdsourcing;question answering;questions crowdsourcing workers;question answering qa;annotate question decomposition;steps answering wikipedia;reasoning shortcuts annotate;reasoning steps answering;answering qa benchmark;crowdsourcing;crowdsourcing workers;steps answering question;answering qa;elicit creative questions;question decomposition reasoning;answering wikipedia paragraphs;crowdsourcing workers covering;questions strategyqa short;questions strategyqa;reasoning shortcuts;answering question;benchmark required reasoning;eliminating reasoning shortcuts;steps answering;answering wikipedia;question decomposition;priming inspire annotators;contain answers step", "pdf_keywords": "reasoning skills explore;eliciting creative questions;questions strategy;questions strategy ability;strategies reasoning skills;question decomposition reasoning;eliciting strategy questions;diverse strategies reasoning;reasoning knowledge skills;reasoning skills strategyqa;reasoning strategy questions;reasoning skills analysis;explore required reasoning;reasoning skills;reasoning strategy;reasoning steps answering;question decomposition;reasoning knowledge;strategies reasoning;required reasoning skills;necessitates reasoning wide;strategy questions strategy;strategyqa necessitates reasoning;steps answering wikipedia;required reasoning knowledge;strategy questions using;annotation pipeline eliciting;wide range reasoning;questions using crowdsourcing;eliciting strategy"}, "76df9c90d5359585f5501a4da1af1078c32be6d7": {"ta_keywords": "transcribe images text;open source ocr;accurately transcribe images;transcribe images;ocr present generative;transcribing images documents;accurately transcribe;source ocr;rendering glyphs unsupervised;images text;ocr;structure accurately transcribe;tesseract google;images text overall;glyphs unsupervised;rendering glyphs;transcribe;modeling text document;able decipher font;transcribing images;process rendering glyphs;modeling text;source ocr present;images documents printing;inspired historical printing;transcribing;processes transcribing images;text document noisy;reduction tesseract google;font structure accurately", "pdf_keywords": "learns font structure;effectively learns font;learns font;inspired underlying printing;learned fonts interpretable;learned fonts;generative modeling;structure historical printing;predicted typesetting layouts;generates images documents;underlying printing;based historical typesetting;historical printing;historical printing process;predicted typesetting;fonts interpretable predicted;generative modeling approach;parameters learned fonts;generative;line present generative;historical typesetting;documents text achieves;font structure;noise generative modeling;fashion improve transcription;text;transcription historical documents;historical typesetting process;fonts;generative process image"}, "88c3f221a6fc8aff014268b0efb5ff119ab40906": {"ta_keywords": "irony detection datasets;irony detection dataset;detection irony detection;existing irony detection;sensitive irony detection;detection irony;ironic tweets emoji;irony detection;emoji sensitive irony;irony detection irony;structures irony detection;irony detection important;tweets emoji classifiers;datasets 10 ironic;emoji classifiers trained;emoji classifiers;emojis social media;trained insensitive emojis;analysis augmentation emoji;cues emojis social;ironic tweets;tweets emoji;10 ironic tweets;verbal cues emojis;harassment existing irony;cues emojis;augmentation emoji sensitive;insensitive emojis;augmentation emoji;structures irony", "pdf_keywords": ""}, "ce2d6de9cec4a6d135c32bb8d2d02bba09928b33": {"ta_keywords": "ripper sleeping experts;sleeping experts phrases;learning algorithms ripper;algorithms ripper sleeping;sleeping experts;sleeping experts differ;text categorization;differences ripper sleeping;text categorization problems;sleeping experts perform;categorization problems generally;classifiers;ripper sleeping;large text categorization;experts phrases evaluated;categorization;experts phrases;categorization problems;classifier;classification;algorithms ripper;classifiers allow;algorithms construct classifiers;classifiers allow context;categorization problems spite;machine learning algorithms;classification recently;classification recently implemented;implemented machine learning;classifier different methods", "pdf_keywords": ""}, "393c5c96e73dd3a82c175f9ab1f6c083830d3b82": {"ta_keywords": "fundamentals financial data;neural networks forecast;forecast future fundamentals;report fundamentals financial;stock portfolio simulator;financial data;clairvoyantly select stocks;portfolios far outperform;forecast future;fundamentals financial;portfolios far;stock portfolio;forecast;portfolio simulator backtester;oracle portfolios far;insight financial health;stocks using factors;stocks;financial health;insight financial;financial data revenue;financial;provide insight financial;annual return;portfolio simulator;calculated future fundamentals;analysis outperform market;income debt paper;annual return 17;train deep neural", "pdf_keywords": "stock market prediction;clairvoyantly select stocks;market prediction;based predicted future;market prediction based;automated stock market;approach automated stock;predicted future fundamentals;predicted future;oracle portfolios far;today based predicted;automated stock;investment strategy;fundamentals oracle portfolios;based predicted;portfolios stocks today;stock market;propose investment strategy;stocks today based;oracle portfolios;future fundamentals oracle;portfolios far;stocks today;stocks using factors;investment strategy constructs;portfolios far outperform;run simulations clairvoyant;prediction based time;prediction;calculated future fundamentals"}, "e929c9b53c66d52ae5ea56f0dc2764aef4cc67f6": {"ta_keywords": "channel speech separation;speech separation using;speech separation;speech separation studied;reverberant speech mixtures;single channel speech;highly reverberant speech;speech mixtures analysis;speech mixtures;separation using corpora;existing separation techniques;mixer chime corpora;deep single channel;channel speech;separation techniques deal;datasets mixer chime;reverberant speech;separation techniques;separation using;datasets mixer;multi condition training;training deep learning;training deep;existing separation;separation;mixer chime;extending existing separation;deep single;construct datasets mixer;robustness deep single", "pdf_keywords": ""}, "be28821d510a99ffce40cdcf6860302def8533ef": {"ta_keywords": "recommendation systems strategic;games propose mediator;mediators induce game;facility location games;strategic content providers;welfare recommendation systems;location games propose;content providers strategic;recommendation systems facility;location games;strategic content;recommendation systems;social welfare equilibrium;location games study;social welfare recommendation;users closest content;content recommended users;welfare recommendation;optimize offered content;content providers conceptual;content providers;socially optimal strategy;principle matching users;welfare equilibrium intervene;offered content recommended;providers strategic optimize;welfare equilibrium;mediator implements socially;implements socially optimal;induce game", "pdf_keywords": "recommendation systems strategic;recommendation systems facility;recommendation systems;facility location games;recommendation systems extremely;location games;location games study;strategic content providers;abstract recommendation systems;design recommendation systems;mediator design recommendation;strategic content;facility location models;location games omer;content providers mathematical;abstract recommendation;games study recommendation;consider mediator design;4n optimal social;optimal social;content providers;mediator facility;mediator design;study recommendation systems;game possesses unique;games study;location models;matching users;consider mediator;mediator facility location"}, "3d2dece28f566792b6dd3a190aa345fc30fee1ff": {"ta_keywords": "transportation networks mathematical;ground transportation networks;capacity vertiports travelers;transportation network model;transportation networks;transportation network;minimizes traffic congestion;capacity minimizes traffic;air ground transportation;transportation networks furthermore;networks mathematical programs;aircraft aggravate congestion;congestion ground vehicles;traffic congestion hybrid;anaheim transportation network;ground travel demands;networks mathematical;location capacity minimizes;location capacity vertiports;minimizes traffic;congestion hybrid;vertiports location capacity;vertiports travelers embark;congestion hybrid air;ground transportation;mathematical programs equilibrium;vertiports travelers;traffic congestion;programs equilibrium constraints;linear program", "pdf_keywords": "hybrid airground transportation;airground transportation network;traffic equilibria hybrid;transportation networks mathematical;ground transportation networks;ground transportation network;urban air mobility;traf\ufb01c congestion hybrid;congestion hybrid air;air ground transportation;optimal vertiport location;vertiports hybrid airground;congestion hybrid;airground transportation;minimizing traf\ufb01c congestion;optimal vertiport;vertiport location capacity;location capacity vertiports;capacity vertiports hybrid;static traffic equilibria;vertiports location capacity;minimizes traf\ufb01c congestion;hybrid air ground;select optimal vertiport;transportation network;transportation networks;air mobility concept;transportation network conclusion;vertiport selection hybrid;equilibria hybrid air"}, "eadd73c3e1c20d16e32ee8656c4f954603b37450": {"ta_keywords": "note onset detection;clarinetist videos;audiovisual dataset;onset detection;world clarinetist videos;auditory scenes;hours clarinetist videos;auditory scenes stereo;clarinet note onsets;complex auditory scenes;musical ensembles vision;clarinetist videos carry;onset detection encourage;dataset hours clarinetist;audiovisual;identifying musicians playing;release audiovisual dataset;onsets release audiovisual;note onsets;clarinetist videos cleaned;approach note onset;audiovisual dataset hours;note onset;detection acoustic timed;scenes stereo mixdown;clarinet note;study clarinet note;acoustic timed events;real world clarinetist;scenes stereo", "pdf_keywords": "acoustic timed events;detection acoustic timed;validated audiovisual sequences;acoustic events visual;audiovisual sequences;audiovisual sequences independent;sounds validated audiovisual;spots independent audio;validated audiovisual;acoustic events;audiovisual;detection acoustic;based detection acoustic;independent audio;acoustic timed;sources isolates sounds;isolates sounds;independent audio sources;audio sources;audio;recall cnn model;multimodal approach barzelay;precision recall cnn;recall cnn;audio sources isolates;multimodal;isolates sounds validated;multimodal approach;counterpart multimodal approach;clarinet note onsets"}, "a4e937f0b6e0688f7f3c4fcaebbabefa4a36da85": {"ta_keywords": "describes espnet2 tts;speech e2e tts;espnet2 tts;text speech e2e;e2e tts toolkit;version espnet tts;espnet2 tts end;tts toolkit;espnet tts;espnet2 tts extends;tts toolkit provide;text speech;results espnet2 tts;tts results espnet2;state art tts;tts models;espnet2 tts extending;espnet tts adding;training neural vocoders;tts models extensions;paper describes espnet2;enhance tts performance;enhance tts;neural vocoders;describes espnet2;speech e2e;espnet2;textto waveform modeling;e2e tts;end text speech", "pdf_keywords": "describes espnet2 tts;espnet tts provides;portability espnet tts;espnet2 tts;espnet2 tts enhances;named espnet2 tts;tts espnet2 tts;espnet2 tts end;espnet tts;tts espnet2;generation tts toolkit;tts toolkit;toolkit named espnet2;called espnet tts;espnet2 tts extending;paper describes espnet2;espnet2 tts espnet2;e2e tts toolkit;toolkit called espnet;describes espnet2;state art tts;espnet tts 13;tts provides;development tts techniques;tts toolkit called;tts models;development tts;tts toolkit named;second generation tts;accelerate development tts"}, "2135c44087e06a6d95d04ad0afa400e926d37944": {"ta_keywords": "continuous speech recognition;feature space maplr;normalization structural maplr;corpus spontaneous japanese;regression maplr approach;speech recognition;feature spaces adaptation;vocabulary continuous speech;feature space normalization;regression maplr;normalization feature spaces;regularizing parameter estimation;smaplr consistency adaptation;maplr smaplr model;prior distribution fmaplr;maplr approach;applied normalization feature;normalization feature;consistently applied normalization;linear regression maplr;fmaplr feature space;large vocabulary continuous;model parameter adaptation;space normalization structural;continuous speech;maplr;using corpus spontaneous;space normalization;maplr approach spaces;maplr fmaplr feature", "pdf_keywords": ""}, "99c80d608ba2aa638333f27bbe3f09cdc580a051": {"ta_keywords": "implementations torchaudio easily;implementations torchaudio;functionalities benchmarks torchaudio;available implementations torchaudio;pytorch audio;torchaudio accelerate development;benchmarks torchaudio;pytorch audio document;torchaudio easily;torchaudio;com pytorch audio;learning applications audio;torchaudio building blocks;10 torchaudio;audio speech processing;objective torchaudio;torchaudio easily installed;torchaudio building;applications audio speech;version 10 torchaudio;building blocks torchaudio;domain objective torchaudio;audio speech;10 torchaudio building;applications audio;torchaudio accelerate;audio;blocks torchaudio;blocks torchaudio building;objective torchaudio accelerate", "pdf_keywords": "functionalities benchmarks torchaudio;torchaudio introduction recent;torchaudio introduction;torchaudio;benchmarks torchaudio;torchaudio designed;torchaudio designed facilitate;functionalities structure torchaudio;new projects audio;structure torchaudio introduction;structure torchaudio;rapidly torchaudio designed;projects audio;rapidly torchaudio;projects audio speech;grown rapidly torchaudio;audio speech;audio speech domain;audio;speech domain document;source toolkits;open source toolkits;toolkits;source toolkits development;toolkits development;learning applications;speech domain;learning applications grown;machine learning applications;art machine learning"}, "29bc6654abd34b2405f7a01341f790aed2aab9a4": {"ta_keywords": "distributed storage codes;storage codes efficient;efficient distributed storage;storage codes;distributed storage;designing distributed storage;storage codes basic;codes smallest repair;smallest repair locality;codes efficient data;binary mds codes;repair locality;storage;exist mds codes;repair locality present;codes exist mds;node repair piggybacking;mds codes smallest;efficient data read;mds codes exist;download efficient distributed;smallest data read;binary mds;codes efficient;efficient distributed;codes smallest;mds codes;designing distributed;codes attain smallest;codes meeting constraints", "pdf_keywords": ""}, "b35ad59ce9a3ea01a0980c90bc750273d1f99e7a": {"ta_keywords": "database integration;domain normalization databases;integration heterogeneous databases;similarity local names;data integration based;database integration called;names correspond entities;databases;heterogeneous databases;data integration;normalization databases;statistical information retrieval;information retrieval;databases contain constants;database;normalization databases contain;databases contain;databases assumed local;heterogeneous databases assumed;implemented data integration;names measured using;logic database integration;databases assumed;names correspond;names measured;local names measured;integrate information;successfully integrate information;propose logic database;information retrieval previous", "pdf_keywords": ""}, "04e0fb8b3bb06e1200288e6d2a17d55773e97504": {"ta_keywords": "fair bandwidth sharing;bandwidth mobile users;fair bandwidth;aware opportunistic bandwidth;dependent opportunistic bandwidth;bandwidth mobile;opportunistic bandwidth sharing;convergence bandwidth sharing;mobile users stochastic;downlink users cellular;opportunistic bandwidth;location dependent bandwidth;bandwidth sharing policies;bandwidth sharing;issue fair bandwidth;learning cellular networks;dependent bandwidth sharing;bandwidth sharing classes;higher bandwidth mobile;location aware opportunistic;bandwidth sharing static;users cellular network;bandwidth static users;users stochastic learning;dependent bandwidth;cellular networks;users cellular;location dependent opportunistic;users notion fairness;cellular network", "pdf_keywords": "bandwidth mobile users;location aware opportunistic;opportunistic bandwidth sharing;location dependent bandwidth;bandwidth mobile;aware opportunistic bandwidth;downlink users cellular;mobile users stochastic;higher bandwidth mobile;dependent opportunistic bandwidth;location dependent opportunistic;opportunistic bandwidth;users cellular network;optimal bandwidth sharing;bandwidth sharing static;throughputs static mobile;throughputs mobile static;sum throughputs mobile;bandwidth static users;bandwidth sharing;sharing static mobile;bandwidth sharing strategy;users cellular;dependent bandwidth sharing;cellular network;cellular networks;static mobile users;throughputs mobile;learning cellular networks;data rate mobile"}, "016d83091a60a6de67ba2395c063967686043380": {"ta_keywords": "speech recognition vbec;acoustic model adaptation;estimation clustering speech;clustering speech recognition;model adaptation vbec;adaptation vbec estimate;recognition vbec acoustic;clustering speech;variational bayesian estimation;speech recognition;supervised adaptation;adaptation vbec;model adaptation;vbec acoustic model;isolated word recognition;adaptation data;recognition vbec;bayesian estimation clustering;variational bayesian approach;using variational bayesian;variational bayesian;apply variational bayesian;vbec acoustic;word recognition;supervised adaptation experiment;vbec estimate;vbec estimate parameter;word recognition advantage;posteriori map vbec;estimate parameter posteriors", "pdf_keywords": ""}, "06708348b64e2e7b11a953389556c701bf3298da": {"ta_keywords": "crowdsourcing language resources;crowdsourcing language;language resources evaluation;crowdsourcing;language resources;resources evaluation;language;evaluation;resources", "pdf_keywords": ""}, "0098123efc851b67137c1028f7bac8d8bffbc8fd": {"ta_keywords": "models semantic parsing;language models plms;grounding pretrained language;pretrained language models;semantic parsing;latent grounding understandable;language models semantic;language models;latent grounding;modeling language;explore syntactic structures;parsing;explore syntactic;awaken latent grounding;latent grounding pretrained;token grounded concept;modeling language efforts;models semantic;grounding understandable;syntactic structures;pretrained language;emerged explore syntactic;grounding understandable human;awakening latent grounding;entailed plms;syntactic;syntactic structures entailed;semantic;entailed plms recent;plms learned", "pdf_keywords": "annotations inference awakening;models semantic parsing;language models semantic;pretrained language models;language models erasing;models semantic;bert oracle learnable;modeling language slsqll;language models plms;human annotations inference;semantic parsing;language models;semantic parsing text;grounding pretrained language;semantic;annotations inference;inference awakening latent;semantic parsing qian;modeling language;learnable schema linking;inference awakening;language slsqll;weakly supervised;simultaneously learn schema;learnable schema;slsql bert oracle;oracle learnable;learn schema linking;oracle learnable schema;auxiliary concept prediction"}, "45f59bd3ef8e1d76474199c08c140675c04a728c": {"ta_keywords": "learning agents anticipate;multi agent learning;learning agents;agent learning agents;agent learning rules;implicit conjecture learning;dynamics opponent anticipation;agent learning;conjectures learning processes;polynomial game gradient;forming conjectures learning;conjectures learning;conjecture learning;simultaneous gradient play;conjecture learning leads;opponent anticipation;opponent anticipation conjectural;polynomial game;multi agent;game gradient;heterogeneous multi agent;gradient play empirically;fast conjecture learning;conjecture learning decrease;gradient play;dynamics opponent;sum polynomial game;learning processes devise;learning leads equitable;processes devise learning", "pdf_keywords": ""}, "a13c580250af3644fe368b08a540f4ea65dac919": {"ta_keywords": "compressive phase retrieval;efficient compressive phase;sparse graph coding;sparse graph codes;phasecode fast efficient;fast efficient compressive;compressive phase;efficient compressive;phasecode algorithm recover;phase retrieval based;phasecode fast;sparse unconstrained fourier;phase retrieval;noiseless case phasecode;phasecode algorithm;algorithms based sparse;proposed algorithms sparse;sparse graph;phasecode;introduce phasecode;based sparse graph;noisy scenarios phasecode;phasecode novel;algorithms sparse;algorithms sparse unconstrained;introduce phasecode novel;compressive;retrieval based sparse;sparse unconstrained;graph coding", "pdf_keywords": ""}, "0b2e9e978898b9fb1116ea964c8c470086ceed87": {"ta_keywords": "salient object detection;rgb salient object;strategy rgb salient;informative depth cues;rgb salient;vision multi level;object detection;depth cues;depth cues channel;object detection devise;multi modal cues;rgb depth modalities;vision multi;rgb depth;computer vision multi;net rgb depth;nature rgb salient;backbone strategy rgb;depth enhanced module;salient object;modal cues;feature fusion;depth enhanced;level features teacher;introduce depth enhanced;modal learning;object detection paper;multi modal learning;multi level features;level feature fusion", "pdf_keywords": "rgb depth features;depth features channel;informative depth cues;depth features;cues depth features;depth cues channel;rgb depth;depth cues;informative cues depth;views introduce depth;depth features particular;merging rgb depth;introduce depth enhanced;depth enhanced;depth enhanced module;rgb datasets provide;cues depth;rgb datasets;informative depth;channel spatial views;multilevel features teacher;introduce depth;features channel spatial;depth;rgb;cues channel spatial;excavate informative depth;student features;different rgb datasets;merging rgb"}, "0b8dbc4a899c836fe2b1a213b9dc064cdf62fd63": {"ta_keywords": "explanations sentences procedural;explanations sentences;constructs explanations sentences;explaining effects;explain explaining effects;recent process comprehension;perturbations procedural text;better explanation accuracy;constructs explanations;process comprehension;explanations;sentences procedural;sentences procedural text;quartet constructs explanations;explanation task identifying;explaining effects perturbations;goal explain effects;explain effects;effects perturbations procedural;process comprehension benchmark;rabbits explanation task;explanation task;explanation accuracy compared;task identifying causal;procedural text achieving;explain illness perturbation;explaining;explanation accuracy;comprehension;comprehension benchmark", "pdf_keywords": "explanations sentences procedural;constructs explanations sentences;modeling explanation task;explanations sentences;explanations paragraphs;recent process comprehension;qualitative reasoning explanations;explanations paragraphs modeling;constructs explanations paragraphs;process comprehension;sentences procedural text;sentences procedural;process comprehension benchmark;explaining effects;explain explaining effects;reasoning explanations takes;constructs explanations;better explanation accuracy;reasoning explanations;explanations;explanations takes input;explanations takes;explanation task;qualitative reasoning;goal explain effects;comprehension benchmark;qualitative effect;events leading qualitative;comprehension benchmark present;qualitative outcome"}, "f07c5c540233b22f0ca154c80c713e2aed3c9606": {"ta_keywords": "music transformer gans;model discriminator gan;discriminator gan experiments;discriminator gan;adversarial losses;gans used sequence;discriminative metric music;transformer gans;music generated;gumbel softmax;music generated approach;metric music generated;use gumbel softmax;gumbel softmax trick;adversarial;adversarial losses complement;music generation;gan experiments;softmax;gans;music generation goal;transformer gans used;approach music generation;minute long compositions;softmax trick;exploration adversarial losses;art music transformer;music transformer;gan experiments helped;trained minimizing negative", "pdf_keywords": ""}, "d15a7d00897f58a94def2a58c0cb0311851f2968": {"ta_keywords": "speech recognition asr;distant speech recognition;kaldi speech recognition;speech recognition toolkit;speech recognition;speech enhancement baseline;evaluation speech quality;speech enhancement measures;asr speech processing;setup speech enhancement;different speech enhancement;baseline automatic speech;speech quality;speech enhancement;speech distortion ratio;automatic speech recognition;speech quality pesq;speech processing;noisy asr speech;speech processing communities;challenge setup speech;setup speech;repository kaldi speech;speech recognition using;automatic speech;perceptual evaluation speech;recognition asr chime;speech distortion;asr chime challenge;neural network tdnn", "pdf_keywords": "speech recognition asr;asr speech processing;asr speech enhancement;recognition asr chime;kaldi speech recognition;distant speech recognition;speech recognition;asr chime challenge;speech recognition toolkit;recognition asr;automatic speech recognition;speech processing;noisy asr speech;speech processing communities;baseline automatic speech;speech recognition using;asr chime;speech enhancement baseline;repository kaldi speech;speech processing johns;setup speech enhancement;asr speech;challenge setup speech;automatic speech;speech enhancement measures;language speech processing;recognition using chime;using chime challenge;different speech enhancement;chime challenge setup"}, "04b44c518b145be625ff270af56cfd2e37900137": {"ta_keywords": "aware speaker diarization;speaker diarization single;single channel recordings;overlap aware speaker;speaker diarization;utterances multiple remote;distributed microphones;distributed microphones replace;inputs hybrid meetings;channel recordings;hybrid meetings utterances;signals distributed microphones;eend multi channel;microphones;aware speaker;meetings utterances multiple;single channel input;diarization single neural;multi channel inputs;microphones replace;neural diarization eend;multi channel input;end neural diarization;single channel;diarization eend en;diarization eend;meetings utterances;multi channel;recordings;using single channel", "pdf_keywords": "multi channel encoders;multi channel recordings;channel encoders;encoders process multichannel;single channel recordings;channel encoders replace;transformer encoders eend;end neural diarization;based distributed microphones;channel recordings;encoders conventional eend;distributed microphones;distributed microphone;distributed microphone settings;distributed microphones proposed;nels distributed microphone;encoders eend;channel recordings achieved;transformer encoders;encoders;microphone;neural diarization eend;microphone settings;ders multichannel inputs;encoders eend types;microphones;encoders conventional;multichannel inputs;eend types encoders;encoders replace transformer"}, "a56dba9cabfc110df231051d7c9d6e439f6757dd": {"ta_keywords": "speech pos tagging;segmentation pos tagging;better domain adaptation;domain adaptation;domain training;methods domain training;trainable partially annotated;domain adaptation situations;domain training data;tagging highly domain;pos tagging;pos tagging highly;tagging japanese;word segmentation pos;pos tagging japanese;tagging;method speech pos;partially annotated data;pointwise pos taggers;tagging highly;sequence based predictors;partially annotated;hidden markov models;speech pos;word segmentation;annotated data;accurate method speech;based pos taggers;sequence based pos;segmentation pos", "pdf_keywords": ""}, "2ab9fd2be2bf82e0bbd558cc64c1c46728fc4f8a": {"ta_keywords": "adaptation automatic speech;multi scale adaptation;multiscale adaptation;adaptation acoustic model;incremental adaptation;incremental adaptations multiscale;adaptations multiscale adaptation;multiscale adaptation potential;original incremental adaptation;model adaptation automatic;scale adaptation scheme;model adaptation;formulation incremental adaptation;time incremental adaptations;multiscale properties speech;method adaptation acoustic;multiple time scale;incremental adaptation assumes;incremental adaptation scheme;scale adaptation;adaptation acoustic;adaptation automatic;multiple time scales;continuous speech recognition;robust speech recognition;incremental adaptations;scale multiple time;robust speech;single time scale;recognition formulation incremental", "pdf_keywords": ""}, "f17e182fcb7fbbff2257824174ed6f7df512a42b": {"ta_keywords": "end speech recognition;speech recognition asr;speech recognition joint;asr multi encoder;ctc attention model;speech recognition;recognition asr multi;model attention based;novel multi encoder;multi encoder;multi encoder multi;attention model;attention model attention;encoder multi;attention model achieved;training joint decoding;encoder multi resolution;automatic speech recognition;end automatic speech;model attention;recognition asr;automatic speech;encoder;joint ctc attention;connectionist temporal classification;end end speech;end speech;joint decoding;multi task training;encoders", "pdf_keywords": "rnn based encoders;rnn based cnn;cnn rnn based;encoder ctc;cnn rnn;attetion single encoder;features carried rnn;model encoder ctc;single encoder models;single encoder;con\ufb01guration encoders compared;various single encoder;encoder models proposed;encoder models reaching;based cnn rnn;encoder memr;network individual encoder;encoder enhance heterogeneous;encoders different architectures;heterogeneous con\ufb01guration encoders;layers heterogeneous encoders;single encoder end;encoder models;encoder ctc han;encoder;con\ufb01guration encoders;encoder memr model;individual encoder;memr model encoder;encoders subsampling convolutional"}, "72b4ff7387223cf0398c298c3cc62ee07d9c0043": {"ta_keywords": "models sentence completion;sentence completion challenge;sentence completion paper;sentence completion;sentence completion challenging;research sentence completion;neural language models;completion challenging semantic;complete sentence dependency;dependency language models;language models sentence;gram language models;sentence dependency language;simple language models;language models;language models probability;syntactic dependency tree;sentence dependency;sentence estimated probability;language models complex;challenging semantic modeling;probability sentence estimated;estimated probability lexicalisation;sentence estimated;completion challenge improves;improves gram language;semantic modeling task;given syntactic dependency;completion challenge;completion", "pdf_keywords": ""}, "57fec656119e82b5e70b1a654f6d87d8c1137ef4": {"ta_keywords": "annotated biological figures;retrieval summarization biological;figures biological literature;mining captioned figures;captioned figures biological;summarization biological information;correspondence topic models;mining captioned;probabilistic topic model;structured probabilistic topic;summarization biological;models mining captioned;biological information literature;topic models;figures biological;topic model;annotated biological;automatic retrieval summarization;biological literature biological;biological figures derive;literature biological articles;biological articles;biological literature;retrieval summarization;biological articles typical;topic model built;sampling information retrieval;biological figures;automatic retrieval;gibbs sampling information", "pdf_keywords": ""}, "2797a36cd15b8c046683247995261546993c289d": {"ta_keywords": "modelbased speech enhancement;model speech enhancement;speech enhancement pre;speech enhancement;temporal speech noise;enhanced features acoustic;speech noise modeling;multi channel speechnoise;channel speechnoise separation;processor recognizer speech;spectral temporal speech;recognizer speech recognition;speech recognition;speechnoise separation method;channel speechnoise;speech noise;acoustic model speech;mllr dynamic adaptive;speech recognition presence;modelbased speech;speechnoise separation;recognizing speech;dynamic variance adaptation;time varying noise;recognizing speech presence;variance gaussians acoustic;features acoustic model;varying noise sources;approach modelbased speech;noise modeling combined", "pdf_keywords": ""}, "99bb811beb5d061d2b8fac5a1973b49cace93e2f": {"ta_keywords": "topic model tracking;topic model;new topic model;tracking timevarying consumer;interests item trends;purchase behavior computational;consumer interests item;current purchase logs;track changes interests;estimated interests trends;timevarying consumer purchase;interests trends use;interests trends based;item trends change;behavior consumer interests;real purchase logs;item trends;changes interests trends;consumer interests;consumer purchase behavior;interests trends;previously estimated interests;purchase logs previously;purchase logs;purchase behavior consumer;timevarying consumer;purchase behavior;trends change time;adaptively track changes;model adaptively track", "pdf_keywords": ""}, "291b651654565cd88e4e56de5250219a71882a50": {"ta_keywords": "peer reviewing algorithm;accurate peer reviewing;peernomination accurate peer;extend peernomination accurate;winners peer reviewed;peer reviewing;agents peer selection;subset winners peer;peernomination accurate;peer selection;winners peer;inaccurate agents peer;extend peernomination;peer selection problem;accurate peer;noisy assessments peers;peernomination;agents peer;paper extend peernomination;assessments peers;peer reviewed;assessors reliability weights;peers;peer;reliability weights way;peer reviewed grants;assessments peers paper;reviewing algorithm;reliability weights;truth ordering agents", "pdf_keywords": "quality peer selection;peer selection peernomination;peer selection algorithm;peer selection;selection peernomination;strategyproof peer selection;weightedpeernomination weighs reviewers;improve quality peer;weighs reviewers based;quality peer;selection peernomination variety;peernomination;weighs reviewers;reviewers based perceived;reviewers based;noisy assessments algorithm;reweighting methods empirically;weighting evaluation methods;peernomination variety;peernomination variety noise;reviewers;selection algorithm weightedpeernomination;peer;strategyproof peer;weighting evaluation;novel strategyproof peer;reweghting protocols empirically;algorithm weightedpeernomination weighs;instances reweighting methods;algorithm weightedpeernomination"}, "9cf4609178e2739ed35f8d3e3d6efb7d5e2e1a41": {"ta_keywords": "traffic dynamics signal;behavior traffic dynamics;traffic dynamics;modeling traffic dynamics;traffic dynamics secondly;unstable behavior traffic;modeling traffic;anticipating queue behavior;traffic signal phase;behavior modified traffic;detection unstable behavior;traffic signal;anticipating queue;method modeling traffic;eigenvalues learned dynamics;behavior traffic;traffic;unstable eigenvalues learned;modified traffic signal;congested condition analysis;learned dynamics anomaly;dynamics anomaly feature;modified traffic;automated detection unstable;congestion;detection unstable;severe congestion;eigenvalues learned;queue lengths accident;sequences unstable eigenvalues", "pdf_keywords": ""}, "74495e9735b601ce9060ac40ac27d196fdbf7462": {"ta_keywords": "codes distributed storage;regenerating codes distributed;bound repair bandwidth;distributed storage flexible;distributed storage;codes distributed;repair bandwidth;repair bandwidth constructive;distributed storage setting;regenerating codes;lower bound repair;stored nodes;stored nodes network;data stored nodes;bound repair;recovered connecting nodes;storage flexible;storage flexible setting;class regenerating codes;storage;data node;codes meeting bound;proof existence codes;repair failed node;distributed;nodes downloading;nodes downloading units;bandwidth constructive proof;connecting nodes downloading;data recovered", "pdf_keywords": ""}, "f75ba81828fd9d8c7fcba89dd98a0ee73d32dce6": {"ta_keywords": "mds distributed storage;code mds distributed;distributed storage code;mds distributed;minimizes repair bandwidth;distributed storage;miser code mds;repair bandwidth systematic;bandwidth systematic nodes;storage code minimizes;code mds;nodes interference alignment;storage code;systematic nodes interference;repair bandwidth;bandwidth systematic;interference alignment;nodes interference;distributed;mds;code minimizes repair;storage;bandwidth;systematic nodes;nodes;miser code;interference;minimizes repair;code minimizes;alignment", "pdf_keywords": ""}, "58fd3001c88e9784b0794eef06cb7c0eab0d8747": {"ta_keywords": "picture synthesis ontology;synthesis ontology based;picture synthesis systems;synthesis ontology;picture synthesis;text picture synthesis;synthesis systems;ontology based;ontology;present ontology based;synthesis;ontology based approach;approach text picture;objects representation;present ontology;interacting objects representation;text picture;paper present ontology;systems;objects representation behaviour;based approach text;approach text;representation;representation behaviour makes;text;interacting objects;coupling components uni;representation behaviour;coupling components;information resources", "pdf_keywords": ""}, "c9472731afe5fca98f362f49e26d17a9d5d0cc8e": {"ta_keywords": "categorization procedures understandable;interpretability problem machine;problems motivated interpretability;interpretability;interpretability critical;say machine learning;making sense interpretability;interpretability problem;machine learning suffers;interpretable explicable;sense interpretability critical;sense interpretability;interpretable explicable terms;interpretable;motivated interpretability;categorization;machine learning algorithms;makes concepts difficult;usefulness machine learning;categorization procedures;problem machine learning;machine learning;concepts difficult;machine learning furthermore;learning algorithms opaque;examination interpretability;interpretability critical examination;concepts difficult use;failing interpretable explicable;concepts possible", "pdf_keywords": "interpretability intelligibility;concept interpretability;black boxes interpretability;notions interpretability intelligibility;interpretability;interpretability intelligibility explicability;interpretability critical;interpretability problem machine;notions interpretability;interpretability responds;concept interpretability questioning;interpretability responds urgent;interrogated concept interpretability;increasingly ubiquitous ml;interpretability problem;critique notions interpretability;interpretability questioning;boxes interpretability responds;boxes interpretability;access interpretability critical;ml algorithms ultimately;ubiquitous ml;access interpretability;usefulness machine learning;open access interpretability;machine learning ml;ubiquitous ml algorithms;black box introduction;surrounding ml algorithms;ml algorithms widespread"}, "b103bb1dc05a48796a3ff0804c11909bf68db11b": {"ta_keywords": "detecting speculative language;task detecting speculative;distinguish words speculative;task1 token classification;speculative language biomedical;detecting speculative;token classification task;treat detection sentences;predicts token sentence;detection sentences;speculative language;classifier based syntactic;detection sentences containing;token classification;language biomedical text;words speculative;classifier predicts token;language biomedical;words speculative non;classification task;task2 learn classifier;biomedical text;information task1 token;sentence label;text treat detection;sentence label distinguish;speculative meaning;determines sentence label;task detecting;syntactic features", "pdf_keywords": ""}, "83ac0851a8f6fa02f5db251b260f635907d7a01e": {"ta_keywords": "vision language navigation;backtracking vision language;action decoding achieves;action decoding;backtracking fast navigator;backtracking vision;navigation challenge tactical;agent tasked navigating;correction backtracking vision;tasked navigating source;language navigation challenge;vision language;tasked navigating;fast navigator;r2r vision language;aware search backtracking;navigation current approaches;framework action decoding;navigation challenge;language navigation;navigator;frontier aware search;navigation;navigating;search backtracking fast;room r2r vision;language navigation current;backtracking fast;decoding achieves;approaches make local", "pdf_keywords": "vision language navigation;train seq2seq captioning;language vision encoded;seq2seq captioning model;language vision;captioning model;21 language vision;language navigation;asynchronous search combines;vision language;approaches vision language;supervised actions;actions train seq2seq;seq2seq captioning;captioning;trainable neural;individually supervised actions;observations actions train;asynchronous search;sequence visual observations;local action knowledge;produce textual description;visual observations actions;language navigation vln;trainable neural network;actions train;supervised actions requires;action knowledge;textual description current;using trainable neural"}, "b02acb3d159b06b2319a164378e1e61c3983676f": {"ta_keywords": "complex query answering;answering knowledge graphs;query answering knowledge;generalizability complex query;complex queries;queries complex;queries types;complex queries complex;query answering cqa;query answering;forms complex queries;queries types 20;benchmarking combinatorial generalizability;different queries types;benchmark combinatorial generalizability;queries;queries complex query;knowledge graphs specifically;knowledge graphs;task knowledge graphs;knowledge graphs code;queries given canonical;different queries;benchmarking combinatorial;complex query;benchmark combinatorial;answering knowledge;combinatorial generalizability cqa;dataset benchmark combinatorial;benchmark cqa", "pdf_keywords": "answering knowledge graphs;query answering knowledge;generalizability complex query;complex query answering;cqa knowledge graphs;benchmarking combinatorial generalizability;knowledge graphs arxiv;benchmark combinatorial generalizability;knowledge graphs extend;knowledge graphs present;knowledge graphs;query answering cqa;queries types;query answering;queries types 20;query types;queries answer sets;hand crafted query;existential order queries;different queries types;task knowledge graphs;combinatorial generalizability cqa;abstract complex query;queries various;query types family;types converting queries;answering knowledge;queries single free;dataset benchmark combinatorial;queries"}, "eaa224ae5c969180503dda4972ab86d3a71c888c": {"ta_keywords": "music recognition omr;optical music recognition;music recognition;polyphonic datasets;music recognition previous;recognition omr monophonic;end end polyphonic;polyphonic optical music;end polyphonic optical;recognition sheet music;end polyphonic omr;polyphonic datasets suitable;end polyphonic;sequence detection piano;end recognition;large scale polyphonic;polyphonic omr;detection piano orchestral;end end recognition;scale polyphonic datasets;polyphonic;polyphonic optical;monophonic homophonic music;polyphonic passages;frequently exhibit polyphonic;detection piano;optical music;end recognition sheet;exhibit polyphonic passages;omr monophonic", "pdf_keywords": "music recognition omr;polyphonic datasets;polyphonic music data;end end recognition;polyphonic datasets suitable;end recognition;optical music recognition;music recognition;end approaches polyphonic;approaches polyphonic omr;scale polyphonic datasets;end polyphonic omr;dataset exclusively polyphonic;large scale polyphonic;recognition sheet music;end end polyphonic;polyphonic omr;end recognition sheet;encoder;encoder proposed;end toend polyphonic;decoder models flagdecoder;polyphonic omr input;sequence detection decoder;end polyphonic;polyphonic optical music;music data;polyphonic music;rnndecoder;approaches polyphonic"}, "baad427b45ac691763fe3de4ea3ac1bffd3c74e3": {"ta_keywords": "partytracker new visualization;political survey data;information visualization increasingly;researchers studying partisanship;demographic backgrounds partytracker;insights political survey;information visualization;partytracker;visualization tool;party affiliation survey;studying partisanship;visualization increasingly;partytracker capable;political survey;affiliation survey data;partytracker new;visualization tool allows;new visualization tool;partisanship;geographic party;survey data;survey data people;visualization increasingly important;partytracker capable revealing;temporal geographic party;present partytracker;visualization;studying partisanship general;geographic party affiliation;partisanship general public", "pdf_keywords": ""}, "64280761641d8f1eb285165160bd96efac0bb5f5": {"ta_keywords": "speech environmental recognition;learning simultaneous speech;recognition speech environmental;simultaneous recognition speech;simultaneous speech environmental;deep bottleneck features;bottleneck features sound;recognize speech environmental;auditory environment recognizing;speech environmental sounds;dnn techniques recognize;features sound dependent;simultaneous speech;sound dependent vectors;dnn techniques;environmental sounds simultaneously;recognition speech;recognize speech;vectors input dnns;neural network dnn;combines bottleneck features;utilizing bottleneck features;speech environmental;deep bottleneck;bottleneck features vectors;network dnn techniques;techniques recognize speech;environmental sounds study;bottleneck features;deep neural", "pdf_keywords": ""}, "2391e7446d47f681ad705c8e75d9d2ce1b92ad5f": {"ta_keywords": "available corpus;corpus new;available corpus search;annotation efforts corpus;corpus;corpus new middle;corpus search tool;investigations combined corpus;published available corpus;corpus search;high german grammar;combined corpus;earlier annotation efforts;annotation efforts;high german texts;corpus consists;german grammar;combined corpus consists;texts digitized annotated;german texts;developing common annotation;annotation process texts;annotation standards;common annotation;german grammar migrako;earlier annotation;annotation;common annotation standards;german texts 1200;annotated parts speech", "pdf_keywords": ""}, "ff7e60b8d336aef5ed974609a63610641085177e": {"ta_keywords": "softmax distribution estimation;raml proposed softmax;softmax;proposed softmax distribution;introduce softmax distribution;softmax distribution regarded;introduce softmax;softmax distribution;proposed softmax;prediction tasks rewards;work introduce softmax;structured prediction tasks;successes softmax;empirical successes softmax;successes softmax distribution;prediction tasks;machine translation;entity recognition tree;reward function structured;structured prediction;machine translation structures;entity recognition;function structured prediction;recognition tree;image captioning;captioning;named entity recognition;task specific reward;analysis reward;specific reward", "pdf_keywords": "softmax distribution estimation;proposed softmax distribution;softmax;raml proposed softmax;iclr 2018 softmax;introduce softmax distribution;introduce softmax;proposed softmax;softmax distribution;2018 softmax distribution;2018 softmax;work introduce softmax;structured prediction tasks;estimation structured prediction;prediction tasks;reward function structured;structured prediction;image captioning demonstrate;abstract reward;structured prediction theoretical;captioning;function structured prediction;image captioning;optimize reward;optimize reward function;reward performing maximum;directly optimize reward;captioning demonstrate;task speci\ufb01c reward;reward augmented maximum"}, "4d991a83d6044b1aaed2c117b3d097ecd23cf6f4": {"ta_keywords": "speaker diarization;outputs speaker diarization;speaker diarization simple;reformulating speaker diarization;speaker diarization results;dealing speaker diarization;diarization reformulating speaker;speaker recording clustering;speaker embedding models;end neural diarization;captures global speaker;speaker embedding;speaker diarization problem;adapting speaker embedding;global speaker characteristics;formulate speaker diarization;speaker recording;neural diarization reformulating;recordings speaker;adapting speaker;global speaker;end neural;neural diarization;multi speaker recording;end end neural;short term memory;reformulating speaker;local speech activity;attention based neural;audio recordings speaker", "pdf_keywords": "outputs speaker diarization;speaker diarization;approach speaker diarization;speaker diarization results;speaker diarization problem;end neural diarization;diarization eend neural;neural diarization eend;neural networks eend;eend neural network;neural diarization;attention based neural;diarization eend;speaker recording;novel approach speaker;end end neural;end neural;multi speaker recording;eend neural;neural network architectures;neural network architecture;diarization eend section;outputs speaker;given multi speaker;neural networks;multi speaker;approach speaker;self attention based;neural network;neural network directly"}, "9f2cf7b35224aad3a8d261e4456fe2d65a5f5d3e": {"ta_keywords": "encoders effective retrieval;encoders generalizable retrievers;generalize domains retrieval;encoders trained domain;domains retrieval tasks;dual encoders effective;dual encoders trained;large dual encoders;domains retrieval;encoders effective;encoders trained;dual encoder;dual encoders generalizable;effective retrieval;dual encoders;retrieval tasks large;encoder;dual encoder model;effective retrieval model;dual encoder final;encoders generalizable;encoders;generalizable retrievers;retrieval model ofdomain;make dual encoders;gtr data efficient;retrieval;size dual encoder;retrieval model;encoder final score", "pdf_keywords": "encoders effective retrieval;retrieval benchmark;shot retrieval benchmark;retrieval benchmark especially;retrieval tasks domains;effective retrieval;effective retrieval model;zero shot retrieval;retrieval performance;improvement retrieval performance;bottleneck embedding dimension;dual encoders effective;generalizable t5 retriever;information retrieval tasks;retrieval model ofdomain;retrieval model;improvement retrieval;retrieval;size dual encoder;information retrieval;dual encoders taking;keeping bottleneck embedding;scaling dual encoders;bottleneck embedding;layer dual encoder;bottleneck embedding size;retrieval tasks;dual encoder;\ufb01xed bottleneck embedding;dual encoders"}, "d18d8d364bb18d66924919feebb2e892ebe6761c": {"ta_keywords": "machine translation nmt;neural machine translation;translation smt neural;machine translation smt;statistical machine translation;machine translation phrase;machine translation;phrase based smt;phrase based decoding;phrase based translation;translation nmt;decoding cost nmt;smt neural machine;translation phrase based;translation smt;smt neural;smt limited phrase;based translation rule;smt nmt;cost nmt output;nmt outputs search;cost nmt;translation rule;best nmt;nmt output using;model compute phrase;translation rule table;rerank best nmt;nmt outputs compared;nmt output", "pdf_keywords": "phrasebased decoding cost;generate nmt translation;nmt translation results;improved translation accuracy;phrasebased decoding;nmt translation;compute phrasebased decoding;phrase based translation;decoding cost nmt;phrase based decoding;given nmt translation;sentence generate nmt;outputs improved translation;based translation model;french translation tasks;phrase based smt;improve translation quality;translation tasks;translation accuracy;translation tasks obtaining;nmt translation force;translation quality;translation model compute;improved translation;translation model;successfully improve translation;decoding cost rerank;rerank nmt outputs;cost nmt output;cost rerank nmt"}, "1b96b89d5b3ba444126cebefdfc665d3866f14f0": {"ta_keywords": "fair outcomes hiring;hiring algorithms machine;hiring algorithms;algorithmic hiring;role hiring algorithms;explainability algorithmic hiring;fairness explainability algorithmic;decision making fairness;making fairness limited;modern hiring pipeline;work fairness specifically;hiring incorporating definitions;need fairness explainability;need fairness;making fairness;fairness specifically online;hiring pipeline;outcomes hiring;work fairness;fairness specifically;processes hiring;fairness explainability;fairness limited work;making processes hiring;limited work fairness;fair outcomes;outcomes hiring incorporating;hiring pipeline need;fairness limited;hiring", "pdf_keywords": ""}, "c888022dec626171d243d2a056709b9b053a0ed9": {"ta_keywords": "end speech recognition;speech recognition midst;noisy speech benchmarks;noise suppression speech;speech enhancement;speech benchmarks chime;speech recognition;speech enhancement acoustic;suppression speech enhancement;enhancement acoustic encoding;speech benchmarks;acoustic encoding network;end neural networks;microphone array;processing noise suppression;microphone;speech recognition experiments;field speech recognition;end end speech;end speech;microphone array signal;noisy speech;encompass microphone array;noise suppression;end end neural;acoustic encoding;experiments noisy speech;end neural;framework encompass microphone;encompass microphone", "pdf_keywords": "end speech recognition;speech multichannel end;enhancement speech recognition;recognition speech enhancement;multichannel speech recognition;speech recognition enhancement;optimization multichannel speech;multichannel speech multichannel;speech multichannel;multichannel speech;speech enhancement;inference multichannel speech;speech enhancement acoustic;speech enhancement speech;suppression speech enhancement;speech recognition midst;enhancement acoustic encoding;speech recognition;enhancement speech;acoustic encoding network;speech recognition objective;speech recognition arxiv;end training acoustic;noise suppression speech;microphones allows beamforming;speech recognition speech;end speech;end end speech;end neural networks;recognition speech"}, "2526c510610c7220ecc56e6b08d09c4cbaf58c3c": {"ta_keywords": "expressions honorifics japanese;honorifics japanese complicated;honorifics japanese situation;mechanism honorifics japanese;honorifics japanese;regular expressions honorifics;appropriate honorific japanese;text correct honorifics;form honorifics japanese;honorifics japanese including;honorific japanese;expressions honorifics;\u5e38\u8a9e honorific speech;correct honorifics mechanism;expressions appropriate honorific;speech \u5e38\u8a9e honorific;honorific speech honorific;honorifics mechanism honorifics;honorific japanese examine;speech honorific;honorific speech;speech honorific speech;proper form honorifics;honorifics mechanism;automatic translating;automatic translating learners;correct honorifics;\u5e38\u8a9e honorific;japanese including automatic;honorifics", "pdf_keywords": ""}, "2a462e2b748d7e78f3af2621071265c1ad2683ea": {"ta_keywords": "dispatchable wind energy;optimal power flow;wind energy resource;wind parks operation;wind parks independent;integration dispatchable wind;consideration wind parks;power producers ipp;wind energy;power flow problem;parks operation optimal;power flow;dispatchable wind;parks independent power;operation optimal power;wind parks;independent power producers;energy resource power;optimal power;presence wind parks;modification optimal power;power producers;producers ipp operation;ipp operation;consideration wind;energy resource;ipp operation large;parks operation;operation optimal;described consideration wind", "pdf_keywords": ""}, "b61c9799f10e4de9cd222dfd8e423bbd950a7c44": {"ta_keywords": "extractive qa recognizing;extractive qa task;qa recognizing textual;dataset extractive qa;recognizing textual entailments;studying unanswerable questions;unanswerable questions analyze;textual entailments task;unanswerable questions squad;recognizing textual;entailments task rte;phenomenon extractive qa;extractive qa;qa recognizing;natural language understanding;squad dataset extractive;textual entailments;task unanswerable questions;build domain corpus;domain corpus;entailments task;qa task unanswerable;idk phenomenon extractive;questions analyze;domain corpus focusing;studying unanswerable;questions squad;enriching squad dataset;questions distinguish;questions analyze similarities", "pdf_keywords": "idk extractive qa;current nlp systems;questions idk extractive;extractive qa task;idk competitive extractive;current nlp;challenge current nlp;nlp systems;trained squad q2;competitive extractive qa;extractive qa;eventbased questions rte;questions rte;rte dataset;idk extractive;nlp systems answer;use rte dataset;qa major challenge;squad q2;questions rte data;answer idk competitive;qa task;extractive qa major;nlp;systems answer idk;explore use rte;rte dataset observe;use rte;idk competitive;rte"}, "4ebe5792fe2890590e7a5bf8ae0a29e0fb147ef9": {"ta_keywords": "utterance rewriting semantic;rewriting semantic segmentation;incomplete utterance rewriting;utterance rewriting;utterance rewriting raised;semantic segmentation task;word level edit;rewriting semantic;incomplete utterance;task incomplete utterance;semantic segmentation;formulates semantic segmentation;semantic segmentation benefiting;segmentation task instead;prediction word level;utterance;segmentation task;segmentation benefiting;segmentation;edit operations;level edit matrix;edit matrix;segmentation benefiting able;rewriting;semantic;word level;introduces edit operations;level edit;rewriting raised;formulates semantic", "pdf_keywords": "utterance rewriting semantic;rewriting semantic segmentation;incomplete utterance rewriting;utterance rewriting;utterance rewriting raised;formulates semantic segmentation;semantic segmentation qian;semantic segmentation task;semantic segmentation1;rewriting semantic;iur semantic segmentation1;semantic segmentation;incomplete utterance;task incomplete utterance;semantic segmentation1 paper;segmentation qian liu;utterance;formulates incomplete utterance;segmentation task furthermore;segmentation qian;segmentation;segmentation task;faster inference speed;faster inference;segmentation1;semantic;segmentation1 paper present;word level edit;formulates semantic;predicts edit operations"}, "e6fe601c44835d3654131d0312d65227d3523373": {"ta_keywords": "structured text networks;text corpus labeled;corpus labeled directed;representing text corpus;text corpus;walk structured text;text networks;corpus labeled;labeled directed graph;graph based similarity;corpus;words use supervised;dependency parsing;walks derive similarity;derived dependency parsing;similarity measure words;nodes represent words;structured text;words weighted edges;task learning walk;parsing given graph;dependency parsing given;labeled directed;based similarity measure;similarity measure based;directed graph nodes;learning walk structured;represent words weighted;derived similarity measure;similarity measure", "pdf_keywords": ""}, "5a4d4c0824b5e113c39105c71d42b93d3900d87e": {"ta_keywords": "crowdsourcing engine;persistence crowdsourcing engine;crowdsourcing engine mechanized;crowdsourcing process;crowdsourcing process including;entire crowdsourcing process;data persistence crowdsourcing;persistence crowdsourcing;crowdsourcing;amazon mechanical turk;entire crowdsourcing;mechanical turk;mechanical turk used;controls entire crowdsourcing;annotation process;annotation implemented;controlling annotation process;annotation;annotation process database;tasks human workers;turk used submit;service controlling annotation;controlling annotation implemented;methods controlling annotation;controlling annotation;task allocation worker;tasks human;semantic similarity assessment;user worker interface;worker interface engine", "pdf_keywords": ""}, "e2b6193a24cd6c9f736139aa66618d1b8bf2a60b": {"ta_keywords": "cost sensitive transcription;speech transcription tool;correction speech transcripts;transcriber speech transcription;speech transcription;transcription tool;manual correction speech;speech transcripts;transcription tool adapts;guides transcription;transcription tool targeted;present speech transcription;guides transcription process;sensitive transcription;transcription;transcriber speech;speech transcripts present;sensitive transcription paper;transcription paper;actively guides transcription;transcripts present speech;sesla transcriber speech;automatically created transcript;transcription paper describes;correction speech;transcripts;efficient manual correction;transcriber;transcript;transcription process taking", "pdf_keywords": ""}, "fc848789b557a7581c51c79fd01897dc5aa7e8a8": {"ta_keywords": "knowledge augmented transformer;model knowledge augmented;relevance retrieved knowledge;explicit knowledge end;answer prediction;knowledge augmented;novel model knowledge;retrieved knowledge;knowledge end end;jointly reasoning knowledge;model knowledge;explicit knowledge integrated;paradigm knowledge retrieval;knowledge retrieval;knowledge end;explicit knowledge integration;domain multimodal task;explicit knowledge;answer prediction leave;answer generation;multimodal task;followed answer prediction;implicit explicit knowledge;retrieved knowledge used;knowledge retrieval followed;encoder;multimodal task ok;end encoder decoder;reasoning knowledge;end encoder", "pdf_keywords": "explicit knowledge retriever;explicit knowledge structured;explicit knowledge reasoning;extracted knowledge implicit;knowledge structured;implicit explicit knowledge;explicit implicit knowledge;knowledge answer generation;explicit knowledge design;knowledge structured knowledge;model explicit knowledge;explicit knowledge;models explicit knowledge;knowledge implicit knowledge;knowledge extraction signi\ufb01cantly;based explicit knowledge;structured knowledge;knowledge retriever;implicit knowledge answer;model retrieved knowledge;retrieved knowledge;knowledge implicit;implicit knowledge design;extracted knowledge;novel reasoning module;answer generation trained;structured knowledge bases;implicit knowledge;relevance extracted knowledge;knowledge reasoning"}, "db729f2f55a92465cf88682ba7917621fd4c000b": {"ta_keywords": "tutor explanations study;students self explanation;student tutor explanations;tutor explanations;tutor learning using;tutor learning;effect tutor learning;using teachable agent;teachable agent asks;teachable agent;agent asks student;tutor;student tutor;explanations study;tutored time;asks student tutor;learner allows students;studying effect tutor;peer learner allows;learning using teachable;effect tutor;tutored;peer learner;non self explanation;showed students self;applied peer learner;allows students learn;learning applied peer;self explanation;explanations study showed", "pdf_keywords": ""}, "b2da0f022a48ebd10a23572b5310b7d7341b6448": {"ta_keywords": "\u56fd\u4f1a\u97f3\u58f0\u8a8d\u8b58\u30b7\u30b9\u30c6\u30e0\u306e\u97f3\u97ff \u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u534a\u81ea\u52d5\u66f4\u65b0;\u56fd\u4f1a\u97f3\u58f0\u8a8d\u8b58\u30b7\u30b9\u30c6\u30e0\u306e\u97f3\u97ff \u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u534a\u81ea\u52d5\u66f4\u65b0 \u97f3\u58f0;\u56fd\u4f1a\u97f3\u58f0\u8a8d\u8b58\u30b7\u30b9\u30c6\u30e0\u306e\u97f3\u97ff;\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u534a\u81ea\u52d5\u66f4\u65b0 \u97f3\u58f0;\u8a00\u8a9e\u30e2\u30c7\u30eb\u306e\u534a\u81ea\u52d5\u66f4\u65b0;\u97f3\u58f0", "pdf_keywords": ""}, "004ddf5a39a735d0f8ec7547629c2bee65eb1f93": {"ta_keywords": "reviewers induces biases;biases peer reviewing;biases scholarly;biases scholarly research;reviewers induces;identities reviewers induces;biases peer;author identities reviewers;issue biases scholarly;peer reviewing;identities reviewers;reviewer calibration;biases;mismatch reviewer calibration;peer reviewing wsdm;peer review;reviewer calibration consider;existence biases peer;reviewers;biases certain;induces biases;biases certain groups;bias;induces biases certain;bias fact;mismatch reviewer;biases set results;specifically peer review;issue biases;bias fact absent", "pdf_keywords": "testing biases;testing biases design;tests detect biases;statistical tests detect;approach testing biases;detect biases;testing biases sections;biases design statistical;design statistical tests;design hypothesis tests;detect biases problem;tests detect;tests minimal assumptions;statistical tests;alarm probability correlations;false alarm probability;non parametric tests;hypothesis tests minimal;control false alarm;hypothesis tests;statistical test improve;tests minimal;error rate test;design statistical procedures;bias fact;biases design;statistical procedures;bias fact absent;statistical procedures coupled;testing"}, "1bd7d16340642948142d7608ef8f085d934d94a3": {"ta_keywords": "policy gradient methods;derivative free optimization;free optimization;algorithms policy gradient;optimization algorithms policy;policy gradient;learning continuous control;free optimization algorithms;unconstrained minimization smooth;unconstrained minimization;minimization smooth objective;gradient methods;smooth objective function;stochastic point method;gradient methods particular;momentum version stochastic;minimization smooth;outperforms stp methods;strongly convex functions;learning continuous;smooth objective;state art derivative;optimization;problem unconstrained minimization;optimization algorithms;continuous control tasks;strongly convex;non convex convex;convex functions;non convex", "pdf_keywords": "free optimization method;derivative free optimization;optimization method momentum;unconstrained minimization smooth;free optimization;unconstrained minimization;minimization smooth objective;smooth objective function;stochastic derivative free;minimization smooth;problem unconstrained minimization;smooth objective;method momentum;stochastic point method;method momentum eduard;momentum version stochastic;optimization method;objective function;zeroth order method;unconstrained;objective function rd;stochastic zeroth order;point method stp;optimization;derivative free;minimization;stochastic zeroth;method stp bergou;analyze stochastic zeroth;ball momentum particular"}, "db8376698c06d6a688a39bff0300780ef0383821": {"ta_keywords": "decentralized control method;decentralized control;local control;corrugated boards;corrugated board;corrugated board make;line manufacturing corrugated;wail corrugated boards;manufacturing corrugated;main local control;double wail corrugated;corrugated;manufacturing corrugated cardboard;faced corrugated board;local control unit;wail corrugated;corrugated boards cut;liner corrugated;single faced corrugated;corrugated cardboard;corrugated board pastes;lengths local control;pastes liner corrugated;decentralized;corrugated medium make;method line manufacturing;corrugated cardboard pastes;faced corrugated;liner corrugated medium;control method", "pdf_keywords": ""}, "4b1555368fd2c5f1234eaac5e41296003481754a": {"ta_keywords": "lithium oxygen battery;performance lithium oxygen;anode lithium oxygen;deep eutectic electrolyte;lithium oxygen;eutectic electrolyte high;eutectic electrolyte dee;eutectic electrolyte;high reactivity lithium;oxygen battery lob;high performance lithium;nma lithium bis;electrolyte high performance;stable electrolyte;lithium metal anode;electrolyte simultaneously withstand;nma lithium;metal anode lithium;anode lithium;oxidizing environment battery;compatibility lithium metal;electrolyte high;oxygen battery;stable electrolyte simultaneously;lithium bis;performance lithium;electrolyte dee;methylacetamide nma lithium;electrochemical stability good;electrolyte", "pdf_keywords": ""}, "005879e6587eb6e05f56c20d345f784ee84a44c4": {"ta_keywords": "models improve parsing;syntax generative models;dependency syntax generative;syntactic lstm language;parsing language modeling;language models recurrent;syntactic lstm;non syntactic lstm;lstm language models;neural network grammars;generative dependency models;recurrent neural nets;syntax generative;language models;grammars generate sentences;parsing language;models recurrent neural;lstm language;language modeling;perform parsing language;dependency models similarly;generative models dependency;models dependency syntax;improve parsing;grammars generate;dependency models;generate sentences;improve parsing performance;models use recurrent;models recurrent", "pdf_keywords": ""}, "f61862b286c9e4894302faf716eedb0eb60a2f5f": {"ta_keywords": "newsgroup style conversations;presentation conversational data;conversational data;structure discussion forums;thread structure newsgroup;conversational data contribution;style conversations;thread structure discussion;discussion forums explicit;line discussions composed;discussions composed multiple;isolate discussion related;discussions composed;recovering thread structure;style conversations prior;conversations;related particular conversational;discussion forums;presentation conversational;implicit thread structure;forums explicit meta;thread structure reconstructed;discussion related specific;inter message similarity;representation presentation conversational;recovering implicit thread;isolate discussion;conversations prior work;conversations prior;message similarity", "pdf_keywords": ""}, "20819855b9517c927a1262850146e525c8083fb4": {"ta_keywords": "utterance model trained;term memory lstm;short term memory;dialog concepts spoken;human dialog data;model words utterance;utterance context dialog;utterance model;memory lstm recurrent;recurrent neural networks;spoken word sequences;dialog concepts;context sensitive lstms;human dialog;human human dialog;end utterance model;memory lstm;term memory;lstm recurrent;sequences dialog concepts;dialog data annotated;predict sequences dialog;lstm recurrent neural;lstms lstm;intentions accurately dialog;lstm;lr lstms lstm;accurately dialog;dialog data;sequence dialog", "pdf_keywords": ""}, "f74d4fa99ccedfea7a2662fe6944d99f34533912": {"ta_keywords": "models improve fairness;fairness machine learning;improve fairness model;fairness mitigate discriminated;fairness methods improve;existing fairness methods;accuracy fairness mitigate;fairness model agnostic;fairness methods;accuracy fairness method;fairness constraints;fairness model;optimize multiple fairness;fairness machine;classification thresholds demographic;accuracy fairness;mitigate discriminated model;fairness constraints group;model output fairness;improve fairness;multiple fairness constraints;process existing fairness;adaptive classification thresholds;fairness method;fairness mitigate;classification thresholds;existing fairness;mitigate discriminated;group aware threshold;discriminated model", "pdf_keywords": "models improve fairness;fairness machine learning;mitigate discriminated model;adaptive classi\ufb01cation thresholds;improve fairness model;fairness model agnostic;discriminated model;discriminated model behaviors;fairness model;fairness constraints group;existing fairness methods;classi\ufb01cation thresholds;fairness methods improve;threshold adaptation fair;classi\ufb01cation thresholds demographic;group aware threshold;fairness methods;abstract fairness machine;fairness machine;fairness constraints;optimize multiple fairness;mitigate discriminated;theoretical accuracy fairness;multiple fairness constraints;process existing fairness;adaptation fair classi\ufb01cation;classi\ufb01cation models improve;accuracy fairness method;fairness method;improve fairness"}, "25fec1e150a273b3bec3655ace0ff6b97c338a96": {"ta_keywords": "regenerating codes resilient;errors erasures codes;erasures codes optimal;resilient errors erasures;storage systems code;errors erasures nodes;regenerating codes;codes resilient errors;operations regenerating codes;explicit regenerating codes;erasures codes;resilience regenerating code;data efficient repair;codes resilient;node repair operations;regenerating code;erasures nodes;nodes distributed storage;regenerating codes class;handling errors erasures;repair failed nodes;node repair;errors erasures;erasures nodes links;distributed storage systems;reconstruction node repair;matrix codes provide;distributed storage;resilient errors;matrix codes", "pdf_keywords": "erasures distributed storage;distributed storage networks;erasures multicast network;erasures multicast;errors erasures multicast;erasure capacity network;multicast network coding;storage networks;distributed storage consisting;distributed storage;consider distributed storage;storage nodes network;distributed storage systems;errors erasures distributed;storage networks using;erasures distributed;class distributed storage;error erasure capacity;multicast networks;storage nodes;network coding studied;networks error erasure;network coding;erasure capacity;network achieved codes;outer bounds storage;consisting storage nodes;capacity class distributed;non multicast networks;multicast network"}, "2875d6cac905c3dfec4c8a8e1d89a2a7e4d71d40": {"ta_keywords": "distributed storage codes;consider distributed storage;distributed storage systems;distributed storage;distributed storage inline;storage codes functional;wireless distributed storage;design distributed storage;storage systems consider;storage systems;storage codes;storage inline;functional broadcast repair;bandwidth partial repair;failures wireless distributed;storage;systems consider distributed;storage capacity repair;capacity repair bandwidth;partially failed nodes;codes functional broadcast;failed nodes based;failed nodes propose;surviving data local;trade storage;storage inline formula;wireless distributed;distributed;repair bandwidth;failed nodes satisfies", "pdf_keywords": "optimal storage repair;storage repair bandwidth;functional broadcast repair;distributed storage nodes;explicit storage repair;partially failed nodes;broadcast repair multiple;node failures broadcast;failures wireless distributed;repair multiple node;node repair present;storage nodes;node repair;broadcast repair;storage repair;storage repair framework;consider distributed storage;distributed storage systems;wireless distributed storage;distributed storage;failed nodes prove;trade node repair;failed nodes;failures broadcast;node failures;multiple node failures;partial failures wireless;optimal functional repair;failures broadcast setting;repair bandwidth"}, "190865e2c3d4908ff20bf9a31f5a2773d6fec5cb": {"ta_keywords": "question retrieval augmenting;domain question answering;question answering;question answering odqa;step question retrieval;question retrieval;stored question answer;question answering unlike;retrieval augmenting pre;retrieval augmenting;generated question answer;language models qa;answering unlike prior;models qa memory;large text corpus;question answer pairs;retrieval;trained language models;answering;corpus knowledge base;answering odqa;answering odqa gener;qa memory;open book;language models;text corpus knowledge;open book approach;answer pairs appealing;corpus knowledge;knowledge base", "pdf_keywords": "question retrievalbased qa;retrievalbased qa knowledge;question retrievalbased;question retrieval furthermore;question retrieval;retrievalbased qa;retrieval furthermore qa;step question retrieval;questions retrieval;\ufb01rst question retrievalbased;question answer corpus;new qa memoryaugmented;hop questions retrieval;memory qa;qa memoryaugmented;answer corpus;memory qa pairs;retrieval;qa memoryaugmented transformer;retrievalbased;retrieval question answer;questions retrieval question;retrieval furthermore;memoryaugmented transformer qamat;qa knowledge addresses;batch memory qa;new qa augments;qa knowledge;qa pair encoder;retrieval question"}, "4dc8ddef938699d0d8a0b685ad9f56d0b735a25d": {"ta_keywords": "learns incrementally;learns incrementally ia;learning algorithm incremental;explanation based learning;knowledge intensive inductive;ebl learns incrementally;inductive learning;inductive learning algorithm;concept learner;intensive inductive learning;concept learner ia;cover based learning;learnability finally;learning longer easily;learnability;learnability finally compared;based learning algorithm;bounded learnability finally;learning algorithm called;alternative learning algorithm;bounded learnability;mistake bounded learnability;learning algorithm ebl;learns;based learning ebl;learning longer;based learning;learning algorithm;learning;ebl learns", "pdf_keywords": ""}, "5677d2b565c8265fef1693a9be861739cb01bf2f": {"ta_keywords": "large vocabulary speech;vocabulary speech recognition;parameters large vocabulary;speech recognition;configuration large vocabulary;speech recognition systems;evolution optimized;deep neural network;model deep neural;markov model deep;objective pareto optimization;result evolution optimized;optimized low word;optimized;optimized result evolution;pareto optimization;highest recognition performance;recognition performance numerous;deep neural;multi objective pareto;vocabulary speech;speech recognition using;evolution optimized low;meta parameters large;model deep;recognition systems automatically;optimization state art;large vocabulary;recognition performance;large computation", "pdf_keywords": ""}, "3d2da57c2de69b02fa0fee5c12ace618718a3926": {"ta_keywords": "subcellular location image;meta learning;associating sub images;model meta learning;sub images;captions associating information;labels sentences stacked;meta learning scheme;sentences stacked graphical;images sub captions;images sub;matching sub figures;stacked graphical model;sub captions associating;text images journal;combination text images;images journal articles;sub images sub;information text image;text images;sub captions;image finder;sub figures;labels sub figures;sentences stacked;images journal;introduce stacked graphical;location image finder;captions associating;learning", "pdf_keywords": ""}, "d21a0e01514732f241b9c138eceb76ecaef17a27": {"ta_keywords": "effective machine translation;better translation models;machine translation;translation models effort;translation models;segments active learning;mt active learning;particularly annotators translate;active learning;active learning mt;train better translation;machine translation mt;learning mt active;annotators translate short;translate short phrases;extensive manual translation;annotators translate;manual translation professional;active learning framework;translation professional translators;manual translation;translators proposed method;words allowing translators;professional translators;translators confident translations1;translators;allowing translators confident;short phrases instead;translators proposed;translated words", "pdf_keywords": ""}, "b4bc1a98eb79545f8da4385a6dfb643b0c62a07e": {"ta_keywords": "simultaneous translation prediction;translation prediction unseen;smt simultaneous translation;translation prediction;translation syntax based;machine translation;communication machine translation;performing translation syntax;based simultaneous translation;translation simultaneous translation;prediction unseen syntactic;predict unseen syntactic;simultaneous translation propose;translation syntax;machine translation mt;simultaneous translation;syntax based smt;fluent translation simultaneous;simultaneous translation method;translation proposed methods;generate fluent translation;performing translation;translation simultaneous;translation propose methods;translation method;unseen syntactic;order target sentences;complete parse trees;segments performing translation;complete parse", "pdf_keywords": ""}, "425a4a9c0598e4101ca2f2b930f5c6986ce40a99": {"ta_keywords": "privacy regularization joint;privacy regularization;privacy preserving regularization;users privacy regularization;regularization joint privacy;privacy utility optimization;optimization utility privacy;privacy use discriminator;differential privacy;training samples privacy;term differential privacy;models privacy guarantees;privacy preserving;models privacy;introduce privacy preserving;train models privacy;privacy utility;joint privacy utility;differential privacy dp;utility privacy;privacy guarantees;regularization methods training;samples privacy;privacy;users privacy;utility privacy use;samples privacy im;joint privacy;privacy guarantees comes;regularization", "pdf_keywords": "privacypreserving regularization methods;privacypreserving regularization;introduce privacypreserving regularization;training differential privacy;propose privacy regularization;privacy regularization;privacy regularization methods;optimization utility privacy;jointly optimize privacy;optimize privacy utility;privacy utility language;privacy use discriminator;optimize privacy;training novel privacy;differential privacy;introduce privacypreserving;models privacy guarantees;privacypreserving;privacy compared differentially;privacy utility;privacy mitigation methods;models privacy;privacy mitigation;privacy utility extensive;utility privacy;novel privacy loss;differentially private model;privacy guarantees;compared differentially private;private model training"}, "4383e714f4535777ffb7b4f618d4ccede4b08bd3": {"ta_keywords": "multilingual allophone database;languages universal allophone;multilingual allophone;phonetic transcriptions;models output phonetic;output phonetic transcriptions;allovera multilingual allophone;allophone database allovera;transcriptions international phonetic;phonetic transcriptions international;allophone database;allophones phonemes;language specific phonetic;output phonetic;phonetic representations;specific phonetic representations;phonemic models language;models speech transcription;international phonetic;allophones phonemes 14;specific phonetic;allophone model allosaurus;phonetic representations stated;speech transcription;transcription task phonemic;mappings 218 allophones;universal allophone;218 allophones phonemes;phonemic representations language;speech transcription task", "pdf_keywords": "multilingual allophone;multilingual allophone database;languages universal allophone;allovera multilingual allophone;allophones phonemes;allophone phonetic;allophones phonemes 14;allophone phonetic realization;phoneme allophone;218 allophones phonemes;database phoneme allophone;mappings 218 allophones;phonemes 14 languages;pairs allophone phonetic;phoneme allophone pairs;allophone database;allophone database david;phoneme 14 languages;allophones;languages universal;allovera multilingual;allophone;allophone pairs;phonetic realization phoneme;multilingual;allophone pairs allophone;universal allophone;minority languages universal;phonemic models language;endangered minority languages"}, "c549b3f2d262efc1f68dfdd842174634f37519ed": {"ta_keywords": "adaptation techniques speech;incremental adaptation;incremental adaptation techniques;novel incremental adaptation;incremental adaptation framework;adaptation using time;utterances proposed incremental;changes speaker speaking;map incremental adaptation;predictor corrector adaptation;speech recognition;adaptation classifier;adaptation classifier parameters;direct adaptation classifier;changes speaker;acoustic models time;mllr direct adaptation;speech recognition aimed;adjusting acoustic models;corrector adaptation using;time variant acoustic;corrector adaptation;adaptation transformation parameters;adaptation framework;adaptation using;techniques speech recognition;adaptation transformation;adaptation techniques;indirect adaptation transformation;acoustic models", "pdf_keywords": ""}, "40f4d7fe800810288a80f84cdb357a8f4c28e880": {"ta_keywords": "cnns depth;dimensions vision transformers;increases conventional cnn;cnns depth increases;design principles cnn;principle cnns depth;vision transformers;conventional cnn increases;conventional cnn;reduction principle cnns;principle cnns;networks cnn;vision transformer vit;vision transformer;cnns;networks cnn particularly;vision transformers successful;cnn increases channel;spatial dimensions vision;architecture existing convolutional;cnn increases;cnn;neural networks cnn;dimensions vision;convolutional neural;existing convolutional neural;principles cnn;based vision transformer;convolutional neural networks;pooling based vision", "pdf_keywords": "pooling convolution strides;cnn architectures;cnns spatial;dimensions vision transformers;cnns spatial dimensional;design principles cnn;vision transformers;cnn architectures investigate;pooling layer transformer;architecture existing convolutional;abstract vision transformer;used cnns spatial;vision transformer;principles cnn architectures;convolution strides considered;convolution strides;vision transformer vit;neural networks cnn;layer transformer architecture;layer transformer;networks cnn;strides considered transformer;transformers language processing;pooling convolution;cnns;performed pooling convolution;convolutional neural;transformer architecture;vision transformers byeongho;existing convolutional neural"}, "8a73eed98873d91086201f41c6e1f613fcdefe18": {"ta_keywords": "self supervised speech;supervised speech;tts self supervised;supervised speech recognition;speech sending asr;speech recognition;asr tts self;self supervised training;features asr tts;context synthesized speech;self supervised;attention context synthesized;enhanced asr tts;synthesized speech;features asr;supervised self supervised;tts asr;enhanced asr;propose enhanced asr;tts tts asr;hypotheses forwarding tts;supervised self;tts asr direction;asr tts;asr tts direction;main features asr;asr tts eat;asr hypotheses forwarding;supervised training significantly;synthesized speech sending", "pdf_keywords": "self supervised speech;self supervised asr;supervised asr tts;tts self supervised;supervised speech;supervised speech recognition;supervised asr;speech sending asr;self supervised training;speech recognition;asr tts models;sequence architecture asr;asr tts self;features asr tts;self supervised;perform self supervised;architecture asr tts;tts asr;supervised self supervised;abstract self supervised;features asr;tts tts asr;hypotheses forwarding tts;asr tts;supervised self;enhanced asr tts;language model reward;propose enhanced asr;speech recognition murali;asr hypotheses forwarding"}, "42be8ed9973b3326a6e3d838c4742bc1d7704704": {"ta_keywords": "controllable speech modification;speech modification based;articulatory controllable speech;novel speech modification;speech modification method;mapping articulatory acoustic;modifying phonemic sounds;modify phonemic sounds;speech modification;acousticto articulatory inversion;manipulating estimated articulatory;applied acousticto articulatory;controllable speech;articulatory parameters based;manipulating articulatory parameters;articulatory acoustic production;acousticto articulatory;estimated input speech;manipulating articulatory;input speech signal;articulatory acoustic;sounds input speech;articulatory inversion mapping;phonemic sounds manipulating;articulatory parameters estimated;phonemic sounds input;input speech;mapping gaussian mixture;mapping separately articulatory;inversion mapping articulatory", "pdf_keywords": ""}, "7c0ada3511b05897fb4d75c5f657b5fbd953caa8": {"ta_keywords": "voter biases;voter biases detecting;avoid voter biases;biases online platforms;biases reputation;reputation bias;voter biases making;mitigate voter biases;underestimates reputation bias;impression signals votes;influence votes;social influence votes;different biases reputation;social influence bias;influence bias;biases online;impression signals reputation;influence votes far;biases reputation poster;websites shows bias;reputation bias 2x;forms biases online;vote content empirical;signals reputation;signals reputation contributing;biases;bias;bias estimates;biases detecting;shows bias", "pdf_keywords": "platforms quantifying voter;biases online platforms;voter biases online;quantifying voter biases;quantifying voter bias;voter bias quantification;online platforms quantifying;method voter bias;voter biases detecting;voter bias;quantifying voter;voter biases;models quantifying voter;avoid voter biases;measuring content quality;biases online;voter bias section;degree voter biases;voter biases making;2019 quantifying voter;platforms quantifying;mitigate voter biases;based online platforms;reputation bias;content quality implications;content quality;forms biases online;online platforms;online platforms use;biases reputation"}, "e79be3f9ce409f1a9b7084ef880298665e5212d0": {"ta_keywords": "aware cascade contrastive;cascade contrastive learning;token aware contrastive;aware contrastive loss;text video retrieval;improves contrastive learning;aware contrastive;crosstask video action;cascade contrastive;video retrieval youcook2;contrastive learning;video retrieval;contrastive loss;improves contrastive;contrastive learning using;crosstask video;localization crosstask video;text video;activitynet video action;video action segmentation;video function words;video text;video retrieval benchmarks;activitynet video;contrastive learning taco;observation video text;visual contents video;action segmentation;contrastive loss computed;vtt activitynet video", "pdf_keywords": "learning video text;text video retrieval;learning video;video retrieval youcook2;crosstask video action;video action segmentation;improve video text;method learning video;localization crosstask video;contrastive learning token;activitynet video action;video text alignment;text video;video retrieval;contrastive learning pipeline;aware cascade contrastive;activitynet video;cascade contrastive learning;video text;video retrieval benchmarks;crosstask video;action segmentation;vtt activitynet video;taco improve video;improves contrastive learning;video action;learning token aware;effective contrastive learning;video action step;video language"}, "e40a5c25d39d0f9add6a26c82613cf29edbcccf5": {"ta_keywords": "channel eegs;eeg device;eeg device investigate;user identification performance;grade eeg device;consumer grade eeg;eegs 25 subjects;analyze brain waves;14 channel eegs;brain waves;identification authentication experimental;eegs;grade eeg;user identification authentication;dimensionality reduction technique;channel eegs 25;comparing user identification;user identification;identification performance;brain waves acquired;eeg;dimensionality reduction;identification accuracy;identification performance various;identification authentication;potential erp data;eegs 25;algorithm analyze brain;erp epoch;related potential erp", "pdf_keywords": ""}, "92bd9e8a83e82dbbcafd8cde4f5a42d7bb4a5859": {"ta_keywords": "transforming individuality using;statistical machine translation;model transforming individuality;machine translation;individuality writer speaker;method transforming individuality;transforming individuality;analysis individuality transforming;individuality transforming;language model adaptation;transforming individuality considers;express individuality writer;individuality writer;adaptation analysis individuality;individuality using;paraphrasing characteristic words;machine translation smt;individuality using technique;paraphrasing characteristic;features express individuality;express individuality;individuality transforming \u6c34\u4e0a;gram clustering language;clustering language model;gram clustering;method paraphrasing characteristic;using gram clustering;clustering language;individuality;paraphrasing", "pdf_keywords": ""}, "d1d23675d2e65cd734f2955c10ec1028b1139b5b": {"ta_keywords": "neural machine translation;translation semantic similarity;machine translation semantic;machine translation;better translations evaluated;translation accuracy bleu;translation semantic;results better translations;translation accuracy;machine translation nmt;final translation accuracy;translations evaluated bleu;translations evaluated;semantic similarity training;translations;improve final translation;better translations;translation nmt systems;bleu semantic similarity;similarity training bleu;evaluated bleu semantic;translation nmt;semantic similarity human;bleu semantic;translation;similarity training;bleu training neural;evaluation metrics bleu;trans lated english;disparate languages trans", "pdf_keywords": "translation semantic similarity;simile semantic similarity;measure semantic similarity;training machine translation;semantic similarity generated;semantic similarity;machine translation semantic;measuring machine translation;semantic similarity model;semantic similarity exact;translation semantic;simile semantic;designed simile semantic;machine translation;translations evaluated embed;similar translations;heavier semantic;heavier semantic content;translating words heavier;corpus paraphrase data;distinctions similar translations;external corpus paraphrase;training using embedding;large external corpus;similarity model high;words heavier semantic;machine translation 5simile;simile alternative;similarity generated;reference translations"}, "a538a05864a23e2f80f9b003d5ecbdfb8025b954": {"ta_keywords": "twitter stance detection;stance detection twitter;classify stance tweet;stance detection weakly;stance tweet target;target stance detection;stance detection goal;stance tweet;stance detection;classify stance;2016 twitter stance;twitter stance;detection twitter autoencoders;realistic stance detection;feature representations tweets;goal classify stance;representations tweets;detection twitter;tweets containing targets;twitter autoencoders;tweets containing;task target stance;detection weakly supervised;unlabelled tweets containing;stance;target stance;tweet target favor;unlabelled tweets;semeval 2016 twitter;weakly supervised task", "pdf_keywords": ""}, "3376118362db3751cfbd88acd0c090b8a3897733": {"ta_keywords": "language models plms;improving bert interpretations;plm focusing semantic;bert interpretations complex;bert example plm;words derivational morphology;plms affect generalization;semantic representations derivationally;generalization new words;language models;pretrained language models;interpretations complex words;words derivational;bert interpretations;complex words stored;complex words derivational;derivationally complex words;semantic representations;segmentation pretrained language;meanings complex words;focusing semantic representations;subwords implies maximally;complex words;example plm;subwords;improving bert;taking bert example;bert example;semantic;derivational morphology", "pdf_keywords": "bert affects interpretations;bert wordpiece segmentation;outperforms bert wordpiece;bert example plm;plms speci\ufb01cally bert;bert using wordpiece;bert wordpiece;plm focusing semantic;taking bert example;generalization new words;bert example;words plms interpreted;speci\ufb01cally bert;substantially outperforms bert;semantic representations english;semantic representations;outperformed bert using;wordpiece segmentation;wordpiece segmentation present;speci\ufb01cally bert affects;using wordpiece segmentation;bert using;complex words stored;focusing semantic representations;outperforms bert;words plms;new words plms;series semantic probing;subwords;semantic probing"}, "1392df13a80a962057e979a294a850a50b7deb7e": {"ta_keywords": "automatically segmenting corpus;supervised language annotation;segmentation optimizes supervision;segmenting corpus chunks;automatic annotations;automatic annotations natural;correcting automatic annotations;annotations natural language;supervised annotator;segmenting corpus;efficient supervised language;supervised language;supervised annotator useful;automatically segmenting;word segmentation;language annotation explicit;labels supervised annotator;language annotation;transcription word segmentation;natural language efficient;segmentation efficient supervised;confident labels supervised;annotator useful labels;annotation explicit;annotation explicit cost;annotations natural;method automatically segmenting;annotation;annotations;annotator useful", "pdf_keywords": ""}, "8df3f3f72eb239eb212bd3fc929bd754ce2e03d6": {"ta_keywords": "scientific articles proteins;articles proteins related;related papers proteins;proteins related topic;articles yeast protein;articles proteins;protein interaction networks;topic coherence improves;topic coherence emergent;modeling pubmed articles;biology literature protein;identifying related papers;lda yeast biology;emergent topics ability;topics ability model;pubmed articles yeast;jointly modeling pubmed;protein interaction matrix;articles yeast;papers proteins;literature protein;literature protein protein;proteins related;protein protein interaction;topic coherence;biology domain jointly;protein interaction;lda yeast;yeast protein protein;emergent topics", "pdf_keywords": ""}, "aae3d5e24d02ae538030ef3995a86118c5323ae1": {"ta_keywords": "cluster storage systems;cluster storage;improving storage efficiency;storage systems;ions enhance programmability;storage efficiency exploiting;improving storage;storage efficiency;storage systems gotta;exploiting disk reliability;storage;cluster;reliability heterogeneity ions;disk reliability;efficiency exploiting disk;disk reliability heterogeneity;ions;enhance programmability portability;heart improving storage;ions enhance;heterogeneity ions enhance;portability;programmability portability;heterogeneity ions;disk;exploiting disk;enhance programmability;reliability;programmability;systems", "pdf_keywords": ""}, "148bca569a0d2833f05df1297788f64bc6686fa8": {"ta_keywords": "nlp systems temporal;temporal misalignment nlp;nlp model trained;temporal adaptation continued;misalignment modern nlp;nlp systems;modern nlp systems;modern nlp;continued domainspecific pretraining;nlp model;suite diverse tasks;diverse tasks;domains study temporal;pretraining followed task;systems temporal adaptation;temporal adaptation;nlp;diverse tasks different;trained text data;misalignment nlp model;trained text;model trained text;adaptation continued;news reviews periods;adaptation continued pretraining;task specific finetuning;misalignment nlp;end task performance;text data time;tasks different", "pdf_keywords": "nlp model performance;affects nlp model;nlp systems facilitate;misalignment affects nlp;affects nlp;summarizes temporal misalignment;misalignment modern nlp;temporally selected unannotated;domain adaptation language;nlp systems;language model generalization;summarizes temporal;adaptation language model;domains study temporal;modern nlp systems;affects language model;nlp model;tasks temporal;metric summarizes temporal;nlp;downstream tasks temporal;modern nlp;model task timestamped;curtail temporal;task timestamped;generalization task performance;task timestamped data;tasks temporal \ufb01netuning;adaptation language;curtail temporal misalignment"}, "bb80f7d2269308c3e91da8c47b290645e9d3d7d5": {"ta_keywords": "modeling human poses;motions movemes training;learning rotation;learning rotation invariant;discovery static poses;projected poses;human motions;human motions movemes;model moveme discovery;poses characterize;activity classification inference;human poses sports;poses sports;poses characterize manifold;poses;human poses;rotation invariant latent;sports dataset demonstrate;projected poses obtained;bases poses;primitive human motions;activity classification;problem learning rotation;static poses;set projected poses;motions movemes;factor model training;sports dataset;poses obtained;set bases poses", "pdf_keywords": "modeling human poses;dimensional bases poses;learning rotation;learning rotation invariant;poses training;discovery static poses;poses training set;bases poses training;model moveme discovery;rotation invariant latent;poses synthetic;poses;manifold human motion;used bases poses;bases poses;human poses;poses mapped;poses sports;human poses sports;learning primitive movements;factor model training;learned representation captures;poses obtained;learned representation;synthetic representation movements;problem learning rotation;sports dataset demonstrate;frame synthetic representation;learned representation supervised;modeling human"}, "ed1da1abf0f50ca758e422fbd945f891b6cda690": {"ta_keywords": "dialog response retrieval;dialog retrieval;paraphrase identification model;based dialog retrieval;neural network paraphrase;sentence distributed representations;dialog retrieval using;paraphrase identification improve;distributed word representations;network paraphrase identification;paraphrase identification;example based dialog;word representations;response retrieval;based dialog response;utilize recursive neural;words sentence distributed;word representations employ;dialog response;query distributed word;dialog pair database;pooling determine sentences;response retrieval previous;sentence distributed;retrieval using recursive;identification model dialog;dialog;relatively simple retrieval;based dialog;recursive neural network", "pdf_keywords": ""}, "7596030e0ce7a8df2f43e6ba4e9de5fa4a19240f": {"ta_keywords": "extragradient stochastic gradients;stochastic extragradient methods;conservatively stochastic extragradient;extragradient stochastic;stochastic extragradient;vanilla extragradient stochastic;convergence speed extragradient;extragradient methods;gradient descent ascent;stochastic gradients jeopardize;extra gradient methods;extragradient methods staple;speed extragradient methods;gradient methods overcome;gradient descent;stochastic gradients;gradients jeopardize convergence;gradient methods;extragradient methods variable;extragradient;plague gradient descent;speed extragradient;update conservatively stochastic;descent ascent schemes;step extra gradient;gradients jeopardize;vanilla extragradient;extra gradient;ascent schemes;gradient", "pdf_keywords": "stochastic extragradient methods;extragradient stochastic gradients;vanilla extragradient stochastic;conservatively stochastic extragradient;extragradient stochastic;stochastic extragradient;stepsize extragradient algorithm;extragradient algorithm exploration;extragradient algorithm;converge stochastic gradients;stochastic gradients jeopardize;stochastic gradients;extragradient methods;update conservatively stochastic;stepsize extragradient;gradients jeopardize convergence;method converge stochastic;double stepsize extragradient;conservatively stochastic;stochastic gradients derive;convergence rates algorithms;converge stochastic;extragradient methods variable;extragradient;fully stochastic;fully stochastic setting;stepsize scaling arxiv;vanilla extragradient;algorithm provably avoids;running vanilla extragradient"}, "0d1cd3baad7d0734c9bbb008a33e2d10846968cd": {"ta_keywords": "grammar induction combinatory;unsupervised grammar induction;combinatory categorial grammars;induction combinatory categorial;grammar induction;unsupervised grammar;categorial grammars;induction combinatory;combinatory categorial;grammars;grammar;combinatory;unsupervised;induction;categorial", "pdf_keywords": ""}, "739c2181f7894050a06b53b41ac5debe8ffc4829": {"ta_keywords": "sgd stochastic differential;sgd stochastic;trajectories sgd stochastic;generalization bounds sgd;stochastic gradient descent;properties stochastic gradient;convex deep learning;gradient descent sgd;generalization properties stochastic;sdes learning;stochastic gradient;sgd rigorous;generalization properties sdes;sdes learning theoretical;sgd assumption trajectories;modeling trajectories sgd;properties sdes learning;trajectories sgd;sgd rigorous treatment;bounds sgd assumption;sgd non convex;characteristics sgd rigorous;stochastic differential;descent sgd;sde representations brownian;bounds sgd;recent sde representations;sgd assumption;sde representations;deep learning problems", "pdf_keywords": "trajectories sgd stochastic;sgd stochastic differential;complexity trajectories stochastic;sgd stochastic;dimensions deep learning;trajectories stochastic learning;sdes learning theoretical;sgd rigorous;sdes learning;stochastic learning;modeling trajectories sgd;generalization properties sdes;trajectories sgd;sgd rigorous treatment;fractal dimensions deep;characteristics sgd rigorous;properties sdes learning;stochastic differential;deep learning models;deep learning;complexity trajectories;deep learning tools;trajectories stochastic;fractal dimensions;accurate metric generalization;accurately estimates generalization;stochastic;deep neural;capacity metric accurately;stochastic learning algorithm"}, "94f8ef5944ecbdd0350d406bf3a16a7f2dff7349": {"ta_keywords": "speech separation recognition;mimo speech end;speaker speech recognition;architecture mimo speech;speech recognition;applications mimo speech;mimo speech;field speech recognition;speech recognition tasks;speech separation;speaker speech separation;wsj1 2mix corpus;far field speech;mimo speech extends;multi speaker speech;speech recognition adopted;channel multi speaker;2mix corpus model;2mix corpus;monaural multi speaker;input multi channel;speech end end;multi channel input;speech end;field speech;compared single channel;channel input multi;end multi channel;multi channel;multi speaker", "pdf_keywords": "channel multispeaker speech;multispeaker speech recognition;speaker training data;speaker speech recognition;speech separation recognition;multi speaker data;multi speaker training;multispeaker speech;channel multispeaker;multichannel multi speaker;channel multi speaker;single speaker data;multi channel multispeaker;speaker data;multi speaker speech;architecture mimo speech;speech recognition model;speaker data xnoisy;speaker speech separation;utterances wsj dev93;multi speaker asr;speech recognition;channel single speaker;speaker training;multi speaker;single speaker training;mimo speech fully;speech recognition shown;speech fully neural;speech separation"}, "ab193c05bc447f368565c1ff37064b1c517a750f": {"ta_keywords": "learning agents dialogue;exploration deep learning;exploration deep;greedy boltzmann exploration;explore thompson sampling;agents dialogue systems;efficiency exploration deep;boltzmann exploration bootstrapping;boltzmann exploration;learning agents;agents dialogue;exploration strategies greedy;algorithm learns;strategies greedy boltzmann;algorithm learns faster;thompson sampling;deep learning agents;learns faster;learns;dialogue systems;agents explore thompson;agents explore;improves efficiency exploration;exploration bootstrapping based;exploration strategies;replay buffer experiences;episodes make learning;dialogue;exploration bootstrapping;common exploration strategies", "pdf_keywords": "reinforcement learning deep;exploration deep reinforcement;deep reinforcement learning;learning agents dialogue;deep qnetworks thompson;qnetworks thompson sampling;neural networks reinforcement;deep reinforcement;networks reinforcement learning;exploration deep learning;networks reinforcement;deep qnetworks;learning deep qnetworks;explore thompson sampling;thompson sampling dialogue;sampling dialogue systems;exploration deep;reinforcement learning;agents dialogue systems;learning agents;reinforcement learning task;deep learning agents;thompson sampling;explores thompson sampling;bbqn explores thompson;ef\ufb01cient exploration deep;agents explore thompson;qnetworks thompson;dialogue systems arxiv;agents dialogue"}, "2b5d553cb2f298f36aff1a1519f7f2f6be4db5da": {"ta_keywords": "supervised taxonomy expansion;self supervised taxonomy;existing taxonomy expansion;taxonomy expansion model;expansion taxonomies;taxonomy expansion;taxonomy expansion taxonomies;taxonomy expansion 11;attachment prediction task;view training prediction;supervised taxonomy;expansion taxonomies important;propose self supervised;self supervised;supervision existing taxonomy;study taxonomy expansion;node attachment prediction;query terms extensive;taxonomies new concept;methods taxonomy expansion;attachment prediction;prediction task anchor;taxonomies new;task learns feature;new concept terms;expand existing taxonomies;attachment task learns;prediction study taxonomy;feature representations query;existing taxonomies", "pdf_keywords": "supervised taxonomy expansion;self supervised taxonomy;taxonomy expansion extensive;supervision existing taxonomy;existing taxonomy expansion;taxonomy expansion model;study taxonomy expansion;taxonomy expansion;taxonomy expansion generate;taxonomies new concept;methods taxonomy expansion;supervised taxonomy;taxonomy expansion 11;taxonomy expansion mini;taxonomies;existing taxonomies;steam self supervised;expand existing taxonomies;terms implementation steam;taxonomies new;taxonomy;benchmarks study taxonomy;propose self supervised;concept terms implementation;query terms propose;taxonomies used;existing taxonomy;new concept terms;taxonomies important knowledge;abstract taxonomies"}, "433b24d63146605d25c0c271062e129608462f03": {"ta_keywords": "reducing dimensionality kernel;nonlinear classification;achieve nonlinear classification;features kernel spaces;subspace pursuit method;dimensional features kernel;kernel spaces cutting;support vector machines;plane subspace pursuit;dimensionality kernel;subspace pursuit;dimensionality kernel spaces;convexity training;hidden markov models;training log linear;convexity training log;high dimensional features;nonlinear classification original;vector machines generalized;speech recognition;maintain convexity training;features kernel;reducing dimensionality;models automatic speech;method reducing dimensionality;kernel spaces framework;classification;dimensional features;kernel spaces;automatic speech recognition", "pdf_keywords": ""}, "a0e0ce316ce0fdca2db61a52fdc7100e24906075": {"ta_keywords": "xstream outlier;xstream outlier dete;feature evolving stream;featureevolving streams studied;stream feature evolving;featureevolving streams;evolving data stream;evolving streams;ensemble outlier detector;overhead evolving streams;evolving stream;problem featureevolving streams;evolving streams exists;based ensemble outlier;stream feature;ensemble outlier;xstream rigorously numerous;data stream;outlier detection;streaming scenarios xstream;xstream rigorously;outlier detector;detector called xstream;evolving stream demonstrate;streams;xstream extreme streaming;xstream extreme;updates stream progresses;streams exists;feature evolving data", "pdf_keywords": ""}, "1a0d8dbd0252193abe9d64f72fc56cc1f05ed3eb": {"ta_keywords": "improve situational reasoning;structured situational graph;situational graphs;situational graphs especially;situational reasoning end;situational reasoning;generated situational graphs;situational graph st;situational graph;explicitly structured situational;reasoning situations;structured situational;natural language queries;reasoning end task;reasoning situations recently;curie iterative querying;improve situational;generated situational;language model curie;natural language;approach reasoning situations;reasoning end;input generated situational;querying approach reasoning;situations recently models;multi hop reasoning;build graph relevant;knowledge multi hop;hop reasoning propose;situational", "pdf_keywords": "reasoning graphs pretrained;situational reasoning datasets;improve situational reasoning;situational reasoning framework;st reasoning graphs;reasoning graphs;structured situational graphs;structured situational graph;effective situational reasoning;reasoning task formulation;reasoning tasks;reasoning tasks general;multiple reasoning tasks;allow situational reasoning;generated situational graphs;situational reasoning method;situational graphs;situational reasoning;situational reasoning end;reasoning datasets;general situational reasoning;situational reasoning curie;situational graphs especially;situational graph st;explicitly structured situational;reasoning framework;framework situational reasoning;graphs pretrained language;construct structured situational;reasoning end task"}, "8529af634b443427d87d62d64467d2f1adfc230f": {"ta_keywords": "clustering malware samples;method clustering malware;clustering malware;analysis malware communities;characterizing malware samples;malware communities;heterogeneous malware samples;characterization malware samples;malware samples primarily;malware samples;multi modal clustering;identification characterization malware;modal clustering techniques;malware communities using;analysis malware;characterization malware;modal clustering;techniques heterogeneous malware;characterizing malware;heterogeneous malware;malware samples threat;samples malware;results characterizing malware;malware samples different;algorithm analysis malware;samples malware remains;modal clustering problem;actors samples malware;malware;malware remains", "pdf_keywords": ""}, "8424dd233577e3bd3fbd7ecdfd8b4d442531a20e": {"ta_keywords": "linear regression hmms;regression hidden markov;regression hmms;regression hmms considering;hmm parameters widely;model hmm parameters;hmm parameters;model hmm;hmms gaussian clusters;hmms considering regression;hidden markov model;gaussians hmms;gaussians hmms gaussian;sets gaussians hmms;hidden markov;markov model hmm;regression tree;regression tree structure;hmms gaussian;speech processing;considering regression tree;speech processing paper;marginalized log likelihood;parameters regression;regression using variational;regression hidden;regression heuristically tweaking;especially speech processing;linear regression hidden;tuning parameters regression", "pdf_keywords": ""}, "7b72f79015a0a5e06cc019bae78f268b16a8e659": {"ta_keywords": "discriminative training low;discriminative training model;discriminative training improved;discriminative training;sequence discriminative training;rank deep neural;low rank deep;perform discriminative training;dnn model low;improved performance dnn;rank sequence discriminative;training low rank;combination discriminative training;speech recognition;speech recognition number;neural networks dnns;networks dnns;dnn model;model low rank;model perform discriminative;low rank model;low rank approximation;low rank approximations;automatic speech recognition;training model reduction;rank deep;low rank sequence;perform discriminative;performance dnn;discriminative criterion sequence", "pdf_keywords": ""}, "0ea90d783a76b7c119fb5471fc71b6bc2defa06d": {"ta_keywords": "codes model distributed;distributed storage;efficient recovery data;distributed storage measure;node distributed storage;nodes minimize download;regenerating codes model;model distributed storage;regenerating codes;storage measure efficiency;minimize download;problem efficient recovery;efficient recovery;distributed storage rest;storage;storage rest nodes;linear codes;storage measure;setting regenerating codes;recovery data;recovery data stored;codes model;distributed;minimum bandwidth;non linear codes;node distributed;individual node distributed;storage rest;data read download;stored individual node", "pdf_keywords": "download optimized recovery;optimized nodes recovery;optimized recovery data;read download storage;download optimized nodes;download storage;nodes download optimized;download storage node;simultaneously optimized recovery;distributed storage;simultaneously optimized download;optimized download optimized;nodes recovery data;storage node recovery;download simultaneously optimized;matrix mbr code;optimized download;ef\ufb01cient recovery data;node distributed storage;storage node;matrix mbr;recovery data node;nodes read download;product matrix mbr;nodes recovery;optimized recovery set;download optimized;recovery set nodes;optimized nodes;productmatrix mbr code"}, "2030551b2590fa70eb5132131e6627c93128f0a1": {"ta_keywords": "robust utility learning;utility learning non;utility learning;utility learning methods;parametric utility learning;estimating utility functions;method estimating utility;estimating utility;utility functions players;learning non cooperative;multiple utility functions;utility functions;cooperative continuous games;estimated mixture utilities;modeling non cooperative;accurate robust utility;combining multiple utility;robust utility;cooperative agents mixture;utility functions creating;ensemble methods;game defined estimated;learning methods constrained;utility;games using probabilistic;agents mixture utilities;ensemble;ensemble methods bagging;multiple utility;energy efficient behavior", "pdf_keywords": ""}, "b34f254083b012bafd9b5ebd6c27450b4213c984": {"ta_keywords": "captions biomedical publications;understanding captions biomedical;captions biomedical;captions dense information;understanding captions;caption understanding;caption understanding recall;biomedical publications extracting;captions;accompanying captions;accompanying captions captions;captions captions;captions captions dense;automated extraction scientific;tool caption understanding;captions dense;figures accompanying captions;accurate tool caption;caption;standpoint automated extraction;publications extracting classifying;extraction scientific knowledge;standard information extraction;automated extraction;extraction classification task;extraction scientific;publications extracting;information extraction;scientific publications figures;tool caption", "pdf_keywords": ""}, "4a19211e077bc4ada685854245ba9fab381cbb06": {"ta_keywords": "relation extraction;relation extraction open;including relation extraction;extracting structured relation;text based relation;qa4ie question answering;tuples information extraction;question answering qa;flexible question answering;information extraction;structured relation tuples;relation tuples unstructured;relation tuples information;question answering based;information extraction order;question answering;answering based framework;automatically extracting structured;extracting structured;tuples unstructured texts;based relation tuples;relation specifications free;relation tuples;structured relation;relation triples sentences;answering qa approaches;informal relation specifications;framework information extraction;information extraction refers;refers automatically extracting", "pdf_keywords": "question answering qa;answering qa machine;qa machine reading;approaches question answering;question answering;machine reading comprehension;\ufb02exible question answering;answering qa approaches;answering qa;qa4ie perform document;reading comprehension mrc;qa machine;qa4ie;qa4ie framework overcome;qa4ie framework;novel qa based;novel qa;reading comprehension;qa4ie perform;named qa4ie;propose novel qa;qa;qa based;named qa4ie perform;qa based framework;machine reading;framework named qa4ie;qa approaches;comprehension mrc;qa approaches produce"}, "409280796e924bfe71421fe5bf4986bd3591ea72": {"ta_keywords": "supports similarity joins;similarity joins based;similarity joins;similarity reasoning built;similarity reasoning;supports similarity;short queries efficient;text inferences whirl;management supports similarity;general purpose similarity;joins based;type similarity reasoning;real world databases;metrics text inferences;web based databases;faster naive inference;built type similarity;efficient typical queries;similarity metrics text;text inferences;short queries;purpose similarity metrics;queries efficient;common object identifiers;databases satisfy;soft database;answer substitutions ranked;joins based certain;names objects web;databases contain informal", "pdf_keywords": ""}, "4e3016617e5e254bafebcbd7e96c509f670bdd37": {"ta_keywords": "music performance synthesis;generative audio model;generative audio;text speech synthesis;speech synthesis;model synthesize music;existing piano dataset;synthesize music;conditional generative audio;novel score audio;piano dataset;piano dataset overall;polyphonic inputs;new violin dataset;speech synthesis present;handling polyphonic inputs;polyphonic inputs providing;violin dataset;score audio music;handling polyphonic;score audio;polyphonic;techniques handling polyphonic;synthesize music clear;violin dataset consisting;music clear polyphony;recordings scores;audio music;piano;polyphony harmonic", "pdf_keywords": "audio music synthesis;musical score synthesis;music synthesis;model synthesize music;music synthesis consists;synthesize music;music synthesis illustrated;generative audio model;polyphonic mixer;generative audio;decoder polyphonic;score audio music;synthesize music clear;proposed polyphonic mixer;decoder polyphonic inputs;unaligned musical scores;encoder decoder polyphonic;stage score audio;performance unaligned musical;score audio;conditional generative audio;natural music performance;generates audio;polyphonic inputs;synthesizes mel spectrogram;musical score;audio music;proposed polyphonic;polyphonic mixer aligning;polyphonic inputs proposed"}, "2a39a4f2d18e376ef8a6e2f45416e7b87957481e": {"ta_keywords": "historical text normalization;text normalization;nlp tasks multi;text normalization suffers;observed nlp tasks;nlp tasks;multi task learning;task learning historical;text normalization size;task learning;learning historical text;observed nlp;task learning used;normalization;normalization suffers;task learning mainly;tasks multi task;normalization suffers small;models multi task;multi task;task data scarce;task data;target task data;nlp;tasks multi;learning historical;historical text;normalization size;contrary observed nlp;normalization size matters", "pdf_keywords": ""}, "bd17620c6cb5ca97ef773499223d1509d123745f": {"ta_keywords": "make deep learning;machine learning scientist;jupyter notebooks;deep learning;learning scientist iii;interactive examples;math interactive examples;learning;deep learning approachable;interactive examples self;open source book;learning scientist;applied machine learning;jupyter notebooks seamlessly;drafted jupyter notebooks;learning approachable;math interactive;open source;resource freely;resource freely available;code entire book;book drafted jupyter;source book;machine learning;notebooks seamlessly integrating;seamlessly integrating exposition;resource;jupyter;interactive;self contained code", "pdf_keywords": ""}, "a30f912f8c5e2a2bfb06351d4578e1ba3fa37896": {"ta_keywords": "code tokens identifiers;learning codet5 identifier;code understanding generation;codet5 identifier aware;code tokens;distinguish code tokens;code comments;code related tasks;written code comments;code semantics conveyed;code semantics;novel identifier aware;code defect detection;identifier aware unified;code comments bimodal;task learning codet5;code understanding;learning codet5;codet5 identifier;code snippet way;identifier aware;leverages code semantics;distinguish code;identifier aware pre;code snippet;transfer programming languages;semantics conveyed developer;code related;tokens identifiers;decoder models code", "pdf_keywords": "code comments bimodal;code comments;code understanding generation;written code comments;code tokens identi\ufb01ers;using code comments;code comments present;distinguish code tokens;code tokens;code understanding;code semantics conveyed;pre trained encoder;comments bimodal dual;code semantics;comments present codet5;code snippet way;encoder;trained encoder decoder;distinguish code;code snippet;encoder decoder;codet5 uni\ufb01ed pre;task better nl;snippet way nl;process code snippet;trained encoder;leverages code semantics;written code;present codet5 uni\ufb01ed;codet5 uni\ufb01ed"}, "70bc4dc0bc72816773006c71b56fa5885c729caa": {"ta_keywords": "model generate music;generate music;neural music generation;music generation model;music generation;generating multi track;generate multi track;multi track pop;gans generate;gans generate multi;networks gans generate;track pop rock;generative adversarial;new neural music;generative adversarial networks;generate music scratch;rock music bars;neural music;music bars;convolutional gan based;networks gans;convolutional gan;multi track piano;applying generative adversarial;music;pop rock music;gan based model;music bars using;adversarial networks gans;track pop", "pdf_keywords": ""}, "78c9181abe18575925fbbb6e6d8c72d7bf90d06d": {"ta_keywords": "low delay iot;fairness qzmac proposed;delay iot;delay iot applications;decentralized mac protocol;fairness properties qzmac;fairness qzmac;delay throughput optimality;term fairness qzmac;decentralized hybrid mac;protocol term qzmac;decentralized mac;mac protocol;delay throughput;performance qzmac;qzmac proposed;zmac qzmac provides;qzmac propose;develop decentralized mac;qzmac provides;demonstrate performance qzmac;contiki operating 4ghz;achieved zmac qzmac;queuing networks;delay ii distributed;nodes sharing time;constrained queuing networks;zmac qzmac;qzmac;queuing networks develop", "pdf_keywords": "performance qzmac;decentralized hybrid mac;zmac implement;demonstrate performance qzmac;qzmac;crossbow telosb motes;contiki operating 4ghz;propose ezmac simple;based crossbow telosb;zmac;propose ezmac;mac called zmac;running 6tisch communication;called zmac implement;ezmac simple;zmac implement demonstrate;operating 4ghz ism;ezmac;4ghz ism;6tisch communication stack;cc2420 based crossbow;performance qzmac test;\ufb01rst propose ezmac;ezmac simple extension;called zmac;qzmac test;communication stack contiki;crossbow telosb;cc2420 based;motes running 6tisch"}, "9bbdcc03d872987eef9165f4a63c3878a5b05189": {"ta_keywords": "text representation encoders;representations efficient text;lm text representation;deep lms encode;encode text sequences;lms encode text;attention structure ready;representation dense encoders;text information dense;lms internal attention;lm text;deep lms;dense encoder novel;models lm text;dense encoder;use dense encoders;representation encoders;dense encoders needs;text representation;attention structure;dense encoders require;train dense encoder;language models lm;dense encoders;text sequences;encode text;text sequences sentences;efficient text;transformer language models;internal attention structure", "pdf_keywords": "language model pretraining;learning condenser language;condenser language model;answering retrieval;contrastive learning condenser;training architecture condenser;question answering retrieval;language model pre;suf\ufb01cient training condenser;learning condenser;answering retrieval web;sentence similarity tasks;condenser language;retrieval question answering;question answering web;similarity question answering;question answering;tasks sentence similarity;similarity tasks retrieval;representation condenser;condenser lm prediction;general language model;representation condenser effective;transformer pre training;dense representation condenser;language model;pre training architecture;sentence similarity;architecture condenser lm;lm pre training"}, "2dd81061e0b11c828446f6a1843741ae51facbd2": {"ta_keywords": "arbitrarily slow neurons;slow neurons inherent;learning networks slow;slow neurons;neurons phase advance;neurons phase;biological neurons phase;neuron;neuron synapse dynamics;neurons;disentangled neuron;neurons inherent property;models cortical microcircuitry;detailed models cortical;neurons inherent;quasi instantaneous inference;derive disentangled neuron;synapse dynamics;biological neurons;disentangled neuron synapse;models cortical;neuron synapse;instantaneous inference;cortical microcircuitry principle;equilibrium unified learning;delayed processing stimuli;learning standard benchmark;connected convolutional architectures;inference learning jointly;networks slow components", "pdf_keywords": "computation including neuron;neuronal computation;computation learning physical;learning physical substrates;analog neuronal systems;neuronal computation including;analog neuronal;synaptic plasticity network;neuron synapse dynamics;deployment analog neuronal;neuron;aspects neuronal computation;dynamics synaptic plasticity;learning networks slow;synapse dynamics;neuronal systems biological;disentangled neuron;neurons;neuronal systems;membrane dynamics synaptic;biological neurons phase;including neuron;dynamics synaptic;disentangled neuron synapse;fast computation learning;neuron synapse;neurons phase advance;neurons phase;computation learning;biological neurons"}, "87f42406de084e60d2365adac8a159ed3e455856": {"ta_keywords": "similarity propose streamspot;propose streamspot clustering;streamspot clustering;streamspot clustering based;propose streamspot;streamspot high performance;streamspot;stream heterogeneous graphs;streamspot exhibits desirable;streaming processing stream;processing stream edge;streaming processing;heterogeneity streaming;stream edge;processing stream;stream edge time;streamspot high;heterogeneity streaming nature;fully streaming processing;graph fast compute;anomaly detection;clustering based anomaly;streaming application;representation graph fast;stream heterogeneous;streaming nature given;streaming nature;stream;based anomaly detection;flow graphs", "pdf_keywords": "anomaly detection streaming;streaming heterogeneous graphs;stream heterogeneous graphs;graphs originating stream;propose streamspot clustering;detection streaming heterogeneous;cluster detect anomalous;streamspot clustering;presented streamspot cluster;streamspot cluster detect;graphs evolve stream;anomalous heterogenous graphs;streamspot cluster;streamspot clustering based;heterogeneity streaming;ef\ufb01cient anomaly detection;stream typed edges;anomaly detection;heterogeneity streaming nature;clustering detects anomalies;clustering based anomaly;detect anomalous;propose streamspot;streaming heterogeneous;memory ef\ufb01cient anomaly;detection streaming;time propose streamspot;detect anomalous heterogenous;approach called streamspot;streamspot handle temporal"}, "4cc70dd760c2c8cfc0107921bade45fb5efe860e": {"ta_keywords": "recommendations using knowledge;latent factorization graphs;factorization graphs hybrid;factorization graphs;knowledge graphs probabilistic;graph recommendation method;recommendation method large;personalized recommendations using;collaborative filtering techniques;personalized recommendations;collaborative filtering;knowledge graphs;knowledge graphs important;based collaborative filtering;recommendation method;using knowledge graphs;graph recommendation;recommender systems using;recommender systems;improving performance recommender;latent factorization;proppr personalized recommendations;knowledge graphs maximum;recommendations using;recommendations using general;graphs probabilistic logic;factorization;demonstrate knowledge graphs;based recommendations using;graph based latent", "pdf_keywords": ""}, "fa2125641578934de12d7f792b094ffcfdf82ee2": {"ta_keywords": "text picture synthesis;text picture russian;ttp designed russian;picture synthesis;picture synthesis described;picture russian language;russian language processing;designed russian language;purpose text picture;synthesis described ttp;text picture;designed russian;picture russian;russian language;ttp designed;subsystem stage processing;stage processing subsystem;subsystem processing stage;rendering subsystem processing;processing subsystem rendering;synthesis;described ttp designed;text;processing stage described;language processing;language processing operates;processing stage;stage processing;described ttp;general purpose text", "pdf_keywords": ""}, "c818f9be503a1ed94f991a2949c29e3ee477e8b8": {"ta_keywords": "uncertainty training decoding;noisy speech recognition;uncertainty decoding training;dnn based uncertainty;uncertainty input features;stochastic representation enhanced;training decoding;training decoding methods;decoding training models;decoding methods deep;asr decoding training;reverberated noisy speech;uncertainty enhanced features;decoding training;uncertainty decoding;speech recognition;based uncertainty decoding;deep neural networks;speech recognition tasks;stochastic representation;techniques uncertainty training;sampling input features;noisy speech;training dnn learn;uncertainty training;deep neural;difficult deep neural;neural network dnn;expectation asr decoding;based stochastic representation", "pdf_keywords": ""}, "da1d5dc331c2839dfc3e6a79ee17f3bdf2231a8b": {"ta_keywords": "entity list completion;relation list extraction;perform entity list;entity list;entity examples seeds;relation list;list extraction techniques;target entity examples;list completion;list extraction;list completion task;retrieval process abstract;query entity target;given query entity;entity examples;query entity;retrieval process;retrieval;objects complete set;entity target entity;entity;target entity;entity target;perform entity;focus relation list;triple dataset ranked;recall second;techniques perform entity;billion triple dataset;list", "pdf_keywords": ""}, "eb28e82ca0bbc5d83e1cc07807da16874105d2fa": {"ta_keywords": "semantic frame induction;verb class clustering;unsupervised semantic frame;semantic frame;unsupervised semantic;framenet derived dataset;frame induction;generalization clustering triadic;clustering triadic;class clustering task;clustering task cast;frame induction using;paper unsupervised semantic;triclustering problem generalization;task framenet derived;frame induction problem;clustering triadic data;class clustering;framenet derived;clustering task;task framenet;generalization clustering;using triclustering supplementary;framenet;problem generalization clustering;clustering;triclustering supplementary;induction using triclustering;cast frame induction;approach triframes", "pdf_keywords": ""}, "572535aff31c400578fdd75313c896c0650b2d4c": {"ta_keywords": "speech recognition asr;speech enhancement asr;asr composing neural;speech recognition;dereverberation beamforming asr;beamforming asr;optimizing neural beamforming;optimize conventional asr;neural beamforming alongside;neural beamforming;far field asr;automatic speech recognition;multichannel speech;automatic speech;enhancement asr components;recognition asr;field asr composing;multichannel speech enhancement;entire multichannel speech;enhancement asr;recognition asr ability;field asr;s2s asr module;speech enhancement;asr composing;conventional asr;paradigm automatic speech;conventional asr components;asr components s2s;asr module", "pdf_keywords": ""}, "8495c1722e1f5107733c842839c2d298b9116921": {"ta_keywords": "conversation history modeling;answer embedding conversational;conversational question answering;model conversation history;conversation history conversational;conversational search model;embedding conversational question;embedding conversational;turn conversational search;conversation history;conversational search;conversation history answer;history answer embedding;bert bidirectional encoder;search model conversation;setting conversational search;conversational search provide;bert bidirectional;answer embedding;insights conversation history;history conversational;built bert bidirectional;model conversation;question answering;bert history;question answering convqa;setting conversational;prepend history turns;question answering enables;integration conversation history", "pdf_keywords": "conversation history modeling;model conversation history;conversation history bert;conversation history conversational;conversational question answering;history answer embedding;conversation history;conversation history explain;conversation history convqa;answer embedding;insights conversation history;answer embedding propose;conversational search;history bert based;embedding better history;answer embedding better;model informationseeking conversations;answer embedding method;modeling approach bert;history conversational;bert bidirectional encoder;bert based model;bert based convqa;better history modeling;conversational search provide;embedding propose history;history modeling convqa;bert bidirectional;setting conversational search;approach bert based"}, "14f925098d57b0fa491a100fa73e52dbc764efa6": {"ta_keywords": "neuroimaging study explained;fmri study tam;dimoka fmri study;neuroimaging study;study neuois published;conduct neuroimaging study;fmri study;explained dimoka fmri;dimoka fmri;study neuois;neuroimaging;fmri;neuois published mis;finally conduct neuroimaging;neuois published;cognitive neuroscience;neurois;cognitive neuroscience used;conduct neuroimaging;theory cognitive neuroscience;neuroscience;neuroscience used;neurois declared dimoka;neuroscience used sociology;neurois declared;neuois;years neurois;years neurois declared;cognitive;recent years neurois", "pdf_keywords": ""}, "32a2a8baf217d29f628d22d793cace95634f51d5": {"ta_keywords": "recommendation leak detection;recipient recommendation leak;mozilla thunderbird popular;mozilla thunderbird;extension mozilla thunderbird;leak detection recipient;open source email;email client cutonce;detection recipient recommendation;recommendation leak;email client;thunderbird;solutions real email;thunderbird popular;thunderbird popular open;real email users;email users;promising usage patterns;information leak detection;source email client;cutonce recipient recommendation;recipient recommendation;recipient recommendation study;client cutonce recipient;email users paper;extension mozilla;leak detection action;usage patterns extension;mozilla;real email", "pdf_keywords": ""}, "1092da11519fe8427c8113b16a012f34f4a3fb6b": {"ta_keywords": "ethical federated learning;accuracy fairness privacy;fairness privacy;ethical federated;federated learning;fairness privacy work;presents ethical federated;fairness metrics federated;predictions data privacy;learning differential privacy;metrics federated learning;federated learning model;federated learning differential;differential privacy;privacy researchers tackle;data privacy researchers;privacy researchers;data privacy;privacy;privacy work;fairness metrics;introducing fairness metrics;federated;accuracy fairness;issues introducing fairness;ethical;introducing fairness;interplay accuracy fairness;privacy work presents;metrics federated", "pdf_keywords": "attributes federated learning;federated learning;federated machine learning;phases fairness privacy;federated learning meets;fairness privacy;fairness privacy allowed;learning meets fairness;privacy agent training;sensitive attributes federated;2021 federated learning;fairness di\ufb00erential privacy;guarantee training data;attributes federated;training data sensitive;dp privacy;privacy 11 reduce;dp privacy agent;privacy 11;attribute protected aggregator;dp guarantee training;privacy;protected aggregator;privacy agent;di\ufb00erential privacy 11;privacy allowed provide;agent training data;data sensitive attributes;data sensitive attribute;phases fairness"}, "ed913bed529d6bb7beac3b6086a853698abf627d": {"ta_keywords": "home assistants spoken;digital home assistants;speech interaction digital;field speech recognition;data speech processing;speech processing digital;data speech;training data speech;speech recognition commands;speech processing;speech recognition;key speech processing;far field speech;assistants spoken language;spoken language interface;home assistants;speech processing algorithms;home assistants purpose;statistical models speech;speech interaction;recognition commands spoken;assistants spoken;speech synthesis sophisticated;high quality speech;microphone array processing;models speech;field speech;speech synthesis;interaction digital home;spoken distance", "pdf_keywords": ""}, "914626e2e13bd42a5f06c28ff02ba7c428e81ff1": {"ta_keywords": "global coupled ionosphere;coupled ionosphere thermosphere;coupled ionosphere;ionosphere thermosphere;ionosphere;modeling elucidation disturbances;thermosphere;disturbances complex global;elucidation disturbances complex;complex global coupled;global coupled;elucidation disturbances;disturbances complex;modeling elucidation;disturbances;complex global;coupled;modeling;global;elucidation;complex", "pdf_keywords": ""}, "e829ee7fe48f4b1e451378b6a21470b2f86c0aa6": {"ta_keywords": "erasure code pir;pir erasure coded;erasure coded systems;explicit erasure code;erasure coded;erasure code;code pir algorithm;pir erasure;code pir;paper erasure code;existing pir algorithms;pir algorithms require;explicit codes algorithms;investigation pir erasure;pir algorithms;pir algorithm requires;perfect privacy notion;codes algorithms;design explicit erasure;codes algorithms design;pir algorithm;explicit erasure;designing explicit codes;provide perfect privacy;privacy notion;precise capacity pir;capacity pir respect;perfect privacy;existing pir;privacy considered", "pdf_keywords": ""}, "31603b3339f4da5bdc6b7de4231bd1ddfb32a50a": {"ta_keywords": "ode rnn based;neural controlled differential;observations neural controlled;backpropagation observations neural;rnn based models;based backpropagation observations;modelling temporal dynamics;irregular time series;backpropagation observations;ode rnn;observations neural;subsequent observations neural;similar ode rnn;neural controlled;neural ordinary differential;observations neural ordinary;based backpropagation;temporal dynamics;modelling temporal;controlled differential equations;adjoint based backpropagation;backpropagation;time series unlike;rnn based;equations irregular time;multivariate time series;controlled differential;time series;emph neural controlled;rnn", "pdf_keywords": "continuous analogue rnn;neural odes continuous;neural ode framework;based neural ode;just neural odes;time analogue rnn;neural odes;analogue resnet neural;analogue rnn;neural cde continuous;resnet neural cde;neural controlled differential;analogue resnets neural;based backpropagation observations;rnn just neural;analogue rnn just;models neural cdes;resnets neural cde;continuous time analogue;backpropagation observations;backpropagation just neural;energy based neural;series models neural;based backpropagation;backpropagation just;continuous analogue resnet;adjoint based backpropagation;time series models;backpropagation;neural ode"}, "5f563da2843e005c4b236f7889e7a22631b53210": {"ta_keywords": "conference peer review;peer review revisiting;conference publications assessing;peer review determine;reviewer quality scores;publications assessing quality;peer review;publications assessing;examined inconsistency conference;quality scores subjective;quality scores impact;inconsistency conference peer;reviewer quality;review revisiting 2014;review revisiting;improving reviewing;quality individual researchers;citation count;assessing quality;papers correlation quality;tier conference publications;citation count paper;review determine;reviewing;quality scores;correlation quality scores;suggestions improving reviewing;inconsistency conference;improving reviewing process;scores subjective", "pdf_keywords": "conference peer review;reviewer quality scores;peer review revisiting;peer review conclude;peer review;correlation reviewer quality;reviewer quality;review revisiting 2014;examined inconsistency conference;analyse rejected papers;quality scores subjective;quality scores impact;quality scores;machine learning conferences;review revisiting;rejected papers conference;inconsistency conference peer;reviewing;paper revisit 2014;identifying poor papers;papers correlation quality;papers conference determining;correlation quality scores;identifying good papers;review conclude reviewing;scoring process machine;papers finally analyse;eventual citation impact;citations;scores impact paper"}, "dd5e54b08b2c1520d179e88cd524e9bd4fe1f6ab": {"ta_keywords": "divergences sds regularized;sinkhorn divergences sds;sds approximating ot;divergences sds;regularization understood computationally;regularization strength;approximating ot;sds regularized variant;sds approximating;regularization strength varepsilon;depending regularization strength;infty optimal transport;computationally ot sds;tradeoff induced regularization;complexity ot mmd;regularized ot bounded;sinkhorn divergences;sds regularized;measures sample complexity;approximating ot function;error sds approximating;complexity bound sds;optimal transport ot;sample complexity ot;approximation error sds;optimizers regularized ot;variant ot distances;regularized ot;sample complexity bound;emph sinkhorn divergences", "pdf_keywords": "sds bound approximation;sds approximating ot;sds approximating;approximation error sds;proble rkhs convergence;sgd computation sds;interpolation property sds;convergence theorems sds;rkhs convergence;error sds approximating;complexity bound sds;rkhs convergence rate;theorems sds bound;sds maximization proble;maximization proble rkhs;theorems sds;sds maximization;computation sds;bounded sobolev rkhs;reformulation sds maximization;reformulating sds maximization;bound sds;kernel sgd computation;maximization expectation rkhs;regularized ot bounded;bound sds obtained;prove optimizers regularized;sds maximization expectation;sds bound;optimizers regularized ot"}, "8b192503f119a0b0cc30ef5179a00f231c20fb93": {"ta_keywords": "modeling events;fake event epochs;modeling events using;event datasets sequences;datasets sequences events;event epochs;neural graphical event;graphical event model;work modeling events;likelihood event datasets;channel rnn;event datasets;event epochs consecutive;event model negative;event model;multi channel rnn;rnn;events;rnn optimally;primarily tasks prediction;evidence observable events;sequences events;channel rnn optimally;events various types;events various;rnn optimally reinforces;sequences events various;capture historical dependencies;model negative evidence;tasks prediction", "pdf_keywords": "channel neural gem;channel rnn;multi channel rnn;modeling event stream;gem dimensional attentions;baseline neural hawkes;parametric deep learning;modeling event sequences;neural gem;attentions modeling event;event epochs;non parametric deep;neural hawkes;deep neural;parametric deep neural;rnn;neural hawkes process;deep learning;channel rnn optimally;fake event epochs;power deep neural;multi channel neural;rnn optimally;event stream;modeling event;neural gem dimensional;parametric deep;relevant neural baseline;event epochs consecutive;dimensional attentions modeling"}, "0ca2575a1ef73930dc2abe205b44e079eadc426c": {"ta_keywords": "decision theoretic lane;tolling schemes;design tolling schemes;model traffic flow;tolling schemes improve;model traffic;multiple lanes;tolling;lanes multiple populations;lanes multiple;theoretic lane changing;multiple lanes multiple;lane changing behavior;traffic flow;traffic flow lane;macroscopic model traffic;incorporates multiple lanes;method design tolling;design tolling;traffic flow incorporates;lanes;improve traffic flow;lane road example;model adjoint method;lane changing;theoretic lane;flow lane;pdes weakly hyperbolic;traffic;drivers local decision", "pdf_keywords": ""}, "b5991b1018bb89b053a2c8229248f97956391bb5": {"ta_keywords": "automatically estimated pronunciation;pronunciation learning;iterative pronunciation learning;pronunciation learning non;estimated pronunciation variations;native speech recognition;pronunciation variations estimated;estimated pronunciation;native pronunciations design;non native pronunciations;crafted pronunciation lexicons;speech recognition asr;native pronunciations directly;hand crafted pronunciation;driven iterative pronunciation;acoustic model adaptation;iterative pronunciation;native pronunciations;native speakers automatically;speech recognition automatically;automatic speech recognition;speech recognition;pronunciations design new;speech non native;grapheme phoneme;pronunciation lexicons;speech using iterative;pronunciations directly speech;crafted pronunciation;significantly native speech", "pdf_keywords": ""}, "b19cba7bfe318c69d5e62f8322cb5d75228452f4": {"ta_keywords": "pretrained language models;performance nlp benchmarks;large scale language;scale pretrained language;scale language models;nlp benchmarks;weights fast rank;large scale pretrained;nlp benchmarks work;weights models millions;language models better;tuning weights models;performance nlp;task adapting large;weight matrices pretrained;pretrained model weights;adapting large scale;optimization parameterized hypercomplex;language models;weights models;tuning large scale;pretrained language;language models downstream;tasks fine tuning;weights computed efficiently;fine tuning weights;matrices pretrained model;layers fine tuning;low rank hypercomplex;matrices pretrained", "pdf_keywords": "language models compacter;large scale language;pretrained language models;scale pretrained language;scale language models;language models better;large scale pretrained;language models excellent;tuning large scale;adapting large scale;tasks \ufb01ne tuning;scale language;pretrained language;trainable parameters task;language models;language models downstream;performance nlp benchmarks;adapting large;task performance memory;parameters task performance;abstract adapting large;tuning compacter;nlp benchmarks;optimization parameterized hypercomplex;trainable parameters;models compacter accomplishes;tuning data limited;tuning method large;models compacter;performance nlp"}, "3ba013b8b56646d66aac8472fb90a5c029ef55a0": {"ta_keywords": "learning differential equations;solving learned dynamics;learned dynamics differential;solve learning differential;equations parameterized neural;learned dynamics;learned dynamics easier;differential equations parameterized;learning differential;solve numerically training;cost solving learned;differentiable surrogate time;neural networks expensive;parameterized neural networks;easier solve learning;introduce differentiable surrogate;solution trajectories optimizing;numerically training progresses;parameterized neural;derivatives solution trajectories;differentiable surrogate;dynamics easier solve;solving learned;numerically training;solve learning;numerical solvers;surrogate time cost;expensive solve numerically;standard numerical solvers;dynamics differential equations", "pdf_keywords": "learning differential equations;learning differential;equations parameterized neural;numerically training progresses;neural networks expensive;solve numerically training;numerically training;speed regularization;learned dynamics easier;solver order regularization;speed regularization various;parameterized neural networks;learned dynamics;parameterized neural;differential equations parameterized;numerical solvers;models solvers;differentiable surrogate time;standard numerical solvers;derivatives solution trajectories;effect speed regularization;various models solvers;regularization various models;models solvers demonstrate;introduce differentiable surrogate;expensive solve numerically;numerical solvers using;neural networks;differentiable surrogate;dynamics easier solve"}, "26299d5fdc5137291dc6a091573b3d18aba1d1c2": {"ta_keywords": "pretrained multilingual models;models multilingual bert;adapting pretrained multilingual;pretrained multilingual model;multilingual bert;multilingual bert xlm;multilingual models;multilingual models multilingual;models multilingual;pretrained multilingual;task cross lingual;multilingual model new;cross lingual transfer;lingual transfer representative;multilingual model;lingual transfer;cross lingual;shot cross lingual;lingual transfer addition;art pretrained multilingual;languages zero shot;model new language;multilingual;bootstrapping nlp applications;art cross lingual;entity recognition achieves;nlp applications low;named entity recognition;low resource languages;languages named entity", "pdf_keywords": "adapting pretrained multilingual;adapts multilingual model;pretrained multilingual model;monolingual models task;target language adaptation;multilinguality adapts;adapts multilingual;multilinguality adapts multilingual;diverse languages tasks;\ufb01netune pretrained multilingual;multilingual model target;multilingual model mlm;pretrained multilingual;multilingual model arbitrary;language adaptation;monolingual models;multilingual model;mlm target language;multilingual model new;language adaptation similar;languages tasks;lingual mlm target;cross lingual mlm;target language;target languages;model target languages;\ufb01netuning monolingual models;model new language;curse multilinguality adapts;variant cross lingual"}, "cce8cf3d7f45113a4cba984b878802a5b16d5967": {"ta_keywords": "speaking style transformation;statistical machine translation;translation approach speaking;speaking style;machine translation;approach speaking style;machine translation approach;translation approach;style transformation;approach speaking;monotonic statistical machine;statistical machine;translation;transformation;speaking;monotonic statistical;statistical;style;machine;monotonic;approach", "pdf_keywords": ""}, "77ce1b8d425b7538c21ce0be976ee24a58e797c1": {"ta_keywords": "speech separation recognition;discriminative nmf application;discriminative training nmf;proposed discriminative nmf;matrix factorization nmf;chime speech separation;discriminative nmf;channel source separation;factorization nmf;source separation accurately;factorization nmf popular;discriminative nmf criterion;nmf criterion discriminative;training nmf basis;criterion discriminative nmf;speech separation;source signals mixtures;respect sparse nmf;source separation;sparse nmf exemplar;sparse nmf;separation recognition;source separation non;signals mixtures novel;separation recognition challenge;single channel source;nmf approaches optimized;recover source signals;negative matrix factorization;matrix factorization", "pdf_keywords": ""}, "5e8c52ddbd3581320f7e536b7cd10d7263b81eb2": {"ta_keywords": "representation learner automatically;learner automatically generate;learner automatically;representation learner simulated;use representation learner;simulate human learning;learning agents;learner simulated;developing intelligent agents;integrated representation learner;representation learner;intelligent agents simulate;intelligent agent developers;learning agents able;unsupervised grammar induction;grammar induction algorithm;feature predicates simstudent;generated feature predicates;representation provide automatically;intelligent agent;feature predicates based;automatically generated feature;artificial intelligence cognitive;grammar induction;feature predicates;intelligent agents;human learning;learner simulated student;algorithm acquires representations;human knowledge engineering", "pdf_keywords": ""}, "4481244de2cc0c55d91cebbb152eec79a76386f3": {"ta_keywords": "dram assembled 3d;flip chip assembly;logic device fabricated;chip assembly process;chip assembly;thermal compression bonding;innovative flip chip;ncf laminated substrates;effective assembly reliability;assembly reliability test;assembly reliability;logic device wide;assembled 3d structure;compression bonding;compression bonding method;bonding method cu;device wide dram;device fabricated middle;3d integrations realize;nm logic device;3d tsv integrations;process 3d integrations;achieved logic device;dram assembled;lpddr3 demonstrated innovative;device fabricated;wide dram assembled;memory die logic;logic device;laminated substrates", "pdf_keywords": ""}, "d26254cf3ec537f37708afaaf7f5a76a7922d4a2": {"ta_keywords": "word reordering models;statistical machine translation;reorderings source words;reorderings word pairs;reordering source words;word reordering information;existing word reordering;word reordering;learn reorderings word;machine translation;helpful word reordering;statistical models reordering;reorderings word;hierarchical phrase based;reordering models learn;enhance hierarchical phrase;models reordering source;learn reorderings source;hierarchical phrase;reordering information improves;models learn reorderings;phrase based statistical;reordering models;models reordering;reordering information;learn reorderings;basic hierarchical phrase;reorderings source;word pairs;sentence contiguous words", "pdf_keywords": ""}, "10e221c7d4636703c5c97b54f53b1cb57c25f3a6": {"ta_keywords": "linear networks trained;unregularized linear networks;networks trained;gradient descent;stochastic gradient descent;trained cross entropy;weighting importance weighting;optimized stochastic gradient;nonlinear unregularized networks;importance weighting;deep nonlinear unregularized;weighting importance;importance weighting iii;loss optimized stochastic;linear networks;importance weighting alters;demonstrate importance weighting;gradient descent converge;loss optimized;learned model;importance weighting importance;networks trained cross;regularization methods interact;weighting alters learned;entropy loss optimized;deep nonlinear;linear networks ii;unregularized networks;effect importance weighting;learned model early", "pdf_keywords": ""}, "6dfecb5915e8b10841abe224c5361bbda7100637": {"ta_keywords": "based morphological parsers;morphological parsers tigrinya;morphological parsers;languages based parser;parsers;parsers tigrinya;parsers produce;parser;parsers tigrinya oromo;based parser;state paradigm parsers;morphological segmentation english;based parser combinator;based morphological;rule based morphological;paradigm parsers produce;parser combinator;paradigm parsers;parsers produce multiple;entity detection;detection linking humanitarian;morphological segmentation;lemmatization morphological segmentation;morphological;parser combinator finite;entity detection linking;segmentation english word;input entity detection;humanitarian need detection;segmentation english", "pdf_keywords": ""}, "73e401dead436aabd0cd9c941f7b13bfdeda9861": {"ta_keywords": "distributed nonconvex optimization;efficient distributed training;efficient distributed nonconvex;communicationefficient training point;gradient communication;compressors communication efficient;nonconvex optimization algorithms;communication efficient distributed;distributed training;distributed training better;gradient communication mechanisms;class gradient communication;nonconvex optimization;communicationefficient training;efficient distributed;point compressors communication;distributed nonconvex;training point compressors;3pc efficient distributed;compressors communication;communication efficient;lazy aggregation;theory lazy aggregation;mechanisms communicationefficient training;lazy aggregation literature;lazy aggregation general;compressors 3pc efficient;foundations lazy aggregation;communicationefficient;optimization algorithms", "pdf_keywords": "distributed nonconvex optimization;point compressors communication;communicationef\ufb01cient training point;gradient communication;compressors communication ef\ufb01cient;ef\ufb01cient distributed training;training point compressors;nonconvex optimization algorithms;distributed training;gradient communication mechanisms;compressors communication;distributed training better;nonconvex optimization;3pc point compressors;point compressors 3pc;class gradient communication;ef\ufb01cient distributed nonconvex;distributed nonconvex;communicationef\ufb01cient training;communication ef\ufb01cient distributed;compressors 3pc ef\ufb01cient;point compressors;compressors 3pc;3pc ef\ufb01cient distributed;mechanisms communicationef\ufb01cient training;optimization algorithms;communication mechanisms communicationef\ufb01cient;optimization;training point;optimization algorithms advantage"}, "1c2c9e5d0588599516a78adda1fe3935dc5ae5d7": {"ta_keywords": "monotone games complexity;free play convex;derivative free play;play convex games;convex games;strongly monotone games;efficiency estimate actually;efficiency estimate;games complexity;games complexity target;free play strongly;known efficiency guarantee;play convex;monotone games;known efficiency;play strongly monotone;shows efficiency estimate;reduces known efficiency;unconstrained optimization;method unconstrained optimization;rates derivative free;efficiency;derivative free;complexity target accuracy;efficiency guarantee;free play;unconstrained optimization 2018;efficiency guarantee method;optimization 2018;shows derivative free", "pdf_keywords": ""}, "311381feeb6346bfcb2ba622bd8f713261a4075d": {"ta_keywords": "comment ranking edit;comments edits document;user comments edits;edits comments crucial;tasks comment ranking;comments edits;edits comments paper;edits comments;comment ranking;relationship comments edits;ranking edit anchoring;comment ranking achieve;relationship edits comments;comments edits defining;94 comment ranking;ranking edit;edits document revision;profusion edits comments;document revision reliably;edits comments multiple;edits document;collaborative documents;user track document;tackles comment ranking;document revision;relationship user comments;modeling relationship edits;user comments;comments paper;related tasks comment", "pdf_keywords": ""}, "efcdb62b59e4dfb3f51b53850a81d6149ec3dfc8": {"ta_keywords": "machine learning iml;interpretable machine learning;iml methods useful;iml methods;iml methods panacea;validate iml methods;viewing iml methods;validate iml;learning iml;iml methods narrower;define validate iml;interpretable machine;field interpretable machine;learning iml significant;instead viewing iml;connect iml methods;systematically connect iml;interpretable;viewing iml;methods formalize;overly broad use;iml;machine learning;target use cases;methods target use;field interpretable;cases methods formalize;broad use cases;methods use;relevant methods target", "pdf_keywords": "machine learning iml;learning iml;interpretable machine learning;iml methods useful;iml methods;existing iml methods;validate iml;iml methods panacea;learning iml grew;validate iml methods;viewing iml methods;de\ufb01ne validate iml;iml methods narrower;interpretable machine;diagnostics \ufb01eld interpretable;systematically connect iml;iml methods moving;argue iml community;existing iml;iml community;new existing iml;iml grew concerns;ai;\ufb01eld interpretable machine;ai instead viewing;connect iml methods;viewing iml;iml community embrace;instead viewing iml;ai instead"}, "null": {"ta_keywords": "hierarchical clustering efficient;hierarchical clustering;hierarchies graphs;model hierarchies graphs;tree sampling divergence;probabilistic model hierarchies;tree sampling;perform hierarchical clustering;high quality hierarchies;hierarchies model learns;cost tree sampling;tree based hierarchies;quality hierarchies;quality hierarchies discovered;hierarchies graphs obtained;quality hierarchies present;relaxation tree;highlighting quality hierarchies;link prediction;clustering efficient end;connections markov chain;hierarchies;clustering efficient;relaxation tree based;continuous relaxation tree;hierarchical;hierarchies discovered;hierarchies model;connections markov;hierarchies present", "pdf_keywords": ""}, "42fc352a0db1e742b0248a02b812db4aaf7b2cd3": {"ta_keywords": "neural machine translation;translation using energy;reranking improving neural;machine translation nmt;energy based ranking;autoregressive neural machine;autoregressive neural;task measure energy;machine translation;energy based reranking;mle task measures;estimation mle task;nmt energy;nmt energy based;studied autoregressive neural;improving neural machine;models mle training;autoregressive nmt computational;alternative training algorithms;mle task;training objective task;ranking ebr energy;improving neural;task measures bleu;translation nmt;machine translation using;train energy;observation train energy;measure energy based;neural machine", "pdf_keywords": "reranking improving neural;translation using energy;neural machine translation;energy based reranking;energy based ranking;machine translation nmt;energy ranker consistently;energy ranker;machine translation;autoregressive neural machine;reranking improving;based reranking improving;estimation mle task;stability energy ranker;autoregressive neural;autoregressive nmt computational;mle task;nmt energy;task measure energy;mle task measures;ranker consistently improves;machine translation using;train energybased model;energybased model;translation nmt;nmt energy based;sentences joint energy;reranking;studied autoregressive neural;improving neural machine"}, "740afdd1619d797145b056877865f941891e6a65": {"ta_keywords": "sfpark dynamic pricing;dynamic pricing pilot;dynamic pricing;loss minimization;parking rates algorithms;expected loss minimization;sfpark dynamic;data sfpark dynamic;loss minimization given;loss function oracle;simply loss function;loss function;parking rates;data sfpark;pricing pilot;distribution dependent decision;problem expected loss;sfpark;pricing;pricing pilot study;rates algorithms operate;duction parking rates;gradient oracle;mix updating decision;updating decision;prices result improvement;decision repeatedly length;novel algorithms information;minimization given data;parking", "pdf_keywords": "minimization geometrically decaying;risk minimization geometrically;loss minimization;expected loss minimization;stochastic gradient methods;order stochastic gradient;decision iteration;risk minimization;decision iteration complexity;loss minimization given;decision length epoch;dependent risk minimization;algorithms dynamic setting;stochastic gradient;geometrically decaying dynamic;algorithms dynamic;updating decision iteration;decaying dynamic;loss function oracle;gradient methods logarithmic;geometrically decaying;distribution dependent decision;gradient oracle;decaying dynamic environments;performatively optimal points;decision length;performatively optimal;propose algorithms dynamic;order gradient oracle;target performatively optimal"}, "b9c026ab6e161a0f8c4b4db82ee8ad10792084cc": {"ta_keywords": "electrolaryngeal speech enhancement;el speech enhancement;speech enhancement;electrolaryngeal el speech;voice conversion electrolarynx;approach electrolaryngeal speech;speech enhancement based;speech enhancement noise;electrolaryngeal speech;el speech experimental;el speech minimizing;parameters el speech;noise el speech;produce el speech;noise reduction voice;sounds enable laryngectomees;speech minimizing degradation;reduction voice conversion;voice conversion;methods el speech;voice conversion hybrid;statistical voice conversion;el speech using;voices converted acoustic;laryngectomees produce el;voice conversion method;speech keeping intelligibility;compared el speech;parameters voice conversion;adding noise el", "pdf_keywords": ""}, "066f2023b2b5ba5df61dc193c205785fa5e73fed": {"ta_keywords": "kernel clustering regularization;regularization kernel clustering;partitioning kernel clustering;bounds kernel clustering;kernel clustering;kernel clustering criteria;kernel clustering work;optimization kernel cut;kernel cut algorithm;data partitioning kernel;partitioning kernel;regularization kernel;clustering regularization;bound optimization kernel;regularization based segmentation;clustering regularization based;kernel spectral bounds;spectral bounds kernel;explain regularization kernel;kernel cut;optimization kernel;kernel spectral;linear kernel spectral;bounds kernel;regularization based;data partitioning;methodologies data partitioning;segmentation;regularization objectives existing;regularization", "pdf_keywords": ""}, "9578679e028777dd709881f938114aa59fbbf481": {"ta_keywords": "matching entity names;entity names overall;java toolkit matching;information retrieval jaro;string distance metrics;probabilistic record linkage;matching entity;compare string distance;information retrieval;entity names;toolkit matching methods;record linkage community;token based distance;string distance scheme;matching methods;experimentally compare string;based distance metrics;used information retrieval;edit distance metrics;toolkit matching;metrics task matching;distance metrics fast;heuristic string comparators;retrieval jaro;distance metrics hybrid;string distance;metrics fast heuristic;linkage community;record linkage;compare string", "pdf_keywords": ""}, "ab627ba77dced941f9f45eeaee17bc6644308d89": {"ta_keywords": "classification email messages;collective classification email;classification email speech;email act classification;classification email;consider classification email;text classification;collective classification;text classification algorithm;based collective classification;new text classification;collective classification method;email speech acts;correlation email messages;features collective classification;certain email acts;act classification;email speech;classification;email messages;email acts;classifiers maximum entropy;email messages thread;certain email;sequential correlation email;email messages contain;consider classification;contain certain email;classifiers;correlation email", "pdf_keywords": ""}, "3429d0529d3e77f9e4606f13b2d252d5d964abad": {"ta_keywords": "extracting information web;issues extracting information;extracting information;issues extracting;information web;extracting;web;information;issues", "pdf_keywords": ""}, "66b83f0801d0c2d4194ff60c5ef9c754b51ce521": {"ta_keywords": "interpretable image segmentation;segmentation pancreas ct;segmentation pancreas;method segmentation pancreas;noise masks interpretable;learnable noise masks;masks interpretable image;segmentation models learning;image segmentation models;pancreas ct scans;networks dnns;segmentation models;noise learnable;masks interpretable;learning regions images;models learning regions;interpreting image segmentation;learnable noise;image segmentation;interpretable image;neural networks dnns;learning regions;networks dnns widely;noise learnable noise;segmentation;image segmentation introduce;noise masks;regions images noise;pancreas ct;deep neural networks", "pdf_keywords": "interpretable image segmentation;masks interpretable image;segmentation models learning;segmentation pancreas ct;method segmentation pancreas;image segmentation models;segmentation pancreas;noise masks interpretable;networks dnns;interpretable image;segmentation models;models learning regions;learnable noise masks;masks interpretable;networks dnns widely;learning regions images;interpreting image segmentation;neural networks dnns;deep neural;learning regions;pancreas ct scans;image segmentation;deep neural networks;noise learnable;segmentation;learnable noise;image segmentation teddy;noise learnable noise;able interpret models;ct scans qualitatively"}, "568efa8d71d8f2a086c8debcdf547e7053269021": {"ta_keywords": "toy enhanced learning;game intelligent toy;digital game intelligent;intelligent toy enhanced;digital game;conference digital game;toy enhanced;intelligent toy;game intelligent;enhanced learning digitel;international conference digital;learning digitel 2012;enhanced learning;toy;digital;2012 ieee fourth;conference digital;digitel 2012 takamatsu;2012 ieee;learning digitel;digitel 2012;learning;ieee fourth international;2012 takamatsu japan;ieee fourth;game;2012 takamatsu;ieee;enhanced;digitel", "pdf_keywords": ""}, "4f0d485cbcde840533f23f0c8b0f3fa1ca2d74df": {"ta_keywords": "transductive linear bandit;transductive linear bandits;generalizes linear bandits;linear bandit problem;linear bandits;linear bandits alternatively;linear bandit;linear bandits mathcal;bandits sequential experimental;bandit problem;bandit problem given;subset combinatorial bandits;combinatorial bandits;combinatorial bandits sequential;bandits sequential;bandits alternatively recommender;bandits alternatively;bandits mathcal;bandits;bandit;bandits mathcal standard;sequentially chosen noisy;example drug discovery;set measurement vectors;measurement vectors limited;transductive setting naturally;discovery;transductive setting;measurement vectors mathcal;introduce transductive linear", "pdf_keywords": "transductive linear bandits;transductive linear bandit;algorithm linear bandits;linear bandits nearly;linear bandit problem;linear bandits;linear bandits tanner;linear bandit;bandit problem;bandit problem given;bandits nearly achieves;bandits nearly;bandits;lower bounds transductive;bandits tanner;bandit;superior algorithm empirically;bounds transductive;bandits tanner fiez;bounds transductive setting;transductive setting algorithm;algorithm empirically competitive;competitive previous algorithms;asymptotic algorithm linear;algorithm empirically;sample complexity rage;sample complexity;non asymptotic algorithm;design transductive linear;paper introduce transductive"}, "7b19e6540c786b80a3615a8ae2ef706242a1fa5b": {"ta_keywords": "compressive phase retrieval;robust compressive phase;noisy compressive phase;phase retrieval sparse;compressive phase;tackle compressive phase;fast robust compressive;robust compressive;noisy compressive;sparse complex signal;phase retrieval problem;sparse graph codes;recover sparse complex;phase retrieval;recover sparse;compressive;phasecode algorithm robustify;sparse complex;retrieval sparse graph;phasecode algorithm;problem recover sparse;noisy quadratic measurements;recovers sample complexity;sparse graph;retrieval sparse;complexity log sublinear;noisy quadratic;complexity log sup;quadratic measurements sub;phasecode", "pdf_keywords": "compressive phase retrieval;complexity compressive phase;robust compressive phase;phase retrieval sparse;noisy compressive phase;compressive phase;tackle compressive phase;phase retrieval problem;phase retrieval;computational complexity compressive;complexity compressive;phasecode algorithm robustify;phasecode algorithm;fast robust compressive;robust compressive;scheme noisy compressive;sparse complex signal;recover sparse complex;phasecode;architecture phasecode algorithm;noisy compressive;sparse graph codes;compressive;sparse complex;recover sparse;architecture phasecode;retrieval sparse graph;retrieval sparse;problem recover sparse;use architecture phasecode"}, "a0cbaf59f563580f68523ab6839a436e38b6db18": {"ta_keywords": "multilingual corpus training;neural machine translation;multilingual neural machine;translation nmt multilingual;selection multilingual neural;sampling distribution multilingual;multilingual data minimizes;nmt multilingual corpus;multilingual corpus;multilingual neural;machine translation;machine translation nmt;data selection multilingual;distribution multilingual data;selection multilingual;distribution multilingual;multilingual data;low resource language;nmt multilingual;low resource neural;machine translation paper;high resource language;multilingual;translation nmt;translation paper seek;resource language experiments;improvements languages;bleu improvements languages;corpus training;translation paper", "pdf_keywords": "sampling helpful multilingual;multilingual data training;selection multilingual neural;selection multilingual mt;multilingual neural machine;neural machine translation;data selection multilingual;multilingual corpora training;translation nmt multilingual;machine translation nmt;machine translation;multilingual mt selects;selection multilingual;multilingual neural;helpful multilingual data;languages minimizing training;multilingual data constructing;nmt multilingual corpora;selection framework multilingual;multilingual data;multilingual mt;machine translation xinyi;nmt multilingual;multilingual corpora;conditioned sampling optimizing;high resource language;data languages minimizing;target conditioned sampling;helpful multilingual;multilingual"}, "105146a7872835a52c8c5c55a3aae62c5d8852a1": {"ta_keywords": "analysis transliteration tokenization;morphological analysis transliteration;transliteration tokenization;translation naive alignment;lexical processing translation;analysis transliteration;character based translation;machine translation traditionally;translation character based;transliteration;transliteration tokenization required;machine translation;processing translation;based machine translation;pairs machine translation;machine translation sees;word based translation;translation distant language;based translation character;translation traditionally formulated;processing translation steps;based translation naive;translation character;based translation distant;additional lexical processing;transduction strings words;lexical processing steps;tokenization;phrase based machine;language pairs machine", "pdf_keywords": ""}, "9d10bbd21f475d500c3a7e24052e02596e052e2e": {"ta_keywords": "speech recognizer;article speech recognizer;speech recognition;speech recognizer called;neural network speech;network speech recognizer;efficient speech recognition;language models jlaser;jlaser neural network;speech recognition state;speech recognizer statement;jlaser based neural;networks efficient speech;models jlaser neural;efficient speech;gram language models;mixtures gram language;gaussian mixtures gram;network speech;recognizer called jlaser;neural networks efficient;recognition vocabulary;neural networks;based gaussian mixtures;recognizer;neural network;language models;huge recognition vocabulary;recognition vocabulary article;jlaser neural", "pdf_keywords": ""}, "36d193c7a9523f55f9fe5ffd0730f248c241f5c7": {"ta_keywords": "bias peer review;peer review sociotechnical;fairness bias peer;peer review;peer review peer;bias peer;improve peer review;ii peer review;review peer;review peer review;tutorial fairness bias;fairness bias;bias unfairness aaai;challenges pertaining bias;2020 tutorial fairness;bias unfairness;peer review backbone;pertaining bias unfairness;tutorial fairness;review sociotechnical intelligent;improve peer;review sociotechnical;need improve peer;sociotechnical intelligent;unfairness aaai tutorial;peer;unfairness aaai;sociotechnical intelligent systems;ii peer;fairness", "pdf_keywords": ""}, "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad": {"ta_keywords": "commonsense natural language;new task commonsense;task commonsense natural;natural language inference;commonsense inference;new challenge dataset;dataset achieve adversarial;task commonsense;state art adversarial;forward nlp;achieve adversarial;nlp research benchmarks;adversarial;achieve adversarial filtering;challenge dataset achieve;commonsense inference proves;path forward nlp;paper commonsense inference;adversarial filtering;challenge dataset;language inference;forward nlp research;adversarial way;nlp;adversarial way present;adversarial filtering af;iteratively select adversarial;adversarial set machine;hellaswag new challenge;commonsense natural", "pdf_keywords": "benchmarks adversarially evolve;benchmarks adversarially;state art adversarial;adversarially evolve;challenging testbed commonsense;forward benchmarks adversarially;adversarially;models achieve adversarial;testbed commonsense reasoning;dataset adversarial robust;constructing dataset adversarial;achieve adversarial;produced dataset adversarial;dataset adversarial;adversarial;models language generation;adversarial robust;adversarially evolve evolving;adversarial robust models;nlp research benchmarks;nlp progress;testbed commonsense;dataset adversarial \ufb01ltering;language generation discrimination;achieve adversarial filtering;adversarial \ufb01ltering;adversarial way;forward nlp research;art adversarial;forward nlp"}, "de7d0c87794c3de6f8ab2c753ecc398c18c26631": {"ta_keywords": "extracts grammatical specification;grammatical specification raw;automatic extraction rules;pass grammatical specification;governing morphological agreement;grammatical specification nearly;grammars world languages;descriptive grammar language;extracting rules describing;grammar language;grammatical specification;core grammars world;describing agreement morphosyntactic;grammars world;grammar language indispensable;morphological agreement;creating descriptive grammar;rules governing morphological;framework extracts grammatical;extraction rules governing;core grammars;annotations language;extracting rules;extraction rules;governing morphological;specification raw text;agreement morphosyntactic;language framework extracts;language documentation preservation;expert annotations language", "pdf_keywords": "governing morphological agreement;describing agreement morphosyntactic;rules governing morphological;morphological agreement;agreement morphosyntactic;governing morphological;morphological agreement aditi;extracting rules describing;grammars world languages;languages creation grammar;automatic extraction rules;descriptive grammar language;agreement morphosyntactic phenomenon;core grammars world;creation grammar sketch;morphological;grammar language;extraction rules governing;grammars world;extraction rules;creating descriptive grammar;extracting rules;core grammars;grammar sketch concise;grammar language indispensable;extracted rules;demonstrating extracted rules;creation grammar;grammar sketch;phenomenon core grammars"}, "b709da495f15a4a9c1173192ecd755d1697dedf0": {"ta_keywords": "recommendation saccharomyces genome;online recommendation saccharomyces;reading recommendations saccharomyces;recommendations saccharomyces genome;personalized reading recommendations;saccharomyces genome database;reading recommendation tasks;recommendation saccharomyces;develop reading recommendation;reading recommendation;based reading recommendation;reading recommendations;genome database;effectively personalized reading;genome database available;reading recommendation requires;reading citation history;literature effectively personalized;users reading citation;recommendation tasks;personalized reading;citation based history;recommendations saccharomyces;genome database experiments;collaborative filtering;recommendation tasks leveraging;collaborative filtering approaches;saccharomyces genome;database experiments citation;citation history rapid", "pdf_keywords": ""}, "bef4548a43fca8a7410734e4200157d50e257a29": {"ta_keywords": "self attention asr;attention asr tasks;speech recognition asr;automatic speech;attention asr;end asr adaptive;automatic speech recognition;asr adaptive span;speech recognition;language modeling transformers;asr adaptive;speech processing;processing speech;concern automatic speech;span self attention;recognition asr input;recognition asr;adaptive span self;language processing speech;processing speech processing;speech processing method;adaptive span;language modeling;asr tasks;input sequence layer;proposed language modeling;called adaptive span;asr tasks originally;asr input;sequence layer", "pdf_keywords": ""}, "dad121213a17c6cc977d2298c7a9927639ca58e6": {"ta_keywords": "hearing impairment simulation;approximation auditory filter;auditory filter loudness;simulation using audiogram;auditory filter;filter loudness compensation;loudness compensation;hearing impairment;based approximation auditory;audiogram based approximation;using audiogram based;impairment simulation using;approximation auditory;using audiogram;audiogram based;impairment simulation;filter loudness;audiogram;auditory;hearing;loudness;simulation using;impairment;simulation;based approximation;compensation;filter;approximation;based;using", "pdf_keywords": ""}, "9f832bdcbc9d9566f7ab07b7455364bee62086fb": {"ta_keywords": "statistical machine translation;code natural language;generate pseudo code;source code pseudo;pseudo code source;machine translation smt;pseudo code python;code source code;code pseudo code;code using statistical;python source code;japanese pseudo code;code python source;automatically generate pseudo;source code smt;machine translation;pseudo code describes;pseudo code;code pseudo;source code using;translation smt pseudogen;corresponding pseudo code;source code way;source code;pseudogen tool automatically;pseudo code pairs;language english pseudogen;code source;smt pseudogen tool;code smt", "pdf_keywords": ""}, "f79361dda56ee755fc56ab83cf0d9f12d42b2d5e": {"ta_keywords": "ranking pairwise comparisons;optimal ranking pairwise;ranking pairwise;pairwise comparisons items;recovering ranking items;alternatively recovering ranking;ranking items analyze;ranking items;pairwise comparisons;number pairwise comparisons;recovering ranking;robust optimal ranking;optimal ranking;algorithm ranks items;counting algorithm ranks;pairwise comparisons won;pairwise comparison probabilities;comparisons items goal;comparisons items;algorithm ranks;ranking;pairwise comparison;ranks items order;copeland counting algorithm;ranks items;form pairwise comparisons;analyze copeland counting;matrix pairwise comparison;copeland counting;underlying matrix pairwise", "pdf_keywords": "countingbased method ranking;counting algorithm ranks;copeland counting algorithm;robust copeland counting;algorithm ranks;algorithm ranks items;classical countingbased method;analyze copeland counting;method ranking;ranking;number pairwise comparisons;counting algorithm imposing;counting algorithm achieves;performance counting algorithm;copeland counting;ranking called;analyze classical countingbased;pairwise comparisons;method ranking called;counting algorithm;ranking called copeland;pairwise comparisons won;counting algorithm following;classical countingbased;analyze performance counting;countingbased method;performance counting;pairwise comparison probabilities;ranks items order;matrix pairwise comparison"}, "092687dc06b0b264a524c6d4ea151780ba85a02a": {"ta_keywords": "verbal communiation autism;communiation autism plus;communiation autism;measured autism spectrum;autism spectrum;traits measured autism;information correlation autistic;measured autism;nonverbal communication training;non verbal communication;autistic traits measured;autism plus uses;verbal communication skills;behavior recognition skills;autism;recognizing non verbal;non verbal communiation;nonverbal communication;autistic traits;correlation autistic traits;recognize non verbal;correlation autistic;modalities nonverbal communication;social communication skills;autism spectrum quotient;verbal behavior recognition;communication skills social;autism plus;non verbal behaviors;training non verbal", "pdf_keywords": ""}, "2d8d51d483a50c6fbf16a0cc120465539f4055da": {"ta_keywords": "microblog posts languages;quantifying said tweet;microblog text twitter;microblog text;languages information character;information character languages;information microblog;posts languages information;languages information;text twitter;information microblog post;languages larger character;correlation information microblog;information character necessarily;languages larger;microblog posts;post microblog text;japanese contain information;expected languages larger;posts languages;paper describes multilingual;information character;multilingual;character languages somewhat;character languages;tweet results;information character does;single post microblog;tweet results expected;text twitter 26", "pdf_keywords": ""}, "27ad78b72c3fb77a117b15855008b65e838314e8": {"ta_keywords": "paper index;index;paper", "pdf_keywords": ""}, "51b2dd5cbec02f016c6fa716705ede9b3846a410": {"ta_keywords": "shinji watanabe gigaspeech_asr_train_asr_raw_en_bpe5000_valid;watanabe gigaspeech_asr_train_asr_raw_en_bpe5000_valid;espnet2 pretrained model;watanabe gigaspeech_asr_train_asr_raw_en_bpe5000_valid acc;espnet2 pretrained;gigaspeech_asr_train_asr_raw_en_bpe5000_valid;gigaspeech_asr_train_asr_raw_en_bpe5000_valid acc ave;gigaspeech_asr_train_asr_raw_en_bpe5000_valid acc;espnet2;pretrained model shinji;fs 16k lang;pretrained model;model shinji watanabe;pretrained;16k lang;16k lang en;fs 16k;model shinji;ave fs 16k;16k;shinji watanabe;shinji;model;fs;en;watanabe;lang en;lang;ave fs;acc ave fs", "pdf_keywords": ""}, "3e86ecbd41ab55b90d3b45601aeb15d2e5c1c8f8": {"ta_keywords": "balanced knockout tournaments;draws given player;knockout tournaments;tournament prove checking;tournament prove;particular player tournament;player tournament prove;draw player wins;knockout tournaments common;player tournament;tournament;tournaments;draws given;given player winner;draws;tournaments common;optimal draw;competitions;counting number draws;finding optimal draw;sports competitions;number draws given;draw player;number draws;player wins np;wins np complete;tournaments common formats;formats sports competitions;competitions used elections;draw particular player", "pdf_keywords": ""}, "7580df14bf01438e7174bbff260508a39a44df84": {"ta_keywords": "erasure codes distributed;explicit erasure codes;sparse codes;distributed storage codes;practice sparse codes;erasure codes;storage codes fast;codes distributed storage;sparse codes roughly;repair optimal codes;storage codes;encoding enabled sparse;codes enabling sparsity;optimal codes;fast encoding sparsity;codes fast encoding;codes distributed;encoding practice sparse;optimal codes enable;encoding sparsity;matrix pm codes;storage repair optimal;encoding sparsity present;fast encoding;sparser standard counterparts;systematic distributed storage;fast encoding practice;storage repair;optimal repair bandwidth;sparse generator matrix", "pdf_keywords": "erasure codes distributed;distributed storage codes;codes distributed storage;explicit erasure codes;storage codes fast;abstract erasure codes;erasure codes increasingly;erasure codes;storage codes;systematic distributed storage;repair optimal codes;distributed storage;reliability minimum storage;distributed storage following;storage repair optimal;codes distributed;distributed storage systems;used distributed storage;reliability lower storage;fast encoding minimize;encoding minimize cost;optimal codes;storage repair;codes fast encoding;optimal repair bandwidth;minimum storage;existing storage repair;storage following desirable;minimum storage overhead;constructing explicit erasure"}, "92fc770721e95249f8db01c5019d1cc4cf79ff00": {"ta_keywords": "query language natural;natural language query;language query hubble;query language;command query language;telescope proposal selection;query program tacos;query program;space telescope proposal;query hubble;telescope proposal;language query;use query program;query hubble space;easy use query;sentence processing;telescope;use query;space telescope assisted;language natural language;space telescope;natural language;hubble space telescope;standard command query;telescope assisted;methods sentence processing;command query;selection process hubble;query;selection proposal selection", "pdf_keywords": ""}, "6c5144872c259611dceb32fe4e4486a6865e6c42": {"ta_keywords": "stationary noise estimation;speech recognition decomposition;decomposition robust speech;robust speech recognition;noise estimation;noise estimation method;robust speech;stationary noise decomposed;speech recognition;noise suppression;noise decomposed stationary;stationary noise sequences;noise sequences components;noise suppression problem;component noise frame;kalman filter;noise decomposed;recognition decomposition;extended kalman filter;bias component noise;noise frame;noise frame problem;kalman filter em;estimation non stationary;component decomposition robust;suppression problem estimation;addresses noise suppression;non stationary noise;residual component decomposition;stationary noise", "pdf_keywords": ""}, "cf6352c789ab51320fa7ca9b1440c685b57fd769": {"ta_keywords": "speaker segments diarization;speech activity detector;xvector embeddings trained;speaker segments;microphone recordings variety;speaker activity currently;speaker activity;microphone recordings;clustered xvector embeddings;provided microphone recordings;microphone data;dihard diarization evaluation;diarization hard experiences;refinement speech activity;task provided microphone;diarization evaluation;consider speaker activity;diarization evaluation new;speech activity;recordings variety difficult;bayesian refinement speech;trained wideband microphone;microphone data followed;embeddings trained wideband;recordings variety;microphone;xvector embeddings;wideband microphone data;recordings;provided microphone", "pdf_keywords": ""}, "2e1a1588955a8a64ec618b3cc04be961ed0cb59c": {"ta_keywords": "oligothiophenes transfer learning;state energy oligothiophenes;energy oligothiophenes tddft;oligothiophenes tddft calculations;energy oligothiophenes;phase oligothiophenes transfer;energies poly hexylthiopnene;state energies poly;p3ht single crystal;oligothiophenes transfer;oligomers polymers ml;gas phase oligothiophenes;energies poly;oligomers polymers;conjugated oligomers polymers;phase oligothiophenes;excited state energies;oligothiophenes tddft;chemistry modeling optoelectronic;excited state energy;poly hexylthiopnene p3ht;transfer learning;transfer learning predicted;learning predicted excited;polymers ml;transfer learning models;oligothiophenes;state energies;polymers;conjugated oligomers", "pdf_keywords": ""}, "9237d6efc603465765e80eb5ca1268c2bd7b5c24": {"ta_keywords": "comparison language generation;machine translation;machine translation implements;language generation systems;language generation tasks;tasks machine translation;language generation;systems language generation;comparison language;holistic comparison language;generation tasks machine;generation tasks;translation implements;tool holistic comparison;linguistic labels source;comparison results systems;results systems language;linguistic labels;analysis accuracy generation;use linguistic labels;sentence accuracies counts;linguistic;use linguistic;histograms sentence accuracies;compare mt tool;features use linguistic;words bucketed histograms;salient differences systems;translation;counts based salient", "pdf_keywords": "comparison language generation;translation language generation;language generation systems;results language generation;machine translation language;holistic comparison language;machine translation;language generation tasks;systems language generation;comparison language;language generation;results machine translation;tool holistic comparison;machine translation conclusion;tasks machine translation;translation language;holistic comparison analysis;comparison results systems;generation tasks machine;source tool holistic;generation tasks;compare mt tool;holistic analysis comparison;holistic comparison;analysis results language;tool holistic analysis;results systems language;generation systems;mt tool holistic;results language"}, "a6336fa1bcdeb7c84d2c4189728f0c1b2b7d0883": {"ta_keywords": "neural networks recurrent;recurrent neural networks;sequence learning recurrent;learning recurrent neural;networks recurrent;recurrent networks;learning recurrent;networks recurrent networks;recurrent networks retain;short term memory;lstm bidirectional brnn;networks rnns;recurrent neural;neural networks rnns;term memory lstm;memory lstm bidirectional;sequence learning;memory lstm;lstm;networks sequence learning;lstm bidirectional;neural networks sequence;review recurrent neural;networks rnns connectionist;rnns;rnns connectionist models;learn inputs sequences;rnns connectionist;large scale learning;recurrent", "pdf_keywords": "recurrent networks translation;recurrent networks;recurrent neural networks;sequence prediction tasks;output recurrent neural;research recurrent neural;recurrent neural;learning sequences;success recurrent networks;input output recurrent;representations natural language;output recurrent;metrics sequence prediction;sequence prediction;translation image captioning;learning sequences year;recurrent;networks learning sequences;image captioning;captioning;research recurrent;image captioning new;captioning new;prediction tasks;survey research recurrent;representations natural;networks translation;natural language;introduce representations natural;learning models"}, "7b7416c90e8d3fc9ad5c9fb3923a638f69294ed7": {"ta_keywords": "entity mention corpus;domain knowledge intensive;question answering;domain question answering;assimilating factual information;retrieving assimilating factual;mention memory approach;knowledge mention memory;large text corpus;mention corpus;mention corpus proposed;representations entity mention;source factual knowledge;knowledge intensive tasks;representation large text;open domain knowledge;entity mention;question answering require;assimilating factual;model natural language;language understanding tasks;million wikipedia;entity mention input;million wikipedia mentions;150 million wikipedia;vector representations entity;reasoning disparate sources;knowledge intensive;text corpus transformer;factual information multiple", "pdf_keywords": "large text corpus;corpus collection mention;representations entity mention;representation large text;corpus retrieval;information corpus;textual information corpus;utilize textual information;text corpus transformer;text corpus;corpus retrieval results;corpus language model;corpus collection;knowledge corpus;memory attention layer;knowledge corpus collection;information text corpus;mention appears corpus;information corpus language;mention memory;text corpus language;textual information;utilize textual;entity mention;appears corpus retrieval;entity mention input;mention memory method;attention layer passage;layers entity mention;corpus language"}, "8be39979cb2eb1aeaba15b57e1e4bc712eb962cb": {"ta_keywords": "encouraging paragraph embeddings;paragraph embedding models;paragraph embeddings remember;paragraph embeddings;paragraph embedding;embeddings remember sentence;paragraph paragraph embedding;paragraph embedding method;improves paragraph reconstruction;paragraph reconstruction;sentence content task;paragraph reconstruction terms;improves paragraph;objective improves paragraph;embeddings remember;sentence content;encouraging paragraph;state art paragraph;sentence identity improves;embedding models remarkably;classification tasks learn;training better generalization;paragraph;paragraph paragraph;art paragraph embedding;formulate sentence content;downstream classification tasks;solving encouraging paragraph;classification tasks;embeddings", "pdf_keywords": "paragraph embeddings remember;encouraging paragraph embeddings;paragraph embedding models;paragraph embeddings capable;investigate paragraph embedding;powerful paragraph embeddings;paragraph embeddings;paragraph embedding;paragraph embedding method;improves paragraph reconstruction;embeddings remember sentence;reconstructing long paragraphs;abstract paragraph embedding;paragraph reconstruction terms;embed paragraph;methods embed paragraph;paragraph reconstruction;embeddings remember;improves paragraph;embed paragraph single;learn powerful paragraph;objective improves paragraph;encouraging paragraph;paragraph single vector;paragraphs;long paragraphs;embedding models remarkably;semantic similarity;semantic similarity relatedness;investigate paragraph"}, "f9ee690d223beac6d893aedae13c09dbf0fb694e": {"ta_keywords": "semantic verb clustering;verb clustering;verb clustering addition;processing nlp lexical;dirichlet process mixture;dpmms particular clustering;nlp lexical semantic;nlp lexical;lexical semantic verb;mixture models dpmms;natural language processing;process mixture models;processing nlp;lexical semantic;language processing nlp;clustering solution;particular clustering;clustering solution using;clustering;mixture models;dirichlet process;particular clustering solution;apply dirichlet process;semantic verb;dpmms learning task;task natural language;language processing;process mixture;semantic;models dpmms learning", "pdf_keywords": ""}, "7354b87a1b4c99ccd9cf25b7314927ced8b156f7": {"ta_keywords": "interactive writing assistant;improved writing assistance;controllable writing assistants;grained author specifications;writing assistant generates;significantly improved writing;writing assistants;improved writing;interactive writing;writing assistant;pretrained language models;writing assistance;writing assistants explored;build interactive writing;assistant generates rephrases;labeled author intent;generates rephrases text;advances language modeling;author intent allows;language modeling;writing assistance functionalities;language models significantly;fine grained author;language models;language modeling build;assistance functionalities autocomplete;generated text users;intent guided assistant;heuristically labeled author;creative writing task", "pdf_keywords": "guided authoring assistant;interactive writing assistant;human ai authoring;authoring assistant capable;authoring assistant;interactive writing;ai authoring;intent guided authoring;build interactive writing;writing assistant generates;ai authoring means;guided authoring;author design interactive;grained author speci\ufb01cations;writing assistant;authoring means intent;authoring means;authoring;participants write help;text generation;text generation based;interactive human ai;allows participants write;participants write;assistant generates rephrases;controlled text generation;rhetorical instructions \ufb02uent;based explicit rhetorical;authoring assistant iga;rhetorical instructions"}, "0f395654e69cd2e063a6ef221fb66fb46e68cefd": {"ta_keywords": "truthful classifiers incentive;classifiers incentive strategically;classifiers incentive;truthful classifiers;training classifiers;classifiers;thebox classifiers;ensemble thebox classifiers;thebox classifiers trained;classifiers trained;training classifiers robust;classifier create incentive;characterization truthful classifiers;classifiers robust;classifier;classifiers robust behavior;problem training classifiers;chosen classifier;classifier specific;specific classifier;machine learning;classifiers trained using;machine learning techniques;ensemble thebox;hierarchical ensemble thebox;depends chosen classifier;chosen classifier specific;hierarchical ensemble;strategically withhold features;classifier specific classifier", "pdf_keywords": ""}, "60dd53fca1f538fabe18e4d6a9326b2f40e358dd": {"ta_keywords": "latent dirichlet allocation;statistical topic models;dirichlet allocation experimental;em latent dirichlet;dirichlet allocation;topic models;topic models latent;models latent dirichlet;latent dirichlet al;latent dirichlet;scalability parallelized variational;parallelized variational em;parallel implementations variational;summarize large document;em algorithm lda;parallelized variational;algorithm lda multiproces;large document collec;variational em algorithm;implementations variational em;algorithm lda;lda multiproces;lda multiproces sor;scalability parallelized;setting statistical topic;parallel implementations;speed scalability parallelized;variational em latent;document collections;implementations variational", "pdf_keywords": ""}, "084ddb77fce5a7f0b6418ef4e38dbb1bedf4ae78": {"ta_keywords": "el speech enhancement;speech enhancement;electrolaryngeal el speech;el speech preserving;speech enhancement applied;speech enhancement methods;laryngectomee el speech;el speech using;quality el speech;allow el speech;el speech demonstrated;intelligible el speech;speech proficient laryngectomees;proposed el speech;speech using device;speech demonstrated method;speech preserving;el speech highly;laryngectomee clear methods;laryngectomee el;proficiency laryngectomee clear;laryngectomees produce intelligible;el speech proficient;speaker evaluation simulation;el speech;statistical voice conversion;sounds generated electrolarynx;proficiency laryngectomee;single laryngectomee el;voice conversion", "pdf_keywords": ""}, "5b2c5eeea9ac8f26908b9dfc8fd0a2d0e7aa5bb1": {"ta_keywords": "deep lstm acoustic;lstm acoustic;lstm acoustic model;deep lstm;lstm architecture adaptively;units deep lstm;memory adaptive beamforming;lstm architecture;lstm;multichannel robust speech;adaptive beamforming networks;predicting beamforming filter;memory lstm;memory lstm architecture;far field speech;short term memory;predicting beamforming;recurrent neural network;term memory lstm;field speech recognition;deep learning;recent deep learning;adaptive beamforming;speech recognition noisy;robust speech;recurrent neural;speech recognition;robust speech recognition;beamforming networks;recognition noisy reverberant", "pdf_keywords": "deep lstm acoustic;lstm acoustic;lstm acoustic model;training lstm adaptive;training lstm;lstm adaptive beamformer;lstm adaptive beamforming;beamformer deep lstm;network lstm adaptive;lstm adaptive;trained deep lstm;lstm architecture adaptively;network deep lstm;deep lstm;integrated network lstm;lstm architecture;respectively lstm adaptive;network lstm;lstm;joint training lstm;updating deep lstm;adaptive beamformer deep;memory lstm;memory lstm architecture;acoustic model predict;recurrent neural network;beamforming network deep;term memory lstm;recurrent neural;use recurrent neural"}, "d38b686b8b68d0b91b294fd8a55ac7dea191706f": {"ta_keywords": "neural abstractive summarization;extensible guided summarization;abstractive summarization models;guided summarization;guided summarization framework;abstractive summarization;guided neural abstractive;summarization models flexible;summarization models;produce coherent summaries;summarization framework;summarization framework gsum;summarization;guided neural;coherent summaries;neural abstractive;contrast neural abstractive;framework guided neural;external guidance input;summaries;coherent summaries unfaithful;external guidance;kinds external guidance;guidance input;guided;guidance input perform;general extensible guided;summaries unfaithful;neural;contrast neural", "pdf_keywords": "neural abstractive summarization;extensible guided summarization;guided summarization;guided summarization framework;abstractive summarization models;sentence guided models;summarization models;abstractive summarization zi;summarization model dataset;summarization model;abstractive summarization;popular summarization datasets;summarization datasets;train summarization model;summarization framework;summarization models \ufb02exible;summarization generally;generate summaries;produce coherent summaries;generate summaries freely;guided neural abstractive;train summarization;summarization generally categorized;rouge popular summarization;summarization datasets using;summarization zi;popular summarization;summaries freely able;summarization framework gsum;text summarization generally"}, "e114618157e025ed17b7e45684d67becd34a14f3": {"ta_keywords": "distance mixtures gaussians;variation distance mixtures;mixtures gaussians;lower bounds mixtures;functional approximations mixtures;distribution learning analytically;approximations mixtures;complexity distribution learning;distribution learning;bounds mixtures exploiting;mixtures high dimensional;total variation distance;bounds mixtures;approximations mixtures high;dimensional gaussian distributions;distance mixtures;statistics learning theory;variation distance characteristic;high dimensional gaussian;variation distance;function mixture;mixtures exploiting;function mixture provide;gaussian distributions studied;characteristic function mixture;distributions studied extensively;gaussian distributions;statistics learning;bounds total variation;mixtures exploiting connection", "pdf_keywords": "distance gaussian mixtures;gaussian mixtures equally;gaussian mixtures;bounds pairs mixtures;weighted gaussians shared;gaussians shared variance;component dimensional mixtures;gaussians shared;gaussian total variation;variation distance gaussian;dimensional mixtures;single gaussian total;pair mixtures shared;mixtures shared component;total variation bounds;dimensional mixtures applying;mixtures component means;equally weighted gaussians;mixtures shared;pairs mixtures;analogous results mixtures;total variation distance;applications single gaussian;shared component variance;single gaussian results;pairs mixtures containing;function pair mixtures;pair mixtures;mixtures components;gaussian total"}, "3ed91aae1038b8b0130fb3974060a50b10de1345": {"ta_keywords": "speech recognition;accurate speech recognition;enable accurate speech;field automatic speech;speech recognition asr;speech recognition distance;automatic speech recognition;machine recognition speech;recognition speech;speech recognition signal;recognition speech spoken;automatic speech;spoken distance microphones;deep learning;speech spoken distance;home assistants spoken;accurate speech;spoken distance;microphones known far;speech end asr;recognition asr;distance microphones known;seen deep learning;recognition accuracy far;distance microphones;source separation acoustic;recognition signal enhancement;spoken language interface;separation acoustic beamforming;digital home assistants", "pdf_keywords": "clustering speech enhancement;spatial clustering speech;clustering speech;estimation spatial clustering;clustering unsupervised spatial;speaker scenarios unsupervised;speech enhancement;clustering multi talker;deep clustering spatial;unsupervised spatial clustering;spatial clustering unsupervised;microphone array judging;mask estimation spatial;microphone array;speech enhancement 55;neural mask estimator;multi speaker scenarios;spatial clustering;clustering spatial;spatial clustering technique;spatial clustering multi;clustering spatial clustering;clustering unsupervised;deep clustering;unsupervised spatial;spatial clustering approaches;speaker scenarios;based mask estimation;mask estimation;multi speaker"}, "addd2d86d19c1e7c8854e827fb2656a50c250440": {"ta_keywords": "aspect based summarization;existing summarization models;summarization models;aspect annotation results;based summarization large;challenges existing summarization;aspect annotation;summarization large;summaries aid efficient;summarization attempts spur;based summarization attempts;summarization attempts;existing summarization;domains sentiment product;based summarization;summarization large differences;summarization models face;summarization;summaries aid;sensitive events summaries;events summaries aid;domains sentiment;sentiment product features;domain aspect based;summaries;dataset using wikipedia;events summaries;proxy aspect annotation;aspects different domains;quickly understanding reviews", "pdf_keywords": "aspect based summarization;existing summarization models;challenges existing summarization;aspect identi\ufb01cation summarization;summarization models;aspect annotation propose;aspect annotation;summarization attempts spur;based summarization allows;based summarization attempts;existing summarization;summarization allows;summarization attempts;summarization allows train;summarization task examine;identi\ufb01cation summarization;identi\ufb01cation summarization using;based summarization;summarization using extractive;summarization;summarization models face;proxy aspect annotation;summarization task;based summarization paper;summarization paper;domain aspect based;summarization using;summarization paper construct;annotation propose straightforward;annotation propose"}, "0c3c4c88c7b07596221ac640c7b7102686e3eae3": {"ta_keywords": "biomedical question answering;pubmedqa answer research;pubmedqa qa dataset;pubmedqa dataset biomedical;answer questions pubmedqa;answering qa dataset;research question answering;question answering;questions pubmedqa dataset;question answering qa;pubmedqa dataset;pubmed abstracts;pubmedqa novel biomedical;pubmed abstracts best;pubmedqa qa;collected pubmed abstracts;dataset collected pubmed;dataset reasoning biomedical;answer research;questions pubmedqa;reasoning biomedical research;reasoning biomedical;conclusion introduce pubmedqa;answering qa;answers research;dataset biomedical research;pubmedqa answer;improvement task pubmedqa;answer research questions;introduce pubmedqa", "pdf_keywords": "biomedical question answering;biomedical qa dataset;answering qa dataset;pubmedqa dataset biomedical;pubmedqa answer research;research question answering;dataset reasoning biomedical;question answering;question answering qa;biomedical qa;building biomedical qa;reasoning biomedical research;qa dataset substantial;dataset collected pubmed;pubmedqa dataset;reasoning biomedical;collected pubmed abstracts;dataset biomedical research;qa dataset reasoning;pubmed use questions;pubmedqa \ufb01rst qa;\ufb01rst qa dataset;pubmed abstracts pubmedqa;pubmed abstracts;pubmedqa novel biomedical;qa dataset;questions task pubmedqa;answering qa;dataset biomedical;qa dataset collected"}, "b7731a9b9142a6deb132e99bc55ddbe458a537a6": {"ta_keywords": "online moment selection;causal effect efficiently;causal graph data;treatment effect estimation;probabilistic model causal;moment selection;effect estimation structural;estimation structural assumptions;estimation structural;subsets mediators confounders;moment selection oms;causal graph;assumptions given causal;estimates moments optimal;data fusion;moment conditions researchers;effect estimation;model causal;mediators confounders instrumental;given causal graph;moments optimal action;causal;estimate functional probabilistic;include subsets mediators;given causal;source query propose;confounders instrumental;confounders instrumental variables;mediators confounders;setup average treatment", "pdf_keywords": "online estimation causal;online moment selection;estimation causal;probabilistic model causal;causal effects deciding;moment selection;estimation causal effects;ef\ufb01cient online estimation;draft lottery dataset;synthetic datasets infant;synthetic datasets;moment selection oms;conditions propose selection;online estimation;semi synthetic datasets;model causal;datasets infant;datasets infant health;selection;lottery dataset;data fusion;selection strategies;causal;data sources;datasets;selection strategies explore;propose selection strategies;data sources available;lottery dataset arxiv;data source"}, "3ea5468e6d3007a94d4318932d7778693526145c": {"ta_keywords": "distributed computing resources;grid computing;grid computing using;resource allocation controller;resource management delay;grid computing called;dynamic resource management;dc dynamic resource;distributed computing;computing resources dynamic;dynamic resources management;area grid computing;grid computing resolve;geographically distributed computing;management delay compensator;delay compensator based;drm dc dynamic;allocation controller quickly;grid computing discrete;parallel computing;parallel computing using;using delay compensator;delay compensator;delay network;computing resources effectively;resource allocation;cluster computer systems;utilizing computing resources;resource management;cluster computer", "pdf_keywords": ""}, "97906df07855b029b7aae7c2a1c6c5e8df1d531c": {"ta_keywords": "nlp pipeline interpretable;nlp pipeline focus;nlp pipeline;traditional nlp pipeline;classical nlp pipeline;steps traditional nlp;parsing ner semantic;rediscovers classical nlp;semantic roles coreference;traditional nlp;nlp;ner semantic roles;linguistic information captured;tagging parsing ner;pos tagging parsing;semantic roles;tagging parsing;classical nlp;linguistic information;ner semantic;disambiguating information higher;linguistic;semantic;coreference;parsing;coreference qualitative;pipeline interpretable;level representations bert;disambiguating information;quantify linguistic information", "pdf_keywords": "nlp pipeline interpretable;deep language models;nlp tasks present;nlp tasks;deep language;nlp pipeline;nlp pipeline ian;classical nlp pipeline;traditional nlp pipeline;corroborating deep language;parsing ner semantic;rediscovers classical nlp;state art nlp;syntactic semantic abstractions;layers bert network;layers bert;art nlp tasks;steps traditional nlp;ner semantic roles;nlp;tagging parsing ner;classical nlp;semantic roles coreference;trained text encoders;syntactic semantic;traditional nlp;resolve syntactic semantic;learned training corpus;syntactic semantic structure;bert network"}, "e31a3f52890dcb68f596020e45f8c9718b700466": {"ta_keywords": "conference logic programming;logic programming technical;logic programming;conference logic;international conference logic;programming technical communications;programming technical;logic;programming;conference;technical communications;communications;international conference;37th international conference;technical;proceedings;proceedings 37th;proceedings 37th international;37th international;international;37th", "pdf_keywords": ""}, "77c98b45c95121fc2a3d2ab4906fc00364cf381c": {"ta_keywords": "training speech separation;speech separation recognition;speech separation;joint training speech;training speech;separation recognition;stage joint training;separation;joint training;training;speech;single stage;scratch single stage;recognition;train scratch single;single stage joint;stage;train scratch;train;stage joint;joint;scratch single;scratch;single", "pdf_keywords": ""}, "11a28f9e6fb6581d0a01428dd27a3fb649454395": {"ta_keywords": "chymotrypsin diphenylcarbamyl chloride;trypsin diphenylcarbamyl chloride;trypsin hydroxylamine ethylamine;activation trypsin diphenylcarbamyl;enzymatic behaviour chymotrypsin;chymotryptic hydrolysis;chymotryptic hydrolysis tryptic;specific substrates chymotrypsin;effect chymotryptic hydrolysis;substrates chymotrypsin;chymotrypsin activated substrates;alpha chymotrypsin diphenylcarbamyl;substrates chymotrypsin activated;hypothesis chymotrypsin;dep trypsin hydroxylamine;chymotrypsin diphenylcarbamyl;substrate activation trypsin;reaction dep trypsin;enzyme mep trypsin;hypothesis chymotrypsin additional;trypsin hydroxylamine;agree hypothesis chymotrypsin;trypsin diphenylcarbamyl;reaction alpha chymotrypsin;behaviour chymotrypsin tryspin;chymotrypsin tryspin;tryptic hydrolysis agee;cyclopropylamine present;behaviour chymotrypsin;agee effect chymotryptic", "pdf_keywords": ""}, "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4": {"ta_keywords": "learning transferable visual;predicting caption;task predicting caption;predicting caption goes;transferable visual models;grained object classification;imagenet;learning transferable;imagenet zero shot;object classification;learn sota image;training examples trained;recognition videos;visual models natural;examples trained;computer vision datasets;vision datasets;vision datasets spanning;sota image representations;training task predicting;recognition videos geo;vision systems trained;image representations;million image text;visual models;transferable visual;trained predict;natural language supervision;imagenet zero;specify visual concept", "pdf_keywords": "task predicting caption;predicting caption;predicting caption goes;reference learned visual;learned visual concepts;learn sota image;learned visual;object classi\ufb01cation learning;training natural language;learning directly raw;sota image representations;training task predicting;learning directly;recognition videos;recognition videos geo;million image text;image representations;vision datasets;vision datasets spanning;computer vision datasets;representations scratch dataset;image representations scratch;visual concepts;grained object classi\ufb01cation;classi\ufb01cation learning directly;visual concepts new;caption;caption goes image;learning;action recognition videos"}, "de0e1f9980afa7949df64d53b8ae7a2f59c55579": {"ta_keywords": "sentence target style;stylistic difference paraphrases;style transfer task;large stylelabelled corpora;stylelabelled corpora;modeling stylistic;available style transfer;indic languages crowdsource;style transfer indian;style transfer;languages crowdsource annotations;style transfer using;style style transfer;modeling stylistic difference;sentence pairs languages;method modeling stylistic;controllable style transfer;style transfer new;stylelabelled corpora recent;transfer indic languages;annotations 4000 sentence;shot style transfer;sentences inference extracting;difference paraphrases facilitate;languages crowdsource;crowdsource annotations;difference paraphrases;style transfer low;crowdsource annotations 4000;paraphrases facilitate", "pdf_keywords": ""}, "1d79d055cf9711944f14e1388a9d054cbe81ddd0": {"ta_keywords": "confusion models turkish;discriminative language models;learning discriminative language;language models negative;confusion models sample;sentences text corpus;text corpus;confusion models;discriminative language;based confusion models;language models;semi supervised learning;corpus;using confusion models;semi supervised;learning discriminative;different language models;semi supervised manner;text corpus generated;models turkish;obtained semi supervised;supervised learning discriminative;language models various;corpus generated;corpus generated using;hypotheses training variant;sentences text;supervised training examples;models turkish various;negative examples sentences", "pdf_keywords": ""}, "8cfd299be05bf3df91e0bf656a7e2fb973056350": {"ta_keywords": "lingual transfer speech;transfer speech processing;acoustic cross lingual;cross lingual transfer;acoustic language similarity;lingual transfer pairs;lingual transfer;transfer speech;similarity speech processing;language similarity speech;languages cross lingual;cross lingual;speech synthesis tasks;lingual transfer offers;language family classification;recognition speech synthesis;languages cross;propose language similarity;language similarity;similarity speech;hundreds languages cross;language similarity approach;speech synthesis;using acoustic language;acoustic language;speech processing systems;classification speech recognition;family classification speech;pairs hundreds languages;speech processing", "pdf_keywords": "acoustic language similarity;speech synthesis tasks;speech synthesis;recognition speech synthesis;crosslingual speech recognition;acoustic cross lingual;speech synthesis demonstrate;downstream crosslingual speech;acoustic language;speech data;proposed acoustic language;crosslingual speech;introduce acoustic language;acoustic approach language;compare features speech;language similarity;monolingual neural tts;neural tts models;features speech recognition;lingual transfer;classi\ufb01cation speech recognition;speech data contains;language similarity approaches;recognition speech;cross lingual transfer;speech recognition;features speech;speech recognition speech;language similarity approach;indic source languages"}, "3ba26e897d0085ecd8cb695e1728a083f9227447": {"ta_keywords": "speech recognition;speech recognition based;motozuku onsei ninshiki;recognition based bayesian;onsei ninshiki;bayesian approach beizu;ni motozuku onsei;based bayesian approach;based bayesian;motozuku onsei;recognition based;ni motozuku;shuho ni motozuku;bayesian approach;approach beizu teki;ninshiki;bayesian;teki shuho ni;speech;beizu teki shuho;beizu teki;motozuku;recognition;onsei;teki shuho;teki;approach beizu;shuho ni;beizu;ni", "pdf_keywords": ""}, "8f6763b339363216794f48895b9381d1a7caa88c": {"ta_keywords": "agent learning quadratic;learning dynamics;subspaces players disturbances;learning dynamics respect;multi agent learning;agent learning noisy;disturbance decoupled players;disturbance decoupling gradient;dynamics subset players;learning dynamics subset;agent learning;subspaces players;bilinear games disturbance;overall learning dynamics;games disturbance decoupling;based learning dynamics;learning quadratic costs;unobservable subspaces players;players disturbance decoupled;disturbance decoupling player;decoupled players actions;robustness gradient;players disturbances;learning quadratic;learning noisy environments;games disturbance;subset players disturbance;lq games disturbance;disturbance decoupling imposes;decoupling player action", "pdf_keywords": "agent learning quadratic;learning dynamics;disturbances player gradient;gradient disturbances player;disturbance decoupling gradient;multi agent learning;learning dynamics paper;disturbances quadratic games;agent learning noisy;dynamical systems gradient;learning dynamics respect;agent learning;based learning dynamics;player gradient based;decoupling gradient based;disturbance decoupling coupled;agent learning 14;player gradient;decoupling coupled dynamical;decoupling gradient;gradient disturbances;systems gradient based;decoupling arbitrary disturbances;learning quadratic costs;learning quadratic;systems gradient;quadratic games;learning 14 disturbance;disturbance decoupling;disturbances player"}, "51e5e7093e0183feab61b00ca6c3df61cd8c46de": {"ta_keywords": "discriminative language modeling;gram language models;training gram language;extracting phrasal cohorts;semi supervised methods;methods discriminative language;language models;language modeling best;machine translation extracting;semi supervised;discriminative language;language modeling;language models using;machine translation;investigates semi supervised;extracting phrase tables;supervised methods discriminative;translation extracting;gram language;fully supervised methods;methods machine translation;reduction fully supervised;training gram;baseline recognizer methods;phrase tables;phrase tables yielded;translation extracting phrase;english cts comparing;based extracting phrasal;baseline english cts", "pdf_keywords": ""}, "9d03a125a9568af8af3fae5091752017d6abe59e": {"ta_keywords": "named entity recognizers;entity recognizers;entity recognizers demonstrate;robust named entity;learned classifiers extractors;producing robust named;domain protein extraction;unanswerable transfer learning;classifiers extractors;classifiers extractors robust;protein extraction biological;unlabeled related domains;extraction biological publications;protein extraction;unlabeled auxiliary data;named entity;labeled unlabeled related;extractors robust;hierarchy learned classifiers;features natural linguistically;biological publications feature;extractors;related domains tasks;using data labeled;unlabeled related;publications feature hierarchies;problem domain protein;transfer learning;data labeled unlabeled;robust named", "pdf_keywords": ""}, "7cbb56da008163df09d254f85b7165f11389f298": {"ta_keywords": "natural language argument;argument reasoning comprehension;reasoning comprehension results;argument reasoning;reasoning comprehension natural;comprehension natural language;reasoning comprehension;key reasoning comprehension;task argument reasoning;language argument composed;argument composed;candidates plausible lexically;comprehension results;comprehension results reveal;language argument;comprehension natural;reasoning;argument composed claim;plausible lexically close;warrant candidates plausible;natural language;comprehension;task argument;argument;neural;neural networks;correct warrant candidates;given premises;claim topic information;plausible lexically", "pdf_keywords": ""}, "3f16887a8e5c81aea554c7a266b08ea70dd2aa9a": {"ta_keywords": "cooperative behaviour assign;induce cooperative behaviour;rewards based agents;unfair allocation rewards;incentives mechanism barocco;cooperative behaviour;allocation rewards;rewards based;cooperative environments agents;efficiently alleviated cooperative;cooperative competitive behaviour;induce cooperative;cooperative environments;assign additional rewards;incentives mechanism;social incentives mechanism;approach induce cooperative;additional rewards based;alleviated cooperative setting;cooperative setting;reinforcement;rewards;alleviated cooperative;individual social incentives;interplay cooperative;interests recent reinforcement;decisions unlike cooperative;unlike cooperative environments;interplay cooperative competitive;incentives", "pdf_keywords": "multiagent environments learning;multi agent;mixed multi agent;actor critic frameworks;environments learning actor;multi agent setups;learning actor critic;multiagent environments;mixed multiagent environments;agent setups propose;agent setups;training social component;learning actor;critic frameworks crucial;multiagent;balance incentives experimentally;critic frameworks;incentives experimentally con\ufb01rm;incentives experimentally;agent;methods mixed multiagent;individual social incentives;environments learning;social incentives formulate;mixed multiagent;incentives formulate;incentives formulate qualitative;balance incentives;social incentives;incentives"}, "a6aed0c4e0f39a55edb407f492e41f178a62907f": {"ta_keywords": "attention incrementally writing;follow paper paperrobot;paperrobot incremental draft;paper paperrobot incremental;attention contextual text;human written papers;contextual text attention;text attention incrementally;present paperrobot;paperrobot incremental;attention incrementally;ideas predicting links;follow paper;generate paper abstract;automatic research;memory attention networks;new ideas predicting;background knowledge graphs;automatic research assistant;generate paper;ideas predicting;attention networks;new paper based;paper paperrobot;incrementally writing;paperrobot;attention contextual;written papers;present paperrobot performs;graph attention", "pdf_keywords": "attention incrementally writing;paperrobot incremental draft;attention contextual text;contextual text attention;attention incrementally;text attention incrementally;future work generate;automatic research;abstract present paperrobot;incrementally writing;human written papers;generate paper abstract;ideas predicting links;entities generate paper;new ideas predicting;new paper based;ideas predicting;generate paper;title follow paper;attention contextual;performs automatic research;paperrobot incremental;incremental draft generation;present paperrobot;text attention;background knowledge graphs;memory attention;written papers target;based memory attention;draft generation scienti\ufb01c"}, "5695847f8ffbb3da078842c3683ef74175eb59e5": {"ta_keywords": "japanese speech synthesis;speech synthesis preserving;speech synthesis effectiveness;lingual speech synthesis;speaker individuality synthetic;speech synthesis;speech synthesis method;based speech synthesis;speech prosody correction;preserve speaker individuality;synthesis preserving speaker;synthetic speech prosody;preserving speaker individuality;speech synthesis generating;synthetic speech compared;individuality synthetic speech;synthetic speech;japanese speakers based;phonetic sounds erj;effects speaker english;speech synthesis tends;speaker individuality based;voice conversion hmm;degradation speaker individuality;prosody correction method;correction prosody phonetic;based voice conversion;read japanese speech;preserving speaker;japanese speech", "pdf_keywords": ""}, "37a0f28f6aa41028e64d0440001ff525d67c1305": {"ta_keywords": "fair efficient allocation;algorithm fair;papers reviewers constrained;efficient allocation welfare;agent resource allocation;algorithm fair efficient;robin algorithm fair;assignment papers reviewers;efficient allocation;constrained round robin;achieving allocation;allocation welfare;allocation welfare requirement;reviewers constrained;round robin algorithm;allocation setting models;allocation;multi agent resource;achieving allocation embed;fairness given oracle;allocation setting;assignment papers;consider multi agent;papers reviewers;reviewers constrained round;resource allocation;oracle welfare;given oracle welfare;multi agent;models assignment papers", "pdf_keywords": "fair e\ufb03cient allocation;agent resource allocation;algorithm fair;allocation process goals;computing allocation;agents deciding exists;agents deciding;robin algorithm fair;allocation setting models;hard agents deciding;fairness serum allocation;complete computing allocation;consider multi agent;algorithm fair e\ufb03cient;multi agent resource;deciding exists allocation;computing allocation um;allocation process;allocations;fairness concerns crr;allocation;resource allocation;feasibility constraints designer;multi agent;allocation setting;allocation um;providing fairness;constrained round robin;computationally hard agents;e\ufb03cient allocation"}, "7bdb04ba2da682e4c0b19b5d61e999d648826edd": {"ta_keywords": "sparse graph coding;decoding complexity;recovering sparse covariance;decoding complexity consider;log decoding complexity;recovering sparse;sparse covariance matrix;sparse covariance;measurements log decoding;algorithm based sparse;low complexity algorithms;decoding;problem recovering sparse;sparse graph;based sparse graph;log decoding;graph coding;algorithms robustified noise;robustified noise log2;algorithm recover arbitrarily;complexity consider noisy;low complexity;based sparse;ai \u2113n measurement;graph coding framework;\u2113n measurement vector;introduce low complexity;\u2113n measurement;sparse;robustified noise", "pdf_keywords": ""}, "d8c1eb86cc4546e4355ed368d8400d7640926cee": {"ta_keywords": "constrained clustering;constrained clustering algorithms;nonparametric bayesian hierarchical;work constrained clustering;clustering;deterministic clustering;bayesian hierarchical;based nonparametric bayesian;clustering algorithms;based clustering;clustering method;model based clustering;deterministic clustering algorithm;proposed posterior inference;nonparametric bayesian;new deterministic clustering;bayesian hierarchical model;clustering method tvclust;efficient gibbs sampling;clustering algorithm;clustering algorithms literature;based clustering method;data efficient gibbs;clustering algorithm rdp;probabilistic model data;posterior inference;clusters;gibbs sampling algorithm;gibbs sampling;algorithm proposed posterior", "pdf_keywords": "view clustering;clustering structure probabilistic;view clustering using;nonparametric bayesian hierarchical;latent clustering structure;latent clustering;tvclust view clustering;probabilistic model tvclust;generated latent clustering;bayesian hierarchical;deterministic clustering;clustering structure;based nonparametric bayesian;observed data instances;nonparametric bayesian;clustering;new deterministic clustering;multi view learning;view learning;clustering algorithm rdp;probabilistic model data;bayesian hierarchical model;nonparametric bayesian model;clustering method tvclust;means nonparametric bayesian;introduce probabilistic model;introduce probabilistic;deterministic clustering algorithm;section introduce probabilistic;model based clustering"}, "fe0ec764813fbcb6b6fd77d82188e81826088103": {"ta_keywords": "speech recognition;sequential pattern recognition;discriminative training;discriminative training applied;suitable discriminative training;automatic speech recognition;automatic speech;information minimum classification;suitable discriminative;pattern recognition;functions suitable discriminative;problems automatic speech;strings analysis generalizes;minimum classification;discriminative;recognition problems automatic;pattern recognition problems;minimum classification error;recognition problems;classification error minimum;strings analysis;classification;observations strings analysis;probabilities observations strings;recognition;minimum phone word;training applied sequential;objective functions weighting;measure strings;objective functions sum", "pdf_keywords": ""}, "659b476a10b0e676a031b1b17ebfe405c1904227": {"ta_keywords": "speech processing toolkit;speech enhancement se;end speech processing;end speech recognition;beamforming speech separation;speech processing;date speech processing;vc speech translation;voice conversation vc;text speech;support beamforming speech;text speech tts;beamforming speech;speech separation denoising;optimized espnet provides;speech enhancement;includes text speech;speech tts voice;st speech enhancement;optimized espnet;conversation vc speech;speech processing experience;speech recognition;speech tts;vc speech;recent development espnet;voice conversation;espnet provides reproducible;tts voice conversation;development espnet", "pdf_keywords": "advances espnet project;espnet project 2018;espnet project;espnet project previous;recent development espnet;espnet update;development espnet;espnet;espnet espnet end;espnet espnet;espnet update new;espnet end;publication espnet project;2020 espnet update;2020 espnet;recent advances espnet;espnet end end;com espnet espnet;advances espnet;com espnet;publication espnet;10 2020 espnet;of\ufb01cial publication espnet;espnet https;github com espnet;development espnet https;speech processing toolkit;espnet https github;speech recognition toolkit;end speech processing"}, "f02948f2976991bb76419775f303c27fc8afb7b5": {"ta_keywords": "learning text classifiers;text classifiers;text classifiers compared;learning rules classify;rules classify mail;classify mail;classifiers;ripper rule learning;classifiers compared;rule learning algorithm;rule learning;methods learning text;learning text;based tf idf;keyword spotting rules;classifiers compared classification;learning rules;classification;rules classify;tf idf weighting;personm mail messages;classification problems;tf idf;compared classification problems;compared classification;mail messages traxiitionm;classification problems arise;classify;rules based ripper;learning algorithm", "pdf_keywords": ""}, "62a5b47def8d21825d06f7407a505ff0b64ecb1a": {"ta_keywords": "semantic parsing ambiguous;semantic parsing;existing semantic parsing;parsing ambiguous ungrammatical;ambiguous input paraphrasing;semantic parsing framework;representation semantic parsing;parsing ambiguous input;parsing ambiguous;method semantic parsing;parsing;parsing framework;input paraphrasing;input paraphrasing verification;natural language paraphrase;paraphrasing verification;grammar takes ambiguous;language paraphrase ambiguous;context free grammars;paraphrasing;parsing framework uses;ambiguous ungrammatical input;meaning representation semantic;representation natural language;free grammars;paraphrase ambiguous;existing semantic;free grammars scfg;paraphrase ambiguous original;building existing semantic", "pdf_keywords": ""}, "05b0c768ecd4a82e486923e83250ddd53bacbf67": {"ta_keywords": "metric nn search;non metric search;metric search tree;metric search;pruning algorithms;pruning algorithms low;approximation pruning methods;algorithms low dimensional;approximation pruning;non symmetric distances;pruning methods;pruning methods rely;short indexing;nn search;nn search case;pruning rule avoid;pruning;efficient accurate retrieval;linear approximation pruning;pruning rule;having short indexing;search tree based;approximation pruning rule;require pruning rule;require pruning;non metric nn;symmetric distances;short indexing time;partitioning require pruning;search tree", "pdf_keywords": "approximation metric pruning;metric pruning;metric pruning rule;metric nn search;min symmetrized distance;algorithms low dimensional;pruning algorithms;non symmetric distances;2019 pruning algorithms;pruning algorithms low;approximation pruning arxiv;approximation pruning;linear approximation pruning;approximation pruning rule;symmetric distances;symmetrized distance;pruning rule hybrid;algorithm min symmetrized;trigen algorithm min;pruning rule;dataadapted pruning rule;pruning arxiv;pruning;dataadapted pruning;dimensional non metric;tree dataadapted pruning;pruning rule enable;rules non metric;low dimensional non;trigen algorithm"}, "1d938731dfad31c09b2f58c365f630c640f2ca1a": {"ta_keywords": "active learning al;label efficient text;efficient text classification;active learning self;active learning;label efficient;improves label efficiency;strongest active learning;better label efficiency;trained language models;enhance label efficiency;text classification extensive;framework label efficient;pre trained language;label efficiency researchers;resorted active learning;label efficiency;label efficiency model;lms natural language;text classification;require excessive labeled;improves label;training framework label;uncertainty aware active;efficient text;label efficiency 51;excessive labeled data;data better label;learning al;unlabeled data agnostic", "pdf_keywords": "samples active annotation;pretrained language models;querying annotations momentum;actively querying annotations;annotation model self;active learning pretrained;active annotation;training semi supervised;semi supervised active;trained language model;supervised active learning;annotation model;learning pretrained language;active learning self;pre trained language;active annotation lowuncertainty;data annotation model;annotations momentum based;strong performance nlp;active learning;querying annotations;strongest active learning;annotations momentum;supervised active;trained language;pretrained language;semi supervised;learning pretrained;annotation lowuncertainty ones;demanding labeled data"}, "ae82f831bda5681edfe40ec15de4e9d2096ea92f": {"ta_keywords": "clustering words senses;clustering improve lexical;clustering words;lexical entailment;improve lexical entailment;vectors infer entailment;term entailment;effects clustering words;narrower term entailment;lexical entailment second;entailment second supervised;improve lexical;lexical;entailment subset broader;learned predict entailment;words senses using;predict entailment;term entailment subset;infer entailment using;words senses;infer entailment;context vectors;context vectors infer;predict entailment given;contexts narrower term;similarity measure;entailment using;entailment;context vector representation;vector representation word", "pdf_keywords": "clustering word senses;clustering words senses;clustering improve lexical;recognizing lexical entailment;lexical entailment techniques;techniques clustering word;lexical entailment results;involve recognizing lexical;words senses using;word senses;clustering words;improve lexical entailment;recognizing lexical;lexical entailment;words senses;word senses effort;clustering word;combining sense consistantly;term entailment;improve lexical;combining senses;effects clustering words;combining sense;narrower term entailment;word senses does;state art lexical;vectors infer entailment;lexical;sense consistantly better;entailment techniques"}, "8c9069641876d025c66ab6800939c278b07f60a3": {"ta_keywords": "document topic hierarchies;topic hierarchies document;hierarchies document graphs;generated document hierarchies;topic hierarchies;document hierarchies;document hierarchy;document hierarchy represent;constructed document hierarchy;groupings document topic;document hierarchies able;topic taxonomies;hierarchies document;topic taxonomies present;hierarchy documents;collection general topics;document graphs;hierarchy documents created;levels constructed document;explicit hierarchy documents;document graph;taxonomy specific topics;groupings document;topics live taxonomy;experiments document graph;document topic;bayesian generative model;practical groupings document;document graph data;bayesian generative", "pdf_keywords": ""}, "423044220d9642a2d5839cfb19e32171e8a16a83": {"ta_keywords": "bandits modeling satiation;rebounding bandits modeling;bandits modeling;item rebounding bandits;rebounding bandits;bandit setup modeling;rebound initial reward;reward model;arm reward model;greedy policy model;model expected rewards;estimate arm reward;satiation dynamics stochastic;bandits;reward model plans;modeling satiation;modeling satiation dynamics;bandit setup;multi armed bandit;bandit;algorithms powering recommender;expected rewards arm;satiation dynamics;expected rewards;satiation dynamics time;modeling satiation effects;greedy policy;goods subject satiation;armed bandit setup;generalization greedy policy", "pdf_keywords": "rebounding bandits multi;rebounding bandits;introduce rebounding bandits;bandit setup satiation;bandit setup;bandits multi;bandits multi armed;bandit;bandits;multi armed bandit;stochastic satiation dynamics;explore estimate plan;optimal time dependent;estimates dynamics plans;time dependent policy;explore estimate;policy equivalent solving;stochastic satiation;propose explore estimate;armed bandit setup;armed bandit;methodically estimates dynamics;\ufb01nding optimal time;satiation dynamics modeled;estimate plan;dynamics plans;estimates dynamics;address stochastic satiation;dynamics plans accordingly;optimal time"}, "be0ad0710bfb09f6c875dd6cd834ac643713c93d": {"ta_keywords": "\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u6280\u8853\u306e\u73fe\u72b6\u3068\u5c55\u671b \u30a8\u30e9\u30fc\u5206\u6790\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u901a\u3057\u3066 \u7ffb\u8a33;\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u6280\u8853\u306e\u73fe\u72b6\u3068\u5c55\u671b \u30a8\u30e9\u30fc\u5206\u6790\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u901a\u3057\u3066;\u30a8\u30e9\u30fc\u5206\u6790\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u901a\u3057\u3066 \u7ffb\u8a33 \u6587\u4f5c\u6210\u652f\u63f4;\u30a8\u30e9\u30fc\u5206\u6790\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u901a\u3057\u3066 \u7ffb\u8a33;\u30a8\u30e9\u30fc\u5206\u6790\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u901a\u3057\u3066;\u7ffb\u8a33 \u6587\u4f5c\u6210\u652f\u63f4 \u5bfe\u8a71;\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u6280\u8853\u306e\u73fe\u72b6\u3068\u5c55\u671b;\u7ffb\u8a33 \u6587\u4f5c\u6210\u652f\u63f4;\u6587\u4f5c\u6210\u652f\u63f4 \u5bfe\u8a71 14;\u6587\u4f5c\u6210\u652f\u63f4 \u5bfe\u8a71;\u6587\u4f5c\u6210\u652f\u63f4;\u5bfe\u8a71 14;\u5bfe\u8a71 14 \u7ffb\u8a33;\u7ffb\u8a33;\u5bfe\u8a71;14 \u7ffb\u8a33;14", "pdf_keywords": ""}, "4a0f96bb17836b4c4d6e19627f176fba8fe05127": {"ta_keywords": "dc circuit breakers;circuit breakers;breaker dc distribution;limiting circuit relay;circuit breaker dc;circuit breaker;circuit relay protection;current limiting circuit;accuracy circuit breaker;circuit relay sensitivity;circuit breaker breaking;breaker dc;circuit breakers problems;effect current limiting;breaker breaking closing;existing limiting circuits;limiting circuit;current limiting;variation circuit breaker;limiting circuits;breaker breaking;circuit relay;relay protection;breakers problems;breaker;limiting circuits solid;relay sensitivity time;breakers problems large;relay sensitivity;breakers", "pdf_keywords": ""}, "1d5d170670889bd82364fbcc594dadcb5481e9e4": {"ta_keywords": "neural machine translation;translation nmt recall;machine translation nmt;machine translation;improves nmt translation;sentences translation pieces;retrieved source sentences;recall appropriate translation;retrieved target sentences;translation tasks;retrieved translation;retrieved translation pieces;nmt translation results;domain translation tasks;translation examples incorporating;translation tasks repetitiveness;sentence retrieved source;retrieve sentence pairs;source sentences translation;translation examples;translation nmt;nmt translation;translation results;engine retrieve sentence;narrow domain translation;retrieved sentence based;translation pieces;sentence retrieved;weight retrieved translation;nmt recall appropriate", "pdf_keywords": "machine translation nmt;neural machine translation;translation nmt recall;machine translation retrieved;machine translation;nmt model translating;translation nmt;translation retrieved translation;memory guide nmt;recall appropriate translation;nmt encoding decoding;retrieved translation;translation retrieved;nmt recall appropriate;nmt encoding;information retrieved sentences;retrieved sentences;sentences translation pieces;translation low frequency;obtain nmt encoding;sentence changes nmt;retrieved sentences key;retrieved target sentences;retrieved translation pieces;nmt recall;retrieved sentence pair;retrieved sentence pairs;nmt model retrieved;model retrieved sentence;model translating"}, "f32c67daa6a93281bd8645fc2fa423dca67aea00": {"ta_keywords": "assignment papers reviewers;automated assignment papers;peer review statistical;accuracy peer review;maximize review quality;reviewers conference peer;assignment algorithms overcomes;various assignment algorithms;peer review;automated assignment;assignment algorithms;conference peer review;peer review process;assignment algorithm based;objective maximize review;experiments peer review;assignment algorithm;accuracy peer;maximize review;papers reviewers;review quality disadvantaged;fairness objective maximize;problem automated assignment;fairness statistical accuracy;subjective score model;papers reviewers conference;near optimally fair;review focus fairness;subjective score;optimally fair", "pdf_keywords": "reviewer assignment algorithm;peer review algorithm;reviewer assignments max;peer review;reviewer assignments;reviewer assignment;accuracy peer review;maximizing review quality;maximizing review;peer review process;review algorithm;assignment algorithm peerreview4all;posits maximizing review;review algorithm based;experiments peer review;design reviewer assignment;accuracy peer;subjective score model;context reviewer assignments;subjective score;various assignment algorithms;paper quali\ufb01ed reviewers;algorithm peerreview4all;assignment algorithms overcomes;review quality;assignment algorithms;quali\ufb01ed reviewers;review quality paper;analysis accuracy peer;novel subjective score"}, "827e0def212f6834d615e4f3f25b55fe27f6460d": {"ta_keywords": "annotations using crowdsourcing;semantic verb relations;manually annotated crowdsourcing;annotated crowdsourcing;verb relations manually;relations manually annotated;annotated crowdsourcing crowdsourcing;semantic verb relation;grained semantic information;fine grained semantic;grained semantic;manually annotated;annotations;annotation approach;annotations using;propose novel semantic;multi step annotation;verb relation scheme;annotation;verb centric propositions;sensitive verb relations;semantic information verb;novel semantic;annotation approach scaling;novel semantic verb;verb relations;context sensitive semantic;verb relation;crowdsourcing large dataset;annotated", "pdf_keywords": ""}, "d8ad713ffde54d0a837e6a9cab4e70739d649d41": {"ta_keywords": "dialog response retrieval;dialog retrieval;based dialog retrieval;dialog retrieval using;distributed word representations;distributed word representation;response retrieval;response retrieval robust;word representations recursive;neural network paraphrase;example based dialog;based dialog response;dialog pair database;user utterance database;network paraphrase identification;representations recusive autoencoders;word representations;utterance database;dialog response;representations recursive autoencoders;paraphrase identification technique;good response dialog;retrieval robust example;utterance database decide;response dialog pair;dialog;recursive autoencoders;paraphrase identification;response dialog;recusive autoencoders", "pdf_keywords": ""}, "f45c777b29e0a00f7b1e1f33daa751853015724a": {"ta_keywords": "sigai acm org;acm org;acm sigai created;acm fellows;report acm sigai;acm honors;acm sigai consists;report acm;industry liaison officer;including acm fellows;acm honors including;acm;acm sigai;awards scope acm;activity report acm;robotics acm sigai;acm sigai proud;honors including acm;year acm sigai;ai;acm fellows receiving;robotics acm;scope acm;course year acm;scope acm sigai;sigai acm;order acm sigai;intelligence realization computer;year acm;liaison officer", "pdf_keywords": ""}, "1f7fb2c16e51f207eb1816f4f3fc3e1649c364f0": {"ta_keywords": "mismatches robust speech;simulation mismatches robust;microphone data simulation;robust speech recognition;robust speech;mismatches robust;environment microphone data;environment microphone;analysis environment microphone;data simulation mismatches;speech recognition;microphone data;simulation mismatches;microphone;mismatches;robust;data simulation;recognition;speech;simulation;environment;analysis environment;data;analysis", "pdf_keywords": ""}, "83f648f01d858d02b20f9327bebb1d5e91d0b6a9": {"ta_keywords": "minimizing speech recognition;speech recognizers discriminative;speech recognizers;optimization decoding networks;mmi methods decoding;speech recognition error;automatic speech recognizers;discriminative optimization decoding;optimize decoding networks;mmi incorporated decoding;minimizing speech;decoding networks;optimization decoding;speech recognition;optimize decoding;minimized wfst decoding;accuracy automatic speech;decoding networks important;decoding decoding network;decoding network optimization;wfst based decoding;decoding networks extending;recognizers discriminative optimization;wfst decoding decoding;important minimizing speech;reported optimize decoding;transition error training;decoding decoding;incorporated decoding network;automatic speech", "pdf_keywords": ""}, "099346e2837c53ded931d98135edbb261039764a": {"ta_keywords": "political polarisation research;fighting political polarisation;computational social choice;voting rules efficiently;political polarisation;polarization artificial intelligence;multi winner voting;elected decision makers;computational social;results computational social;decision making algorithms;polarisation research agenda;voting rules;political scientists economists;polarization contentious numerous;group decision making;polarization contentious;winner voting rules;design multi winner;decision makers creating;sociologists political scientists;political scientists;voting;decision makers;artificial intelligence;choice mitigate polarization;sociologists political;combining machine learning;research fighting political;numerous sociologists political", "pdf_keywords": ""}, "cbdccaa4a5bceaae190f78b1ac0a0cf47391968d": {"ta_keywords": "platforms creators;curation creation algorithms;curation creation;creation algorithms microcontent;distinction platforms creators;platforms;curation;algorithms microcontent;vanishing distinction platforms;creators;algorithms microcontent vanishing;distinction platforms;creation algorithms;creation;microcontent;algorithms;microcontent vanishing distinction;microcontent vanishing;distinction;vanishing distinction;vanishing", "pdf_keywords": ""}, "09fcc7ed1f867bcf9133ab12065ee7366cfaa652": {"ta_keywords": "compared checkpointing ecrm;checkpointing ecrm;dlrm size checkpointing;efficient fault tolerance;checkpointing ecrm reduces;checkpointing;fault tolerance training;compared checkpointing;checkpointing primary approach;size checkpointing;used fault tolerance;achieves efficient fault;size checkpointing slated;fault tolerance using;checkpointing slated;recovering failures ecrm;erasure coding overheads;checkpointing slated larger;fault tolerance;imparting efficient fault;checkpointing primary;efficient fault;size checkpointing primary;fault tolerance systems;recovering failures;erasure coding;recovers failures;erasure coding imparting;using erasure coding;ecrm dlrm training", "pdf_keywords": "recommendation model training;recommendation models dlrms;recommendation model;learning based recommendation;recommendation models;tolerance recommendation model;training erasure coding;fault tolerance recommendation;model training erasure;based recommendation models;fault tolerance training;erasure coding imparting;using erasure coding;serving personalized content;erasure coding;promise erasure coding;potential erasure coding;erasure coding superior;imparting fault tolerance;personalized content users;erasure coding results;erasure coding kaige;training erasure;fault tolerance using;tolerance using erasure;deep learning;fault tolerance;ef\ufb01cient fault tolerance;serving personalized;personalized"}, "8d939637b3a5ecf681130619cd35f295dbb9db03": {"ta_keywords": "susceptibility venous thrombosis;aptt associated snps;experiencing venous thrombosis;venous thrombosis;venous thrombosis vt;thrombosis vt investigated;aptt levels observed;effects aptt associated;thrombosis vt;thrombosis;associated snps;susceptibility venous;ile581thr susceptibility venous;patients experiencing venous;vt tested snps;tested snps;investigated effects aptt;snps rs710446 associated;tested snps rs710446;associated snps rs2731672;shortened aptt levels;effects aptt;aptt levels;experiencing venous;aptt associated;596 patients odds;kng1 ile581thr susceptibility;rs710446 risk vt;snps rs710446;snps", "pdf_keywords": ""}, "0bcd8210e9b90b33ab8467b94fd9b9511aad0f86": {"ta_keywords": "percomp ubiquitous multiprocessor;multiprocessor based pipeline;processors proposed ump;ubiquitous multiprocessor;ubiquitous multiprocessor based;performance ump;pipeline processing architecture;performance ump experience;best performance ump;ump percomp ubiquitous;performance pervasive application;based pipeline processing;speed ump;ubiquitous processors proposed;ump network;pipeline processing;optimize ump;ubiquitous processors;performance ump great;speed ump reduce;parallelism ubiquitous processors;high performance pervasive;evaluation optimize ump;proposed ump percomp;transmission ump network;multiprocessor based;performance pervasive;reduce network overhead;ump reduce network;processing architecture support", "pdf_keywords": ""}, "cece2d2f7cc38a512325122401f8aa658121b80e": {"ta_keywords": "dialog detect deception;deception propose dialog;effective detecting deception;deception detection;detecting deception;measure deception detection;deception detection accuracy;detect deception;detect deception propose;attempt detect deception;deceptive conversation partner;deceptive conversational partner;detect deception perform;potentially deceptive conversation;deception asking questions;deceptive conversation;deceptive conversational;deception asking;measure deception;deception perform actions;deception propose;task measure deception;deception;partner dialog detect;intelligent dialog strategy;signs deception asking;intelligent dialog;finding intelligent dialog;conversational partner dialog;dialog detect", "pdf_keywords": ""}, "4bc9d6596069c9277b57a7ee1e1127d231f28663": {"ta_keywords": "scoring parse tree;recursive autoencoder diora;outside recursive autoencoders;parse tree;unsupervised parsing diora;unsupervised parsing;recursive autoencoders deep;outside recursive autoencoder;tree encoding deep;recursive autoencoders;tree chart parsing;recursive autoencoder;parse tree chart;single tree encoding;trees sentence soft;parsing diora;tree encoding;encodes single tree;scoring parse;highest scoring parse;parse;autoencoders deep inside;parsing diora single;autoencoder diora;parsing;chart parsing;autoencoders deep;binary trees sentence;autoencoders;chart parsing fix", "pdf_keywords": ""}, "dbb4035111c12f4bce971bd4c8086e9d62c9eb97": {"ta_keywords": "graph learning models;graph machine learning;graph learning;ways graph learning;graph convolutional networks;gcn graph convolutional;multi view networks;networks multi view;view networks;labelling citation networks;graph convolutional;citation networks;large scale graph;graph machine;networks people related;networks;view networks including;networks developing countries;social networks;nature social networks;networks including;multi gcn graph;social networks people;networks multi;citation networks rapid;gcn graph;poverty multi gcn;networks people;global poverty multi;scale graph machine", "pdf_keywords": "view graph convolutional;graph convolutional networks;multi view networks;view networks;graph convolutional;network learning multi;networks multi view;learning multi view;multi view graph;graph based convolutional;convolutional network learning;view networks including;network learning;citation networks;networks;view graph;labelling citation networks;networks multi;view graph structured;view networks designed;networks including;learning multi;mobile network datasets;citation networks method;network datasets;community predicting;convolutional network;graph structured data;semi supervised;convolutional networks"}, "8376b18a4dd228ea4c33d606b32b081cee9bf80a": {"ta_keywords": "stochastic convex optimization;smooth stochastic convex;minimizing smooth convex;stochastic optimization;optimization similar stochastic;similar stochastic optimization;stochastic convex;convex optimization;convex optimization similar;stochastic optimization problems;smooth stochastic;smooth convex function;free smooth stochastic;convex function available;minimizing smooth;smooth convex;optimization problems stochastic;derivative free algorithm;propose accelerated derivative;accelerated derivative free;problem minimizing smooth;derivative free smooth;accelerated method derivative;function available noisy;convex function;unconstrained problem minimizing;accelerated derivative;convex;optimization similar;method derivative free", "pdf_keywords": "complexity bounds gradient;free convex optimization;derivative free optimization;convex optimization;convex optimization point;free optimization;free optimization problems;optimization point feedback;derivative free convex;gradient descent;unconstrained optimization;gradient descent step;progress gradient descent;bounds gradient;bounds gradient based;algorithms stochastic smooth;better complexity bounds;optimization point;free convex;unconstrained optimization problems;stochastic derivative free;method norm proximal;new algorithms stochastic;optimization problems point;complexity bounds;setup unconstrained optimization;complexity bounds typically;convex;algorithms stochastic;prove complexity bounds"}, "1e5b826ddf0754f6e93234ba1260bd939c255e7f": {"ta_keywords": "machine translation nat;autoregressive machine translation;translation nat systems;translation quality knowledge;performance nat models;accuracy nat models;translation nat;data best translation;machine translation;improve performance nat;translation quality;best translation quality;nat systems predict;gains accuracy nat;nat models;nat model variations;nat models usually;existing nat models;models knowledge distillation;performance nat;nat model optimal;knowledge distillation empirically;nat model;data pretrained autoregressive;accuracy nat;nat models reason;knowledge distillation reduce;nat;knowledge distillation;autoregressive models knowledge", "pdf_keywords": "autoregressive machine translation;neural machine translation;translation benchmark;machine translation nat;german translation benchmark;translation benchmark paper;machine translation nmt;machine translation;machine translation chunting;translation quality experiments;translation nmt systems;autoregressive baseline wmt14;wmt14 en benchmark;translation nat systems;best translation quality;autoregressive machine;autoregressive models achieve;translation quality achieve;translation quality;translation nat;speed compared autoregressive;non autoregressive machine;autoregressive models;compared autoregressive models;performance nat models;provides best translation;generate sequences autoregressive;wmt14 english;translation nmt;results best translation"}, "41d4763792db8ea420efcfbd112a55deec971fee": {"ta_keywords": "knowledge commonsense knowledge;knowledge envision wordnet;encyclopedic commonsense knowledge;knowledge commonsense;commonsense knowledge lexical;knowledge encyclopedic knowledge;commonsense knowledge commonsense;knowledge encyclopedic;commonsense knowledge;common knowledge encyclopedic;lexical common knowledge;knowledge lexical common;encyclopedic commonsense;encyclopedic knowledge;knowledge lexical;wordnet;encyclopedic knowledge helps;envision wordnet;like wikipedia mediator;wikipedia mediator;marriage encyclopedic commonsense;wikipedia mediator linked;wordnet mediator systems;like wikipedia;encyclopedic;common knowledge;envision wordnet mediator;wordnet mediator;knowledge envision;commonsense", "pdf_keywords": ""}, "4cc97c3858b558b4fa80ad73a894fcc7df841114": {"ta_keywords": "optimizing accuracy fairness;predictive performance fairness;notions fairness model;fairness model agnostic;accuracy fairness objectives;fairness confusion tensor;agnostic characterization fairness;fairness definitions;fairness definitions expressed;used fairness definitions;fairness objectives directly;fairness model;characterization fairness;different notions fairness;fairness objectives;characterization fairness trade;accuracy fairness;fairness different notions;notions fairness;definitions expressed fairness;fairness trade offs;fairness confusion;widely used fairness;performance fairness;expressed fairness confusion;expressed fairness;wider range fairness;range fairness definitions;performance fairness different;fairness different", "pdf_keywords": "fairness confusion tensor;accuracy fairness empirically;group fairness incompatibilities;optimize accuracy fairness;group fairness notions;diagnostic group fairness;group fairness class;accuracy fairness objectives;abstract group fairness;fairness functions accuracy;fairness objectives elements;fairness frame optimization;group fairness frame;group fairness;fairness empirically demonstrated;fairness empirically;including group fairness;fairness class fairness;fairness objectives;linear fairness functions;fairness functions;linear fairness;fairness incompatibilities;accuracy fairness;fairness notions expressed;class fairness notions;require linear fairness;fairness notions;fairness class;fairness notions measure"}, "72d89aa7cd77c3f22a667f2b0707758eb8d52a7a": {"ta_keywords": "context aware translation;contexts human translators;context ambiguous translations;aware machine translation;aware translation models;human translators use;aware translation;machine translation models;translation models;machine translation;human translators;translations professional translators;professional translators useful;translators use;translators useful;translations;translators;professional translators;supporting context ambiguous;ambiguous translations;translation models designed;supporting context words;useful pronoun disambiguation;ambiguous translations new;translations professional;context used disambiguate;attention context aware;translators useful pronoun;words context aware;attention context", "pdf_keywords": "context aware translation;context ambiguous translations;aware translation models;translation models;pronoun disambiguation analyses;supporting context words;professional translators useful;translators useful;aware translation;supporting context ambiguous;language pairs context;useful pronoun disambiguation;disambiguation analyses demonstrate;translations professional translators;professional translators;regularizing attention using;translators;disambiguation analyses;translations;translators useful pronoun;attention using scat;regularizing attention;random context words;attention \ufb01nd regularizing;ambiguous translations;pronoun translation suggesting;context par;words source context;ambiguous translations new;improves anaphoric pronoun"}, "d170bd486e4c0fe82601e322b0e9e0dde63ab299": {"ta_keywords": "adaptive softmax;adaptive softmax grave;extend adaptive softmax;softmax;neural language modeling;character input cnn;representations neural language;adaptive embeddings;input representations neural;adaptive input representations;softmax grave;softmax grave et;representations neural;adaptive embeddings twice;perplexity adaptive input;input cnn;input cnn having;billion word benchmark;model words characters;neural language;perplexity adaptive;layers model words;language modeling extend;input representations;embeddings twice fast;word benchmark;model words;language modeling;embeddings;equipped adaptive embeddings", "pdf_keywords": "improve adaptive softmax;adaptive word representations;softmax input word;adaptive softmax input;adaptive softmax;extend adaptive softmax;word representations outperform;adaptive softmax introducing;character input cnns;adaptive input embeddings;softmax;softmax input;adaptive softmax grave;input word representations;neural language modeling;softmax introducing;softmax introducing additional;embeddings extend adaptive;representations neural language;word representations substantially;input embeddings train;embeddings train faster;word representations;adaptive input representations;representations substantially improve;input representations neural;input embeddings;representations outperform;representations neural;models adaptive word"}, "3d3f01feee0dd3eea22e390c80deaadc6f11eb9a": {"ta_keywords": "speech recognition everyday;end speech recognition;tdnn chime corpus;sound input cnn;speech recognition;speech recognition systems;cnn based multichannel;speech recognition study;chime corpus;encoder decoder neural;automatic speech;attention based encoder;automatic speech recognition;decoder neural network;network cnn;input cnn;convolutional neural network;convolutional neural;neural network cnn;input cnn based;chime corpus respectively;decoder neural;cnn based;employing convolutional neural;network cnn based;encoder;mmi tdnn chime;performances automatic speech;text output sound;end speech", "pdf_keywords": "kinect microphone arrays;multi microphone speech;kinect microphone;distant multi microphone;microphone speech captured;microphone arrays features;microphone speech;microphone pairs kinect;microphone arrays;multi microphone;binaural microphone pairs;microphone;pairs kinect microphone;binaural microphone;microphone pairs;captured binaural microphone;speech captured binaural;cnn layers rnn;gpu deep models;deep models;rnn layers;layers rnn;model employed cnn;speech captured;employed cnn layers;tracks single array;rnn layers 512;layers rnn layers;deep models possible;single channel end"}, "b2fd7297f7681f9e3ea860cecf1ec97b2cc8ccc3": {"ta_keywords": "tobacco cessation destination;therapy ventricular assist;tobacco cessation;destination therapy ventricular;ventricular assist device;ventricular assist;assist device patients;therapy ventricular;device patients results;cessation destination therapy;tobacco;device patients;ventricular;destination therapy;assist device;patients results international;patients results;cessation destination;patients;device;assist;therapy;destination;cessation;results international survey;results international;international survey;survey;results;international", "pdf_keywords": ""}, "ff783c4709a095cc581534fec58ef9515613ebc9": {"ta_keywords": "catastrophic forgetting remembering;catastrophic forgetting;explainability continual learning;forgetting remembering;continual learning cl;continual learning;goal continual learning;forgetting remembering right;phenomenon catastrophic forgetting;tasks hypothesize forgetting;forgetting;remembering;leveraging memory;hypothesize forgetting;shown leveraging memory;memory form replay;called remembering;remember evidence previously;encouraged remember evidence;hypothesize forgetting reduced;explainability continual;leveraging memory form;remember evidence;model encouraged remember;remembering right reasons;forgetting reduced;learn sequence tasks;remembering right;called remembering right;decisions goal continual", "pdf_keywords": "incremental learning;shot class incremental;2017 guided backpropagation;class incremental learning;explainability continual learning;incremental learning cil;gradient class activation;guided backpropagation;backpropagation smoothing gradients;2014 backpropagation;overall accuracy forgetting;explanations form saliency;demonstrate rrr remembers;continual learning;accuracy forgetting;2014 backpropagation smoothing;experience replay memory;accuracy forgetting experience;rrr remembers earlier;rrr remembers;class activation mapping;backpropagation;replay memory;backpropagation smoothing;learning;generate model explanations;vanilla backpropagation;forgetting experience replay;replay memory based;fergus 2014 backpropagation"}, "f2e7598464a0b9376771ffc4ba243233ee12c677": {"ta_keywords": "lexical sememe prediction;words lexical sememe;lexical sememe;sememes minimum semantic;chinese sememe knowledge;methods lexical sememe;annotated sememes linguists;hownet chinese sememe;characters words lexical;words incorporating chinese;external context words;existing methods lexical;chinese sememe;chinese characters words;manually annotated sememes;annotated sememes;words lexical;minimum semantic units;sememes linguists;sememes linguists form;incorporating chinese characters;sememe prediction typically;lexical;various nlp tasks;semantic units;languages word sense;context information words;nlp tasks;sememe prediction existing;various nlp", "pdf_keywords": "lexical sememe prediction;prediction character sememe;prediction word character;sememe prediction character;external information lexical;character sememe embeddings;sememe prediction word;prediction character;enhanced sememe prediction;context sememe prediction;information lexical sememe;character information external;character enhanced sememe;word character filtering;sememe prediction framework;internal character information;propose sememe prediction;lexical sememe;sememe embeddings;sememe prediction;character sememe;hownet chinese sememe;words 2017 model;sememe prediction csp;character filtering;chinese sememe knowledge;sememe prediction proposed;character information;external context sememe;prediction word"}, "48b18bf5c9cad0e4c36b2d885f380c5c637e1a09": {"ta_keywords": "priors disentangled representations;disentanglement learning;conditional priors disentangled;guarantees disentanglement learning;disentanglement learning optimal;priors disentangled;disentangled representations;optimal conditional priors;disentangled representations experimental;optimal conditional prior;learning optimal conditional;vae based generative;conditional priors;factorized prior distribution;conditional prior;guarantees disentanglement;generative model;factorized prior;compact optimal conditional;based generative model;prior serves regularization;disentangled;model learns sufficient;conditional prior serves;theoretical guarantees disentanglement;prior distribution latent;generative model theoretical;regularization latent space;developments demonstrate disentanglement;based generative", "pdf_keywords": "datasets established disentanglement;improvements disentanglement;vae based disentanglement;disentanglement performance;improvements disentanglement practice;vae based generative;generative;disentanglement performance experiments;achieves superior disentanglement;generative model;disentanglement methods;superior disentanglement performance;generative model like;based generative;disentanglement practice;novel generative model;generative model theoretical;based disentanglement methods;based generative model;established disentanglement metrics;novel generative;based disentanglement;propose novel generative;disentanglement methods using;disentanglement;disentanglement metrics;established disentanglement;disentanglement practice iii;tangible improvements disentanglement;disentanglement metrics compare"}, "ee5dc631a682696a4704b742ea087e8abb5df897": {"ta_keywords": "speech recognition tasks;unsupervised speech recognition;speech sending asr;speech recognition;unsupervised speech;context synthesized speech;features asr tts;corpus respectively tts;features asr;self supervised training;attention context synthesized;propose enhanced asr;benefits unsupervised speech;asr tts pipeline;synthesized speech;tts asr;enhanced asr tts;hypotheses forwarding tts;data self supervised;main features asr;self supervised;asr hypotheses forwarding;enhanced asr;asr tts;respectively tts asr;speech sending;corpus;synthesized speech sending;language model;supervised training", "pdf_keywords": ""}, "162c3cf78af48ddf826ec76a1a3767a88a730170": {"ta_keywords": "\u8133\u6ce2\u306e\u30c1\u30e3\u30cd\u30eb\u9593\u76f8\u95a2\u306e\u4e8b\u524d\u5206\u5e03\u3092\u5229\u7528\u3057\u305f\u78ba\u7387\u7684\u76ee\u7684\u6210\u5206\u5f37\u8abf \u6a5f\u68b0\u5b66\u7fd2\u306b\u3088\u308b\u30d0\u30a4\u30aa\u30c7\u30fc\u30bf\u30de\u30a4\u30f3\u30cb\u30f3\u30b0 \u4e00\u822c;\u8133\u6ce2\u306e\u30c1\u30e3\u30cd\u30eb\u9593\u76f8\u95a2\u306e\u4e8b\u524d\u5206\u5e03\u3092\u5229\u7528\u3057\u305f\u78ba\u7387\u7684\u76ee\u7684\u6210\u5206\u5f37\u8abf \u6a5f\u68b0\u5b66\u7fd2\u306b\u3088\u308b\u30d0\u30a4\u30aa\u30c7\u30fc\u30bf\u30de\u30a4\u30f3\u30cb\u30f3\u30b0;\u8133\u6ce2\u306e\u30c1\u30e3\u30cd\u30eb\u9593\u76f8\u95a2\u306e\u4e8b\u524d\u5206\u5e03\u3092\u5229\u7528\u3057\u305f\u78ba\u7387\u7684\u76ee\u7684\u6210\u5206\u5f37\u8abf;\u6a5f\u68b0\u5b66\u7fd2\u306b\u3088\u308b\u30d0\u30a4\u30aa\u30c7\u30fc\u30bf\u30de\u30a4\u30f3\u30cb\u30f3\u30b0 \u4e00\u822c;\u6a5f\u68b0\u5b66\u7fd2\u306b\u3088\u308b\u30d0\u30a4\u30aa\u30c7\u30fc\u30bf\u30de\u30a4\u30f3\u30cb\u30f3\u30b0;\u4e00\u822c", "pdf_keywords": ""}, "c8a5d05cb741b3448ec4106d2006ae24a7a401b4": {"ta_keywords": "streaming automatic speech;streaming asr sequence;speech recognition asr;latency streaming asr;models streaming asr;streaming asr;streaming asr networks;prediction sequence transducer;asr networks;sequence transducer models;end streaming asr;streaming asr applying;sequence transducer;transducer models streaming;automatic speech;recognition asr;rnn transducer;alignment streaming automatic;asr networks including;sequence probability training;alignment streaming;speech recognition;asr sequence level;require alignment streaming;automatic speech recognition;including rnn transducer;regularization directly sequence;latency regularization directly;regularization demonstrate fastemit;rnn transducer transformer", "pdf_keywords": "streaming automatic speech;streaming asr sequence;speech recognition asr;sequence transducer models;latency streaming asr;prediction sequence transducer;models streaming asr;sequence transducer;transducer models streaming;automatic speech;streaming asr;regularization directly sequence;streaming asr applying;automatic speech recognition;speech recognition;streaming asr networks;sequence probability training;latency regularization;latency regularization directly;end streaming asr;asr networks;recognition asr;training transducer models;applies latency regularization;fastemit suitable sequence;optimization transducer models;probability training transducer;sequence level optimization;rnn transducer;asr sequence level"}, "9918ea4b68e90e1257953b6f2665b2ce29f2bc8b": {"ta_keywords": "speech enhancement 3d;speech enhancement espnet;channel speech enhancement;l3das22 challenge dnns;speech enhancement;dnn refines estimation;consists speech enhancement;multi channel speech;estimated signal dnn;3d ambisonic microphones;challenge dnns trained;network dnn driven;ambisonic microphones;dnns trained perform;l3das22 challenge task;dnn driven complex;l3das22 challenge;challenge dnns;beamforming result enhanced;dnn driven;network dnn;submission l3das22 challenge;ambisonic microphones core;neural network dnn;signal dnn;microphones core approach;channel speech;combines deep neural;deep neural;dnns trained", "pdf_keywords": "beamforming dnn based;linear beamforming dnn;beamforming dnn;neural beamforming enhancement;iterative neural beamforming;channel enhancement beamforming;beamformer method dnns;beamforming enhancement ineube;multi channel enhancement;enhancement beamforming directly;beamforming enhancement;enhancement beamforming;pipeline linear beamforming;neural beamforming;dnn based complex;3d ambisonic microphone;ambisonic microphone format;channel enhancement;ambisonic microphone;dnn driven complex;complex spectral mapping;linear beamforming;multi channel wiener;microphone format;use complex spectral;mapping linear beamformers;spectral mapping linear;spectral mapping;based complex spectral;network dnn driven"}, "30109a213aa10765486c676ecfa511db227ab543": {"ta_keywords": "sentences mini batch;machine translation nmt;machine translation training;neural machine translation;translation training neural;translation nmt models;machine translation;mini batches;shorter sentences mini;mini batch;translation training;translation nmt;mini batched training;uses mini batches;speed mini batched;sentences mini;mini batched;mini batch creation;making mini batches;pad shorter sentences;study mini batch;mini batches reduces;fact mini batch;mini batch equal;mini batches efficiency;sentence length making;longest sentence efficient;shorter sentences;sentence length;batches", "pdf_keywords": "machine translation nmt;neural machine translation;translation nmt models;nmt models language;machine translation;mini batch training;introduction mini batch;translation nmt;training nmt models;mini batches;mini batch creation;mini batch;uses mini batches;training examples batches;learning nmt;mini batches ef\ufb01ciency;study mini batch;analyzed mini batch;machine learning nmt;machine translation makoto;models language pairs;fact mini batch;minibatch creation strategy;batch training;nmt models;batch creation strategies;training corpus;learning nmt toolkits;batch training standard;sorting training corpus"}, "5e6acc5c73f22c2dbbb4910f656a03cf40a2fe15": {"ta_keywords": "recursive prolog clauses;non recursive prolog;recursive prolog;pac learning non;learning non recursive;pac learning;prolog clauses;prolog;non recursive;learning non;recursive;pac;learning;clauses;non", "pdf_keywords": ""}, "03e62d5f0265608c6ebdebba0870131b056b79a6": {"ta_keywords": "online harms relate;harmful content online;harm content moderation;severity harmful online;online harms;experiences harm online;mitigating harm content;harmful online content;harm online;harmful online;harm content;harmful content moderated;understandings harm approaches;harmful content;harm online development;harm approaches mitigating;harm approaches;harm harassment prioritization;harmful content implemented;harm mitigation understandings;approaches mitigating harm;implemented harmful content;mitigating harm;mitigation understandings harm;content moderation;variety online harms;content implemented harmful;prioritize harms relevant;practices harm;practices harm mitigation", "pdf_keywords": "harmful content online;harmful online content;online harms relate;harm content moderation;online harms;severity harmful online;experiences harm online;harmful online;mitigating harm content;harmful content moderated;harm online;harm content;harmful content;online harms employing;harm online development;severity harmful content;content experts harm;content moderation;harmful content implemented;variety online harms;implemented harmful content;content implemented harmful;understandings harm approaches;content online social;policy harmful content;harm approaches mitigating;content moderation implicitly;experts harm content;harm approaches;online content"}, "33ce3cd897a3473973f338c154f3fe5c1175643c": {"ta_keywords": "question answering;domain question answering;question answering recommendation;applications question answering;reasoning virtual knowledge;question answering tasks;virtual knowledge bases;reasoning trained structured;knowledge bases open;open predicate relations;open predicate query;large knowledge bases;language model opql;text large knowledge;language opql;knowledge bases;query language opql;predicate query language;kb reasoning tasks;answering recommendation;knowledge bases kbs;reasoning trained;enables reasoning trained;query language;recommendation instead opql;bases open predicate;answering recommendation instead;virtual knowledge;open predicate;answering tasks reasoning", "pdf_keywords": "question answering;language model opql;question answering tasks;reasoning virtual knowledge;language opql;effectively fact opql;query language opql;proposed opql construct;domain question answering;external memory neural;proposed opql;language opql summarize;opql lm;opql construct;demonstrate opql outperforms;kb reasoning tasks;structured queries effectively;2015 demonstrate opql;vkb open predicate;fact opql outperforms;fact opql;opql;semi structured queries;demonstrate opql;distant kb supervision;open predicate relations;reasoning tasks additionally;query language;open predicate query;reasoning operations vkb"}, "b00bc4dcce60e7c631a23d60894e51001de1c630": {"ta_keywords": "ebola virus glycoprotein;hepatocytes ebola virus;antibodies ebola virus;ebola virus gp;neutralizing antibodies ebola;properties ebola virus;virus glycoprotein proteolytic;antibodies ebola;cells hepatocytes ebola;hepatocytes ebola;ebola virus;virus glycoprotein;glycoprotein cleavability viral;virus glycoprotein gp;indicating ebola virus;functional properties ebola;filoviruses believed target;gp filoviruses believed;pseudotyped gp filoviruses;types indicating ebola;gp filoviruses;properties ebola;filoviruses believed;ebola;cleavability viral infectivity;filoviruses;cleavability viral;indicating ebola;viral infectivity;vesicular stomatitis virus", "pdf_keywords": ""}, "2583e7e279e2969493c3290c8f300ab32da40bf9": {"ta_keywords": "lingual entity linking;referents target language;entity linking xel;entity linking;entities target language;cross lingual entity;entity mentions;target language knowledge;resource cross lingual;lingual entity;language knowledge base;entity candidate generation;mention cross lingual;entity linking step;finding referents target;target language;referents target;mentions extracted source;low resource languages;candidate entities target;languages utilizing resources;entities target;language kb mention;linking xel;finding referents;related languages performance;task finding referents;mentions extracted;resource languages utilizing;entity mentions kb", "pdf_keywords": "lingual entity linking;entity linking xel;entity linking;referents target language;cross lingual entity;inter language links;lingual entity;mismatch mention entity;entity mentions;entity mention;entity linking shuyan;mention entity;resource cross lingual;entity mention text;kb entity mention;language knowledge base;language links;target language knowledge;language links specifically;mention entity sub;entity candidate generation;linking xel;entity mentions kb;referents target;finding referents target;mismatch entity;entity sub optimal;target language;abstract cross lingual;mismatch entity kb"}, "0bb30ed3340d2c34fe9f37c5002929bc5f458c23": {"ta_keywords": "biomedical relation extraction;relation extraction bert;biomedical text mining;relation extraction tasks;extraction bert graph;protein relation datasets;biomedical relation;relation extraction task;graph transformer bert;relation extraction;biomedical text;bert graph transformer;bidirectional encoder representations;existing biomedical text;bert graph;ary relation extraction;extract information biomedical;use graph neural;extraction bert;lstm attention;graph neural;biomedical relation statement;datasets suggesting bert;memory lstm attention;information biomedical literature;sentences use graph;attention mechanism bert;mechanism bert architecture;applicable biomedical relation;token neighbor attention", "pdf_keywords": "biomedical relation extraction;graph transformer bert;bert architecture benchmarking;protein relation datasets;relation extraction tasks;attention mechanism bert;architecture improves bert;mechanism bert architecture;attention mechanism graph;datasets suggesting bert;bert architecture;architecture bert;improves bert integrating;architecture bert bert;sentence relation pubmed;transformer bert;relation extraction;gt architecture bert;mechanism bert;representations transformers graph;biomedical relation;relation datasets;bert used classify;pubmed abstract attention;encoder representations transformers;encoder representations;relation datasets suggesting;transformer bert gt;bert integrating neighbor;transformer bert used"}, "9bbc8ca94810e8a21e4a6a55a5913c5b0b6c787f": {"ta_keywords": "speech transcription respeaking;speech transcription;correction efficiency speech;transcription respeaking;line speech transcription;transcription respeaking results;efficient line speech;transcription;efficiency speech segmented;automatic transcript;smaller utterances using;initial automatic transcript;segmented smaller utterances;speaking vs typing;efficiency speech;smaller utterances;speech segmented;experiments comparing speaking;speech segmented smaller;utterances using;line speech;transcript;utterances;outperform typing;correction efficiency demanding;utterances using initial;comparing speaking;typing sequential;comparing speaking vs;speech", "pdf_keywords": ""}, "7e0570f498a5de4f2a861546d4e67ba208f71d12": {"ta_keywords": "speaker recognition benchmark;dnn speaker recognition;speaker recognition diarization;speaker recognition;available speaker recognition;introduce speaker recognition;speaker recordings naturally;multi speaker recordings;speaker recordings;speaker recordings validate;available chime corpus;field multi speaker;chime corpus;chime corpus goal;dnn speaker;chime corpus results;art dnn speaker;microphone state art;using chime corpus;publicly available speaker;benchmark using chime;multi speaker;recordings validate evaluation;state art dnn;microphone state;spoken interactions;single microphone state;available speaker;recognition benchmark;recordings naturally", "pdf_keywords": ""}, "1410f7d9470a24fb4055c6685c2dda758b9d995f": {"ta_keywords": "evolutionary game theory;coevolution agents games;evolutionary game;infinitely populations agents;initializations agents games;game theory;population dynamic agents;dynamic agents;game theoretic;games play evolve;evolutionary learning dynamic;chaotic coevolution agents;games network generalizations;zero sum games;game theoretic setting;evolve strategically time;learning dynamic replicator;paradigm evolutionary game;archetypal game theoretic;play evolve strategically;dynamic agents interact;agents static games;coevolution agents;sum competition evolves;evolve strategically;competition evolves adversarially;games lying recurrent;competition evolves;agents games prove;dynamic agents static", "pdf_keywords": "game theoretic graph;concept polymatrix games;game theoretic analysis;polymatrix games revisit;polymatrix games;polymatrix games de\ufb01ne;games evolutionary zero;static polymatrix games;systematic game theoretic;sum games evolutionary;zero sum games;polymatrix game;dynamics class games;polymatrix game begin;evolving games;games evolutionary;time evolving games;sum polymatrix game;reduces game theoretic;learning agents time;sum games preliminaries;game theoretic;agents time evolving;evolving dynamical systems;sum games rescaled;games rescaled network;evolving games reduces;sum games;evolving zero sum;games preliminaries de\ufb01nitions"}, "e8e62a80c7355bcf5dbc9fabafff4025e00cf540": {"ta_keywords": "categorial grammars pos;combinatory categorial grammars;categorial grammars;grammars pos tagged;induction combinatory categorial;grammars pos;pos tagged text;combinatory categorial;linguistically plausible lexicons;induces linguistically plausible;novel nonparametric bayesian;nonparametric bayesian;induces linguistically;model induction combinatory;bayesian model induction;nonparametric bayesian model;pos tagged;grammars;linguistically plausible;languages induces linguistically;plausible lexicons;tagged text achieves;tagged text;linguistically;induction combinatory;languages induces;bayesian;lexicons;categorial;bayesian model", "pdf_keywords": ""}, "4ef46d5daf6a7a9536e2ebe3c7aa2296bffcf43e": {"ta_keywords": "unsupervised pos tagging;unsupervised speech tagging;unsupervised markov models;infinite hmm unsupervised;models pos tagging;markov models pos;speech tagging;unsupervised markov;hmm unsupervised pos;infinite hmm;states unsupervised markov;pos tagging;fully unsupervised speech;unsupervised pos;called infinite hmm;pos tagging experiment;unsupervised speech;pos tagging extend;algorithm infinite hmm;markov models;parametric version hmm;infinite hmm ihmm;hmm unsupervised;hidden states unsupervised;priors dirichlet;parametric priors dirichlet;markov;tagging;priors dirichlet pitman;states unsupervised", "pdf_keywords": ""}, "467b14cc8337dd7efe1d374f9a7feb90ae9d2c12": {"ta_keywords": "controllable speech modification;articulatory controllable speech;speech modification based;controllable speech;speech modification;evaluation articulatory controllable;articulatory controllable;waveform modification;modification based gaussian;based gaussian mixture;direct waveform modification;gaussian mixture models;evaluation articulatory;articulatory;gaussian mixture;mixture models direct;mixture models;models direct waveform;waveform;direct waveform;modification based;based gaussian;controllable;speech;gaussian;modification;mixture;models;models direct;evaluation", "pdf_keywords": ""}, "63a604942f1238e9678aebd697a2379981e9a20a": {"ta_keywords": "tutor learning effect;inhibit tutor learning;simstudent peers learning;tutor learning studied;tutor learning understood;facilitate inhibit tutor;tutor learning;studying tutor learning;effect tutor learning;learning teaching simstudent;inhibit tutor;teaching computer agent;studying tutor;teaching simstudent;tutor;effect tutor;area studying tutor;peers learning teaching;suggest students learn;students learn;teaching computer;peers learning;learning teaching;students learn teach;learning environment;students learn teaching;learn teaching computer;learning studied various;learning studied;teach cognitive", "pdf_keywords": ""}, "db392858262b17aa9c8ff8659738f68fbf832ebe": {"ta_keywords": "code completion generating;abstract syntax tree;code completion;syntax tree;approach code completion;language models code;code completion leverages;syntax programming languages;code snippet tree;code programming language;strict syntax programming;syntax programming;tree structural language;arbitrary code programming;syntax tree ast;programming languages model;languages model code;code programming;problem code completion;program abstract syntax;generate arbitrary code;programming languages;structural language modeling;node structural language;structural language models;snippet tree structural;snippet tree;programming language;arbitrary code;abstract syntax", "pdf_keywords": ""}, "040a1abdbef2a0e087a586d719259c32c95bfc78": {"ta_keywords": "dialog management task;solving dialog management;dialog management;dialog;solving dialog;model solving dialog;task planning learning;action belief propagation;planning learning;optimal action belief;planning learning optimize;log linear probabilistic;belief propagation inference;pomdp like log;belief propagation;task planning;estimation optimal action;reward outperforming pomdp;choose actions;context free grammars;policy optimization;planning;performing policy optimization;learning optimize;probabilistic model solving;linear probabilistic;action belief;reward performing policy;management task planning;function expected reward", "pdf_keywords": ""}, "e1a20480e4168d58deec743035b7ff02720672d7": {"ta_keywords": "character based lstm;end speech recognition;speech recognition character;based lstm rnn;lstm rnn;vocabulary recognition character;recognition character lms;speech recognition asr;speech recognition;attention connectionist temporal;open vocabulary recognition;lstm rnn lm;language modeling decoding;language models end;hybrid attention connectionist;character based lms;connectionist temporal classification;decoding lms character;dnn hmm systems;hmm systems japanese;based lstm;lstm;lm hybrid attention;recognition asr architecture;recognition character based;decoding open vocabulary;level language modeling;asr decoding lms;long sequences characters;character lms improved", "pdf_keywords": ""}, "a13d9c8e5a2fc028ad597e2bd46a9c60aca0ede4": {"ta_keywords": "speech synthesis hmm;speech synthesis prosody;based speech synthesis;input speech synthetic;speech synthetic speech;text speech synthesis;speech synthesis;prosody synthetic speech;speech synthesis text;synthesis text speech;synthetic speech;modification based speech;synthesis hmm based;speech synthetic;speech synthesis report;prosody modification based;speech inputs hmm;synthetic speech target;synthesis prosody modification;patterns synthetic speech;synthesis hmm;hmm based speech;synthetic speech investigate;expressive speech synthesis;based speech input;user speech inputs;speech synthesis order;synthesis prosody;speech input;prosody modification method", "pdf_keywords": ""}, "d26a7a86013b3be57acc0f5df73393cab7c302d9": {"ta_keywords": "flows constraining sdes;deterministic particle methods;iterative stochastic path;particle flows constraining;solving fokker planck;deterministic particle flows;fokker planck equations;stochastic path sampling;forward probability flows;resort iterative stochastic;stochastic path;particle methods solving;constraining sdes;iterative stochastic;particle methods;solve deterministic particle;flows employing deterministic;based pde solvers;reformulating optimal interventions;pde solvers resort;pde solvers;optimal interventions diffusive;grid based pde;probability flows employing;planck equations introduce;methods solving fokker;flows constraining;particle flows;constraining sdes existing;existing control methods", "pdf_keywords": "carlo particle methods;deterministic particle methods;particle methods;particle methods solving;iterative stochastic monte;stochastic simulations;solving fokker planck;particle methods rely;stochastic monte carlo;fokker planck equations;maruyama stochastic simulations;euler maruyama stochastic;monte carlo particle;stochastic monte;stochastic simulations comparing;iterative stochastic;non iterative stochastic;particles 500 methods;planck equations introduce;discretising pde solutions;methods increasing particle;maruyama stochastic;space discretising pde;methods rely feynaman;methods solving fokker;discretising pde;carlo particle;fokker planck;reformulating optimal interventions;forward probability \ufb02ows"}, "73c401e29cb83157bc6dfb33d5ce4364a7d2731b": {"ta_keywords": "shot image generation;diverse realistic images;image generation cross;shot image;shot model;shot model automatically;photorealistic domains;domain correspondence extensive;image generation;cross domain correspondence;photorealistic domains demonstrate;generates diverse realistic;non photorealistic domains;domains generates diverse;source domain pretraining;quantitatively shot model;domain correspondence;correspondences source target;generation cross domain;target domains generates;transfer diversity information;source target domains;extensive results photorealistic;preserve relative similarities;realistic images previous;transfer diversity;domain distance consistency;source domain;results photorealistic;domain pretraining transfer", "pdf_keywords": "transferring gans;target transferring gans;transferring gans tgan;gans tgan;generated samples adaptation;gans;trained source model;source domain pretraining;domain distance consistency;gans tgan 34;corresponding images source;domain pretraining transfer;correspondences source target;batch statistics adaptation;samples adaptation;trained source;adaptation propose preserve;statistics adaptation;layers iii minegan;samples adaptation propose;loss enforces similarity;photorealistic domains;trained source work;pretraining transfer diversity;photorealistic domains demonstrate;shot model automatically;preserve relative similarities;transfer diversity information;images source target;shot model"}, "4702bfd200ceb6de126a60afb4db9da5c413476e": {"ta_keywords": "dnn based acoustic;uncertainties layers dnn;uncertainty propagation deep;speech estimate;dnn baseline uncertainty;estimate distribution acoustic;intractable nonlinearities dnn;results speech estimate;asr accuracy chime;highly optimized dnn;optimized dnn;speech estimate containing;processed speech enhancement;speech enhancement;nonlinearities dnn;nonlinearities dnn employ;optimized dnn baseline;distribution acoustic scores;accuracy chime;dnn employ approximate;acoustic score distribution;layers dnn based;propagation deep neural;accuracy chime database;environments distorted speech;speech enhancement algorithm;layers dnn;distribution acoustic;improve asr accuracy;baseline uncertainty propagation", "pdf_keywords": ""}, "b946ce2c3405969bf615bedc623845b0d3d9b010": {"ta_keywords": "transformer automatic speech;e2e automatic speech;speech recognition asr;speech recognition novel;speech recognition;transformer encoder introducing;encode local acoustic;recurrent neural networks;encoder introducing;transformer self attention;neural networks end;transformer encoder;attention network;encoder introducing context;transformer attentions;self attention network;automatic speech;encoder;transformer attentions monotonic;automatic speech recognition;alternative recurrent neural;speaker attributes transformer;method transformer encoder;recognition asr systems;performance alternative recurrent;recognition asr;helps encode;acoustic information global;recurrent neural;properties transformer attentions", "pdf_keywords": "transformer automatic speech;speech recognition asr;e2e automatic speech;transformer encoder introducing;online transformer decoder;recognition asr systems;transformer encoder;speech recognition;encoder introducing;encoder;transformer decoder outperforms;recognition asr;introducing online decoding;encoder entirely online;transformer decoder;transformer decoder evaluations;encoder entirely;method transformer encoder;online decoding;decoder outperforms;automatic speech;automatic speech recognition;encoder introducing context;decoder outperforms conventional;speech recognition emiru;transformer self attention;performance alternative recurrent;alternative recurrent neural;recurrent neural networks;transformer decoder outperformed"}, "3ad287cf3b17cb109bf991731d2c0dcf8b7db2b1": {"ta_keywords": "lingual morphological tagging;morphological tagging improves;cross lingual morphological;morphological tagging;morphological tagging aims;models cross lingual;work morphological tagging;lingual approaches neural;lingual morphological;cross lingual approaches;existing cross lingual;superior tagging accuracies;lingual approaches;tagging accuracies;tagging accuracies existing;cross lingual;method cross lingual;tagging improves performance;universal dependencies treebank;tagging improves;lrls cross lingual;tags accurately generate;dependencies treebank;tagging;superior tagging;dependencies treebank demonstrate;lingual;languages lrls cross;generate tag sets;accurately generate tag", "pdf_keywords": "lingual morphological tagging;morphological tagging;cross lingual morphological;morphological tagging improves;morphological tagging aims;models cross lingual;tagging combines neural;task morphological tagging;lingual morphological;work morphological tagging;cross lingual approaches;grained annotations syntactic;universal dependencies treebank;lingual approaches;dependencies treebank;existing cross lingual;predicting syntactic;annotations syntactic;superior tagging accuracies;involves predicting syntactic;tagging accuracies;sequence tagging combines;annotations syntactic properties;treebank;tagging accuracies existing;predicting syntactic traits;method cross lingual;morphological tagging chaitanya;sequence tagging;lingual approaches introduction"}, "27e1dbe9f7c71cd6cc1b0357f49aef497e572d09": {"ta_keywords": "pseudo code generator;pseudo code source;generate pseudo code;source code pseudo;statistical machine translation;pseudo code generated;generated pseudo code;machine translation smt;code source code;code generated automatically;pseudo code python;code generator;create pseudo code;japanese pseudo code;code pseudo code;machine translation;automatically generate pseudo;source code;pseudo code;pseudo code largely;code pseudo;production pseudo code;given source code;effort pseudo code;code using statistical;code generated;source code using;machine translation paper;code generator human;learning generate pseudo", "pdf_keywords": ""}, "1abd1efae8c3849e28de926e52d166b7800965a1": {"ta_keywords": "recommendation systems deepaggregation;graph embeddings aggregating;alternatives ranked edges;especially graph embeddings;layer graph embedding;graph embeddings;ranked edges;incomplete ranked lists;aggregating incomplete ranked;graph embedding;embeddings aggregating;embeddings aggregating collection;graph embedding proposed;incomplete rank lists;ranked edges capture;rank aggregation known;specifically rank aggregation;ranked lists;nodes alternatives ranked;incomplete ranked;ranked lists using;aggregation specifically rank;rank aggregation;rank lists preference;handle incomplete rank;rank lists ties;ties compute deep;lists preference aggregation;rank lists;preference aggregation specifically", "pdf_keywords": ""}, "d513a3583bd168ee341ce3b26d54a4e4096da471": {"ta_keywords": "performance mds queues;mds queue analytically;archival storage systems;mds queues;reliability archival storage;mds queue analysing;codes storage frequently;mds queue;archival storage;data storage;storage systems data;data storage systems;hot data latency;data latency;storage systems;storage frequently;latency performance mds;queueing theory;improved reliability archival;tight mds queue;study data storage;storage frequently accessed;queueing;queue analysing latency;mds queues present;codes storage;queue analytically;storage systems based;reliability archival;queue analytically characterize", "pdf_keywords": ""}, "1d634d645bfe0b289fd6a2a0d9210b2a04c9237b": {"ta_keywords": "private learning pretrained;private nlp models;private stochastic gradient;private learning;private nlp;sgd nlp tasks;reveal private learning;private dp learning;sgd nlp;obtain private nlp;large deep learning;stochastic gradient descent;dp sgd nlp;differentially private stochastic;pretrained models hyperparameters;differentially private;learning pretrained models;nlp models outperform;gradient descent dp;learning models text;large pretrained models;differentially private dp;gradient descent;applying differentially private;sgd large;private training approaches;private stochastic;learning high dimensional;private training;deep learning", "pdf_keywords": "differentially private learners;nlp tasks privacy;private dp learning;sgd nlp tasks;private learners;language models strong;private stochastic gradient;large language models;differentially private;differentially private dp;private learners xuechen;language models dp;differentially private stochastic;strong differentially private;dp language models;pretrained language models;dp sgd nlp;abstract differentially private;sgd nlp;applying differentially private;models dp sgd;\ufb01xed privacy budgets;privacy budgets;language models;learning models text;tuning large language;classi\ufb01cation language generation;language generation tasks;\ufb01xed privacy;language models sentence"}, "5ee580fba44c6efb2a9b06f4c62de6b053db7784": {"ta_keywords": "peer review;empirical risk minimization;learning community aggregate;risk minimization;scores final recommendations;common handful reviewers;community aggregate mapping;reviewers reject highly;reviewers;risk minimization erm;reviewers major source;peer review deal;reviewers major;handful reviewers;inconsistency peer review;lp losses vectors;losses vectors choice;recommendations different reviewers;desirable values hyperparameters;empirical risk;community aggregate;mapping criteria scores;lp losses;minimization erm learning;inspired empirical risk;different reviewers major;class loss functions;handful reviewers reject;reviewers reject;hyperparameters", "pdf_keywords": "learning community aggregate;subjectivity peer review;community aggregate mapping;peer review;reviewer community;peer review combining;criteria peer review;visualizing community aggregate;community aggregate;entire reviewer community;reviewer community discussion;review data;understanding preferences community;recommendations capturing opinion;visualizations interpretations aggregate;review data \ufb01rst;mapping visualizing community;preferences community;aggregate mapping visualizing;mapping criteria scores;review combining;scores recommendations capturing;aggregation;aggregation ijcai review;subjectivity peer;aggregate mapping;visualizing community;preferences community use;capturing opinion;machine learning social"}, "45dcccef42ed09cfd2babb630c117e95136b35d1": {"ta_keywords": "dialogue semantics schema;example dialogue semantics;schema guided dialogue;task oriented dialogue;dialogue semantics;labeled example dialogue;dialogue state tracking;building universal dialogue;example dialogue;universal dialogue systems;dialogue systems;guided dialogue dataset;dialogue dataset;dialogue systems seamlessly;dialogue state;outperform descriptions schema;oriented dialogue requiring;popular dialogue state;dialogue requiring similar;guided dialogue;descriptions schema guided;dialogue dataset multiwoz;dialogue;schema guided task;oriented dialogue;descriptions schema;generalization popular dialogue;dialogue requiring;benchmarks schema guided;universal dialogue", "pdf_keywords": "dialogue semantics schema;example dialogue semantics;guided dialogue dataset;dialogue dataset;dialogue semantics;dialogue state tracking;schema guided dialogue;convey semantics apis;benchmarks schema guided;large language models;language models results;semantics apis;annotations alternative description;labeled example dialogue;tracking benchmarks schema;dialogue dataset multiwoz;state annotations alternative;schema representations large;schema guided dataset;natural language descriptions;dialogue example state;single dialogue example;language models;benchmarks schema;annotations alternative;schema representations;semantics schema elements;language descriptions;comprises dialogue history;popular dialogue state"}, "6b9c3f82a0c0fd62f8ae527126b118890cfd452d": {"ta_keywords": "learning logic programs;learning learning logic;learning logic;grammatically biased learning;logic programs;logic programs using;grammatically biased;biased learning learning;antecedent description language;biased learning;logic;explicit antecedent description;learning learning;description language;learning;programs using explicit;using explicit antecedent;explicit antecedent;antecedent description;biased;programs;grammatically;antecedent;language;using explicit;programs using;explicit;description;using", "pdf_keywords": ""}, "1b06fe6ca5f4404e68b066cdea1a74a36e3e0e13": {"ta_keywords": "egocentric manipulation data;objects improving robot;improving robot success;robot success detection;object data improve;robot success;objects improving;robot;improving robot;action outcomes images;objects improves;egocentric manipulation;objects;detection stacking objects;outcomes images objects;classifying action;objects improves performance;pipeline classifying action;images objects language;robot successfully;egocentric;classifying action outcomes;manipulation data training;images objects;placed unseen objects;data objects improves;unseen objects;objects language;unseen objects use;objects use", "pdf_keywords": "vision language data;human annotations object;robot egocentric;objects robot;robot action outcomes;vision language;robot action;egocentric manipulation data;robot egocentric camera;compared robot egocentric;human annotations;trained robot observations;contributions vision language;data objects robot;natural language object;robot;robot observations outcomes;action outcome detection;images object language;robot observations;object referring expressions;detection compared robot;objects robot manipulates;object language referring;objects improves detection;egocentric camera data;egocentric data;object ot manipulation;4000 human annotations;trained robot"}, "4cb3275ec95f4ad407f153aa9dc2d527bc2744e5": {"ta_keywords": "based speech synthesis;speech synthesis technologies;using speech synthesis;speech synthesis using;synthesis using speech;speech synthesis;text speech synthesis;synthesis text speech;speech synthesis text;synthesized speech speech;synthesized speech;synthesize speech specific;speech synthesis makes;synthesize speech;prosody synthesized speech;synthetic speech module;generates synthetic speech;speech synthesis paper;synthetic speech;synthetic speech parameters;synthetic speech target;prosody synthetic speech;speech input modules;interfaces synthesize speech;speech speech input;reflected synthetic speech;using speech input;control prosody synthesized;speech module generates;speech input", "pdf_keywords": ""}, "34c9e3152c9a14af711994230d8a3909daeaa7cf": {"ta_keywords": "empirical risk minimization;risk minimization erm;risk minimization;learning community aggregate;loss functions axioms;empirical risk;inspired empirical risk;minimization erm learning;lp losses vectors;class loss functions;paper loss functions;loss functions;community aggregate mapping;losses vectors choice;class lp losses;lp losses;axioms peer review;consider class loss;loss function erm;minimization erm;losses vectors;specification loss function;community aggregate;loss functions matrix;loss function;vectors choice loss;erm learning;desirable values hyperparameters;loss function amounts;hyperparameters", "pdf_keywords": ""}, "46f88a062df05673ae0731aa17f9f9cc9d3e87bf": {"ta_keywords": "cross sections flows;sections flows;cross sections;flows;sections;cross", "pdf_keywords": ""}, "c4e83bfddb38642debb31097501aec8768f9020e": {"ta_keywords": "contest artificial intelligence;essay contest artificial;ai researcher 2018;acm sigai student;contest artificial;student essay contest;ai researcher;2018 acm sigai;essay contest;ai;win prizes;win prizes including;prizes;contest;acm sigai;artificial intelligence technologies;leading ai researcher;prizes including;artificial intelligence;sigai student;researcher 2018 acm;sigai student essay;prizes including 500usd;intelligence technologies;2018 acm;chat leading ai;leading ai;acm;intelligence;sigai", "pdf_keywords": ""}, "f5813bb0b398007cae10ffdddeab221d4b9b0dc7": {"ta_keywords": "fault characteristics pumped;pumped storage power;storage power station;characteristics pumped storage;simulation technology fault;pumped storage;dynamic simulation technology;research dynamic simulation;power station;dynamic simulation;fault characteristics;technology fault characteristics;storage power;characteristics pumped;simulation technology;simulation;fault;technology fault;station;pumped;dynamic;storage;research dynamic;power;characteristics;research;technology", "pdf_keywords": ""}, "27de5fb45af9799ed0020c978fe3a3080c60401e": {"ta_keywords": "embryo morphokinetic annotations;predicting embryo morphokinetic;morphokinetic annotations;morphokinetic annotations time;predicting embryo;embryo morphokinetic;annotations time lapse;embryo;videos using convolutional;convolutional neural;time lapse videos;convolutional neural networks;morphokinetic;using convolutional neural;annotations time;lapse videos;annotations;neural;convolutional;lapse videos using;time lapse;videos;using convolutional;videos using;neural networks;predicting;lapse;networks;time;using", "pdf_keywords": ""}, "bab35e88a510938d22cb28f2ecc6f6e189c3d8ea": {"ta_keywords": "arabic speech recognition;speech recognition end;arabic speech;speech recognition;arabic;recognition end end;recognition end;modular systems human;end modular systems;systems human;end end modular;end modular;speech;recognition;modular systems;systems;modular;end end;human;end", "pdf_keywords": "transformer arabic asr;challenges arabic asr;e2e transformer arabic;performance arabic language;arabic speech hsr;arabic asr dialects;arabic asr;transformer arabic;asr hsr arabic;hsr arabic speech;arabic language considerably;arabic asr wild2;performance arabic;asr dialects;hsr arabic;arabic speech;native speaker performance;language considerably better;human performance arabic;hsr evaluate linguist;linguist performance;evaluate linguist performance;challenges arabic;dialectal datasets mgb3;arabic language;arabic;asr modular hmm;speech hsr evaluate;evaluated dialectal datasets;speech hsr"}, "0132cb4384c3a6402353d8f349f8dd450d8ea4a2": {"ta_keywords": "deep bi lstm;bi lstm;bi lstm network;spelling normalization bi;historical spelling normalization;bi directional lstms;lstm;spelling normalization;deep neural network;deep neural;lstm network;improving historical spelling;lstms multi task;normalization bi;directional lstms;directional lstms multi;lstms multi;language processing historical;lstm network applied;german natural language;lstms;normalization bi directional;neural network architecture;normalization;neural network;particularly deep bi;natural language processing;task particularly deep;deep bi;neural", "pdf_keywords": "term memory networks;deep bi lstm;short term memory;bi lstm;term memory;spelling normalization;using bi lstms;bi lstms;spelling normalization using;historical spelling normalization;spelling normalization character;bi lstms multi;term memory model;model bi lstm;frame spelling normalization;learning additional normalization;diverse corpus early;bi lstm setting;bi lstm network;lstm;lstms multi task;sequence labeling task;diverse corpus;sequence labeling;outperforms existing normalization;based sequence labeling;corpus early new;lexical resource like;dataset diverse corpus;texts dataset diverse"}, "39e734da43eb8c72e9549b42e96760545036f8e5": {"ta_keywords": "question answering context;existing machine comprehension;machine comprehension datasets;dataset question answering;question answering;reading comprehension architecture;seeking qa dialogs;quac question answering;qa dialogs;comprehension architecture;answering context;comprehension datasets;unanswerable meaningful dialog;machine comprehension;comprehension architecture extended;meaningful dialog context;answering context report;dialogs involve;dialogs 100k questions;meaningful dialog;freeform questions learn;comprehension datasets questions;dialogs involve crowd;qa dialogs 100k;reading comprehension;dialogs;dialog context detailed;information seeking qa;answers questions;dialog context", "pdf_keywords": "question answering context;reading comprehension architecture;question answering;information seeking dialogs;answering context;dialogs sections wikipedia;comprehension architecture;dataset question answering;seeking qa dialogs;comprehension architecture extended;questions highly contextual;seeking dialogs;quac question answering;dialog context;answering context contains;dialogs 100k questions;qa dialogs;contextual openended unanswerable;considers dialog context;dialog context section;seeking dialogs sections;reading comprehension;information seeking qa;model dialog context;dialogs;dialog context conclusion;highly contextual;14k information seeking;qa dialogs 100k;highly contextual openended"}, "68b3905c2f82814294631f2ce29d5be4165e6b1f": {"ta_keywords": "relay network walking;wireless relay nodes;deployment wireless relay;wireless relay network;relay nodes;wireless relay;hop wireless relay;relay node overall;relay network;sequential deployment wireless;relay nodes deployed;relay nodes person;number relay nodes;multi hop wireless;expected number relay;relay;relay node;place relay node;network walking;place relay;impromptu multihop wireless;network walking sink;decision place relay;optimal sequential deployment;deployment multi hop;wireless network;multihop wireless network;multi hop;hop wireless;number relay", "pdf_keywords": "mdp countable state;cost mdp countable;increasing \u03b3r stochastically;\u03b3r stochastically;stochastically increasing ej\u03be;\u03b3r stochastically increasing;relay network arpan;iteration computational complexity;countable state space;mdp countable;stochastic monotonicity \u03b3r;computational complexity;relay network;relay nodes;hop wireless relay;complexity;multi hop wireless;total cost mdp;optimal sequential asyou;network arpan;complexity memory;optimal sequential;wireless relay network;relay;wireless relay;wireless relay nodes;relay nodes person;problem optimal sequential;optimal;stochastic"}, "dc8ebb6d9908542ae474dc2b21bfb6a14216f678": {"ta_keywords": "large multilingual translation;competitive translation results;multilingual translation models;translation models;translation systems;translation models using;translation systems metrics;shortcomings translation systems;evaluate large multilingual;multilingual translation;competitive translation;shows competitive translation;translation results;large multilingual;translation results obtained;baselines pmi corpus;pmi corpus;pmi corpus discover;multilingual;known shortcomings translation;translation;shortcomings translation;corpus;100 hours benchmark;corpus discover;benchmark;corpus discover known;memory compute optimization;optimization far gpu;benchmark standard baselines", "pdf_keywords": ""}, "010df54445ab5f47582eb668dc3488a3e46b55d3": {"ta_keywords": "existing generative models;unsupervised hidden markov;generative models;neuralizing unsupervised hidden;outperforms existing generative;hidden markov model;neuralizing unsupervised;hidden markov;existing generative;generative;results neuralizing unsupervised;generative models competitive;neuralizing;markov;unsupervised hidden;markov model;present results neuralizing;results neuralizing;tag duction;unsupervised;approach tag duction;models;simpler model easily;markov model evaluate;tag;state art simpler;hidden;model easily;include additional context;context work", "pdf_keywords": "unsupervised hidden markov;hidden markov models;neural hidden markov;model hidden markov;neuralizing unsupervised hidden;hidden markov model;neuralizing unsupervised;hidden markov;models hmms;unsupervised neural hidden;markov models hmms;topic models speech;models speech induction;hmms implemented neural;models hmms implemented;unsupervised neural;existing generative models;results neuralizing unsupervised;speech tag induction;models speech;markov models;generative models;topic models;sequence model hidden;induction speech tags;markov models ke;unsupervised systems;speech induction;generative models using;train generative models"}, "90b9d19af75c86f42865052c21305c70f884b5fe": {"ta_keywords": "cost congestion uncertainty;multicommodity selfish routing;congestion uncertainty multicommodity;selfish routing game;congestion uncertainty;congestion costs multiplicative;uncertainty multicommodity routing;selfish routing;underestimates congestion costs;cost congestion;user underestimates congestion;estimates cost congestion;congestion costs;network congestion;routing game uncertain;inefficiencies network congestion;underestimates congestion;congestion decreases users;users overestimate costs;multicommodity routing networks;routing game;urban transportation networks;equilibrium behavior multicommodity;transportation networks;multicommodity routing;network congestion decreases;multicommodity selfish;congestion;favorable network conditions;dynamic pricing policies", "pdf_keywords": ""}, "124385efee78010a4408329dffea4798f5a1ad47": {"ta_keywords": "simultaneous speech translation;speech translation systems;phrasebased translation systems;speech translation language;translation unit segmentation;translation systems wait;translation timing;begin translation delay;large delay translation;delay translation process;delay translation;conventional speech translation;speech translation;translation delay;speech translation conventional;translation timing simultaneous;translation systems;translation conventional speech;translation language pairs;useful speech translation;translation process;translation delay work;phrasebased translation;utterance pause boundaries;choice translation timing;translation systems decide;translation language;translation process particular;utterance pause;lexicalized choice translation", "pdf_keywords": ""}, "0eac6cbd150b1a7e9d757ccc871eea2bf0d89e42": {"ta_keywords": "labels ordinal regression;label representations;soft labels ordinal;deep neural;truth label representations;classification segmentation networks;labels ordinal;segmentation networks;soft labels;classification;ordinal regression attempts;regression ordinal;segmentation networks wildly;ordinal regression;deep neural networks;labels;ordinal regression ordinal;neural networks automatically;depth estimation encoding;regression ordinal regression;networks automatically learn;ground truth label;classification problems categories;truth label;regression monocular depth;image quality ranking;categories;solve classification problems;relationships categories;intraclass interclass relationships", "pdf_keywords": "classi\ufb01cation segmentation networks;softmax soft labels;la softmax;softmax;la softmax soft;softmax soft;segmentation networks plethora;segmentation networks;classi\ufb01cation segmentation;segmentation networks wildly;distance jointly learned;penalties la softmax;label representations;different classi\ufb01cation segmentation;interclass distance jointly;image quality ranking;regression monocular depth;labels ordinal regression;seamlessly incorporating metric;known interclass distance;truth label representations;networks;regression traditional classi\ufb01cation;soft labels ordinal;depth estimation;encodes metric;categories known interclass;interclass distance;encodes metric penalties;soft labels"}, "59f3e3cad309eb4965d67773d68bc2f91b2e376f": {"ta_keywords": "translation corpus endangered;access speech translation;speech translation corpus;translation corpus highland;corpus endangered language;nahuatl speech translation;endangered language documentation;speech translation;translation corpus;mt speech translation;machine translation mt;speech translation st;machine translation;endangered languages els;translation mt speech;puebla nahuatl speech;documentation endangered languages;asr machine translation;endangered language;puebla nahuatl glottocode;corpus endangered;endangered languages;corpus highland puebla;automatic speech;automatic speech recognition;corpus;nahuatl speech;corpus highland;speech recognition asr;translation mt", "pdf_keywords": ""}, "2ac98a28fdae4c01a89f09393c736e72445a4c4e": {"ta_keywords": "interconnecting speech enhancement;speech enhancement pre;adaptation interconnecting speech;speech enhancement;processor speech recognizer;variance adaptation interconnecting;interconnecting speech;pre processor speech;enhancement pre processor;speech recognizer;processor speech;adaptation interconnecting;dynamic variance adaptation;variance adaptation;enhancement pre;pre processor;based dynamic variance;cluster based dynamic;dynamic variance;interconnecting;enhancement;cluster based;cluster;speech;adaptation;variance;recognizer;processor;based dynamic;dynamic", "pdf_keywords": ""}, "da660ca9e6fedefe815e305efd0dcd3bf9b4bb60": {"ta_keywords": "relation extraction limited;approaches relation extraction;relation extraction;relation extraction process;driven relation extraction;relation extraction following;seed relation extraction;learned entity filters;knowledge bases extract;named entity recognizers;entity filters improved;entity recognizers;large knowledge bases;extract substantial supervision;knowledge bases;entity filters;distant supervision paradigm;appropriate named entity;entity recognizers necessary;relation jointly learned;names building completion;application driven relation;jointly learned entity;limited distant supervision;named entity;approach architect names;architect names;following distant supervision;distant supervision;learned entity", "pdf_keywords": ""}, "2078d466766b6876d73ac1981392fa8bd2b9520d": {"ta_keywords": "paper bidding peer;bidding peer review;optimize paper bidding;paper bidding;bidding peer;super ast algorithm;algorithm optimize paper;bidding;peer review;ast algorithm optimize;optimize paper;ast algorithm;super ast;texttt super ast;peer;paper;algorithm optimize;algorithm;ast;review;texttt super;optimize;super;texttt", "pdf_keywords": ""}, "dcac1abd2ae5af180e51994a9c8334a6de915765": {"ta_keywords": "sgd arbitrary compressions;variants quantized sgd;distributed sgd arbitrary;quantized sgd;distributed sgd;quantized sgd error;variants distributed sgd;error compensated sgd;sgd error compensated;sgd arbitrary;biased compression operator;compression operator quantization;feedback biased compression;learning rate convex;quantization gradient;quantization gradient differences;operator quantization gradient;sgd sgd;biased compression;compensated sgd;error feedback quantization;sgd sgd delayed;feedback quantization;new variants sgd;updates sgd;sgd;arbitrary compressions delayed;ec sgd sgd;variants sgd;sgd ec sgd", "pdf_keywords": "distributed sgd;sgd arbitrary compressions;distributed sgd arbitrary;variants distributed sgd;error compensated sgd;variants quantized sgd;sgd error compensated;quantized sgd;sgd delayed updates;sgd sgd delayed;updates sgd;sgd arbitrary;quantized sgd error;compensated sgd;sgd delayed;sgd sgd;delayed updates sgd;sgd;compensated sgd arxiv;compensated sgd ec;sgd ec sgd;ec sgd sgd;sgd arxiv 2010;sgd error;sgd ec;sgd arxiv;ec sgd;arbitrary compressions delayed;compressions delayed updates;converging error compensated"}, "6a9394e5d49c1251c0fb6d7fb0c0813d26c6a907": {"ta_keywords": "zero shot parsers;shot parsers;semantic parsing benchmarks;zero shot learning;semantic parsers;intents semantic parsers;paraphrasing utterances improve;semantic parsing;performance semantic parsing;canonical utterances programs;paraphrasers efficient learning;training examples canonical;paraphrasing utterances;paraphrasers efficient;grammar paraphrasing utterances;programs grammar paraphrasing;grammar paraphrasing;stronger paraphrasers efficient;semantic parsers map;canonical utterances;map natural language;grammars stronger paraphrasers;parsers;paraphrasing;examples canonical utterances;parsing;shot learning synthesizing;stronger paraphrasers;zero shot;shot learning", "pdf_keywords": "zero shot parsers;zero shot parser;train semantic parsers;semantic parsing benchmarks;shot parsers;grammars automatic paraphrasing;semantic parsers;semantic parsers emerging;automatic paraphrasing;shot parser based;performance semantic parsing;automatic paraphrasing using;shot parser;semantic parsing;paraphrasers ef\ufb01cient learning;parsers emerging domains;paraphrasers expressive grammars;paraphrasers expressive;parsers emerging;stronger paraphrasers expressive;paraphrasing using pre;grammars stronger paraphrasers;parsers;parser based idiomatic;paraphrasing;paraphrasers ef\ufb01cient;stronger paraphrasers ef\ufb01cient;paraphrasing using;paraphrasers;stronger paraphrasers"}, "5446a8bbadc2ba2c575353b257f26abae27b3b2a": {"ta_keywords": "items user comparisons;user comparisons;preferences based comparisons;user comparisons specifically;recommendation systems;learning embedding;model user preference;recommendation systems learn;embedding set items;preference user represented;embedding;learning embedding representation;method learning embedding;embedding representation;model user preferences;comparisons relative attractiveness;user preferences based;user represented point;lifted method learning;problem recommendation systems;embedding representation p1;embedding set;consider problem embedding;user preference user;based comparisons relative;preferences based;comparisons relative;attractiveness different items;localizing items user;point model user", "pdf_keywords": ""}, "f49ccfb32aad8e6893e8cbb037c1282572fe6e21": {"ta_keywords": "adversarial samples deep;vulnerable adversarial samples;vulnerable adversarial;adversarial attacks mutated;deep testing methods;vulnerability dnn systems;deep testing;confidence adversarial samples;various adversarial samples;adversarial samples;detect various adversarial;adversarial samples generated;vulnerability dnn;proposed vulnerability dnn;high confidence adversarial;known vulnerable adversarial;adversarial samples makes;various adversarial;kinds adversarial attacks;adversarial samples detection;confidence adversarial;adversarial attacks;adversarial;kinds adversarial;dnn known vulnerable;different kinds adversarial;number deep testing;networks dnn known;dnn models;networks dnn", "pdf_keywords": ""}, "62dc7bdae6700c4409e6d9773d6ecb5c0fab75a4": {"ta_keywords": "approximate dictionary searching;dictionary searching indexing;dictionary searching comparative;dictionary searching;approximate dictionary;methods approximate dictionary;indices optimized retrieval;indexing methods approximate;searching indexing methods;dictionaries frequent words;searching indexing;dictionaries frequent;optimized retrieval update;optimized retrieval;dictionaries dictionaries frequent;dictionaries;russian dictionaries dictionaries;retrieval update solutions;indexing methods;frequent words extracted;indexing;state art indexing;russian dictionaries;guarantee retrieval strings;dictionaries dictionaries;searching comparative;english russian dictionaries;retrieval strings;retrieval update;searching comparative analysis", "pdf_keywords": ""}, "02aebef93baeef3396f3cb4468a7054067f190c6": {"ta_keywords": "entities text empirical;matching entities references;entities references;entities references raw;database entities text;entities text;extract structured database;structured database entities;matching entities;canonical entities;accurate database entities;approach extract structured;extract structured;bayesian approach extract;entity iii matching;inferring schema;inferring schema fields;entities;characterize entity provided;text empirical evaluation;canonical entities ii;learns accurate database;references raw text;characterize entity;entities fields;set canonical entities;entities ii inferring;nonparametric bayesian approach;iii matching entities;schema fields entity", "pdf_keywords": ""}, "f262ef2f50dfcaf07dc6598f22fb9b2470b37cf1": {"ta_keywords": "confident entity spans;entity spans;information extraction;information extraction using;span entities;entity spans linking;information extraction tasks;nested span entities;detecting nested span;span representations using;dynamically constructed span;span entities significant;framework information extraction;dynamic span graphs;span representations graphs;constructed span graphs;spans linking nodes;span enumeration approach;span graph allow;dynamic span graph;span graphs;coreferences observe span;spans linking;dynamic span;nested span;span graphs dynamic;graphs dynamic span;refine span representations;span enumeration;iteratively refine span", "pdf_keywords": "relation extraction tasks;relation detection tasks;entity relation detection;relation extraction;recognition relation extraction;relation detection;entity recognition relation;contextualized information relations;entity recognition;information extraction tasks;relations coreferences introduce;information relations coreferences;relations coreferences;leveraging contextualized information;information extraction;information extraction framework;general information extraction;results entity recognition;leveraging contextualized;multiple information extraction;extraction framework;re\ufb01ned leveraging contextualized;extraction tasks shared;extraction framework demonstrated;extraction tasks diverse;protocol corpus;coreferences introduce dynamic;contextual information making;contextualized information;global contextual information"}, "36906613dcef29263afe711f128da1fc916cbbee": {"ta_keywords": "connectionist temporal classification;temporal classification ctc;temporal classification;corpus spontaneous japanese;evaluation corpus spontaneous;gaussian kernel architecture;proposed gaussian kernelized;corpus;shift invariant kernel;sequence data windowing;connectionist temporal;corpus spontaneous;proposed gaussian kernel;segments efficient training;applied connectionist temporal;classification ctc;gaussian kernel shift;kernel proposed gaussian;kernel shift invariant;long sequence data;data training;gaussian kernelized sa;normalized kernel;classification ctc based;gaussian kernel;gaussian kernelized;ctc based asr;normalized kernel function;variant gaussian kernel;evaluation corpus", "pdf_keywords": "recognition performance long;ctc based asr;encoder ctc based;encoder ctc;recognition performance;improve recognition performance;corpus spontaneous japanese;structure encoder ctc;encoder;conventional masking kernel;gaussian kernelized sa;gaussian kernel architecture;asr model improve;long sequence data;improve recognition;proposed gaussian kernelized;normalized kernel;based asr model;evaluation corpus spontaneous;sequence data windowing;based asr;kernelized sa;proposed gaussian kernel;structure encoder;performance long sequence;model improve recognition;corpus spontaneous;normalized kernel function;novel structure encoder;asr replacing kernel"}, "9a334566b79bc6c6906e2b5285d5ea50b9b99479": {"ta_keywords": "adversarial feature;invariance adversarial feature;adversarial feature learning;invariance adversarial;learning representations invariant;representation maximizing certainty;controllable invariance adversarial;adversarial;predictions controllable invariance;learning representations;representation maximizing;task specific predictions;maximizing certainty;maximizing certainty making;learning meaningful representations;maximizing uncertainty inferring;problem learning representations;specific predictions controllable;feature learning;representations invariant specific;predictions controllable;representations maintain content;problem learning;meaningful representations maintain;representations maintain;maximizing uncertainty;given representation maximizing;representations invariant;meaningful representations;certainty making task", "pdf_keywords": "representation learning minimax;representation learned;adversarial feature learning;adversarial feature;invariant representation learned;invariance adversarial feature;invariance adversarial;adversarial learning;adversarial;controllable invariance adversarial;representation learned resulting;adversarial learning goodfellow;learning minimax;learning meaningful representations;representation make predictions;learning minimax game;advancement adversarial learning;recent advancement adversarial;representations maintain;meaningful representations maintain;formulate representation learning;advancement adversarial;representation learning;representation tries identify;meaningful representations;representation tries;performance invariant representation;representations;representations maintain content;discriminative models benchmark"}, "939dfa4dec88ca167e6572904c4ad2fcbf726f48": {"ta_keywords": "wasserstein gradient flows;potential wasserstein gradient;gradient flows probability;gradienttype potential wasserstein;wasserstein gradient;gradient flows;euclidean gradient flow;edgeweights converges novel;flows probability measures;described gradient flow;edgeweights converges;gradient flow;function edgeweights converges;potential wasserstein;gradient flow suitable;gradient flow technically;limits called graphons;function edgeweights;functions scalar entropy;suitable function edgeweights;gradienttype potential;graphons appropriately;curve space graphons;scalar entropy covered;scalar entropy;involving gradienttype potential;graphons appropriately described;graphons;large graphs;flows", "pdf_keywords": "limits called graphons;functions graphons;edgeweights converges novel;natural functions graphons;edgeweights converges;curve space graphons;function edgeweights converges;graphons;graphons appropriately described;graphons appropriately;large graphs;function edgeweights;space graphons;graphons size;space graphons appropriately;suitable function edgeweights;graphs known converge;graphons homomorphism functions;che16 large graphs;described gradient \ufb02ow;large graphs known;functions scalar entropy;graphs known;novel continuum limit;continuum limits;functions graphons homomorphism;limit exchangeable particle;called graphons size;euclidean gradient \ufb02ow;continuum limit exchangeable"}, "c377bf3ae52dee4075c1e807de9c5579d553de22": {"ta_keywords": "nl dialogue systems;processing automatic dialogue;text speech dialogue;intertwining nl dialogue;automatic dialogue systems;dialogue systems multimodal;automatic dialogue;speech dialogue;dialogue systems;text speech;text speech semantic;speech dialogue tsd;processing text speech;speech recognition corpora;text speech processing;parsing text speech;nl dialogue;conference text speech;speech spoken language;dialogue tsd 2013;dialogue;text speech integrating;texts transcription speech;speech processing automatic;applications text speech;recognition corpora language;speech semantic;speech semantic processing;dialogue tsd;speech analysis", "pdf_keywords": ""}, "ea71f5f59727b63b8912c6db097ba811da41bf5b": {"ta_keywords": "deterministic particle flows;flows constraining stochastic;particle flows constraining;stochastic nonlinear systems;constraining stochastic nonlinear;particle flows;deterministic particle;flows constraining;stochastic nonlinear;constraining stochastic;nonlinear systems;nonlinear systems 24;flows;deterministic;stochastic;modelling quantitative biomedicine;systems modelling;nonlinear;systems modelling quantitative;particle;modelling;quantitative biomedicine university;systems;modelling quantitative;quantitative biomedicine;constraining;centre systems modelling;theoretical computer;biomedicine university;theoretical computer science", "pdf_keywords": "stochastic control;stochastic kuramoto oscillators;precise stochastic control;stochastic control frameworks;stochastic nonlinear systems;deterministic \ufb02ow control;coupled stochastic kuramoto;constraining stochastic nonlinear;control networks coupled;constraining stochastic;\ufb02ows constraining stochastic;brownian bridge dynamics;adaptive evolution;coupled stochastic;adaptive evolution created;networks coupled stochastic;studying adaptive evolution;phenotypes adaptive landscapes;stochastic kuramoto;stochastic nonlinear;adaptive landscapes;synchronisation control networks;networks kuramoto phase;brownian bridge;control networks;reweighted brownian bridge;constraining diffusive systems;oscillators section deterministic;kuramoto oscillators;kuramoto phase oscillators"}, "5a26eeda7c2ca58c2d56f1d580fbbae9eb1a19cd": {"ta_keywords": "approximating pdes neural;pdes neural networks;bounds approximating pdes;deep networks approximate;neural networks approximating;approximating pdes;high dimensional pdes;dimensional pdes;pde coefficients representable;networks approximating solutions;dimensional pdes seemingly;networks approximate solutions;small neural networks;neural networks parameters;pdes neural;elliptic pdes dirichlet;elliptic pdes;linear elliptic pdes;networks approximate;networks approximating;pdes dirichlet;representable small neural;pdes dirichlet boundary;solutions high dimensional;neural networks recent;deep networks;pdes;pde coefficients;neural networks;power neural networks", "pdf_keywords": "approximating pdes neural;deep networks approximate;pdes neural networks;neural networks approximating;bounds approximating pdes;small neural networks;approximating pdes;networks approximating solutions;networks approximate solutions;networks approximating;simulates gradient descent;neural networks parameters;high dimensional pdes;networks approximate;deep networks;representable small neural;dimensional pdes;gradient descent;coe\ufb03cient networks bound;dimensional pdes seemingly;growing neural network;neural networks;pdes neural;networks bound;neural network architecture;small neural;networks bound size;power neural networks;complexity bounds approximating;parametric complexity bounds"}, "7647a06965d868a4f6451bef0818994100a142e8": {"ta_keywords": "trained word embeddings;character aware neural;aware neural language;training transfer learning;neural language models;leveraging character;embeddings character aware;transfer learning;task model training;leveraging character level;transfer learning techniques;neural framework extract;word embeddings character;empower sequence labeling;word embeddings;sequence labeling task;word level knowledge;sequence labeling;transfer learning methods;aware neural;different transfer learning;character level knowledge;effectiveness leveraging character;efficient training transfer;training example conll03;pre trained word;knowledge efficiency training;embeddings character;ner task model;trained word", "pdf_keywords": "sequence labeling model;sequence labeling task;sequence labeling framework;model sequence labeling;sequence labeling;multi task learning;task learning transfer;lstm crf effectively;task learning;language model sequence;effective sequence labeling;task learning strategy;lstm crf;proposed sequence labeling;empower sequence labeling;neural framework extract;language model extract;lm lstm crf;labeling task;texts empower sequence;lstm crf leverages;learning transfer learning;framework lm lstm;labeling task paper;lstm;transfer learning;language model;lm lstm;neural framework;learning transfer"}, "e65676b43338e914ad77afd0fd6ce4bef87943a1": {"ta_keywords": "singing voice conversion;convert singing voice;voice conversion;voice preserving conversion;converted singing voice;convert voice;singing voice waveforms;statistical singing voice;generate converted singing;singing voice preserving;convert singing;possible convert singing;quality converted singing;voice conversion svc;convert voice timbre;differential convert voice;singing voices experimental;speech quality converted;conversion accuracy singer;statistical singing;singing voice various;converted singing;singing voice characteristics;novel statistical singing;voice waveforms;voice preserving;voice waveforms speech;singing voice significantly;natural singing voice;singing voice", "pdf_keywords": ""}, "d29d33f3b92b447d6011606be41b64439a1da088": {"ta_keywords": "contextual bandit algorithm;contextual bandit;regret contextual bandit;bandit algorithm;learns constrained policy;constrained thompson sampling;behavior constrained thompson;agent learns constrained;bandit algorithm underlies;online exploration exploitation;online agent learns;bandit setting provide;online rewards guiding;bandit setting;reward based online;thompson sampling;learns constrained;constrained thompson;online learning obeying;online exploration;learn reward;agent learns;reward feedback ai;bandit;policy guide reward;multi armed bandit;agent learns set;online rewards;armed bandit setting;constrained policy", "pdf_keywords": "contextual bandit algorithm;contextual bandit;regret contextual bandit;bandit algorithm;learns constrained policy;agent learns constrained;constrained thompson sampling;online agent learns;behavior constrained thompson;bandit algorithm underlies;bandit setting provide;teaching agent reward;online exploration exploitation;constrained behavior learned;agent reward driven;bandit setting;agent learns;learns constrained;learned online recommendation;thompson sampling;reward based online;online learning obeying;bandit;online exploration;agent learns set;reward feedback agent;agent reward;feedback agent learns;constrained thompson;reward driven"}, "721d7c82b80f14246d353251837e1711824a9e60": {"ta_keywords": "speech corrupted reverberation;channel speech recognition;multi channel speech;reverberation experiments spatialized;reverberation;speech recognition;reverberation experiments;speech recognition explore;conventional methods reverberant;channel speech;corrupted reverberation experiments;corrupted reverberation;methods reverberant;convolutional beamformer;methods reverberant scenarios;speech recognition performance;convolutional beamformer perform;reverberant scenarios;wpd convolutional beamformer;reverberant;decomposition matrix inverse;speech corrupted;end multi channel;2mix corpus reverb;beamformer;degrades severely speech;severely speech corrupted;multi channel;beamformer perform simultaneous;approaches multi channel", "pdf_keywords": "reverberant speech results;multi channel speech;channel speech recognition;reverberant speech;reverberant single speaker;speech dereverberation separation;mimo speech model;speech recognition framework;speech model end;speech recognition architecture;performing speech dereverberation;\ufb01eld speech recognition;speech recognition;dereverberation beamforming capable;applications reverberant single;speech dereverberation;uni\ufb01ed dereverberation beamforming;channel speech;speech recognition model;mimo speech;dereverberation beamforming;multi speaker scenarios;multi speaker;dereverberation separation recognition;applications reverberant;reverberant single;speaker multi;single speaker multi;speech model;single speaker"}, "a45c3120c077994409093771077a2d16f77674c5": {"ta_keywords": "efficient transfer learning;transfer learning methods;transfer learning;text classification benchmarks;parameters pretrained model;tuning parameters tasks;machine translation;machine translation text;pretrained models;parameters pretrained;parameter efficient transfer;classification benchmarks utilize;efficient fine tuning;new parameter efficient;classification benchmarks;text classification;pretrained model;finetune parameters pretrained;translation text summarization;fine tuning methods;learning methods fine;fine tuning parameters;pretrained models define;parameters tasks;learning methods;transfer design elements;states pretrained models;parameter efficient;studies machine translation;learning methods present", "pdf_keywords": ""}, "62d17b6f6ad77fd71ef9954c7784700d5e316f1f": {"ta_keywords": "privacy language models;natural language privacy;privacy language;language privacy;language privacy social;notion privacy language;meaningful notion privacy;model preserve privacy;language reflects private;making privacy;preserve privacy;sanitization differential privacy;privacy;privacy broadness natural;making privacy concerns;notion privacy;differential privacy;privacy argue existing;privacy concerns;preserve privacy argue;identities making privacy;privacy argue;privacy social norm;differential privacy broadness;privacy social;privacy broadness;privacy concerns broad;violate expectations privacy;expectations privacy;data protection", "pdf_keywords": "privacy language models;natural language privacy;privacy language;language privacy;language privacy social;privacy context language;notion privacy language;human privacy norms;privacy norms;privacy defenses;preserving privacy;models common privacy;sanitization di\ufb00erential privacy;common privacy defenses;meaningful notion privacy;methods preserving privacy;privacy complexity human;preserving privacy context;respect privacy complexity;respect privacy;privacy;di\ufb00erential privacy;human privacy;privacy defenses section;understand respect privacy;privacy social norm;di\ufb00erential privacy broadness;complexity human privacy;privacy complexity;private information"}, "211a6838b9550d227ce81d0bec542ec5b70e290b": {"ta_keywords": "prediction unseen politicians;predict voting patterns;predict voting;freebase prediction voting;predict future votes;forecasting key political;prediction voting;voting patterns politicians;politicians voting records;politicians voting record;prediction voting behavior;patterns politicians voting;outperform prior politicians;votes cast predict;unseen politicians prior;voting behavior politicians;voting records;voting record exists;unseen politicians;patterns politicians;historical voting records;behavior politicians voting;official voting records;voting records official;reduction politicians voting;politicians voting;fails predict voting;votes fails predict;complete historical voting;voting patterns", "pdf_keywords": ""}, "cf9fa9ebbefab1877aa7a501c888a8a618c31abb": {"ta_keywords": "blogs predict polarity;predicting comment polarity;sentiment polarity blog;polarity blog comments;comment polarity content;blog comments learning;polarity blog;sentiment polarity;comments learning community;political blog posts;assign sentiment polarity;blog comments;community discussed blog;polarity based topics;polarity content political;content political blog;topics discussed blog;comments learning;comment polarity;predicting comment;political blog;blogs predict;blog post predicting;tackled blogs predict;comments using computational;assign sentiment;issues tackled blogs;sentiment;methods assign sentiment;blogs", "pdf_keywords": ""}, "42605dca59a3aafe2e5b33741a98dad9ba117395": {"ta_keywords": "measure entity similarity;entity similarity;similarity reranking;based similarity reranking;entity similarity viewed;walk based similarity;personal information graph;similarity reranking case;disambiguation addressed search;addressed search queries;person disambiguation addressed;personal email collections;similarity metric preferable;useful search pim;email collections derived;addressed search;reranking improve graph;threading person disambiguation;similarity viewed tool;email collections;based similarity metric;graph walk based;improving graph walk;graph walk performance;messages terms persons;similarity metric;personal information management;person disambiguation;semistructured domains researchers;enron corpus", "pdf_keywords": ""}, "2a94fa0de804b5efaae1a66f50c3ea96539c46b8": {"ta_keywords": "build conversational agent;dialog based;dialog based examples;dialog utilizing human;database drama conversations;dialog example database;constructing dialog;conversational agent interact;dialog example based;build conversational;human conversation examples;constructing dialog example;filtering constructing dialog;drama conversations retrieving;human human conversation;conversational agent;dialog utilizing;human conversation;finding best dialog;best dialog example;goal dialog based;conversation examples drama;dialog example;aim build conversational;goal dialog utilizing;example database drama;dialog;conversation examples;conversations retrieving;best dialog", "pdf_keywords": ""}, "d0895dccd61c567034d197eecfa5d7d59332061f": {"ta_keywords": "regenerating codes distributed;distributed storage codes;codes distributed storage;erasure codes optimal;minimum storage regenerating;exact regenerating codes;minimum bandwidth regenerating;regenerating codes;regenerating codes allow;storage codes;codes distributed;matrix construction regenerating;bandwidth regenerating;optimal exact regenerating;erasure codes;traditional erasure codes;distributed storage msr;regenerating mbr codes;constructions minimum bandwidth;storage regenerating;regenerating codes class;storage codes allow;codes optimal;distributed storage;regenerating code;framework regenerating codes;codes class distributed;codes allow efficient;bandwidth regenerating mbr;regenerating msr codes", "pdf_keywords": "storage codes optimally;distributed storage codes;codes distributed storage;regenerating codes distributed;constructions mbr codes;storage codes;distributed storage msr;mbr codes feasible;storage repair bandwidth;abstract regenerating codes;exact regenerating codes;codes distributed;codes optimally;regenerating codes;nodes distributed storage;regenerating codes allow;storage msr;codes feasible values;distributed storage;distributed storage network;codes feasible;codes optimally trade;codes class distributed;storage network;storage network chosen;mbr codes;class distributed storage;regenerating codes class;mbr point storage;storage msr mbr"}, "ffe6d7573bb2c4fbfac0cc474804b5b1734a1179": {"ta_keywords": "contextual bandits framework;using contextual bandits;contextual bandits behavioral;contextual bandits;extension contextual bandits;bandits behavioral constraints;bandits framework learns;bandits framework;reward feedback ai;online movie recommendation;recommendation cases rewards;bandits behavioral;learn reward feedback;movie recommendation cases;constrained online movie;learn reward;reward feedback;systems learn reward;reward feedback actions;decisions online;making decisions online;decisions online setting;constraints constrained online;bandits;rewards guiding;constrained online;behavioral constraints constrained;feedback ai;movie recommendation;behavioral constraints observation", "pdf_keywords": ""}, "40fc6e46f2921be346eacff86ce765ff5b28fbdd": {"ta_keywords": "funding rates bitcoin;perpetual swap contracts;contracts based granger;granger causality bitmex;funding correlation bitcoin;rates bitcoin;bitcoin exchange rate;causality bitmex funding;rates bitcoin inverse;perpetual swap;swap contracts bitcoin;exchange rate;futures margin funding;bitmex funding correlation;bitcoin inverse perpetual;inverse perpetual swap;based granger causality;contracts bitcoin derivative;granger causality;market trend discussed;contracts bitcoin;bitmex funding;bitcoin derivative;bitcoin derivative akin;futures;causality bitmex;swap contracts based;swap contracts;market trend;correlation bitcoin", "pdf_keywords": "funding rates bitcoin;contracts based granger;funding rates exchange;heteroskedasticity funding rates;perpetual swap contracts;proof heteroskedasticity funding;bitcoin exchange rate;rates bitcoin;bitmex funding rate;heteroskedasticity funding;based granger causality;exchange rate \ufb01nally;rate correlation bitcoin;bitmex funding;rates exchange;bitmex exchange implications;granger causality;exchange rate;rates exchange rate;rates bitcoin inverse;futures margin funding;exchange rate preprint;perpetual swap;granger causality conclusion;bitcoin inverse perpetual;paper proves heteroskedastic;causal relationship funding;begins proof heteroskedasticity;funding rates predictive;modelling funding rates"}, "f7c9521dcd80127d6d4a72fb407e81a9c518ae8d": {"ta_keywords": "knowledge inductive learning;knowledge intensive inductive;knowledge intensive induction;inductive algorithms;inductive learning;inductive learning frameworks;use knowledge inductive;knowledge inductive;intensive inductive algorithms;inductive algorithms relatively;intensive induction algorithms;induction algorithms utilize;problem induction algorithms;inductive learning critical;induction algorithms;induction algorithms single;induction algorithms point;knowledge free algorithms;cover induction algorithms;algorithms relatively knowledge;point inductive learning;inductive;attempts cover induction;concept definitions generated;knowledge extract;intensive inductive;extracted knowledge;extract relevant knowledge;learn effective concept;knowledge extract relevant", "pdf_keywords": ""}, "737aff546a9112127d7a13a5b835e27a6e1e935e": {"ta_keywords": "speech recognition asr;masked language model;factorized masked language;speech recognition;recognition asr;networks automatic speech;asr audio;new decoding strategy;asr audio conditional;audio factorized masked;automatic speech recognition;automatic speech;predict masked tokens;deep transformers outperform;conditional masked language;masked language;tokens input speech;autoregressive network asr;optimized predict masked;audio conditional masked;new decoding;training frameworks decoder;term memory networks;autoregressive transformer;predict masked;structures asr audio;tokens new decoding;network asr;memory networks;transformer structures asr", "pdf_keywords": ""}, "39fdea1c34832f9bb1644bff81f53fb8ce6b2679": {"ta_keywords": "machine learning;machine learning proceedings;icml 2008 helsinki;conference icml 2008;icml 2008;learning;international conference icml;icml;conference icml;finland june 2008;2008 helsinki finland;2008 helsinki;machine;finland;finland june;helsinki finland;helsinki;helsinki finland june;international conference;fifth international conference;june 2008;fifth international;2008;international;conference;fifth;june;learning proceedings fifth;proceedings fifth international;proceedings fifth", "pdf_keywords": ""}, "b58e80ad8c6e6844c41535080ccbdef06bce3b6e": {"ta_keywords": "3d house simulator;house simulator;chalet 3d house;3d house;house simulator support;autonomous agents including;chalet 3d;planning dynamic environment;dynamic environment chalet;autonomous agents;evaluate autonomous agents;rooms 10 house;present chalet 3d;house room layouts;navigation manipulation chalet;house room;simulator support navigation;agents including tasks;environment chalet;room layouts;rooms;new house room;simulator;planning;autonomous;vision planning;3d;rooms 10;dynamic environment;room layouts present", "pdf_keywords": "learning environment chalet;chalet interactive house;environment chalet interactive;3d house simulator;interactive house environment;interactive house;house simulator;house agent learning;chalet interactive;chalet 3d house;agent learning environment;learning environment;objects environment manipulation;manipulation objects environment;interactive;environment manipulation features;3d house;navigation manipulation objects;learning environment claudia;house simulator support;environment manipulation;objectoriented programming;house environment chalet;testbed studying language;chalet 3d;environment chalet;supports navigation manipulation;present chalet 3d;simulator support navigation;level objectoriented programming"}, "b0efb62aa2a435704a3412d592e73faf6be5ecea": {"ta_keywords": "modeling relationships characters;character relationships;inter character relationships;character relationships fundamental;text model relationships;character relationship types;character relationship;inter character relationship;relationships characters;semantic representations;understanding character intentions;relationships characters enables;relationship categories learned;character intentions;sentiment polarities incorporating;lexical semantic representations;semantic representations leveraging;linguistic features capture;unsupervised modeling relationships;fundamental understanding character;linguistic features;understanding character;inference inter character;text model;modeling relationships;understanding inter character;relationships fundamental understanding;types simple sentiment;raw text model;character intentions goals", "pdf_keywords": ""}, "8b48c55808636a52699b38869df3eba9c8b999d9": {"ta_keywords": "voice conversion dsp;statistical voice conversion;voice conversion;voice conversion promising;voice conversion non;silent speech interfaces;time voice conversion;dsp silent speech;statistical voice;speech produced alternative;real time voice;conversion non audible;speech interfaces implementation;speech interfaces;speaking methods laryngectomees;communication alaryngeal speech;laryngectomees successfully implemented;speech communication alaryngeal;alaryngeal speech produced;time statistical voice;murmur whisper dsp;whisper dsp silent;silent speech communication;speech produced;methods laryngectomees successfully;methods laryngectomees;speech communication;whisper dsp;speech silent speech;alaryngeal speech", "pdf_keywords": ""}, "a0b47c7162d1a3b04b27e27c9fadd2eabc4dab0e": {"ta_keywords": "machine translation iwslt2012;statistical machine translation;translation systems;translation iwslt2012 evaluation;building translation systems;machine translation;translation systems focus;phrase based decoder;naist statistical machine;building translation;11 languagepairs tasks;translation iwslt2012;languagepairs tasks;decoder experiment management;iwslt2012 evaluation campaign;languagepairs tasks use;naist statistical;ted talk tasks;bayes risk decoding;ted task exploring;decoder;based decoder;iwslt2012 evaluation;translation;risk decoding;11 languagepairs;base building translation;describes naist statistical;techniques ted task;statistical machine", "pdf_keywords": ""}, "c3f9c1f702d0c3b35b99502674757b3d8e7dd352": {"ta_keywords": "crosslingual speech synthesis;speech synthesis technique;approach speech synthesis;speech synthesis;speech synthesis preserves;based speech synthesis;speech synthesis based;synthetic speech proposed;synthesized speech;synthetic speech;synthesis based voice;adaptation phonetic;model adaptation phonetic;speech preserving speaker;text speech preserving;based voice conversion;speech preserving;synthesis preserves speaker;individuality synthetic speech;speaker individuality synthetic;synthesized target speech;naturalness preserving speaker;voice conversion;adaptation phonetic correction;hmm based speech;speech waveform directly;synthesized speech waveform;speaker individuality synthesized;native text speech;speech proposed methods", "pdf_keywords": ""}, "89b2a1dc68a7232bc3c68eb4b3e597f99755f7fe": {"ta_keywords": "question answering;factoid question answering;text entities quiz;modeling textual compositionality;textual compositionality matching;modeling textual;question answering typically;learns word phrase;sentences reason entities;compositionality matching text;textual compositionality;input modeling textual;tasks like factoid;phrase level representations;text classification;trivia competition called;matching text entities;competition called quiz;text entities;learns word;called quiz;textual;reason input modeling;representations combine sentences;entities quiz bowl;entities quiz;words representations;qanta learns word;like factoid;questions trivia competition", "pdf_keywords": ""}, "0025b963134b1c0b64c1389af19610d038ab7072": {"ta_keywords": "metasearch formulated ordering;learning preference functions;learning order instances;learning preference;learning order;learning order things;algorithm learning preference;algorithm finally metasearch;learned preference function;web search engine;strategy web search;web search;metasearch;preference judgments statements;search experts;preference judgments;ordering;learning combination search;finally metasearch;preference functions based;problem learning order;preference function np;complete learning order;finding ordering;binary preference function;search engine;finally metasearch formulated;search experts domain;results learning combination;best learned preference", "pdf_keywords": "\u00f6\u00f3 \u00f8\u00f6 \u00f0\u00fd;\u00f3\u00f6\u00f1 \u00f8\u00bd \u00f0\u00f3;\u00f0\u00f3 \u00f8\u00fd \u00f3\u00f6;\u00f6\u00f2 \u00f8\u00f3 \u00f0\u00f9;\u00f3\u00f2 \u00f2\u00f3\u00f8 \u00f0\u00fd;\u00f3\u00f6 \u00f3\u00f4\u00f8 \u00f0\u00fd;\u00f0\u00fb \u00f8\u00f3\u00f8 \u00f3\u00f6;\u00f2\u00f3\u00f8 \u00f0\u00fb \u00f8\u00f8;\u00f0\u00ba \u00f2\u00f8\u00f6 \u00f8\u00fd;\u00f2\u00fd \u00f0\u00f3 \u00f8\u00f3;\u00f8\u00f6 \u00f0\u00fd \u00f4\u00f6;\u00f0\u00f9 \u00f6\u00f2 \u00f8\u00f3;\u00f2\u00f3\u00f8 \u00f0\u00fd \u00f9\u00f6;\u00f2\u00f3\u00f8 \u00f0\u00fd \u00f3\u00f2;\u00f0\u00fd \u00f3\u00f4\u00f8 \u00f8\u00f3\u00f8;\u00f0\u00ba \u00f4\u00f6 \u00f2\u00f8;\u00f6\u00f8 \u00f6\u00fd \u00f0\u00f9;\u00f0\u00ba \u00f3\u00f6 \u00f8\u00f3\u00f6;\u00f0\u00fd \u00f8\u00f6 \u00f3\u00f6\u00f1;\u00f8\u00f3 \u00f0\u00f9 \u00f3\u00f6;\u00f0\u00fd \u00f6\u00f8 \u00f3\u00f6;\u00f0\u00fd \u00f2\u00f8 \u00f2\u00f6;\u00f3\u00f4\u00f8 \u00f0\u00ba \u00f3\u00f6;\u00f3\u00f4\u00f8 \u00f0\u00ba \u00f4\u00f6;\u00f6\u00f2 \u00f8\u00f3 \u00f0\u00f3;\u00f0\u00f8 \u00f4\u00f6 \u00f3\u00f6;\u00f0\u00fd \u00f6\u00f6 \u00f2\u00f8;\u00f0\u00fd \u00f8\u00f3 \u00f2\u00f8;\u00f0\u00fd \u00f3\u00f4\u00f8 \u00f3\u00f6;\u00f0\u00fd \u00f8\u00f3 \u00f4\u00f6\u00f3"}, "448406c38e739695b926d112b2b7aebd4e840322": {"ta_keywords": "speaker diarization information;conversation analyzer;automatically speaks online;speaker diarization;conversation analyzer group;method speaker diarization;time conversation analyzer;real time conversation;particularly speaker diarization;speaker diarization speaks;speaks estimation method;estimate automatically speaks;estimation method speaker;speaks estimation;diarization speaks estimation;speaks online;analyzer group meetings;speaks online manner;automatically speaks;group meetings;online manner speaks;method speaker;speaks information obtained;diarization speaks;speaks information;conversation;estimating directions arrival;meetings goal estimate;particularly speaker;time conversation", "pdf_keywords": ""}, "06431546c21d7c2528aaa170c2e1078e0a82d12e": {"ta_keywords": "pretrained multilingual models;tuning pretrained multilingual;trained multilingual models;multilingual zero shot;pretrained multilingual;pre trained multilingual;trained multilingual;languages zero shot;multilingual models;english transfer languages;multilingual models completely;transfer languages;languages fine tuning;cross lingual transfer;lingual transfer emerging;target languages;target languages zero;multilingual zero;transfer language;lingual transfer finding;target languages diverse;target languages revisiting;impact multilingual zero;lingual transfer;shot cross lingual;multilingual;compare english transfer;transfer languages fine;multilingual models mbert;collect target languages", "pdf_keywords": "multilingual benchmarks english;lingual transfer languages;tuning pretrained multilingual;multilingual benchmarks;english transfer languages;pretrained multilingual models;standard multilingual benchmarks;language transfer;pretrained multilingual;cross lingual transfer;transfer languages english;benchmarks english dominant;transfer languages;language transfer reinforced;zeroshot cross lingual;lingual transfer;source language transfer;target languages;compare english transfer;transfer language;transfer languages \ufb01ne;benchmarks english;multilingual models;target languages diverse;cross lingual;transfer language \ufb01ne;language zeroshot cross;languages english effective;dominant transfer language;multilingual"}, "7570afa31c68e24fce1342b7d67c591787219bc1": {"ta_keywords": "generating wikipedia summarizing;extractive summarization coarsely;document summarization;wikipedia summarizing;summarization source documents;extractive summarization;generating english wikipedia;document summarization source;wikipedia summarizing long;summarization coarsely;generating wikipedia;use extractive summarization;multi document summarization;summarization coarsely identify;summarizing long;summarization source;summarizing long sequences;summarization;generate article generating;summarizing;article generating english;article generating;transduction generating wikipedia;generate article;generating english;english wikipedia articles;model generate article;salient information neural;information neural abstractive;source documents abstractive", "pdf_keywords": "generating wikipedia summarizing;generating english wikipedia;english wikipedia supervised;2018 generating wikipedia;generating wikipedia;wikipedia summarizing;wikipedia supervised machine;generating wikipedia approached;wikipedia summarizing long;wikipedia supervised;summarization source documents;document summarization;documents target wikipedia;multi document summarization;wikipedia articles conditioned;wikipedia article text;shown generating wikipedia;multidocument summarization source;target wikipedia article;input comprised wikipedia;english wikipedia articles;task multidocument summarization;multidocument summarization;summarizing long;multidocument summarization input;abstract generating english;document summarization problem;summarization problem large;summarization source;target wikipedia"}, "4f9a4afc0ba500d839f7ee245513af9b87add8be": {"ta_keywords": "audio visual representations;learning cross modal;similarity video audio;learn audio visual;cross modal discrimination;modal discrimination video;contrastive learning cross;discrimination video audio;representations video audio;visual similarities seeking;visual representations video;exploring cross modal;audio visual;measuring similarity video;modal discrimination positive;similarity video;similarities seeking modal;visual similarities;learn audio;optimizing cross modal;modal discrimination;self supervised learning;seeking modal discrimination;approach learn audio;present self supervised;good representations video;positives measuring similarity;self supervised;video audio feature;audio feature spaces", "pdf_keywords": "optimizing crossmodal similarity;audio visual learning;semantically related videos;visual audio representations;modal similarity improve;crossmodal similarity;optimize modal similarity;similarity related videos;cross modal similarity;learning video audio;contrasting video representations;learn visual audio;learning optimizing crossmodal;learns cross modal;learning cross modal;related videos optimizing;similarity modal similarity;improve visual audio;crossmodal similarity bene\ufb01cial;similarity improve visual;audio representations leverage;representations multiple audios;audio representations;modal similarity;audio visual instance;audio visual;videos optimizing visual;video representations;visual audio;visual similarity"}, "2fb54dfcb1a62deac6565e82f2a87919d33074da": {"ta_keywords": "minimizing convex functions;methods minimizing convex;near optimal methods;minimizing convex;convex functions lipschitz;optimal methods minimizing;optimal methods;convex functions;near optimal;lipschitz th derivatives;methods minimizing;convex;functions lipschitz th;optimal;functions lipschitz;minimizing;lipschitz th;lipschitz;th derivatives;derivatives;methods;near;functions;th", "pdf_keywords": ""}, "5aa3c6ab6cc55c24bab224505e8ad5a4d9863706": {"ta_keywords": "focus attention decoding;attention decoding;attention decoding ways;task learning attention;learning attention historical;learning attention;normalization learning pronounce;named task learning;learn focus attention;text normalization learning;attention historical text;task learning;proposed attention mechanisms;training encoder decoder;task learning learn;focus attention;attention mechanisms;attention mechanisms analyze;historical text normalization;learning learn focus;learning pronounce interestingly;text normalization;attention;multi task learning;learning pronounce;phoneme dictionary auxiliary;grapheme phoneme dictionary;encoder decoder architectures;normalization learning;novel encoder decoder", "pdf_keywords": ""}, "86db47e228167439f15ee320a8a81d386f529a0c": {"ta_keywords": "environment manipulation tasks;language based environment;generative language;manipulation execution guided;environments language;based environment manipulation;using generative language;tasks lemon language;manipulation tasks specifically;environments language model;environments using generative;manipulation tasks;environment manipulation execution;generative language model;environment manipulation requires;execution guided;natural language instructions;lemon language based;execution guided pretraining;knowledge environments language;language instructions challenging;environment manipulation;pre training corpus;agents manipulate environment;execution guided pre;manipulate environment;synthetic pre training;manipulation requires agents;propose execution guided;environments propose execution", "pdf_keywords": "generative language;environment manipulation tasks;using generative language;language based environment;manipulation execution guided;lemon language based;generative language model;environments language model;environments language;environments using generative;propose lemon language;manipulation tasks;based environment manipulation;execution guided;lemon language;execution guided pretraining;pre training corpus;environment manipulation execution;knowledge environments language;manipulation tasks speci\ufb01cally;language model experimental;execution guided pre;lemon execution guided;training corpus;lem tasks propose;language model pure;lem tasks;environment manipulation;language model;propose execution guided"}, "2842c21e879ee581aa50704817454f21b539fc69": {"ta_keywords": "opinion sentiment hindi;sentiment hindi english;opinion detection languages;classifying opinionated tweets;languages classifying opinionated;sentiment hindi;twitter linguistic research;twitter linguistic;tweets indian users;speakers twitter linguistic;tweets indian;classifiers opinion detection;expression opinion sentiment;opinion detection;unique tweets indian;expressing opinion sentiment;language expression emotion;english speakers twitter;expression emotion sentiment;opinionated tweets;opinionated tweets positive;classifying opinionated;develop classifiers opinion;opinion sentiment;emotion sentiment;users specifically hindi;sentiment dewaele 2010;linguistic research multilingual;hindi english bilinguals;specifically hindi english", "pdf_keywords": ""}, "a010b3aa83d7d80e52c84d5f239f940eb33df904": {"ta_keywords": "speech recognition asr;architecture automatic speech;speech corpora training;recognition asr trained;speech recognition;automatic speech;modal data augmentation;automatic speech recognition;transcribed speech corpora;encoders acoustic input;data augmentation end;asr trained using;data augmentation network;end asr architecture;encoders acoustic;recognition asr;acoustic symbolic input;trained using emph;data augmentation;separate encoders acoustic;augmentation network mmda;smaller transcribed speech;acoustic input symbolic;modal acoustic symbolic;transcribed speech;augmentation network;acoustic input architecture;multi modal acoustic;asr trained;modal acoustic", "pdf_keywords": "multilingual training augments;languages pretrain encoder;data augmentation;new data augmentation;modal data augmentation;augmentation scheme encoderdecoder;pretraining augmenting data;augments training data;data augmentation scheme;multilingual training;data augmentation mmda;speech multilingual training;language models bolstering;bolstering decoder language;mmda data augmentation;augmentation mmda data;augmenting data;variants improve multilingual;pretrain encoder decoder;improve multilingual;training data transcribed;pretrain encoder;languages pretrain;target languages pretrain;augmentation mmda;trained supplemental text;outperforms monolingual;speech multilingual;unsupervised language modeling;pretraining augmenting"}, "f784ab218692364b9c8a1f8064809e4524116f3a": {"ta_keywords": "training algorithms byzantine;train decentralized datasets;tolerant decentralized training;distributed training;federated learning;decentralized training;distributed training algorithms;computing federated learning;decentralized datasets rigorously;algorithms byzantine;secure byzantine;federated learning work;algorithms byzantine tolerance;train decentralized;decentralized datasets;decentralized training emphasizes;secure byzantine tolerant;protocol secure byzantine;byzantine tolerant decentralized;byzantine tolerance algorithms;byzantine sybil attacks;bounds resistance byzantine;byzantine attackers;specialized distributed training;byzantine sybil;volunteer computing federated;resistance byzantine sybil;resources train decentralized;presence byzantine attackers;byzantine", "pdf_keywords": "byzantinetolerant decentralized training;distributed training protocol;distributed training;novel distributed training;decentralized training;secure byzantinetolerant decentralized;protocol secure byzantinetolerant;secure byzantinetolerant;nonconvex losses byzantine;losses byzantine attackers;decentralized training emphasizes;deep learning workloads;byzantinetolerant decentralized;learning workloads rigorously;training protocol;byzantine attackers;losses byzantine;training protocol designed;large scale deep;scale deep learning;deep learning;byzantine;convex nonconvex losses;byzantinetolerant;novel protocol secure;protocol designed large;novel distributed;learning workloads;bounds convex nonconvex;distributed"}, "2dd1504d54f8d7e01e1323a9f876f35bb86356da": {"ta_keywords": "incentives given transportation;agent preference models;policy nudging commuters;intelligent transportation;demand mobility services;solutions intelligent transportation;altruistic behavior agent;personal demand mobility;integrate agent preference;agent based simulation;incentive policy city;demand mobility;planners need simulation;transportation use personal;intelligent transportation technologies;agent preference;models capture altruistic;transportation use;incentive policy;disutility monetary incentives;transportation;policy city nudging;nudging commuters;mobility services;incentive policy nudging;agent addition;behavior agent addition;preference models capture;preference models;learned data driven", "pdf_keywords": ""}, "c9d65eee1b5df8ccda87c024b88e1b620099b316": {"ta_keywords": "humans instructions robots;natural language commands;instructions robots;robot instructed human;grounded natural language;instructions robots using;robots collect meaningful;robot instructed;language commands instruction;unrestricted natural language;robots;language commands;commands instruction sequences;humans robots;robot;blocks robot instructed;setting humans instructions;natural language;empirically testable algorithms;architectures interpreting contextually;gap humans robots;humans instructions;propose neural architectures;testable algorithms bridging;contextually grounded natural;commands instruction;neural architectures interpreting;testable algorithms;robots collect;instructed human", "pdf_keywords": ""}, "f6db40e1f0477d27a34240b2e11d6893b9e85b7b": {"ta_keywords": "afford air purifiers;air purifier alerts;affordable air purifier;air purifiers;air purifier;air purifiers protect;purifier alerts;purifier alerts users;purifiers protect;purifiers protect unaware;hazardous air quality;activate filters;change wildfires pollution;activate filters universally;create affordable air;air quality conditions;air quality;people afford air;time activate filters;purifiers;purifier;affordable air;hazardous air;wildfires pollution;filters;severely hazardous air;filters universally accessible;filters universally;pollution;afford air", "pdf_keywords": ""}, "9a36d6b76b3b223aa877b4243e5fdfe5c998689e": {"ta_keywords": "simulation electromagnetic pump;electromagnetic pump tc;magnetohydrodynamic simulation electromagnetic;electromagnetic pump;magnetohydrodynamic simulation;pump tc;magnetohydrodynamic;simulation electromagnetic;pump;electromagnetic;tc;simulation", "pdf_keywords": ""}, "3f59122d4cac12f27ad6ae379deefd9f3fa81f29": {"ta_keywords": "robot prospection interpretable;execution robot prospection;plans language predicting;robot prospection;goals execution robot;command predict;prescribed command predict;plans language;interpretable plans language;language predicting future;prospection interpretable plans;robot;future order robots;natural language command;execution robot;interpretable plans;robots;robots useful real;robots useful;command predict horizon;predicting future;language predicting;prospection interpretable;order robots useful;sequence intermediate goals;order robots;motions intermediate goals;learning representations;natural language;predicting future order", "pdf_keywords": "plans natural language;language commands robot;interpretable plans natural;inferring interpretable plans;commands robot;commands robot simulated;execution robot dreamcell;interpretable plans;goals execution robot;language raw sensor;robot simulated scenes;natural language commands;robot dreamcell;plan inference;robot;plan inference agnostic;plans natural;level plan inference;task suf\ufb01cient planning;execution robot;language raw sensory;plans generated;robot dreamcell architecture;planning;natural language command;learn representations task;suf\ufb01cient planning execution;language learning component;plan inference converted;natural language raw"}, "0b2ff02ab23e5c9910b98fb87c4d58045dbe89ce": {"ta_keywords": "reverberant speech recognition;reverberant speech;includes reverberant speech;reverberation time estimation;reverberation;challenge includes reverberant;method reverberation;reverberation time;speech recognition;speech recognition task;dereverberation method reverberation;reverberant;method reverberation time;includes reverberant;reverb challenge includes;introduced reverb challenge;asr techniques discriminative;reverb challenge;reverb simulated;reverb;error rate reverb;rate reverb simulated;rate reverb;state art asr;deep neural networks;discriminative training;mixture model deep;reverb simulated real;techniques discriminative training;asr techniques", "pdf_keywords": ""}, "d72a1579074a1a2bc500f257474144b1957d5166": {"ta_keywords": "learning coded computation;coded computation neural;machine learning coded;learning coded;coded computation framework;learning based coded;coded computation enables;coded computation imparting;coded computation approaches;coded computation impart;performing coded computation;existing coded computation;coded computation;computations existing coded;based coded computation;coded computation general;potential coded computation;computation neural;widely deployed neural;reach coded computation;deployed neural networks;performing coded;coded;deployed neural;challenges performing coded;computation neural network;linear computations support;computations support;based coded;existing coded", "pdf_keywords": ""}, "c5141ed9ed785a6a1df61b36883e6dfa19a59ff7": {"ta_keywords": "channel speech separation;speech separation;speech separation multiple;speech separation conducted;source separation performance;single channel speech;source separation;chime mixer corpora;corpora single channel;separation performance;mixer corpora;channel speech;separation performance demonstrate;data training robust;mixer corpora evaluate;separation multiple;separation multiple domains;training robust;corpora single;chime mixer;training robust models;building corpora single;separation;separation conducted using;field read speech;using chime mixer;current source separation;using chime;separation conducted;building corpora", "pdf_keywords": "speech separation evaluation;speech separation performance;speech separation;standard speech separation;channel speech separation;investigate speech separation;speech separation technologies;evaluation source separation;speech created isolate;source separation performance;single channel speech;speaker speech regions;pairing utterances finally;single speaker segmentation;pairing utterances;separation evaluation clean;source separation;speaker segmentation;separation evaluation;overlap datasets training;speech regions developed;separation performance;method pairing utterances;speaker segmentation algorithmic;single speaker speech;realistic conversational speech;synthetic overlap datasets;speech regions;cleanup single speaker;channel speech"}, "77000dba4b0638bb8f4222efcd731e040938c846": {"ta_keywords": "interaction vehicle hmi;driver prediction;improve interaction vehicle;driver prediction improve;vehicle task performance;vehicle interface;improve vehicle task;interaction vehicle;vehicle task;prediction improve interaction;prediction methods user;vehicle hmi;capability vehicle interface;vehicle interface terms;vehicle hmi proposed;user actions intentions;performance proposed prediction;driver;hmi proposed prediction;actions intentions used;actions intentions;improve vehicle;vehicle;proposed prediction methods;user actions;prediction methods;methods user actions;task performance proposed;proposed prediction;capability vehicle", "pdf_keywords": ""}, "ab8be9e585e599db99d8451e63a2311d88ff9293": {"ta_keywords": "cache clusters twitter;effectiveness memory caching;memory caching systems;caching extensively;memory caching extensively;cache clusters;cache workloads collecting;caching systems;memory cache clusters;memory caching;cache workloads;caching extensively increase;caches;production caches;use memory caching;world cache workloads;real world cache;value cache clusters;caching;strategy production caches;production caches surprising;cache;memory cache;153 memory cache;caching systems fine;caches surprising;caches surprising example;world cache;clusters twitter work;twitter work significantly", "pdf_keywords": ""}, "c612905cffc5a9aa9f0d8ac7ce1fd17f90413dab": {"ta_keywords": "modeling argumentative dialogue;modeling argumentative;dialogue explicitly models;argumentative dialogue;argumentative dialogue explicitly;attention model addressed;predicting argument;argument goal predicting;predicting argument successfully;sentences picked attention;dialogue;attention model;dialogue explicitly;architecture modeling argumentative;frequently successful arguments;attention;present neural architecture;detection attention model;picked attention model;goal predicting argument;discussions change view;argument based evaluation;detection attention;discussions change;attention model identifies;evaluation discussions change;argumentative;argument goal;models interplay opinion;neural architecture modeling", "pdf_keywords": "modeling argumentative dialogue;modeling argumentative;dialogue explicitly models;argumentative dialogue explicitly;argumentation detecting;argumentative dialogue;view argumentation detecting;predicting argument;view argumentation;architecture modeling argumentative;argument goal predicting;dialogue explicitly;reasoning modeling interaction;argument bridge;argumentation booktitle 2018;modeling interaction reasoning;argumentation detecting vulnerable;view argumentation yohan;changes view argumentation;dialogue;computational linguistics;2018 attentive interaction;predicting argument successfully;argumentation booktitle;models interplay opinion;argumentative;computational linguistics human;argumentation;argument order predict;change view argumentation"}, "5547eff5376c56358be56f8bcc3a4b6ce4600bb5": {"ta_keywords": "toolkits robust speech;toolkits speech enhancement;robust automatic speech;model toolkits speech;toolkits speech;speech recognition asr;robust speech processing;robust speech;available robust asr;asr toolkits;robust asr;general asr toolkits;build robust asr;asr toolkits language;robust asr systems;automatic speech recognition;speech recognition;deep learning toolkits;toolkits deep learning;automatic speech;speech enhancement microphone;end asr toolkits;speech enhancement;speech processing;recognition asr;toolkits available robust;recognition asr techniques;speech processing aim;toolkits robust;asr applications", "pdf_keywords": ""}, "f430c43018f17cabccd3a2e9258aff3da508afe1": {"ta_keywords": "gaze features svm;unknown word detection;word detection;detection foreign language;word detection foreign;language based eye;features svm;eye gaze features;eye gaze;svm;gaze features;gaze;based eye gaze;foreign language based;foreign language;detection foreign;unknown word;eye;language based;language;detection;based eye;word;features;foreign;unknown;based", "pdf_keywords": ""}, "4b34a4cc5bc9defb0f530d61f9b0f843071e227c": {"ta_keywords": "significance menstrual hygiene;menstrual hygiene;menstrual hygiene important;effects menstrual hygiene;menstrual hygiene excessive;association menstrual hygiene;hygiene excessive vaginal;vaginal bleeding delivery;excessive vaginal bleeding;blood stains menstrual;hygiene excessive;vaginal bleeding;stains menstrual;bleeding delivery;prevalence excessive vaginal;examine significance menstrual;women use hygienic;hygiene;stains menstrual period;vaginal bleeding high;hygiene important;use hygienic method;bleeding delivery addition;bleeding delivery chi;significance menstrual;prevent blood stains;bleeding delivery ranged;use hygienic;bleeding;effects menstrual", "pdf_keywords": ""}, "39025112f6a40d8aae38f2e966bb27cbc35ea25d": {"ta_keywords": "research showcase cmu;showcase cmu;research showcase;cmu;showcase;research", "pdf_keywords": ""}, "0db557c4315b1e08ef65ff15b96eb7630014bf72": {"ta_keywords": "digressions discussion detecting;discussion detecting;discussion detector based;discussion detecting unnecessary;digressions discussion detector;discussion detector;detecting unnecessary utterances;unnecessary utterance detection;utterance detection avoiding;utterance detection;utterances having dialogue;having dialogue;unnecessary utterances;automatic summarization;typical automatic summarization;utterances;automatic summarization method;having dialogue intervene;summarization method unnecessary;dialogue;dialogue intervene;dialogue intervene evaluation;avoiding digressions discussion;summarization method;method unnecessary utterance;unnecessary utterance;utterance;summarization;topic shifts;unnecessary utterances having", "pdf_keywords": ""}, "09a169c853e24b3a5196eefeab4c94eaac744cda": {"ta_keywords": "political ideology detection;ideology detection;ideology detection using;crowdsource political annotations;political annotations phrase;political annotations;sentence political ideology;task identifying political;recursive neural networks;identify ideology text;identifying political;recursive neural network;crowdsource political;sentence political;elements crowdsource political;recursive neural;using recursive neural;identifying political position;sentiment analysis;sentiment analysis successfully;ideology text focus;ideology text;evinced sentence political;apply recursive neural;subsentential elements crowdsource;neural network rnn;annotations phrase;techniques identify ideology;work sentiment analysis;annotations phrase sentence", "pdf_keywords": ""}, "3a72f1346f3cd41e14b45c7fba5259bc77357ed4": {"ta_keywords": "pac learning recursive;learning recursive logic;recursive clauses programs;model pac learnability;cryptographically hard learn;clauses efficiently learnable;pac learnability;recursive logic programs;efficiently learnable;learning recursive;ary recursive clauses;learnability;efficiently learnable particular;recursive clauses efficiently;learnable particular following;programs linear recursive;clause hard learning;learnable particular;recursive logic;ary recursive;learnable;determinate ary recursive;classes cryptographically hard;clauses programs constant;pac learning;linear recursive clauses;recursive clause constant;hard learning boolean;learn programs unbounded;clauses linear recursive", "pdf_keywords": "learning recursive programs;limitations learning recursive;cryptographically hard learn;recursive programs;recursive clauses programs;learning recursive;recursive programs valiant;programs linear recursive;depth determinate programs;recursive clauses learning;program classes cryptographically;classes cryptographically hard;program linear recursive;recursive calls programs;learn programs unbounded;containing linearly recursive;linearly recursive clauses;recursive calls;depth linear recursive;linearly recursive;cryptographically hard;computational limitations learning;non recursive;single linear recursive;classes cryptographically;linear recursive clauses;clause hard learning;hard learning boolean;depth determinate program;recursive clauses linear"}, "b62d63580b81a2cbb20c3c1593dd62d118e4cb07": {"ta_keywords": "code natural language;programs natural language;natural language specifications;models code generation;semantic example selection;code generation;synthesizing programs natural;trained models code;synthesizing programs;synthesizing code;using constrained semantic;constrained semantic;descriptions using gpt;synthesizing code natural;trained language models;syntax scope typing;generate code;code providing flexible;natural language descriptions;language descriptions using;programs target language;model samples programs;constrained semantic decoding;syntax scope;including syntax;examples pre trained;semantic decoding csd;pre trained language;interface synthesizing programs;methods synthesizing code", "pdf_keywords": "programming language constraints;reliable code generation;code natural language;language constraints language;code generation pre;models code generation;programming languages boosting;code generation;code generation observe;synthesizing code;world programming languages;programming language;synthesizing code natural;constraints language model;constraints language;language constraints;programming languages;real world programming;trained models code;reliable code;methods synthesizing code;languages boosting prediction;programs target language;constrained semantic decoding;programs using constrained;constrained semantic;using constrained semantic;aligns programming language;framework reliable code;languages boosting"}, "af85c67a1f30f8359be1091234118492b511a088": {"ta_keywords": "\u540c\u6642\u97f3\u58f0\u7ffb\u8a33\u306b\u304a\u3051\u308b\u7ffb\u8a33\u7cbe\u5ea6\u3068\u9045\u5ef6\u6642\u9593\u3092\u540c\u6642\u306b\u8003\u616e\u3057\u305f\u8a55\u4fa1\u5c3a\u5ea6 \u8a00\u8a9e\u7406\u89e3\u3068\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3 \u7b2c6\u56de\u96c6\u5408\u77e5\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0;\u540c\u6642\u97f3\u58f0\u7ffb\u8a33\u306b\u304a\u3051\u308b\u7ffb\u8a33\u7cbe\u5ea6\u3068\u9045\u5ef6\u6642\u9593\u3092\u540c\u6642\u306b\u8003\u616e\u3057\u305f\u8a55\u4fa1\u5c3a\u5ea6 \u8a00\u8a9e\u7406\u89e3\u3068\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3;\u540c\u6642\u97f3\u58f0\u7ffb\u8a33\u306b\u304a\u3051\u308b\u7ffb\u8a33\u7cbe\u5ea6\u3068\u9045\u5ef6\u6642\u9593\u3092\u540c\u6642\u306b\u8003\u616e\u3057\u305f\u8a55\u4fa1\u5c3a\u5ea6;\u8a00\u8a9e\u7406\u89e3\u3068\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3 \u7b2c6\u56de\u96c6\u5408\u77e5\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0;\u7b2c6\u56de\u96c6\u5408\u77e5\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0;\u8a00\u8a9e\u7406\u89e3\u3068\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3", "pdf_keywords": ""}, "cd9e1eac4c93a314254cf8a8682ed5f01b6a808f": {"ta_keywords": "reasoning neural retrieval;kb reasoning neural;reasoning neural;accurate neural reasoning;reasoning kb neural;reasoning rely embedded;neural approaches reasoning;neural reasoning;deeper language understanding;reasoning question answering;embedded generalizations knowledge;question answering;generalizations knowledge base;rely embedded generalizations;deeper language;knowledge base;kb neural approaches;embedded generalizations;memorization highly specific;novel kb embedding;reasoning rely;neural reasoning question;knowledge base kb;embedding;generalizations knowledge;embedded triples memorization;question answering propose;neural retrieval;kb neural;memorization highly", "pdf_keywords": "emql neural;incorporate emql neural;emql neural module;emql generalizes;emql natural language;concluding discussion emql;emql;possible incorporate emql;qe scheme expressive;emql natural;discussion emql;query embedding qe;qe methods;embedding qe method;performing prior qe;showed emql generalizes;embedding qe;expressive set relational;incorporate emql;qe systems;relational operators including;discussion emql new;relational operators;qe systems disagree;emql new;prior qe;qe method combines;qe method;qe module kbqa;relational \ufb01ltering operation"}, "9712ebfbc4f86c68403f64918463edad3e553ac6": {"ta_keywords": "sensor selection dynamic;selection dynamic sensor;dynamic sensor subset;selection centralized tracking;sensor activation centralized;dynamic sensor activation;sensor subset selection;centralized tracking;sensor selection;activation centralized tracking;sensor networks cyberphysical;active sensor selection;dynamic sensor;energy efficient tracking;centralized tracking time;stochastic approximation learning;sensor networks;sampling stochastic approximation;sensor subset;sampling stochastic;tracking time varying;time varying stochastic;efficient tracking;tracking proposed methods;selection dynamic;algorithm tracking proposed;tracking proposed;stochastic approximation;sensor activation;varying stochastic", "pdf_keywords": "sensor activation centralized;dynamic sensor subset;centralized tracking stochastic;sensor subset selection;optimal dynamic sensor;sensor networks cyberphysical;selection centralized tracking;sensor subset;sensor networks;dynamic sensor activation;algorithm centralized tracking;centralized tracking;centralized tracking time;activation centralized tracking;sensors inactive sensors;tracking stochastic;inactive sensors;dynamic sensor;subset selection tracking;tracking stochastic process;active sensors inactive;inactive sensors proposed;active sensors;sensors inactive;2018 dynamic sensor;examine centralized tracking;sensor activation;centralized tracking address;subset selection centralized;problem dynamic sensor"}, "873dff010c00f0601d6939324929eeabb1ddbd6e": {"ta_keywords": "threshold secret sharing;secret sharing;secret sharing dealer;network sneak algorithm;sneak algorithm requiring;network sneak imposes;problem secret sharing;protocols secure multiparty;shares secret participant;communication network sneak;secret sharing important;sneak algorithm;shares secret;secure multiparty computation;sharing dealer;shares network sneak;algorithm disseminating shares;network sneak;secure multiparty;pass shares secret;sharing dealer does;secret participant;shamir threshold secret;threshold secret;distributed algorithm disseminating;protocols secure;consider problem secret;cryptographic protocols;component cryptographic protocols;secure message transmissions", "pdf_keywords": "distributed secret;distributed secret dissemination;algorithm secret sharing;threshold secret sharing;secret dissemination network;network sneak algorithm;secret dissemination;sneak distributed ef\ufb01cient;sneak distributed;secret sharing general;secret sharing;sneak algorithm requiring;sneak algorithm;secret sharing setting;2014 distributed secret;presents sneak distributed;shares secret participants;secret sharing important;network sneak;collusionresistance irrespective network;shares network sneak;secure multiparty computation;ef\ufb01cient algorithm secret;cost secret sharing;algorithm secret;shares secret;disseminate shares secret;protocols secure multiparty;algorithm called sneak;threshold secret"}, "35cb2b9febada179689724c78dfe31d9fa3f74c4": {"ta_keywords": "store erasure coding;codes erasure coding;erasure coding;erasure coding fragments;azure storage;windows azure storage;codes erasure;erasure coding called;erasure coding windows;coding windows azure;azure storage paper;local reconstruction codes;azure storage cloud;storage cloud storage;cloud storage;reconstruction codes;storage cloud;cloud storage provides;store erasure;use store erasure;reconstruction codes lrc;set codes erasure;number erasure coding;offline keeping storage;keeping storage;storage;azure;storage provides;coding called local;windows azure", "pdf_keywords": "erasure coding;erasure coding design;erasure coding fragments;codes erasure coding;erasure coding implementation;storage codes erasure;erasure coding called;codes erasure;number erasure coding;modern storage codes;reconstruction codes;reconstruction codes lrc;local reconstruction codes;reconstruction codes section;addition erasure coding;storage codes;set codes erasure;codes higher overhead;coding fragments;coding design;coding implementation important;coding implementation;coding fragments need;coding;examples shorter codes;shorter codes;codes section illustrate;reconstructing data fragments;modern storage;reed solomon code"}, "35c6bdab35e8fd4e982302b5270da3c8098c58b1": {"ta_keywords": "networks compositional instruction;instructions sequences diverse;compositional instruction;language instructions sequences;modularization improves generalization;natural language instructions;training sequence modules;compositional instruction following;instructions sequences;instructions predicting subgoal;subgoal compositions environments;subgoal compositions;modularization improves;modular networks compositional;novel subgoal compositions;following natural language;sequence modules;benchmark modularization improves;challenging instruction following;learning segment instructions;sequences diverse subgoals;instructions specific subgoal;carry natural language;generalization novel subgoal;improves generalization novel;compositions;benchmark modularization;modularization;natural language;challenging instruction", "pdf_keywords": ""}, "32feca141fce06c6588b4014d27953a3fc25f19b": {"ta_keywords": "piglet model learns;learns world dynamics;learns physical commonsense;knowledge ground language;language propose piglet;model linguistic;language model giving;language summaries physical;unified model linguistic;language model;ground language;representation natural language;model learns physical;model linguistic form;ground language able;dynamics model learns;physical commonsense knowledge;natural language;effectively learns world;model learns;sentence simulate neurally;learns just objects;model effectively learns;natural language propose;interface language model;likewise natural language;learns world;language;model learns just;commonsense knowledge interaction", "pdf_keywords": "piglet language grounding;interaction grounding language;grounding language simulated;grounding language;language factorize embodied;language grounding neuro;ground language;knowledge ground language;language grounding;piglet language;grounding neuro symbolic;performs grounding language;study language grounding;piglet model learns;factorize embodied agent;grounding language transformers;language simulated;language simulated world;embodied agent;embodied agent explicit;neuro symbolic interaction;grounding common sense;model linguistic;symbolic interaction;factorize embodied;model language;ground language factorize;language grounding common;dynamics model language;sense reasoning representation"}, "d9c2242e3aa17db649c92d7d4db46509f3d203db": {"ta_keywords": "confidence reinforcement learning;constrained markov decision;constraints learning probability;reward function constraints;reinforcement learning settings;upper confidence reinforcement;reinforcement learning;reward satisfying constraints;sub linear regret;confidence reinforcement;constrained upper confidence;learning settings reward;constraints learning;constrained markov;markov decision processes;satisfying constraints learning;stochastic decision;stochastic decision problems;linear regret;linear regret frac;markov decision;cost constraints;auxiliary cost constraints;constraints described cost;priori transition kernel;cost constraints present;class stochastic decision;reinforcement;learning probability;settings reward function", "pdf_keywords": "uncertainty reward constraints;constrained markov decision;known reward constraint;reward constraint costs;reward constraints;reward constraint;reward constraints model;reinforcement learning ucrl;reward optimal stochastic;safe reinforcement learning;con\ufb01dence reinforcement learning;constraint satisfaction learning;ucrl guarantees constraint;learning reward optimal;guarantees constraint;learn optimal policy;constrained markov;abstract constrained markov;optimal stochastic policy;guarantees constraint satisfaction;constraint costs;auxiliary cost constraints;reward optimal;constraints abstracted costs;constraint costs unknown;cost constraints de\ufb01ne;learn optimal;cost constraints;known uncertainty reward;algorithm learning reward"}, "b990517fbbf4499861d7aa00407b0422874ab990": {"ta_keywords": "computer assisted interpreting;interpreters accurate translation;untranslated interpreter;predicting untranslated terminology;assisted interpreting;interpreter struggle translating;interpreters accurate;likely untranslated interpreter;improve interpreter;interpretation translation speech;challenges faced interpreters;interpreter reduce translation;interpretation predicting untranslated;improve interpreter performance;translation speech language;assisted interpreting cai;indicate interpreter struggle;interpreter struggle;untranslated interpreter reduce;interpreters;simultaneous interpretation translation;faced interpreters accurate;interpreter;tools analyze spoken;indicate interpreter;faced interpreters;translation speech;translation difficult terminology;interpreter performance;analyze spoken word", "pdf_keywords": "provide terminology streaming;untranslated terminology prediction;terminology streaming;terminology streaming asr;talk corpus;predict untranslated terminology;annotations untranslated terminology;task predicting terminology;predicting terminology;terminology prediction;terminology simultaneous interpreters;terminology prediction si;predicting terminology simultaneous;capture words;ted talk corpus;terminology tagger embedded;capture words likely;computer assisted interpreting;annotations untranslated;information source speech;assisted interpreting interface;manual annotations untranslated;proposed terminology tagger;terminology tagger;source speech text;designed capture words;terminology tagger display;terminology si supervised;svm based tagging;interpreter computer untranslated"}, "fb38451ff87254ac1ff15e79154ef958b4efb6a6": {"ta_keywords": "neural networks nlp;networks nlp;neural net models;nlp recurrent networks;formalism practical neural;neural net;recurrent networks sequence;practical neural networks;deep learning neural;complex neural net;ground nlp applications;nlp theory code;transition based parsing;sequential structured prediction;sequence tagging prediction;parsing sequential structured;structured networks;neural networks turn;deep learning;practical neural;learning neural networks;networks nlp theory;networks sequence tagging;nlp recurrent;recurrent networks;neural networks;learning neural;algorithm states cnn;based parsing sequential;nlp applications", "pdf_keywords": ""}, "e2d770b9ab691753a7ec1eb439185303118c8455": {"ta_keywords": "multilingual ai agent;multilingual ai;multilingual artificial intelligence;build multilingual ai;multilingual artificial;ai agent assistants;presents multilingual artificial;ai agent;intelligence agent assistant;assistants eliminating language;build multilingual;agent assistant;multilingual;artificial intelligence agent;technologies build multilingual;agent assistants;agent assistant maia;intelligence agent;presents multilingual;paper presents multilingual;real time language;language human;ai;assistant;agent;agent assistants eliminating;translation layer;language processing technologies;language human quality;agents provide customer", "pdf_keywords": ""}, "c8f78575bfb642b2dab6ed542a683ade9527c17d": {"ta_keywords": "fairness stability;fairness stability strategyproofness;types fairness patients;algorithmic decisions propose;strategyproofness matching donations;algorithmic decisions;fairness patients regions;fairness patients;efficiency fairness;efficient stable fairer;efficiency fairness stability;mechanism perform matching;different types fairness;fairer compared mechanism;strategyproofness matching;strategyproofness algorithmic decisions;algorithms given responsibility;types fairness;properties efficiency fairness;strategyproofness algorithmic;stability strategyproofness matching;kidney transplants;decisions propose simple;organ tissue authority;matching donations;matching;compared mechanism currently;mechanism currently consideration;tissue authority australia;online mechanisms", "pdf_keywords": ""}, "4cc07c367e4a1f932e159678ef711e1802edf49f": {"ta_keywords": "test speech processing;evaluation decomposable tasks;test speech;speakers test speech;utterances assessing;end evaluation decomposable;dataset decomposable task;utterances assessing natural;language understanding abilities;spoken language understanding;decomposable tasks;decomposable task;evaluation decomposable;assessing natural language;speech processing;differences sub tasks;decomposable task method;performance benchmarks unobserved;study spoken language;benchmarks unobserved performance;held utterances assessing;decomposable tasks case;end end evaluation;speech processing skills;utterances;benchmarks unobserved;task individually assess;similar performance benchmarks;sub tasks;benchmarks", "pdf_keywords": "spoken intent prediction;models spoken intent;subtasks spoken intent;prediction using spoken;intent prediction;intent prediction offering;intent prediction using;challenges model speech;using spoken intent;spoken intent;datasets task challenges;intent prediction case;model speech;speech recognition semantic;model speech recognition;subtasks spoken;nlu subtasks spoken;speech recognition;asr nlu subtasks;phrasings similar intents;using spoken;intents;splits decomposable tasks;model decomposable tasks;models spoken;subtasks;nlu subtasks;speech;21 datasets task;decomposable tasks"}, "8fa6b06cb96e5ae98dfff1c50f6940ef43af223f": {"ta_keywords": "erasure coded storage;coded storage;storage overheads codes;coded storage reduces;storage fault tolerance;erasure coded data;data reconstruction erasure;reconstruction erasure coded;io erasure codes;erasure codes;new erasure coded;data centers hitchhiker;data additional storage;coded data centers;erasure coded;lower storage overheads;storage;methods lower storage;data replication;storage reduces network;disk io erasure;storage overheads;lower storage;bandwidth disk io;disk io;storage reduces;disk io reconstruction;erasure codes reed;data replication methods;traffic disk io", "pdf_keywords": ""}, "1941f5b053ccc80fa44980d38ac074145591b4ec": {"ta_keywords": "sentence embedding bilingual;embedding bilingual data;vectors bilingual generative;sentence embeddings monolingual;embeddings monolingual data;embedding bilingual;predict sentence embeddings;latent vectors bilingual;sentence embedding models;language sentences vectors;semantic sentence embedding;embeddings monolingual;bilingual generative;sentence embeddings;sentence embedding;sentences vectors closeness;vectors bilingual;sentences vectors;latent semantic vector;bilingual data;learning embeddings;bilingual generative transformer;shared sentences translation;monolingual data;parallel sentences isolating;latent semantic;encode natural language;generative transformer semantic;common latent semantic;semantic divergent", "pdf_keywords": "embeddings monolingual data;sentence embeddings monolingual;bilingual generative;predict sentence embeddings;sentence embedding models;latent semantic embedding;language sentences vectors;encode monolingual sentences;semantic sentence embedding;embeddings monolingual;bilingual generative transformer;latent semantic vector;generative transformer semantic;latent vector linguistic;semantic latent vector;sentence embeddings;english inference net;semantic sentence encoding;sentence embedding;latent semantic;semantic embedding;semantic latent;latent vector english;sentences vectors;common semantic latent;transformer semantic sentence;bilingual data;common latent semantic;vector linguistic variation;encode monolingual"}, "553028f7f7c850371379c621e40d7d00e75303a6": {"ta_keywords": "multilingual models trained;interference multilingual models;improving multilingual representations;negative interference multilingual;multilingual models;multilingual models findings;modern multilingual models;improving multilingual;languages negative interference;multilingual models potential;exist multilingual models;multilingual representations;directions improving multilingual;interference multilingual;cross lingual transferability;lingual transferability;better cross lingual;lingual transferability alleviates;multilingual;resource languages negative;languages negative;high resource languages;cross lingual;parameters exist multilingual;text multiple languages;interference adding language;low resource languages;language specific layers;shared learn language;multiple languages", "pdf_keywords": "monolingual model trained;monolingual bilingual models;multilingual models monolingual;multilingual models suggesting;multilingual models;bilingual models;bilingual models evaluate;improving multilingual representations;pretraining language models;cross lingual tasks;compare multilingual models;models monolingual;multilingual representations;exist multilingual models;monolingual model;improving multilingual;models monolingual model;monolingual bilingual;multilingual representations introduction;lingual tasks analyze;language tasks;lingual tasks;cross lingual;language models;multilingual;languages substantially impacts;directions improving multilingual;advances pretraining language;pretraining language;pretrain set monolingual"}, "1606dc1e966ad59dd96dc8e74722dca06b1f1a58": {"ta_keywords": "evolutionary causal;using evolutionary causal;evolutionary causal matrices;interventions using evolutionary;educational neuroscience;based educational neuroscience;educational interventions;outcomes educational interventions;educational interventions using;causal matrices markov;causal;neuroscience;outcomes educational;evolutionary;term outcomes educational;causal matrices;long term outcomes;using evolutionary;interventions;predicting long term;chain based educational;interventions using;educational;markov chain based;term outcomes;markov chain;outcomes;based educational;markov;predicting long", "pdf_keywords": "different frequencies plot;frequencies plot;outcomes ylabel ratio;xlabel frequency interventions;ylabel mean ratio;intervention outcomes ylabel;frequencies plot amt;outcomes ylabel;ylabel ratio students;different frequencies figure;outcomes xlabel time;frequency interventions;prepare plots 50;conducted months subplot;mean participation ratio;subplot trends interventions;figure different mean;xticklabel months 37;frequencies figure;ylabel ratio;prepare plots;mean ratio students;trajectories participation ratio;plots 50 figure;xticklabel months;participation ratio throught;gca xticklabel months;different mean participation;rate time intervention;tsc prepare plots"}, "e3480d9395e692833b722b2e957d51139985f310": {"ta_keywords": "generative question answering;question answering;question answering qa;macaw versatile generative;question classes;300 challenge questions;pretrained language models;answering qa making;identify question classes;question types produces;question types;challenge questions despite;versatile generative;answering qa;absolute challenge300 suite;challenge questions;produce question answer;language models high;challenge300 suite;question answer question;versatile generative question;macaw versatile;answers outside training;suite 300 challenge;example macaw;present macaw versatile;300 challenge;example macaw question;question classes appears;language models", "pdf_keywords": "macaw proves useful;macaw allows different;example macaw question;macaw including;billion parameters macaw;example macaw;macaw allows;parameters macaw exhibits;macaw question produce;macaw question;macaw built;macaw;macaw exhibits strong;macaw including comparing;addition macaw allows;used example macaw;macaw built uni\ufb01edqa;parameters macaw;options hope macaw;addition macaw;300 challenge questions;macaw proves;hope macaw proves;experiments macaw;experiments macaw including;macaw exhibits;absolute challenge300 suite;challenge questions designed;hope macaw;parameters addition macaw"}, "6d19d73909ffaa6c94cae6a2535ed52d138cb63b": {"ta_keywords": "chat oriented dialog;build dialog corpus;build conversational agent;dialog corpus propose;dialog corpus;building chat oriented;scripts twitter conversations;dialog management;developing chat oriented;domain chat oriented;constructing appropriate dialog;oriented dialog management;appropriate dialog corpora;approaches building chat;build conversational;dialog corpora;dialog utilizing;twitter conversations experimental;method build conversational;building chat;conversation turn extraction;dialog corpora raw;conversational agent interact;movie scripts twitter;human conversation examples;human human conversation;human conversation;twitter conversations;developing chat;conversational agent", "pdf_keywords": ""}, "d1678032a9eee94ec0a9c54fb008e1addc7213d4": {"ta_keywords": "robust utility learning;utility learning framework;utility learning;utility learning method;estimated utility functions;cfgls utility learning;game robust utility;defined estimated utility;energy efficiency social;estimated utility;social game robust;efficiency social game;robust utility;building energy efficiency;utility functions performance;encourage energy efficient;utility functions;smart building energy;occupants using heteroskedasticity;energy efficient behavior;feasible generalized squares;heteroskedasticity inference adapt;energy efficiency;learning method estimator;game robust;game defined estimated;building energy;occupant voting data;data social game;using heteroskedasticity inference", "pdf_keywords": ""}, "33cd5965745dc2e8bb8d0400d0b3c18d4e6369d4": {"ta_keywords": "cache clusters twitter;caching extensively;effectiveness memory caching;memory caching systems;memory caching extensively;cache workloads collecting;cache clusters;caching systems;cache workloads;memory caching;caches;memory cache clusters;production caches;caching extensively increase;real world cache;world cache workloads;use memory caching;caching;hundreds memory cache;strategy production caches;cache;production caches surprising;memory cache;153 memory cache;caches surprising;world cache;caches surprising example;large number workloads;clusters twitter sifting;workloads fine grained", "pdf_keywords": ""}, "0b79cb7fe16aa8b99d521989f39e49034394f701": {"ta_keywords": "human computation research;human computation;defining human computation;human computation new;human computation suggesting;future human computation;human computation systems;questions human computation;game crowdsourcing;game crowdsourcing marketplaces;users perform computation;crowdsourcing;verification tasks;enjoyable game crowdsourcing;crowdsourcing marketplaces;verification tasks users;identity verification tasks;ai;human intelligence;mechanicalturk human computation;captchas;ai machine;games purpose esp;users solve captchas;including ai machine;intelligence ai;games purpose;harnessing human intelligence;online gamers generate;human intelligence solve", "pdf_keywords": ""}, "61cd4ffdaf2c0daa3d432ff9fecdd064d6e72886": {"ta_keywords": "fol reasoning commonsense;reasoning commonsense inference;logic fol reasoning;benchmark logicnli;benchmark logicnli logicnli;lms fol reasoning;language models lms;proposed benchmark logicnli;logicnli logicnli nli;reasoning commonsense;logic fol;logicnli nli;logicnli nli style;reasoning ability recently;logicnli logicnli;fol reasoning;fol reasoning motivates;reasoning ability;commonsense inference;fol reasoning new;language models;logicnli;enhance reasoning;target fol reasoning;enhance reasoning ability;lms fol;commonsense inference used;weaknesses lms fol;robustness generalization interpretability;order logic fol", "pdf_keywords": "natural language reasoning;logical natural language;language inference logicnli;natural language inference;order logical reasoning;logic fol reasoning;fol reasoning commonsense;benchmark named logical;order logical fol;order logic fol;reasoning commonsense inference;diagnosing order logical;logical fol reasoning;reasoning ability logicnli;language reasoning fols;language reasoning;fol reasoning ability;logic fol;order logical;inference logicnli;interpretable fol reasoning;\ufb01rst order logic;order logic;benchmark logicnli;proposed benchmark logicnli;language inference;logical fol;benchmark logicnli based;logicnli diagnosing order;abstract order logical"}, "a829d65de0cc19da49ad6b4a294dd31545aed2bb": {"ta_keywords": "jstor org;support jstor org;information jstor;jstor;support jstor;jstor org american;jstor profit service;jstor contact;information jstor contact;collaborating jstor;jstor profit;scholarship jstor;jstor digitize;jstor contact support;scholarship jstor profit;archive information jstor;contact support jstor;forms scholarship jstor;collaborating jstor digitize;association collaborating jstor;jstor digitize preserve;access journal business;org american statistical;journal business;journal business economic;access journal;helps scholars researchers;digital archive information;digital archive;journal", "pdf_keywords": ""}, "bc5e4b9fb3a40057df4994354a403202218d53a6": {"ta_keywords": "declarative program search;transformed program heuristic;semantics preserving transformations;program transformations;program heuristic search;described nlp;program search;program empirically speed;sequence semantics preserving;semantics preserving;declarative program;automated search;programmers substantial improvements;probabilistic parser;program transformations simple;program search sequence;practice automated search;automated search like;search sequence semantics;set program transformations;program heuristic;probabilistic parser marginalize;described nlp literature;program empirically;instance probabilistic parser;employ dynamic programming;transformed program;correct declarative program;human programmers substantial;automated", "pdf_keywords": "transformed program heuristic;program transformations;declarative program search;program transformations simple;program heuristic search;declarative program;set program transformations;transformed program;program heuristic;program search sequence;program search;asymptotically faster algorithms;automatically discover asymptotically;semantics preserving transformations;dynamic programming;optimal asymptotic runtime;asymptotic runtime;carlo tree search;faster algorithms;correct declarative program;probabilistic parser;sequence semantics preserving;employ dynamic programming;asymptotic runtime unintuitive;discover asymptotically faster;dynamic programming unique;probabilistic parser marginalize;ef\ufb01ciency transformed program;instance probabilistic parser;runtime unintuitive"}, "ab17c315f7ee4fe69fde2f3d8ae0e30e4e2f3a2b": {"ta_keywords": "structured documents answer;documents answer complex;hop question answering;questions dochopper achieves;long document answer;answering qa systems;long complex documents;document answer question;question answering;navigating long document;question answering qa;structured documents;dochopper iteratively;representation hierarchical document;heirarchically structured documents;dochopper iteratively attends;long document;document answer;complex documents answer;qa systems dochopper;complex documents;retrieve short passages;hierarchical document;hierarchical document potentially;dochopper uses query;dochopper achieves;documents answer multi;questions dochopper;documents answer;document combines retrieved", "pdf_keywords": "attention answering complex;modeling similarly retrieval;query embeddings discuss;attention answering;hierarchical attention answering;query embeddings;iterative hierarchical attention;pi query embeddings;hierarchical attention;computing query vectors;documents answer complex;query vectors;query vector model;similarly retrieval;structured documents answer;retrieval;answering complex questions;questions long documents;heirarchically structured documents;retrieval multi hop;similarly retrieval multi;query vectors types;structured documents;retrieval multi;embeddings discuss strategy;navigation modeling similarly;attention;answer complex questions;iterative hierarchical;query present iterative"}, "e54ffc76d805c48660bb0fd20019ca82ac94ba0d": {"ta_keywords": "pretrained language models;tuning pretrained language;empirically pre training;millions parameters datasets;intrinsic dimensionality low;pre training implicitly;task representations compression;dimension based generalization;training implicitly minimizes;minimizes intrinsic dimension;pretrained language;trainable parameters;intrinsic dimension surprisingly;hundreds millions parameters;dimensional task representations;provide intrinsic dimension;representations compression;compression based generalization;strong regularization tune;training implicitly;low dimensional task;intrinsic dimension based;intrinsic dimensionality explains;lower intrinsic dimension;intrinsic dimensionality;language models;dimensionality low;fine tuning pretrained;generalization bounds provide;millions parameters", "pdf_keywords": "length natural language;optimizes description length;task representations compression;minimal description length;description length framework;natural language tasks;pre training implicitly;interpretation pre training;generalization bounds large;provide generalization bounds;dimension based generalization;generalization bounds provide;compression based generalization;nlp tasks parameters;pre trained models;intrinsic dimensionality low;large set nlp;implicitly optimizes description;dimensional task representations;generalization bounds;training implicitly optimizes;interpretation intrinsic dimension;provide intrinsic dimension;description length natural;length average nlp;average description length;based generalization bounds;intrinsic dimension later;minimizes intrinsic dimension;description length finally"}, "2a82a16bdb793dc388391be57d6424f0d5090513": {"ta_keywords": "review incorporates rankings;integrate ranking information;ensuring rankings;ranking information;ranking information scores;using ranking information;challenges ensuring rankings;datasets peer reviews;ensuring rankings incorporated;incorporates rankings;rankings incorporated updates;approach integrate ranking;ranking;ranking information area;using ranking;rankings;score pertaining review;integrate ranking;incorporates rankings second;peer review;arbitrariness peer review;rankings second suitable;peer reviews;peer reviews iclr;procedure using ranking;updated score pertaining;rankings second;updates scores manner;scores manner papers;peer review process", "pdf_keywords": "quantized scores rankings;rankings quantized scores;integrating rankings quantized;rankings given reviewers;rankings quantized;scores rankings;combining rankings quantized;scores peer review;quantized scores peer;scores rankings given;rankings;baselines quantized scores;quantized scores;scores review aggregating;aggregating quantized scores;reviewers estimate score;integrating rankings;scores review;quantized scores bre;scores papers empirically;score paper compare;datasets peer reviews;speci\ufb01cally dequantization scores;scores peer;rankings given;dequantization scores review;combining rankings;dequantization scores;quantized scores focusing;review aggregating quantized"}, "188fd1373aefdbf564e90a76fed43e1b8b7052dc": {"ta_keywords": "ntt communications science;ntt communications;report ntt communications;communications science laboratories;communications science;ntt;laboratories open house;science laboratories;report ntt;science laboratories open;communications;laboratories open;laboratories;open house 2011;house 2011;science;open house;2011;house;report;open", "pdf_keywords": ""}, "69e2d1f5374918111432fae23212c2759b1357c2": {"ta_keywords": "ranking pairwise comparisons;sequential ranking;analyze sequential ranking;pairwise ranking;pairwise ranking based;sequential ranking algorithm;ranking pairwise;sequential active ranking;active ranking pairwise;recovering ranking;ranking algorithm counts;recovering ranking using;ranking algorithm;succeeds recovering ranking;active ranking;noisy pairwise comparisons;ranking based parametric;ranking;ranking set items;work pairwise ranking;ranking based;active ranking set;ranking set;ranking using;pairwise comparisons;ranking using number;comparisons optimal;pairwise comparisons second;problem active ranking;number comparisons optimal", "pdf_keywords": "analyze sequential ranking;sequential ranking;sequential ranking algorithm;ranking items btl;ranking sample complexity;recovering ranking sample;\ufb01nding ranking items;recovering ranking;\ufb01nding ranking;ranking algorithm counts;ranking items;recovering ranking using;ranking algorithm;ranking sample;pairwise comparison model;succeeds recovering ranking;ranking;comparisons optimal;problem \ufb01nding ranking;btl pairwise comparison;26 recovering ranking;ranking using;pairwise comparison probabilities;number comparisons optimal;ranking using number;comparison model provides;pairwise comparison;counts number comparisons;comparison model;probabilistic model btl"}, "926d827aef568ed97431a7845c9a8138930c80fd": {"ta_keywords": "reactions social sharing;responses political information;social affective responses;affective reactions social;affective responses political;sources social affective;political information hypothesize;reactions social;social sharing behavior;political information;group sources social;affective responses;social sharing;responses political;sharing behavior;information group sources;exploring social affective;social affective;sharing behavior subjects;link affective reactions;information hypothesize people;information outgroup sources;messages group sources;people treat messages;sources social;sources differently information;link affective;information outgroup;differently information outgroup;treat information group", "pdf_keywords": ""}, "9b534639bcadc9ad232b338e760c523a4d74c8de": {"ta_keywords": "linguistics evaluate descriptions;grammar descriptions consumption;descriptions language;grammar descriptions;descriptions language terms;creating descriptions language;summarized grammar descriptions;descriptions help language;automated evaluation;evaluate descriptions;automatic framework linguistic;linguistic exploration;linguistics evaluate;descriptions consumption linguists;evaluation infeasible autolex;automated evaluation human;help language experts;language learners manual;evaluate descriptions help;creating descriptions;language experts propose;framework linguistic exploration;understanding language;manual creation descriptions;language experts;language terms;method automated evaluation;learners manual creation;linguistic;deep understanding language", "pdf_keywords": "automatic syntactic;automatic syntactic analysis;linguistic exploration description;syntactic analysis extract;text syntactic analysis;grammar descriptions consumption;syntactic analysis;syntactic analysis using;descriptions linguistic;morphology phrase construction;syntactic analysis experts;automatic framework linguistic;grammar descriptions;treebanks automatic syntactic;linguistic exploration;semantics autolex;extraction concise descriptions;semantics autolex automatic;concise descriptions linguistic;descriptions linguistic phenomena;aid linguistic exploration;summarized grammar descriptions;meaning semantics autolex;raw text syntactic;text syntactic;learners treebanks automatic;languages treebanks automatic;descriptions help language;phrase construction syntax;experts evaluate descriptions"}, "987c5ad75d5092bed03e9f523aec00dc43bc17e4": {"ta_keywords": "urban air quality;traffic urban air;air quality correlation;road network density;impacts road traffic;road traffic characteristic;road area occupancy;analyzes impacts road;aod road parameters;impact air quality;traffic characteristic;air quality jinan;area occupancy intersection;road parameters correlation;parameters urban air;air quality quantitatively;correlation aod road;air quality;areas traffic;traffic characteristic parameters;especially areas traffic;road traffic urban;road traffic;traffic urban;network density road;bus emission reduction;congestion delay index;aod road area;air quality policy;impacts road", "pdf_keywords": ""}, "89c2b3bfcc309ce16c85d2ab0c8cac5295400715": {"ta_keywords": "sequential learning stacked;stacked sequential learning;learning stacked sequential;sequential learning meta;learners sequential stacking;learning stacked;base learners sequential;sequential learning;learning meta learning;meta learning algorithm;meta learning;sequential learning scheme;new sequential learning;learners sequential;maximum entropy learner;entropy learner generally;entropy learner;sequentially stacked;base learner;sequential stacking improves;stacked sequential;arbitrary base learner;learning scheme called;tasks sequentially stacked;learning meta;learning scheme;learning algorithm arbitrary;learners crfs;learners crfs designed;learning", "pdf_keywords": ""}, "9b263129548dc09369e8bc34560fe5bb6047fcee": {"ta_keywords": "simulator greek electricity;simulating market dynamics;simulating market;capable simulating market;greek electricity market;electricity market greek;developing simulator greek;transportation developing simulator;simulator wholesale market;simulator greek;electricity market intended;scheduling based;electricity market;scheduling;implementation simulator wholesale;day ahead scheduling;ahead scheduling;use simulator;ahead scheduling based;simulator;developing simulator;scheduling based bids;implementation simulator;simulator wholesale;schedule deviations;market operations dynamics;design implementation simulator;simulator consists;use simulator elaborate;intended use simulator", "pdf_keywords": ""}, "9333d372ad3887e02029d2eab0dbc0c0478582c7": {"ta_keywords": "unsupervised speech tagging;evaluation unsupervised learning;speech tagging;evaluating unsupervised learning;using unsupervised speech;speech tagging case;unsupervised learning;unsupervised speech;unsupervised learning methods;evaluation unsupervised;unsupervised learning natural;learning natural language;methods evaluating unsupervised;evaluating unsupervised;unsuitable evaluation unsupervised;tagging;natural language processing;using unsupervised;development unsupervised learning;tagging case study;context evaluation;annotated data learn;methods natural language;unsupervised;used context evaluation;context evaluation appropriate;natural language;learning methods evaluating;language processing;tagging case", "pdf_keywords": ""}, "35b376ad9e03e5e0b930c53a48817bfb5703108d": {"ta_keywords": "machine translation models;neural machine translation;semantic similarity metrics;machine translation;leverage semantic similarity;translation models;text style transfer;translation models explicitly;semantic similarity;style transfer task;supervised models text;similarity metrics;parallel corpora;parallel corpora makes;lack parallel corpora;corpora;automatic metrics propose;style transfer;models text style;existing automatic metrics;metrics training;similarity metrics originally;evaluation strong baselines;automatic metrics;corpora makes impossible;models text;using metrics training;automatic human evaluation;corpora makes;tuning neural machine", "pdf_keywords": "metric content reward;neural machine translation;machine translation models;leverage semantic similarity;content reward empirically;existing automatic evaluation;automatic evaluation metrics;machine translation;text style transfer;translation models;semantic similarity metrics;semantic similarity metric;transfer leveraging semantic;automatic evaluation;reward content preservation;automatic human evaluation;text generated;evaluation strong baselines;leveraging semantic similarity;preservation text style;reward content;text generated text;style transfer leveraging;generated text;translation models explicitly;similarity metric content;content preservation text;content reward;ef\ufb01cient reward content;improving content"}, "c96970cfb1c13ae6dccc30de482ce6b0d4414f2b": {"ta_keywords": "predicate invention;predicate invention based;predicate invention instead;creating new predicates;invented predicate subroutine;new predicates introduced;version predicate invention;soft version predicate;common invented predicate;invented predicate;new predicates;predicates introduced;new predicates implicitly;predicates;predicate subroutine;predicate invention pi;predicates implicitly;predicates introduced logical;predicate;predicate subroutine suggest;pi new predicates;predicates implicitly group;version predicate;rules using structured;baselines structure learning;using structured;strong baselines structure;structure learning large;invention based structured;structured", "pdf_keywords": ""}, "4fee3d5d476568deb971768f8a5191eb627309d0": {"ta_keywords": "stability differential nash;game dynamics linearized;differential nash equilibria;game jacobian symmetric;nash equilibria robustness;game dynamics;players different learning;decomposing game jacobian;game jacobian;differential nash;player continuous games;nash equilibria;optimization landscape games;continuous games general;continuous games;components game converge;linearized local equilibria;equilibria robustness variation;games continuous games;spectrum game dynamics;equilibria robustness;game converge faster;gradient based dynamics;continuous games continuous;equilibria analyze stability;games continuous;game converge;stability equilibrium;rotational components game;agent learning rates", "pdf_keywords": "stability differential nash;nash stable robust;stable points nash;game jacobian symmetric;differential nash equilibria;learning dynamics;equilibria variations learning;games nash stable;robust potential games;nash stable;game dynamics;differential nash;games stable points;nash numerical example;stability gradientbased dynamics;decomposing game jacobian;game jacobian;nash numerical;game dynamics prove;optimization landscape games;nash equilibria;continuous games results;robustness equilibria variations;nash equilibria introduce;stability gradientbased;points nash numerical;continuous games;gradientbased dynamics;player continuous games;robustness equilibria"}, "5df0b8b80aecda1efdebac5d1ab7bcf94a88c68f": {"ta_keywords": "downstream nlp tasks;biomedical embeddings language;embeddings language models;trained contextualized embeddings;improvements downstream nlp;contextualized word embeddings;embeddings contextualized word;probing biomedical embeddings;contextualized embeddings;embeddings contextualized;word embeddings;biomedical embeddings;contextualized embeddings contextualized;downstream nlp;trained language models;word embeddings derived;embeddings language;domain trained contextualized;nlp tasks;trained contextualized;nlp tasks 2018;embeddings;language models lms;language models;embeddings derived pre;pubmed abstracts probing;sequence modeling layers;downstream task models;embeddings derived;domain trained", "pdf_keywords": "biomedical contextual embeddings;bioelmo pubmed corpus;biomedical citations3 train;biomedical contextual;contextual embeddings bioelmo;pubmed corpus;24m biomedical citations3;10m pubmed abstracts;trained biomedical corpus;biomedical corpus;pubmed abstracts;bioelmo2 compared embeddings;biomedical citations3;embeddings bioelmo;noncontextual biomedical;pubmed corpus used;tokens pubmed;biomedical corpus 5b;domain corpus;compare noncontextual biomedical;domain corpus 3b;general domain corpus;domain corpus 5b;methods biomedical contextual;biomedical ner nli;pubmed abstracts called;tokens pubmed train;tokens9 biobert10 conclusion;nli bioelmo works;embeddings bioelmo train"}, "5930efbf01efa8944258b1c0f7349111702f779e": {"ta_keywords": "nlp tools;corpora nlp community;corpora annotations data;development nlp applications;processing nlp;corpora annotations;nlp tools augment;corpora nlp;annotation modules integrated;reading corpora annotations;nlp applications;processing nlp requires;language processing nlp;nlp community module;development nlp;popular corpora nlp;corpus reader module;challenges provide corpus;implementing natural language;nlp applications providing;annotators python;annotation modules;natural language processing;nlp community;shelf nlp tools;easily use annotators;annotations data structures;corpus reader;annotators python interface;corpus", "pdf_keywords": ""}, "44268b5a208e8f48a5883bb12e3e80a13101e752": {"ta_keywords": "heart failure hf;contrast induced acute;congestive heart failure;factor contrast induced;serum baseline creatinine;acute kidney injury;factor contrast;kidney injury ci;creatinine level;kidney injury according;severity heart failure;kidney injury;baseline creatinine level;failure incidence contrast;severity hf ci;hf decreased levels;patients hf decreased;induced acute kidney;risk factor contrast;contrast agent;contrast induced;classification severity heart;acute kidney;heart failure incidence;administration contrast agent;decreased levels ef;classification severity hf;severity hf;heart failure;\u03bcmoi serum baseline", "pdf_keywords": ""}, "96ed7a7da69d654668b35b50344debd44e87c1a1": {"ta_keywords": "topic identification speech;contextual topic identification;topic id spoken;language topic classification;topic identification;id spoken segments;classification topic identification;topic identification topic;topic classification;identification topic id;spoken segments;topic classification topic;topic id;topic id real;topic shifts;contextual topic;variable topic shifts;identification speech;identification speech instead;world unstructured audio;speech instead classifying;resource contextual topic;unstructured audio;unstructured audio audio;spoken segments low;method topic id;acoustic modeling translation;topic shifts broken;classification topic;identification topic", "pdf_keywords": "topic id spoken;acoustic modeling recurrent;language topic classi\ufb01cation;contextual models;contextual modeling;modeling recurrent neural;topic identi\ufb01cation universal;id spoken segments;models contextual modeling;attention based contextual;models training languages;contextual modeling using;modeling using rnn;topic identi\ufb01cation;contextual models lorelei;topic id performance;modeling recurrent;contextual model;languages emergent incidents;recurrent neural networks;models contextual;modeling translation lexicons;spoken segments;language universal acoustic;languages attention based;non contextual models;context independent models;recurrent neural network;topic id;acoustic modeling translation"}, "b9c3e87bc09c4c6167a03a835c30b1c23bef7a40": {"ta_keywords": "generalization question answering;embeddings like bert;question answering knowledge;answering knowledge bases;trained contextual embeddings;bert generalization kbqa;like bert generalization;bert based kbqa;question answering;contextual embeddings;bert generalization;contextual embeddings like;pre trained contextual;trained contextual;answering knowledge;novel bert based;bert based;knowledge bases;generalization kbqa;propose novel bert;generalization kbqa achievable;embeddings;like bert;embeddings like;contextual;novel bert;models stronger generalization;stronger generalization;bert;generalization levels generalization", "pdf_keywords": "contextual embeddings generalization;trained contextual embeddings;contextual embeddings;embeddings generalization generalization;generalizable question answering;contextual embeddings like;embeddings like bert;embeddings generalization;pre trained contextual;trained contextual;question answering supports;question answering;bert compositional;like bert compositional;bert based kbqa;stronger generalization levels;compositional generalization novel;models stronger generalization;generalization unseen schema;contextual;shot generalization kbqa;stronger generalization;compositional generalization;bert compositional zero;shot generalization unseen;answering supports evaluation;embeddings;generalization unseen;embeddings like;generalization novel compositions"}, "05b6be9aec266072669f6f287a846637eedf19b5": {"ta_keywords": "soil nematode communities;warming canadensis invasion;canadensis invasion soil;canadensis invasion warming;invasion soil nematode;soil nematode community;invasion effect soil;warming canadensis;effect soil nematode;interaction warming canadensis;nematode communities measured;global warming canadensis;warming enhanced canadensis;canadensis invasion effect;canadensis invasion significantly;invasion soil;soil nematode;soil microbial community;enhanced canadensis invasion;alter soil nematode;solidago canadensis invasion;stoichiometry microbes soil;microbes soil findings;plant invasion affects;canadensis invasion directly;canadensis invasion;microbes soil;nematode communities;invasion warming enhanced;soil microbial", "pdf_keywords": ""}, "449310e3538b08b43227d660227dfd2875c3c3c1": {"ta_keywords": "black box differential;deep neural;derivative hidden state;normalizing flows generative;backpropagate ode solver;deep neural network;neural network models;generative model train;scalably backpropagate ode;derivative hidden;sequence hidden layers;normalizing flows;neural network;parameterize derivative hidden;flows generative model;layers parameterize derivative;continuous normalizing flows;neural network introduce;solver continuous depth;hidden layers parameterize;hidden layers;training scalably backpropagate;neural;flows generative;state using neural;differential equation solver;using neural network;backpropagate ode;using neural;family deep neural", "pdf_keywords": "latent ode model;recurrent neural network;neural network models;deep neural;black box ode;latent ode experiments;residual networks;box ode solvers;experiments recurrent neural;ode solvers model;ode solvers;depth residual networks;deep learning generative;deep learning;recurrent neural;ode experiments recurrent;learning generative modeling;ode solvers automatic;neural networks;series latent ode;ability latent ode;deep neural network;solvers automatic differentiation;time series latent;latent ode;models time series;neural ordinary differential;neural network;learning density;learning density estimation"}, "ab1e5a3c5521b6204dc7c6f1fa72b88000bc30ee": {"ta_keywords": "answering qa systems;question answering qa;precede answering engine;answering engine;answering engine introduce;question answering;centric question answering;answering qa;trained qa models;pipeline precede answering;pre trained qa;trained qa;precede answering;assistants typing questions;engine translating questions;identifying correct answers;questions search engine;answers passages;qa systems;answering;qa models;translating questions languages;qa;questions search;voice assistants typing;question components pipeline;languages supported qa;question components;speaking voice assistants;correct answers passages", "pdf_keywords": "qa engine speech;interfaces speech recognizers;engine speech recognizers;interfaces speech;google text speech;converting spoken queries;speech text optimized;spoken queries;spoken queries text;interfaces translation engines;google speech;google speech text;optimized english speech;speech recognizer;speech recognizers converting;engine speech;kinds interfaces speech;speech recognizers;queries translation systems;question answering;translation engines;text speech;english speech recognizer;keyboard interfaces translation;speech recognizers keyboard;recognizers converting spoken;accuracy models interfaces;settings google speech;speech text;questionanswering benchmark"}, "5f609f252d8815c5fb660d83c0dc71af21ecf65d": {"ta_keywords": "monitoring twitter classify;twitter classify np;event monitoring twitter;keywords event monitoring;twitter classify;known keywords streaming;monitoring twitter;phrases nps keywords;keywords streaming text;keywords streaming;automatically finding noun;finding noun phrases;noun phrases nps;nps keywords event;counting known keywords;keywords event;known keywords;nps keywords;phrases nps;noun phrases;event monitoring;keywords;activities event monitoring;classify nps using;twitter;social activities event;finding noun;automatically finding;words represent np;event monitoring systems", "pdf_keywords": ""}, "33c691ca050e1806d44c08e55e63fcd7e555899a": {"ta_keywords": "negative classifier;positive negative classifier;alongside classifying unlabeled;negative classifier generally;classifying unlabeled sample;classify unlabeled sample;classifying unlabeled;classification case positive;positive unlabeled data;classify unlabeled;unlabeled pu learning;distinguish positive unlabeled;positive unlabeled;classifier;unlabeled mixture;trained distinguish positive;supervised binary classification;classifying;objectives classify unlabeled;classifier trained distinguish;classification;alongside classifying;binary classification;classifier trained;classify;classifier instead;considered unlabeled mixture;classification case;supervised binary;classifier generally", "pdf_keywords": ""}, "a8ea980b63deaf1404cd9f539a575b4e7135466e": {"ta_keywords": "parm prediction serving;parity model neural;models parm prediction;parm prediction;applicability parity models;parity models new;parity models parm;parity models;introduce parity models;parity;implement parity models;using parity models;parity models image;using parity;localization tasks parity;resilience erasure coding;systems using parity;applicability parity;predictions introduce parity;potential parity models;implement parity;models parm;parity models unlock;erasure coded resilience;resourceefficient resilience prediction;coded resilience prediction;parity model;systems implement parity;tasks parity;introduce parity", "pdf_keywords": ""}, "fb7caddac20dca012f48c90b2e1e2383f7185051": {"ta_keywords": "interpretability tools;existing interpretability tools;models interpretability tools;interpretability tools designed;interpretability tools machine;use interpretability tools;interpretability understanding data;interpretability tools uncover;misuse interpretability tools;models interpretability;use interpretability;interpretability;end interpretability tools;tools end interpretability;interpretability understanding;scientists use interpretability;mental models interpretability;interpretability tools interpretml;existing interpretability;interpreting interpretability;interpreting interpretability understanding;use existing interpretability;misuse interpretability;interpretability tools highlight;end interpretability;data scientists trust;trust misuse interpretability;understanding data scientists;observe use interpretability;package interpreting interpretability", "pdf_keywords": "interpretability machine;interpretability machine learning;existing interpretability tools;interpretability tools;keywords interpretability machine;interpretability tools interpretml;existing interpretability;evaluation existing interpretability;interpretability;use existing interpretability;keywords interpretability;author keywords interpretability;machine learning user;tools interpretml implementation;tools interpretml;interpretml;models shap python;shap python;introduction machine learning;learning introduction machine;interpretml implementation;machine learning ml;ml models author;interpretml implementation gams;learning ml ubiquitous;human centered computing;gams shap python;machine learning;ml ubiquitous;user centric evaluation"}, "b1d8c868e1d6d4980ee2f8c50e6fc5e4e7027ca2": {"ta_keywords": "dialogue modeling character;improves story continuation;advancing story telling;person narration dialogue;person narration;story playing role;collaboratively telling story;character driven story;story continuation accuracy;telling story playing;story emerges characters;telling stories multi;character relationships;narration dialogue;interacting characters personas;dialogue transcripts people;second person narration;story continuation;character persona relationships;user dialogue modeling;driven story continuation;character relationships improves;relationships improves story;dialogue modeling;narration dialogue requiring;narration;persona relationships characters;stories multi;modeling character relations;characters personas", "pdf_keywords": "dialogue modeling character;character driven storytelling;introduction automated storytelling;automated storytelling thought;automated storytelling;improves story continuation;driven storytelling corresponding;storytelling corresponding;character driven story;person narration;storytelling corresponding methodology;story emerges characters;person narration dialogue;collaboratively telling story;user dialogue modeling;telling stories multi;story playing role;dialogue modeling;modeling character relations;story continuation accuracy;character relationships;telling story playing;interacting characters personas;second person narration;driven story continuation;story continuation;character persona relationships;narration dialogue;driven storytelling;character relation persona"}, "e785441f5ccd6e4e29b3123e61121df5c65b88f7": {"ta_keywords": "variational autoencoder vae;variational autoencoder;aggressively optimize inference;target variational autoencoder;training inference network;training reduce inference;accompanying variational learning;training inference;reduce inference lag;neural inference;variational learning;inference network approximate;optimize inference network;autoencoder vae popular;model learns;learns ignore latent;autoencoder vae;inference network;optimize inference;stages training inference;variational learning technique;collapse model learns;inference network performing;vaes efficiently parameterize;deep latent variable;model accompanying variational;strong autoregressive baselines;neural inference network;inference lag;deep latent", "pdf_keywords": "variational autoencoders;variational autoencoder;collapse variational autoencoders;variational autoencoder vae;variational autoencoders vaes;introduction variational autoencoders;variational autoencoders junxian;abstract variational autoencoder;variational learning;accompanying variational learning;lagging inference networks;faster introduction variational;autoencoder vae popular;autoencoders;autoencoders vaes;autoencoders junxian;autoencoder vae;variational learning technique;model accompanying variational;strong autoregressive baselines;autoencoders vaes kingma;autoencoders junxian daniel;autoencoder;training reduce inference;reduce inference lag;inference networks;deep latent variable;inference networks posterior;aggressively optimize inference;introduction variational"}, "f7a2f2ae829545ee992a2214b3600cf914544e22": {"ta_keywords": "modeling intelligent tutoring;cognitive tutor;intelligent tutoring systems;intelligent tutoring;cognitive tutor authoring;tutoring systems;student modeling intelligent;block cognitive tutor;tutoring;learns cognitive;learns cognitive skills;tutor;agent learns cognitive;learning agent;tutor authoring;build cognitive model;tutor authoring tools;simstudent machine learning;cognitive skills;learning agent learns;modeling intelligent;cognitive skills demonstration;agent learns;tutoring systems current;student modeling;build cognitive;predict human students;creates cognitive model;machine learning agent;students performance simstudent", "pdf_keywords": ""}, "9112be1801598125d463febb96a525227c32acc1": {"ta_keywords": "deep neural;automatic differentiation weighted;deep neural network;learn latent decomposition;propose convolutional wfst;automatic differentiation;network propose convolutional;convolutional wfst;convolution separation graphs;handwriting recognition speech;structured loss functions;dynamically training;new structured loss;structured loss;learning algorithms learn;speech recognition;used dynamically training;recognition speech;neural;finite state transducers;interior deep neural;convolutional wfst layer;neural network;differentiation weighted finite;recognition speech recognition;replacement traditional convolution;traditional convolution;latent decomposition phrases;higher level representations;differentiation weighted", "pdf_keywords": "propose convolutional wfst;wfsts propose convolutional;convolutional wfst;convolutional wfst layer;deep neural;deep neural network;dynamically training;automatic differentiation weighted;used dynamically training;interior deep neural;network propose convolutional;state transducers wfsts;replacement traditional convolution;differentiable wfsts allows;differentiable wfsts;traditional convolution;knowledge encoded wfsts;learn training data;differentiable weighted;convolution arxiv 2010;higher level representations;representations framework differentiable;neural;automatic differentiation;neural network;wfst layer;transducers wfsts allowing;differentiation weighted;propose convolutional;transducers wfsts"}, "4cf633d0893a1d3af97723ce1f2fae33c2a30043": {"ta_keywords": "similarity relations knowledge;relations knowledge bases;relational classification;redundant relations extracted;quantifying similarity relations;relations extracted;quantify similarity relations;models relational classification;detect redundant relations;relational classification make;relations fact distribution;open information extraction;similarity relations;similarity relations fact;information extraction open;knowledge bases;similar relations;similar relations approach;relations extracted open;information extraction;relations knowledge;redundant relations;knowledge bases specifically;entity pairs quantifying;relations fact;entity pairs;distributions entity pairs;relations;pairs quantifying similarity;models relational", "pdf_keywords": "similarity relations knowledge;quantifying similarity relations;quantify similarity relations;similarity relations based;similarity relations provide;similarity relations;similarity relations fact;evaluate similarity relations;similarity pairs relations;similarity score entity;relations knowledge bases;judgment similarity relations;relations empirically;measuring similarity;quantify similarity;relations empirically outputs;measuring similarity pairs;relations fact distribution;quantifying similarity;similarity pairs;method quantify similarity;framework measuring similarity;knowledge bases;similarity;distributions entity pairs;pairs relations empirically;similarity score;similarity score correlate;human judgment similarity;judgment similarity"}, "9f1059006e4ba303f8945114eddadd50d58a9f3e": {"ta_keywords": "soft symbolic database;neural query language;query language tensorflow;tensorflow large knowledge;knowledge base query;knowledge bases;symbolic database;large knowledge bases;knowledge bases kbs;neural query;text neural query;knowledge base;accessing soft symbolic;query language knowledge;symbolic database using;soft symbolic;language knowledge base;base query language;language tensorflow;kbs useful ai;database using differentiable;accessing soft;learn instantiate query;query language;write neural models;database;conveniently write neural;tensorflow;facts soft kb;access rules learn", "pdf_keywords": "queries kb neural;neural query language;soft symbolic databases;knowledge base query;tensor\ufb02ow nql accesses;knowledge bases;knowledge bases kbs;large knowledge bases;query language tensor\ufb02ow;query language knowledge;soft symbolic database;implemented tensor\ufb02ow nql;base query language;neural query;knowledge base;text neural query;kbs useful ai;integrate queries kb;symbolic databases;symbolic databases using;tensor\ufb02ow nql;language knowledge base;query language;queries kb;convenient integrate queries;kb neural;symbolic database;learn instantiate query;nql query language;abstract large knowledge"}, "02b932416751674dc25353620a1df4b53c3a5f6f": {"ta_keywords": "annotating audio linguistic;model automatic speech;speech recognition asr;phonemic transcripts speech;transcribing annotating audio;automatic speech;audio linguistic;audio linguistic information;nlp proposed asr;annotations train asr;transcripts speech;systems speech translation;combine asr nlp;transcripts speech pos;phonemic transcripts;linguistic annotations train;asr nlp method;spoken dialogue systems;annotating audio;automatic speech recognition;phoneme sequence;speech translation combine;useful speech interface;speech interface applications;speech recognition;information phonemic transcripts;asr transcript;asr nlp;linguistic annotations accurately;phoneme sequence pos", "pdf_keywords": ""}, "811531c959b0543a8e7abe1e827770e36b96f817": {"ta_keywords": "emphasis transfer speech;emphasis estimation using;emphasis estimation;level emphasis estimation;emphasis transfer;accurately translate emphasis;emphasis target language;level emphasis transfer;speech speech translation;speech translation;translate speech languages;word level emphasis;transfer speech speech;speech translation article;transfer speech;translate speech;speech speech s2s;speech s2s translation;translation systems;level emphasis target;speech s2s;emphasis translation;translate emphasis;emphasis target;emphasis translation translates;global speech speech;speech languages;hsmm emphasis translation;translates word level;s2s translation systems", "pdf_keywords": ""}, "6c34b7b0441bff66cce2418d36acfd9776ad7bd2": {"ta_keywords": "existing rule learning;proposed rule learning;rule learning systems;rule learning algorithm;rule learning;fast ective rule;ective rule induction;rule induction;higher rules;learning algorithm irep;large noisy datasets;equivalent rules bench;examples machine learning;machine learning proceedings;learning algorithm;noisy datasets;higher rules propose;ml fast ective;noisy datasets containing;learning systems computationally;process noisy datasets;existing rule;rules bench mark;ective rule;noisy datasets paper;machine learning;lower equivalent rules;ripperk competitive rules;benchmark problems irep;benchmark problems", "pdf_keywords": ""}, "d723630c585aa0e4084fdd6e71bc6586cfa30e9d": {"ta_keywords": "pause prediction dependency;labeled syntax pause;syntax pause information;better pause prediction;prediction dependency parsing;joint pause prediction;pause prediction;pause prediction measure;pauses consecutive words;syntax pause;dependency parsing model;pause information;data labeled syntactic;prosodic information pauses;labeled syntactic information;information pauses;pause information data;dependency parsing;parsing model;annotation syntactic information;labeled syntactic;treats pauses;syntactic information;pauses;consecutive words latent;introducing joint pause;parsing model obtains;syntactic information prosodic;better pause;pause", "pdf_keywords": ""}, "81bc64ce5553798c058f25fe5bd537d4bed67aed": {"ta_keywords": "xas quantum dots;vapour deposition inxga1;deposition inxga1 xas;barriers qd luminescence;quantum dots qds;inxga1 xas quantum;quantum dots;deposition inxga1;qd luminescence narrow;qd luminescence;xas quantum;chemical vapour deposition;vapour deposition;dots qds;luminescence narrow pl;inxga1 xas;luminescence narrow;emission energies;dots qds 35;tuning emission energies;organometallic chemical vapour;xas;70as barriers qd;qds 35 grown;emission energies varying;inxga1;barriers qd;luminescence;chemical vapour;emission", "pdf_keywords": ""}, "a8c62c42509c45a708ba477b603ee3fb81c77056": {"ta_keywords": "false news weibo;false news detection;dissemination false news;news weibo social;false news social;news weibo;propagation false news;weibo social platform;news detection social;news social media;wide dissemination false;detection social media;false news news;news social;society false news;weibo social;published social media;social media verifiably;news detection;news detection approaches;news news posts;false news;social media set;social media;weibo;social media order;news posts;fnews help evaluate;wide dissemination;time false news", "pdf_keywords": "detection falsenews multi;falsenews multi modal;image detection falsenews;false news detection;detection false news;falsenews multi;detection falsenews;detecting false news;false news weibo;modal false news;news weibo social;news detection;subtasks false news;news weibo;news detection dataset;news image detection;multi modal detection;false news image;news e\ufb00ective fusion;modal detection aims;news text detection;news detection problem;false news text;falsenews;news image;weibo social platform;modal detection;news text;fnews largest multi;features di\ufb00erent modalities"}, "44775500a5380be3776e876aedc43921d42d8de9": {"ta_keywords": "urban mobility data;uncovers urban dynamics;activity patterns urban;patterns urban mobility;characterizes urban dynamics;urban dynamics;recovers urban dynamics;urban dynamics highly;urban activities;urban dynamics urban;urban mobility;urban dynamics massive;people urban activities;rich urban dynamics;dynamics urban;patterns urban;data extracts urban;urban dynamics different;dynamics people urban;urban activities recovers;urban activities extremely;massive urban mobility;dynamics urban region;activities recovers urban;mobility data;method uncovers urban;rhythms urban activities;dynamics massive urban;scale urban mobility;discovering activity patterns", "pdf_keywords": ""}, "470bfbde1dc0ed6ca989957dcd551213720657c0": {"ta_keywords": "neural machine translation;translation neural machine;translation neural;translation systems require;machine translation;machine translation systems;fluency machine translation;needs translation neural;translation systems;machine translation nmt;translates inducing dependency;syntax model needs;syntax model;syntax alleviates burden;model needs translation;simultaneously translates inducing;attention mechanism decoder;inducing dependency trees;translation nmt systems;question syntax model;structure investigating syntax;syntax alleviates;explicit syntax alleviates;investigating syntax nmt;requiring parses;incorporating explicit syntax;syntax nmt induce;knowledge grammatical understanding;translates inducing;parses", "pdf_keywords": "attention encoder;self attention encoder;attention encoder predicts;structured self attention;latent structures encoding;joint syntactic semantic;syntactic structure;capture syntactic;attention;bene\ufb01t syntactic structure;determiner attention;syntactic semantic;trees joint syntactic;joint syntactic;syntactic semantic objective;semantic syntactic;sentence transferred decoder;syntactic structure providing;syntactic;aims capture syntactic;latent structures;learning latent trees;syntactic compo boy;capture syntactic verb;learning latent;self attention;capturing target syntax;bene\ufb01t syntactic;semantic syntactic compo;projective dependency tree"}, "bc494b9c6d9602a69b76ab9ea0e95d348a2fce19": {"ta_keywords": "evaluation summarization highres;reference evaluation summarization;evaluation generated summaries;evaluation summarization;summarization highres;summarization highres summaries;summarization research;summaries assessed;generated summaries;highres summaries assessed;readers summary assessment;annotators source document;progress summarization research;summarization research enabled;manual evaluation highlight;summarization;highlights used evaluating;summaries assessed multiple;highlight based reference;annotators source;generated summaries inconsistent;improves inter annotator;summary assessment;highlighted salient content;substantial progress summarization;evaluation highlight based;assessed multiple annotators;multiple annotators source;annotator agreement comparison;summary assessment source", "pdf_keywords": "reference evaluation summarization;evaluation summarization highres;evaluation document summarization;evaluation summarization;highlight annotation quality;sourcing highlight annotation;document summarization highres;extreme summarization dataset;summarization research;summarization highres summary;extreme summarization;document summarization;summarization highres summaries;summarization highres;introduced extreme summarization;highlight based reference;summarization dataset;highlight annotation;annotation quality;manual evaluation highlight;summarization methods pointer;progress summarization research;summarization research enabled;summaries assessed;summarization methods;highlight based referenceless;abstractive summarization methods;annotation quality judgments;evaluation highlight based;annotators source document"}, "e2ffd0ea7aa9cebaafba4afaee3cbe78070c8aa2": {"ta_keywords": "triphone hmms based;state triphone hmms;speech based bayesian;clustering speech recognition;hmms based variational;estimation clustering speech;clustering speech;triphone hmms;speech recognition;speech recognition vbec;recognizes speech based;classification variational bayesian;recognizes speech;approach recognizes speech;shared state triphone;isolated word recognition;variational bayesian estimation;based variational bayesian;state triphone;prediction classification variational;word recognition;variational bayesian approach;bayesian estimation clustering;variational bayesian;bayesian approach recognizes;hmms based;triphone;classification variational;bayesian prediction classification;speech based", "pdf_keywords": ""}, "3321c947a4a399803592f26879927e58f587fd74": {"ta_keywords": "predicting future arrests;participants predict offender;future arrests studies;arrests studies examine;arrests studies;predict offender rearrested;future arrests;predict offender;algorithmic risk assessment;criminal justice conduct;likelihood arrest;algorithmic risk;deem likelihood arrest;arrests;criminal justice;decision makers predictive;participants predictions;participants predictions depend;focused criminal justice;likelihood arrest paper;recent crowdsourcing;participants predict;participants cases assessed;likelihood arrest 50;building recent crowdsourcing;arrest;recent crowdsourcing works;laypersons tasked predicting;arrest 50 participants;interactants algorithmic risk", "pdf_keywords": "predictions arrest felonies;likelihood incarceration predictions;incarceration predictions arrest;investigated likelihood incarceration;predictions arrest participants;incarceration predictions;likelihood incarceration;rates predicted arrest;rates black offenders;predictions arrest;offenders compared blacks;arrest felonies;rate arrest virtually;predicted arrest;white offenders compared;offenders compared;offenders share predictions;probability arrest;predicted arrest slightly;risk white offenders;rate arrest;40 predicted arrest;arrest felonies rai;arrest participants;average probability arrest;felony offenders;arrest virtually identical;black offenders;arrest participants rai;predicted arrest 29"}, "0554246ebb6c53e88d7ac2aabf0c96a91ad500a0": {"ta_keywords": "speech enhancement simulation;training speech enhancement;channel speech enhancement;speech enhancement capability;domain speech enhancement;speech enhancement;based speech enhancement;speech enhancement model;speech enhancement real;speech enhancement speech;enhancement speech recognition;strong speech enhancement;enhancement speech;speech enhancement paper;tasnet based speech;multi channel speech;reduce speech recognition;speech recognition performance;conv tasnet beamforming;tasnet beamforming model;chime corpus proposed;channel speech;reduce speech;chime corpus;channel conv tasnet;tasnet beamforming;training speech;experiments chime corpus;preserving strong speech;speech recognition", "pdf_keywords": "speech enhancement simulation;training speech enhancement;channel speech enhancement;domain speech enhancement;speech enhancement capability;enhancement speech recognition;speech enhancement real;speech enhancement;speech enhancement speech;speech enhancement approaches;based speech enhancement;strong speech enhancement;enhancement speech;speech recognition performance;reduce speech recognition;tasnet based speech;multi channel speech;experiments chime4 corpus;chime4 corpus proposed;time domain speech;speech recognition models;speech recognition;channel speech;chime4 corpus;training speech;reduce speech;domain speech;preserving strong speech;joint training speech;greatly reduce speech"}, "c2bd176f8f9c84f9ba52ffb8f8bd4e9299c0f0cf": {"ta_keywords": "quantile regression prediction;quantile regression methods;quantile regression;quantile regression approach;nonparametric quantile regression;multiple quantile regression;regression prediction parking;prediction parking data;nonparametric quantile;general nonparametric quantile;quantile;multiple quantile;sets nonparametric quantile;prediction parking;parking data;paper multiple quantile;regression approach wind;data sets nonparametric;parking;general nonparametric;regression prediction;nonparametric;regression;solar price tracks;regression methods implemented;regression approach;regression methods;prediction;discuss general nonparametric;price tracks gefcom2014", "pdf_keywords": ""}, "a9a7058b39768ece13608e31341cfb16c4faf2c3": {"ta_keywords": "fair machine learning;fairness natural formulation;decisions justice fairness;machine learning ideal;approach fair machine;machine learning hopes;justice fairness;justice fairness natural;fairness natural;ideal approach political;observe fair world;fair machine;fairness;automated decisions;drive automated decisions;automated decisions straightforward;approach fair;consequential decisions justice;learning ideal approach;learning ideal;statistical parities hope;predictions drive automated;fair world;machine learning literature;policy arguing;concerning consequential decisions;observe fair;misguided policies;decisions justice;consequential decisions", "pdf_keywords": "fair machine learning;algorithmic fairness;algorithmic injustice reasonably;algorithmic fairness section;algorithmic injustice;addressing algorithmic injustice;work algorithmic fairness;approach fair machine;machine learning literature;fairness section argue;shortcomings proposed fair;proposed fair machine;political philosophy;fairness section;ideal approach political;political philosophy argue;fair machine;machine learning ml;machine learning algorithms;proposed fair;approaches political philosophy;approach political philosophy;machine learning;injustice reasonably usefully;fairness;approach fair;methodological approaches political;introduction machine learning;approach political;literature political philosophy"}, "156323f4d87af6cf105c97bf29d324c9e3bc8f92": {"ta_keywords": "techniques speech translation;asr machine translation;speech recognition asr;mt speech synthesis;optimized improve translation;optimization techniques speech;speech translation;speech synthesis;machine translation mt;speech translation st;speech translation addition;speech synthesis ss;translation quality independently;machine translation;translation mt speech;automatic speech;translation evaluation measure;speech recognition;translation quality;improve translation quality;translation evaluation;automatic speech recognition;components automatic speech;changes translation evaluation;algorithm mira speech;recognition asr;recognition asr machine;optimization mt empirical;mt jointly optimized;minimize word error", "pdf_keywords": ""}, "3cd4797725ca9cf954946ed5309e15ebab80b92a": {"ta_keywords": "conditional image synthesis;multimodal conditional image;generative adversarial networks;art multimodal conditional;experts generative adversarial;generative adversarial;gan learns synthesize;multimodal conditional;gan framework synthesize;experts gans;synthesis poe gan;image synthesis poe;setting multimodal conditional;learns synthesize images;generative;unimodal conditional image;experts generative;gan learns;synthesize images conditioned;gans;gans carefully designed;image synthesis;gan framework;gans carefully;art multimodal;multimodal;experts gans carefully;networks poe gan;product experts gans;image synthesis frameworks", "pdf_keywords": "gan approach multimodal;conditional image synthesis;multimodal conditional image;multimodal synthesis;multimodal conditional;introduce multimodal conditional;approach multimodal conditional;multimodal synthesis work;image synthesis datasets;prior multimodal synthesis;unimodal conditional image;multimodal discriminator architecture;image synthesis;multiscale multimodal discriminator;multimodal discriminator;multimodal;image synthesis approaches;image synthesis model;multiscale multimodal;conditional image;proposed multiscale multimodal;images conditional inputs;approach multimodal;introduce multimodal;prior multimodal;images conditional;art unimodal conditional;concatenated images conditional;conclusion introduce multimodal;superior prior multimodal"}, "365e049ecb7299cc6925483127f2f2123a97c35f": {"ta_keywords": "change point detection;kernel change point;kernel sample test;kernel selection sample;novel kernel learning;changes time series;kernel learning framework;models kernel sample;studies kernel selection;cpd novel kernel;kernel learning;generative models kernel;kernel sample;kernel selection algorithms;kernel selection;generative model detecting;developed kernel selection;kernel change;novel kernel;time series cpd;models kernel;simulation studies kernel;samples change point;studies kernel;change point;detecting emergence abrupt;generative models;deep generative models;samples change;abrupt property changes", "pdf_keywords": "cpd kernel learning;change point detection;change points detection;kernel change point;model deep kernel;new kernel learning;deep kernel parameterization;cpd novel kernel;data driven kernel;kernel learning framework;novel kernel learning;driven kernel detect;deep kernel;applications deep kernel;present deep kernel;kernel sample test;changes time series;novel kernel;kernel learning;time series cpd;deep kernel parametrization;kernels effectively detect;cpd kernel;kl cpd kernel;kernel twosample test;cpd new kernel;rnns rbf kernels;kernel kernel sample;kernel sample;kernel change"}, "464a75c05a5ce709fc515a2577b43acc8e3d45ce": {"ta_keywords": "hop explanations textgraphs;explanations textgraphs 2021;explanations textgraphs;inference explanation regeneration;subtask determining relevance;explanations questions assembling;manual explanatory relevancy;multi hop explanations;hop explanations questions;2020 shared task;explanatory relevancy;hop explanations;2021 shared task;task multi hop;multi hop inference;explanations questions;inference chain;hop inference;determining relevance large;complete inference chain;explanation regeneration asks;hop inference explanation;explanatory completeness finding;relevance large multi;shared task makes;textgraphs 2021 shared;determining relevance;relevance large;improvement previous;regeneration asks participants", "pdf_keywords": ""}, "a6e61164e7b385cec0e12093bc270eafd3ef1dbc": {"ta_keywords": "proposes activity recognition;activity recognition;activity recognition method;users sensor data;user activities using;user activities;activities using labeled;users sensor;activities using;end user activities;gender users sensor;activity;acceleration sensor data;user labeled training;sensor data similar;labeled sensor data;sensor data;activities;user physical characteristics;training data;labeled training data;data similar users;sensor data obtained;acceleration sensor;end user physical;using labeled sensor;hours sensor data;user physical;information end user;labeled training", "pdf_keywords": ""}, "d7c1bdafb51fe1a757604f9daeaea812f124320f": {"ta_keywords": "contracts technology forecasting;technology forecasting english;technology forecasting systems;technology forecasting study;technology forecasting;nowadays technology forecasting;suitable technology forecasting;systems technology forecasting;forecasting systems languages;technology forecasting think;technology forecasting multidisciplinary;russian patent databases;forecasting english russian;government contracts technology;parameter technology forecasting;forecasting study government;information retrieval pipeline;russian government contracts;forecasting english;technologies analyzed russian;forecasting think government;sources order forecast;patent databases;contracts indicate government;government contracts utilizing;study government contracts;patent databases citation;forecast trends future;forecast trends;english russian patent", "pdf_keywords": ""}, "fba7c0a51a6301ca4086a5ce59b1f13af9acad7f": {"ta_keywords": "japanese morphological analysis;approach japanese morphological;japanese morphological;structured approach japanese;morphological analysis;annotation active learning;morphological analysis ma;partial annotation active;partial annotation;pointwise approach japanese;learning tagging;annotation active;structured predictors using;structured predictors;morphological;approach japanese;annotation;information learning tagging;similar structured predictors;combination partial annotation;tagging;approach japanese ma;structure information learning;japanese;active learning;predictors using feature;structured;state art structured;accuracy similar structured;similar structured", "pdf_keywords": ""}, "5d07db93e6fbd9e10713a2f372131c777077062d": {"ta_keywords": "deep quantization bits;deep quantization neural;utility deep quantization;effort deep quantization;deep quantization;automatic deep quantization;quantization neural networks;deep reinforcement learning;bitwidth assignment quantization;assignment quantization large;quantization neural;deep quantization lead;releq reinforcement learning;dnn computation storage;quantization bits;assignment quantization;quantization bits significantly;deep reinforcement;quantization large;reduce dnn computation;deep networks minimal;assignment bitwidths layers;end deep reinforcement;quantization;dnn computation;deep networks;releq reinforcement;automatic deep;utility deep;approach automatic deep", "pdf_keywords": ""}, "e63e1db25f33162cc6e498f983dc3f6e10c9867e": {"ta_keywords": "model speech separation;overlapped speech separation;separation speaker extraction;speech separation benchmarks;speech separation extraction;conventional speech separation;speaker extraction;speech separation speaker;speaker information observation;speech separation;speaker extraction multi;fully overlapped speech;predicted speaker information;speaker conditional chain;overlapped speech;separation speaker;complex speech recordings;chain model speech;extract speech;speech recordings;speech recordings takes;speakers observation based;speaker information;speakers conditions extract;conditions extract speech;extract speech sources;model speech;information inferred speakers;speaker conditional;speakers observation", "pdf_keywords": "reconstruct speech separation;overlapped speech inference;integrating speaker information;speaker independent tasks;speaker extraction;fully overlapped speech;speech separation;speaker conditional chain;speakers overlapped speech;processing speech speakers;speaker information model;tasks speech separation;speech separation extraction;extraction speaker diarization;speech separation speech;separation speech denoising;overlapped speech;speech inference toextraction;separate speech sources;model processing speech;speech inference;probability based speaker;speaker diarization;overlapped speech variable;recordings multiple speakers;diarization speaker independent;speaker extraction speaker;integrating speaker;aim speaker extraction;separate speech"}, "c3d2c60e70cad17ea37cb116ab30e1239405dbdd": {"ta_keywords": "eeg\u3092\u7528\u3044\u305fasr\u51fa\u529b\u306b\u9055\u548c\u611f\u306e\u691c\u51fa \u6587\u732e\u60c5\u5831 global;eeg\u3092\u7528\u3044\u305fasr\u51fa\u529b\u306b\u9055\u548c\u611f\u306e\u691c\u51fa \u6587\u732e\u60c5\u5831;eeg\u3092\u7528\u3044\u305fasr\u51fa\u529b\u306b\u9055\u548c\u611f\u306e\u691c\u51fa;global \u79d1\u5b66\u6280\u8853\u7dcf\u5408\u30ea\u30f3\u30af\u30bb\u30f3\u30bf\u30fc;\u79d1\u5b66\u6280\u8853\u7dcf\u5408\u30ea\u30f3\u30af\u30bb\u30f3\u30bf\u30fc;\u6587\u732e\u60c5\u5831 global \u79d1\u5b66\u6280\u8853\u7dcf\u5408\u30ea\u30f3\u30af\u30bb\u30f3\u30bf\u30fc;\u6587\u732e\u60c5\u5831;\u6587\u732e\u60c5\u5831 global;global", "pdf_keywords": ""}, "6a730aff0b3e23423a00cb3407eb04e7f6e83878": {"ta_keywords": "statistical machine translation;machine translation;models word alignment;variational bayes ibm;machine translation terms;bayes ibm models;word alignment statistical;alignment statistical machine;bayes ibm;moses machine translation;ibm models word;word alignment;variational bayes improves;bayesian technique variational;using variational bayes;variational bayes;alignment statistical;translation terms;ibm models;translation terms bleu;technique variational bayes;em algorithm;bayesian approaches;bayes improves performance;score apply bayesian;bayesian approaches shown;bayesian technique;translation;bayes improves;em algorithm placing", "pdf_keywords": ""}, "c06410ce8f9b1c941115d6d96780794e66b27eac": {"ta_keywords": "adaptation techniques speech;incremental adaptation based;incremental adaptation techniques;fast incremental adaptation;incremental adaptation;novel incremental adaptation;adaptation bayesian detection;incremental adaptation framework;posterior distributions adaptation;utterance update approximating;evolution incremental adaptation;time evolution incremental;adaptation bayesian;adaptation japanese broadcast;adaptation based macroscopic;changes speakers speaking;including changes speakers;adaptation japanese;continuous speech recognition;adaptation based;distributions adaptation;acoustic models quickly;distributions adaptation used;utterance utterance update;updating posterior distributions;utterance update;evolution incremental;fast incremental;line adaptation bayesian;speech recognition", "pdf_keywords": ""}, "68167af17980a14ed5fa2514e61d76d5a6a9bed7": {"ta_keywords": "vaccine conspiracies online;anti vaccine conspiracies;vaccine conspiracies;traditional news conspiracy;news conspiracy theories;conspiracies online;pandemic vaccine conversation;conspiracies online hindered;news conspiracy;news scientific dubious;coronavirus created information;information spread coronavirus;conspiracy theories examined;theories fake news;sources information spread;government news scientific;news scientific;conspiracy theories fake;vaccine conversation;legitimate sources information;news competed legitimate;fake news competed;responses pandemic;vaccine conversation distinct;dubious dominate pandemic;articles social media;government news;pandemic vaccine;scientific messaging coronavirus;conspiracy theories", "pdf_keywords": "vaccine conspiracies online;vaccination discussion twitter;content vaccination discussion;anti vaccine conspiracies;vaccine conspiracies;conspiracies online;news conspiracy theories;traditional news conspiracy;conspiracies online hindered;news scientific dubious;news conspiracy;pandemic examined content;coronavirus created information;external content vaccination;content vaccination;theories fake news;messaging mainstream misinformation;pandemic vaccine conversation;conspiracy theories examining;legitimate sources information;news scientific;article content believe;conspiracy theories fake;vaccine conversation discussion;government news scientific;news competed legitimate;vaccination discussion;articles social media;fake news competed;social media posts"}, "4a96b2b33786301d59670fa647f99e3dd807abb8": {"ta_keywords": "agents converge learning;agents learn deterministic;multiagent learning algorithms;multiagent learning;games agents learn;gradient based multiagent;agents learn;based multiagent learning;continuous games agents;stable nash equilibrium;equilibrium agents converge;non uniform learning;nash equilibrium;neighborhood stable nash;agents converge;nash equilibrium support;individual gradient stochastic;stable nash;converge learning;gradient stochastic settings;algorithms non cooperative;learning algorithms non;equilibrium agents;gradient stochastic;uniform learning;continuous games;uniform learning rates;converge learning path;consider continuous games;learn deterministic", "pdf_keywords": ""}, "d9b458d39e0912524032887aaaf922f0e950f0c1": {"ta_keywords": "superpixel segmentation unsupervised;aware superpixel segmentation;superpixel segmentation;edge aware superpixel;segmentation unsupervised convolutional;aware superpixel;segmentation unsupervised;superpixel;segmentation;edge aware;unsupervised convolutional neural;unsupervised convolutional;edge;convolutional neural networks;convolutional neural;unsupervised;convolutional;networks;neural;neural networks;aware", "pdf_keywords": ""}, "76f9f4bf8d97de5e95d2fd9dd8b50041524fb1cc": {"ta_keywords": "entity alignment joint;joint knowledge embeddings;entity alignment improve;improvements entity alignment;entities wikipedia links;entity alignment;knowledge embeddings;knowledge embeddings specifically;knowledge embeddings existing;entities wikipedia;alignment joint knowledge;approach entity alignment;knowledge graph completion;aligned entities;joint semantic space;alignment improve knowledge;semantic distance joint;align entities;information entities wikipedia;encodes entities relations;improve knowledge graph;joint semantic;align entities according;entities relations;set aligned entities;distance joint semantic;entities according semantic;jointly encodes entities;entities relations various;knowledge graph", "pdf_keywords": ""}, "476ff888fe3917f92b221c522ffb7bfaa4e1861b": {"ta_keywords": "open retrieval conversational;response ranking conversational;retrieval conversational;ranking conversational;conversational question answering;conversational search systems;approaches conversational search;functional conversational search;ranking conversational question;conversational search;conversational search ultimate;retrieval conversational question;conversational search simplified;transformers conversational search;question answering;question answering orconvqa;response ranking;question answering answer;introduce open retrieval;collection extracting answers;open retrieval;information retrieval;information retrieval create;extracting answers;reranker reader based;building functional conversational;research orconvqa reranker;reranker reader;goals information retrieval;retrieval", "pdf_keywords": "open retrieval conversational;retrieval conversational;conversational search systems;functional conversational search;conversational question answering;conversational search;retrieval conversational question;introduce open retrieval;open retrieval;question answering;question answering orconvqa;history modeling components;collection extracting answers;building functional conversational;reranker reader based;retrieval;reranker reader;extracting answers;enable history modeling;learn retrieve evidence;featuring retriever reranker;history modeling;functional conversational;retriever reranker reader;large collection extracting;search systems;answering orconvqa;retriever reranker;extracting answers step;search systems build"}, "b169c4b6c23efe8cbd4dc29eb97939cbcfba0f28": {"ta_keywords": "persuasive dialogue systems;corpus persuasive dialogues;speaker persuasive dialogue;persuasive dialogues;persuasive dialogues real;persuasive dialogue;actions dialogue participants;persuasiveness speaker persuasive;persuasiveness speaker;speaker persuasive;dialogue participants;dialogue participants gained;persuader utterances argumentation;contribute persuasiveness speaker;corpus persuasive;persuader utterances;construction corpus persuasive;common dialog acts;factors derived dialogue;dialogue professional salespeople;dialog acts;annotate dialogue acts;alternative annotate dialogue;dialogue acts particularly;annotate dialogue;dialogue literature factors;utterances argumentation framing;dialogue acts;dialogue systems;dialog acts information", "pdf_keywords": ""}, "65d3575b1c380b1bcc14ec69ccf6989c04be9493": {"ta_keywords": "asynchronous graph algorithms;semantic web graph;collect graph algorithms;signal collect framework;graph algorithms semantic;synchronous asynchronous graph;algorithms semantic web;asynchronous graph;signal collect graph;web semantic;signal collect programming;web semantic web;semantic web;web graph growing;web graph;algorithms semantic;semantic web semantic;collect graph;signal collect;graph algorithms furthermore;various algorithms graph;graph algorithms;algorithms graphs concise;essence algorithms graphs;graphs concise elegant;algorithms graph;algorithms graphs;giving signal collect;prototype signal collect;algorithms demonstrate abstraction", "pdf_keywords": ""}, "cf7e8f47ad1c57738dc586109dcf28a22ab67b72": {"ta_keywords": "bidding peer review;papers super algorithm;papers reviewer bids;bids review papers;sufficiently bids paper;reviewer bids;optimize paper bidding;sequentially reviewer needs;sequentially reviewer;review papers super;reviewer bids review;sufficiently bids;ordering papers competing;paper bidding peer;super algorithm optimize;bids review;algorithm optimize paper;obtaining sufficiently bids;deciding ordering papers;focus paper bidding;considerably outperforms baselines;conference peer review;bids paper;outperforms baselines;paper bidding;paper bidding process;super algorithm;popular baselines considerably;peer review experiments;peer review", "pdf_keywords": "optimize paper bidding;paper bidding peer;paper bidding;bidding peer;bidding peer review;algorithm optimize paper;user ordering;bidding;optimize paper;super algorithm optimize;user ordering items;ordering;super algorithm;sequential arrival users;showing user ordering;ordering items;algorithm optimize;applications involve sequential;algorithm;paper;peer review;sequential arrival;involve sequential arrival;peer;arrival users;optimize;sequential;peer review tanner;arrival users require;involve sequential"}, "adc273bd25ab1e2a66543f23c7a801af0dd80e5b": {"ta_keywords": "speech recognition speaker;estimation speaker embeddings;recognition speaker diarization;estimated speaker embeddings;speaker automatic speech;method estimation speaker;speaker diarization single;dialogue recordings speaker;speaker diarization;sample utterance speaker;estimation speaker;simultaneous speech recognition;automatic speech;based estimated speaker;speech recognition ts;automatic speech recognition;speech recognition;recognition speaker;extract recognize speech;asr simultaneous speech;speaker embeddings;speaker embeddings ii;speaker embeddings alternately;challenging dialogue recordings;speech target speaker;channel dialogue recordings;recognize speech target;dialogue recordings;recognize speech;estimated speaker", "pdf_keywords": "estimation speaker embeddings;estimated speaker embeddings;recognition speaker diarization;speech recognition speaker;method estimation speaker;dialogue recordings speaker;speaker automatic speech;speaker diarization single;estimation speaker;speaker diarization monaural;speech recognition ts;speaker diarization;recognition speaker;speaker embeddings;speaker embeddings ii;based estimated speaker;simultaneous speech recognition;challenging dialogue recordings;sample utterance speaker;speaker embeddings alternately;dialogue recordings target;speech target speaker;channel dialogue recordings;automatic speech;dialogue recordings;speaker acoustic models;monaural dialogue recordings;recordings target speaker;speech recognition;extract recognize speech"}, "b20cadef0c59e80f7dfdf825b07442619d920fd5": {"ta_keywords": "decoding rnnlm achieved;attention decoding rnnlm;decoding rnnlm;ctc attention decoding;parallelized decoding algorithms;parallelized decoding;recognition asr attentionbased;speech recognition asr;attention decoding;use parallelized decoding;attentionbased encoder decoder;language model rnnlm;asr attentionbased encoder;implementation accelerate decoding;accelerate decoding process;rnnlm connectionist temporal;speech recognition;accelerate decoding;recurrent neural network;utterances batch;attentionbased encoder;rnnlm connectionist;asr attentionbased;utterances batch simultaneously;rnnlm achieved 11;rnnlm achieved;model rnnlm;recognition asr;model rnnlm connectionist;hypotheses extend decoding", "pdf_keywords": ""}, "16326359081a42c0b254ee6be39824fd2db07e48": {"ta_keywords": "improving pivot translation;pivot translation allows;pivot target translation;pivot translation remembering;pivot translation;translation remembering pivot;pivot language model;pivot phrases triangulation;translation language pairs;translation models;target translation models;pivot language;use pivot language;translation models source;translation language;high translation accuracy;pivot phrases;information source translation;translates combining source;phrases triangulation stage;target translation;translation time improving;translation accuracy;phrases triangulation;translation remembering;triangulation method translates;translates combining;torememberthe pivot phrases;source pivot;translation allows translation", "pdf_keywords": ""}, "336ee50043b916c9e932338c02fd1abc87a6e849": {"ta_keywords": "compositional generalization solving;achieve compositional generalization;compositional generalization learning;compositional generalization;compositional generalization model;cognition argues compositionality;accuracies compositional generalization;expressions achieve compositional;achieve compositional;compositionality captured variable;ability compositional generalization;learning analytical expressions;compositionality captured;argues compositionality captured;compositional;argues compositionality;compositionality;generalization learning analytical;accuracies compositional;generalization solving;memory augmented neural;generalization solving challenges;ability compositional;cognitive argument trained;generalization learning;100 accuracies compositional;consists cooperative neural;augmented neural;neural modules composer;great ability compositional", "pdf_keywords": "achieve compositional generalization;compositional generalization;compositional generalization basic;compositional generalization automatically;compositional generalization learning;abstract compositional generalization;compositionality analytical expressions;2020 compositional generalization;cognition argues compositionality;compositionality captured variable;expressions propose memory;achieve compositional;compositionality captured;compositionality analytical;argues compositionality captured;compositionality;language compositionality analytical;compositional;generalization learning analytical;argues compositionality;abstract compositional;language compositionality;model achieve compositional;learning analytical expressions;memory augmented neural;generalization learning;propose memory augmented;generalization basic;generalization basic essential;generalization"}, "89ba434b30a3f1b61bcbcf917842899fe3d2eea4": {"ta_keywords": "linguistic individuality transformation;individuality transformation spoken;individuality writer speaker;transformation spoken language;statistical machine translation;linguistic individuality;transforming individuality using;transform individuality;machine translation;express individuality writer;method transforming individuality;individuality writer;individuality transformation;transforming individuality;dialogue systems;text speech various;machine translation smt;individuality using;express individuality;effective transform individuality;identifiable text speech;text speech;dialogue systems consider;creation dialogue systems;individuality using technique;speech various features;transform individuality degree;speech various;features express individuality;transformation spoken", "pdf_keywords": ""}, "63a35d8822a042f6d6cd919fd5d3c9e94df6ee18": {"ta_keywords": "change point detection;detect change points;supervised change point;unsupervised change point;divergences supervised change;detect change;supervised change;labeled change point;change point instances;improve change point;existing unsupervised change;unsupervised change;point detection unsupervised;windows detect change;change points;point detection framework;change point;dimensional change point;novel change point;detect kinds changes;changes want detect;true change point;point instances supervision;point detection performance;point detection;change points online;sinkhorn divergences supervised;detection unsupervised;divergences supervised;point detection experiments", "pdf_keywords": "detecting change points;change point detection;detect change points;supervised change point;detecting change;detect change;change points learn;supervised change;labeled change points;detect kinds changes;divergences supervised change;changes want detect;windows detect change;online change point;change point instances;improve change point;require detecting change;change points;change point;point instances supervision;detection unsupervised consequence;novel change point;change points online;labeled change;true change point;metric highlights changes;detection existing;point detection existing;learning metric highlights;change points complex"}, "00717c695e4a33318fe5655e2b69e1ba8b61f981": {"ta_keywords": "genre broadcast challenge;rescoring gram rnnlm;challenge combination deep;automatic speech;speech text english;multi genre broadcast;speech recognition;speech text;genre broadcast;automatic speech recognition;decoding rescoring gram;broadcast challenge combination;speech recognition understanding;broadcast challenge official;ieee automatic speech;dnn acoustic models;combination deep learning;broadcast challenge;acoustic models language;presented speech text;deep learning systems;challenge subsets training;fbank dnn acoustic;language models decoding;deep learning;naist asr 2015;2015 multi genre;gram rnnlm paper;score based weighting;gram rnnlm", "pdf_keywords": ""}, "ba201da15899e78629ee5471e8d336b6b2eb7279": {"ta_keywords": "modal routing algorithms;socially considerate routes;routing models traffic;transportation networks communities;routing models;mobility services socially;public transit demand;effects routing models;models traffic congestion;transportation networks;considerate routes;routing algorithms proactive;riding public transit;public transit;constrained transportation networks;routing algorithms;modal routing platform;mobility decision;multi modal routing;mobility decision support;traffic congestion;transit demand nature;transit demand;assesses effects routing;routes;typically mobility decision;modal routing;mobility services;efficacy mobility services;considerate routes increasing", "pdf_keywords": "socially considerate routes;modal routing algorithms;routing models traffic;transportation networks communities;routing models;effects routing models;routing algorithms proactive;routing algorithms;considerate routes;models traffic congestion;transportation networks;modal routing platform;constrained transportation networks;multi modal routing;routes;traffic congestion;modal routing;assess effects routing;models traffic;routing;mobility services results;routing platform;efficacy mobility services;mobility services;effects routing;socially optimal multi;considerate routes arxiv;constrained transportation;traffic;mobility"}, "0af2ff552ab0555914dee90ccfae18297b2792c9": {"ta_keywords": "networks discriminative speaker;speaker embeddings trained;discriminative speaker embeddings;discriminative speaker;speaker embeddings;learning using speaker;performs meeting diarization;using speaker identification;meetings recorded using;speakers local global;speaker identification component;meetings recorded;speaker identification;simulated meetings recorded;handling speaker;discriminative training;handling speaker overlap;using speaker;meeting diarization;end deep network;channel audio recordings;number speakers local;audio recordings;speaker;handling discriminative training;deep network;global networks discriminative;simulated meeting data;audio recordings end;simulated meetings", "pdf_keywords": "attention network speaker;global speaker modeling;speaker modeling;learning using speaker;attention network;speech self attention;speaker detection;self attention network;local features speech;speaker detection module;network speaker classi\ufb01cation;performs meeting diarization;speaker identi\ufb01cation component;features speech self;loss speaker detection;attention neural network;end diarization models;features speech;global speaker;speaker classi\ufb01cation auxiliary;using speaker identi\ufb01cation;focus global speaker;help diarization performance;speaker identi\ufb01cation;attention neural;self attention module;meeting diarization;speaker classi\ufb01cation;diarization performance including;attention module"}, "12f3bc02d649645fa8734977e28b0ac839e56371": {"ta_keywords": "parking network queues;allowable congestion queues;queueing network;curbside parking network;parking scarcity congestion;kind queueing network;parking network;network queues;constraints allowable congestion;optimizing curbside parking;congestion queues;network maximizing occupancy;congestion constraints;scarcity congestion block;queueing network subject;price control queues;allowable congestion;available server convex;server convex optimization;scarcity congestion;network queues present;congestion queues searching;congestion block;curbside parking resources;congestion constraints gain;parking scarcity;control queues;relationship parking scarcity;queues subject constraints;curbside parking", "pdf_keywords": "queueing network;maximizing occupancy queues;queue network;queue network customers;queues according network;network customers queues;allowable congestion queues;queueing network demonstrate;kind queueing network;network maximizing occupancy;constraints allowable congestion;spatial utilization parking;congestion queues;utilization parking;occupancy queues;kind queue network;server convex optimization;future parking policies;congestion queues searching;utilization parking resources;view queue network;parking policies;price control queues;queues subject constraints;customers queues;available server convex;parking resources;allowable congestion;network maximizing;parking resources provided"}, "b7637d1148da569d2211b5dd9851bca82c6aac43": {"ta_keywords": "dependency parsing feature;selection dependency parsing;dynamic parser;dynamic parser achieve;based dependency parsing;dependency parsing;parsing feature;sentence dynamic feature;parsing feature computation;superior parsers using;parsers;superior parsers;parsing;comparable superior parsers;parser;parsers using;parser achieve accuracies;parser achieve;online sentence dynamic;sentence dynamic;feature selection dependency;dynamic feature selection;graph based dependency;dynamic feature;selection dependency;features computing;online sentence;parsers using set;framework dynamic feature;faster framework dynamic", "pdf_keywords": "dependency parsing feature;dependency parsing graph;based dependency parsing;dependency parsing;based dependency parsers;parsing graph based;dependency parsers;dependency parsing approach;dependency parsing word;parsing feature;improves parsing accuracy;dependency parsers including;higher order parsing;parsing accuracy graph;methods dependency parsing;improves parsing;parsing graph;non projective parsing;projective parsing;parsing higher order;parsing approximations higher;graph based dependency;parsers including;parsing accuracy;parsing;projective parsing higher;parsers;order parsing approximations;parsing approximations;parsing feature templates"}, "5f8d2da91a6c4b9dd079ccb2706c31bda14ef320": {"ta_keywords": "recognition audio captioning;automated audio captioning;audio captioning;audio captioning aac;audio captioning create;speech recognition audio;automatic speech;joint speech recognition;speech recognition asr;audiocaps dataset propose;automatic speech recognition;monaural speech recognition;captioning aac thoroughly;recognition audio;speech recognition;audiocaps dataset;speech recognition systems;captioning aac;chosen audiocaps dataset;field automated audio;automated audio;studied automatic speech;clean speech wall;mixing clean speech;sounds using speech;audiocaps;using speech enhancement;speech enhancement train;captioning;joint speech", "pdf_keywords": "automated audio captioning;audio representations;audio representations separately;audio captioning;effective audio representations;audio captioning aac;speech recognition asr;audio events;automatic speech;automated audio;audio events 14;temporal structure audio;\ufb01eld automated audio;modeling asr;automatic speech recognition;structure audio events;propose shared encoder;recognition asr;encoder decoder;dual output decoding;separately decoding wasr;encoder decoder frameworks;speech recognition;representations separately decoding;captioning aac thoroughly;extract effective audio;audio;encoder;recognition asr perform;encoder dualdecoder"}, "298b72096b8a770b0cdb263dd53cf2463b8a1a1d": {"ta_keywords": "embeddings suffice causal;embeddings text predict;causal inference text;dimensional embeddings text;deep language models;embeddings text;inference text documents;adapts deep language;low dimensional embeddings;low dimensional text;text predictive treatment;text predictive;embeddings suffice;deep language;predict values embeddings;text predict;causal inference;inference text;dimensional text representation;embeddings;address causal inference;documents estimate effects;language models learn;causal inference key;key insight causal;used causal inference;values embeddings suffice;aspects text predictive;dimensional embeddings;suffice causal", "pdf_keywords": "document embeddings causal;embeddings causal bert;causal topic model;embeddings causal;causally suf\ufb01cient embeddings;bert causal topic;bert topic modeling;causal effects text;suf\ufb01cient document embeddings;causally suf\ufb01cient document;causal bert causal;modeling bert topic;causal bert;document embeddings;document representations;text representation learning;topic modeling;bert causal;improve causal estimation;topic modeling produce;topic model;topic model contributions;embedding methods develop;lowdimensional document representations;causal topic;suf\ufb01cient embeddings;empirically validating causal;suf\ufb01cient embeddings semi;related embedding methods;embeddings semi"}, "ffa07e4d7c8fade2ded5ffeea7265d22d8a0c0ab": {"ta_keywords": "3d constructionlovers;provides 3d constructionlovers;3d constructionlovers easy;3d construction;result 3d construction;3d construction methods;lidar fortunately neural;images construct models;gan provides 3d;networks designed image;best result 3d;neural network gan;lidar;constructed model ground;cnn generative neural;ranging lidar;generative neural network;cnn generative;provides 3d;network cnn generative;3d;network cnn;detection ranging lidar;network gan;ranging lidar fortunately;model ground;neural network cnn;constructed model;neural network;lidar fortunately", "pdf_keywords": ""}, "41e49fd3af628f1c8201942a659769f7cc21d812": {"ta_keywords": "supervised learning ssl;graph based ssl;learning ssl algorithms;ssl algorithm compactly;learning ssl;ssl algorithms;ssl algorithm;graph based semi;ssl algorithms requiring;based ssl algorithms;ssl algorithms successfully;based ssl algorithm;ssl algorithms usually;randomized data structure;label distribution node;semi supervised learning;state art graph;novel graph based;unlabeled nodes propagating;stores label distribution;unlabeled nodes;novel graph;nodes propagating label;information structure graph;complexity graph based;label information structure;initially unlabeled nodes;scaling graph based;supervised learning large;propagating label information", "pdf_keywords": "supervised learning ssl;graph based ssl;learning ssl algorithms;graph based semi;semi supervised learning;ssl algorithm compactly;stores label distribution;semi supervised;label distribution node;millions nodes edges;based semi supervised;datasets million labels;learning ssl;supervised learning large;randomized data structure;novel graph based;state art graph;ssl algorithms mad;compactly stores label;containing millions nodes;ssl algorithms;label distribution;scaling graph based;novel graph;ssl algorithm;large number labels;millions nodes;ssl algorithms analytically;min sketch randomized;ssl algorithms requiring"}, "3bfa808ce20b2736708c3fc0b9443635e3f133a7": {"ta_keywords": "bottleneck graph neural;graph neural networks;networks gnns;graph neural;networks gnns shown;gnn based models;implications graph neural;gnns mechanism propagating;neighbors bottleneck graph;squashing gnn;neural networks gnns;networks;squashing gnn types;gnns shown effectively;extensively tuned gnn;gnns mechanism;gnn based;bottleneck graph;nodes relationships edges;tuned gnn based;susceptible squashing gnn;gnn types demonstrate;problem gnns mechanism;inherent problem gnns;bottleneck node;gnns fitting training;relationships edges;propagating information neighbors;nodes relationships;creates bottleneck node", "pdf_keywords": "gnns susceptible bottleneck;gnns demonstrate bottleneck;gnn models long;gnn architectures;gnn models;additional gnn architectures;limitation training graph;squashing gnns demonstrate;extensively tuned gnn;squashing gnns;training data gnns;neural networks bottleneck;tuned gnn models;training graph neural;problem squashing gnns;graph neural networks;gnns \ufb01tting long;adjacent layer gnn;gnns \ufb01tting;gnn architectures bene\ufb01t;graph neural;data gnns absorb;data gnns;popular gnns \ufb01tting;layer gnn;networks bottleneck;training graph;gnns demonstrate;gnns absorb incoming;tuned gnn"}, "9ab3622b3a801b90907f3ee399f881764db05d06": {"ta_keywords": "nodes kalman consensus;kalman consensus filtering;kalman consensus;estimates agent nodes;nodes kalman;linear attack learnt;neighboring nodes kalman;observations agent nodes;consensus filtering;attacker steer estimates;consensus filtering parameters;sensor observations agent;stochastic approximation algorithms;online stochastic gradient;stochastic gradient descent;linear attack;sensors external attacker;gradient descent stochastic;algorithms goal attacker;multiple agent nodes;parameters linear attack;constraint attack detection;stochastic approximation based;attack detection;attack learnt;attack detection probability;agent nodes;manipulating sensor observations;agent node computes;stochastic approximation", "pdf_keywords": "online linear attack;linear attack scheme;learning based attack;linear attack algorithm;attack algorithm distributed;attack algorithm karush;distributed estimation;attack algorithm;linear attacks;linear attack;popular linear attack;stochastic gradient descent;linear attacks provide;algorithm distributed estimation;linear attack used;attack schemes paper;distributed estimation design;distributed estimation using;kalman consensus;optimization parameters attack;class linear attacks;following kalman consensus;novel attack scheme;attack scheme;attack schemes;stochastic approximation spsa;parameters attack algorithm;attack design algorithm;kalman consensus \ufb01lter;attack detection probability"}, "f2e544c5333125ee30c1c34b08936b6ef87c97dd": {"ta_keywords": "dnn based spoken;implementations spoken language;speech recognition;development dnn based;example speech recognition;automated development dnn;deep learning;speech recognition forefront;spoken language processing;spoken language systems;deep learning research;development dnn;dnn based;implementations spoken;evolutionary algorithms;using evolutionary algorithms;systems using evolutionary;based implementations spoken;implementations resulted spoken;based spoken language;neural network research;neural network;systems example speech;automate tuning meta;forefront deep learning;neural network based;spoken language;language systems;automate tuning;evolutionary algorithms chapter", "pdf_keywords": ""}, "ecfac0d377db229d58bc88698ad3bfd4b384ef37": {"ta_keywords": "evaluation argument mining;argument mining;automatic argument identification;argument mining based;propose argument mining;argument mining used;argument identification helpful;argument identification;annotated arguments extensive;conferences annotated arguments;publications driven arguments;annotated arguments;automatic argument;arguments automatic argument;arguments automatic;peer review dataset;arguments used peer;peer reviewing;findings arguments used;difficult peer reviewing;scientific publications driven;science conferences annotated;driven arguments automatic;findings arguments;new peer review;peer review;scientific publications;meta reviewers;arguments extensive empirical;editors meta reviewers", "pdf_keywords": "evaluation argument mining;argument mining;argument mining based;propose argument mining;argument mining used;new argument mining;conferences annotated arguments;annotated arguments presented;annotated arguments;arguments present peer;peer reviews identi\ufb01cation;peer review dataset;present peer reviews;peer review;peer reviews;arguments presented;arguments presented new;new peer review;reviews identi\ufb01cation;actors peer review;arguments;peer review process;arguments present;conferences annotated;propose argument;presented new argument;science conferences annotated;meta reviewers;reviews identi\ufb01cation different;argument"}, "6ec6fa4e34200e13d80ee79b95d1cc6ec0f6b424": {"ta_keywords": "human interactive dialogues;embodied agents chat;interactive dialogues;human interactive;interactive dialogues complete;human human interactive;driven embodied agents;natural language interaction;teach task driven;tasks simulation robots;task driven embodied;agents chat;embodied agents;interactive;dialogues complete household;dialogues;simulation robots;language interaction;robots;engage natural language;teach task;instructions using conversation;language interaction understanding;robots operating human;interacts environment;simulation robots operating;using conversation;interacts;household tasks simulation;task driven", "pdf_keywords": "human interactive dialogues;interactive dialogues;human dialogs simulating;interactive dialogues complete;human dialogs;human human dialogs;agent task completion;dialogs simulating experience;models abilities dialogue;dialogs simulating;dialogues;embodied intelligence execution;intelligence execution dialog;embodied intelligence challenges;dialogue messages actions;agent task;tasks simulated;human interactive;agents teach data;tasks simulation commander;dialogue messages;task completion;natural language follower;robot complete tasks;dialogs;interleaves dialogue messages;dialogues complete;human human interactive;dialogues complete household;task communicates natural"}, "51b0609155e3a63afd1dd7dcc3034a5950f90ee0": {"ta_keywords": "disentangled influence audits;disentangled representations audit;affects classifier audited;influence audits detect;audited specifically disentangled;audit model predictions;influence audits;classifier audited specifically;predictions disentangling influence;classifier audited;influence using disentangled;experiments disentangled influence;disentangled influence;influence model predictions;disentangling influence using;outcomes feature influence;feature influence direct;feature influence expressed;feature influence;data features influence;model predictions disentangling;representations audit model;features feature influence;features influence;disentangling influence;audits detect proxy;features influence model;audits detect;representations audit;influenced proxy features", "pdf_keywords": "audit race gender;audits allow fairness;disentangled in\ufb02uence audits;fairness audit race;allow fairness audit;fairness audit;race gender feature;indirect in\ufb02uence audits;in\ufb02uence audits detect;audit race;feature speci\ufb01cally disentangled;disentangled representations provide;outcomes idea disentangled;audits theory experiments;disentangled representations;combined race gender;idea disentangled representation;in\ufb02uence audits theory;speci\ufb01cally disentangled representations;gender feature speci\ufb01cally;audits theory;classi\ufb01er audited;affects classi\ufb01er audited;audits detect;gender feature;disentangled representation learn;in\ufb02uence audits allow;race gender;in\ufb02uence audits;disentangled representation"}, "1a27c23453d3f718d854ac4b57dcf3e81ac51aa8": {"ta_keywords": "collected active learners;active learning persist;training dataset actively;active learning;training active learner;use active learning;dataset actively acquired;active learner;active learners;active learner iteratively;datasets collected active;training transferable datasets;active learning practice;datasets remain valuable;learning persist;dataset actively;benefits active learning;labeled datasets remain;training transferable;learning persist different;active learners paper;acquired dataset underlying;transferability datasets;transferable datasets collected;development labeled datasets;acquired dataset;transferability datasets collected;datasets collected acquisition;updated frequently labeled;transferable datasets", "pdf_keywords": ""}, "9c403ca58853fbb223f6e9fce446bb638f291692": {"ta_keywords": "entity mentions materials;named entity recognition;annotations 595 material;entity recognition task;annotating entity mentions;entity mention annotations;entity recognition;corpus entity mention;mention annotations;domain scientific nlp;annotating entity;new annotation approach;mentions materials;new corpus entity;annotations;annotation approach;annotation;annotating;annotation approach intended;text material science;consistently annotating entity;mentions consistently annotating;mention annotations 595;new annotation;corpus entity;scientific nlp;annotations new;annotations new annotation;consistently annotating;consistent annotations new", "pdf_keywords": ""}, "33e5e4b079535957d1275497f8870ea57762a03d": {"ta_keywords": "sentence attackability online;demonstrate sentence attackability;attackable sentences arguments;sentence attackability associated;analysis sentence attackability;sentence attackability;detect attackable sentences;attackable sentences;attackability online arguments;attacks argumentation identify;attacks argumentation;attackability online;reasons attacks argumentation;information attackability;information attackability analyze;useful information attackability;sentences arguments significantly;attackability analyze;attackability;argumentation identify;sentences arguments;attackability associated;argumentation identify relevant;attackability associated characteristics;sentences building;sentences building findings;online arguments;automatically detect attackable;reasons attacks;attackable", "pdf_keywords": "attackable sentences arguments;sentence attackability online;demonstrate sentence attackability;detecting attackable sentences;demonstrated sentence attackability;detect attackable sentences;sentence attackability associated;attackable sentences;attackability online arguments;analysis sentence attackability;sentence attackability;attacks argumentation identify;attacks argumentation;attackable sentences setting;attackable sentences comparably;reasons attacks argumentation;attackability online;useful information attackability;argumentation identify;information attackability;argumentation identify relevant;information attackability analyze;persuasion dialogue;sentences arguments;attackability analyze;online arguments demonstrate;persuasion dialogue \ufb01rst;attackability;skill persuasion dialogue;attack argument present"}, "81dfa45c568d7c1d9771ba2a1f07dad96558cff6": {"ta_keywords": "hidden markov kernel;markov kernel machine;hmms gaussian mixture;markov kernel;hidden markov models;pdfs hidden markov;conventional hmms gaussian;based hidden markov;gaussian mixture emission;gaussian mixture models;hidden markov;classification using kernel;conventional hmms;classification performance mixture;sequential pattern classification;mixture models gaussian;mixture emission pdfs;conventional hmms completely;models gaussian mixture;phoneme classification;kernel methods proposed;models hmms;classifier based hidden;markov models hmms;novel classifier sequential;gaussian mixture;classification derived kernel;compared conventional hmms;emission pdfs trained;pattern classification", "pdf_keywords": ""}, "be12a8d9ddb12c9ed292430c38d50093191dd442": {"ta_keywords": "fuzzy graph clustering;synset induction algorithm;graph clustering;graph clustering called;nodes clustering;nodes clustering soft;equidistant nodes clustering;clustering soft clustering;nodes clustering enc;soft clustering algorithm;soft clustering;synset induction;applied synset induction;clustering soft;clustering;clustering called;clustering enc equidistant;clustering enc;clustering algorithm;clustering called equidistant;algorithm fuzzy graph;algorithm applied synset;clustering algorithm applied;fuzzy graph;nodes community;task synset induction;nodes community reach;assumption nodes community;natural language processing;nodes", "pdf_keywords": ""}, "90db4ddb08df23a4c587e6136e66cb388311473b": {"ta_keywords": "filters learned users;learned users collaborative;transfer learned filters;learns filter documents;symbolic learning methods;collaborative learning;learns filter;filters learned;collaborative learning way;symbolic learning;learned filters;transfer learned;learning methods direct;direct transfer learned;conclude symbolic learning;learning methods diverse;training classifier learned;classifier learned;learned users;case collaborative learning;holds learning methods;learned classifiers;different learning methods;learning methods;exploit filters learned;learning methods use;learned filters setting;training classifier;learns;classifier learned classifiers", "pdf_keywords": ""}, "d409ff05d70f7b9787baf6431a84a178ad726e8d": {"ta_keywords": "inverse reinforcement learning;novel inverse reinforcement;inverse reinforcement;demonstrations environments constraints;constrained environments learning;reinforcement learning irl;learning demonstrations;environments learning demonstrations;learning demonstrations evaluate;constraints demonstrations enabling;constraint learning;reinforcement learning;demonstrations enabling agents;constraints demonstrations;use constraint learning;soft constraints demonstrations;demonstrations environments;reinforcement;constraints total reward;learning soft constraints;reward demonstrating agent;constraint learning method;constraints states actions;enabling agents quickly;offs demonstrations environments;environments learning;agent architecture;agents transfer knowledge;environments constraints explicit;agent architecture general", "pdf_keywords": "inverse reinforcement learning;learning constraints demonstrations;novel inverse reinforcement;inverse reinforcement;methods inverse reinforcement;reinforcement learning irl;general constraint learning;constraints demonstrations enabling;constraint learning;alternative decision field;use constraint learning;learning constraints;reinforcement learning;soft constraints demonstrations;alternative decision \ufb01eld;constraints demonstrations;demonstrations enabling agents;constraint learning method;aimed learning constraints;constraints demonstrations exhibiting;reinforcement;decision \ufb01eld theory;model human decision;multi alternative decision;decision processes altman;orchestrate competing objectives;reinforcement learning ng;decision field;decision making multi;decision making framework"}, "f16c0699a873b0209a370e8e6301b0189785c614": {"ta_keywords": "learning constrained dirichlet;verb clustering incorporating;constrained dirichlet process;verb clustering;active learning constrained;task verb clustering;clustering incorporating supervision;sampling active learning;dirichlet process mixture;constrained dirichlet;active learning approach;active learning;process mixture models;learning constrained;applied dirichlet process;dirichlet process;clustering incorporating;mixture models task;mixture models;introduce active learning;constraint selection employing;constraint selection;links constraints instances;learning approach constraint;based sampling active;sampling active;constraints instances;process mixture;approach constraint selection;clustering", "pdf_keywords": ""}, "363eb288abf76f7ab52d7789b30399b4b909dd5a": {"ta_keywords": "bribery schemes voting;optimal bribery schemes;bribery voting;bribery setting computationally;dependencies bribery voting;finding optimal bribery;optimal bribery;bribery voting cp;bribery schemes;traditional bribery problem;bribery setting;bribery problem;combinations parameters bribery;generalize traditional bribery;parameters bribery;inter dependencies bribery;dependencies bribery;bribery;schemes voting;traditional bribery;parameters bribery setting;schemes voting domains;vote inter dependencies;set variables voters;voting cp nets;voting domains;bribery problem account;voting domains candidate;variables voters;agents vote", "pdf_keywords": ""}, "0d6a4e45acde6f47d704ed0752f17f7ab52223af": {"ta_keywords": "dataset demonstrations crafting;learn quickly demonstrations;introduce dataset demonstrations;dataset demonstrations;descriptions actions generalization;demonstrations facilitate automatic;instructions unseen tasks;step human demonstrations;improve generalization reinforcement;human demonstrations form;level descriptions actions;generalization reinforcement;generalize unseen tasks;demonstrations crafting based;human demonstrations;demonstrations form natural;instructions action trajectories;actions generalization;demonstrations crafting;descriptions actions;trained agent;actions agent generated;actions agent;quickly demonstrations;generalization reinforcement learning;trained agent interpretable;generated natural language;human instructions improve;quickly demonstrations facilitate;hierarchical tasks", "pdf_keywords": "supervised reinforcement;improve generalization reinforcement;sparse reward reinforcement;dataset demonstrations crafting;generalization reinforcement;supervised reinforcement learning;dataset human demonstrations;demonstrations natural language;learn quickly demonstrations;combined supervised reinforcement;generalization reinforcement learning;reward reinforcement learning;reinforcement learning valerie;generalize unseen tasks;hierarchical tasks crafting;dataset demonstrations;quickly demonstrations model;reinforcement learning;introduce dataset demonstrations;hierarchical tasks;demonstrations crafting based;solve hierarchical tasks;demonstrations crafting;sparse reward;reward reinforcement;human demonstrations natural;human demonstrations;ef\ufb01ciently sparse reward;reinforcement;human instructions improve"}, "88b66f705a329da8292e7b8aa4bfe26de4759cfa": {"ta_keywords": "transduction grammar alignment;accurate machine translation;character based translation;machine translation words;machine translation possible;inversion transduction grammar;machine translation;based translation model;framework machine translation;phrasal inversion transduction;translation words substring;translation model;systems effectively translating;transduction grammar;phrase based mt;words substring alignment;translation model using;grammar alignment techniques;effectively translating unknown;substring alignment evaluation;effectively translating;transformation character strings;phrasal inversion;grammar alignment;words treating mt;translation achieve results;alignment techniques character;inversion transduction;translation words;substring alignment", "pdf_keywords": ""}, "a901185ee0710770420044cace33003109d478e3": {"ta_keywords": "questions rating systems;rating systems better;rating systems learn;rating systems;rating systems informative;illustrates rating systems;rating choosing;consider questions rating;rely rating systems;questions rating;rating choosing answer;illustrates rating;ratings;rating;design rating choosing;ratings highly inflated;study illustrates rating;rating overall;practice ratings highly;practice ratings;ratings highly;choices design rating;optimize design rating;rating overall study;design rating;design rating overall;choosing answer labels;rating second;quality market participants;manner practice ratings", "pdf_keywords": "designing informative rating;informative rating substantially;informative rating systems;rating systems learn;phrasing rating scale;ratings informative;ratings informative ratings;rating designs;ratings ratings informative;informative ratings;informative ratings obtained;rating systems;informative rating;rating systems evidence;rating systems applying;identify informative rating;select rating designs;rating designs apply;rating substantially improve;rating substantially;numeric rating systems;rely rating systems;rating scale;question phrasing rating;ratings ratings;choice rating scale;rating scale particular;ratings obtained;choices present rating;de\ufb02ated ratings ratings"}, "ee9f40f1c1e77b0b39b6e4a158208614fb4995c0": {"ta_keywords": "predict urban anomalies;urban anomaly detection;urban anomalies automatically;anomaly urban anomalies;urban anomaly;driven urban anomaly;urban anomalies;urban anomaly urban;research urban anomaly;urban anomalies presented;techniques urban anomalies;urban anomalies result;detect predict urban;anomaly urban;predicting anomalies happening;urban datasets;urban big data;predicting techniques urban;predict urban;anomaly detection prediction;data driven urban;urban datasets obtained;predicting anomalies;urban sensors event;types urban datasets;automatically alerting anomalies;anomaly detection;alerting anomalies;urban sensors;anomalies happening great", "pdf_keywords": "urban anomaly analytics;urban anomaly analysis;urban anomaly detection;anomaly detection urban;predict urban anomalies;urban anomalies automatically;urban anomaly;based urban anomaly;driven urban anomaly;research urban anomaly;urban anomalies;urban anomalies presented;used urban anomaly;techniques urban anomalies;anomaly analytics;anomaly detection prediction;types urban anomalies;urban anomalies traf\ufb01c;anomaly analytics survey;environment anomaly detection;anomaly detection;anomaly analytics subsequently;anomaly detection represent;urban big data;anomaly analysis frameworks;anomaly analysis;big urban data;anomaly individual anomaly;detect predict urban;urban data"}, "5bcbc4554a68b38ff4a22b848fb0817b809608b2": {"ta_keywords": "processor srx 503;processor srx;evaluation processor srx;srx 503;srx;processor;evaluation processor;503;evaluation", "pdf_keywords": ""}, "c159725940750adbad262ac946ce161bb68e41b5": {"ta_keywords": "attention based asr;speech recognition asr;e2e automatic speech;speech recognition;automatic speech;transformer attention based;automatic speech recognition;recognition asr sequence;performance rnn based;recognition asr;rnn based e2e;convolution e2e asr;performance rnn;better performance rnn;e2e asr alternative;attention simple model;transformer attention;rnn based;connectionist temporal classification;asr alternative architecture;asr alternative;rnn;e2e asr;asr sequence;asr sequence sequence;obtained transformer attention;attention simple;temporal classification convolution;based asr techniques;reverberant tasks", "pdf_keywords": "speech recognition asr;performance rnn based;performance rnn;speech recognition;convolution e2e asr;better performance rnn;rnn based e2e;automatic speech;automatic speech recognition;recognition asr endto;performance rnn performance;attention convolutional layer;e2e asr alternative;recognition asr;rnn performance;self attention convolutional;attention convolutional;e2e asr;rnn based;rnn;dynamic convolution e2e;transformer various asr;\ufb01eld automatic speech;asr alternative architecture;asr benchmarks;convolution e2e;various asr benchmarks;reverberant tasks propose;asr alternative;models mainstream asr"}, "ec6499842d3e51b7dda94f5d0620d6df5c1a1b6d": {"ta_keywords": "trained speech text;speech technology unwritten;language speech technology;speech technology;speech text;text speech;unwritten language speech;speech text text;utterance unwritten language;text text speech;speech technology unfortunately;pre trained speech;trained speech;representations regenerate speech;speech meaning representations;systems directly speech;text speech subsystems;speech bypassing need;learn speech meaning;speech bypassing;speech translated text;regenerate speech translated;speech used human;technology unwritten languages;language speech;regenerate speech;learn speech;directly speech;meaning speech bypassing;unwritten language", "pdf_keywords": ""}, "55faed1fbb1575ffa2609bdc4490586e30df441a": {"ta_keywords": "automatic translation evaluation;translation evaluation metrics;lingual question answering;machine translation evaluation;machine translations;automatic translation;manual machine translations;manual automatic translation;machine translations perform;translation evaluation;machine translation;question answering knowledge;answer questions language;translations perform clqa;translation results;cross lingual qa;question answering;answering knowledge bases;question answering qa;bases question answering;translations perform;answering qa systems;lingual qa clqa;metrics cross lingual;investigation machine translation;questions language based;lingual qa;translations;questions language;source cross lingual", "pdf_keywords": ""}, "e52115834ac7a529b1f4a7769dd538f143cf3eea": {"ta_keywords": "erasure codes distributed;general erasure codes;designing erasure codes;erasure codes impossibility;erasure codes;codes distributed storage;constructions piggybacking codes;reed solomon codes;piggybacking codes;bandwidth piggybacking codes;solomon codes;piggybacking codes low;solomon codes analyze;analyze piggybacking codes;piggybacking codes certain;codes distributed;codes analyze piggybacking;codes desirable properties;codes impossibility;codes impossibility results;codes low substriping;repair bandwidth piggybacking;achievable repair bandwidth;low repair bandwidth;repair bandwidth complexity;codes desirable;codes analyze;subcategories piggybacking codes;design codes desirable;repair schemes", "pdf_keywords": "general erasure codes;erasure codes impossibility;erasure codes;code repair schemes;erasure code repair;characterization piggybacking code;piggybacking code repair;constructions piggybacking codes;analyzing erasure code;erasure code;bandwidth piggybacking codes;piggybacking codes achieve;analyze piggybacking codes;piggybacking codes;codes analyze piggybacking;piggybacking codes low;general linear codes;piggybacking codes certain;code repair;linear repair schemes;mds array codes;piggybacking codes introduced;linear codes;piggybacking codes contributions;array codes additionally;solomon codes analyze;codes impossibility;codes impossibility results;reed solomon codes;piggybacking code"}, "777d7b4141c9ce163de99b747e94c8d1db12e11e": {"ta_keywords": "privacy deep learning;interpretable privacy deep;service interpretable privacy;privacy deep;interpretable privacy;machine learning services;receive machine learning;deep learning inference;based perturbation maximization;perturbation maximization;privacy;learning services cloud;deep learning;gradient based perturbation;service interpretable;services cloud;learning inference work;learning inference;perturbation maximization method;data models reveals;gradient based;problem gradient based;maximization method discovers;service provider;machine learning;cloud;models reveals information;learning services;service provider actually;accurate prediction", "pdf_keywords": ""}, "89e53f116ef732d0abe81ee2218fa862ddc5ddce": {"ta_keywords": "speech translation systems;translation text speech;speech processing toolkit;speech speech translation;functions speech translation;recognition machine translation;speech translation;automatic speech;speech translation provide;machine translation text;translation systems;implements automatic speech;machine translation;end speech processing;text speech;text speech functions;translation systems single;speech processing;training decoding pipelines;extraction training decoding;speech recognition machine;quick development speech;speech recognition;processing toolkit espnet;automatic speech recognition;toolkit espnet integrates;training decoding;toolkit espnet;espnet integrates;espnet integrates newly", "pdf_keywords": "speech translation toolkit;speech translation systems;speech processing toolkit;translation text speech;benchmark datasets espnet;translation toolkit;speech speech translation;processing toolkit espnet;toolkit espnet integrates;datasets espnet;implements automatic speech;toolkit espnet;speech translation;automatic speech;functions speech translation;espnet integrates;datasets espnet st;espnet integrates newly;translation systems;recognition machine translation;text speech;machine translation text;espnet;translation toolkit publicly;text speech functions;speech processing;machine translation;extraction training decoding;espnet espnet;present espnet"}, "6d654bab72d062d91f731331f16ea01d7cac0812": {"ta_keywords": "gendered topics tropes;genderedness trope tvtropes;bias narrative tropes;genderedness trope;biases use tropes;tropes relationship gender;score genderedness trope;gender bias narrative;tropes narrative elements;gender bias popular;media analyzing gender;narrative tropes;topics tropes;use tropes narrative;narrative tropes enable;topics tropes relationship;tropes associated;tropes narrative;tropes use popular;analyzing gender bias;use tropes;correlates types tropes;tropes use;gender bias;trope tvtropes dataset;trope tvtropes;tropes;highly gendered topics;gendered topics;tropes relationship", "pdf_keywords": "gender bias tropes;analysis tropes genderedness;gendered topics tropes;tropes genderedness reveals;genderedness trope tvtropes;tropes genderedness;genderedness trope;explicitly gendered trope;gendered trope;score genderedness trope;bias tropes collect;tropes relationship gender;bias tropes;analysis tropes;topics tropes;tropes commonly;topics tropes relationship;tropes commonly occurring;tropes associated;tropes various;correlates types tropes;gendered trope opposed;gender bias popular;tropes use;characters gender author;trope tvtropes dataset;author characters gender;analyze gender bias;highly gendered topics;computational analysis tropes"}, "e79d1206292bc5e67ba19737d87d4b2ea4a37105": {"ta_keywords": "learns subword tokenization;subword representations;bias learns subword;subword blocks learns;subword representations characters;subword tokenization algorithms;subword tokenization module;latent subword representations;subword tokenization;based subword tokenization;learns subword;subword tokenization end;rigid subword tokenization;learns latent subword;gradient based subword;par outperforming subword;candidate subword blocks;subword blocks;subword based models;outperforming subword based;introduce charformer deep;outperforming subword;charformer deep;subword based;latent subword;characters data driven;charformer deep transformer;models natural language;bias learns;subword", "pdf_keywords": "subword tokenization novel;subword tokenization;subword tokenization algorithms;subword tokenization module;based subword tokenization;subword tokenization gbst;subword tokenization enabling;subword representations;subword representations characters;rigid subword tokenization;latent subword representations;subwords directly characters;ef\ufb01ciency subword tokenization;gradient based subword;subword tokenization yi;learning latent subwords;learns latent subword;latent subwords directly;latent subwords;tokenization novel;subwords directly;novel lightweight tokenization;tokenization;tokenization algorithms;tokenization novel lightweight;character level representations;lightweight tokenization;introduce charformer deep;charformer deep;latent subword"}, "4cdd533963d8fb21fbf4bb3487bf6a6d60e14e93": {"ta_keywords": "cell image segmentation;segmentation method cell;segmentation using mrf;segmentation improved markov;image segmentation;segmentation;image segmentation improved;cell images;cell image;segmentation improved;improve segmentation;cell images using;segmentation nucleus;mucosal cell images;robust cell image;preprocessing cell images;accurate cell image;effectively improve segmentation;segmentation experiments;image segmentation using;segmentation method;image segmentation results;improve segmentation nucleus;images using markov;segmentation using;segmentation experiments performed;segmentation results;proposed method segmentation;segmentation nucleus binuclear;segmentation results method", "pdf_keywords": ""}, "396e942542904dd32d0d70daa39613e5a27cc059": {"ta_keywords": "stacked graphical learning;collective classification efficient;collective classification methods;collective classification;learning stacked learning;existing collective classification;online learning stacked;stacked learning;collective classification predicts;learning stacked;proposed collective classification;learning proposed collective;online stacked graphical;problem collective classification;stacked learning save;graphical learning;stacked graphical;graphical learning proposed;overhead stacked graphical;experimentally online stacked;graphical models learning;graphical learning gives;online stacked;memory problem collective;maintaining large graphs;classification efficient;large graphs;large streaming datasets;graphs related instances;classification efficient inference", "pdf_keywords": ""}, "d7851e80f6072991bc99e2157f05515564f894f4": {"ta_keywords": "online discriminative training;classification structured adaptive;structured adaptive regularization;conversion structured learning;discriminative training;classification adaptive regularization;adaptive regularization weight;discriminative training method;classification structured;multiclass classification adaptive;conversion model learning;regularization weight vectors;classification adaptive;online discriminative;binary classification structured;adaptive regularization;regularization weight;phoneme conversion;mira online discriminative;structured learning;multiclass classification;regularization;grapheme phoneme conversion;phoneme conversion model;structured learning problem;weight vectors robust;binary classification;overfitting;structured adaptive;vectors robust grapheme", "pdf_keywords": ""}, "254491f0d981fb5d796c374287d439d8d1967088": {"ta_keywords": "cardiometabolic risk children;supplementation markers cardiometabolic;markers cardiometabolic risk;cardiometabolic risk;effect vitamin supplementation;effect vitamin;vitamin;vitamin supplementation;vitamin supplementation markers;markers cardiometabolic;cardiometabolic;supplementation markers;risk children adolescents;adolescents meta analysis;supplementation;children adolescents meta;risk children;children adolescents;adolescents meta;clinical trials;randomized clinical trials;meta analysis randomized;meta analysis;adolescents;analysis randomized clinical;children;randomized clinical;trials;risk;meta", "pdf_keywords": ""}, "e68762a32ec91587d9761030fc75a8f5ee71c45b": {"ta_keywords": "unsupervised topic mixture;topic mixture modeling;topic mixture language;topic tracking model;topic mixture;allocation topic tracking;latent dirichlet allocation;approaches topic mixture;latent topic estimate;topic tracking;topic estimate;dirichlet allocation topic;unsupervised adaptation speech;modeling latent dirichlet;mixture language model;mixture modeling latent;observations unsupervised topic;unsupervised topic;accurate latent topic;dirichlet allocation;adaptation speech recognition;language model adaptation;latent topic;latent dirichlet;speech recognizer;mixture modeling;unsupervised adaptation;adaptation speech;mixture language;speech recognizer proposed", "pdf_keywords": ""}, "e66ade4e28d9f401277194ed8feea5c6e9f18253": {"ta_keywords": "similarity terrorist organizations;similarity terrorist groups;operational similarity terrorist;clusters terrorist groups;detecting clusters terrorist;terrorist groups sharing;similarity terrorist;terrorism intelligence monitoring;clusters terrorist;terrorist organizations;terrorist groups;counter terrorism intelligence;operational similarity organizations;terrorist groups critical;terrorism intelligence;insights counter terrorism;counter terrorism;similarity patterns decade;similarity organizations driven;groups sharing similar;patterns operational similarity;operational similarity;terrorism;similarity organizations;similarity patterns;demonstrate operational similarity;networks reveal patterns;clustering groups;attacks 1997 2018;deployed tactics attacked", "pdf_keywords": "similarity terrorist organizations;similarity terrorist groups;operational similarity terrorist;clusters terrorist groups;detecting clusters terrorist;similarity terrorist;terrorism intelligence monitoring;terrorist groups sharing;active terrorist organizations;terrorist organizations;counter terrorism intelligence;terrorism intelligence;clusters terrorist;terrorist groups;map terrorist behaviors;terrorism political violence;terrorist organizations considered;terrorist groups critical;terrorist behaviors;insights counter terrorism;terrorist organizations gian;networks map terrorist;counter terrorism;unitn published terrorism;terrorism political;terrorist behaviors terms;terrorism;operational similarity organizations;published terrorism political;published terrorism"}, "82c4be27b0803c08c56bba4352669c1230a3ea19": {"ta_keywords": "minimum repair bandwidth;repair bandwidth provide;regenerating codes efficient;storage capacity codes;total repair bandwidth;repair bandwidth;storage networks;repair regenerating codes;regenerating codes study;storage storage networks;storage nodes;storage networks node;storage nodes network;regenerating codes;regenerating codes framework;capacity codes assumptions;original regenerating codes;capacity codes;nodes repair;data reconstruction repair;distributed storage;repair bandwidth mbr;al regenerating codes;bound storage;methods distributed storage;distributed storage storage;bound storage capacity;minimum repair;upper bound storage;reconstruction repair resilient", "pdf_keywords": "repair regenerating codes;nodes repair;minimum repair bandwidth;repair bandwidth;regenerating codes ef\ufb01cient;regenerating codes;nodes repair \ufb01xed;abstract regenerating codes;regenerating codes framework;helper nodes repair;original regenerating codes;adaptive error resilient;repair bandwidth mbr;regenerating codes kaveh;storage networks;data reconstruction repair;storage networks node;repair simultaneously robust;methods distributed storage;repair resilient;resilient mbr repair;repair regenerating;storage storage networks;reconstruction repair resilient;mbr repair regenerating;schemes exact repair;repair simultaneously;repair resilient presence;networks node failures;bandwidth adaptive error"}, "4c42d6412c080fef23ad95b4469efe9cf321ae5d": {"ta_keywords": "sequence automatic speech;asr text speech;speech recognition asr;speech text data;unpaired speech text;automatic speech;text speech;semi supervised loss;speech tts models;leverage unpaired speech;data quantity speech;speech recognition;speech text;automatic speech recognition;speech text modalities;text speech tts;semi supervised training;new semi supervised;recognition asr models;unpaired speech;unsupervised semi supervised;speech tts;quantity speech text;recognition asr;semi supervised;rightarrow asr loss;supervised loss;supervised loss combining;sequence sequence automatic;text data outperform", "pdf_keywords": "unpaired speech text;integrating unpaired speech;speech unpaired text;leverage unpaired speech;speech text pipeline;information unpaired speech;speech text data;speech unpaired;unpaired speech;unpaired speech unpaired;performance seq2seq asr;seq2seq asr systems;seq2seq asr;speech text;rnn language model;data quantity speech;speech text modalities;rnn language;improve asr;improve asr performance;hypothesis unpaired speech;individually improve asr;integration rnn language;quantity speech text;loss tts asr;wsj librispeech corpus;unpaired text improve;asr performance combining;tts asr loss;speech text individually"}, "a714ca5254fb3cd7b06ead36d026c4eb154a7134": {"ta_keywords": "induce class discrimination;fairness algorithmic decision;study fairness algorithmic;fairness algorithmic;class discrimination;denote disparate learning;discrimination;study fairness;disparate learning;disparate learning processes;class discrimination finally;eliminate forms unfairness;equivalent disparate treatment;unfairness simultaneously introducing;discrimination finally argue;forms unfairness;disparate treatment;apply disparate treatment;unfairness simultaneously;policy notions prejudice;unfairness;forms unfairness simultaneously;discrimination finally;shape study fairness;fairness;reduce disparate;disparate treatment favor;apply disparate;exactly apply disparate;equivalent disparate", "pdf_keywords": "implement treatment disparity;treatment disparity undermining;disparity require treatment;exhibit treatment disparity;treatment disparity formally;treatment disparity;purposeful treatment disparity;disparity formally treat;require treatment disparity;treatment disparity naturally;induce class discrimination;treatment disparity experimental;disparity outcomes;impact disparity outcomes;disparity undermining;discrimination;discrimination iii general;disparity undermining policy;disparity outcomes differ;class discrimination;class discrimination iii;discrimination iii;ml impact disparity;impact disparity require;disparity require;impact disparity;exhibit impact disparity;disparity naturally achieve;parity purposeful treatment;disparity"}, "69320030be096e78380a097810554b648e7409c0": {"ta_keywords": "bayesian speaker clustering;preliminary speaker clustering;speaker clustering experiments;speaker clustering;bayesian speaker;fully bayesian speaker;clustering;clustering experiments using;clustering experiments;speaker;non parametric bayesian;algorithm uo dpmm;bayesian information criterion;method conventional bayesian;conventional bayesian;approximate bayesian approach;method approximate bayesian;bayesian manner;parametric bayesian manner;approximate bayesian;bayesian approach;carried preliminary speaker;realizes fully bayesian;bayesian information;bayesian approach results;conventional bayesian information;bayesian manner realizes;preliminary speaker;uo dpmm based;bayesian", "pdf_keywords": ""}, "0639cbb07ec3e03de7c8c1d828a90049c92cf5df": {"ta_keywords": "mn4 orthorhombic perovskites;orthorhombic perovskites asno3;spectroscopic properties mn4;perovskites asno3;perovskites asno3 ca;properties mn4 3d3;mn4 3d3 ion;ion orthorhombic perovskites;luminescence mn4 orthorhombic;sr luminescence mn4;luminescence mn4;properties mn4;orthorhombic perovskites;mn4 3d3;mn4 orthorhombic;deviation octahedral mn;octahedral mn bond;mn bond angle;3d3 ion orthorhombic;perovskites;casno3 higher srsno3;higher srsno3;sn4 o2 bond;mn4;srsno3;asno3 ca sr;mn bond;asno3;3d3 ion;asno3 ca", "pdf_keywords": ""}, "62924cef027a66a75b5465ebb7a926c06f95790f": {"ta_keywords": "domain adversarial algorithms;domain adversarial approaches;domain adversarial;standard domain adversarial;alignment domain adaptation;proposed domain adversarial;adversarial approaches;adversarial algorithms;distribution alignment domain;adversarial approaches consist;adversarial algorithms characterize;domain adaptation asymmetrically;adversarial;relaxed distribution alignment;datasets domain adaptation;distribution alignment new;domain adaptation;distribution alignment;domain adaptation addresses;synthetic real datasets;source training distribution;aligning source target;target distribution generating;drifts source training;training distribution;source target encodings;target distribution;adaptation asymmetrically relaxed;alignment domain;adaptation asymmetrically", "pdf_keywords": "alignment loss adversarial;loss adversarial domain;adversarial domain adaptation;optimizable adversarial training;loss adversarial;adversarial domain;domain adversarial algorithms;adversarial learning objectives;domain adversarial;adversarial training;standard domain adversarial;distribution alignment loss;optimizable adversarial;distance adversarial learning;relaxed distribution alignment;domain adaptation gives;new distance adversarial;distance adversarial;domain adaptation;adversarial learning;adversarial algorithms;adversarial;adversarial algorithms propose;distribution matching losses;distribution alignment translating;distribution alignment new;relaxed distribution matching;distribution alignment provide;distribution alignment;distribution shift target"}, "9b5cf607f9cd3eb5ef47d3597bb9360ea6034264": {"ta_keywords": "unfairness peer review;peer review biases;challenges peer review;source unfairness peer;unfairness peer;challenges peer;peer review;academia source unfairness;review biases;peer review discussed;review biases subjectivity;source unfairness;systemic challenges peer;young researchers tutorial;unfairness;wsdm audience;exciting wsdm audience;researchers tutorial discuss;peer;wsdm audience lead;biases subjectivity;researchers tutorial;biases subjectivity miscalibration;subjectivity miscalibration dishonest;biases;young researchers;computer science conferences;submissions various;wsdm;researchers", "pdf_keywords": ""}, "9a7a4f125d8016e0fad9f6f5e9e0bca4e38b0784": {"ta_keywords": "scalable probabilistic logic;probabilistic logic programming;stochastic logic programs;learning inference graphs;probabilistic logic;learning scalable inference;extends stochastic logic;stochastic logic;probabilistic logic order;order probabilistic logic;inference graphs;probabilistic logic called;parameter learning scalable;inference graphs using;efficient learning inference;logic programming structure;scalable inference;scalable probabilistic;learning inference;new scalable probabilistic;logic programming;parameter learning weight;logic programs;discovery parameter learning;logic programs slp;generated parameter learning;graph inference;supervised personalized pagerank;small graph inference;parameter learning", "pdf_keywords": ""}, "684e712f59f11d2bdc98be4c210824ab9e6f11f4": {"ta_keywords": "learning transferable relational;embeddings graphs trained;unsupervised learning transferable;transferable relational graphs;deep transfer;learning transferable;embeddings graphs;deep transfer learning;transferred different embeddings;tasks learned graphs;embeddings task;vectors task transferable;learned graphs;graphs trained;graphs trained including;different embeddings graphs;transfer learning;embeddings language pretrained;modern deep transfer;learned graphs generic;latent relational graphs;embeddings;embedding free;transferable relational;tasks word embeddings;relational graphs capture;embeddings task specific;transfer learning approaches;graphs generic transferred;units embedding free", "pdf_keywords": ""}, "22655979df781d222eaf812b0d325fa9adf11594": {"ta_keywords": "answering qa datasets;question answering qa;question answering;existing question answering;answering qa;hop question answering;answer questions diverse;hotpotqa dataset diverse;question answering introduce;answering introduce hotpotqa;qa datasets;dataset diverse explainable;documents answer questions;reasoning allowing qa;answer questions;dataset 113k wikipedia;level supporting facts;knowledge bases;113k wikipedia based;wikipedia based question;questions test qa;explanations answers;train qa systems;answering;existing knowledge bases;based question answer;extract relevant facts;diverse explainable multi;supporting documents answer;question answer pairs", "pdf_keywords": "question answering qa;question answering dataset;question answering;answering qa provides;task question answering;answering qa;reasoning allowing qa;answering dataset aimed;natural language hotpotqa;challenging latest qa;scale question answering;documents answer questions;systems supporting facts;factoid comparison questions;latest qa systems;factoid comparison;diverse natural language;questions test qa;existing knowledge bases;wikipedia based question;qa systems capable;knowledge schemas provide;extract relevant facts;knowledge schemas;answer questions diverse;compare various entity;level supporting facts;knowledge bases;reasoning diverse;qa systems supporting"}, "0f726fcd676baff957574b223b99fd84163ebe6e": {"ta_keywords": "stacked graphical learning;stacked graphical models;stacked graphical model;meta learning scheme;graphical learning efficient;graphical learning;relational datasets hyperlinked;methods stacked graphical;learning given relational;traditional machine learning;meta learning;algorithm stacked graphical;relational template stacked;relational datasets;describes algorithm stacked;stacked graphical;machine learning methods;graphical models;algorithm stacked;base learner;learning efficient;graphical learning given;datasets hyperlinked web;learning methods;machine learning;learning scheme;called stacked graphical;learning efficient capable;methods stacked;learning scheme called", "pdf_keywords": ""}, "4dfa9de9b3b2b222ddbdda934975bf608b8e1fda": {"ta_keywords": "collaborative dialogues;collaborative conversations solving;predict constructiveness conversation;conversation dialogue systems;collaborative dialogues explored;systems collaborative dialogues;collaborative conversations;conversation dialogue;dialogue systems research;constructiveness conversation dialogue;conversations solving;engagement chatbots research;user engagement chatbots;dialogues annotated;engagement chatbots;containing collaborative conversations;conversations solving cognitive;group conversations;dialogue systems;conversation;focused dialogues interlocutors;conversations;50 dialogues annotated;group dialogues;constructiveness conversation;dialogues;dialogues interlocutors largely;dialogues interlocutors;group conversations restaurant;focused dialogues", "pdf_keywords": "conversations release annotated;conversation dataset people;dialogues annotated;conversation dataset;generating deliberation utterances;deliberation cues collaborative;conversational dynamics deliberation;captures conversational dynamics;abridged conversation dataset;50 dialogues annotated;dialogues annotated furthermore;schema captures conversational;deliberation utterances;group deliberation probing;deliberation utterances work;collaborative conversations;cues collaborative conversations;group dialogues;captures deliberation cues;captures conversational;dialogues 14k utterances;containing collaborative conversations;annotated corpus capture;group dialogues 14k;deliberation dataset;conversations;deliberation probing;schema annotated corpus;captures deliberation;release annotated corpus"}, "bdf6ad58338279634d647447751442db8a6e2f77": {"ta_keywords": "error surfaces neural;convex error surfaces;surfaces neural networks;critical point neural;convergence loss;trivial neural network;convergence loss does;point neural networks;weights converge point;apparent convergence loss;networks deep learning;converge point weight;error surfaces locally;neural networks typically;weights converge;point neural;neural networks deep;networks deep;neural networks;network error surfaces;surfaces neural;error surfaces globally;deep learning;suggest weights converge;trivial neural;non convex error;neural network error;error surfaces;neural;partial training", "pdf_keywords": ""}, "88051a6dce3b67541d8096647da2f6d31daa9e9a": {"ta_keywords": "relation language models;knowledge graph relations;latent relation language;text relations experiments;text relations;incorporates knowledge graph;knowledge graph information;knowledge graph;language models;graph relations model;relation language;words document entities;given text relations;language modeling;based language models;language models previous;probability entity spans;improves language modeling;entity spans;entities occur knowledge;occur knowledge graph;relations context;relations model;document entities;language models lrlms;document entities occur;entity spans given;latent relation;graph relations;appropriate relations context", "pdf_keywords": "relation language models;latent relation language;knowledge graph context;conditional language modeling;knowledge graph relations;text relations;entities knowledge graph;language models;knowledge graph information;text relations work;incorporates knowledge graph;relational information entities;language modeling tasks;relation language;language modeling;knowledge graph;baseline language model;words document entities;improves language modeling;probability entity spans;introduction language models;graph relations model;language model;probability textual data;language models lms;entity spans;given text relations;relational information;lms relational information;nlp marginalization latent"}, "bc33c151a375d30d85a99d4e269185bad360b7bf": {"ta_keywords": "semitransparent organic photovoltaics;organic photovoltaics;organic photovoltaics employing;ternary strategy optical;photovoltaics employing synergy;photovoltaics;neutrality semitransparent organic;efficiency color neutrality;color neutrality semitransparent;photovoltaics employing;semitransparent organic;device efficiency color;employing synergy ternary;synergy ternary strategy;efficiency color;color neutrality;ternary strategy;synergy ternary;ternary;neutrality semitransparent;strategy optical engineering;strategy optical;optical engineering;optical;enhanced device efficiency;semitransparent;device efficiency;organic;color;neutrality", "pdf_keywords": ""}, "72ae4bba9aaa30dfba45f6e7e076952a76e2d751": {"ta_keywords": "conversational model;dialog corpus model;response ranking conversational;ranking conversational model;conversational model incorporates;dialog corpus;ranking conversational;conversational model function;present conversational model;ubuntu dialog corpus;role party conversations;lstm language model;conversations;conversational;party conversations;lstm language;party conversations experiments;conversations experiments ubuntu;memory lstm language;conversations experiments;paper present conversational;present conversational;term memory lstm;dialog;measured language model;traditional lstm model;lstm;short term memory;memory lstm;language generation model", "pdf_keywords": "lstm based conversation;conversational language model;conversation model incorporating;conversation models;conversational model;based conversation models;conversation model;based conversation model;conversational model incorporates;conversational language;lstm language model;present conversational model;lstm language;conversation models yi;role party conversations;summary propose lstm;conversational;propose lstm based;memory lstm language;introduce conversational language;term memory lstm;language model;propose lstm;language model incorporates;lstm based;response generation;short term memory;lstm;conversation;conversations"}, "9b52f250376e07c2caddb5f43b8db8b2f300bb51": {"ta_keywords": "big social data;social data analysis;social data;big social;data analysis;social;analysis;data;big", "pdf_keywords": ""}, "fd8b33299ce6ca81ce54e7d2de555a1a96ca96f1": {"ta_keywords": "speech recognition asr;discriminative models speech;discriminative models asr;asr automatic speech;models speech recognition;hmm acoustic models;speech recognition;recognition asr systems;recognition asr;automatic speech recognition;speech recognition overview;model hmm acoustic;models speech;structured discriminative models;automatic speech;acoustic models gram;model hmm;waveform structured discriminative;discriminative models;models gram language;classify structured sequence;gram language models;asr systems classify;structured discriminative;models asr automatic;markov model hmm;models asr;language models;applying structured discriminative;discriminative approaches", "pdf_keywords": ""}, "457e1c9476f08fa2c253982e3effcb364487073e": {"ta_keywords": "speech recognition iwslt;english speech recognition;naist english speech;speech recognition;recognition iwslt 2013;naist english;recognition iwslt;iwslt 2013;english speech;naist;iwslt;speech;english;recognition;2013", "pdf_keywords": ""}, "b80ce55fbb4aa427439009985c0ce28a34324dc6": {"ta_keywords": "nutritional supplements pregnantwomen;supplements pregnantwomen attending;supplements pregnantwomen;pregnantwomen attending antenatal;intake nutritional supplements;antenatal clinic;attending antenatal clinic;assessment intake nutritional;intake nutritional;nutritional supplements;pregnantwomen attending;pregnantwomen;supplements;nutritional;attending antenatal;antenatal;assessment intake;intake;clinic;assessment;attending", "pdf_keywords": ""}, "e23c5dafc718f9e55ccf7729ce2d2834b650540a": {"ta_keywords": "bayesian speaker clustering;speaker clustering method;speaker clustering based;speaker clustering;speaker clustering systems;novel speaker clustering;accuracy speaker clustering;speaker fully bayesian;bayesian speaker;fully bayesian speaker;speaker variability;intra speaker variability;utterance oriented dirichlet;dirichlet process mixture;utterances varied speaker;speaker variability successfully;mixture modeling proposed;mixture modeling;process mixture model;varied speaker;mixture model proposed;mixture model;scale mixture modeling;varied speaker speaker;hierarchically structured utterance;clustering based hierarchically;nonparametric bayesian manner;number utterances varied;intra speaker;speaker speaker", "pdf_keywords": ""}, "773e752ab6dc04b43aaf984bcbdd4895c9ab8c2f": {"ta_keywords": "continuous speech recognition;phoneme recognition;speech recognition task;speech recognition;discriminative training acoustic;timit phoneme recognition;decoding using distributed;based discriminative training;models large vocabulary;vocabulary continuous speech;phoneme recognition paper;discriminative training;training acoustic models;speech recognition using;employed wfstbased decoding;gram models;recognition using wfst;wfstbased decoding;linear classifier structured;using distributed perceptron;models gram models;wfstbased decoding using;large vocabulary continuous;distributed perceptron algorithm;decoding using;markov models gram;acoustic models large;decoding;distributed perceptron;scores decoding", "pdf_keywords": ""}, "510aef8370d82c4c4ec50de0f645f34f11e549a7": {"ta_keywords": "semicrfs dictionary hmms;protein entity recognition;dictionary hmms semicrfs;datasets dictionary hmms;entity recognition;recognizes phrases dictionary;semicrfs dictionary;dictionary information features;dictionary hmms technique;entity recognition using;task semicrfs dictionary;measure dictionary hmms;recall protein entity;best finding entities;crfs datasets improvement;dictionary hmms performed;recognizes phrases;dictionary hmms;dictionary information;hmms technique dictionary;datasets dictionary;crfs datasets;hmms semicrfs;finding entities actually;use dictionary information;recognition using dictionary;hmm recognizes phrases;finding entities;effective use dictionary;normal crfs datasets", "pdf_keywords": ""}, "7ddddea393c2cd70fe716e2dfc5d77daf58449c0": {"ta_keywords": "content influence twitter;influence twitter;ego people retweets;alters super influencers;people retweets alters;narratives super influencers;person content influence;influencers;content influence;super influencers introduce;influencers spread;retweets alters;super influencers;influencers introduce;alter influence;influence vary topics;super influencers spread;people retweets;single alter influence;influencers spread large;influencers introduce novel;observed misinformation researchers;influence;influence vary;alter influence vary;influence multiple topics;influence multiple;retweets;influence given topic;misinformation researchers platforms", "pdf_keywords": "ego people retweets;twitter using ego;people retweets alters;person in\ufb02uence twitter;examine twitter;examine twitter accounts;in\ufb02uence twitter using;trump examine twitter;social media granger;people retweets;in\ufb02uence twitter;twitter accounts retweets;accounts retweets identify;retweets alters;retweets identify;content in\ufb02uence twitter;accounts retweets;in\ufb02uence social media;twitter accounts;twitter;retweets;ego posts social;twitter using;in\ufb02uence ego posts;in\ufb02uence social networks;posts social media;analyzing person person;retweets alters case;social network;social network analysis"}, "1890775da6ba2627a5d6c17a639e2dca7cdc388d": {"ta_keywords": "microphone speech recognition;multi microphone speech;speech recognition everyday;microphone speech;multi microphone;speech recognition;microphone;recognition everyday environments;recognition everyday;speech;everyday environments;recognition;environments;multi;everyday", "pdf_keywords": ""}, "ccad27088b9098de4eaca8dc449b18766db4b3ab": {"ta_keywords": "style transfer paraphrase;transfer paraphrase generation;automatically generated paraphrase;paraphrase generation;generated paraphrase data;paraphrase generation finally;transfer systems paraphrases;transfer paraphrase;paraphrase generation problem;generated paraphrase;unsupervised style transfer;paraphrase data;style transfer inherently;paraphrase data existing;paraphrases inputs;systems paraphrases inputs;systems paraphrases;style transfer;paraphrases;style transfer systems;paraphrases inputs paper;style transfer modifying;pretrained language models;dataset 15m sentences;transfer modifying style;reformulating unsupervised style;sentiment reformulating unsupervised;world style transfer;transfer changes semantic;reformulate unsupervised style", "pdf_keywords": "style transfer paraphrase;transfer paraphrase generation;diverse paraphrasing pretrained;controlled paraphrase generation;paraphrasing pretrained;style transfer evaluation;paraphrase generation;paraphrasing pretrained language;automatically generated paraphrase;syntactically diverse paraphrasing;transfer paraphrase;diverse paraphrasing;output paraphrases crucial;generated paraphrase data;unsupervised style transfer;paraphrase generation problem;style transfer modeling;generated paraphrase;semantic preservation paraphraser;style transfer;existing style transfer;diversity output paraphrases;output paraphrases;paraphrases crucial effective;paraphrasing;style transfer systems;preservation paraphraser;style transfer method;paraphraser signi\ufb01cantly outperforms;controlled paraphrase"}, "703a8252585948a96f5815025f7f03d68033b8bf": {"ta_keywords": "training agent bots;bots self play;dialog agents self;agents self play;dialog agents;agent bots;self play autonomously;oriented dialog agents;agent bots user;play autonomously;self play human;approaches training agent;agents self;user bots self;learning game theoretic;autonomously explore api;learning game;training agent;bots self;agents;self play;task oriented dialog;reinforcement learning game;agent;api;play human;play autonomously explore;collaborative task oriented;results reinforcement learning;discovering communication strategies", "pdf_keywords": "bot learn strategies;play agent bot;training agent bots;bots self play;user bot learn;self play autonomously;bot develop communication;self play agent;bots obtain rewards;bot learn;self play task;agent bot;agent bots;play autonomously;dialog self play;learning game;learning game theoretic;agent bot user;play task oriented;user bot develop;rewards autonomously solving;user bots self;bot develop;rewards autonomously;agent bots user;play agent;allows bots obtain;obtain rewards autonomously;autonomously solving tasks;reinforcement learning game"}, "fa6c76d466fef633df51745bad85e991c371622c": {"ta_keywords": "multi microphone speech;microphone speech recognition;speech recognition everyday;microphone speech;multi microphone;issue multi microphone;speech recognition;microphone;recognition everyday environments;speech;recognition everyday;special issue multi;recognition;everyday environments;issue multi;multi;guest editorial special;editorial special;environments;special;editorial special issue;special issue;guest editorial;everyday;editorial;guest;issue", "pdf_keywords": ""}, "41a47363d261459c594525ef330e5fccaa8518a0": {"ta_keywords": "features authorship attribution;authorship attribution accuracy;approaches authorship attribution;authorship attribution;authorship attribution outperform;approach authorship attribution;affect authorship attribution;features authorship;authorship attribution task;features affect authorship;useful features authorship;approaches authorship;authorship;affect authorship;approach authorship;attribution accuracy;attribution outperform;author document based;existing approach authorship;attribution;attribution accuracy varying;identifying author;author document;attribution task;identifying author document;attribution task identifying;analysis individuals writing;individuals writing style;individuals writing;author", "pdf_keywords": ""}, "98e6197e21ae530cd33eeff144ee556c5cf91dc8": {"ta_keywords": "modeling cognitive tutors;cognitive tutors;environment cognitive tutors;cognitive tutors author;manually write cognitive;machine learning cognitive;learning cognitive modeling;simulated student automatically;cognitive model writing;trained cognitive;model writing cognitive;learning cognitive;writing cognitive model;modeling cognitive;write cognitive model;testing trained cognitive;automatically generates cognitive;learning agent;generates cognitive model;simulated student;cognitive modeling cognitive;cognitive modeling;trained cognitive scientist;authoring environment cognitive;cognitive model sample;learning detailed demonstration;learning agent called;called simulated student;writing cognitive;intelligent authoring", "pdf_keywords": ""}, "af679d69fcc1d0fcf0f039aba937853bcb50a8de": {"ta_keywords": "softmax attention;nested linear attention;softmax attention nested;sparse dense attention;linear attention;unified nested attention;approximates softmax attention;attention efficient sparse;attention operation linearly;linear attention functions;attention efficient;dense attention methods;attention nested linear;nested attention;sequence modeling neural;rank attention efficient;nested attention mechanism;attention nested;attention mechanism approximates;attention methods;dense attention;attention operation;attention functions yielding;attention functions;attention mechanism limited;attention;perform attention operation;sequence modeling tasks;transformer attention;neural machine translation", "pdf_keywords": "nested linear attention;softmax attention nested;nested attention functions;attention nested linear;regular softmax attention;softmax attention;nested attention;uni\ufb01ed nested attention;linear attention;attention nested;nested attention arxiv;uses nested attention;softmax attention transformer;linear attention functions;nested attention mechanism;approximates softmax attention;attention architecture;attention architecture luna;attention function luna;sequence modeling neural;luna attention architecture;attention functions approximate;attention functions;attention functions yielding;second attention function;luna attention;attention function;attention;attention mechanism approximates;attention transformer"}, "682e69be87f181edcf71800b54083595874d4ec6": {"ta_keywords": "speaker traits hierarchical;speaker trait prediction;predict persuasive speaker;related speaker traits;task speaker trait;personality traits speaker;speaker traits;speaker trait;trait prediction;traits speaker;traits speaker perceived;ability predict persuasive;subtasks predictions networks;trait prediction aims;learning hierarchical models;persuasive speaker speaker;speaker speaker trait;learning hierarchical;task hierarchical models;predict persuasive;traits hierarchical;trained subtasks predictions;computationally identify personality;persuasive speaker;traits hierarchical structure;improve learning hierarchical;networks trained subtasks;predictions networks;improving learning hierarchical;hierarchical models demonstrate", "pdf_keywords": "predict persuasive speaker;speaker traits hierarchical;ternary persuasiveness prediction;persuasive speaker table;ability predict persuasive;predict persuasive;persuasive speaker;persuasiveness prediction;persuasiveness prediction present;content credibility speaker;models ternary persuasiveness;speaker traits introduction;multimodal late fusion;related speaker traits;identifying speaker traits;speaker traits passion;classi\ufb01cation performance persuasiveness;credibility speaker;propose novel neural;speaker traits;emotion content credibility;speaker perceived worthy;layers neural;credibility speaker perceived;trust task hierarchy;novel neural architecture;persuasive;ternary persuasiveness classi\ufb01cation;traits passion speaker;ternary persuasiveness"}, "7c8314e6138ce968f3b9f3bc55d5461ffbbec4aa": {"ta_keywords": "optimization tourism route;tourism route based;optimization tourism;tourism route;research optimization tourism;genetic algorithm;based genetic algorithm;route based genetic;tourism;route based;optimization;algorithm;research optimization;route;genetic;based genetic;research;based", "pdf_keywords": ""}, "97883f37c62b4b0e52cdc31dea1a375597db3804": {"ta_keywords": "learned tasks masks;network learning mask;tasks masks learned;learning mask;learn binary masks;network quantization sparsification;fixed deep neural;fixed network learning;network quantization;learned tasks;single fixed deep;binary masks piggyback;tasks masks;concepts network quantization;masks learned;binary masks;quantization sparsification learn;masks learned end;performance learned tasks;deep neural;weights allows learning;filters piggyback adding;task underlying network;sparsification learn binary;mask certain weights;filters piggyback;deep neural network;fixed ability mask;allows learning large;network learning", "pdf_keywords": "convolutional network semantic;convolutional network;imagenet dataset;imagenet pre trained;fully convolutional network;convolutional network architecture;imagenet;domain imagenet dataset;network semantic segmentation;image domain imagenet;task imagenet;semantic segmentation starting;domain imagenet;imagenet dataset wikiart;task pixelwise segmentation;popular imagenet;imagenet pre;train fully convolutional;task imagenet variety;trained network;semantic segmentation;initial task imagenet;datasets network architectures;fully convolutional;imagenet variety network;performance popular imagenet;pixelwise segmentation;datasets network;pre trained network;popular imagenet pre"}, "e4c8447e56fc9cc3867087748acc4b259b9efe19": {"ta_keywords": "model text comprehension;model coreference relations;knowledge memory recurrent;text comprehension tasks;coreference relations text;coreference relations;memory recurrent neural;recurrent neural networks;explicit memory recurrent;memory recurrent;model coreference;linguistic knowledge memory;coreference;text comprehension;recurrent neural;grained entity information;external linguistic knowledge;fine grained entity;external linguistic;use model coreference;graphs explicit memory;comprehension tasks;memories utilize linguistic;linguistic knowledge explicit;learned representations;comprehension tasks achieve;model text;grained entity;relations text;use external linguistic", "pdf_keywords": "coreference text comprehension;model text comprehension;text comprehension tasks;model coreference text;model coreference relations;extract coreference relations;coreference relations;coreference relations text;text recurrent neural;coreference text;extract coreference;model coreference;coreference relations replacing;coreference;comprehension models mage;preprocessing extract coreference;text comprehension;recurrent units comprehension;recurrent neural networks;memory recurrent neural;tokens text recurrent;comprehension models;use model coreference;models mage rnn;comprehension tasks preprocessing;comprehension tasks;comprehension tasks achieve;explicit memory recurrent;text recurrent;units comprehension models"}, "be8d6a8d3dfe87a4d9171f25bf9a18d502498756": {"ta_keywords": "fuzzy graph clustering;graph unsupervised semantic;synonymy graph unsupervised;global graph clustering;graph clustering widely;watset meta algorithm;graph clustering;distributional thesaurus watset;induction synonymy graph;unsupervised semantic class;unsupervised semantic;graph clustering applications;thesaurus watset local;thesaurus watset;unsupervised semantic frame;meta algorithm fuzzy;semantic frame induction;semantic class induction;clustering widely;sense frame induction;graph unsupervised;watset meta;synonymy graph;unsupervised synset induction;analysis watset meta;clustering;algorithm fuzzy graph;fuzzy graph;synset induction synonymy;induction distributional thesaurus", "pdf_keywords": ""}, "bd1cf4279d834699db871e1451d289c49ff2b6de": {"ta_keywords": "step charts dance;rhythm based video;charts dance;steps dance;dance song chart;steps dance platform;charts dance dance;perform steps dance;rhythm based;audio track;step charts;audio track goal;dance;dance dance;rhythm;dance platform;dance platform synchronization;dance dance revolution;dance revolution ddr;screen step charts;dance revolution;step charts available;charts wish dance;learning choreograph;players perform steps;step chart;raw audio track;new step chart;features predict steps;audio features predict", "pdf_keywords": "learning choreograph deep;choreograph deep neural;musical onset detection;audio features predict;jointly learns convolutional;choreograph deep;recurrent convolutional neural;algorithm onset detection;onset detection;learns convolutional;onset detection statistical;learns convolutional neural;cnn representation recurrent;onset detection introduce;combine recurrent convolutional;recurrent convolutional;deep neural;recurrent neural network;lstm;task combine recurrent;jointly learns;pipeline learning choreograph;insights musical onset;learning choreograph performance;deep learning;recurrent neural;network cnn;learning choreograph;network rnn;model cnn"}, "cf46ecac1cb1bdae153be2b909ff3e313034ac9e": {"ta_keywords": "context social skills;social skills communication;social skills training;social communication skills;verbal communication training;skills communication;non verbal communication;social skills;communication training;existing social skills;communication skills types;communication skills;social communication difficulties;trouble social skills;skills communication greater;modality context social;skills types contextual;social communication;verbal behaviour incorporating;non verbal behaviour;context social;verbal communication;communication difficulties improve;training tools modality;improve social communication;people social communication;context existing social;skills training aid;skills training tools;modality contextual differences", "pdf_keywords": ""}, "ef59f05a30972742a714b8903848e4b5dfc5cdaf": {"ta_keywords": "interpretable machine learning;interpretable machine;field interpretable machine;consumers use cases;evaluation actionable taxonomy;machine learning iml;interpretable;methods evaluation actionable;machine learning;actionable taxonomy;useful use cases;use cases taxonomy;tool conceptualize;evaluation actionable;consumers work discover;targeted researchers methods;use cases;cases taxonomy serves;methods use cases;conceptualize;researchers consumers illustrated;serves tool conceptualize;cases taxonomy;objectives targeted researchers;field interpretable;persists technical objectives;technical objectives targeted;researchers methods high;researchers consumers;researchers methods", "pdf_keywords": ""}, "9b9ee9a25fc4d9f8ad22c2923c49b8d5d0b83356": {"ta_keywords": "extracting disambiguated hypernymy;hypernyms sets synonyms;disambiguated hypernymy relationships;improve hypernymy extraction;hypernymy extraction;hypernymy extraction evaluation;synonyms synsets;hypernyms sets;disambiguated hypernymy;synonyms synsets constructs;sets synonyms synsets;hypernymy relationships propagates;extracting disambiguated;propagates hypernyms sets;relationships propagates hypernyms;recognizes hypernymy relationships;method extracting disambiguated;sets synonyms;synonyms;propagates hypernyms;successfully recognizes hypernymy;hypernyms;hypernymy relationships standard;recognizes hypernymy;hypernymy relationships;unsupervised sense representations;patterns wiktionary datasets;relationships matching synsets;wiktionary datasets;synsets constructs embeddings", "pdf_keywords": "extracting disambiguated hypernymy;russian hypernymy extraction;extracting disambiguated;hypernymy extraction;hypernyms extracted unsupervised;disambiguated hypernymy relationships;method extracting disambiguated;hypernymy extraction task;hypernyms sets synonyms;noisy ambiguous hypernyms;propagate hypernyms sets;hypernyms extracted;disambiguated hypernymy;ambiguous hypernyms using;distributional semantics;relationships propagate hypernyms;hypernyms sets;disambiguation;propagate hypernyms;distributional semantics tested;unsupervised method disambiguation;hypernyms based;ambiguous hypernyms;synonyms synsets;synonyms synsets constructs;hypernymy relationships propagate;database hypernyms extracted;sets synonyms synsets;hypernyms using;postprocessing hypernyms based"}, "923ddc71f8a453c7995e97b0681a674224a5fc09": {"ta_keywords": "translation error analysis;accuracy machine translation;machine translation mt;machine translation error;machine translation;translation mt systems;identify errors mt;methods machine translation;analyzing mt errors;manual error analysis;human translators methods;translators methods proposed;translation error;translation mt;translators methods;errors mt;mt errors proposed;error analysis;errors mt output;error analysis used;identify errors;mt errors;error selection;human translators;analysis error selection;error selection methods;error analysis work;independently human translators;analysis error analysis;translations references translated", "pdf_keywords": ""}, "407eacc5ade80b54126c300b57b81f4b4f411487": {"ta_keywords": "quality machine translation;professional human translations;professional human translation;machine translation increased;human translations;human translation;human translations contained;machine translation;translation professional human;human translation number;language translation professional;translation increased remarkably;parity language translation;language translation;reference translations;translations;translation professional;quality human evaluation;creation reference translations;news translation showing;raters availability linguistic;translations contained significantly;investigation chinese english;human evaluation depends;translation increased;translation showing;translation number;news translation;translation;human evaluation", "pdf_keywords": "machine translation outputs;professional human translators;human translators conclusion;human translators;raters use linguistic;quality machine translation;machine translation;raters availability linguistic;translators conclusion compared;translation outputs;translations produced professional;translations produced;translations output;english translations output;outputs translations;translations output strong;outputs translations produced;human chinese english;human machine parity;compare outputs translations;translators;translations;reference translations;translators conclusion;human mt evaluation;assessing human machine;chinese english translations;creation reference translations;general assessing human;translation outputs highlight"}, "4bf1ea102e1eb1246929bb77c11ebbd6b6d27500": {"ta_keywords": "prototypes text generation;sparse prototypes text;learning sparse prototypes;sentence prototypes;learns emph sparse;library sentence prototypes;sentence prototypes modify;driven text generation;prototype driven text;text generation;prototypes text;learn prototype retrieval;text generation achieved;sparse prototypes;inference emph learn;learned prototypes;emph sparse prototype;text generation uses;automatically learns emph;interestingly learned prototypes;emph learn prototype;prototype generate;variational inference emph;learned prototypes able;propose novel generative;sparse prototype;novel generative;prototype retrieval;learning sparse;inducing prior prototype", "pdf_keywords": "learns sparse prototype;sentence prototypes;learning sparse prototypes;sparse prototypes text;prototypes text generation;library sentence prototypes;automatically learns sparse;sentence prototypes modify;prototype driven generative;learns sparse;driven text generation;generative models;novel generative model;novel generative;propose novel generative;driven generative models;prototype driven text;generative model automatically;automatically optimizing variational;sparse prototypes;generative;prototypes useful generating;language models;2020 learning sparse;learned prototypes;generative model discovers;amortized variational inference;prototypes text;variational inference;text generation"}, "93a55f3341aa70bb42c0f76b112e2e8da27b3df2": {"ta_keywords": "entrainment affects dialogue;dialogue based entrainment;dialogue like entrain;dialogue acts entrainment;ebdm dialogue based;selection ebdm dialogue;ebdm dialogue like;entrainment lexical choice;ebdm dialogue;entrainment lexical level;effect entrainment lexical;entrainment lexical;affects dialogue abstract;affects dialogue;dialogue abstract structural;dialogue abstract;dialogue like;dialogue based;dialogue;choice given dialogue;acts entrainment lexical;dialogue acts;response selection ebdm;example selection ebdm;given dialogue;entrain users similar;given dialogue acts;entrain users;entrainment analysis;manner response selection", "pdf_keywords": ""}, "bed0452305633791340f80cb0be02f46e4a34b0d": {"ta_keywords": "voice conversion challenge;voice conversion;approach voice conversion;baseline voice conversion;voice conversion vc;sequence baseline voice;models convert speaker;generate voice;transcribe input speech;convert speaker identity;transcriptions generate voice;generate voice target;speech processing toolkit;target text speech;convert speaker;end speech processing;voice target text;input speech automatic;using transcriptions generate;speech tts model;automatic speech;speech automatic;speech recognition asr;text speech tts;speech automatic speech;voice target;using transcriptions;text speech;speech tts;input speech", "pdf_keywords": "voice conversion challenge;baseline voice conversion;approach voice conversion;voice conversion;sequence baseline voice;voice conversion vc;seq2seq baseline voice;transcribe input speech;generate voice;speech processing toolkit;transcriptions generate voice;generate voice target;end speech processing;speech recognition asr;input speech automatic;target text speech;speech tts model;using transcriptions generate;voice target text;text speech tts;baseline voice;automatic speech;sequence sequence baseline;speech automatic;conversion challenge vcc;speech automatic speech;speech processing;e2e speech processing;conversion challenge 2020;input speech"}, "ce97452d031a1a156212f038bab6f47a51575236": {"ta_keywords": "stance way recognition;prosodic features boosting;detection stance;detection stance way;text stance taking;stance taking behavior;recognition stance;way recognition stance;exclusively text stance;stance taking;stance taking activity;lexical speaking style;speaking style prosodic;behavior polarity stance;conversational speech designed;style prosodic features;features speaking style;polarity stance;corpus spontaneous conversational;annotated strength stance;people stances;conversational speech;stances;spontaneous conversational speech;text stance;recognition stance strength;speaking style;new annotated corpus;stance;prosodic features yielding", "pdf_keywords": ""}, "995f4e670c0cdcd5afdef08719c2528a682bff05": {"ta_keywords": "asr decoder fast;speech translation model;speech recognition asr;autoregressive nar decoding;faster decoding;asr decoder teacher;nar decoding based;recognition asr decoder;nar decoding;achieved faster decoding;asr decoder;fast multi decoder;faster decoding speed;intermediate automatic speech;asr decoder states;decoder fast;high translation quality;transformer asr decoder;followed asr decoder;decoding speed na\u00efve;recognition asr;asr decoder masked;speech recognition;decoder fast md;speech translation;automatic speech;masked language model;end speech translation;mismatch asr decoder;translation model", "pdf_keywords": "faster decoding;nar decoding based;achieved faster decoding;speech recognition asr;fast multi decoder;autoregressive nar decoding;speech translation model;faster decoding speed;nar decoding;recognition asr decoder;asr decoder;asr decoder states;asr machine translation;decoding speed na;asr decoder reduce;asr decoder masked;followed asr decoder;multi decoder md;transformer asr decoder;intermediate automatic speech;high translation quality;mismatch asr decoder;multi decoder;decoder masked hi;cpu comparable translation;asr decoder teacher;decoding speed;multi decoder end;masked language model;recognition asr"}, "e2198b039ee5bfa233cf06e65f26a9f3233ada9f": {"ta_keywords": "dialogue act selection;dialogue act entrainment;entrainment dialogue act;word dialogue act;measures entrainment dialogue;switchboard corpus comparing;entrainment dialogue;dialogue participants;dialogue act word;dialogue acts word;word dialogue;switchboard corpus;dialogue act;specific dialogue acts;dialogue participants converge;using switchboard corpus;dialogue speech speaker;speech different speakers;choice specific dialogue;act word selection;dialogue speech;comparing speech different;comparing speech;dialogue acts;speech different;speech speaker different;corpus comparing speech;acts word dialogue;dialogue;selection observable dialogue", "pdf_keywords": ""}, "29da62b3f8aed3fe98b3f02bbfd436dd8e65a532": {"ta_keywords": "optimal csma;optimal csma experimentation;optimal csma despite;evaluation optimal csma;optimal csma denote;throughput based utility;guarantees performance protocols;utility wireless networks;performance protocols;maximize throughput;maximize throughput based;networks message passing;csma experimentation;approach maximize throughput;performance protocols evaluation;throughput;wireless networks message;wireless networks;protocols;channels drive protocol;protocol;throughput based;csma despite theoretical;protocols evaluation;message passing synchronization;protocols evaluation real;protocol state extreme;networks message;real networking scenarios;protocol state", "pdf_keywords": ""}, "4264599665522594d9ecb521dd2e1d002e85a961": {"ta_keywords": "computing fair matchings;matching reviewers papers;fair matchings;automatically matching reviewers;fair matchings approximately;matching reviewers;produce fair matchings;fair matchings 2x;fairness respect reviewer;fairflow computing fair;directly optimize fairness;computing fair;optimize fairness;algorithms fairir fairflow;fairness formulation paper;fairness formulation;paper matching algorithms;relaxation local fairness;matchings approximately optimize;local fairness formulation;common paper matching;novel local fairness;fairness formulation employs;new algorithms fairir;faster matching algorithms;fairflow computing;algorithms fairir;matchings 2x efficient;valid matching provably;matching algorithms", "pdf_keywords": "computing fair matchings;fairness standard matching;fairflow improve fairness;fair matchings;algorithms optimize fairness;optimize fairness fairir;fair matchings approximately;optimize fairness;fairflow computing fair;fairness constraints ensure;fairness formulation paper;fairness respect reviewer;fairness constraints;local fairness constraints;improve fairness standard;computing fair;fairness formulation;fairness formulation employs;competitive fairness capable;local fairness formulation;fairness capable evenly;relaxation local fairness;algorithms fairir fairflow;fairflow achieves;improve fairness;fairflow computing;achieves competitive fairness;competitive fairness;fairness fairir achieves;fairflow achieves competitive"}, "1578fba4a2b2ba819986e32c7da6ebbaf9aacf41": {"ta_keywords": "context morphology task;context morphology;crosslinguality context morphology;treebanks;sequence 107 treebanks;morphological analysis lemmatization;107 treebanks approach;treebanks approach task;treebanks approach;107 treebanks;2019 task morphological;task morphological analysis;morphological analysis;morphological;task morphological;syntactic description token;morphology task;hierarchical neural conditional;morphology task requires;syntactic description;morphology;lemmatization context cmu;hierarchical neural;syntactic;description token sequence;sigmorphon 2019 task;task hierarchical neural;morpho syntactic description;neural conditional random;lemmatization context", "pdf_keywords": "model contextual morphological;contextual morphological;contextual morphological analysis;predicting morpho syntactic;grained morphological feature;grained morphological;morphological analysis shared;morphological feature incorporating;introduction morphological;introduction morphological analysis;multi lingual transfer;multi lingual;entity recognition;neural model contextual;morphological;machine translation;morphological analysis;including machine translation;morphological feature;coarse grained morphological;entity recognition gu;multiple related languages;propose hierarchical neural;named entity recognition;lingual;languages share similar;related languages;hierarchical neural conditional;hierarchical neural;task predicting morpho"}, "6e2e7df21a5b5457ea4167133a40bc729028250d": {"ta_keywords": "passage ranking datasets;passage ranking development;document passage ranking;neural ranking;modern neural ranking;neural ranking stack;passage ranking;queries passage ranking;ranking datasets leaderboards;ranking datasets;modern neural rankers;neural rankers;ranking stack judged;ranking;preference judgments pools;information retrieval tasks;core information retrieval;ranking stack;preference judgments item;retrieval tasks;ranking development;leaderboards;neural rankers test;retrieval tasks including;information retrieval;datasets leaderboards;preference judgments;observation employed crowdsourced;judged relevant item;leaderboards particular", "pdf_keywords": "document passage ranking;passage document ranking;passage ranking;passage ranking development;document ranking;queries passage ranking;passage ranking ms;document ranking allowing;passage ranking leaderboard;core informational retrieval;neural ranking;informational retrieval tasks;core information retrieval;information retrieval tasks;retrieval tasks;neural ranking stack;ranking;near passage ranking;retrieval tasks including;retrieval tasks adhoc;modern neural ranking;information retrieval;pooling sparse labels;retrieval;informational retrieval;leaderboard pooled document;ranking stack judged;ranking leaderboard;ranking allowing;sparse labels"}, "80257b7d02ad4d6a762ebc0d7f1560e0ef182354": {"ta_keywords": "polite sentences preserving;content politeness transfer;politeness transfer tasks;automatically labeled politeness;politeness transfer tag;politeness transfer;politeness transfer involves;converting non polite;source content politeness;labeled politeness;content politeness;task politeness transfer;polite sentences;polite sentences polite;sentences polite;sentences preserving;politeness encourage benchmark;non polite sentences;labeled politeness encourage;sentences polite sentences;sentences preserving meaning;new task politeness;politeness;style transfer;style preserving;generates sentence target;politeness encourage;task politeness;transfer tag generate;style preserving source", "pdf_keywords": "politeness transfer task;datasets politeness transfer;task politeness transfer;task politeness;transfer task politeness;tasks datasets politeness;tasks like politeness;politeness transfer;sentence polite process;speci\ufb01cally politeness domain;datasets politeness;datasets speci\ufb01cally politeness;concept politeness 1st;politeness domain;notion politeness widely;notion politeness;concept politeness;attention notion politeness;subtle concept politeness;politeness widely;make sentence polite;politeness domain highlighting;polite process;politeness 1st;politeness widely accepted;sentence polite;speci\ufb01cally politeness;politeness;human evaluations grammaticality;polite process highlight"}, "09093e29b1f705bb7a68ea2e9240b3f122efe92b": {"ta_keywords": "stereo input speech;sparseness based blind;blind source separation;input speech recognition;stereo input;speech recognition;recognition using sparseness;using sparseness based;sparseness based;source separation;input speech;based blind source;speech recognition using;using sparseness;stereo;blind source;sparseness;based blind;blind;recognition;input;recognition using;separation;speech;source;based;using", "pdf_keywords": ""}, "3edfccbe6adf18f5263cd2adf3d977bbc5811e0b": {"ta_keywords": "speech recognition e2e;trained e2e asr;neural text encoder;e2e asr encoder;augmentation method attention;acoustic features translation;automatic speech;attention based end;recognition e2e asr;end automatic speech;faster attention learning;speech recognition;attention learning;data augmentation end;automatic speech recognition;novel data augmentation;asr encoder sequence;text encoder model;asr encoder;text encoder;translation build neural;text paired speech;e2e asr utilizing;e2e asr;data augmentation;encoder sequence characters;sampling e2e asr;encoder sequence;encoder;trained e2e", "pdf_keywords": "attentionbased e2e asr;attention based e2e;e2e asr encoder;neural text encoder;attentionbased e2e;trained e2e asr;e2e asr decoder;text decoder e2e;neural textto encoder;decoder e2e asr;augmentation method attentionbased;augmentation method attention;attention based end;recognition e2e asr;speech recognition e2e;sequence characters translation;translation built neural;text encoder;asr encoder sequence;e2e asr;encoder sequence characters;text encoder model;asr encoder;e2e asr utilizing;decoder sequence characters;method attentionbased e2e;translation build neural;based e2e asr;asr decoder sequence;e2e asr models"}, "e6aaac94df717786a467d057cb2157b9d49f0974": {"ta_keywords": "convergent opponents regret;opponents regret algorithms;regret algorithms;regret arbitrary adversarial;achieve regret arbitrary;regret algorithms created;equilibrium individual regret;regret variationally stable;regret arbitrary;optimistic mirror descent;algorithm measured regret;game theoretic guarantees;regret variationally;regret induced sequence;guarantee social regret;opponents regret;individual regret variationally;regret policies based;regret policies;converges nash equilibrium;response convergent opponents;optimistic mirror;zero sum games;achieve regret;arbitrary adversarial opponents;play converges nash;measured regret;convergent opponents;variationally stable games;converges nash", "pdf_keywords": "games optimal regret;continuous games optimal;learning continuous games;online learning variationally;algorithms online learning;optimal regret bounds;games optimal;online learning continuous;adaptive learning continuous;optimistic dual averaging;regret bounds convergence;adaptive algorithms online;optimal regret;speci\ufb01c adaptive learning;learning variationally stable;result online learning;convergence nash equilibrium;variationally stable games;online learning;convergence nash;continuous games;player speci\ufb01c adaptive;optimistic mirror descent;adaptive learning;adaptive learning rate;bounds convergence nash;variationally stable game;2021 adaptive learning;methods optimistic dual;continuous games solely"}, "efaf07d40b9c5837639bed129794efc00f02e4c3": {"ta_keywords": "continuous representations authorship;representations authorship attribution;authorship attribution;representations authorship;authorship attribution contrast;authorship;learns continuous representations;representations gram features;feature representations model;continuous representations gram;feature representations;using continuous representations;continuous representations;representations model learns;network jointly classification;discrete feature representations;gram features neural;features neural network;gram features;features neural;jointly classification;representations gram;representations model;neural network jointly;representations;attribution;learns continuous;jointly classification layer;model learns continuous;art datasets producing", "pdf_keywords": ""}, "9d9159026023f21e633f84fd61f3efad2e410214": {"ta_keywords": "logic embeddings;logic embeddings matrix;learn embeddings formulas;logic embeddings comparing;order logic embeddings;factorization learn embeddings;embeddings formulas;learning order logic;embeddings order logic;knowledge base completion;learn embeddings;inference formulas columns;embeddings formulas map;matrix inference formulas;training examples inference;generate plausible inference;embeddings matrix factorization;matrix factorization learn;factorization learn;relation extraction knowledge;examples inference formulas;inference formulas facts;plausible inference formulas;extraction knowledge base;inference formulas;grounded proof graphs;knowledge base;training examples rows;formulas facts build;binary matrix inference", "pdf_keywords": ""}, "46f66dd37e6366ce102cfd97e718947151d5b1eb": {"ta_keywords": "perception fake news;news environment fake;fake news detection;news environment perception;differentiate fake news;observe news environment;news environment;news detection crucial;news detection;environment fake news;news environment propose;news fabrication fake;news environment represents;propose news environment;external news environment;fake news designed;news fabrication;signals news posts;inspiration fake news;information external news;observe news;news posts;news real;nep fake news;news posts zoom;fake news fabrication;news real ones;news post zoom;news detection capture;zoom observe news", "pdf_keywords": "fake news detectors;fake news detection;news data fake;perception fake news;data fake news;news fabrication fake;fake news online;abstract fake news;spread fake news;detection news;misinformation social media;fake news fabrication;news detectors;detection news environment;dissemination misinformation social;news detection qiang;fake news designed;mainstream news data;news detection crucial;news detection news;news detectors introduction;inspiration fake news;preventing dissemination misinformation;dissemination misinformation;fabrication fake news;data fake;news fabrication;news detection;news online social;social media experiments"}, "8122eaeb63098e94416108df918c9669e9105e65": {"ta_keywords": "replication ec cache;latency cluster cache;cluster cache uses;cluster cache;popularity ec cache;ec cache improves;cached replicas;cache load balanced;selective replication ec;ec cache;memory ec cache;ec cache does;cache uses;ec cache load;cached replicas object;cache employs erasure;reads ec cache;ec cache employs;cache improves load;caching;replication ec;number cached replicas;cache improves;memory object caching;object caching;cache;caching meet performance;selective replication;replication commonly used;data intensive clusters", "pdf_keywords": ""}, "8d64be0d3bb2650ff99a4c1ae8049eb5fece27a1": {"ta_keywords": "emotional speech recognition;features emotional speech;speech recognition bottleneck;bottleneck features emotional;speech using bottleneck;emotional speech using;emotional speech;speech recognition;layer automatic speech;speech recognition asr;recognition bottleneck features;using bottleneck features;features bottleneck features;automatic speech recognition;bottleneck features deep;bottleneck features;features bottleneck;features deep neural;features emotional;essential features bottleneck;emotion adaptation;recognition bottleneck;used emotional speech;emotion adaptation model;features represent phoneme;bottleneck features represent;automatic speech;extract features bottleneck;speech recognition confirm;compared emotion adaptation", "pdf_keywords": ""}, "f8f17f32e651840531276423c7196856d27bcdd0": {"ta_keywords": "semantic web search;swsnl semantic web;swsnl semantic;semantic web;web search using;web search;semantic;search using natural;search using;natural language;using natural language;search;swsnl;web;language;using natural;natural;using", "pdf_keywords": ""}, "ee7af49291c030a3e29ad7a9cb5c1975d1b644f4": {"ta_keywords": "nutritional supplements pregnant;supplements pregnant women;supplements pregnant;intake nutritional supplements;assessment intake nutritional;intake nutritional;antenatal clinic;attending antenatal clinic;nutritional supplements;supplements;nutritional;women attending antenatal;pregnant women attending;attending antenatal;pregnant women;antenatal;assessment intake;pregnant;intake;women attending;clinic;women;assessment;attending", "pdf_keywords": ""}, "1cbb43b4d7f79d986a4a78ad3b53368c49e496ee": {"ta_keywords": "speech recognizer;speech recognizer using;speech recognition;automatic speech recognition;art speech recognizer;feature transformations utterance;utterance based adaptation;automatic speech;calls automatic speech;speech recognition systems;transformations utterance based;asr channel input;trained recognizer;asr channel;proposed asr channel;drnn based feature;input feature enhancement;reverb challenge calls;recognizer complementary improved;channel feature enhancement;channel input feature;training results reverb;channel audio;room acoustics generalized;adaptation discriminative training;network drnn based;clean trained recognizer;discriminative training results;discriminative training;reverb challenge", "pdf_keywords": ""}, "d0fbae81d870bbfb34430654f70fd6a21e8bd1cc": {"ta_keywords": "coreference annotations;coreference annotations extracted;uses coreference annotations;reading comprehension model;comprehension model improves;coreference;entity mentions;uses coreference;layer uses coreference;mentions entity;multiple mentions entity;information multiple mentions;nlp require;reading comprehension;entity mentions belonging;mentions entity far;connect entity mentions;comprehension model;nlp require aggregating;annotations extracted external;problems nlp require;ai tasks large;text present recurrent;cluster problems nlp;problems nlp;annotations;coreferent dependencies;annotations extracted;multiple mentions;mentions belonging", "pdf_keywords": "comprehension model improves;reading comprehension model;tackling reading comprehension;reading comprehension problems;reading comprehension;nlp build;powerful reading architecture;goal nlp build;ai tasks large;comprehension model;nlp build systems;recurrent layer bias;mentions entity experiments;adapt standard rnn;rnn layer;rnn layer introducing;mentions entity;standard rnn layer;annotations;reading architecture;reading;recurrent layer;nlp;standard rnn;ai tasks;rnn;annotations produced;goal nlp;use annotations;state art reading"}, "071216d944bcd2f05deafdb94e657167cce148d9": {"ta_keywords": "recognizing personal names;personal names email;names email;recall precision;recall precision tradeoff;learned sequential classifier;names email newswire;classifier change recall;personal names;change recall precision;task recognizing personal;change recall;recall;recognizing personal;sequential classifier;email newswire text;learned sequential;existing learned sequential;sequential classifier change;recognizing;classifier;tweaking existing learned;names;task recognizing;email;email newswire;text;sequential;newswire text;evaluated task recognizing", "pdf_keywords": ""}, "1ebf54c0a8b38e8c26ed857cb9d4e565a8f17f17": {"ta_keywords": "measure entity similarity;entity similarity;entity similarity viewed;email corpus;walk based similarity;enron email corpus;email corpus corpora;personal information graph;person disambiguation addressed;links relational;threading person disambiguation;links relational semi;similarity metric preferable;person disambiguation;corpus corpora tasks;similarity viewed tool;reranking improve graph;graph walk based;based similarity metric;disambiguation addressed;search graph graphs;information graph;disambiguation addressed uniformly;graph walks;similarity metric;graph walk performance;entities directed;improve graph walk;corpora tasks;search graph", "pdf_keywords": ""}, "72a5c01afe276d06ca9179e24b1c925e206454f3": {"ta_keywords": "recommendations knowledge graphs;produce recommendations explanations;entity based recommendations;recommendations knowledge;based recommendations knowledge;recommendations explanations;knowledge graph entities;explanations content reviews;items knowledge graph;personalized pagerank;knowledge graph;personalized pagerank procedure;explainable entity based;using personalized pagerank;knowledge graphs methods;generate explanations content;knowledge graphs;pagerank;explainable entity;leveraging external knowledge;pagerank procedure;pagerank procedure produce;based recommendations;explanations content;generate explanations;knowledge graphs paper;content reviews;external knowledge form;ranks items knowledge;external knowledge", "pdf_keywords": "produce recommendations explanations;explanations jointly ranking;predictions recommender systems;predictions recommender;personalized pagerank;ranking corresponding movies;recommendations explanations;using personalized pagerank;recommender systems;personalized pagerank procedure;accuracy predictions recommender;recommendations explanations personal;recommender systems important;pagerank;explanations personal agent;recommender;pagerank procedure;knowledge graph;ranking corresponding;ranked list entities;ranking;leveraging external knowledge;pagerank procedure produce;items knowledge graph;ranked list;list entities explanations;entities explanations jointly;ranks items knowledge;knowledge graphs;jointly ranking"}, "f800f60db4427a51e564f1b875ae01d2c642fdce": {"ta_keywords": "data repair transfer;transfer data repair;repair bandwidth;data repair;codes distributed storage;bandwidth required repair;possible repair bandwidth;repair bandwidth case;node regenerating codes;distributed storage;repair replacement node;repair regenerating codes;distributed storage like;code perform repair;functional repair tradeoff;repair mere transfer;repair transfer;regenerating codes possess;repair tradeoff data;storage bandwidth tradeoff;tradeoff exact repair;exact repair code;explicit exact repair;minimum possible repair;functional repair;repair transfer second;data recovery;regenerating codes;exact repair regenerating;functional repair exact", "pdf_keywords": "distributed storage codes;storage codes repair;codes distributed storage;network regenerating codes;codes repair transfer;bandwidth required repair;repair bandwidth;possible repair bandwidth;recovery subset nodes;distributed storage;repair replacement node;repair bandwidth case;codes repair;regenerating codes possess;distributed storage like;node network regenerating;codes distributed;helper node pooling;abstract regenerating codes;repair failed node;storage codes;node pooling;2010 distributed storage;node pooling necessity;replacement node;storage bandwidth tradeoff;regenerating codes;replacement node required;exact repair code;repair code point"}, "b2f46145f2a50b609482a69d0581b218a6767cef": {"ta_keywords": "similarity web based;similarity web;textual similarity web;reasoning textual similarity;databases ranked retrieval;deductive databases ranked;ranked retrieval methods;information retrieval;information retrieval ir;web based information;deductive databases;textual similarity;knowledge integration reasoning;queries integrate information;style deductive databases;ranked retrieval;like queries structured;retrieval methods;retrieval methods information;similarity metrics text;using ir similarity;allows queries integrate;queries structured;similarity metrics;ir similarity metrics;methods information retrieval;similarity;databases require equality;text database like;whirl allows queries", "pdf_keywords": ""}, "80fdacd50ba9ad2e594dd2ddb0b1fa0e591f37ea": {"ta_keywords": "biomedical event extraction;event extraction abstracts;backgroundbiomedical event extraction;focused abstracts bionlp;extraction abstracts;event extraction;biology biomedical event;abstracts bionlp 2011;event extraction attracted;decomposes event extraction;structured prediction article;structured prediction;based structured prediction;extraction abstracts papers;biomedical event;event extraction set;classification tasks learned;abstracts bionlp;structured prediction framework;classification tasks;genes described publications;prediction article submission;molecular biology biomedical;focused abstracts;set classification tasks;structured prediction exceed;backgroundbiomedical event;systems abstracts;work focused abstracts;bionlp 2011", "pdf_keywords": ""}, "d46ecbacf42748ac9ce1fecd9f1b4ed0b9e34980": {"ta_keywords": "email speech acts;email conversational analysis;improving email speech;speech acts analysis;email act classification;email speech;classification email messages;task email conversational;intents email acts;email conversational;email acts;certain intents email;email acts propose;speech acts;intents message exchange;improve email act;intents email;conversational analysis;conversational analysis useful;contextual information messages;email act;acts analysis gram;gram sequence features;consider classification email;effective task email;email messages;classification email;message preprocessing highly;intents message;intents", "pdf_keywords": ""}, "b350be3836c3d183464642815b26b061f24e8314": {"ta_keywords": "integer embeddings mathematical;number embeddings learned;embeddings mathematical knowledge;trained embeddings integers;number embeddings;integer embeddings;embeddings mathematical;embeddings integers;improve number embeddings;embeddings integers capture;probe integer embeddings;integers capture concepts;embeddings learned;learning representations mathematical;trained embeddings;embeddings learned english;embeddings;mathematical knowledge;learning mathematical;numerical reasoning tasks;similarly trained embeddings;mathematical properties integers;learning mathematical properties;reasoning tasks learning;integers;mathematical sequence data;useful mathematical;representations mathematical;representations mathematical sequence;integers capture", "pdf_keywords": "learn integer embeddings;number embeddings learned;integer embeddings mathematical;integer embeddings trained;embeddings learned integer;integer embeddings;embeddings mathematical knowledge;embeddings mathematical resources;number embeddings;introduce integer embeddings;improve number embeddings;embeddings mathematical;types integer embeddings;probe integer embeddings;integer representations vocabulary;integer embeddings set;learned integer sequences;knowledge integer representations;mathematical regularities embeddings;embeddings data oeis;online encyclopedia integer;regularities embeddings learned;encyclopedia integer sequences;embeddings learned;integer representations;mathematical knowledge integer;word embeddings;embeddings learned english;word embeddings data;embeddings trained"}, "e602bde46bca5f424a3d53675c1275386544eb1e": {"ta_keywords": "shift concept learning;concept learning;representation shift concept;representation shift;concept;analysis representation shift;learning;shift concept;representation;shift;analysis representation;analysis", "pdf_keywords": ""}, "7fcc2cc70498e409168a6c3dfd7c59652b1160c2": {"ta_keywords": "transforms acoustic features;dnn adaptation decoding;feature space adaptation;transformation matrices feature;dnn adaptation;adaptation decoding;feature transformation matrix;applied dnn adaptation;adaptation applied dnn;feature transformation;transformation matrices gmm;single feature transformation;adaptation decoding share;acoustic features adapted;multiple transformation matrices;transformation matrices;transforms acoustic;adaptation applied deep;estimates multiple transformation;single transformation matrix;transformation matrices tend;adaptation data;estimation feature space;features adapted;transformation matrix;fit adaptation data;second transformation matrices;transformation matrix frame;space adaptation framework;regression fmllr transforms", "pdf_keywords": ""}, "dda3f2a2803c80e5b3332868bf86901d6239befc": {"ta_keywords": "stochastic distributed methods;distributed stochastic optimization;distributed methods;sgd distributed stochastic;distributed method reasonable;tolerant distributed method;distributed stochastic;distributed method;fault tolerant distributed;distributed methods error;local sgd distributed;sgd distributed;stochastic distributed;stochastic optimization methods;analysis stochastic distributed;scalable decentralized fault;centralized local sgd;decentralized fault tolerant;converging local sgd;tolerant distributed;error compensated sgd;compensated sgd linearly;stochastic optimization;sgd linearly converging;methods gradient compression;converging error compensated;fault tolerant;methods error compensation;decentralized fault;distributed", "pdf_keywords": ""}, "9dc4a5284ecfd37ab8bc8990eddf1b39113e004b": {"ta_keywords": "translation degradation;affect translation degradation;translation degradation alleviated;translation self training;target monolingual data;combining translation self;source target language;machine translation postulate;target monolingual;context machine translation;machine translation;alleviated combining translation;increasing target monolingual;translation postulate causes;translation self;target language;machine translation formalize;severely affect translation;target language greatly;problem machine translation;combining translation;translation formalize;monolingual data;monolingual data live;source target domain;translation formalize concept;language greatly mismatch;translation postulate;domains source target;source target", "pdf_keywords": "source target language;resource machine translation;target language;context machine translation;target language greatly;machine translation postulate;machine translation;translation task particularly;mismatch languages spoken;language greatly mismatch;greatly mismatch languages;mismatch languages;translation task;languages uncurated content;source target domain;source target;machine translation conclusion;particularly distant languages;domains source target;translation conclusion stdm;translation;distant languages uncurated;distant languages;causes domains source;spoken apart regions;target domain mismatch;local context machine;local context;neubig social media;de\ufb01nition source target"}, "128610c7df12bff1610949c551b6236cb350dcd9": {"ta_keywords": "language models wav2vec2;models wav2vec2 bert;e2e automatic speech;speech recognition asr;models wav2vec2;ctc attention model;acoustic language models;speech text representations;nar ctc attention;wav2vec2 bert;based ctc attention;wav2vec2 ctc baseline;automatic speech;recognition asr;attention joint decoding;speech recognition;wav2vec2 ctc;enables bert predict;trained acoustic language;recognition asr autoregressive;automatic speech recognition;ctc attention;strong wav2vec2 ctc;outperforms strong wav2vec2;ctc attention joint;attention model;bert predict tokens;wav2vec2;representations obtained pretrained;bottleneck speeding decoding", "pdf_keywords": "speech recognition pre;language models wav2vec2;end speech recognition;speech recognition asr;e2e automatic speech;nar ctc attention;ctc attention architecture;recognition performance nar;ctc attention model;models wav2vec2 bert;speech recognition;models wav2vec2;acoustic language models;automatic speech recognition;based ctc attention;performance nar models;trained acoustic language;wav2vec2 bert improving;automatic speech;recognition asr autoregressive;speech text representations;novel nar ctc;nar models;ctc attention;pre trained acoustic;recognition pre trained;speech acoustics content;recognition asr;speech acoustics;wav2vec2 bert designed"}, "4e1d27c68a60bfd8393462107677469bf286f0f8": {"ta_keywords": "program synthesis pragmatic;pragmatic program synthesizer;synthesis pragmatic communication;program synthesis;examples program synthesis;modeling program synthesis;effectively pragmatic program;specification program synthesis;program synthesis techniques;pragmatics specifications;pragmatic program;program synthesis task;specification communicate program;pragmatics specifications especially;synthesis pragmatic;pragmatic communication;models pragmatics specifications;communicate effectively pragmatic;specification communicate;reasoning models pragmatics;specification rational speaker;particular specification communicate;program synthesizer;synthesizer non pragmatic;pragmatic communication user;synthesis techniques construct;specification program;construct infer programs;communicate program work;synthesis task rational", "pdf_keywords": "pragmatic communication synthesis;pragmatic program synthesis;pragmatic program synthesizer;think program synthesis;pragmatic utterances;program synthesis;computational linguistics pragmatics;pragmatic communication;program synthesis kind;examples build pragmatic;pragmatic program;pragmatic utterances looking;program synthesis building;build pragmatic;obtain pragmatic program;intent pragmatic utterances;possible obtain pragmatic;build pragmatic program;modeling program synthesis;program synthesis task;program synthesizer simple;synthesis kind communication;reasoning models pragmatics;program synthesizer;pragmatics framed communication;communication synthesis;obtain pragmatic;pragmatics;user synthesizer;user synthesizer develop"}, "09e4e0eee756da5658c6d572871130d53a89c72b": {"ta_keywords": "bayesian incentive compatible;optimal bayesian incentive;game bayesian persuasion;bayesian incentive;bayesian persuasion algorithmic;bayesian persuasion decision;bayesian persuasion;recommendation policy optimization;outcome bayesian persuasion;persuasion algorithmic;action recommendation decision;persuasion decision maker;persuasion algorithmic recourse;action recommendation policy;incentivize desirable actions;action recommendation;persuasion decision;incentive compatible;recommendation policy simplified;incentive;recommendation policy;game bayesian;desirable outcome bayesian;incentive compatible bic;persuasion;decision subject incentivize;bic recommendation policy;recommendation decision;bic action recommendation;formulate decision maker", "pdf_keywords": "game bayesian persuasion;bayesian persuasion decision;bayesian persuasion;bayesian persuasion algorithmic;persuasion decision maker;2022 bayesian persuasion;using persuasion decision;persuasion algorithmic;persuasion algorithmic recourse;persuasion decision;persuasion mechanism semi;empirically evaluate persuasion;evaluate persuasion mechanism;persuasion;persuasion model strategic;persuasion mechanism;persuasion model;using persuasion;evaluate persuasion;optimal signaling policy;signaling policy arbitrarily;receiving favorable decision;decision subjects strategically;expectation decision maker;signaling policy;favorable decision section;favorable decision;subjected automated decision;expected utility recommending;decision maker expected"}, "933b03a81110676f4c61c449f1926ebd58bc47f7": {"ta_keywords": "touchscreens accessible interacting;access interface touchscreens;interacting dynamic touchscreens;user interfaces;interface touchscreens;interface touchscreens way;dynamic touchscreens accessible;statelens ios;visual user interfaces;existing dynamic touchscreens;actions exploring screen;dynamic touchscreens;exploring screen;statelens ios application;exploring screen technical;touchscreens;interface actions exploring;touchscreens accessible;touchscreens way control;touchscreens way;entertainment systems statelens;inaccessible dynamic touchscreens;screen technical evaluation;dynamic touchscreens difficult;allowing statelens ios;screen technical;statelens reverse engineering;ios;screens easy;visually visual user", "pdf_keywords": "statelens enables blind;interface statelens ios;touchscreen interfaces;access touchscreen interfaces;blind users interact;including touchscreen accessibility;touchscreen accessibility;accessibility touchscreen;interface statelens;statelens ios;statelens provides interactive;screen input assistive;interact touchscreen devices;state diagrams interface;touchscreen interfaces second;touchscreen accessibility touchscreen;statelens ios application;accessibility touchscreen stylus;provides interactive guidance;users interact touchscreen;accessibility;access interface statelens;guide blind users;enables blind users;terms including touchscreen;diagrams statelens provides;interact touchscreen;task access touchscreen;provide interactive guidance;help blind users"}, "d462eae8dd5c1415e03651b9fc1c2ca80a69521f": {"ta_keywords": "tutors like simstudent;automated tutors like;automated tutors;knowledge simulated student;authoring automated tutors;tutors like;tutors;knowledge simulated;math background knowledge;learning;amounts background knowledge;world knowledge simulated;world background knowledge;learning external;smart authoring;simulated student;text extensive understanding;background knowledge includes;background knowledge;integrating perceptual learning;task extending learning;learning external world;learning mechanisms used;knowledge needed like;learning mechanisms;knowledge;prior perceptual skills;perceptual learning external;phrases integrating perceptual;knowledge needed performance", "pdf_keywords": ""}, "730e5e83586dd5784051f933e7bb82571cec4c94": {"ta_keywords": "speech separation flexible;audio separation network;diarization speech separation;speech separation methods;separated speech signal;domain audio separation;speech separation;audio separation;speaker diarization encoder;neural speaker diarization;end neural speaker;separated speech;end speaker diarization;speaker diarization;diarization separation performance;speaker counting;neural speaker;speaker counting performance;speaker diarization speech;separation network;re\ufb01ning separated speech;corresponding number speakers;signal speech activity;estimating separation;end speaker;estimating separation masks;speech activity;separation network convtasnet;end end speaker;diarization separation", "pdf_keywords": "audio separation network;speech separation methods;speaker diarization separation;diarization speech separation;separated speech signal;separated speech signals;speech separation;audio separation;domain audio separation;timedomain audio separation;end neural speaker;speaker diarization encoder;end speaker diarization;neural speaker diarization;separated speech speech;separated speech;convolutional timedomain audio;neural speaker;speaker diarization;diarization separation eend;end speaker;separation network;end end speaker;separation network conv;speaker diarization speech;re\ufb01ning separated speech;speech activity addition;separation network convtasnet;signal speech activity;speech speech activity"}, "1548142a6be92f41e45dcbde9ff8afd71134ac1d": {"ta_keywords": "lung cancer risk;cancer risk pm10;aromatic hydrocarbons pahs;potential lung cancer;polycyclic aromatic hydrocarbons;inhalation exposure;hydrocarbons pahs;inhalation exposure median;aromatic hydrocarbons;sources lung cancer;lung cancer;owing inhalation exposure;hydrocarbons pahs summer;polycyclic aromatic;pollution level;cancer risk;cancer risk study;concentrations pahs;mean concentrations pahs;risk pm10;worldwide pm10 samples;pahs extracted analyzed;concentrated pollution level;concentrations pahs ng;pm10 samples collected;pollution level sources;pollution;incremental lung cancer;risk pm10 bound;bound polycyclic aromatic", "pdf_keywords": ""}, "f48792e8a24e369c80e39a2a2b7451d108f02941": {"ta_keywords": "question answering xqa;answering qa interfaces;question answering qa;question answering;explainable question answering;answering qa;web similar ai;answering xqa;tackle question answering;answering xqa alleviate;qa interfaces;ai;details learning reasoning;questions xqa ii;similar ai applications;following questions xqa;questions xqa;qa interfaces simplified;qa;similar ai;ai applications;learning reasoning steps;answering;ai applications black;xqa explainable;information web similar;learning reasoning;access information web;xqa ii need;user friendly interfaces", "pdf_keywords": ""}, "642c85d35b4a3cc9648b269e32fe9d0a18907c98": {"ta_keywords": "recording speech separation;level speech separation;speech separation meetings;separated speech significantly;continuous speech separation;speech separation;evaluation separated speech;speech separation css;task separate speech;long recording speech;separated speech;separately continuous speech;recording speech;separate speech;separate speech sources;recorded multi talk;overlapped recording involves;segment long recording;multi talk dataset;speech sources long;separation meetings;partially overlapped recording;overlapped recording;utterance level speech;separation meetings proposed;speech significantly reduces;modeling long recording;meetings proposed models;long recording;speech significantly", "pdf_keywords": "speech separation performance;recording speech separation;long recording speech;improves speech separation;recording speech;meeting recordings rnn;rate automatic speech;speech separation;meeting recordings compared;speech recognition evaluation;recordings rnn;real meeting recordings;speech recognition;recordings rnn based;speech separation real;meeting recordings;method improves speech;improves speech;automatic speech;libricss meeting recordings;recordings compared;long recording;modeling long recording;modeling rnn transformer;recordings compared baseline;automatic speech recognition;explored rnn transformer;rnn transformer based;recordings;path modeling rnn"}, "acf0ccc8b67cc441c51d4281c305359073b9c7cc": {"ta_keywords": "speech translation evaluation;translation speech transcription;speech translation systems;transcription text translation;end speech translation;speech transcription;translation evaluation campaign;speech transcription text;speech translation speech;translation evaluation;speech translation;iwslt2018 transfer learning;submissions speech translation;translation speech;translation systems;text translation components;translation components;systems speech transcription;speech transcription parameters;text translation;translation systems based;transfer learning;transcription text;attention based encoder;network systems speech;transcription;transfer learning approach;end end speech;end speech;systems speech", "pdf_keywords": ""}, "9cfc4e94e76d8025cd86d6652a641b1440681d28": {"ta_keywords": "combinatory categorial grammar;categorial grammar ccg;categorial grammar;grammar ccg lexicon;ccg lexicon based;grammar ccg;combinatory categorial;specific combinatory categorial;language specific combinatory;ccg lexicon;linguistic principles verbs;induces language;induces language specific;linguistic;linguistic principles;em based induction;based induction algorithm;lexicon based;sentences nouns arguments;grammar;lexicon based small;sentences nouns;based induction;induction algorithm bisk;combinatory;induction algorithm;2012 induces language;specific combinatory;roots sentences nouns;verbs roots sentences", "pdf_keywords": ""}, "d0a6b70c9dc1942169f48211d47843732c57a3a9": {"ta_keywords": "learning generalized navigation;language grounded navigation;multitask learning natural;vision language navigation;navigation model trained;agnostic multitask learning;multitask navigation model;navigation dialog;introduce multitask navigation;multitask navigation;agnostic representations navigation;representations navigation policy;trained vision language;navigation dialog history;grounded navigation;multitask learning;generalized navigation model;generalized navigation;perspectives introduce multitask;multitask learning significantly;grounded navigation photo;vln navigation dialog;natural language grounded;vision language;training generalizing better;agnostic multitask;navigation model novel;natural language guidance;representations navigation;navigation", "pdf_keywords": "learning generalized navigation;language grounded navigation;agnostic multitask learning;grounded navigation tasks;navigation tasks;tasks vision language;introduce multitask navigation;representations navigation policy;multitask navigation model;approaches multitask learning;multitask learning;vision language navigation;multitask learning environment;multitask navigation;tasked natural language;learn generalized policies;multitask learning framework;perspectives introduce multitask;generalized multitask;navigation tasks vision;agnostic multitask;learning environment agnostic;natural language guidance;navigation dialog;environment agnostic learning;generalized navigation;generalized navigation model;agnostic representations navigation;trained vision language;propose generalized multitask"}, "cdf17da4a7638985cb62a5dbf1161239b315eb85": {"ta_keywords": "jointly modeling links;link modeling jointly;entity link modeling;topic models;link modeling;topic models improve;models topic models;modeling links text;links text entities;text entities linked;modeling links;entities linked;publications annotated proteins;annotated proteins;entities linked model;entity link;model datasets protein;category prediction proteins;entity entity link;pairs entities frequently;induced topics;linked model;prediction proteins perplexity;link text information;induced topics understand;text entities;inspecting induced topics;like analysis protein;datasets protein protein;modeling jointly modeling", "pdf_keywords": ""}, "c0484ac1677b942e8b06ea0ac3cad5b01e52ced4": {"ta_keywords": "question answering robust;answering robust relevant;relevant context selection;answering robust;robust relevant context;making question answering;question answering;context selection;robust relevant;context;answering;relevant context;robust;making question;selection;relevant;question;making", "pdf_keywords": ""}, "092b80cc6250f74a2c1e0ba7820c31a8f0153c0a": {"ta_keywords": "cities literary;cities literary critics;imaginary cities literary;literary analyses;novel invisible cities;literary works;computationally engaging literary;descriptions imaginary cities;literature;large collections literary;literary analyses remains;engaging literary criticism;groups literary;distant reading;embed city description;literary;postmodern novel invisible;literary critics;literary critics attempt;thematic groups literary;literary criticism;engaging literary;collections literary;distant reading algorithmically;literary criticism calvino;city description use;methods single literary;aid literary analyses;methods aid literary;groups literary scholars", "pdf_keywords": "novel invisible cities;processing literary criticism;computationally engaging literary;language processing literary;processing literary;engaging literary criticism;postmodern novel invisible;literary works;literary critics attempt;abstract literary critics;literary criticism;novel invisible;literary critics;abstract literary;2019 abstract literary;descriptions imaginary cities;literary;literature;distant reading;postmodern novel;engaging literary;large collections literary;literary criticism conclusion;narrative threads novel;literary criticism single;calvino postmodern novel;literature careful reading;literary works sharpen;collections literary;invisible cities computationally"}, "63cd8df0041638b0aa74834a81f99ff136951ff1": {"ta_keywords": "binary neurons gan;gans propose binarygan;gan binary neurons;binarygan novel generative;train gan binary;neurons gan objectives;neurons gan;discrete distributions gans;gan binary;adversarial networks binary;distributions gans propose;gan uses binary;propose binarygan;distributions gans;binarygan;network gan;training generative adversarial;gan objectives network;propose binarygan novel;generative adversarial networks;generative adversarial network;training generative;adversarial network gan;novel generative adversarial;generative adversarial;binarygan novel;network gan uses;train gan;architectures training generative;gans", "pdf_keywords": "binary neurons gan;gan binary neurons;binarygan novel generative;train gan binary;neurons gan objectives;neurons gan;gan binary;adversarial networks binary;gan uses binary;propose binarygan;binarygan;abstract propose binarygan;network gan;binarygan using;training generative;training generative adversarial;discrete distributions gans;gan objectives network;propose binarygan novel;distributions gans experimentally;distributions gans;network gan uses;binarygan using different;train gan;adversarial network gan;gans experimentally;2018 training generative;binarygan novel;novel generative adversarial;possible train gan"}, "655b842ae905756b2949758bd7e52e5fd32c3642": {"ta_keywords": "search speech recognition;beam search speech;recognition lvcsr pruning;continuous speech recognition;speech recognition lvcsr;speech recognition;minimization large vocabulary;viterbi beam search;search speech;best speech recognizers;speech recognizers;hypotheses pruned decoding;speech recognition parameters;efficiently minimize search;minimization viterbi;minimize search;search speed decoding;lvcsr pruning;minimize search error;pruned decoding;beam search based;beam search;pruned decoding conventional;minimization viterbi beam;optimize viterbi;method optimize viterbi;lvcsr pruning step;speech recognizers employ;risk minimization viterbi;optimize viterbi beam", "pdf_keywords": ""}, "28421c7f28adfb9ab8aeb56c196ac3ba326efdbb": {"ta_keywords": "active center trypsin;center trypsin;trypsin;studies active center;active center;center;studies active;studies;active", "pdf_keywords": ""}, "7c72e63aa112193590861887c5d03b640ce90911": {"ta_keywords": "conference machine learning;machine learning;international conference machine;proceedings international conference;international conference;learning;conference machine;conference;machine;proceedings international;international;proceedings", "pdf_keywords": ""}, "3a6334953cd2775fab7a8e7b72ed63468c71dee7": {"ta_keywords": "human social skills;social skills training;skills social interaction;social skill training;interaction social skills;social skills;features audiovisual;evaluate social skills;features audiovisual features;audiovisual features;social skills trainers;considering audiovisual features;audiovisual features regarding;simulating social skills;appropriate skills social;social skill;effectiveness social skill;skills social;computer interaction social;features human social;training using audio;audio features audiovisual;based social skills;using audio features;parts social skills;computer based social;audio features;considering audiovisual;audiovisual;linguistic features human", "pdf_keywords": ""}, "4e9328b2801e158647dff69606ed47d47045eca8": {"ta_keywords": "datasets datalab features;datasets support;datasets;datasets datalab;dataset;datasets support various;datalab features dataset;datalab features;propose datalab;propose datalab unified;features dataset;datalab platform data;datalab unified data;paper propose datalab;datalab unified;data ecosystem datalab;datalab;covers 715 datasets;dataset recommendation;datalab platform;data oriented;715 datasets;proliferation datasets datalab;728 datasets support;datalab covers 715;features dataset recommendation;datalab covers;data provides;728 datasets;data oriented platform", "pdf_keywords": "data preprocessing tokenization;machine learning ecosystem;natural language processing;platform allows nlp;outputs machine learning;nlp researchers;natural language;nlp researchers perform;allows nlp researchers;preprocessing tokenization;training deep learning;data deserves deeper;preprocessing tokenization indispensable;citizen machine learning;deep learning machine;deep learning;allows nlp;nlp;data preprocessing;language processing 9th;models data diagnostics;data diagnostics aim;conference natural language;data diagnostics data;language processing;methods natural language;learning models data;researchers preprocessing data;tokenization;training deep"}, "1e9771a264334c45020421b1c847f6bcd88adc60": {"ta_keywords": "topology deep learning;precisely delineating 3d;annotations connectivity;approaches delineating 3d;topology deep;delineating 3d;annotations train networks;truth annotations connectivity;annotations connectivity based;delineating 3d structure;accurate annotations train;annotations active contour;accurate annotations;preserving topology deep;interpret visually 3d;annotations train;ground truth annotations;visually 3d;learning delineate;3d;networks;3d structure depend;annotation inaccuracies;connectivity based learning;annotations;learning delineate end;3d structure;annotation;deep learning;3d large scale", "pdf_keywords": "annotations usually imprecise;annotations;training deep;annotation;annotation inaccuracies delivers;annotation inaccuracies;training deep network;annotations usually;treat annotations;annotations active contour;approach training deep;deep network;deep network joint;annotations end;original annotations;annotation inaccuracies paper;original annotations end;annotations active;treat annotations active;annotations end treat;errors original annotations;joint optimization network;optimization network;jointly train network;explicitly accounts annotation;end treat annotations;insight annotations;deep;insight annotations usually;main insight annotations"}, "80111013916dae3306316c34e13fe856cb08b87b": {"ta_keywords": "intuitionistic default rulebase;queries intuitionistic default;intuitionistic default;default rulebase explicit;inheritance hierarchies;inheritance hierarchies recent;defaults analogous queries;logic seminormal defaults;default rulebase;exceptions default rules;common sense reasoning;inheritance network;rulebase explicit exceptions;analogous queries intuitionistic;defaults analogous;rulebase explicit;queries intuitionistic;topology inheritance network;default rules research;sense reasoning produced;default rulebase answered;inheritance network paper;default rules;sense reasoning;claim default rulebase;hierarchies;explicit exceptions easy;explicit exceptions advantage;work inheritance hierarchies;classical logic", "pdf_keywords": ""}, "69e8c4327193af4549c06809c821c99deb4022cd": {"ta_keywords": "distributed storage coding;storage coding;storage coding problem;distributed storage;stored nodes;capable storing symbols;data stored nodes;nodes downloading symbols;stored nodes network;data nodes;downloading data nodes;network capable storing;storing symbols;storage;data reconstructed downloading;distributed;coding problem consider;nodes;capable storing;nodes downloading;reconstructed downloading data;reconstructed downloading;storing;storing symbols paper;coding;coding problem;consider data stored;connecting nodes downloading;downloading symbols;node", "pdf_keywords": ""}, "a77643bff6f50ccc4f80ec081e4d078a2e788ae7": {"ta_keywords": "segmentations multilingual pretrained;multilingual pretrained representations;trained multilingual representations;probabilistic segmentations multilingual;multilingual representations improves;segmentations multilingual;xtreme multilingual benchmark;pre trained multilingual;multilingual benchmark;rely subword segmentation;multilingual representations;trained multilingual;segmentation especially languages;subword segmentation;multilingual pretrained;view subword regularization;subword segmentation algorithms;shared multilingual vocabulary;subword regularization methods;existing subword regularization;subword regularization mvr;subword regularization;multilingual benchmark hu;results xtreme multilingual;multilingual vocabulary;xtreme multilingual;shared multilingual;multilingual vocabulary demonstrate;multilingual;effectiveness cross lingual", "pdf_keywords": "multilingual pretrained representations;multilingual representations improves;rely subword segmentation;view subword regularization;trained multilingual representations;subword regularization methods;existing subword regularization;subword segmentation;subword regularization;subword segmentation algorithms;abstract multilingual pretrained;multilingual pretrained;pre trained multilingual;subword regularization mvr;unigram language models;multilingual representations;multilingual vocabulary;shared multilingual vocabulary;trained multilingual;subword regularization xinyi;subword segmentation \ufb01rst;background subword segmentation;examination subword regularization;multilingual vocabulary background;generally rely subword;view subword;multi view subword;language models;multilingual;effectiveness cross lingual"}, "bf50833a46839d3932663b472d6145418f9d0bd6": {"ta_keywords": "attention networks stream;asr stream attention;speech recognition;speech recognition asr;stream attention based;microphone arrays;architecture automatic speech;multiple microphone arrays;attention networks;attention based multi;automatic speech recognition;regular attention networks;stream attention;connectionist temporal classification;ctc attention networks;classification ctc attention;microphone arrays achieved;work microphone arrays;microphone arrays acting;automatic speech;networks stream attention;attention networks experiments;stream attention introduced;recognition asr;encoder decoder architecture;separate encoders decoded;multi array corpora;attention based;microphone;corpora using encoder", "pdf_keywords": "attention architecture end;ctc attention architecture;attention based multi;attention architecture;ctc attention model;attention model e2e;attention model hierarchical;attention model;model hierarchical attention;stream attention achieved;hierarchical attention;stream attention;joint ctc attention;attention achieved hierarchical;hierarchical attention mechanism;ordinary attention based;attention based;microphone array;distributed microphone array;ctc attention;outperforms ordinary attention;microphone array situation;microphones arrays;32 microphones arrays;attention;targeting distributed microphone;attention based ones;microphones arrays dirha;ctc attention substantial;attention achieved"}, "d6e21619df572d04b2b2d97b4c5d1fd604f185fb": {"ta_keywords": "neural machine translation;machine translation autoregressively;syntactically supervised transformer;syntactically supervised transformers;predicted parse syntactically;synst decodes sentences;parse syntactically supervised;machine translation;predicts chunked parse;machine translation series;translation autoregressively generate;transformers faster neural;predicted parse;faster neural;decoders neural machine;faster neural machine;chunked parse;decodes sentences;conditioned predicted parse;propose syntactically supervised;translation autoregressively;syntactically supervised;chunked parse tree;sentences 5x faster;standard decoders neural;decoders neural;parse syntactically;synst decodes;decodes sentences 5x;supervised transformer synst", "pdf_keywords": "syntactically supervised transformer;predicts chunked parse;propose syntactically supervised;target syntactic chunks;syntactic chunks;syntactically supervised;model syntactically supervised;chunked parse tree;chunked parse;conditioned predicted parse;syntactically supervise latent;predicted parse;predicted parse work;parse decoder reasonably;translations model syntactically;sequence target syntactic;external constituency parser;parses;parse decoder bottleneck;parse tree generating;faster translations model;synst terms decoding;output vocabulary parse;syntactic chunks non;parser;vocabulary parse decoder;predicted chunk sequence;parse decoder;parses allows;conditioned predicted chunk"}, "a5f42552b2368a587aea0a81175b4a79aa614601": {"ta_keywords": "web concept learning;web based extraction;extracting information web;learning collaborative filtering;specifically collaborative filtering;concept learning collaborative;extracting web data;concept learning cl;collaborative filtering cf;collaborative filtering;learned classifiers;extracting web;classifiers;web concept;information web concept;rate learned classifiers;concept learning;extracting information;web data certain;collaborative filtering previous;learned classifiers wide;learning cl systems;machine learning systems;cf concept learning;web data;classifiers wide;useful information web;certain machine learning;machine learning;information web generally", "pdf_keywords": ""}, "0e61536550b7263d67b2928473355171dc37c0ae": {"ta_keywords": "detecting argument components;argument components structures;detecting argument;argument components;argument;components structures;structures;detecting;components", "pdf_keywords": ""}, "7f0dbd30dc839fd95ea953a9229c879396ca11c0": {"ta_keywords": "symbolic knowledge base;representing symbolic knowledge;reasoning symbolic knowledge;symbolic knowledge;hop inferences scalable;knowledge base kb;sparse matrix reified;inferences scalable;reified kb sparse;matrix reified kb;faster naive sparse;inferences scalable use;scalable neural;knowledge base;semantics kb expressive;scalable neural methods;neural modules fully;sparse matrix implementations;naive sparse;implementations scalable neural;kb sparse matrix;semantics kb;original semantics kb;matrix implementations scalable;neural modules;matrix reified;enables neural modules;multi hop inferences;naive sparse matrix;matrix implementations", "pdf_keywords": "learn neural kbqa;reasoning symbolic knowledge;symbolic knowledge base;architectures neural semantic;neural semantic parsing;incorporating reasoning large;kb neural;neural semantic;representing symbolic knowledge;neural kbqa;knowledge base kb;relations represented sparse;neural kbqa models;independent kb neural;semantic parsing;knowledge base;simpler architectures neural;symbolic reasoning realistic;kb relations represented;kb relations;parsing denotations kb;reasoning synthetic tasks;kb neural network;neural methods reasoning;incorporating reasoning;symbolic knowledge;kbs relations;kbs relations faster;semantic parsing denotations;symbolic reasoning"}, "c2c6c9947dc9d28bb4fc6f965310be517f4d8c57": {"ta_keywords": "gans achieve task;synthesis text wgans;networks gans achieve;shape synthesis text;networks gans;gans achieve;generative adversarial networks;adversarial networks gans;text wgans approach;gans;generative adversarial;text wgans;generative;use generative adversarial;shape synthesis build;shape synthesis;based shape synthesis;use generative;shape messages descriptions;text color shapes;adversarial networks;realize shape synthesis;descriptions realize shape;wgans approach;voxel based shape;adversarial;shape messages;voxel based;wgans approach extract;method use generative", "pdf_keywords": ""}, "81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85": {"ta_keywords": "language models know;know language models;language models lms;language models;relational knowledge lms;profession know language;knowledge lms;accurately estimate knowledge;extracting relational knowledge;discovering better prompts;contained language models;knowledge contained language;estimate knowledge;knowledge contained lms;knowledge lms demonstrate;knowledge contained lm;predicting correct profession;know language;lms automatically discovering;results examining knowledge;models know;examining knowledge contained;models lms having;automatically discovering better;__ profession know;relational knowledge;estimate knowledge contained;extracting relational;prompts use querying;examining knowledge", "pdf_keywords": "query knowledge;knowledge language models;mining based paraphrasing;query knowledge stored;best query knowledge;incorporating knowledge lms;retrieving factual knowledge;knowledge language;relation propose mining;relational knowledge;knowledge stored lms;factual knowledge language;pieces relational knowledge;discovering better prompts;relational knowledge released;paraphrasing based;knowledge lms;based paraphrasing based;knowledge lms conclusion;lms automatically discovering;use querying;knowledge contained lms;paraphrasing based methods;diverse prompts query;prompts use querying;language models;lm prompt query;querying;automatically discovering better;knowledge stored"}, "2c5a410b781f90c145efac05fea235c5c3e44861": {"ta_keywords": "voice conversion framework;source voice conversion;voice conversion;voice conversion vc;speech representation s3r;self supervised speech;supervised speech representation;open source voice;supervised speech representations;supervised speech;speech representation;source voice;speech representations;speech representations work;voice;recognition synthesis vc;vc self supervised;conversion vc framework;introduces s3prl vc;based s3prl toolkit;conversion framework self;vc systems s3prl;s3prl vc;conversion vc;conversion framework;lingual a2o vc;s3prl toolkit;systems s3prl vc;s3r comparable vcc2020;expensive supervised representation", "pdf_keywords": "voice conversion framework;voice conversion;voice conversion vc;source voice conversion;open source voice;source voice;based s3prl toolkit;s3prl vc competitive;introduces s3prl vc;s3r comparable vcc2020;self supervised speech;vc extension s3prl;supervised speech representations;supervised speech;s3prl vc extension;performance s3r based;s3prl vc;framework based s3prl;s3prl toolkit;voice;s3prl toolkit systematic;speech representations;extension s3prl toolkit;s3prl toolkit superb;conversion vc framework;performance s3r;state art s3r;toolkit superb s3prl;vc paper s3prl;extension s3prl"}, "3db9649f2ae986cac13f3e748375f8802f9b07fc": {"ta_keywords": "machine translation improves;pruning translations;translation improves robustness;pruning translations english;magnitude pruning translations;machine translation;translation improves;resource regimes sparsity;memorization low;task machine translation;double bind compression;curbing memorization low;parameters deep neural;deep neural;bind compression;parameters deep;bind compression techniques;number parameters deep;compression;memorization low frequency;low resource;translations;low resource double;limitations compute resource;german low resource;capacity generalization data;performance frequent sentences;sparsity preserves performance;term low resource;deep neural networks", "pdf_keywords": "neural machine translation;translation nmt models;models trained translate;machine translation nmt;resource machine translation;trained translate;machine translation orevaoghene;pruning methods train;trained translate english;machine translation;nmt models trained;pruning method nmt;translation nmt;high resourced language;parameters deep neural;pruning low resource;transformer nmt models;pruning methods;nmt models low;resource african languages;study pruning low;pruning;english low resource;magnitude based pruning;empirical study pruning;pruning low;deep neural;apply pruning methods;study pruning;models low resourced"}, "12442420adf1c36887fafd108f4b7f4fc822ae60": {"ta_keywords": "neural sequence generation;sequence generation tasks;sequence generation;baseline neural sequence;empirically self training;neural sequence;generation tasks empirical;generation tasks;machine translation text;margin self training;machine translation;summarization benchmarks;text summarization benchmarks;complex sequence generation;noisy self training;improve supervised baseline;summarization benchmarks shows;standard machine translation;performance supervised baseline;supervised baseline neural;improve performance supervised;semi supervised;supervised baseline;self training extensively;simplest semi supervised;performance supervised;version self training;translation text summarization;supervised baseline large;generation tasks encourage", "pdf_keywords": "neural sequence generation;self training neural;training neural sequence;sequence generation tasks;empirically selftraining;abstract self training;machine translation;baseline neural sequence;machine translation text;\ufb01rst empirically selftraining;improve performance supervised;summarization benchmarks;text summarization benchmarks;standard machine translation;sequence generation;performance supervised baseline;summarization benchmarks shows;sequence generation junxian;improve supervised baseline;supervised baseline neural;neural sequence;semi supervised;deep neural;training neural;simplest semi supervised;revisiting self training;performance supervised;noisy self training;decently improve supervised;introduction deep neural"}, "299ab255f3d940a20891128dfa9e0736d74a936c": {"ta_keywords": "robotic vision goal;directed robotic vision;robotic vision;vision pipeline;perceptual systems robotics;goal directed robotic;vision architectures;robust existing attention;vision goal specific;vision architectures require;computer vision pipeline;vision goal;building perceptual systems;building perceptual;fusion goal directed;vision pipeline modern;directed robotic;modern vision architectures;existing attention mechanisms;goal specific representations;computer vision;vision;perceptual systems;attention mechanisms domain;goal directed;attention mechanisms;pipeline modern vision;goal early fusion;attention;traditional computer vision", "pdf_keywords": "perception pipeline vision;vision goal action;perception pipeline;representations downstream tasks;imitation learning focus;pipeline vision scene;vision architectures;scene representation goal;perceive vision goal;earlyfusion vision models;imitation learning;traditional perception pipeline;vision architectures work;vision goal;pipeline vision;goal directed object;visual processing pipeline;introducing earlyfusion vision;vision scene representation;earlyfusion vision;vision scene;manner imitation learning;testing vision architectures;representation goal action;learning focus retrieving;approaches learns faster;scene representation;neural architecture goal;vision models;vision"}, "5c6ff5836639e87e8afeaad47e64d0e2234566e8": {"ta_keywords": "fake news detection;fact check textual;fake news misinformation;aware fake news;evidence aware fake;attentive network evidence;automatically fact check;attentive network fact;news misinformation various;trend fake news;evidence level attention;widespread fake news;check textual claims;news misinformation;fact check information;news detection;news detection paper;attention evidence level;news detection utilize;word level evidence;news detection methods;evidence external sources;head attentive network;textual claims;attention evidence;level attention evidence;fake news;network evidence aware;automatically fact;textual claims recent", "pdf_keywords": "fake news detection;fact check textual;fake news misinformation;evidence aware fake;word level evidencelevel;aware fake news;evidence level attention;news misinformation various;automatically fact check;attentive network evidence;attentive network fact;news detection utilize;news detection;fact check information;widespread fake news;attention evidence level;news misinformation;network evidence aware;news detection methods;trend fake news;check textual claims;level attention evidence;evidencelevel;evidence external sources;attention evidence;word level attention;network fact check;textual claims;level evidencelevel;textual claims recent"}, "963c62b7c4b44ff1fe6aa1f45fa8a7d62b3d5051": {"ta_keywords": "domain question answering;question answering;question answering qa;textual entailment trained;answering science exam;answering science;knowledge conceptnet;background knowledge conceptnet;knowledge conceptnet tandem;ai nlp emerging;questions selected challenging;challenge dataset contains;conceptnet;challenging current qa;arc challenge dataset;questions authored grade;answering qa;entailment trained scitail;ai nlp;rewriting background knowledge;entailment trained;chance answering science;textual entailment;generic textual entailment;answering qa important;qa task;exam questions using;challenge dataset;science exam;exams questions selected", "pdf_keywords": "domain question answering;question answering;automated knowledge base;automated knowledge;question answering qa;answer query expansion;ai nlp emerging;knowledge base construction;research ibm watson;ai nlp;problem ai nlp;knowledge base;ibm watson research;information retrieval;query expansion reformulation;nlp emerging;reformulation query expansion;watson research;nlp emerging bellwether;information retrieval ir;knowledge textual;qa task;generalizability ai methods;progress generalizability ai;answering qa;knowledge textual entailment;generalizability ai;query reformulation;generic textual entailment;background knowledge textual"}, "e28b9bc26f5f7eb3b0532d823713400202372da2": {"ta_keywords": "actor critic algorithms;actor critic algorithm;critic algorithms leader;stackelberg actor critic;critic algorithms;critic algorithm counterparts;critic game theoretic;critic algorithm;actor critic game;environments stackelberg actor;critic based reinforcement;framework stackelberg actor;model actor critic;known stackelberg game;critic interaction player;stackelberg game;stackelberg game hierarchical;critic algorithms perform;critic game;interaction actor critic;actor critic interaction;game theoretic reinforcement;actor critic based;interpretation stackelberg actor;hierarchical interaction actor;reinforcement learning algorithms;critic interaction;critic based;theoretic reinforcement;critic actor critic", "pdf_keywords": "actor critic algorithms;actor critic reinforcement;stackelberg actor critic;critic game theoretic;reinforcement learning player;model actor critic;known stackelberg game;stackelberg game particular;stackelberg game;stackelberg game policy;critic reinforcement learning;deterministic policy gradient;critic algorithms;actor critic game;sum stackelberg game;game theoretic reinforcement;policy player model;critic reinforcement;policy gradient;critic interaction player;critic based reinforcement;actor critic interaction;critic deep deterministic;interaction actor critic;critic game;critic algorithms optimized;hierarchical interaction actor;gradient soft actor;theoretic reinforcement learning;policy gradient soft"}, "b63bd17d4bb28ba90cc6ff66b51ba5b0377467bf": {"ta_keywords": "rnnlms speech recognition;models rnnlms speech;language models rnnlms;models rnnlms;training method rnnlms;rnnlms speech;lstm rnnlm;speech recognition minimum;speech recognition training;memory lstm rnnlm;term memory lstm;method rnnlms;rnnlm advanced model;models speech recognition;short term memory;language models speech;word error training;training recurrent neural;recurrent neural network;rnnlms usually trained;models speech;speech recognition;training recurrent;rnnlm advanced;type rnnlm;rnnlms;elman type rnnlm;neural network language;rnnlms usually;mwe training recurrent", "pdf_keywords": ""}, "93d3e45395117e21214d404c8753b578c29266d1": {"ta_keywords": "open question answering;question answering tables;evidence retrieval challenging;text question answering;question answering ott;making evidence retrieval;retrieval challenging baseline;question answering;evidence retrieval;question answering qa;answering tables;answering tables text;documents contain answers;retrieving aggregating evidence;iterative retriever bert;retrieval challenging;unstructured text evidence;answering ott qa;answering qa;bert based reader;qa tabular textual;aggregating evidence ott;highly relevant tabular;relevant tabular textual;retriever bert based;answering qa answer;answering ott;retrieving analyzing documents;analyzing documents contain;analyzing documents", "pdf_keywords": "strategies fusion retrieval;fusion retrieval;text question answering;question answering ott;fusion retrieval cross;question answering;retrieval cross block;sparse attention;retrieving aggregating evidence;aggregating evidence ott;retrieved evidence global;retrieval cross;qa tabular textual;relevant tabular textual;local sparse attention;multiple retrieved evidence;retrieved evidence;highly relevant tabular;answering ott qa;textual data;tabular textual data;retrieval;textual data present;aggregating evidence;attention;evidence ott qa;tabular textual;sparse attention conduct;cross block reading;answering ott"}, "a0035379f93e0e95bdadd77a1d8eb27ba89dcf60": {"ta_keywords": "story generation models;evaluate story generation;story generation;narrative forming robust;systems story generation;loop story generation;stories given input;grained natural language;annotations character goals;online collaborative storytelling;collaborative storytelling;narrative forming;creative text systems;natural language annotations;collaborative storytelling community;story generation asked;storytelling community;evaluate story;storytelling community large;story continuations;evaluate language models;existing evaluations crowdsourced;interspersed narrative forming;attributes interspersed narrative;text systems story;6k lengthy stories;language models;suggested story continuations;lengthy stories;evaluations crowdsourced automatic", "pdf_keywords": "story generation storium;generated stories;storytelling community evaluation;evaluate generated stories;interact generated stories;collecting dataset stories;dataset stories;systems story generation;online collaborative storytelling;loop story generation;storytelling systems;story generation built;generated stories develop;computational storytelling systems;story generation;computational storytelling;collaborative storytelling;collaborative storytelling community;storytelling community provides;storytelling community;guiding computational storytelling;generated stories section;storytelling community conclusion;stories given input;research story generation;story generation nader;dataset stories section;stories develop;stories section storium;generation storium dataset"}, "3e4d80e43346b9538504c0a7ee5562f3c6a09178": {"ta_keywords": "bandit algorithms;traditional bandit algorithms;bandit algorithms results;classical bandit algorithms;bandit algorithms upper;bandit approach;bandit approach problem;variants classical bandit;applying traditional bandit;armed bandit approach;traditional bandit;classical bandit;greedy algorithms reinforcement;bandit;multi armed bandit;theoretical bounds regret;algorithms reinforcement;algorithms reinforcement learning;epsilon greedy algorithms;bounds regret;incentives recommendations users;bounds regret design;armed bandit;ucb epsilon greedy;reinforcement learning;algorithms upper confidence;regret design incentives;greedy algorithms;epsilon greedy;reinforcement learning learning", "pdf_keywords": ""}, "740ecaa7fc1f4fc02116181b1757f03c815c7ea9": {"ta_keywords": "multilabel classification diagnoses;lstm recurrent;lstm;lstm recurrent neural;novel application lstm;application lstm recurrent;neural networks multilabel;recurrent neural networks;classification diagnoses;networks multilabel classification;application lstm;multilabel classification;time series clinical;classification diagnoses given;recurrent neural;multilabel;networks multilabel;length time series;diagnoses;series clinical measurements;recurrent;classification;clinical measurements;clinical measurements method;diagnoses given;series clinical;neural networks;clinical;metrics;length time", "pdf_keywords": "lstm;time series lstm;novel application lstm;study lstm rnns;clinical time series;empirical study lstm;lstm rnns;study lstm;term memory lstm;lstm recurrent;recurrent neural networks;series lstm recurrent;lstm powerfully model;series lstm;lstm recurrent neural;application lstm recurrent;time series clinical;lstm powerfully;application lstm;results indicate lstm;memory lstm;memory lstm powerfully;short term memory;health data prediction;neural networks health;indicate lstm;lstm rnns applied;lstm rnns successfully;lstm layers;indicate lstm rnns"}, "191ef1408406569f0e9a69344add1ae350365431": {"ta_keywords": "characterizing predictive fairness;predictive fairness;different predictive fairness;predictive fairness properties;characterizing fairness;model better fairness;better fairness properties;fairness properties characterizing;characterizing fairness set;properties characterizing fairness;fairness properties;fairness properties framework;fairness properties set;fairness set good;fairness set;better fairness;predictive bias;fairness;audit predictive bias;predictive models deliver;bias;characterizing predictive;decision outcome unconfounded;good models selective;predictions individual cases;models selective labels;framework characterizing predictive;audit predictive;selection decision outcome;predictive bias replace", "pdf_keywords": "predictive fairness;predictive fairness properties;probe predictive fairness;model characterizing fairness;predictive disparities fairs;characterizing fairness set;characterizing fairness;algorithmic framework fairness;propose algorithm fairness;algorithm fairness;fairs investigate predictive;fairness properties;fairness set;algorithm fairness rashomon;disparities fairs propose;predictive disparities benchmark;fairness set good;disparities fairs;predictive disparities black;fairness properties set;framework fairness rashomon;develop framework fairness;framework fairness;disparities benchmark model;predictive disparities set;predictive disparities;lower predictive disparities;fairs improves statistical;regression \ufb01nding fairs;larger predictive disparities"}, "9eeaeadc1e0e300337b47d867a314caeae5c10a9": {"ta_keywords": "estimating brain age;brain age based;brain age;estimating brain;population deep learning;healthy population deep;age based;deep learning structural;population deep;age;age based uniform;deep learning;healthy population;brain;magnetic resonance imaging;uniform healthy population;estimating;learning structural magnetic;resonance imaging;deep;structural magnetic resonance;population;magnetic resonance;healthy;learning;learning structural;imaging;uniform healthy;structural magnetic;based uniform healthy", "pdf_keywords": ""}, "9813446d9545b600de9a4972c1382c5e3b22a351": {"ta_keywords": "modeling intelligent tutoring;intelligent tutoring systems;intelligent tutoring;cognitive tutor;cognitive tutor authoring;student modeling intelligent;problems intelligent tutoring;tutoring systems;block cognitive tutor;tutoring systems simstudent;tutoring;student tutor;programming predicting students;simstudent student modeling;tutor;intelligent tutoring current;student tutor interactions;tutor interactions;human students solving;tutor authoring;student modeling;tutor interactions showed;simstudent learns cognitive;learns cognitive;tutor authoring tools;learns cognitive skills;predict human students;predicting students;predicted human students;cognitive skills observation", "pdf_keywords": ""}, "78dadbfb6710ac65f178b5e12bd975184aae62fe": {"ta_keywords": "integration web information;integration web;web information;observations integration web;web;information;practical observations integration;integration;observations integration;practical;practical observations;observations", "pdf_keywords": ""}, "241e890c70f6d013de7fe5e174e061ff824dc5e9": {"ta_keywords": "tutoring simulated students;tutoring simulated;tutoring simstudent students;teachable agents learning;students teachable agents;learning agent;tutoring simstudent;problems tutoring simulated;tutoring;tutoring simstudent used;agents learning companion;teachable agents;simulated students teachable;learning agent called;study showed tutoring;minutes tutoring simstudent;simulated students;teaching live machine;15 problems tutoring;showed tutoring simstudent;learning companion systems;minutes tutoring;showed tutoring;learns skills student;70 minutes tutoring;problems tutoring;learning environment;machine learning agent;simstudent students improved;learning companion", "pdf_keywords": ""}, "ca00ead4e5ddd14cbbbce03d89a57d14b430e320": {"ta_keywords": "insurance companies privacy;probability privacy;using probability privacy;consumer valuation privacy;varying guarantees privacy;guarantees privacy;probability privacy breach;companies privacy customer;privacy customer;valuation privacy design;companies privacy;privacy design;privacy breach offered;valuation privacy;privacy customer segmentation;guarantees privacy finally;privacy finally design;privacy;privacy breach;privacy design screening;privacy finally;smart grid electricity;smart grid;insurance contracts;insurance contracts using;segmentation smart grid;electricity grid networked;design insurance contracts;electricity grid;grid networked sensors", "pdf_keywords": "probability privacy;using probability privacy;privacy using probabilities;probability privacy breach;consumer valuation privacy;varying guarantees privacy;attack model privacy;valuation privacy design;guarantees privacy;guarantees privacy using;valuation privacy;model privacy;infer private information;model privacy breaches;ensuring adversary;privacy breach offered;privacy customer;privacy design;privacy customer segmentation;results ensuring adversary;privacy;adversary;private information certain;privacy breaches;attack adversarial agent;attack adversarial;adversary fail;privacy breach;privacy design screening;ensuring adversary fail"}, "83c7335904002d2b7c7cb403f3538703c9a69025": {"ta_keywords": "enhanced speech noisy;nonaudible murmur enhancement;murmur enhancement noisy;statistical voice conversion;speech noisy environments;speech nonaudible murmur;voiced speech effective;silent speech communication;allows silent speech;listens enhanced speech;intelligible speech noisy;murmur enhancement;target speech nonaudible;nonaudible voice propose;voice conversion;speech noisy;speech nonaudible;process voiced speech;generating intelligible speech;statistical voice;speech whispered voice;voiced speech;enhanced speech;speech communication nam;nonaudible voice;target speech effective;effective unvoiced speech;message nonaudible voice;speech communication;speech generating intelligible", "pdf_keywords": ""}, "9f1a1d2cb6b278b7ee24e67d4c2ac38c1161fa1d": {"ta_keywords": "indonesian ethnic speech;data multilingualism indonesia;multilingualism indonesia;multilingualism indonesia gradually;speech translation ethnic;analysis indonesian ethnic;ethnic speech data;analysis indonesian;languages english indonesian;speech data multilingualism;translation ethnic languages;ethnic speech corpora;vowel analysis indonesian;language preservation;indonesian ethnic;english indonesian collection;ethnic languages;ethnic speech;language preservation preliminary;english indonesian;corpora language preservation;ethnic languages english;speech speech translation;indonesian;indonesian collection analysis;speech translation;translation ethnic;collection analysis indonesian;speech corpora language;data multilingualism", "pdf_keywords": ""}, "5bf7f468b763f181c31a5e1edc57bce9a6dbd00c": {"ta_keywords": "critically ill patients;patients eligible ecmo;risk patients critical;critically ill;highlight critically ill;oxygenation ventilation therapy;extracorporeal membranous oxygenation;risk mortality decompensation;critical care databases;patients critical care;sustaining oxygenation ventilation;available critical care;life peer score;critical care patients;patients critical;survival clinicians;survival clinicians report;increase survival clinicians;oxygenation ventilation;ecmo increase survival;ventilation therapy;critical care;calculation patient risk;mortality decompensation;predicts mortality existing;oxygenation ecmo life;membranous oxygenation ecmo;oxygenation ecmo;pneumonia high risk;mechanical ventilation", "pdf_keywords": "pneumonia high risk;coronavirus pneumonia covid;coronavirus pneumonia;sepsis campaign guidelines;pneumonia eligible;pneumonia eligible ecmo;coronavirus pneumonia provide;highlight critically ill;critically ill patients;pneumonia covid 19;risk patients critical;coronavirus disease 2019;novel coronavirus pneumonia;pneumonia covid;pneumonia provide risk;score critical care;ill adults coronavirus;highlight risk patients;predict severe 2019;risk score critical;predict severe;critically ill adults;early predict severe;critically ill;provide risk score;ecmo surviving sepsis;surviving sepsis campaign;2019 novel coronavirus;management critically ill;adults coronavirus disease"}, "ea2b138583e587850153f2825fe9e4339aa5f5f9": {"ta_keywords": "speech separation diarization;speaker meetings separated;integration speech separation;multi speaker meetings;speech separation;speaker meetings;recognition multi speaker;meetings separated libricss;separation diarization recognition;meetings separated;separation diarization;integration speech;multi speaker;diarization recognition multi;separated libricss dataset;diarization recognition;meetings;speech;recognition multi;separated libricss;speaker;separated;separation;diarization;libricss dataset;recognition;libricss;multi;dataset;integration", "pdf_keywords": ""}, "ca1645abedae3b4caa3345aa8720c8b90f7c37db": {"ta_keywords": "voting rules rank;votes positional scoring;scoring rules voting;rank dependent scoring;voting compute score;rank score ordered;scores obtained votes;instead aggregate scores;aggregate scores;score ordered;score alternative summing;rank score;dependent scoring rules;votes positional;voting rules;rules voting compute;equally score alternative;score alternative;score ordered list;alternative summing scores;ordered list scores;scores taking account;summing scores;summing scores alternative;compute score alternative;aggregate scores taking;obtained votes positional;scores alternative;include scoring rules;dependent scoring", "pdf_keywords": ""}, "b2c3d660aaefb80085fe72c80ce81c5fa71980e9": {"ta_keywords": "pivot translation approaches;pivot translation methods;pivot language syntactic;based pivot translation;pivot translation useful;pivot target translation;methods pivot translation;pivot language words;distinguishing pivot language;subtrees pivot language;pivot translation;syntactic subtrees pivot;superior pivot translation;target translation models;language syntactic matching;translation approaches tested;pivot language;translation approaches;method translating languages;translation models;translation methods;ambiguities pivot language;pivot language avoid;phrase pairs interlingual;translation methods merge;translation useful method;syntactic matching methods;translating languages;useful method translating;translating languages contain", "pdf_keywords": ""}, "b7d6829d9eccdbd3d3a5d6f5321a87158588033b": {"ta_keywords": "labeling pairwise ratings;labeling crowd sourced;labeling large scale;labeling images videos;challenge worker bias;labeling short videos;labeling crowd;videos people;score reconstruction pairwise;camera personality traits;trait challenge worker;personality trait challenge;camera personality;pairwise ratings;alleviate bias problems;videos humans continuous;short videos people;bias problems;images videos humans;typically labeling crowd;labeling images;chalearn apparent personality;labeling pairwise;crowd sourced overcoming;labeling;videos humans;contribute just labels;pairwise ratings application;videos people facing;facing camera personality", "pdf_keywords": ""}, "47adb249ce8f7f5f1e92112ba0f3757f8fbfbfc3": {"ta_keywords": "lexical semantic models;question answering;order lexical semantic;improves lexical semantic;semantic models investigated;semantic models chain;text lexical semantic;question answer texts;semantic models;question answering general;language models learn;semantic models provide;monolingual alignment models;lexical semantic;learn term embeddings;semantic drift;higher order lexical;language models;answer texts;approach improves lexical;factoid answer reranking;improves lexical;controlling semantic drift;semantic models non;network language models;text lexical;unstructured text lexical;semantic;performance question answering;controlling semantic", "pdf_keywords": ""}, "21ac57d41843ac5367e11b8b784aa57f2ef7a1fc": {"ta_keywords": "convex stochastic optimization;stochastic convex optimization;smooth stochastic optimization;stochastic optimization heavy;stochastic optimization;convex optimization complexity;smooth stochastic convex;smooth convex stochastic;stochastic convex;stochastic optimization problems;convex stochastic;high probability convergence;optimization complexity bounds;non smooth stochastic;convex optimization;high probability complexity;probability complexity bounds;smooth stochastic;optimization heavy tailed;stochastic order methods;optimal iteration oracle;highly suboptimal objective;non sub gaussian;optimization complexity;optimal non smooth;near optimal;assumption sub gaussian;iteration oracle complexity;sub gaussian heavy;practical efficiency random", "pdf_keywords": "stochastic optimization heavy;stochastic optimization;smooth stochastic optimization;stochastics germany institute;stochastic \ufb01rst order;high probability complexity;probability complexity bounds;stochastic \ufb01rst;data stochastic \ufb01rst;smooth stochastic;analysis stochastics;stochastic;stochastics;near optimal;non smooth stochastic;optimization heavy tailed;optimal high probability;stochastics germany;complexity bounds;analysis stochastics germany;probability complexity;applied analysis stochastics;data stochastic;near optimal high;complexity bounds non;optimization heavy;optimal;training large scale;tailed noise eduard;learning models"}, "6cdff2505560390b28db5a96c2ae3070712077cf": {"ta_keywords": "games gradient dynamics;gradient based bandits;reinforcement learning gradient;gradient reinforcement learning;games gradient;policy gradient reinforcement;competitive gradient based;competitive gradient;gradient reinforcement;learning continuous games;agents employing gradient;policy gradient;smale games gradient;agents employ gradient;including policy gradient;framework competitive gradient;learning gradient;online convex optimization;gradient dynamics;learning gradient based;continuous games;algorithms including policy;gradient dynamics correspond;reinforcement learning;theory potential games;general sum games;dynamics correspond gradient;gradient based learning;class games morse;gradient like flows", "pdf_keywords": "gradient multi agent;learning continuous games;multi agent reinforcement;policy gradient;agent reinforcement learning;optimization gradient play;policy gradient multi;adversarial networks game;behavior competitive gradient;agent reinforcement;multi agent learning;continuous games generative;games generative adversarial;online optimization gradient;competitive gradient;agent learning;multi armed bandits;competitive gradient based;agent learning settings;reinforcement learning;generalize potential games;sum continuous games;games generative;games stronger convergence;behavior gradient based;behavior gradient;continuous games;agent learning algorithms;games generalize potential;bandits"}, "7301c7aba3c0824b91f69747e7e50f4db56d7fc1": {"ta_keywords": "translation sensitive paralinguistic;translation paralinguistic information;model speech speech;model speech;perform paralinguistic translation;paralinguistic translation single;paralinguistic information neural;translation paralinguistic;paralinguistic translation;speech speech translation;express paralinguistic translation;spoken words model;speech translation;proposed model speech;speech translation sensitive;paralinguistic information;paralinguistic information duration;space translation paralinguistic;express paralinguistic;sensitive paralinguistic information;perform paralinguistic;paralinguistic;word dependent models;words model;model words;sensitive paralinguistic;translation single model;power perform paralinguistic;speech speech;regression model words", "pdf_keywords": ""}, "8837530b23a2d51054d8752ae2f0ffef8998da8e": {"ta_keywords": "users location privacy;location privacy extensive;privacy preserving cross;location privacy;collective matrix factorization;domain location recommendation;criterion differential privacy;privacy preserving;matrix factorization;differential privacy based;recommendation target domain;differential privacy;differential privacy second;adopt differential privacy;location recommendation works;privacy;privacy extensive experiments;privacy extensive;privacy second;protecting users location;named privacy preserving;framework named privacy;factorization ccmf effectively;improve recommendation;matrix factorization ccmf;locations user meet;factorization;named privacy;improve recommendation target;privacy based", "pdf_keywords": ""}, "cb15c1c51e8a7da42d5b2ebac955bf1cd9dd4022": {"ta_keywords": "graph generating texts;knowledge graph generating;text generation knowledge;generating texts;generation knowledge graphs;knowledge graphs imposing;knowledge graphs;knowledge graph;structure knowledge graphs;graphical knowledge representations;particular knowledge graph;generating texts express;knowledge graphs graph;produce text generation;text generation;text generation techniques;information extraction;graph transforming encoder;structured representation content;introduce novel graph;extraction particular knowledge;novel graph transforming;graph generating;graph transformers introduce;graphs graph transformers;output information extraction;relational structure knowledge;knowledge representations;produce text;information extraction particular", "pdf_keywords": "graph text generation;trainable graph text;graph text;attention model graph;knowledge graph;graph encoding;knowledge graphs;graphwriter featuring;graph transforming encoder;graphwriter;model graph encoding;graphwriter featuring new;knowledge incorporated encoder;introduced graphwriter;introduced graphwriter featuring;knowledge graphs imposing;structure knowledge graphs;text generation;particular knowledge graph;constraints introduced graphwriter;information extraction;introduce novel graph;novel graph transforming;graph encoding demonstrated;output information extraction;agenda dataset abstracts;sentence texts output;novel graph;encoder leverage relational;end trainable graph"}, "a54019645dd8e9cfd8d71ab60155449307de3d83": {"ta_keywords": "incentive mechanisms crowdsourcing;payment spammers crowdsourcing;mechanisms crowdsourcing;crowdsourcing cheap fast;spammers crowdsourcing;crowdsourcing cheap;mechanism possible crowdsourcing;possible crowdsourcing cheap;spammers crowdsourcing gained;crowdsourcing propose simple;crowdsourcing;crowdsourcing gained;fundamental challenge crowdsourcing;challenge crowdsourcing;challenge crowdsourcing propose;crowdsourcing propose;possible crowdsourcing;crowdsourcing gained immense;incentive mechanisms;mechanism incentive;incentive;incentive compatible mechanisms;possible incentive compatible;mechanism incentive compatible;multiplicative incentive mechanisms;possible incentive;requirement mechanism incentive;incentive compatible;incentive compatible payment;payment spammers", "pdf_keywords": "incentive mechanisms crowdsourcing;reward mechanism crowdsourcing;mechanism crowdsourcing;mechanisms crowdsourcing;crowdsourcing gained;mechanism crowdsourcing ensure;crowdsourcing cheap;crowdsourcing cheap fast;amazon mechanical turk;crowdsourcing ensure;practice crowdsourcing cheap;crowdsourcing;abstract crowdsourcing gained;crowdsourcing gained immense;practice crowdsourcing;abstract crowdsourcing;mechanisms crowdsourcing nihar;crowdsourcing ensure collection;crowdsourcing nihar;2015 abstract crowdsourcing;data designed reward;mechanical turk;mechanical turk evaluate;work practice crowdsourcing;crowdsourcing nihar shah;incentive mechanisms;incentive;multiplicative incentive mechanisms;popularity machine learning;turk evaluate"}, "2cc7db7b17ee7349800334b3a154f708850c6410": {"ta_keywords": "archival storage systems;replication powerful erasure;reliability archival storage;data storage systems;data replication powerful;mds queues;codes mds queue;data storage;data replication;archival storage;mds queues use;mds queue;storage systems data;storage systems;improved reliability archival;data storage methods;mds queue analysing;data latency;versions mds queues;queueing arising codes;mds queue provide;simple data replication;hot data latency;performance erasure codes;latency centralized;queues use codes;queueing theory;storage systems based;queue analysing latency;job latency centralized", "pdf_keywords": ""}, "0ff5b1e61bbebd2f077a4ef24c3afdb344e5b3d4": {"ta_keywords": "vantage", "pdf_keywords": ""}, "8e992116bbc8afb075577a30672de7a90fbeba78": {"ta_keywords": "speech recognition asr;streaming transformer asr;e2e automatic speech;encoder perform streaming;e2e transformer asr;performance alternative recurrent;encoder;encoder perform;recognition asr systems;processing encoder;transformer self attention;chunkwise attention;transformer asr blockwise;recognition asr;transformer asr;speech recognition;processing encoder perform;recurrent neural networks;blockwise processing encoder;streaming e2e transformer;chunkwise attention mocha;monotonic chunkwise attention;alternative recurrent neural;asr systems streaming;streaming transformer algorithm;automatic speech;transformer asr ablation;streaming transformer;automatic speech recognition;csj japanese tasks", "pdf_keywords": ""}, "3163392f56cdffaa009fbc59f299989a1b8baec1": {"ta_keywords": "classification leveraging unlabeled;leveraging unlabeled data;classification positive unlabeled;leveraging unlabeled;unlabeled data reliable;dealing binary classification;learns labeled positive;unlabeled data improve;unlabeled data reliability;unlabeled pu learning;unreliable unlabeled data;far unlabeled data;classification data labeled;binary classification;learns labeled;robust unreliable unlabeled;think unlabeled data;algorithms unlabeled data;classification positive;learning learns labeled;labeled class data;utilizes unlabeled data;unlabeled data furthermore;classification;unlabeled data;oc classification positive;labeled positive data;oc classification;class classification;unreliable unlabeled", "pdf_keywords": "classi\ufb01cation leveraging unlabeled;leveraging unlabeled data;unlabeled data reliable;unlabeled data reliability;leveraging unlabeled;unreliable unlabeled data;reliability unlabeled data;robust unreliable unlabeled;reliable unreliable unlabeled;algorithms unlabeled data;labeled class data;far unlabeled data;classi\ufb01cation data labeled;binary classi\ufb01cation data;classi\ufb01cation positive unlabeled;unlabeled pu learning;classi\ufb01cation leveraging;data labeled class;class classi\ufb01cation leveraging;unreliable unlabeled;unlabeled data;dealing binary classi\ufb01cation;reliability unlabeled;unlabeled data using;classi\ufb01cation data;labeled class;algorithms unlabeled;oc algorithms robust;binary classi\ufb01cation;class data scientists"}, "73472692b6090a72e36e03127bb99fc2e6bc8de0": {"ta_keywords": "derive networks communities;mixture models affinity;communities means gaussian;communities;latent network community;latent networks created;networks communities socio;latent networks;networks communities;unsupervised machine learning;analyze latent networks;communities community assignments;communities community;communities means;gaussian mixture models;communities socio cultural;network modularity maximization;socio cultural data;communities socio;network community assignments;community relationships;models affinity propagation;latent network;network modularity;mixture models;affinity propagation;derive networks;nature communities;methodology derive networks;community assignments entities", "pdf_keywords": ""}, "2f6843f9345ca56af3fd9df5512daa1e7f80bedf": {"ta_keywords": "structured prediction;structured prediction relies;imitation learning;approach structured prediction;based imitation learning;imitation learning approach;learning approach structured;word level quality;estimation shared task;tag predictions;quality estimation shared;structure prediction;previous tag predictions;tag predictions use;compared structure prediction;based imitation;prediction relies;imitation;structured;structure prediction approaches;prediction relies classifier;learning;shared task explore;arbitrary information previous;quality estimation;word level;task explore;shared task;relies classifier trained;prediction", "pdf_keywords": ""}, "faa4468f2ad1c7cedaf04bf56ebb20ae4b349952": {"ta_keywords": "language models lstm;language models rnn;recurrent neural network;short term memory;memory recurrent neural;term memory recurrent;models lstm;models lstm lms;lstm lms;lstm;models rnn lms;memory neural network;memory recurrent;term memory neural;lstm lms superior;recurrent neural;information recurrent neural;trials lstm;lstm lms complex;trials lstm lms;memory neural;effort trials lstm;neural network language;term memory;matrix adaptation evolution;language models;models rnn;models various speech;information recurrent;rnn lms", "pdf_keywords": ""}, "872c2d9d8b27ff49367854a7cf67b5dff2010406": {"ta_keywords": "biomedical event extraction;bionlp 2009 event;event extraction training;2009 event detection;biomedical event;event extraction;bionlp 2009;event detection task;possible biomedical event;event detection;bionlp;2009 event;unsupervised possible biomedical;event classes;extraction training data;detection task precisions;event;theme event classes;single theme event;biomedical;theme event;detection task;reasonable recall designed;event classes range;extraction training;maintaining reasonable recall;possible biomedical;training data;reasonable recall;recall designed domain", "pdf_keywords": ""}, "c6713071291729386955586c6309778b1637b852": {"ta_keywords": "hvac resource allocation;allocation buildings social;control strategy selfish;strategy selfish agents;agents context hvac;quadratic game dynamically;resource allocation buildings;planner propose neighborhood;dynamic game;dynamic game theory;allocation buildings;selfish agents;game dynamically coupled;simplification dynamic game;standard dynamic game;game dynamically;selfish agents context;buildings social planner;linear quadratic game;planner influences game;agent cost function;hvac resource;pricing mechanisms;quadratic game;hvac;propose neighborhood based;zone occupants agents;control strategy;game choosing quadratic;game theory", "pdf_keywords": ""}, "d5084f48212bed80e8c11e1e69669deea3ba2f83": {"ta_keywords": "script corpus;parallel script corpus;script corpus cloze;corpora tasks;captions missing procedural;instructions nlp community;corpus cloze task;instructions nlp;lacks corpora tasks;corpora tasks evaluating;matches video captions;concrete instructions nlp;captions;nlp community;video captions;corpus cloze;nlp community lacks;nlp;corpus;procedural language;procedural language requires;event making pasta;procedural details;video captions missing;lacks corpora;understanding procedural language;cloze task matches;captions missing;kidscook parallel script;corpora", "pdf_keywords": ""}, "8451d8e20bb9a94c6a576e52ca1a63470f8d2390": {"ta_keywords": "decentralized distributed optimization;distributed optimization;distributed optimization prove;distributed optimization factor;convex composite optimization;method decentralized distributed;composite optimization;order decentralized distributed;composite optimization problem;decentralized distributed;bound order decentralized;convex composite;sliding algorithm lan;optimization prove bounds;method decentralized;2019 convex composite;apply method decentralized;sliding algorithm;optimization prove;distributed;order decentralized;optimization;derivative free method;algorithm lan;optimization problem includes;convex;optimization factor;based sliding algorithm;algorithm lan 2016;2019 convex", "pdf_keywords": ""}, "a6a9c06d138537002aaca79dba359cc320b951df": {"ta_keywords": "language processing bayesian;bayesian speech language;bayesian speech;processing bayesian approach;processing bayesian;speech language processing;bayesian approach;speech language;language processing;bayesian;speech;language;processing;approach", "pdf_keywords": ""}, "03bbbaa03cb57413c2581cc8dc5cbfa532bbea15": {"ta_keywords": "channel eegs;eeg device;eeg device investigate;user identification performance;grade eeg device;consumer grade eeg;eegs 25 subjects;analyze brain waves;14 channel eegs;brain waves;identification authentication experimental;eegs;grade eeg;user identification authentication;dimensionality reduction technique;channel eegs 25;comparing user identification;user identification;identification performance;brain waves acquired;eeg;dimensionality reduction;identification accuracy;identification performance various;identification authentication;potential erp data;eegs 25;algorithm analyze brain;erp epoch;related potential erp", "pdf_keywords": ""}, "73e1dcf5f0f3cf4e645b0bba62d9b1e2ef47b706": {"ta_keywords": "parsing semantic;parsing semantic knowledge;semantic parsing;semantic parsing semantic;free grammars parsing;grammars parsing;natural language understanding;parsing;lexical class identification;parsing methods semantic;lexical classes based;natural language;classes based grammars;method semantic parsing;grammars parsing methods;ai speech natural;called natural language;grammars enriched semantic;language understanding nlu;semantic knowledge;lexical classes;parsing methods;lexical class;ai speech;extracting meaning utterance;implementation lexical class;semantic analysis lexical;language understanding;context free grammars;utterance computer", "pdf_keywords": ""}, "122b75042daae44f93153dedda15b0fb11b3f279": {"ta_keywords": "question answering datasets;answering qa datasets;question answering;learn question answering;bert based models;datasets evaluating bert;question answering qa;popular question answering;answering datasets;task question answering;question answering paper;learning qa datasets;evaluating bert based;models learn question;answering datasets evaluate;qa datasets models;evaluating bert;datasets models learn;answering qa;bert based;popular qa datasets;learning qa;qa datasets squad;models popular qa;qa datasets;bert;answering paper;really learning qa;models learn;qa datasets evaluating", "pdf_keywords": "comprehension qa datasets;comprehension answer questions;question answering reading;answering reading comprehension;reading comprehension qa;datasets qa models;bert models trained;datasets evaluating bert;question answering;bert based models;learning reading comprehension;learn reading comprehension;datasets answer questions;task question answering;answer questions dataset;methods reading comprehension;questions evaluate bert;bert models;questions dataset fully;future qa datasets;evaluate bert models;reading comprehension;comprehension new qa;models learning reading;questions dataset;qa datasets generalize;evaluating bert based;individual qa datasets;qa datasets qa;reading comprehension new"}, "e6602786132e040e02df93f729f737f65a116677": {"ta_keywords": "home assistants spoken;field speech recognition;digital home assistants;device speech processing;speech processing digital;speech recognition;speech recognition commands;speech processing;far field speech;speech interaction digital;device speech;capturing device speech;key speech processing;assistants spoken language;home assistants;speech processing algorithms;speech synthesis sophisticated;high quality speech;microphone array processing;recognition commands spoken;assistants spoken;speech synthesis;models speech;spoken language interface;home assistants combining;nonspecialist key speech;statistical models speech;field speech;speech interaction;processing deep learning", "pdf_keywords": ""}, "d8682a269523a868f2bc9714b00f0519aa0e931f": {"ta_keywords": "information retrieval whirl;databases ranked retrieval;deductive databases ranked;deductive databases;style deductive databases;ranked retrieval methods;queries integrate information;information retrieval;ranked retrieval;retrieval whirl approach;like queries structured;allows queries integrate;retrieval methods;text database like;queries structured;text database;using ir similarity;databases require equality;retrieval methods information;database like queries;database like;databases ranked;similarity metrics text;queries integrate;ir similarity metrics;whirl allows queries;conventional databases;methods information retrieval;queries structured collection;operations conventional databases", "pdf_keywords": ""}, "8cb74fe4f598699c9c24d88acd4906e2489267af": {"ta_keywords": "adaptive mapping navigation;mapping navigation teams;teams simple robots;mapping navigation;navigation teams simple;adaptive mapping;navigation teams;simple robots;mapping;robots;navigation;teams simple;adaptive;teams;simple", "pdf_keywords": ""}, "48aced0919e29722d6eed9544353d5507c541cfc": {"ta_keywords": "named entities drosophila;drosophila gene recognition;entities drosophila articles;gene recognition anaphoric;drosophila articles extending;entities drosophila;drosophila articles;linking gene names;flybase sequence ontology;drosophila gene;anaphoric linking named;demonstrates drosophila gene;drosophila;anaphoric linking gene;gene names products;recognition anaphoric linking;gene names;paper demonstrates drosophila;anaphora resolution algorithm;linking named entities;demonstrates drosophila;information flybase sequence;named entities;names products achieved;anaphora resolution;sequence ontology;baseline anaphora resolution;anaphoric linking;linking gene;flybase sequence", "pdf_keywords": ""}, "b3d9a0308ba6c4ca583a2b4e5be2b3eed466ccbc": {"ta_keywords": "obstacles millimeter wave;d2d channel needs;d2d channel;d2d communication highly;d2d communication;d2d channels using;d2d channels;d2d channel condition;delay d2d channel;communication moving obstacles;device d2d communication;learn d2d channels;relay exploring possible;channel needs propagated;quality d2d channel;relay exploring;good relay exploring;communication path quality;relay link communication;channel information useful;channel information;markov decision process;minimize delay channel;delay channel;mmwave device;delay channel quality;wave mmwave device;wave mmwave;channel condition;channels using", "pdf_keywords": "distributed relay selection;relay exploring;relay exploring possible;relay selection presence;good relay exploring;relay selection;problem relay selection;obstacles millimeter wave;relay link objective;distributed relay;relay selection pomdp;relay link communication;relay link successive;relay link explore;relay;failures current relay;switch good relay;good relay;relay link;explore switch relay;continue current relay;dynamic obstacles millimeter;millimeter wave mmwave;millimeter wave d2d;problem relay;wave mmwave;mmwave;minimize delay channel;formulate problem relay;dynamic obstacles outperforms"}, "fc8e226c20800c8ccc095bb6a3c0f8dcb637b683": {"ta_keywords": "lung metastases rectal;lung metastases china;colorectal cancer lung;lung metastases compared;cancer lung metastases;lung metastases conducted;management lung metastases;managing lung metastases;metastases rectal cancer;lung metastases crc;crc lung metastases;lung metastases;lung metastases 2019;incidence lung metastases;therapy colorectal cancer;metastases rectal;metastases china;rectal cancer cases;rectal cancer;metastases china encourage;mdt lung metastases;proportion rectal cancer;rectal cancer associated;colorectal cancer;metastases compared colon;metastases conducted extensive;metastases crc;metastases crc based;metastases compared;compared colon cancer", "pdf_keywords": ""}, "6c0a3029afd65c83982b3fb96f623da382344286": {"ta_keywords": "tensor factorization methods;tensor matrix factorization;matrix tensor factorization;tensor factorization;tensor factorization theory;topic models neural;learning topic models;matrix factorization methods;recent nlp applications;topic models;methods transductive learning;matrix factorization;transductive learning topic;transductive learning;factorization methods natural;discuss recent nlp;natural language processing;semantics dependency parsing;factorization methods attracted;recent nlp;factorization theory optimization;factorization methods;nlp applications;basics matrix tensor;methods natural language;nlp applications methods;lexical semantics dependency;matrix tensor;networks matrix tensor;factorization theory", "pdf_keywords": ""}, "928f942baf03dd56aae662fa94d85d22b5600f83": {"ta_keywords": "paraphrases searching tms;tms using paraphrases;paraphrases retrieve sentences;use paraphrases searching;paraphrases searching;paraphrases retrieve;using paraphrases retrieve;paraphrase pairs;paraphrase pairs used;domains paraphrase pairs;using paraphrases;use paraphrases;paraphrases;retrieve sentences meaning;propose use paraphrases;retrieve sentences;retrieve sentences statistical;narrow domains paraphrase;domains paraphrase;translation memories tms;used retrieve sentences;paraphrase;searching tms using;translation memories;searching tms;exactly translation memories;translating texts;tools translating texts;sentences statistical framework;parallel corpora", "pdf_keywords": ""}, "e30b22e692b3c7d2653832bf2901abd8a9375b6e": {"ta_keywords": "caching strategy cellular;distributed content caching;content caching strategy;content caching;caching strategy;contents cellular network;cache based knowledge;distributed content;caching;optimal content placement;cache based;equivalently maximize cache;content base station;corresponding cache based;maximize cache;store content base;optimal content;line distributed content;content stored serving;store content;placement contents cellular;decide store content;cellular network mobile;base station content;base stations content;strategy cellular networks;maximize cache hit;content base;minimizes rate download;cache", "pdf_keywords": "caching strategy cellular;distributed content caching;content caching strategy;caching strategy;content caching;content update cellular;cache content;algorithms cache content;caching;cache content update;contents cellular network;caches based;cache;rate cellular networks;update cellular network;caches based knowledge;algorithms cache;cellular networks;improve cache;update cellular;cellular network;contents cached neighbouring;provided algorithms cache;improve cache hit;cache update;cached neighbouring;caches;strategy cellular networks;cellular networks arpan;cellular network motivated"}, "2406cf39805c70264c4226b7325a09b506c70921": {"ta_keywords": "learning neural sql;tables pre training;training synthetic corpus;table pre training;trained language models;existing language models;neural sql executor;language models hit;modeling natural language;structured tables pre;pre trained language;boosting existing language;neural sql;corpus obtained automatically;synthetic corpus;trained language;language models;tables pre;improving wikisql denotation;wikisql denotation accuracy;corpus;structured tables;semi structured tables;executor synthetic corpus;table pre;language models 19;wikitablequestions denotation accuracy;synthetic corpus obtained;pre training learning;training learning neural", "pdf_keywords": "learning neural sql;neural sql executor;table pretraining achieved;synthetic corpus;neural sql;queries pre training;table pre training;data scarcity challenge;highquality synthetic corpus;unstructured textual data;executor synthetic corpus;language model pre;unstructured textual;textual data;language model mimic;table pretraining;mimic sql;synthetic corpus arxiv;corpus;challenge guiding language;recent progress language;synthetic corpus obtained;scale unstructured textual;model mimic sql;progress language model;language model;leveraging large scale;achieved learning neural;training learning neural;corpus obtained automatically"}, "3ee38da21d8cf9cb7d4077b729e57f68e9c8d671": {"ta_keywords": "generation machine translation;text generation largely;offline reinforcement learning;policy gradient summarization;generation policy learning;generation offline reinforcement;approaches text generation;learns demonstrations importance;text generation;text generation offline;learns demonstrations;policy learning demonstrations;evaluation models trained;algorithm learns demonstrations;machine translation;question generation machine;learning demonstrations;policy learning;gradient summarization;summarization question generation;question generation;learning demonstrations easy;reference training avoiding;algorithm learns;automatic human evaluation;frame text generation;reference training;models trained;models trained gold;model generated histories", "pdf_keywords": "training text generation;text generation learning;text generation likelihood;text generation largely;generation likelihood learning;approaches text generation;text generation;2021 text generation;text generation matched;mle training text;generation learning;generation learning demonstra;training text;framework text generation;generated history inference;learns high precision;generation likelihood;discrepancies mle training;models sensitive decoding;model generated history;likelihood learning objective;likelihood learning;rely autoregressive models;mle training;learns;gold learns;model generated;generation matched;generation matched train;vs model generated"}, "fdaad09b1a897c0a04b9a9579081d542e2b4546c": {"ta_keywords": "sars cov omicron;potency sars cov;omicron breakthrough infection;vaccination infection;vaccination infection interval;cov omicron breakthrough;neutralization potency sars;breakthrough infection variants;sars cov;infection variants;infection interval determines;vaccination;potency sars;breakthrough infection;cov omicron;omicron breakthrough;infection interval;cross neutralization potency;determines cross neutralization;infection;sars;cross neutralization;neutralization potency;omicron;cov;neutralization;interval determines cross;determines cross;cross;potency", "pdf_keywords": ""}, "74276a37bfa50f90dfae37f767b2b67784bd402a": {"ta_keywords": "mt5 massively multilingual;multilingual pre trained;partially translate prediction;mt5 multilingual;introduce mt5 multilingual;mt5 multilingual variant;translate prediction;massively multilingual pre;language mt5 massively;language nlp tasks;pre trained text;massively multilingual;multilingual variant t5;language mt5;trained text;translate prediction wrong;trained text text;language nlp;multilingual;multilingual pre;nlp tasks;text transfer transformer;prediction wrong language;text text transfer;text transfer;text text transformer;languages recent text;chooses partially translate;translation zero shot;nlp tasks simple", "pdf_keywords": "trained generative multilingual;generative multilingual;generative multilingual language;multilingual language models;mt5 multilingual;mt5 introduce multilingual;introduce mt5 multilingual;multilingual variants t5;mt5 multilingual variant;multilingual variant t5;massively multilingual variants;mc4 massively multilingual;massively multilingual;languages train mt5;multilingual;multilingual variants;multilingual language;multilingual variant c4;straightforwardly applicable multilingual;introduce multilingual;multilingual setting achieved;language models zero;introduce multilingual variant;multilingual variant;multilingual setting;prediction wrong language;language models;languages train;translate prediction;applicable multilingual"}, "00b2afaf5935b4dea41f134fe11a21a1ed56fa0e": {"ta_keywords": "sentiment analysis czech;supervised sentiment analysis;supervised sentiment;sentiment analysis;czech social media;analysis czech social;analysis czech;czech social;sentiment;czech;social media;supervised;social;analysis;media", "pdf_keywords": ""}, "1bf36cb3453b51550ebadd904a840c75d59f171b": {"ta_keywords": "robust asr;robust asr research;robust speech recognition;overview robust asr;robust speech;era robust speech;speech recognition;robustness issues deep;asr signal processing;basic formulations asr;network based asr;robust;new era robust;formulations asr;formulations asr signal;based asr;deep neural network;speech recognition chapter;deep learning;robustness;asr;asr research;robustness issues;asr signal;deep learning era;based asr new;overview robust;asr research including;neural networks;neural networks chapter", "pdf_keywords": ""}, "8b20173b98914f36302389e4c761c334fe867dcd": {"ta_keywords": "directly dependency treebanks;dependency parse morphosyntactic;dependency treebanks;robust parsers;robust parsers present;train robust parsers;treebanks;parse morphosyntactic rules;morphosyntactic formedness text;language evaluating morphosyntactic;parse morphosyntactic;evaluating morphosyntactic formedness;morphosyntactic rules language;parsers;morphosyntactic formedness generated;systems translating morphologically;evaluating morphosyntactic;machine translation diachronic;translating morphologically rich;evaluate morphosyntactic formedness;dependency parse;morphologically rich languages;morphosyntactic rules;parsers present;parsers present way;translating morphologically;machine translation;morphosyntactic formedness;using dependency parse;morphosyntactic", "pdf_keywords": "dependency parsers morphological;treebanks effectiveness metric;parsers morphological feature;existing treebanks effectiveness;nlg systems;parsers morphological;treebanks effectiveness;machine translation diachronic;language generation nlg;robust dependency parsers;nlg systems metric;produced nlg systems;existing treebanks;machine translation wmt;generation nlg;treebanks;language processing nlp;machine translation;dependency parse morphosyntactic;morphosyntactic wellformedness text;machine translation mt;dependency parsers;generation nlg paper;systems translating morphologically;parsers;text produced nlg;evaluate morphosyntactic wellformedness;processing nlp;parse morphosyntactic rules;morphologically rich languages"}, "3dcba175248d0e8d2da44e3731e4adbfb9f00e97": {"ta_keywords": "open information extraction;sentence level extractions;extractions leveraging facts;information sentence extract;extracted individual sentences;sentence extract relation;extractions using corpus;information extraction;sentence extract;knowledge bases supervision;entity relation phrases;extractions leveraging;facilitate sentence level;corpus collectively leveraged;sentence level tuple;leveraging facts external;external knowledge bases;sentences based local;large corpus;using corpus level;large corpus collectively;tuple extractions using;tuple extractions;corpus collectively;using corpus;knowledge bases;learning subtasks jointly;statistics large corpus;level tuple extractions;individual sentences based", "pdf_keywords": "open information extraction;extracting entities relations;information extraction;extracting entities;abstract extracting entities;massive text corpora;entities relations text;challenges understanding extracting;insights massive corpora;information extraction arxiv;emergence massive text;extract relation tuples;remine extract relation;framework remine extract;extracting insights massive;understanding extracting insights;extract relation;edu abstract extracting;abstract extracting;entities relations;relations text important;extracting insights;cohesiveness open information;text corpora;understanding extracting;relations text;context global cohesiveness;massive corpora;distant supervision framework;massive corpora propose"}, "96dbffb71e4d62a985f826197845623b1415c267": {"ta_keywords": "learners better metacognition;faster learners;metacognitive learning acquiring;metacognitive learning;learn faster learners;faster learners better;yielding metacognitive learning;knowledge better learning;learners better;metacognition acquire knowledge;learn faster;better metacognition;better learning;future learning;algorithm yielding metacognitive;acquire knowledge faster;assist future learning;yielding metacognitive;future learning preliminary;students learn faster;learning;better metacognition acquire;understand human learning;human learning;knowledge faster;learners;metacognitive;better learning strategy;human learning computational;speed learning", "pdf_keywords": ""}, "ce9919ffb9dab701babd67a945b1590917345789": {"ta_keywords": "explanation based learning;multiple explanations training;explanations training instance;explanations training;explanations training example;possible explanations training;applying explanation based;produces multiple explanations;multiple inconsistent explanation;inconsistent explanation problem;choose possible explanations;explanation based;learning ebl solves;theories multiple inconsistent;inconsistent explanation;explanations;explanation problem occurs;based learning solution;learning ebl imperfect;abductive explanation based;learning called abductive;multiple explanations;based learning ebl;training example;applying explanation;training example problem;learning solution multiple;learning solution;based learning;knowledge level ebl", "pdf_keywords": ""}, "53e161d4434576355fc5f63fe56afd8e135174b2": {"ta_keywords": "scripts generation;script generation;models generate scripts;trained language models;neural language models;ordered scripts generation;language models collected;combining language generation;scripts generation pre;language generation structure;language generation;language models;trained neural language;generate scripts;high quality scripts;generate events organize;script generation given;neural language;scripts combining language;language models lms;generate events;crowdsourced partially ordered;trained language;generate scripts combining;pre trained language;understand narratives providing;scenario generate events;event sequences describing;partially ordered scripts;narratives providing expectations", "pdf_keywords": "script generation tasks;learning script knowledge;script generation;entire script generation;learning script;prediction entire script;neural language models;script generation investigate;trained neural language;high quality scripts;script knowledge;partial order scripts;neural language;script edge prediction;direction learning script;application script generation;generation tasks dataset;pre trained neural;language models;order scripts;language models adapted;scripts;quality scripts;generation tasks;quality scripts including;trained neural;order scripts second;scripts second;language models lms;scripts second collected"}, "6c59a6ad00d82ca9f76fef92232ff3e2f3c1acc8": {"ta_keywords": "speaker diarization;overlap aware diarization;speech speaker diarization;aware diarization outputs;voice activity detection;speaker diarization modify;aware diarization;handling overlapping speech;overlapping speech speaker;overlapping speech;diarization outputs speech;speaker voice activity;target speaker voice;combining outputs diarization;diarization systems majority;voice activity;networks target speaker;outputs speech;speaker voice;diarization systems;outputs diarization systems;diarization outputs;combining overlap aware;speech speaker;partite graph matching;combining outputs diverse;region proposal networks;diverse systems clustering;outputs speech natural;target speaker", "pdf_keywords": "fusion multichannel diarization;fusion array microphones;late fusion multichannel;fusion multichannel;voice activity detection;multichannel diarization;diarization systems clustering;multichannel diarization compares;speaker voice activity;target speaker voice;voice activity;combining diarization;combining outputs diverse;diverse systems clustering;array microphones;fusion methods;diarization systems;networks target speaker;diarization compares favorably;late fusion array;early fusion methods;speaker voice;voice;effective combining outputs;meeting datasets;label voting;fusion methods like;outputs diverse systems;method combining diarization;diarization compares"}, "a5881560968963d0c845c468a273261fde0b7248": {"ta_keywords": "robust trustworthy nlp;text interpretability;trustworthy nlp;text interpretability methods;language model predictions;predictions relative word;interpretations need robust;word importance scores;natural language model;trustworthy nlp applications;swaps adversarial perturbations;generate fragile interpretations;nlp datasets small;adversarial perturbations;input text interpretability;word perturbations input;relative word importance;nlp;swaps adversarial;level swaps adversarial;interpretability methods;interpretability;words perturbed average;words perturbed;adversarial perturbations aim;natural language;models different nlp;resulting text semantically;10 words perturbed;interpretability methods like", "pdf_keywords": "inputs fragile interpretations;fragile interpretations deep;interpretability methods;interpretability;interpretability approaches;abstract interpretability methods;interpretability approaches lime;fragile interpretations;interpretability methods like;fragile interpretations demonstrate;language model predictions;2021 abstract interpretability;abstract interpretability;deep natural language;explaining natural language;natural language model;box interpretability approaches;white box interpretability;evidence fragile interpretations;gradient lime nlp;interpretations deep natural;models adversarially trained;interpretations deep;predictions relative word;perturbing inputs fragile;adversarially trained;perturbed text examples;models adversarially;word importance scores;nlp models"}, "3004a3e4d8969dc3c36c9274b0f76ecc874f2e6a": {"ta_keywords": "separation speech embedded;boosted speech separation;speech separation using;speech separation;speech recognition outputs;speech embedded;recognition separation promising;recognition boosted speech;networks incorporating speech;separation speech;incorporating speech recognition;speech recognition;separation using deep;recognition separation;phase sensitive recognition;speech embedded non;boosted speech;incorporating speech;deep recurrent neural;recurrent networks incorporating;bidirectional recurrent networks;noisy spectrum viable;recurrent neural networks;recurrent networks;integration recognition separation;snr reconstructed signal;noisy spectrum;spectral input features;snr reconstructed;applied noisy spectrum", "pdf_keywords": ""}, "e29e43d9c0772d44cff53044484970599db30d5f": {"ta_keywords": "domain adaptation neural;domain adaptation;results domain adaptation;domain differential adaptation;neural machine translation;machine translation experimental;improvements alternative adaptation;differential adaptation neural;task domain differential;machine translation;adaptation neural;adaptation neural machine;differential adaptation dda;translation experimental results;adaptation dda instead;adaptation dda;domains tasks inherently;different domains tasks;alternative adaptation;differential adaptation;translation experimental;domains tasks;modeling difference domains;source target domains;machine translation demonstrate;labeled data domain;alternative adaptation strategies;adaptation;domains drawback statistics;target domains drawback", "pdf_keywords": "domain adaptation nmt;domain adaptation;unsupervised domain adaptation;domain adaptation machine;approach domain adaptation;adaptation machine translation;domain differential adaptation;metrics domain adaptation;neural machine translation;domain adaptation settings;adaptation nmt neural;differential adaptation dda;adaptation dda instead;adaptation dda;differential adaptation neural;machine translation;adaptation neural;adaptation dda utilizes;capture domain differences;adaptation framework domain;machine translation analyses;adaptation neural machine;machine translation zi;unsupervised adaptation framework;adaptation nmt;modeling difference domains;new unsupervised adaptation;hungry domain sensitive;domain differences;adaptation settings language"}, "f6eafb82d2450f28f668443b689c91e896a0d63e": {"ta_keywords": "bandits thompson sampling;linear bandits thompson;thompson sampling;thompson sampling 2010;linear bandits;lecture linear bandits;bandits thompson;bandits;web advertisement;adaptive machine learning;web advertisement introduction;adaptive;advertisement introduction problem;context web advertisement;adaptive machine;sampling 2010 context;sampling;advertisement;advertisement introduction;thompson;sampling 2010;machine learning;learning;machine learning winter;explored variants problem;problem works improved;web;improved analysis;improved;explored", "pdf_keywords": ""}, "8225b047e0fe90c2d5f9bb77fd94396a9d0fd21e": {"ta_keywords": "dictionary geneid ranking;gene identifiers documents;named entity recognition;entity recognition;dictionary geneid;associating gene identifiers;gene identifiers;soft dictionary geneid;entity recognition ner;geneid ranking;dictionary gene synonyms;gene identifiers experimentally;identifiers documents;article identifier gene;solving geneid ranking;geneid ranking backgroundone;geneid ranking problem;recognition ner geneid;geneid finding accurately;possible gene identifiers;gene synonyms;identifier gene;gene synonyms evaluate;ner geneid finding;identifier gene discussed;geneid finding;dictionary gene;organism database;named entity;organism database curation", "pdf_keywords": ""}, "3873e60de2d20aa33829e2d3d79221e716785546": {"ta_keywords": "unlabeled speech discriminative;discriminative language modeling;semi supervised learning;features unlabeled speech;discriminative language models;speech discriminative language;semi supervised;conversation based features;estimate discriminative language;unlabeled conversations;gram features;speech discriminative;proximity unlabeled conversations;unlabeled conversations used;used semi supervised;unlabeled speech;discriminative language;language modeling;language models;deriving conversation based;language modeling paper;supervised learning;model deriving conversation;language models correct;gram features appear;perceptron trains discriminative;supervised;features labeled data;deriving conversation;supervised learning ssl", "pdf_keywords": ""}, "229c0c13e5c2d8e189efccf77b8179ec16500212": {"ta_keywords": "string machine translation;machine translation;machine translation andfind;machine translation mt;englishjapanese machine translation;greater accuracy translation;accuracy translation using;accuracy translation;tree transducers provides;based tree transducers;parsers alignment techniques;phrase based translation;tree transducers;forest string machine;syntactic parsers alignment;syntactic parsers;decoder englishjapanese machine;decoding evaluation training;parsers;based translation auxiliary;translation mt engine;phrase based hierarchical;different syntactic parsers;translation auxiliary results;hierarchical phrase based;tuning decoding evaluation;decoder englishjapanese;decoding evaluation;travatar forest string;translation using", "pdf_keywords": ""}, "14fce3cfa503894f244fc6ea8a7a00fa0ddfd94e": {"ta_keywords": "teach computer ethics;computer ethics science;computer ethics;ethics science fiction;ethics science;ethics;teach computer;science fiction;submission cacm research;cacm research;cacm;research highlights teach;cacm research highlights;teach;submission cacm;fiction;computer;highlights teach computer;science;research;submission;research highlights;highlights teach;highlights", "pdf_keywords": ""}, "a54a3a7b02cacd92b3bc633be7ea54e4f365fa65": {"ta_keywords": "malware communities;detecting malware communities;malware binary communities;malware communities using;malware features explored;computer malware features;malware features;detecting malware;visualize analyze malware;computer malware;characterized malware;malware binary;malware;characterized malware binaries;berlin characterized malware;analyze malware;malware binaries;analyze malware binary;functions detecting malware;malware binaries benign;scm computer malware;benign malicious based;malicious based;binaries benign malicious;malicious based 1024;remote access trojan;benign malicious;access trojan;malicious;binary communities", "pdf_keywords": ""}, "935c275868bec7301f4bd254159978d8ded138b9": {"ta_keywords": "xylazole exerts anesthesia;analgesia fetal rat;xylazole;anesthesia analgesia fetal;anesthesia analgesia;nerve cells cgmp;exerts anesthesia analgesia;xylazole exerts;analgesia fetal;fetal rat nerve;analgesia;rat nerve;cgmp signaling pathway;cgmp signaling;cells cgmp signaling;rat nerve cells;anesthesia;cells cgmp;nerve cells;exerts anesthesia;nerve;cgmp;fetal rat;signaling pathway;signaling;rat;fetal;cells;pathway;exerts", "pdf_keywords": ""}, "b9f0c7e99bcc94c2cd75fd8e1cef45188f51270e": {"ta_keywords": "graph temporal classification;speech represented graph;graph speaker information;learning transcriptions speaker;represented graph speaker;graph semi supervised;graph speaker;connectionist temporal classification;speaker speech dataset;speech dataset;transcriptions speaker information;label sequences graph;supervised learning transcriptions;automatic speech;speech recognition asr;speech recognition;temporal classification loss;transcriptions speaker;graph based supervision;learning transcriptions;speech dataset derived;label transitions neural;automatic speech recognition;temporal classification gtc;classification multi speaker;semi supervised;temporal classification multi;improve automatic speech;temporal classification;based temporal classification", "pdf_keywords": "speaker transition posterior;label transitions neural;speaker asr transcribes;transitions neural network;speech processing;speaker transition;asr transcribes speech;speaker asr considered;speaker asr;speech processing example;transition posterior predictions;networks predict posterior;token alignment speaker;\ufb01eld speech processing;label transitions;multi speaker asr;transcribes speech;single speaker asr;asr transcribes;alignment speaker transition;transcribes speech similar;tasks multi speaker;transitions neural;gtc multi speaker;gtc model posteriors;labels label transitions;predict posterior probabilities;posterior probabilities labels;transition posterior;predict posterior"}, "dfa34a10e2ba861545549c3188ef245b1e69bcdf": {"ta_keywords": "bio event extraction;text event extraction;word embedding bio;word embedding features;event extraction;embedding bio event;event extraction important;word embedding;event extraction using;word embedding methods;extracting biological networks;using word embedding;latest word embedding;introduction word embedding;bio event;words text event;extracting biological;goal extracting biological;embedding bio;using bag ofwords;biological networks scientific;networks scientific literature;embedding features;words text;extracting;biological networks;bag ofwords;embedding methods;embedding methods using;extraction using", "pdf_keywords": ""}, "2ea5b0f5e476ddc00ae4450f2888a51fa25dd1d3": {"ta_keywords": "tackling nlp tasks;nlp tasks large;training task augmentation;nlp tasks;trained language models;better shot learning;task augmentation broad;task augmentation better;successes tackling nlp;task unlabeled texts;task augmentation approach;shot learning experiments;shot learning;self training task;task augmentation novel;task augmentation;tackling nlp;created task augmentation;pre trained language;language models perform;target task unlabeled;data auxiliary task;trained language;uses task augmentation;training task;12 shot benchmarks;unlabeled texts;unlabeled texts second;augmentation better shot;language models", "pdf_keywords": "training task augmentation;training data auxiliary;selftraining task augmentation;domain nli training;task natural language;trained generative language;task augmentation selftraining;generative language model;task augmentation train;data auxiliary task;language inference nli;trained language model;auxiliary task data;approach task augmentation;pre trained generative;large domain training;natural language inference;nli data generation;model auxiliary task;texts data generation;task augmentation effectively;data self training;trained generative;domain training;domain auxiliary task;nli training data;generative language;combining task augmentation;language model auxiliary;task augmentation"}, "92f93c0014ba4da59180c4cd141ad0dcaad5803f": {"ta_keywords": "multilingual transfer learning;transfer learning multilingual;multilingual deep retrieval;multilingual transfer;learning multilingual deep;multilingual deep;learning multilingual;type multilingual transfer;improve performance multilingual;transfer learning instance;transfer learning improve;performance multilingual;target language improvement;transfer learning;auxiliary languages pooled;target language auxiliary;languages similar target;languages pooled;multilingual;languages pooled single;deep retrieval;performance multilingual setting;transfer learning help;based transfer learning;target language;transfer learning data;deep retrieval immediately;language improvement;multilingual setting instance;direct vocabulary overlap", "pdf_keywords": "multilingual deep;multilingual vocabulary embeddings;formulated multilingual deep;cross language instance;multilingual deep table;transfer learning surprisingly;transfer learning large;transfer learning significantly;vocabulary embeddings end;target languages tasks;surprisingly effective multilingual;multilingual vocabulary;cross language;tasks sentence prediction;sentence prediction inverse;embeddings languages;deep retrieval tasks;transfer learning;deep retrieval;vocabulary embeddings;languages tasks;embeddings languages mapped;learn multilingual vocabulary;tested sentence prediction;learn multilingual;network learn multilingual;examine deep retrieval;multilingual;sentence prediction nsp;siamese network learn"}, "4a9c80e263fd0a88ad8220aa076ede4a3e77fcc1": {"ta_keywords": "adversarial samples deep;vulnerable adversarial samples;vulnerable adversarial;deep testing methods;adversarial attacks mutated;deep testing;vulnerability dnn systems;detect various adversarial;confidence adversarial samples;various adversarial samples;adversarial samples;adversarial samples generated;vulnerability dnn;high confidence adversarial;proposed vulnerability dnn;known vulnerable adversarial;adversarial samples makes;various adversarial;adversarial samples detection;confidence adversarial;kinds adversarial attacks;adversarial;adversarial attacks;kinds adversarial;different kinds adversarial;number deep testing;dnn known vulnerable;dnn models;networks dnn known;dnn models ggt", "pdf_keywords": "testing ggt adversarial;detects adversarial samples;ggt detects adversarial;detects adversarial;adversarial samples 93;vulnerability dnn systems;detect various adversarial;adversarial samples generated;vulnerability dnn;deep testing methods;adversarial sample detection;various adversarial samples;adversarial samples;adversarial sample;ggt adversarial sample;deep testing;kinds adversarial attacks;various adversarial;ggt adversarial;\ufb01nd vulnerability dnn;adversarial attacks;adversarial;kinds adversarial;adversarial attacks conclusion;different kinds adversarial;graph guided testing;model mutation testing;graph prune dnn;number deep testing;prune dnn model"}, "885fe11ed7ab81c8609ccddb3e10f62577c04ab9": {"ta_keywords": "learning agents dialogue;exploration deep learning;exploration deep;efficiency exploration deep;explore thompson sampling;agents dialogue systems;learning agents;algorithm learns;algorithm learns faster;learns;learns faster;deep learning agents;thompson sampling;agents dialogue;exploration strategies epsilon;improves efficiency exploration;dialogue systems;agents explore thompson;agents explore;bootstrapping intrinsic reward;episodes make learning;intrinsic reward based;replay buffer experiences;exploration strategies;greedy boltzmann bootstrapping;learning feasible;strategies epsilon greedy;dialogue;common exploration strategies;exploration", "pdf_keywords": "reinforcement learning deep;exploration deep reinforcement;deep reinforcement learning;learning agents dialogue;deep qnetworks thompson;qnetworks thompson sampling;neural networks reinforcement;deep reinforcement;networks reinforcement learning;exploration deep learning;networks reinforcement;deep qnetworks;learning deep qnetworks;explore thompson sampling;thompson sampling dialogue;sampling dialogue systems;exploration deep;reinforcement learning;agents dialogue systems;learning agents;reinforcement learning task;deep learning agents;thompson sampling;explores thompson sampling;bbqn explores thompson;ef\ufb01cient exploration deep;agents explore thompson;qnetworks thompson;dialogue systems arxiv;agents dialogue"}, "a1340029d8a5c57bee8a5995ac3beafd3d0ba96c": {"ta_keywords": "sensing tracking markov;kalman consensus filtering;distributed tracking;active sensing tracking;tracking markov chain;distributed tracking systems;operation distributed tracking;gibbs sampling sensor;sensing tracking;tracking markov;kalman consensus;estimates process node;consensus filtering process;filtering process estimation;averaged sensors presence;constraint kalman consensus;estimates averaged sensors;sampling sensor subset;tracking systems problem;sensor subset selection;collection wireless sensors;consensus filtering;tracking systems;sensor subset;averaged sensors;sensors presence computing;active sensing;process node measurements;active sensors mean;process estimation", "pdf_keywords": ""}, "c6854064cb5053e67d23394eee6d1646108f6d56": {"ta_keywords": "textual entailment task;textual entailment;novel textual entailment;setting textual entailment;standard textual entailment;entailment task;textual entailment define;trivial lexical inferences;entailment task requires;lexical inferences;lexical inferences emphasizes;multiple premise sentences;premise sentences;entailment;inferences emphasizes knowledge;entailment define;inferences emphasizes;inference multiple premise;task requires inference;entailment define novel;textual;inference multiple;premise task;challenging setting textual;sentences;inferences;requires inference multiple;multiple premise task;inference;novel textual", "pdf_keywords": "neural entailment baselines;neural entailment;textual entailment task;strong neural entailment;entailment baselines dataset;entailment task premise;novel textual entailment;textual entailment;entailment baselines;entailment task;setting textual entailment;separate premise sentences;textual entailment conclusion;standard textual entailment;entailment task involves;challenging realistic entailment;multiple premise sentences;entailment task requires;independent premise sentences;textual entailment evaluate;sentences introduced dataset;task premise text;entailment;entailment conclusion presented;premise sentences;entailment conclusion;premise texts aggregation;entailment problem;entailment problem solved;trivial lexical inferences"}, "74b05adf1ec74849a4f7963fe3f17fd61b92af4b": {"ta_keywords": "query analysis nlidb;queries contextual;learn query;queries contextual information;learn query languages;databases nlidb attracted;having learn query;follow query analysis;databases nlidb;multiple queries contextual;termed follow query;query intents propose;follow query scenarios;interfaces databases nlidb;like query languages;natural language interfaces;query intents;query languages;language interfaces databases;query analysis;query analysis studied;query scenarios lack;like query;users query intents;involves multiple queries;queries employs ranking;users search databases;queries employs;queries;query languages multi", "pdf_keywords": "query analysis nlidb;queries employs ranking;follow query analysis;databases nlidb attracted;termed follow query;ranking model weakly;natural language interfaces;typical follow query;weakly supervised max;new followup dataset;query analysis qian;databases nlidb;followup dataset;follow query scenarios;interfaces databases nlidb;language interfaces databases;followup dataset 1000;query analysis;follow query;weakly supervised;like query languages;ranking model;ranking;query languages;like query;perform follow query;queries employs;model weakly supervised;queries;query analysis considers"}, "a5f214e23b8cd35a370a182c155ef333d77c5bb2": {"ta_keywords": "phonetic correlates stance;stance natural speech;stance speaker attitudes;stance speaker;correlates stance taking;correlates stance;annotated stance;acoustic indicators stance;annotated stance strength;stance complex activity;stance natural;corpus collaborative conversational;stance taking using;stance complex;textually conversation;speaker attitudes;investigated textually conversation;neutral phonetic correlates;ranges stance speaker;stance taking;given stance complex;conversation discourse;speaker attitudes opinions;natural speech;suggests increases stance;hand annotated stance;stance;phonetic correlates;conversation discourse analysis;acoustic phonetic", "pdf_keywords": ""}, "b33caf27fe5584b9b773c75fc35ee0e8b1421864": {"ta_keywords": "population potential games;continuous population potential;potential games population;population potential;equilibrium type based;wardrop equilibrium type;subset strategies potential;equilibrium type;strategies potential game;wardrop equilibrium;equilibrium means optimizing;games population mass;extension wardrop equilibrium;potential game;games population;strategies subset dimension;continuous population;equilibrium means;strategies potential;optimizing specific potential;strategies subset;equilibrium;potential games;subset strategies;compute equilibrium means;preference strategies subset;population mass represented;population member type;model compute equilibrium;extension continuous population", "pdf_keywords": ""}, "5ee96dd7e3395d8a53d6d3ceb62593477a4e0fe1": {"ta_keywords": "color words captions;language automatic colorization;automatic colorization;color language automatic;automatic colorization process;words captions;colorizations;colorization;colorized;users manipulate colorized;manipulating descriptive color;alter colorizations;learning color language;colorizations simply manipulating;color language;captions;colorizations simply;feeding different captions;alter colorizations simply;dramatically alter colorizations;descriptive color words;learning color;manipulate colorized;color words;captions furthermore;captions furthermore demonstrate;manipulate colorized image;colorized image;colorization process adding;colorization process", "pdf_keywords": "colorization natural language;language based colorization;learning color language;learning color;color language;colorization natural;based colorization augment;abstract automatic colorization;colorization augment;colorization augment existing;automatic colorization;colorized art;colorization;based colorization;automatic colorization process;accuracy based colorization;\ufb01nal colorized art;colorized art varun;task colorization natural;colorization process;colorized;colorist;color language mohit;based colorization metric;colorization metric;colorization process adding;\ufb01nal colorized;task colorization;learned image captions;introduce task colorization"}, "53f1fb4dc887540ef134a8d08c152789c313aa5c": {"ta_keywords": "end speech recognition;speech recognition;language speech data;trained phoneme transcripts;triphone based dnn;phoneme transcripts based;speech recognition systems;speech data;speech data using;dnn hmm based;multi language speech;kaldi toolkit corpus;corpus spontaneous japanese;phoneme transcripts;vectors extracted multilingual;hmm trained phoneme;extracted multilingual end;using character transcripts;toolkit corpus spontaneous;trained multi language;trained phoneme;based dnn hmm;hmm character based;language speech;character transcripts;multilingual end;japanese asr;hmm based csj;multilingual end end;end asr based", "pdf_keywords": ""}, "7262bc3674c4c063526eaf4d2dcf54eecea7bf77": {"ta_keywords": "paraphrase pairs paranmt;paraphrase generation;paraphrastic sentence embeddings;sentential paraphrase pairs;paraphrase pairs;used paraphrase generation;neural machine translation;sentence embeddings millions;sentence embeddings outperform;train paraphrastic sentence;english sentential paraphrase;millions machine translations;large parallel corpus;semantic textual similarity;train paraphrastic;sentential paraphrase;paraphrastic sentence;machine translation;textual similarity competition;paraphrastic;parallel corpus;sentence embeddings;showing used paraphrase;machine translations;paraphrase;parallel corpus following;textual similarity;semeval semantic textual;50m train paraphrastic;semantic textual", "pdf_keywords": "paraphrastic sentence embeddings;sentence embeddings outperform;generating paraphrases;sentence embeddings;sentence embeddings using;paraphrases purposes data;generating paraphrases purposes;used generating paraphrases;train paraphrastic sentence;modeling sentence pair;dataset aligned sentences;train paraphrastic;paraphrases;embeddings outperform supervised;embeddings using learning;paraphrases purposes;using train paraphrastic;paraphrastic;embeddings outperform;paraphrastic sentence;embeddings using;dataset code embeddings;paranmt50m train paraphrastic;million sentential dataset;embeddings available;embeddings;modeling sentence;augmentation robustness grammar;sentences;curated lexical"}, "6d2d86cf5e80b58a03360559095ea3603548248f": {"ta_keywords": "prior algorithms;projected gradient descent;prior algorithms underlying;outperforms prior algorithms;constraint continuous regularization;estimate matrix rows;regularization;convex objective heuristic;algorithm outperforms prior;non convex objective;estimator optimal logarithmic;estimator optimal;gradient descent;squares estimator optimal;convex objective;increasing unknown permutation;optimal;minimum non convex;efficient algorithms;continuous regularization;statistical seriation empirically;rankedness signal noise;constraint permutation columns;statistical seriation;heuristic statistical seriation;statistical seriation problem;gradient descent obtain;regularization term;goal estimate matrix;estimate matrix", "pdf_keywords": ""}, "2d9769ce319a8acbe97438b45b0d381db2a538d1": {"ta_keywords": "shinji watanabe librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid;watanabe librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid;librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid acc best;watanabe librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid acc;espnet2 pretrained model;librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid;librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid acc;espnet2 pretrained;fs 16k lang;pretrained model shinji;espnet2;16k lang;fs 16k;pretrained model;16k lang en;best fs 16k;pretrained;model shinji watanabe;model shinji;16k;fs;best fs;model;shinji watanabe;acc best fs;lang en;lang;en;shinji;watanabe", "pdf_keywords": ""}, "c143d2b09bdfc0dff784dce2668fd5657806dbf2": {"ta_keywords": "challenge task explanations;inference explanation regeneration;explanation ranked;explanation ranked highest;task explanations consist;task explanations;explanation regeneration tasks;shared task knowledge;inference speed challenge;fact explanations standardized;explanations standardized;explanations standardized science;explanations consist;multi fact explanations;answer knowledge base;answer knowledge;regeneration shared task;explanations consist average;topics increase difficulty;tasks participants regenerating;task knowledge;appear explanation ranked;2020 shared task;rank fact knowledge;task knowledge base;inference speed;challenging scientific topics;core scientific knowledge;scientific knowledge world;scientific topics increase", "pdf_keywords": ""}, "2873053aa18059a61ead5880d449f5bccda2d213": {"ta_keywords": "interdependent scheduling games;scheduling games;scheduling games player;nash dynamics;players interdependent services;computation nash equilibria;nash dynamics existence;game theoretic analysis;undertake game theoretic;responses nash dynamics;existence computation nash;computation nash;schedule services;game theoretic;interdependent scheduling;services schedule;services schedule independently;scheduling;welfare maximization computing;model interdependent scheduling;welfare maximization;nash equilibria;independently undertake game;set services schedule;schedule services time;interdependent services;interdependent services motivated;services motivated problems;player predecessor services;player free schedule", "pdf_keywords": "services game theoretic;schedule services game;interdependent scheduling games;scheduling games;undertake game theoretic;game theoretic analysis;services game;welfare maximization existence;computation nash equilibria;nash dynamics existence;nash dynamics;game theoretic;questions welfare maximization;scheduling games motivated;existence computation nash;welfare maximization;responses nash dynamics;nash equilibria conclusions;interdependent scheduling;computation nash;nash equilibria consider;welfare maximization computing;welfare maximization best;nash equilibria;issues welfare maximization;undertake game;scheduling;humanitarian logistics answering;schedule services;players setting independent"}, "fee62123e1d2ac56065675983475b079e1e9106f": {"ta_keywords": "beam search decoding;trained beam decoding;trained greedy decoding;training neural sequence;neural sequence models;search decoding;sequence models beam;greedy decoding;entropy trained beam;decoding algorithm neural;beam decoding baselines;decoding baselines;cross entropy training;decoding cross entropy;entropy training;search decoding procedure;cross entropy trained;models beam search;final decoding;end training neural;entropy training procedures;greedy decoding cross;entropy trained;algorithm neural sequence;beam decoding;decoding baselines order;neural sequence;decoding;beam search end;test time decoding", "pdf_keywords": "neural sequence models;beam search decoding;trained beam decoding;training neural sequence;trained greedy decoding;beam search desirable;abstract beam search;search decoding;beam decoding baselines;beam search;algorithm neural sequence;beam search composed;beam search continuous;decoding baselines approach;decoding baselines;relaxation beam search;beam search end;approximation beam search;process beam search;end training neural;greedy decoding;entropy trained beam;decoding algorithm neural;results sequence tasks;search decoding procedure;sequence tasks;neural sequence;sequence models;beam decoding;entity recognition ccg"}, "a96e05353032cc6f3d72eb5eca192295beac065e": {"ta_keywords": "stacked graphical learning;graphical learning meta;meta learning scheme;graphical learning;graphical learning learning;graphical learning efficient;instances stacked graphical;learning learning markov;learning meta learning;meta learning;markov chains stacked;stacked graphical;learning markov random;learning markov;base learner;related instances stacked;learning meta;instances stacked;learning learning;learning scheme;described stacked graphical;learning scheme base;learning efficient;markov random fields;learning;base learner augmented;predictions related instances;described stacked;chains stacked graphical;stacked", "pdf_keywords": ""}, "c52ac453e154953abdb06fc041023e327ea609a4": {"ta_keywords": "attention acoustic modeling;self attention acoustic;attention acoustic;baseline based lstms;lstms;context acoustic signal;local context acoustic;based lstms;acoustic modeling;attention memory;context range models;context acoustic;self attention memory;acoustic modeling computational;lstms network;self attention method;acoustic modeling proposing;attention method encoding;based lstms network;apply acoustic modeling;attention method;encoding sequences vectors;lstms network network;attention;self attention;attention memory grows;acoustic signal;context range;propose gaussian biasing;memory", "pdf_keywords": "attention acoustic modeling;attentional architectures acoustic;self attention acoustic;attention acoustic;self attentional architectures;attentional architectures;self attention memory;acoustic modeling;attentional architectures work;acoustic modeling computational;attention memory;acoustic modeling challenging;context acoustic;acoustic modeling using;acoustic modeling challenges;local context acoustic;context acoustic signal;architectures acoustic modeling;acoustic modeling proposing;self attentional;apply acoustic modeling;work acoustic modeling;self attention;make self attentional;attentional;applying self attention;explore self attentional;apply self attention;attention;spell model"}, "7ac4227d0b4d38b16da27ed55bd53ce240a32404": {"ta_keywords": "autoregressive modelings speech;automatic speech;modelings speech text;speech recognition asr;speech text generation;modelings speech;end automatic speech;automatic speech recognition;autoregressive nar models;speech text;speech recognition;non autoregressive modelings;end speech translation;long form utterances;autoregressive nar;autoregressive modelings;nar asr accuracy;non autoregressive nar;speech translation;autoregressive baselines;recognition asr;compared autoregressive baselines;autoregressive baselines work;asr accuracy;asr accuracy speed;understanding nar asr;autoregressive;utterances comparative;nar models;utterances comparative study", "pdf_keywords": "autoregressive modelings speech;modelings speech text;non autoregressive modelings;modelings speech;autoregressive nar models;speech text generation;autoregressive baselines effort;autoregressive baselines;compared autoregressive baselines;masked language modeling;autoregressive modelings;autoregressive nar;non autoregressive nar;study non autoregressive;language modeling;compared autoregressive;speech text;autoregressive;performance nar models;non autoregressive;abstract non autoregressive;language modeling 28;nar models insertion;text generation;nar models simultaneously;nar models \ufb01lling;nar models;models simultaneously generate;drop compared autoregressive;reduces inference speed"}, "2f153172b92ea32f242d9cb6b94d162e52ef5f0b": {"ta_keywords": "dual dfa learning;dfa learning;programming demonstration learning;dfa learning problem;learning description logic;demonstration learning straightline;functionfree prolog clauses;subconcepts learning arity;functionfree prolog;dfa paclearning problem;learning straightline programs;learning problems;hardness natural learning;dfa paclearning;prolog clauses ground;demonstration learning;natural learning problems;hardness formal problems;learning problems including;examples dfas string;demonstration learning order;prolog;dual dfa;problem concepts strings;examples dfas;learning problem extended;accept dual dfa;version dfa paclearning;learning arity determinate;learning arity", "pdf_keywords": ""}, "4a348e4725a2bc677e4aa40aa63c1421e8f335c9": {"ta_keywords": "multilabel classification micro;multilabel classification special;multilabel classification;used multilabel classification;success binary classifier;classification special;binary classifier;classification micro average;binary classification;classification special case;mean precision recall;classifier;classification micro;classifier outputs;optimal f1 score;classification;binary classification context;binary classifier class;context multilabel classification;maximizing f1 scores;classifier completely;classifier class rare;classify;classification context multilabel;recall f1 score;probabilities optimal threshold;optimal behavior classify;achievable f1 score;f1 score special;f1 score decision", "pdf_keywords": "predicting rare labels;predictions optimally thresholded;maximized predicting examples;features rare labels;thresholded random guesses;predicting instances positive;rare labels classi\ufb01er;predicting rare;mean precision recall;rare labels frequently;precision recall;rare labels;subset rare labels;predicts instances positive;choosing predictions maximize;predictive features rare;classi\ufb01er optimal thresholding;class rare multilabel;predictions optimally;predicting instances;rare multilabel;maximized predicting;predictions maximize expectation;uninformative predicting instances;predicting examples;predictions maximize;batch examples prediction;perfect predictions optimally;optimally thresholded random;examples prediction"}, "c2a79e2a65b721d4de5f6d4806323174b9f8f393": {"ta_keywords": "nlp human annotated;zero label learning;human annotated data;pretrained language models;real human annotations;human annotations;human annotated;annotated data;label learning natural;training data real;fewshot inference gpt;processing nlp human;annotated data used;annotations;human annotations paper;learning natural language;trained human labeled;label learning train;unsupervised data generation;training data creation;human labeled data;explores zero label;nlp human;trained purely synthetic;baseline models trained;annotated;fewshot inference;leveraging powerful pretrained;powerful pretrained language;pretrained language", "pdf_keywords": "pretrained language models;annotations demonstrate nlp;language models shot;demonstrate nlp models;zero label learning;human annotated label;zero label training;real human annotations;results human annotated;human annotations;human annotated;nlp models;nlp models obtain;human annotations demonstrate;learning research nlp;demonstrate nlp;unsupervised data generation;annotated label;training data real;annotations;synthetic data fully;training data creation;nlp core;annotations demonstrate;powerful pretrained language;label learning train;transfer learning;research nlp core;trained human labeled;baseline models trained"}, "3b00e642de51d0f8378c7c35eca89f2ecb6f3af8": {"ta_keywords": "familial novo microduplications;inherited microduplication;11 inherited microduplication;inherited microduplication parent;microduplication parent reportedly;individuals microduplications 22q11;18 individuals microduplications;clinical features duplication;microduplication parent;microdeletion syndrome region;21 microdeletion syndrome;individuals microduplications;microdeletion syndrome;identification familial novo;velocardiofacial microdeletion region;novo microduplications 22q11;microduplications 22q11 21;phenotypes preponderance familial;velocardiofacial microdeletion;23 identification familial;identification familial;microduplications 22q11;cases parental dna;novo microduplications;familial cases;digeorge velocardiofacial microdeletion;parental dna samples;preponderance familial cases;familial novo;retardation congenital anomalies", "pdf_keywords": ""}, "7a6c61b57bac074f7cd85963fd13da8f3321e087": {"ta_keywords": "topic discovery model;latent dirichlet allocation;unsupervised topic discovery;topics influential blogs;topic discovery;topic discovery estimation;blogs topic experiments;modeling topics topic;modeling topics;lda modeling topics;hyperlinks models topic;models topical relationship;dirichlet allocation;effectiveness topic discovery;unsupervised topic;topical relationship linking;influential blogs topic;explicitly models topical;hyperlinks models;called latent dirichlet;specific influence blogs;latent dirichlet;link documents topic;topics;influential blogs;models topical;topics influential;influential blog postings;blogs topic;exploit topical relationship", "pdf_keywords": ""}, "8c25e1c223fc70509172a32111c91fe4b9f86a56": {"ta_keywords": "virtualizable wireless networking;sdn cognitive wireless;wireless networks architectural;networking sdn cognitive;software defined networking;programmable wireless networks;network level programmability;virtualizable wireless;wireless networking vwn;future programmable wireless;cwn virtualizable wireless;building programmable wireless;sdn cognitive;software defined radio;cognitive wireless networking;defined networking;networking cwn virtualizable;defined networking sdn;internet architecture needs;programmable wireless;internet architecture;realization wireless community;sdr cognitive radio;radio sdr cognitive;processor programmable routers;wireless community future;networking sdn;networks architectural;wireless networks require;networks architectural survey", "pdf_keywords": "programmable wireless networks;networking programmable wireless;radio networks virtualizable;cognitive radio networking;de\ufb01ned networking programmable;cognitive wireless networking;networks cognitive radio;cognitive radio networks;networking programmable;software de\ufb01ned networking;software de\ufb01ned networks;building programmable wireless;programmable wireless;radio networking software;\ufb02exible programmable wireless;networking software de\ufb01ned;networks virtualizable;networks virtualizable networks;virtualizable networks;cognitive wireless;virtualizable networks particular;radio networks;sdn;software de\ufb01ned radio;wireless networks;wireless networking;de\ufb01ned cognitive wireless;radio networking;programmable wireless processors;sdn crn"}, "4a36a00db217fd98f1bd943aa2f2d6303adbc456": {"ta_keywords": "fair pca optimization;fair principal component;pca optimization stiefel;based fair pca;fair pca subject;fair pca;pca optimization;formulation fairness;pca minimizing maximum;defines fair principal;problem fair pca;pca minimizing;formulation fairness good;analysis pca minimizing;mmd based fair;mathematical formulation fairness;pca subject mmd;optimization stiefel manifold;convex optimization stiefel;principal component analysis;component analysis pca;mmd constraints;mmd constraints non;mmd dimensionality reduced;riemannian exact penalty;pca;pca subject;analysis pca;non convex optimization;subject mmd constraints", "pdf_keywords": "fair pca optimization;fair pca constrained;pca constrained optimization;pca optimization stiefel;pca optimization;pca constrained;analysis pca minimizing;fair principal component;pca minimizing maximum;pca minimizing;fairness pca fast;principal component analysis;incorporate fairness pca;component analysis pca;fair pca;fairness pca;fair pca subject;based fair pca;pca fast ef\ufb01cient;problem fair pca;pca algorithmically;de\ufb01ned pca algorithmically;pca algorithmically incorporate;pca subject mmd;fairness de\ufb01ned pca;optimization stiefel manifold;convex optimization stiefel;analysis pca;mmd constraints;mmd constraints non"}, "101d619b5911e9c2fda6f02365c593ae61617cb6": {"ta_keywords": "cooperative persuasive dialogue;persuasive dialogue systems;persuasive dialogue policies;persuasive dialogue humans;persuasive dialogue based;persuasive dialogues human;learning cooperative persuasive;tailored persuasive dialogue;persuasive dialogue;dialogue policies using;persuasive dialogues;common persuasive dialogue;corpus persuasive dialogues;dialogue policies;dialogue humans experimental;dialogue humans;reinforcement learning automatically;policies using framing;dialogues human interlocutors;dialogue systems;user simulators reward;dialogue based;framing experiments user;cooperative persuasive;effect framing experiments;dialogue systems use;dialogues human;based corpus persuasive;framing experiments;policy effect framing", "pdf_keywords": ""}, "d4762619b55c65120307ceebe4a0646984f6045a": {"ta_keywords": "statistical machine translation;accurately model translation;machine translation;dependent translation model;translation probabilities log;model translation probabilities;translation model;context dependent translation;translation probabilities;model translation;frequently observed translation;observed translation patterns;automatic speech;translation model allows;translation patterns;translate asr results;machine translation smt;readable transcripts evaluation;speech recognition asr;transcripts meetings japanese;translation patterns given;manual transcripts meetings;transcripts evaluation using;dependent translation;readable transcripts;translate asr;create readable transcripts;priority automatic speech;manual transcripts;automatic speech recognition", "pdf_keywords": ""}, "50ec3d960ac458573a1e4a1556420c5e96d58609": {"ta_keywords": "answering evidence annotation;question answering evidence;evidence annotations training;training intermediate annotations;evidence annotation open;annotations training intermediate;supervision answer labels;corpus distant supervision;supervised evidence retrieval;distantly supervised evidence;annotations training;learn evidence large;answering evidence;evidence annotation;evidence annotations;intermediate evidence annotations;intermediate annotations expensive;question answering;models learn evidence;domain question answering;answer labels model;evidence large corpus;evidence retrieval;question answering answers;learn evidence;evidence retrieval enables;annotations expensive methods;annotations expensive;intermediate annotations;retrieved large corpus", "pdf_keywords": "evidence large corpus;evidence corpus;\ufb01nding evidence corpus;distantly supervised odqa;corpus using evidence;evidence corpus using;evidence reranking span;fully supervised odqa;retriever \ufb01nds evidence;distdr distantly supervised;retriever \ufb01nd evidence;par fully supervised;supervised odqa improves;tasked evidence reranking;evidence reranking;distantly supervised;evidence distant supervision;reranking span extraction;supervised odqa;useful evidence iteration;large corpus reader;large corpus;supervised odqa dense;learn likely evidence;supervised odqa section;accurate evidence iterations;using evidence distant;evidence labels distdr;benchmarks using evidence;preliminary fully supervised"}, "d59c7b1c85f8c459863762361f251575785347a8": {"ta_keywords": "permeability polydisperse pores;pores collision dynamics;permeability smaller particle;polydisperse pores average;permeability hard spheres;permeability polydisperse;porous membranes permeability;membranes permeability hard;diffusion mechanism size;mean permeability polydisperse;polydisperse pores;membranes permeability;permeability smaller;pores collision;cylindrical pores collision;exclusion mean permeability;porous membranes;permeability hard;pore size distribution;selectivity permeability smaller;membranes consists hard;regularly porous membranes;porous membranes consists;individual pores weighted;diffusion mechanism;molecular dynamics simulations;sieving porous membranes;pores average permeabilities;mean permeability;molecular dynamics", "pdf_keywords": ""}, "35750f1908f405bb38b0708972f33fe07b378b64": {"ta_keywords": "provability predicate modal;ilm interpretability logic;g\u00f6del ilm interpretability;modal propositional logic;interpretability logic il;predicate modal operator;axioms interpretability logic;ilm interpretability;propositional logic axioms;logic axioms interpretability;interpretability logic language;predicate modal;treating provability predicate;logic language interpretability;provability predicate;modal operator binary;logic axioms;modal operator gl;il tautologies propositional;logic contains propositional;l\u00f6b modal propositional;axioms interpretability;interpretability logic;logic language;modal propositional;propositional logic;language interpretability logic;logic il tautologies;operator binary modal;operator gl g\u00f6del", "pdf_keywords": ""}, "90766546b29836eb96f54fe8fd70ec51a3e699ba": {"ta_keywords": "voronoi tessellation protein;proteins complexes vorocnn;complexes vorocnn deep;3d structures proteins;tessellation protein structures;3d cnn architectures;cnn constructed voronoi;tessellation protein;3d cnn;complexes vorocnn;vorocnn deep convolutional;recognition protein binding;cnn architectures built;cnn architectures;tessellation 3d molecular;cnn constructed;previous 3d cnn;3d molecular structures;3d molecular;recognition protein;vorocnn example recognition;network cnn constructed;network cnn;3d voronoi tessellation;voronoi tessellation 3d;protein structures;vorocnn deep;structures proteins;example recognition protein;structures proteins complexes", "pdf_keywords": "voronoi tessellation protein;learn 3d protein;tessellation protein structures;bioinformatics geometric deep;3d protein;tessellation protein;3d protein folds;3d macromolecular data;protein folds 3d;learning voronoi tessellation;tessellation convolutional neural;cnn constructed voronoi;tessellation 3d molecular;deep learning voronoi;voronoi tessellation convolutional;learning 3d voronoi;3d molecular structures;3d molecular;3d voronoi tessellations;vorocnn deep convolutional;3d voronoi tessellation;voronoi tessellation 3d;tessellation convolutional;3d macromolecular;protein structures;learning voronoi;voronoi tessellations using;macromolecular data;voronoi tessellations;structural bioinformatics geometric"}, "7181a5139301c8a407da75a105dd457bf03d7057": {"ta_keywords": "stochastic network optimization;markovian network optimization;generalize network optimization;network optimization primal;network optimization problems;network optimization;network optimization accommodate;flows heterogeneous planning;stochastic extension wadrop;stochastic network;wadrop equilibrium;wadrop equilibrium principle;provide dynamic stochastic;dynamic stochastic;class stochastic network;heterogeneous planning time;dynamic programming;generalize network;markovian network;heterogeneous planning;extension wadrop equilibrium;principle generalize network;optimization;optimization problems;optimization primal;multi commodity flows;flow multi commodity;stochastic;dynamic stochastic extension;dynamic programming based", "pdf_keywords": ""}, "af553d6121d338fc74dbd5faa43d5383a222198d": {"ta_keywords": "communication skills autism;skills autism spectrum;skills autism;skills communication difficulties;verbal communication skills;social skills communication;autism spectrum;social communication skills;non verbal communication;communication skills use;communication skills members;autism;communication skills;skills communication;social skills;people social skills;autism spectrum quotient;non verbal behaviors;verbal communication;improve social communication;communication difficulties;communication skills exploits;social communication;verbal behaviors;communication difficulties greater;enhance non verbal;difficulties improve social;non verbal;verbal behaviors number;training tool social", "pdf_keywords": ""}, "a8315b5d3ff1b834fb58420397b13b9d169efad1": {"ta_keywords": "names publication;cluster citation records;proposed disambiguation mechanism;propose disambiguation mechanism;author names publication;paper propose disambiguation;names publication venues;disambiguation mechanism;handling split citation;cluster citation;disambiguation mechanism uses;citation records;distinguishing similar entities;disambiguation mechanism attributes;split citation;multiple publication attributes;citation records generally;clustering entities;split citation problem;similarity entities clustering;publication attributes metadata;proposed disambiguation;entities clustering;determining similarity entities;propose disambiguation;including author names;disambiguation;entities clustering entities;titles distinguishing similar;publication venues titles", "pdf_keywords": ""}, "83ddc47f6dd0434c12eff9e4e42b727217a200a8": {"ta_keywords": "learning dynamics convergence;nash equilibrium convergence;non uniform learning;multi agent learning;probability agents converge;agents learn deterministic;agents converge neighborhood;learning dynamics;nash equilibrium finite;agent learning;uniform learning rates;learning non uniform;uniform learning;local nash equilibrium;cooperative multi agent;gradient ii stochastic;agent learning algorithms;algorithms non cooperative;learning algorithms non;rates learning dynamics;agents converge;agents learn;stable local nash;concentration bounds guaranteeing;nash equilibrium;unbiased estimator gradient;games agents learn;learn deterministic;learning rates non;equilibrium convergence", "pdf_keywords": "agent policy gradient;multi agent gradient;learning multi agent;agent gradient;multi agent learning;agent gradient based;online optimization;non uniform learning;policy gradient;policy gradient gradient;agent learning;adversarial learning multi;agent learning algorithms;gradient based online;learning algorithms non;algorithms non cooperative;uniform learning rates;uniform learning;multi agent policy;nash equilibrium asymptotic;based online optimization;learning games;stable local nash;local nash equilibrium;learning rates;uniform learning rate;optimization dynamical;online optimization burden;nash equilibrium;learning rate"}, "48ea80b65f42e9fbb96b856286d12347d1df52d2": {"ta_keywords": "machines reasoning;commonsense reasoning dataset;evaluation commonsense reasoning;reasoning models evaluations;reasoning dataset dense;evaluation machines reasoning;commonsense reasoning facilitate;novel commonsense reasoning;reasoning dataset;reasoning models;machines reasoning process;language understanding reasoning;tiered reasoning intuitive;introduce tiered reasoning;commonsense reasoning;evaluating underlying reasoning;reasoning facilitate future;understanding reasoning models;tiered reasoning;reasoning intuitive physics;reasoning facilitate;reasoning process empirical;understanding reasoning;better language understanding;reasoning intuitive;underlying reasoning;tiered evaluation machines;verifiable evaluation commonsense;underlying reasoning process;dense annotations", "pdf_keywords": "commonsense reasoning dataset;evaluation commonsense reasoning;reasoning dataset dense;tiered reasoning intuitive;machines reasoning;commonsense reasoning facilitate;novel commonsense reasoning;introduce tiered reasoning;tiered reasoning;commonsense reasoning;reasoning intuitive physics;evaluation machines reasoning;better language understanding;language understanding reasoning;language understanding tasks;reasoning dataset;reasoning models;reasoning facilitate future;commonsense language understanding;understanding reasoning models;evaluating underlying reasoning;machines reasoning process;reasoning facilitate;tiered evaluation machines;reasoning process empirical;breadth language understanding;understanding reasoning trip;reasoning trip dataset;reasoning intuitive;language understanding"}, "fa2657c0d66f048dee6b080536abbd1f947e822f": {"ta_keywords": "brain age predictions;brain age estimation;estimated brain age;model brain age;brain age derived;brain age;contribution brain age;brain age neuropsychological;superior age estimation;age estimation accuracy;age neuropsychological measures;age estimation demonstrate;age estimation performance;estimation demonstrate age;age neuropsychological;estimated brain;chronological brain age;age estimation;age predictions;demonstrate age estimation;age predictions multiple;age derived statistical;established estimated brain;biomarker predictive cognitive;brain mris healthy;predictive cognitive decline;train deep learning;estimated chronological brain;life span evaluation;age derived", "pdf_keywords": "estimating brain age;brain age estimation;brain age predictions;estimated brain age;model brain age;brain age derived;disease aging brain;aging brain;alzheimer disease aging;degeneration neuroimaging initiative3;brain age based;brain age;brain age neuropsychological;alzheimer disease neuroimaging;degeneration neuroimaging;contribution brain age;age estimation demonstrate;age neuropsychological;age estimation framework;age neuropsychological measures;aging lifespan;chronological brain age;2019 estimating brain;age estimation;accuracy age estimation;estimating brain;contributions aging lifespan;lobar degeneration neuroimaging;aging lifespan using;aging"}, "eb1b89751cac821792df36d3a1a2fb01dc4db2d1": {"ta_keywords": "\u8aac\u5f97\u5bfe\u8a71\u30b7\u30b9\u30c6\u30e0\u306b\u304a\u3051\u308b\u8a71\u984c\u8a98\u5c0e\u306b\u57fa\u3065\u304f\u5bfe\u8a71\u5236\u5fa1 \u8a00\u8a9e\u30e2\u30c7\u30eb \u97f3\u58f0\u5bfe\u8a71;\u8aac\u5f97\u5bfe\u8a71\u30b7\u30b9\u30c6\u30e0\u306b\u304a\u3051\u308b\u8a71\u984c\u8a98\u5c0e\u306b\u57fa\u3065\u304f\u5bfe\u8a71\u5236\u5fa1 \u8a00\u8a9e\u30e2\u30c7\u30eb;\u8aac\u5f97\u5bfe\u8a71\u30b7\u30b9\u30c6\u30e0\u306b\u304a\u3051\u308b\u8a71\u984c\u8a98\u5c0e\u306b\u57fa\u3065\u304f\u5bfe\u8a71\u5236\u5fa1;\u8a00\u8a9e\u30e2\u30c7\u30eb \u97f3\u58f0\u5bfe\u8a71;\u8a00\u8a9e\u30e2\u30c7\u30eb;\u97f3\u58f0\u5bfe\u8a71", "pdf_keywords": ""}, "120839995e64f8ed734b5249ab681328c4955f5d": {"ta_keywords": "mdp congestion game;mdp congestion games;congestion games designer;congestion game;toll design problem;tolls mdp congestion;consider toll design;congestion games;congestion game online;toll design;toll synthesis problem;congested stochastic network;playing markov decision;constraint satisfaction tolls;designer congested stochastic;game online constraint;tolls mdp;mdp congestion;process mdp congestion;consider toll;stochastic network decision;satisfaction tolls mdp;congestion;optimal strategy clear;markov decision process;congested stochastic;network decision makers;toll synthesis;decision makers adaptively;decision process mdp", "pdf_keywords": ""}, "4d16a47fb6708704b155855045c9e5d2ea380bb0": {"ta_keywords": "sentiment analysis czech;social media corpus;learning methods sentiment;current sentiment analysis;sentiment analysis;sentiment analysis research;methods sentiment analysis;czech social media;sentiment analysis believe;social media dataset;social media english;methods sentiment;sentiment;media corpus;media corpus furthermore;extend current sentiment;analysis czech social;current sentiment;analysis czech;humanannotated czech social;czech language systematical;czech language;large humanannotated czech;czech social;social media;humanannotated czech;corpus;product reviews;case czech language;evaluation datasets", "pdf_keywords": ""}, "7212cca9be971997434c2b3a27411a163bbd89c3": {"ta_keywords": "conditioned ctc inference;searched intermediate conditioning;previous inference conditioning;conditioning uses predictions;conditioned ctc probabilistic;conditioning inference;conditioning inference new;intermediate conditioning;conditioning re\ufb01nes intermediate;new conditioning methods;self conditioned ctc;intermediate prediction latent;conditioning methods;intermediate conditioning re\ufb01nes;inference conditioning inference;conditioning framework;inference conditioning;predictions previous inference;conditioning methods based;tractable conditioning framework;conditioned ctc;multi pass conditioning;ctc inference improve;improved ctc inference;intermediate predictions beam;intermediate prediction;model intermediate prediction;conditioning framework paper;ctc inference;conditioning", "pdf_keywords": "conditioned ctc probabilistic;searched intermediate conditioning;improved ctc inference;self conditioned ctc;improve ctc inference;previous inference conditioning;conditioned ctc;conditioned ctc baseline;ctc inference;intermediate conditioning;conditioning re\ufb01nes intermediate;conditioning inference;ctc inference searched;new conditioning methods;conditioning uses predictions;conditioning inference paper;inference conditioning inference;intermediate conditioning re\ufb01nes;conditioning framework;multi pass conditioning;conditioning methods;tractable conditioning framework;inference conditioning;conditioning methods based;intermediate prediction latent;ctc inference tatsuya;predictions previous inference;ctc probabilistic;ctc probabilistic model;model intermediate prediction"}, "6bb2b856d9a9b873259ba9dc48bc450c96eb3318": {"ta_keywords": "transcribing time;transcribing;time", "pdf_keywords": "transcripts new faster;correction speech transcripts;transcription process cost;cost sensitive correction;speech transcripts;study ongoing transcription;speech transcripts new;manual correction transcript;transcripts;transcript series transcripts;transcripts new;enrollment updating cost;correction transcript series;ongoing transcription;sensitive correction speech;transcriber agnostic cost;transcript series;transcripts conclusion proposed;transcription;ongoing transcription process;updating cost;updating cost models;correction transcript;transcript;\ufb02y ongoing transcription;transcripts conclusion;transcription process framework;correction speech;updated cost model;re\ufb02ect updated cost"}, "79b8ef3905a42b771248719495a2117271906445": {"ta_keywords": "emissions large neural;energy use carbon;carbon footprint;carbon footprint recent;carbon emissions;carbon emissions large;estimating energy cost;energy efficiency;use carbon footprint;improve energy efficiency;costs estimating energy;carbon free energy;dnns sacrificing accuracy;estimating energy;sparsely activated dnns;efficiency equivalent emissions;energy cost;energy efficiency equivalent;emissions large sparsely;information carbon emissions;emissions;dense dnns sacrificing;emissions large;dnns consume;detailed information carbon;large dense dnns;equivalent emissions large;energy use;large neural network;calculate energy use", "pdf_keywords": "carbon footprint ml;energy use carbon;energy usage co2e;energy efficiency co2;energy usage training;improve energy efficiency;dnns sacrificing accuracy;improving energy efficiency;energy consumed co2e3;neural architecture search;energy efficiency;energy efficiency algorithms;developers include energy;carbon emissions 27;models calculate energy;carbon footprint recent;carbon emissions;carbon footprint;dense dnns sacrificing;energy usage;improving energy;believe energy usage;include energy usage;large dense dnns;emissions co2e large;emissions co2e;reduce carbon footprint;include energy;estimates neural architecture;efficiency co2"}, "1c709eef701d933af1383c790c13209f06806b60": {"ta_keywords": "rationalization language modeling;greedy rationalization language;annotated sequential rationales;translation sequential rationales;rationales sequence models;machine translation sequential;sequential rationales greedy;modeling machine translation;nlp systems predictions;baselines greedy rationalization;greedy rationalization best;sequence models critical;sequence models;greedy rationalization;study greedy rationalization;language modeling machine;language modeling;sequential rationales;rationalization language;greedy rationales;model explanations rationales;rationalization best optimizing;machine translation;greedy rationales similar;rationales greedy rationales;rationales greedy;annotated sequential;sequential rationales solving;rationalization best;explanations rationales", "pdf_keywords": "explanations detect biases;plausible gradient attention;language model relies;gradient attention;model explanations detect;rationalization language modeling;biases appropriated training;modeling machine translation;gradient attention based;language modeling;context subsets training;detect biases appropriated;model explanations;model study greedy;intractable marginalization;attention based;relies model explanations;greedy rationalization language;language modeling machine;biases;language model;circumventing intractable marginalization;attention based methods;explanations detect;study greedy rationalization;detect biases;biases appropriated;attention;marginalization;greedy rationalization"}, "0de86afbf91d0cf3e595a23a5b7a4d19deefb891": {"ta_keywords": "bifurcation diagram estimation;model bifurcations;bifurcation measure gradients;model bifurcations match;bifurcating parameter regimes;minimal model bifurcations;optimisers bifurcating parameter;specified bifurcation diagram;bifurcations;optimisers bifurcating;bifurcation diagram;targets bifurcation measure;bifurcation;targets bifurcation;specified bifurcation;bifurcating parameter;bifurcation measure;specified targets bifurcation;bifurcations match;bifurcations match specified;user specified bifurcation;push optimisers bifurcating;differential equation models;inferring parameters differential;equation models;models explore;equation models achieved;demonstrate parameter inference;pitchfork diagrams genetic;bifurcating", "pdf_keywords": "locating bifurcations parameter;method locating bifurcations;locating bifurcations;bifurcations parameter space;julia package bifurcationinference;bifurcationinference jl leveraging;bifurcations parameter;bifurcationinference;bifurcations;bifurcationinference jl;bifurcation;user speci\ufb01ed bifurcation;package bifurcationinference;speci\ufb01ed bifurcation diagram;bifurcation diagram;bifurcation diagram propose;package bifurcationinference jl;speci\ufb01ed bifurcation;switch synthetic biology;implementation method julia;method julia;inferring parameters differential;synthetic biology;differential equation models;method julia package;gradients computational;models explore;demonstrate parameter inference;synthetic biology work;parameter space matching"}, "4f02d8775123624088a91fcfff20625463e5239a": {"ta_keywords": "intelligent learning analytics;collaborative filtering logistic;learning analytics personalized;collaborative filtering regression;personalized prediction test;learning analytics collaborative;learning analytics based;learning analytics;predicting test outcomes;analytics personalized prediction;propose collaborative filtering;analytics collaborative filtering;collaborative filtering;collaborative filtering algorithm;importance learning analytics;filtering regression experts;personalized prediction;approaches collaborative filtering;predicting test;filtering logistic regression;algorithms predicting test;large education data;prediction test responses;analytics personalized;experts evaluate prediction;education data;intelligent learning;filtering regression;learning algorithms predicting;filtering logistic", "pdf_keywords": ""}, "614dc4001ad68cac31484887f16542f04693eca4": {"ta_keywords": "lobbying probabilistic;model lobbying probabilistic;lobbying probabilistic environment;lobbying stochastic;lobbying stochastic environment;conclude lobbying stochastic;formal model lobbying;model lobbying;problems conclude lobbying;influence voter preferences;lobbying;voter preferences voting;voter preferences;bribery criteria models;probability vote desired;conclude lobbying;voter preferences represented;issues voter preferences;influence voter;bribery criteria;preferences voting;criteria bribery;bribery;actor influence voter;voting multiple issues;probability vote;preferences voting multiple;criteria bribery criteria;vote desired issue;voting multiple", "pdf_keywords": ""}, "3261728694c0a53a2e8f95326f94147a28e03a83": {"ta_keywords": "deep quantized training;accuracy deep quantization;deep quantization;learn layer quantization;sinareq deep quantized;quantized training algorithms;deep quantized;deep quantization reduces;quantized training;learn multiple quantization;enhancing quantized training;layer quantization bitwidth;layer quantization;bitwidth assignment quantization;sinareq enhancing quantized;quantization bitwidth;quantization reduces bitwidth;assignment quantization large;quantization parameterization;quantization bitwidth ii;quantization large;multiple quantization parameterization;quantization;assignment quantization;quantized training time;weights values quantized;multiple quantization;enhancing quantized;sinareq deep;quantization reduces", "pdf_keywords": "deep quantized training;learn layer quantization;quantizing weights gradient;deep quantized;waveq deep quantized;layer quantization;layer quantization bitwidth;layers quantizing weights;learn multiple quantization;quantized training;quantizing weights;bitwidth assignment quantization;quantization bitwidth;assignment quantization large;quantization parameterization;layers quantizing;quantized training speci\ufb01cally;quantization bitwidth ii;quantization large;assignment quantization;multiple quantization parameterization;waveq deep;quantization;weights values quantized;optimization sinusoidal regularization;multiple quantization;sinusoidal regularization waveq;sinusoidal regularization;levels layers quantizing;regularization waveq balance"}, "80ef8b8a1284790e0d8f7cbf9727c9e0b2a89332": {"ta_keywords": "distribution shift training;shift correct classifiers;black box predictors;better predictors tighter;bbse detect shift;shift estimation bbse;predictors tighter;labels better predictors;detect shift bbse;predictors tighter estimates;shift training test;shift label marginal;detect quantify shift;bbse works predictors;tighter estimates bbse;predictors biased inaccurate;classifiers test set;classifiers test;classifiers;better predictors;label marginal changes;box predictors;shift bbse;black box shift;dimensionality prior shift;shift bbse exploits;box predictors reduce;prior shift correction;shift estimation;predictors biased", "pdf_keywords": "distribution shift training;estimate label shift;black box predictors;black box predictor;shift estimation;box shift estimation;detect quantify shift;shift estimation bbse;black box shift;detect shift experiments;shift training test;dimensionality prior shift;prior shift correction;quantify shift;box predictors arxiv;improved prediction;detect shift;box predictors;distribution shift;shift black box;prior shift;estimates improved prediction;box predictor;quantify shift correct;improved prediction high;bbse detect shift;arbitrary black box;introduce black box;shift using black;box predictors reduce"}, "843966d4b567033abff9775c5958f7be4db5c0ad": {"ta_keywords": "families responding disaster;responding disaster bereavement;disaster bereavement;children families responding;responding disaster;families responding;children families;bereavement;disaster;families;children;responding", "pdf_keywords": ""}}