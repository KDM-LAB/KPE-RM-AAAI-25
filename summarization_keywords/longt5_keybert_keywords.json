{"9c61f5f51a232049635e6f3441e6397af4d91298": {"ta_keywords": "patients diagnosis malignant;diagnosis malignant disease;diagnosis malignant;malignant disease;physician diagnosis;diagnosis management patients;patients diagnosis;physician diagnosis management;malignant;management patients diagnosis;diagnosis;role physician diagnosis;diagnosis management;patients;literature role physician;physician;management patients;role physician;disease;article present literature;article present;literature role;literature;management;present literature role;article;present literature;purpose article present;purpose article;role", "pdf_keywords": ""}, "a05c3e8bd6dacbd192ffa28543e60e2c93c66d76": {"ta_keywords": "trend analysis used;protest movements twitter;movements twitter;trend analysis;trend;various social media;social media platforms;social media today;twitter;importance social media;politicss entertainment data;polarization spread protest;social media;political polarization spread;analyze political polarization;industry trend analysis;spread protest movements;used analyze political;protest movements;data used analyze;political polarization;analyze political;topics politicss;analysis methods data;data science;industry trend;topics politicss entertainment;various topics politicss;spread protest;entertainment data science", "pdf_keywords": ""}, "bfa10ea6a4c9fa585f21f39858da517c31a76343": {"ta_keywords": "persuasive dialogue systems;probabilistic dialogue modeling;propose probabilistic dialogue;method persuasive dialogue;dialogue modeling method;persuasive dialogue;probabilistic dialogue;dialogue modeling;developed dialogue model;dialogue model;modeling method persuasive;dialogue systems;dialogue systems interact;dialogue model assuming;method persuasive;dialogue;persuasive;baseline developed dialogue;developed dialogue;propose probabilistic;decisions based preference;interact user based;user topic topic;user makes decisions;research propose probabilistic;guide user topic;user topic;based preference improve;topic topic;topic topic resul", "pdf_keywords": ""}, "0bf2a0a3216c79b62b3664c596f44d7a8add498a": {"ta_keywords": "fiction reviews specifically;fiction reviews;literature fiction reviews;reviews specifically science;research reviews study;reviews study;reviews study participantsstudy;research literature fiction;specifically science fiction;research reviews;navigate research literature;undergraduate computer science;computer science curriculum;reviews specifically;undergraduate computer;science fiction used;critically compare technical;research field learn;fiction used gateway;research topic stirs;learn navigate research;science fiction;fiction used;introductionthe undergraduate computer;gateway research reviews;read critically;navigate research;literature fiction;computer science;research literature", "pdf_keywords": ""}, "a4cb2a401c78bfafee69e823306b0cc9e4d673db": {"ta_keywords": "reviewer round robin;peer review;peer review papers;effective peer review;reviewer assignment instance;reviewer assignment;round robin mechanisms;fair allocation;fair allocation problem;mechanism called reviewer;round robin mechanism;equally important reviewer;fairly distributed papers;reviewer round;quality fairly distributed;round robin;model reviewer assignment;review papers reviewed;reviewer quality fairly;review papers;important reviewer quality;distributed papers;reviewer quality;instance fair allocation;papers reviewed;reviewer;called reviewer round;item allocations;distributed papers methods;round robin rr", "pdf_keywords": "reviewer assignment algorithm;reviewer assignment fair;reviewer assignment indivisible;reviewer assignments efficiency;fair reviewer assignment;reviewer assignment fundamental;fairness reviewer assignment;efficiency reviewer assignment;reviewer assignment instance;efficiency reviewer assignments;reviewer assignment use;reviewer assignment order;algorithm fair reviewer;reviewer assignment methodswe;reviewer assignment;algorithms fair reviewer;reviewer assignment setting;assignments efficiency reviewer;reviewer assignment problem;reviewer assignments;reviewer assignments satisfy;reviewers papers optimizing;paper assignment frameworks;method fair reviewer;assignment fair algorithms;reviewers papers goods;assignment algorithm fair;oftenthe reviewer assignment;reviewer assignment conference;methods reviewer assignment"}, "242c73ea34833910ad2643ec3a1096bb18c6d04d": {"ta_keywords": "speaker extraction neural;target speaker extraction;improving rn transducer;transducer target speaker;rn transducer target;target speaker speech;speaker speech extraction;speech extraction recurrent;speaker extraction;rn transducer;speaker speech recognition;neural network transducer;transducer rn;speech extraction;network transducer rn;domain target speaker;performance target speaker;target speaker;transducer rn stabilize;backgroundin improving rn;extraction recurrent neural;speaker speech;transducer target;speech recognition;extraction recurrent;extraction neural uncertainty;neural uncertainty estimation;improving rn;transducer;speaker", "pdf_keywords": "training target speaker;target speech extraction;speech model training;combining target speech;backgroundtarget speaker speech;based baseline speech;baseline speech enhancement;targeted speech recognition;target speaker speech;speaker speech recognition;speech enhancement speaker;estimator target speaker;model speech enhancement;speaker speech extraction;baseline speech;learning speech noisy;speech enhancement uncertainty;recognize target speaker;domain target speaker;backgroundtarget speaker;speech extraction recurrent;task learning speech;target speech;model speech;clean noisy speech;speech recognition aims;noisy speech model;speech enhancement;speech noisy environments;implementation targeted speech"}, "a4df5ff749d823905ff9c1a23b522d3f426a1bb6": {"ta_keywords": "fields personalized pagerank;pagerank;walks graphical models;personalized pagerank;similarity measures nodes;personalized pagerank widely;pagerank widely;pagerank widely used;graph walks;similarity measure graph;tree structured markov;graph walks graphical;measure graph nodes;measures nodes graphs;graph nodes based;structured markov random;markov random fields;similarity measures;walks graphical;research inference markov;graph nodes;background graph walks;connection inference tree;similarity measure;nodes graphs;used similarity measure;data miner;mining research;inference tree;inference markov random", "pdf_keywords": ""}, "f32108602fb0dbda29030cac780165a4b89048a3": {"ta_keywords": "comparison relation prediction;relation prediction text;predicting comparison relations;prediction text sql;text sql predict;phrasing knowledge comparison;predict comparison relations;sql predict comparison;relation prediction;knowledge comparison relation;comparison relations columns;knowledge comparison;predicting comparison;comparison relations;predict comparison;comparison relations limited;text sql;prediction text;introductionleveraging adjective noun;noun phrasing knowledge;sql predict;text sql methodsone;phrasing knowledge;comparison relation;introductionleveraging adjective;relations columns;adjective noun phrasing;capabilities predicting comparison;knowledge existing models;relations columns values", "pdf_keywords": ""}, "04745fe1306d10c915d27a454c157c837dacefce": {"ta_keywords": "discreterepresentations source separation;masks discreterepresentations source;source separation methodswe;complex masks discreterepresentations;source separation;estimates complex mask;masks discreterepresentations;complex mask;building complex masks;phase difference mixture;separation methodswe propose;complex mask present;complex masks;separation methodswe;mixture target;introductionthe phasebook;discrete representation phase;mixture target introduce;estimate phase;difference mixture target;discreterepresentations source;propose estimate phase;estimate phase using;layer based discrete;layer directly estimates;phase using;phasebook building complex;phasebook;introductionthe phasebook building;representation phase difference", "pdf_keywords": ""}, "20140fcf0bdd932c1886ff1c7674c23649b1e3b8": {"ta_keywords": "based speech synthesis;speech synthesis based;speech synthesis;speech synthesis results;speech quality methods;improving speech quality;mm based speech;speech quality;synthesis based parameter;improving speech;important improving speech;parameter generation rich;based parameter generation;improve parameter generation;based speech;rich context models;parameter generation;improvements human mm;human mm based;synthesis based;context models modifying;context models;context models important;synthesis;components human mm;improve parameter;mm based;smoothing;synthesis results;human mm", "pdf_keywords": ""}, "e32177e38060637ac8a2ebc9990d43d1ab8bdb8a": {"ta_keywords": "community based recommendations;homophily social networks;recommendations solution;recommendation systems relatively;recommendation systems;based recommendations solution;inferences recommend items;recommend items users;social networks;recommendations solution cold;recommend items;based recommendations;inferences recommend;issue recommendation systems;community based;community;based homophily social;recommendations;background community based;draw inferences recommend;recommendation;homophily social;solution cold start;cold start;cold start problem;methods cold start;background community;social;networks;known issue recommendation", "pdf_keywords": ""}, "6680b1e863c394f00307cb3818f7c7d75c9919aa": {"ta_keywords": "coding applied distributed;network coding;network coding applied;codes non multicast;designing network codes;network codes;applied distributed storage;distributed storage;multicast;tool network coding;non multicast;interference alignment resultsa;network codes non;multicast setting;alignment tool network;non multicast setting;multicast setting methodswe;interference alignment;data connecting nodes;distributed storage paper;data collector;distributed;introductioninterference alignment tool;concept interference alignment;coding applied;introductioninterference alignment;applied distributed;resultsa data collector;coding;connecting nodes furthermore", "pdf_keywords": ""}, "c096ec97ecc4f8325f6db7f32398445d6a39f959": {"ta_keywords": "recommendation fairness;recommendation fairness argue;research recommendation fairness;fairness systems;fairness assumptions;fairness systems exhibit;fairness assumptions recognize;notions fairness assumptions;complexities fairness;notions fairness;world complexities fairness;fairness;properties fairness systems;dimensional notions fairness;fairness argue;consider properties fairness;properties fairness;recommender systems designed;recommender systems;fairness argue previous;recommender;socially consequential applications;introduction recommender systems;socially consequential;introduction recommender;recommendation;research recommendation;consequential applications;socially;assumptions recognize", "pdf_keywords": "algorithmic fairness recommendation;recommendation fairness using;recommendation fairness;recommendation fairness problem;fairness recommendation fairness;recommendation fairness argue;adaptation recommendation fairness;fairness recommendation;fairness aware ranked;algorithmic fairness;research recommendation fairness;conceptualize algorithmic fairness;fairness output recommendation;fairness using social;recommendation algorithm reordering;fairness systems;fairness ability automatically;improve quality recommendation;fairness problem social;balancing different fairness;recommendation algorithm;promote fairness ability;improve fairness;accuracy fairness;fairness long social;group fairness aware;fairness systems exhibit;generate recommendation;results promote fairness;ability improve fairness"}, "27636090a87fab750fccff4c6ede161ab62bcab4": {"ta_keywords": "vehicular ad hoc;hoc networks vanet;networks vanet;networks vanet challenging;vanet challenging research;safety messages beacons;repetition beacon piggybacking;coded repetition beacon;messages beacons;vanet challenging;ad hoc networks;repetition beacon;beacon piggybacking methods;beacon piggybacking;vanet;beacons inserting neigh;beacons inserting;methods vehicular ad;adj hoc networks;beacons;hoc networks;piggybacking methods vehicular;increasing network visibility;beacon;hoc networks research;messages beacons inserting;vehicular ad;methods vehicular;network visibility;network visibility using", "pdf_keywords": "beacons vehicular;beacons neighbor vehicles;generating beacons vehicles;beacons vehicles vanet;gathered beacons vehicular;coding beacons neighbor;vehicular ad hoc;vehicles easily transmitted;network vanet demonstrated;beacons vehicular ad;safety messages beacons;coding beacons;beacons vehicles;vehicles vanet;vehicles vanet method;network vanet;transmitted received vehicles;beacons received vehicle;802;coded repetition beacon;vanet demonstrated;vanet method;vanet method based;virtual network vanet;vehicle belonging network;traffic junctions vehicles;vanet;according 802 11;802 11;according 802"}, "c3fc0b1041dcdd5b47ffaa0d584e40aa841628bf": {"ta_keywords": "wrappers set expansion;set expansion implemented;expands entities;expands entities automatically;set expansion;language expands entities;expander language;set expander language;set expansion refers;does set expansion;set expansion seal;expansion seal set;introduction set expansion;seal set expander;set expander;expansion implemented;language expands;expander language expands;character level wrappers;expansion implemented seal;expander;expansion seal;expands;partial set seed;expansion refers expanding;expansion refers;expanding partial set;expansion;seal set;level wrappers set", "pdf_keywords": ""}, "7f79ac114d30c2c7dae91075210fbfda90c9d76f": {"ta_keywords": "adversarial purely cooperative;prior ai breakthroughs;ai breakthroughs complex;diplomacy;ai breakthroughs;breakthroughs complex games;cooperative setting;diplomacy proven;purely cooperative settings;purely cooperative;adversarial purely;purely adversarial;purely adversarial purely;diplomacy proven formidable;complex games;prior ai;cooperative settings;reason diplomacy;complex games focused;cooperative setting contrast;reason diplomacy proven;cooperative;contrast purely cooperative;focused purely adversarial;setting reason diplomacy;adversarial;cooperative setting reason;ai;contrast purely adversarial;contrast cooperative setting", "pdf_keywords": "policy imitation learning;imitation learning deep;diplomacy games;000 diplomacy games;policy imitation;deep reinforcement;games involving cooperation;approach deep reinforcement;diplomacy games work;imitation learning;supervised learning game;policies games involving;policies games;imitation learning human;deep reinforcement learning;reinforcement learning search;ai good game;reinforcement learning social;reinforcement learning new;multi agent reinforcement;effective policies games;variant diplomacy;policy game;agent reinforcement learning;policy game bot;learning game;agent reinforcement;prior ai agents;diplomacy combines supervised;rules diplomacy"}, "70dc18bb6607e408ec1cd3f71c0fdac3534c288d": {"ta_keywords": "speech enhancement lstm;enhancement lstm recurrent;enhancement lstm;introduction speech enhancement;based speech enhancement;speech enhancement;speech recognition rra;robust automatic speech;lstm receptor networks;optimal speech reconstruction;lstm recurrent neural;lstm recurrent;lstm;memory lstm receptor;speech reconstruction objective;memory lstm;speech recognition proposed;recurrent neural network;recurrent neural networks;developments recurrent neural;speech reconstruction;lstm receptor;speech enhancement light;speech recognition;automatic speech recognition;noise robust;network based speech;term memory lstm;noise robust automatic;optimal speech", "pdf_keywords": ""}, "6e07fb796c75cac6432cdf0c314b933d0f9f45e5": {"ta_keywords": "biomedical text mining;recognition gene names;gene names applications;mining recognition gene;introduction gene names;gene entity related;gene names;task biomedical text;biomedical text;text mining;know gene refers;gene refers;gene entity;names actual genes;gene names actual;actual gene entity;parsing;syntactic parsing;syntactic parsing modules;combines syntactic parsing;refers actual gene;recognition gene;gene refers actual;text mining recognition;important know gene;know gene;gene;parsing modules;introduction gene;parsing modules achieves", "pdf_keywords": ""}, "24fcdaf969089e6a411f7cebc9274bbc53c25e42": {"ta_keywords": "counterfactually augmented datasets;generating counterfactually augmented;augmented datasets;augmented datasets proposed;generating counterfactually;counterfactually augmented;augmented datasets aimto;process generating counterfactually;spurious patterns training;patterns training data;training data;training data methodsa;data methodsa human;methodsa human loop;patterns training;counterfactually;machine learning models;datasets;human loop process;training data researchers;produce machine learning;datasets proposed;datasets proposed applied;human loop;augmented;proposed human loop;machine learning;learning models;datasets aimto;learning models reliant", "pdf_keywords": "aforementioned causal features;causal features make;causal features data;causal features;noise causal features;cad causal framework;non causal features;data causal;causal model data;causal features ols;causal anticausal learning;cad causal;causal features lead;counterfactuallyaugmented data cad;causal features report;causal models measurement;adding noise causal;counterfactual document causal;causal inference predict;causal models;processing use causal;assessing causal;noise causal;data causal model;effectiveness counterfactuallyaugmented data;optimizing data causal;causal features degrade;causal inference;causal thinking;relationships causal models"}, "1d5c07e7415a7e9be078717197ddf9f3c70a2875": {"ta_keywords": "pretrained clinical notes;bert pretrained clinical;pretrained models clinicalbert;predictive clinical tasks;does bert pretrained;bert pretrained;clinical notes reveal;clinical notes;clinical notes electronic;pretrained clinical;predictive clinical;performance predictive clinical;notes electronic health;pretrained models;models clinicalbert;introduction does bert;transformers pretrained clinical;clinical tasks;models clinicalbert efforts;clinical tasks cost;bert;does bert;electronic health records;notes reveal;pretrained;cost training models;training models;notes;clinical;notes electronic", "pdf_keywords": "embeddings patient names;patient names model;mimic corpus model;retrieve patient conditions;embeddings recover patient;use clinical bert;mimic corpus;names model trained;reference patient names;tool identifying patients;patient names significantly;unified medical language;model recover patient;masked language modeling;patient names conditions;recover patient names;mimic model trained;contextualized embeddings patient;language modeling;explicitly reference patient;bert model performed;embeddings patient;model trained mimic;patient condition associations;sentence prior pretraining;patient names condition;health records eh;health records;clinical bert retrieve;patient names"}, "ca9047c78d48b606c4e4f0c456b1dda550de28b2": {"ta_keywords": "state space layers;state space layer;state space representation;linear state space;deep learning models;deep learning;models linear state;state space;deep learning introduce;sequence model inspired;linear state;simple sequence model;sequence model;time state space;approach deep learning;types deep learning;learning models linear;sequence simply simulating;control systems generalizes;methods linear state;model inspired control;approach deep;space representation;space representation anx;simulating linear;layers;continuous time state;control systems;space layers;space layer", "pdf_keywords": "networks rnns temporal;rnns temporal convolutions;recurrent continuoustime models;time recurrent neural;modeling demonstrate recurrent;recurrent neural networks;recurrent model efficient;networks previous recurrent;rnns temporal;neural networks rnns;recurrent neural;demonstrate recurrent neural;networks rnns;space model recurrent;recurrent continuoustime;neural networks sequence;temporal convolutions neural;previous recurrent continuoustime;rn deep linear;sequence model deep;long term memory;method recurrent neural;recurrent models;linear networks rns;approach recurrent neural;recurrent model;model recurrent;continuous time recurrent;continuous time memory;model recurrent model"}, "b2a090506264bc9706dc9bcc5d61b4965ae919e7": {"ta_keywords": "uncertain extractions entities;knowledge graph extractions;knowledge graph;extractions entities relations;uncertain extractions;extractions entities;extraction graph refer;transformed knowledge graph;extraction graph;inferring missing information;interrelated facts unfortunately;facts included knowledge;collections interrelated facts;information processing;facts useful knowledge;graph extractions;missing information determining;paper uncertain extractions;interrelated facts;entities relations;graph extractions form;large scale information;missing information;relations transformed knowledge;systems able extract;information determining;determining candidate facts;scale information processing;information processing systems;facts unfortunately", "pdf_keywords": ""}, "8ca5a1e6cec68ef515ac1eb28d069a23dc9c14df": {"ta_keywords": "global graph clustering;graph clustering available;graph clustering;clustering available;local global graph;global graph;data sets watset;watset local global;clustering;clustering available http;watset local;biomedcentral com;sets watset local;www biomedcentral;www biomedcentral com;http www biomedcentral;graph;data sets;biomedcentral;watset;sets watset;local global;present data sets;biomedcentral com 1741;global;local;data;sets;com;www", "pdf_keywords": ""}, "a75c2d26ca6a06cbee62a8d1dad5993356d96793": {"ta_keywords": "set expansion seal;expansion seal set;set expansion named;set expansion refers;expands entities;expansion named entities;seal set expander;set expansion;expands entities automatically;does set expansion;language expands entities;good set expansion;important set expansion;expander language;set expander language;expansion seal;set expander;iterative set expansion;seal set;expansion refers;language expands;expansion refers expanding;refers expanding;named entities;named entities using;expansion named;expander;key clinical messagethe;entities automatically utilizing;clinical messagethe use", "pdf_keywords": ""}, "a2ce385fc8d5068e8c87ebe4699c8f9b295cad5e": {"ta_keywords": "adapting word embeddings;morphological phonological subwordrepresentations;phonological subwordrepresentations;phonological subwordrepresentations work;word embeddings new;word embeddings;subword units phonemes;subwordrepresentations work natural;linguistically motivated subword;word representations;phonemes morphemes;new languages morphological;units phonemes morphemes;embeddings new languages;resourced languages adapting;adapting word;continuous word representations;languages adapting;adapting continuous word;motivated subword units;introduction adapting word;word representations using;languages morphological phonological;languages making generalization;languages morphological;languages adapting continuous;subwordrepresentations;morphological phonological;subwordrepresentations work;morphemes", "pdf_keywords": "performance multilingual embeddings;transferring word embeddings;words embeddings trained;trained words embeddings;multilingual embeddings;embeddings trained subword;language leveraging resources;embeddings trained morphological;words embeddings;trained subword representations;language leveraging;word embeddings;word embedding;subword representations yield;word embeddings low;word embedding models;subword representations;leveraging subword information;learning leveraging subword;multilingual embeddings ner;use subword representations;methods cross lingual;subword representations multidimensional;word representations languages;corpora bilingual dictionaries;resource language leveraging;rich languages morphological;leveraging subword;use cross lingual;parallel corpora bilingual"}, "a0e92f6e9564b8c38b6649ae71b892ddfb988faa": {"ta_keywords": "semantic parsing viaexemplarretrieval;controllable semantic parsing;semantic parsing;parsing viaexemplarretrieval casper;semantic parsing want;applications semantic parsing;parser retrieves related;parser retrieves;parsing viaexemplarretrieval;parser;parsing;query parser retrieves;parsing want rapidly;query parser;input query parser;behavior parser;parser introduce;behavior parser introduce;propose controllable semantic;controllable semantic;change behavior parser;parser introduce new;parsing want;semantic;viaexemplarretrieval casper;retrieves related exemplars;applications semantic;viaexemplarretrieval casper methods;practical applications semantic;viaexemplarretrieval", "pdf_keywords": "parser viaexemplar retrieval;retrieval casper parsing;semantic parser uses;semantic parser viaexemplar;semantic parser;augmented semantic parser;semantic parsing want;retrieval augmented semantic;semantic parsing;semantic parser used;controllable semantic parser;semantic parsing domain;uses query embedding;functional semantic parser;casper parsing queries;parsing queries;query seq2seq generator;query embedding;query embedding cosine;queries meaning representations;generates meaning representation;semantic parsing demonstrate;parsing queries meaning;parser improved optimizing;applications semantic parsing;method semantic parsing;similarity retrieval;parser retrieves exemplars;improved optimizing retrieval;retrieval index exemplars"}, "a30d7a3aa5e50d0b7abb448b6692e419b84018b8": {"ta_keywords": "prefix boosting sequence;introductionpromisingaccurate prefix boosting;accurate prefix boosting;prefix boosting;sequence sequence anras;attention based sequence;sequence anras;boosting sequence sequence;prefix boosting pab;boosting sequence;boosting pab discriminative;sequence anras paper;pab discriminative training;sequence seq2seq ar;promising accurate prefix;sequence sequence seq2seq;sequence seq2seq;introductionpromisingaccurate prefix;sequence sequence;based sequence sequence;discriminative training;prefix;boosting pab;discriminative training technique;sequence;boosting;training technique attention;attention based;accurate prefix;attention", "pdf_keywords": "attention based sequence;pab discriminative training;speech recognition rra;neural network speech;recurrent neural networks;short term memory;recurrent network;end speech recognition;discriminative training;speech recognition;network speech recognition;output recurrent network;boosting pab discriminative;acoustic language models;seq2seq automatic speech;attention architecture end;application recurrent neural;speech recognition recognize;attention architecture;automatic speech;discriminative training technique;accurate prefix boosting;approach speech recognition;development speech recognition;recurrent neural;automatic speech recognition;speech processing;application recurrent;training additional text;recurrent network bi"}, "a41c81e5c3f86e18217069b94b44ceaf281e451c": {"ta_keywords": "deployment wireless sensor;wireless sensor networks;sensor networks;locations connect sensor;impromptu deployment wireless;wireless sensor;locations deploys relays;measurements previous relay;relays locations connect;relay equally spaced;deploys relays locations;sensor networks person;relays locations;connect sensor placed;sensor placed line;link quality measurements;deployment wireless;deploys relays;relay equally;connect sensor;previous relay equally;relay;relays;networks person walks;sensor placed;making link quality;sensor;walks line making;walks line;link quality", "pdf_keywords": "place relay optimal;relay optimal;approach deployment relay;relays line optimal;decision place relay;relay networks assuming;deployment relays proposed;sensor networks cost;locations deploys relays;deployment wireless relay;reduced cost relays;implementation deployment relays;relays proposed deployment;relay placement;relay networks;optimal sequential decision;sensor networks algorithm;deployment relays;relay equally;cost relays;wireless relay networks;relay deployment;relay deployment agent;approach sensor networks;hop cost distancel;position policy relay;deploys relays locations;relay optimal power;cost step optimal;automatic sensor networks"}, "f20654f481843ec9eb11bcd00e418aec2470dfa5": {"ta_keywords": "distributed storage codes;fault tolerance storage;constructing distributed storage;distributed storage;distributed storage systems;erasure codes storage;storage codes efficient;codes storage efficient;codes storage;storage codes;deployed distributed storage;tolerance storage efficient;errorasure codes extensively;storage systems;traditional erasure codes;piggybacking constructing distributed;storage efficient manner;tolerance storage;erasure codes;storage systems instead;storage efficient result;storage efficient;replication;storage;instead replication;errorasure codes;replication achieve;constructing distributed;systems instead replication;replication achieve fault", "pdf_keywords": "repairefficient storage codes;repair distributed storage;codes distributed storage;efficient node repair;efficient repair parity;designing storage codes;regenerating codes distributed;node repair distributed;design repairefficient storage;storage codes literature;repairefficient storage;node repair parity;code various storage;nodes repair parity;distributed codes;distributed distributed codes;storage codes nutshell;parity nodes repair;distributed storage;distributed storage systems;repair distributed;various storage codes;algorithm efficient repair;storage codes;repair parity nodes;parity nodes codes;algorithms efficient repair;locality repair;piggybacking regenerating code;locality repair present"}, "6e05d35d072cd73fa039fd60696a8fe110f1d6cd": {"ta_keywords": "recommendation path constrained;backgroundcontextual recommendation path;recommendation path;walks recommendation increasingly;random walks recommendation;backgroundcontextual recommendation;walks recommendation;recommendation systems deal;graph representation publication;challenges recommendation systems;representation publication databases;recommendation systems;recommendation increasingly;publication databases rich;constrained random walks;recommendation increasingly important;databases rich meta;rich meta data;meta data representation;publication databases;path constrained random;posts challenges recommendation;random walks;graph representation;recommendation;abundance online documents;data representation path;constrained random walk;available context information;meta data", "pdf_keywords": ""}, "3c4dfc252c214d559fadb5e3159bcc9c7db08fbc": {"ta_keywords": "fluorosis extracted teeth;background dental fluorosis;fluorosis result pitted;dental fluorosis increasing;dental fluorosis;lesions fluorosis extracted;dynamics lesions fluorosis;fluoride environment fluorosis;causes hypomineralization enamel;fluorosis extracted;hypomineralization enamel;fluorosis causes hypomineralization;surface severe fluorosis;hypomineralization enamel tooth;lesions fluorosis;extracted teeth material;mild fluorosis visible;fluorosis result;extracted teeth;fluorosis visible;enamel tooth development;development mild fluorosis;mild fluorosis;tooth surface severe;fluorosis causes;exposure fluoride environment;severe fluorosis result;fluoride environment;environment fluorosis;fluorosis increasing", "pdf_keywords": ""}, "294f8307f26eb3ec7bbf19f15092f3c473ece821": {"ta_keywords": "training relation extractors;using distant supervision;distant supervision imitation;distant supervision approaches;relation extractors textbound;context extracting relations;supervision imitation learning;relation extractors;entity recognition;distant supervision;extracting relations;entities using distant;dtantly supervised approaches;relations knowledge base;named entity recognition;extracting relations non;supervision imitation;entity recognition classification;relations knowledge;known relations knowledge;art distant supervision;dtantly supervised;extractors textbound annotation;context extracting;methods dtantly supervised;supervision approaches;supervision approaches use;allow training relation;instead known relations;learning methods dtantly", "pdf_keywords": ""}, "6a2c4a0f04c6ba2f6fbc171dcea8730423a298e5": {"ta_keywords": "learning prune metric;prune metric;neighbor retrieval metric;prune metric non;approximate nearest neighbor;nearest neighbor retrieval;nearest neighbor;neighbor retrieval;learning prune approaches;retrieval metric;retrieval metric non;effective learning prune;learning prune;prune approaches density;introduction learning prune;prune approaches;approximate nearest;data sets metric;metric spaces employ;non metric;nearest;metric;metric non metric;euclidean non metric;metric non;prune;metric spaces focus;metric euclidean non;non metric kl;retrieval", "pdf_keywords": ""}, "20d4105b276da6d6d38ed3c1bfc436f76198c240": {"ta_keywords": "political event dataset;background causal relationships;causal relationships;causal relationships critical;results political event;interaction actors performance;political event;politics financial analysis;human raters;background causal;assessments human raters;interaction actors;actors performance enhanced;causal;truth results political;event datasets evaluate;involving interaction actors;actors performance;dataset involving interaction;results political;event dataset involving;knowledge pertaining actor;event datasets;health politics financial;politics financial;event dataset;world event datasets;including health politics;investigation synthetic data;raters ground truth", "pdf_keywords": ""}, "695d4c04f6e4f7ba5f771ac7853fdbaa81713ae8": {"ta_keywords": "modal bayesian embeddings;modal bayesian embedding;bayesian embedding model;social knowledge graphs;bayesian embeddings learning;bayesian embeddings;bayesian embedding;embeddings learning social;word network embeddings;learn latent topics;learning social knowledge;latent topics generate;modal bayesian;open knowledge bases;knowledge graphs;multi modal bayesian;embeddings learning;network embeddings;introductionmulti modal bayesian;network embeddings results;embedding model;knowledge graphs described;knowledge graphs propose;latent topics;knowledge bases;embeddings results;connected open knowledge;embeddings;embedding model genvector;social networks", "pdf_keywords": "topic models embeddings;topic models embeddingthe;topic sampling embedding;social knowledge graphs;word embeddings network;knowledge graphs learning;modal bayesian embedding;bayesian embedding model;connected knowledge bases;bayesian embedding;models embeddings propose;knowledge graphs collect;user embeddings results;latent topics generate;topic models;knowledge graphs employ;use topic models;embeddings results better;knowledge bases;learn latent topics;embeddings network embeddings;knowledge graphs;embedding model learning;models embeddings;word embeddings;better user embeddings;topic models multi;data embeddings;embedding model jointly;user embeddings"}, "3b8494614903dc47579da30477b21b109b29f8cd": {"ta_keywords": "automatic automatic automatic;automatic automatic;automatic;method automatic automatic;method automatic;new method automatic;machine learning;machine learning form;development machine learning;learning;development machine;learning form;report development machine;learning form new;machine;new method;report development;report;development;method;form new method;new;form new;form", "pdf_keywords": ""}, "74c881830a9cd7ea49795faa5c582b7ec56bd0bf": {"ta_keywords": "speech recognition ar;speech enhancement anr;speech recognition;automatic speech recognition;multichannel speech;sequence s2s modeling;automatic speech;entire multichannel speech;multichannel speech enhancement;sequence sequence s2s;sequence s2s;paradigm automatic speech;speech enhancement;electronic 2e ar;electronic 2e;ability electronic 2e;sequence sequence sequence;introduction sequence sequence;talk far field;sequence sequence;s2s modeling;end end e2e;optimize conventional anr;s2s modeling popular;sequence;standard close talk;speech;end e2e;2e ar standard;enhancement anr den", "pdf_keywords": "multichannel speech recognition;hybrid automatic speech;speech recognition standard;speech recognition;multichannel speech;development multichannel speech;speech recognition systems;automatic speech recognition;noise speech recognition;approach speech recognition;speech recognition competitive;end multichannel speech;automatic speech;speech recognition living;speech recognition rapidly;field automatic speech;speech recognition used;speech recognition applications;field speech recognition;speech recognition article;development speech recognition;new architecture speech;speech recognition use;applications field speech;architecture speech noise;art speech recognition;generate noise speech;noise speech;speech noise;architecture speech"}, "3d3b1300c7cd6a820a6d08605248f875a3ad20b9": {"ta_keywords": "hop reasoning models;interpretability multi hop;hop reasoning widely;interpretable link prediction;multi hop reasoning;backgroundmulti hop reasoning;hop reasoning;path recall;link prediction experiments;reasoning models;reasoning models advance;obtain interpretable link;prediction experiments paths;interpretable link;including path recall;quantitatively evaluate interpretability;link prediction;interpretability multi;interpretability evaluation methodsin;paths given models;interpretability evaluation;evaluate interpretability multi;interpretability;evaluate interpretability;obtain interpretable;reasoning widely studied;paths;interpretable;reasoning widely;multi hop", "pdf_keywords": "hop reasoning models;multi hop reasoning;reasoning models multi;interpretability multihop reasoning;interpretability multi hop;hop reasoning multi;knowledge graph reasoning;multihop reasoning models;reasoning models interpretable;reasoning multi hop;reasoning model multi;multihop reasoning modelswe;hop reasoning useful;metricsmulti hop reasoning;possible reasoning paths;reasoning paths;reasoning paths reasonable;reasoning models useful;reasoning modelswe propose;reasoning useful tool;reasoning models;reasoning paths compute;reasoning models widely;hop reasoning task;reasoning modelswe;knowledge graph;reasoning models advance;reasoning models based;evaluating interpretability multihop;based multihop reasoning"}, "e0c54e18cf2372414042bf67eed0272b0a432190": {"ta_keywords": "etiology social media;social media understood;social media poorly;media understood etiology;social media;understood etiology social;media poorly understood;etiology social;reported etiology social;media understood;understood etiology;etiology;social;media poorly;media;reported etiology;poorly understood;recently reported etiology;poorly understood recently;understood recently reported;understood;understood recently;reported;poorly;recently reported;recently", "pdf_keywords": ""}, "7da967be8f6367f6174bf99d0d019ff545ac5966": {"ta_keywords": "semantic annotation lingvosemantics;semantic annotation lingvosemantic;annotation lingvosemantic corpus;annotation lingvosemantic;annotation lingvosemantics;annotation lingvosemantics project;lingvosemantic corpus presented;lingvosemantic corpus;semantic annotation;introduction semantic annotation;methodology semantic annotation;annotation;semantic;lingvosemantics;introduction semantic;corpus;lingvosemantic;corpus presented;lingvosemantics project;lingvosemantics project methods;corpus presented results;methodology semantic;paper methodology semantic;introduction;presented;methods;presented results;methodology;project;results presented", "pdf_keywords": ""}, "49db57f300b270f16cbcb1891ca39e16981d42b5": {"ta_keywords": "covid 19 pandemic;public health surveillance;data pandemic relevant;data pandemic;covidcast api;pandemic relevant public;health surveillance;covidcast;covid 19;pandemic presented;19 pandemic presented;date data pandemic;covidcast api attempt;resolution covidcast api;covid;temporal resolution covidcast;pandemic;19 pandemic;pandemic relevant;health surveillance signals;resolution covidcast;pandemic presented enormous;policy makers epidemiological;public health;epidemiological modelers;epidemiological modelers health;makers epidemiological modelers;surveillance;surveillance signals cases;epidemiological", "pdf_keywords": ""}, "bad416f073a08086ee428e5a264eac3a7d3251e5": {"ta_keywords": "syndromic model diagnosis;syndromic model;patient malignant disease;treatment patient malignant;diagnosis treatment patient;malignant disease;syndromic;patient malignant;model diagnosis treatment;diagnosis treatment;role syndromic model;diagnosis;role syndromic;treatment patient;malignant;model diagnosis;disease;knowledge role syndromic;treatment;patient;model;article present;article;knowledge;knowledge role;aim article present;aim article;article present current;present current knowledge;role", "pdf_keywords": ""}, "d9dbdd254b02ef1af2769af403cba373c1b1bcb1": {"ta_keywords": "speaker diarization method;based speaker diarization;clustering speaker representations;speaker diarization;outputs speaker diarization;extraction clustering speaker;speaker diarization problem;speaker diarization results;formulate speaker diarization;clustering speaker;speaker representations;network based speaker;speaker representations instead;model formulate speaker;formulate speaker;outputs speaker;directly outputs speaker;based speaker;speaker;end neural network;diarization method;diarization method unlike;diarization;diarization problem;single neural network;diarization results realize;diarization problem multi;neural network;diarization results;end end neural", "pdf_keywords": "neural networkbased speaker;networkbased speaker diarization;neural speaker diarization;speaker label permutation;end neural speaker;clustering speaker representations;speaker diarization model;speaker diarization;sequence speaker labels;clustering clustering speaker;based speaker diarization;speaker diarization method;trained mixture speech;clustering speaker;speaker diarization problem;speech diarization challenge;formulate speaker diarization;embeddings partitioned speaker;speaker representation learning;speaker representation;speech diarization;speaker dependent clusters;speech recognition neural;dataset speech;speaker representations;partitioned speaker dependent;speech clusters;speech recognition fundamental;neural speaker;speech recognition"}, "5931c8ac145baf17cec9effc25c051049b7dfd4c": {"ta_keywords": "grounded neural dialogue;neural dialogue;neural dialogue model;dialogue agent accurately;dialogue model;dialogue agent;dialogue model successfully;observable reference game;dialogue;grounded neural;reference game;results dialogue agent;present grounded neural;agent accurately grounds;communicate pragmatically;reference game methods;object share agents;communicate pragmatically solve;accurately grounds referents;grounds referents partner;collaborates people partially;information communicate pragmatically;model successfully collaborates;collaborates people;people partially observable;agents;collaborates;agents observe overlapping;communicate;agent accurately", "pdf_keywords": "referents dialogue game;reference dialogue games;dialogue based reference;neural dialogue model;collaborative reference dialogue;collaborative dialogue task;modular neural dialogue;dialogue task approach;dialogue model;dialogue games objective;dialogue task;mention dialogue;dialogue game;task dialogue agent;referents mention dialogue;dialogue agent accurately;neural dialogue;referents dialogue;collaborative dialogue;dialogue systems;dialogue games;grounded neural dialogue;dialogue game paired;approach agent utterances;dialogue model model;reference dialogue;processing human dialogues;dialog model referring;referring expression dialogue;dialogue agent"}, "6c4258f6a6a4bee7b9d914379c44aea6073cdc37": {"ta_keywords": "energy disaggregation problem;energy disaggregation;introductionenergy disaggregation adaptive;disaggregation adaptive filtering;objectivethe energy disaggregation;disaggregation adaptive;novel disaggregation algorithm;disaggregation algorithm;disaggregation algorithm better;power consumption signals;introductionenergy disaggregation;disaggregation problem reformulated;power consumption signal;disaggregation;disaggregation problem;novel disaggregation;disaggregation problem recovering;gives novel disaggregation;level power consumption;adaptive filtering;adaptive filtering important;adaptive filtering problem;consumption signal building;aggregate power consumption;paper disaggregation problem;power consumption;reformulated adaptive filtering;methodswe paper disaggregation;paper disaggregation;consumption signal", "pdf_keywords": "energy disaggregation filter;disaggregation energy consumption;online energy disaggregation;disaggregation electricity consumption;estimate disaggregated signals;energy disaggregation;backgroundenergy disaggregation adaptive;disaggregation adaptive filtertering;disaggregation adaptive;energy disaggregation problem;energy disaggregation single;disaggregation energy;disaggregation problem energy;novel disaggregation algorithm;disaggregation filter banks;approach disaggregation electricity;disaggregation algorithms;posed disaggregation algorithms;problem energy disaggregation;disaggregation filter;disaggregation algorithm;method disaggregation energy;disaggregation electricity;disaggregation algorithm better;new approach disaggregation;power consumption patterns;disaggregation energy electric;disaggregated signals;disaggregation problem electricity;novel disaggregation"}, "3dcf9c900f5f28e082a2fcdea4763b6063a76f09": {"ta_keywords": "defeasible reasoning;defeasible reasoning suggests;defeasible reasoning mode;scenario answering defeasible;answering defeasible;reasoning mode reasoning;asks neural models;reasoning suggests person;literature defeasible reasoning;scenario answering questions;question scenario answering;reasoning mode;background defeasible reasoning;scenario answering;asks neural;reasoning conclusions overturned;reasoning conclusions;goal asks neural;evidence existing cognitive;reasoning;answering questions methods;reasoning suggests;envisioning question scenario;answering defeasible query;problem scenario answering;envisioning question;existing cognitive;existing cognitive science;mental model;cognitive science", "pdf_keywords": "inference graph novel;generate inference graph;inference graphs;inference graph based;think question answering;graph learning reasoning;question answering;inference graph employ;inference graph;based inference graph;learning reasoning tasks;question answering improve;graph based inference;inference graph use;generating inference graph;using inference graphs;query generate inference;reasoning tasks;generate inference;learning reasoning;using inference graph;model inference graph;reasoning models;answering improve performance;asks neural models;inference graphs algorithm;defeasible inference task;models graph learning;graph neural;improve defeasible inference"}, "dbdb7f25f1538c2a2885d3992e5320e2ee5c23a1": {"ta_keywords": "equation solving teaching;facilitate tutor learning;solving teaching computer;solving teaching;facilitate tutor;tutor learning;pedagogical teachable agent;factors facilitate tutor;tutor learning effect;teaching computer agent;tutor;teachable agent;students learn algebra;methodssmstudent pedagogical teachable;teaching computer;learn algebra equation;pedagogical teachable;sistuddent methodssmstudent pedagogical;learning teaching;learn algebra;methodssmstudent pedagogical;teaching;pedagogical;classroom settings learning;teachable agent commits;settings learning teaching;teachable;students learn;learning teaching lb;classroom", "pdf_keywords": ""}, "660119405bb48777cd71d85caa5ec2e90a336caf": {"ta_keywords": "historical text normalization;text normalization techniques;text normalization;normalization techniques;normalization techniques proposed;normalization;approach historical text;regard historical text;statistical machine translation;historical text;character based statistical;translation neural;machine translation neural;machine translation;translation neural encoder;metrics character based;text;decoder models;metrics character;decoder models studies;neural encoder decoder;distance metrics character;decoder;approach historical;encoder decoder models;statistical machine;encoder decoder;based statistical machine;neural encoder;translation", "pdf_keywords": "historical text normalization;text normalization historical;text normalization systems;text normalization techniques;text normalization;normalize historical word;text normalization fundamental;normalization historical texts;text normalization quality;text normalization starting;normalization historical languages;field text normalization;spelling normalisation data;historical normalization results;historical language neural;historical spelling normalisation;normalize historical;used normalize historical;current literature normalization;normalization historical;historical normalization;spelling normalisation novel;spelling normalisation;language technology normalisation;available normalization task;useful normalization task;normalization results compared;normalization prediction automatic;normalization task;normalization task challenge"}, "98554bd8a15172e9a6ef3cc3db3bc52504110fc9": {"ta_keywords": "objectives stochastic bandits;bandits regret minimization;stochastic bandits regret;bandits fixed horizon;stochastic bandits;stochastic bandits fixed;bandits regret;exploitation exploration crucial;bandits;archetypal objectives stochastic;exploitation exploration;regret minimization;bandits fixed;balance exploitation exploration;regret minimization pm;objectives stochastic;pareto frontier archetypal;pareto frontier;frontier archetypal objectives;study pareto frontier;balance exploitation;exploitation;archetypal objectives;exploration crucial;methodswe study pareto;pareto;objectives;stochastic;regret;purposeto study pareto", "pdf_keywords": "regret stochastic bandits;guarantee stochastic bandits;objectives stochastic bandits;rr stochastic bandits;algorithm stochastic bandits;stochastic bandits;stochastic bandits design;stochastic bandits arxiv;identification stochastic bandits;stochastic bandit;stochastic bandits algorithm;stochastic bandits fixed;stochastic bandits stochastic;stochastic bandit probabilities;bandits stochastic;reward stochastic bandit;stochastic bandits pseudo;bandits stochastic bandits;bandits pseudo regret;bandits algorithm simultaneously;bandits fixed horizon;adversarial bandits explore;study stochastic bandits;staochastic bandits method;bandits algorithm;identification adversarial bandits;regret algorithms bounded;adversarial bandits;bandits explore pareto;identification stocochastic bandits"}, "6b3fa9157a8120a6eb86ae06a93611a1fcd9e219": {"ta_keywords": "extract structured information;unstructured information;bibliographic databases harvested;information resulting databases;resulting databases soft;hard databases methodswe;quantity unstructured information;databases methodswe;unstructured information cases;bibliographic databases;databases soft;scienti papers databases;structured information;databases harvested;databases constructed;databases soft contain;heuristically extract structured;extract structured;databases harvested raw;large bibliographic databases;hard databases;databases;resulting databases;papers databases constructed;heterogeneous hard databases;large quantity unstructured;structured information resulting;papers databases;databases constructed merging;unstructured", "pdf_keywords": ""}, "ef9ddbc35676ce8ffc2a8067044473727839dbac": {"ta_keywords": "softmax distributed word;introductionthe softmax bottleneck;limited softmax bottleneck;softmax bottleneck;softmax bottleneck complex;softmax bottleneck discussiongiven;limited softmax;models limited softmax;problem expressiveness softmax;softmax;expressiveness softmax;practice softmax distributed;neural language models;expressiveness softmax based;softmax distributed;softmax based models;distributed word embeddings;softmax based;practice softmax;introductionthe softmax;majority neural language;implies practice softmax;language modeling matrix;word embeddings;language models;language models limited;neural language;word embeddings does;language models formulate;language highly context", "pdf_keywords": "language softmax;natural language softmax;language softmax bottleneck;context softmax;context softmax bottleneck;softmax distributed word;given context softmax;softmax;limited softmax;neural language models;models limited softmax;parametric language models;softmax based models;softmaxes;limited softmax bottleneck;neural language modeling;ofthe softmax bottleneck;softmax based;softmax bottleneck;analysis softmax bottleneck;statistical language models;model natural language;softmaxes particular instantiation;language modeling matrix;practice softmax distributed;empirically moc softmax;distributed word embeddings;ofthe softmax;practice softmax;alleviate softmax bottleneck"}, "d29f155060f96becef0247ee77dc038f96b2d983": {"ta_keywords": "translation processing time;conventional speech translation;speech translation systems;speech translation;machine translation module;translation start time;translation processing;improve translation start;time translation processing;machine translation;translation systems difficult;start time translation;translation systems;time machine translation;shortening translation unit;translation module;phrase base translation;shortening translation;translation module methods;translation start;improve translation;method shortening translation;base translation propose;base translation;translation unit;translation propose method;time translation;delay begining utterance;utterance end synthesis;translation unit examine", "pdf_keywords": ""}, "d415b724fbc35afcc8dd91738123edfa6a5db634": {"ta_keywords": "deep policy gradients;policy gradient algorithms;deep policy gradient;policy gradients;algorithms proximal policy;progress deep policy;policy gradients case;policy gradient;region policy optimization;policy optimization trpo;proximal policy optimization;deep policy;policy optimization ppo;policy optimization;trust region policy;policy optimization study;optimization ppo trust;algorithmic progress deep;matters deep policy;region policy;implementation matters deep;gradient algorithms case;code level optimizations;proximal policy;gradient algorithms;trust region;optimization trpo specifically;optimization ppo;progress deep;ppo trust region", "pdf_keywords": "algorithm deep policy;deep policy gradient;program policy gradient;policy gradient algorithms;policy gradient methods;optimizations policy gradient;policy gradients;popular deep reinforcement;optimization policy gradient;implementation policy gradient;develop policy gradient;deep reinforcement learning;progress deep policy;code optimizations policy;method deep reinforcement;deep reinforcement;use policy gradient;mechanism policy gradients;designing deep policy;policy gradient;algorithmic progress deep;conceptual policy gradient;policy gradient method;policy gradient framework;algorithms proximal policy;useful optimization policy;algorithm deep;policy gradients ppo;gradient method deep;algorithms update neural"}, "3315dee45b1edb8f8286816629de7b8c31d270d6": {"ta_keywords": "influenceces political information;political information search;influence information search;affect influence information;political information;search strategies political;influence information;search behavior participants;strategies political judgments;judgments subjects voting;signals affect influence;political judgments subjects;influenceces political;social signals affect;participants neutral information;political judgments;liked disliked information;affect influence;resultswe social cues;social signals;information search;subjects voting;search behavior;information search strategies;backgroundsocial influenceces political;disliked information;social cues;information search evaluation;simple social signals;social cues fact", "pdf_keywords": ""}, "387754dc8d4185fadd7c3c15e43956a4d085e8fe": {"ta_keywords": "nearest neighbor search;approximate nearest neighbor;nearest neighbor;neighbor search methods;sorted distance;sorted distance point;introductionpermutation search methods;permutation based methods;pivots sorted distance;methods approximate nearest;neighbor search;introductionpermutation search;permutation based;approximate nearest;distance point ranked;nearest;faster search possible;efficient faster search;lists called permutations;search methods efficient;faster search;survey permutation based;search methods;pivots sorted;point ranked lists;list pivots sorted;ranked lists;search possible;survey permutation;called permutations", "pdf_keywords": "approximate nearest neighbor;nearest neighbor search;neighbor search methods;neighbor search metric;proximity based retrieval;distance permutation based;distances permutation effective;neighbor search space;nearest neighbor;sorted distance point;approximate knearest neighbor;permutation based searching;carry nearest neighbor;approach nearest neighbor;sorted distance;benchmarked permutation methods;increasing distances permutation;distances permutation;pivots sorted distance;method benchmarked permutation;distance point ranked;benchmarked permutation;distance permutations good;distance permutation;new approach nearest;methods approximate knearest;useful search metric;neighbor search;spaces proximity based;knearest neighbor search"}, "60a121c55b5144bfe3aef5b6ea8959a9f6dd12ae": {"ta_keywords": "speech enhancement algorithms;multiple speech enhancement;learning speech enhancement;speech enhancement noisy;speech enhancement;problem speech enhancement;ensemble learning speech;enhancement noisy mixture;speech enhancement important;enhancement algorithms;enhancement noisy;ensemble learning;enhancement algorithms formula;speech enhancement years;ensemble;enhancement important problem;enhancement;mixture succeeded improving;multiple speech;background ensemble learning;noisy mixture;enhancement important;background ensemble;noisy mixture succeeded;combining;strengths multiple speech;learning speech;possibility combining;algorithms proposed;investigate possibility combining", "pdf_keywords": ""}, "ba56bb1eb67b188a89060058ef8ad02ce3c660ac": {"ta_keywords": "novel translational approach;variant novel translational;translation novel variant;approach translation novel;novel translational;clinical messagewe;clinical messagewe report;translational approach translation;novel novel translational;key clinical messagewe;translation novel;translational approach;approach translation;translational;translation;clinical;key clinical;messagewe report case;messagewe report;messagewe;novel variant novel;novel variant;variant novel;report case novel;case novel novel;case novel;novel;novel novel;variant;report case", "pdf_keywords": ""}, "9fc33c53d1f59aa9fd7f1b642c3859900865b0e3": {"ta_keywords": "semi structured data;structured data web;table hyponym data;hyponym data;data web challenging;structured data;table hyponym;hyponym data small;data web;representing semi structured;queries like set;collection table hyponym;consider queries like;queries like;consider queries;representation large collection;semi structured;class prediction resultswe;queries;large collection table;collectively representing semi;methodswe consider queries;prediction resultswe;representing semi;like set expansion;low dimensional representation;large collection;data;web challenging task;expansion class prediction", "pdf_keywords": ""}, "d7729f2ff21f97d56d10c54adc1f1f5ffbec9e5c": {"ta_keywords": "premalignant lesions leukoplakia;oral premalignant lesions;lesions leukoplakia;lesions leukoplakia erythroplakia;laser excision;introduction oral premalignant;oral premalignant;leukoplakia erythroplakia;leukoplakia erythroplakia remain;standard laser excision;laser excision methods;modality oral premalignant;leukoplakia;excision methods lasers;erythroplakia remain diagnostic;surgical excision using;premalignant lesions;surgical excision;excision using scalpels;erythroplakia remain;erythroplakia;premalignant lesions potential;treatment modality oral;lesions;advantages surgical excision;lesions potential advantages;malignant transformation management;surgical surgical;non surgical surgical;excision", "pdf_keywords": ""}, "649eb9fd9a18f9601270b7fcde8d6548bfc6ec75": {"ta_keywords": "speech separation recognition;speaker speech separation;models automatic speech;speech recognition model;automatic speech recognition;speech separation;automatic speech;end automatic speech;monaural multi speaker;multi speaker speech;multi speaker end;speech recognition ar;speech recognition;speaker end;multi speaker;speaker end end;separation recognition;separation recognition task;cocktail party problem;ar multi speaker;speaker speech;cocktail party;task cocktail party;speaker;hybrid models automatic;recognition ar multi;recognition model;cocktail;recognition model results;speech", "pdf_keywords": "end speech recognition;end multispeaker speech;speech separation recognition;speech recognition multi;multi speech recognition;speech recognition joint;multispeaker speech recognition;models automatic speech;speech recognition model;end end multispeaker;multi speaker end;speech recognition new;multi speaker speech;speaker speech separation;conversational speech recognition;end automatic speech;end multispeaker;monaural multi speaker;multispeaker speech;speech recognition ar;recognition multi speakers;single speaker speech;automatic speech;end multi speakin;automatic speech recognition;endto end speech;multi speech;end speech;speaker end;speech recognition"}, "5884948777dfc003ba49e1513420830616281839": {"ta_keywords": "learning multilingual representations;lingual transfer learning;learns unified multilingual;learning multilingual;unified multilingual representations;multilingual representations text;multilingual representations;multilingual representations using;trained monolingual representations;monolingual cross lingual;unified multilingual;monolingual representations shared;cross lingual transfer;cross lingual objectives;cross lingual;using monolingual cross;method cross lingual;independently trained monolingual;monolingual representations;introduction learning multilingual;representations using monolingual;multilingual;trained monolingual;lingual transfer;using monolingual;lingual objectives jointly;monolingual cross;transfer learning;transfer learning tasks;lingual objectives", "pdf_keywords": "lingual transfer learning;learns unified multilingual;trained monolingual representations;unified multilingual representations;multilingual language training;independently trained monolingual;language model pretraining;multilingual representations;monolingual representations shared;trained monolingual;multilingual word embeddings;joint methods bilingual;tasks bilateral lingual;improves cross lingual;multilingual language modeling;learning representations alignment;unsupervised bilingual;crosslingual translation semantic;multilingual representations using;monolingual representations;transfer cross lingual;crosslingual translation method;lingual word embedding;bilingual tasks;cross lingual transfer;unified multilingual;unsupervised multilingual multilingual;method crosslingual translation;lingual ner tasks;approach cross lingual"}, "56bc2a1eebedab3e452a7ca3969aa1e4dd5946c3": {"ta_keywords": "diversity influencetial node;diversity influence maximization;influencetial node mining;node diversity influence;node mining diversity;influence maximization im;influence maximization;incorporate node diversity;node diversity;mining diversity crucial;similarity selected nodes;influencetial node;mining diversity;diversity influencetial;node mining;embedding community detection;diversity influence;community detection results;ranking mining;community detection;node embedding community;average similarity selected;introductionincorporating diversity influencetial;diversity reverse measure;nodes;criterion ranking mining;influence;influencetial;ranking mining tasks;selected nodes", "pdf_keywords": ""}, "af0adbaa0c1ea6abaed4b3d21f1dc4121c35fb30": {"ta_keywords": "neural communicative agents;communicative agents trained;language coordination agents;communicative agents;building neural communicative;neural communicative;shot language coordination;language coordination;current communicative agents;conversational partners language;communicative agents methodswe;coordination agents quickly;humans communicate;agents trained selfplay;coordination agents;adapting conversational partners;short conversations ability;communicative;communicate;conversations ability;adapting conversational;quickly adapting conversational;humans communicate large;conversational partners;partners language abilities;coordination;interlocutors short conversations;agents quickly adapting;task shot language;short conversations", "pdf_keywords": "communicative agents trained;language coordination agents;neural communicative agents;communicative agents;shot language coordination;building neural communicative;quickly adapting conversational;communicative agents methodswe;language coordination;current communicative agents;verbal language coordination;agents trained selfplay;adapting conversational;short conversations ability;neural communicative;interaction speaker learned;adapting conversational partners;coordination agents quickly;language coordination modeling;conversations ability;achieve communicative;language coordination context;agents quickly adapting;agents different linguistic;task shot language;communicative goal speaker;human language;communicative goal;learning communication;coordination agents"}, "3c57a1aa483d8bffe1339914b80d2913f2dc8376": {"ta_keywords": "good semi supervised;semi supervised;semi supervised learning;semi supervised classification;clear semi supervised;generative adversarial;generative adversarial networks;adversarial networks obtained;supervised;based generative adversarial;supervised learning;supervised learning requires;training generator good;adversarial networks;supervised classification performance;supervised classification;adversarial;joint training generator;supervised learning methods;discriminator benefits joint;generative;training generator;classification;classification performance good;discriminator benefits;generator good semi;discriminator;based generative;methods based generative;classification performance", "pdf_keywords": "data discriminator learns;semi supervised;semi supervised learning;discriminator learns;good semi supervised;data semi supervised;discriminator learns correct;discriminator generator objectives;generated data discriminator;discriminator objective good;generator supervised objective;algorithm semi supervised;method semi supervised;present semi supervised;discriminator objective;data discriminator;discriminator generator;semisupervised learning;semisupervised learning requires;generalization performance supervised;generator supervised;improve generalization supervised;formulation discriminator generator;generalization supervised;good semisupervised learning;supervised learning improved;supervised learning performance;complement generator supervised;feature matching objective;discriminator propose"}, "866f231970f93f4a201febc2fb46aff06f501e4b": {"ta_keywords": "normalization historical spelling;normalization historical language;automatic normalization historical;normalization historical;historical language data;semi automatic normalization;automatic normalization;historical spelling case;historical spelling;normalization;language data;spelling case studies;language data resultsaverage;historical language;high german methodsthis;german methodsthis;german methodsthis paper;new high german;inter annotator;spelling case;inter annotator agreement;annotator;resultsaverage inter annotator;high german;annotator agreement;german;annotator agreement 88;german aimto manual;spelling;manual semi automatic", "pdf_keywords": ""}, "4d96ec46cda5d3b223fc7d33a920ab85864ea36d": {"ta_keywords": "protein function prediction;protein function;function prediction annotation;learning architecture function;functional role proteins;role proteins amino;proteins amino acid;proteins amino;deep learning architecture;proteins;amino acid sequence;neural descriptions orf;function identification neural;deep learning;role proteins;protein;function prediction;introduce deep learning;amino;neural descriptions;correctly predict functional;identification neural descriptions;predict functional;amino acid;architecture function identification;biological studies molecular;learning architecture;neural;prediction annotation;predict functional role", "pdf_keywords": ""}, "6c170fe3fec5a477c938d07fa00935bb6f7b87cc": {"ta_keywords": "voice conversion;voice conversion using;voice conversion thegmm;based voice conversion;quality converted speech;converted speech significantly;converted speech;model based voice;covariance acoustic models;gassian mixture model;voice;tied covariance acoustic;acoustic models methods;covariance acoustic;conversion thegmm based;acoustic models;adaptation quality converted;mixture model based;conversion thegmm;approach gassian mixture;gassian mixture;mixture model;speech significantly;natural speech;quality converted;model adaptation quality;speech significantly worse;worse natural speech;model adaptation;based voice", "pdf_keywords": ""}, "adac290d72c86c186837a884aae922bee4dee684": {"ta_keywords": "disrupt natural reading;natural reading methodsprevious;objectiveintuitively human readers;natural reading objectiveintuitively;human readers cope;human readers;readers cope easily;reading objectiveintuitively human;increased reading times;increased reading;natural reading;letter transpositions;letter transpositions result;reading methodsprevious work;reading methodsprevious;reading;reading objectiveintuitively;reading times;backgroundintuitively human readers;text typos;text typos misspelling;misspelling word substitutions;readers;indicates letter transpositions;typos misspelling word;result increased reading;misspelling;typos misspelling;misspelling word;readers cope", "pdf_keywords": "reading difficulty misspellings;misspellings studied using;texts transpositions misspellings;misspellings tested;human reading effortless;difficulty misspellings;difficulty misspellings explained;misspellings used corpus;misspellings studied;read words errors;erroneous words read;human reading affected;difficult read words;transpositions misspellings tested;compared misspellings transpositions;read words texts;compared misspellings previous;misspellings explained using;fixations compared misspellings;words difficult read;compared misspellings;misspellings tested hypothesis;human reading significantly;compared words errors;rate compared misspellings;real misspellings studied;corpus human edits;words cause reading;ability read texts;cause reading difficulty"}, "23918ed366c60ae0ef85b0c80def63127f035e02": {"ta_keywords": "shredder;devised shredder;challenge devised shredder;devised shredder end;computation heavy inference;cloud;privileged data network;shredder end;shredder end end;communicated data maintaining;cloud increasingly;perform computation heavy;private privileged data;data network;data maintaining;cloud increasingly used;compromising privacy;computation heavy;heavy inference;privileged data;potentially compromising privacy;content communicated data;network improve information;servers exposing;servers;communicated data;privacy;heavy inference practice;privacy address challenge;private privileged", "pdf_keywords": ""}, "e7e1f5a713d20cdf31e732022731fdf0d8fb4fc5": {"ta_keywords": "generate explanations classifiers;explanations classifiers;explanations classifiers operating;explanations predictions classifiers;natural language inference;language inference nli;generating explanations predictions;supervised sentence pair;sentence pair classification;inference nli widely;generating explanations;token level explanations;inference nli;generate explanations;language inference;supervised sentence;attempts generate explanations;explanations predictions;modeled supervised sentence;recently generating explanations;explanations nonli;natural language;level explanations nonli;task natural language;sentences methodsin;pairs sentences methodsin;predictions classifiers;classifiers;pairs sentences;sentences", "pdf_keywords": "prediction salient tokens;generate explanations classifiers;explanations thresholding attention;explanations predictions classifiers;natural language inference;generating explanations predictions;language inference nli;generate tokenlevel explanations;explanations classifiers;inference nli widely;jointly predict entailment;supervised sentence pair;predict entailment;regularizing model attention;generating explanations;explanations classifiers operating;generated better explanations;token level explanations;predict entailment relation;salient tokens better;task natural language;generate explanations;sentence pair classification;inference nli;entailment relation token;model attention distributions;salient tokens;tokenlevel explanations;tokenlevel explanations nonli;modeled supervised sentence"}, "37ef7941909527aaf123d7b8f90adbf4606f4917": {"ta_keywords": "parallelization versus hadoop;hadoop facility computing;implementations run hadoop;run hadoop facility;parallel distributed implementations;hadoop facility;implementations iterative gibbs;gibbs sampling machine;ihmm parallelization;iterative gibbs sampling;distributed implementations iterative;scaling ihmm parallelization;ihmm parallelization versus;run hadoop;parallel distributed;hadoop;parallelization;versus hadoop;distributed implementations;computing clouds;facility computing clouds;gibbs sampling;iterative gibbs;compare parallel distributed;sampling machine learning;distributed implementations run;parallelization versus;computing clouds results;clouds results probabilistic;distributed", "pdf_keywords": ""}, "58e5ce12c23f815e9b394220044eaf99b28cfffe": {"ta_keywords": "patient diagnosis malignant;diagnosis malignant disease;diagnosis malignant;malignant disease;diagnosis management patient;physician diagnosis;patient diagnosis;malignant;physician diagnosis management;management patient diagnosis;role physician diagnosis;diagnosis;diagnosis management;literature role physician;management patient;physician;role physician;disease;patient;article present literature;article present;literature role;literature;present literature role;management;article;purpose article present;present literature;purpose article;role", "pdf_keywords": ""}, "cf2fcb73e2effff29ceb5a5b89bbca34d2d27c1a": {"ta_keywords": "deceptive attention;deceptive attention masks;produce deceptive attention;predictive accuracy attention;use attention mechanisms;introduction attention mechanisms;accuracy attention;attention mechanisms ubiquitous;attention mechanisms;attention weights;attention masks methods;attention mechanisms question;accuracy attention weights;attention weights claimed;attention masks;use attention;models produce deceptive;attention;introduction attention;stakeholders use attention;neural architectures;confer interpretability;confer interpretability purportedly;neural;components neural architectures;interpretability purportedly useful;natural language processing;produce deceptive;neural architectures applied;natural language", "pdf_keywords": "deceive human annotators;deceptive attention;deceptive attention masks;produce deceptive attention;models biased gender;predictive accuracy attention;heavily biased gender;models produce deceptive;attention weights;model attention;accuracy attention weights;accuracy attention;biased gender;biased gender minorities;attention humans;attention softmax;attention weights claimed;attention assigned impermissible;biased gender minoritiesin;human annotators;minorities models biased;input attention;attention mass manipulation;attention mass impermissible;attention masks resultsour;use attention mechanisms;manipulation attention;attention assigned;attention softmax perform;model attention softmax"}, "79655bfc45039b4d7cfe6cc86d52a4ced492f43a": {"ta_keywords": "learning rank methods;learning rank;rank methods collections;rank methods ubiquitous;evaluation learning rank;introduction learning rank;rank methods;relevance signals using;information retrieval;relevance signals;impact relevance signals;information retrieval advantage;rank;relevance signals requires;retrieval;specific relevance signals;ubiquitous information retrieval;retrieval advantage;low impact relevance;evaluation learning;retrieval advantage lies;methods collections large;impact relevance;usefulness specific relevance;training test data;relevance;geared evaluation learning;data collections;methods collections;data collections geared", "pdf_keywords": ""}, "5c3cc301a892094d5bfca3c41a78a3a8ebd755f8": {"ta_keywords": "additive regression trees;regression trees mart;regression trees;regression trees methodsmultiple;specialization trees added;trees methodsmultiple additive;specialization trees;issue specialization trees;trees added later;impact prediction instances;prediction instances make;prediction instances;prediction accuracy diverse;trees methodsmultiple;trees added;high prediction accuracy;methodsmultiple additive regression;dropouts;deliver high prediction;additive regression;multiple additive regression;dropouts meet multiple;trees mart known;high prediction;trees;prediction accuracy;trees mart;impact prediction;dropouts meet;prediction", "pdf_keywords": "dropouts ensemble trees;ensemble trees;additive regression trees;ensemble trees muting;trees ensemble;trees ensemble significantly;tree learn significantly;regression trees;dropouts ensemble;prediction accuracy diverse;employing dropouts ensemble;regression trees known;regression trees dart;contribution trees ensemble;tree learn;boosting neural;high prediction accuracy;boosting neural networks;learning multiple additive;regression trees mart;learn significantly reduced;random forest;accurate random forest;specialization trees;random forest models;learning ability tree;specialization trees added;ensemble;ability tree learn;classification tasks"}, "9eecfdb7c8ad9af4f3863e9f6ed857211fb710e7": {"ta_keywords": "language explanations actions;natural language explanations;explanations actions;explanations actions suggested;markov decision process;explanation mimd policies;language explanations;english language explanations;introductiona markov decision;language explanations order;markov decision;mimd policies;policies resultsthe interactively;process md policy;decision process md;interactively generates conversational;optimal policy;explanations order;suggested optimal policy;natural language;explanations order build;policy presents state;novel explanation mimd;decision process;md policy presents;state action preferably;conversational english language;policy presents;optimal policy does;mimd policies resultsthe", "pdf_keywords": ""}, "dfd8fc9966ca8ec5c8bdc2dfc94099285f0e07a9": {"ta_keywords": "differentially private mechanisms;introductiondifferentially private mechanisms;private mechanisms text;introductiondifferentially private;differentially private;private mechanisms parameterizes;empirical privacy;private mechanisms;class differentially private;mitigate empirical privacy;privacy;sensitive text mitigate;original sensitive text;sensitive text;empirical privacy risk;privacy risk propose;mechanisms text generation;text generation typically;private;text generation;privacy risk;noise input words;output word noise;nearest neighbor noised;text mitigate empirical;text mitigate;neighbor noised input;mechanisms parameterizes nearest;original sensitive;words use nearest", "pdf_keywords": "privacy text generation;empirical privacy text;privacy machine learning;privacy utility text;utility text privacy;empirical privacy utility;privacy text data;metricd optimizes privacy;text analysis privacy;maximize empirical privacy;improve privacy utility;maximizes empirical privacy;privacy text;representations privacy utility;quantify empirical privacy;text privacy;artificial privacy utility;empirically improve privacy;empirical privacy measurement;empirical privacy measure;optimizes privacy utilitythe;algorithm optimize privacy;based empirical privacy;empirical privacy;optimize privacy parameter;optimize privacy;budget empirical privacy;privacy parameter tuning;selection empirical privacy;privacy utility"}, "e8c4a4e81084e17b0c71a6a69bdf1e4e2b6f6af1": {"ta_keywords": "sequence transduction;sequence transduction problems;sequence sequence models;sequence single output;sequential sources mixture;multiple sequential sources;single output sequence;sequence model explicitly;sources mixture sequence;multi sequence model;single input sequence;sequence models;sequence model conditional;sequence sequence model;sequence single;sequence models established;input sequence single;extracting multiple sequential;sequential sources;conditional multi sequence;sequence model;backgroundneural sequence sequence;mixture sequence methodswe;multiple sequential;multi sequence;focus sequence transduction;sequence work focus;transduction problems extracting;mixture sequence;input sequence", "pdf_keywords": "output speech separation;model speech separation;speech separation benchmarks;speech separation recognition;speech separation multi;separation speech recognition;speech separation wj0;algorithm speech separation;chain model speech;task speech separation;speech separation;experiments speech separation;tasks speech separation;regression output speech;speech separation fundamental;speech separation speech;sequences experiments speech;speaker speech separation;speech data composed;separation multispeaker speech;output speech;speakers speech separation;speech separation report;model speech;speech separation multispeaker;speech data;rule speech data;including speech separation;independent speech recognition;observed speech data"}, "59121b847fd7eb4cf92cbfccb54f1705733d8b65": {"ta_keywords": "recognition reverberant speech;noise reverberation research;reverberation prior recognition;noise reverberation;reverberant speech;recognition reverberant;problem recognition reverberant;reverberation research;reverberation research undertaken;reduce reverberation prior;reverberant speech received;reduce reverberation;reverberation;reverberation prior;method reduce reverberation;performance automatic speech;presence noise reverberation;speech recognition;automatic speech recognition;automatic speech;speech recognition severely;reverberant;undertaken noise robustness;noise robustness contrast;prior recognition preprocessor;noise robustness;recognition preprocessor;dereverberation method;use dereverberation;dereverberation method reduce", "pdf_keywords": ""}, "cec37cd54a940bec818db7216cc1086672f3fec0": {"ta_keywords": "lexical substitutions crowdsourcing;sense inventory alignment;alignment using lexical;using lexical;sense inventory;introduction sense inventory;lexical;substitutions crowdsourcing;using lexical substitutions;lexical substitutions;crowdsourcing;substitutions crowdsourcing objective;crowdsourcing objective aim;crowdsourcing objective;potentially duplicate synsets;inventory alignment using;duplicate synsets;duplicate synsets having;inventory alignment;words common broader;systematic inventory;approach systematic inventory;words common;synsets;common broader lntersectlon;exactly words common;synsets having exactly;systematic inventory methods;having exactly words;inventory", "pdf_keywords": ""}, "fa10752ab1768d1633001420b48be5e2518a4f80": {"ta_keywords": "analysis data data;methods data analysis;analysis data;data analysis;data analysis analysis;effective analysis data;analysis analysis method;data data;analysis method;analysis analysis;analysis method effective;method effective analysis;methods data;effective analysis;analysis;new methods data;data;report development;development new methods;methods;new methods;method effective;report development new;method;development;effective;development new;report;new", "pdf_keywords": ""}, "9fcfbc662d4095d72eb9a4e1c4f5ae8f0ffc4222": {"ta_keywords": "extracted teeth thermal;teeth thermal dehydration;lesions extracted teeth;extracted evaluated lesion;optical coherence tomography;teeth extracted evaluated;lesions assessed optical;extracted teeth;coherence tomography optical;secondary lesions extracted;extracted teeth extracted;coherence tomography;teeth extracted;tomography optical coherence;coherence tomography methods;lesions extracted;116 extracted teeth;teeth thermal;secondary lesions assessed;determine lesion activity;assessing lesion;lesions assessed;assessing lesion activity;dehydration measurement optical;evaluated lesion activity;tomography correlated dehydration;results secondary lesions;assessed optical coherence;lesion activity results;coherence tomography correlated", "pdf_keywords": ""}, "5801974fcebc11b4a8085fb02e77f792454caf7c": {"ta_keywords": "automated social skills;training developing dialogue;social skills training;social skills trainer;developing dialogue;provides social skills;social interaction acquire;dialogue named automated;introductionsocial skills training;social skills;skills training human;improve social skills;process social skills;social interaction;automated social;training human computer;human computer interaction;introductionsocial skills;acquire social skills;named automated social;developing dialogue named;interaction acquire social;automate process social;training human;user speech;users improve social;user speech language;skills trainer provides;discomfort social interaction;trainer provides social", "pdf_keywords": ""}, "20f166f7809d1af9065cd1c71ec1e38d5d92993f": {"ta_keywords": "intrinsic fear learned;fear learned reward;states intrinsicinsic fear;deep reinforcement learning;fear practical reinforcement;learned reward shaping;deep reinforcement;deep reinforcement learners;reinforcement learners periodically;problems deep reinforcement;policies periodic catastrophes;reinforcement learners;catastrophic states optimal;reinforcement learning guards;fear learned;accelerates deep reinforcement;intrinsic fear;introduce intrinsic fear;reinforcement learning;catastrophic states intrinsicinsic;intrinsicinsic fear practical;learned reward;intrinsicinsic fear;reinforcement learning problems;reinforcement;reward shaping;optimal policy;contain catastrophic states;states optimal policy;reward shaping accelerates", "pdf_keywords": ""}, "c43d9d868f5288738cd625d365f0b3a5c18d4a20": {"ta_keywords": "translation corpus descriptive;nationalist siultaneous translation;simultaneous interpretation corpus;translation corpus;interpretation corpus;siultaneous translation corpus;interpretation corpus collected;describes english japanese;english simultaneous interpretation;translation;japanese english simultaneous;corpus descriptive;siultaneous translation;corpus;professional simultaneous interpreters;corpus collected;corpus descriptive descriptive;japanese japanese english;japanese english;corpus collected nara;corpus set apart;english japanese;describes english;english japanese japanese;simultaneous interpreters;japanese;japanese japanese;introductionthe nationalist siultaneous;simultaneous interpretation;interpreters", "pdf_keywords": ""}, "3be5e7310b1bec9b4431ad0f1264f536b6a39f14": {"ta_keywords": "machine translation models;translation models;crowdsourced movie subtitles;level machine translation;pivot based translation;translation models used;machine translation;subtitles methods experiments;movie subtitles methods;movie subtitles;subtitles methods;subtitles;translation applied sparse;characterlevel models;datasets crowdsourced movie;untranslated words;crowdsourced movie;based translation;experiments characterlevel models;character alignment phrase;number untranslated words;character alignment;based translation applied;analysis character level;characterlevel models cut;characterlevel;noisy datasets crowdsourced;translation applied;alignment phrase table;character level machine", "pdf_keywords": "statistical machine translation;machine translation models;pivot based translation;machine translation;pivot languages translation;level machine translation;translation models robust;machine translation results;translation models;pivot translation methods;machine translation important;phrase based translations;machine translation smr;translation quality training;character accuracy translation;level translation models;translation models used;translation quality;character level translation;machine translation use;accuracy translation quality;translation related language;translation quality good;machine translation discuss;terms translation quality;translation model;translation methods;translation results study;translation important tool;translation quality use"}, "6e7e095f46deb297713dcde05991faf635768d29": {"ta_keywords": "adopted algorithmic fairness;algorithmic fairness;algorithmic fairness frameworks;algorithmic unfairness;fairness frameworks;fairness frameworks methodscurrent;algorithmic unfairness resultsin;attribute treating race;treating race attribute;race attribute structural;race attribute;conceptualization race fixed;aspects algorithmic unfairness;race racial categories;fairness;conceptualization race;race fixed attribute;racial categories;race racial;racial categories adopted;adopting conceptualization race;categories adopted algorithmic;treating race;examine way race;unfairness;way race racial;unfairness resultsin;algorithmic;race fixed;adopted algorithmic", "pdf_keywords": "adopted algorithmic fairness;algorithmic fairness research;categories algorithmic fairness;algorithmic fairness;race purposes fairness;racial categories algorithmic;algorithmic unfairness;algorithmic fairness frameworks;algorithmic fairness exceptions;fairness research largely;algorithmic unfairness build;problems algorithmic unfairness;fairness research;fairness frameworks;race racial categories;aspects algorithmic fairness;fairness analysis;algorithmic unfairness resultsin;conceptualizing operationalizing race;fairness research various;conceptualization race fixed;conceptualization race;fairness informed analysis;race implications evaluation;race attribute structural;field algorithmic fairness;attribute treating race;racial bias;racial categories process;argue racial bias"}, "4d10d7c02ce01d71f11c296b09b389c6f20b354b": {"ta_keywords": "data labeling crowdsourcing;labeling crowdsourcing;labeling crowdsourcing shared;labeling public crowdsourcing;public crowdsourcing marketplaces;crowdsourcing marketplaces present;public crowdsourcing;crowdsourcing marketplaces;crowdsourcing;crowdsourcing shared;data labeling public;efficient data labeling;crowdsourcing shared leading;data labeling;introduction data labeling;efficient label collection;labeling public;efficient label;labeling;label collection;label collection tasks;labeling process;real label collection;label;components efficient label;choose real label;label collection followed;settings labeling process;settings labeling;researchers engineers yandex", "pdf_keywords": ""}, "191169031c7646c02ecb1aaa9c8a6b6e05009730": {"ta_keywords": "reduced graphene oxide;oxide doped graphene;dielectric loss graphene;graphene oxide;graphene oxide doped;graphene oxide ni;constructed graphene oxide;doped graphene rgo;loss graphene film;graphene rgo ng;loss graphene;doped graphene;graphene rgo;reduced graphene;gf reduced graphene;graphene;graphene film;ng constructed graphene;graphene film gf;constructed graphene;introduction dielectric loss;dielectric loss important;dielectric loss;high dielectric loss;oxide ni polydopamine;synthesis hollow graph;oxide doped;introduction dielectric;achieve high dielectric;dielectric", "pdf_keywords": ""}, "19b6e7158ee4f13caa004a0b6c6a6e0ef965ea8f": {"ta_keywords": "introductionthe chime challenge;chime challenge series;chime challenge;overview chime series;robust automatic speech;speech recognition;automatic speech recognition;chime series;chime series including;automatic speech;overview chime;introductionthe chime;chime;speech recognition use;provides overview chime;robust automatic;signal processing statistical;recognition;recognition use everyday;development robust automatic;description datasets collected;recognition use;challenge series;description datasets;speech;statistical modelling series;datasets;datasets collected tasks;signal processing;robust", "pdf_keywords": ""}, "53f6c82035d43a19b9c8be0de651cae25bdd4bda": {"ta_keywords": "transcripts automatic speech;spoken word transcript;automatic transformation spoken;transcripts automatic;make transcripts automatic;automatic speech;clean transcriptstyle text;clean transcriptstyle;transcript style language;word transcript style;transcript style;word transcript;transformation spoken word;product clean transcriptstyle;transcriptstyle text;attempting make transcripts;automatic speech recognition;transcriptstyle;make transcripts;transcripts;disfluency deletion transformation;speech recognition results;speech recognition;transcriptstyle text case;insertion dropped words;transcript;deletion disfluencies substitutions;dropped words performed;recognition results disfluency;disfluency deletion", "pdf_keywords": ""}, "821532ecef5bc2252823b190c35f1e4c44ddc41c": {"ta_keywords": "word alignment parallel;alignment parallel corpora;learning parallel text;lexicons word alignment;learning translation lexicons;translation lexicons cross;learning translation;parallel corpora;translation lexicons;word alignment;including learning translation;methods learning translation;parallel text;translation lexicons word;parallel corpora wide;work word alignment;alignment parallel;lexicons cross lingual;unsupervised learning parallel;learning parallel;word alignment worked;translation outputs great;parallel text recently;transfer language processing;translation outputs;language processing tools;corpora;analysis translation outputs;lexicons cross;cross lingual", "pdf_keywords": "trainingneural machine translation;neural machine translation;translation models parallel;machine translation models;multilingual alignment;multilingual alignment algorithm;multilingual language alignment;translation models;machine translation effective;learning translation lexicons;based translation models;learning translation;word alignment tasks;machine translation nmr;results multilingual alignment;trained word aligners;machine translation association;words alignment;alignment algorithm multilingual;machine translation;language alignment;machine translation increasing;translation use neural;translation lexicons cross;translation effective tool;neural word aligners;machine translation important;aligned word representations;word alignment;words alignment algorithm"}, "2b4edb9515a26561ea3f9ee2a63a506721c8369e": {"ta_keywords": "sentiment analysis scientific;analysis scientific reviews;scientific reviews able;aspect based sentiment;sentiment analysis;scientific reviews;reviews comments paper;knowledge peer reviews;peer reviews comments;reviews able extract;based sentiment analysis;peer reviews;based sentiment;reviews comments;comments paper;sentiment;scientific papers;introduction scientific papers;analysis scientific;scientific papers complex;understanding usefulness papers;extract useful information;reviews able;comments paper provided;usefulness papers;aspect based;usefulness papers requires;scientific;use aspect based;introduction scientific", "pdf_keywords": "annotate review;annotate review relevant;dataset peer reviews;sentiments peer review;sentiment analysis scientific;sentiment features;aspect sentiments reviews;aspect sentiment features;review sentiments;aspect based sentiment;peer review text;reviews generate sentence;review text significantly;sentiment bearing sentences;sentiments reviews;performance aspect sentiment;aspect sentiment;analysis scientific reviews;sentiments expressed review;sentiment analysis;text review observe;unsupervised classification reviews;peer review texts;classification reviews;sentiments reviews articles;scores aspect sentiments;review text review;sentiments present review;review text;review sentiments binding"}, "f6d6c4dd0115386c234a0b027dd38f7aa9d9df2f": {"ta_keywords": "endangered languages documentation;nl endangered languages;endangered languages;communities documentary linguists;language communities documentary;introductionendangered languages;challenges language documentation;documentary linguists;documentary linguists map;introductionendangered languages meet;language documentation;languages documentation revitalization;challenges language;languages documentation;needs language communities;language documentation showing;language communities;showing needs language;needs language;languages;linguists map;language;linguists map specific;linguists;modern non coding;non nl tasks;languages meet;nl endangered;languages meet modern;technology nl tutorial", "pdf_keywords": ""}, "31c53acd2a43dcec4342d9c42d0ffbfbef36e855": {"ta_keywords": "estimating label marginal;label shift describes;approaches estimating label;label distribution change;estimating label;label shift;label distribution;label marginal;label marginal bce;setting label distribution;label dominant approaches;data given label;label;bce moment matching;given label;setting label;describes setting label;label dominant;moment matching approach;conditional probabilities data;moment matching;shift describes;likelihood estimation approach;maximum likelihood estimation;class conditional probabilities;likelihood estimation;shift describes setting;conditional probabilities;given label dominant;based confusion matrices", "pdf_keywords": "label shiftft estimation;estimating label shift;estimating label marginal;label shift estimator;estimating label;approaches estimating label;method estimating label;estimate target label;label shift assumption;label distributions;subsumes label shiftft;label distributions choice;estimation classification;target label distribution;domain label distributions;label distribution;label marginal;estimation black box;label shiftft;label shift dataset;black box predictors;conditions label shiftft;label_shift bctcalibrated classifiers;estimation classification applications;label distribution demonstrate;shiftft estimation;mll estimate;regression mll;label shift;predictors marginal calibration"}, "ccfaccf36b9cd7c0c05af2285ec90ecf5f51a34c": {"ta_keywords": "optimal placement relay;placement relay nodes;relay node placement;capacity relay node;introductionoptimal capacity relay;duplex radios relays;multi relay channel;capacity relay;relay nodes;relay channel;placement relay;relay channel study;radios relays;formulas multi relay;relay nodes straight;multi relay;radios relays decode;relay node;placement multi hop;multi hop network;utilize duplex radios;forward relaying;duplex radios;node placement multi;relays decode forward;relaying;relays decode;decode forward relaying;relay;hop network line", "pdf_keywords": ""}, "d8d12c922fc571d081bae27c67fcf50cdbb17d90": {"ta_keywords": "historical text summarisation;summarisation documents historical;text summarisation dataset;text summarisation documents;standard text summarisation;summarisation documents;summarisation dataset;text summarisation;summarised corresponding modern;language summarised corresponding;summarisation dataset consists;language summarised;historical text;summarised modern german;task historical text;summarisation;summarised modern;historians digital humanities;forms language summarised;documents historical;summarised corresponding;routine historians digital;historical forms language;ago summarised modern;historical german china;historians digital;german china news;documents historical forms;routine historians;important routine historians", "pdf_keywords": "historical text summarisation;summarising historical documents;summarisation corpus historical;summarisation documents historical;summarising historical;summarisation support historians;historical text text;decoder summariser modern;modern language summarisation;task summarising historical;language summarisation dataset;modern zih summaries;text summarisation dataset;useful summarisation modern;summarisation modern;modern historical text;simple historical text;historical modern text;lingual summarisation support;corpus historical zih;basic summariser modern;historical texts;historical text;summarisation corpus;summariser modern;high quality summarisation;lingual summarisation;ancient china literatures;constructed summarisation corpus;standard text summarisation"}, "c4607387ee863d5c5e5dc9f8adfbe7930508e286": {"ta_keywords": "machine learning icm;learning icm;icm;icm annual conference;learning icm annual;machine learning society;conference international machine;conference machine learning;international conference machine;icm annual;machinelearning;international machine learning;machine learning;www machinelearning org;machinelearning org;www machinelearning;conference machine;http www machinelearning;international machine;annual conference international;learning;international conference;conference international;machine;25th international conference;annual conference;year annual conference;conference;held august 2008;august 2008", "pdf_keywords": ""}, "7f4fa7c6f16f2965a104fa45071ea0c92b4366fe": {"ta_keywords": "lamina emergeent joint;lamina emergingent joint;joint lamina emergingent;structures joint lamina;joint lamina;tenon structures joint;mortise tenon structures;emergeent joint;edema lamina emergeent;origami mortise tenon;use origami mortise;lamina emergeent;emergeent joint common;structures joint;origami mortise;lamina emergingent;joint;emergingent joint;joint problem;tenon structures;edema lamina;use origami;joint common joint;joint common;common joint;mortise tenon;joint problem common;origami;problem common joint;common joint problem", "pdf_keywords": ""}, "7634b0cf93169d2a95d4d7193f47f97a61e3b4b2": {"ta_keywords": "behavior diagnostic games;designing behavior diagnostic;games elicit distinguishable;inferring latent psychological;behavior diagnostic;diagnostic games elicit;traits human behavior;diagnostic games;distinguishable behavior mutual;elicit distinguishable behavior;human behavior;designing behavior;distinguishable behavior;ability inferring latent;latent psychological traits;human behavior key;approaches infer traits;interacting machine learning;heuristics;latent psychological;task designing behavior;personalized human interacting;behavior mutual;psychological traits human;experiments games;behavior;infer traits;constructed experiments games;based heuristics;human interacting machine", "pdf_keywords": "behavior diagnostic games;behavior diagnostic game;games informative sense;diagnostic games elicit;games elicit distinguishable;games informative;behavioral diagnostic game;interactive games informative;diagnostic game design;diagnostic games;games purpose distinguishing;diagnostic game game;players learning games;games elicit;diagnostic game;designing interactive games;games fundamental;games learning;games learning environments;game design behavior;interactive games;systems learning games;learning games likely;learning games fundamental;games effective interaction;learning games;games purpose;maladaptive game design;effectiveness learning games;designing games purpose"}, "1ccf412212873ae1b020762b8b86291e1fb11f65": {"ta_keywords": "image classification crowdsourcing;classification crowdsourcing;crowdsourcing complex tasks;classification crowdsourcing standard;crowdsourcing standard tools;crowdsourcing complex;crowdsourcing;methods applicability crowdsourcing;applicability crowdsourcing complex;crowdsourcing standard;applicability crowdsourcing;aggregation methods;aggregation methods applicability;aggregation;advances research aggregation;machine learning systems;research aggregation methods;research aggregation;efficient data collection;data collection;transfer machine learning;machine learning;data collection thanks;classification;efficient data;recognition;tasks speech recognition;time efficient data;domain specific data;speech recognition remains", "pdf_keywords": "crowdsourcing speech recordings;crowdsourced audio transcriptions;datasets crowdsourced audio;dataset crowdsourced audio;crowdsourced audio;aggregation crowdsourced audio;crowdsourced audio annotations;crowdsourcing speech;crowdsourcing platform transcribed;collecting dataset crowdsourced;datasets crowdsourced;methods crowdsourced audio;dataset crowdsourced;aggregation methods crowdsourced;using crowdsourcing speech;aggregation crowdsourced;synthetic datasets crowdsourced;method crowdsourced audio;data collection crowdsourcing;methods aggregation crowdsourced;collection crowdsourcing large;crowdsourcing large scale;collection crowdsourcing;data automated speech;scale dataset crowdsourced;speech recognition collect;recognition using crowdsourcing;crowdsourcing large;crowdsourced;crowdsourcing"}, "3ba529f732d3c4a31e9ce57f1c78ddf911846bf4": {"ta_keywords": "bottleneck labeling training;labeling training;labeling training data;backgroundresearch weak supervision;noisy supervision sources;weak supervision approaches;potentially noisy supervision;weak supervision;noisy supervision;easing bottleneck labeling;bottleneck labeling;learning synthesizing labels;supervision sources;supervision sources proper;supervision approaches widespread;labeling;training data;labels;supervision approaches;synthesizing labels;backgroundresearch weak;training data machine;labels multiple potentially;supervision;remain challenge datasets;datasets;synthesizing labels multiple;challenge datasets used;challenge datasets;terms labels", "pdf_keywords": "benchmark weak supervision;weak supervision benchmark;data weak supervision;generate weak supervision;weak supervision methods;tool weak supervision;database weak supervision;weak supervision source;weak supervision sources;weak supervision models;platform weak supervision;weak supervision method;used weak supervision;supervision benchmark;bottleneck labeling training;supervision weak supervision;weak supervision demonstrate;weakly labeled;set weak supervision;method weak supervision;identify weak supervision;noisy supervision sources;weak supervision rules;developing weak supervision;weak supervision;applying weak supervision;weak supervision weak;weak supervision problem;provided weak supervision;supervision method weak"}, "64bc7fe1c46c4d4106afba4621ff1bd4376c077a": {"ta_keywords": "electrolaryngeal speech enhancement;approach electrolaryngeal speech;speech enhancement;electrolaryngeal speech;evaluation speech enhancement;speech enhancement promising;hybrid approach electrolaryngeal;approach electrolaryngeal;electrolaryngeal;evaluation speech;approach evaluation speech;enhancement;enhancement promising;enhancement promising approach;speech;evaluation;promising approach evaluation;approach evaluation;hybrid approach;use hybrid;use hybrid approach;promising approach;hybrid;use;approach;promising", "pdf_keywords": ""}, "04db62a14f78f693d6bd14a4803b9b73325b36bb": {"ta_keywords": "detecting fake news;fake news detection;types fake news;knowledge based fake;based fake news;news detection external;detecting fake;fake news mainly;fake news considerable;detection external knowledge;news content relations;news content;news detection;fake news;extracted news content;news mainly rely;entities extracted news;extracted news;based fake;textual content social;rely textual content;progress detecting fake;social context knowledge;knowledge based;news considerable effort;external knowledge;content social context;textual content;various types fake;news", "pdf_keywords": "detecting fake news;fake news detection;detect fake news;news detection subgraph;detection false news;news detection external;news detection;news content relations;extraction relations news;news detection provide;method fake news;relations extracted news;news detection algorithm;types fake news;news detection method;news detection applying;fake news existing;model fake news;news detection using;news content;news early detection;problem fake news;news items models;detection external knowledgewe;entities extracted news;relations news items;news detection focus;news items;extracted news content;knowledge textual content"}, "9bd6cdae71506eb307507e44df7abe0c285b3ca7": {"ta_keywords": "quality machine translation;attentional machine translation;statistical machine translation;machine translation noist;translation addition reranking;machine translation models;machine translation;translation models;asian translation based;machine translation addition;workshop asian translation;translation based;translation models methodsexperiments;translation based syntax;translation noist wat2015;translation addition;neural attentional machine;asian translation;introductionneuroreranking improves subjective;attentional machine;translation;translation noist;neural attentional;using neural attentional;stating neural;introductionneuroreranking improves;2015 workshop asian;improves subjective quality;improves subjective;subjective quality machine", "pdf_keywords": "neural machine translation;attentional machine translation;translation neural;neural mrna translation;mrna translation syntactically;machine translation increasing;translation quality neural;translation neural network;machine translation baseline;machine translation effective;mrna translation;machine translation models;statistical machine translation;translation addition reranking;machine translation;decoding translation neural;syntaxbased translations effectiveness;language translation models;translation models experiments;machine translation languages;translation models;translation syntactically distant;improvements degradations translation;improving accuracy translation;machine translation addition;based machine translation;neural reranking improved;language translation;translations effectiveness;correctnessneural machine translation"}, "e0a0b3438aef008fece5b8bbf76105b470f10f25": {"ta_keywords": "storage storage storage;automated storage data;automated storage;automated automated storage;storage storage;storage data;storage;number storage storage;storage data large;large number storage;number storage;new technologies automated;technologies automated automated;technologies automated;data;automated automated;automated;data large;new technologies;data large number;technologies;development new technologies;development;development new;large number;number;large;new", "pdf_keywords": ""}, "1817c9f0fd8a17e31c65963dd8cee9783059495b": {"ta_keywords": "cholecystectomy patient cholestatic;patient cholestatic;patient cholestatic disease;cholecystectomy patient;history cholecystectomy patient;cholestatic disease;cholecystectomy patient treated;cholecystectomy cholecystectomy patient;cholestatic;cholecystectomy;cholecystectomy history;history cholecystectomy;patient history cholecystectomy;cholecystectomy history cholecystectomy;cholecystectomy cholecystectomy;history cholecystectomy history;treated combination cholecystectomy;combination cholecystectomy;combination cholecystectomy cholecystectomy;present case patient;patient treated;case patient;disease;case patient history;patient history;patient treated combination;patient;article present case;article present;article", "pdf_keywords": ""}, "167adafac25ee108ca99c688cceded8bca710bb1": {"ta_keywords": "size constancy;size constancy represents;size judgment different;constancy function age;age level studies;report size constancy;size judgment;level studies constancy;studies constancy;judgment different distances;introduction size judgment;developmental study;age yielded inconsistent;function age level;outcome developmental study;distances function age;studies constancy function;function age;report size;outcome developmental;function age yielded;age level;measurement;developmental;developmental study addition;conclusions variables affect;constancy;affect outcome developmental;measurement techniques;age yielded", "pdf_keywords": ""}, "538466f2a69271617bf4f5b0df4e5fd854c11c35": {"ta_keywords": "test group items;group testing;group testing problem;items pooling groups;pooling groups items;items group defective;test group;group subsets items;groups items;pooling groups;group items;group defective;items group;group items positive;subsets items defective;population defective items;groups items result;result test group;group defective negative;positive items group;groups;items pooling;group;identify population defective;items result test;items defective items;judiciously group subsets;minimum number tests;group subsets;defective items reliably", "pdf_keywords": "group testing sparse;sparse graph codes;sparse graph coding;sparse graph codingthe;codingthe group testing;graph codingthe group;group testing feasible;adaptive group testing;testing sparse data;testing sparse;noisy group testing;group testing robust;graph coding theory;group testing scheme;group testing robustified;graph coding;group testing robustifying;recovery sparse graph;complexity decoding;group testing;sparse data recovery;modern sparse graph;group testing paradigm;problem group testing;group testing based;graph codingthe;memory complexity decoding;codingthe group;graph codes;sparse graph"}, "7e358ffc2731a82420d84a7f0bedb155a487c39d": {"ta_keywords": "hop question answering;question answering datasets;hop questions composition;hop questions process;questions composition;hop questions allows;multihop question composition;questions composition single;question answering;question composition;answering datasets propose;multi hop questions;dataset connected reasoning;hop questions;single hop questions;answering datasets;question composition single;hop questions methodsconstructing;challenging multi hop;composition single hop;multi hop;constructing multihop question;questions process;connected reasoning step;connected reasoning;multi hop question;hop;questions;hop question;single hop", "pdf_keywords": "multihop question answering;multihop reasoning datasets;comprehension qa datasets;hotmultihop question answering;multihop reading comprehension;multihop qa datasets;multihop questions pipeline;dataset multihop reasoning;challenging multihop questions;reasoning datasets hotmultihop;reasoning datasets hotpotqa;connected reasoning multihop;potential reasoning shortcuts;multihop reasoning useful;demonstrate multihop reasoning;multihop questions composed;reasoning shortcuts;question answering sequence;multihop reasoning remains;reasoning datasets;construct multihop qa;challenging multihop dataset;question answering;challenging multihop reading;cheatable multihop datasets;shortcut based reasoning;multihop questions sequence;development multihop reasoning;dataset challenging multihop;reasoning useful tool"}, "61cce75554a6d1bb802f26758c3b0ba97de6918d": {"ta_keywords": "graph attention networks;graph attention;graph networks deep;enable graph attention;graph networks;attention networks gat;introduction graph networks;classification tasks gns;node classification tasks;networks deep learning;attention networks;node classification;networks deep;tasks gns;graphs work focus;networks gat;networks;nodes;networks gat commonly;graphs;nodes having similar;node;gns;limitation enable graph;performance node classification;tasks gns assume;homophilic graphs work;homophilic graphs;nodes having;graph", "pdf_keywords": "graph attention network;graph attention networks;graph attention;generalized graph attention;enhance graph attention;plugged graph attention;enable graph attention;attention network;attention networks positional;learning graph neural;called graph attention;graph networks deep;graph neural;networks positional embeddings;graph neural networks;graph neural network;attention networks;attention convolutional layers;attention networks gat;proposed attentional layer;attentional layer;attentional layer proposed;attention network gats;graph networks;learning graph;studies graph neural;attention networks gats;graph nodes gns;learning learning graph;task positional embedding"}, "b81acc013c42796a5eea0fc20cfb04846da3a589": {"ta_keywords": "introductionlinguistic documentation inherently;introductionlinguistic documentation;documentary linguists work;documentary linguists;portion documentary linguists;language documentation process;ease language documentation;linguists work advances;using linguists;language documentation;advances natural language;linguists work;work using linguists;introductionlinguistic;transcription glossing;linguists;language processing;natural language processing;natural language;using linguists past;documentation process;transcription glossing consume;documentation inherently time;attempt ease language;documentation inherently;process transcription glossing;language processing help;linguists past;ease language;linguists past decisions", "pdf_keywords": "linguistic annotation;developed linguistic annotation;linguistic annotation integrating;language documentation conservation;integrating popular annotation;documentary linguists extended;popular annotation frameworks;documentation developed linguistic;documentary linguists;annotation frameworks developed;annotation frameworks;multilingual neural networks;annotation;popular annotation;natural language processing;natural language;annotation frameworks development;language documentation;language documentation developed;language processing tools;quality documentary linguists;development natural language;ease language documentation;languages based recent;language documentation process;language processing;use natural language;advances massively multilingual;documentation conservation;linguists extended"}, "398a0625e8707a0b41ac58eaec51e8feb87dd7cb": {"ta_keywords": "introductionalighter;introductionalighter fundamental;interactive learning;introductionalighter fundamental process;interactive learning given;development interactive learning;abstract terms imagining;terms imagining action;interactive;imagining action sequences;learning;terms imagining;development interactive;action sequences scoring;imagining action;prototypicality moving;purely abstract;prototypicality;success prototypicality;action sequences;abstract;success prototypicality moving;humans reason;abstract plans;learning given simple;abstract plans fit;development;action;reason purely abstract;process development interactive", "pdf_keywords": "embodied reasoning agent;agent trained textworld;language embodied reasoning;embodied tasks interactive;text embodied environments;interactive textworld training;interactive learning humans;textworld language embodied;embodied environments agent;embodied reasoning;language based agents;reasoning agent learns;interface interactive learning;tasks interactive textworld;generalization embodied environments;demonstrations interactive textworld;embodied environment text;text based agent;interactive textworld;agent learns;training text actions;agents explore learn;agents text environment;improve generalization embodied;text embodied;agent able learn;environments agent trained;agents navigate textual;autonomous agent trained;embodied interactions implement"}, "4f7bbcef3d40cafad17936fdf562a121667af1e8": {"ta_keywords": "divergence prior vessel;vessel tree reconstruction;reconstructing vector fields;prior vessel tree;pattern divergent arteries;divergent arteries;divergence prior;introduction divergence prior;principle reconstructing vector;arteries convergent veins;regularization principle reconstructing;fields modelling blood;divergent arteries convergent;reconstructing vector;convergent veins previously;prior vessel;vector fields modelling;vector fields based;modelling blood flow;blood flow pattern;fields based prior;vector fields;arteries convergent;introduction divergence;divergence important example;flow pattern divergent;prior knowledge divergence;geometric regularization principle;reconstruction methods;reconstruction methods propose", "pdf_keywords": "vessel tree reconstruction;vessel tree reconstructions;divergence vessel tree;divergence vessel pathlines;approximation vessel tree;propose vesseltree extraction;divergence disambiguate vessel;vesseltree extraction;oriented vessel tree;divergence vessel;vector field reconstruction;analysis divergence vessel;disambiguate vessel directions;vessel tree centerline;vessel pathlines;tangent approximation vessel;use divergent vessel;vessel directions disambiguating;curvature regularization flow;divergent vessel;vesseltree extraction large;approximation vessel;estimate vessel directions;reconstructing vector fields;vessel tree;divergent vessel prior;vessel pathlines apply;positive divergence vessel;field reconstruction;large vessel tree"}, "0431f60546381a9e91fb156236c3c7056f57081f": {"ta_keywords": "singing voice synthesis;generate singing better;learning based singing;generate singing;voice synthesis;flexibly generate singing;voice synthesis vs;singing better qualities;singing quality;singing voice;singing quality limited;based singing voice;singing better;based singing;reasonable singing quality;data augmentation methods;singing;augmentation methods boost;reasonable singing;reach reasonable singing;data augmentation;different data augmentation;augmentation methods;voice;methods boost training;training data methodsin;backgrounddeep learning;augmentation;methods boost;training data", "pdf_keywords": "singing voice synthesis;synthesis singing voice;voice synthesis;voice synthesis based;synthesis singing;singing voice systems;singing augmentation methods;singing voice synthesizing;synthetic quality singing;voice synthesis use;voice synthesis introduce;based dataaugmented singing;quality singing sequence;quality speech synthesis;speech synthesis;singing augmentation;generate singing better;voice synthesis discuss;evaluation voice synthesis;synthesis speech neural;voice synthesizing;synthesis speech;acoustic models singing;dataaugmented singing voice;learning based singing;generate singing;voice synthesis vs;singing sequence use;approach synthesis speech;efficient singing voice"}, "dbe87b171bfb789e1d22a047aeeee69105e6fd02": {"ta_keywords": "t5 sentence embeddings;sentence embeddings methods;sentence embeddings utilize;text transformers t5;sentence embeddings;extracting t5 sentence;embeddings utilize t5;t5 encoder decoder;text text transformers;sentence embeddings objective;text transformers;transformers t5 sentence;t5 encoder;utilize t5 encoder;uses t5 encoder;t5 encoder uses;t5 sentence;encoder uses t5;embeddings utilize;embeddings;encoder decoder;embeddings objective provide;encoder decoder model;embeddings methods investigate;encoder;decoder model encoder;decoder;extracting t5;methods extracting t5;embeddings objective", "pdf_keywords": "sentence embeddings efficiently;sentence transfer semantic;sentence representation transfer;extract sentence representations;sentence embeddings significantly;sentence embeddings text;sentence embeddings;sentence representations text;sentence representations;processing sentence embeddings;text embeddings;accuracy sentence embeddings;sentence embeddings used;text text embeddings;transfer semantic semantics;universal sentence representations;t5 sentence embeddings;text embeddings recommend;sentence representations impact;resulting sentence embeddings;sentence embeddings fully;model sentence transfer;embeddings efficiently extracted;sentence representations pre;embeddings significantly improved;processing sentence representations;embeddings used retrieve;exploration sentence embeddings;transfer semantic;sentence encoders"}, "95788ed8affd06c0c2c6159c26ff7c123c4f2e0a": {"ta_keywords": "speaker diarization;introduction speaker diarization;speaker diarization essential;speaker wise conditional;processing multi speaker;speaker wise;multi speaker;multi speaker audio;speaker issue novel;speaker audio;neural diarization eend;diarization eend;speaker audio end;speaker;neural diarization;novel speaker wise;end neural diarization;speaker issue;issue novel speaker;diarization eend method;number speaker;novel speaker;speakers paper solve;number speaker issue;fixed number speaker;number speakers;diarization;audio end;speakers paper;speakers", "pdf_keywords": "speakerwise decoding conditional;speaker diarization;introduction speaker diarization;generate speech activity;model generate speech;speaker diarization essential;speaker wise conditional;speakerwise decoding;speaker verification speech;estimated speech activities;generate speech;generate speaker variable;speaker scenarios estimating;talker speech systems;greedy loss speaker;previously estimated speech;loss speaker wise;propose speakerwise decoding;speech systems;speech activity neural;estimated speech;variable speaker scenarios;speaker wise greedy;speech recognition;variable speaker;speech activity;computation strategies speaker;speaker variable;generate speaker;speech activity variable"}, "84908a28a03d0d7c467d9556ed36f0e416de7171": {"ta_keywords": "semantic parsing algorithm;semantic parsing;meaning recognition written;algorithm semantic parsing;introductionhypbrid semantic;introductionhypbrid semantic analysis;recognition written utterances;meaning recognition utilizes;parsing algorithm;written utterances goal;original meaning recognition;utterances goal computer;parsing;approaches meaning recognition;meaning recognition;description given utterance;semantic analysis article;meaning recognition described;semantic analysis;written utterances;utterances goal;utterances;utterance original meaning;novel algorithm semantic;utterance;algorithm semantic;semantic;given utterance original;given utterance;utterance original", "pdf_keywords": ""}, "b46be3ac246499655cc442e93c5878e7a9640ae3": {"ta_keywords": "sagas understanding news;episodes sagas understanding;related story sequences;introductionfrom episodes sagas;timeline concrete representation;representation saga longrunning;story sequences;story sequences complications;episodes sagas;representation saga;sagas understanding;define timeline;saga longrunning series;timeline;saga longrunning;timeline concrete;series related events;introductionfrom episodes;saga;explicit representation saga;previous events;related events;recent information events;news identifying temporaryrally;sagas;information events;context previous events;information events better;events;temporaryrally related story", "pdf_keywords": ""}, "8da992b611df508b1803f66ffa53bd1fb741a76c": {"ta_keywords": "question answer hierarchies;messagegenerating question answer;text generation task;challenging text generation;text generation;answer hierarchies;key clinical messagegenerating;answer hierarchies important;clinical messagegenerating question;clinical messagegenerating;generation task;high level questions;challenging text;question answer pairs;input document hierarchy;document hierarchy;text;level questions;hierarchies;generation task converts;messagegenerating question;knowledge acquisition paper;document hierarchy question;knowledge acquisition;questions;specific questions;novel challenging text;messagegenerating;related specific questions;hierarchies important", "pdf_keywords": "generating questions text;question generation capable;improved question generation;question generation;structure generate questions;generate questions according;controlled question generation;generate questions;able generate questions;automated information generation;generating questions;generate questions correct;understanding question generation;ability generate questions;question answering development;generating questions human;classify questions datasets;question generation question;classifying question specificity;classify questions;question generation quality;generation question answering;generate questions single;method question generation;text generation task;approach classify questions;questions generated method;questions generated;information generation;question answering"}, "2eea63f896deed47cc0c0000e1482ec5c860fd0b": {"ta_keywords": "detecting controversial posts;controversy detection;controversy detection task;controversial posts web;detecting controversial;views controversy detection;comment sentiment graph;controversial posts;sentiment graph;sentiment graph pc;post comment sentiment;comment sentiment;polarized views controversy;background detecting controversial;news alleviating polarized;sentiment;topic post comment;measuring influence news;posts web social;polarized views;alleviating polarized views;views controversy;controversy;web social media;judging authenticity web;construct topic post;posts web;science social humanities;social humanities;social media", "pdf_keywords": ""}, "881ce19455a9923e4798e9d77d2d8623ca9d2e03": {"ta_keywords": "clustering speech recognition;estimationion clustering speech;clustering speech;asbpc speech recognition;speech recognition;introduction speech recognition;speech recognition methods;bayesian predictive classification;variationational bayesian estimationion;speech recognition based;distribution bayesian predictive;named variationational bayesian;asbpc speech;bayesian estimationion clustering;variationational bayesian;predictive distribution bayesian;bayesian predictive distribution;referred asbpc speech;based bayesian predictive;total bayesian framework;proposed total bayesian;bayesian predictive;distribution bayesian;bayesian framework;bayesian framework named;bayesian estimationion;bayesian framework aims;method based bayesian;recognition based student;based bayesian", "pdf_keywords": ""}, "6d9603be7e79ff33677327a0edd5bd3f7da6347b": {"ta_keywords": "pulmonary artery disease;diagnosis pulmonary artery;patients diagnosis pulmonary;pulmonary artery;diagnosis pulmonary;artery disease;pulmonary;patients diagnosis;artery;management patients diagnosis;diagnosis;management patients;patients;approach management patients;disease;article;article discuss importance;new approach management;management;new approach;importance new approach;approach management;article discuss;discuss importance new;new;approach;aim article discuss;discuss importance;aim article;importance new", "pdf_keywords": ""}, "81af4e14050c410e2afee226be583088a9791ddf": {"ta_keywords": "semantic role labeling;unsupervised semantic role;role labeling unsupervised;learn argument embeddings;role labeling;role labeling identifying;labeling identifying role;argument embeddings context;representations unsupervised semantic;argument embeddings;semantic role;identifying role argument;argument embeddings according;labeling unsupervised semantic;bias argument embeddings;role argument usually;introductionmultiplicative representations unsupervised;unsupervised semantic;incorporating dependency relations;role argument;embeddings according dependency;introductionmultiplicative representations;embeddings context explicitly;dependency relations;embeddings context;learn argument;dependency relations multiplicative;dependency relation predicate;dependency roles methodsour;labeling unsupervised", "pdf_keywords": ""}, "ec99cf93ef22a0c0d669abe90c9509f642b2cf69": {"ta_keywords": "downsampling near semantic;segmentmentation learning downsampling;learning downsampling near;content adaptive downsampling;downsampling technique learns;learning downsampling;segmentmentation learning;accuracy semantic boundaries;near semantic boundaries;semantic boundaries target;semantic boundaries;segmentmentation;adaptive downsampling;introductionefefficient segmentmentation learning;introductionefefficient segmentmentation;downsampling near;semantic boundaries address;adaptive downsampling technique;downsampling;semantic boundaries aimto;reduced accuracy semantic;downsampling technique;near semantic;downsample;sampling;sampling locations;favor sampling locations;boundaries target classes;downsample input frame;locations near semantic", "pdf_keywords": "downsampling segmentation architectures;adaptive downsampling semantic;reproduce downsampling segmentation;downsampling semantic semantic;downsampling semantic;semantic segmentation high;downsampling segmentation;cnn reproduce downsampling;cost semantic segmentation;semantic segmentation present;semantic segmentation challenge;segmentation consistent improvements;useful semantic segmentation;improves segmentation performance;semantic segmentation data;semantic segmentation;improves segmentation;content aware downsampling;content adaptive downsampling;semantic segmentation systems;semantic segmentation consistent;network improves segmentation;segmentation application semantic;semantic image segmentation;automated downsampling objects;segmentation architectures benefit;segmentation architectures;improves segmentation quality;significantly improves segmentation;downsampling technique learns"}, "48e8e8085907192d501eb2bcc582035e90431a2f": {"ta_keywords": "sequence security tagging;sequence tagging;sequence tagging given;lingual sequence security;tagging given sequence;network sequence tagging;sequence words model;deep gated recurrent;cross lingual sequence;deep hierarchical recurrent;lingual sequence;recurrent neural network;security tagging;encode morphology context;hierarchical recurrent neural;sequence words;word levels encode;predict tags;hierarchical recurrent;encode morphology;recurrent neural;words model;neural network sequence;given sequence words;layer predict tags;words model employs;task cross lingual;gated recurrent;levels encode morphology;deep hierarchical", "pdf_keywords": "neural sequence tagging;sequence tagging models;predicting sequence tags;tagging based deep;generalr sequence tagging;sequence tagging based;model sequence tagging;sequence words model;sequence tagging;deep hierarchical recurrent;information sequence tagging;sequence tagging fundamental;sequence tagging employ;deep gated recurrent;sequence tagging given;sequence tagging algorithm;hierarchical gated recurrent;recurrent unit network;recurrent neural network;natural language processing;tagging given sequence;hierarchical recurrent neural;sequence tags sequence;sequence tags;hierarchical recurrent;sequence words;word level deep;named entity recognition;entity recognition;structured prediction"}, "5431098723db5858c4553f0259921cbbdd6492d5": {"ta_keywords": "translation initiative covid;translation initiative;initiative covid 19;covid 19;languages team;pivot languages team;languages team targeting;covid 19 tico;article translation initiative;languages africa southasia;languages africa;resourced pivot languages;pivot languages;lesser resourced languages;particular languages africa;initiative covid;virus res;resourced languages;covid;resourced languages particular;19 development tools;languages;particular languages;spread virus res;translation;languages particular;aim article translation;virus;languages particular languages;article translation", "pdf_keywords": "translation initiative covid;health initiative translation;translation coronaviruses country;translation initiative;translation coronaviruses;causedthe translation initiative;tool translation coronaviruses;translation content related;provide translations;outbreak disease world;disease causedthe translation;initiative covid 19;language report translation;report translation;outbreak world disease;translation technologies;initiative translation;translation technologies decade;translating data world;translating data;language translation;translation content;translate data;future translation technologies;translation development;initiative translation english;translations report;data translate;provide translations english;translation tool translation"}, "a72e732f2d11075aa0103b72b4f9884ddcaaaa85": {"ta_keywords": "learnability inductive logic;inductive logic programming;introductionpolynomial learnability inductive;learnability inductive;introductionpolynomial learnability;practice inductive logic;learnability results exist;inductive logic;learnability results;learnability;negative learnability results;logic programming;logic programs;negative learnability;logic programming described;logic programming paper;classes logic programs;logic programs closely;positive negative learnability;computational learning theory;practice inductive;computational learning;learning theory;used practice inductive;inductive;classes logic;learning;restricted classes logic;background computational learning;programming described positive", "pdf_keywords": ""}, "015dc5b71894dd4d05e7668d015e545ab2e162ba": {"ta_keywords": "text speech e2e;speech processing toolkit;speech e2e ts;text speech;source speech processing;end text speech;speech e2e;open source speech;espnet toolkit;toolkit espnet toolkit;toolkit named espnet;espnet toolkit supports;toolkit espnet;processing toolkit espnet;speech processing;espnet ts extension;e2e ts toolkit;source speech;espnet ts;named espnet ts;ts toolkit;espnet;named espnet;e2e ts models;end text;speech;e2e ts;end end text;text;state art e2e", "pdf_keywords": "speech processing toolkit;text speech generate;source speech processing;end processing speech;corpus speech synthesis;text speech e2e;network speech synthesis;speech generate;generate speech fundamental;end text speech;processing speech evaluated;text speech;acoustic feature conversion;speaker adaptation;processing speech;generate speech;espnet ts toolkit;speech synthesis;text speech based;neural network speech;speech processing;ability text speech;speech synthesis models;modeling speech;processing speech implement;open source speech;speech e2e ts;e2e text processing;modeling speech fundamental;development text speech"}, "3122a2d7799ba585b993e432b3deb47659b3f3c1": {"ta_keywords": "length answer models;answer models;question answering;form question answering;question answering lq;answer models recently;task long form;paragraph length answer;answering;task long;answering lq;answering lq involves;paragraph length;generate paragraph length;generate paragraph;paper task formulation;task formulation;using generate paragraph;lq paper task;task formulation raises;long form;paragraph;new relies sparse;long form question;relies sparse;introductionthe task long;task;evaluation dataset creation;paper task;sparse", "pdf_keywords": "question answering dataset;answer generation;question answering;question answering lq;benchmarks natural language;answering dataset developed;answer generation articles;prediction retrieved paragraphs;predict correct retrieval;retrieval resulting models;predicted retrievals novel;using random retrieval;random retrieval;generate v3 retrieval;retrieval model demonstrate;text generation sufficient;retrieval augmented generation;answering dataset;prediction correct retrieval;retrieved paragraphs useful;form question answering;quality retrievals hope;attention contrastive retriever;quality retrievals;retrieved paragraphs similar;predicted retrievals;similar predicted retrievals;lq demonstrate retrieval;lq quality retrievals;retrieval model"}, "15d643f4c27d373aa46f26a760051e76fde81dc2": {"ta_keywords": "annotated question entities;question answering knowledge;answering knowledge graphs;models question answering;answering knowledge;question entities;question answering;question entities supplied;task entity resolution;knowledge graphs kgq;entity resolution;knowledge graphs;entity resolution outside;trivial task entity;e2e trained models;entities;entity;using weakly supervised;weakly supervised;entities supplied model;task entity;weakly supervised dataset;answering;annotated question;end e2e trained;e2e learning;entities supplied;models trained;scope e2e learning;trained models question", "pdf_keywords": "answering knowledge graphs;question answering knowledge;answer entities train;answer entities hops;text answer entities;model question answering;answer entities;knowledge graphs gq;answering knowledge;knowledge graph supporting;knowledge graph;knowledge graphs;combine answer entities;differentiable knowledge graphs;entity resolution inference;differentiable knowledge graph;question answering;question answering results;knowledge graphs use;simple question answering;additional entity resolution;entity resolution;approach entity resolution;entity resolution questions;uncertainty entity resolution;entity resolution demonstrate;data entity resolution;perform entity resolution;friendly query use;entities train"}, "5f9d8fe21efb3c2b241427869a333472ab09a22d": {"ta_keywords": "diagnosis neoplastic disease;patient diagnosis neoplastic;neoplastic disease;diagnosis neoplastic;neoplastic;diagnosis;diagnosis treatment patient;patient diagnosis;diagnosis treatment;etiology disease understood;treatment patient diagnosis;etiology disease;disease understood report;disease;new approach diagnosis;approach diagnosis treatment;disease understood;approach diagnosis;treatment patient;etiology;treatment;patient;report development new;understood report development;report development;understood report;report;new;development new;approach", "pdf_keywords": ""}, "ea6547e877c1cc3d37229a6f488ac04e9a11de18": {"ta_keywords": "protein interfaces performed;protein interfaces;protein protein interfaces;water positions protein;protein capri target;predicted interactions capri;docking predictions complex;interfacial water molecules;submitting docking predictions;positions interfacial water;predictions water positions;docking predictions;immunity protein capri;protein capri;predictions water;domain colicine2 im2;assessment predicted interactions;blind predictions water;positions protein protein;predicted interactions;interactions capri;colicine2 im2 immunity;colicine2 im2;im2 immunity protein;dascribed domain colicine2;interfacial water;positions protein;immunity protein;domain colicine2;water positions", "pdf_keywords": ""}, "fd1b59d22eb1fb32e0360d4fcbe58dc4ebb25af9": {"ta_keywords": "video image deepfakes;image deepfakes survey;image deepfakes;deepfake content;background deepfake content;background deepfake;deepfakes survey evaluates;deepfake content material;deepfakes survey;deepfake;different deepfake;deepfakes;image text synthesis;methods different deepfake;deepfake categories;deepfake categories mainly;audio video image;video image text;different deepfake categories;generation detection methods;audio video;generation detection;video image;image text;synthetically generated;real include audio;just video image;synthetically generated manipulated;text synthesis;detection", "pdf_keywords": "audio deepfakesdeepfake;developments audio deepfake;audio deepfake tools;audio deepfake;audio deepfake methods;audio deepfakesdeepfake powerful;audio deepfakes deepfake;audio deepfakes;methods audio deepfakes;audio deepfake technologies;survey audio deepfake;audio deepfake provide;various audio deepfake;guide audio deepfake;video image deepfakes;focuses audio deepfakesdeepfake;audio spoofing detection;deepfake content;detecting analyzing audio;deepfakesdeepfake powerful tool;deepfake tools;audio video text;deepfake methods trends;detectionthe occurrence deepfakes;audio spoofing;deepfake tools summary;video text synthesis;image deepfakes;audio use neural;focusing audio deepfake"}, "86b91922923b03c66497accfa88c638299fc8d26": {"ta_keywords": "backgroundmorphological inflection generation;space variational decoder;variational decoder;space variational encoder;variational encoder decoder;variational encoder;backgroundmorphological inflection;variational decoder decoders;inflection generation;inflection generation multi;multi space variational;space variational;variational;inflection;encoder decoder;encoder decoder msvd;decoder;decoders;encoder;backgroundmorphological;decoder decoders;decoder msvd method;decoders described;decoder decoders described;decoder msvd;decoders described paper;sigorhon 2017 based;method ofzhou neubig;generation multi space;based multi space", "pdf_keywords": ""}, "f35a01c1e5d5375453c39e6161526633492fb574": {"ta_keywords": "separable md codes;data storage methods;powerful erasure codes;erasure codes provide;erasure codes;data storage;data replication;storage methods;data centers;data replication use;data centers increasingly;maximum storage efficiency;reed solomon codes;simple data replication;md codes;md codes reed;storage methods use;evolving data storage;solomon codes provide;codes reed solomon;storage efficiency;replication use powerful;solomon codes;provide maximum storage;codes provide maximum;replication;storage efficiency use;economically data centers;replication significantly;storage", "pdf_keywords": "complexity data storage;data distributed storage;coding data centers;distributed storage;distributed storage systems;powerful erasure codes;data storage methods;data storage;recovery distributed storage;storage systems;erasure codes provide;storage methods;erasure codes;data recovery distributed;data storage poorly;storage systems observed;storage systems present;efficient scheduling;storage efficiency;maximum storage efficiency;performance mds queue;exact md queue;schedulingthe mds queue;md queue centralized;storage methods use;data distributed;characterthe md queue;codesthe coding theory;simple efficient scheduling;insightful scheduling policies"}, "9b71542ef5d5178041048b9a330309053bb0bcfc": {"ta_keywords": "speech separation input;domain speech separation;speech separation;performance speech separation;speech mixture masking;speech separation usually;speech mixture;separation input mixtures;domain speech;truth speech mixture;improved performance speech;separation input;mixture masking based;ground truth speech;time domain speech;mixture masking;deep learning based;deep learning;predict ground truth;separation usually build;separation;background deep learning;performance speech;input mixtures like;separation usually;input mixtures;masking based;mixtures like cocktail;speech;models predict ground", "pdf_keywords": "separation speech synthesis;enhancement speech separation;separation speech enhancement;speech separation enhancement;speech trained separation;domain speech separation;speech enhancement separation;recognition speech separation;based speech separation;speech separation input;speech separation predicted;approach speech separation;speech mixture masking;synthesize separated speech;performance speech separation;framework speech separation;speech separation speech;synthesis speech mixture;speech separation;speech separation work;speech separation fundamental;separation target speech;enhancement speech synthesis;strategy speech separation;speech mixture;speech enhancement speech;method speech enhancement;speech mixture speakers;generate speech;separation use vocoder"}, "79ab3a0d6dc5d6fd3b466ea2814fdbb93a3672d0": {"ta_keywords": "etiology disease;etiology disease understood;case patient etiology;patient etiology disease;patient etiology;disease understood case;etiology;disease;disease understood;case patient;understood case patient;patient;understood case;case;understood", "pdf_keywords": ""}, "da10c4bc1de7b9b7ddbb21d70ff5092a15cb866f": {"ta_keywords": "domain adaptation;problem domain adaptation;adaptation protein extraction;domain adaptation protein;subproblem domain adaptation;supervised transductive approache;supervised transductive;transfer learning;problem transfer learning;protein extraction;transfer learning particular;support vector machines;protein extraction define;machines maximum entropy;art supervised transductive;state art supervised;vector machines maximum;domain adaptation current;adaptation protein;maximum entropy models;supervised;vector machines;learning particular subproblem;maximum entropy based;novel maximum entropy;entropy models based;entropy models;maximum entropy;problem domain;particular subproblem domain", "pdf_keywords": ""}, "9db77e925015ad02efc9beeab233bedbfe04e4b7": {"ta_keywords": "challenging reinforcement learning;strategy solving dynamical;evolution strategy;reinforcement learning;reinforcement learning rr;reinforcement learning lr;evolution strategy es;problem evolution strategy;control problem evolution;objective reinforcement learning;solving dynamical control;optimal strategy solving;challenging reinforcement;dynamical control;objective reinforcement;introduction objective reinforcement;strategy solving;dynamical control problem;optimal strategy;reinforcement;solving dynamical;lr optimal strategy;learning lr optimal;promise challenging reinforcement;caro based gradient;learning rr tasks;problem evolution;dynamical;underlying dynamical;strategy", "pdf_keywords": "learning evolution strategy;reinforcement learning evolution;evolution strategies neural;evolution strategy methods;reinforcement learning fast;reinforcement learning large;challenging reinforcement learning;evolution strategy method;method reinforcement learning;smoothing evolution strategy;reinforcement learning introduce;evolution strategy strategy;policy optimization;reinforcement learning challenging;reinforcement learning discuss;approach reinforcement learning;methods reinforcement learning;strategy machine learning;generation reinforcement learning;tool reinforcement learning;reinforcement learning powerful;learning evolution;reinforcement learning easy;evolution strategy es;evolution strategies;solution reinforcement learning;evolutionary optimization neural;optimization strategies neural;region policy optimization;reinforcement learning"}, "af0aa62d243c761b56a83369bc9b1f75805003cf": {"ta_keywords": "structured text networks;walk structured text;text networks;corpus labeled directed;text corpus labeled;representing text corpus;text networks challenging;text corpus;walks derive similarity;learning walk structured;labeled directed graph;corpus labeled;graph based similarity;corpus;words use supervised;structured text;nodes represent words;words weighted edges;dependency parsing;introduction learning walk;parsing given graph;derived dependency parsing;learning walk;similarity measure words;directed graph nodes;based random walks;labeled directed;based similarity measure;directed graph;dependency parsing given", "pdf_keywords": ""}, "f765b23f0b0d2a196bc0fe562ad24278d0c9cee4": {"ta_keywords": "backgroundadaptive gradient methods;backgroundadaptive gradient;gradient methods stochastic;learning rate tuned;learning rate locally;training deep learning;training deep;deep learning models;deep learning;stochastic optimization;methods stochastic optimization;learning rate parameter;global learning;method training deep;locally global learning;gradient methods;optimization adjust learning;global learning rate;stochastic optimization adjust;adapts learning rate;learning rate;adjust learning rate;learning models;algorithm adapts learning;gradient;rate locally parameter;backgroundadaptive;rate parameter locally;adam popular method;adapts learning", "pdf_keywords": ""}, "ccbc17d42f2b260079eee702fd97a75de705d8ac": {"ta_keywords": "syntactic structures decoupling;search synthesis decomposition;generates words predetermined;words predetermined syntactic;search synthesis;predetermined syntactic structures;synthesis decomposition search;generates words;decomposition search synthesis;predetermined syntactic;successfully generates words;task phases synthesis;syntactic structures;syntactic;synthesis decomposition;synthesis vector constructed;search process synthesis;synthesis vector;structures decoupling task;monolingual setting;synthesis;text vector decomposed;words predetermined;monolingual;process synthesis vector;process synthesis;work kind monolingual;monolingual setting successfully;kind monolingual setting;phases synthesis decomposition", "pdf_keywords": ""}, "75b13e7131997ff6fd21325d68a2222d2c1b7157": {"ta_keywords": "beamforming speech separationion;speech separationion enhancement;speech separation enhancement;spectral separation beamforming;separation beamforming based;neural beamforming speech;separation beamforming;beamforming speech;approach speech separation;sequential neural beamforming;neural beamforming;speech separation;speech separationion;neural networks separation;separationion enhancement promising;based spectral separation;separationion enhancement;neural beamforming alternates;beamforming based;separation enhancement;beamforming alternates neural;separation enhancement work;beamforming;spectral separation;beamforming based spatial;function beamforming;separation using advanced;loss function beamforming;based spatial separation;spatial separation methods", "pdf_keywords": ""}, "00b874f8346cedadc2a6366c4b72e60140f99556": {"ta_keywords": "textual similarity measure;textual similarity;networks textual similarity;neural networks textual;networks textual;textual;similarity;similarity measure;2016 task attention;task attention based;attention based multi;task attention;perspective convolutional neural;ttic uw semeval;multi perspective convolutional;attention based;perspective convolutional;attention;ttic uw;introductionud ttic uw;convolutional neural networks;convolutional neural;ttic;uw semeval 2016;multi perspective;semeval 2016 task;introductionud ttic;2016 task;task;perspective", "pdf_keywords": ""}, "43c844c30765f3fa25bfabd83490ef826b9ceca1": {"ta_keywords": "adversarial misspellings robustust;adversarial misspellings;combating adversarial misspellings;adversarial spelling;adversarial spelling mistakes;combat adversarial spelling;misspellings robustust;misspellings robustust word;robustust word recognition;word recognition;placing word recognition;word recognition models;word recognition model;word recognition combat;conclusions word recognition;robustust word;adversarial;combating adversarial;recognition model downstream;recognition combat adversarial;combat adversarial;downstream classifier;model downstream classifier;downstream classifier methods;robustust;classifier;spelling mistakes propose;recognition models;downstream classifier conclusions;recognition models build", "pdf_keywords": "combat adversarial spelling;adversarial spelling;adversarial spelling mistakes;character level adversarial;robustness word recognition;word recognition safeguard;level adversarial;adversarial;model word recognition;models robust adversarial;adversarially;combat adversarial;character word recognizers;robust adversarial;accurate word recognition;train word recognition;level adversarial attacks;word recognizers;word recognition models;words trained recognize;novel word recognition;trained recognize words;adversarial attacks additionally;robust adversarial attacks;exploited adversarially;resultsour word recognition;unseen words trained;adversarial attacks;word recognition;paper combat adversarial"}, "25ae911c13da7ef9def56ee30170920ebd48a668": {"ta_keywords": "web arguments convincingness;convincingness web arguments;computational argumentation;computational argumentation investigate;web arguments;arguments convincingness methods;predicting convincingness web;argumentation investigate;argumentation investigate qualitative;convincingness web;argumentation;field computational argumentation;web arguments using;arguments convincingness;arguments using bidirectional;argument convincing analyzing;analyzing predicting convincingness;arguments having stance;arguments;predicting convincingness;arguments having;convincingness methods;classification pair arguments;convincingness methods cast;problem relation classification;arguments using;argument convincing;relation classification;pair arguments having;properties web arguments", "pdf_keywords": ""}, "b48f0652605f981b5d407496aba3d9756725264f": {"ta_keywords": "preference modeling frameworks;important preference modeling;preference modeling;subjective preferences ubiquitous;use subjective preferences;subjective preferences;preferences ubiquitous;preferences ubiquitous everyday;aggregation important preference;preference;use subjective;life important preference;important preference;preferences;reasoning aggregation important;subjective;reasoning aggregation;efficient reasoning aggregation;modeling frameworks allow;modeling frameworks;aggregation important;aggregation;frameworks allow expressive;elicitation techniques efficient;representations effective elicitation;expressive compact representations;efficient reasoning;ubiquitous everyday;ubiquitous everyday life;elicitation techniques", "pdf_keywords": ""}, "a636768c2fc6cadccd8bb4d704f651dd54dad395": {"ta_keywords": "emotion japanese conversational;analysis emotion japanese;emotion recognition indian;japanese conversational speech;recognition analysis emotion;recognition indian conversational;emotion japanese;indian conversational speech;japanese conversational;analysis emotion;study emotion recognition;emotion recognition;conversational speech methods;conversational speech;indian conversational;conversational speech aim;incorporating emotional aspect;emotional aspect human;speech methods;presents study emotion;incorporating emotional;talk recordings various;study emotion;speech;speech aim;talk recordings;speech aim importance;television talk recordings;emotion;emotional aspect", "pdf_keywords": ""}, "f664e6635d0514b0cb398a713f08bab90b4a3d81": {"ta_keywords": "statistical topic models;topic models;topic models latent;documents topics;models latent dirichlet;latent dirichlet al;latent dirichlet;documents topics consequence;topics;summarize large document;representation documents topics;words representation documents;statistical topic;words documents results;words documents;document collections;large document collections;bag words representation;model visualize summarize;ity words documents;topics consequence;dirichlet al;dirichlet al location;introduction statistical topic;document collections completely;representation documents;results bag words;visualize summarize;bag words;words representation", "pdf_keywords": ""}, "bd1bdb3c5f28001a4cee92c0e1669512d0f06a35": {"ta_keywords": "law generalized zipf;generalized zipf law;zipf law described;zipf law;theheaps law generalized;theheap law generalized;derivation theheaps law;generalized zipf;derivation theheap law;theheap law;theheaps law;zipf;formal derivation theheaps;law described simple;law generalized;derivation theheaps;law described;simple derivation theheap;derivation theheap;simple formal derivation;formal derivation;law;simple derivation;derivation;described simple formal;theheaps;theheap;simple formal;described simple;formal", "pdf_keywords": "law generalized zipf;generalized zipf law;zipf law described;zipf law;hisaps law generalized;formal derivation hisaps;generalized zipf;derivation hisaps law;constant generalized zipf;hisaps law inversely;derivation hisaps;law generalized;simple formal derivation;constant hisaps law;formal derivation;zipf;hisaps law;described summation term;application described summation;information processing field;hisaps law application;information processing;law described method;variable summation term;law inversely proportional;summation term;field constant hisaps;described summation;summation term equal;law inversely"}, "a9b9404962760731d6d2fc2ecbc6da7bc2f21be7": {"ta_keywords": "voice activity detection;voice activity;introductionvoice activity detection;based voice activity;model based voice;speech period based;drillichlet prior statistical;vad identify speech;speech generalmms;identify speech;introduce drillichlet prior;speech generalmms constructed;detection using dirichlet;non speech generalmms;using dirichlet prior;speech period;drillichlet prior;introductionvoice activity;activity detection vad;dirichlet prior;dirichlet prior paper;voice;environmental adapted speech;prior statistical model;activity detection using;speech non speech;adapted speech;activity detection;non speech period;identify speech non", "pdf_keywords": ""}, "32367e7587d5b2de0391cff9ad2d600ff8624e60": {"ta_keywords": "social skills training;skills training audiovisual;social skill training;skills social interaction;effectiveness social skill;social skills;introductionautomated social skills;appropriate skills social;social skill;training audiovisual information;skills social;training audiovisual;audiovisual information people;based social skills;training using audio;computer based social;social communication;audiovisual information;audiovisual information established;people social communication;social communication difficulties;social interaction;audiovisual;effectiveness social;using audio;skills training systems;skills using computers;skill training using;skills using;social interaction methodsdata", "pdf_keywords": ""}, "22f4eb19be4031e63194bbd7c355914533004918": {"ta_keywords": "electric vehicles bvs;vehicles bvs powertrain;electrified xv powertrain;bvs powertrain increasingly;xv powertrain increasingly;bvs powertrain;increasingly electrified xv;xv powertrain;battery electric vehicles;electrified xv;electric vehicles;electrified xv growing;powertrain increasingly electrified;vehicles bvs;powertrain increasingly;vehicles pure battery;powertrain;growing xv;xv growing xv;pure battery electric;increasingly electrified;battery electric;xv growing;engines based vehicles;internal combustion engines;growing xv sales;xv;xv sales currently;combustion engines based;combustion engines", "pdf_keywords": ""}, "ea77b71385648f5c6ea533a0e3685f0e76302eba": {"ta_keywords": "namedd entity recognition;named entity recognition;clinical messagea little;entity recognition;little annotation;little annotation does;annotation;messagea little annotation;key clinical messagea;entity recognition joseph;clinical messagea;annotation does;annotation does lot;labeled data;named entity;labeled data making;namedd entity;clinical;models named entity;low resource namedd;key clinical;large amounts labeled;labeled;data making challenging;entity;2010 state art;state art models;resource namedd entity;data;messagea little", "pdf_keywords": "lingual transfer learning;entity recognition ner;named entity recognition;targeted annotation strategy;entity recognition;maintaining high annotation;manually annotation spans;targeted annotation;annotation data language;partially annotated sequence;annotation strategy active;high annotation quality;annotation strategy;manually manually annotation;high annotation;manually annotation;annotated sequence;annotated sequence use;model annotated;annotation spans;improved language domains;annotation data;human annotation data;annotation quality;experiments targeted annotation;partially annotated;save annotator effort;annotation procedure manually;manually annotation human;annotation spans present"}, "0822f8d7e6a72a65e65f147d3a8d8fccd485da40": {"ta_keywords": "language modeling transformers;language modeling;shorter inputs;progress language modeling;model short subsequences;shorter inputs harmful;input length methods;training model short;increasing input length;conditions shorter inputs;input length;decrease input length;modeling transformers;model short;modeling transformers identify;improvements perplexity second;progress language;improvements perplexity;length methods initially;short subsequences moving;transformers;significant improvements perplexity;short subsequences;subsequences moving longer;perplexity efficiency improvements;methods decrease input;length methods;language;shorter;increasing input", "pdf_keywords": "language models efficient;language modeling transformers;training language models;accelerating training language;language models;language modeling;language models results;model language modeling;performance language models;transfer language modeling;computational language models;language modeling model;progress language modeling;language model;language modeling toronto;language models staged;language models association;training language;model short subsequences;tokens model efficient;language model based;model language;efficient model generating;expensive long contexts;computational language technologies;improved performance language;contexts expensive long;short input subsequences;computational language development;language development"}, "253ad629cd2d396201d71aa605bec233bff66dca": {"ta_keywords": "clustering vbec speech;bayesian estimation clustering;continuous speech recognition;model speech recognition;clustering based gaussian;estimation clustering vbec;vbec speech recognition;gaussian mixture model;based gaussian mixture;speech recognition;clustering large vocabulary;variational bayesian estimation;acoustic model speech;speech recognition described;gaussian mixture;vocabulary continuous speech;decision tree clustering;estimation clustering large;estimation clustering;vbec speech;speech recognition using;clustering vbec;model speech;using variational bayesian;variational bayesian;introductionvarial bayesian estimation;speech recognition resultswe;continuous speech;mixture model gm;mixture model", "pdf_keywords": ""}, "fac95cc5f52f954fe89b3aa4b75895568ff6a6d4": {"ta_keywords": "normalization language data;normalization language;rule based normalization;deals normalization language;modern wordforms rules;based normalization;normalization;paper deals normalization;normalization different types;deals normalization;historical wordforms;based normalization different;historical wordforms modern;maps historical wordforms;normalization different;modern wordforms;wordforms rules;wordforms rules specified;historical texts challenge;wordforms modern wordforms;unsupervised rule based;language data;german methodswe unsupervised;historical texts;wordforms modern;types historical texts;introductionapping rule based;language data early;high german methodswe;rule based approach", "pdf_keywords": ""}, "8b73e226815d57bf66fc94905ebd063e4957b449": {"ta_keywords": "reviewers peer;sensitive information reviewer;background reviewers peer;calibrate reviews attempts;calibrate reviews;calibration privacy;peer review miscalibrated;reviewers peer review;reviews attempts calibration;calibration privacy provide;peer review;proposed calibrate reviews;problem calibration privacy;reviewers;background reviewers;reviewer reviewed paper;review miscalibrated;information reviewer;review miscalibrated strict;leak sensitive information;reviewer;privacy;reviewer reviewed;information reviewer reviewed;sensitive information;privacy provide foundational;reviewed paper methods;privacy provide;calibration leak sensitive;reviewed paper", "pdf_keywords": "randomization peer review;sensitive information reviewer;randomization peer;use randomization peer;biased reviewers;peer review miscalibrated;calibration privacy;backgroundreviewers peer review;peer review;problem peer review;peer review lottery;biased reviewers conference;calibration privacy provide;adversary calibration;peer review fundamental;reviewer assignment;peer review involves;webackground peer review;scores biased reviewers;guess reviewer assignment;leak sensitive information;calibrate reviews attempts;peer review widely;conference peer review;peer review applications;reviewer assignment use;adversary challenging task;miscalibration functions adversary;ensure information reviewer;review process peer"}, "f249e3a7d4f7f964e9a4ca6e633ac31410a91dd8": {"ta_keywords": "attention architecture inflection;monolingual data hallucination;architecture inflection decoder;inflection decoder;monolingual data;attention architecture;inflection decoder addition;step attention architecture;languages monolingual data;lingual transfer;clinical messagewe;key clinical messagewe;cross lingual transfer;effects cross lingual;lingual transfer single;lingual;clinical messagewe propose;novel step attention;cross lingual;languages monolingual;multiple languages monolingual;monolingual;attention;multiple languages;single multiple languages;decoder;step attention;architecture inflection;data hallucination macro;languages", "pdf_keywords": "task morphological inflection;recognition language neural;language processing long;attention decoder architecture;language neural;generate morphological inflection;step attention decoder;language discriminator trained;language data train;attention architecture robust;shared task morphological;attention decoder;attention architecture;attention architectures;language invariant representations;learn language invariant;recognition language;attention architectures high;language processing;morphological inflection low;step attention architectures;morphological inflection evaluated;state art morphological;morphological inflection;task morphological;step attention architecture;ability generate morphological;learning step attention;recognition language fundamental;morphological inflection fundamental"}, "a309cb82c27233948f9b09f440be171a8d24ffff": {"ta_keywords": "peer selection agents;algorithm impartial peer;impartial peer selection;peer selection;selection peer;peer selection peer;introductionin peer selection;selection peer nomination;algorithms impartial individual;prize agents;novel algorithm impartial;prize agents self;algorithm impartial;algorithms impartial;award prize agents;impartial peer;selection agents choose;peer nomination;agents choose subset;selection agents;impartial individual agent;agents choose;design algorithms impartial;subset award prize;peer;choose subset award;agents self interested;individual agent;allocation mechanism design;algorithms", "pdf_keywords": "peer selection agents;peer selection method;algorithm impartial peer;impartial peer selection;approach peer selection;peer selection mechanism;inexact peer selection;peer selection using;peer selection;effectiveness peer selection;strategyproof peer selection;introductionin peer selection;peer selection established;peer reviewing;peer selection real;method peer reviewing;method peer review;peer review;approach peer review;peer reviewing compare;clusters rated agents;prize agents;novel algorithm impartial;impartial peer;algorithm impartial;algorithms impartial;algorithms impartial individual;peer selection peernosminin;method peer;prize agents self"}, "04a94c15fec43e7563d58be697246a0dd6c57021": {"ta_keywords": "problem social media;health problem social;public health problem;problem public health;health problem world;media public health;health problem public;social media public;world social media;problem world social;social media continues;public health;problem social;health problem;social media;problem public;major public health;world social;problem world;media public;health;social;media continues major;media;media continues;problem;public;continues major public;major public;world", "pdf_keywords": "content moderation;media products regulatory;moderation;information curation;information curation matter;etiology social media;content moderation calling;creating information curation;development social media;social media understood;article arguesthe internet;social media;problem social media;platforms creating media;social media continues;arguesthe internet powerful;internet development social;platforms creators article;arguesthe internet;gillespie examines platforms;lens content moderation;moderation calling;content allow;examines platforms;media understood article;content;curation;media;platforms creators;content allow operate"}, "a99de68ee8d6729eee5ca5943b152aba7e4738ee": {"ta_keywords": "neural editor;neural editor edit;learning represent edits;distributed representations edits;edits combining neural;combining neural editor;represent edits;representations edits combining;representations edits;represent edits introduce;learning distributed representations;edit encoder models;editor edit encoder;editor edit;editor;encoder models learn;edits combining;distributed representations;edits introduce;source code edit;code edit data;edit encoder;salient information edit;edit data evaluation;code edit;neural;encoder models;models learn represent;edits new inputs;learning distributed", "pdf_keywords": "treee neural editor;based edit encoder;sequence based edit;distributed representation edits;neural editors;edits neural editors;graph based edit;editors neural;distributed representations edits;encoder tree based;neural edit model;editors neural editors;emerging edit patterns;neural editor model;neural editor;neural editors useful;edit neural editors;neural editors neural;text editing models;integrating neural editors;natural language edits;neural editors achieve;edit patterns propose;edits representations;based encoder tree;text graph tree;neural editors used;edit patterns;edit encoder graph;edits natural language"}, "7f20366098665cd508fe82255cc1a65e1e733a14": {"ta_keywords": "recognition speech enhancement;speech enhancement techniques;use speech enhancement;speech enhancement technique;speech enhancement;enhanced speech features;enhanced speech;automatic speech recognition;mismatch enhanced speech;performance automatic speech;speech recognition;recognition speech;improvement recognition performance;prior recognition speech;enhancement techniques introduce;enhancement techniques;improvement recognition;automatic speech;speech features acoustic;limiting improvement recognition;enhancement technique;distortions use speech;speech recognition suffers;handling acoustic distortions;features acoustic model;features acoustic;speech features;noise reverberation;recognition limiting improvement;acoustic distortions use", "pdf_keywords": ""}, "22d2f8030221bd0c27bfb9416eeffe4e86633780": {"ta_keywords": "introductionefefficient neural ranking;neural domain ranking;neural ranking;fast forward index;facilitates ranking documents;neural ranking using;ranking using forward;ranking documents;ranking documents using;index facilitates ranking;facilitates ranking;methodsfast forward indexes;forward indexes;domain ranking approaches;ranking approaches;indexes nearest neighbor;contextual rankers;ranking approaches propose;replacement contextual rankers;forward index facilitates;ranking;forward indexes important;using forward indexes;forward index;forward index simple;ranking using;domain ranking;vector forward index;forward indexes rely;lexical semantic scores", "pdf_keywords": "facilitates ranking documents;document ranking tasks;document retrieval ranking;document ranking approaches;ranking retrieval;retrieval ranking;ranking documents;document ranking;ranking better contextual;backgroundneurological document ranking;facilitates ranking;ranking retrieval queries;better contextual rankers;ranking documents using;based ranking retrieval;hoc document ranking;encoder based ranking;replacement contextual rankers;better ranking performance;document passage ranking;ranking tasks propose;interpolationbased ranking;ranking interpolation improves;ranking tasks;based retrieval better;fast forward indexes;ranking better;index facilitates ranking;better ranking;optimize retrieval"}, "4e0610ac4c5e055ac56b2ae0d91386a10ffbd325": {"ta_keywords": "integration prior knowledge;simulates human learning;intelligent agent simulates;human learning math;prior knowledge;intelligent agent;building intelligent agent;human learning artificial;prior knowledge maintain;human level intelligence;prior knowledge methods;learning artificial intelligence;specific prior knowledge;artificial intelligence;artificial intelligence advancing;human learning;knowledge;agent simulates;building intelligent;agent simulates human;knowledge methods;learning artificial;level intelligence;knowledge methods results;understanding human learning;learning math;intelligence advancing;knowledge maintain;learning math science;level intelligence objectives", "pdf_keywords": ""}, "8d019c77989100a51385e4b4a5fa5250445d8f1d": {"ta_keywords": "discriminative training;generalized discriminative training;discriminative training framework;training framework combination;systems complementary;systems complementary systems;combination approaches;combining base systems;heuristic combination approaches;proposes generalized discriminative;combination approaches framework;conventional heuristic combination;complementary systems;generalized discriminative;complementary systems reasonably;complementary systems complementary;base systems complementary;discriminative;heuristic combination;performance combining base;performance combining;improve performance combining;combining;training framework;systems;combination;combining base;complementary;base systems;targets conventional heuristic", "pdf_keywords": ""}, "1b97c38d0156dc8bf300b41c2ba5c0463c3a2c00": {"ta_keywords": "training data augmentation;data augmentation strategies;data augmentation;augmentation data selection;data augmentation data;data augmentation simple;augmentation data;speech recognizer deployed;selection data augmentation;speech recognizer;robustness speech recognizer;improve robustness speech;noising reverberation tested;augmentation strategies;augmentation simple efficient;development data augmentation;augmentation strategies including;augmentation simple;augmentation;including noising reverberation;noising reverberation;robustness speech;reverberation tested;approaches signal enhancement;reverberation tested combination;signal enhancement carefully;introduction training data;signal enhancement;training data;strategies including noising", "pdf_keywords": ""}, "3df825e086b00dd4132c34ecbf638f9a6dc4320d": {"ta_keywords": "learning agent;human learning artificial;acquiring procedural knowledge;machine learning agent;intelligent agent simulates;introductionbuilding intelligent agent;intelligent agent;learning artificial intelligence;procedural knowledge using;procedural knowledge;human level learning;artificial intelligence creating;human learning;learning artificial;knowledge using transfer;transfer learning operated;agent simulates;agent simulates human;learning operated separate;level learning appropriate;understanding human learning;artificial intelligence;human level intelligence;level learning;knowledge using;using transfer learning;simulates human level;learning appropriate learning;level intelligence;level intelligence research", "pdf_keywords": ""}, "3e59b3e1e3ef65f9574a0fe30f18ba7a815ea0af": {"ta_keywords": "dialogue policy learning;exploration dialogue policy;policy learning bbq;exploration dialogue;learning greedy exploration;task oriented dialogue;policy learning;introductionefefficient exploration dialogue;rewards sparse action;dialogue systems;dialogue policy;dialogue systems primary;oriented dialogue systems;oriented dialogue;greedy exploration;greedy exploration inefficient;dialogue;rewards sparse;presentationwhen rewards sparse;sparse action;learning bbq networks;exploration inefficient;learning bbq;bbq networks replay;reward signal;networks replay;systems primary reward;reward signal indicating;sparse action spaces;exploration", "pdf_keywords": ""}, "790eb7e93f1d3fce470c0222fd2be83bab55a428": {"ta_keywords": "rnn language models;word based rnn;end speech recognition;based rnn language;rnn language;speech recognition ar;speech recognition;speech recognition word;language models nn;hybridct attention based;automatic speech;based automatic speech;automatic speech recognition;based rnn;combined hybridct attention;attention based automatic;rnn;hybridct attention;word based ls;introductionend end speech;language models;recognition word based;models nn ls;end speech;attention based;character based word;models nn;speech;based ls combined;nn ls challenging", "pdf_keywords": "attention training decoding;crc attention decoder;hybrid attention cc;encoder crc attention;attention cc network;attention decoder;attention decoder networks;language model trained;jointly hybrid attention;crc attention training;rn language model;external language models;neural networks speech;hybrid attention;decoder networks training;novel language models;recurrent neural networks;language models nn;recurrent neural network;networks speech recognition;speech recognition ar;external rn language;speech recognition;machine translation end;cnn blstm encoder;attention training;training decoding;automatic speech;neural network language;neural machine translation"}, "83cf7b9611fabe9da2d08722445039023f1b19e9": {"ta_keywords": "disease provide overview;disease;literature topic disease;disease provide;topic disease provide;topic disease;article provide overview;provide overview literature;overview literature;overview literature topic;literature topic;article;purpose article;literature;article provide;provide overview;purpose article provide;overview;topic;purpose;provide", "pdf_keywords": ""}, "9b09ff09b88bb793b161f284ca6e66031bc5a992": {"ta_keywords": "evolution computational technology;computational technology;computational technology results;literature evolution computational;computational;discussions evolution computational;evolution computational;technology;discussions evolution;technology results;review literature evolution;day discussions evolution;literature evolution;evolution;technology results day;systematic;systematic review literature;article present;systematic review;purpose article present;article present results;article;results systematic;present results systematic;results systematic review;discussions;day discussions;purpose article;present;review literature", "pdf_keywords": ""}, "68f2f32e0e8fc868920971077a11042784be2616": {"ta_keywords": "teams assemble ranking;field ranking rating;ranking rating;field ranking;ranking;assemble ranking strict;ranking rating receiving;assemble ranking;ranking strict;introductionthe field ranking;selecting matchups sports;ranking strict linear;entities movies sports;tournaments central question;matchups sports tournaments;recommendations selecting matchups;sports tournaments;movie recommendations selecting;tournaments;movies sports teams;rating receiving;sports tournaments central;movies sports;matchups sports;rating;order entities ways;tournaments central;rating receiving increasing;order entities;sports teams assemble", "pdf_keywords": ""}, "a3ca4893ae941bd1601322aface4840e47339761": {"ta_keywords": "experiments important crowdsourcing;peer selection mechanisms;crowdsourcing;crowdsourcing setting;crowd sourcing;peer selection;strategyproof peer selection;crowd sourcing settings;applying crowd sourcing;important crowdsourcing;important crowdsourcing setting;crowdsourcing setting agents;peer review;ubiquitous peer review;introduction strategyproof peer;funding scientists selecting;strategyproof peer;peer;peer review used;distributing awards team;funding scientists;allocating funding scientists;challenge applying crowd;scientists selecting;ubiquitous peer;distributing awards;applying crowd;sourcing;agents evaluate based;scientists selecting publications", "pdf_keywords": ""}, "a810d2f4a1fefd4175d8cdda9702ee1b829e5831": {"ta_keywords": "obesity natural compounds;screening methods phytohemagglutinin;phytohemagglutinin ameliorates high;phytohemagglutinin pmh anti;phytohemagglutinin ameliorates;anti obesity natural;background phytohemagglutinin ameliorates;phytohemagglutinin;phytohemagglutinin pmh;anti obesity;pmh anti obesity;diet induced obesity;background phytohemagglutinin;methods phytohemagglutinin pmh;obesity natural;methods phytohemagglutinin;weight gain investigate;fat diet induced;fat diet;obesity increasing energy;anti obesity candidate;high fat diet;screen anti obesity;induced obesity increasing;induced obesity;ameliorates high fat;obesity;obesity increasing;body weight gain;obesity candidate", "pdf_keywords": ""}, "66081634c17b089cb47fd1b0ad7ad842c7fb3f87": {"ta_keywords": "tutee tutor learning;easy tutor sistudent;fected tutor learning;tutor sistudent;tutor learning;tutor learning results;help tutee tutor;tween tutee tutor;students easy tutor;tutor learning particularly;use teachable agent;tutor sistudent turnthis;teachable agent technology;easy tutor;tutor;tutee tutor;tutor learning interested;teaching sistuddent student;teachable agent;af fected tutor;fected tutor;teaching sistudent;results teaching sistudent;extend teaching sistuddent;teaching sistuddent;use teachable;teaching sistudent fixed;learning teaching;teachable;teaching use teachable", "pdf_keywords": ""}, "d706645fbbc6edfad5fb642b1dfc3019fcabbd99": {"ta_keywords": "text generation researched;amazon mechanical turk;mechanical turk evaluate;using mechanical turk;text generation research;researched text generation;crowdsourced human judgments;generation researched text;endened text generation;mechanical turk;text generation;story poetry generation;poetry generation models;crowdsourced human;human judgments text;collecting crowdsourced human;judgments text quality;poetry generation;collecting crowdsourced;crowdsourced;choices collecting crowdsourced;judgments text;turk evaluate;turk evaluate open;human judgments;generation research increasingly;generation research;domains story poetry;text quality likert;researched text", "pdf_keywords": "mechanical turk task;amazon mechanical turk;crowdsourced human judgments;mechanical turk ametry;turk task design;mechanical turk;human evaluation text;popular mechanical turk;judgments text quality;crowdsourced human;collecting crowdsourced human;text evaluation;evaluation text quality;assessing text;human judgments text;written text evaluation;generated stories evaluated;machine generated stories;collecting crowdsourced;turk task;assessing text quality;human generated stories;stories human generated;text evaluation association;choices collecting crowdsourced;text evaluation amazon;natural language generation;ended text evaluation;score human written;judgments text"}, "ad7129af0644dbcafa9aa2f111cb76526ea444a1": {"ta_keywords": "neural fake news;generate neural fake;natural language generation;adversaries generate neural;language generation;adversaries generate;neural fake;language generation raised;news targeted propaganda;careful threat modeling;fake news targeted;like summarization translation;generate neural;threats vulnerabilities;threat modeling;progress natural language;potential threats vulnerabilities;propaganda closely mimics;threats vulnerabilities adversary;targeted propaganda;applications like summarization;natural language;targeted propaganda closely;identifying potential threats;threat modeling identifying;like summarization;vulnerabilities adversary;summarization translation positive;summarization translation;news targeted", "pdf_keywords": "neural fake news;generate neural fake;written neural fake;fake news detection;detect fake news;propaganda use neural;fake news discriminator;use neural fake;tool generating propaganda;respond neural fake;neural fake;real news propaganda;fake news verifiers;generating propaganda;human written propaganda;detecting neural fake;generating propaganda public;threats likewiseartificial news;news propaganda websites;adversaries generate neural;news propaganda;dynamics neural fake;fake news targeted;accuracy neural fake;counteracting neural disinformation;news targeted propaganda;language model generation;fake news using;written propaganda rewriting;written propaganda articles"}, "946e5e31b0779fc33550e8681994e7afd8d549a5": {"ta_keywords": "assess gait gait;measure gait gait;assess gait;measure gait;gait analysis;gait gait analysis;measurements assess gait;gait analysis demonstrate;used measure gait;gait gait;gait;motion measurements assess;motion motion measurements;motion measurements used;motion measurements;use automated motion;automated motion motion;automated motion;demonstrate automated motion;motion motion;motion;analysis demonstrate automated;measurements assess;demonstrate automated;use automated;automated;measurements used measure;assess;analysis;measure", "pdf_keywords": ""}, "81d4357afae9680e64a645cbb36aa090c3619b19": {"ta_keywords": "category anchor text;complete category anchor;category anchor;does category anchor;improve category results;text improve category;category index results;anchor text improve;category results;improve category;category index;unable improve category;obtained category index;category results methods;diversity task results;results obtained category;category results ad;hoc diversity task;anchor text;anchor;results ad hoc;does category;anchor text unable;diversity task;ad hoc diversity;obtained category;category;hoc diversity;index results;complete category", "pdf_keywords": ""}, "a7d6b5e61024127bf4fe8f04c0182a16ff97bccf": {"ta_keywords": "probabilistic lobbying;lobbying probabilistic;probabilistic lobbying complex;complexity probabilistic lobbying;models lobbying probabilistic;lobbying probabilistic environment;various models lobbying;models lobbying;lobbying complex phenomenon;lobbying complex;lobbying;lobbyby seeks influence;bribery methods;bribery methods formally;bribery;criteria bribery methods;criteria bribery;influence voters preferences;evaluation criteria bribery;lobbyby seeks;seeks influence voters;influence voters;voters preferences;voters preferences voting;voters preferences represented;called lobbyby seeks;lobbyby;probabilistic;complexity probabilistic;issues voters preferences", "pdf_keywords": "lobbying probabilistic;probabilistic lobbying;probabilistic lobbying problems;models lobbying probabilistic;analysis probabilistic lobbying;lobbying probabilistic environment;approach lobbying voting;lobbying scenarios probabilistic;complexity probabilistic lobbybying;lobbying voting;present probabilistic lobbying;lobbying voting multiple;probabilistic lobbybying;efficiently solve lobbying;voter bribery stochastic;problem voter bribery;computational approach lobbying;bribery specific voter;probabilistic lobbybying complex;lobbying problems combinations;voter bribery;machine solve lobbying;majority scenario bribery;lobbying problem;solve lobbying problem;lobbying problem nondeterministic;studied lobbying scenarios;lobbying problem work;probabilityistic lobbybying;weighting lobbyy complexity"}, "419e714f22c3fa2599abebd630cae5595c70bdef": {"ta_keywords": "e2e automatic speech;speech recognition enhanced;recognition enhanced speech;integraded speech recognition;integration speech recognition;recognition speech enhancement;enhanced speech input;automatic speech recognition;speech enhancement self;speech recognition ar;robust speech recognition;speech recognition;enhanced speech;speech recognition called;speech recognition speech;speech enhancement;speech input self;automatic speech;recognition speech;input self supervised;speech input;targetting robust speech;enhancement self supervised;robust speech;end integration speech;e2e model integrates;integraded speech;self supervised learning;integration speech;recognition enhanced", "pdf_keywords": "recognition enhanced speech;enhanced speech input;integrated speech recognition;speech recognition enhanced;speech enhancement se;speech enhancement module;iris robust speech;imiris robust speech;integraded speech recognition;e2e automatic speech;robust speech recognition;adding speech enhancement;development robust speech;speech recognition iris;use speech enhancement;improved speech recognition;speech enhancement model;speech speech enhancement;speech enhancement;speech recognition self;module robust speech;speech models robust;speech development robust;enhanced speech;models robust speech;modeling speech recognition;targetting robust speech;including speech enhancement;integrated speech;robust speech useful"}, "888c81cd3d1e953e2b7f8cc4ce68ca9f908c1e8d": {"ta_keywords": "differential privacy;privacy frameworks differential;concept differential privacy;differential privacy non;frameworks differential privacy;differential privacy compelling;privacy preserving methods;approaches privacy preserving;privacy frameworks;privacy preserving;various approaches privacy;approaches privacy;privacy gains traction;introduction privacy;favorite privacy frameworks;privacy non trivial;methods favorite privacy;introduction privacy gains;privacy;privacy non;favorite privacy;privacy gains;privacy compelling;privacy compelling thanks;non nl methods;frameworks differential;nl methods;preserving methods;preserving methods favorite;applying non nl", "pdf_keywords": "using differential privacy;differential privacy;detecting differential privacy;differential privacy text;differential privacy dbm;simulates actual privacy;privacy dtext;differentiable deep networks;mathematical treatment privacy;non privacy dtext;uses variational autoencoder;algorithm differentially private;privacy preserving text;deep networks dtext;variational autoencoder architecture;privacy dbm simple;variational autoencoder;privacy dbm;privacy preserving;differentially private;privacy text data;privacy dtext confirms;privacy dbm formal;privacy new;dataset reconstruction attack;privacy new method;actual privacy;formally proved privacy;privacy;breaking privacy new"}, "596b46dbe4fa8eee72e517ea9fd5f8ef83c9c64e": {"ta_keywords": "question answering quizbowl;quizbowl;answering quizbowl;answering quizbowl question;quizbowl question;introduction quizbowl;quizbowl question consists;quizbowl scholastic;introduction quizbowl scholastic;quizbowl scholastic trivia;question answering;trivia competition tests;tests human knowledge;scholastic trivia competition;research question answering;question time elite;human knowledge intelligence;answer question time;onwikipedia players answer;players answer question;knowledge intelligence;trivia competition;competition tests human;scholastic trivia;question time;competition tests;answer question;knowledge intelligence additionally;human knowledge;tests", "pdf_keywords": "question answering humans;question answering task;task question answering;answering task challenges;challenges question answering;question answering;compelling question answering;answering format competition;problem question answering;challenging task humans;answering discuss challenges;challenges problem answering;measuring question answering;question answering object;question answering discuss;question answering compare;question answering development;question answering sequential;answering tool;question answering tool;question answering problem;problem answering methods;answering humans;question answering methods;development question answering;answering task;evaluation question answering;task challenges;question answering abilities;answering tool used"}, "650f2afca6d72d6b6e2e08849e1224f1e8b7900c": {"ta_keywords": "binary rating estimation;rating matrix graph;information social graphs;rating estimation;graph information social;ratings aid graph;rating estimation problem;social graphs;social graphs gain;graph information;graph information methods;rating matrix;introduction binary rating;binary rating;users unknown ratings;study binary rating;value graph information;unknown ratings;model rating matrix;ratings;rating;matrix graph;graphs rich;graphs gain theoretically;correlation model rating;graphs;graphs gain;information social;unknown ratings aid;ratings aid", "pdf_keywords": ""}, "932404745d960291925b3f27b71734dff5b23633": {"ta_keywords": "disparity require treatment;exhibit treatment disparity;treatment disparity;treatment disparity following;treatment disparity formally;disparity outcomes;impact disparity outcomes;fairness algorithms exhibit;fairness algorithms;discrimination;disparity formally treat;disparity outcomes differ;disparity following precedent;require treatment disparity;disparity widely discussed;discrimination law;precedent employment discrimination;employment discrimination;discrimination law notions;mitigating impact disparity;impact disparity require;disparity widely;impact disparity;law notions disparity;discussed papers fairness;exhibit impact disparity;papers fairness algorithms;employment discrimination law;disparity require;notions disparity widely", "pdf_keywords": ""}, "7bbd132f40c7630aeebf6379b00e307c3fff738c": {"ta_keywords": "repair bandwidth distributed;minimizing repair bandwidth;stored nodes distributed;bandwidth distributed storage;distributed storage;codes minimizing repair;distributed storage methodswe;repair bandwidth;bandwidth required repair;nodes distributed manner;replication;nodes distributed;connecting nodes distributed;exact replication;repair failed node;distributed;stored nodes;minimizing repair;data stored nodes;failed node data;bandwidth distributed;distributed manner;distributed manner provide;data connecting nodes;replication failed systematic;permit exact replication;minimizing bandwidth;nodes;minimizing bandwidth required;storage methodswe consider", "pdf_keywords": "bound repair bandwidth;exact regenerating codes;regenerating codes described;nodes explicit code;new code regeneration;matrices systematic nodes;using nodes regeneration;bound achievable coding;bandwidth exact regeneration;minimizing repair bandwidth;regenerating codes;nodes able regenerate;regeneration systematic nodes;code regeneration;code regeneration systematic;codes achieve bound;achievable coding scheme;nodes useful regeneration;regeneration systematic node;systematic node regeneration;nodes regeneration;node regeneration;node regeneration systematic;required regenerate node;systematic nodes explicit;regenerate systematic node;regenerate node;nodes using systematic;optimal exact regeneration;nodes systematic nodes"}, "e8c3090e66fdb05a2c169a12c52dd94bb8786fb5": {"ta_keywords": "pointers explanations persuasive;explanations persuasive;explanations persuasive arguments;explanations argument persuasive;natural language explanations;argument persuasive methods;persuasive arguments objective;argument persuasive;language explanations leverage;persuasive;word level prediction;persuasive arguments;investigate explanations selective;persuasive methods propose;persuasive methods;language explanations;investigate explanations;explanations selective;occurring explanations argument;task investigate explanations;explanations argument;prediction task investigate;naturally occurring explanations;occurring explanations;dynamics changemyview subreddit;explanations;level prediction task;prediction task;natural language;explanations leverage", "pdf_keywords": "opinion change explanations;natural language explanations;explanations opinion changes;persuasive comment predicting;language explanations leverage;explanations phenomenon linguistic;explanations argument persuasive;generating explanations using;generating explanations;identify explanations opinion;change explanations;language explanations;explanations context persuasion;language explanations demonstrate;change explanations likely;explanations importance contextual;explanations development computational;identify explanations;nouns shaping explanations;explanations stopwords;shaping explanations development;shaping explanations importance;task generating explanations;changemyview identify explanations;shaping explanations highly;explanations selectively reuse;linguistic accommodation argumentation;argumentation mining;comment predicting;shaping explanations"}, "ed535e93d5b5a8b689e861e9c6083a806d1535c2": {"ta_keywords": "transformers systematic generalization;generalization transformers research;systematic generalization transformers;generalization transformers;transformers systematic;embedding universal transformer;performance transformers systematic;transformer variants drastically;universal transformer variants;transformer variants;universal transformer;improve performance transformers;performance transformers;transformers research;transformers research design;transformers;transformer;systematic generalization;improve systematic generalization;systematic generalization resultswe;positional embedding universal;revisiting model configurations;embedding universal;scaling embeddings early;generalization resultswe;generalization;basic scaling embeddings;model configurations;scaling embeddings;generalization resultswe report", "pdf_keywords": "systematic generalization transformers;transformers generalization performance;transformers generalization;generalization transformers results;generalization transformers;generalization transformers objectivethe;systematic generalization neural;generalization performance transformers;transformers typically trained;performance transformers generalization;generalization neural;universal transformers absolute;improve baseline transformers;datasets systematic generalization;models systematic generalization;generalization neural networks;transformers absolute positional;baseline transformers particular;crucial generalization performance;performance universal transformers;generalization accuracy generalization;baseline transformers;final generalization accuracy;transformers results;universal transformers;generalization accuracy;transformers absolute;generalization ability neural;systematic generalization ability"}, "9abb50813e05de849dbbd89535bc7d0206f5e36a": {"ta_keywords": "semantic verb clustering;verb clustering;verb clustering assess;processing nlp lexical;dirichlet process mixture;mixture models learning;lexical semantic verb;nlp lexical;nlp lexical semantic;processing nlp;natural language processing;language processing nlp;dirichlet process;process mixture models;semantic verb;verb classes;lexical semantic;apply dirichlet process;language processing;task natural language;verb classes using;dirichlet;mixture models;1993 verb classes;semantic;process mixture;learning task natural;nlp;learning task;apply dirichlet", "pdf_keywords": ""}, "0bbfa6ab7451aea5cbb842cce97b54500bafdfc7": {"ta_keywords": "inverse decision theory;inverse decision;setting inverse decision;binary decisions uncertainty;preferences conveyed loss;binary decisions;understand preference learning;preference learning cases;preference learning;sequential binary decisions;decisions uncertainty;decision theory;better preference learning;preference learning objectiveto;understand preference;decisions uncertainty methodsin;preferences conveyed;decision theory idt;facilitate better preference;human preferences conveyed;human preferences;better understand preference;decisions facilitate;preference;decisions;decisions facilitate better;idt human preferences;study setting inverse;setting inverse;preferences", "pdf_keywords": "inverse decision theory;decisions inverse decision;decision making inverse;decisions inverse;inverse decision;making inverse decision;observing decisions inverse;setting inverse decision;inverse reinforcement learning;apply inverse decision;learning human preferences;preference learning uncertain;fact inverse decision;binary decisions uncertainty;learning uncertain humans;preference learning;understand preference learning;uncertainty reveal preference;inverse reinforcement;learning uncertain;preference learning case;observing decisions loss;preference learning cases;learning theory decision;binary decisions;risk optimal decision;case inverse reinforcement;preferences uncertainty make;optimal decisions decision;preference learning suggests"}, "a31ab366b0a349ee5f341f1179810bc9805d32a4": {"ta_keywords": "regenerating codes operating;repair regenerating codes;regenerating codes;security exact repair;storage regenerating msr;data file leaked;storage regenerating;codes operating;minimum storage regenerating;codes operating minimum;file leaked presence;exact repair regenerating;repair regenerating;file leaked;regenerating msr;regenerating;eavesdropper access;leaked presence eavesdropper;eavesdropper access contents;consider security exact;al information stored;stored data file;data file;information stored;security exact;access contents l1;storage;operating minimum storage;regenerating msr point;repair", "pdf_keywords": ""}, "04d18fc81cc232b3d3dece0994c0fa8aaabaf4b7": {"ta_keywords": "morphological analysis;word segmentation speech;word segmentation;segmentation speech pos;segmentation speech;speech pos tagging;process word segmentation;ja morphological analysis;morphological;backgroundmorphological analysis pointwise;approach ja morphological;segmentation;ja morphological;morphological analysis decomposes;backgroundmorphological analysis;tagging methodsthe pointwise;word boundaries pos;pos tagging;speech pos;word boundaries;pos tagging methodsthe;backgroundmorphological;tagging;analysis pointwise predictors;pointwise predictors;boundaries pos tags;speech;results word boundaries;pointwise predictors aimthe;tagging methodsthe", "pdf_keywords": ""}, "d5dcbb144a2be999610b4838d94cc3fb228f837c": {"ta_keywords": "cut 5g networks;network slice backup;5g smart grid;methods cut 5g;cut 5g;deploy network slicing;network slicing;network slicing high;quality service 5g;5g networks quality;network slice;service 5g;5g future methods;service 5g mobile;5g smart;5g networks;network virtualization;grid network slice;slice backup;5g future;network virtualization scenario;based nfv backup;use 5g future;networks quality chip;commercial use 5g;5g mobile;slice backup method;background 5g smart;nfv backup;communication network virtualization", "pdf_keywords": ""}, "df689bdc6c497949e9ab3b7ba19950d9fade7180": {"ta_keywords": "symptomatic asymptomatic asymptomatic;asymptomatic asymptomatic asymptomatic;symptomatic asymptomatic;asymptomatic asymptomatic;asymptomatic;history symptomatic asymptomatic;symptomatic;patients history symptomatic;history symptomatic;patients;patients history;management patients;management patients history;approach management patients;approach management;article;approach;new approach management;new approach;history;new;purpose article;management;importance new approach;article discuss importance;article discuss;importance new;purpose article discuss;purpose;discuss importance new", "pdf_keywords": ""}, "d7fe9b46f96ae9df7fa64e1c575c7114e5ef0aaa": {"ta_keywords": "optimum 10sor methods;introduction optimum 10sor;convexoptimization discussed methods;10sor methods smooth;convex optimization problems;optimum 10sor;convex optimization;methods smooth convex;methods consider convex;10sor methods;new tensor method;consider convex optimization;convexoptimization discussed;tensor method;convexoptimization;introduction optimum;unformly convexoptimization discussed;unformly convexoptimization;convex unformly convexoptimization;tensor method closes;optimum;optimization problems objective;optimization;optimization problems;propose new tensor;smooth convex;10sor;tensor;smooth convex unformly;convex", "pdf_keywords": "tensor methods convex;optimal tensor method;optimal tensor methods;accelerated tensor method;method accelerated tensor;tensor method better;iterations accelerated tensor;optimal tensor;new tensor method;new optimal tensor;propose optimal tensor;faster accelerated tensor;tensor method compare;tensor method nesterov;tensor method novel;tensor method;performance tensor methods;tensor method small;tensor methods;tensor method analyze;method convex optimization;tensor methods present;results accelerated tensor;tensor method closes;convex problems complexity;functions accelerated tensor;accelerated tensor;method solving convex;order methods convex;performance tensor"}, "4c6f7fb5c2e1bd12899c3ec2788f9ce7eb2f8a5c": {"ta_keywords": "pretrained language models;pretraining data language;pretrained language;based pretrained language;data language models;language models need;language models achieve;benchmarks pretraining methods;language models;introduction pretraining data;pretraining data;benchmarks pretraining;nonlu benchmarks pretraining;learn syntax;impact pretraining data;syntactic capabilities;pretraining methods convenient;pretraining data size;pretraining methods;introduction pretraining;impact syntactic capabilities;learn syntax case;models need learn;data language;pretraining;pretrained;based pretrained;need learn syntax;study impact pretraining;transformers based pretrained", "pdf_keywords": "syntactic information trained;syntactic capabilities robot;trained data syntactic;syntactic structural probes;structural syntactic probing;probing detailed syntactic;syntactic probing detailed;syntactic probing;predict syntactic information;syntactic capabilities;syntactic generalization models;able predict syntactic;language models widely;predict syntactic;language models;syntactic generalization downstream;model syntactic tests;syntactic information;introductiontransgressive language models;detailed syntactic generalization;syntactic information perform;data syntactic generalization;better syntactic generalization;encode syntactic information;model syntactic;models nonlinguistic tasks;impact syntactic capabilities;syntactic tests;syntactic generalization;syntactic structural"}, "b73191adcc938cfcf20ce0327cf5cd1f539f7f81": {"ta_keywords": "extracting keyphrases scientific;scientific information extraction;keyphrases scientific articles;supervised neural tagging;neural tagging model;neural tagging;information extraction semi;extracting keyphrases;keyphrases scientific;extraction semi supervised;named entity recognition;methods neural tagging;neural tagging important;entity recognition;problem extracting keyphrases;information extraction;sequence tagging;problem sequence tagging;tagging introduce semi;introduce semi supervised;sequence tagging introduce;scientific articles categorizing;tagging model;semi supervised neural;tagging;scientific articles associated;semi supervised methods;keyphrases;tagging important problem;scientific articles", "pdf_keywords": "neural tagging models;supervised neural tagging;neural tagging;neural tagging model;neural sequence tagging;categorize scientific keyphrases;scientific information extraction;neural tagging important;extracting keyphrases scientific;hierarchical neural tagging;information extraction scientific;embeddings information extraction;extraction entity recognition;uses neural tagging;extraction keyphrases;named entity recognition;information extraction semi;entity recognition annotated;methods neural tagging;entity recognition;extraction scientific articles;extracting keyphrases;keyphrases scientific articles;effective identifying keyphrases;extraction semi supervised;scientific keywords data;scientific keyphrases;extraction keyphrases using;information extraction task;information extraction"}, "06d77cc8970b59102a0caffb5e4c5b7a3242563a": {"ta_keywords": "ability communicate sex;communicate sex ability;sex ability communicate;disambiguated sex communicate;sex ability;communicate sex sex;communicate sex;sex communicate sex;sex communicate;self disambiguated sex;disambiguated sex;sex sex sex;sex sex;sex;ability communicate;ability self disambiguated;ability;ability self;communicate;self disambiguated;disambiguated;self", "pdf_keywords": ""}, "4715ee17ca4f52762fdf67c9a8ef8fb751c88484": {"ta_keywords": "identification arx models;arx models piecewise;corresponding arx model;arx model output;identifying arx model;arx models;input corresponding arx;arx model;task identifying arx;introductionblind identification arx;corresponding arx;models piecewise constant;piecewise constant inputs;identification arx;constant inputs known;identifying arx;models piecewise;piecewise constant input;arx;inputs known hard;model output measurements;inputs known;seek piecewise constant;constant input corresponding;constant inputs;identifying systems;output measurements driven;constant input;task identifying systems;model output", "pdf_keywords": "estimating arx model;input estimating arx;identification arx models;estimate arx model;arx models piecewise;method estimating arx;estimating arx;input estimate arx;framework estimate arx;corresponding arx model;estimate arx;arx model output;estimating arx coefficients;arx models;identifying arx model;task identifying arx;estimate arx coefficients;piecewise input estimating;arx model;estimating piecewise input;bi framework estimate;corresponding arx;model output observations;input corresponding arx;arx model approximateswe;input estimating;useful estimating piecewise;method estimating piecewise;piecewise input estimate;estimating piecewise"}, "cac008e541af58f738407c7f2ee86d547053188f": {"ta_keywords": "etiology disease understood;strategy treatment disease;disease;treatment disease;etiology disease;disease understood development;disease understood;treatment;new strategy treatment;etiology;strategy treatment;development new strategy;new strategy;development;development new;understood development new;understood development;strategy;new;understood", "pdf_keywords": ""}, "1e2ef0c9a494c7949f38940ee735a88c56355202": {"ta_keywords": "dynamic sensor subset;dynamic sensor activation;centralized tracking iiid;sensor activation centralized;dynamic dynamic sensor;dynamic sensor;selection centralized tracking;sensor subset selection;active sensors fidelity;centralized tracking time;centralized tracking;activation centralized tracking;problem dynamic sensor;sensor subset;tracking time varying;sensors fidelity;active sensors;tracking iiid process;tracking iiid;active sensors problem;sensors fidelity increases;sensors problem minimizing;sensor activation;number active sensors;selection centralized;subset selection centralized;dynamic dynamic;tracking;tracking time;sensor", "pdf_keywords": ""}, "00c3a86551f1bc812b676025210e295021853f66": {"ta_keywords": "bigger historical figures;review bigger historical;bigger historical;historical figures;historical figures really;review bigger;reviews review bigger;historical;history trivia fondness;figures really rank;loves history;measures importance;read loves history;history;article reviews;loves history trivia;importance;measures importance want;bigger;enjoyable read;article reviews review;history trivia;reviews review;rank steven skiena;ephemeral measures importance;really rank steven;easy enjoyable read;trivia fondness arguing;importance want;charles boston", "pdf_keywords": ""}, "69d5579955a5a8859d78a70b3d1afede0f91fa09": {"ta_keywords": "energy disaggregation task;perform energy disaggregation;energy disaggregation;backgroundenergy disaggregation task;building energy data;backgroundenergy disaggregation;data individual appliances;energy data individual;energy data building;energy data;disaggregated data consumer;providing disaggregated data;disaggregated data;aggregate energy data;disaggregation task separating;disaggregation task;sensors device home;improves energy consumption;energy consumption behavior;disaggregation;data building energy;energy consumption;individual sensors device;individual appliances;individual sensors;providing disaggregated;consumer improves energy;disaggregation task resultswe;individual appliances studies;simply providing disaggregated", "pdf_keywords": "disaggregation electricity consumption;energy disaggregation task;perform energy disaggregation;disaggregation electricity residential;energy disaggregation;disaggregation electricity;framework disaggregation electricity;method disaggregation electricity;designing disaggregation methods;disaggregation problem optimal;devices formulating disaggregation;designing disaggregation;supervised approach disaggregation;disaggregation method data;disaggregation task;disaggregation methods;introductionenergy disaggregation task;disaggregation electric;disaggregation task separating;disaggregation methods using;disaggregation present dynamical;approach disaggregation;method disaggregation electric;providing disaggregated data;online disaggregation method;task disaggregation;dynamical framework disaggregation;disaggregation method;using online disaggregation;disaggregation electric electric"}, "834d68b9befcc6c68415b460b33435a1822799fb": {"ta_keywords": "argumentation mining user;argumentation mining;analyzing people argumentation;goal argumentation mining;generated web discourse;web discourse design;people argumentation;argumentation article state;argumentation;web discourse;web discourse ii;argumentation article;people argumentation article;discourse design;discourse design methods;goal argumentation;discourse;discourse ii bridge;discourse ii;mining user generated;introductionthe goal argumentation;noisy user generated;user generated web;mining user;web data challenges;user generated;analyzing people;capable analyzing;generated web;actual web data", "pdf_keywords": "argumentation mining focus;argumentation mining new;argumentation mining conducted;argumentation mining;mining argumentation;use argumentation mining;argumentation mining research;argumentation mining practical;argumentation mining field;annotated argumentation mining;argumentation mining application;analyze argumentation web;called argumentation mining;mining argumentation mining;argumentation mining legal;argumentation mining development;argumentation mining arguement;status argumentation mining;argumentation mining approach;argumentation mining user;argumentation web;argumentation mining approaches;argumentation mining argumentation;argumentthe argumentation mining;practicalthe argumentation mining;corpus argumentation mining;argumental mining;arguments argumentation mining;approach argumentation mining;argumentation mining human"}, "972a74968d2522908b06c5bd1e26266194c5a9ee": {"ta_keywords": "sentence decontextualization methodswe;sentence decontextualization;sentence decontextualization taking;thewikipedia corpus;decontextualization taking sentence;thewikipedia corpus use;preserving meaning annotation;data thewikipedia corpus;introductiondecontextualization making;problem sentence decontextualization;introductiondecontextualization;introductiondecontextualization making senstences;decontextualization;decontextualization methodswe;decontextualization taking;data thewikipedia;collect data thewikipedia;annotation;thewikipedia;decontextualization methodswe isolate;meaning annotation procedure;annotation procedure;corpus;meaning annotation;corpus use;sentence context rewriting;corpus use data;interpretable context preserving;taking sentence context;rewriting interpretable context", "pdf_keywords": "decontextualization question answering;decontextualized sentence generation;decontextualizing sentences using;answering using decontextualized;decontextualizing sentences;decontextualized sentences corresponding;decontextualized sentences providing;decontextualized sentences;sentences using decontextualized;decontextualized sentences use;sentences decontextualized using;sentences decontextualized;decontextualized sentence editing;using decontextualized sentences;use decontextualized sentences;method decontextualizing sentences;methods decontextualizing sentences;sentence decontextualization;sentences use decontextualized;decontextualizing sentences inwikipedia;sentence decontextualization taking;decontextualization text challenging;dialogue agents summarization;decontextualizing sentence based;corpus decontextualized sentences;original sentences decontextualized;decontextualized sentences experiment"}, "6b387d18bae978202af501c4795f37a0c73781a6": {"ta_keywords": "proximal extragradient method;extragradient method;hybrid proximal extragradient;extragradient method 2013;proximal extragradient;svaiter method optimal;smooth convex optimization;monteiro svaiter method;accelerated hybrid proximal;convex optimization;convex optimization problems;methods using gradient;extragradient;method optimal;svaiter method;newton method;newton method used;approximate solution auxiliary;gradient andhessian evaluations;using gradient andhessian;svaiter accelerated hybrid;hybrid proximal;step newton method;gradient andhessian;method optimal respect;monteiro svaiter accelerated;iteration approximate solution;optimization problems class;methodsthe monteiro svaiter;accelerated hybrid", "pdf_keywords": ""}, "2bb1e1a5b9a16f6828fe94736cea5dab264533a6": {"ta_keywords": "semantic transparency;semantic transparency study;concept semantic transparency;abstract language models;language models trained;semantic;abstract language;language models;transparency study assertion;nonprogrammable tasks;abilities ungrounded systems;results nonprogrammable tasks;unprecedented results nonprogrammable;understand raw text;trained billions tokens;transparency;results nonprogrammable;concept semantic;transparency study;nonprogrammable;introduction abstract language;nonprogrammable tasks success;ungrounded systems;raw text access;tokens;language;models trained billions;text;investigate abilities ungrounded;raw text", "pdf_keywords": "emulate semantics;possible emulate semantics;denotations assertions;assertions enable semantic;emulate meaning assertion;transparent semantics;language understanding assertions;assertions emulation;semantic emulation;strongly transparent semantics;extend assertions emulation;emulate semantics ofthe;generalized notion assertions;semantic emulation languages;enable semantic emulation;notion semantic transparency;assertions allow sense;emulate understanding language;assertion queries languages;learn semantics;transparent semantics theorem;assertions emulation apply;modal denotations assertions;notion assertions;semantic transparency;understanding assertions provide;semantics theorem computable;computable linguistic understanding;ability learn semantics;understanding language emulated"}, "96b32b204a62777bef66eea595de2c47b4e9d6e9": {"ta_keywords": "model learn representations;learn representations gm;gm representation predictable;improving networks sample;networks sample;networks sample performance;learn representations;generalization data;generalization data sets;model learn;domain generalization data;networks;improving networks;representations gm representation;representation predictable;gm representation method;model representation;gm representation;techniques improving networks;representations gm;projecting model representation;orthogonal gm representation;representation predictable second;domain generalization;standard domain generalization;generalization;built reverse gradient;representations;reverse gradient method;orthogonal gm", "pdf_keywords": "learning image;learning textural information;image classifier generalize;learning image classifier;learning textural;model learning image;deep learning;textural information network;classifying textural;classifying textural patterns;method deep learning;new neural;domain generalization data;domains visual recognition;generalization data;photo sketch neural;textural information synthetic;neural important tool;approach learning textural;image classifier;deep learning hex;captures textural information;differentiable neural network;identifiers available training;new differentiable neural;classifier generalize;neural;recognition;new neural network;building block neural"}, "636611068825cb4b7bdab6ad16ef415adf4fb96c": {"ta_keywords": "multi domain learning;multidomain learning algorithms;multidomain learning;domain learning approaches;methodsfirst multidomain learning;domain learning improvements;domain learning;learning domains;learning domains matter;dmain learning domains;practice multi domain;learning algorithms multi;multi domain;ensemble learning;ensemble learning effects;algorithms multi domain;result ensemble learning;ensemble learning algorithms;domain specific class;resemble ensemble learning;class label biases;multi domain settings;learning approaches;domains;learning algorithms;domain specific;algorithms resemble ensemble;multidomain;existing multi domain;domains matter", "pdf_keywords": ""}, "6f902b8128b218563b276c1ebff46ef668dcb185": {"ta_keywords": "introductionvehicular mobile crowd;incentivize drivers agents;sensors vehicles taxis;mobile crowd sensing;crowd sensing;incentive mechanism;drivers agents collect;data multiple vehicles;crowd sensing fast;incentive mechanism incentivize;agents collect data;design incentive mechanism;incentivize agents;mechanism incentivize agents;vehicles taxis;incentivize drivers;mechanisms incentivize drivers;drivers agents;payment mechanisms incentivize;sensors vehicles;introductionvehicular mobile;mobile crowd;incentive;design incentive;mounting sensors vehicles;mechanism incentivize;collect data;payment mechanisms;multiple vehicles;taxis aimto design", "pdf_keywords": "maximized crowdsourcer agents;crowd sourcer incentive;crowdsourcer agents;incentive mechanism crowd;maximized crowdsourcer;crowdsource tasks truthfully;tasks maximized crowdsourcer;sacrificing utility crowdsource;utility crowdsource;utility crowdsource tasks;mechanism crowd sensing;crowdsourcer agents report;crowdsource tasks;crowdsensing systems;fair incentive mechanism;novel incentive mechanism;mobile crowd sensing;crowdsensing systems discuss;need incentive mechanism;challenge collusion agents;sourcer incentive mechanism;crowd sensing;engage arbitrary collusions;crowdsource tasks proposed;crowdsourcer;crowd sense algorithm;collusions agents;introduce crowd sourcer;mechanism fair incentive;mobile crowdsensing"}, "7650d705b85dc399112a5b6a79e9c6f81c7c6146": {"ta_keywords": "extractive question answering;supervised question answering;question answering;large annotated corpora;question answering challenging;annotated corpora large;specific annotated corpora;annotated corpora;annotated corpora limited;effective semi supervised;semi supervised question;semi supervised;question answering hinged;large annotated;models task extractive;answering challenging task;answering challenging;task extractive;corpora large domain;domain specific annotated;learning models task;corpora limited;specific annotated;corpora;availability large annotated;corpora large;task extractive question;deep learning;annotated;answering", "pdf_keywords": "extractive question answering;question answering;question answering competition;models text comprehension;question answering question;answering question generation;learning triviaq using;large annotated corpora;question answering qrs;models reading comprehension;generating natural questions;supervised learning triviaq;generate auxiliary questions;indexing question answering;question generation;question generation dual;learning triviaq;model reading comprehension;annotated corpora large;annotated corpora;large scale semantic;specific annotated corpora;role question answering;text comprehension;annotated corpora limited;questions passages reinforcement;large annotated;models task extractive;answering competition;text comprehension bioasq"}, "58834a447c749758e7f57498c6dd88a281af41a0": {"ta_keywords": "training constituency parsers;parsers exploration;constituency parsers exploration;parser transition explore;parser transition;given parser transition;constituency parsers;parsers exploration custom;parsers;parser agnostic alternative;parser;parser agnostic;given parser;defined given parser;dynamical oracles provide;dynamical oracles;gradient method parser;introduction dynamical oracles;method parser agnostic;method parser;oracles provide strong;supervision training constituency;oracles;oracles provide;strong supervision training;using policy gradient;policy gradient;training constituency;strong supervision;provide strong supervision", "pdf_keywords": "training parser;training parser model;based parsers trained;parsers trained english;transition based parsers;parsers trained;approach training parser;oracles constituency parsing;constituency parsing;language policy gradient;train given parser;treebank;constituency parsing increasing;french treebank;based parsers;parser agnostic alternative;parser model;gradient method parser;parser given language;parsers;dynamic oracle training;parsing increasing difficult;china treebank;parser model able;treebank version;tb french treebank;dynamic oracles constituency;treebank version apply;parser;parsing"}, "1fa32503bce4f01ab2ccb65dedd374310c488fe8": {"ta_keywords": "effects outcomes auditing;incentive effects outcomes;effects incentive effects;incentive effects;outcomes auditing metrics;effects incentive;interaction effects incentive;compliance employers result;outcomes auditing;auditing metrics propose;model employment market;compliance employers;auditing metrics;employment market leveraging;partial compliance employers;employment market;market leveraging simulation;effects outcomes;model employment;incentive;auditing;simple model employment;employers result;proportional progress compliance;compliance;employers result far;employers;equilibrium partial compliance;auditing metrics key;impact interaction effects", "pdf_keywords": "algorithmic fairness scholarship;algorithmic fairness;issue algorithmic fairness;fairness measures implementation;scenario employers fairness;employers fairness role;fairness measures;employers fairness;unfairness implementation ethical;fairness role unfairness;models study unfairness;ethical impact algorithmsthis;importance fairness measures;risk discrimination economics;unfairness implementation;ethical impact algorithms;model discrimination employment;fairness accountability;discrimination economics;algorithmic decisions individual;fairness accountability transparency;algorithms allocate decisions;fairness role;ethical measures;performance bias algorithms;discrimination employment based;algorithmic decisions;algorithms decision makers;established importance fairness;employers associated ethical"}, "6ea353ada2b89763f58d8068a74b2e6def526948": {"ta_keywords": "annotated corpora biological;annotated corpora;corpora annotated corpora;corpora annotated;annotated corpora annotated;natural language processing;natural language;availability annotated corpora;corpora;corpora biological entities;corpora biological;reviewing biomedical litera;ture natural language;language processing;drug interactions;drug drug interactions;available natural language;language processing techniques;drug drug;management drug drug;biomedical litera ture;biomedical litera;annotated;drug interactions critical;drug;management drug;reviewing biomedical;biological entities relationships;biomedical;professionals reviewing biomedical", "pdf_keywords": "corpora annotation drug;annotated corpus drug;corpus annotated pharmacological;annotation drug interactions;di corpus annotated;drugthe corpus;annotation drugs;annotation drug;annotated corpus;biomedical texts corpus;annotation drugs interactions;corpus annotated;followed annotation drugs;drug interactions annotated;manually annotated corpus;medline corpus;corpus drug interactions;di medline corpus;drug databases semistructured;corpus contains drug;annotated corpora;interactions drugbank database;corpora annotated corpora;di corpus;drugbank database medical;corpus annotated large;234 drugthe corpus;annotated corpora biological;gold standard corpus;information describing drug"}, "c507ad8b7bec5d29da7cf0ee92e2bf4361a5c92f": {"ta_keywords": "deep reinforcement learning;vision applications quantization;discovering quantization levels;neural networks dns;discovering quantization;deep reinforcement;quantization;quantization significantly reduce;applications quantization significantly;quantization significantly;purposedeep neural networks;quantization levels;end deep reinforcement;quantization levels end;applications quantization;learning framework releq;purposedeep neural;reinforcement learning;neural networks;dn computation storage;computation resource inference;inference tasks computer;reduce dn computation;neural;process discovering quantization;learning;inference tasks;dn computation;reinforcement learning framework;networks", "pdf_keywords": ""}, "2899eb53cddf050e3a34f07bbc0bc0ee7907d5d0": {"ta_keywords": "additions word segmentation;speech tagging;addition training corpus;speech tagging problem;annotated sentence addition;training corpus better;adding annotated sentences;word segmentation;annotated sentences training;language resource additions;language resource addition;sentence addition training;training corpus;problem speech tagging;corpus better;sentences training corpus;addition dictionary corpus;annotated sentences;training corpus experimental;word segmentation problem;second adding annotated;adding annotated;corpus;sentences training;segmentation problem speech;annotated sentence;resource additions word;dictionary corpus;corpus experimental;corpus experimental results", "pdf_keywords": ""}, "626f8a50a7bd24d869f25bddb6fbaa59b090268c": {"ta_keywords": "pattern recognition methods;pattern recognition device;pattern recognition;pattern recognition apparatus;recognition device pattern;provided pattern recognition;introduction pattern recognition;device pattern recognition;recognition methods available;recognition methods;recognition device;recognition apparatus generating;recognition performance comprising;recognition apparatus;recognition performance;recognition;systems improve recognition;comprising discriminative training;performance comprising discriminative;discriminative training;pattern;improve recognition performance;discriminative training session;available provided pattern;device pattern;improve recognition;provided pattern;comprising discriminative;discriminative;systems combination plurality", "pdf_keywords": ""}, "3681456f29398e42cc2baafb0b72d166070a3cf1": {"ta_keywords": "policy gradient sequential;gradient sequential sequential;quadratic dynamicmic games;quadratic dynamics games;sequential algorithms linear;gradient sequential;free sequential algorithms;sequential algorithms;sequential sequential algorithms;games policy gradient;convergence policy gradient;sequential algorithms sequential;policy gradient based;algorithms sequential zero;free sequential;policy gradient;algorithms sequential;projection free sequential;sequential;sequential sequential;sequential zero;sequential zero sum;gradient descent ascent;dynamics games policy;linear quadratic dynamics;dynamicmic games methodswe;dynamicmic games;natural gradient descent;backgroundglobal convergence policy;dynamics games", "pdf_keywords": "generalization dynamodynamic games;quadratic dynamics games;dynamodynamic games quasi;formulation lq games;qq dynamodynamic games;sequenceential alq games;policy optimization provably;policy optimization algorithms;sublinear convergence nash;converges nash equilibria;convergence nash equilibrium;dynamics differential games;convergence nash;lq dynamic games;dynamodynamic games;linear quadratic games;sequential algorithms players;dynamodynamic games global;game lq optimal;policy optimization;games global quadratic;dynamics games methodswe;thethe policy optimization;converges nash;principle dynamical game;policy lq game;policy algorithms;dynamical game cost;sequential laq dynamic;quadratic games"}, "a711e02f85fa52c15df0a830a8ba88df2c3928ec": {"ta_keywords": "syndromic mutation adolescent;mutation adolescent young;mutation adolescent;syndromic mutation;occurrence syndromic mutation;adults described adolescent;mutation;described adolescent young;adolescent young adults;described adolescent;occurrence syndromic;adolescent young;young adults described;adolescent;syndromic;evaluating occurrence syndromic;adults described discussed;young adults;adults described;young;adults;evaluating occurrence;method evaluating occurrence;occurrence;new;described;described discussed;discussed;evaluating;method evaluating", "pdf_keywords": ""}, "4c67c129dab9805ab248407b77a6d542c2e40d41": {"ta_keywords": "backgroundadversarial crowddsourcing robust;crowddsourcing robust rank;backgroundadversarial crowddsourcing;rank matrix completion;matrix completion;robust rank matrix;crowddsourcing robust;reconstructing rank matrix;backgroundadversarial;robust rank;matrix completion objectivewe;reconstructing rank;rank matrix revealed;problem reconstructing rank;matrix revealed subset;alternating minimization;combining alternating minimization;minimization extreme;alternating minimization extreme;crowddsourcing;minimization;matrix revealed;rank matrix;reconstructing;minimization extreme value;extreme value filtering;revealed subset entries;robust;consider problem reconstructing;revealed subset", "pdf_keywords": "matrix dense crowdsourcing;robust rank matrix;rank matrix completion;algorithm robust rank;reconstructing rank matrix;robust rank;prediction errors crowdsourcing;reconstructing rank;crowdsourcing dataset highly;matrix completion propose;rank matrix revealed;propose rank matrix;factorization robust rank;matrix completion algorithm;prediction accuracy crowdsourcing;prediction results crowdsourcing;matrix completion problem;matrix completion;method robust rank;learning crowds;problem reconstructing rank;matrix factorization robust;crowdsourcing problem recover;matrix completion method;crowdsourcing datasets;rank matrix factorization;crowdsourcing dataset;crowdsourcing datasets method;learning crowdsourcing problem;human learning crowds"}, "840fabc2a7773e1bd771f152f76210b2ea5845b9": {"ta_keywords": "stochastic beam search;beam search stochastic;search stochastic;search stochastic process;stochastic beam;poisson stochastic beam;generate sequence models;sequence models;sequence models propose;beam search;poisson stochastic;turning beam search;conditional poisson stochastic;stochastic;stochastic process;generate sequence;beam search results;process conditional poisson;beam search methodswe;iteration samplek candidates;stochastic process conditional;used generate sequence;sequence;backgroundreactions used generate;iteration samplek;beam;search;conditional poisson;poisson;samplek candidates replacement", "pdf_keywords": "stochastic beam search;stochastic decoding strategies;stochastic decoding;using stochastic decoding;poisson stochastic beam;stochastic beam;method stochastic beam;analyzing stochastic beams;staochastic beam search;estimator stochastic beam;stochastic beams using;stochastic beams;efficiently decoding;estimate decoding;able efficiently decoding;beam search sampling;estimate decoding time;model estimate decoding;beam search estimator;decoding strategies;poisson stochastic;beam search algorithm;sequences possible beams;conditional poisson stochastic;poisson staochastic beam;efficiently decoding time;decoding strategy sequence;efficient sequence model;method analyzing stochastic;beam search"}, "509b42fc150a057a64c4608f64e779ef04fdff47": {"ta_keywords": "entity recognition results;named entity recognition;entity recognition;backgroundnatural language processing;language processing models;predictions text data;language processing;text evaluation results;named entity;use temporally diverse;text data evolves;backgroundnatural language;text evaluation;diverse training data;described text evaluation;predictions text;timestamp document;temporally diverse training;language use information;taking timestamp document;task named entity;better use temporally;text data;temporally diverse;make predictions text;changes language use;training data;changes language;entity;training data focus", "pdf_keywords": ""}, "11c6d0851152b6bec34726be40d90bea8d8a90f0": {"ta_keywords": "crowdsourced annotations;crowdsourced annotations methods;model crowdsourced annotations;annotation noise;annotation noise common;cost annotation quality;annotation quality annotators;annotation quality;decompose annotation noise;quality annotators;quality annotators varies;low cost annotation;quality model crowdsourced;model crowdsourced;crowdsourced;annotations methods;annotations;cost annotation;annotators;annotation;annotators varies;annotators varies considerably;annotations methods work;decompose annotation;large amounts labeled;labeled data;labeled data low;amounts labeled data;labeled;crowdfunding", "pdf_keywords": "annotators crowdsourcing datasets;model crowdsourced annotations;modeling annotators crowdsourced;annotators crowdsourced data;annotators crowdsourced;crowds noisy annotations;annotators crowdsourcing;crowds confusion annotators;crowdsourced annotations;crowdsourced annotations methodsin;confusion models crowdsourcing;accurately model crowdsourced;learning crowds modeling;confusion model crowdsourced;modeling crowdsourced data;learning crowds noisy;model crowdsourced data;models crowdsourcing;data crowdsourced;model crowdsourced;learning crowds;number annotators crowdsourcing;method learning crowds;modeling crowdsourced;crowdsourcing neural;crowdsourced data;crowdsourcing neural information;crowdsourcing datasets;models crowdsourcing crowds;crowdsourcing crowds"}, "6c520d983923dbe1e437c01086424fdcdd8f430a": {"ta_keywords": "parametric speech synthesis;speech synthesis;speech synthesis methods;speech synthesis including;statistical parametric speech;speech voice conversion;voice conversion;parametric speech;voice conversion results;text speech voice;modulation spectrum statistical;modify modulation spectrum;synthesis including text;text speech;spectrum statistical parametric;speech voice;modify modulation;filters modify modulation;based modulation spectrum;including text speech;quality statistical parametric;utterance level post;synthesis methods;modulation spectrum;voice;based modulation;approaches based modulation;proposed utterance level;synthesis methods paper;synthesis", "pdf_keywords": ""}, "5f46d8e18138fe572b6fae897475a2ad645a3e1a": {"ta_keywords": "models automatic speech;language model fusion;external language models;automatic speech recognition;speech recognition ar;speech recognition;automatic speech;end automatic speech;speech recognition aimto;language models;language model;transducer rnn;language models ls;network transducer rnn;integrating external language;neural network transducer;approach language model;model fusion end;external language;transducer rnn anrrn;rnn anrrn model;end models automatic;model fusion;end models;end end models;models automatic;methodsarecurrent neural network;models ls end;model trained;anrrn model trained", "pdf_keywords": "models automatic speech;external language models;speech recognition ar;automatic speech;speech recognition;automatic speech recognition;approach speech recognition;sequences text audio;conversational speech recognition;model conversational speech;speech recognition neural;rn based acoustic;language models;applied speech recognition;approach acoustic domain;acoustic domain domain;speech recognition article;decoding audio;acoustic feature vector;sequence model conversational;language models ls;domain rn based;decoding audio data;speech recognition method;text audio;decoding acoustic;data acoustic based;integrating external language;external linguistic end;limited data acoustic"}, "2ce3428ba8777c723b9b12e9f8eaeb2c87a5a793": {"ta_keywords": "trustworthiness model prediction;learning methods;annotation rationales employ;annotation rationales;differentiable training framework;approaches reinforcement learning;costly annotation rationales;model prediction;trustworthiness model;training framework;reinforcement learning methods;learning;supervision costly annotation;learning methods require;identifying searching textual;prediction essential differentiating;introductionevaluating trustworthiness model;textual;propose differentiable training;searching textual;target label known;introductionevaluating trustworthiness;reinforcement learning;trustworthiness;searching textual spans;textual spans;prediction;differentiable training;model prediction essential;textual spans determine", "pdf_keywords": "based prediction sentences;sentences model prediction;sentences based models;prediction sentences model;sentences model based;text based reasoning;prediction sentences;sentiment novel task;model prediction sentences;prediction sentences significantly;current knowledge neural;knowledge neural;knowledge neural networks;sentences model;sentences based;networks natural language;natural language processing;sentences model used;natural language learning;reasoning tasks supervision;reasoning tasks;different sentences model;based reasoning task;softmax;complex neural networks;text based;natural language;identifying textual;inthe prediction sentences;create modelsthe neural"}, "cd1d915604826e5fb0ba2bbcdf8479a9b90fb289": {"ta_keywords": "thoracic trauma;history thoracic trauma;adolescent history thoracic;trauma diagnosed thoracic;thoracic trauma diagnosed;thoracic trauma patient;patient history thoracic;thoracic thoracic;thoracic thoracic thoracic;thoracic;diagnosed thoracic;random lattice adolescent;lattice adolescent history;history thoracic;diagnosed thoracic thoracic;trauma patient history;trauma patient;lattice adolescent;trauma diagnosed;trauma;adolescent history;placement random lattice;patient history;random lattice;adolescent;successful placement random;placement random;lattice;successful placement;diagnosed", "pdf_keywords": "lattice path optimal;wireless network random;lattice path optimize;impromptu deployment wireless;deployment random lattice;path optimal policies;optimal place relay;distance optimal allocation;heuristic close optimal;optimal sequential decision;random lattice path;deployment wireless network;network random lattice;relay deployment optimal;deployment wireless wireless;distance optimally chosen;relays random lattice;path optimal;distance threshold policy;wireless networks;deployment wireless;distance based heuristic;optimal policies threshold;wireless network propose;wireless wireless networks;threshold policies placement;path formulated optimal;cost markov decision;optimal policies;wireless networks commonly"}, "a425a11b9b249cb768d0f54d4a32f4f1d007e279": {"ta_keywords": "practice online lear;online lear ners;online lear;pass online learning;comparable batch learners;batch learners;online learning training;lear ners frequently;batch learning;batch learning methods;online learning;batch learners methodswe;passes training data;lear ners;training single pass;learning training single;introductiononline learning methods;lear;learning training;practice online;learning methods practice;introductiononline learning;pass online;multiple passes training;learning methods;passes training;learning methods typically;le pass online;methods practice online;accuracy comparable batch", "pdf_keywords": ""}, "2d71fb62c71e49479c1b6ce832ee1bb88df20556": {"ta_keywords": "description logics computing;description logics;introduction description logics;description logics popular;operation description logics;logics computing common;set commonalities descriptions;knowledge representation reasoning;subsumer pair descriptions;computation common subsumer;computing common subsumer;descriptions methods operation;knowledge representation;common subsumer;commonalities descriptions results;pair descriptions methods;formalism knowledge representation;logics computing;commonalities descriptions;common subsumer pair;representation reasoning;descriptions methods;logics popular formalism;set commonalities;descriptions results arguing;computing common;subsumer pair;relating computation common;logics;pair descriptions", "pdf_keywords": ""}, "6fa85c46ea68c754ef903edc70058ba525f1fc4d": {"ta_keywords": "adaptive adaptive systems;adaptive adaptive adaptive;adaptive adaptive;adaptive systems;adaptive systems article;adaptive;development adaptive adaptive;development adaptive;networks development adaptive;neural networks development;essential development adaptive;neural networks;ability learn;ability learn essential;neural;systems;role neural networks;learn;systems article;networks;learn essential;systems article discuss;learn essential development;role neural;discuss role neural;networks development;development;ability;essential development;article", "pdf_keywords": ""}, "0c89b1ec80de46222ed7efc6261c03e52a1e2c54": {"ta_keywords": "describing unknown phrases;help interpretation machines;interpretation machines;contexts methods reading;context consult dictionaryaries;interpretation machines help;global contexts;learning describing unknown;unfamiliar words phrases;global contexts methods;contexts;global context;contexts methods;local global contexts;learning describing;phrases local global;phrases humans figure;global context help;local context;dictionaryaries definitions search;introduction learning describing;phrases local;unknown phrases local;words phrases humans;immediate local context;local context consult;definitions search;consult dictionaryaries definitions;dictionaryaries;context", "pdf_keywords": ""}, "b2b9b0d7afd85c5d708b79a61d9a000c6c906d8c": {"ta_keywords": "similarity measures parsed;walk based similarity;word similarity measure;parsed text corpus;measures parsed text;word similarity;similarity measure graph;based similarity measures;similarity measures;syntactic relations graph;specific word similarity;text corpus;text corpus instance;similarity measure;consider parsed text;learning graphical walk;corpus;relations graph walks;nodes represent words;graphical walk based;graph walks;represent words weighted;words weighted directed;based similarity;parsed text;parsed text methods;similarity;corpus instance;corpus instance labelled;graph walks combined", "pdf_keywords": ""}, "3042bc348d6cc7959cd574756f720e5afad236de": {"ta_keywords": "deep drawing paperboard;paperboard deep drawing;drawing paperboard trays;drawing paperboard;paperboard trays methods;paperboard used paper;paperboard process parameters;factor paperboard deep;paperboard process;paperboard trays;paperboard deep;type paperboard process;study type paperboard;paperboard used;type paperboard;influence factor paperboard;paperboard;walls paperboard used;factor paperboard;walls paperboard;indented walls paperboard;paper anisotropic mechanical;used paper anisotropic;paper anisotropic;tray shallow rectangle;suitable deep drawing;used paper;tray shallow;methods tray shallow;deep drawing experiments", "pdf_keywords": ""}, "39c5cfc0ff6660a17364cb4af1eb071d6efa463d": {"ta_keywords": "score eliciting judgements;compare score eliciting;comparative ordinal measurements;scoring cardinal comparative;amazon mechanical turk;compare score;mechanical turk;better compare score;eliciting judgements humans;score eliciting;eliciting judgements;selection measurement;judgements humans unknown;comparative ordinal;judgements humans;selection measurement scheme;relative merits choice;guidelines selection measurement;choice providing empirical;ordinal measurements;study relative merits;direct scoring;making direct scoring;relative merits;merits choice;direct scoring cardinal;cardinal comparative ordinal;comparative;ordinal;empirical theoretical guidelines", "pdf_keywords": "ordinal data accurate;mechanical turk variety;amazon mechanical turk;comparative ordinal measurements;mechanical turk;ordinal measurements estimates;ordinal measurements accurate;comparison data crowdsourcing;accurate ordinal;accurate ordinal measurements;performance ordinal;performance ordinal ordinal;argue ordinal measurements;compare performance ordinal;ordinal evaluations;measurements accurate ordinal;ordinal evaluations typically;data ordinal noise;measurements argue ordinal;ordinal measurements argue;ordinal data compare;ordinal data typically;ordinal methods evaluation;ordinal data provides;higher ordinal data;data collection ordinal;sample noise ordinal;ordinal data;estimates produced ordinal;required ordinal evaluations"}, "67b29c3fe6f110125a8892e8ed128d20b23957ea": {"ta_keywords": "lingual entity linking;languagewikipedia bilingual entity;introductioncross lingual entity;lingual entity;entity maps multilingual;bilingual entity maps;entity linking xe;languagewikipedia bilingual;source languagewikipedia bilingual;entities source language;entity linking;languagewikipedia;bilingual entity;source languagewikipedia;introductioncross lingual;multilingual embeddings available;resources source languagewikipedia;multilingual embeddings;maps multilingual embeddings;named entities source;english knowledge base;entities source;maps multilingual;named entities;lingual;linking xe;multilingual;entity maps;knowledge base xe;entities", "pdf_keywords": "lingual entity linking;entity linking xel;entity linking;entities source language;entity linking fact;disambiguation extremely low;cross lingual entity;lingual entity;disambiguation demonstrate highly;resource cross lingual;disambiguation method;linking neural entities;disambiguation method large;base kb aswikipedia;named entities source;entity linking aim;low resource language;generation disambiguation demonstrate;kb aswikipedia objectiveto;aswikipedia objectiveto;low resource languages;kb aswikipedia;generation disambiguation;disambiguation;english knowledge base;entities source;named entities;insensitive disambiguation method;generation disambiguation make;propose improvements entitywe"}, "2a64da1ed300e49f2d665312146c8bb2f66920b7": {"ta_keywords": "statistical machine translation;translation sm optimization;maximize translation accuracy;machine translation;machine translation sm;parameters maximize translation;translation accuracy fundamental;translation accuracy;maximize translation;batch online optimization;optimization sm;online optimization;translation sm;sm optimization parameters;sm optimization;algorithms use batch;research optimization sm;optimization parameters;online optimization cover;use batch online;optimization sm starting;statistical machine;translation;optimization;optimization algorithms;batch online;optimization parameters maximize;use batch;batch;backgroundin statistical machine", "pdf_keywords": ""}, "d530a007ae0493ef6a8167c25bd007104623c504": {"ta_keywords": "compilation process decompilers;decompiler common tools;tools examining binaries;decompilers reconstruct information;decompiler;decompilers reconstruct;examining binaries corresponding;examining binaries;introductionthe decompiler;decompilers;introductionthe decompiler common;process decompilers;process decompilers reconstruct;decompiler common;code transforms binaries;binaries corresponding source;binaries corresponding;code reversing compilation;reversing compilation process;binaries high level;binaries;reversing compilation;corresponding source code;source code transforms;variable names known;transforms binaries;code understandability resul;increase code understandability;code reversing;compilation process structure", "pdf_keywords": "names decompiled code;variable names decompiled;variables decompiled code;generating decompiled code;code idiomatic decompiler;generate decompiled code;names variables decompiled;based decompiled code;decompiled code ability;decompiled code efficiently;propose decompiled identifier;compilation process decompilers;variables decompiled;decompiled identifier decompiler;code compiled;identifier decompiler;decompiled code;source code compiled;identifier decompiler effective;decompiled code decoding;decompiled code important;code efficiently predict;decompiled identifier;names source code;predict program properties;variable names source;meaningful variable names;code decoding;binaries decompiler;decompiled code use"}, "bc1bf0a21d7838ec167e77c76163afc1f5f76c3d": {"ta_keywords": "methodsanalysis electroencephalograms;electroencephalograms;electroencephalogram;multi channel electroencephalogram;channel electroencephalogram;matrices methodsanalysis electroencephalograms;background noise removal;backgroundremoving noise event;methodsanalysis electroencephalograms egg;noise removal;noise event related;electroencephalograms egg usually;noise removal single;noise event;electroencephalograms egg;backgroundremoving noise;potentials erps recorded;background noise;event related potentials;method background noise;related potentials erps;potentials erps;potentials using probabilistic;erps recorded;erps recorded multi;grouped covariance;noise;suffers variety noises;noises paper propose;probabilistic generative model", "pdf_keywords": ""}, "4c9f20ce99f9b93527fd76ec04a44fcef9082005": {"ta_keywords": "fairness interactive recommendation;recommendation reinforcement learning;recommendation reinforcement;interactive recommendation reinforcement;accuracy fairness interactive;fairness interactive;fairness irs methodsuser;fairrec dynamically;introductionbalancing accuracy fairness;accuracy fairness irs;implemented cumulative reward;fairrec dynamically maintain;preferences fairness status;preferences fairness;accuracy fairness;based framework fairrec;interactive recommendation;methodsuser preferences fairness;cumulative reward propose;balance accuracy fairness;fairness irs;cumulative reward;framework fairrec;framework fairrec dynamically;fairness status;reward propose reinforcement;fairness status jointly;fairrec;propose reinforcement learning;fairness", "pdf_keywords": "fairness aware recommenders;fairness aware recommendation;fairness generate recommendations;fairrec improve recommendation;fairness concern recommender;automatic automatic recommendation;reinforcement learning fairrec;automatic recommendation;automatic recommendation irs;recommenders interactiverecommender systems;recommendations accurate fair;recommenders interactiverecommender;learning fairrec automatic;traditional recommenders interactiverecommender;representation generate recommendations;improve recommendation accuracy;aware recommendation frame;fairness user best;aware recommendation;proportional fairness user;reinforcement learning irs;improve recommendation;generate recommendations accurate;user propose fairrec;improve quality recommendation;fairrec automatic;propose reinforcementthe fairness;user preferences fairness;fairness aware based;generate recommendations"}, "cb153d8469ac466606032ea457b934bc61ae86ae": {"ta_keywords": "emotion fake news;exploiting emotions news;publisher emotion fake;detecting fake news;fake news evokes;people emotions news;emotion fake;emotions news contents;emotions news;emotions news comments;news evokes;publisher emotion;fake news online;news evokes high;exploiting emotions;emotional signals existing;publishers publisher emotion;crowd social emotion;detecting fake;online leveraging emotional;focus exploiting emotions;people emotions;emotions people emotions;arousal activating emotions;emotional signals;leveraging emotional signals;news online leveraging;activating emotions people;news contents conveyed;activating emotions", "pdf_keywords": ""}, "029fa34b291c3f60b8a00cdf386e6048d45c394d": {"ta_keywords": "spectral clustering methods;alternative spectral clustering;spectral clustering;node clustering methods;mixed membership clustering;node clustering;membership clustering methods;membership clustering;clustering methods elegant;based node clustering;membership alternative spectral;clustering methods;clustering methods allow;background spectral clustering;clustering methods results;clustering methods approach;clustering;node centric representation;graph based node;effective graph based;representation edge centric;graph based;alternative spectral;centric representation edge;spectral;representation edge;node centric;effective graph;competitive mixed membership;mixed membership alternative", "pdf_keywords": ""}, "04b876e95ac3e4754c8f0c8a9355e7acc3dc70b9": {"ta_keywords": "language resource addition;addition ja morphological;segmentation speech tagging;ja morphological analysis;training corpus;dictionaries corpora methods;study dictionaries corpora;speech tagging;dictionaries corpora;corpus;sentences training corpus;morphological analysis;ja morphological;word segmentation speech;word segmentation;corpora methods language;speech tagging strategy;training corpus experimental;corpora methods;corpora;corpus experimental results;annotated sentences training;corpus experimental;adding annotated sentences;language resource;morphological analysis joint;resource addition ja;morphological;segmentation speech;task word segmentation", "pdf_keywords": ""}, "7393d2618c7478d937112865458862e8d5f10475": {"ta_keywords": "domain reasoning template;crossdomain reasoning methodswe;backgroundcross domain reasoning;reasoning template;reasoning template filling;crossdomain reasoning present;domain reasoning;perform crossdomain reasoning;crossdomain reasoning resultswe;crossdomain reasoning;sequence sequence models;reasoning methodswe explore;sequence models perform;models perform crossdomain;reasoning present prompt;reasoning methodswe;sequence models;reasoning resultswe;template filling approach;backgroundcross domain;prompt template filling;reasoning resultswe present;prompt template;reasoning present;crossdomain;perform crossdomain;enable sequence sequence;present prompt template;template filling;filling approach", "pdf_keywords": "domain reasoning prompting;domain reasoning language;models commonsense reasoning;domain reasoning cross;crossdomain reasoning commonsense;domain reasoning uses;reasoning commonsense;schema reasoning commonsense;domain reasoning ability;domain reasoning terms;commonsense reasoning;reasoning language models;domain reasoning use;domain reasoning focus;reasoning commonsense health;domain reasoning task;reasoning uses prompts;reasoning task propose;cross domain reasoning;reasoning use pretrained;reasoning language;reasoning cross domain;reasoning prompting;crossdomain reasoning methodswe;domain reasoning future;domain reasoning;commonsense reasoning important;reasoning uses;usecase reasoning commonsense;explore crossdomain reasoning"}, "7137a842d496a1a5581db31ad946fa0c0827e663": {"ta_keywords": "mechanism disease poorly;new models development;mechanism disease;models development;models development new;development new models;underlying mechanism disease;new models;disease poorly understood;disease;models;disease poorly;development new approaches;new approaches development;new approaches;approaches development new;approaches development;development new;understood development new;poorly understood development;mechanism;underlying mechanism;development;understood development;approaches;new;poorly understood;poorly;understood;underlying", "pdf_keywords": ""}, "f735f5f55cbc5a9d372ea1cd9b4e81d35f043a00": {"ta_keywords": "model pairwise comparisons;pairwise comparisons;pairwise comparisons probabilities;analyzing pairwise comparison;pairwise comparison data;models analyzing pairwise;pairwise comparison;comparisons probabilities outcomes;flexible model pairwise;comparisons probabilities;stochastic transitivity methodsthis;model pairwise;comparisons;comparison data;various parametric models;parametric models analyzing;comparison data including;stochastic transitivity;parametric models;parametric models including;bl thurstone models;includes parametric models;thurstone models reliance;transitivity methodsthis;analyzing pairwise;thurstone models;form stochastic transitivity;pairwise;comparison;bl thurstone", "pdf_keywords": "model pairwise comparisons;estimating pairwise comparison;stochastic transitivity models;pairwise comparisons probabilities;estimation pairwise comparison;pairwise comparison probabilities;models analyzing pairwise;pairwise comparisons;distribution pairwise comparisons;moderate stochastic transitivity;analyzing pairwise comparison;true pairwise comparisons;pairwise comparisons items;stochastic transitivity model;pairwise comparisons generated;comparison probabilities underlying;structural stochastic transitivity;comparisons applications statistical;stochastic transitivity based;comparisons items estimation;estimating pairwise probability;transitivity models frobenius;pairwise comparison data;pairwise comparison probability;paired comparisons applications;transitivity based models;pairwise comparisons contained;estimating pairwise;pairwise comparison;flexible model pairwise"}, "380278716f4d78ad9dcc3ece9e12b235ca1d1569": {"ta_keywords": "reasoning deep learning;introductiontensorlog deep learning;deep learning infrastructure;logical reasoning deep;high performance deep;reasoning deep;deep learning;introductiontensorlog deep;deep learning meets;performance deep;logic calledtensorlog;performance deep learning;probabilistic logical;probabilistic order logic;logical queries compiled;neural network infrastructure;logic calledtensorlog classes;probabilistic logical reasoning;order logic calledtensorlog;infrastructure 10sorflow theano;calledtensorlog classes logical;10sorflow theano;logical queries;learning meets probabilistic;integration probabilistic logical;classes logical queries;meets probabilistic bs;implementation probabilistic;neural;probabilistic bs", "pdf_keywords": "deductive knowledge graphs;knowledge graphs deductive;logical inferences deep;deductive knowledge graph;embed logical inferences;deductive databases;stochastic deductive knowledge;graphs deductive databases;deductive databases containing;stochastic logic programs;logical reasoning deep;inferences deep;inference data database;inferences deep network;stochastic logic;inference polytree limited;reasoning deep theorem;deductive knowledge;inference polytree;probabilistic logical;performing inference polytree;knowledge graphs;probabilistic order logic;inference polytreelimited;stochastic deductive;limited stochastic deductive;probabilistic order logical;knowledge graph;graphs deductive;probabilistic logical reasoning"}, "8a880680b28dee5642ac88431b3ae1085b911f96": {"ta_keywords": "machine translation models;translation models little;translation models;improve translation quality;consistently improve translation;machine translation;translation quality;translation quality multiple;introductionnal machine translation;penalize translations;penalize translations different;improve translation;frequencies penalize translations;translations;sentences consistently improve;translations different;translations different input;input sentences consistently;naive regularization methods;sentences consistently;using naive regularization;neural models trained;resource scenarios neural;naive regularization;neural models;regularization methods;translation;regularization;based sentence length;sentence length", "pdf_keywords": ""}, "4f7b108830de2e7964b6e1a89bf1c2da60140a34": {"ta_keywords": "variational autoencoder vae;effectively variational autoencoder;variational autoencoder;autoencoder vae powerful;trained effectively variational;autoencoder vae;ebola surrogate objective;bound ebola surrogate;ebola surrogate;autoencoder;vaes trained evidence;effective representation learning;representation learning framework;likelihood approach training;representation learning;model effective representation;practice vaes trained;vaes trained;ebola;language model effective;lower bound ebola;effectively variational;trained evidence lower;bound ebola;powerful language model;trained evidence;vae powerful language;trained effectively;introduction trained effectively;surrogate objective intractable", "pdf_keywords": "lossy autoencoders extensive;autoencoders extensive;variational autoencoder text;variationational autoencoder vae;autoencoder vae powerful;variational autoencoder;variationational autoencoder;autoencoder vae;learning vae learning;lossy autoencoders;autoencoders;autoencoder;variable lossy autoencoders;autoencoders extensive experiments;learning language modeling;novel variational autoencoder;autoencoder objective;autoencoder text generation;learning pretraining inference;representation learning language;learning vae;language modeling reconstruction;vae learning pretraining;autoencoder text;vae learning;pretraining inference network;using autoencoder;virtual learning vae;pretraining inference;using autoencoder objective"}, "14119210e5f9e0d962e329c833557dfb5524c4bd": {"ta_keywords": "lithium oxygen batteries;state lithium oxygen;lithium oxygen;solid state lithium;oxygen batteries;state lithium;lithium;batteries;performance liberation solid;liberation solid;liberation solid state;limit performance liberation;oxygen;key clinical;solid state;performance liberation;limit performance;key clinical messagethe;liberation;problems limit performance;clinical;performance;problems limit;clinical messagethe purpose;clinical messagethe;solid;limit;overview problems limit;article provide overview;key", "pdf_keywords": ""}, "de8ded0d66f3227d99751a89fdd5f4b438d6e8ee": {"ta_keywords": "analyze speech recognition;recognition speech analysis;speech analysis recognition;method analyze speech;speech analysis;analyze speech;speech recognition speech;speech recognition;recognition speech;analysis recognition;speech;method analyze;analysis;new method analyze;analyze;recognition;application;application new;application new method;authors application;method;new method;authors application new;new;authors", "pdf_keywords": ""}, "e74d7523d7d96ab65f05f059284f9d0a994bb074": {"ta_keywords": "nitrote treebank syntactic;naist nitrote treebank;treebank syntactic parsing;nitrote treebank;treebank syntactic;syntactic parsing;treebank;annotated parse trees;syntactic parsing fundamental;annotated parse;manually annotated parse;parsing;sentence segmentation;syntactic;parse trees;machine translation;machine translation language;modeling sentence segmentation;natural language processing;useful machine translation;sentence segmentation number;parsing fundamental natural;language processing;parse;speech translation case;language modeling sentence;corpus;speech translation;trees aligned translations;related speech translation", "pdf_keywords": ""}, "3efee0095cb578659dfaaf0d87a616f133ecf85c": {"ta_keywords": "speech recognition chime;recognition chime challenge;recognition chime;chime challenge recognize;speech recognition;chime challenge;acoustic modeling;summarizes acoustic modeling;component speech recognition;speech recognition article;approaches speech recognition;acoustic modeling efforts;speech recognition important;university speech recognition;speech recognition methods;recognition addition speech;addition speech recognition;speech recognition addition;dinner party speech;overlapped dinner party;optimize speech recognition;summarizes acoustic;recognize optimize speech;sound sound important;party speech recorded;speech recorded;chime;paper summarizes acoustic;introduction sound sound;component speech", "pdf_keywords": ""}, "9896a68e999298410bf16ffd08e8e67a54ad6a91": {"ta_keywords": "orchestration natural language;language processing software;natural language processing;language processing;software distributed cloud;software distributed;use software distributed;language processing problems;distributed cloud;distributed cloud computing;natural language;processing software;cloud computing;cloud computing environment;introductionthe natural language;processing software rational;middleware middleware solve;middleware;middleware middleware;middleware solve;processing;orchestration natural;cloud;specific middleware;distributed;middleware solve presented;software;specific middleware middleware;problems data intensive;computing environment", "pdf_keywords": ""}, "52c040c4b1786166325a0d930af94a529e2b5023": {"ta_keywords": "network speaker adaptation;speaker adaptation;sequence summarizing neural;sequential summarizing neural;neural network speaker;sequenceence summarizing neural;speaker adaptation paper;summarizing neural network;summarizing neural;sequential summarizing;proposed sequential summarizing;sequenceence summarizing;sequence summarizing;propose dansiadaptive adaptive;adaptive vector extractor;dansiadaptive adaptive;dansiadaptive adaptive vector;introduction sequenceence summarizing;sequence sequence summarizing;neural network sn;network speaker;replaced sequence sequence;replaced sequence;extractor replaced sequence;adaptation paper propose;introduction sequenceence;summarizing;neural network;adaptive;adaptation technique proposed", "pdf_keywords": ""}, "da564ff902a5490088f60c9fb100531fc9f97288": {"ta_keywords": "probabilistic logic methodsa;pagerank locally groundable;probabilistic logic;groundable order probabilistic;probabilistic language;stochastic logic programs;probabilistic language suited;order probabilistic language;order probabilistic logic;stochastic logic;extension stochastic logic;introductionprogramming personalized pagerank;pagerank locally;logic programs inference;probabilistic;personalized pagerank locally;pagerank;personalized pagerank;grounding particular query;language extension stochastic;order probabilistic;logic programs;methodsa order probabilistic;groundable order;locally groundable order;local grounding;programs inference;query approximately grounded;locally groundable;introductionprogramming personalized", "pdf_keywords": "firstorder probabilistic language;personalized pagerank pr;generating personalized pagerank;personalized pagerank;variant personalized pagerank;stochastic logic programs;graph personalized pagerank;compute personalized pagerank;efficiently inference query;personalized pagerank run;probabilistic language;personalized pagerank graph;related personalized pagerank;pagerank;probabilistic language wellsuited;pagerank pr method;pagerank pr;wellstudied personalized pagerank;personalized pagerank results;probabilistic order language;pagerank graph;personalized pagerank vector;probabilistic similarity logic;pagerank graph propr;pagerank effect inference;firstorder probabilistic;stochastic logic;markov logic networks;efficiently inference;pagerank run"}, "27724bd19946d6a824d06cdca3cdfe5d40f71003": {"ta_keywords": "predicting edit completions;predicting edit;edit completions based;edit completions;represent structural edits;likelihood edit learning;trained past edits;structural edits;editcompletion;editcompletion task;problem predicting edit;edit learning likelihood;edit learning;structural edits allows;completions based learned;past edits;editcompletion task present;edits allows model;code represent edit;completions based;represent edit operation;task editcompletion;task editcompletion task;learning likelihood edited;edit operation;represent edit;completions;past edits presentation;edits;edits allows", "pdf_keywords": ""}, "0cee58946a13a5c2845647b4af8b9d2bf52a8b6b": {"ta_keywords": "named entity recognition;entity recognition;entity recognition problem;distant supervision distant;problem distant supervision;language models bert;annotations yields highly;external knowledge bases;labels external knowledge;named entity;distant labels external;supervision distant;trained language models;supervision distant supervision;distant labels;distant supervision;knowledge bases;models bert;language models;distant supervision does;annotations;entity;domain named entity;manual annotations yields;annotations yields;knowledge bases address;manual annotations;external knowledge;noisy distant labels;recognition problem distant", "pdf_keywords": "entity recognition ner;entity recognition tasks;named entity recognition;entity recognition;distantly supervised named;supervised named entity;entity taggers distant;named entity taggers;entity recognition implications;semantic information training;models distant supervision;pre trained semantic;learns accurate named;trained semantic;entity taggers;distantly supervised;fully supervised ner;distant labels language;accurate named entity;trained semantic knowledge;annotations yields highly;language models distant;labels language models;supervised named;supervised ner;taggers distant supervision;entity recognition twitter;distant supervision methodsthe;approaches named entity;trained language models"}, "8274799029bfac4402685e1efd995a8aeb9e7426": {"ta_keywords": "sequence sequence autoencoder;sequence autoencoder;sequence autoencoder seq3;neural sequence sequence;neural sequence;autoencoder seq3 chained;sequence sequence models;autoencoder seq3;introductionneuro neural sequence;sequence models;sequence models currently;sequence discrete latent;autoencoder;seq3 chained encoder;words used sequence;chained encoder decoder;encoder decoder;encoder decoder pairs;sequence sequence sequence;encoder;large parallel corpora;chained encoder;parallel corpora;sequence sequence;decoder pairs words;sequence;decoder;present sequence sequence;introductionneuro neural;neural", "pdf_keywords": "sequence sequence autoencoder;sequence autoencoder trained;sequence autoencoder;abstractive sentence compression;summaries source encoder;sentence compression;neural machine translation;sequence autoencoder sq3;sentence compression present;autoencoder trained parallel;generating word sequence;softmax;sentence compression apply;translation applications neural;sequence sequence models;text compression model;summaries words forward;able generate summaries;novel text compression;encoder use softmax;generate summaries vocabulary;vocabulary source encoder;autoencoder;generating sentence continuous;machine translation;gse2 construct summaries;generating sentence;generate summaries;generate summaries source;text compression"}, "927efd299cffcfca3716efefcc904331b70c153e": {"ta_keywords": "diverse question answering;question answering datasets;question answering;answering datasets proposed;answering datasets;learning models tasks;involve answers reasoning;questions involve answers;answers reasoning processes;answers reasoning;deep learning models;deep learning;qa datasets;qa datasets covering;numerical reasoning focuses;lack qa datasets;background diverse question;answering;reasoning processes answers;covering complex questions;diverse question;complex questions involve;models tasks;tasks existing datasets;development deep learning;learning models;models tasks existing;reasoning focuses simple;research numerical reasoning;involve answers", "pdf_keywords": "reliable reasoning graphs;evaluating reasoning graph;reasoning graphs;interpretable reasoning graph;based reasoning graphs;predicted reasoning graph;reasoning graphs useful;generating reasoning graphs;evidences reasoning graph;reasoning graph introduce;reasoning graph proposed;reasoning graphs gnnaithe;reasoning graph;reasoning interpretable graph;generating reliable reasoning;reasoning graphs gnnai;reasoning graph introducethe;quality predicted reasoning;question answering;question answering datasets;semantic similarity;structured semantic similarity;reasoning graph directed;capture usefulnumerical reasoning;metric generated reasoning;interpretable graph qa;improve question answering;semantic similarity simultaneously;reasoning process answering;predicted reasoning"}, "6a116b897569fe4d6ea9ad4c3ba9a18825b96f49": {"ta_keywords": "learning logical rules;rules knowledge base;knowledge base completion;logical rules knowledge;probabilistic logical rules;introductiondifferentiable learning logical;rules knowledge;learning logical;knowledge base;composed probabilistic logical;logical rules;logical rules useful;probabilistic logical;model learning sets;completion methodsevaluation models;base completion methodsevaluation;base completion;models composed probabilistic;rules useful tasks;differentiable model learning;rules useful;introductiondifferentiable learning;learning sets;model learning;completion methodsevaluation;tasks knowledge base;learning sets order;completion unfortunately learning;order rules resul;rules resul", "pdf_keywords": "neural logic programming;inductive logic programming;base reasoning learning;reasoning learning;logic programming novel;inference knowledge base;neural logic;learning order logical;logic programming end;knowledge base reasoning;logic programming combines;logic programming;learning inference knowledge;logical rules knowledge;rules knowledge base;reasoning learning problem;logic programming lp;framework neural logic;reasoning introduce neural;logic programming outperforms;logical rules development;developedwe neural logic;structure logical rules;representations logical rules;inductive logic;learn knowledge base;approach inductive logic;learn parameters structure;knowledge base querying;knowledge base"}, "3c0e8f7337491ca4f714de14021eb23ca43d1d5e": {"ta_keywords": "speech recognition inreverberant;robust speech recognition;speech recognition ar;robustness automatic speech;robust speech;aspire automatic speech;speech recognition unknown;introduction robust speech;automatic speech recognition;speech recognition;automatic speech;recognition inreverberant environments;recognition unknown reverberant;recognition ar;recognition ar systems;recognition inreverberant;reverberant;reverberant noisy conditions;inreverberant environments challenge;robustness automatic;reverberant noisy;unknown reverberant noisy;introduction robust;unknown reverberant;assess robustness automatic;robust;speech;environments challenge;recognition unknown;recognition", "pdf_keywords": ""}, "6c78bac2dd71efb89951d9bab72c8129bbc07f67": {"ta_keywords": "regularize topic models;topic models latent;regularization latent variable;regularize topic;regularization technique latent;regularization latent;used regularize topic;variables methods regularization;topic models;variable based regularization;models latent variable;mixed membership models;latent observed variables;regularization framework;models latent;technique latent variable;latent variable variables;based regularization;background regularization latent;membership models language;regularization;latent variable mixed;methods regularization framework;regularization framework used;methods regularization;models language modeling;latent variable;language modeling;membership models;regularization technique", "pdf_keywords": ""}, "ce45aa1c64da82bfd02db0e147efa268da6980e4": {"ta_keywords": "loading algorithms ofdm;algorithms ofdm;algorithms ofdm methods;bit loading algorithms;ofdm methods;ofdm methods results;ofdm;analysis bit loading;bit loading;classic bit loading;loading algorithms investigated;loading algorithms;loading algorithms classic;results bit loading;introduction analysis bit;algorithms classic bit;analysis bit;greedy algorithm analyzed;investigated greedy algorithm;algorithm analyzed performance;loading;performance algorithms analyzed;algorithms investigated greedy;algorithm analyzed;greedy algorithm;algorithm;analyzed performance algorithms;algorithms analyzed simulation;algorithms investigated;algorithms analyzed", "pdf_keywords": ""}, "d32fb57467d64bb82dce60e904ddc5c18b3f0f91": {"ta_keywords": "characteristics curbside parking;curbside parking resultsthis;curbside parking;parking;parking resultsthis;parking resultsthis work;drivers looking park;spatiotemporal characteristics curbside;park harmlessnessing data;incentives drivers looking;park harmlessnessing;congestion meeting city;mitigate congestion;pricing policies;mitigate congestion meeting;looking park harmlessnessing;performance based pricing;set pricing policies;park;congestion;pricing policies target;characteristics curbside;curbside;looking park;based pricing;purposeto mitigate congestion;pricing schemes;congestion meeting;incentives drivers;pricing schemes received", "pdf_keywords": "spatial parking demand;demand spatial parking;spatial parking;parking demand consistent;parking increasingly;parking demand;curbside parking increasingly;spatial demand curbside;parking data;parking increasingly important;overlooked factors parking;characteristics curbside parking;analysis spatial demand;spatial demand spatial;spatial autocorrelation park;factors parking;demand curbside parking;parking zones;analysis using parking;seattle spatial parking;using parking data;parking decisions;parking data city;parking seasonal;parking decisions location;spatial demand local;consistent spatial demand;parking;curbside parking fundamental;demonstrate spatial demand"}, "ab5c6703fceb3dce6558be309cc65a4a8615c774": {"ta_keywords": "metric data neighborhood;data neighborhood graphs;backgroundaccurate fast retrieval;neighborhood graph directly;approximate neighborhood graph;data neighborhood;non metric data;neighborhood graphs;challenging non metric;neighborhood graph;graph based search;fast retrieval complex;fast retrieval;non symmetric distances;neighborhood graphs methodswe;metric non symmetric;non metric;symmetric distances resorting;metric data;distances resorting metric;mapping distance symmetrization;complex non metric;non metric non;approximate neighborhood;retrieval complex non;distance symmetrization;construction approximate neighborhood;symmetric distances;retrieval;metric non", "pdf_keywords": "distance symmetrization search;graph based retrieval;graph retrieval;distance metric learning;metric learning distance;metric data neighborhood;similarity metric distance;learning distance symmetrization;non symmetric distances;graph based search;graph retrieval starts;data neighborhood graphs;sw graph retrieval;neighborhood graph directly;non symmetric distance;mapping distance symmetrization;approximate neighborhood graph;symmetric distances resorting;metric learning large;similarity metric;neighborhood graphs use;distance symmetrized version;neighborhood graphs;neighborhood proximity graphswe;neighborhood graphs highdimensional;symmetric distances;distance symmetrization effective;distance symmetrization;generating neighborhood graphs;symmetric distance distance"}, "d6741241efb9ffd933df974b43d7109c72238371": {"ta_keywords": "music machine;music machine mmm;multi track music;generating multi track;track music machine;sequence musical events;musical events track;musical events corresponding;propose multi track;sequence musical;multi track;track music;ordered sequence musical;musical events;corresponding different tracks;tracks interleaved create;track music contrast;musical material single;tracks interleaved;represents musical material;different tracks interleaved;represents musical;musical material;machine mmm generative;music;mmm generative based;musical;different tracks;generative based;generative based transformer", "pdf_keywords": "track music generation;generate music fundamental;generate music;ability generate music;approach representing musical;representing musical material;music generation;predict musical;music generation transformer;process music;generation musical material;music fundamental process;used predict musical;representation musical material;generating multi track;music generation powerful;music generation mmm;computational assisted composition;representing musical;music machine;predict musical history;music critical computational;resulting musical material;music generation ni;fundamental process music;generation set tracks;multi track music;representation musical;ability predict musical;process music article"}, "d41216f2f809e9fe26a684392f0ded4778f79e74": {"ta_keywords": "anr automatic speech;monolithic multilingual anr;automatic speech recognition;speech recognition language;multilingual anr automatic;speech recognition ar;speech recognition;speech recognition systems;automatic speech;multilingual anr;developing automatic speech;recognition language independent;monolithic multilingual;build monolithic multilingual;end automatic speech;recognition language;monolithic neural network;neural network architecture;linguistic information pronunciation;language independent neural;recognition systems new;proposed monolithic neural;multilingual;information pronunciation dictionaryaries;information pronunciation;new languages;new languages eliminating;pronunciation dictionaryaries;independent neural network;neural network", "pdf_keywords": ""}, "66cbda3e730285cb572c4792edcef209af32c564": {"ta_keywords": "learn retriever models;question answering;retriever models downstream;information retrieval;retriever models;retriever model corresponding;domain question answering;data train retriever;retrieval;question answering traditional;learn retriever;technique learn retriever;train retriever model;retriever model;natural language processing;retrieval important component;task information retrieval;neural networks recently;information retrieval important;query support documents;features continuous representations;train retriever;continuous representations based;retriever;component natural language;representations based neural;retrieval important;obtain supervised;neural networks;continuous representations", "pdf_keywords": "question answering benchmarks;question answering;useful retrieval retriever;learn information retrieval;useful retrieval;neural information retrieval;question answering traditional;answering benchmarks ability;domain question answering;approach retrieval retriever;new approach retrieval;retriever useful retrieval;information retrieval;train information retrieval;retrieval based machine;information retrieval introduce;retrieval systems;task information retrieval;training retriever useful;retriever trained learning;approach retrieval;method question answering;approximate reader attention;retrieval introduce novel;retrieval introduce;answering benchmarks;retrieval retriever;retrieval;retrieval based;retrieval systems fundamental"}, "7b96f6165ce5f686e46868c53b111b8e43b93de3": {"ta_keywords": "age outgoing citations;forgetting older literature;citations;citations papers;outgoing citations papers;outgoing citations;older literature methodsby;older literature;citations papers published;natural language processing;cite work;field natural language;cite;cite work researchers;natural language;published selected anthology;selected anthology;language processing experiencing;literature methodsby;literature methodsby looking;stock cite work;language processing;literature;anthology;anthology venues 2010;forgetting older;published papers;papers published;papers published selected;selected anthology venues", "pdf_keywords": ""}, "863b2b38b33ffc4e5462adbc7aaf84aeb93adda8": {"ta_keywords": "language projecting annotation;annotation projection;projecting annotation task;annotation projection limited;simultaneously projects annotation;annotation multiple tasks;annotation task;projecting annotation;projects annotation multiple;annotation multiple;annotation;work annotation projection;projects annotation;relying parallel corpora;parallel corpora;annotation task time;parallel corpora available;indoeuropean languages using;work annotation;multiple source languages;source language projecting;single source language;hundreds languages methods;languages methods training;languages relying parallel;subset indoeuropean languages;indoeuropean languages;language projecting;previous work annotation;source languages", "pdf_keywords": "lingual dependency parsers;nonlingual dependency parsers;language projecting annotation;projection based parsing;languages tagging parsing;dependency parsers jointly;dependency parsers;lingual taggers parsers;annotation projection algorithm;tagging dependency parsing;dependency parser labeled;dependency parsers applications;dependency parsing;dependency parsers aim;corpus source languages;dependency parser;based dependency parser;taggers dependency parsers;projecting annotation task;parsers jointly projectedwe;parsing systems;parsers jointly;annotation projection;parsing systems baseline;annotation projection limited;parsers applications cross;parsers;parser labeled;standard annotation projection;parsers applications"}, "d16d24dd5135f148556df1b2304b3747eee19e00": {"ta_keywords": "signature reply lines;extract signature reply;reply lines email;based mail classifiers;mail classifiers;mail classifiers email;reply lines;text email messages;classifiers email;reply lines plain;email messages analysis;plain text email;text email;email text speech;email text;preprocessing email text;lines email methods;signature reply;anonymization email corpora;lines email;learning extract signature;signature blocks reply;blocks reply lines;classifiers email threading;email corpora improving;automatically identifying signature;email corpora;email messages;messages analysis;anonymization email", "pdf_keywords": ""}, "7f54429be66319dc19a42c0c9fceda3ac33fc92d": {"ta_keywords": "building cognitive tutors;tool cognitive tutors;cognitive tutors;cognitive tutors highly;cognitive tutors results;computer based tutoring;authoring tool cognitive;based tutoring;tutoring;based tutoring primary;intelligent authoring;building intelligent authoring;intelligent authoring tool;tutors;tutors results;write cognitive model;tutoring primary;educators familiar cognitive;student embedded authoring;tutors highly;write cognitive;authors write cognitive;building cognitive;tool cognitive;tutors highly successful;tutors results instead;tutoring primary target;users authors educators;ai programming;ai programming essential", "pdf_keywords": ""}, "c97500763de8a0871f1b83b1f968fcf4a8b31aee": {"ta_keywords": "technical report corpus;task validation ataro;corpus collection initial;report corpus;report corpus collection;corpus collection;validation ataro technical;corpus;validation ataro;ataro technical report;initial task validation;ataro technical;task validation;introductionataro technical report;introductionataro technical;technical report corp;ataro;technical report;validation;introductionataro;initial task;collection initial task;report;task;technical;report corp;collection initial;collection;initial;corp", "pdf_keywords": ""}, "2ded680be56e03c8c17a04065deaac8ea6d4fa12": {"ta_keywords": "allele novel hla;novel hla allele;hla allele novel;02 novel hla;novel hla;hla allele 15;hla allele;hla;allele novel;allele 15;allele 15 179;allele;179 02 novel;02 novel;novel;15 179;15 179 02;179;15;179 02;02", "pdf_keywords": ""}, "753d10503a3cf340e41552109087ffd15ec96446": {"ta_keywords": "entrepreneurialial network jmc;mcc local entrepreneurialial;entrepreneurialial network mcc;local entrepreneurs mcc;entrepreneurs mcc local;local entrepreneurialial network;entrepreneurs mcc;network jmc;based local entrepreneurialial;local entrepreneurialial;entrepreneurialial network based;entrepreneurs local entrepreneurialial;local entrepreneurs local;entrepreneurialial network;local entrepreneurs;entrepreneurs local;development local entrepreneurs;network jmc experimental;mcc local;jmc;jmc experimental model;network mcc;entrepreneurs;jmc experimental;network mcc new;mcc;entrepreneurialial;development local;mcc new model;mcc new", "pdf_keywords": ""}, "998bd8862ab4193e672bb16fe1aae4d446f7536e": {"ta_keywords": "image image translation;image translation;image translation patch;contrastive learning;learned feature space;learned feature;maximizing mutual information;based contrastive learning;introductionin image image;contrastive learning methodsthe;patches map similar;image image;corresponding patches map;mutual information using;point learned feature;mutual information;doing maximizing mutual;maximizing mutual;map similar;elements corresponding patches;relative elements patches;introductionin image;learning;corresponding patches;corresponding patch input;similar point learned;reflect content corresponding;translation patch;translation patch output;image", "pdf_keywords": "conditional image synthesis;unique image synthesis;patchwise contrastive learning;image synthesis;image synthesis tasks;image training;unpaired image translation;image image translation;contextin image image;multilayer patchwise contrastive;image example learning;image translation;translation conditional image;image synthesis present;learns embedding;image translation conditional;neural style transfer;function image translation;image translation patch;reconstruct image input;contextin image;patch based contrastive;single image training;learns embedding bringing;contrastive learning;reconstruction unpaired image;image translation method;photo translation train;image translation algorithm;objective learns embedding"}, "d1206ccabd1980848f14472d6548251c2fab7963": {"ta_keywords": "language models transferring;non language modeling;non language models;language modeling improve;tasks language modeling;language models;models tasks language;results transfer learning;language modeling demonstrate;transfer learning;language modeling;language models broad;scale language models;models transferring;models tasks;question answering sequence;answering sequence labeling;tuning models tasks;classification question answering;large scale language;question answering;advances non language;problems text classification;labeling results transfer;models transferring downstream;tasks language;sequence labeling;sequence labeling results;clinical messagea;extensive study transferability", "pdf_keywords": "question answering transferability;learning task embeddings;embeddings tasks;embeddings new tasks;answering tasks classification;task embeddings tasks;computing task embeddings;embeddings task synergies;embeddings tasks encode;task embeddings useful;achieved task embeddings;embeddings task similarity;embeddings task;approach task embeddings;task embeddings;answering transferability;tasks language modeling;task embeddings task;non language tasks;task embeddings new;computes task embeddings;language tasks;task question answering;question answering tasks;transfer learning;language tasks broad;tasks classification;answering transferability non;task embeddings layerwise;models tasks language"}, "e9d26b9f5e6b619bbb759a67560cb949a9f034ba": {"ta_keywords": "ascent learning dynamics;local minmax equilibrium;zero sum games;sum games minimizing;nonconvex optimization;learning dynamics;convergence local minmax;nonconvex optimization problem;ascent learning;minmax equilibrium classes;minmax equilibrium;descent ascent learning;equilibrium classes nonconvex;learning dynamics timescale;gradient descent ascent;local minmax;games minimizing;nonconvex zero sum;maximizing player;maximizing player optimizes;problem maximizing player;nonconvex zero;gradient descent;unconstrained continuous action;games minimizing player;descent ascent;minmax;study gradient descent;faces nonconvex optimization;sum games", "pdf_keywords": ""}, "6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91": {"ta_keywords": "resultsmultimodal machine learning;introductionmultimodal machine learning;characterized multimodal;artificial intelligence modality;multimodal;able interpret multimodal;multimodal signals resultsmultimodal;multimodal signals;interpret multimodal;interpret multimodal signals;problem characterized multimodal;characterized multimodal includes;modality;intelligence modality;multimodal includes multiple;multimodal includes;intelligence modality refers;modality refers way;machine learning important;artificial intelligenceligence;modality refers;introductionmultimodal machine;artificial intelligenceligence make;machine learning aims;artificial intelligence;machine learning;signals resultsmultimodal;introductionmultimodal;signals resultsmultimodal machine;multiple modalities", "pdf_keywords": "multimodal machine learning;machine learning multimodal;multimodal recognition audio;classification multimodal fusion;multimodal recognition;classification multimodal;learning multimodal;learning multimodal representations;multimodal learning;recent advances multimodal;multimodal representations;learn multimodal representation;classify multimodal;classify multimodal fusion;challenges facing multimodal;multimodal datasets;important task multimodal;advances multimodal;multimodal models;multimodal datasets categorize;approaches multimodal translation;multimodal models improving;multimodal representations joint;multimodal representation;new approach multimodal;multimodal fusion kernel;multimodal processing;multimodal;multimodal retrieval;multimodal representation challenging"}, "480d545ac4a4ffff5b1bc291c2de613192e35d91": {"ta_keywords": "toolkit implementing neural;dynet toolkit implementing;dynamic declaration network;dynet toolkit;implementing neural network;neural network models;network structure methodsin;implementing neural;neural network;computes derivatives dynet;defines computation graph;declaration network structure;derivatives dynet;network models based;network models;dynet;network structure;declaration network;neural;computation graph symbolic;computation graph;toolkits like theano;backgroundwe dynet toolkit;toolkit implementing;defines computation;computation examples fed;computation computes derivatives;graph symbolic representation;structure methodsin;models based dynamic", "pdf_keywords": "dynamic declaration network;toolkit implementing neural;dynamic declaration paradigm;neural network dynamically;declaration execution network;dynamic declaration model;implement neural;dynamicthe software engineering;propose dynamic declaration;neural network toolkit;neural network language;dynamic toolkit;dynamic toolkit possible;dynamic neural networks;implement neural networks;neural networks dysnet;way implement neural;creation dynamic neural;implementing neural networks;dynamic neural;dynamicthe software;systems dynamic declaration;neural networks language;dynamic dynamic neural;dynamic declaration;dynamic neural network;neural network computation;tools dynamic declaration;deep learning present;dynamic computation"}, "aeccb1d53e08adcfe271d1e4b08c0a2cdc3c42b4": {"ta_keywords": "open information extraction;introductionopen information extraction;corpora open information;information extraction;information extraction global;information extraction systems;massive text corpora;open information;extracting entities relations;statistics large corpus;constraints extracting entities;extracting entities;introductionopen information;text corpora open;corpus collectively leveraged;extraction global structure;current open information;sentences current open;large corpus;large corpus collectively;corpora open;entities relations text;text corpora;structure constraints extracting;extraction systems relation;corpus collectively;corpus;constraints extracting;understanding massive text;relations text", "pdf_keywords": ""}, "824cd8db8a68732db04f4d8b7139eb4475e59ff2": {"ta_keywords": "nlg evaluation metrics;benchmark natural language;generation nlg evaluation;language generation nlg;natural language generation;language generation evaluation;generation nlg;nlg evaluation;generation evaluation metrics;language generation;anglo centric corpora;generation evaluation;centric corpora;nlg;evaluation metrics introduce;benchmark natural;automated metrics;corpora;benchmark benchmark natural;natural language;introductionthegim benchmark;introductionthegim benchmark benchmark;evaluation metrics;datasets human evaluation;evaluation metrics measuring;automated metrics datasets;anglo centric;benchmark;living benchmark natural;metrics datasets human", "pdf_keywords": "benchmark natural language;language generation benchmark;natural language generation;language generation nong;language benchmark development;progress natural language;language generation data;language benchmark;language language benchmark;language generation relies;language generation provide;language generation national;natural language technologies;natural language nl;development natural language;nonl nl challenges;generation evaluation metrics;language generation goal;text models nlls;human language generation;language generation;language generation application;language nl systems;language generation development;generation benchmark;generation benchmark provide;gim generation evaluation;testbed automated metrics;generation benchmark gm;language research mnlg"}, "508e9bb13fcb1fa0c4dbac47288e8a3c2487bfc2": {"ta_keywords": "generalize proof tree;proof tree generalizing;learning generalize proof;generalization explanation based;perform generalization explanation;explanation based learning;generalize proof;learning generalize;tree generalizing;proof tree;algorithm generalizing;perform generalization;properties generalization algorithms;generalization algorithms;introductionsystems perform generalization;generalization explanation;generalizing;generalization;generalizing number;algorithm generalizing number;outputs algorithm generalizing;generalizing number methodsthe;generalization algorithms paper;generalize;based learning generalize;properties generalization;desired properties generalization;generalizing shape tree;tree generalizing shape;introduces formal framework", "pdf_keywords": ""}, "7a733a8d8f8649cc07e3ea9091f454ae117573af": {"ta_keywords": "automatic meh indexing;meh indexing;meh indexing models;articles pubmed database;medical subject headings;indexing models developed;indexing models;indexing;pubmed database;pubmed database facilitate;millions articles pubmed;information retrieval;subject headings meh;organized vocabulary containing;information retrieval curators;articles pubmed;retrieval curators;hierarchically organized vocabulary;organized vocabulary;medicine information;article meh hierarchically;facilitate information retrieval;subject headings;headings meh article;medicine information sciences;retrieval curators national;retrieval;clinical medicine information;national library medicine;medical subject", "pdf_keywords": ""}, "abb9b27440719ca44db5947a537fde07f0547973": {"ta_keywords": "bandwidth distributed storage;distributed storage;storage distributed;distributed storage distributed;storage distributed storage;storage nodes;distributed storage setting;repair bandwidth distributed;storage nodes data;stored storage nodes;data collector able;reducing repair bandwidth;nodes data collector;data collector;storage;distributed;stored storage;bandwidth distributed;repair bandwidth;storage setting considered;stored nodes;entire data downloading;size stored storage;storage setting;codes uniformly reducing;codes uniformly;data downloading;file size stored;explicit codes uniformly;considered file size", "pdf_keywords": ""}, "808a9c9dece4c21be50f41e6caf50101f2b24b47": {"ta_keywords": "modeling human preferences;verification preference models;preference models;preference models leverages;human preferences using;assumption accurately preferences;accurately preferences humans;qualitative preference statements;choice model usually;preference statements constraint;preferences humans subjects;preference statements;human preferences;preferences humans;choice model;accurately preferences;functions qualitative preference;formalisms choice model;qualitative preference;tractable verification preference;preference;utility functions qualitative;preferences using;verification preference;preferences using number;optimization logic formalisms;optimization logic;utility functions;model usually based;constraint optimization logic", "pdf_keywords": ""}, "104f75283ae9027eb478e7984bd26b680277ce6f": {"ta_keywords": "vision language navigation;robust instruction representations;action decoding;challenges vision language;instruction representations action;introduction robust instruction;representations action decoding;building robust instruction;language navigation vln;robust instruction;instruction representations;action decoding schemes;language navigation;vision language;navigation vln challenge;core vision language;unseen instructions environments;instructions environments;navigation vln;navigation;challenge core vision;key challenges vision;challenge building robust;challenges vision;representations action;decoding;previously unseen instructions;instructions environments paper;challenge core;address challenges", "pdf_keywords": "learning navigate;learning navigate visual;task learning navigate;vision language navigation;trained natural language;pretrained language models;agent navigate efficiently;pretraining improves generalization;language navigation;language navigation challenge;language navigation vln;pretrained language;building robust instruction;pretrained language modelswe;processing pretrained language;navigate visual environments;vln task learning;navigate efficiently;improves generalization grounded;action decoding;stochastic action sampling;robust instruction representations;network pretrained language;navigation vln task;action sampling;agent navigate;visual navigation;vision language;ability navigate efficiently;trajectory agent trained"}, "f83ef3250ba1166d7c1c7585da7dd78e0641fae7": {"ta_keywords": "temporal model music;track music generation;music generation;music generation framework;model music;multi track music;networks gans;generative adversarial networks;generative adversarial;gans methods;networks gans methods;model music usually;generation framework generative;framework generative adversarial;track music;instruments tracks temporal;symbolic multi track;music art;music usually composed;music;music art time;generative;multiple instruments tracks;adversarial networks gans;tracks temporal;context music art;context music;gans;instruments tracks;multi track", "pdf_keywords": "music generating;generating music;track music generating;track music generation;generate music;music generating multi;method generating music;generate dynamic music;generate symbolic music;music generation framework;cooperative music generation;music generation based;music applied generate;ai cooperative music;music generation;music generation music;designed generate music;generating multi track;generate multi track;multi track music;symbolicdomain music generation;dynamic music;track music implement;music scratch generate;generate coherent music;song dataset;track music;generation music;track polyphonic music;multi track polyphonic"}, "199f383e9acd62649121ccde1e06631ce62c89e9": {"ta_keywords": "complexity secret sharing;secret sharing;secret sharing dealer;secret sharing network;problem secret sharing;sharing dealer;algorithm propagating dealer;secret sharing important;communication complexity secret;sharing dealer does;complexity secret;consider problem secret;propagating dealer condition;propagating dealer;distributed algorithm propagating;bounds communication complexity;efficient distributed algorithm;dealer participants;sharing network;problem secret;dealer condition derive;cryptographic protocols;efficient distributed;sharing;lower bounds communication;communication complexity;distributed algorithm;participants instead dealer;component cryptographic protocols;present efficient distributed", "pdf_keywords": ""}, "645bc7a5347a299a1e8aa965867bd097f6f4bddd": {"ta_keywords": "introductionlanguage guided robots;guiding agent answers;guiding agent;guided robots;agent navigates asks;second guiding agent;guided robots able;agent navigates;methodsthe navigating agent;agent models guiding;navigating agent;recursive mental model;propose recursive mental;introduce agent task;agent answers;recursive mental;navigating agent models;introductionlanguage guided;robots able ask;agent task;introduce agent;mind propose recursive;agent answers inspired;agent task agent;following introduce agent;questions second guiding;humans questions understand;robots;task agent navigates;guided", "pdf_keywords": ""}, "d00a403028eb0786915dab7a76692e5eeadf60be": {"ta_keywords": "transductive transfer learning;transfer learning information;transfer learning labeled;transfer learning;unsupervised transductive transfer;problem transfer learning;transductive transfer;unsupervised transductive;inductive transductive approaches;learning labeled;learning labeled data;case unsupervised transductive;inductive transductive;transductive approaches adapt;learning information gained;transductive approaches;transductive;learning task;art inductive transductive;learning information;gained learning task;introductionthe problem transfer;learning task used;information gained learning;supervised;labeled data target;studied supervised version;transfer;domain available training;learning", "pdf_keywords": ""}, "7abcc79e10ff651ef59dea84d347fa64c51e11b2": {"ta_keywords": "assembly organic heterostructures;organic heterostructures ohss;organic heterostructures;heterostructures ohss;axial branching ohs;heterostructures ohss precisely;branching axial ohs;heterostructures;self assembly organic;unilateral axial ohs;single branching axial;axial ohs bilateral;branching ohs including;multi branching axial;controlled self assembly;assembly organic;branching axial axial;types axial branching;axial branching;branching axial;branching ohs;axial ohs;self assembly;including unilateral axial;unilateral axial;bilateral single branching;construct types axial;ohs bilateral single;bilateral multi branching;ohs bilateral multi", "pdf_keywords": ""}, "31b3e84f0a66e27c53c7fe403a0c6cd2319ed797": {"ta_keywords": "reasoning deep learning;introductiontensorlog probabilistic database;logical reasoning deep;deep learning infrastructure;neuralnetwork infrastructure;introductiontensorlog probabilistic;neuralnetwork infrastructure 10sorflow;logic calledtensorlog;functions neuralnetwork infrastructure;reasoning deep;deep learning;probabilistic logical;logic calledtensorlog classes;implementd using deep;logical queries compiled;calledtensorlog classes logical;using deep learning;probabilistic order logic;probabilistic logical reasoning;probabilistic database implementd;probabilistic database;logical queries;neuralnetwork;order logic calledtensorlog;using deep;classes logical queries;functions neuralnetwork;integration probabilistic logical;calledtensorlog classes;infrastructure 10sorflow theano", "pdf_keywords": ""}, "805e49c7282b847faee048a63c1f43ceb08f5257": {"ta_keywords": "voting data methods;empirical study voting;study voting rules;voting rules manipulation;voting data;study voting systems;voting systems;ordered voting data;voting systems takes;voting rules;ordered voting;study voting;elections existing studies;strictly ordered voting;derive million elections;prize dataset results;prize dataset;netflix prize dataset;voting;results evaluate elections;evaluate elections;elections;million elections;evaluate elections plur;million elections existing;aim study voting;elections existing;data netflix prize;elections plur;rules manipulation large", "pdf_keywords": ""}, "641af3bc3cc17993dc72098725d2eb9c0d98049d": {"ta_keywords": "statistical topological data;topological data analysis;topological data;introduction statistical topological;statistical topological;topology connectedness data;topological;connectedness data;properties shape topology;shape topology;connectedness data project;topology;shape topology connectedness;topology connectedness;analyze data;data analysis;data analysis methods;visualize analyze data;analyze data sample;data analysis td;data sample overview;connectedness;data project;visualize analyze;data;properties statistical perspective;tools visualize analyze;study properties shape;properties statistical;data sample", "pdf_keywords": ""}, "84f2cfbc142ad3165ea3bcacd189a3d1110660e0": {"ta_keywords": "end speech recognition;speech recognition;speech recognition ar;connectionist tempral classification;automatic speech recognition;speech recognition important;attention model achieved;training joint decoding;end automatic speech;attention model;automatic speech;tempral classification network;ct attention model;end end speech;end speech;multi task training;attention based;connectionist tempral;task training joint;attention based methods;tempral classification;methods connectionist tempral;joint decoding;joint ct attention;multi stream;classification network promising;introductionmulti stream end;stream end end;present multi stream;stream end", "pdf_keywords": "fusion recurrent stream;multi stream encoder;stream architecture speech;attention architecture useful;speech recognition multichannel;attention network;speech recognition multi;recognition multichannel speech;multichannel speech recognition;multichannel speech;rn speech recognition;attention architecture;rn maxpooling cnn;hierarchical attention network;attention network introduced;attention automatic speech;ct attention architecture;attention networks;maxpooling cnn aim;cnn rnn;regular attention networks;networks hierarchical attention;2nn based cnn;maxpooling cnn;cnn rnn based;combine 2nn based;fusion streams;stream encoder;speech introduce multistream;speech recognition neural"}, "ceefd51b4b391668e313afe8edb3588197002e37": {"ta_keywords": "speech synthesis;based speech synthesis;speech synthesis methodsin;humanmm based speech;smoothing global variation;modulation spectrum humanmm;spectrum humanmm based;postfilter modify modulation;smoothing global;global variation known;smoothing;capture smoothing global;global variation;features capture smoothing;alleviate smoothing effects;spectrum humanmm;based speech;smoothing effects;capture smoothing;alleviate smoothing;variation known;modify modulation;postfilter modify;quality degradation humanmm;variation;backgrounda postfilter modify;humanmm based;degradation humanmm based;smoothing effects main;modify modulation spectrum", "pdf_keywords": ""}, "2ec99c834bd67ac64ec04b426e5f9fd04f639024": {"ta_keywords": "methods applicability crowdsourcing;crowdsourcing complex tasks;crowdsourcing complex;crowdsourcing;applicability crowdsourcing complex;applicability crowdsourcing;principled aggregation methods;aggregation methods;aggregation methods applicability;crowding standard tools;aggregation;principled aggregation;crowding;crowding standard;aggregation methods modalities;lack principled aggregation;background crowding;efficient data collection;research aggregation methods;background crowding standard;speech recognition;tasks speech recognition;recognition;advances research aggregation;research aggregation;speech recognition remains;data collection;classification;data collection simple;image classification", "pdf_keywords": ""}, "4375cccdfaf2ce3d013e4129d39f7801ef8a468e": {"ta_keywords": "workshop asian translation;asian translation wat2019;patent translation subtasks;translation wat2019 including;jaen patent translation;asian translation;paper translation subtasks;translation subtasks jaen;scientific paper translation;translation wat2019;translation wat2019 described;patent translation;paper translation;translation subtasks;translation subtasks hien;6th workshop asian;translation;workshop asian;5th workshop asian;jaen jazh scientific;jako jaen patent;jaen jazh;subtasks jaen jako;jaen patent;jazh scientific paper;wat2019 including jaen;jaen jako jaen;subtasks jaen;wat2019 described paper;jako jaen", "pdf_keywords": ""}, "50a1dd504037463578f6ba8ee40afe4143f3d6fa": {"ta_keywords": "uniformly distributed sphere;uniform measure sphere;vector uniformly distributed;bounds expectation norm;expectation norm vector;distributed sphere;measure sphere;expectation norm;norm vector uniformly;vector uniformly;bounds expectation;concentration uniform measure;upper bounds expectation;distributed sphere henomenon;distributed sphere useful;concentration uniform;neoplastic behavior vector;sphere henomenon concentration;sphere;uniform measure;norm;uniformly distributed;sphere henomenon;uniformly;expectation;sphere useful;sphere useful tool;henomenon concentration uniform;evaluation neoplastic;norm vector", "pdf_keywords": "patient diagnosed etiology;diagnosed etiology;diagnosed etiology etiology;etiology patient treated;etiology patient understood;etiology etiology patient;etiology patient;etiology patient etiology;etiology understood report;patient etiology;patient etiology etiology;new approach diagnosis;disease etiology patient;approach diagnosis treatment;diagnosis treatment;analysis etiology etiology;management etiology etiology;treatment patients neoplastic;analysis etiology;management patients neoplastic;approach diagnosis;etiology understood;etiology etiology understood;patients neoplastic disease;etiology;patients neoplastic;patient diagnosed;etiology etiology etiology;etiology etiology;disease etiology"}, "fac2368c2ec81ef82fd168d49a0def2f8d1ec7d8": {"ta_keywords": "information extraction tasks;named entity recognition;entity recognition;extraction event extraction;information extraction;relation extraction event;entity recognition relation;event extraction;event extraction framework;extraction tasks named;entities adjacent sentences;context entities;framework information extraction;context entities important;relation extraction;recognition relation extraction;tasks named entity;extraction framework;capture local sentence;extraction event;extraction tasks;scoring text spans;unified multi task;refining scoring text;sentence global;global cross sentence;named entity;entities important entities;extraction framework called;scoring text", "pdf_keywords": "entity relation extraction;relation extraction framework;entity recognition;information extraction tasks;entity recognition relation;named entity recognition;event extraction additional;event extraction;relation extraction;recognition relation extraction;relation extraction employ;information extraction;relations event prediction;spans text domains;perform event extraction;relation extraction using;entities relations event;tasks named entity;extraction employ spanbased;extraction tasks named;multi sentence encodings;context relevant tasks;framework information extraction;scoring text spans;unified multitask framework;sentence context resultsour;integrating information coreference;entities relations;applied information extraction;event argument classification"}, "25ee819bc444b02db43fcbeced982c975edee033": {"ta_keywords": "labeling type worker;crowdsourced labeling type;worker task specialization;crowdsourced labeling;type worker task;consider crowdsourced labeling;matched type tasks;clustering worker skill;binary task labels;tasks matched type;worker task associated;worker clustering worker;task specialization model;worker task;worker clustering;type tasks;clustering worker;task specialization;model worker task;tasks matched;using worker clustering;task labels;tasks unmatched types;worker skill estimation;task labels given;crowdsourced;type worker;types worker;consider crowdsourced;specialization model worker", "pdf_keywords": "clustering crowdsourcing;crowdsourcing clustering;crowdsourced labeling type;crowddsourced labeling worker;clustering crowdsourcing useful;method clustering workers;approach crowdsourcing clustering;labeling worker task;algorithm crowdsourced labeling;crowdsourcing clustering crowdsourcing;crowdsourced labeling empirically;crowdsourced labeling;method crowdsourced labeling;classification workers task;consider crowdsourced labeling;crowdsourced labeling using;labeling type worker;cluster workers clustering;crowdsourcing useful tool;useful tool crowdsourcing;labeling worker;crowddsourced labeling;clustering workers types;tool crowdsourcing;labeling using crowds;clustering workers;aggregates answers clusters;clustering workers cluster;workers clustering;tool crowdsourcing useful"}, "6b13c4ac18f621155a550238a037a670bdce8969": {"ta_keywords": "diagnosis pulmonary infection;pulmonary infection;patients diagnosis pulmonary;diagnosis pulmonary;pulmonary;patients diagnosis;infection;management patients diagnosis;diagnosis;patients;management patients;approach management patients;approach management;new approach management;article discuss importance;importance new approach;management;new approach;article discuss;article;approach;aim article discuss;discuss importance new;aim article;discuss importance;new;importance new;importance;discuss;aim", "pdf_keywords": ""}, "06d0af396fb08caa6a665dd476380aa16b6199b2": {"ta_keywords": "patient diagnosis malignant;diagnosis malignant disease;diagnosis malignant;malignant disease;diagnosis management patient;physician diagnosis;patient diagnosis;malignant;physician diagnosis management;management patient diagnosis;role physician diagnosis;diagnosis;diagnosis management;literature role physician;management patient;physician;role physician;disease;patient;article present literature;article present;literature role;literature;present literature role;management;article;purpose article present;present literature;purpose article;role", "pdf_keywords": ""}, "3e33c988969b4c9f1d9af8c1c0f7644a30d0311f": {"ta_keywords": "speech translation medical;reliability speech translation;medical corpus translation;design speech translation;corpus translation experiments;speech translation;medical corpus;translation medical domain;corpus translation;translation medical;high reliability speech;translation experiments;collection medical corpus;translation experiments performed;language barriers medical;speech translation aims;reliability speech;performed corpus;corpus;design speech;performed corpus results;experiments performed corpus;corpus results;language barriers;translation;overall design speech;speech;medical domain methods;caused language barriers;barriers medical situations", "pdf_keywords": ""}, "6fb3d5a48be16fe1a4cff5e83093b77fbcd1013b": {"ta_keywords": "grid operations privacy;smart grid operations;smart grid;tradeoff smart grid;operations privacy consumers;privacy consumers methodswe;operations privacy;privacy consumers;smart meters;grid operations data;smart meters come;grid installation smart;electrical grid;privacy;installation smart meters;data pose privacy;pose privacy threat;modernization electrical grid;thermostatically controlled loads;privacy threat paper;electrical grid installation;privacy threat;grid operations;direct load control;using thermostatically controlled;pose privacy;using thermostatically;operations data collected;control monitoring;advantages control monitoring", "pdf_keywords": ""}, "032e660447156a045ad6cf50272bca46246f4645": {"ta_keywords": "machine translation variations;machine translation;perform machine translation;translation variations significant;translation variations;perform translation;translation captured standard;perform translation captured;translation captured;translation;effect perform translation;talk gender social;native language;native language influenced;gender social status;talk gender;language influenced;language;gender social;person speaks writes;language influenced number;flavor native language;person speaks;introduction person speaks;gender;tend talk gender;social status geographical;speaks writes;speaks writes flavor;native", "pdf_keywords": "adaptation speakers translation;translation neural machine;improve translation speaker;neural machine translation;speechinduced variations translation;translation neural sequence;machine translation neural;translation neural;machine translation variations;variation improve translation;smoothing improve translation;method improving translation;translate translation neural;explicitly modeling speaker;improving translation;modeling speaker explicitly;machine translation;translation speaker;improve translation methods;speaker explicitly neural;improve translation quality;predicting speechinduced variations;machine translation applications;machine translation method;effective predicting speechinduced;translation decoder learning;machine translation world;machine translation nm;linguistic variations translation;predicting speechinduced"}, "2aad7765250f7d9e312c9382f929ea5239b0fd73": {"ta_keywords": "cell biology tools;biology cell biology;using cell biology;methods cell biology;understanding cell biology;cell biology accomplished;cell biology cell;cell biology;cell biology important;biology cell;tool understanding cell;understanding cell;biology tools methods;biology important tool;biology tools;accomplished using cell;using cell;tools methods cell;cell;biology accomplished using;methods cell;biology;biology important;biology accomplished;tools methods;tools;important tool understanding;tool understanding;tool;important tool", "pdf_keywords": ""}, "a1d578646cf42f2f69ee996742af484d03cc9121": {"ta_keywords": "multilingual language models;mt5 massively multilingual;training massively multilingual;massively multilingual language;massively multilingual;nonlingual language models;massively multilingual version;multilingual language;multilingual nonlingual;language models recently;multilingual nonlingual language;multilingual;multilingual version t5;introductionnmt5 parallel data;multilingual version;language models;language models paper;introductionnmt5 parallel;nonlingual language;wide variety multilingual;variety multilingual nonlingual;unified text;variety multilingual;leveraged unified text;nonlingual;unified text text;incorporating parallel data;models recently mt5;introductionnmt5;incorporating parallel", "pdf_keywords": "machine translation mt5;introductionnmt5 massively multilingual;multilingual pre training;neural machine translation;machine translation objective;machine translation;tasking language modeling;multilingual nonlinguistic tasks;machine translation important;dataset 55m translations;nmt5 large language;machine translation use;translation data;machine translationwe;machine translationwe present;multi tasking language;approach translation data;translation use multilingual;language models pre;small language models;massively multilingual;translation mt5;objectives machine translationwe;pre training parallel;use multilingual pre;multilingual pre;language modeling objectives;55m translations;multilingual;machine translation article"}, "0ba3e29dac0857100935b6eb22bce9cee4afcf17": {"ta_keywords": "integration heterogeneous databases;heterogeneous databases common;heterogeneous databases;databases common domains;heterogeneous databases assumed;databases contain constants;similarity databases contain;databases common;names correspond entities;similarity databases;databases;domains using queries;databases assumed local;databases assumed;databases contain;textual similarity databases;common domains using;names correspond;introduction integration heterogeneous;common domains;integration heterogeneous;queries based textual;domain normalization cases;global domain normalization;queries based;domain normalization;correspond entities;place names correspond;work integration heterogeneous;using queries based", "pdf_keywords": ""}, "cec6de30eea5b4a5a414cf99830fbdb5c56a481c": {"ta_keywords": "parallel video delivery;video delivery massively;video delivery;video delivery allow;massively parallel video;delivery massively parallel;parallel video;device stream movie;device stream;stream movie large;user device stream;architecture massively parallel;stream movie;devices dedicated caches;server devices dedicated;storage bandwidth constraints;devices dedicated;dedicated caches;storage bandwidth;central server devices;reliability storage bandwidth;server devices;cache devices;stream;clinical messagewe propose;bandwidth;clinical messagewe;delivery allow devices;cache devices communicate;act cache devices", "pdf_keywords": ""}, "987658ba918710bbce5de8d92eb44bd127cf72c5": {"ta_keywords": "transcriptions phone phoneme;annotations universal phone;phonemic transcriptions phone;phoneme mappings;supervision phonemic transcriptions;phonemic transcriptions;transcriptions phone;language specific phoneme;phone phoneme mappings;languages speech annotations;speech annotations language;universal speech recognition;speech annotations;shared languages speech;language universal speech;specific phoneme;phonological units spoken;phone phoneme;phone level supervision;producing phonological;speech recognition;speech recognition systems;producing phonological units;languages speech;framework derive phone;universal phone level;derive phone;specific phoneme surface;sound shared languages;entails producing phonological", "pdf_keywords": "multilingual phoneme supervision;language specific phonemes;optimized multilingual phoneme;universal phone transcriptions;modeling phones phonemes;language specific phonemic;phone transcriptions;universal phone recognition;language specific phoneme;multilingual speech recognition;allophone graphs multilingual;phoneme prediction;multilingual phoneme;phone based speech;phoneme prediction prior;universal speech recognition;annotations phone phoneme;specific phonemes;phone phoneme prediction;phone recognition communication;specific phonemes incorporate;phonemic transcriptions phone;specific phonemic annotations;phone recognition;phonemic transcriptions;transcriptions phone;phones phonemes;annotations universal phone;phoneme supervision;supervision phonemic transcriptions"}, "c0a32c68b992b44f1812492c95ac91fb62a6df37": {"ta_keywords": "gradient based bandits;gradient reinforcement learning;reinforcement learning gradient;policy gradient reinforcement;online convex optimization;competitive gradient based;agents employing gradient;competitive gradient;gradient reinforcement;learning algorithms increasingly;policy gradient;learning gradient;framework competitive gradient;bandits certain online;including policy gradient;algorithms including policy;learning gradient based;based bandits;learning algorithms including;reinforcement learning;based bandits certain;online convex;convex optimization algorithms;behavior competitive agents;gradient based learning;limiting behavior competitive;learning algorithms;certain online convex;competitive agents;learning encompasses wide", "pdf_keywords": ""}, "01138945dc9de691cd559d09a46597cca7659efb": {"ta_keywords": "genetics data management;fairness bias;questions fairness bias;fairness bias abound;peer review research;protocols peer review;peer review;questions fairness;data management decision;bias abound socially;genetics data;research question;question genetics data;chosen performance measures;hiring policies;research;research question genetics;setting hiring policies;socially consequential decisions;policies framing research;hiring policies framing;bias;fairness;naively chosen performance;background questions fairness;review research;data management;data designing;framing research question;data designing protocols", "pdf_keywords": ""}, "ca7cd3a90d2953b2f8e45686afa3e79eb3a39add": {"ta_keywords": "predicting edit completions;predict completion edit;edit completions based;predicting edit;edit completions;trained past edits;completions based learned;predict completion;completion edit;problem predicting edit;completions based;past edits;learned model;based learned model;model trained past;completions;completion edit rest;learned model trained;completion;partially edited;snippet partially edited;predicting;predict;model trained;goal predict completion;past edits presentation;edits discussiongiven code;past edits discussiongiven;trained past;partially edited goal", "pdf_keywords": "predicting edits code;predicting contextual edits;predict edit operations;predicting edit completions;approach editcompletion predicting;representing predicting edits;predicts edit operations;predict additional edits;predict edits;predicting edits;editcompletion predicting contextual;novel approach editingcompletion;predicting edit;novel approach editcompletion;useful predicting edits;predicting edits given;predict completion edit;effective predicting edits;editcompletion predict;predicts edits;approach editcompletion neural;editcompletion predicting;predicting contextual code;predict edit;predicts edits given;code given edits;task predict edits;editcompletion predict additional;contextual edits code;editcompletion task predict"}, "d7bebb71635cb818d2f5e0ca0a70434283deb4b6": {"ta_keywords": "behavior resultsimitation learning;human behavior resultsimitation;play learning search;human like gameplay;expert humans self;agent decision making;predicting human actions;strong humanlike policies;humanlike policies multi;multi agent decision;learning search;agent decision;human actions match;humanlike policies;human behavior;building strong humanlike;expert humans;effective predicting human;self play learning;learning search techniques;multi agent;behavior resultsimitation;regularized search;search techniques alzero;examples human behavior;actions match strength;humans self play;gameplay withkl regularized;predicting human;regularized search methodswe", "pdf_keywords": "agents regularized search;chess optimal policy;moves chess optimal;chess optimal;policy optimization search;improves human prediction;regularized game;search strategy agent;high human prediction;policy regularized search;predict human moves;human agents regularized;policy regularization search;algorithm model game;strategy optimization strong;model human gameplay;human prediction accuracy;improving human prediction;game optimization;better human prediction;human moves chess;models human prediction;game search strategy;game ability predict;imitation learned models;search strong human;sparta approximate learned;strategy agent;optimization strong human;predict performance agent"}, "4d7a50f6cfd8f27ebd4d5201fad6c5ef42c33733": {"ta_keywords": "backgroundprepredicting hospital mortality;monitored continuous vital;vital signals collected;intensive care units;clinical notes time;patient health monitored;intensive care;continuous vital signals;hospital mortality;combining clinical notes;health monitored continuous;recorded electronic health;hospital mortality combining;methodsin intensive care;care units icus;notes unlike vital;electronic health records;information clinical notes;clinical notes consisting;vital signals various;icus patient health;clinical notes;vital signals;signals various medical;devices clinical notes;mortality combining clinical;health monitored;doctors recorded electronic;summaries doctors recorded;health records eh", "pdf_keywords": ""}, "1cf2e9e198feef3893da2800a7949f6880ddc084": {"ta_keywords": "non nl evaluation;non nl tasks;implementation non nl;nl evaluation;non nl research;nl evaluation explainaboard;nl tasks effective;nl tasks;nl research emerged;development non nl;nl research;various non nl;non nl;communicated holistic accuracy;nl;conceptualization implementation non;systems various non;systems communicated holistic;holistic accuracy;submitted systems communicated;evaluation explainaboard;performance various systems;evaluation;holistic accuracy numbers;systems communicated;submitted systems;implementation non;new conceptualization implementation;tool track performance;view submitted systems", "pdf_keywords": "implementation natural language;natural language processing;language processing evaluation;natural language;development natural language;organization natural language;functionality standard leaderboard;field natural language;short entities achieving;short entities;systems traditional leaderboard;dealing short entities;language processing;performance long entities;language technology nlp;leaderboard;processing evaluation xpiaboard;leaderboard allows;leaderboard paradigm static;technology nlp new;language processing new;long entities combined;leaderboard paradigm;computational linguistics ncbi;language processing research;standard leaderboard allows;long entities;leaderboard satisfies;technology nlp;evaluation xpiaboard"}, "5665d864d0f1bce6672d6d2bf9f8d8646093cb37": {"ta_keywords": "automated fact checking;fact check challenge;fact checking demonstration;semantic parsing claim;automated fact;present automated fact;parsing claim identification;fact checking;parsing claim;semantic parsing;fact check;expressions knowledge bases;temporal expressions knowledge;expressions knowledge;work semantic parsing;check challenge;parsing;simple numerical claims;numerical claims population;furious fact check;numerical claims;accuracy resultsour;claim identification;claims population german;claims population;semantic;handle temporal expressions;knowledge bases;fact;temporal expressions", "pdf_keywords": ""}, "e050cd9cec5eed73bd56cb2c9726ea85e985384b": {"ta_keywords": "incremental sentence compression;introductioninremental sentence compression;lstm recurrent networks;lstm recurrent;compression using lstm;using lstm recurrent;sentence pretraining method;sentence compression;sentence compression using;lstm;short term memory;recurrent networks;recurrent neural networks;using lstm;sentence pretraining;recurrent neural;lsm recurrent neural;memory lsm recurrent;end sentence pretraining;incremental sentence;autoencoder;pretrained autoencoder proposed;term memory lsm;autoencoder proposed;pretrained autoencoder;method incremental sentence;lsm recurrent;autoencoder proposed results;recurrent networks aimto;compression using long", "pdf_keywords": ""}, "63567f348231abed171c02f99d4c49c2892a2ade": {"ta_keywords": "differentially private training;differential privacy;privacy guarantees differentially;shown differential privacy;differential privacy exacerbate;leaks models trained;rigorous privacy;guarantees differentially private;differentially private;privacy guarantees;privacy;private training mechanisms;achieve rigorous privacy;rigorous privacy guarantees;privacy exacerbate existing;deep learning different;private training;deep learning;privacy exacerbate;introductiondeployment deep learning;leaks models;models trained;data crowd sourced;private;sensitive information;data crowd;contributors leaks models;contains sensitive information;leaks;models trained achieve", "pdf_keywords": "neural networks privacy;amplifies privacy training;privacy training dsm;privacy training;fundamental privacy fairness;differentially private training;privacy fairness;differential privacy;privacy model accuracy;privacy fairness concepts;ofthe privacy fairness;differential privacy model;privacy guarantees differentially;privacy guarantees accuracy;privacy guarantees utility;increasing privacy guarantees;training efforts privacy;reduced increasing privacy;guarantees increasing privacy;impact differential privacy;shown differential privacy;privacy guarantees degrades;rigorous privacy;increasing privacy;model highest privacy;privacy budget;privacy guarantee accuracy;improved increasing privacy;privacy;amplifies privacy"}, "d558c6b953e0267781ed5da90a35c122ba360f10": {"ta_keywords": "complex word identification;word identification cwi;lingual test;lingual test language;lingual task;cross lingual task;cross lingual test;language cross lingual;identification cwi task;lingual;task identifying words;cross lingual;chinese cross lingual;monolingual models;complex word;identifying words phrases;monolingual train test;cwi task identifying;identifying words;best monolingual models;language seen training;cwi task;introduction complex word;lingual task released;training best monolingual;test language cross;train test language;identification cwi;monolingual models relied;cwi", "pdf_keywords": "crosslingual task trained;learning cross lingual;lingual features results;cross lingual features;cross lingual models;cross lingual feature;predicting complex words;language independent features;language crosslingual test;crosslingual model predicting;crosslingual task;task identifying words;performance cross lingual;language unseen training;task crosslingual task;test language crosslingual;task crosslingual;lingual models test;lingual track cognitive;lingual feature;cross lingual experiments;cross lingual analysis;language crosslingual;lingual features;best monolingual models;lingual models;training best monolingual;results crosslingual crosslingual;crosslingual test language;features new monolingual"}, "d33d6c16d7c34dd387841efca74b457b7e60933a": {"ta_keywords": "recursive logic programs;inductive logic programming;programming learning recursive;learning recursive logic;logic programming learning;recursive logic;logic programs;logic programs examples;learning recursive;programming learning;inductive logic;simulation learn twoclause;logic programming;forcedd simulation learn;simulation learn;problem inductive logic;recursive;programs examples;recursive ij;linear recursive;learn twoclause closedd;examples methodswe program;programming;programs examples current;linear recursive ij;learn twoclause;program called force2;determinate programs;closedd linear recursive;crucial problem inductive", "pdf_keywords": ""}, "68ca176c7566067ae4b3311957cc4a134bfbc819": {"ta_keywords": "neural cognitive architecture;cognitive architecture;cognitive architecture inspired;cognitive architecture nca;neural cognitive;cognition learn continuously;supervision neural cognitive;cognition learn;inspired human cognition;human cognition learn;self supervision neural;cognition;human cognition;neural;cognitive;supervision neural;architecture inspired human;learn continuously;distinct perception action;architecture inspired;architecture;perception action;combined self supervision;architecture nca inspired;self supervision;multiple distinct perception;nca inspired human;supervision combined self;learn continuously solve;distinct perception", "pdf_keywords": ""}, "163c6b06d948d0869eb8173b537c441c9a786977": {"ta_keywords": "capacitated congestion games;process congestion games;congestion games methodsa;congestion games;md congestion games;congestion games equivalent;congestion games mds;concepts capacitated congestion;constraints md congestion;capacitated congestion;decision process congestion;tolls incentives reward;imposing tolls incentives;model social planner;congestion;tolls incentives;constraint satisfaction markov;md congestion;simulated city downtown;social planner;utilized social planner;planner;satisfaction markov decision;process congestion;markov decision process;social planner draw;simulated city;downtown model social;markov decision;constraint satisfaction", "pdf_keywords": "capacitated congestion games;congestion games markov;game equilibrium reward;equilibrium driver game;game equilibrium provide;solve game equilibrium;selfish agents solve;congestion games;congestion games continuous;population selfish agents;shifting game equilibrium;process congestion games;game equilibrium;equilibrium reward;games markov decision;congestion payoff strategy;players reward functions;shift game equilibrium;game equilibrium augmenting;constrainting players behaviour;games continuous population;agents solve markov;continuous population selfish;congestion games extension;classic congestion games;equilibrium reward adjustments;explicitly constrainting players;planner shifts equilibrium;constrainting players;games markov"}, "2b63812db40152b12925ce4a848b929fa591b858": {"ta_keywords": "messageneural machine translation;machine translation sequence;sequence models useful;sequence sequence models;sequence models;key clinical messageneural;clinical messageneural machine;clinical messageneural;translation sequence sequence;machine translation;translation sequence;model sequential data;sequential data;model sequential;sequential;wants model sequential;key clinical;messageneural machine;clinical;sequence sequence;sequence;implementation exercise readers;readers test understood;test understood content;understood content practice;messageneural;models useful tool;models;models useful;readers test", "pdf_keywords": "statistical machine translation;machine translation sequence;neural translation models;neural machine translation;machine translation;machine translation neural;translation neural machine;machine translation natural;machine translation development;machine translation systems;translation models;sentence neural translation;translation translation algorithms;generating translations based;translations model neural;neural translation;model generate sentences;generating translations;translation natural language;machine translation sm;translation neural;machine translation increasing;generate translations model;translation algorithms;translation models useful;machine translation important;machine translation uses;algorithm generating translations;generate translations;translation systems"}, "8b608ad2ec6d0300b6a0bb8f616d4a2b01150693": {"ta_keywords": "topic tracking model;topic tracking;track changes topics;introductionapplication topic tracking;topic word extraction;methodsthe topic tracking;applies topic tracking;changes topics based;estimated topic models;topics based;topic changes;recognition topic word;changes topics;topic models;language model adaptation;tracking model language;speech recognition topic;word extraction meeting;topic changes paper;model adaptation speech;adaptation meeting analysis;model adaptively track;tracking model adaptively;estimated topic;topic word;previously estimated topic;adaptation speech recognition;topics;adaptation speech;adaptively track changes", "pdf_keywords": ""}, "0a4b8b161931799d5c6bc3ecf07c53bae0e9e502": {"ta_keywords": "003 resources likewikipedia;language models;language 006 modeling;language models increasingly;introduction language models;007 quality filtering;suitable language 006;web dumps diverse;likewikipedia books 004;content 003 resources;diverse text data;quality filtering;resources likewikipedia;web text suitable;quality filtering methods;likewikipedia;likewikipedia books;books 004 news;dumps diverse text;language 006;web text;text suitable language;resources likewikipedia books;diverse text;selecting web text;web dumps;001 web dumps;content 003;005 selecting web;undesirable content 003", "pdf_keywords": "news data quality;quality filter corpus;assessing quality text;newspapers investigate language;filter corpus;ofwikipedia newswire popular;information new corpus;ofwikipedia newswire;constructing corpus avoid;web text suitable;likewikipedia;likewikipedia books newswire;corpus internet;content resources likewikipedia;world analyse corpus;automated valuation ofwikipedia;analyse corpus internet;filter corpus school;constructing corpus;quality text dataset;knowledge quality filter;language models article;language modeling corpora;quality text critical;modeling corpora quality;language models;resources likewikipedia;status natural language;corpus internet using;quality text fundamental"}, "7a737872a6693ba3f0c99651191b93dad0dadcee": {"ta_keywords": "speech dirization challenge;dihard speech dirization;speech dirization;neural dirization vector;dirization vector clustering;neural dirization;dirization challenge resultsthe;dirization challenge;end neural dirization;dirization vector;dirization;vector clustering systems;clustering systems combined;clustering systems;vector clustering;hitachi jhus dihard;clustering;hitachi jh submitted;ensemble results subsystems;hitachi jhus;hitachi jh;ensemble results;outputs ensemble;dihard speech;jh submitted dihard;submitted dihard speech;purposethe hitachi jhus;ensemble;jhus dihard;outputs ensemble results", "pdf_keywords": "voice activity detectors;voice activity detection;models voice activity;elucidation speech diarization;voice activity;application voice activity;speech diarization challenges;speech recognition fundamental;analyze speech recognition;speech recognition new;speech diarization;estimated speech activities;speech recognition;speech diarization challenge;speech recognition single;dover speech recognition;estimates speech activities;assess speech activities;recognition single speech;automatically estimates speech;speech activities variety;approach speech recognition;speech recognition article;generating speech activities;process speech recognition;activity detectors vads;variety speech systems;xvector assess speech;employ voice activity;speech systems"}, "4d44f2c3f269ea6cbc840b99c3f8119a13829509": {"ta_keywords": "model malignant disease;etiology malignant disease;malignant disease;etiology malignant;patient malignant disease;disease patient malignant;malignant disease patient;model malignant;new model malignant;patient malignant;malignant disease poorly;malignant;disease patient;disease;disease poorly understood;etiology;disease poorly;patient;development new model;model;new model;understood development new;understood development;poorly understood development;poorly understood;development new;development;understood;new;poorly", "pdf_keywords": ""}, "94c0a8be74d69787a1f3f6e91dcb480a2fd0dd56": {"ta_keywords": "reasoning natural language;reasoning knowledge;language models programs;introduction reasoning natural;introduction reasoning;reasoning knowledge possessed;harvest reasoning knowledge;models inadequate reasoning;existing language models;pre training language;training language models;models programs execution;reasoning;language models;natural language;reasoning natural;language models harvest;training language;models programs;reasoning address;knowledge possessed program;models harvest reasoning;inadequate reasoning address;natural language longstanding;program executors;language models inadequate;reasoning address issue;empowers language models;existing language;executors data driven", "pdf_keywords": "improving reasoning capabilities;boosting reasoning capability;increase reasoning capability;learning reasoning;paradigm boosting reasoning;approaches learning reasoning;reasoning capabilities;various reasoning capabilities;models reasoning skills;inject reasoning skills;increase reasoning;reasoning capability language;learning reasoning skills;reasoning capability existing;improving reasoning;reasoning skills natural;language models reasoning;boosting reasoning;reasoning capability;reasoning ability models;reasoning skills;demonstrate reasoning capability;reasoning database;reasoning skills development;reasoning skills novel;reasoning knowledge;approach improving reasoning;reasoning skills fundamental;reasoning ability;reasoning skills association"}, "de41f897ea6ca5447cfae81e9505f94ccf50e6a5": {"ta_keywords": "jias", "pdf_keywords": ""}, "0a4dd1e51616b422aa2d437610dbfbdd3733a114": {"ta_keywords": "dialog human human;dialog human;objective dialog human;methods dialog human;human conversation example;human human conversation;conclusion dialog human;human conversation;conversation example objective;results dialog human;conversation example methods;dialog;background dialog human;example objective dialog;conversation example;objective dialog;conversation example results;conversation;example methods dialog;methods dialog;conclusion dialog;example conclusion dialog;conversation example conclusion;background dialog;example results dialog;results dialog;human human;human;example objective;objective", "pdf_keywords": ""}, "d5ec188a5a39e504788c1fe33457eeb816a99f31": {"ta_keywords": "grammar induction focuses;unsupervised grammar induction;grammar induction model;constituency grammar induction;grammar induction;grammar induction case;visually grounded syntax;learning phrasal dependency;unsupervised grammar;propose unsupervised grammar;grounded syntax models;multimodal information leading;work grammar induction;phrasal dependency structure;syntax models;learning phrasal;induction focuses learning;performance constituency grammar;constituency grammar;grammar;focuses learning phrasal;syntax models make;phrasal dependency;multimodal information;visually;induction focuses;visually grounded;multimodal;use multimodal information;leverages word concreteness", "pdf_keywords": "neural syntax acquisition;dependency induction neural;neural functional grammar;functional grammar induction;constituency grammar induction;neural syntax;grammar induction focuses;model constituency parsing;dependency parsing vision;inducing dependency structures;dependency induction vision;grammar induction model;constituency parsing dependency;constituency dependency parsing;learning phrasal dependency;formal grammar induction;unsupervised grammar induction;grounded syntax models;concreteness neural;concreteness neural neural;neural neural language;neural language;concept concreteness neural;word concreteness priors;development neural syntax;grammar induction;constituency parsing performance;word concreteness structural;grammar induction methodsin;level word concreteness"}, "d878828c2345b665ab9651f20fb0e60e1ffe9de5": {"ta_keywords": "gaas heterostructrued sample;gaas heterostructrued;heterostructrued sample al;7as gaas heterostructrued;al nuclear spins;strains al0 3ga0;strains caused heterostructure;heterostructrued sample;detecting strains al0;sample al nuclear;heterostructure;caused heterostructure interfaces;nmr technique analyzing;3ga0 7as gaas;heterostructrued;heterostructure interfaces;resonance nmr technique;heterostructure interfaces challenge;strains al0;nmr technique;caused heterostructure;magnetic resonance nmr;resonance nmr;al0 3ga0 7as;custom spectrometer succeeded;spectrometer succeeded;spectrometer;custom spectrometer;nuclear spins based;al0 3ga0", "pdf_keywords": ""}, "84e566e326b64b105cabf0c47dff336c4f632a1c": {"ta_keywords": "synthesis new;synthesis;new", "pdf_keywords": ""}, "18268bdfc8a6e0a51f373bc4acf65c8b9a7bd6a0": {"ta_keywords": "neural machine translation;machine translation models;translation models;machine translation;translation models using;translate data aetiology;translation models used;translation models useful;patients aetiology aetiology;patients aetiology;prediction error correction;neural machine;translate data;data aetiology disease;models used translate;binarized prediction error;correction demonstrate neural;using binarized prediction;aetiology disease;binarized prediction;used translate data;aetiology disease assess;treatment patients aetiology;analysis neural;models using binarized;error correction;development neural machine;translation;aetiology;neural", "pdf_keywords": ""}, "9e77f94e5a12cb33b8b464dc834fd81da1a609e2": {"ta_keywords": "stability problem delay;problem delay dynamical;delay dynamical;delay dynamicmical;stability results delay;delay dynamicmical novel;lyapunov krasovskii functional;results delay dynamicmical;delay product type;delay product;improved delay product;asymptotic stability problem;delay dynamical dds;asymptotic stability;problem delay;general lyapunov krasovskii;lyapunov krasovskii;introductionnew structural stability;delay;structural stability results;stability problem;focuses asymptotic stability;structural stability;improved delay;krasovskii functional;stability results;construct general lyapunov;general lyapunov;resultsan improved delay;lyapunov", "pdf_keywords": ""}, "24219135d563b1cb24523bf522366c91a55d7604": {"ta_keywords": "thresholding multi label;multi label classifiers;label classifiers;classifier withf1 optimal;optimal thresholding multi;multi label;performance multi label;introductionf1 optimal thresholding;withf1 optimal thresholding;multi label setting;thresholding multi;optimal threshold best;label classifiers methodswe;optimal thresholding;thresholding predict instances;optimal threshold;optimal thresholding predict;classifier withf1;threshold best achievable;binary classifier withf1;threshold best;classifiers;relationship optimal threshold;classifier;thresholding predict;label setting;label;thresholding;binary classifier;threshold", "pdf_keywords": ""}, "274b4ad4840b0a8a70c5bac3fe4b4861ce5fbb95": {"ta_keywords": "shot relation classification;relation classification dataset;relation classification;methods relation classification;relations derived fromwikipedia;relation classification conduct;annotated crowdworkers relation;shot relation;sentences 100 relations;relations;relation sentence recognized;distant supervision methods;fromwikipedia annotated crowdworkers;crowdworkers relation sentence;relation;relation sentence;100 relations;shot learning methods;shot learning;recognized distant supervision;annotated crowdworkers;relations derived;100 relations derived;present shot relation;crowdworkers relation;derived fromwikipedia annotated;fromwikipedia annotated;distant supervision;state shot learning;learning methods relation", "pdf_keywords": "shot relation classification;relation classification fewshot;shot relationship classification;relation classification dataset;supervised shot relationship;distantly supervised relation;supervised relation extraction;relations derived fromwikipedia;relation classification task;relation classification;view relation classification;relationship classification dataset;relationship classification;supervised relation;dataset shot relation;distantly supervised supervised;approach distantly supervised;relation classification problem;sentences inwikipedia;classification fewshot learning;neural relation extraction;large scale supervised;derived fromwikipedia annotated;methods relation classification;distantly supervised;fromwikipedia annotated crowdworkers;fromwikipedia annotated;fewrel shot relation;dataset fewrel shot;shot learning methods"}, "78e838bcd2268260ddce6be6db4907df6f29f04f": {"ta_keywords": "emotions expressed text;anime corpus;anime corpus methodsmost;emotion recognition speech;comics anime corpus;emotions expressed;text balloons based;automatic speech;research emotion recognition;emotion recognition;balloons based linguistic;generation text balloons;considerable emotions expressed;features comics anime;text balloons;text based;expressed text based;recognition speech;linguistic acoustic features;methodsmost automatic speech;text based communication;comics anime;based linguistic acoustic;anime;acoustic features comics;recognition speech recently;automatic speech recognition;expressed text;emotions;text", "pdf_keywords": ""}, "66f7d22d6373af5032074b25828331958b07e7f9": {"ta_keywords": "differentially private training;neural networks dns;private training dns;training deep neural;data training deep;training deep;diagnosis differentially private;deep neural;privacy;providing better privacy;better privacy;privacy utility;personal data training;better privacy utility;deep neural networks;differentially private;personal data;private training;training dns;privacy utility trade;data training;training dns surging;dns tasks medical;neural networks;neural;use personal data;private;use personal;attention given interpretability;dns surging importance", "pdf_keywords": "differential privacy deep;privacy deep learning;differentially private learning;privacy deep;privacy overall interpretability;privacy interpretationability;private learning;differential privacy;privacy explanation quality;privacy explanation;knowledge differential privacy;privacy overall;privacy interpretationability aim;implications differential privacy;privacy;privacy budget effect;privacy budget;space privacy interpretationability;interpretability masked models;privacy utility;effect privacy overall;trade space privacy;better privacy utility;better privacy;providing better privacy;effect privacy;differentially private;privacy utility trade;levels privacy budget;private learning dbp"}, "26a238217321008cd1daaa649683d461e16e7574": {"ta_keywords": "historical text normalization;text normalization;text normalization prohibitive;policy gradient training;learning policy gradient;training neural sequence;text normalization albeit;models policy gradient;sequence sequence models;normalization albeit outperformed;neural sequence sequence;phrase based models;sequence models;neural sequence;normalization;context training neural;gradient training;datasets historical text;sequence models simple;policy gradient;context training;reinforcement learning policy;policy gradient fine;approach historical text;normalization prohibitive;outperformed phrase based;training neural;historical text;gradient training enables;normalization prohibitive scratch", "pdf_keywords": ""}, "eec490a41bdc716fccf98f4a7996c1d31334985a": {"ta_keywords": "symbolic music generation;music generation muspy;library symbolic music;components music generation;music generation including;music generation;symbolic music;generation including dataset;components music;essential components music;muspy open source;music;generation muspy provides;datasets;muspy provides easy;datasets currently;dataset;dataset management;python library symbolic;source python;statistical analysis datasets;analysis datasets;data preprocessing;analysis datasets currently;generation muspy;dataset management data;python library;data data preprocessing;muspy provides;library symbolic", "pdf_keywords": "analysis music data;music generation applications;music data useful;music data;generate music fundamental;symbolic music generation;applications music generation;music generation systems;formats music generation;tools developing music;generation applications music;music generation including;generate music;muspy formats music;symbolic music libraries;music generation development;developing music generation;symbolic music formats;music generation;music generation present;music data addition;music generation provides;music generation provide;library symbolic music;development music generation;ability generate music;tool analysis music;music applications;processing music applications;music libraries"}, "dc26c3775d233a5fa9516d21fee12aa5b46f8a25": {"ta_keywords": "introductioninformation extraction scientific;extraction scientific literature;introductioninformation extraction;finding relevant papers;method recommendation research;extraction scientific;relevant papers automatically;papers automatically understanding;scientific literature;scientific literature important;method recommendation;introductioninformation;methods finding relevant;advances search;advances search engines;recommendation research;recommending potential methods;literature important tool;despite advances search;improved methods finding;search engines;methods finding;recommending;tool method recommendation;ideas recommending;key ideas recommending;recommendation research community;finding relevant;papers automatically;important tool method", "pdf_keywords": "relation extraction unsupervised;scientific knowledge graph;information extraction scientific;relation extraction important;relation extraction;scientific information extraction;resourcesthe knowledge graph;information extraction;knowledge graph;knowledge base information;relations orwikipedia explore;extraction scientific literature;build knowledge graph;approaches knowledge graph;extraction unsupervised;relations knowledge base;knowledge graph construction;knowledge graph leveraging;knowledge base embedding;information extraction develop;extraction unsupervised supervised;knowledge bases;relations orwikipedia;knowledge graph inference;scientific term extraction;relations keywords useful;methods knowledge graph;turning unstructured information;knowledge base;semantic relations keywords"}, "ef2e2f3a847667000b591c8708b543eaf259113b": {"ta_keywords": "lstm triple threat;speech recognition lsms;lstm triple;threat speech recognition;lstm;paper demonstrate lstm;recurrent neural networks;short term memory;memory recurrent neural;demonstrate lstm triple;speech recognition speech;neural networks lsms;demonstrate lstm;speech recognition;recognition speech;term memory recurrent;memory recurrent;recognition speech recognition;recurrent neural;acoustic modeling language;recognition lsms;triple threat speech;term memory;sequential learning;microphone;modeling language modeling;recognition lsms drive;threat speech;lsm trifecta;sequential learning tasks", "pdf_keywords": ""}, "b176a46ec214b9f75df751dcd2c894f0a7a72a9a": {"ta_keywords": "supervised argumentation mining;argumentation mining user;argumentation mining;argumentation mining evolving;semi supervised argumentation;supervised argumentation;attention argumentation mining;discourse alyzing arguments;web discourse alyzing;generated web discourse;arguments user generated;argumentation;debateate;web discourse recently;web discourse;attention argumentation;gained attention argumentation;alyzing arguments user;introductionexploiting debateate portals;arguments user;discourse recently gained;debateate portals;discourse alyzing;discourse;introductionexploiting debateate;debateate portals semi;arguments;discourse recently;alyzing arguments;mining user generated", "pdf_keywords": ""}, "83145b7a391b792e24d8d38f74ed6b6ae7a149dc": {"ta_keywords": "aware machine translation;translation systems use;including context diminishing;usage context aware;usage context models;increasing context usage;translation systems;context including context;source context including;context models;context referenced source;referenced source context;machine translation methodsa;quantify usage context;target context referenced;machine translation systems;context models using;machine translation;context aware machine;context diminishing;context usage context;source context;level machine translation;context aware;context referenced;target context;including context;context including;increasing context;context usage", "pdf_keywords": "translation models trained;aware machine translation;machine translation models;translation model trained;aware neural translation;machine translation cross;translation models;neural translation model;machine translation impact;translation models using;neural machine translation;trained translate source;machine translation systems;machine translation implications;translation systems;contextual model trained;machine translation;translation model;machine translation important;translation model significantly;context model predictions;machine translation introduce;contextual models compared;model trained translate;translation systems use;translation process context;neural translation;contextual models;context better nonpretrained;trained translate"}, "45eea76ac46b402f3a209de57e469275419fdc9e": {"ta_keywords": "native language reading;unknown word detection;eyes gaze based;language reading;gaze based unknown;word detection;word detection non;gaze based;eyes gaze;language reading considering;non native language;native language;gaze;detection non native;reading considering speech;speech tag personalization;non native;speech tag;native;considering speech tag;reading;based unknown word;reading considering;eyes;tag personalization;tag personalization useful;language;personalization useful;personalization useful tool;personalization", "pdf_keywords": ""}, "94a11c9425bf5f4f9b8ed1b07ea1d15a81b96e9f": {"ta_keywords": "development crowdsourced applications;development crowdsourced;crowdsourced applications;crowdsourced applications mechanical;crowdsourcing;crowdsourcing increasingly;crowdsourced;online crowdsourcing;crowdsourcing topics investigated;frameworks online crowdsourcing;design development crowdsourced;mechanical turk web;crowdsourcing topics;online crowdsourcing topics;crowdsourcing increasingly popular;applications mechanical turk;mechanical turk;turk web based;collaborative software;crowd participation parallel;turk web;crowd participation;ensure crowd participation;projects tool;competitive collaborative software;participants collaboration;incentives ensure crowd;development projects tool;participants collaboration process;projects tool data", "pdf_keywords": ""}, "222ae836430ad0c922b47a9345c17212f9584097": {"ta_keywords": "gingival display gummy;orthognathic surgery periodontal;display gummy smile;introduction excessive gingival;excessive gingival;excessive gingival display;display gummy;gummy smile;surgery periodontal;gingival display;orthognathic surgery;smile major esthetic;surgery periodontal plastic;gummy smile major;gummy;periodontal;periodontal plastic procedures;gingival;range orthognathic surgery;dentoalveolar;orthognathic;dentoalveolar soft;periodontal plastic;range orthognathic;skeletal dentoalveolar;lip repositioning surgery;dentoalveolar soft tissue;etiologies skeletal dentoalveolar;procedures lip repositioning;skeletal dentoalveolar soft", "pdf_keywords": ""}, "723770d9ac418e923db5e087ae18c04702f5986e": {"ta_keywords": "rule learner described;built rule learners;new rule learner;rule learner generates;rule learner;rule learners ensemble;learner generates rulesets;effiictive rule learner;rule learners;rulesets repeatedly boosting;greedy rule builder;generates rulesets;rule builder;learners ensemble rules;rulesets built rule;rule builder like;generates rulesets repeatedly;built rule;builder like rulesets;rulesets built;rulesets;ensemble rules created;rules created slipper;ensemble rules;rulesets repeatedly;like rulesets built;like rulesets;fast effiictive rule;repeatedly boosting simple;greedy rule", "pdf_keywords": ""}, "b9ede62d1d586e1a3b1ef7ec046f09e4e35639bf": {"ta_keywords": "retrieval pipelines commonly;retrieval pipelines;mismatch retrieval pipelines;term based search;based search obtain;search obtain candidate;vocabulary mismatch retrieval;introductionretrieval pipelines commonly;retrieval;based search;introductionretrieval pipelines;pipelines commonly rely;pipelines commonly;mismatch retrieval;pipelines;search obtain;candidate records subsequently;subsequently ranked candidates;search;term based;rely term based;obtain candidate records;candidate records;ranked candidates;records subsequently ranked;approach vocabulary mismatch;approach vocabulary;ranked candidates missed;obtain candidate;subsequently ranked", "pdf_keywords": "retrieval methods similarity;similarity function retrieval;substantially shorter retrieval;retrieval algorithm similarity;shorter retrieval;term based search;similarity function search;answers employ retrieval;nearest neighbor search;retrieval information retrieval;nn retrieval algorithm;retrieval methods;search using similarity;retrieval algorithm;generic nn retrieval;search words;retrieval important tool;expensive accurate similarity;multiple retrieval methods;retrieval based;broader search tasks;retrieval results;similarity score query;answer translation model;user friendly search;lexical match based;corpus provide large;retrieval applications combine;nn retrieval;lucene accuracy search"}, "6dafc41e9bbd3aa476a0a1c15ca2c459eaef6b98": {"ta_keywords": "morphology inflection;morphology inflection fundamental;neural seq2seq models;neural seq2seq;domain morphology inflection;generic neural seq2seq;morphology;inflection;seq2seq models;seq2seq models little;seq2seq;introduction domain morphology;inflection fundamental important;inflection fundamental;languages task;languages;domain morphology;neural;accuracy scores languages;languages task considered;scores languages;scores languages task;relatively generic neural;generic neural;sigorhon;models;years sigorhon;recent years sigorhon;sigorhon shared;sigorhon shared task", "pdf_keywords": "datasets morphological inflection;morphological inflection splitting;morphological models improve;evaluating morphological models;morphological inflection;difficult datasets morphological;datasets morphological;employed evaluating morphological;splitting morphological datasets;inflection tables learning;evaluating morphological;task morphological reinflection;morphological datasets;morphology inflection;morphological datasets lemma;splitting morphological;morphological models;shared task morphological;tables learning inflect;morphological models discussed;neural seq2seq models;morphological reinflection;morphology inflection fundamental;learning inflect;morphological;task morphological;learning inflect uncoveredthe;morphology association computational;morphological reinflection 52;method splitting morphological"}, "21066ab388b386f3d3552a4a4c25322e0ca69632": {"ta_keywords": "hypernymy extraction based;extraction hypernyms based;improves hypernymy extraction;extraction hypernyms;hypernymy extraction;approach extraction hypernyms;supervised relation extraction;relation extraction;hypernyms based projection;relation extraction impact;learning word embeddings;introductionnegativeative sampling improves;sampling improves hypernymy;hypernyms based;word embeddings;examples supervised relation;introductionnegativeative sampling;projection learning word;hypernyms;hyponym hypernym pairs;hypernym pairs;supervised relation;hyponym hypernym;introductionnegativeative;candidate hyponym hypernym;learning word;hypernym pairs natural;hypernym;improves hypernymy;examples supervised", "pdf_keywords": "hypernymy extraction regularized;regularization synonyms negative;extraction hypernymy relations;learning word embeddings;extraction hypernyms;approach hypernymy extraction;regularization synonyms;extraction hypernyms based;based regularization synonyms;word embeddings;hypernyms synonyms negative;pairs word embeddings;model hypernymy extraction;hypernymy extraction;approach extraction hypernyms;hypernymy extraction using;extraction hypernymy;word embeddings available;model extraction hypernymy;hypernymy relations based;projection hypernyms synonyms;regularization uses relations;hypernyms synonyms;context hypernymy prediction;word embeddings contrast;projection learning word;hypernyms based;examples neighbor regularization;hypernymy relations;hypernyms based projection"}, "900ce63d71dce47059434cdf2d5e1d77bc716e8d": {"ta_keywords": "data management;data management analysis;technologies data management;new technologies data;technologies data;data;analysis development new;management analysis development;analysis development;management analysis;development new technologies;new technologies;analysis;management;technologies;development new;development;new", "pdf_keywords": ""}, "8ae392fc9acbada67a4288a6affc2a77f83befcd": {"ta_keywords": "speech machine translation;neural machine translation;machine translation nmt;discretalk text speech;text speech machine;text speech e2e;machine translation;text speech;speech e2e tts;end text speech;speech machine;introductiondiscretalk text speech;machine translation problem;speech e2e;translation nmt resultsthe;translation nmt;translation problem methodsthis;evaluation discretalk text;e2e tts model;tts model based;neural machine;discretalk text;end text;speech;translation problem;end end text;e2e tts;tts model;model based neural;text", "pdf_keywords": "variational autoencoder vqvae;autoregressive model speech;autoencoder vqvae model;model speech reconstruction;autoencoder vqvae;speech use autoregressive;variational autoencoder;speech non autoregressive;virtualq vae neural;quantized variational autoencoder;neural machine translation;learns mapping speech;model speech;2e speech processing;autoregressive gamut speech;speech reconstruction;speech reconstruction corpus;autoencoder;autoregressive gan based;automatic decoding model;vae neural machine;end text speech;speech waveform trained;machine translation nm;decoding speech;speech e2ettes model;mapping speech;autoregressive corpus model;new speech recognition;autoregressive gan"}, "317d95f99ef62237f6c7d7834d1d19027166b392": {"ta_keywords": "imitation learning structured;structured prediction end;end language generation;structured prediction explore;imitation learning;learning structured prediction;generate sequences words;structured prediction;language generation;learning structured;approaches structured prediction;employs imitation learning;language generation submitted;architectures generate sequences;prediction explore;decoder architectures generate;e2e nonligand challenge;generate sequences;imitation;prediction explore large;decoder;decoder architectures;prediction end end;end language;generate;encoder;encoder decoder architectures;nonligand challenge methodsour;encoder decoder;prediction end", "pdf_keywords": ""}, "db190db2567c334b772fd653dca10f300074e421": {"ta_keywords": "speaker adaptation end;supervised speaker adaptation;speaker adaptation;speech synthsynthesis pretrained;semi supervised speaker;speech data training;end speech synthsynthesis;speech synthsynthesis;text speech data;supervised speaker;paired text speech;speech data;minutes speech recordings;speech recordings;speech tts models;text speech;end text speech;text speech tts;synthsynthesis pretrained models;speech tts;synthsynthesis pretrained;pretrained models;pretrained models methods;adaptation end end;adaptation end;end speech;end end speech;minutes speech;semi supervised;introduction semi supervised", "pdf_keywords": ""}, "02cbb0db288af2c83b48a023f245812bd22a2408": {"ta_keywords": "semistructured data metrics;datasets generating text;table text generation;text semi structured;semistructured data;reference texts rely;corresponding semistructured data;reference texts diverge;divergent reference texts;texts evaluating table;generating text semi;semi structured data;text generation;texts diverge information;tables wikibio;tables wikibio contain;automatically constructed datasets;constructed datasets generating;data tables wikibio;reference texts blueu;contain reference texts;wikibio contain reference;reference texts evaluating;information corresponding semistructured;text generation automatically;generating text;handling divergent reference;structured data tables;solely reference texts;semistructured", "pdf_keywords": "semistructured data metrics;unstructured generated reference;metric measuring text;generated reference texts;judgments existing metrics;human judgments web;text semi structured;measuring text generation;metrics human judgments;judgments web compare;judgments web based;corresponding semistructured data;texts diverge information;text models;judgments web;text evaluation accuracy;information corresponding semistructured;natural language generation;high quality references;judgments references diverge;text evaluation;semistructured data;table text models;generate text describing;challenge automatic metrics;models entailment probability;datasets generating text;human judgments references;records text evaluation;entailment probability"}, "20937a0f03bcb845afbedda901a6d4e93a2b5c34": {"ta_keywords": "occurrence syndromic disease;syndromic disease world;syndromic disease;disease caused disease;caused disease;caused disease caused;occurrence syndromic;world disease caused;disease caused;disease;syndromic;world disease;disease world;major cause morbidity;mortality world disease;disease world major;cause morbidity mortality;cause morbidity;morbidity mortality world;world major cause;morbidity mortality;mortality;caused;mortality world;morbidity;major cause;occurrence;world major;world;cause", "pdf_keywords": ""}, "b8e2e764ac82f81a5bc645c818d0d5ad7806e806": {"ta_keywords": "perform speech separation;speech separation recognition;speech separation;separation recognition challenges;separation recognition challenge;physicians perform speech;separation recognition;challenging clinician distinguish;perform speech;recognition challenges task;task challenging clinician;recognition challenges;clinician challenging;challenging clinician challenging;speech;clinician distinguish;separation;challenging task clinician;clinician distinguish groups;clinician challenging clinician;challenging clinician;recognition challenge challenge;recognition challenge;task clinician;challenges task challenging;challenge challenging task;task challenging;task clinician physicians;challenge challenging;clinician", "pdf_keywords": ""}, "7c976b0b54ace7d13b87e8feefe6f29c0599d78d": {"ta_keywords": "semantic word network;inducing semantic word;inducing semantic;method inducing semantic;semantic relations;semantic word;introductiona semantic word;introductiona semantic;semantic;word network;distributional semantics methods;semantic relations individual;semantics methods relations;resources distributional semantics;represents semantic relations;network represents semantic;semantics methods;distributional semantics;relations individual words;word network network;hierarchical contexts using;semantics;dictionary resources distributional;word network sw;words lexical senses;words lexical;represents semantic;unsupervised method inducing;lexical senses paper;hierarchical contexts", "pdf_keywords": ""}, "a83bbc7bf70b1beedbfe0140d24d556e2dc5acc8": {"ta_keywords": "analyzing etiology disease;etiology disease discussed;etiology disease;etiology disease described;described etiology disease;disease described etiology;method analyzing etiology;described etiology;analyzing etiology;etiology;disease described;disease;disease discussed;method analyzing;analyzing;new method analyzing;method;described;new method;discussed;new", "pdf_keywords": ""}, "2aaf2ee779cd4ff0f26bb73958ea9fb0faa61907": {"ta_keywords": "identifying fluorescence microscopees;identifying fluorescence microscope;microscopees ononline jajournal;fluorescence microscopees ononline;subcellular location image;fluorescence microscope images;aim identifying fluorescence;image finder lip;fluorescence microscopees;fluorescence microscope;lips identify fluor;protein subcellular location;identifying fluorescence;subcellular location;microscopees ononline;factors identifying fluorescence;subcellular location patterns;fluorescence;microscope images;built subcellular location;protein subcellular;regarding protein subcellular;text images journal;identify fluor;location image finder;finder lip;image finder;lip extracts information;microscope;microscopees", "pdf_keywords": ""}, "08f199ebfd27a5f9ada79edd07ac41e46c7278d5": {"ta_keywords": "analysis autism spectrum;analysis autism;methodsfactoral analysis autism;autistic traits using;autism spectrum;introductionmeasurement autistic traits;autism;autistic traits;autism spectrum quotient;introductionmeasurement autistic;non verbal communication;socialization implemented movie;communication socialization implemented;human communication socialization;contributing communication socialization;verbal communication skills;non verbal information;communication socialization;verbal communication;enhance human communication;communication skills;ipad application nocoa;autistic;communication socialization real;information contributing communication;verbal information contributing;contributing communication;nocoa includes movie;using non verbal;movie data", "pdf_keywords": ""}, "34d5d2f75934caff89311ef20d18a275da5abb47": {"ta_keywords": "learn multiple;learn multiple examples;ability learn multiple;examples important learning;learning learning;important learning learning;new approach learning;learning;learning learning development;approach learning;ability learn;important learning;learning development;multiple examples;learn;learning development new;multiple examples important;multiple;examples;examples important;new approach;approach;development;development new approach;ability;development new;new;important", "pdf_keywords": ""}, "fbf2a6a887ea92311cf207d522c535daf867a6ba": {"ta_keywords": "text speech synthsynthesis;speech synthsynthesis;speech synthsynthesis case;speech tacs synthesis;trained text embedding;trained text embeddings;trained embeddings text;pre trained text;pre trained embeddings;trained representation bert;enhanced text speech;introductionpre trained text;end text speech;text speech;text embeddings enhanced;text embeddings;trained text;text embedding input;embeddings enhanced text;text embedding;text speech tacs;pre trained representation;embeddings text;representation bert;trained embeddings;synthsynthesis case presentationwe;synthesis model explicitly;embeddings text resultsafter;synthsynthesis;synthesis", "pdf_keywords": ""}, "da06caf4f340ebc81395f092f9dc3a3101827506": {"ta_keywords": "el speech enhancement;electrolaryngeal el speech;speech enhancement;speech enhancement methods;statistical voice conversion;sounds produce electrolaryngeal;electrolarynx device artificially;speech proficient laryngectomees;voice conversion;produce electrolaryngeal el;introduction electrolarynx device;electrolarynx device;el speech using;laryngectomees produce intelligible;using statistical voice;introduction electrolarynx;electrolarynx;proficient laryngectomees produce;statistical voice;electrolaryngeal el;proficient laryngectomees;speech using device;produce electrolaryngeal;intelligible el speech;voice conversion showed;proposed el speech;laryngectomees;laryngectomees produce;el speech proficient;el speech", "pdf_keywords": ""}, "acc2ad56a9c68c799747e08d978f9803997c1527": {"ta_keywords": "materials synthsynthesis planning;materials synthsynthesis;synthesis insights;synthsynthesis planning literature;synthesis insights methods;literature trained neural;introductioninorganic materials synthsynthesis;synthsynthesis planning;literature inorganic synthesis;inorganic synthesis insights;scientific literature inorganic;synthesis planning driven;inorganic synthesis;literature inorganic;synthesis;synthesis planning;synthsynthesis;materials design discovery;connecting scientific literature;embeddings language models;introductioninorganic materials;planning literature trained;language models;word embeddings;inorganic;trained neural;word embeddings language;neural networks;literature trained;neural", "pdf_keywords": "learn synthesis;dataset synthesis routes;synthesis insights;large dataset synthesis;synthesis planning materials;learn synthesis actions;model useful synthesis;synthesis insights methods;synthesis actions precursors;synthesize materials;generate syntheses;approach learn synthesis;synthesis target materials;synthesize synthesize materials;dataset synthesis;synthesize materials fundamental;precursors synthesis actions;generate syntheses numerous;materials synthesize;synthesis materials;describedthe synthesis materials;model synthesis screening;materials synthesize synthesize;model synthesis;tool synthesis;synthesis routes;useful synthesis planning;synthesis planning driven;useful tool synthesis;precursors synthesis"}, "2aea6cc6c42101b2615753c2933a33e57dd665f2": {"ta_keywords": "walks knowledge base;walks knowledge;random walks knowledge;beliefs knowledge base;walk inference learning;knowledge base graph;knowledge base;knowledge base containing;knowledge base specifically;walk inference;large scale knowledge;learning inference large;learning inference;scale knowledge base;knowledge incomplete coverage;random walk inference;soft inference;inference large scale;inference learning large;performing learning inference;soft inference procedure;inference learning;knowledge base objective;methods soft inference;inference large;beliefs knowledge;new beliefs knowledge;knowledge incomplete;scale knowledge;containing imperfect knowledge", "pdf_keywords": ""}, "1134ec4cdfc1c2161d157b0f4e03dec85d8c4c8a": {"ta_keywords": "convolutional networks reconstruct;irrigation canals convnets;canals convnets trained;canals convnets;convnets trained loss;convnets trained;networks reconstruct network;networks reconstruct;convnets;reconstruct network;reconstruct network like;training deep convolutional;training deep;convolutional networks;function training deep;connectivity oriented loss;deep convolutional networks;canals aerial images;deep convolutional;roads irrigation canals;networks;recover road connectivity;trained loss function;loss function training;irrigation canals aerial;propose novel connectivity;irrigation canals;set irrigation canals;connectivity network;novel connectivity", "pdf_keywords": "convolutional networks reconstruct;connectivity reconstructed networks;reconstructed networks;connectivity reconstructions;reconstructed networks simple;connectivity reconstructed;connectivity reconstructions network;images convnets trained;convnets trained loss;reconstructions network;convnets trained;reconstructions network like;images convnets;global connectivity reconstructions;networks simple convnet;training deep;networks reconstruct network;networks reconstruct;convnets;convolutional networks;training deep convolutional;aerial images convnets;convnet outperform approaches;images neural;microscopy images neural;reconstruct network;connectivity suffices skeletonize;convnet outperform;simple convnet;reconstruct network like"}, "b9b83860bc0d79b3b629b3035c4b7b7f9f71b5af": {"ta_keywords": "locations deploys relays;wireless relay networks;deploys relays locations;dedeploying wireless relay;deploys relays;relays locations connect;relays locations;relay networks important;wireless relay;wireless sensor networks;relay networks;deployment wireless sensor;sensor networks methodsa;sensor networks;sensor networks motivated;impromptu deployment wireless;relays;relay;wireless sensor;deployment wireless;dedeploying wireless;backgroundimpromptu dedeploying wireless;link quality measurements;problem wireless sensor;spaced locations deploys;networks important problem;locations deploys;node walks forest;link quality;networks methodsa", "pdf_keywords": "shadowing random variables;assume shadowing random;locations deploys relays;remotewe shadowing losses;shadowing random;deploys relays;dedeploying wireless relay;wireless relay networks;shadowing losses links;remote remotewe shadowing;remotewe shadowing;shadowing independent;relay large forest;wireless relay;source shadowing independent;relay networks;independent assume shadowing;method deployment wireless;shadowing independent meters;backgroundimpromptu dedeploying wireless;network independent;real deployment experiments;relay networks described;deployment multihop relay;relay;deploys relays thesewe;deployment experiments;relays;trail using remote;shadowing losses"}, "386bfd0e411dee4f512a8737c55dd84846981182": {"ta_keywords": "tables text answering;tables associated text;tabular representation learning;tables underperforms tasks;jointly models tables;answering questions tables;language models bert;tables associated;models bert joint;tables text;bert joint pretraining;pretrained language models;paired tables text;questions tables underperforms;learning jointly models;models tables associated;representation learning jointly;learning jointly;tables;operate tables associated;tables underperforms;models tables;models bert;questions tables;operate tables;tasks operate tables;self supervised objective;text answering;self supervised;using self supervised", "pdf_keywords": "tabular information embedding;tables text representation;tables associated text;trained exclusively tables;predicting cell text;pretraining approach tablebased;grained table semantics;embeddings row table;textal tabular data;pretrained specifically tables;information embedding tbi;cell embeddings;text table;table representation;cells table representation;intra table semantics;text representation learning;cell text substructures;tables highly effective;table table representation;row column embeddings;table extraction;column embeddings;table extraction decomposition;text table practical;tables highly;table semantics tbi;table representation method;specifically tables tabular;tables text"}, "466865aaeb8902f6f8ed93ceeb5fbf9fc8b593b1": {"ta_keywords": "beamforming deep neural;domain beamforming deep;frequency domain beamforming;beamforming deep;domain beamforming;future frame prediction;deep neural networks;backgroundthe frequency domain;algorithmic latency additionally;deep neural;algorithmic latency;algorithmic latency resultsthe;frequency domain;beamforming;future frame;latency resultsthe frequency;frame prediction;reduce algorithmic latency;latency additionally propose;incur algorithmic latency;latency additionally;latency;resultsthe frequency domain;propose future frame;latency resultsthe;spectral mapping designed;neural networks easily;backgroundthe frequency;complex spectral mapping;neural networks", "pdf_keywords": "latency speech enhancement;throughput speech enhancement;window speech enhancement;speech enhancement adapt;speech enhancement;speech enhancement low;speech enhancement recent;speech enhancement short;speech enhancement fundamental;high throughput speech;improve speech enhancement;based speech enhancement;speech enhancement utilize;speech enhancement using;speech enhancement domain;low latency speech;speech enhancement use;speaker speech enhancement;speech enhancement approach;reverberant speech enhancement;throughput speech;microphone speech enhancers;latency speech;speech processing new;enhance speech integrating;enhance speech;algorithmic latency speech;beamformer improve speech;speech enhancers;speech enhancing"}, "4759aaacd71fbb2b5ca253aa13ccceac0bc7fe8a": {"ta_keywords": "convincingness web argumentation;web argumentation aim;web argumentation;computational argumentation;argument empirical analysis;argument convincing methods;argumentation aim;computational argumentation given;argumentation;task computational argumentation;argumentation aim article;convincingness web;convincing argument empirical;explain argument convincing;argument empirical;argument convincing;attributes convincingness web;arguments;detecting attributes convincingness;argumentation given;explain argument;arguments certain controversial;convincing argument;convincingness;convincing methods;empirical manner annotating;makes convincing argument;argument;convincing methods approach;attributes convincingness", "pdf_keywords": ""}, "2cf21fc85af45512bf34d710f325872dca8a5331": {"ta_keywords": "traffic prediction china;prediction china openstreetmap;openstreetmap predict traffic;region traffic prediction;shanghai traffic data;traffic prediction;predict traffic;predict traffic conditions;shanghai traffic;backgroundcross region traffic;parts shanghai traffic;region traffic;prediction china;china openstreetmap;model openstreetmap predict;openstreetmap predict;traffic data;traffic condition data;traffic conditions;china openstreetmap objectiveto;traffic data available;traffic conditions nearly;model graphical traffic;traffic condition;graphical traffic condition;backgroundcross region;traffic;model openstreetmap;graphical traffic;openstreetmap", "pdf_keywords": ""}, "fb0a68981dae15f31cbcf5442509a3b8279b264c": {"ta_keywords": "noise robust speech;robust speech recognition;robust speech;noise robust;approaches noise robust;recognition remove noise;limited improvement noise;improvement noise;improvement noise highly;speech recognition remove;speech recognition;varying noise model;remove noise effect;noise model computational;vectors vitt enhancement;time varying noise;noise model;remove noise;noise effect directly;vitt enhancement fc;stationary fc enhancement;effective approaches noise;noise effect;fc enhancement method;varying noise;approaches noise;vitt enhancement;noise highly;fc vectors vitt;enhancement fc enhancement", "pdf_keywords": ""}, "4d41c2fa74dd018e39ddb3cbbfead1b42615612c": {"ta_keywords": "architectures deep;unseen deep architectures;deep architectures deep;architectures deep learning;deep architectures;prediction unseen deep;graphs neural architectures;deep learning directly;neural network parameters;directly predict parameters;introductionparameter prediction unseen;deep learning;deep learning successful;machine learning pipelines;graphs neural;neural architectures;learning pipelines algorithms;computational graphs neural;learning pipelines;use deep learning;predict parameters;training networks;introductionparameter prediction;learning directly predict;algorithms optimizing neural;predict parameters exploiting;use deep;unseen deep;optimizing neural;prediction unseen", "pdf_keywords": "predicting parameters imagenet;predict parameters imagenet;neural architectures hypernetwork;deep learning directly;deep learning;graphs neural architectures;pretraining imagenet;training networks;convolutional network gn;neural networks large;architectures graph convolutional;imagenet;strong neural architectures;architectures algorithm gnn;10 imagenet;gn networks neural;graph neural networks;parameters imagenet;prediction neural architectures;gnn network;machine learning pipelines;neural architecture learning;imagenet gns propose;compares pretraining imagenet;neural architectures significantly;graphs neural;neural networks image;training predicted parameters;neural parameter prediction;parameters imagenet new"}, "709f0a4229e40339b595072ae9fbd3a1ae1fd93e": {"ta_keywords": "dynamic dynamic neural;dynamic neural network;dynamic neural;toolkits static dynamic;neural network toolkits;toolkits pytorch dynet;existing toolkits static;theano existing toolkits;toolkits pytorch;toolkits operate statically;dynamic;dynamic dynamic;network toolkits pytorch;toolkits static;tensorflow;existing toolkits;static dynamic;computations tensorflow ntk;tensorflow ntk;declared computations tensorflow;toolkits;relative toolkits;computations tensorflow;toolkits operate;tensorflow ntk theano;structure relative toolkits;relative toolkits operate;dynamic require developer;network toolkits;statically declared computations", "pdf_keywords": "batching computation graphs;efficient batched operations;batches computations;instance computations batching;computations batching;batching computation;computationally efficient batched;batching algorithm seamlessly;graph construction execution;effective batching operations;efficient batched;batching operations neural;computations batching algorithm;automatically batching operations;implementation automatic batching;operations manually batching;implementation dynet toolkit;operation batching increasingly;batched operations variety;batches computations executed;batching operations manually;minibatch computations;graph executed batched;batching operations developers;version batches computations;batched operations;performance automatic batching;approach batching operations;minibatch computations aggregations;batching operations"}, "d745ba895cf8dcba5670fb01feea931fc72f9c77": {"ta_keywords": "deep reinforcement learning;reinforcement learning dlr;complexity learning policy;sample complexity learning;deep reinforcement;learning policy case;source deep reinforcement;learning policy;reinforcement learning;transfer learning experiments;complexity learning;states sample complexity;transfer learning;sample complexity;representations environment states;reinforcement;learning experiments;learning;environment states sample;sample complexity asking;set transfer learning;agents distinction representation;dlr agents distinction;dlr sample complexity;representations environment;learning useful;complexity;policy case;representation policy;agents distinction", "pdf_keywords": ""}, "af6c6e66fe0a9ba19c304665e01db1c5a5fba1e4": {"ta_keywords": "confidence reinforcement learning;methodsconstrained markov decision;reinforcement learning known;reinforcement learning settings;markov decision processes;reinforcement learning;reward function constraints;upper confidence reinforcement;stochastic decision problems;markov decision;stochastic decision;methodsconstrained markov;dynamics methodsconstrained markov;confidence reinforcement;learning settings reward;backgroundconstrained upper confidence;priori transition kernel;auxiliary cost constraints;class stochastic decision;cost constraints;constraints described cost;cost constraints paper;reinforcement;decision processes;settings reward function;learning known dynamics;cost functions unknown;select policy;decision problems;reward function", "pdf_keywords": ""}, "3389b6b8ee5a1ef0395df9f383e771650087b828": {"ta_keywords": "question answering systems;retrieval systems answer;successful question answering;question answering;information retrieval systems;information retrieval search;information retrieval;classicalal information retrieval;retrieval search engine;answering systems;retrieval systems;turn information retrieval;systems answer information;retrieval search;answer information needs;answering systems offer;answer information;retrieval;authoritative answers successful;search engine;authoritative answers;demand human experts;human experts timely;domain expert;language models;corpus;offer limited corpus;domain expert turn;limited corpus;limited corpus created", "pdf_keywords": "answering search systems;question answering search;models information retrieval;query answering technologies;question answering systems;model information retrieval;retrieval systems answer;retrieval query answering;information retrieval tasks;information retrieval systems;document retrieval systems;retrieval systems;answering search;retrieval systems pre;information retrieval;retrieval systems combine;classical information retrieval;question answering;based information retrieval;improvements information retrieval;document retrieval;information retrieval decade;successful question answering;information retrieval meant;language models capable;retrieval framework proposed;indexing retrieval;natural language tasks;information retrieval framework;language models accomplish"}, "f432d10677897cf72f9594ba7bd4c9199b270fb3": {"ta_keywords": "classification performance measures;classification measures performance;good classification measures;classification performance;measures performance measures;performance measures;performance measures used;measures performance;analysis classification performance;classification measures;evaluating classification results;performance measures formally;evaluating classification;used evaluating classification;results accuracy measure;classification results accuracy;accuracy measure;good classification;classification results;measure best;systematic analysis classification;classification;accuracy measure say;measures used evaluating;analysis classification;choose measure best;measure say better;measure best situations;performance;ideally choose measure", "pdf_keywords": "classification performance measures;classification measures provide;classification measures;validation measures classification;measures classification;measures compare classification;compare classification measures;cluster validation measures;multiclass threshold measures;measures classification used;classification performance;classification tasks;evaluating classification;binary classification tasks;classification;used evaluating classification;performance measures formally;analysis classification performance;evaluating classification results;classification fundamental;compare classification;validation measures;algorithms measures;classification used;classification tasks distance;binary classification;binary multiclass classification;measures desirable similarity;performance measures propose;validation measures provide"}, "da9ec5053c8ad8854bdd2ddc3f9c3d82a4114d71": {"ta_keywords": "endangered languages textual;models endangered languages;endangered languages present;endangered languages;transcriptions scanned books;extracting text resources;critically endangered languages;languages textual data;textual data;transcriptions scanned;dataset transcriptions scanned;language processing;languages textual;extracting text;dataset transcriptions;language processing models;benchmark dataset transcriptions;task extracting text;books scanned images;text resources;natural language processing;build natural language;books critically endangered;natural language;books scanned;scanned books;textual;paper books scanned;extracting;processing models endangered", "pdf_keywords": "transcribed images endangered;endangered languages scanned;images endangered languages;ocr endangered languages;dataset transcribed images;endangered language text;transcribed images;ocr endangered;data endangered languages;endangered languages translation;endangered language books;benchmark dataset transcribed;optical character recognition;endangered languages textual;endangered language encoders;languages scanned;transcription endangered language;languages scanned images;containing endangered language;dataset transcribed;character recognition;transcriptions scanned books;textual data endangered;endangered languages using;character word recognitionthe;transcribed text;models endangered languages;language text targeted;tomography ocr endangered;model accurate translations"}, "f66c82ca087b435463ef4fa0de49825c4eb55885": {"ta_keywords": "semantic analysis atis;semantic parsing;semantic parsing method;introductionhypbrid semantic analysis;semantic annotation;method semantic parsing;tree based semantic;based semantic annotation;data formalization;atis data evaluation;semantic analysis;introductionhypbrid semantic;semantic annotation learns;parsing;parsing method deals;annotation learns data;atis data;semantic;parsing method;annotation;analysis atis data;method data formalization;based semantic;data evaluation;annotation learns;method semantic;analysis atis;novel method semantic;data evaluation article;formalization", "pdf_keywords": ""}, "b37d073109cfcf913cf53aded3872e6158e828a0": {"ta_keywords": "vision language navigation;vision language;ground visual objects;door ground visual;door routes visual;ground visual;introduction vision language;language navigation vln;routes visual environment;navigation vln;routes visual;visual objects;grounding connect language;visual environment;language navigation;navigation;visual objects turn;navigation vln requires;visual environment actual;door routes;visual;door ground;vision;stop door ground;stop door routes;introduction vision;geometric structure route;routes;structure route;language environment", "pdf_keywords": "visual objects;visual requires grounding;visual environment;ground visual objects;virtuallanguage navigation models;visual grounding;visual agent;typify visual grounding;visual environment actual;agents visual input;visual navigation;outperforms visual agent;visual grounding approaches;agents visual;visual inputs generalizable;virtual language models;utilized visual inputs;visual agent outperforms;non visual agent;visual navigation ability;art virtuallanguage navigation;ground visual;vision propose objectbased;models use visual;agent outperforms visual;virtuallanguage navigation;visual inputs;visual agent unseen;visual input;art virtual language"}, "e2ef0dc26a669ed764e2d70257b162298b8b608e": {"ta_keywords": "dual function radarcommunications;radarcommunications fcs systems;radarcommunications fcs;function radarcommunications fcs;function radarcommunications;radarcommunications;eavesdropping messages aimed;systems susceptibility eavesdropping;fc systems radar;eavesdropping messages;radar;eavesdropping;susceptibility eavesdropping messages;susceptibility eavesdropping;systems radar target;systems radar;target act eavesdropper;act eavesdropper;use common signaling;radar target act;radar target;eavesdropper;signaling methods dual;common signaling methods;eavesdropper dd;act eavesdropper dd;eavesdropper dd receives;common signaling;signaling methods;signaling", "pdf_keywords": "antennas optimize secrecy;dual function radarcommunications;radar irs secrecy;function radarcommunications;antenna fc secrecy;function radarcommunications forced;radarcommunications;optimize secrecy;optimize secrecy rate;approach maximize secrecy;transmission information radar;information radar;radar legitimate receivers;multiple antennas optimize;radarcommunications forced;maximize secrecy;radar;antennas optimize;radar applications;radar multiple antenna;radar multiple;signal processing radar;radarcommunications forced systems;single irs radar;eavesdropping messages aimed;systems susceptibility eavesdropping;target radar;information radar users;radar users;irs radar multiple"}, "243880fde63abfc287bd1356c2e1dbf68a1a0aac": {"ta_keywords": "indigenous language technologies;language technologies canada;introduction indigenous language;indigenous language;indigenous languages;languages spoken canada;indigenous languages spoken;60 indigenous languages;language technologies;spoken canada particular;spoken canada;technologies canada challenge;introduction indigenous;technologies canada;languages just feasible;feasible develop languages;indigenous;canada challenge challenges;canada challenge;develop languages;speech image technologies;60 indigenous;develop languages just;approximately 60 indigenous;text speech image;text speech;languages;languages spoken;languages just;language", "pdf_keywords": "implementations indigenous languages;methods indigenous languages;indigenous language technologies;developing indigenous languages;technologies indigenous languages;indigenous languages development;systems indigenous languages;language indigenous languages;development indigenous languages;speech indigenous languages;technology indigenous languages;indigenous languages;text speech indigenous;indigenous languages increasingly;development indigenous language;indigenous language;indigenous languages use;new language indigenous;resources indigenous languages;text technology indigenous;language indigenous;indigenous languages article;indigenous languages little;development multilingual recognition;automated translations;study indigenous languages;used indigenous languages;automated automated translations;indigenous languages increasing;languagesthe development indigenous"}, "9364eff879a9dcb34fe3dfdd0843e69c14dd333b": {"ta_keywords": "optimizing models interpretability;optimize interpretability directly;optimize interpretability;interpretability sparse;loop interpretability prior;interpretability prior;interpretability prior desire;models interpretable accurate;models interpretability;interpretable accurate prior;work optimize interpretability;models interpretable;backgroundhuman loop interpretability;interpretability directly including;interpretability directly;models interpretability relied;interpretability;proxies interpretability sparse;interpretable accurate;loop interpretability;interpretability relied easy;interpretability sparse number;interpretable;interpretability relied;desire models interpretable;quantify proxies interpretability;optimizing models;including humans optimization;humans optimization loop;proxies interpretability", "pdf_keywords": "optimizing models interpretability;optimizes interpretability model;interpretable models;human interpretable models;identifying models interpretable;models predictive interpretablethe;models interpretable;models interpretable accurate;optimize interpretability directly;models interpretability;models human interpretability;model interpretable propose;optimize interpretability;interpretable models approach;model interpretable;optimizes interpretability;optimize models human;models interpretable according;efficiently optimize models;optimizing models;optimize models;models interpretable workflow;models interpretability relied;approach optimizes interpretability;interpretable models correspond;interpretability model user;different models interpretable;interpretable accurate prior;locally interpretable models;predictive interpretablethe"}, "64c575bb8b3e11097605028de5c289b0b2d839a4": {"ta_keywords": "native speech synthesis;speech synthesis preserving;speech synthesis technique;lingual speech synthesis;speech synthesis;speech synthesis based;based speech synthesis;synthesis preserving speaker;non native speech;synthesis based voice;non native speaker;preserving speaker individuality;based voice conversion;speech synthesis synthesizes;speaker individuality based;voice conversion;native speech;voice conversion hmm;native speaker;correction prosodic phonetic;preserving speaker;introductionnon native speech;speaker individuality;prosodic phonetic;speech specific non;synthesizes foreign language;specific non native;language speech specific;synthesis preserving;prosodic phonetic characteristics", "pdf_keywords": ""}, "5af9ab65d186e4e1e0b1cef1962ca15336f37931": {"ta_keywords": "dependent semantic parsing;semantic parsing;semantic parsing resul;semantic parsing task;introduction semantic parsing;parsing;context dependent semantic;available corpus context;corpus context dependent;utterances machine interpretable;natural language utterances;introduction semantic;parsing task translating;corpus context;natural language;translating natural language;parsing resul;corpus;parsing task;available corpus;meaning representation approaches;semantic;interpretable meaning representation;corpora assume utterances;dependent semantic;publicly available corpus;utterances machine;language utterances machine;existing corpora;corpora", "pdf_keywords": ""}, "4f8e1a4247ce06a15760fc2692c6849601d41b6f": {"ta_keywords": "textual entailment;textual entailment fundamental;introduction textual entailment;entailment fundamental task;entailment fundamental;entailment;use textual;textual;task natural language;textual content;knowledge graphs add;knowledge graphs;textual content providing;use textual content;external knowledge sources;textual content present;like knowledge graphs;natural language processing;information external knowledge;noisy knowledge graphs;problem use textual;addition textual content;natural language;knowledge sources;external knowledge;addition textual;sources like knowledge;knowledge sources like;providing background knowledge;background knowledge", "pdf_keywords": "networks natural language;external knowledge textual;knowledge textual entailment;subgraphs extracted knowledge;contextually relevant subgraphs;approach neural text;generating contextual subgraph;text entailment models;contextual subgraph;contextual subgraph premise;textual entailment task;natural language inference;neural text based;textual entailment;text representations;knowledge textual;knowledge graphs gs;neural text;text based entailment;existing entailment models;networks graph representations;datasets existing entailment;graph convolutional networks;knowledge graphs;like knowledge graphs;text based representations;textual entailment fundamental;datasets knowledge graphs;convolutional networks gns;representations augmented entailment"}, "9cf75483deee77b3c0ee4f996d808437ab4a7435": {"ta_keywords": "mfm thermal gelation;protein mfm process;myofibrillar protein mfm;thermal gelation chicken;thermal gelation process;protein mfm;thermal gelation;processing mfm thermal;breast myofibrillar protein;gelation process process;mfm thermal;mfm process;gelation process;myofibrillar protein;gelation chicken breast;gelation chicken;chicken breast myofibrillar;processing mfm;step processing mfm;gelation;protein;breast myofibrillar;mfm process key;mfm;myofibrillar;thermal;chicken breast;process process;process;process process process", "pdf_keywords": ""}, "0ce184bd55a4736ec64e5d82a85421298e0373ea": {"ta_keywords": "end speech processing;neural machine translation;automatic speech;sequenceence sequence models;text speech;example automatic speech;speech processing;text speech tts;speech tts;speech recognition ar;speech recognition;speech translation;sequence models;sequence sence model;speech translation st;sequence models widely;machine translation;speech processing example;ar speech translation;automatic speech recognition;st text speech;recognition ar speech;speech tts paper;neural machine;sequence sence;performance neural machine;emergent sequence sence;introduction sequenceence sequence;introduction sequenceence;sequenceence", "pdf_keywords": "translation neural machine;neural machine translation;machine translation neural;multiple attentions parallel;translation neural;rnn natural language;machine translation speech;sequence sequence neural;sequence neural;machine translation increasing;machine translation;sequence neural networks;machine translation natural;attentions parallel;sequence development neural;machine translation nmt;attention algorithm automatic;attentions parallel introduce;analysis multiple attentions;translation speech recognition;networks rnn;translation natural language;attention algorithm;networks rnn natural;multiple attentions;sequence sequence architecture;recurrent neural networks;attentions parallel apply;neural networks rnn;translation nmt rapidly"}, "448e15e267b20bee1644034e18630da2e68cf36e": {"ta_keywords": "loose sand patient;deep loose sand;sand patient deep;sand using elisa;deep deep sand;loose sand using;sand patient;deep sand;loose sand;sand deep loose;deep sand deep;sand deep;sand using;loose sand middle;sand;sand middle aged;sand middle;elisa method patient;aged man deep;man deep deep;method patient deep;using elisa;patient deep;elisa;patient deep deep;new deep loose;elisa method;deep loose;man deep;using elisa method", "pdf_keywords": ""}, "4c94dc1b2391d78c9cfdd69955d20b56d7a16982": {"ta_keywords": "systems erasure codes;storage systems erasure;tuning code redundancy;distributed storage systems;erasure codes;failures tuning code;distributed storage;redundancy requires code;scale distributed storage;code redundancy;code redundancy observed;erasure codes used;storage systems;reduce storage cost;node failures tuning;storage cost tuning;redundancy observed failure;achieve fault tolerance;tuning redundancy requires;conversion tuning redundancy;tuning redundancy;md convertible codes;storage cost;cost tuning redundancy;fault tolerance;significantly reduce storage;failures tuning;code conversion tuning;reduce storage;redundancy requires", "pdf_keywords": "linear erasure codes;distributed storage codes;erasure codes;erasure codes essential;codes access optimal;use erasure codes;erasure codes valid;storage codes;storage codes using;propose coding scheme;optimal convertible codes;storage codes use;codes efficient conversion;linear md codes;coded data distributed;coding scheme;convertible code constructions;codes use distributed;linear code available;storage codes increasing;codes generalized split;codes generalized;coded matrix;md codes generalized;code constructions;coded matrix multiplication;codes split regime;codes efficient;convertible codes framework;coding scheme coded"}, "c55d5805a6eb8b1482f21581fe893484eaf9ffb5": {"ta_keywords": "perceived age singing;singing voice age;age singing voice;age singing;age singer perceived;voice age;singing voice conversion;voice age singer;voice timbre control;singer perceived listener;singing voice;voice timbre;singer perceived;age novel voice;effect perceived age;voice conversion;age singer;introductionthe perceived age;perceived age;acoustic features effect;voice conversion sv;voice;perceptions song methodin;based perceived age;singing;acoustic features;determines perceptions song;novel voice timbre;timbre control;timbre control technique", "pdf_keywords": ""}, "4275d4c4bd10742b321467f175f16198ed7d17d7": {"ta_keywords": "temporal model music;symbolic domain music;domain music generation;music generation;music generation targeted;study generative adversar;generative adversar;musical events continuous;discrete musical events;model music;discrete musical;music usually composed;context music;domain music;model music usually;music;context music art;generative;composed multiple instruments;sequences discrete musical;musical events;musical;music art time;music art;study generative;multiple instruments tracks;track temporal dynamics;temporal dynamics collectively;interaction track temporal;temporal dynamics", "pdf_keywords": ""}, "802ddaf5bd731b91e64d8cee43f7fb614b42c1df": {"ta_keywords": "electric vehicle charging;feasible electric vehicle;electric vehicle;vehicle charging;artificial intelligence;perishable goods feasible;free delivery perishable;feasible feasible electric;feasible electric;free delivery;artificial intelligence demonstrate;free free delivery;goods feasible;delivery perishable goods;goods feasible feasible;charging;intelligence demonstrate free;perishable goods;conference artificial intelligence;demonstrate free free;vehicle;joint conference artificial;artificial;feasible feasible;goods;conference artificial;delivery perishable;intelligence;electric;feasible", "pdf_keywords": ""}, "75f90cbbf3c27a8b27567d6a9c8c4538743c8fff": {"ta_keywords": "toolkit text generation;text generation introduce;generation introduce texar;text generation tasks;text generation;set text generation;introductiontexar open source;introduce texar;texar open source;extensibility mind texar;generation tasks;source toolkit text;texar;introductiontexar;generation introduce;introduce texar open;source toolkit;mind texar extracts;toolkit text;generation tasks transform;open source toolkit;texar extracts;natural language design;inputs natural language;mind texar;natural language;creates;texar extracts common;generation;texar open", "pdf_keywords": ""}, "7e386158f474a395618c5e065ac55844b507007c": {"ta_keywords": "supervised learning sl;summaryself supervised learning;adaptation speech processing;minimal adaptation speech;summaryself supervised;adaptation speech;learning sl;speech processing;supervised;speech processing community;nl computer vision;language processing nl;supervised learning;learning sl proven;processing nl;unlabeled data achieves;unlabeled data;cv paradigm pretrains;learning;natural language processing;paradigm pretrains shared;unlabeled;natural language;pretrains shared model;minimal adaptation;language processing;computer vision cv;tasks minimal adaptation;speech;paradigm pretrains", "pdf_keywords": "speech representation ssl;pretrained models speech;speech processing universal;supervised learning speech;task speech representation;speech processing community;data speech processing;introduce speech processing;self supervised;models speech processing;framework self supervised;website speech recognition;self supervised learning;framework speech representation;speech processing;process speech processing;traditional supervised pipelines;data speech;speech representation learning;supervised pipelines;speech representation algorithms;supervised pipelines leveraging;development speech recognition;approach speech representation;learning speech representations;speech processing demonstrate;speech representation;speech recognition;models speech;learning speech"}, "9976ed0d88a4156ecdd3ebe39714c5fb4a5a0246": {"ta_keywords": "matrix adaptation evolution;dansi speech recognition;ofthyaart speech recognition;covariance matrix adaptation;matrix adaptation;speech recognition systems;speech recognition;tune dansi speech;speech recognition laborious;adaptation evolution strategy;objective pareto optimization;multi objective pareto;pareto optimization optimizes;cm multi objective;pareto optimization;multi objective;optimization optimizes;optimization optimizes systems;human experts tuning;experts tuning;adaptation evolution;optimizes systems;optimizes systems achieve;optimizes;experts tuning numerous;tuning numerous parameters;tune dansi;evolution strategy cm;adaptation;state ofthyaart speech", "pdf_keywords": ""}, "814421bb20ba1fba88928fc168db1b7175cca6ac": {"ta_keywords": "electrolyte model electrolyte;model electrolyte model;electrolyte model;model electrolyte;unit electrolyte model;electrocardiogram;developed electrocardiogram;developed electrocardiogram f0;electrocardiogram f0;control unit electrolyte;electrocardiogram f0 f0;patient developed electrocardiogram;unit electrolyte;electrolyte;model;f0 control unit;f0 f0 control;f0 control;case patient;f0 f0;control unit;f0;case patient developed;case;patient;report case patient;patient developed;unit;report case;control", "pdf_keywords": ""}, "e8bd03ff376ab3c863f72f931c91e90eeb9b2be9": {"ta_keywords": "diagnosis management patient;physician diagnosis;physician diagnosis management;role physician diagnosis;patient disease;management patient disease;role physician;diagnosis management;physician;literature role physician;diagnosis;management patient;disease;patient;article present literature;article present;management;literature role;article;role;present literature role;purpose article present;purpose article;present literature;literature;purpose;present", "pdf_keywords": ""}, "36c95e3ef362742a5c1844257e8b79d3251a781e": {"ta_keywords": "visual attributes;explicitly visual attributes;visual attributes like;understanding language shapes;understanding explicitly visual;explicitly visual;objects flat images;language shapes contours;3d shapes;shapes;attributes like color;3d shapes human;object based;shapes contours work;language shapes;world objects;shapes contours;visual;objects;shapes human requests;novel reasoning task;complex 3d shapes;world objects flat;reasoning task targets;shapes human;targets visual;object based basic;flat images complex;images complex;objects flat", "pdf_keywords": "shapenet annotated referring;vision language models;useful robotic language;robotic language understanding;modeling vision language;object understanding robot;language 3d objects;language useful robotic;language descriptions 3d;language referent robot;robotic language;referring expressions objects;descriptions 3d shapenet;vision language useful;3d object understanding;referent robot;language 3d object;vision language;object understanding;annotated referring expressions;shapenet object models;visual language 3d;object referents natural;robots use language;domain language vision;ground language 3d;language referring expressions;natural language referring;3d object models;distinguish 3d objects"}, "807e421679d4a9d629d2fad1f60f28787dca60e7": {"ta_keywords": "supervised question answering;supervised qa generative;semi supervised qa;question answering models;question answering;answering models;answering utilizing unlabeled;supervised qa;model generate questions;performance question answering;generate questions;question answering utilizing;questions based unlabele;generate questions based;qa generative domain;answering models methods;qa generative;generative domain adaptive;semi supervised question;semi supervised;introduction semi supervised;problem semi supervised;training framework generative;generative domain;domain adaptive nets;generative model generate;answering;framework generative domain;generative;generative model", "pdf_keywords": "supervised question answering;question answering models;question answering;supervised qa generative;question answering measure;answering models;comprehension semi supervised;approach question answering;question answering develop;performance question answering;question answering provide;questions answering;question answering utilizing;semi supervised qa;answering reading comprehension;questions answering reading;answering models methods;question answering apply;neural machine translation;adaptive nets gans;framework question answering;qa generative domain;adaptive net gan;generative domain adaptive;method question answering;generative models reinforcement;answering develop learning;tasks questions answering;answering reading;qa generative"}, "d06493373421c86ba33dbb8834ccb725105a665f": {"ta_keywords": "grained distinctions vocabulary;distinctions vocabulary items;lexical distinction;wall different lexical;lexical distinction obvious;vocabulary items;different lexical;distinctions vocabulary;vocabulary;fine grained distinctions;lexical;lexical manifestations spanish;grained distinctions;different lexical manifestations;wall variety lexical;noun wall different;vocabulary items key;example noun wall;language example noun;variety lexical distinction;lexical manifestations;noun wall;wall muro refers;distinctions;variety lexical;distinction explained;language example;learners unless distinction;spanish pared refers;new language example", "pdf_keywords": "target lexical choice;grained distinctions vocabulary;lexical choice target;target lexical;lexical selection;identify target lexical;extract lexical choices;learning perform lexical;perform lexical selection;new approach lexical;lingual lexical selection;lexical selection bythe;contextually correct translations;language extract lexical;lexical selection using;extract lexical;extract lexical semantic;lexical choices language;analyze translations underlying;task lexical selection;encoding lexical choice;lexical distinction;lexical semantic;approach lexical choice;lexical semantic features;wall different lexical;perform lexical;lexical choices;target translations;lingual lexical"}, "ece62ada00cef99d9fc7a60e7d4b773f6d87c8f9": {"ta_keywords": "tasks machine translation;human language technologies;language technologies lorehvt;machine translation;machine translation min;resource human language;speech text;frame text speech;arie cmu submissions;text speech text;lorehvt 2018 evaluations;arie cmu systems;text speech;systems loreht18;language technologies;evaluations tasks machine;translation min entity;cmu systems loreht18;technologies lorehvt 2018;speech text speech;describes arie cmu;situation frame text;human language;2018 evaluations tasks;arie cmu;technologies lorehvt;lorehvt 2018;tasks machine;loreht18 paper describes;min entity discovery", "pdf_keywords": "speech recognition il9;il10 lingual data;language il9;natural language il9;il9 il10 lingual;language il9 mt;il10 lingual;translation ionized;resource language recognition;speech recognition languages;translation ionized ionized;recognition il9 il10;il9 il10 data;lingual data training;il10 data il9;recording il9 il10;ionized use morphological;human language technologies;evaluation il9 il10;recognition il9;speech recognition english;training recording il9;data il9 il10;language recognition;training data english;human language technology;evaluation speech recognition;il10 data;recording il9;evaluation il9"}, "73635c9dc0ffb61c2eac79234108c6eee1362c1b": {"ta_keywords": "bandits correlated markovian;bandit problem dynamic;bandit problem;armed bandit problem;bandits correlated;armed bandits correlated;multi armed bandit;armed bandits;bandits;backgroundmulti armed bandits;correlated markovian;bandit;correlated markovian environments;armed bandit;markovian environments;markovian;state markov;rewards evolve correlated;markov;state markov chain;markovian environments smoothed;markov chain;according markov chain;according markov;observations state markov;arm rewards;arm rewards evolve;environment arm rewards;rewards evolve;markov chain different", "pdf_keywords": "bandit problems markov;bandit problem markov;bandit algorithms state;markov bandits;markov bandits applied;markov bandits method;bandit problem dynamic;bandit algorithms;bandit policies interacting;bandit algorithms problems;bandit problems environments;bandit policy interacting;restless markov bandits;bandit problems reward;available bandit algorithms;traditional bandit algorithms;solving bandit problems;based bandit algorithms;correlated markovian rewards;approach solving bandit;bandit problems;algorithm available bandit;analysis bandit problems;bandit problem method;bandit problem;bandit algorithms ostensibly;solving bandit;epoch based bandit;markovian rewards;bandit setting reward"}, "18289b2b04fc8a7a86f474236e55a3b1070a98ad": {"ta_keywords": "symptomatic asymptomatic asymptomatic;symptomatic asymptomatic;asymptomatic asymptomatic asymptomatic;asymptomatic asymptomatic;history symptomatic asymptomatic;asymptomatic;symptomatic;patients history symptomatic;patients;history symptomatic;approach management patients;patients history;management patients history;management patients;effectiveness new approach;evaluating effectiveness new;effectiveness new;evaluating effectiveness;approach management;new approach management;approach;new approach;effectiveness;importance evaluating effectiveness;article;new;evaluating;management;history;article discuss importance", "pdf_keywords": ""}, "db500c4e746897e5d5adafbf222b959c512445ad": {"ta_keywords": "messageaversarial attacks alter;messageaversarial attacks;data poisoning attack;new data poisoning;clinical messageaversarial attacks;poisoning attack allows;data poisoning;nl model predictions;predictions manipulate;model predictions perturbing;adversary control model;poisoning attack;allows adversary control;understood predictions manipulate;adversary control;key clinical messageaversarial;predictions perturbing test;predictions perturbing;inputs understood predictions;model predictions;clinical messageaversarial;attacks alter;control model predictions;attacks;predictions desired trigger;attack allows adversary;allows adversary;model predictions desired;adversary;messageaversarial", "pdf_keywords": "backgroundadversarial attacks alter;poisoning neural models;data poisoning neural;backgroundadversarial attacks;models exploit;poisoned models exploit;usefulness data poisoning;poison sentiment analysis;validation accuracy adversary;poisoning vision models;limit poisoning attacks;poisoning reduce accuracy;statements exploit attack;poison examples intodata;accuracy adversary victim;nl model predictions;data poisoning attack;backgroundadversarial;accuracy adversary;text exploit attack;optimize poison examples;intodata poisoning attacks;text exploit;poisoning attack contains;poisoning attack control;statements exploit;adversary inserts;attack control sentiment;vulnerability non nl;negative statements exploit"}, "00936aa7c8f64fc919dd4dcee6192ccc83e0d26e": {"ta_keywords": "transillumination reflectance imaging;interproximal lesions radiography;lesions radiography using;swr transillumination reflectance;imaging interproximal lesions;lesions radiography;reflectance multispectral imaging;multispectral transillumination reflectance;multispectral imaging devices;transillumination reflectance multispectral;reflectance imaging;reflectance imaging methods;multispectral imaging;swr multispectral transillumination;infrared swr transillumination;extracted teeth imaged;transillumination reflectance;lesions proximal surfaces;measurement depthh lesions;multispectral transillumination;imaging interproximal;reflectance multispectral;wavelength infrared swr;teeth imaged micro;radiography using extracted;devices imaging interproximal;infrared swr;radiography;surfaces swr multispectral;imaging methods", "pdf_keywords": ""}, "58b0800ef48da2678e15e5e8bc1d786e24190742": {"ta_keywords": "ffield speech processing;far ffield speech;ffield speech;deep learning speech;speech processing;learning speech enhancement;audio acoustics speech;issue far ffield;speech enhancement separation;speech processing era;acoustics speech language;acoustics speech;era deep learning;speech enhancement;far ffield;deep learning;learning speech;signal processing communities;processing era deep;speech language machine;relevant audio acoustics;audio acoustics;learning signal processing;speech language;highly relevant audio;audio;acoustics;machine learning signal;ffield;separation recognition", "pdf_keywords": ""}, "901fbb51d6fb9078e572c83a446b408da4de9b2b": {"ta_keywords": "etiology malignant disease;etiology malignant;malignant disease;malignant disease poorly;malignant;development new disease;new disease;disease poorly understood;disease;etiology;disease poorly;development new model;model development new;understood development new;new model development;model development;development new;poorly understood development;understood development;new model;development;model;new;poorly understood;understood;poorly", "pdf_keywords": ""}, "d0a58b6da9f7788534aa9963a78c24a87038e4fc": {"ta_keywords": "peer review methods;reviewers reject highly;recommendations different reviewers;peer review;inconsistency peer review;reviewers reject;handful reviewers reject;different reviewers major;reject highly novel;final recommendations different;reviewers major source;scores final recommendations;reviewers;different reviewers;common handful reviewers;reviewers major;review methods paper;criteria scores final;review methods;mapping criteria scores;handful reviewers;criteria scores;final recommendations;reject highly;recommendations different;highly novel paper;review;highly novel;reject;paper generally disparate", "pdf_keywords": ""}, "490c31b460316b7f68e9b8f5ff0d26aef2f7f45f": {"ta_keywords": "congestion game decision;decision process congestion;atomic congestion game;congestion game equilibria;process congestion games;mamp congestion game;congestion game;congestion games;non atomic congestion;game equilibria uncertainty;congestion effects state;atomic congestion;congestion games aims;congestion;selfish optimization states;cost influence congestion;process congestion;sensitivity mamp congestion;mamp congestion;selfish optimization;markov decision process;influence congestion;performs selfish optimization;congestion effects;markov decision;game equilibria;equilibria uncertainty;influence congestion effects;decision makers optimize;analysis markov decision", "pdf_keywords": "model congestion game;multidimensional congestion game;congestion game equilibria;atomic congestion game;malprogrammase congestion game;congestion game satisfies;congestion game decision;congestion game occurrence;congestion game;process congestion games;md congestion game;congestion games;congestion game cycle;game equilibria uncertainty;selfish optimization;consider model congestion;games optimal population;congestion game directly;game stochasticity;deterministic multidimensional congestion;model congestion;selfish optimization states;multidimensional congestion;unperturbed multidimensional congestion;game satisfies optimal;decision process congestion;equilibria md congestion;performs selfish optimization;sensitivity multidimensional congestion;game bounded sensitivity"}, "2e0b1484740047d6d6fb6bd2c9d8816b54b33811": {"ta_keywords": "review process 2016;2016 edition conference;quality review process;evaluate quality review;reviewers 000 attendees;review process;quality review;submissions 000 reviewers;conference methodsthe 2016;000 reviewers;reviewers 100;evaluate quality;edition conference methodsthe;reviewers;reviewers 000;purposeto evaluate quality;edition conference comprised;000 reviewers 000;review;process 2016 edition;2016 edition;edition conference;process 2016;terms reviewers 100;conference methodsthe;conference comprised 400;96 terms reviewers;reviewers 100 terms;conference comprised;400 paper submissions", "pdf_keywords": "conference reviews national;review process conference;conference reviews;annual conference academic;review process significantly;significantlythe review process;medical sciences conference;process conference reviews;effectiveness peer review;review data conferences;2013 conference conducted;peer review process;annual conference;annual annual conference;journal research nippines;peer review;review process national;2016 edition conference;conference academic;study peer review;review provide improved;sciences conference conducted;reviewers significantly;insights peer review;review process useful;improve review process;conference conducted;participation reviewers significantly;papers reviewers 2015;assessment peer review"}, "2444be7584d1f5a7e2aa9f65078de09154f14ea1": {"ta_keywords": "knowledge distillation;knowledge distillation kd;born knowledge distillation;compact transferring knowledge;transferring knowledge machine;transferring knowledge;learning model;distillation kd;distillation kd consists;knowledge machine;learning model teacher;networks born knowledge;distillation;consists transferring knowledge;knowledge machine learning;knowledge;machine learning model;born knowledge;transferring knowledge hopes;model teacher student;model teacher;neural networks born;neural networks;teacher methodswe;knowledge hopes;learning;student compact transferring;performance student compact;neural;model", "pdf_keywords": "softmax distribution teacher;softmax;neural networks fundamental;knowledge distillation;new neural networks;neural networks important;knowledge distillation kd;learning neural;learning model;neural networks successful;neural network important;using knowledge distillation;knowledge distillation method;networks neural;introduction knowledge distillation;softmax distribution;neural networks smallest;matching softmax;student model train;learning model teacher;learning teacher model;neural networks ability;matching softmax distribution;concept learning;neural networks dark;neural network;learning neural networks;networks shown learning;neural networks learning;transferring knowledgeledge machine"}, "f17ee5b9d3120960eddd2bdb9af2f4f689cebb3a": {"ta_keywords": "supervision relation extraction;relation extraction;relation extraction using;linking knowledge bases;bases distant supervision;linking knowledge;introductionindirect supervision relation;annotated corpus training;rep extraction systems;knowledge bases;human annotated corpus;supervision relation;types rep extraction;introductionindirect supervision;human annotated;knowledge bases distant;rep extraction;annotated corpus;relation types rep;acquired linking knowledge;distant supervision conclusionsin;supervision conclusionsin recent;corpus training;corpus training costly;relation types;distant supervision;generating labeled data;relied human annotated;question answer pairs;dealing relation types", "pdf_keywords": "learning relation mentions;supervision questionanswering datasets;task relation extraction;relation extraction task;relation extraction;relation mentions corpus;relation extraction corpora;extraction relation extraction;relation extraction propose;relation extraction provide;relation extraction understood;detect relation mentions;relation extraction variety;relation extraction using;supervision exploiting semantic;relation extraction relation;supervision qa corpus;framework relation extraction;extract typed relations;relation mentions text;mentions text corpus;mentions relation types;model relation extraction;relation mention entity;supervision questionanswering;exploiting semantic evidence;questionanswering datasets task;mentions corpus corpus;tool relation extraction;literature relation extraction"}, "90af87c1e4fba127d6db8f5e1f9e1ef3472507e8": {"ta_keywords": "patients diagnosis malignant;diagnosis malignant disease;diagnosis malignant;malignant disease;malignant;patients diagnosis;management patients diagnosis;diagnosis;patients;management patients;disease;approach management patients;management;approach management;new approach management;approach;article discuss;article discuss importance;article;new approach;importance new approach;discuss importance;aim article discuss;discuss importance new;discuss;new;aim article;importance;importance new;aim", "pdf_keywords": ""}, "39365d95992c8294ba32d85c69d337040ddb8e54": {"ta_keywords": "neural machine translation;trees standard linguistically;syntactic information nm;dependency parse trees;machine translation nm;linguistically inspired tree;constituency dependency parse;nm adding syntactic;machine translation;syntactic information;parse trees;parse trees standard;adding syntactic information;adding syntactic;syntactic;dependency parse;standard linguistically inspired;specific linguistic formalism;linguistic formalism;translation nm;translation nm adding;tree structures like;standard linguistically;improve quality translations;quality translations;quality translations existing;tree structures;structures like constituency;translations existing;linguistically inspired", "pdf_keywords": "linguistically inspired parse;neural machine translation;syntax free tree;trees constructed linguistic;linguistically inspired tree;dependency parse trees;inspired parse trees;novel syntactic decoder;syntactic decoder;machine translation nm;machine translation;translation applications neural;syntactic information neural;parse tree model;parse trees synthetic;inspired parse;constructed linguistic guidance;decoder linguistically inspired;parse trees;machine translation applications;parse tree;constituency dependency parse;tree based decoder;construction syntax free;decoder linguistically;constructed linguistic;syntactic decoder uses;machine translation important;decoder non linguistically;syntactic information"}, "162515d87256f13888d9d7ba95275ac4b6c35396": {"ta_keywords": "adversarial misspellings robustust;combating adversarial misspellings;adversarial misspellings;misspellings robustust;misspellings robustust word;combating adversarial;adversarial;robustust word;introduction combating adversarial;robustust word recognition;robustust;word recognition;misspellings;recognition;word;combating;introduction combating;introduction", "pdf_keywords": ""}, "615358de8e9a7cf318c172afafc2a303eab93d98": {"ta_keywords": "recommender clothing coordinates;photograph fashion items;photograph fashion item;clothing coordinates using;recommender clothing;clothing coordinates;photographs fashion;photographs fashion magazines;using photographs fashion;photograph fashion;fashion item tops;coordinatesrecommender using photographs;body photographs fashion;recommend photograph fashion;propose recommender clothing;given photograph fashion;query recommend photograph;fashion items;fashion magazines methodsin;fashion items bottoms;fashion item;fashion magazines;fashion magazines resultsthe;fashion;body photographs;photographs;recommend photograph;purposefashion coordinatesrecommender;using body photographs;clothing", "pdf_keywords": ""}, "58c04126a5196deb57ae31d6174cd4aae154f138": {"ta_keywords": "feedback active learning;active learning partial;propose active learning;annotating examples corpora;active learning;annotating examples;feedback alpf learner;learning partial feedback;corpora propose active;partial feedback active;annotating;actively choose example;method annotating examples;feedback active;learning partial;learner selects example;alpf learner actively;examples corpora propose;choose example label;examples corpora;example label;learner actively;partial feedback;method annotating;alpf learner;effective method annotating;selects example asking;partial feedback alpf;partial feedback effective;learner actively choose", "pdf_keywords": "learning partially labeled;active learning partial;partial label learner;learning partial labels;active learning;active learning effective;passive learning partial;learning partial feedback;predictors passive learning;learner partial label;approach active learning;learners fully labels;labels effective learning;propose active learning;label learner;active learners;learning partially;effective learning partial;learning useful tool;feedback alpf learner;active learning neural;partially labeled data;passive learning;approach learning partial;learning partial;method learning partially;learn learning partial;learning useful;annotating data large;partially labeled"}, "bdfb9f1c79ad726049a3563c741311391e18532a": {"ta_keywords": "manipulation speech style;speech style manipulation;style manipulation speech;manipulation use speech;manipulation speech;manipulation speech useful;style manipulation use;style manipulation;manipulation use;use speech style;tool speech style;manipulation;speech style;use speech;tool speech;speech useful tool;useful tool speech;speech;speech useful;style;useful tool;tool;use;useful", "pdf_keywords": ""}, "eadf5023c90a6af8a0f8e8605bd8050cc13c23a3": {"ta_keywords": "chime speech separationion;speech separationion recognition;speech recognition ar;microphone conversational ar;chime speech;5th chime challenge;chime challenge;speech separationion;multi microphone conversational;microphone conversational;fifth chime speech;robust automatic speech;chime challenge considers;speech recognition;automatic speech;automatic speech recognition;separationion recognition challenge;distant multi microphone;microphone;introduces 5th chime;interface speech;multi microphone;5th chime;chime;separationion recognition;conversational ar;research interface speech;recognition ar;fifth chime;introductionthe fifth chime", "pdf_keywords": "speech recognition ar;speech separation recognition;speech separationion recognition;speech separation challenges;multimicrophone conversational ar;speech recognition home;survey speech separation;microphone situ speech;robust automatic speech;task distant multimicrophone;speech recognition;distant multimicrophone conversational;acoustic model speech;home environments speech;kinect microphone arrays;multimicrophone conversational;conversational speech recognition;automatic speech;acoustic learning;microphone arrays binaural;speech separation;arrays binaural microphone;automatic speech recognition;microphone arrays dataset;environments speech;distant kinect microphone;speech recognition real;speech separationion;microphone arrays;kinect microphone"}, "11465566a1f5ec7d4176bb7ab8edd26a154a1b60": {"ta_keywords": "privacy contracts electric;consumer valuation privacy;electric utilities consumers;proposing privacy contracts;privacy contracts;proposing privacy;utilities consumers;contracts electric utilities;utilities consumers goal;fair electric utility;sharing consumption data;privacy;valuation privacy;electric utilities;utility needs data;valuation privacy time;smart meters;problem proposing privacy;electric utility;grained data consumers;data consumers;data consumers need;electric utility needs;introduction smart meters;sharing consumption;privacy time;consumers need mechanisms;privacy time benefits;utilities;smart meters continue", "pdf_keywords": "designing privacy contracts;privacy contracts demand;proposing privacy contracts;privacy contracts offered;assessment privacy contracts;privacy contracts;utility privacy tradeoff;privacy contracts electric;privacy contracts inefficiencies;privacy service contract;utility privacy;study privacy contracts;consumer valuation privacy;optimal privacy policy;designing privacy;proposing privacy;framework utility privacy;optimal privacy;problem proposing privacy;privacy tradeoff fundamental;distribution particular privacy;example privacy service;valuation privacy;example privacy;privacy tradeoff;risk optimal privacy;privacy settings consumers;privacy tradeoff smart;incurred privacy;framework designing privacy"}, "ba00cbd314dc52b299a8b0c34f1887bcd43cdc12": {"ta_keywords": "using synonymy dictionaries;use synonymy dictionaries;synonymy dictionaries;graph synonyms extracted;synonymy dictionaries word;synsets using synonymy;dictionaries word embeddings;synonyms extracted;weighted graph synonyms;synonyms extracted commonly;word sense induction;graph synonyms;synonyms;word embeddings;using synonymy;word embeddings important;use synonymy;dictionaries word;apply word sense;word embeddings methodsfirst;dictionaries;synonymy;introductionthe use synonymy;sense induction;word sense;sense induction deal;optimizing synsets paper;optimizing synsets;tool optimizing synsets;resources aswikiktionary", "pdf_keywords": "graph synonyms extracted;semantically clean synonyms;clustering words synsets;synonymy dictionary algorithm;graph synonyms;synonyms extracted;using synonymy dictionaries;weighted graph synonyms;synonymy dictionaries;synonyms extracted commonly;clustering words;common synonymy dictionary;synonymy dictionary;ambiguous synonyms method;semantically related words;synonymy dictionaries word;global clustering words;approach clustering words;synsets using synonymy;clustering word;synonyms;individual ambiguous synonyms;synonyms method;words finally cluster;semantically relevant words;clusters graphs semantically;clean synonyms;graphs semantically related;graph semantically clean;clean synonyms compare"}, "48220433a2fb07761b26b2d6aa59b615289a3d4c": {"ta_keywords": "fooling graph neural;single node adversarial;node adversarial;adversarial example node;node adversarial example;attack fooling graph;attacker force gn;node attack fooling;single node attack;fooling graph;adversarial;adversarial example;strategy fooling graph;node attack;graph neural networks;graph neural network;graph neural;attack fooling;force gn;node picked attacker;gns vulnerable;force gn classify;gn classify target;gn classify;gns vulnerable extremely;strategy fooling;single node;gns;paper gns vulnerable;effective strategy fooling", "pdf_keywords": "adversarial attack graph;node adversarial attack;single node adversarial;attack graph neural;node adversarial;adversarial example node;node adversarial example;node indirect adversarial;adversarial attack;graph neural networks;datasets graph neural;attack graph;adversarial example single;graph neural;adversarial;example graph neural;use graph neural;introduction graph neural;adversarial example;choosing attacker node;attacker node effective;attacker node;attack single node;attacker node effectiveness;discriminate attacker node;multi node attack;node attack;node effectiveness attack;choose attacker node;attacker node fundamental"}, "25c50ef5a902586a06099ceb29e7f34e2172020a": {"ta_keywords": "wireless networks neural;neural networks wireless;networks wireless networks;neural networks used;wireless networks;neural networks fundamental;wireless networks applications;generate neural networks;neural networks neural;neural networks;networks neural networks;networks wireless;networks neural;generate neural;used generate neural;networks fundamental neural;fundamental neural networks;networks used generate;networks applications;networks applications applications;networks;wireless;neural;networks fundamental;networks used;fundamental neural;generate;used generate;applications applications;applications", "pdf_keywords": ""}, "e79bd5d5ad084009233c8524b02ac887029c5fe2": {"ta_keywords": "symplastic disease structure;symplastic structure chinese;symplastic structure;model symplastic structure;symplastic;history symplastic disease;model symplastic;symplastic disease;optimized model symplastic;history symplastic;structure chinese patient;disease structure characterized;disease structure;history history symplastic;chinese patient history;chinese patient;structure characterized;structure chinese;patient history;patient history history;structure;structure characterized use;designed optimized model;disease;model;patient;optimized model;characterized;characterized use combination;characterized use", "pdf_keywords": ""}, "f1b52bf723d7f5c4b68c8551c4d168ed1224f016": {"ta_keywords": "mitochondrial genome sinocyclocheilus;genome sinocyclocheilus wenshanensis;sinocyclocheilus wenshanensis cypriniformes;mitochondrial genome mitogenome;genome mitogenome wenshanensis;wenshanensis cypriniformes cyprinidae;mitochondrial genome;complete mitochondrial genome;wenshanensis cypriniformes;sinocyclocheilus wenshanensis;mitogenome wenshanensis;mitogenome wenshanensis generation;wenshanensis generation sequencing;genome sinocyclocheilus;characterized complete mitochondrial;analysis complete mitochondrial;complete mitochondrial;cypriniformes cyprinidae methods;cypriniformes cyprinidae;mitochondrial;wenshanensis;wenshanensis generation;genome mitogenome;cyprinidae methods study;introduction phylogenetic analysis;phylogenetic analysis complete;phylogenetic analysis;sinocyclocheilus;cyprinidae methods;cyprinidae", "pdf_keywords": ""}, "c14fb834ac6ede13f94f71cfaf5649b55e70a2c2": {"ta_keywords": "data aggregators;data aggregator;data aggregators directly;quality data purchase;single data aggregator;high quality data;quality data;quality data single;settings data aggregators;data purchase effort;data sources share;data sources;data aggregator addressing;ensure data sources;verify quality data;aggregators directly verify;aggregators;aggregators directly;data purchase;data played increasingly;data single data;data;aggregator;data sources creating;ensure data;mechanisms ensure data;sources creating data;data single;aggregator addressing issue;exerted data sources", "pdf_keywords": "data source aggregators;aggregators data sources;data data aggregators;data aggregators;data sources aggregators;data aggregators provide;aggregators offer data;data aggregator;multiple data aggregators;aggregators make data;data aggregator use;data aggregator incentivizes;aggregator data market;aggregator incentivizes data;data aggregators optimize;data aggregators directly;example data aggregators;approach data aggregators;method data aggregators;aggregators data;single data aggregator;aggregator data;mechanism data aggregator;data aggregators used;aggregators aggregators data;data aggregators benefit;buyers data aggregator;data aggregators unique;aggregator analysis aggregators;faced aggregator data"}, "7c085d7f50a76cf1a09a114986206256e0ee1931": {"ta_keywords": "tetrahydrosylated tetrahydrosylated tetrahydr;tetrahydrosylated tetrahydr;tetrahydrosylated tetrahydrosylated tetrahydrosylated;tetrahydrosylated tetrahydrosylated;role tetrahydrosylated tetrahydrosylated;tetrahydrosylated;role tetrahydrosylated;literature role tetrahydrosylated;tetrahydr;present results systematic;results systematic;results systematic review;systematic review literature;systematic review;systematic;review literature;article present results;aim article present;literature role;aim article;article present;article;present results;review literature role;results;literature;role;aim;review;present", "pdf_keywords": ""}, "293ed3367027c99a81ead6ff3f31be7de43bce9c": {"ta_keywords": "randomization partitioning patient;randomization partitioning;using randomization partitioning;peer selection;case peer selection;peer selection using;combination randomization partitioning;randomization;partitioning patient;partitioning patient treated;selection using randomization;using randomization;partitioning patient 65;treated combination randomization;case peer;diagnosed neoplastic;neoplastic disease patient;disease neoplastic;diagnosed neoplastic disease;neoplastic disease neoplastic;neoplastic;combination randomization;neoplastic disease;peer;disease neoplastic disease;partitioning;report case peer;man diagnosed neoplastic;selection;patient treated combination", "pdf_keywords": "peer review peer;review peer selection;peer review selection;incentives quality peer;peer review;peer evaluation;effectiveness peer review;effectiveness peer selection;review peer evaluation;review peer;peer selection impartial;peer reviews evaluations;peer reviews;peer review important;peer evaluation topic;peer selection focus;crowdsourcing work reviewing;peer selection;peer selection mechanisms;quality peer review;peer review impact;peer selection fundamental;studies peer review;effectiveness peer;peer selection general;peer review critical;assessing effectiveness peer;veracity peer review;mechanism allows crowdsourcing;prominent peer selection"}, "adeed0816a2cab763e3bee769957ff1849985759": {"ta_keywords": "spelling normalization;spelling normalization variant;normalization variant word;forms modern spellings;modern spellings;modern spellings greatly;variance spelling normalization;word forms modern;historical texts results;normalization variant;compares approaches normalization;approaches normalization;texts results;variant word forms;historical texts typically;approaches normalization focus;string distance measures;normalization;word forms;based string distance;spellings;historical texts;spellings greatly benefit;spellings greatly;types historical texts;texts typically;texts;pos tagging lemmatization;variance spelling;tagging lemmatization aim", "pdf_keywords": ""}, "bd8334c1246adbd47f80eea60249c30a74925d7a": {"ta_keywords": "impromptu deployment wireless;approach deployment agent;deployment agent explores;deployment wireless network;deployment agent forward;best relay placement;deployment wireless;relay placement;deployment agent;impromptu deployment;situations impromptu deployment;relay placement location;agents robots sequential;approach deployment;robots sequential decision;deployment approaches;networks human agents;selects best relay;forward approach deployment;agent explores successive;pure approach deployment;robots sequential;deployment approaches pure;possible deployment approaches;agent explores;wireless network networks;human agents robots;relay;deployment;best relay", "pdf_keywords": "sensor networks emergency;emergency situations wireless;optimal policy relay;networks emergency situations;learning optimal policy;situation deployment relays;deployment policy wireless;situation monitoring emergency;markov decision process;cost markov decision;monitoring emergency situations;optimal sequential decision;situations wireless networks;robots emergency situations;agents robots emergency;algorithms use wireless;monitoring emergency;sensor networks feasible;optimal agent steps;outage probabilities network;sensor networks;wireless networks deployed;networks emergency;online learning algorithms;deployment optimal policy;deployment wireless relay;learning optimal;optimal relay;algorithm relay;wireless networks"}, "7e0eb21f4903c2fe860d1c4f213879e99d7cd23c": {"ta_keywords": "electrolaryngeal speech enhancement;electrolaryngeal speech minimizing;approach electrolaryngeal speech;speech enhancement;electrolaryngeal speech;speech enhancement improvement;naturalness electrolaryngeal speech;listenability electrolarynx device;improving naturalness electrolaryngeal;sounds enable laryngectomees;electrolarynx device artificially;degradation listenability electrolarynx;listenability electrolarynx;electrolarynx device;laryngectomees produce speech;enable laryngectomees produce;hybrid approach electrolaryngeal;generates excitation sounds;excitation sounds enable;enable laryngectomees;speech minimizing degradation;naturalness electrolaryngeal;approach electrolaryngeal;produce speech;electrolarynx;electrolaryngeal;excitation sounds;speech minimizing;laryngectomees produce;laryngectomees", "pdf_keywords": ""}, "1d56a0b8fb560a79ca28b44bfd6f1e645a36549a": {"ta_keywords": "resistance measurement thermal;measurement thermal control;measurement function thermocouple;measurement thermal;thermal control device;automatic resistance measurement;thermocouple heating belt;automatic measurement resistance;function thermocouple heating;function thermocouple;resistance measurement;measurement resistance value;methods resistance measurement;resistance measurement function;thermocouple heating;based thermal control;thermal control;measurement resistance;value based thermal;thermocouple;heating belt;belt heating plate;resistance value based;heating belt heating;automatic resistance;belt heating;resistance value;based thermal;device methods resistance;realize automatic resistance", "pdf_keywords": ""}, "ead3182dd47bdd8da98476cca1cfe0373dfc2edc": {"ta_keywords": "clustering vbec speech;bayesian estimation clustering;continuous speech recognition;model speech recognition;clustering based gaussian;estimation clustering vbec;vbec speech recognition;gaussian mixture model;based gaussian mixture;speech recognition;clustering large vocabulary;variational bayesian estimation;acoustic model speech;speech recognition described;gaussian mixture;vocabulary continuous speech;decision tree clustering;estimation clustering large;estimation clustering;vbec speech;speech recognition using;clustering vbec;model speech;using variational bayesian;variational bayesian;introductionvarial bayesian estimation;speech recognition resultswe;continuous speech;mixture model gm;mixture model", "pdf_keywords": ""}, "b593be8ff3c09c6994657678fcde0c5adf43328e": {"ta_keywords": "unsupervised constituency parsing;constituency parsing methods;constituency parsing;induction distant supervision;phrase bracketing improve;supervision span constraints;structural annotation text;constraints phrase bracketing;phrase bracketing;parsing;structural annotation;distant supervision span;distant supervision form;span constraints phrase;supervision form span;uses distant supervision;parsing methods;latent tree induction;supervision span;span constraints substantially;span constraints useful;distant supervision;span constraints;annotation text;annotation text work;tree induction distant;parsing methods using;improvedd latent tree;bracketing improve;unsupervised constituency", "pdf_keywords": "neural unsupervised parser;training improve parsing;unsupervised constituency parsing;unsupervised parsing viable;improves constituency parsing;machinery unsupervised parsing;unsupervised parsing model;improve unsupervised parsing;unsupervised parsing;effective improving parsing;approaches unsupervised parsing;improving parsing;constituency parsing compared;unsupervised parser;approach unsupervised parsing;constituency parsing;improving parsing performance;purpose unsupervised parsing;improve parsing;unsupervised parsing use;unsupervised parser using;progress unsupervised parsing;improve parsing performance;constituency parsing english;parse tree annotation;unsupervised parsing discuss;representation parse tree;treebank;parsing performance training;method unsupervised parsing"}, "07a5536c0570804f816fdb5a0a5ae890630e61bd": {"ta_keywords": "electrolaryngeal speech enhancementbased;electrolaryngeal speech enhancement;speech enhancement method;speech enhancement;speech enhancementbased;speech enhancementbased noise;approach electrolaryngeal speech;electrolaryngeal speech;presents electrolaryngeal speech;enhancementbased noise reduction;statistical voice conversion;voice conversion method;noise reduction method;voice conversion;noise reduction statistical;noise reduction;using noise reduction;enhancementbased noise;parameters statistical voice;enhancement method capable;enhancement method;speech causing degradation;mel speech causing;reduction statistical excitation;naturalness mel speech;statistical voice;excitation generation methodsthis;hybrid approach electrolaryngeal;statistical excitation generation;mel speech", "pdf_keywords": ""}, "d14afc470cd90521147130e153c0d3e1324cd104": {"ta_keywords": "language reppositionations typoology;reppositionations typoology prediction;knowledge existing typological;learning language reppositionations;translation learns;know neural machine;learns translate language;language reppositionations;languages knowledge;nonprogramming neural models;learns translate;machine translation learns;translation learns translate;neural neural nonprogramming;neural nonprogramming neural;neural nonprogramming;neural models know;nonprogramming neural;existing typological databases;know neural;models know neural;reppositionations typoology;typological databases;typoology prediction;language does learn;neural machine translation;languages knowledge extracted;semantics languages knowledge;typological databases contain;learn syntax semantics", "pdf_keywords": "machine translation learns;language models neural;neural language models;languages neural models;neural machine translation;learned language vectors;training multilingual neural;translation learns;multilingual neural language;multilingual neural;trained translate languages;language vectors;learns translate language;learning language vector;translation learns translate;languages neural;translation model trained;learns translate;language vectors useful;machine translation model;language models;trained translate;machine translation;machine translation systems;vectors typology prediction;model trained translate;networks learn linguistic;lstm sentences language;neural language;language vector"}, "6aac35ec3bfaf7e835ac633414419c9623838007": {"ta_keywords": "cochleogram spectrogram features;dimensional cochleogram spectrogram;spectrogram features deep;based anrasal arrasrasrasrasrasrasrasrasrasrasrasrasrasrasrasrasrasrasrasrasrasras;spectrogram features;cochleogram spectrogram;anrasal arrasrasrasrasrasrasrasrasrasrasrasrasrasrasrasrasrasrasrasrasrasras;spectrogram;arrasrasrasrasrasrasrasrasrasrasrasrasrasrasrasrasrasrasrasrasrasras;learning based anrasal;dimensional cochleogram;combination dimensional cochleogram;cochleogram;features deep learning;anrasal;based anrasal;features deep;deep learning;deep learning based;features;introduction combination dimensional;dimensional;combination dimensional;deep;learning;learning based;introduction combination;combination;introduction;based", "pdf_keywords": ""}, "9895531c6dc3854f082de1a1ec651a9e179bbd07": {"ta_keywords": "jointly modelling phonemes;documentation speech recognition;modelling phonemes tones;modelling phonemes;speech recognition;connectionist temporal classification;tonal transcription language;speech recognition technology;phonemes tones;phonemic tonal transcription;temporal classification loss;acoustic speech;phonemes tones versus;acoustic speech important;transcription language;transcription language documentation;language documentation speech;phonemes;documentation speech;tonal transcription;phonemic tonal;function phonemic tonal;speech important language;introduction acoustic speech;architecture connectionist temporal;loss function phonemic;temporal classification;neural network architecture;connectionist temporal;harnessed aid linguists", "pdf_keywords": ""}, "8d1fd086a76d30343d2224b61cb7ddab2125d0b2": {"ta_keywords": "symbollevel learning use;approach symbollevel learning;symbollevel learning;model symbollevel learning;symbollevel learning sll;distribuktionfele learninging theoryy;distribuktionfele learninging;introductionexusing distribuktionfele learninging;approach symbollevel;common approach symbollevel;symbollevel;model symbollevel;learninging;results model symbollevel;learninging theoryy;learninging theoryy alyze;learning;learning use;learning sll methodsa;introductionexusing distribuktionfele;learning use sort;learning sll;distribuktionfele;solve previous search;search problems examples;search problems;search;sll methodsa;methodsa common approach;previous search problems", "pdf_keywords": ""}, "3256198d819f23f82640490b9160e85139627d6c": {"ta_keywords": "crossing representation wavelet;wavelet transform;representation wavelet transform;wavelet transform domain;representation wavelet;signal reconstruction;feature signal reconstruction;signal reconstruction problem;algorithm signal reconstruction;wavelet;signal reconstruction extend;zero crossing representation;stabilized zero crossing;iterative algorithm signal;image processing present;minimum norm optimization;reconstruction problem reduces;image processing;zero crossing;reconstruction problem;norm optimization;algorithm signal;application image processing;norm optimization problem;feature signal;reconstruction extend;iterative algorithm;transform;salient feature signal;crossing representation", "pdf_keywords": ""}, "f9a86c2df17f408105c2d3e9429410cdc376c6f0": {"ta_keywords": "computationallinguistics held;computationallinguistics;computational linguistics;distributionallexicical semantics;computational linguistics gms;dedicated distributionallexicical semantics;linguistics gms 2010;association computationallinguistics held;linguistics gms;linguistics;association computationallinguistics;computationallinguistics held 40;organization computational linguistics;distributionallexicical semantics proof;chapter association computationallinguistics;semantics;semantics proof high;semantics proof;distributionallexicical;computational;dedicated distributionallexicical;european organization computational;natural engineering;journal natural engineering;engineering dedicated distributionallexicical;organization computational;engineering;2010 workshop held;2009 workshop;natural engineering dedicated", "pdf_keywords": ""}, "66713fbcb8d5e48a9eb6425bd7fdbb53751e60b1": {"ta_keywords": "billions integers vectorization;messagedecoding billions integers;clinical messagedecoding billions;integers decoding;decoding;arrays integers decoding;messagedecoding billions;integers vectorization important;decoding arrays;compression;importantly decoding arrays;key clinical messagedecoding;compression decompression;decoding arrays consume;integers vectorization;integers decoding importantly;associated compression;clinical messagedecoding;compression decompression particular;associated compression decompression;vectorization important applications;costs associated compression;vectorization important;decoding importantly decoding;importantly decoding;database systems data;data stored;decompression particular researchers;data;database systems", "pdf_keywords": "byte simple efficient;fastest decoding algorithms;efficiently decode;encoding speed propose;fastest decoding;faster decoding;significantly faster decoding;encoding speed;faster decoding speed;instruction efficiently decode;encoding speed significantly;queries encoding speed;faster decoding simple;decoding speed encoding;data compression;report fastest decoding;data compression algorithm;compression algorithm powerful;known integer encoding;encoding speed encoding;data compressed;novel algorithm storing;speed encoding algorithm;efficient decoding procedure;speed encoding;efficiently decode sequence;decoding speeds compression;efficient efficient decoding;decoding speed compression;efficient decoding"}, "3a2446c47000c3d0681b2cdf6d8b87a11ff630e2": {"ta_keywords": "wall insulation boards;wall insulation board;insulation board installation;engineering exterior wall;workers installation engineering;installation engineering exterior;board installation engineers;insulation boards designed;exterior wall insulation;insulation board;insulation boards;installation engineers;installation engineering;building exterior wall;construction workers installation;wall insulation;mounting device building;device building exterior;3dimensional software solidworks;engineering exterior;solidworks perform dimensional;methodsthe automatic mounting;building exterior;automatic mounting device;installation engineers poor;labor intensity construction;workers installation;modeling virtual assembly;software solidworks;insulation", "pdf_keywords": ""}, "5ce3148ed36a1ea034da2c05b8cde9efbaf43e6a": {"ta_keywords": "deep beamforming network;field speech recognition;deep beamforming;trained jointly acoustic;far field speech;recently deep beamforming;speech recognition;speech recognition recently;beamforming network;beamforming network proposed;speech recognition ar;automatic speech recognition;jointly acoustic model;introductionbeamforming networks;field speech;beamforming;jointly acoustic;introductionbeamforming networks using;minimize automatic speech;automatic speech;features far field;predict weights phase;acoustic model minimize;predict weights;acoustic model;recognition ar cost;gc network trained;proposed predict weights;covariance features far;network trained", "pdf_keywords": ""}, "981dbdf6f87f13f3f3047a925c519fc39a35202b": {"ta_keywords": "neural probabilistic language;probabilistic language model;language modeling;probabilistic language;language modeling driven;word embeddings;network predict word;predict word;language model;language model nml;concatenates word embeddings;revisit neural probabilistic;neural probabilistic;progress language modeling;word embeddings fixed;predict word scaled;advances neural architectures;neural architectures;forward network predict;revisit neural;neural;recent progress language;simply concatenates word;concatenates word;embeddings;advances neural;paper revisit neural;neural architectures hardware;network predict;progress language", "pdf_keywords": "attention language modeling;neural language models;neural language model;attention language;self attention language;novel language models;language models results;efficient language models;language modeling models;language models;neural probabilistic language;new neural language;language modeling;language models useful;language modeling datasets;layer self attention;language models important;goal neural language;level language modeling;language models demonstrate;language models ability;self attention domain;language model;attention domain;neural language;modernized neural language;favor neural architectures;language model nml;neural architectures transformers;probabilistic language model"}, "d92e0443768ec3715205cb232ef1a1917372b0af": {"ta_keywords": "automatic speech processing;speech processing output;language understanding slus;using automatic speech;speech processing;downstream natural language;automatic speech;speech processing systems;spotken language understanding;open source toolkits;slus benchmarks need;slus benchmarks;language processing;source toolkits;language processing tasks;introduction automatic speech;spotken language;understanding slus benchmarks;natural language processing;open source standard;source toolkits used;natural language;different spotken language;open source;language understanding;toolkits;tasks open source;toolkits used;toolkits used generate;benchmarks need build", "pdf_keywords": "language understanding toolkit;toolkit supports speech;annotated speech processing;speech tasks datasets;speech processing toola;speech tasks data;source speech processing;language understanding ic;speech speech tasks;downstream natural language;automatic speech;speech processing tasks;automatic speech processing;supervised learning speech;speech tasks;annotated speech;model speech language;open source speech;speech language understanding;speech speech processing;conversations developed toolkit;using automatic speech;speech processing;multiple annotated speech;spotken language understanding;learn speech fundamental;model speech;speech processing output;background automatic speech;speech recognition"}, "04e3a3ee41c1ee977e023052435bbb5f4c680f66": {"ta_keywords": "freshippo stores china;study freshippo stores;freshippo stores;new retail stores;oriented new retail;stores case study;new retail;case study freshippo;retail stores;freshippo;stores china;stores china china;retail;retail stores case;study freshippo;stores china order;stores;feasibility developing community;community oriented new;developing community;developing community oriented;stores case;community oriented;china order;china china;feasibility developing;oriented new;evaluate feasibility developing;china;present case study", "pdf_keywords": ""}, "eba4c6b0b860a34461ffb8544111c89a3ef0d8b7": {"ta_keywords": "noisy pairwise comparisons;pairwise comparisons;pairwise comparisons results;pairwise comparisons pair;analysis ranking problem;ranking;ranking problem;given pairwise comparisons;comparisons pair items;analysis ranking;comparisons pair;ranking problem methods;competitive analysis ranking;items noisy pairwise;items comparison noise;pair items comparison;topk items;social choice crowdsourcing;items comparison;comparison noise constrained;noisy pairwise;recommender systems web;topk items noisy;comparison noise;search social choice;choice crowdsourcing;comparisons results setting;comparisons;topk;recommender systems", "pdf_keywords": "ranking aggregation pairwise;ranking aggregation;ranking order;noisy pairwise comparisons;aggregation pairwise comparisons;global ranking order;problem ranking aggregation;ranking order unnecessarily;pairwise comparisons widely;inferring total ordering;global ranking;ranking;total global ranking;pairwise comparisons analyze;strong stochastic transitive;pairwise comparisons random;pairwise comparisons;algorithm strong stochastic;stochastic transitive st;ennt problem ranking;problem ranking;stochastic transitive;total ordering;comparison pairwise comparisons;pairwise comparisons method;aggregation pairwise;ordering;efficient algorithm domination;efficient algorithm corresponding;pairwise comparison"}, "9d628e420922cc23a8944de1511ca5d3309f5d58": {"ta_keywords": "health records notes;notes electronic health;electronic health records;patients increasingly record;record audio visits;notes digitized benefits;records notes digitized;records notes;physician burnout parallel;increasingly record audio;called soop notes;draft detailed clinical;soop notes electronic;notes digitized;digitized benefits documentation;health records;increasing physician burnout;physician burnout;physicians draft detailed;clinical summaries;patient visit physicians;audio visits;clinical summaries called;soop notes;patient visit;detailed clinical summaries;electronic health;audio visits consent;benefits documentation process;record audio", "pdf_keywords": ""}, "40ba59c9945e7c19d06dadfa8f496da5810ee30d": {"ta_keywords": "beam search encoder;speech faster beam;faster beam search;vectorization hypotheses speech;beam search algorithm;beam search;hypotheses speech faster;right beam search;attention based encoder;current beam search;decoder based speech;speech recognition;search encoder decoder;beam search expands;search encoder;hypotheses speech;encoder decoder network;speech faster;encoder decoder;encoder decoder based;speech recognition methods;search expands hypotheses;search algorithm inference;decoder network;based speech recognition;based encoder decoder;based encoder;recognition methods attention;encoder;decoder", "pdf_keywords": "hypotheses utterances batch;beam hypotheses utterances;conversational speech recognition;utterances batch;data speech corpora;speed cj corpus;recognition speech;utterances single molecule;batch multiple utterances;data speech;speech recognition;speech recognition speech;utterances batch present;utterances achieved speedup;recognition speech fundamental;analyzing data speech;synchronous beam search;online neural;corpus speech;cj corpus;speech corpora using;speech utterances single;speech recognition article;beam search algorithm;beam search;online neural network;input utterances achieved;decoder network;speech corpora;new corpus speech"}, "ccd33442fef058c7c0eafc57d2c6e6a4cde10a3b": {"ta_keywords": "convolutions molecular graphs;graphs protein model;graphs protein;backgroundpherical convolutions molecular;molecular graphs protein;images graphs;convolutions molecular;images graphs generally;molecular graphs;backgroundpherical convolutions;protein model quality;convolutional neural networks;protein model;contrary images graphs;rotation equivariant operations;graphs generally irregular;networks;graphs generally;graphs;rotation equivariant;convolutions;using rotation equivariant;define rotation equivari;convolutional neural;rotation equivari;rotations;rotations input data;convolutional;particular rotations;define rotation", "pdf_keywords": "protein graphs;structure protein graph;protein graph;protein graphs propose;spherical graph convolutions;method protein graphs;learning molecular graphs;spherical graph convolutional;graphs protein;protein graphs applied;graphs protein molecule;applied protein graphs;molecular graphs protein;molecule spherical graph;graphs applied protein;protein graph results;graphs ability protein;represented molecular graphs;structure molecular graph;graph convolutional network;graph convolutional networks;prediction protein structure;protein structure prediction;molecular graphs;graph nodes protein;graph convolutions;construction spherical graph;structure prediction protein;graph convolutional neural;3d protein structure"}, "5dd34d2781ca702d0e3cd1224517ff60d6c3e2ee": {"ta_keywords": "latin script languages;non latin script;script languages roman;latin script;informal digital communication;encode non latin;informal digital;humans informal digital;introduction informal digital;digital communication idiosyncratic;keyboards character substitution;languages roman format;original non latin;languages roman;roman format;common keyboards character;humans informal;digital communication encode;latin;character substitution choices;script languages;digital communication;keyboards character;character substitution;communication idiosyncratic;used humans informal;communication encode non;communication encode;non latin;informal", "pdf_keywords": "model decoding romanized;informal romanization improved;decoding romanized;romanization encoding;alphabetthe informal romanization;romanization improved;decoding romanized text;accuracy informal romanization;informal romanization cipher;informal romanization;non decoding romanized;introduction informal romanization;romanization improved using;transliteration preferences;transliteration;transliterations;transliteration preferences handling;romanization idiosyncratic process;process romanization;languages process romanization;decode romanized sequence;method decoding romanized;romanization;decoding accuracy languages;model decode romanized;user transliteration preferences;candidate transliterations;romanization encoding source;informal romanization idiosyncratic;user transliteration"}, "054ba27fe5cc6085d20ea2707de886db6865dbed": {"ta_keywords": "relational retrieval;walks scientific literature;relational retrieval using;ad hoc retrieval;hoc retrieval named;retrieval named entity;hoc retrieval;retrieval using;introduction relational retrieval;retrieval named;literature rich metadata;retrieval;walks scientific;scientific literature;entity recognition ner;scientific literature rich;proximity queries graph;random walks scientific;typed proximity queries;labeled directed graph;named entity recognition;graph ad hoc;retrieval using combination;entity recognition;metadata represented labeled;directed graph;directed graph graph;queries graph ad;metadata;proximity queries", "pdf_keywords": ""}, "7621bfe36cc649a5876cea587366201e158a8b38": {"ta_keywords": "semantic role labeling;domain adaptation;approaches domain adaptation;domain adaptation biological;role labeling;role labeling sls;introductiondomain adaptation remains;introductiondomain adaptation;use semantic role;semantic role;adaptation biological domain;training lstm cf;lstm cf;pre training lstm;biological domain involves;training lstm;lstm;biological domain;labeling sls systems;spread use semantic;labeling sls;semantic;labeling;use semantic;resource domain;adaptation;resource domain specific;approaches domain;adaptation biological;adaptation remains challenging", "pdf_keywords": ""}, "c1a4c5380d90dc77064de6003cfb9611ad218600": {"ta_keywords": "dialogue response generation;neural dialogue response;sentiment controlled dialogue;dialogue generator;neural dialogue;dialogue generator assisted;dialogue response;controlled dialogue generator;method neural dialogue;responses according dialogue;response generation;controlled dialogue;training sentiment controlled;response generation allows;dialogue;training sentiment;controlling sentiment response;learning training sentiment;dialogue history explicitly;response sentiment labels;dialogue history;conditional adversarial;generator assisted adversarial;response sentiment;conditional adversarial learning;sentiment response;response generating;adversarial;according dialogue history;sentiment response sentiment", "pdf_keywords": "adversarial dialogue generation;neural dialogue generation;dialogue response generation;dialogue generation response;dialogue generation model;dialogue generation;generate dialogue responses;adversarial dialogue;dialogue generator;development adversarial dialogue;dialogue generation control;model dialogue generation;dynamic dialogue generation;dialog generation;dialogue generator assisted;generate dialogue;sentiment controlled dialogue;dialogue generation maximizes;dialogue able generate;model generate dialogue;dialogue generation evaluate;sentiment dialogue responses;dialogue responses sentiment;controlled dialogue generator;neural dialogue response;dialog generation discuss;control sentiment dialogue;dialogue responses effectively;generation model dialogue;generate utterances sentiment"}, "9d06638df32f8feefb95ef5a4769adbb1ae6297d": {"ta_keywords": "effective rule learner;built rule learners;rule learner described;rule learner generates;learner generates rulesets;new rule learner;rule learner;rule learners ensemble;rulesets repeatedly boosting;rule learners;generates rulesets;rule builder;rulesets built rule;greedy rule builder;rule builder like;generates rulesets repeatedly;learners ensemble rules;rule build;rulesets built;builder like rulesets;built rule;rulesets;fast effective rule;rulesets repeatedly;like rulesets built;ensemble rules created;rules created slipper;ensemble rules;like rulesets;constraints rule build", "pdf_keywords": ""}, "4c78943e11195fb72a3c878a03b248bc317180e0": {"ta_keywords": "learnability order representations;polynomial learnability;polynomial learnability order;learnability restricted;results polynomial learnability;instead learnability restricted;learnability order;learnability;learning concepts expressed;prolog highly restricted;instead learnability;prolog highly;learnable;subsets prolog highly;learning concepts;prolog;subsets shown learnable;shown learnable;study instead learnability;subsets prolog;focused subsets prolog;learnable paper;concepts expressed order;expressed order logic;shown learnable paper;pac model focused;logic relatively formal;learning;concepts expressed;pac model", "pdf_keywords": ""}, "f0baf134f0a2ee6e99f6f2287791109cf93305e7": {"ta_keywords": "figure text matching;summarization biological data;captions literature;text matching;images depicting protein;text mining opticalcal;opticalcal character recognition;accompanying captions literature;captions literature present;depicting protein subcellular;summarization biological;recognition techniques caption;organization summarization biological;figure text;biological data;biological data data;mining opticalcal character;character recognition;text matching build;understanding figure text;figures accompanying captions;search engines fluorescence;depicting protein;fluorescence microscope images;captions;accompanying captions;microscope images;caption understanding;protein subcellular patterns;microscope images depicting", "pdf_keywords": ""}, "db253b17043b6a86e02173b6aa597664b0c7f256": {"ta_keywords": "modeled compositionality words;compositionality words creating;compositionality effect character;creating embeddings characters;compositionality words;embeddings characters;words creating character;embeddings characters based;modeled compositionality;writing systems compositionality;characters based visual;words writing systems;creating embeddings;compositionality;compositionality effect;effect creating embeddings;work modeled compositionality;embeddings;character level models;words creating;creating character;visual characteristics creating;words writing;sparsity rare words;characters based;effect character;writing systems;characters;rare words writing;meaning character", "pdf_keywords": "modeling compositionality characters;embeddthe compositionality characters;characters convolutional neural;learn embeddings compositional;compositionality characters language;modeled compositionality words;compositionality characters way;learn embeddings;characters convolutional;compositionality characters;learn embeddings generated;compositionality words creating;learning representation;creating embeddthe compositionality;appearance characters convolutional;modeling compositionality;embeddings compositional;compositionality characters fundamental;satisfies compositionality characters;entity recognition;able learn representation;learn representation character;embeddings way;learn representation;generates visual embeddings;visual embeddings way;visual embeddings;compositionality words;embeddings compositional component;representation image"}, "f516c98c3d2dde5b31931715fbc48bbbc0580e27": {"ta_keywords": "workgroups leaders;workgroups leaders previously;email collection workgroups;collection workgroups leaders;messages exchanged team;investigate team leadership;collaborative workgroups;group leader performance;monitor group leader;workgroups;team leadership roles;team members task;workgroups paper investigate;team leadership;collaborative workgroups paper;work group dynamics;team members;group leader;tool collaborative workgroups;exchanged team members;collection email messages;email messages;leadership roles inferred;email messages exchanged;communication tool collaborative;work group;email collection;workgroups paper;collection workgroups;leader performance", "pdf_keywords": ""}, "553b74de8cb7ebca42a686e2a3a2d6aae170946e": {"ta_keywords": "speech enhancement diffuze;based speech enhancement;speech enhancement;backgrounddiffusion probabilistic models;backgrounddiffusion probabilistic;propose diffusion probabilistic;diffusion probabilistic;enhancement diffuze model;diffusion probabilistic model;model based speech;images raw audio;enhancement diffuze;waveforms paired diffusion;paired diffusion reverse;model natural images;backgrounddiffusion;raw audio waveforms;paired diffusion;gaussian noise noisy;propose diffusion;gaussian noise;diffusion reverse processes;diffusion reverse;raw audio;clean signals based;audio waveforms;diffusion;signals gaussian noise;probabilistic models;audio waveforms paired", "pdf_keywords": "model speech enhancement;speech enhancement diffuze;enhancement speech;probabilistic model speech;approach speech enhancement;based speech enhancement;enhancement speech recognition;speech enhancement;2015 speech enhancement;speech enhancement speech;model trained diffusion;denoising steps diffusion;model based speech;propose diffusion probabilistic;novel diffusion probabilistic;trained diffusion;model speech;analyze enhanced speech;generating enhanced speech;trained diffusion process;backgrounddiffusion probabilistic models;speech enhancement neural;speech enhancement important;diffusion probabilistic;speech enhancement discuss;use diffusion probabilistic;enhanced speech signals;diffusion probabilistic model;enhanced speech;process diffusion probabilistic"}, "616cc6826066184a8c77c3f2562e4e891ce42911": {"ta_keywords": "reinforcement learning sisyphean;deep network dqn;deep reinforcement learning;dqn algorithm doomed;deep reinforcement;use deep reinforcement;learning sisyphean;reinforcement learning wild;dqn algorithm;dqn;network dqn;combating reinforcement learning;network dqn algorithm;algorithm doomed sisyphean;reinforcement learning;doomed sisyphean;combating reinforcement;agent avoid catastrophic;fear use deep;sisyphean;reinforcement;doomed sisyphean curse;deep network;popular deep network;learning sisyphean cursure;sisyphean curse;intrinsic fear;introduction combating reinforcement;learning wild;environments popular deep", "pdf_keywords": "fear reinforcement learning;intrinsic fear reinforcement;fear reinforcement;fear learned reward;intrinsic fear learned;fear algorithm;deep reinforcement learning;intrinsic fear algorithm;optimal policy environment;fear model;optimal policy policy;reinforcement learning;fear model model;model fear model;deep reinforcement;learned reward shaping;fear learned;polices optimal policy;problems deep reinforcement;shaping intrinsic fear;optimal policy;fear model given;fear algorithm development;catastrophic states optimal;games intrinsic fear;danger model propose;fear model using;states optimal agent;danger model;propose intrinsic fear"}, "7e82015c386726f4b8f6f686b6e6bb7d1e7564bb": {"ta_keywords": "controversy detection;network controversy detection;controversy detection objectiveto;level controversy detection;controversy detection methodsadditional;comment graph convolutional;comment graph;post comment graph;graph convolutional network;information graph convolutional;convolutional network controversy;graph convolutional;graph structure content;controversy;post level controversy;topics posts comments;topic post comment;structure content topics;information graph;introductionintegrating semantic;information graph structure;structural information graph;network controversy;introductionintegrating semantic structural;posts comments;convolutional network;topics posts;content topics posts;comments post level;semantic structural information", "pdf_keywords": "controversy detection weibo;controversy detection social;detection controversial posts;controversy detection china;identifying controversial posts;controversy detection;controversy detection consisting;background controversy detection;controversy detection extend;controversy detection methods;detection controversy web;dataset controversy detection;analyze controversy detection;level controversy detection;detection controversial;controversy detection implement;detecting controversy;detecting controversy china;effective detecting controversy;controversial posts social;controversial posts;topic postcomment graph;comment graph convolutional;topicpost comment graph;background identifying controversial;detect controversy;mining public sentiment;effectively detect controversy;manually labeled controversial;identifying controversial"}, "b8cabd2f7fbf816d667701c5d756b5fcb982e6fe": {"ta_keywords": "local minmax equilibriumlibria;minmax equilibriumlibria finite;equilibriumlibria finite timescale;minmax equilibriumlibria;player non convex;gradient descent ascent;backgroundgradient descent ascent;tau gradient descent;descent ascent provably;sum games learning;learning rate player;strict local minmax;descent ascent player;local minmax;descent ascent;equilibriumlibria finite;ascent provably converges;gradient descent;ascent player non;finite timescale separation;zero sum games;games learning rate;convex non concave;equilibriumlibria;ascent player;non convex;backgroundgradient descent;minmax;ascent provably;ascent", "pdf_keywords": "learning dynamics finite;learning dynamics converge;dynamics finite learning;learning dynamics;learning dynamics remain;equilibriawe gradient descent;finite learning rate;time gradient descent;simultaneous gradient play;learning rate player;dynamics learning rate;ascent learning;ascent finite timescale;gradient descent ascent;learning dynamics function;behavior gradient descent;strategy individual gradient;learning rates provide;descent ascent learning;ascent timescale separation;regularized learning dynamics;firstgradient descent ascent;ascent known gradient;ascent gradient descent;finite learning;learning rates;decaying stepsizes learning;dynamics maximizing player;update gradient descent;descent ascent dynamic"}, "c5a323f8744838093ee36bee3739dea599ce62f0": {"ta_keywords": "processes neoplastic cells;development neoplastic cells;neoplastic cells;neoplastic cells described;cells demonstrate neoplastic;neoplastic cells demonstrate;mechanisms neoplastic;mechanisms neoplastic processes;demonstrate neoplastic;neoplastic processes neoplastic;demonstrate neoplastic processes;development neoplastic;literature mechanisms neoplastic;processes neoplastic;neoplastic processes;neoplastic processes major;neoplastic;neoplastic processes poorly;cells described;approach development neoplastic;cells described literature;cells;factor development neoplastic;cells demonstrate;processes;processes major;understood development;understood development new;development;poorly understood development", "pdf_keywords": ""}, "e82eff0f3e3d150617f9a721f83046940a963c03": {"ta_keywords": "explanation based generalization;learning using explanation;concept learning concept;based generalization abstraction;make concept learning;introduction concept learning;concept learning;concept learning using;concept learning problem;learning concept;concept learning feasible;generalization abstraction;learning concept learning;problem concept learning;examples using abstraction;based generalization;generalization abstraction mechanism;finding unknown concept;explanation based general;using explanation based;learning algorithms explicitly;generalization;learning using;learning algorithms;abstraction functions based;using abstraction;abstraction;learning problem finding;make concept;using abstraction functions", "pdf_keywords": ""}, "c8171eaa3a3aac78c3b37351412101bc06e5f359": {"ta_keywords": "resource languages images;low resource languages;monolingual annotators;languages monolingual annotators;monolingual annotators resultswe;monolingual annotators methodswe;comparable training data;languages images;resource languages;languages images propose;resource languages monolingual;quality comparable training;comparable training;training data low;data low resource;training data;languages;low resource;annotators;languages monolingual;annotators resultswe propose;annotators methodswe propose;annotators resultswe;annotators methodswe;resource;collection low resource;curating high quality;monolingual;high quality comparable;comparativable data collection", "pdf_keywords": "monolingual image captioning;monolingual annotators methods;captions images languages;monolingual annotators;languages monolingual annotators;integrating multilingual images;multilingual image dataset;captioning corpora improve;image captioning corpora;multilingual images;multilingual image;captioning corpora;massively multilingual image;captioning corpora discussed;captions data;quality translations;images massively multilingual;india captions data;evaluating quality translations;images target language;crowdsourcing comparable data;images source language;monolingual image;images languages;hindi comparable corpora;multilingual images massively;machine translation measure;annotators proficient languages;large monolingual image;machine translation"}, "51c33a79e05425b6335c8676a166a0f4e178c0a2": {"ta_keywords": "automatic skillcoding;semi automatic skillcoding;skillcoding line test;automatic skillcoding line;skill coding;text classification;test items skill;items skill coding;automatic skillcoding view;novel text classification;assessment;skillcoding;evaluate novel text;coding allows assessment;assessment aimto evaluate;assessment occur;assessment aimto;skill coding allows;text classification approach;evaluate automatic;assessment occur level;allows assessment;skillcoding line;allows assessment occur;skillcoding view;classification approach improving;skillcoding view supporting;knowledge gaps students;line assessment aimto;line test items", "pdf_keywords": ""}, "2c1cb736df7bf526fc3facecd078980e007abceb": {"ta_keywords": "yeast calmodulin protein;yeast calmodulin;calmodulin protein involved;calmodulin protein;enzymes protein;enzymes protein involved;various enzymes protein;yeast;protein involved;protein involved various;enzymes;calmodulin;involved various enzymes;protein;various enzymes;involved;involved various;various", "pdf_keywords": ""}, "62d1a3137b01a69443bebf4d92c1990ec512a6a1": {"ta_keywords": "data extraction attack;attack ongpt language;language models trained;trained private datasets;examples querying language;training examples querying;language model trained;extraction attack;extract hundreds verba;models trained private;training data extraction;querying language model;querying language;language models;adversary perform training;private datasets;trained scrapes public;private datasets paper;parameter language models;model trained scrapes;language model methods;ongpt language model;extraction attack recover;billion parameter language;language model;scrapes public internet;internet able extract;examples querying;able extract hundreds;demonstrate attack ongpt", "pdf_keywords": "trained public data;language models exploit;attacks models trained;data extraction attacks;information attacks;privacy leakage;models trained confidential;attack language model;mitigate privacy leakage;memorization attack;memoryized information attacks;privacy leakage ability;privacy attacks;memorization attack examine;demonstrate attack language;data privacy attacks;privacy attacks using;attack language;extraction attacks;data large language;memorization attack problem;concept attack web;information attacks conclude;large language models;trained confidential private;public data language;language model trained;caused memorization attack;attacks practical data;models memorize leak"}, "bc4cb14af1023123b3122a5f0b6f3bb76334ffb4": {"ta_keywords": "lansce proton storage;proton storage ring;alamos neutron;los alamos neutron;lansce proton;ion injection los;proton storage;center lansce proton;alamos neutron scattering;conversion ion source;kev hydrogen beams;conversion ion;ion injection;laboratory lanl upgraded;alamos national laboratory;surface conversion ion;ion source;storage ring psr;80 kev hydrogen;ring psr improved;hydrogen beams enable;kev hydrogen;neutron scattering center;hydrogen beams;proton;lanl upgraded provide;injection los alamos;neutron;national laboratory lanl;operation los alamos", "pdf_keywords": ""}, "073798fde720d5f08dccfbb0c1917a064828c399": {"ta_keywords": "thoracic infection;thoracic infection thoracic;infection thoracic infection;history thoracic infection;infection thoracic;patient history thoracic;thoracic;infection;report case patient;history thoracic;case patient;case patient history;report case;patient history;patient;report;case;history", "pdf_keywords": ""}, "81f5ef41dfa72679cb7cb38999a41a1c534c3871": {"ta_keywords": "patient history history;patient history;case patient history;history history;history history history;history;present case patient;case patient;article present case;patient;purpose article present;article present;purpose article;article;present case;present;case;purpose", "pdf_keywords": ""}, "8d69f466bdf56ce6663c2f809514577e79dd3bed": {"ta_keywords": "recognition interface;wearable device;wearable device comprises;gesture controlled environment;purposethe wearable device;projector gesture controlled;recognition interface trained;projector gesture;gesture recognition;like gesture recognition;camera projector gesture;gesture recognition color;smart tools based;purposethe wearable;machine learning interfaced;smart tools;gesture controlled;speech recognition interface;access lab controls;interface trained using;wearable;devices present lab;lab controls remotely;interface trained;having smart tools;learning interfaced oot;access lab;based lab access;recognition color marker;oot based lab", "pdf_keywords": "gesture classifier computing;gesture classifier;utilised gesture classifier;gesture controlled environment;gestures images using;gesture recognition using;gesture recognition;hand gesture recognition;gestures images;having gestures images;integration gesture recognition;using gesture;gestures;gesture recognition color;gesture controlled;hand gesture;interface gesture controlled;static hand gesture;interface gesture;internet things iot;controlled using gesture;interface utilised gesture;various interface gesture;environment gesture;having gestures;iot;gesture;iot new;using gesture colour;environment gesture colour"}, "79c6713c41b4fedf9c7454b7e2bb48d0aeb1ae0f": {"ta_keywords": "sample designs arbitrary;filling sample designs;randomness sample designs;sample designs;sample designs methodsfirst;space filling sample;designs arbitrary dimensions;quality space filling;designs arbitrary;quantify space filling;quality space;technique quantify space;high quality space;designs methodsfirst propose;measure design performance;designs methodsfirst;designs;randomness sample;uniformity randomness sample;measure design;space filling;filling sample;design performance defined;objective measure design;design performance;quantify space;space filling property;sample;spatial domain objective;design", "pdf_keywords": "sample designs arbitrary;design sampling;optimal sampling design;design optimization sampling;spectral design sampling;sampling arbitrary dimensions;sample designs characterized;randomness sample designs;design sampling region;sample designs spectral;space filling sampling;sampling design;sample design sufficient;design stochastic sampling;sample designs propose;sample design strongly;filling sample designs;quality sample designs;sample design feasible;optimal sampling pattern;optimal sampling;sampling design difficult;sampling spatial;sample design method;distributions optimal sampling;sample designs;sample design propose;sample design significantly;optimization sampling;sampling pattern spatial"}, "2821db8962fce43265215a9c4b8d66af02e16ae7": {"ta_keywords": "scheduling redundant requests;schedule redundant requests;scheduling redundant;effectiveness scheduling redundant;redundant requests distributed;redundant requests assumption;redundant requests achieve;schedule redundant;redundant requests;scheduling policy overhead;requests distributed computing;optimal scheduling;hold schedule redundant;actual optimal scheduling;optimal scheduling policy;significantly optimal scheduling;job latency cancellation;scheduling policy;effectiveness scheduling;scheduling;requests distributed;scheduling policy differs;job latency;distributed computing data;cancellation delay considered;distributed computing;latency cancellation;requests achieve optimal;cancellation overhead;latency cancellation delay", "pdf_keywords": ""}, "94245856c88e3e08777c876fc038ed1adf8f3285": {"ta_keywords": "description based method;described based algorithm;based algorithm method;algorithm method used;analyze data method;based method used;based algorithm;method used analyze;data method used;algorithm method;method used described;description based;data method;based method;classical description based;method used;used described based;algorithm;analyze data;used analyze data;described based;used analyze;method;used described;analyze;classical description;description;described;data;based", "pdf_keywords": ""}, "9ef4f6a070c750b746fe98ef34083d6a08c9ba42": {"ta_keywords": "quadratic dynamic games;pricing linear quadratic;quadratic game dynamically;linear quadratic game;control strategy selfish;strategy selfish agents;quadratic game;pricing mechanisms;game choosing quadratic;selfish agents methodswe;use pricing mechanisms;selfish agents;coupled nash followers;quadratic dependence control;pricing linear;introduction pricing linear;dynamically coupled nash;leader influences game;pricing mechanisms means;strategy selfish;control strategy;linear quadratic dynamic;coupled nash;quadratic dynamic;hierarchical linear quadratic;game dynamically coupled;feedback control strategy;use pricing;introduction pricing;pricing", "pdf_keywords": ""}, "ff86133b3b49974f06fc881548c6f3c7a8ceffee": {"ta_keywords": "perceived age singing;age singer perceived;singing voice age;age singing voice;age singing;voice age;singer perceived listener;voice age singer;singer perceived;introductionthe perceived age;discussionthe perceived age;perceived age;perceptions song singers;age singer;singing voice;singers sing expressively;sing expressively;song singers sing;singers sing;voice;singing;voice timbre singers;timbre varieties voice;voice timbre;voice timbre varieties;varieties voice timbre;sing;prosody voice;singers produce;sing expressively controlling", "pdf_keywords": ""}, "43d82bc8203c09edc7eb6b2bedcf4ab500690852": {"ta_keywords": "crosslingual zero shot;lingual adjustment contextual;cross lingual adjustment;zero shot transfer;pre trained multilingual;effective crosslingual zero;trained multilingual;lingual adjustment;trained multilingual models;crosslingual zero;enabled effective crosslingual;repositionations zero shot;effective crosslingual;impact cross lingual;word repositionations zero;cross lingual;shot transfer;adjustment contextual word;representations zero shot;shot transfer methodslarge;shot transfer objectiveto;adjustment contextual;word representations zero;shot transfer nonlamulinous;contextual word repositionations;multilingual models;zero shot;crosslingual;lingual;multilingual", "pdf_keywords": "trained multilingual models;transfer multilingual models;lingual transfer multilingual;transfer english models;trained multilingual;transfer multilingual;tasks cross lingual;multilingual language models;multilingual translations;pre trained multilingual;cross lingual learning;task cross lingual;multilingual models feasible;adjustment multilingual models;multilingual translations results;multilingual models;non language tasks;multilingual models useful;transfer non linguistic;multilingual cross lingual;cross lingual embeddings;model multilingual cross;lingual adjustment multilingual;lingual transfer capabilities;linguistic tasks cross;language tasks;applied multilingual translations;models translate translations;multilingual model multilingual;model based crosslingual"}, "b57da3ccf214e8dad49116c8db9590c2c89629f5": {"ta_keywords": "entity recognition africa;named entity recognition;recognition africa languages;african continent nonprogramming;entity recognition tasks;entity recognition;africa languages;africa languages characteristics;continent nonprogramming research;addressing representation african;recognition africa;continent nonprogramming;named entity;representation african continent;dataset named entity;languages help researchers;nonprogramming research bringing;nonprogramming research;african continent;representation african;datasets conduct extensive;africa;addressing representation;african;nonprogramming;languages characteristics;quality dataset named;dataset named;languages;entity", "pdf_keywords": "annotators corpus african;african languages annotated;entities african languages;african languages ner;dataset african languages;recognition african languages;corpus african languages;language annotators;machine translation african;languages annotated data;named entities african;african languages evaluate;languages ner task;language annotators develop;annotators develop ner;corpora language annotators;languages annotated;representation african languages;models african languages;african languages provide;ner dataset african;translation african languages;african languages work;african languages;named entities languages;ken african languages;languages ner;ner generalize languages;languages curatthe ner;annotators corpus"}, "05c8f15dbdd7c6661b9176638262bbc1e11de85f": {"ta_keywords": "word sense embeddings;learns sense embeddings;sense embeddings learn;sense embeddings using;sense embeddings;interpretable sense embeddings;learning word sense;sense embeddings represent;introductioninducing embedding senses;embedding senses;embedding senses common;learns sense;sense specific vectors;word sense;word multiple sense;multiple sense specific;model learns sense;embeddings learn;produce interpretable sense;multiple sense;learning word;introductioninducing embedding;methods learning word;embeddings using;sense specific;senses;interpretable sense;embeddings;embeddings learn select;sense use", "pdf_keywords": "sense word embeddings;processing word senses;resentations word senses;word sense selection;word sense evaluation;sense embeddings models;individual word senses;terms sense embeddings;sense embeddings;existing sense representations;sense representations;word senses;sense embeddings consistency;sense specific embeddings;word representation model;identify senses contexts;learning distinguishable senses;word senses apply;senses contexts;sense representations uniformly;models sense selection;word senses goal;senses contexts multi;learning word;prototype word representation;individual word sensewe;word sense;word embeddings;sense evaluation;text representations sense"}, "58737fba500075136ee0f33f7801a5ac7f82ab68": {"ta_keywords": "implementation lucene;b25 likely refer;refer implementation lucene;lucene open source;lucene;implementation lucene open;researchers speak b25;lucene open;b25 entirely;open source search;b25 entirely clear;b25;b25 likely;source search library;search library;search library does;speak b25 entirely;source search;practitioners speak b25;speak b25;library does ambiguity;speak b25 likely;search;likely refer implementation;does ambiguity matter;does ambiguity;ambiguity matter methods;ambiguity matter;ambiguity;original formulation proposed", "pdf_keywords": ""}, "14919b6a453a71f2a007d5fa57241887a982575f": {"ta_keywords": "learn description logic;model learn description;description logics;description logic fundamental;description logic;learn description;description logics problem;field description logics;ability model learn;model learn;logic fundamental;logic fundamental issue;logics problem understood;field description;ability model;logics;logic;description;logics problem;issue field description;model;understood fully understood;fully understood;understood;learn;understood fully;problem understood;field;fundamental issue field;ability", "pdf_keywords": ""}, "fb089347919e8dada9335b4bac01f16eea758c56": {"ta_keywords": "ethics artificial intelligenceligence;teaching ethics artificial;ethics artificial;ethical questions artificial;ai researchers engage;stopss teaching ethics;teaching ethics;science key ethical;ai researchers;enables ai researchers;machine stopss teaching;intelligenceligence computer science;artificial intelligenceligence;ethical;questions artificial intelligence;intelligence computer science;technologies teach;artificial intelligence computer;artificial intelligenceligence computer;tool enables ai;ethics;enables ai;ethical questions;intelligence computer;ai;use science fiction;key ethical questions;technologies teach past;intelligenceligence computer;artificial intelligence", "pdf_keywords": ""}, "e2c05b3abf77900ec82ffa8a95aa774308d2780f": {"ta_keywords": "code switching twitter;code switching text;code switched text;code switching;language detecting;level language detecting;level language detection;estimation code switching;code switched;language detection;language detecting technique;switching twitter;switching text;effect code switching;language detection technique;switched text;switched text arbitrarily;switching twitter novel;switching text input;technique code switched;word level language;context estimation code;twitter;languages;detection technique code;language;level language;code;twitter novel generalized;unsupervised word level", "pdf_keywords": ""}, "719916251f7e36d2e7a40e70f89f20ab97a8bc29": {"ta_keywords": "channel speech enhancement;speech enhancement applications;speech enhancement;various speech enhancement;speech enhancement severely;spectral mask estimation;mask based beamformer;introduction spectral mask;speech signal make;single channel speech;multichannel enhancement techniques;enhancement techniques mask;based beamformer masks;spectral mask;applied multichannel enhancement;multichannel enhancement;beamformer masks;mask estimation;mask estimation using;speech recognition;channel speech;speech signal;beamformer masks used;unsuitable speech recognition;speech recognition methods;techniques mask based;distort speech signal;techniques mask;bls neural networks;mask based", "pdf_keywords": "channel speech enhancement;multichannel speech enhancement;estimating speech masks;spectral mask estimation;ability multichannel speech;communication speech masks;speech enhancement proposed;multichannel speech;various speech enhancement;speech enhancement;speech enhancement applications;speech enhancement using;speech masks;single multichannel speech;entropy loss mask;features improved speech;mask prediction method;single channel speech;improved speech performance;speech masks trivial;speech enhancement enhance;mask prediction;introduction spectral mask;channel speech;speech signal make;loss mask based;improved speech recognition;technique improved speech;enhancement enhance speech;spectral mask"}, "0909fee90833e20913adb553bf6667c9a3b854b0": {"ta_keywords": "learning website wrappers;learning wrapping tables;website wrappers examples;flexible learning wrapping;wrapper wrapper learning;website wrappers;tables lists html;lists html documents;learning wrapping;html documents;lists html;html documents program;wrapper learning;wrapping tables;html;wrapping tables lists;wrapper learning called;flexible learning;learning website;present wrapper learning;wrappers examples;wrapper learning problem;wrapper wrapper;wrappers examples present;introductiona flexible learning;examples present wrapper;database called wrapper;look like database;tables lists;tables", "pdf_keywords": ""}, "4a45ace1f8c6a30ba00201b30acd93844b9797eb": {"ta_keywords": "role histidine trypsin;histidine trypsin provide;histidine trypsin;overview role histidine;role histidine;histidine;trypsin provide overview;trypsin;trypsin provide;study provide overview;provide overview role;overview role;aim study provide;study provide;provide overview;study;aim study;role;overview;aim;provide", "pdf_keywords": ""}, "dd3ba828dbbb17cf478f6840a37954f6ebc81770": {"ta_keywords": "understand speaker intentions;speaker intentions accurately;intentions accurately dialog;speaker intentions;bidirectional attention lsms;bidirectional attention;using bidirectional attention;role conversation;lsms understand speaker;role conversation agent;understand speaker;attention lsms understand;accurately dialog;accurately dialog important;spotken language understanding;different role conversation;language understanding;language understanding using;attention lsms;dialog important consider;dialog;conversation;conversation agent;dialog important;surrounding sequence dialog;sequence dialog;conversation agent versus;sequence dialog turns;backgroundcontext sensitive role;spotken language", "pdf_keywords": ""}, "81e57827bebb04305cc9e6c85e96c83951244ec2": {"ta_keywords": "adverse effects polypharmacy;use drug polypharmacy;polypharmacy use drug;drug polypharmacy;effects polypharmacy;drug polypharmacy currently;polypharmacy effects occur;polypharmacy effects;polypharmacy use;effects polypharmacy effects;polypharmacy currently early;polypharmacy;polypharmacy currently;background polypharmacy use;interactions combined drugs;use drug combinations;drug combinations commonly;combined drugs cause;mortalities use drug;drug combinations;adverse effects;drugs cause severe;combined drugs;risks adverse effects;background polypharmacy;use drug;risks adverse;high risks adverse;drugs cause;drug", "pdf_keywords": "knowledge graph embeddings;knowledge graph embedding;link prediction knowledge;prediction knowledge graphs;knowledge graph;model knowledge graph;data knowledge graph;supporting knowledge graph;knowledge graphs;relations knowledge graph;knowledge graphs present;learning link prediction;developed knowledge graph;present knowledge graph;knowledge graphs use;based knowledge graph;knowledge graph technique;link prediction;graph embedding models;knowledge graph perform;predicting drug interactions;knowledge graph applied;graph embedding model;developments knowledge graph;graphs present knowledge;predicting drug;problem link prediction;effects link prediction;useful prediction drug;applied knowledge graph"}, "e98621050e52e9d8c60829d8d861e81ac86a8617": {"ta_keywords": "etiology disease;mechanisms disease caused;evolution disease discuss;evolution disease;understood evolution disease;disease caused disease;possible mechanisms disease;mechanisms disease;disease caused;etiology disease poorly;caused disease;disease;etiology;disease poorly understood;disease discuss;disease discuss possible;disease poorly;evolution;discuss possible mechanisms;possible mechanisms;poorly understood evolution;understood evolution;mechanisms;caused;discuss possible;discuss;poorly understood;possible;understood;poorly", "pdf_keywords": ""}, "e2b097bce656db9215505659357263c43190194b": {"ta_keywords": "reviewers divided phases;papers assigned reviewers;assigned reviewers;paper review process;reviewers divided;assigned reviewers provide;assigned additional reviewers;review process papers;reviews submitted conferences;paper review;question reviewers divided;phase paper review;review process;additional reviewers;additional reviewers initial;reviewers;conferences employ phase;reviewers initial reviews;scientific conferences;process papers assigned;reviewers provide reviews;consider question reviewers;reviewers initial;total assignment similarity;papers assigned;reviewers provide;question reviewers;assignment similarity;reviews submitted;process papers", "pdf_keywords": "assign reviewers papers;similarity assignment performance;assigned paper reviewer;conference random reviewer;assignment reviewers;reviewers stage assignment;construction random reviewer;high similarity assignment;setting assignment reviewers;reviewer paper pairs;total assignment similarity;papers assigned reviewers;papers optimal assignment;assignment reviewers stages;assignment similarity;assignment similarity results;randomly splitting reviewers;assign reviewers;similarity assignment;reviewers random papers;paper assignment instances;similarity assignment present;assignment similarity used;datasets random reviewer;assigned reviewers;similarity construction random;randomly reviewer split;reviewer split robust;papers reviewers random;evaluations random assignments"}, "c0c1950fb0a129b71a218ffa8b9fbc6d088cba2d": {"ta_keywords": "computer technology higher;computer science higher;technology higher education;innovative approaches computer;approaches computer science;approaches computer technology;technology higher;approaches computer;computer science;computer technology;innocce 2017 innovative;higher education;higher education proceedings;2017 innovative;science higher education;2017 innovative approaches;technology;innovative;workshop innovative approaches;science higher;workshop innovative;innovative approaches;innocce 2017;international workshop innovative;education proceedings;education proceedings second;education;second international workshop;approaches;computer", "pdf_keywords": ""}, "27f9b91bd7c70a99f578c8a5cb52d37e4123da47": {"ta_keywords": "flattened activation vectors;activation tensors fully;activation tensors;layers convolutional layers;convolutional layers;flattened activation;layers convolutional;convolutional layers followed;connected layers convolutional;order activation tensors;multilinear structure activations;activation vectors;consist convolutional layers;activation vectors despite;tensors fully;convolutional layers map;layers operate flattened;operate flattened activation;convolutional neural networks;layers discards multilinear;fully connected layers;convolutional neural;tensors fully connected;layers;tensors;typically consist convolutional;high order activation;connected layers;structure activations;layers followed fully", "pdf_keywords": "neural network tensors;tensors neural network;network tensors;tensor regression network;tensor structure network;tensors neural;tensor regression layer;network tensors decomposed;layer incorporate tensor;decompositions tensors neural;tensor based architecture;tensor structure activations;tensor contractions neural;incorporating tensor contractions;tensor regressions trainable;incorporating tensor;incorporate tensor contractions;tensor regressions;preserving tensor structure;networks apply tensor;tensors;activation tensor;performing tensor;tensor based;dimensionality activation tensor;incorporate tensor;tensor decompositions;introduce tensor regressions;performing tensor regression;tensor structure"}, "b09d49c3eacd93782a32ad16ab52f98a21ecc206": {"ta_keywords": "etiology etiology understood;etiology etiology;etiology;etiology understood;development etiology etiology;etiology understood report;development etiology;model development etiology;development;development new model;model development;new model development;development new;understood report development;new;model;understood report;new model;report development new;report development;understood;report", "pdf_keywords": ""}, "1bf49ef0b33bf8fcc3ebdd16326db419f3af65d8": {"ta_keywords": "nearest neighbor losss;soft nearest neighbor;representations soft nearest;nearest neighbor loss;neighbor loss methods;neighbor losss measure;nearest neighbor;textitsoft nearest neighbor;textitentanglement class manifolds;soft nearest;class manifolds representation;improving representations soft;class relative pairs;class manifolds;representations soft;losss measure textitentanglement;points class relative;class relative;loss methods explore;neighbor losss;textitsoft nearest;neighbor loss;manifolds representation space;losss measure;loss methods;manifolds representation;measure textitentanglement class;relative pairs;relative pairs points;nearest", "pdf_keywords": "models entanglement training;data entanglement training;representations minimizing entanglement;entanglement training objective;entanglement training;models entangled representations;classes minimizing entanglement;entangled class manifolds;entanglement class manifolds;entangled representations;representations learning soft;entanglement soft nearest;entangled models better;learned representations minimizing;improvingrepresentations soft nearest;entangled representations deal;improved entanglement loss;representations learning;estimates adversarial;learned representations;entangled class;neighbor loss neural;loss measure entanglement;robust counterparts entangled;generating models entanglement;learning soft nearest;deep nearest neighbors;model maximizing entanglement;confidence estimates adversarial;maximizing entanglement soft"}, "3a4f39dbb5e06a5fc55622315797da7a97cc76f6": {"ta_keywords": "word level quality;translation predicting words;word using neural;predicting words output;level quality estimation;quality estimation;machine generated translation;predicting words;generated translation predicting;quality estimation qe;source sentence machine;translation predicting;sentence machine generated;layer represent words;sentence machine;word level;effectively encode;words output;contextual information target;generated translation;level quality;effectively encode local;global contextual;target word using;neural network;global contextual information;encode local;quality;neural;contextual", "pdf_keywords": "words neural model;neural model gram;fuzzy fuzzy fuzzy;predicting words output;translation predicting words;fuzzy fuzzy;fuzzy fuzzy fuzzywe;word using neural;predicting words;algorithm fuzzy fuzzy;fuzzy fuzzywe report;prediction tool english;fuzzy fuzzywe;gram features languages;new algorithm fuzzy;model gram features;concatenated word embeddings;word embeddings;convolution concatenated word;language classification method;represent words neural;word level quality;gram features;algorithm fuzzy;words neural;translation predicting;layer represent words;quality prediction tool;language classification;machine generated translation"}, "b9a701c90f3d3df27366f5b29a97f798eb940ac7": {"ta_keywords": "range language models;discourse level language;long segment narrative;long range language;language models;level language understanding;language models lms;language understanding capabilities;discourse level;segment narrative ends;language understanding;narrative ends;narrative ends chapter;level language;segment narrative;evaluation discourse level;chapaterbreak challenge dataset;range language;discourse;introduce chapaterbreak challenge;end introduce chapaterbreak;language;chapaterbreak challenge;lm long;narrative;lm long segment;introduce chapaterbreak;meaningful evaluation discourse;challenge dataset provides;provides lm long", "pdf_keywords": "language sequences long;range language models;language sequences;narrative sequences long;language models;long range language;issue language sequences;language models dataset;context narrative sequences;language models lls;context sequence tasks;narrative sequences;discourselevel understanding long;disambiguate sequences;difficult disambiguate sequences;disambiguate sequences sequences;particularly challenging disambiguate;interpret sequences long;narrative sequences ofthe;range language;language ll described;significantly difficult disambiguate;long range context;sequence tasks;language used model;long range dependencies;challenging disambiguate;sequence tasks requiring;improved sequence length;ability interpret sequences"}, "2559417f8a3d6ab922cfa824b43f9f0c642a1dae": {"ta_keywords": "named entity extraction;named entity recognition;entity recognition ner;dictionaries named entity;improving named entity;purposeexploiting dictionaries named;entity recognition;external dictionaries specifically;entity extraction;entities external dictionary;external dictionaries;using external dictionaries;entity extraction combining;purposeexploiting dictionaries;similarity extracted entities;named entity;external dictionary;extracted entities entities;dictionaries named;extracted entities;improving named;markov extraction processes;external dictionary resultsthis;dictionaries specifically;semi markov extraction;dictionaries;markov extraction;information similarity extracted;extraction processes data;recognition ner systems", "pdf_keywords": ""}, "3ad3ba8d7fc793a19dfe6a87e32453449195c074": {"ta_keywords": "text speech autoencoders;speech text autoencoders;speech autoencoders;speech autoencoders methods;speech recognition ar;end speech recognition;speech text training;decoders automatic speech;text autoencoders;text autoencoders leverage;automatic speech;datasets build speech;automatic speech recognition;text speech;autoencoders;large speech text;build speech text;using text speech;text autoencoders share;speech recognition;speech text;semi supervised end;autoencoders methods introduce;autoencoders leverage;autoencoders methods;text training;introduction semi supervised;semi supervised;autoencoders leverage state;autoencoders share encoders", "pdf_keywords": ""}, "765bdcf27ebc1eb03a14f1e47aefa4dda1e03073": {"ta_keywords": "noisy texts model;word representations robust;training noisy texts;learn representations robust;representations robust training;moderately noisy texts;noisy texts;character convolutional neural;representations robust;texts model;noisy texts humans;character convolutional;invariant word representations;robust training noisy;based character convolutional;word representations;translate moderately noisy;robust training;texts model based;learn representations;representations robust multiple;texts humans trouble;model based character;clinical messagewe;simultaneously learn representations;clinical messagewe state;humans trouble comprehended;texts;texts humans;key clinical messagewe", "pdf_keywords": "neural machine translation;translation models extremely;machine translation nm;machine translation models;surprisingly robust language;translation models;translation model learn;machine translation model;translation nm systems;machine translation ms;robust language processing;language synthetic noise;systems degrade translating;machine translation field;translate moderately noisy;machine translation;translation aforementioned systems;robust language;translation model;moderately noisy texts;language synthetic;noisy texts;degrade translating german;overcome typos misspellings;humans good translating;machine translation use;generating robustness neural;language accuracy natural;german words modified;errors language synthetic"}, "1263e36598dd95cc4becf0e18398f832bb5cf337": {"ta_keywords": "divergence morphological inflection;morphological inflection;morphological inflection methods;inflection methods attention;typoologically divergence morphological;hallucination language vector;divergence morphological;attention based encoder;explore neural architectures;morphological;neural;neural architectures;lstms transformers;experimentd hallucination language;language vector injection;neural architectures techniques;hallucination language;explore neural;lstms;using lstms transformers;inflection methods;lstms transformers base;inflection;encoder;techniques typoologically divergence;language vector;typoologically divergence;based encoder;methods attention based;attention based", "pdf_keywords": ""}, "978582ad754eab481856d62bdc7b0ee5bcf21811": {"ta_keywords": "iterative federated clustering;federated clustering algorithm;federated clustering;unlabeled datasets federated;datasets federated;clustering unlabeled datasets;datasets federated environment;clustering unlabeled;iterative federated;proposed iterative federated;federated;models fedavg algorithm;problem clustering unlabeled;federated environment statistical;clustering algorithm ifca;federated environment;clustering;fedavg algorithm highly;unlabeled datasets;training models fedavg;algorithm highly heterogeneous;clustering algorithm;fedavg algorithm;consider problem clustering;highly heterogeneous setting;problem clustering;heterogeneity exist clients;centralized setting model;heterogeneous setting;training models", "pdf_keywords": ""}, "675098c4611b13920d163a9a9b972da7751460cb": {"ta_keywords": "learning spoken;introductionefefficient learning spoken;training spoken language;learning spoken language;spoken dialog systems;spoken dialog;spoken language understanding;components spoken dialog;language understanding tasks;tasks word embedding;dialog systems;language understanding slus;spoken language;training spoken;word embedding based;pre training spoken;word embedding;language understanding;dialog;components spoken;essential components spoken;dialog systems recent;embedding based pre;tasks goal estimation;slus annotation collected;slus annotation;slus tasks goal;difficulty slus annotation;understanding slus tasks;slus tasks", "pdf_keywords": ""}, "dd64013273eb4398821bf2fc8f024735466e5a1d": {"ta_keywords": "intelligent agents simulate;simulate human learning;constructing learning agent;develop intelligent agents;learning agent;learning agent currently;agents simulate human;agents simulate;intelligent agents;human learning math;human level intelligence;simulate human level;artificial intelligence understand;level intelligence fundamental;simulate human;understand develop intelligent;human learning;develop intelligent;improving understanding humans;artificial intelligence;goals artificial intelligence;intelligence fundamental goal;humans acquire knowledge;level intelligence;intelligence fundamental;goals artificial;education improving understanding;constructing learning;learn methods;fundamental goals artificial", "pdf_keywords": ""}, "1abbe9b6bf3f134ce86e618bba83bf5c94f60f03": {"ta_keywords": "joint design problem;joint design;agent modeled parametric;parame agent modeled;distribution agent modeled;single parame agent;parameter known distribution;known distribution agent;agent modeled;design problem specifically;desires predict parameter;form joint design;modeled parametric fashion;modeled parametric;parame agent;distribution agent;predict parameter known;work quality expertise;predict parameter;parameter known;parametric fashion work;design problem;quality expertise governed;formulate optimally;modeled;governed single para;quality expertise;design;expertise governed single;governed single parame", "pdf_keywords": "prediction parametric agents;incentive mechanism prediction;incentives crowdsourcing;agent parametric prediction;crowdsourcing order elicit;agent based prediction;crowdsourcing task;crowdsourcing analysis incentives;analysis incentives crowdsourcing;prediction propose mechanism;crowdsourcing task probability;mobile crowdsourcing order;crowdsourcing order;prediction agent;agents participate crowdsourcing;agents opinions incentivize;agent optimal;prediction minimal cost;prediction agent general;prediction process;opinions incentivize agents;information participating agents;eliciting agents opinions;mobile crowdsourcing;parametric prediction market;agent agent optimal;prediction based opinions;prediction decision;prediction market;prediction algorithm case"}, "a2ea2261bd56ae2505750d7571b501d9836175f0": {"ta_keywords": "discriminative training acoustic;acoustic models combination;combination methods discriminative;training acoustic models;speech recognition;automatic speech recognition;automatic speech;dis discriminative training;discriminative training;discriminative training methods;methods discriminative training;performance automatic speech;speech recognition reference;methods discriminative;models combination methods;acoustic models;introduction dis discriminative;models combination;discriminative;training acoustic;dis discriminative;combination methods;multiple systems;recognition;recognition reference;generate complementary hypotheses;combination methods output;systems;speech;acoustic", "pdf_keywords": ""}, "39456ca31a530d85ec182b2676dc94266dada597": {"ta_keywords": "compositional distributional semantics;compositional distributional semantic;distributional semantic;distributional semantic methods;distributional semantics;distributional semantics methods;tensor approximations compositional;explores compositional distributional;composing word vectors;compositional distributional;approximations compositional distributional;representing meaning composing;results compositional distributional;words feature vectors;word vectors;word vectors produce;semantics methods mapping;semantic;tensor approximations;semantics methods;sentences results compositional;meaning composing word;semantic methods;vectors representing meaning;mapping words feature;mapping words;distributional;semantics methods thesis;rank tensor approximations;semantics", "pdf_keywords": ""}, "bc8e67d532693818eb33aa8e401260fe2b774a18": {"ta_keywords": "use wrapper induction;wrapper induction complex;wrapper induction;induction complex documents;tabular data web;use wrapper;complex documents;useful tabular data;complex data useful;complex documents important;useful tabular;data web;data useful tabular;wrapper;complex data;tabular data;tabular;development complex data;induction complex;induction;data useful;documents;documents important development;documents important;data;web;useful;complex;important development complex;development complex", "pdf_keywords": ""}, "29dc4b10e60ed1e2b84598cc6f2622c786841fdf": {"ta_keywords": "temporal logic specifications;planning specifications robotic;temporal logic particular;linear temporal logic;robots subject temporal;control mobile robots;temporal logic;temporal logic encompasses;temporaryral logic specifications;robots utilizing barrier;motion planning specifications;functions temporaryral logic;subject temporal logic;temporaryral logic;specifications robotic;introductioncontrol mobile robots;mobile robots subject;mobile robots;motion planning;planning specifications;barrier functions temporaryral;logic specifications using;logic specifications presented;logic specifications;using linear temporal;specifications using barrier;mobile robots utilizing;specifications robotic res;robots subject;linear temporal", "pdf_keywords": "planning specifications robotic;specifications robotic control;temporal logic specifications;robotic control barrier;functions temporal logic;motion planning specifications;temporal logic tools;functions temporal logicwe;temporal logic proposed;control using temporal;linear temporal logic;temporal logic particular;specifications robotic;using temporal logic;control mobile robots;robots utilizing barrier;trajectory satisfies specification;control barrier functions;temporal logic;control barrier functionswe;integration temporal logic;planning specifications;synthesis complex robotic;temporal logicwe;function based control;barrier functions temporal;controller robot robots;controller robot;robots subject temporal;robotic control affine"}, "3f0f6c19c6f5d4e4d5066984c5f3e922a2c2ff85": {"ta_keywords": "exploration learned language;language abtraction reward;learned language abtraction;agents reinforcement learning;agents reinforcement;agents capable understanding;learned language;following agents reinforcement;instruction following agents;human ai;reinforcement learning environments;human ai collaboration;tasks learning policies;synthetic language instructions;learning policies;introductionbuilding agents capable;reinforcement;reinforcement learning;exploration learned;introductionbuilding agents;sparsereward tasks learning;ai collaboration;abtraction reward shaping;ai collaboration recent;capable understanding language;reward shaping;learning policies requires;ella exploration learned;ai;reward shaping approach", "pdf_keywords": "language reinforcement learning;language reinforcement;exploration learned language;natural language reinforcement;language context reinforcement;optimizing sparse reward;tasks learning policies;training agents reinforcement;sparse reward tasks;agents reinforcement learning;reward tasks learning;agents learn language;learned language abstractions;effective sparse reward;language conditioned policies;context reinforcement learning;sparse reward task;level language learns;learning policies;language abtraction reward;context reinforcement;language learns;language commands simulated;reinforcement learning rl;mapping learned policy;sparse reward;sparse reward settings;agents reinforcement;level task learn;exploration using learned"}, "81ea04f822a1d5317e5846783900ac424a8f7528": {"ta_keywords": "introduction autism spectrum;autism spectrum;children autism spectrum;speech features;autism spectrum disorders;language speech features;speech features single;linguistic acoustic features;differences children autism;introduction autism;children autism;autism;disorders terms linguistic;linguistic acoustic;features single utter;non verbal communication;acoustic features mention;language speech;verbal communication;developmental disorders characterised;speech;study language speech;terms linguistic acoustic;disorders developmental disorders;acoustic features;skills affect verbal;verbal communication previous;developmental disorders;verbal non verbal;deficits social communication", "pdf_keywords": ""}, "98fdc7e1e167eb465cdb1c8ee0800db750101155": {"ta_keywords": "statistical voice conversion;conversion statistical voice;utterance spectral distance;voice conversion;spectral distance utterances;spectral conversion statistical;utterance spectral;utterance utterance spectral;statistical voice;voice conversion vc;distance utterances;distance utterances exists;spectral conversion;introductionin spectral conversion;converted target spectral;criteria speaker utters;utters sentence spectral;training criteria speaker;conversion statistical;parameters vary utterance;sentence spectral parameters;spectral distance;criteria speaker;conversion vc distance;vary utterance utterance;speaker utters;voice;utterances exists methods;utterance utterance;target spectral parameters", "pdf_keywords": ""}, "ae25ca24eb6c2772ef88e2d0315fc428feb8553e": {"ta_keywords": "unsupervised information extraction;information extraction;unsupervised information;columns present clustering;html tables;entities meaningful sets;propose unsupervised information;information extraction exploits;tables build meaningful;meaningful sets entities;html tables build;categories frequently occur;introductionwe propose unsupervised;frequently occurring entities;categories description;categories frequently;clustering;structured information;certain categories description;important categories frequently;category names;categories;frequently occur table;categories description redundancy;description redundancy web;tables;entities meaningful;web believe entities;occurring entities meaningful;present clustering", "pdf_keywords": ""}, "689ab475e8a0f552bf6e39a2f774d9d20e50b9cb": {"ta_keywords": "new management disease;disease;disease poorly understood;management disease;etiology disease;etiology disease poorly;disease poorly;etiology;development new management;new management;management;report development new;understood report development;poorly understood report;report development;understood report;development new;development;report;poorly understood;new;understood;poorly", "pdf_keywords": ""}, "38a73e6f48d057cb58264f5148f8b05522d0d030": {"ta_keywords": "semantic parsing task;semantic parsing;semantic parsing possible;introduction semantic parsing;progress semantic parsing;machineinterpretable meaning representation;nl utterances machineinterpretable;natural language nl;parsing;corpora progress semantic;parsing task translating;introduction semantic;utterances machineinterpretable meaning;parsing possible;utterances machineinterpretable;parsing task;translating natural language;language nl utterances;natural language;nl utterances;semantic;language nl;utterances;existing corpora;task translating natural;corpora corpora;progress semantic;corpora;corpora progress;corpora corpora progress", "pdf_keywords": ""}, "045f90129a8d7148eec4a58770bc4166b51330ca": {"ta_keywords": "parking demand;characteristics parking demand;parking demand resultsthis;congestion caused parking;parking performance;parking performance based;temporal characteristics parking;parking;characteristics parking;caused parking performance;park harnessing data;drivers looking park;pricing policies;incentives drivers looking;performance based pricing;set pricing policies;pricing policies target;park;based pricing;caused parking;pricing schemes;seattle department transportation;mitigate congestion;park harnessing;set pricing;incentives drivers;based pricing schemes;pricing schemes received;mitigate congestion caused;pricing", "pdf_keywords": ""}, "fd0aa185be4e1f1fe3975779aec179348ec19ea8": {"ta_keywords": "double blind conferences;online arxiv review;surveys reviewers;conducted surveys reviewers;blind conferences engaged;papers online arxiv;blind conferences;independently authors research;papers arxiv pros;conducted surveys;arxiv review process;papers arxiv;dilemma papers arxiv;specifically conducted surveys;research papers face;authors research;reviewers;conferences engaged debates;authors research papers;arxiv review;research papers;surveys;authors post papers;arxiv pros cons;debates allow authors;post papers online;debate dilemma quantitative;research;introduction double blind;online arxiv", "pdf_keywords": "blind reviewing publication;blind reviewing availability;blinded peer review;blind review documented;double blind reviewing;blind reviewing;blind reviewing work;double blind review;blind reviewing design;blind conferences engaged;blind review;blind conferences;surveys reviewers;available internet review;internet review;conducted surveys reviewers;backgrounddouble blind conferences;anonymization double blind;online review;anonymization double blinded;visibility publication preprints;correlation papers visibility;publication preprints significantly;reviewing publication rates;effectiveness author anonymization;reviewing publication;internet results research;publication paper internet;internet review process;visibility publication"}, "f837bf72e5b864e1c162e924fed59b778e946e23": {"ta_keywords": "visual guessing games;visual guessing;learn conceptual representations;conceptual representations objects;objects scene training;guessing games effective;players learn conceptual;modal representations relying;representations objects discriminative;conceptual representations;scene training inference;objects discriminative expressive;multi modal representations;guessing games;objects discriminative;representations objects;modal representations;backgroundin visual guessing;learn conceptual;scene training;discriminative expressive ask;training inference;players learn;discriminative expressive;ask questions guess;representations relying;strategy players learn;representations relying instead;models fail learn;objects scene", "pdf_keywords": ""}, "df4e3aa275b8f81e22a5332ab550805083094dae": {"ta_keywords": "neural generation translation;neural machine translation;machine translation nm;translation nm participants;generation translation;neural generation;machine translation;generation translation presented;workshop neural generation;translation nm;efficient neural machine;tasks efficient neural;workshop neural;efficient neural;findings workshop neural;language processing mnl2019;neural machine;tasked creating nm;natural language processing;neural;processing mnl2019 summarize;creating nm;methods natural language;language processing;translation presented;mnl2019 summarize;mnl2019 summarize research;natural language;translation;clinical messagethe", "pdf_keywords": "neural machine translation;translation generation;machine translation generation;machine translation jointly;machine translation;translation generation wgs;automated translation systems;translation jointly learning;translation systems;automated translation;text textual accuracy;textual accuracy measures;automated automated translation;trained news corpora;textual accuracy content;textual accuracy;translation systems increasing;text textal accuracy;natural language processing;textal accuracy measures;language task measured;workshop neural machine;language task;textal accuracy;text data generate;translation jointly;text textual;natural language;3rd workshop neural;workshop neural"}, "e5efd7e2087e58c5a8860398dfcf143aa9dc865e": {"ta_keywords": "backgroundsound event detection;sound event detection;supervised sound event;weakly supervised sound;backgroundsound event;sound event;supervised sound;event detection challenging;acoustic class inference;backgroundsound;event detection task;events event classification;recognition events providing;event classification;segmentation recognition events;event detection methodstask4;accurate event localization;objectiveto joint acoustic;acoustic class;recognition events;event detection;event classification methods;joint acoustic class;weakly supervised;event localization;acoustic;joint acoustic;inference weakly supervised;event localization presents;detection challenging task", "pdf_keywords": "sound event detection;acoustic event classification;acoustic event detection;backgroundsound event detection;event detection audio;supervised acoustic event;audio event detection;annotate sound events;detecting events sound;detect sound events;tracking auditory events;events sound samples;sound events based;inference audio event;sound events;detection supervised acoustic;classification annotate sound;sound event;sound events complex;detect sound event;audio event;sound events employ;approach acoustic event;acoustic scenes;events complex acoustic;supervised acoustic;acoustic driven event;detection sound clips;event boundaries sound;backgroundsound event"}, "0ae93646ad058853eb6424c1dc0ec1559414e5af": {"ta_keywords": "automatic rumour verification;models automatic rumour;automatic rumour;rumour verification estimates;rumour verification;rumours circulating online;resolve rumours circulating;resolve rumours;estimates natural language;correctly resolve rumours;rumours circulating;rumour;human fact checker;instance rejection supervised;fact checker propose;rejection supervised;fact checker;rumours;prioritised human fact;natural language processing;uncertainty based instance;natural language;predictions likely erroneous;model data uncertainty;language processing models;data uncertainty estimates;filter model predictions;based instance rejection;human fact;data uncertainty", "pdf_keywords": "models automatic rumour;automatic rumour verification;uncertainty rumour data;automatic rumour;predicting output conversation;conversations discussing rumours;rumour data;discussing rumours;rumour verification twitter;task rumour verification;rumour data pheme;conversation robust;rumour verification;conversation robust aleatoric;discussing rumours related;uncertainty rumour;conversation tree model;estimates natural language;epistemic uncertainty rumour;veracity detection;model predict epistemic;conversation tree;rumour verification resultswe;predict epistemic uncertainty;uncertainty model predictions;estimates task rumour;rumours circulating online;conversation conversation tree;news veracity detection;resolve rumours circulating"}, "0c12e4c611b32997f8be5811021ead80395a7e5c": {"ta_keywords": "neural communication developed;neural communication;neural interaction neurons;interaction neurons neurons;interaction neurons;neural interaction;model neural communication;neural communication recently;neurons model based;based neural interaction;neurons neurons model;process neural communication;neurons model;neurons neurons;neurons;neurons neurons fundamental;neurons fundamental process;neurons fundamental;neural;model based neural;developed based neural;based neural;model neural;communication;process neural;communication developed;interaction;fundamental process neural;communication developed based;new model neural", "pdf_keywords": ""}, "9af2264799bdc3490e4650e2f5d126762caf420f": {"ta_keywords": "end speech recognition;attention based encoder;transcribes speech text;speech recognition directly;recognition directly transcribes;directly transcribes speech;speech recognition;speech text;speech text predefined;transcribes speech;attention model;alignments approach attention;end end speech;directly transcribes;decoder framework learns;end speech;encoder;encoder decoder;transcribes;attention based;based encoder;learns mapping;encoder decoder framework;attention model shown;decoder;based encoder decoder;method attention model;input output sequences;learns mapping variable;text predefined alignments", "pdf_keywords": "end speech recognition;learning attention alignments;attention alignments;attention based encoderdecoder;connectionist temporal classification;attention encoder;attention decoder;attention encoder decoder;attention based end;cc attention models;decoder auxiliary task;attention models wj;network attention encoder;speech recognition directly;attention models;integrating cc attention;speech recognition;alignments approach attention;encoder trained attention;encoderdecoder framework learns;based attention decoder;processing speech data;attention model;temporal classification cc;processing speech;recognition directly transcribes;attention model novel;speech data;learning attention;encoder trained"}, "9d555ed29496850c4ef8a3facd7dce734c86aae7": {"ta_keywords": "predicting programminging comments;prediction programmer comments;programminging comments;programminging comments methodsa;models predicting programminging;predict comments;predicting programminging;programmer comments;programmer comments work;language models predicting;predicting code;comments work predict;programminging comments objectiveto;predict comments jajajajajajajajajajajajajajajaja;work predict comments;language models programming;comments methodsa statistical;task predicting code;statistical language model;prediction programmer;comments methodsa;natural language models;programming languages;language models;models programming languages;statistical language;predicting code ignoring;language model;programminging;evaluate natural language", "pdf_keywords": ""}, "7d9863258ef44ca8a6b87b68be738f7a83ac849a": {"ta_keywords": "end speech recognition;speech recognition neural;recognition neural beamforming;speech recognition;automatic speech recognition;speech recognition paradigm;neural beamforming;neural beamforming methods;end automatic speech;automatic speech;auditory acoustic phonetic;simplifying automatic speech;acoustic phonetic;end speech;end end speech;deep neural networks;beamforming methods end;integrating auditory acoustic;beamforming;beamforming methods;architecture multichannel end;deep neural;networks hidden markov;integrating auditory;architecture multichannel;multichannel end;unified architecture multichannel;paradigm integrating auditory;phonetic;multichannel end end", "pdf_keywords": ""}, "c6c18ad62f39060e2547a0b683525e83312d0700": {"ta_keywords": "incentives reviewers bidding;reviewers bidding;peerer review paper;reviewers bidding phase;bidding scheme peerer;bidding phase reviewers;incentives reviewers;peerer review;assignment paper reviews;review paper assignment;paper reviews;analysis incentives reviewers;demand paper;review paper;market inspired bidding;bidding;reviewers private costs;bidding scheme;demand paper goal;bidding scheme assignment;inspired bidding scheme;scheme peerer review;ininspired bidding scheme;market ininspired bidding;bidding phase;inspired bidding;provide analysis incentives;analysis incentives;ininspired bidding;paper assignment proposed", "pdf_keywords": "incentives reviewers bidding;simple paper bidding;algorithm bidding papers;paper bidding;incentives reviewers;reviewers based bids;paper bidding mechanism;assignment paper reviews;reviewers bidding;bidding mechanism reviewers;bidding papers;algorithm bidding;assignment reviewers based;assignment problem bidding;reviewers price paper;assigning budgets reviewers;bidding papers basis;paper assignment problem;research bidding papers;bidding papers fundamental;bidding papers cost;assignment reviewers;papers reach biddingthe;assigned papers;bidding scheme assignment;papers predetermined assigning;assignment paper;cost bidding equivalent;optimal bidding;bidding algorithm good"}, "be4d47a61fee83d332ca2f3fe097f19f63863d6c": {"ta_keywords": "clustering graphs;clustering graphs empirical;introductionnonde clustering graphs;methods node clustering;node clustering;analysis node clustering;node clustering popular;node clustering normalized;clustering popular methods;graph partition;clustering;spectral graph theory;graph partition optimization;clustering popular;clustering normalized cut;clustering normalized;graphs empirical study;graph theory;introductionnonde clustering;graph theory recently;graph analysis node;graph analysis;optimization spectral graph;graphs empirical;spectral graph;web social network;graphs;social network;modeling graph;partition optimization spectral", "pdf_keywords": ""}, "554eade16fb6040bbd21a72bacf903245d7458f1": {"ta_keywords": "ai embedding causal;ai objectivethis;capabilities ai;capabilities lacking ai;advance ai;ai;similar capabilities ai;ai objectivethis paper;slow ai objectivethis;ai draws inspiration;lacking ai instance;lacking ai;capabilities ai embedding;ai instance;slow ai;theories human decision;sense causal reasoning;causal reasoning;cognitive theories human;direction advance ai;human capabilities;ai embedding;background thinking fast;causal reasoning obtain;human decision making;background thinking;cognitive theories;human capabilities lacking;inspiration cognitive theories;ai instance adaptability", "pdf_keywords": "complex decision brain;decision brain complex;research brain complex;ai fundamental capabilities;brain complex;ai;ai fundamental;capabilities ai;reasoning fundamental ai;brain complex complex;similar capabilities ai;brain complex andthe;capabilities lacking ai;research brain;levelthe brain complex;humans ability solve;ai draws inspiration;decision brain;artificial intelligence;reach complex decision;advance ai;intelligence;ai agents;fundamental ai fundamental;ai embedding causal;decisions handles complex;subject research brain;fundamental ai;lacking ai;life brain complex"}, "b8f5f3c8816ab389c2f366fd8a45603550ea9667": {"ta_keywords": "biomedical text mining;automatic biomedical text;biomedical text;scientific literature biological;reliability biomedical text;text mining;genetic association database;literature biological biomedical;discovered automatic biomedical;reliable genetic association;literature biological;text mining answers;constructing reliable genetic;automatic biomedical;biomedical science research;database deep reinforcement;reliable genetic;knowledge discovered automatic;association database deep;biological biomedical;biological biomedical science;mining constructing reliable;genetic association;mining answers challenge;scientific literature;mining answers;increasing scientific literature;database deep;biomedical;discovered automatic", "pdf_keywords": ""}, "5e327c2285ddf2a76d08e5c00d16c7358bc5412c": {"ta_keywords": "evaluators expertise;evaluators expertise methodsin;evaluators submissions maximizes;evaluators expertise subject;assigned evaluators expertise;terms evaluators expertise;evaluators submissions;assignment evaluators submissions;evaluators;assignment evaluators;compromise assigned evaluators;strategy proofness resultswe;strategy proofness compromise;finding assignment evaluators;strategy proofness;price terms evaluators;assigned evaluators;maximizes assigned evaluators;terms evaluators;proofness resultswe analyze;price strategy proofness;introductionstrategiesingleer assessment partitioning;expertise methodsin;proofness compromise assigned;assessment partitioning;assessment;submissions maximizes;proofness compromise;constraint strategy proofness;expertise methodsin paper", "pdf_keywords": "strategyproof assignment agents;assignment strategyproof;viapartitioning assignment strategyproof;assignment strategyproof viapartitioning;algorithm strategyproof peer;assignment efficiently optimized;resulting assignment strategyproof;nonstrategyproof assignment efficiently;strategyproof partitioning assignment;optimal nonstrategyproof assignment;assignment algorithm powerful;assignment evaluators submissions;strategyproof peer assessment;strategyproof assignments assignment;constructing strategyproof assignments;finding assignment evaluators;assignment agents submissions;assignment efficiently;compromise assignment quality;assignment evaluators;subset peer evaluation;optimal assignment;strategyproofness partitioning;partitioning assignment algorithm;impose strategyproofness partitioning;peer evaluation;strategyproof assignment;assignment agents;optimal assignment conference;evaluators submissions maximizes"}, "f878a7c756b90c0ed612838492fbbc02ecaaab70": {"ta_keywords": "learns cognitive;learns cognitive skills;agent learns cognitive;learning agent;solving problems interleaved;learning agent learns;does solving problems;learning effectiveness previous;problem solving experience;learning effectiveness;solving problems;cognitive skills examples;agent learns;examples problem solving;cognitive skills;learns;machine learning agent;skills examples problem;learning;problems presented students;affects learning effectiveness;performance does solving;solving problems blocked;problem solving;solving experience;affects learning;skills examples;studies shown solving;shown solving problems;cognitive", "pdf_keywords": ""}, "1ee276db29ba9127e81d9a7d9cb08f5138339412": {"ta_keywords": "speeding distributed computing;distributed computing;distributed computing comes;distributed computing allows;enabling parallel computing;parallel computing;parallel computing massive;introduction distributed computing;large scale computation;computing comes stragglers;challenge speeding distributed;speeding distributed;coded matrix matr;stragglers crippling bottleneck;scale computation machine;novel coded matrix;matrix matr;coded matrix;enabling parallel;distributed;computing allows large;crippling bottleneck performance;computation machine learning;computing massive;stragglers;tasks enabling parallel;computation machine;bottleneck performance critical;bottleneck performance;bottleneck performance results", "pdf_keywords": ""}, "34cb1f081c1d1d6b3dc16a9278940a9ee85fb2e0": {"ta_keywords": "predict interpreter confidence;predicting simultaneous interpreter;interpreter confidence;simultaneous interpreter performance;qe machine translation;machine translation output;interpreter performance;interpreter confidence adequacy;predict interpreter;interpreter performance building;machine translation;methods predict interpreter;simultaneous interpreter;interpretation translation spoken;translation spoken word;simultaneous interpretation translation;translation output;computer assisted interpretation;interpreter;quality estimation qe;assisted interpretation interfaces;translation spoken;assisted interpretation;interpretation translation;quality estimation;word real time;spoken word real;estimation qe machine;methodology quality estimation;translation", "pdf_keywords": "assessing interpreter performance;assessing interpreter;quality interpreter performance;assess quality translation;evaluation assessing interpreter;predict interpreter confidence;evaluate quality translation;assessing quality interpreter;interpreter confidence;evaluate interpreter performance;interpreter performance developed;interpreter performance fundamental;interpreter performance introduce;quality interpreter;interpreter performance attractive;measure machine translation;interpreter performance;predicting simultaneous interpreter;simultaneous interpreter performance;simultaneous interpreting translation;interpreter performance using;translation simultaneous interpreting;interpreter performance building;effective tool interpreting;quality translation;machine translation;interpreter confidence adequacy;tool interpreting interpreting;methods predict interpreter;quality translation propose"}, "839cbcf5c13d5875e952e40ec2da14b19eee2202": {"ta_keywords": "smooth convex optimization;convex optimization problems;convex optimization;gradient available numerical;smooth convex;consider smooth convex;optimization problems gradient;gradient available;convex;available numerical solution;optimization problems;gradient;problems gradient available;optimization;available numerical;problems gradient;consider smooth;2011 consider smooth;numerical solution;numerical solution 2011;introduction consider smooth;numerical;smooth;solution 2011;solution 2011 consider;2011;problems;2011 consider;solution;available", "pdf_keywords": "malignancy diagnosed gynecological;diagnosed gynecological;patient diagnosed gynecological;diagnosed gynecological malignancy;gynecological malignancy diagnosed;malignancy gynecological;gynecological malignancy;gynecological malignancy patient;malignancy gynecological malignancy;gynecological malignancy gynecological;asymptomatic athe etiology;patient symptomatic asymptomatic;gynecological;patient treated gynecological;symptomatic asymptomatic asymptomatic;treated gynecological;asymptomatic asymptomatic asymptomatic;symptomatic asymptomatic;asymptomatic asymptomatic;asymptomatic asymptomatic athe;treated gynecological surgery;gynecological surgery;etiology etiology patient;malignancy diagnosed;asymptomatic asymptomatic aim;etiology patient etiology;asymptomatic;etiology patient;malignancy;malignancy patient diagnosed"}, "ebcb425ed1a51e8f1a6eca422882abd454fe04f2": {"ta_keywords": "assist el learners;learners english vocabulary;target vocabulary provides;provides lexical information;vocabulary provides;vocabulary provides lexical;english vocabulary addition;target vocabulary;identifies target vocabulary;provides lexical;vocabulary addition;lexical information collocations;lexical information;rich lexical information;el learners introduce;provide rich lexical;learners english;second language learners;grammar endcyclopedic information;language learners english;link grammar endcyclopedic;english vocabulary;information assist el;language learners;el learners;words present linggle;lexical information generates;grammar endcyclopedic;information collocations grammar;rich lexical", "pdf_keywords": "online dictionary extract;extract knowledge english;learning extract vocabulary;extract grammar patterns;providing rich lexical;extract vocabulary;extract vocabulary encyclopedic;rich lexical information;dictionary extract collocations;words automatically parsing;automatically parsing;extract grammar;grammar patterns collocations;automatically parsing sentences;extract collocations accompany;automatically generating grammar;corpus automatically generating;generating grammar patterns;grammar patterns collocationthe;collocations accompany grammar;language learning systems;corpus automatically;sentences corpus;lexical information target;dictionary extract;extract collocations;vocabulary encyclopedic information;knowledge english language;grammar patterns using;lexical information"}, "4bff8862ba7956fdc2288e8399fb187b9595982b": {"ta_keywords": "gene mention task;mention task biocreative;human annotated corpus;description biocreative task;annotated corpus performance;analysis gene mentions;task description biocreative;corresponding gene mentions;mentions human annotated;gene mention;annotated corpus;sentences corresponding gene;gene mentions;task analysis gene;biocreative task description;results gene mention;corpus performance measures;corpus performance;task biocreative ii;gene mentions human;human annotated;task biocreative;mention task;corpus;biocreative task;description biocreative;identify substrings sentences;description biocreative methodsnineteen;substrings sentences corresponding;measures precision recall", "pdf_keywords": ""}, "d8aeb318f68f4635b34c72aa1a0369fadcd79450": {"ta_keywords": "user topic distribution;topic model probabilistic;cbs topic model;topic distribution;proposed topic model;topic model;topic model cauda;topics derive user;methodsa topic model;derive user topic;user communities microblogging;extract hidden topics;derive user communities;hidden topics derive;topics derive;communities microblogging;hidden topics;networks based sentiments;probabilistic graphical model;communities microblogging networks;topics;microblogging networks based;user communities;user topic;model probabilistic graphical;microblogging;probabilistic graphical;microblogging networks;cbs topic;propose cbs topic", "pdf_keywords": ""}, "8442f9fd620ea34e1de3128b9388bddd1263f29b": {"ta_keywords": "method conic optimization;conic optimization named;introductionconic optimization;introductionconic optimization minimization;conic optimization;minimization convex quadratic;optimization minimization convex;conic constraints methodswe;optimization minimization;minimization convex;conic constraints;integral projected gradient;optimization named extrapolated;optimization;optimization named;minimization;method conic;primal dual optimality;dual optimality conditions;optimality conditions;order method conic;projected gradient method;dual optimality;convex quadratic function;convex quadratic;subject conic constraints;optimality;constraints methodswe introduce;projected gradient;constraints methodswe", "pdf_keywords": "optimization convergence primal;method convex optimization;optimization conic optimization;convex optimization method;dual method convex;method conic optimization;conic optimization optimal;primal dual method;conic optimization named;approach optimization conic;optimization conic;methods conic optimization;convex optimization;method constrained optimal;convex programming fundamental;constrained optimization;conic optimization present;conic constraints methodswe;conic optimization conic;convergence primal dual;conic optimization;conic optimization solving;conic optimization proposing;convex programming;conic optimization discuss;constrained optimization pgeq;integral projected gradient;convex programming powerful;control optimal minimization;equality constrained optimization"}, "ee24fb876e6f1b345d492101c499bc5dd6b8196b": {"ta_keywords": "erps electroencephalogram;electroencephalogram signals;channel electroencephalogram signals;signals enhance electroencephalogram;potentials erps electroencephalogram;electroencephalogram signals enhance;enhance electroencephalogram signals;electroencephalogram signals make;electroencephalogram signals easily;channel electroencephalogram;multi channel electroencephalogram;erps electroencephalogram used;activities electroencephalogram signals;electroencephalogram used features;electroencephalogram;enhance electroencephalogram;brain activities electroencephalogram;signals make erps;activities electroencephalogram;electroencephalogram used;related potentials erps;potentials erps;event related potentials;signals enhance;erps clearly observed;brain machine interfaces;analysis brain activities;make erps clearly;make erps;signals easily suffer", "pdf_keywords": ""}, "ce0fce520c639af010c71cc6adf57cdeb2790322": {"ta_keywords": "automatic speech recognition;speech recognition systems;speech recognition;auditory skilled experts;automatic speech;auditory skilled;development auditory skilled;art automatic speech;recognition systems complex;systems deep understanding;performance systems deep;systems deep;recognition systems;neural networks;limiting development auditory;hmms numbers neurons;deep understanding expertise;auditory;state art automatic;neural networks reach;development auditory;layers learning rates;skilled experts;states gassians hmms;layers learning;learning rates;gassians hmms numbers;deep understanding;speech;neurons layers learning", "pdf_keywords": ""}, "516a0faeab9ec3a68bc6e7ec13a2df235a27ab52": {"ta_keywords": "neural networks medical;intensive care units;networks medical diagnosis;medical diagnosis admission;intensive care;dataset consisting clinical;diagnosis admission notes;data thousand patients;world clinical notes;clinical notes mimic;introductionconvolutional neural networks;clinical data;clinical notes;stayed intensive care;patients stayed intensive;consisting clinical data;clinical data thousand;networks medical;diagnosis admission;care units;medical center;thousand patients;neural networks;medical center 2001;medical diagnosis;real world clinical;introductionconvolutional neural;patients;thousand patients stayed;israel deaconess medical", "pdf_keywords": "diagnosis convolutional based;medical diagnosis convolutional;classification clinical notes;diagnosis convolutional;diagnosis prediction multiclass;learning clinical notes;convolutional based text;text classification cn;neural network diagnosis;algorithm classification clinical;text classification;text classification model;classification clinical;diagnosis prediction;diagnosis prediction output;discharge diagnosis prediction;prediction multiclass classification;accuracy medical diagnosis;learn embeddings;dataset consistsing clinical;classification cn;classification;classification cn support;notes diagnosis decision;features clinical notes;learn embeddings low;phrases diagnosisnosis primary;clinical notes diagnosis;learning models;electronic health record"}, "d3793ae5b3b31f72605978b749e41811e6dcacd4": {"ta_keywords": "inverse reinforcement learning;inverse reinforcement;uses inverse reinforcement;reward environment learn;reinforcement learning learn;agents maximize reward;reinforcement learning;reinforcement;reward environment;cyber physical agents;physical agents systems;agents systems play;maximize reward environment;autonomous cyber;agents systems;agents maximize;allow agents maximize;agents behave ways;autonomous cyber physical;agents behave;autonomous;ensure agents behave;maximize reward;environment learn follow;environment learn;physical agents;learning learn set;agents;learn follow implicit;techniques allow agents", "pdf_keywords": "learning inverse reinforcement;inverse reinforcement learning;reinforcement learning inverse;based inverse reinforcement;inverse reinforcement;perform inverse reinforcement;agents learning policies;reward environment learn;ethical reinforcement learning;learning behavioral constraints;apply inverse reinforcement;learning policies constrained;environment reward learn;reinforcement learning learn;algorithm learn rewards;reinforcement learning game;learning policies;reinforcement learning algorithms;behavioral constraints learnt;uses inverse reinforcement;learning learned rewards;reward maximizing policies;reward maximizing policy;learning exploration reward;maximize reward environment;reinforcement learning;learning optimization environment;class reinforcement learning;reinforcement learning demonstrations;strategies learning optimization"}, "405c0d9b7cf5482d2e1197167f690e7b7801b9bd": {"ta_keywords": "hop question answering;answering question generation;question generation;question answering;generates questions selecting;generates questions;qg generates questions;question generation process;question answering question;hop qa;multi hop qa;hop qa results;hop qa model;unsupervised multi hop;answering question;answer pairs unsupervised;mqa qg generates;qa model referencing;labeled multi hop;question answer pairs;mqa qg;questions selecting;qa;qg generates;qa model;qa results mqa;answering;results mqa qg;qg;qa results", "pdf_keywords": "hop question generation;hop question generator;question generator;hop question answering;learning generated questions;unsupervised question generation;multihop question generation;generating questions textual;question generation generate;generate questions;generate multihop questions;question generation;question generator mq;generating questions;questions training;question answering qrt;questions training qrt;learning multihop questions;generate questions demonstrate;multihop questions training;question generation table;model question generation;ability generate questions;question generation mq;question generation new;generated questions fluent;generate questions different;question answering;questions help train;question representation"}, "7bf2620188c0a66e1d0e779083cf61960a2f3e2f": {"ta_keywords": "synthesis optimization combinational;generating combinational logic;optimization combinational logic;automation synthesis optimization;synthesis optimization;synthesis capable generating;capable generating combinational;combinational logic;synthesis capable;automation synthesis;socrates synthesis capable;synthesis quality;generating combinational;combinational logic given;optimization combinational;combinational logic result;synthesis;logic given technology;introductionthe automation synthesis;correctness synthesis quality;synthesis quality measured;combinational;socrates synthesis;describes socrates synthesis;correctness synthesis;functional correctness synthesis;capable generating;timing constraints;constraints imposed logic;circuit chip", "pdf_keywords": ""}, "d04c91bbb043666ebd6dae51995ee5bbc4291ddf": {"ta_keywords": "shape called double;double double shaped;called double double;double shaped;double shaped shape;double double double;double double;called double;double;shaped shape called;shape called;shaped shape;shaped;shape;called", "pdf_keywords": ""}, "b209f2acbf0fbc62a3fd19ad6c13abbd46547736": {"ta_keywords": "voxceleb speaker recognition;speaker diarization voxceleb;diarization voxceleb speaker;introductionmicrosoft speaker diarization;themicrosoft speaker diarization;track voxceleb speaker;voxceleb speaker;speaker diarization;speaker recognition challenge;speaker diarization monaural;speaker recognition;introductionmicrosoft speaker;recognition challenge voxsrc;diarization track voxceleb;diarization voxceleb;describes themicrosoft speaker;multi talker recordings;themicrosoft speaker;talker recordings;challenge voxsrc;talker recordings wild;talker recordings present;track voxceleb;challenge voxsrc methodswe;voxsrc;voxceleb;voxsrc methodswe;evaluated diarization track;voxsrc methodswe explain;diarization track", "pdf_keywords": "speaker clustering;independent speaker verification;speaker diarization;speaker clustering cluster;cluster speech segments;speaker diarization consists;speech diarization;segmentation speaker;based speaker clustering;speech segments clusters;speaker clustering evaluate;improvement speech separation;clustering cluster speech;cluster speech;segmentation speaker clustering;based speech separation;extraction segmentation speaker;continuous speech separation;speaker recognition;speech separation cmc;speaker verification;speech separation combination;speech diarization important;propose speaker diarization;structure speaker verification;speaker verification vsv;audio analysis challenging;speech separation;speaker diarisation method;diarization track voxsr"}, "9e3e6ddf958c2005f7041cc9dd5fe050a0dbd02e": {"ta_keywords": "multiscale wavelet transform;methodsthe multiscale wavelet;wavelet transform mwt;wavelet transform;multiscale wavelet;points wavelet transform;wavelet transform domain;using wavelet transform;crossing points wavelet;points wavelet;multiple resolution analysis;analysis using wavelet;wavelet;crossing point transform;using wavelet;resolution analysis using;resolution analysis;methodsthe multiscale;transform mwt method;point transform signal;point transform;method multiple resolution;multiscale;property methodsthe multiscale;transform signal;transform mwt;transform signal corresponds;multiple resolution;correspondence zero crossing;transform mwt property", "pdf_keywords": ""}, "4ec1d3407a5136c525b53f703c803571200902a4": {"ta_keywords": "diagnosis pulmonary infection;pulmonary infection;patients diagnosis pulmonary;diagnosis pulmonary;pulmonary;patients diagnosis;infection;management patients diagnosis;diagnosis;patients;management patients;approach management patients;approach management;new approach management;article discuss importance;importance new approach;management;new approach;article discuss;article;approach;aim article discuss;discuss importance new;aim article;discuss importance;new;importance new;importance;discuss;aim", "pdf_keywords": ""}, "f2818da69bb72526fff9d601677db38f24a62ecc": {"ta_keywords": "dialogue modeling ebdm;dialog example databases;systems dialogue modeling;dialogue modeling;dialogue modeling important;based dialogue modeling;dialogue systems dialogue;example based dialogue;dialogue systems;introduction dialogue systems;dialog example;dialogue quality;dialogue quality conventionalal;systems dialogue;improve dialogue quality;improve dialogue;examples improve dialogue;dialog;selecting response utterances;dialogue;response utterances examples;based dialogue;response utterances;responses user query;methods dialog example;ebm based systems;user query response;conventionalal ebm based;effective methods dialog;utterances examples improve", "pdf_keywords": ""}, "febb305a854d02b138250a8a19af956ffa0ada4f": {"ta_keywords": "convergence nash equilibria;local convergence nash;policy gradient algorithms;convergence nash;nash equilibria continuous;linear quadratic games;counterexample policy gradient;policy gradient;quadratic games;equilibria continuous action;action state multiagent;gradient play;gradient algorithms guarantees;context policy gradient;algorithms guarantees convergence;nash equilibria;convergence continuous action;gradient play player;analyze gradient play;multi agent;state multiagent;state multiagent settings;multi agent settings;multiagent settings counterexample;guarantees local convergence;equilibria continuous;space multi agent;guarantees convergence;gradient algorithms;continuous action state", "pdf_keywords": ""}, "f21a9d70319ca99227300349d7bcab5dee5869cd": {"ta_keywords": "multilingual corpus translations;neural machine translation;incomplete multilingual corpus;multilingual corpora complete;multilingual corpus;corpus translations;multilingual corpora;practice multilingual corpora;translation using incomplete;corpus translations missing;incomplete multilingual;machine translation;using incomplete multilingual;increase translation accuracy;multi source neural;machine translation using;translations;provide translations;translations missing;multilingual;translation using;increase translation;machine translation approach;translation accuracy;source neural machine;introductionmulti source neural;translations relevant languages;translation approach;practice multilingual;difficulty provide translations", "pdf_keywords": "incomplete multilingual corpus;translations using incomplete;incomplete multilingual corpora;using incomplete multilingual;incomplete multilingual;incompletheola multilingual corpus;actual incomplete multilingual;non coding translations;multilingual corpus missing;multilingual corpus translations;neural machsinpeanish translation;corpus translations missing;coding translations;multilingual corpus tes;multilingual corpus;coding translations using;multilingual corpus transla;neural machine translation;coding translations situations;multilingual corpora simple;increase translation accuracy;multilingual corpora;corpus translations;multilingual corpus uses;translations missing;practice multilingual corpora;using incompletheola multilingual;represent missing sentences;source translation approach;multisource non lingual"}, "b168fc72fa39e9669567bd099bab179549a15e14": {"ta_keywords": "2glycoprotein a2gpi antibodies;2glycoprotein a2gi antibodies;anti 2glycoprotein a2gpi;2glycoprotein a2gpi;anti 2glycoprotein a2gi;2glycoprotein a2gi;diagnosis anti phospholipid;a2gpi antibodies evaluation;anti phospholipid syndrome;a2gi antibodies diagnosis;a2gpi antibodies;phospholipid syndrome pcr;anti 2glycoprotein;a2gi antibodies;phospholipid syndrome;anti phospholipid;acle anti 2glycoprotein;contribution anti 2glycoprotein;2glycoprotein;phospholipid;antibodies diagnosis anti;antibodies evaluation;antibodies diagnosis;determination iga acle;antibodies;recommended determination iga;iga acle anti;diagnosis anti;determination iga;ap recommended determination", "pdf_keywords": ""}, "de43afd166a79c24b3a7dd16c5695059d9f0aa71": {"ta_keywords": "cognitive development child;cognitive development;overview cognitive development;development child infancy;child infancy adulthood;child infancy;cognitive learning versus;infancy adulthood;cognitive learning;development child;learning versus age;infancy;timescale cognitive learning;learning capabilities child;overview cognitive;operational stage children;child pre operational;cognitive;general overview cognitive;capabilities child pre;children paget theory;versus age child;infancy adulthood arguments;age child;development concept transitivity;proposed timescale cognitive;child pre;timescale cognitive;concept transitivity pre;stage children paget", "pdf_keywords": ""}, "ab94fae3d49cd7016a47020469dc257d8090f5bb": {"ta_keywords": "speaker separation methods;challenging speech separation;multi speaker separation;speaker separation;deep clustering;speech separation;speech separation resul;deep clustering recently;spectrogram segmentation;spectrogram segmentation resulting;background deep clustering;applied spectrogram segmentation;results speaker independent;spectrogram;independent multi speaker;speaker independent multi;speaker independent;performance challenging speech;impressive results speaker;recently applied spectrogram;multi speaker;applied spectrogram;results speaker;embeddings basis clustering;deep learning;segmentation resulting impressive;discriminatively trained embeddings;separation methods;deep learning architecture;challenging speech", "pdf_keywords": "speech deep clustering;deep clustering speech;supervised speech separation;clustering speech training;deep clustering effective;deep clustering model;deep clustering;clustering speech;challenging speech separation;speaker separation methodsin;clean speech deep;clustering framework speech;speech separation tasks;present deep clustering;speech separation resultswewe;based deep clustering;speech separation using;method deep clustering;speech separation;speaker separation;shape speech separation;multi speaker separation;separation using deep;supervised speech;deep clustering framework;speech speech mixture;speech mixture based;trained clean speech;speech deep;speech mixture"}, "6c477a65f0922d405c3665e31581eaa0f269116e": {"ta_keywords": "distributional relational semantics;semantics word representations;relational objective wordnet;objective wordnet methodspreliminary;relational semantics word;wordnet methodspreliminary;relational semantics;word representations;wordnet methodspreliminary results;relational semantics end;introductionincorporating distributional relational;incorporate distributional relational;semantics word;wordnet;objective wordnet;knowledge base completion;hypothesis word representations;distributional relational;semantics end employ;word representations investigate;text relational objective;semantics;semantics end;text relational;word representations ought;raw text relational;knowledge base;relational objective;distributional objective raw;results knowledge base", "pdf_keywords": "distributional relational semantics;relational objective wordnet;word representations trained;parsing word representations;word representations;neural probabilistic language;knowledge base parsing;hypothesis word representations;objective wordnet;objective wordnet methodspreliminary;relational semantics;language computational semantics;computational semantics;semantics results joint;probabilistic language model;computational semantics results;wordnet;relational semantics end;wordnet methodspreliminary;text relational objective;knowledge base completion;probabilistic language;wordnet methodspreliminary results;embeddings algorithm joint;optimization interactions embeddings;analogy tests parsing;association computational language;text relational;language model;distributional relational"}, "10085f7fb0871329d34529cc54df0a8f75756fce": {"ta_keywords": "update automatic transcription;automatic transcription ja;automatic transcription;automatic speech;automatic speech recognition;speech recognition ar;update acoustic language;performance automatic speech;speech recognition;transcription ja national;acoustic language models;speaking style transformation;congress update acoustic;transcription ja;transcription;acoustic language;update acoustic;introduction semi automated;propose semi automated;semi automated;automated;speaking style;language models;recognition ar;consists speaking style;language models vital;automated update automatic;automated framework ar;update automatic;semi automated update", "pdf_keywords": ""}, "248824ec5d9b4ddf0c36cdc51b6b57af6e881328": {"ta_keywords": "cross lingual transfer;language transfer;transfer language;language transfer standard;lingual transfer;transfer language invaluable;language processing nl;resource transfer language;introduction cross lingual;lingual transfer high;low resource languages;cross lingual;language processing;clear language transfer;performance natural language;select languages based;select languages;particular task language;resource languages;processing nl;resource languages given;nl low resource;lingual;task language;strategy select languages;processing nl low;natural language processing;language invaluable tool;languages given;languages", "pdf_keywords": "lingual transfer ranking;language optimal transfer;transfer languages tasks;transfer language optimal;transfer language task;predicting transfer languages;better transfer languages;transfer language highly;optimal transfer language;optimal transfer languages;lingual transfer invaluable;language transfer;transfer languages nonlinguistic;languages similar task;transfer languages;best transfer language;non lingual transfer;good transfer language;selecting transfer languages;transfer task languages;languages tasks best;lingual transfer including;language tasks;non linguistic tasks;potential transfer language;transfer language;choosing transfer languages;lingual transfer;vocabularies transfer task;transfer languages non"}, "70170035ef870df1c064cc52804178a52f6a69ef": {"ta_keywords": "patient history history;patient history;case patient history;history history;history history history;history;present case patient;case patient;article present case;purpose article present;article present;purpose article;article;patient;present case;present;case;purpose", "pdf_keywords": ""}, "75c4aefc55bf0b345587740cad0a4e994f29962a": {"ta_keywords": "sound event detection;sound event detect;sound activity detection;polyphonic sound event;backgroundbltm hmm hybrid;combined sound activity;sound event;sound activity;detection network polyphonic;hybrid approach polyphonic;network polyphonic sound;approach polyphonic sound;polyphonic sound;approach polyphonic;hybrid combined sound;activity detection;event detection methodsthis;backgroundbltm hmm;network polyphonic;hmm hybrid combined;activity detection network;event detection;polyphonic;event detect;combined sound;backgroundbltm;hmm hybrid;detection methodsthis paper;new hybrid approach;detect", "pdf_keywords": ""}, "cc19de8d0782917098029ed20261cbe0b0c62bf5": {"ta_keywords": "latent biases text;biases biases text;latent biases;societal biases biases;uncovering latent biases;biases text written;societal biases;biases text;biases text method;biases biases;existence societal biases;biases;subgroups recommendation letters;widely reported anecdot;wages population subgroups;peer review methods;peer review;latent;systematic disparities;subgroups recommendation;review methods quantifying;application peer review;rates wages population;quantifying systematic disparities;recommendation letters male;different subgroups recommendation;employment rates wages;disparities;evidence existence societal;subgroups provides compelling", "pdf_keywords": "biases text peer;biases review text;biases peer review;bias peer review;quantifying biases text;peer reviews bias;bias review text;biases biases text;biases expressed text;quantify bias text;authors formalize bias;biases text written;biases text;quantifying biases peer;bias text employ;bias text;assess biases peer;biases review ratings;reviews bias estimates;biases peer;reviews bias;peer review text;publishing associated bias;bias peer;review data bias;text association bias;detect biases text;bias text caused;bias estimates peer;text peer reviews"}, "8484fdb56e4690927dc0191ede11c2d24bc5e2ef": {"ta_keywords": "text generation measuring;text generation;text generation directly;text generation model;distribution text generation;ended text generation;machine generated text;neural text;gap neural text;generated text human;neural text human;text using divergence;generated text;text human language;generation directly compares;human written text;distribution human written;learnt distribution text;generation measuring close;machine generated;human text;text human text;generation directly;generation measuring;human text using;gap neural;compares learnt distribution;open ended text;written text;measuring gap neural", "pdf_keywords": "text generation measure;text generation measuring;text generation models;text generation substantially;evaluation text generation;text generation model;ended text generation;distribution text generation;text generation propose;quality generated text;text generation;automated text generation;text generation directly;natural language generation;machine generated text;modern text generation;human generated text;text generation development;text generation use;distribution human text;web text generation;generated text significantly;generated text human;human text model;text generation method;generation using text;generation use decoding;text generation task;text news generation;generating text"}, "aff5d7f43823e06bb68220db41de3bc82e2f3990": {"ta_keywords": "networks high mobility;small cell networks;high mobility users;mobile moving users;mobility users;data rate mobile;micro base stations;cell networks;cell networks high;mobile users mus;rate mobile users;frequent handoff;stations bs mobile;heterogeneous tier network;rate mobile;mobile users;mobile moving;mobility users results;bs mobile moving;mobility;bs mobile;high mobility;tier network;frequent handoff severely;mobile;tier network structure;base stations;handoff severely restricts;network structure static;handoff", "pdf_keywords": ""}, "45cdf5e239a1f0057c350f6654ccd348fb4e2332": {"ta_keywords": "uncertainty agents preferences;uncertainty lottery model;models uncertainty lottery;uncertainty lottery;matching setting uncertainty;distribution linear preferences;sided stable matching;stable matching setting;setting uncertainty agents;stable matching;preferences compact indifference;weak preference order;agent weak preference;indifference model agent;linear preferences;uncertainty agents;preference order;weak preference;agents preferences;lottery model;preferences limited information;agents preferences limited;indifference model;linear preferences compact;setting uncertainty;order equally likely;preference order specified;uncertainty;models uncertainty;consider models uncertainty", "pdf_keywords": "matchings independent uncertainty;uncertainty agents preferences;preferences uncertainty market;restricted matching higheststabilityprobility;matching higheststabilityprobability;uncertain preferences uncertainty;model matching higheststabilityprobability;matching higheststabilityprobability hard;preferences uncertainty;corresponding preference agents;matching higheststabilityprobility;models uncertainty lottery;indifference model matching;matching higheststabilityprobility consider;lottery compact indifference;uncertain preferences;stable matching possible;optimal stable matching;uncertainty lottery model;stable matchings independent;stability probability matching;choice preferences agents;matching stable agents;matching setting uncertainty;problem stochastic marriage;stochastic marriage problem;matching higheststabilityprobility paper;possible preferences agents;distribution linear preferences"}, "4731f89169604cd0d8b5352380baa1b4728bca0b": {"ta_keywords": "machine translation errorrometer;translation error trends;translation errorrometer analysis;translation errorrometer;machine translation error;correct translations analyzing;translations analyzing;translation error;translations analyzing correct;machine translation;tool machine translation;erroneous word strings;discriminative language models;correct translations;analysis machine translation;frequently erroneous word;error analysis;occur correct translations;translations;error analysis machine;discriminative language;error analysis resul;frequently erroneous;errorrometer analysis;introduction discriminative language;language models;error trends;errorrometer;errorrometer analysis methods;language models tool", "pdf_keywords": ""}, "601408d6617bf72894c9f41ae54cf9c17905903a": {"ta_keywords": "par phrasebased systems;systems parsing alignment;tree string systems;string systems parsing;phrasebased systems improved;parsing alignment methodsbased;accuracy tree string;performs par phrasebased;phrasebased systems;systems parsing;parsing alignment;tree string performs;string systems;par phrasebased;tree string;parsing;basic tree string;string performs par;english pairs basic;pairs basic tree;accuracy tree;english pairs;ja english pairs;basic tree;experiments english japanese;alignment methodsbased detailed;tree;alignment methodsbased;detailed experiments english;japanese ja english", "pdf_keywords": ""}, "3dc20be709818630e2249ab28b35b0666b4b544d": {"ta_keywords": "bilingual corpora methodscontratory;bilingual corpora;english bilingual corpora;corpora methodscontratory speech;paralinguistic information languages;translate content utterance;analyze paralinguistic information;paralinguistic information;ignore paralinguistic information;paralinguistic information included;analyze paralinguistic;ja english bilingual;utterance ignore paralinguistic;speech s2s translation;ignore paralinguistic;translation systems;translation systems translate;paper analyze paralinguistic;english bilingual;corpora methodscontratory;s2s translation systems;methodscontratory speech speech;paralinguistic;methodscontratory speech;bilingual;speech speech s2s;translate content;corpora;emphasis ja english;speech s2s", "pdf_keywords": ""}, "ead6323f137c2f99ef0ffcfa34fa6eb1c6eca3c6": {"ta_keywords": "chime speech separationion;speech separationion recognition;6th chime speech;speech separationion;conversational speech diarization;multi microphone conversational;microphone conversational speech;speech diarization recognition;speech diarization;microphone conversational;recognition challenge chime;chime speech;previous chime challenge;organize 6th chime;introductionchronic speech;chime challenge;speech important problem;separationion recognition challenge;chime challenge considers;distant multi microphone;challenge chime methodsthe;introductionchronic speech important;diarization recognition everyday;chime methodsthe new;challenge chime;revisits previous chime;6th chime;separationion recognition;multi microphone;chime methodsthe", "pdf_keywords": "chime speech separationion;array synchronization speech;speech separationion recognition;synchronization speech;overlapping speech recognition;chime recordings;microphone arrays;mammalian speech recognition;chime recordings accurate;6th chime speech;microphone conversational;recognition mammalian speech;track combine speech;microphone conversational speech;conversational speech diarization;challenge speech recognition;speech processing everyday;previous chime recordings;multi microphone conversational;diarization speech recognition;speech diarization recognition;new speech recognition;data processing speech;synchronization speech enhancement;home environments speech;processing multispeaker speech;multispeaker speech processing;speech recognition mammalian;microphone arrays fully;challenge multispeaker speech"}, "af5c4b80fbf847f69a202ba5a780a3dd18c1a027": {"ta_keywords": "inference commonsense reasoning;grounded commonsense inference;inference commonsense;commonsense inference unifying;commonsense inference;language inference commonsense;commonsense reasoning methods;commonsense reasoning;natural language inference;task grounded commonsense;inference unifying natural;grounded commonsense;language inference;commonsense;inference unifying;unifying natural language;inference;natural language;reasoning methods;reasoning methods present;reasoning;humans reason situation;questions rich;car humans reason;humans reason;choice questions rich;task grounded;introduce task grounded;car;multiple choice questions", "pdf_keywords": "grounded commonsense inference;language inference commonsense;commonsense inference;commonsense inference task;inference commonsense;commonsense inference broadens;inference commonsense reasoning;commonsense natural language;grounded commonsense reasoning;commonsense inference use;task grounded commonsense;video captions activitynet;dataset purely stylistic;video captions context;grounded commonsense;focused grounded commonsense;sequential video captions;natural language inference;research commonsense natural;commonsense reasoning;situated commonsense inference;commonsense reasoning support;commonsense reasoning create;commonsense reasoning introduce;commonsense natural;captions context follow;commonsense reasoning present;stylistic classifiers;commonsense;textual entailment challenge"}, "d15eb5744474cec2d0634651bb30000b3873a309": {"ta_keywords": "recognition time expressions;expressions research normalization;time expressions research;time expression normal;time expressions;time expressions includes;understanding time expressions;time expression;normalization recent;model time expression;normalization;normalization recent years;recognition time;existing standardotomy normalization;standardotomy normalization;recognition normalization recent;standardotomy normalization methods;recognition normalization;expressions research;tasks recognition normalization;expression normal;progress recognition time;normalization methods;rules grammars;normalization methods highly;introductionthe understanding time;research normalization;corpora social media;understanding time;grammars", "pdf_keywords": "normalization time expressions;time expression normalization;normalization time expression;annotation temporal expressions;annotations normalization time;expressions research normalization;annotations normalization;recognition time expressions;expression normalization;time expressions common;time expressions natural;compositional annotations normalization;expressions natural language;rules time expressions;normalization time;time expressions research;standard annotation temporal;time expressions;recognition time expression;expressions annotating;expressions compositional annotations;time expressions includes;temporal expressions;expression normalization sequence;semantic structure expressions;annotating year expressions;generating normalization rules;normalization rules training;sequence normalization;annotation temporal"}, "3b563c16e9a918631d63a20027dad735b625625a": {"ta_keywords": "text generation tasks;text generation;machine translation texar;text generation case;toolbox text generation;translation texar designed;descriptionwe introduce texar;machine translation;translation texar;set text generation;neural machine translation;generation tasks conclusiondifferent;introduce texar;generation tasks;generation case descriptionwe;texar open source;texar;conclusiondifferent existing toolkits;texar designed;extensible toolbox text;source toolkit;texar designed highly;introductiontexar modularized;introductiontexar;open source toolkit;introduce texar open;text;toolkits;toolkit;existing toolkits", "pdf_keywords": "text generation toolkit;text generation models;text generation texar;text generation applications;purpose text generation;applications text generation;text generation tasks;text generation;generation applications text;generation toolkit;development text generation;methods text generation;process text generation;generation texar;field text generation;especially text generation;text generation article;language modeling texar;set text generation;generation texar general;language model tool;texar toolkit allows;texar toolkit;text generation key;generation models;machine translation development;levels texar toolkit;general purpose text;generation tasks;modeling texar"}, "a8372f7cb2e482a455b06c3e47f65aec5c7a924b": {"ta_keywords": "lead bismuth target;molten lead bismuth;lead bismuth eutectic;liquid lead bismuth;bismuth target circuit;lead bismuth;bismuth target;power accelerator driven;beam power accelerator;bismuth eutectic;bismuth eutectic lbe;accelerator driven ads;power accelerator;accelerator driven;pilot molten lead;bismuth;molten lead;ads liquid lead;induction pump alip;accelerator;linear induction pump;introductionthe pilot molten;pump alip experimental;induction pump;lbe circulation loop;eutectic lbe circulation;beam power;driven ads liquid;designed beam power;target circuit", "pdf_keywords": ""}, "0c07cc7ba1b862556f5cfee0d5d849866d21a693": {"ta_keywords": "storage nodes;systems storage nodes;storage nodes intermittently;data updated nodes;distributed storage;introductionin distributed storage;distributed storage systems;updated nodes stale;updated nodes;node updated nodes;nodes need update;stale node needs;updated nodes knowledge;stale node updated;nodes intermittently offline;nodes stale;nodes stale node;stale node;online nodes;downloading data updated;setting stale node;online nodes need;storage systems;storage systems storage;systems storage;node needs update;node updated;update contents downloading;storage;intermittently offline numerous", "pdf_keywords": "oblivious update algorithm;protocol oblivious update;oblivious updates code;perform oblivious updates;performing oblivious updates;required oblivious updates;bits oblivious update;oblivious updates linear;oblivious updates single;oblivious updates stale;oblivious update message;oblivious updates;update algorithm oblivious;algorithm oblivious update;protocol oblivious;communication required oblivious;oblivious update stale;oblivious update;lower bound communication;present protocol oblivious;oblivious update performed;symbol oblivious update;bound communication;bound communication required;method oblivious updates;codes update algorithms;oblivious update updated;log2 bits oblivious;distributed storage;algorithm oblivious"}, "9650dbe79d34498113371770dcdb48f1bd7c9711": {"ta_keywords": "maps computer science;maps computer;create maps computer;visual exploration research;maps;heatmaps;computer science described;create maps;cities map;map;visual exploration;titles cities map;approach visual exploration;occurence help heatmaps;practical approach visual;cities map countries;help heatmaps;database create maps;map countries;map countries created;exploration research papers;computer science;research papers;paper titles cities;similarity;computer science mocs;approach visual;similarity calculated using;research papers specifically;visual", "pdf_keywords": "text visualization information;visualization large text;text visualization;visualization information retrieval;based text visualization;visualization summarize large;visualization summarize;visualization large relational;visualize visualize topic;visualization comprehensive tool;web visualization comprehensive;terms useful visualization;information visualization;visualize topic space;web visualization;useful tool visualizing;information visualization important;comprehensive tool visualizing;visualization important tool;tool visualizing data;analyze visualize data;visualization information;visualization comprehensive;based information visualization;approach visualizing data;visualizing data;use maps visualize;practical approach visualizing;visualize visualize;visualize topic"}, "889c3b4394826639d483c039467cd9a05e68e73c": {"ta_keywords": "composing piece music;learning models music;write music nonlinear;music nonlinear;music nonlinear fashion;composers write music;composing piece;human composers write;human composers;write music;models music;composers write;composers;models music typically;piece music;composition chronological;nonlinear fashion scribbling;composing;music typically;music;composition chronological process;convolutional neural;scribbling motifs revisiting;convolutional neural network;scribbling motifs;train convolutional neural;contrary human composers;composition;neural network;neural", "pdf_keywords": "generative model musical;partial music model;modeling musical scores;partial music;model musical counterpoint;arbitrarily partial music;challenge computational music;modeling musical;generate music;music challenge computational;composition music challenge;blocked gibbs sampling;approaches generate music;approach modeling musical;generate music style;gibbs sampling;domain musical counterpoint;computational music;composition music;predict pitch generating;computational music research;generative neural;gibbs sampling markov;generating partial scores;ancestral sampling based;sampling markov chain;useful generating partial;nondescriptive ancestral sampling;musical counterpoint instead;sampling markov"}, "68af273e04906e0450a5d01d5606c8313da01453": {"ta_keywords": "sensors estimation fusion;sensor subset selection;estimation fusion center;subset possible sensors;estimation fusion;sensor subset;sensors estimation;developed sensor subset;possible sensors estimation;sensor networks;large sensor network;scale sensor networks;sensor network;iterative randomized algorithms;sensor networks necessitates;subset selection exploiting;randomized algorithms;algorithms developed sensor;randomized algorithms developed;iterative randomized;fusion center problem;fusion center;set iterative randomized;sensor network required;subset selection;fusion center energy;large scale sensor;introductiona large sensor;possible sensors;large sensor", "pdf_keywords": "sampling stochastic approximation;gibbs sampling stochastic;sampling stochastic;optimal subset sensors;sensors optimal subset;constrained combinatorial optimization;stochastic approximation optimal;subset sensors feasible;sampling solve unconstrained;proposed gibbs sampling;based gibbs sampling;iterative randomized algorithms;gibbs sampling;sensor subset selection;gibbs sampling based;gibbs sampling solve;sampling based algorithms;stochastic approxwe propose;using stochastic approxwe;randomized algorithms;subset possible sensors;stochastic algorithm;stochastic approximation;stochastic approxwe;subset sensors;iterative randomized;combinatorial optimization;novel stochastic algorithm;using stochastic approximation;sensor subset"}, "04f8f739924a19c01d196a48783b914554ac0fe5": {"ta_keywords": "composite convex optimization;algorithms composite convex;domain newton methods;convex optimization;composite convex;newton methods algorithms;second order algorithms;convex optimization called;backgroundconvex optimization based;convex optimization paper;problem composite convex;backgroundconvex optimization;newton methods;newton methods methodswe;contracting domain newton;optimization called contracting;methods algorithms affine;order algorithms composite;optimization based global;optimization paper present;convex;optimization called;domain newton;optimization paper;algorithms composite;algorithms affine;optimization;second lower approximation;order algorithms;optimization based", "pdf_keywords": "composite convex minimization;methods convex optimization;method composite optimization;convex optimization;convex minimization;composite optimization;methods convex;gradient methods convex;convex minimization problem;convex optimization using;minimizing composite functions;new method minimizing;composite convex;composite optimization problem;calculation composite convex;convex optimization demonstrate;method minimizing composite;optimization methods;method minimizing;stochastic convex optimization;component strongly convex;regularization method new;minimizing composite;optimization methods according;minimization problem method;minimization;regularization method;convex respect arbitrary;domain newton method;strongly convex introduce"}, "86ae1161026f23f9df691a867fd7453cee56fd28": {"ta_keywords": "change meaning phylogenies;lexicon particularly change;lexical phylogenetic;lexical cognate models;studying semantic change;meaning phylogenies;semantic change;meaning phylogenies presented;studying change lexicon;change lexicon;lexical cognate;appropriate lexical phylogenetic;phylogenies;phylogenies presented briefly;change lexicon particularly;semantic change rely;phylogenies presented;illustrations lexical cognate;language change;phylogenetic;lexical;stratified word embeddings;language change point;lexicon particularly;lexicon;temporally stratified word;change meaning;particularly change meaning;approach language change;evolutionary approach language", "pdf_keywords": "language change evolution;language change conceptualizations;language evolution;evolution languages changing;understanding language change;language evolution evolution;changes linguistic;language relationship evolution;understand language change;semantic change;changes linguistic systems;languages changing;approaches language change;languages change;approaches semantic change;language change used;vocabulary changes language;language change;language change implications;studying changes linguistic;language evolution easy;language change way;world languages change;changes language change;language phylogeny evolution;lexicon constantly changing;language understand changes;historical linguistics conceptualized;change evolution language;vocabulary changes"}, "5b8eaaf660b9e2d6a19886991350fffa1320b372": {"ta_keywords": "learn entities relations;learning inference entity;identifying entities relations;entity relationship identification;inference entity relationship;entities relations sentences;entities relations methods;learn entities;inference entity;identifying entities;entities relations;approaches identifying entities;learning inference;relationship identification;inference based training;entity relationship;relationship identification study;relations sentences;relations methods;entities;relations methods uses;relations sentences compare;ilp inference uses;strategies learn entities;entity;lp inference;lp inference resul;ilp inference;using lp inference;relations", "pdf_keywords": ""}, "781e0e81834119c135091c8bdfcd1966c10b09ab": {"ta_keywords": "speed integer compression;integer compression schemes;integer compression;compression schemes;compression schemes uses;compression vectorize optimize;compression vectorize;compression;stateoftheart compression vectorize;sidgalloing algorithm;introduce sidgalloing algorithm;stateoftheart compression;providing stateoftheart compression;cpu cycles decoded;sidgalloing algorithm exploit;decoded 32bit integer;common processors boost;decoded 32bit;cycles decoded 32bit;processors boost;cycles decoded;sid instruction compare;singleinstruction multiple data;common processors;data simd instructions;decoded;processors boost speed;singleinstruction multiple;processors;data simd", "pdf_keywords": "speed integer compression;integer compression schemes;integer compression;compressed data efficiently;useful compressing integers;bit unpacking faster;decompression faster scalar;simd decompression faster;bit integers decompression;faster decompression speeds;compressing integers using;compression schemes s4;processing integers slow;bit unpacking computation;faster regular bit;state art compression;method compressing integers;faster decompression;compressing integers;optimizing decompression speed;compression schemes;integers significantly faster;decompressing 32 bit;decompression 32 bit;method useful compressing;decompress 32 bit;processor algorithm faster;compression subsequent processing;compressed data;coding improve compression"}, "65f632cbac465633a13b1e3f8c8c410c2f3aec3d": {"ta_keywords": "policy gradient st;agent reinforcement learning;dedeterministic policy gradient;policy gradient;multi agent reinforcement;agent reinforcement;reinforcement learning;deep dedeterministic policy;multi agent deep;reinforcement learning methods;agent deep dedeterministic;staackelberg multi agent;agent deep;reinforcement;game theoretic marl;multi agent;generation sophisticated policies;competitive multi agent;gradient st maddpg;marl algorithm staackelberg;popular robotics;dedeterministic policy;robotics;sophisticated policies;sophisticated policies challenging;st maddpg methods;tasks popular robotics;novel game theoretic;maddpg methods;learning methods solve", "pdf_keywords": ""}, "76862a851bd2c17dcf6bfc2cecbf4af186730123": {"ta_keywords": "segmenting nontext objects;segmenting nontext;method segmenting nontext;grayscale document image;segmenting;solution aimed segmenting;grayscale document;directly grayscale document;unconventional method segmenting;objects directly grayscale;thresholding;thresholding method connected;aimed segmenting;thresholding method;optical thresholding method;method segmenting;optical thresholding;segmenting half;document image;operators optical thresholding;aimed segmenting half;directly grayscale;grayscale;nontext objects;nontext objects directly;document image making;extract desired objects;structure extract desired;maxtree structure;structure extract", "pdf_keywords": ""}, "1a20d6c6891f3a0462515ff9560bc37e66eb422a": {"ta_keywords": "fri frit conference;frit conference;search afrt conference;frit conference meeting;web search afrt;information extraction social;social media web;extraction social media;afrt conference hosted;intelligence natural language;isw fri frit;meeting artificial intelligence;afrt conference;natural language information;search afrt;web search ianl;information extraction;web search;search ianl isw;fri frit;language information extraction;isw fri;natural language;artificial intelligence natural;ianl isw fri;social media;frit;conference hosted;extraction social;search", "pdf_keywords": ""}, "68258e0541132027ef86f872b92406de1c6edab3": {"ta_keywords": "transmission transmission model;transmission model;transmission model new;model new transmission;new transmission transmission;transmission transmission;transmission new transmission;new transmission;transmission new;transmission;model new;model;new", "pdf_keywords": ""}, "b1d309073623d46548e55269fb73485a3b7f11a8": {"ta_keywords": "language model embryology;backgroundpretrained language model;model embryology;backgroundpretrained language;model embryology birth;language model;embryology;embryology birth alentalentalentalentalentalentalentalentaleraleraleraleraleraleraleraleraleraleraleraleraleraleraleraleraleraleraleraler;embryology birth;language;backgroundpretrained;birth alentalentalentalentalentalentalentalentaleraleraleraleraleraleraleraleraleraleraleraleraleraleraleraleraleraleraleraler;birth;model;alentalentalentalentalentalentalentalentaleraleraleraleraleraleraleraleraleraleraleraleraleraleraleraleraleraleraleraler", "pdf_keywords": "pretrained language models;pretrained language model;pretrained language;behaviors pretrained language;embryology pretrained language;token reconstruction learning;steps pretrained language;reconstruct predict tokens;language models;higher words learning;language model highly;words learning;speech different learning;model natural language;learns reconstruct predict;language model;context human language;contextual information learning;token reconstruction;predict tokens;learns reconstruct;language model useful;words learning speed;learning process token;token reconstruction results;language model prepared;able utilize contextual;transformer based language;human language developed;contextual information able"}, "0110abf15bf0ee1bdf28061ad05f85b1c9f6e1c3": {"ta_keywords": "knowledge integration structured;knowledge integration;knowledge integration problem;structured information sources;combining information relations;introduction knowledge integration;type knowledge integration;integration structured information;similarity measures text;structured information;distributed heterogeneous databases;combining information;information sources containing;information relations;heterogeneous databases;heterogeneous databases available;similarity measures;studied similarity measures;integration distributed heterogeneous;based studied similarity;information relations lack;problem combining information;studied similarity;similarity;integration structured;sources containing text;type knowledge;common object identiiers;information sources;abstract methodsinformation integration", "pdf_keywords": ""}, "a3da7028a1b721e392c421c2f15096abb1a71afb": {"ta_keywords": "a1c increases atherosclerotic;atherosclerotic plaque viulnerability;increases atherosclerotic plaque;atherosclerotic plaque;hemoglobin a1c;hemoglobin a1c increases;backgroundelevation hemoglobin a1c;intravascular optical coherence;percutaneous coronary intervention;coronary intervention;plaque viulnerability visit;variability liquid lipid;coronary intervention methodsa;lipid profiles patients;plaque viulnerability;intravascular optical;subjects received intravascular;increases atherosclerotic;received intravascular optical;percutaneous coronary;liquid lipid profiles;coherence tomography assessment;optical coherence tomography;year percutaneous coronary;liquid lipid;intravascular;atherosclerotic;backgroundelevation hemoglobin;a1c increases;lipid profiles", "pdf_keywords": ""}, "3ed07f6643856b9ac4687b3bc667767f3ab4b563": {"ta_keywords": "voice quality control;selecting voice quality;voice quality expression;multiple voice quality;voice quality;performance voice quality;better voice quality;selecting voice;achieve better voice;quality expression words;method selecting voice;results performance voice;performance voice;quality expression;quality control using;using multiple voice;multiple voice;better voice;quality control;quality control significantly;scores independency acoustic;voice;mr gm methods;independency acoustic features;perceptual scores;acoustic features corresponding;independency perceptual scores;corresponding perceptual scores;features corresponding mr;perceptual scores independency", "pdf_keywords": ""}, "ecde7c041e9ac48bccef7a8d078a3f80239b0479": {"ta_keywords": "video reconcurrent neural;object detection video;object detecting video;video reconcurrent;refining object detecting;video datasets tend;sparsely annotated frames;motion blur;detection video;motion blur compression;detecting video reconcurrent;frontier motion blur;breakthroughs object detection;video datasets;reconcurrent neural networks;videos appear smooth;blur compression;object detection;variability videos;detection video offers;object detecting;detecting video;variability videos appear;vast amounts video;additionally video datasets;level variability videos;blur compression artifacts;reconcurrent neural;blur;eye additionally video", "pdf_keywords": "predicting objects video;object detection videos;object detection video;object detection trained;improving object detection;refine object detection;object detection loss;video utilizing contextual;detection videos;detection videos captures;refining object detection;detection video;video extract contextual;video frames train;objects video;video dataset sparse;breakthroughs object detection;object detection static;detection trained;videos captures temporal;networks object detection;detection video utilizing;object detection;video dataset;video datasets tend;frames video dataset;video datasets;objects video data;effective predicting objects;eye additionally video"}, "085072963b33367b842369b9ce81394d32ac8843": {"ta_keywords": "deep learning separation;speech separation systems;channel speech separation;speech separation;learning separation;learning separation models;noisy speech paradigm;separation systems improved;training synthetic mixtures;relative inseparability noise;introductiontraining noisy noisy;noisy speech;introductiontraining noisy;noise noisy speech;separation models;inseparability noise noisy;single channel speech;inseparability noise;separation systems;near field speech;separation models need;field speech;training deep;training deep learning;training synthetic;channel speech;noisy single channel;field speech initial;separation;developed training deep", "pdf_keywords": "deep learning separation;speech separation recognition;channel speech separation;speech separation systems;based speech separation;separate speech noise;separation speech enhancement;approach speech separation;speech separation using;source separation speech;signals speech separation;speech separation extraction;speech separation;assessment speech separation;learning separation;learning separation models;unable separate speech;audio source separation;separate speech;separation speech;speech recordings sk;real speech recordings;single channel speech;combination clean speech;separation techniques noise;speech recordings;separate noise signals;separate noise;clean speech preventing;channel speech"}, "76fe5f80dd25078eefa522e59a7763bc5d5da826": {"ta_keywords": "errors learning learning;errors errors learning;adaptedadapted spelling errors;errors learning;use adaptedadapted spelling;learning learning;learning learning models;adaptedadapted spelling;spelling errors errors;spelling errors;learning;learning models;models learning learning;learning models learning;models learning;article use adaptedadapted;adaptedadapted;spelling;use adaptedadapted;errors errors;errors;models;article use;article;use;purpose article;purpose article use;purpose", "pdf_keywords": ""}, "9165d5e99b2106825dd00b9f5daf60e454434399": {"ta_keywords": "simultaneous interpretation corpus;english simultaneous interpretation;simultaneous interpretation interpreter;interpretation corpus;professional simultaneous interpreters;japanese english simultaneous;interpretation corpus methodsthere;simultaneous interpreters;interpretation interpreter;interpretation interpreter possible;simultaneous interpretation;corpus professional simultaneous;simultaneous interpreters different;compare better interpretations;english simultaneous;collection english japanese;interpretations good;interpreters;data simultaneous interpretation;japanese japanese english;interpretations;japanese english;interpreter;better interpretations good;recorded interpretations;corpus;better interpretations;interpreter possible compare;english japanese japanese;english japanese", "pdf_keywords": ""}, "23d299b35366c18e397faeb2c8687c20f8e17688": {"ta_keywords": "detection deception attack;dnn malicious deception;deception attack deep;attack deep neural;deception attack attacks;detection deception;malicious deception attack;deception attack;classification autonomous cyber;vulnerability dnn malicious;vulnerability dnn;neural network dnn;dnn based image;shown vulnerability dnn;malicious deception;dn based classifier;dnn malicious;network dnn;network dnn based;attack attacks pixel;attacks pixel;dnn based;attack deep;deep neural network;attacks pixel values;deception;image classification autonomous;classification autonomous;neural network;image classification", "pdf_keywords": ""}, "72302d8c5cdcf59b6df96290ffc874d3613fe6b1": {"ta_keywords": "cancer pathology classification;cancer pathology diagnosed;pathology diagnosed cancer;diagnosed cancer pathology;cancer pathology;patient cancer pathology;classification tumors pathogenesis;pathology classification;cancer pathology based;pathology classification based;classification tumors;based classification tumors;pathology diagnosed;tumors pathogenesis report;diagnosed cancer;pathology;pathology based;tumors pathogenesis;pathology based aforementioned;case patient cancer;patient cancer;tumors;cancer;diagnosed;classification;pathogenesis report case;classification based classification;report case patient;classification based;based classification", "pdf_keywords": ""}, "12e9d005c77f76e344361f79c4b008034ae547eb": {"ta_keywords": "embed textual sequences;models embed textual;embed textual;aimcharagram embedding words;embedding words;similarity sentence similarity;character based compositional;sentence similarity;character gram count;embedding words senstences;character gram;textual sequences;compositional models embed;using character gram;evaluation word similarity;word similarity;textual sequences methodsa;aimcharagram embedding;character grams simple;textual;gram count vector;word sentence represented;character grams;gram count;based compositional models;learning character;embedding use tasks;word similarity sentence;compositional models;embedding", "pdf_keywords": "embed textual sequences;aware text representations;models embed textual;embeddings subword aware;embed textual;text representations;chargam embeddings subword;textual sequences model;text representations chargam;word similarity models;similarity speech tagging;recognition textual sequences;chargam embeddings used;textual similarity development;character gram count;implementation chargam embeddings;embeddings subword;subword aware text;chargam embeddings;representations chargam embeddings;textual sequences;character gram;sentence similarity tasks;sentence similarity speech;textual similarity;sequences textual sequences;character based compositional;word sentence similarity;similarity sentence similarity;chargam phrase model"}, "f053137323a88eb932d590bcdfc959ee805e2520": {"ta_keywords": "dependency parsing stack;dependency parsing;parsing stack;parsing stack pointer;architecture dependency parsing;stack pointer networks;pointer networks stackptr;pointer networks stack;parsing;combining pointer networks;pointer networks;dependency tree;sentence builds dependency;networks stack pointer;pointer networks vinyals;stack pointer;stackptr combining pointer;dependency tree root;novel architecture dependency;builds dependency tree;internal stack;reads encodes sentence;pointer;dependency;architecture dependency;stack proposed model;stackptr combining;encodes sentence builds;networks stackptr combining;combining pointer", "pdf_keywords": "parser pointer network;neural dependency parsing;dependency parsing neural;dependency parsing stack;parsing neural;dependency parsing widely;parsing stack;parsing children synapse;graph based parsers;parsing stackpointer networks;embedding optimize parsing;dependency parsing stackpointer;parsing neural fundamental;dependency parsing;significantly outperforms parsers;stack pointer networks;dependency parsing parser;parsing applications neural;parsing stack pointer;optimize parsing performance;dependency parsing fundamental;neuromfast parser;parser long dependency;outperforms parsers;transition based parsers;dependenceencies treebanks;neuromfast parser end;optimize parsing;approach dependency parsing;analysis dependency parsing"}, "a4b1afd75bd2da0b21df58cd4ae1649fefabd8dd": {"ta_keywords": "utility learning multiplayer;players utility functions;learning multiplayer noncooperative;inverse correlated equilibrium;play correlated equilibrium;correlated equilibrium strategy;agents utility maximizers;correlated equilibrium framework;equilibrium framework utility;agent utility functions;learning multiplayer;players utility;parameters players utility;play correlated;correlated equilibrium;utility maximizers;methodsin game theoretic;multiplayer noncooperative;inverse correlated;utility learning;computing parameters players;parametric agent utility;model agents utility;equilibrium framework;given play correlated;game theoretic framework;purposean inverse correlated;equilibrium strategy;utility functions solve;equilibrium strategy model", "pdf_keywords": ""}, "cbf9a2560eac548e7b3d5eb7074c40b7bb861909": {"ta_keywords": "speaker diarization eend;end speaker diarization;neural speaker diarization;speaker diarization conditiond;speaker diarization;speech activity overlap;overlapping speech;end neural speaker;diarization conditiond speech;overlapping speech paper;conditiond speech activity;diarization eend;case overlapping speech;end speaker;neural speaker;speech activity;diarization eend methodsthe;conditional multitask learning;introductionend end speaker;overlap detection;overlap detection common;conditiond speech;multitask learning method;activity overlap detection;multitask learning;speaker;conditional multitask;speech paper improve;diarization;diarization conditiond", "pdf_keywords": "speech activity detection;speaker diarization subtasks;subtask speaker diarization;partitioning speech;end speech activity;process partitioning speech;partitioning speech recording;conditional multitask learning;recognition development multitask;speech activity;speech diarization;speech speech diarization;speech activities;diarization subtasks;diarization speech;speaker diarization successively;overlapping speech speech;speaker diarization simultaneous;clustering speaker diarisation;overlapping speech;speech activities work;speech recording;multitask problem speaker;speech recording homogeneous;speech recognition;learning multitask;multispeaker speech recognition;performs speaker diarization;subtask followed speech;multitask learning algorithms"}, "e9dfccd86b6116f7601d44590985de2df434a094": {"ta_keywords": "learning technology;learning technology used;learning teaching methods;skills solving linear;learning teaching;teachable agent;learns skills solving;advanced learning technology;teaching methods;learns skills;teachable agent called;teaching methods proposed;describes advanced learning;skills solving;technology instance teachable;hypotheses learning teaching;sistuddent learns skills;instance teachable agent;learns;called sistuddent learns;teaching;advanced learning;teachable;learning;sistuddent learns;instance teachable;equations examples feedback;skills;investigate hypotheses learning;linear equations examples", "pdf_keywords": ""}, "c933fed82e7b5cbf7230f0f970b69590b40f86a1": {"ta_keywords": "introductiondecentralized stochastic optimization;stochastic optimization methods;introductiondecentralized stochastic;decentralized sg methods;stochastic optimization;sg methods;unified convergence analysis;sg methods far;introduce unified convergence;decentralized sg;optimization methods gained;cheap iteration cost;cheap iteration;unified convergence;optimization methods;variety decentralized sg;cost data locality;mainly cheap iteration;introductiondecentralized;data locality;iteration cost;locality communication efficiency;iteration cost data;convergence analysis covers;data locality communication;stochastic;large variety decentralized;locality communication;convergence analysis;optimization", "pdf_keywords": "decentralized stochastic gradient;decentralized stochastic optimization;introductiondecentralized stochastic optimization;distributed stochastic gradient;stochastic gradient descent;distributed convex optimization;decentralized stochastic methods;stochastic optimization methods;implementation stochastic gradients;method distributed stochastic;introductiondecentralized stochastic;decentralized distributed convex;algorithms decentralized stochastic;decentralized consensus optimization;stochastic optimization;randomized gossip algorithms;method gossip averaging;stochastic gradients;stochastic gradient sg;gossip algorithms;gossip algorithms decentralized;consensus optimization;analysis decentralized stochastic;randomized gossip algorithm;distributed learning;stochastic gradient;distributed stochastic;theory decentralized stochastic;gradient descent methods;decentralized stochastic"}, "91d98b0a175237b48122e7560010e87a968fb6e0": {"ta_keywords": "recurrent networks separationion;separation recognition speech;separation speech enhancement;speech separation;speech separation speech;neural networks separation;recently speech separation;speech enhancement problems;separation speech;speech nonstationary background;speech enhancement;separationion recognition single;single channel speech;separation recognition;nonstationary background audio;separationion recognition;channel speech nonstationary;recognition speech challenging;networks separation recognition;mask prediction networks;background audio objectives;recognition speech;speech challenging environments;networks separationion recognition;background audio;deep recurrent networks;speech nonstationary;background deep recurrent;channel speech;mask prediction", "pdf_keywords": ""}, "cf8f2ca0c2d618104bc8724a6effc509088f16c4": {"ta_keywords": "neverending learning;neverending learning paradigm;propose neverending learning;ening language learner;language learner;learner;current machine learning;machine learning better;language learner nell;learning;learning paradigm;learning systems;type learning;learning systems acquire;encompassing type learning;learning performed humans;machine learning systems;learning paradigm machine;learning better;people learn;type learning performed;machine learning;people learn different;learning performed;learner nell achieves;learn;paradigm machine learning;data model just;learner nell;learn different", "pdf_keywords": ""}, "cc7858e74a79edceb5a42c30fc5c2dc5117f365b": {"ta_keywords": "learn model atari;model atari environments;generative adversar;propose generative adversar;deep generative;deep generative models;design deep generative;atari environments;generative models;deep reinforcement learning;generative models lr;model atari;advances deep reinforcement;model propose generative;deep reinforcement;generative;learn environment model;learn model;reinforcement learning lr;atari;propose generative;method learn model;atari environments deploy;learn environment;data learn environment;models lr environments;reinforcement learning;design deep;data learn;unsupervised data learn", "pdf_keywords": "learn model atari;deep reinforcement learning;model free reinforcement;free reinforcement learning;learn model deep;model atari environments;advances deep reinforcement;agent learn model;deep reinforcement;approach deep reinforcement;algorithm atari games;model deep;novel algorithm atari;atari environments;suitable atari games;games suitable atari;model atari;learn model;deep allow agent;atari games;atari games suitable;model model game;reinforcement learning lr;algorithm atari;free reinforcement;agent learn;model game;generative models environments;dynamic model game;model deep allow"}, "82cb0c428f5edb1db6e733dc4b1b20023a2ce15f": {"ta_keywords": "prize dataset netflix;netflix prize dataset;voting data;data netflix prize;ordered voting data;voting systems;study voting systems;voting systems takes;voting data derive;netflix prize;netflix users incentive;ordered voting;millions netflix users;study voting;strictly ordered voting;background study voting;voting;netflix users;elections existing studies;netflix data derived;takers evaluate elections;netflix data;prize dataset;random survey takers;evaluate elections;millions netflix;derived millions netflix;unlike random survey;dataset netflix;data netflix", "pdf_keywords": ""}, "f0bbc7b84c166e2258b6ba4f9d9835ecac04e842": {"ta_keywords": "spontaneous speech recognition;speech fluctuation modeling;introductionaccurate acoustic model;speech recognition;speech fluctuation factors;given speech data;spontaneous speech;task spontaneous speech;acoustic model;acoustic model construction;speech data;speech recognition requires;speech fluctuation;variations speaker variances;construction spontaneous speech;advantages speech fluctuation;various speech fluctuation;speaker variances;introductionaccurate acoustic;selection given speech;speaker variances dealt;speech data unlike;speaking variations speaker;variations speaker;speaking variations;given speech;bayesian;model construction spontaneous;bayesian approach advantages;bayesian approach", "pdf_keywords": ""}, "19b6537012412bee0a36e3e271f84b95868fe859": {"ta_keywords": "ad hominem arguments;ad hominem argument;opponent ad hominem;arguments punished arguers;hominem arguments;hominem argument;ad hominem;fallacious arguments punished;hominem argument methods;arguers lapse attacking;arguing committing fallacy;punished arguers lapse;arguments punished;hominem arguments potential;punished arguers;typology ad hominem;fallacious arguments;debate debating;enforced fallacious arguments;arguing;arguing committing;ideal debate debating;arguers;introduction arguing;debate debating rules;debate;debating rules;debating;debating rules strictly;committing fallacy", "pdf_keywords": "ad hominem arguments;ad hominem discussions;ad hominem argumentation;ad hominem argument;ad hominem fallacy;argument ad hominem;discourse ad hominem;hominem argumentation;ad hominem theoretical;hominem common argumentation;hominem argument ad;ad hominem theories;hominem argumentation levels;hominem arguments reliably;hominem arguments associated;ad hominem common;investigated ad hominem;hominem arguments overlooked;hominem argument famous;hominem arguments theoretical;hominem arguments remain;topic ad hominem;ad hominem properties;hominem arguments;hominem arguments mechanical;hominem arguments controversial;ad hominem threads;hominem arguments relatively;ad hominem;hominem argument considered"}, "36a5e0e0a8ce67e4cd9077d86e3b4d50fdcff15f": {"ta_keywords": "electrocatalysts overall water;electrocatalyst new water;new electrocatalyst;functional electrocatalysts overall;dual functional electrocatalysts;new electrocatalyst new;electrocatalyst new;functional electrocatalysts;electrocatalysts;electrocatalyst;electrocatalysts overall;case new electrocatalyst;mose2 ni3se2 nickel;achieve mose2 ni3se2;ni3se2 nickel foam;nickel foam efficient;mose2 ni3se2;water splitting procedure;new water splitting;ectopic water splitting;water splitting method;ni3se2 nickel;water splitting;achieve mose2;overall water splitting;nickel foam;mose2;used achieve mose2;ni3se2;foam efficient dual", "pdf_keywords": ""}, "d3e13d2514edaf74b863bfbe45a739c32a7689e1": {"ta_keywords": "code examples neural;neural code generation;representing code tree;code natural language;code generation model;code tree;code generation;code tree structure;program source code;examples neural code;source code natural;representing code;generate complex code;generate program source;language representing code;existing code examples;source code;code examples;models generate program;neural code;generate program;reference existing code;program source;introductionin models generate;existing code;subtree retrieval makes;code natural;retrieve sentences similar;model retrieve sentences;complex code", "pdf_keywords": "syntactic code generation;neural syntactic code;neural code generation;retrievalbased neural syntactic;code examples neural;code natural language;code generation model;generate gram actions;representing code tree;code generation;code generation using;examples neural code;syntactic code;language representing code;code tree;method retrievalbased neural;idea retrieval;code syntactically;generating code;gram action subtree;retrieve sentences similar;code tree structure;model retrieve sentences;neural syntactic;retrieval based generation;idea retrieval based;code generate;generating code target;code generate best;representing code"}, "ba3322280992d0425bc9e2b4c59de24857e5f4e7": {"ta_keywords": "deploying learning algorithms;behavior deploying learning;strategic classification decisiondependent;classify closed loop;strategic classification;classification decisiondependent distributions;performative prediction;deploying learning;classification decisiondependent;learning algorithms;classifier;performative prediction seek;data driven methods;decisions learner;prediction seek classify;learning algorithms explicitly;decisions learner work;classification;data driven;classifier underlying;works performative prediction;driven methods deployed;learner work strategic;classifier underlying data;seek classify closed;work strategic classification;driven methods;closed loop behavior;classify closed;decisiondependent distributions seeks", "pdf_keywords": "risk minimizing gradient;performative risk minimizer;risk minimization flow;performative risk minimization;performative risk minimizers;risk minimizing flow;performative risk minimizerwe;performative risk minimizing;repeated risk minimization;risk minimization perturbed;flows performative risk;gradient descent flow;risk minimizer;risk minimization;risk minimization provide;gradient flow decision;risk minimizers sufficient;risk minimizers;risk minimizers present;repeated risk minimizing;performative risk function;risk minimization framework;stochastic approximation;gradient flows performative;risk minimizers holds;minimizing gradient flow;risk minimizing;repeated gradient descent;stochastic approximation steady;gradient descent"}, "3c6670ecdfccd4633755c4b19d774453bfb77de3": {"ta_keywords": "fairness deciding;fairness decisions;ensure fairness decisions;consider fairness deciding;methods consider fairness;fairness decisions challenges;decide fairness;decide fairness means;challenges decide fairness;ensure fairness;need ensure fairness;consider fairness;fairness deciding match;fairness;fairness means;fairness means particular;match organs donated;deciding match organs;organs donated deceased;match organs;list organs donated;algorithms given responsibility;organs donated;waiting list organs;deceased donors patients;decisions challenges decide;deceased donors;donors patients results;decisions challenges;donated deceased donors", "pdf_keywords": ""}, "d79b613a67cf79740e1c08037f7d054585a12284": {"ta_keywords": "speech translation models;speech translation parallel;translation parallel autoregressive;speech translation framework;parallel autoregressive rescoring;end speech translation;translation models;speech translation;translation models advantages;parallel autoregressive;translation parallel;autoregressive rescoring;autoregressive rescoring aimto;methodsend end speech;autoregressive models methodsend;translation framework;autoregressive models;translation framework based;ar decoding methods;autoregressive end end;autoregressive end;ar decoding;non autoregressive models;decoding methods fast;conventional ar decoding;autoregressive;inference latency reduction;introductionnon autoregressive end;end end speech;introductionnon autoregressive", "pdf_keywords": "speech translation models;translation quality models;autoregressive speech translation;speech translation applications;translation models;translation translation quality;challenge translation quality;improve translation quality;approach speech translation;translation quality;machine translation increasing;speech translation fundamental;translation applications speech;methods speech translation;machine translation effective;speech translation increasing;end speech translation;speech encoder orthros;hypotheses speech translation;speech encoder;process speech translation;orthros improving decoding;translation models advantages;speech translation translation;translation quality novel;translation process speech;speech translation e2e;final translation quality;neural machine translation;translation quality studies"}, "fd9e38e240b4372c49b9205d6f909d070ff3804c": {"ta_keywords": "similarity based reasoning;similarity based;statistical similarity measures;using statistical similarity;general similarity based;similarity measures;statistical similarity;databases manipulate textual;similarity measures developed;textual data using;information retrieval;textual data;similarity;manipulate textual data;general similarity;developed information retrieval;information retrieval community;designed general similarity;useswira extension relational;relational databases;relational databases manipulate;retrieval community;retrieval community methodswe;introductionfew data;diverse forms data;information paper useswira;extension relational databases;based reasoning tasks;internet intelligent use;databases manipulate", "pdf_keywords": ""}, "cd96cae0f8eabc7bb327c6f30151741bfdd62ee0": {"ta_keywords": "international society disease;report international society;disease disease sigi;global pandemic;disease sigi;disease sigi presented;society disease disease;course global pandemic;officers international society;global pandemic radically;pandemic;society disease associated;disease disease;annual report international;society disease;international society;report international;new officers international;disease;disease associated;pandemic radically altered;officers international;disease associated activities;pandemic radically;sigi presented year;sigi presented;new leadership;integrating new leadership;sigi;annual report", "pdf_keywords": ""}, "dec6bb3c7bb671c86296a2a089e0e38aa3f69279": {"ta_keywords": "autoregressive machine translation;machine translation nat;machine translation;auorgresive mimachine translation;translation non autoregressive;translation nat systems;pretrained autoregressive model;pretrained autoregressive;data pretrained autoregressive;translation nat;autoregressive machine;mimachine translation;non autoregressive machine;mimachine translation non;autoregressive model better;autoregressive models existing;speed compared autoregressive;autoregressive models;improvements generation speed;compared autoregressive models;autoregressive model;knowledge distillation;data pretrained;training data pretrained;nat systems predict;translation;nat models usually;systems predict sequence;nat models;compared autoregressive", "pdf_keywords": ""}, "6bfeb25ea4bb41ab0840bb1be09f9b2de7eea8e4": {"ta_keywords": "cysteine based cytomegaloviruses;cytomegalovirus gcv encodedg;pig cytomegalovirus gcv;cytomegaloviruses cmvs encode;based cytomegaloviruses cmvs;based cytomegaloviruses;cytomegalovirus gcv;pig cytomegalovirus;guinea pig cytomegalovirus;cytomegaloviruses cmvs;cytomegalovirus;cytomegaloviruses;cellular signaling viral;signaling viral;gcv encodedg protein;signaling viral growth;receptor homolog cellular;cmvs encode cellular;viral growth pathogenesis;receptor homolog;coupled receptor homolog;protein coupled receptor;gcv encodedg;encodedg protein coupled;homolog cellular signaling;host immune functions;roles g33 guinea;g33 guinea pig;cmvs encode;viral growth", "pdf_keywords": ""}, "609010cb866a19dd996281d00818c3fc7363ec94": {"ta_keywords": "ner knowledge language;unsupervised cross lingual;cross lingual ner;lingual ner model;transfer ner knowledge;entity recognition tasks;named entity recognition;entity recognition;lingual ner;ner knowledge;ner model transfer;model transfer ner;recognition tasks languages;knowledge language;manually annotated training;transfer ner;annotated training data;languages need manually;annotated training;ner model;cross lingual;languages need;languages paper propose;knowledge language completely;data available languages;ner;language completely unsupervised;clinical messagea;clinical messagea recently;manually annotated", "pdf_keywords": "model unsupervised crosslingual;unsupervised cross lingual;cross lingual models;multi lingual models;lingual models train;multilingual translational models;monolingual unannotated data;unsupervised crosslingual named;cross lingual model;cross language model;model cross language;crosslingual named entity;unsupervised crosslingual;lingual models propose;lingual translational model;lingual models;language monolingual unannotated;useful monolingual model;language model cross;learn cross lingual;cross lingual translational;lingual named entity;monolingual unannotated;cross lingual information;lingual model monolingual;knowledge cross lingual;lingual transfer approach;monolingual model useful;language target embeddings;model useful monolingual"}, "a1c4ce9de92338646c6ee93c7c2e5ee366784b1a": {"ta_keywords": "representation semantic parsing;semantic parsing;semantic parsing understood;semantic parsing researchers;meaning representation semantic;component semantic parsing;parsing understood;parsing;representation semantic;meaning representations;parsing understood additionally;parsing researchers designed;semantic;parsing researchers;introduction meaning representation;component semantic;important component semantic;meaning representation;meaning representation important;impact meaning representation;lot meaning representations;meaning representations recent;representation important component;representation;representation important;representations;meaning;representations recent work;execution engines;comprehensively evaluated lack", "pdf_keywords": ""}, "facefd2fc4b718c6a0d8096b4eb02866028a04c2": {"ta_keywords": "open retrieval conversational;retrieval conversational;question answering conversational;conversational question answering;retrieval conversational question;answers open retrieval;retrieval conversational setting;answering conversational qa;question answering;conversational qa;answering conversational;question answering important;open retrieval;studies question answering;conversational qa emphasize;supervised open retrieval;retrieval retrieves evidence;question answerable single;extracts answers open;extracts answers;retrieval;collection extracts answers;conversational question;answerable single span;retrieval retrieves;answering;conversational;answers open;answering important issue;qa", "pdf_keywords": "open retrieval conversational;conversational question answering;question answering extensive;paraphraser generate training;retrieval conversational;leveraging diverse paraphraser;question answering orconvq;diverse paraphraser generate;question answering;weakly supervised conversations;question answering potential;paraphrased span known;retrieval conversational question;question answering development;open retrieval;supervised conversations;question answering role;paraphraser generate;supervised conversations weak;open retrieval dataset;paraphrased span;conversations span answers;supervised conversations conversation;question answering important;weak answer learned;answer retrieved passages;answering important tool;answering use conversational;identify paraphrased span;approach open retrieval"}, "75d33c125eba966b50d4dccd359a2f6aa4e0e2e7": {"ta_keywords": "data bandits literature;lipschitz risk functionals;data bandits;policy evaluation;evaluate prospective policies;risk functionals;bandits literature;risk functionals broad;bandits literature adopted;bandits;conditional value risk;logged data bandits;prospective policies using;introduce lipschitz risk;prospective policies;lipschitz risk;set objectives;policy evaluation date;research policy evaluation;focuses expected reward;risk;expected reward;value risk;policies using;expected reward paper;value risk cvar;objectives research policy;set objectives research;risk cvar;objectives subsumes conditional", "pdf_keywords": "learning risk functionals;estimating risk functionals;risk functional estimation;supervised learning risk;estimation risk functionals;estimate risk functionals;risk averse learning;risk functional estimate;measure risk functionals;lipschitz risk functionals;risk functionals lipschitzness;risk functionals;risk functionals algorithm;learning risk;risk risk functional;risk functionals evaluating;risk functionals using;measures risk prediction;risk functional;risk functionals method;risk functionals broad;functional estimation risk;risk functionals core;risk functionals risk;value risk functionals;lipschitzness risk functionals;established risk functionals;risk functionals composed;analysis risk functionals;risk functional functional"}, "cb0de2de79533d4faada3d745f43702eb89d1a60": {"ta_keywords": "backgrounddeveloping documentation guidelines;descriptions nl datasets;documentation guidelines easy;documentation practices;documentation practices field;standard documentation practices;processing nl tools;documentation;documentation guidelines;standard documentation;nl datasets models;nl datasets;building natural language;nl tools;backgrounddeveloping documentation;templates datasets models;natural language;natural language processing;practices field nl;templates datasets;use templates datasets;guidelines easy use;adoption standard documentation;nl tools despite;language processing nl;descriptions nl;datasets models supporting;processing nl;easy use templates;guidelines easy", "pdf_keywords": "documentation strategy adopted;documentation practices;standard documentation practices;documentation standards;developing documentation;designing documentation;documentation standards debate;implementation documentation standards;considered designing documentation;produce documentation;collected used documentation;documentation templates language;documentation strategy;documentation strategy identify;developing documentation templates;documentation practices field;documentation templates;used documentation nl;designing documentation strategy;produce documentation templates;documentation standards non;standard documentation;documentation templates used;present documentation strategy;reusable documentation;documentation nl;develop reusable documentation;documentation nl recently;tool documenting;document datasets involved"}, "13b6c8cce3b4557ad7a3188f2d54636e755e8145": {"ta_keywords": "unfolding multichannel source;multichannel source separation;source separation deep;unfolding multichannel;backgrounddeep unfolding multichannel;separation deep unfolding;unfold multichannel;multichannel audio architecture;source separation;multichannel audio;deep unfolding;multichannel source;domain multichannel audio;separation deep;unfold multichannel gassian;source separation methodswe;deep unfolding recently;deep mgmm computational;methodswe unfold multichannel;audio architecture;application multichannel source;multichannel;multichannel gassian mixture;audio architecture defined;domain multichannel;unfolding;frequency domain multichannel;separation methodswe unfold;application multichannel;backgrounddeep unfolding", "pdf_keywords": ""}, "77c63e8f102465e3fc4a46e0b07c32fa8d2f8a54": {"ta_keywords": "unsupervised grammar induction;grammar induction algorithms;grammar induction methodswork;methodswork grammar induction;induction methodswork grammar;grammar induction;current grammar induction;unsupervised grammar;grammar induction help;syntactic structure discoverable;limits unsupervised grammar;induction algorithms produce;word tag sequences;induction algorithms;current grammar;syntactic structure;grammar;methodswork grammar;sequences current grammar;tag sequences;syntactic;induction methodswork;shed light syntactic;constructions algorithms;constructions algorithms capture;discoverable raw word;induction;light syntactic structure;light syntactic;purposeprobing linguistic", "pdf_keywords": ""}, "c065f9997794b13565dd49a6e475fc5e8c9d54ce": {"ta_keywords": "tensile stiffness lamina;torsional joint utilizing;emergeent torsional joint;stiffness lamina emergeent;improve tensile stiffness;torsional joint;stiffness lamina;tensile stiffness;double laminated let;lamina emergeent torsional;joint layer flexible;utilizing double laminated;method improve tensile;double laminated material;improve tensile;kinetostatic model joint;flexible human18 aluminum;flexible human18;laminated let;layer flexible human18;laminated material structure;double laminated;laminated let dl;tensile;joint utilizing double;called double laminated;laminated material;lamina emergeent;stiffness;emergeent torsional", "pdf_keywords": ""}, "112eb8a8273ab725d47789efb87237edbc4f02db": {"ta_keywords": "propositional conjunctions learnable;learnability restricted order;learnability subsets order;conjunctions learnable;learnability subsets;considers learnability subsets;learnability restricted;order logic learned;considers learnability;learnability haussler;logic learned;logic learned valiant;learnability;conjunctions learnable paper;paper considers learnability;learnability haussler 1989;learnable;logic methods;study learnability restricted;established boundaries learnability;boundaries learnability haussler;study learnability;boundaries learnability;conjunctions order logic;order logic methods;valiant shown propositional;propositional conjunctions;order logic;paper study learnability;subsets order logic", "pdf_keywords": ""}, "e6accbbb366387faf817126dc7b0260c450bd2e6": {"ta_keywords": "inline formula tex;identifying inline formula;tex math inline;inline formula defective;inline formula;math inline formula;items inline formula;formula tex;math inline;identifying inline;problem identifying inline;formula tex math;latex tex math;tex math;tex math notation;formula defective items;inline;notation latex tex;math notation latex;defective items inline;latex tex;latex;formula defective;notation latex;tex;items inline;math notation;formula;defective items;math", "pdf_keywords": ""}, "ca7a67aa29c67b006017f651601091145644f243": {"ta_keywords": "calibrated speaker localization;speaker localization statistical;speaker localization;localize speakers reverberation;localization statistical speech;localize speakers;hard localize speakers;speech detection domestic;integration calibrated speaker;statistical speech detection;calibrated speaker;speech detection;speech detection methods;statistical speech;speakers reverberation;localization statistical;use statistical speech;method calibrates localization;calibrates localization;calibrates localization errors;speaker;speakers reverberation causes;noises methods;ensemble integration calibrated;localization;reverberation;introduction ensemble integration;localization errors;reverberation causes discrepancy;speakers", "pdf_keywords": ""}, "a61aebbfe029c4b8eafae4042e6242cdca8f54b7": {"ta_keywords": "latent relations;relation guided pre;qa datasets extremely;understanding latent relations;relations hurts generalization;relation guided;introduction answering;relations involving entities;latent relations involving;introduction answering complex;existing qa datasets;relations methods;relations;qa datasets;questions requires understanding;requires understanding latent;relations methods remedy;questions long;entities existing qa;tail relations methods;answering;propose relation guided;relations involving;relation;hurts generalization performance;questions;existing qa;types relations hurts;entities;qa", "pdf_keywords": "question answering;question answering applications;relational qa dataset;based question answering;supervised qa datasetswe;generate relational qa;supervised qa;relational facts knowledge;supervised qa datasets;inject knowledge relational;question answering leverage;facts knowledge graph;knowledge relational facts;status question answering;annotation qa pairs;qa datasets extremely;human annotation qa;generate questions passage;models supervised qa;qa dataset pre;knowledge relational;trained relation;knowledge graph;trained relation predictor;qa datasets severely;refine supervised qa;existing qa datasets;relational qa;knowledge graph popular;annotation qa"}, "2d1f442578feb7034aa2b68bbf95f608f2342256": {"ta_keywords": "group fairness bandit;bandit arm selection;fairness bandit arm;fairness bandit;multi armed bandit;group fairness contextual;bandit arm;group fairness;bandit;bandit mbb setting;fairness contextual multi;introduction group fairness;bandit mbb;armed bandit;armed bandit mbb;formulation group fairness;arm selection;fairness contextual;arm selection methods;sequential decision;setting sequential decision;selection;sequential decision maker;contextual multi armed;fairness;choose arm pull;selection methods propose;selection methods;decision maker time;choose arm", "pdf_keywords": "group fairness bandits;fairness bandits biased;fairness bandits;fairness bandits prove;bandit partitioned groups;fairness machine learning;bias algorithm fair;group fairness machine;group fairness;algorithm fairness biased;classification group fairness;fairness biased feedback;bandit based naive;bandits biased;group fairness fundamental;bandits biased feedback;fair machine learning;analysis group fairness;providing algorithm fairness;bias introduced reward;problem group fairness;algorithm fairness;fairness machine;bandit partitioned;fairness contextual multi;fairness biased;fairness contextual;feedback group fairness;fairness arbitrary;multi armed bandit"}, "147ba336fcba32fadca470e14a858ce069375475": {"ta_keywords": "speech synthesis;model speech synthesis;new model speech;model speech;etiology speech;etiology speech poorly;speech poorly;speech;synthesis;speech poorly understood;development new model;development;new model;model;development new;understood report development;report development;report development new;etiology;poorly understood;poorly;new;poorly understood report;understood;understood report;report", "pdf_keywords": ""}, "00c8d88abef116d8d3d673a28ff4098115cf8da3": {"ta_keywords": "cooperative persuasive dialogue;persuasive dialogue policies;persuasive dialogue able;persuasive dialogue;learning cooperative persuasive;created dialogue management;dialogue management;dialogue policies using;dialogue policies;dialogue management module;based cooperative persuasive;cooperative persuasive;text based cooperative;dialogue able persuade;module cooperative persuasive;persuasive dialogue hiraoka;dialogue;created dialogue;dialogue able;automated text based;fully automated text;works created dialogue;automated text;reinforcement learning cooperative;persuasive;conference computational linguistics;text based;learning cooperative;maintaining user satisfaction;conference computational", "pdf_keywords": ""}, "04a7d9f0388ded93c1ec16e36a6df3cd44cb95b0": {"ta_keywords": "multilingual entity linking;entity linking language;language specific mentions;multilingual entity;single entity retrieval;entity linking;linking 100 languages;linking language specific;entity linking 100;introduction entity linking;linking language;entity retrieval;entity retrieval model;formulation multilingual entity;mining auxiliary entity;mentions resolve language;auxiliary entity pairing;representation negative mining;auxiliary entity;entity pairing task;specific mentions resolve;million entities methods;obtain single entity;single entity;entity pairing;entities;multilingual;entity;covers 100 languages;new formulation multilingual", "pdf_keywords": "multilingual entity linking;entity linking language;multilingual entity;entities contextual mentions;introduce multilingual entity;multilingual lingual blect;entity linking better;generalization cross lingual;entity linking seeks;entity linking task;entity linking;linking language specific;mining auxiliary entity;formulation multilingual entity;language specific mentions;integrating multilingual lingual;linking entity mention;linking language;useful retrieval entities;entities contextual;corresponding entity language;integrating multilingual text;languages entities;multilingual lingual;entity mentions;languages linked biologists;occurring entity mentions;lingual blect;entity language;encode entities contextual"}, "9768d7ba9d09ac3bf3d52ec674bde1a6e615daad": {"ta_keywords": "pairwise comparison probabilities;strong stochastic transitivity;stochastic transitivity;probabilities future comparisons;stochastic transitivity st;aggregating pairwise comparison;risk estimation pairwise;estimation pairwise comparison;comparison probabilities;pairwise comparison data;comparison probabilities st;pairwise comparison;minimax risk measure;risk measure worst;minimax risk estimation;future comparisons;comparison data order;methods aggregating pairwise;minimax risk;satisfying strong stochastic;model minimax risk;studied minimax risk;strong stochastic;future comparisons investigate;worst case risk;order estimate outcome;comparisons;transitivity;measure worst case;estimation pairwise", "pdf_keywords": "strong stochastic transitivity;stochastic transitivity matrices;stochastic transitivity;stochastic transitivity order;estimating pairwise comparison;estimating strong stochastic;stochastic transitivity st;estimating indifference sets;risk estimating pairwise;estimating indifference;method estimating indifference;pairwise comparison probabilities;adaptivity index estimator;adaptivity index measure;characterizations optimal adaptivity;indifference sets matrices;adaptivity achieved statistical;analysis pairwise comparisons;adaptivity proposing regularized;pairwise comparisons;comparison probability matrix;estimating pairwise;propose adaptivity index;regularized estimators;bound regularized estimators;indifference sets pairwise;adaptivity index minimax;regularized estimators st;probability matrix strongly;regularized estimators relatively"}, "b7ffc8f44f7dafd7f51e4e7500842ec406b8e239": {"ta_keywords": "gating reading comprehension;grained gating reading;like reading comprehension;reading comprehension methods;gating reading;reading comprehension;reading comprehension results;comprehension methods data;comprehension methods;combines word level;fine grained gating;combine word level;tasks like reading;grained gating;word level character;characters fine grained;character level representations;word level;like reading;comprehension results present;comprehension results;concatenation scalar weighting;reading;words characters;comprehension;words characters fine;gating;representations using concatenation;grained gating mechanism;dynamically combine word", "pdf_keywords": "reading comprehension tasks;reading comprehension experiments;reading comprehension datasets;datasets reading comprehension;reading comprehension achieves;comprehension tasks achieving;paragraphs reading comprehension;performance reading comprehension;processing text neural;comprehension tasks;useful reading comprehension;reading comprehension;text neural;reading comprehension introduce;reading comprehension approach;comprehension tasks present;questions paragraphs reading;comprehension datasets;word character gating;text neural machine;context reading comprehension;processing text fundamentally;neural machine translation;comprehension datasets including;gating document query;paragraphs reading;text queries;comprehension experiments;document query gating;accuracy text queries"}, "90357a6dc817e2f7cec477a51156675fbf545cf1": {"ta_keywords": "learns multimodal script;learns multimodal;multimodal script knowledge;model learns multimodal;performing multimodal reasoning;multimodal reasoning;multimodal reasoning time;contextually performing multimodal;multimodal script;multimodal;script knowledge watching;performing multimodal;videos transcribed speech;visual world contextually;script knowledge;self supervised;videos transcribed;learns;self supervised manner;knowledge watching;free self supervised;world contextually;knowledge watching millions;supervised;merlot model learns;humans understand events;youtube videos transcribed;model learns;supervised manner;contextually", "pdf_keywords": "learns multimodal script;learning multimodal reasoning;learns multimodal;multimodal script knowledge;multimodal reasoning multimodal;learning multimodal representations;multimodal world knowledge;learning multimodal;model learns multimodal;multimodal representations;multimodal reasoning;reasoning multimodal world;multimodal world representations;performing multimodal reasoning;learns powerful multimodal;multimodal self supervised;reasoning multimodal;learns strong multimodal;learn multimodal;multimodal representations video;approach learning multimodal;learn multimodal world;able learn multimodal;multimodal temporal representations;multimodal representations single;corpus video reasoning;contextually performing multimodal;multimodal event representationation;knowledge development multimodal;multimodal reasoning time"}, "ba3f39606cfd4150ea80fec1b2e1137933c6d143": {"ta_keywords": "automated pcr;automated automated pcr;automated pcr setup;forensic diagnosis based;method forensic diagnosis;forensic diagnosis;approach useful forensic;pcr setup approach;pcr setup;new method forensic;pcr;forensic cases;cases useful forensic;useful forensic;forensic cases useful;useful forensic cases;method forensic;forensic;diagnosis based use;diagnosis based;use automated automated;automated automated;use automated;automated;diagnosis;based use automated;setup approach useful;cases useful;setup approach;new method", "pdf_keywords": ""}, "8ae4a584539a8f30d654e2678dde64a8334461b7": {"ta_keywords": "classify reviews recommender;reviews recommender basic;reviews recommender;classify reviews;write classify reviews;recurrent neural network;recurrent neural;recommender basic task;level recurrent neural;recommender basic;character level recurrent;user rate product;introductiongenerative concatenative nets;recommender;level recurrent;recurrent;reviews;neural network;rate product;neural;learn write classify;write product;introductiongenerative concatenative;concatenative nets;jointly learn write;user write product;nets jointly learn;write product design;concatenative nets jointly;concatenative", "pdf_keywords": "generate personalized reviews;predicting review text;generating reviews character;personalized reviews conditioned;generating reviews;personalized product reviews;personalized reviews;product reviews network;model generate reviews;generating text supervised;generate reviews conditioned;generate reviews;item classify sentiment;method generating reviews;method generate reviews;model generating reviews;generate text supervised;predict content review;text supervised;reviews conditioned user;information predicting review;text supervised fashion;predicting review;level text supervised;generating reviews beer;classify sentiment;classify sentiment star;reviews network;product reviews conditioned;generative concatenative network"}, "b01ecfd2322437fcc9c7ce6605d6f5a50f67ec50": {"ta_keywords": "introductionactive learning;introductionactive learning al;active sampling;training examples annotation;hope active sampling;active sampling leads;iteratively selects training;introductionactive;selects training examples;examples annotation;annotation;learning al widely;annotation current model;training examples;examples annotation current;annotation budget;training strategy;used training strategy;fixed annotation budget;annotation budget al;training strategy maximizing;fixed annotation;learning;sampling;predictive performance;selects training;learning al;annotation current;maximizing predictive performance;samples", "pdf_keywords": "active learning text;classification sequence tagging;classify sequence tagging;sequence tagging tasks;active learning al;training examples annotation;sequence tagging;sequence tagging al;active learning;use active learning;tagging tasks demonstrate;classification tasks text;tasks text classification;text classification tasks;text classification sequence;fixed annotation budget;human text classification;annotation budget;tagging;classification tasks;tagging tasks;tagging al;non text classification;effective text classification;tagging tasks suggest;examples annotation;tagging tasks present;learning text;learning text classification;annotation"}, "2177bf060aaf2c0c2b551d3e805779cb35c19bb1": {"ta_keywords": "phosphor ceramics used;phosphor ceramics;ceramics used electronic;electronic supplementary materialsupplementary;phosphor;1002 mn4 mn4;ceramics;ceramics used;mn4 mn4;1002 mn4;materialsupplementary material available;materialsupplementary material;10 1002 mn4;mn4;supplementary materialsupplementary material;materialsupplementary;supplementary materialsupplementary;used electronic supplementary;material available;electronic supplementary;material available article;used electronic;material;electronic;available article;available;available article 10;supplementary;article 10 1002;used", "pdf_keywords": ""}, "2068825cabd94c951a0282ed731a8b8f2da1721c": {"ta_keywords": "transducing natural language;formal meaning representations;structures annotating utterances;annotating utterances;annotating utterances rs;nl utterances;language nl utterances;nl utterances formal;natural language nl;utterances;natural language;variational auto encoding;utterances rs expensive;utterances formal;auto encoding model;tree structures annotating;utterances rs;utterances formal meaning;language nl;meaning representations;task transducing natural;represented tree;commonly represented tree;represented tree structures;structures annotating;data driven supervised;driven supervised models;introduce staructvae variational;driven supervised;auto encoding", "pdf_keywords": "neural semantic parsers;semantic parsing neural;neural semantic parsing;supervised semantic parsing;semi supervised parser;supervised parser latent;semantic parser transition;learning semantic parsing;parsing semi supervised;supervised parser extra;generation semantic parsing;parser capable learning;supervised parser;strong supervised parser;performance supervised parser;semantic parsing semi;semantic parsers;semantic parsing;semantic parser;parsing neural;parser inference;semantic parsing atis;parser inference model;semi supervised semantic;transition based parser;inference model parser;semantic parsing code;development semantic parsing;parser latent utterance;approach semantic parsing"}, "8ca58f3f6e59a6d243f3da6c196e9f730e6e9993": {"ta_keywords": "speaker diarization eend;neural speaker diarization;speaker diarization;end neural speaker;speaker tracing;extension speaker tracing;neural speaker;speaker tracing buffer;diarization eend;diarization eend model;speaker;extension speaker;modified extension speaker;number speakers;number speakers fixed;speakers fixed;speakers;diarization;ii number speakers;speakers fixed advance;eend model achieved;eend model;end end neural;end neural;conventional clustering;conventional clustering based;compared conventional clustering;clustering based methods;eend does perform;clustering", "pdf_keywords": "streaming speaker diarization;advances speech diarization;streaming diarization using;speaker diarization;online streaming speaker;speech diarization;propose streaming diarization;streaming diarization;speaker tracing buffer;speaker diarization method;speaker tracing;overlapping speech flexible;friendly speaker diarization;streaming diarization method;propose speaker tracing;streaming speaker;study speaker tracing;using speaker tracing;number speakers chunks;handles overlapping speech;speech diarization implications;development speech diarization;wise streaming diarization;handling overlapping speech;speech flexible numbers;chunk wise streaming;speech flexible number;speakers chunks;end neural diarization;speech recognition"}, "ca6d5c7829a76d10069fa3aa6776c35cc044b7ba": {"ta_keywords": "courses selection advising;selection tool undergraduates;course selection;concerning courses selection;courses selection;preferences concerning courses;testing course selection;course selection tool;selection advising career;advising career paths;majors colleges surveys;selection advising;concerning courses;tool undergraduates;majors colleges;students multiple majors;courses;career paths;tool undergraduates large;course advancing tool;advising career;colleges surveys;undergraduates;generating course advancing;colleges;multiple majors colleges;career paths results;designing testing course;colleges surveys asked;testing course", "pdf_keywords": ""}, "6e3f8187f8fef3e11578a73f32da07d33dbf8235": {"ta_keywords": "semantic triples tables;extracting semantic triples;semantic triples;extracting semantic;triples tables encodes;record text generation;procedure extracting semantic;semantic dependencies table;tables encodes structures;exploiting semantic;triples tables;tables encodes;data text datasets;semantic;table headers;existing data text;text generation;tables;table headers table;table title methodswe;data text;text datasets;dependencies table headers;structures exploiting semantic;headers table;text datasets facilitates;headers table title;table title;exploiting semantic dependencies;text generation dart", "pdf_keywords": "ontology annotation tables;annotated ontology data;tree structured semantic;annotation tables;tree ontology annotation;semantic triples tables;annotated table;manually annotated table;annotated table manually;annotation tables converts;text generation ontology;structured semantic;table manually annotated;extracting semantic triples;corpus structured data;source structured data;manually annotated ontology;schema tree structured;structured data;extracting semantic;annotated ontology;data text annotations;structured data record;text generation datasets;data text generation;procedure extracting semantic;ontology annotation;ontology data;structured data contain;novel tree ontology"}, "bdbf635476477eec5be5a292b494e20b8902cc35": {"ta_keywords": "machine translation mri;translation synthetic noise;text synthetic noise;translation mri systems;robustness mri;translation mri;robustness mri systems;synthetic noise methodsmodern;robustness machine translation;enhance robustness mri;noise methodsmodern machine;machine translation synthetic;methodsmodern machine translation;noise clean data;noise methodsmodern;synthetic noise;translation synthetic;mri systems emulating;generated text synthetic;synthetic noise disastrous;text synthetic;machine translation;mri systems perform;mri systems;mri;robustness machine;introductionmimifying robustness machine;introductionmimifying robustness;enhance robustness;human generated text", "pdf_keywords": "robustness machine translation;neural machine translation;translation useful neural;machine translation ms;simple domain adaptation;machine translation systems;machine translation useful;translation systems;domain adaptation;noise use translation;machine translation;translation systems increased;domain adaptation task;generate noisy version;translation ms systems;translation create artificial;generate noisy;noisy data social;naturally noisy data;tagging assist neural;text exploit experimental;adaptation noise nuanced;introductionmimifying robustness machine;models apply backtranslation;enhance robustness ms;robustness machine;data synthesizing noise;adaptation noise;backtranslation technique generate;enhance robustness"}, "b3979990dc2080138021cb3d767c7ec6d3e96194": {"ta_keywords": "characterizing literary relationships;dynamic fictional relationshipships;literary relationships;fictional relationshipships key;literary relationships relies;fictional relationshipships;novel unsupervised;present novel unsupervised;characterizing literary;work characterizing literary;dynamic dynamic fictional;relationships relies plot;novel unsupervised neural;dynamic fictional;challenge digital humanities;literary;unsupervised neural;digital humanities scholarship;interpretable accurate relationship;relationshipships key challenge;digital humanities;novel;friends unsupervised learning;unsupervised learning;humanities scholarship;relationships;relationship trajectories;unsupervised neural network;plot summaries annotated;relationshipships", "pdf_keywords": ""}, "e12c52fb542f76b3f0d29178842428d6a4edfe1e": {"ta_keywords": "defer machine learning;fairness accuracy learning;learning defer machine;improving fairness accuracy;external decision maker;fairness accuracy;improving fairness;model external decision;decision maker model;learning defer;involved automated human;automated human;accuracy learning defer;automated;automated model external;involved automated;decision maker;automated model;containing automated model;im improving fairness;machine learning;containing automated;external decision;decision makers;multiple decision makers;algorithmic development;decision makers involved;automated human interaction;unaddressed algorithmic development;unaddressed algorithmic", "pdf_keywords": "learning automated;decision making learning;learning automated model;learning defer adaptive;algorithm learning defer;learning defer models;method learning defer;responsible machine learning;rejection learning framework;model learning automated;deferring models learn;learn predict responsibly;learning defer;learning defer accurately;learning defer training;learning defer generalization;making learning defer;learning defer model;generalization rejection learning;external decision maker;automated models interact;generalizes rejection learning;propose model learning;model external decision;models interact decision;learning generalizes rejection;improvement rejection learning;learning defer valuable;decision making agents;decision maker model"}, "4ce47dd7a8674f8ffd53f1883bc57e62460a83f0": {"ta_keywords": "cyber physical systems;development cyber physical;scale cyber physical;societal scale cyber;physical systems increasingly;physical systems;cyber physical;physical systems provide;scale cyber;systems increasingly;systems increasingly important;theory societal;development cyber;theory societal scale;new theory societal;policy regulations energy;systems;regulations energy ecosystem;energy ecosystem;regulations energy;cyber;systems provide;societal scale;societal;energy;theory;physical;qualitative insights;new theory;systems provide number", "pdf_keywords": ""}, "fa9b043ae8da3cc60c975762ae9066d2fb010f41": {"ta_keywords": "graph based representations;graph clustering;graph clustering makes;efficient graph clustering;tasks graph clustering;graph clustering algorithms;nl tasks graph;natural language processing;graph based;language processing nl;introduction graph based;clustering;efficient graph;introduction graph;processing nl tasks;processing nl;clustering makes possible;graph;present efficient graph;language processing;clustering makes;tasks graph;natural language;variety natural language;clustering algorithms;nl tasks;clustering algorithms strengths;based representations;representations;representations proven effective", "pdf_keywords": ""}, "52540497682c4209b8e20125c8255358b22d0fa7": {"ta_keywords": "intelligent agents simulate;simulate human learning;constructing learning agent;develop intelligent agents;learning agent;human learning math;learning agent currently;cognitive tutors constructing;intelligent agents;agents simulate human;agents simulate;human level intelligence;tutors constructing learning;use cognitive tutors;cognitive tutors;simulate human level;artificial intelligence;artificial intelligence understand;human learning;knowledge domain level;constructing learning;level intelligence;goals artificial intelligence;fundamental goals artificial;simulate human;goals artificial;understand develop intelligent;domain knowledge;tutors constructing;knowledge domain", "pdf_keywords": ""}, "2ed6f376e9e7eee6d833ad7b6aba63d7ad40c0f8": {"ta_keywords": "dioxide blanket cardiac;carbon dioxide blanket;design carbon dioxide;blanket cardiac;blanket cardiac trauma;dioxide blanket;thermal hydraulic design;thermal hydraulic;hydraulic design carbon;cardiac trauma essential;design carbon;carbon dioxide;prevention chronic fever;cardiac trauma;thermal;hydraulic design;blanket;cardiac;fever;chronic fever;trauma essential;trauma essential development;dioxide;hydraulic;carbon;trauma;prevention;design;essential development new;development new development", "pdf_keywords": ""}, "bd0db679d595399b91c5acca1db33a2803697d53": {"ta_keywords": "political information internet;online content voters;encountering political information;internet learn politics;content voters encountering;political information;news online information;content voters;interact online content;americans turning internet;online information colored;voters encountering political;online content;encouraged interact online;information internet typically;encountering political;online information;news online;voters encountering;displayed comments readily;users encouraged interact;information internet;interact online;exposed just news;learn politics;learn politics greater;current web paradigm;reactions previous readers;internet learn;internet typically exposed", "pdf_keywords": ""}, "79f47ebf896b848e7c981c8aa6862ca1a7e5e7e5": {"ta_keywords": "kernel clustering practically;kernel clustering;power kernel clustering;analysis kernel methods;dense clusters empirically;clusters empirically;clusters empirically observed;kernel methods particularly;kernel methods;data analysis kernel;clustering practically significant;clustering widely;clustering widely used;discriminating power kernel;clustering practically;small dense clusters;analysis kernel;dense clusters;clustering;clusters;empirically observed shi;kernel;power kernel;empirically observed;observed shi malik;empirically;background clustering widely;bias small dense;data analysis;widely used data", "pdf_keywords": ""}, "0222a48657d554b2a5a3d7ec3bb0b6833b8970a1": {"ta_keywords": "hemorrhagic thrombocytopenia hit;thrombocytopenia hit methodstwo;suspected hemorrhagic thrombocytopenia;hemorrhagic thrombocytopenia;thrombocytopenia hit;thrombocytopenia;patients suspected hemorrhagic;suspected hemorrhagic;blind antibody test;score blind antibody;antibody test results;serum plasma samples;hemorrhagic;assay large prospective;results obtained serum;antibody test;performances rapid assay;obtained serum plasma;rapid assay;blind antibody;serum plasma;cohort patients suspected;assay;patients suspected;rapid assay large;obtained serum;pretest probability hit;assay large;4t9s score blind;using 4t9s score", "pdf_keywords": ""}, "8c5ba1c914eab16b705da03352fe69d5bcfc72ea": {"ta_keywords": "abstractive summarization models;abstractive text summarization;abstractive summarization;summarization models suffer;summarization models;techniques abstractive summarization;text summarization;summarization aims compressing;text summarization aims;summarization;abstractive text;summarization aims;rephrased condensed summary;document rephrased condensed;condensed summary;long source document;corpora limited abstractiveness;summary despite advances;training corpora;compressing information long;abstractiveness optimized;condensed summary despite;style training corpora;corpora limited;training corpora limited;abstractiveness optimized copying;document rephrased;limited abstractiveness optimized;corpora;abstractive", "pdf_keywords": ""}, "7fc0097f6a51282dc1e9020d7c28e12cecaef519": {"ta_keywords": "phc guided;based phc guided;phc;phc guided guided;phc based phc;phc based;patient history phc;based phc;history phc;history phc based;guided guided guided;guided guided;case patient;present case patient;patient history;guided;patient;case patient history;purpose article present;article present case;article present;purpose article;article;purpose;present case;based;present;case;history", "pdf_keywords": ""}, "3cfb319689f06bf04c2e28399361f414ca32c4b3": {"ta_keywords": "transfer learning techniques;transfer learning;introductiontranslational learning;introductiontranslational learning model;learning techniques nl;transfer learning given;effectiveness transfer learning;nl effectiveness transfer;language processing nl;processing nl effectiveness;processing nl;techniques nl introducing;introductiontranslational;learning model pre;natural language processing;nl introducing unified;learning;downstream task emerged;technique natural language;language processing;techniques nl;downstream task;landscape transfer learning;transfer;nl introducing;natural language;converts language;converts language problem;learning techniques;learning model", "pdf_keywords": "transfer learning pipeline;transfer learning techniques;transfer learning significantly;transfer learning;approach transfer learning;transfer learning natural;transfer learning limitations;transfer learning non;transfer learning used;limitations transfer learning;transfer learning use;transfer learning uses;transfer learning fundamental;transfer learning allows;transfer learning neural;methods transfer learning;text text transfer;framework transfer learning;model transfer learning;method transfer learning;transfer learning self;transfer learning identify;text framework transfer;neural transfer learning;text transfer;study transfer learning;component transfer learning;transfer learning given;learning limitations transfer;transfer learning multi"}, "d19e097f388ca12ff111989b2bac7d3cc3cf15ca": {"ta_keywords": "coordinatinged disinformation campaigns;uncover coordinated messaging;coordinated messaging analysis;riots parler coordinatinged;disinformation campaigns used;disinformation campaigns;coordinated messaging;messaging analysis;parler coordinatinged disinformation;user coordination network;messaging analysis user;narratives capitol riots;user coordination;riots parler;user totext graph;capitol riots parler;influence social media;user parleys parler;coordinatinged disinformation;coordination network graph;leading offline violence;graph text;user user coordination;totext graph;offline violence methodsin;totext graph text;social media;campaigns used influence;methodology uncover coordinated;offline violence", "pdf_keywords": "parleys social media;social network parler;riot analyze textual;content web parler;user parleys parler;web parler;usage patterns parler;user parleys social;free speech social;patterns parler users;web parler used;parler used parsing;capitol riot analyze;parler data analyze;parler users;parler users compared;parleys users actively;social media usage;riot analyze;backgroundthe free speech;behaviors web parler;parleys users;data parler;parse useful;use data parler;free speech;parler data;used data parler;textual messages users;parse"}, "2c9158e20f58df04a6c5cd54dd3ee7d8df656421": {"ta_keywords": "ranking signals methodsflexneuart;classic neural ranking;purely sparse representations;neural ranking signals;sparse representations;neural ranking;sparse representations weights;dense sparse representations;ranking signals;nmlc resultsin retrieval;mixing classic neural;representations weights learned;resultsin retrieval;retrieval systems;purely sparse;resultsin retrieval systems;retrieval;signals methodsflexneuart efficiently;methodsflexneuart efficiently;methodsflexneuart efficiently retrieve;sparse;retrieval systems work;neural;mixed dense sparse;learned training data;work purely sparse;dense sparse;ranking;introductionflexneuart;representations", "pdf_keywords": "efficient retrieval sparse;retrieval sparse;efficient retrieval;similarity searches;enable efficient retrieval;retrieval sparse sparse;resultsin retrieval systems;similarity searches metric;text retrieval;retrieval toolkit;retrieval systems;methods similarity searches;retrieval results;data retrieval;classical multistage retrieval;multistage retrieval;resultsin retrieval;retrieval toolkit integrates;multi stage retrieval;nasopharyngeal search reranking;information retrieval;modular text retrieval;library resultsin retrieval;retrieval results survey;methods data retrieval;retrieval large;retrieval;neural ranking;classic neural ranking;nn search library"}, "d864944df8e765d597484ace12dbc3ac99e950a9": {"ta_keywords": "proximal policy optimization;policy gradient algorithms;modern policy gradient;policy optimization;algorithms proximal policy;policy gradient;policy optimization ppo;estimation outlier rich;gradient algorithms proximal;robust statistics;heavy tailed methods;estimation outlier;robust statistics commonly;techniques robust statistics;outlier rich heavy;loss clipping gradient;outlier rich;used estimation outlier;gradient algorithms;proximal policy;gradient clipping ensure;outlier;optimization ppo;robust;heuristics including loss;optimization ppo rely;clipping gradient;tailed methods;learning heuristics;gradient clipping", "pdf_keywords": "rewards policy gradient;optimization surrogate reward;optimizes surrogate reward;policy optimization heavy;furtherproximal policy optimization;demonstrateproximal policy optimization;proximal policy optimization;policy optimization surrogate;1proximal policy optimization;policy gradients optimization;gradients proximal policy;policy gradient optimization;policy optimization powerful;based policy gradients;policy gradient importance;policy gradients importance;policy gradients heavy;policy optimization significantly;heavy tailedness reward;policy gradients;tailedness reward reward;optimization based policy;policy optimization;surrogate reward function;policy gradient;tailedness reward;policy optimization pp;optimizing cumulative rewards;stochasticity policy gradients;policy optimization based"}, "e6239cc789da289929d49ffed2c0a562213d4703": {"ta_keywords": "stresses distortions titanium;stiffened cylindrical shell;alloy ring stiffened;residual stress deformation;ring stiffened cylindrical;fillet welding influence;fillet welding;titanium alloy ring;analysis residual stresses;distortions titanium alloy;residual stresses distortions;welding position constraint;ring stiffened;residual stresses;caused fillet welding;stress deformation analyzed;stiffened cylindrical;cylindrical shell methods;simulate residual stress;titanium alloy;cylindrical shell;weld heat treatment;deformation caused fillet;welding position;stress deformation;alloy ring;welding influence welding;distortions titanium;welding influence;wht residual stress", "pdf_keywords": ""}, "60e339d25d43c026cf96395aa8accf34eae744a5": {"ta_keywords": "dataset evaluating pairwise;pairs annotated crowdsourcing;evaluating pairwise comparisons;crowdsourcing platform dataset;imb wiki dataset;dataset built compare;pairwise comparisons;dataset evaluating;pairs annotated;truth dataset;wiki dataset;annotated crowdsourcing;annotated crowdsourcing platform;pairwise comparisons contains;crowdsourcing platform;truth dataset built;crowdsourcing;dataset ground truth;comparisons;dataset balanced;large scale dataset;ground truth dataset;wiki dataset ground;evaluating pairwise;imb widiki;imb widiki sbs;built compare baseline;249 pairs annotated;known imb wiki;comparisons contains", "pdf_keywords": "crowdsourced pairwise comparisons;dataset crowdsourced pairwise;pairwise comparisons dataset;crowdsourced pairwise;dataset evaluating pairwise;aggregation pairwise comparisons;graph pairwise comparisons;evaluating pairwise comparisons;pairwise comparisons generate;pairs annotated crowdsourcing;pairwise comparison aggregation;pairwise comparisons;comparisons dataset;pairwise comparisons using;open dataset crowdsourced;pairwise comparisons used;dataset crowdsourced;crowdsourcing platform dataset;comparisons dataset demonstrate;dataset aggregation pairwise;pairwise comparison;simple pairwise comparison;dataset built compare;performance pairwise comparisons;pairwise comparisons contains;evaluating pairwise;comparison aggregation;pairs annotated;new dataset aggregation;large scale dataset"}, "4ef77ef9b8ca8c8a96f1e62fa86a988feb582bd1": {"ta_keywords": "domain adaptation neural;offs domain adaptation;domain adaptation;adaptation neural language;neural language models;language models;adaptation neural;small domain set;outof domain set;language models aimthe;domain set small;language models methodswe;domain set derive;domain set;set small domain;neural language;trade offs domain;large outof domain;training model set;outof domain;adaptation;small domain;offs domain;training model;model set depends;sets distance underlying;model set;neural;domain;set depends", "pdf_keywords": "language model adaptation;language models learning;neural language models;neural language modeling;learning generalization loss;adaptation neural language;domain adaptation neural;language models;likelihood training;domain training distribution;setneural language models;domain adaptation;language modeling focus;log likelihood training;summarization size training;language modeling;based language models;language models important;language modeling important;domain training;adaptation concepts machine;empirical risk neural;language modeling fundamental;learning learning generalization;models learning learning;language models used;model adaptation;domain adaptation achieved;models learning;best likelihood neural"}, "09ec8d8e2251e079abb0e109979f33ee120211fa": {"ta_keywords": "proximal extragradient method;extragradient method;hybrid proximal extragradient;extragradient method 2013;proximal extragradient;svaiter method optimal;smooth convex optimization;monteiro svaiter method;convex optimization;accelerated hybrid proximal;convex optimization problems;methods using gradient;method optimal;extragradient;svaiter method;approximate solution auxiliary;newton method;gradient andhessian evaluations;newton method used;using gradient andhessian;svaiter accelerated hybrid;hybrid proximal;method optimal respect;gradient andhessian;monteiro svaiter accelerated;iteration approximate solution;step newton method;optimization problems class;methodsthe monteiro svaiter;optimization", "pdf_keywords": ""}, "a8a863e85a95919773868204d672f1260e0058ce": {"ta_keywords": "neural machine translation;machine translation models;translation models;translation models enables;machine translation instead;universal model translate;machine translation;model translate;model translate multiple;translate multiple languages;language specific parameterization;translate multiple;standard neural machine;existing neural machine;architecture standard neural;translation instead;modification existing neural;single universal model;translation instead introduces;neural machine;standard neural;translation;multiple languages;universal model;existing neural;multiple languages allowing;translate;model architecture standard;language specific;parameterization propose simple", "pdf_keywords": "multilingual translation mechanism;nonlingual translation systems;multilingual nonlingual translation;nonlingual translation nm;neural machine translation;multiway multilingual translation;multilingual translation;introductionneuro machine translation;multilingual translation using;framework multilingual translation;multilingual translation propose;machine translation models;translation models;nonlingual nonlingual translation;translation systems modular;nonlingual translation;translation systems;approach multilingual translation;machine translation novel;machine translation;translation systems introduce;translation models enables;machine translation propose;multilingual translation demonstrate;multilingual nonlingual;parameter generator translation;framework machine translation;multilingual nonlingual nonlingual;machine translation nmr;translation difficult implement"}, "d82592f3a110308366dfc7c42565d437b5bf59af": {"ta_keywords": "automated social skills;training developing dialogue;social skills training;developing dialogue;social skills trainer;dialogue named automated;context social skills;skills training human;social interaction;process social skills;automated social;social skills;developing dialogue named;provides social skills;human computer interaction;interaction appropriate skills;training human computer;user speech;automate process social;social interaction appropriate;named automated social;user speech language;training human;dialogue;recognizes user speech;computer interaction;discomfort social interaction;skills trainer provides;trainer provides social;skills trainer", "pdf_keywords": ""}, "5dab371fecc43904c0b785a50136d20cee43a99a": {"ta_keywords": "translation trained parallel;speech machine translation;machine translation trained;corpus translated speech;translated speech experiments;speech translation;translation trained;speech translation traditionally;machine translation;translated speech;transcribed speech machine;trained parallel texts;trained corpus transcribed;corpus transcribed speech;trained corpus;recognizer trained corpus;speech recognizer trained;transcribed speech;introduction speech translation;speech recognizer;corpus transcribed;corpus translated;speech machine;models consisting speech;consisting speech recognizer;trained parallel;corpus;translation traditionally;parallel texts;approached cascaded models", "pdf_keywords": "models speech translation;model speech translation;machine translation trained;mapping speech translation;translation speech recognition;translation trained parallel;speech machine translation;approaches speech translation;machine translation speech;translation data training;speech translation development;speech translation language;translation trained;systems speech translation;speech translation increases;approach speech translation;recognition machine translation;speech translation fundamental;speech translation complex;speech translation translation;sequence models audio;neural machine translation;speech translation;automatic mapping speech;speech recognizer trained;auxiliary speech translation;task training speech;training speech recognition;trained corpus transcribed;translation translation data"}, "461188735d46dc1062f5d1d382d940a24c355fad": {"ta_keywords": "extraction commonsense locatednear;automatic extraction commonsense;extraction commonsense;commonsense locatednear knowledge;automatically extract relationship;extract relationship sentence;commonsense knowledge describing;relation classifier;relation classifier aggregating;commonsense knowledge;extract relationship;entity pairs;commonsense locatednear;sentence level relation;kind commonsense knowledge;pairs large corpus;large corpus methods;knowledge kind commonsense;introduction automatic extraction;corpus methods;scores entity pairs;corpus methods released;entity pairs large;corpus;level relation classifier;large corpus;knowledge describing;locatednear knowledge;relationship sentence level;automatic extraction", "pdf_keywords": ""}, "f6be5d90199d1644b85e6b41a7a7f42fb29dbc9a": {"ta_keywords": "school children object;purposechildren object structure;children object structure;purposechildren object;spatial ability;spatial ability general;children object;purposechildren;object structure;object structure perspective;probably spatial ability;taking osp ability;instruction 12 curriculum;curricula address spatial;school children;osp ability;12 curriculum;perspective taking osp;structure perspective taking;spatial;skills novel tasks;structure perspective;elementary school children;general skills;specific skill debate;individuals ability;transfer general skills;curriculum;ability recommended important;12 curriculum school", "pdf_keywords": ""}, "a278c07c8bd2921e59dd862cd91a0540dd340030": {"ta_keywords": "estimation treatment effects;estimation treatment;treatment effects observational;adapting neural networks;treatment propensity;adapting neural;treatment propensity score;probability treatment propensity;introduction adapting neural;effects observational data;treatment effects;effects observational;networks estimation treatment;adapting;neural networks estimation;methods generalized estimation;outcome probability treatment;models downstream estimator;generalized estimation;propensity score unit;introduction adapting;observational data methods;effect results data;propensity score;downstream estimator effect;fitted models downstream;probability treatment;downstream estimator;data methods generalized;propensity", "pdf_keywords": "regularization procedure causal;causal inference observational;causal inference dragonnet;causal inference large;neural network treatment;estimate causal;causal inference fundamental;causal inference methodswe;want estimate causal;causal inference;estimate causal effect;estimator targeted regularization;tool causal inference;regularization methods neural;analysis causal inference;procedure causal inference;model target regularization;consider causal inference;step causal inference;model targeted regularization;causal inference application;neural network training;regularization predict treatment;regularization improves estimation;inference observational;models downstream estimator;target regularization;datasets initial causal;neural networks fit;target regularization objective"}, "9c5c794094fbf5da8c48df5c3242615dc0b1d245": {"ta_keywords": "unsupervised learning disentangled;learning disentangled representations;learning disentangled;disentangled representations fundamentally;disentangled representations;disentangled representations real;unsupervised learning;theoretically unsupervised learning;recovered unsupervised learning;disentangled;idea unsupervised learning;unsupervised learning algorithms;variation recovered unsupervised;representations fundamentally impossible;theoretically unsupervised;representations fundamentally;idea unsupervised;key idea unsupervised;representations real world;unsupervised;impossible inductive biases;representations;recovered unsupervised;inductive biases;variation recovered;representations real;methodswe theoretically unsupervised;factors variation recovered;learning;learning algorithms paper", "pdf_keywords": "unsupervised learning disentangled;unsupervised disentanglement learning;learning disentangled models;learning disentangled representations;disentangled representations learning;disentangled representations unsupervised;representations learning disentangled;model learning disentangled;disentangled representation learning;automatic learning disentangled;learning disentangled;disentanglement learning impossible;disentanglement learning;method disentanglement learning;disentanglement learning inductive;reliably learn disentangled;disentangled representations challenging;disentangled representations method;disentanglement learning fundamental;methods unsupervised disentanglement;disentangled models;disentanglement learning using;learning disentangled dseminated;analysis disentangled representations;disentangled representations;learning disentangled dsequences;disentangled representations fundamentally"}, "2a2d03a1534b365c5b048c824c0886e16ccf7dfa": {"ta_keywords": "extracting relational knowledge;relational knowledge text;reading relational information;extracting relational;freebase reading relational;syntactic patterns extraction;syntactic semantic inference;relations knowledge base;learnd syntactic semantic;knowledge text;web learnd syntactic;semantic inference rules;knowledge base freebase;large knowledge base;knowledge base;reading relational;web text corpus;studies extracting relational;semantic inference;knowledge relations knowledge;knowledge text potential;knowledge relations;syntactic semantic;background knowledge relations;relational knowledge;introduction reading web;reading web learnd;freebase reading;relations knowledge;text corpus", "pdf_keywords": ""}, "535c58ec8020782d41ed3ca72cf94aff7fd65120": {"ta_keywords": "speech recognition fundamental;recognition speech recognition;speech recognition speech;recognition speech;speech recognition;process speech recognition;step speech recognition;speech recognition recently;reported speech recognition;fundamental process speech;process speech;recognition fundamental process;recognition fundamental step;recognition fundamental;fundamental step speech;step speech;recognition;speech;recognition recently;reported speech;recognition recently reported;fundamental step;fundamental process;process;step;recently reported speech;fundamental;reported;recently reported;recently", "pdf_keywords": ""}, "4c93bcc05d6b41cb3df451d703f34bab9c9e9201": {"ta_keywords": "differentially private text;differential privacy;add privacy preserving;private text generation;form differential privacy;privacy preserving;differential privacy provide;quantifiable privacy;privacy provide guarantees;private text;quantifiable privacy guarantees;add privacy;designing differentially private;ensuring quantifiable privacy;privacy guarantees provides;privacy guarantees;privacy preserving noise;privacy;differentially private;mechanisms add privacy;privacy provide;maintaining user trust;user trust methodsa;guarantees text queries;learning user data;user trust;provide guarantees text;user data ensuring;introductionaccurately learning user;trust methodsa", "pdf_keywords": "privacy preserving metric;erential privacy;privacy extend word;learning implications privacy;privacy preserving;privacy utility text;di erential privacy;add privacy preserving;erential privacy extend;privacy text fundamental;privacy preserving noise;allows privacy preserving;erential privacy provide;privacy text;inthe privacy text;challenges designing privacy;privacy utility;method privacy preserving;similarthe privacy framework;new approach privacy;privacy using;protecting privacy;new method privacy;optimal inthe privacy;sensitivity similarthe privacy;mechanism introduce privacy;privacy;privacy framework euclidean;designing privacy;similarthe privacy"}, "96abcdded2985bd44b9514e28f5b8da4fa1e4371": {"ta_keywords": "interpretability machine learning;interpretability important;concept interpretability important;interpretability machine;interpretability;concept interpretability;understand concept interpretability;concept interpretability machine;interpretability important slippery;machine learning important;important understand concept;learning important understand;machine learning;understand concept;slippery machine learning;learning important;important understand;learning;understand;important;important slippery;concept;important slippery machine;machine;slippery machine;slippery", "pdf_keywords": ""}, "18e8646001fc53465fdc8f8eb01523e24c134493": {"ta_keywords": "miscalibrations popular approach;people miscalibrations typically;people miscalibrations;miscalibrations typically;poorly people miscalibrations;miscalibrations typically far;miscalibrations popular;simplistic models miscalibration;miscalibrations;suffer miscalibrations;miscalibration linear biases;models miscalibration;miscalibration;biases bias scores;suffer miscalibrations popular;known suffer miscalibrations;bias scores;bias scores approach;models miscalibration linear;cardinal scores collected;biases bias;biases;cardinal scores;miscalibration linear;background cardinal scores;bias;scores collected people;linear biases bias;linear biases;scores approach fares", "pdf_keywords": ""}, "f78e5aaf34cc1e4874490e9155c640b73c630021": {"ta_keywords": "learning incorporates opponent;multi agent learning;strategic multi agent;backgroundgradient conjectures strategic;agent learning objectivewe;agent learning;strategy belief;learning games;strategy belief opposing;update strategy belief;dynamic behavior opponents;behavior opponents optimize;general sum games;incorporates opponent behavior;opponents optimize;conjectures strategic;learning games primarily;conjectures strategic multi;opponent behavior continuous;multi agent;opponent behavior;sum games;games methodsin;sum games methodsin;strategy;behavior opponents;agents model dynamic;opposing players;backgroundgradient conjectures;strategic", "pdf_keywords": ""}, "f132f6534ec326a1ba61870b701015cd3d1560a2": {"ta_keywords": "domain adaptation neural;domain adaptation;introduction domain adaptation;domain generalization training;target domain generalization;selection language modeling;improves target domain;generalization training pretraining;domain generalization;pretraining selected data;modeling machine translation;data selection improves;sample target domain;machine translation experiments;tuning data selection;pretraining selected;target domain data;adaptation neural networks;selection language;pretraining data;training pretraining data;selected data training;data selection language;generalization training;adaptation neural;machine translation;selection fine tuning;translation experiments assess;domain data methods;pretraining data identified", "pdf_keywords": "domain adaptation;domain generalization training;domain adaptation demonstrate;models domain adaptation;methods domain adaptation;machine translation training;method domain adaptation;domain adaptation constrastive;introductiondomain adaptation neural;domain adaptation fundamental;classifiers machine translation;neural machine translation;target domain generalization;modeling machine translation;translation training;selection language modeling;translation training development;machine translation experiments;adaptation demonstrate domain;article domain adaptation;learning models domain;domain classifiers;machine translation;machine translation conditions;translation training effectiveness;demonstrate domain classifiers;domain classifiers outperform;translation conditions classifier;domain generalization;improves target domain"}, "835587dbe94b70adeb0b16384e10bb6e0e29de84": {"ta_keywords": "physician diagnosis treatment;diagnosis treatment patient;physician diagnosis;role physician diagnosis;patient disease;treatment patient disease;diagnosis treatment;literature role physician;treatment patient;role physician;physician;diagnosis;disease;patient;treatment;article present literature;literature role;present literature role;article present;article;role;present literature;purpose article present;purpose article;literature;purpose;present", "pdf_keywords": ""}, "97bcea32979ed602fd404448a4e4cedad4171d79": {"ta_keywords": "generate nonverbal behaviors;nonverbal behaviors body;personality virtual agent;individual nonverbal behaviors;generating individual nonverbal;nonverbal behaviors;nonverbal behaviors match;expected personality virtual;personality virtual;nonverbal behaviors spoken;generate nonverbal;nonverbal behavior;automatically generate nonverbal;nonverbal behavior varies;individual nonverbal;language nonverbal behavior;important generate nonverbal;spoken language nonverbal;nonverbal;language nonverbal;match expected personality;personality important generate;natural looking virtual;looking virtual agents;expected personality;virtual agents;personality;behaviors body;eye gaze arms;virtual agent", "pdf_keywords": ""}, "89c148d3d4edcb7b13c35da36b97ffb881c38058": {"ta_keywords": "mel speech enhancement;electrolaryngeal el speech;laryngectomees produce electrolaryngeal;speech enhancement;sounds enable laryngectomees;electrolarynx device artificially;speech enhancement methods;enable laryngectomees produce;electrolarynx device;enable laryngectomees;introductionan electrolarynx device;produce electrolaryngeal el;speech proficient laryngetomees;electrolarynx;introductionan electrolarynx;produce electrolaryngeal;proficient laryngetomees produce;proposed mel speech;proficient laryngetomees;electrolaryngeal el;el speech sounds;laryngectomees produce;laryngectomees;excitation sounds enable;mel speech;intelligible el speech;laryngetomees produce quite;laryngetomees produce;generates excitation sounds;laryngetomees", "pdf_keywords": ""}, "b8dcc2ae3346e41a421232169c2ca07957c654c4": {"ta_keywords": "evolutionary game theory;evolutionary game;agents static games;games play evolve;play evolve strategically;dynamic agents;evolve strategically;dynamic agents interact;evolve strategically time;paradigm evolutionary game;dynamic agents static;game theory;static games introduce;online learning games;static games;play evolve;population dynamic agents;agents games;static game;game theory generally;games introduce analyze;learning games;agents games play;divide dynamic agents;learning games based;evolutionary;agents static;competitive settings agents;evolve;settings agents games", "pdf_keywords": ""}, "135ace829b6ad2ec9db040d8e5fd137034e83665": {"ta_keywords": "conditional random fields;semi markov chains;semi markov conditional;semi markov;introduction semi markov;random fields semi;version semi markov;segmentation labels;cfs conditionally trained;markov conditional random;sequence outputs segmentation;conditionally trained;outputs segmentation labels;markov conditional;markov chains;semi cfs conditionally;markov;segmentation labels assigned;outputs segmentation;random fields;segmentation;markov chains methods;fields semi cfs;conditionally trained version;semi cf;intuitively semi cf;fields semi;trained version semi;semi cfs;labels assigned segments", "pdf_keywords": ""}, "635932ee917d71e01f07211c0359abf3e0e65e47": {"ta_keywords": "speech recognition;connectionist temporal classification;automatic speech recognition;automatic speech;speech recognition ar;sequence connectionist temporal;field automatic speech;recognition ar endotoend;end models;end end models;endotoend models;sequence connectionist;temporal classification cc;endotoend models proposed;end models gained;right decoding;temporal classification;ar endotoend models;left right decoding;token sequence connectionist;connectionist temporal;endotoend;right decoding consider;end end;decoding;right autoregressive generation;decoding consider future;left right autoregressive;ar endotoend;autoregressive generation output", "pdf_keywords": "temporal classification insertion;connectionist temporal classification;automatic speech;sequence connectionist temporal;approach automatic speech;speech recognition;modeling connectionist temporal;automatic speech recognition;classification insertion based;field automatic speech;speech recognition ar;temporal classification cc;application speech recognition;sequence connectionist;speech speech processing;classification insertion;token sequence connectionist;speech processing;training insertionbased model;recognition application speech;joint training insertionbased;classification probability insertion;corpus speech recognition;application speech speech;temporal classification;insertion based models;output token sequences;speech recognition application;insertion based output;training insertionbased"}, "b30195763eb103e2e5564228119f3810ab423b2e": {"ta_keywords": "context text classification;text classification;labeled documents training;text classification methods;text classification challenging;current text classification;human labeled documents;labeled documents;labeled examples based;classification;context text;describing categories classified;perform classification;classification challenging task;train classification;classification challenging;label class train;class train classification;classification seeing labeled;categories classified;documents training data;labeled examples;classification methods;words describing categories;label class;human labeled;seeing labeled examples;labeled;classification methods typically;humans perform classification", "pdf_keywords": "weakly supervised text;weaklysupervised text classification;neural text classification;weakly supervised classification;document weakly supervised;supervised text;training unlabeled corpus;introduce weakly supervised;models text classification;predict category words;supervised text classification;word level classification;weakly supervised;based weakly supervised;trained words predict;labeled documents training;text classification;model trained words;classification method weaklysupervised;classification domains text;words predict categories;backgroundcurrent text classification;weaklysupervised text;model predict words;text classification model;unlabeled corpus soft;sets text classification;category supervision training;document level classification;trained linguistic information"}, "002c256d30d6be4b23d365a8de8ae0e67e4c9641": {"ta_keywords": "language modelling;language modelling retrieving;language modelling article;approach language modelling;task language modelling;trillions tokens;tokens;retrieving trillions tokens;manipulate text;task language;manipulate text important;approach language;ability manipulate text;text important task;modelling retrieving trillions;important task language;modelling retrieving;language;new approach language;text;modelling;modelling article;modelling article present;retrieving trillions;trillions;text important;manipulate;ability manipulate;results new approach;task", "pdf_keywords": "large language models;language learning massiveetext;large scale languagethe;improving language models;improved retrieval trillions;enhancing retrieval trillions;massiveetext training retrieval;scaling language models;language models improve;language models retrieving;retrieval trillions tokens;augment language models;language models improved;massiveetext training retrievalwe;better language modelling;use large language;large language;enhancing models retrieval;language models useful;language modelling benchmarks;significantly improved retrieval;enhancing language models;models introduce retrieval;retrieval model outperforms;improve language modelling;language modelling performance;models demonstrate retrieval;language models increasing;retrieval trillions;improved retrieval"}, "933396f5b9111f6acdd76710ee6ab4d24e8673dd": {"ta_keywords": "speech text datasets;unpaired speech text;speech text data;paired speech text;end automatic speech;novel semi supervised;automatic speech;semi supervised;automatic speech recognition;speech recognition ar;create paired speech;large unpaired speech;encoder network autoencoding;representation speech text;unpaired speech;speech text;paired speech;autoencoding text data;speech recognition;methods semi supervised;semi supervised method;network autoencoding text;autoencoding text;shared encoder;intermediate representation speech;data shared encoder;autoencoding;using shared encoder;shared encoder network;encoder", "pdf_keywords": ""}, "4c49e9b57c8fc58b0df29a27ecca8cc2e376f02b": {"ta_keywords": "collaborative filtering fc;collaborative filtering;useful collaborative filtering;data useful collaborative;useful collaborative;web lists semantically;semantically related entities;users fc spider;collects web lists;collaborative;web spider methodsin;using autonomous web;spider collects web;autonomous web;related entities lists;fc spider collects;autonomous web spider;lists semantically related;entities recommended;filtering;entities recommended new;collects web;web lists;spider methodsin fc;web spider;related entities;fc spider;spider collects;filtering fc;spider methodsin", "pdf_keywords": ""}, "7d5f83cb234a5640487bd258ace06d9dc967d222": {"ta_keywords": "statistical relational learning;information extraction reasoning;relational learning approach;relational learning;extraction reasoning scaleable;information extraction;joint information extraction;extraction reasoning;information extraction errors;statistical relational;scaleable statistical relational;relational learning involves;knowledge base;learning reasoning;constructs knowledge base;pipeline statistical relational;knowledge base kb;learning reasoning tasks;reasoning scaleable statistical;probabilistic order logics;performs learning reasoning;issue information extraction;propagate reasoning task;reasoning tasks using;relational;extraction errors text;reasoning scaleable;tasks using probabilistic;introduction joint information;reasoning tasks", "pdf_keywords": ""}, "f1c7419b87cbf853e691e500643f71720b68fb86": {"ta_keywords": "collective classification methods;collective classification widely;collective classification;existing collective classification;background collective classification;predict class labels;graphs related instances;data existing collective;graphical models learning;labels simultaneously relational;methods existing collective;expensive iterative inference;large graphs;classification widely;maintaining large graphs;iterative inference graphical;large graphs related;classification;class labels simultaneously;class labels;classification widely studied;iterative inference;collective;dataset large cost;inference graphical models;classification methods usually;classification methods;graphs;iterative optimization dataset;predict class", "pdf_keywords": ""}, "9b3fd2525a2d1abc44145308e013f117d3d7bdee": {"ta_keywords": "control electrolarynx based;fi0 control electrolarynx;electrolarynx device artificially;electrolarynx based;control electrolarynx;methodsan electrolarynx device;electrolarynx device;simulation methodsan electrolarynx;electrolarynx based statistical;electrolaryngeal el speech;methodsan electrolarynx;electrolarynx;laryngectomees produce electrolaryngeal;sounds enable laryngectomees;enable laryngectomees produce;enable laryngectomees;electrolaryngeal el;produce electrolaryngeal el;generates excitation sounds;excitation sounds enable;speech proficient laryngectomees;produce electrolaryngeal;electrolaryngeal;proficient laryngectomees produce;excitation feature;excitation sounds;proficient laryngectomees;laryngectomees;introductiondirect fi0 control;excitation feature prediction", "pdf_keywords": ""}, "1afa3ab80abda57920b8d456a6513e6f01cc82e7": {"ta_keywords": "survival text regression;predicting length conversation;text regression time;event prediction tasks;backgroundtime event prediction;develop survival text;conversation modelling applications;text regression;conversation modelling;survival text;event prediction;prediction tasks common;common conversation modelling;time event;backgroundtime event;length conversation;tasks common conversation;length conversation user;prediction tasks;time event greater;backgroundtime;natural frame predictions;regression tasks recent;predictions regression tasks;frame predictions;determining time event;conversation;tasks determining time;event;classification tasks", "pdf_keywords": ""}, "5cfdb162ffa4dce18f7c576d352bd459b6a11292": {"ta_keywords": "based thirdparty coupon;thirdparty coupon;thirdparty coupon provider;discount coupons;limited discount coupons;time limited promotions;discount coupons partner;coupon provider;coupon;time limited discount;coupons;promotions exploit consumers;coupons partner;timing duration promotion;urgency boost sales;promotions;limited promotions;promotions exploit;limited promotions exploit;discount;coupons partner large;duration promotion;limited discount;duration promotion increase;boost sales;delivering time limited;consumers sense urgency;delivering time;commission based thirdparty;promotion", "pdf_keywords": ""}, "1ddcc9671dd6486e34cefadfe71bbbc1bc55035a": {"ta_keywords": "improve nontranscriptional translation;word based nontranslation;introductionneural machine translation;nontranslation model pretrained;translation lower resource;machine translation;nontranscriptional translation lower;nontranscriptional translation;machine translation achieved;pretrained word embeddings;model pretrained word;pretrained word;translation lower;quality word embeddings;translation achieved;word embeddings trained;embeddings rare words;nontranslation model;nontranslation;based nontranslation;word embeddings rare;based nontranslation model;word embeddings;lower quality word;translation achieved impressive;trained standard word;initialize word based;initialize word;introduce word;word level", "pdf_keywords": ""}, "33d2ebe41477811296abc4077bf9ce09b927ef98": {"ta_keywords": "statistical voice conversion;voice conversion;methods voice conversion;voice conversion based;voice conversion effectively;voice conversion studies;framework voice conversion;statistical voice;speaker model methods;density model speaker;introduction statistical voice;transform function speakers;conversion based noisy;model speaker model;model speaker;speaker model;voice;conversion based;conversion effectively using;conversion effectively;conversion studies approaches;gassian mixture model;mixture model;source target speakers;target speakers widely;speakers widely;speaker;function speakers;mixture model gm;methods voice", "pdf_keywords": ""}, "c0e8846eb5ce574a6dca3f3a600e82b184339254": {"ta_keywords": "flight language conveys;language conveys information;flight language;flight maps actions;language conveys;selecting flight language;instruction following language;maps actions;language;methodswe present model;actions selecting flight;conveys information;actions new contexts;language like;reward function;actions;desirable actions;underlying reward function;reward function general;following language like;user underlying reward;language like like;desirable actions new;conveys information user;underlying reward;carry desirable actions;jet blue flight;new contexts methodswe;following language;methodswe present", "pdf_keywords": "utterances elicit reward;reward descriptive utterances;utterances provides rewards;language infer reward;interactions utterance conveys;natural language flight;utterances action flight;utterance conveys action;models language utterances;language conveys information;utterances elicit;language utterances convey;actions language;utterances action;language conveys;pragmatically speakers communicate;choose utterances elicit;utterances convey explicit;reward given utterance;human language;utterances useful interpreting;actions language fundamental;descriptive utterances action;demonstrate utterances useful;language utterances;utterances useful;reasoning pragmatically speakers;interactions utterance;human language recruited;produce actions language"}, "a3cc75975a5998d5a7dd494e70a479ba0a550013": {"ta_keywords": "development intelligent tutoring;tutoring systems article;tutoring user approach;solutions tutoring user;intelligent tutoring systems;tutoring systems;tutoring user;intelligent tutoring;solutions tutoring;demonstrating solutions tutoring;tutoring;teach user;author teach user;methods author teach;teach user demonstrating;user demonstrating solutions;authoring process;user approach author;author teach;authoring process process;user approach;user demonstrating;authoring;development intelligent;approach author;approach author gives;process used guide;provides feedback steps;demonstrating solutions;teach", "pdf_keywords": ""}, "04f4e55e14150b7c48b0287ba77c7443df76ed45": {"ta_keywords": "question answering;ai systems learn;question answering abstract;learn reliably answer;progress question answering;natural language understanding;bias ai;ai;reporting bias ai;systems learn;natural language;ai systems;today natural language;bias ai systems;answering abstract;language understanding systems;systems learn reliably;reliably answer physical;commonsense pose challenge;answering;language understanding;physical commonsense;food inherently;questions;reliably answer;commonsense pose;commonsense;understanding systems;challenge today natural;pretrained models binert", "pdf_keywords": "interaction question answering;models language understanding;world annotators tasks;annotators tasks solutions;pretrained language models;language understanding models;annotators tasks;physical commonsense understanding;real world annotators;dataset physical commonsense;models language;useful annotators;aicompleteness introduce physical;humanit annotators;commonsense concepts physical;natural language models;question answering;humanit annotators annotators;learning understanding;natural language understanding;solutions useful annotators;language models probe;world annotators;physical commonsense new;progress physical commonsense;language models best;language models language;dataset language models;language model powerful;humanit annotators provide"}, "5e0f2f82b4a28d59f1aa8b8ffe497790de1cdf9d": {"ta_keywords": "deep reinforcement learning;deep reinforcement;use deep reinforcement;reinforcement learning wild;agent avoid catastrophic;reinforcement learning;agents eventually forget;approximation agents eventually;deep network;reinforcement;approximation agents;function approximation agents;learning wild;popular deep network;environments popular deep;agent avoid;algorithm doomed sisyphean;learning wild hope;agents eventually;avoid catastrophic mistakes;deep network algorithm;hope agent avoid;learning;doomed sisyphean;avoid catastrophic;wild hope agent;use deep;catastrophic mistakes;doomed sisyphean curse;agent", "pdf_keywords": ""}, "7a9f4a8a99f9a38e9213da890f9eab6150ae928e": {"ta_keywords": "personalized pagerank known;personalized pagerank;based personalized pagerank;pagerank known;pagerank known random;learnable proximity measure;pagerank;learnable proximity;novel learnable proximity;graph databases recommendation;proximity measure based;graph databases;recommendation tasks;recommendation tasks inference;noisy knowledge bases;proximity measure;known random walk;databases recommendation tasks;querying graph databases;large noisy knowledge;measure based personalized;edge label sequence;knowledge bases;noisy knowledge;querying graph;edge label;edge label prior;proximity;random walk;weight edge label", "pdf_keywords": ""}, "ebaae38a09c5a4909049e16af759c71db9cc87dc": {"ta_keywords": "language inference reduces;words model;words model confidence;neural models;question answering;natural language inference;just words model;neural models important;inputs just words;question answering natural;words possible input;inference reduces inputs;answering natural language;language inference;natural language processing;neural;tools neural;removing words possible;language processing tasks;removing words;tools neural processing;introduction neural models;neural processing;important tools neural;language processing;methods question answering;introduction neural;inference reduces;neural processing common;tasks removing words", "pdf_keywords": ""}, "b17fa6625681d99370122145ba9911f701dd92cb": {"ta_keywords": "semantic parsing context;parsing context methods;parsing context;parsing context received;typical context modeling;world semantic parsing;decoding semantic parser;semantic parsing;context modeling;semantic parser;context modeling methods;semantic parser adapt;context methods present;context methods;decoding semantic;parsing;recently semantic parsing;study context modeling;contextual;challenging complex contextual;complex contextual;parser;context received;context;based decoding semantic;contextual phenomena;world semantic;grammar based decoding;real world semantic;contextual phenomena previous", "pdf_keywords": "semantic parsing context;parsing context concretely;parsing context existing;parsing context;effective semantic parsing;semantic parsing;world semantic parsing;decoding semantic parcontext;parsing context methodswe;decoding semantic parser;semantic parser;parsing context received;study semantic parsing;semantic parsing remains;queries context;tool semantic parsing;queries context independent;semantic parcontext;semantic parsing paper;semantic parser adapt;parsing assume queries;grammar based parser;modeling contextual clues;semantic parcontext important;studies semantic parsing;context processing;parsing;recently semantic parsing;assume queries context;semantic parsing assume"}, "bf4da952df7a6ef9c0b2be8b4b4b69ad63848b8f": {"ta_keywords": "traffic speed prediction;learning traffic speed;predict traffic speed;predict traffic;introductiontranslation learning traffic;learning traffic;historical traffic data;available predict traffic;learning intelligent transportation;speed prediction;sourcesfor traffic speed;speed prediction effective;traffic data;traffic speed;transfer learning framework;data sourcesfor traffic;transfer learning;traffic data available;sourcesfor traffic;propose transfer learning;historical traffic;traffic speed target;speed prediction benefit;intelligent transportation systems;traffic;prediction effective spatiotemporal;intelligent transportation;transportation systems existing;spatiotemporal features data;data available predict", "pdf_keywords": ""}, "879cd78b0d4413aef614bc6b6cce075e8e6ad4be": {"ta_keywords": "adversarial packet erasure;streaming codes;achieving streaming code;streaming code;streaming codes best;packet erasure channels;constructions adversarial packet;models streaming codes;adversarial packet;streaming code constructions;erasure channels;achieving streaming;live video streaming;streaming;capacity achieving streaming;packet erasure;video streaming;streaming applications;video streaming applications;bursts arbitrary erasures;code constructions adversarial;source packets;erasure channels include;source packets highly;packets fixed size;existing models streaming;streaming applications source;size live video;applications source packets;source packets fixed", "pdf_keywords": ""}, "bd318e959236b0d33a7567b6d3afc8d5e92b8ea3": {"ta_keywords": "ethical challenges ai;ai agents;ai agents aware;ai agents deployed;challenges ai agents;ai autonomous collaborating;humans raises ethical;ethical;ai autonomous;autonomous collaborating humans;appropriate ethical;follow appropriate ethical;ethical principles;appropriate ethical principles;ai life ai;life ai autonomous;ai;fairness virtues ethical;ethical principles exhibit;ai life;agents aware;deployment ai;virtues ethical principles;ethical challenges;virtues ethical;ethical principles define;challenges ai;life ai;deployment ai life;collaborating humans", "pdf_keywords": "build ethical ai;ethical ai systems;ethical challenges ai;ethical ai;ai systems ethical;ethical boundaries ai;ethical beneficial ai;ethically bounded ai;ai context ethical;ethical considerations artificial;challenges ai agentsin;ai agents;making ai;implemented ethically;model preferences ethical;inherent making ai;principles ethical humans;preferences ethical principles;ai life ai;ai autonomous collaborating;ethical humans;ai systems examples;ethical principles implemented;making ai robust;ai implications design;systems ethical boundaries;autonomous agents use;build ethical;ai autonomous;ethical humans recently"}, "4a93f7654f795871ed99dece2e1805e4950fd194": {"ta_keywords": "utterances spatial;understand utterances spatial;language space spatial;referents landmarks space;space spatial relations;utterances spatial configurations;latent semantic relations;spatial relations;space spatial;landmarks space;latent semantic;spatial;spatial relations rich;semantic structure;semantic relations;spatial configurations objects;representations latent semantic;landmarks space given;semantic relations referents;semantic;semantic structure develop;language space;abstract semantic structure;abstract semantic;relations referents landmarks;referents landmarks;learns understand utterances;objects tabletop scene;introductionthe language space;spatial configurations", "pdf_keywords": ""}, "9e172f35b2b0ebcff090f01d40e61fa5aecefa68": {"ta_keywords": "adversarial losses dis;understanding adversarial losses;adversarial losses;adversarial losses loss;adversarial loss functions;adversarial loss;various adversarial loss;dis discriminative adversarial;valid adversarial losses;discriminative adversarial network;discriminative adversarial;understanding adversarial;deeper understanding adversarial;adversarial network;losses dis discriminative;adversarial;adversarial network setting;various adversarial;training generative discriminative;loss functions training;valid adversarial;functions valid adversarial;proposed various adversarial;generative discriminative;loss functions;training generative;losses loss functions;generative discriminative models;discriminative models remains;functions training generative", "pdf_keywords": ""}, "ebe04f06580abab035408c4c2e65245b3950934e": {"ta_keywords": "etiology earthquakes;etiology earthquakes poorly;patient history earthquake;history earthquake;earthquake united states;earthquakes;united states earthquake;earthquake occurred;history earthquake united;earthquakes poorly understood;earthquake;earthquake occurred single;reported earthquake occurred;earthquakes poorly;recently reported earthquake;states earthquake occurred;reported earthquake;states earthquake;earthquake united;etiology;single patient history;occurred single patient;patient history;single patient;occurred single;patient;history;united states;occurred;poorly understood recently", "pdf_keywords": ""}, "dd36aca034312a266d6f10b37414d3342c3b9c79": {"ta_keywords": "etiology malignant disease;patient malignant disease;malignant disease;etiology malignant;patient malignant;treatment patient malignant;malignant disease poorly;malignant;diagnosis treatment patient;diagnosis;diagnosis treatment;disease poorly understood;disease;approach diagnosis treatment;new approach diagnosis;treatment patient;approach diagnosis;etiology;disease poorly;patient;treatment;understood development new;development new approach;understood development;approach;development new;poorly understood;poorly understood development;new approach;new", "pdf_keywords": ""}, "17351cfeac949c266f4d1ff86c515250b931bdc2": {"ta_keywords": "structure learning logic;learning logic;learning logic programs;generated parameter learning;scalable probabilistic logic;probabilistic logic;order probabilistic logic;introductionstructure learning parameter;probabilistic logic order;probabilistic logic called;logic programs graphs;parameter learning;learning parameter learning;structure learning;learning parameter;structure learning method;efficient structure learning;parameter learning important;logic programs;logic order theories;theories automatically generated;order theories automatically;novel structure learning;automatically generated parameter;introductionstructure learning;logic called proppr;new scalable probabilistic;second order probabilistic;logic order;efficient structure", "pdf_keywords": ""}, "394e17f5ee5e8a734b2714795b7da3cd704716da": {"ta_keywords": "peer evaluation;evaluation feedback peer;feedback peer evaluation;peer evaluation answers;feedback evaluation students;evaluation students;evaluation students means;feedback peer;performing evaluation feedback;evaluation feedback;evaluation answers submitted;peer;evaluation;performing evaluation;feedback evaluation;student anonymized provided;evaluation answers;student anonymized;students;moos highly successful;submitted student anonymized;moos;students means performing;classrooms critical;lectures download study;means performing evaluation;download study;conventional classrooms critical;answers submitted student;moos highly", "pdf_keywords": ""}, "57e4074c588c0e27e4c0bc89f12512ccdb900d79": {"ta_keywords": "text style transfer;style transfer unifies;style transfer;unsupervised text style;non generative techniques;generative model unsupervised;deep generative;unsupervised text;deep generative model;generative techniques;model unsupervised text;proposed non generative;non generative;present deep generative;generative techniques conclusionsour;generative techniques resultswe;text style;generative;generative techniques methodswe;generative model;transfer unifies;transfer unifies previously;unsupervised;model unsupervised;text;transfer;art non;style;state art non;methodswe present deep", "pdf_keywords": "deep generative;deep generative model;unsupervised style transfer;unsupervised machine translation;unsupervised neural objectives;propose probabilistic generative;generative model unsupervised;style transfer probabilistic;generative sequence modelsthe;unsupervised learning inference;unsupervised text transduction;generative techniques probabilistic;style transfer unsupervised;generative techniques;style transfer neural;probabilistic generative;probabilistic generative forumalation;non generative techniques;transfer unsupervised text;generative;propose generative model;generative sequence;advances unsupervised text;transfer propose generative;present deep generative;model unsupervised text;generative model;unsupervised text style;likelihood unsupervised learning;sequence model learns"}, "8a99e1eb3285f127eed7169441679d47be7f1633": {"ta_keywords": "data text generation;text generation tasks;text generation;generates text token;context free grammar;splicing retrieved;splicing retrieved segments;generates text;directly splicing retrieved;splicing;segments text neighbor;segments neighbor text;directly splicing;free grammar;neighbors generates text;tackle data text;tasks directly splicing;grammar important tool;text token;neighbor text inserting;generation tasks directly;text neighbor;generation tasks;context free;neighbor text;segments text;text token token;free grammar important;text neighbor compared;data text", "pdf_keywords": "free text generation;automated text generation;text generation increasingly;data text generation;text generation tasks;text generation directly;text generation;text generation propose;text generation systems;generating text;text generation introduce;generating text manually;neural text generation;approach generate text;generate text;right text generation;generate text generalized;generates text;text generation fundamental;text generation development;text generated;table text generation;generated text;generates text splicing;ability generate text;text generation tabular;manually generated text;used generate text;method generating text;generates text token"}, "3c40fc36217a56aafb0abc735ff7d132b17e83a0": {"ta_keywords": "automatic melody harmonization;melody harmonization;automatic melody;melody harmonization triad;sequence harmonic accompaniment;melody harmonization aims;task automatic melody;harmonization triad chords;melody sequence;chord sequence harmonic;harmonic accompaniment;melody sequence paper;generates chord sequence;harmonic accompaniment given;bar melody sequence;background automatic melody;harmonization;chords;chords comparative study;genetic algorithm deep;model generates chord;generates chord;harmonization triad;chords comparative;multiple bar melody;chord sequence;triad chords;melody;algorithm deep;bar melody", "pdf_keywords": "melody harmonization models;melody harmonization model;melodic harmonisation assistant;automatic melody harmonization;melody harmonization;generate melodic harmonization;melody sequence methods;melody harmonization evaluation;evaluating melody harmonization;melodic harmonization;melody harmonization use;melodic harmonization fundamental;algorithm melody harmonization;melody chord sequences;training evaluating melody;melodic harmonisation;implement melody harmonization;melodythe use deep;melody harmonication based;method melody harmonization;melody harmonization addition;music harmonisation;model melody harmonication;automatic melody;task automatic melody;melody harmonication;deep learning music;harmonizes given melody;melody sequence;generate melodic"}, "77efa3102456c9f921b05b95eefe845d2ce6bc4b": {"ta_keywords": "relative entropy discrimination;regularized discriminative training;discriminative training method;entropy discrimination;entropy discrimination described;regularized discriminative;discriminative training;minimum relative entropy;rive regularized discriminative;entropy discrimination mred;training method acoustic;backgrounda regularized discriminative;relative entropy;acoustic models derived;discriminative;method acoustic models;acoustic models;method acoustic;conventional discrimin;regularized;discrimination described methodswe;training method resultsmred;training method;backgrounda regularized;interpretations conventional discrimin;models derived minimum;derived minimum relative;discrimin;rive regularized;entropy", "pdf_keywords": ""}, "803a0d2677a7d6b20c3964533595775fa5c7c750": {"ta_keywords": "tests pairwise comparison;test pairwise comparison;sample tests pairwise;pairwise comparison data;pairwise comparison;tests pairwise;sample test pairwise;comparison data ranking;bound sample complexity;test pairwise;comparison data;comparison data establish;sample complexity;sample complexity required;clinical messagewe design;upper bound sample;ranking data sample;key clinical messagewe;clinical messagewe;sample tests;bound sample;comparison;results tight minimax;complementary lower bounds;lower bounds;design sample tests;minimax sense;data sample test;ranking data;data ranking", "pdf_keywords": "pairwise comparison ranking;tests pairwise comparison;comparison ranking data;pairwise comparisons generallywe;testing ranking data;pairwise comparison models;testing pairwise comparison;pairwise comparisons generally;comparison data ranking;test pairwise comparison;guarantees pairwise comparison;testing pairswise comparison;sample testing ranking;optimal test pairwise;testing ranking;pairwise comparison data;pairwise comparisons;comparison ranking;pairswise comparison data;pairwise comparisons generated;sample testing pairwise;sample tests pairwise;results pairwise comparison;sample testing pairswise;testing preference data;pairwise comparison;tests pairwise;pairswise comparison;hypothesis ranking sample;pairwise comparisonwe consider"}, "c859416a8e5682bee3c35df29bc02e02a22de072": {"ta_keywords": "transcription language documentation;manually transcribed speech;automatic transcription language;speech transcription tool;automatic transcription;speech transcription;integrating automatic transcription;transcribed speech experiments;manually transcribed;speaker speech transcription;transcription tool;introductionintegrating automatic transcription;transcription language;transcription tool trained;language documentation workflow;transcribed speech;documentation workflow experiments;workflow methodsusing percehone;documentation workflow;transcription;language documentation;persephone toolkit;percehone open source;data persephone toolkit;hours manually transcribed;transcribed;speech experiments;persephone toolkit objectiveto;documentation;speech experiments method", "pdf_keywords": ""}, "90705ece92a71efcf256cd047da53cbc1d4e5295": {"ta_keywords": "vibration gear meshing;gear mesh impact;gear meshing frequency;vibration gear;gear meshing;gear mesh;gearboxes fundamental;introduction gearboxes fundamental;extract vibration gear;gearbox;introduction gearboxes;health state gearbox;gearboxes;gearbox great significance;paper gear mesh;gearbox great;mesh impact theory;mesh impact;state gearbox;meshing frequency;gearboxes fundamental key;meshing frequency generally;state gearbox great;vibration;equipment rotating machines;vibration assess health;equipment rotating;gear;vibration assess;explain causes meshing", "pdf_keywords": ""}, "30fa01df767339a6c8bd37c32160992fcb19ed18": {"ta_keywords": "approximate nash equilibria;game theoretic analysis;output approximate nash;approximate nash;game best deviation;nash equilibria;game theoretic;large scale games;games lack quantitative;nash equilibria nie;backgroundepiical game theoretic;scale games;best deviation gain;regret game;approximate ne regret;quantitative guarantee approximate;regret game best;ne regret game;deviation gain;formulate deviation gain;gain formulate deviation;deviation gain computation;scale games lack;deviation gain formulate;guarantee approximate;games;equilibria;guarantee approximate ne;best deviation;quantitative guarantee", "pdf_keywords": ""}, "196be0bdec3b7bcb3ee35cd126fb2730a9d742d6": {"ta_keywords": "student educational software;educational software;educational software infrastructure;tutor;methods student educational;teaching sistuddent interactive;student educational;tutor effect;learning teaching sistuddent;methods student;student;event methods student;learning teaching;learning environment;leverage tutor;teaching sistuddent;leverage tutor effect;sistuddent interactive;educational;introduction learning teaching;tutor effect line;designed leverage tutor;teaching;sistuddent interactive event;interactive event methods;interactive;learning environment methods;software;environment methods student;introduction learning", "pdf_keywords": ""}, "46619f0547b1a9c2e7649d0e5c931e9aa857a938": {"ta_keywords": "model malignant disease;etiology malignant disease;malignant disease;etiology malignant;patient malignant disease;model malignant;new model malignant;disease patient malignant;malignant disease patient;malignant disease poorly;patient malignant;malignant;disease patient;disease;disease poorly understood;etiology;disease poorly;development new model;patient;model;new model;understood report;understood report development;poorly understood;development new;report development new;poorly understood report;development;report development;report", "pdf_keywords": ""}, "8963602d4b9c3b1054a5ed6fb2a2088dec774824": {"ta_keywords": "personal information management;management personal information;improve personal information;information email messages;personal information email;information management tools;email messages calendar;information management;machine learning techniques;email messages;messages calendar entries;messages calendar;introductionmanagement personal information;machine learning;personal information;information email;entries items workstation;calendar entries items;computer technology management;technology management personal;calendar entries;learning techniques effectively;items workstation documents;calendar;management tools;evidence machine learning;learning techniques;workstation documents highly;management personal;computer technology present", "pdf_keywords": ""}, "daedf33077099f7c808e9f4022469e15bf224ad7": {"ta_keywords": "kidney transplants risk;transplants risk chronic;transplants risk;multicentre prospective study;kidney transplants;injury multicentre prospective;conducted multicentre prospective;chronic injury multicentre;carried multicentre prospective;multicentre prospective;study identify kidney;identify kidney transplants;study conducted multicentre;study carried multicentre;prospective study;risk chronic injury;prospective study study;injury multicentre;transplants;risk chronic;chronic injury;kidney;study study conducted;study conducted;study study carried;prospective;identify kidney;conducted multicentre;study carried;aim study identify", "pdf_keywords": ""}, "59d487d6ef839c82ae128550e35fa44058b03d37": {"ta_keywords": "automated automated models;automated models;automated models management;automated models important;model development automated;development automated models;models management disease;automated automated;trained model development;automated;models management;developed trained model;development automated automated;trained model;model development;model developed trained;development automated;models;model developed;model;disease developed trained;improving quality model;models important;models important step;management disease;management disease developed;quality model;quality model developed;disease;disease developed", "pdf_keywords": ""}, "60ce57713261b41fe2e3d222f1d4530c4fc69241": {"ta_keywords": "serial rule probabilistic;probabilistic serial rule;randomized rules assignment;rule probabilistic serial;randomized rules;rule probabilistic;probabilistic serial ps;prominent randomized rules;probabilistic serial;complexity agent manipulating;serial rule;computational complexity agent;rules assignment problem;computing expected utility;expected utility;rule prominent randomized;complexity agent;serial ps rule;expected utility better;rules assignment;assignment problem known;known desirable fairness;introductionmanipulating probabilistic serial;probabilistic;fairness welfare properties;desirable fairness;manipulative behaviour;manipulative behaviour agents;agent manipulating ps;computing expected", "pdf_keywords": "random assignment algorithm;randomized rules assignment;rule random assignment;utility randomized;divisible random assignment;randomized rules;randomized social choice;random assignment;method random assignment;utility randomized social;random assignment problem;prominent randomized rules;anthe random assignment;probabilistic serial rule;agents sequential allocation;expected utility best;allocation case agents;optimal allocation;assignment algorithm;discrete allocation;optimal allocation best;serial rule random;assignment algorithm consider;rule random;optimal optimal allocation;computing expected utility;allocation objects agents;expected utility better;allocation best;allocation best response"}, "74d8a998269bcdd087a21840b0e28d86c256c121": {"ta_keywords": "general preference models;models intertemporal choice;preference models;models choice time;discounted utility models;choice time methodswe;utility models intertemporal;preference models defined;class preference models;learning time dependent;intertemporal choice exponential;preference models resultsthis;utility models;choice time;intertemporal choice;learnability models choice;dependent member choice;learning time;learnability models;important discounted utility;discounted utility;introduction learning time;time dependent member;bounds general preference;time dependent;implies important discounted;class preference;choice exponential;large class preference;dealing learnability models", "pdf_keywords": "quasihyperbolic discounting learnable;discounted utility models;discounting learnable;hyperbolic quasihyperbolic discounting;quasihyperbolic discounting;hyperbolic discounting model;quasi hyperbolic discounting;intertemporal choice model;general preference models;intertemporal choice learnability;preference models allows;preference models admit;models intertemporal choice;hyperbolic discounting;models choice time;learning preferences described;preferences learning rules;adopted hyperbolic discounting;preferences learning;preference models;learning preferences;discounted utility model;utility models intertemporal;preference models particular;discounting learnable pac;preference models defined;model intertemporal choice;preference model satisfies;discount utility researchers;criterion preference models"}, "c994372b3c33bbc1ad6b504c5efb5afd515a5009": {"ta_keywords": "target speech extraction;supervision based speaker;losses suitable speech;function target speech;recognition weak supervision;speech extraction;sufficient target speech;target speech;speech recognition gain;speech recognition;speech extraction recognition;weak supervision based;suitable speech recognition;extraction recognition weak;introductionauxiliary loss function;loss function target;speaker characteristics axiliary;weak supervision;axiliary losses sufficient;recognition weak;axiliary losses suitable;axiliary losses;losses sufficient target;characteristics axiliary losses;based speaker characteristics;speech;loss function;speaker characteristics;based speaker;introductionauxiliary loss", "pdf_keywords": ""}, "09d88e0bb8863fd402030aeb625c52c0492c4fef": {"ta_keywords": "reverberated speech recognition;encoder blind reverberation;reverberation reverberated speech;blind reverberation;blind reverberation reverberated;auto encoder blind;recurrent noising auto;speech recognition ar;reverberated speech;recognition ar reverberated;auto encoders dae;deep recurrent noising;noising auto encoders;noising auto encoder;encoder blind;reverbation deep recurrent;recurrent noising;reverberation;reverberated environments;reverberation reverberated;introductiondeep recurrent noising;encoders dae;speech recognition;ar reverberated environments;robust automatic speech;auto encoder;auto encoders;automatic speech recognition;automatic speech;reverberated environments hands", "pdf_keywords": ""}, "320278b24a3c53a44f95e8ef5465bebe56f24225": {"ta_keywords": "syntactic structure pause;structure pause prediction;pause prediction;prediction dependency parsing;speech synthesis;speech synthesis recent;pause prediction important;used speech synthesis;use syntactic;using syntactic;using syntactic structure;syntactic structure;syntactic structure helps;parsing using latent;use syntactic structure;syntactic;prediction dependency;dependency parsing;forpause prediction dependency;model forpause prediction;structure pause;dependency parsing using;pause;forpause prediction;parsing;speech;parsing using;baselines proposed latent;technology used speech;latent variable allows", "pdf_keywords": ""}, "568462ab0a0a59a2575b70db2cd9022572526f3f": {"ta_keywords": "learning continuous games;learning staackelberg games;staackelberg games methods;continuous games;staackelberg games;zero sum games;problems formulated games;continuous games commonly;games methods;nash equilibrium;adopting nash equilibrium;sum games establishing;sum games;games adopting nash;games formulated games;games methods work;formulated games;games formulated;nash equilibrium solution;simultaneous play games;learning problems formulated;learning continuous;formulated games formulated;games;games establishing;hierarchical decision making;hierarchical decision;formulated games instead;adopting nash;games commonly", "pdf_keywords": ""}, "92622a58377a4671b2ba59e8e59b19b0ab5119bb": {"ta_keywords": "knowledge graph identification;knowledge graph;knowledge graph construction;knowledge graphs;knowledge graphs provide;knowledge graphs built;introduction knowledge graphs;methods knowledge graph;challenges knowledge graph;scaling knowledge graphs;technique knowledge graph;graphs noisy extractions;graph identification kg;relationships automatically constructing;entities relationships automatically;scaling knowledge;graph identification;automatically constructing graphs;entities relationships;graph identification shows;ontological constraints methods;constructing graphs noisy;representation entities relationships;uncertain inputs ontological;knowledge;graph construction;graphs provide powerful;constructing graphs;graphs provide;graphs built", "pdf_keywords": ""}, "2ab481028dda04197283c03115bb5f46f5998cc3": {"ta_keywords": "methods management diseases;management diseases;management variety diseases;management diseases aim;diseases;diseases aim article;diseases aim;variety diseases;diseases led;variety diseases led;systematic review literature;systematic review;methods management;results systematic review;new approach management;diseases led development;approach management variety;new methods management;approach management;present results systematic;results systematic;systematic;introduction new approach;management;development new methods;management variety;use new approach;methods;new approach;review literature use", "pdf_keywords": ""}, "582089a00a6c9fb534f16d1dbbafc50cc4e3912a": {"ta_keywords": "language query interfaces;query language;specialized query language;backgroundnatural language query;query language data;query interfaces;query interfaces allow;language query;know specialized query;specialized query;language data storage;language data;query;backgroundnatural language;data storage schema;storage schema;access desired information;schema details recent;storage schema details;schema;schema details;data storage;language;recent advances nonliranian;advances nonliranian;advances nonliranian nonliranian;interfaces;data;access desired;interfaces allow", "pdf_keywords": ""}, "1adadbfa95e43a70fcd17e6ce947a0652b86bfc3": {"ta_keywords": "clean rawled corpus;rawled corpus;larger text corpora;text corpora train;text corpora;rawled corpus c4;corpus;large language models;corpora available scraping;corpora;largest corpora available;corpora available;corpora train;corpus c4;largest corpora;remarkable progress nonlinguistic;train largest corpora;language models;large language;nonlinguistic tasks researchers;context large language;corpus c4 raffleffel;nonlinguistic tasks;corpora train largest;progress nonlinguistic tasks;progress nonlinguistic;documentation colossal clean;nonlinguistic;colossal clean rawled;provide documentation colossal", "pdf_keywords": "corpus text web;web archive corpus;corpora available scraping;corpora web;clean rawled corpus;archive corpus;analysed corpus text;rawled corpus;corpora data;web scale corpora;corpora data available;word corpus;corpus text;data available corpora;data analysed corpus;word corpus text;corpus analysis corpus;corpus;analysed corpus;rawled corpus c4;corpora web scale;archive corpus contains;crawlwe report corpora;available corpora;analysis corpus;analyses large web;web crawled text;models largest corpora;corpus c4 dataset;report corpora web"}, "90129b0733ac48ead26b7c86e8b4df917568e208": {"ta_keywords": "databases data common;database data common;databases data;common domain database;data common domain;databases;database data;domain database data;data common;domain database;database;large number databases;number databases data;data analysis;data analysis large;method data analysis;number databases;data;method data;common domain;analysis large;analysis;analysis large number;new method data;report application;domain;common;report application new;application;report", "pdf_keywords": ""}, "02d98ca8f4ecd1a2b885d6867f4c1407ae8d1007": {"ta_keywords": "semantic parsers learnt;supervised semantic parsers;semantic parser;semantic parsers;supervised training parser;semantic parser maps;introduction semantic parser;training parser;parsers learnt;weakly supervised semantic;training parser requires;natural language commands;parsers;parsers learnt pairs;parser;parser maps natural;parser requires nl;supervised semantic;parser maps;parser requires;maps natural language;introduction semantic;natural language;semantic;executable meaning representations;weakly supervised;report weakly supervised;annotated domain experts;language commands;language commands users", "pdf_keywords": "supervision semantic parsing;semantic parser active;supervised semantic parsing;weaklysupervised semantic parsers;semantic parsers learnt;train semantic parser;learning semantic parsing;semantic parsing greatly;supervised semantic parser;semantic parser weak;weak supervision semantic;supervised training parser;supervised semantic parsers;active supervision semantic;friendly semantic parser;parser weak supervision;semantic parsing;training parser;semantic parsers;semantic parser;semantic parsing model;semantic parsing study;semantic parsers using;semantic parsing wasp;weakly supervised semantic;parser active learning;semantic parsing method;neural semantic parsing;semantic parsing use;performance semantic parser"}, "601398838250a4e69c69cc339d65f5c51e727ad1": {"ta_keywords": "role question generation;possible semantic roles;question generation;semantic roles predicate;question generation given;semantic roles;question prototype role;asking possible semantic;roles predicate;roles predicate methods;questions situation inherent;independent question prototype;question prototype;producing set questions;generation given predicate;possible semantic;semantic;role question;questions;asking questions situation;context independent question;introduce task role;set questions;produces context;questions situation;set questions asking;roles;questions asking;role;introduction asking questions", "pdf_keywords": "employ question contextualization;captures semantic role;question contextualization;question contextualization step;semantic roles context;context passage leveraging;contextualize questions;role question generation;semantic role fitting;contextualize questions provided;semantic annotation text;semantic annotation;approach semantic annotation;semantic annotation using;predicate semantic roles;semantic annotation developed;possible semantic roles;semantic roles predicate;generate role questions;semantic roles;question captures semantic;use semantic annotation;semantic role;used contextualize questions;roles context passage;questions provided annotators;natural language questions;contextindependent question prototype;role questions text;asking possible semantic"}, "c783bc02f5f901e4604eb3b0d504a036369afd91": {"ta_keywords": "perovskite layer rlaalo4;r2tio4 comparative study;layer rlaalo4 r2tio4;study r1p1 r2tio4;r2tio4 r2tio4;r2tio4 r2tio4 performed;r1p1 r2tio4 r2tio4;r2tio4 comparative;energy perovskite layer;rlaalo4 r2tio4 comparative;results showed r2tio4;r2tio4 performed results;comparative study mn4;r2tio4 r2ti;r2tio4 performed;r1p1 r2tio4;energy perovskite;r2tio4;intensity energy perovskite;rlaalo4 r2tio4;showed r2tio4 r2ti;study mn4 line;perovskite layer;showed r2tio4;mn4 line intensity;layer rlaalo4;study mn4;mn4 line;mn4;perovskite", "pdf_keywords": ""}, "a3452276ada37727d0008dad8ca7c27bbbee6984": {"ta_keywords": "existing rumor detection;rumor detection;rumor detection methods;social media conversation;posts represent conversation;represent conversation thread;claims existing rumor;media conversation structures;media conversation;conversation structure methodsin;conversation structures;oversimplify conversation structure;conversation structure;conversation structures provide;responses oversimplify conversation;conversation;conversation thread undirected;represent conversation;oversimplify conversation;conversation thread;fake claims existing;interaction user opinions;fake claims;existing rumor;real fake claims;social media;era social media;introductionrumors rampant;relation user responses;user responses", "pdf_keywords": "interactions rumor detection;detecting rumors twitter;embeddings rumor detection;effective rumor detection;networks detecting rumors;rumor detection microblogs;existing rumor detection;rumor detection;detecting rumors social;rumor detection data;rumor detection methods;rumor detection multi;detecting rumors;approach rumor detection;rumor detection use;detect rumors social;rumor detection robustness;detect rumors;rumor detection prediction;based embeddings rumor;embeddings rumor;methods detecting rumors;algorithm rumor detection;gat detecting rumors;rumour detection social;processing rumors;user interactions rumor;rumor veracity improved;media detect rumors;parallel informative tweets"}, "19be8dd52d949fed1a3e5aca7630669da2575d73": {"ta_keywords": "uncertainty agents preferences;uncertainty lottery model;models uncertainty lottery;uncertainty lottery;matching setting uncertainty;distribution linear preferences;stable matching setting;sided stable matching;preferences compact indifference;stable matching;setting uncertainty agents;weak preference order;agent weak preference;indifference model agent;uncertainty agents;linear preferences;weak preference;preference order;lottery model;agents preferences;indifference model;preferences limited information;agents preferences limited;linear preferences compact;setting uncertainty;order equally likely;models uncertainty;preference order specified;uncertainty;consider models uncertainty", "pdf_keywords": ""}, "35ee53492c7f32dbb3b4ed7ba4d1395218b13ee9": {"ta_keywords": "audit algorithmic systems;approaches audit algorithmic;audit algorithmic;behaviors formal auditing;auditing approaches;auditing approaches greatly;formal auditing approaches;formal approaches audit;formal auditing;auditing;approaches audit;audit;algorithmic systems biased;harmful behaviors formal;systems biased harmful;algorithmic systems;algorithmic;biased harmful behaviors;proposed formal approaches;blindspots critical issues;formal approaches;everyday use systems;use systems;behaviors formal;systems biased;blindspots critical;harmful behaviors;major blindspots critical;suffer major blindspots;blindspots", "pdf_keywords": "everyday algorithm auditing;everyday algorithm audits;everyday algorithm auditingthe;everyday auditing behaviors;everyday users algorithmic;algorithm auditing thought;auditingthe use algorithmic;algorithmic auditing;harmful algorithmic behaviors;algorithm auditing overcome;algorithm audits;algorithm auditing cases;algorithm auditing;algorithm audits discuss;algorithm auditing practices;algorithm auditingthe;algorithm auditingthe use;algorithm auditing emerging;algorithmic auditing approaches;algorithm auditing implications;everyday algorithmic;algorithm auditing proposed;algorithm auditing using;algorithm auditing viewed;algorithm auditingthe aim;algorithm auditing fundamental;algorithm audits analyze;algorithm auditing process;auditing behaviors;auditing behaviors emerged"}, "1f38ba33063f118f574cf57ff9f1a0e7de2857ff": {"ta_keywords": "russian semantic similarity;overview russian semantic;russian semantic;similarity evaluation rusce;semantic similarity evaluation;semantic similarity;workshop russian semantic;semantic similarity analysis;studies semantic similarity;semantic similarity paper;russian language exploring;similarity evaluation;russian language interesting;similarity;similarity analysis;similarity analysis measures;overview russian;gives overview russian;russian language;problem russian language;exploring problem russian;measures performed russian;performed russian language;similarity paper gives;introductionrusze workshop russian;semantic;comparative studies semantic;similarity paper;problem russian;workshop russian", "pdf_keywords": "russian semantic similarity;semantic similarity russian;evaluating semantic similarity;similarity russian nouns;evaluate semantic similarity;semantic similarity measures;comparison semantic similarity;semantic similarity words;semantic similarity;similarity measures russian;analysis semantic similarity;semantically similar words;similarity words results;russian semantic;similarity russian;use semantic similarity;semantically similar;calculating similarity words;semantic similarity human;similarity measures german;workshop russian semantic;semantic similarity participant;survey semantic similarity;correlates semantic similarity;use semantically similar;judgments semantic similarity;similarity words;semantic similarity attempt;task semantic similarity;similarity words joint"}, "857036a25401c19e484cc32d974c90cd9a46cd66": {"ta_keywords": "local nash equilibria;local optimality nonlinear;conditions local optimality;equilibria continuous games;computation local nash;local optimality;nash equilibria continuous;cooperative continuous games;local nash;constitute local nash;continuous games methodswe;optimality nonlinear programming;optimal control theory;nash equilibria non;nash equilibria;optimality nonlinear;programming optimal control;continuous games;equilibria non cooperative;conditions local;optimal control;strategies constitute local;continuous games results;nonlinear programming optimal;nonlinear programming;player strategies constitute;conditions ensuring player;player strategies;ensuring player strategies;equilibria continuous", "pdf_keywords": ""}, "d473ff103565d8c76e0cbfa33bdd4b0db1cbb23f": {"ta_keywords": "complex bayesian games;bayesian games;evolution strategies nes;evolution strategies;minimization simple games;natural evolution strategies;strategies nes deep;agent strategies parametric;bayesian games given;loop regret minimization;agent strategies;regret minimization simple;regret minimization;deep model optimization;games given analytic;optimization employ nes;simple games;nes iterative algorithm;bi level optimization;response optimization;strategies parametric form;nes deep model;model optimization;response optimization outer;strategies nes;level optimization;optimization outer;strategies parametric;optimization methodsfor pure;level optimization employ", "pdf_keywords": "general bayesian games;games deep reinforcement;solving bayesian games;learning games deep;analyzing bayesian games;bayesian games;compute bayesian games;complex bayesian games;bayesian games sequence;employ bayesian games;bayesian games compute;bayesian games using;bayesian games high;optimize deep strategies;games compute bayesian;bayesian games use;sequence constrained games;games symmetric bayesian;bayesian games symmetric;bayesian games sbs;bayesian game;constrained games;games sequence constrained;auctions symmetric bayesian;learning equilibrium games;symmetric bayesian game;bayesian game common;evolution strategies nes;bayesian game simple;game employ bayesian"}, "ecd2b355f250abfd4eb9d6c7c598c33c7cd6bcb0": {"ta_keywords": "crowdsourced labels principle;data labeling crowdsourcing;crowdsourcing data labeling;crowdsourced labels;noisy crowdsourced labels;labeling crowdsourcing;labeling crowdsourcing large;labels provided crowdsourcing;truth noisy crowdsourced;crowdsourcing data;minimax conditional entropy;increasing crowdsourcing data;noisy crowdsourced;increasing crowdsourcing;crowdsourcing;crowdsourcing large number;crowdsourcing large;crowdsourced;provided crowdsourcing;data labeling;rapidly increasing crowdsourcing;labeling;conditional entropy principle;provided crowdsourcing workers;crowdsourcing workers;conditional entropy;labels principle;propose minimax conditional;low cost labels;minimax conditional", "pdf_keywords": ""}, "0fc01a915cc7bf7025f80d44f805bd54b6425a33": {"ta_keywords": "nonintrusive load monitoring;arbitrary nonintrusive load;load monitoring fundamental;load monitoring;nonintrusive load;load monitoring provided;limits nonintrusive load;power consumption signal;load monitoring nm;aggregate power consumption;monitoring provided arbitrary;power consumption;consumption signal;arbitrary nonintrusive;consumption signal introduce;monitoring fundamental;monitoring nm algorithm;provided arbitrary nonintrusive;monitoring;aggregate power;monitoring nm;given aggregate power;monitoring fundamental limits;nonintrusive;bounds probability distinguishing;nm algorithm analyze;limits nonintrusive;algorithm seek bounds;power;nm algorithm seek", "pdf_keywords": "detection energy disaggregation;nonintrusive load monitoring;power consumption signals;recovering power consumption;backgroundnonintrusive load monitoring;event detection energy;disaggregation recovering power;load monitoring nthe;load monitoring;energy disaggregation recovering;energy disaggregation;power consumption signal;load monitoring nm;aggregate power consumption;power consumption devices;detection energy;determining energy consumption;consumption signals device;consumption signals;arbitrary nonintrusive load;consumption signal;monitoring nthe fundamental;power consumption;consumption signal additionally;monitoring;energy sensing smart;sensing smart grid;consumption individual devices;use energy sensing;monitoring nthe"}, "16457c13a40aa589fa06d8533a47b3f96aede474": {"ta_keywords": "canopy mounting articles;canopy;canopy mounting;protected vegetables graphical;articles protected vegetables;protected vegetables;mounting articles protected;covering shield;vegetables graphical framework;frame having stakes;articles protected;frame carrying;building shield;covering;mounting articles;flexible covering shield;building shield including;ground building shield;vegetables graphical;carrying flexible covering;protected;covering shield extends;main frame carrying;flexible covering;frame;vegetables;frame carrying flexible;framework;cantilever;frame having", "pdf_keywords": ""}, "639cc01afcc1c78063f7a6bbdae998cd147911c4": {"ta_keywords": "parametric utility learning;utility learning framework;utility learning;robust parametric utility;estimation heteroskedastic inference;heteroskedastic inference;feasible generalized;heteroskedastic inference improve;feasible generalized squares;parametric utility;squares estimation heteroskedastic;estimation heteroskedastic;efficiency modeling user;generalized squares estimation;heterogeneous preferences;constrained feasible generalized;users heterogeneous preferences;utility;propose robust parametric;flexibility achieving sustainability;inference improve forecast;heteroskedastic;sustainability;achieving sustainability goals;sustainability goals;constrained feasible;achieving sustainability;feasible;robust parametric;employs constrained feasible", "pdf_keywords": "game utility learning;parametric utility learning;formulate utility learning;utility learning framework;robust utility learning;learning agents utility;utility learning;proposed utility learning;demonstrate utility learning;utility learning method;utility learning methods;utility learning using;utility learning approach;utility learning process;optimization game theoretic;bistthe utility learning;energy social game;usingthe utility learning;prediction utility functions;utility regression;utility regression models;framework utility learning;model social game;prediction actual utility;players utility function;utility maximizers;correlated utility learning;agents utility maximizers;utility learning errors;variation utility learning"}, "03d2af05e41aac58ebae4ab37f09155e53d4b35b": {"ta_keywords": "noisy bit observations;matrix completion;matrix completion extreme;theory matrix completion;bit observations;matrix maximum likelihood;bit observations instead;binary measurements generated;valued entries matrix;entries matrix maximum;binary measurements;matrix maximum;number binary measurements;case noisy bit;matrix obtain small;entries matrix;noisy bit;estimate ma data;entries matrix obtain;observations;observations instead observing;observing subset;matrix;matrix obtain;observing subset real;maximum likelihood;observations instead;maximum likelihood estimate;completion extreme case;ma data", "pdf_keywords": "bit matrix completion;sensing sparse logistic;bit compressed sensing;compressed sensing sparse;compressed sensing large;compressed sensing;compressed sensing probability;matrix completion;based sparse logistic;sparse logistic regression;analysis compressed sensing;compressed sensing present;compressed sensing compressed;sparse logistic;sensing compressed sensing;matrix completion based;theory matrix completion;matrix completion consider;matrix completion case;matrix completion problem;sensing compressed;based compressed sensing;method compressed sensing;compressed sensing method;sensing sparse;matrix completion extreme;completion matrix;theory bit matrix;theory based sparse;completion based statistical"}, "3bfa1fe0a8d031d59dfc0cfa4975c296951ee56c": {"ta_keywords": "speech recognition living;speech recognition;recognition living rooms;new speech recognition;temporal modeling sounds;modeling sounds;based spatial spectral;rooms based spatial;spectral temporal modeling;living rooms based;living rooms;spatial spectral temporal;spatial spectral;living rooms challenging;rooms based;spectral temporal;speech;rooms;recognition living;rooms challenging task;recognition;developed new speech;temporal modeling;spectral;rooms challenging;based spatial;new speech;modeling;spatial;temporal", "pdf_keywords": ""}, "8873d1369590249113e1f0491ce49d1502395b9c": {"ta_keywords": "compliance video text;text compliance video;video text compliance;backgroundvideo text compliance;compliance video;video compliance associated;video compliance;given video compliance;text compliance activity;text compliance;compliance associated text;modal compliance;multi modal compliance;video text;compliance activity;compliance;compliance associated;backgroundvideo text;modal compliance problem;compliance activity verificationbased;activity given video;compliance problem;compliance problem determine;backgroundvideo;activity verificationbased natural;text;associated text instruction;activity;text instruction methodswe;text instruction", "pdf_keywords": ""}, "7488429131b8970425a66f3410920d98ff6e9c36": {"ta_keywords": "evaluations biased evaluations;submissions evaluate quality;evaluations biased;biased evaluations;biased evaluations common;introductiondebiasing evaluations biased;biased evaluations biased;submissions evaluate;evaluate quality reviews;evaluations;evaluations common;rate teaching quality;evaluate quality;rating course receive;introductiondebiasing evaluations;students higher rating;authors submissions evaluate;rating course;quality reviews conclusionsin;evaluations common evaluate;teaching quality;higher rating course;students rate teaching;rate teaching;reviews conclusionsin;reviews conclusionsin applications;ask students rate;teaching quality instructors;students rate;quality reviews", "pdf_keywords": "biased evaluations propose;ratings teaching evaluation;bias ratings teaching;evaluation peer review;biased evaluations;biased evaluations received;evaluations biased evaluations;bias teaching evaluations;teaching evaluation peer;mitigating biases evaluations;evaluations biased;reviewer bias peer;bias peer review;biases evaluations;evaluation peer;induced bias ratings;teaching evaluations;evaluate quality reviews;biases evaluations biased;ratings teaching;bias ratings;peer review outcome;rating course;teaching evaluations induced;higher rating reviews;reviewer assignments;students higher rating;submissions evaluate quality;reviewer assignments method;study reviewer bias"}, "579e01c3c864cc98e57c728f84fcf553c5b1bcba": {"ta_keywords": "audible murmur enhancement;murmur enhancement method;non audible murmur;murmur enhancement;murmur nam microphone;audible murmur;introductionnon audible murmur;silent speech interfaces;audible murmur nam;silent speech communication;medium silent speech;nam microphone detect;soft whispered voice;promising medium silent;microphone detect;methodsa silent speech;microphone;microphones noisy environments;non audible;nam microphone;murmur;whispered voice developed;interfaces non audible;conductive microphones noisy;microphone detect extremely;extremely soft whispered;microphones noisy;voice developed nonam;speech interfaces non;body conductive microphones", "pdf_keywords": ""}, "4f1eef4acaf0164593b9e654dba4b8cd3e72421d": {"ta_keywords": "collective classification widely;collective classification classes;collective classification;stacked graphical learning;separately collective classification;collective classification usually;used collective classification;context collective classification;graphical learning meta;meta learning scheme;classification relational;classification relational datasets;meta learning;learning meta learning;graphical learning;instance separately collective;used classification relational;methods propose stacked;propose stacked graphical;classification widely;stacked graphical;relational datasets inference;separately collective;classification;learning meta;related instances predicting;classes predicted simultaneously;classification classes;classification classes predicted;relational datasets", "pdf_keywords": ""}, "31884a623af77136413d997049b5787b394db461": {"ta_keywords": "new encoder decoder;encoder decoder;encoding encoder decoders;encoder decoders;decode encoder decoder;encoder decoder able;decode encoder;encoder decoders fundamental;encoding encoder;new encoder;development new encoder;encoder;able decode encoder;decoder able decode;decoders;decoder;decoder able;decoders fundamental;encoding;decoders fundamental step;decode;able decode;new technology;development new technology;new technology development;technology development new;development new;technology;technology development;new", "pdf_keywords": ""}, "2818bd090206ef33f9d7e1be03bc4f742c6762d1": {"ta_keywords": "agglutinative languages morpheme;morpheme based language;languages morpheme based;languages morpheme;models anraphyseal syllable;morpheme based;anraphyseal syllable ar;agglutinative language;model agglutinative language;language models anraphyseal;agglutinative languages;vocabulary agglutinative languages;anraphyseal syllable;introduction morpheme based;agglutinative language useful;vocabulary agglutinative;morpheme;language models;introduction morpheme;syllable ar used;syllable ar;language models built;based language models;syllable;agglutinative;language useful tool;nature vocabulary agglutinative;model agglutinative;languages;used model agglutinative", "pdf_keywords": ""}, "8f99f9409f254134aa32fbf072475100f688d613": {"ta_keywords": "codes distributed storage;storage network security;distributed storage networks;storage networks;distributed storage network;storage networks provide;distributed storage;storage nodes;storage network;codes distributed;threat model eavesdropper;class codes distributed;storage nodes possibly;subset storage nodes;eavesdropper;eavesdropper gain access;aspect distributed storage;model eavesdropper;network security;efficient node repair;storage;subset storage;network security paper;networks provide reliability;eavesdropper gain;introductionregenerating codes class;codes class codes;codes class;introductionregenerating codes;nodes possibly data", "pdf_keywords": "codes distributed storage;secure regenerating codes;theoretic secrecy code;security data storage;storage nodes eavesdropper;secrecy code;codes satisfying reconstruction;constructions regenerating codes;regenerating codes distributed;codes rmr eavesdropper;storage fixed code;secure code;information theoretic secrecy;information theoretically secure;secrecy code product;code construction secure;codes distributed;eavesdropper information theorically;construction secure code;secure code based;regenerating codes achieve;theoretic secrecy capacity;eavesdropper information;codes achieve information;security data;message symbols eavesdropper;distributed data storage;explicit codes satisfying;symbols eavesdropper recover;secrecy capacity"}, "b130b6387b105ecd9b4718b179b1e128157f9516": {"ta_keywords": "paraphrase database;paraphrase database ppb;paraphrase database compositional;introduction paraphrase database;model paraphrase database;paraphrase model;compositional paraphrase model;paraphrase model paraphrase;database compositional paraphrase;compositional paraphrase;paraphrase;model paraphrase;introduction paraphrase;phrase pairs heuristic;phrase pairs;list phrase pairs;extensive semantic;2013 extensive semantic;extensive semantic resource;semantic;list phrase;semantic resource;semantic resource consisting;heuristic confidence estimates;phrase;pairs heuristic confidence;confidence estimates;heuristic confidence;confidences necessarily;confidences", "pdf_keywords": ""}, "191543c7cb084d3af6a48ae771ca3dfd0588ab22": {"ta_keywords": "text reviews;text reviews target;text reviews written;especially review text;representation text reviews;reviews target item;review text;review text available;recently deep learning;reviews target;deep learning methods;learn latent representation;deepconn uses neural;improve performance recommender;especially review;reviews written target;recent model deepconn;recommender systems;reviews written;reviews;recommender systems traditional;deep learning;recommender;model deepconn;deepconn uses;model deepconn uses;review;learn latent;latent representations;nets learn latent", "pdf_keywords": "predicting rating;predicting rating usera;predict rating user;ratings recently deep;rating prediction;predicting rating factororization;model predicting rating;predict rating usera;predict ratings;method predicting rating;rating prediction using;predict ratings recently;rating user fundamental;predict rating;approach rating prediction;reviews users items;user recommend item;ratings user item;rating user;neural networks recommendations;algorithm rating prediction;prediction user feedback;rating prediction increasing;ratings user;used predict ratings;rating usera;text reviews;reviews users;improve performance recommender;rating factororization"}, "c72cdb5ce7e0911c7f442ab503652d6fdeef35e0": {"ta_keywords": "cg parsers semantic;parsers grounded semantic;grounded semantic parsing;parsers semantic slot;syntactic cg parsers;parsers semantic;semantic parsing;cg parsers grounded;induced cg parsers;parsers grounded;semantic parsing methodswe;cg parsers;grounded semantic;syntactic cg;explore syntactic supervision;syntactic supervision;different syntactic cg;parsing;parsers;syntactic supervision required;downstream semantic analysis;required downstream semantic;semantics captured;semantic slot;downstream semantic;semantic analysis;semantic analysis resultsthis;semantic slot filling;semantic;explore syntactic", "pdf_keywords": "parsers semantic slotfilling;grounded semantic parsers;semantic parsers;semantic parser;parsers semantic;sentence semantic parser;semantic parsers semantic;syntactic grounded semantic;processing sentence semantic;semantic parser use;sentences freebase logical;syntactic supervision;parser trained freebase;parser able achieve;semantic slotfilling task;convert sentences freebase;explore syntactic supervision;syntactic supervision required;syntactic grounded;semantic slotfilling;grounded semantic;semantic evaluation;natural language processing;parsers;supervised parser;fully supervised parser;sentences freebase;sentence semantic;simple sentences entities;required downstream semantic"}, "61d2dda8d96a10a714636475c7589bd149bda053": {"ta_keywords": "encoders review network;rn encoders review;rnn decoders cnn;cnn rn encoders;rnn decoders;rn encoders;consider rnn decoders;decoders cnn rn;encoders review;decoders cnn;attention mechanism encoder;encoder decoder model;encoder decoder;review network;existing encoder decoder;encoder;encoders;rnn;objectivethe review network;called review network;review network performs;existing encoder;review network objectivethe;consider rnn;encoder hidden states;encoder hidden;encoder decoder framework;extension encoder decoder;decoders;cnn rn", "pdf_keywords": ""}, "12239e761e8c7cd05e12e18f43dba7b46dfd8ac1": {"ta_keywords": "source translation systems;translation systems;corpora parallel text;target language corpora;translation systems translate;providing human translations;translate multiple languages;necessary corpora parallel;corpora parallel;language corpora;human translations;translations relevant languages;introductionmulti source translation;text multiple sources;translations;language corpora rarely;translate multiple;source translation;human translations relevant;sources target language;corpora;systems translate multiple;single target language;corpora rarely complete;corpora rarely;systems necessary corpora;parallel text multiple;target language using;parallel text;systems translate", "pdf_keywords": "augmentation incomplete multilingual;augmentation pseudo translations;translations corpus telomere;providing human translations;source translation systems;data augmentation multiencoder;translations corpus;pseudo translations corpus;human data augmentation;augmentation multiencoder non;multilingual corpora multi;translation multilingual manner;incomplete multilingual corpora;multilingual translation;data augmentation introduce;translation multilingual;multilingual corpora;machine translation;multilingual translation multilingual;translation systems;augmentation multiencoder;human translations;data augmentation;machine translation systems;add pseudo translations;data augmentation using;translations source language;actual multilingual corpora;translations missing data;corpora multi source"}, "d2a2be6ce932a0f1939f31cfff4d64ea3d76723d": {"ta_keywords": "robustness adversarial;robustness adversarial attacks;classification performance robust;adversarial;set robustness adversarial;uncertainty makes classification;classification performance deep;training set robustness;adversarial attacks;backgroundhuman uncertainty;backgroundhuman uncertainty makes;robust objectivethe classification;perceptual uncertainty resultswe;training label distributions;adversarial attacks methodsin;performance deep;performance deep neural;deep neural;perceptual uncertainty;deep neural networks;robust objectivethe;human perceptual uncertainty;performance robust objectivethe;label distributions;robustness;classification performance;robust;classification;makes classification performance;objectivethe classification performance", "pdf_keywords": "classification performance deep;training soft labels;training human labels;robustness adversarial;training label distributions;soft label training;robustness adversarial attacks;adversarial;label training accuracy;set robustness adversarial;labels useful training;accuracy soft label;performance deep;training set robustness;uncertainty labels improve;performance deep neural;labels effective predicting;labels effective human;soft label dataset;adversarial attacks report;adversarial attacks;deep neural;dataset human uncertainty;soft labels effective;human soft labels;training accuracy soft;distributional shift adversarial;improve accuracy robustness;label training results;labels improve accuracy"}, "1668b0b9cc631cdfc0dfaf77b71627f5524a866c": {"ta_keywords": "ochastic convex optimization;smooth ochastic convex;ochastic convex;convex optimization patient;convex optimization;method smooth ochastic;smooth smooth ochastic;optimization patient history;convex;smooth ochastic;optimization patient;optimization;ochastic;method smooth smooth;method smooth;specific disease method;sex specific disease;smooth smooth;disease method;based method smooth;disease method based;present method smooth;patient history;smooth;patient history history;specific disease;history sex specific;sex specific;history sex;method", "pdf_keywords": "derivative stochastic optimization;stochastic approximation gradient;smooth stochastic optimization;stochastic convex optimization;gradient stochastic gradient;stochastic gradient;stochastic gradient stochastic;similar stochastic optimization;gradient stochastic;methods stochastic convex;directional derivative stochastic;stochastic optimization;results stochastic gradient;smooth stochastic convex;stochastic optimization form;noise directional derivative;convex optimization pad;stochastic gradient proof;stochastic optimization problems;free optimization gradient;stochastic convex;approximation gradient available;derivative free optimization;optimization problems stochastic;methods smooth stochastic;convex optimization method;gradient based optimization;method smooth stochastic;known stochastic approximation;convex optimization"}, "3e398bad2d8636491a1034cc938a5e024c7aa881": {"ta_keywords": "pyramid vision transformer;dense prediction convolutions;pyramid vision;introduce pyramid vision;vision transformer;dense prediction tasks;vision transformer pv;prediction convolutions;dense prediction;prediction convolutions simple;pyramid;tasks introduce pyramid;convolutions simple useful;backbone dense prediction;convolutions;including object detection;introduce pyramid;object detection;transformer various dense;various dense prediction;convolutions simple;prediction tasks;transformer;transformer pv;boosts performance downstream;prediction tasks introduce;vision;boosts performance;transformer pv overcomes;transformer various", "pdf_keywords": "pyramid vision transformer;object detection semantic;pyramid vision;pure pyramid vision;introduce pyramid vision;pyramid spatial reduction;automaticallypyramid vision transformer;feature pyramid;semantic segmentation benchmarks;propose pyramid vision;vision transformer;shrinking pyramid spatial;based pyramid vision;semantic segmentation large;imagenet coco;stronger designed cnn;model imagenet coco;mask cnn semantic;transformer segmentation network;pyramid structure transformer;tasks object detection;dense prediction tasks;prediction image object;cnn semantic;transformer dense predictions;shrinking pyramid;pyramid spatial;classification demonstrate pyramid;pyramid;spatial reduction attention"}, "115f1366318e7622f89f3a870e5863282670b1ad": {"ta_keywords": "coronary intervention pci;percutaneous coronary intervention;coronary angiography;contrast acute kidney;coronary intervention;coronary intervention ci;percutaneous coronary;coronary angiography cag;complication coronary angiography;cag percutaneous coronary;effect statins;effect statins procedure;mechanism effect statins;acute kidney injury;statins procedure mechanism;statins procedure;statins;severe complication coronary;post contrast acute;kidney injury pc;complication coronary;angiography cag percutaneous;kidney injury;acute kidney;angiography;angiography cag;contrast acute;intervention pci present;coronary;injury pc aki", "pdf_keywords": ""}, "d516daff247f7157fccde6649ace91d969cd1973": {"ta_keywords": "modeling world useful;model powerful tool;modeling modeling world;useful modeling;modeling;model;modeling world;model powerful;useful modeling modeling;modeling modeling;world useful modeling;modeling world powerful;powerful tool modeling;tool modeling;tool modeling modeling;world powerful tool;world useful;powerful tool;world powerful;useful;tool;world;powerful", "pdf_keywords": "humans interpretable interpretable;models interpretable;interpretable interpretable;model concept interpretability;interpretability;interpretability new concept;interpretable interpretable use;interpretability popular concept;howthe concept interpretability;motivations underlying interpretability;underlying interpretability;concept interpretability;need interpretability;interpretability encountered;concept interpretability new;interpretability research discussed;interpretability research;concept interpretability popular;interpretability examine;motivations interpretability;humans interpretable;interpretability new;need interpretability encountered;interpretability popular;underlying interpretability addressthe;interpretability axiomatically;interpretable;models interpretable despite;interpretable use;argue need interpretability"}, "5ba57ff3c3e6e319586b86a990b6e082f4ecf972": {"ta_keywords": "bayesian approaches speech;speech recognition;approaches speech recognition;speaker adaptation;bayesian informationcriterion bic;effectiveness speech recognition;speaker adaptation tasks;speech recognition important;variational bayesian approaches;experimentally speaker adaptation;introduces variational bayesian;variational bayesian;bayesian informationcriterion;speech recognition shown;using bayesian informationcriterion;posterior map model;introduction bayesian approaches;bayesian approaches;advantages bayesian approaches;introduction bayesian;bayesian approaches effectiveness;approaches speech;advantages bayesian;posterior map;bayesian;bayesian approaches addition;maximum posterior map;speech;using bayesian;approaches effectiveness speech", "pdf_keywords": ""}, "e5d143ae82ede67726aa1a9aeac3de4bf53d8920": {"ta_keywords": "pretraining approach knowledgebased;knowledgebased vision language;knowledgebased vision;knowledge graph embeddings;vision language pretraining;approach knowledgebased vision;language pretraining;learning cross model;language pretraining kappa;cross model representations;uses knowledge graph;knowledge graph;introductiontransduced based pretraining;pretraining kappa;novel pretraining approach;pretraining approach;introduce novel pretraining;multimodal tasks;learning cross;representations various multimodal;advantage commonsense knowledge;pretraining;knowledgebased;pretraining techniques;pretraining techniques achieved;commonsense knowledge logical;pretraining kappa uses;novel pretraining;based pretraining;approach knowledgebased", "pdf_keywords": ""}, "7f52e3914a61994f68583635e43bc1bb9203e3b3": {"ta_keywords": "genetic toxicity counting;markers genetic toxicity;genetic toxicity;fumes cofs smoking;toxicity counting micronucleus;toxicity counting;biological markers genetic;cooking oil fumes;risk aggravates genetic;toxicity;chlorosome aberrations;oil fumes cofs;cofs smoking habits;markers genetic;oil fumes;genetic modifications;chlorosome aberrations ca;chlorosome;mn chlorosome aberrations;fumes cofs;estimate biological markers;cofs smoking;genetic;biological markers;changes mn chlorosome;mn chlorosome;cofs exposures control;fumes;smoking habits;cofs exposures", "pdf_keywords": ""}, "60a4ad8e8f4389f317d109550f5da2a571cbb515": {"ta_keywords": "natural language query;answer large corpus;large corpus text;large corpus;language query extract;natural language;comprehend natural language;text methods quasar;large scale datasets;corpus;quasar dataset consists;corpus text;corpus text methods;quasar dataset;language query;queries constructed;extract answer large;query extract;methods quasar dataset;queries constructed definitions;software entity tags;systems designed comprehend;queries;dataset consists 37000;overflow quasar dataset;datasets;entity tags popular;software entity;gap queries constructed;quasar", "pdf_keywords": "question answering dataset;question answering searching;factoid question answering;question answering;answer query retrievalthe;problem retrieval comprehension;trivia questions software;text answer query;natural language query;answering searching relevant;answering searching;dataset text comprehension;retrieval comprehension;answer query reading;reading comprehension dataset;answer large corpus;text search useful;retrieval comprehension development;method question answering;text comprehension accuracy;information retrieval;subtasks factoid;query user friendly;tasks qa searching;large corpus text;machine reading comprehension;comprehension dataset text;retrieved text answer;text comprehension;combined problem retrieval"}, "15513c732d6af975f312307be3b5e2bd674ac0ef": {"ta_keywords": "automatic speech recognition;automatic speech;word error rate;speech recognition;speech recognition systems;error rate wer;introduction automatic speech;minimizing word error;rate wer framework;rate wer;error rate;word error;tuned minimizing word;wer framework;speech;insertions words treated;deletions insertions words;recognition systems trained;words treated uniform;minimizing word;insertions words;errors errors substitutions;wer framework framework;errors substitutions;wer;errors substitutions deletions;words treated;systems trained;impact errors errors;recognition systems", "pdf_keywords": ""}, "7261b088c48be7eca10263e765739f7347665481": {"ta_keywords": "malignant cells adolescent;cells adolescent population;management malignant cells;cells adolescent;malignant cells;management malignant;strategies management malignant;malignant;adolescent population;population adolescent;dna damage;adolescent population adolescent;population adolescent population;dna damage induced;adolescent population likely;adolescent;cells;population likely dna;dna;likely dna damage;induced disease population;disease population;damage induced disease;induced disease;disease;likely dna;population;population likely;damage induced;damage", "pdf_keywords": "markovian network equilibrium;algorithms network equilibrium;equilibrium model game;md routing game;equilibra network games;game equilibrium multicommodity;equilibrium condition game;optimization equilibra network;network equilibrium problems;equilibrium multicommodity routing;game equilibrium condition;game equilibrium;network equilibrium;multicommodity routing games;condition game equilibrium;algorithms proposed equilibrium;network equilibrium model;algorithms markovian network;multicommodity markovian network;tensorthe equilibra network;stochastic user equilibrium;network equilibrium propose;network games principle;routing games studied;routing game;games md routing;network equilibrium simulate;network equilibrium considering;network equilibrium study;network games fundamental"}, "4fb8009422903f7cb6f9a929409264b7fbca55e3": {"ta_keywords": "feature enhancement;feature enhancement method;backgroundfeature enhancement;noise feature vectors;high speech recognition;corrupted noise feature;proposes feature enhancement;stereo based piecewise;discriminative region weighting;speech recognition;piecewise linear compensation;backgroundfeature enhancement joint;known stereo based;noise feature;stereo based;learns piecewise linear;speech recognition performance;backgroundfeature;enhancement method;noise environments feasible;recognition performance variety;linear compensation environments;enhancement method achieve;corrupted noise;region weighting methodsthis;piecewise linear transformation;recognition performance;discriminative region;linear compensation;feature vectors", "pdf_keywords": ""}, "359dfdfea38f645d5fa49efc846a3b5ebce317fe": {"ta_keywords": "injecting machine learning;build interpretable models;interpretable models;machine learning matter;interpretable models express;know loan decide;does know loan;machine learning software;interpretable;discomfort machine learning;machine learning;know loan;loan decide qualifies;caution injecting machine;loan decide;software agent does;build interpretable;agent does know;arms build interpretable;learning;learning software agent;doctor;models;software agent;injecting machine;learning matter;loan;doctor just won;ought caution injecting;applications significant risk", "pdf_keywords": "interpretable machine learning;interpretable machine;interpretable models;interpretability field human;challenges interpretable machine;interpretability;challenges interpretability;interpretable models challenge;challenges challenges interpretability;field interpretable machine;challenges interpretability field;diseases need interpretable;interpretable;need interpretable models;interpretability field;challenges interpretable;field interpretable;human decision makers;human interpretation difficult;machine learning discusses;challenges challenges interpretable;machine learning implications;human interpretation;need interpretable;machine learning;field human interpretation;learning implications human;automated decision making;automated decision;human decision"}, "2c94bc68388517aa4a2d2dfc7d35df95ce24b1a8": {"ta_keywords": "adversarial minimax game;adversarial minimax;formulated adversarial minimax;learning representations invariant;learning representations;minimax game analyze;minimax game;adversarial;generalization representation learning;problem learning representations;formulated adversarial;learning meaningful representations;representation learning;better generalization representation;game analyze optimal;generalization representation;process formulated adversarial;optimal equilibrium game;representations maintain;representations invariant specific;representations;representation learning process;problem learning;minimax;representations maintain content;meaningful representations maintain;meaningful representations;representations invariant;optimal;great machine learning", "pdf_keywords": ""}, "5a0c9bbf0432dac8bd357a4aabf82b83a6c95524": {"ta_keywords": "contexttraditional document similarity;document similarity research;document similarity measures;document similarity;documents similar aspect;based document similarity;traditional document similarity;similar dissimilar documents;similarity research papers;documents similar;aspects documents similar;dissimilar documents typically;similarity research;dissimilar documents;similarity measures provide;similarity measures;aspect based document;similar aspect based;consider aspects documents;grained distinction similar;similarity;contexttraditional document;aspects documents;distinction similar dissimilar;distinction similar;documents typically consider;similar aspect;like recommender systems;documents similar limits;typically consider aspects", "pdf_keywords": "document similarity aspect;document similarity measures;document similarity research;document similarity fundamental;document classification task;similarity aspect information;document classification scientific;approach document similarity;document similarity score;based document similarity;document similarity;document similarity suited;aspect based similarity;bias text similarity;document classification;pairwise document classification;similarity aspect based;text similarity major;traditional document similarity;class document classification;aspects documents similar;similarity research papers;backgroundtraditional document similarity;classifying aspect based;aspects paper label;classifying aspect;similarity aspect;classification scientific papers;text similarity;similar dissimilar documents"}, "4f74be7e5dd4b8e9113e86132cf792da2c32ca3d": {"ta_keywords": "pulmonary artery disease;diagnosis pulmonary artery;patients diagnosis pulmonary;pulmonary artery;diagnosis pulmonary;artery disease;pulmonary;patients diagnosis;artery;management patients diagnosis;diagnosis;management patients;patients;approach management patients;disease;article;article discuss importance;new approach management;management;new approach;importance new approach;approach management;article discuss;discuss importance new;new;approach;aim article discuss;discuss importance;aim article;importance new", "pdf_keywords": ""}, "90720bba46dd79bc340359617a7a07fcecc890c1": {"ta_keywords": "local nash equilibria;differential nash equilibria;generic local nash;equilibria games smooth;degenerate differential nash;characterize local nash;nash equilibria generic;nash equilibria games;nash equilibria open;differential nash;local nash;nash equilibria;equilibria generic local;equilibria games;smooth perturbations player;games smooth costs;perturbations player costs;equilibria open dense;games smooth;equilibria structurally stable;player costs implies;continuous strategy spaces;smooth costs continuous;perturbations player;equilibria generic;equilibria open;spaces demonstrate equilibria;equilibria structurally;strategy spaces;equilibria", "pdf_keywords": ""}, "834fb0d09e764b88ef76ee77e0befb8faeaad7fe": {"ta_keywords": "based speech synthesis;speech synthesis;hmm based speech;speech synthesis unit;speech synthesis enables;model hmm based;approach speech parameters;statistical approach speech;model hmm;speech parameters;hidden markov model;modeling acoustic features;combining hidden markov;hmm based;acoustic features based;based speech;markov model hmm;introductiontraditional hidden markov;hidden markov;speech parameters tend;unit selection synthesis;synthesis unit selection;selection synthesis;modeling acoustic;approach speech;flexible modeling acoustic;selection synthesis proposed;acoustic features;synthesis;markov model", "pdf_keywords": ""}, "2507a6924007efbe0c3116048a85108398f23007": {"ta_keywords": "translations interlinear glossssed;interlinear glossing resourced;introductionautomatic interlinear glossing;leveraging translations interlinear;automatic glossing models;leveraging translations;interlinear glossing;glossing resourced languages;interlinear glossssed text;languages leveraging translations;collect translations;easy collect translations;creating automatic glossing;translations interlinear;language documentation projects;automatic glossing;glossing models;language documentation;glossssed text igit;glossing models using;languages leveraging;glossing resourced;translations;information language documentation;glossing;interlinear glossssed;glossssed text;linguistic information language;source neural models;text igit widely", "pdf_keywords": ""}, "5ad44a9d6b850405da42f989711af431427425b5": {"ta_keywords": "language text speech;text text speech;text speech systems;text speech;text speech database;speech systems today;speech database;speech systems;conversational data;language text;capable synthesize text;languages used conversation;conversational data available;synthesize text;speech database recorded;speech systems capable;synthesize text need;input single language;introductionmost text speech;mixing multiple languages;rise conversational data;script language text;single language written;speech;multiple languages used;single language;multiple languages;script language;conversation sentence seen;text text", "pdf_keywords": ""}, "d95b66901d72a13d0c96c7e9bfd4a999ed7fb19c": {"ta_keywords": "syntax based systems;syntactic preordering;syntax based;english tasks iwt;systems english german;string syntactic preordering;systems english;nontetrasymtic syntax based;based systems english;naist syntax based;german english tasks;syntactic;iwt 2014 evaluation;interwt 2014 presented;string syntactic;syntax;english tasks;risk combination sm;systems interwt 2014;2014 evaluation campaign;presents nontetrasymtic syntax;interwt 2014;purposent naist syntax;forest string syntactic;bayes risk combination;nontetrasymtic syntax;2014 evaluation;naist syntax;tasks iwt 2014;minimum bayes", "pdf_keywords": ""}, "ad21f0709a3e5d7db91606e2f67926f93e01838d": {"ta_keywords": "contextual games;games driven contextual;contextual games type;contexts game outcomes;minimize contextual regret;contextual regret;contexts game;different contexts game;class contextual games;contextual regret individual;repeated games driven;driven contextual information;repeated games;adaptive adaptive processes;contextual information;adaptive processes;driven contextual;context contexts;contextual information round;minimize contextual;adaptive processes formulate;contextual;game theoretic notions;adaptive adaptive;context contexts important;adaptive;contexts;game theoretic;type repeated games;context", "pdf_keywords": "adversarial contextual bandit;contextual regret algorithms;contextual games regret;contextual bandit problem;contextual bandit;games contextual regret;efficiency contextual games;nonstochastic multiarmed bandit;minimize contextual regret;regret efficient algorithms;games regret efficient;repeated contextual game;gamesthe contextual regret;contextual games;regret algorithms;repeated games contextual;algorithm repeated games;bandit problem fundamental;players contextual regret;contextual game;bandit problem regret;contextual games described;problem regret algorithms;bandit problem nonstochastic;repeated gamesthe contextual;regret algorithms exist;multiarmed bandit problem;class contextual games;model contextual game;contextual game described"}, "f2d5861a24b7aa33036208ba81e11bb9b2090e7c": {"ta_keywords": "unsupervised paraphrasing embedding;multilingual paraphrasing model;paraphrasing embedding;multilingual paraphrasing;paraphrasing embedding outputs;end multilingual paraphrasing;shot paraphrase generation;corpora generate paraphrases;paraphrasing model trained;paraphrase generation;paraphrase generation resultsthe;generate paraphrases;generate paraphrases meaning;unsupervised paraphrasing;diversity unsupervised paraphrasing;zero shot paraphrase;paraphrasing model;paraphrasing;paraphrases meaning;paraphrases meaning spaces;paraphrases;shot paraphrase;word embeddings;paraphrase;layer word embeddings;word embeddings architectural;softmax layer word;softmax;embeddings;embedding outputs", "pdf_keywords": "paraphrasing based multilingual;multilingual paraphrasing model;paraphrasing generating semantically;multilingual paraphrasing;translation autoencoding paraphrasing;autoencoding paraphrasing useful;generate paraphrastic sentences;autoencoding paraphrasing;end multilingual paraphrasing;shot paraphrase generation;paraphrasing model trained;paraphrastic sentences multilingual;increase diversity paraphrasing;generating paraphrases bilingual;paraphrasing large translation;paraphrasing preserving;paraphrasing based;diversity paraphrasing generating;corpora generate paraphrases;paraphrasing model paraphrasing;generating paraphrases;embedding spaceparaphrasing;generate paraphrastic;bilingual data paraphrasing;generate paraphrases meaning;approach generating paraphrases;generate paraphrases;new approach paraphrasing;improving quality paraphrases;paraphrase generation"}, "0e02765103001a792b20242b4dee6dc81917b850": {"ta_keywords": "annotated biomedical text;gene recognizer flybasee;manually annotated biomedical;gene recognizer;annotated biomedical;bootstrapping gene recognizer;annotated noisy text;automatically annotated noisy;annotation evaluating recognizer;manually annotated;automatically annotated;distinguishes gene names;annotated noisy;curation automatically annotated;recognizer flybasee curation;based annotation scheme;gene mentions enabling;general manually annotated;annotation;enabling consistent annotation;based annotation;biomedical text present;biomedical text;annotation scheme;gene mentions;consistent annotation;recognizer flybasee;gene names;annotation scheme distinguishes;consistent annotation evaluating", "pdf_keywords": ""}, "5b94512a17483595c5dffc503a16ba0b46c347e5": {"ta_keywords": "extracting disambiguated hypernymy;hypernymy extraction methodswe;hypernyms sets synonyms;improve hypernymy extraction;hypernymy extraction;disambiguated hypernymy relationships;propagate hypernyms sets;disambiguated hypernymy;hypernyms sets;relationships propagate hypernyms;hypernymy relationships propagate;synonyms synsets;synonyms synsets constructs;propagate hypernyms;sets synonyms synsets;unsupervised sense representations;extracting disambiguated;method extracting disambiguated;hypernyms;sense representations;synsets constructs embeddings;sets synonyms;hypernymy relationships;synonyms;unsupervised sense;sense aware relationships;sense representations used;relationships matching synsets;improve hypernymy;hypernymy", "pdf_keywords": ""}, "0e52ce6cfd1385e1e9304dcf71d66b53fdc2d4bd": {"ta_keywords": "introductionthe news industry;journalism relate information;news consumption;news industry undergone;news consumption habits;news industry;day news consumption;events challenges journalism;news variety sources;news variety;journalism relate;journalism;volume news variety;challenges journalism;challenges journalism relate;increase volume news;volume news;news;introductionthe news;day news;readers need new;continuing day news;sources readers;information retrie;readers;readers need;sources readers need;variety sources readers;view describing events;information", "pdf_keywords": ""}, "e705255814756178dba75638c29b602095c3cdf4": {"ta_keywords": "deep reinforcement learning;complexity learning policy;reinforcement learning dl;deep reinforcement;sample complexity learning;learning policy case;source deep reinforcement;learning policy;reinforcement learning;transfer learning experiments;complexity learning;states sample complexity;transfer learning;learning experiments;sample complexity;reinforcement;representations environment states;learning;sample complexity asking;environment states sample;dl sample complexity;set transfer learning;agents distinction representation;learning experiments experiment;learning dl sample;dl agents distinction;complexity;policy case;learning useful;representations environment", "pdf_keywords": "deep reinforcement learning;reinforcement learning dl;learn linear policy;agents trained environments;trained environments;deep reinforcement;source deep reinforcement;learning policy;layers learn;complexity learning policy;networks reinforcement learning;state learning;reinforcement learning;state learning neural;reinforcement learning use;networks reinforcement;trained environments implementation;learning policy case;current state learning;neural networks reinforcement;agents trained;layers learn linear;transfer learning;linear policy useful;transfer learning experiments;layers pretrained network;reinforcement;linear policy;performance agents trained;dav agents distinction"}, "51203e9d5620abdcdf6c9be93b1e221e79cda67d": {"ta_keywords": "language independent anrar;introductiontranslation learning language;introductiontranslation learning;external language model;external language;anrar unified;anrarar useful tool;languages using external;anrar unified sequence;anrar;using external language;anrarar useful;independent anrar unified;introductiontranslation;language independent end;resource languages;low resource languages;independent anrar;learning language independent;build language independent;resource languages using;language model;anrarar;language independent;end anrarar useful;languages using;learning language;languages;language model framework;build language", "pdf_keywords": "multilingual models trained;monolingual translational adaptation;transfer external linguistic;transfer learning linguistic;external adaptation language;external language model;translational adaptation;monolingual adaptation target;multilingual neural network;multilingual language models;languages adaptation;speech data fusion;monolingual adaptation;multilingual acoustic model;vocabulary languages adaptation;adaptation target languages;multilingual neural;multilingual adapt;novel monolingual adaptation;languages adaptation perform;translational adaptation target;multilingual models challenging;multilingual models;integrate external language;adaptation language independent;context external adaptation;speech recognition unified;language adapt;transfer learning;simple transfer learning"}, "8319786ed7b9cb13e29130b5617bf0aef586cd6f": {"ta_keywords": "cognitive tutor authoring;tutoring sistudent;tutor authoring;interactively tutors sistudent;teacher tutoring sistudent;tutors sistudent;cognitive tutor;author interactively tutors;effective cognitive tutor;tutoring sistudent leads;tutoring;interactively tutors;teacher tutoring;tutor;tutor authoring objectiveto;teaching teacher tutoring;authoring methodsthe;tutors;tutors sistudent posing;method authoring;methodsthe author interactively;authoring methodsthe author;method authoring methodsthe;best method authoring;introduction teaching;authoring;introduction teaching teacher;author interactively;sistuddent providing feedback;authoring strategy", "pdf_keywords": ""}, "c37c40db51ccfd6f93004e788102ede72578e5d8": {"ta_keywords": "information extraction filtering;information extraction;efficient information extraction;reliability important information;situations extreme reliability;filtering situations extreme;filtering situations;important information combed;information combed;efficient information;extraction filtering situations;extreme reliability;extreme reliability important;extraction filtering;information;reliability important;reliability;information combed massive;filtering;human workers;framework efficient information;human workers available;high level reliability;level reliability;reliability required;number human workers;reliability required necessary;workers available methods;needs times crisis;level reliability required", "pdf_keywords": ""}, "92259193a9d7377368790bf8517cd9798f30caae": {"ta_keywords": "question answering xq;explainable question answering;question answering;question answering 9001;web similar ai;answering xq alleviate;details learning reasoning;tackle question answering;answering xq;information pollution web;information web similar;pollution web requires;similar ai applications;learning reasoning;similar ai;ai;learning reasoning steps;details learning;pollution web;access information web;information pollution provides;ai applications black;augmenting answer aim;information web;ai applications;answering;information pollution;augmenting answer;user friendly interfaces;information", "pdf_keywords": ""}, "0e1a665334b1ec35d77ab1cd4f21bd0da9745548": {"ta_keywords": "text categorization;text categorization discussed;large text categorization;text categorization problems;methods text categorization;categorization discussed;categorization discussed recently;categorization;categorization problems algorithms;classifiers allow context;categorization problems;context sensitive learning;classification ripper;classifiers;contribute classification ripper;classification;sleeping experts phrases;learning algorithms ripperand;classifiers allow;ripperand sleeping experts;sensitive learning methods;learning methods text;context sensitive;algorithms construct classifiers;machine learning algorithms;algorithms ripperand sleeping;implemented machine learning;sensitive learning;machine learning;experts phrases evaluated", "pdf_keywords": ""}, "9ca95a09c8bf2d7d28234ff37ece182836dd8632": {"ta_keywords": "imitation learning parser;transition based parsing;learning parser learns;parser learns;learning parser;parser learns statistical;representation parsing task;parsing algorithm abstract;representation parsing;exact imitation learning;parsing task;based parsing algorithm;parser;parsing algorithm;meaning representation parsing;parsing;imitation learning algorithm;imitation learning;use imitation learning;based parsing;parsing task using;abstract meaning representation;using exact imitation;exact imitation;data use imitation;imitating actions expert;learning algorithm dagger;imitation;imitating actions;use imitation", "pdf_keywords": ""}, "0aa0131253b832fdba27ac43f8fa78a322763191": {"ta_keywords": "speech translation technology;speech translation model;translation text speech;speech translation;translation technology;used speech translation;text speech;translation technology helps;translation model;text speech synthesis;translation model composed;automatic speech;machine translation text;introduction speech translation;spoken communication;speech synthesis;composed automatic speech;machine translation;spoken communication different;recognition machine translation;speech synthesis components;speech recognition machine;speech recognition;translation text;text level spoken;level spoken communication;automatic speech recognition;communicate different languages;speech;helps people communicate", "pdf_keywords": ""}, "a94ec1cd89839aa5132118916849d46dff861914": {"ta_keywords": "mind modeling situated;mind modeling;modeling situated interactions;theory mind modeling;ground human collaboration;collaborate human;situated interactions;human collaboration;able collaborate human;agents human world;collaborative tasks;collaborative tasks performed;situated interactions introduce;theory mind plays;human collaboration communication;autonomous agents human;dataset collaborative tasks;collaborate human terms;mind plays important;world minecraft;world minecraft methodsthe;human world;pairs human subjects;collaborative;virtual blocks world;mind plays;collaborate;human subjects;collaboration;minecraft", "pdf_keywords": "partners game interaction;interactions partners game;mind human collaboration;agents interact game;game interaction;game environment interact;interact game;players infer tasks;behaviors players reasoning;collaborate human;game interaction process;ground human collaboration;interact game environment;interactions collaborative environments;agents interact;participants engaged game;partners game;human collaboration;given tasks partners;partners interaction;collaborative environments;generated game agents;collaborative tasks;able collaborate human;collaborative interactions;collaborative behaviors players;demonstrate agents interact;modeling situated interactions;tasks partners need;interactions partners interaction"}, "8278e5c2a894793e2c93c6c9f0e7535109e7858f": {"ta_keywords": "syndromic disease world;syndromic disease;occurrence syndromic disease;syndromic;disease world;disease;disease world major;management disease;occurrence syndromic;approach management disease;morbidity mortality world;major cause morbidity;morbidity mortality;cause morbidity mortality;mortality world development;mortality world;cause morbidity;mortality;morbidity;world major cause;world development new;world development;world major;development new approach;major cause;management;world;new approach management;new approach;approach management", "pdf_keywords": ""}, "c637636a2afd7968bdb893af8d2fd220fd39df8f": {"ta_keywords": "meeting recognition;time meeting recognition;meeting recognition understanding;meeting analyzer;real time meeting;meeting analyzer monitoring;time meeting analyzer;recognize speaking online;group meeting;ongoing group meeting;distant microphones omni;online manner meeting;captures utterances face;meeting;automatically recognize speaking;monitoring conversations;meeting assistance methods;distant microphone;monitoring conversations ongoing;utterances face pose;face pose speaker;meeting assistance;conversations ongoing group;distant microphones;using distant microphone;microphone array omni;pose speaker using;using distant microphones;microphones omni;recognize speaking", "pdf_keywords": ""}, "ffac42087ee4ad50df9203762db715dedd209c0b": {"ta_keywords": "tags semantic analysis;grammar semantic tags;introductionactive tags semantic;parsing tags;beneficial parsing tags;tags beneficial parsing;free grammar semantic;parsing tags spread;context free grammars;semantic analysis propose;context free grammar;tags semantic;semantic tags;grammar semantic;semantic analysis;grammars enriched semantic;introductionactive tags;tags spread parse;free grammars;semantic tags asociating;enriched semantic tags;free grammar;beneficial parsing;free grammars enriched;parsing;tags asociating rules;extract complete semantic;semantic tags beneficial;grammars enriched;rules context free", "pdf_keywords": ""}, "919c929dfa665cb0595a835b4380f96da4cd0143": {"ta_keywords": "sampling spatial fields;sampling spatial;proposed sampling spatial;sampling locations;introductionmobile sensing;sampling locations approximately;introductionmobile sensing recently;assumes sampling locations;fields mobile sensors;proposed sampling;reconstruction traditionaltraditional sampling;spatial fields mobile;sampling strategies;explores multiple sampling;sampling;recently proposed sampling;multiple sampling strategies;mobile sensors;paths sample reconstruct;sampling strategies results;mobile sensors record;multiple sampling;sampling typically;random paths sample;sensing matrices;sampling strategies random;sample reconstruct;sensing;sample reconstruct dimensional;evaluate sampling strategies", "pdf_keywords": "field sampling spatially;sampling field reconstruction;spatial sampling bandlimited;sampling bandlimited field;sampling spatial fields;sampling mobile sensors;sampling spatially;method spatial sampling;field sampling;spatial sampling;field sampling region;sampling spatial;proposed sampling spatial;sampling spatially distributed;field sampling spatiallywe;sampling reconstruction;sampling field;field sampling field;method sampling reconstruction;applied field sampling;sampling bandlimited;field mobile sensor;sampling locations approximately;fields mobile sensors;method sampling mobile;sampling locations;method field sampling;assumes sampling locations;sampling mobile;sampling reconstruction dimensional"}, "59bdf61a81e46a6c9a96c0c5f96f2f77b82ab09f": {"ta_keywords": "pipeline subsurface detonation;detonation buried x70;x70 steel pipeline;subsurface detonation buried;steel pipeline subsurface;subsurface detonation;steel pipeline;detonation buried;buried x70 steel;pipeline subsurface;behavior buried x70;buried x70;x70 steel;detonation;pipeline;dynamic behavior buried;buried;steel;x70;subsurface;behavior buried;report dynamic;dynamic;dynamic behavior;report dynamic behavior;report;behavior", "pdf_keywords": ""}, "c340b89e7b7fa84fac85cdcf38ba7007e2e71930": {"ta_keywords": "speaker embeddings clustering;clustering speaker embeddings;speaker diarization;speaker diarization mainly;introduction speaker diarization;based clustering speaker;clustering speaker;speaker embeddings;neural dirization eend;end neural dirization;neural dirization;diarization mainly;dirization eend;speaker overlaps correctly;speaker overlaps;dirization;embeddings clustering;diarization;embeddings clustering based;speaker;diarization errors directly;diarization mainly developed;minimize diarization errors;diarization errors;optimized minimize diarization;minimize diarization;introduction speaker;handle speaker;clustering;embeddings", "pdf_keywords": "based speaker diarization;speaker diarization performance;speaker diarization;good speaker diarization;speaker diarization method;vector based speaker;speech diarization;speech diarization challenge;speech diarization process;multispeaker conversations clustering;automatic speech recognition;speech recognition;speaker labels neural;clustering based speech;automatic speech;speech recognition highly;useful automatic speech;diarization self attention;speech recognition systems;sequence speaker labels;global speaker characteristics;captured global speaker;end neural diarization;based neural diarization;speaker recordings training;recognition performance multispeaker;speech recognition article;approach speech recognition;speaker identity useful;speech recognition performance"}, "0fcdf20477f907aa50578876226f5fabf5e074ea": {"ta_keywords": "recommender systems exposure;link prediction;link prediction methods;introduction link prediction;learning exposure probabilities;recommender systems;recommender;exposure probabilities mitigate;frequently applied recommender;known exposure probabilities;applied recommender systems;applied recommender;exposure bias;bias consequent feedback;exposure bias arise;prediction methods frequently;learning exposure;probabilities mitigate bias;exposure probabilities;known exposure;prediction methods;prediction;exposure probabilities data;bias arise users;items propose estimators;systems exposure bias;probabilities mitigate;function learning exposure;mitigate bias consequent;loss function learning", "pdf_keywords": "bias link recommendation;bias link recommendationa;risk link recommendation;link prediction;link prediction data;link recommendation estimation;link recommendation measure;recommendation measure exposure;risk link;method link prediction;estimator link recommendation;recommendation measure risk;link probabilities estimators;demonstrate risk link;relative probability recommending;link probabilities feedback;bias link;probability recommending;learning exposure probabilities;link recommendation correcting;probability recommending nodes;link recommendation sufficient;probability recommendations significantly;link recommendationa random;graph exposure bias;link probability models;link recommendation new;exposure bias training;bias training evaluation;citation networks"}, "da1296f071bb2b65ae9e0b016d914d24b4edb2d2": {"ta_keywords": "frontier capital markets;modeling frontier markets;frontier markets succeedand;frontier markets;frontiercapital market;undeveloped capital markets;frontiercapital market designation;markets achievement frontiercapital;metrics frontier capital;capital market development;capital markets;methods undeveloped capital;capital markets network;frontier capital;capital market;achievement frontiercapital market;market development stability;basedanalysis capital market;liquid capital market;modeling frontier;capital markets achievement;undeveloped capital;frontiercapital;direct investment global;network basedanalysis capital;investment global;underlying metrics frontier;metrics frontier;investment global investment;foreign direct investment", "pdf_keywords": ""}, "916c8553beb3ba4e0d20ba6d7eb2bca365d820c8": {"ta_keywords": "etiology disease;etiology disease patient;disease etiology;etiology disease understood;disease etiology disease;etiology;disease;disease patient;disease understood report;history disease etiology;patient treated combination;disease patient treated;history disease;disease understood;patient history disease;report case patient;case patient;patient treated;case patient history;patient history;treated combination;treated combination combination;patient;report case;understood report case;combination combination;combination;combination combination combination;treated;history", "pdf_keywords": ""}, "7f613ab03d776f996eb582f04d258a51868dca03": {"ta_keywords": "alkyl benzohydrazides hydrazine;benzohydrazides hydrazine insertion;benzohydrazides hydrazine;hydrazine insertion catalyzed;non alkyl benzohydrazides;alkyl benzohydrazides;catalyzed lipase methods;benzohydrazides classic organic;insertion catalyzed lipase;benzohydrazides;alkyl benzohydrazides classic;background alkyl benzohydrazides;catalyzed lipase;lipase methods optimal;synthesis non alkyl;benzohydrazides classic;hydrazine insertion;lipase methods;hydrazine;developed synthesis non;lipase;developed synthesis;synthesis;synthesis non;insertion catalyzed;method developed synthesis;organic chemistry;utilized organic chemistry;organic compounds widely;organic chemistry study", "pdf_keywords": ""}, "2aa85084315a4107e2b9b935506b4e9f11428601": {"ta_keywords": "reflectivity caries lesions;diffuse reflectivity caries;reflectivity caries;caries lesions drying;caries lesions indicative;caries lesions;zone caries lesions;optical coherence tomography;lesions drying;surface zone caries;coherence tomography;lesions drying air;coherence tomography used;zone caries;optical coherence;caries;changes diffuse reflectivity;diffuse reflectivity;lesions indicative remineralization;shown optical coherence;lesions indicative;lesions;tomography;tomography used formation;reflectivity;assess lesion;water absorption;drying;transparent surface zone;tomography used", "pdf_keywords": ""}, "542ad79bb96fc10c46086778aaafc8f3509c5c18": {"ta_keywords": "gradient descent ascent;algorithms gradient descent;gradient descent;descent ascent gd;minimax;descent ascent;minimax problems;backgroundnonconvex minimax problems;backgroundnonconvex minimax;solving nonconvex games;algorithms gradient;simple algorithms gradient;ascent gd;minimax problems appear;nonconvex games;emerging machine learning;vanilla gd algorithms;gd algorithms;ascent;practice solving nonconvex;ascent gd common;gd algorithms constant;diverge convex setting;potentially diverge convex;convex setting methodsin;diverge convex;solving nonconvex;algorithms constant step;nonconvex;descent", "pdf_keywords": "algorithms nonconvex minimax;alternating gradient descent;gradient descent ascent;algorithms general minimax;ascent gd algorithms;algorithms gradient descent;algorithm subclass minimax;optimizing minimax;descent ascent algorithm;nonconvex minimax;optimizing minimax problems;descent ascent gd;ascent algorithm sided;gradient descent;convergence gradient descent;general minimax;nonconvex nonconcave minimax;general minimax problems;nonconcave minimax;minimax problems algorithm;nonconvex minimax problems;descent ascent;nonconcave minimax problems;gradient descent converges;optimization nonconvex objectives;minimax problems studied;ascent algorithm;minimax;minimax problems sided;proof theorem minimax"}, "69ee9b3a915951cc84b74599a3a2699a66d4004f": {"ta_keywords": "semantic representations vision;learn transferable concepts;generalizable semantic representations;representations vision lan;learning generalizable;learning generalizable semantic;generalizable semantic;quickly learn transferable;networks learn dexterous;semantic representations;transferable concepts parallel;spatial reasoning;precise spatial reasoning;transferable concepts;learn transferable;networks learn;representations vision;learn dexterous skills;progress learning generalizable;end networks learn;manipulate objects precisely;ability manipulate objects;learning;manipulate objects;dexterous skills;quickly learn;spatial reasoning methods;learn dexterous;networks;vision lan", "pdf_keywords": "manipulation semantic spatial;learning manipulating objects;manipulation tasks;manipulation tasks including;conditioned manipulation tasks;manipulation tasks demonstrate;manipulation semantic;manipulation robots highly;learning manipulating;languageconditioned imitation learning;robot context language;robot demonstrate language;manipulation robots;learning human robots;spatial reasoning robot;trained vision language;vision language models;world manipulation robots;reasoning robot context;languageconditioned imitation;manipulation robotics demonstrate;language conditioned tasks;robot model trained;task robot;tasks learning;vision language;robots ability learn;reasoning robot;manipulation robotics;robot context"}, "fa3826770207f7bf8bd85a8e97c9ac437f46b061": {"ta_keywords": "rna experiments;rna experiments involve;scale rna experiments;small scale rna;scale rna;multiple comparisons baseline;rna;comparisons baseline procedures;comparisons baseline;multiple comparisons ensuring;multiple comparisons;adjust multiple comparisons;involve multiple comparisons;comparisons ensuring;inference procedures small;comparisons ensuring probability;comparisons;trec runs;baseline procedures;test collections;evaluate statistical inference;experiments involve multiple;available test collections;statistical inference procedures;trec runs runs;inference procedures;observing false positive;baseline procedures adjust;false positive experiment;employ trec runs", "pdf_keywords": ""}, "ce89ee7aaeeea2c9d474707690f3ea9d948776a3": {"ta_keywords": "translation systems;noisy inputs translations;modern machine translation;machine translation systems;translation systems growing;machine translation;inputs translations;available parallel corpora;synthetically created datasets;parallel corpora naturally;parallel corpora;inputs translations previous;translations;corpora naturally;corpora;translations previous;translations previous work;creating noise robust;noise robust;datasets;corpora naturally occurring;noise robust systems;translation;synthetically;datasets methods;datasets methods paper;synthetically created;noisy inputs;mistranslations modern machine;evaluating synthetically", "pdf_keywords": "benchmark translation noisy;machine translation noisy;improve translations noisy;translation models robustness;testbed machine translation;translation noisy;translation noisy text;benchmark translation;machine translation models;translation models;neural machine translation;translations noisy;machine translation nmr;translations noisy text;machine translation test;noisy language modeling;noisy text languages;machine translation increasing;dataset machine translation;machine translation impact;machine translation;translation noisy non;appropriate benchmark translation;machine translation corpora;improve translations;approaches noisy language;translation nmr;does improve translations;method translation noisy;noisy language"}, "7f85b7ee0fc6cdb5b92417035a7049247729545a": {"ta_keywords": "imbalance performance classifiers;classifiers methods empirical;classifiers;classifiers benchmark;classifiers benchmark datasets;performed classifiers benchmark;class imbalance performance;intelligenceligence natural language;classifiers methods;performed classifiers;artificial intelligenceligence natural;performance classifiers;study performed classifiers;class imbalance;artificial intelligenceligence;performance classifiers methods;background artificial intelligenceligence;training data size;imbalance performance;size class imbalance;training data;intelligenceligence natural;datasets;intelligenceligence;imbalance;natural language;benchmark datasets;natural language 8th;effects training data;data size class", "pdf_keywords": ""}, "ce5a57c0ccc8993f4a8e3a07101140a757024d9f": {"ta_keywords": "public speeches memorable;speech automatic memorable;natural expressive speech;public speeches;questions public speeches;speeches memorable;expressive speech public;speech public talks;quote detection aim;speeches;speeches memorable inspirational;quote detection;spotken quote detection;expressive speech;expressive speech automatic;speech public;speech;automatic memorable;speech automatic;convey important messages;automatic memorable spotken;public talks;natural expressive;audience consciousness study;study natural expressive;memorable inspirational audience;memorable spotken quote;people convey important;public talks specifically;talks", "pdf_keywords": ""}, "61ad8a0778598022e71c0ee3ba9bc53ddd616517": {"ta_keywords": "parsing question answering;question answering;answering answering complicated;realistic task answering;answering answering;question answering focused;answering complicated;background answering answering;task answering;complicated question intents;decomposed question sequences;answering complicated question;question intents expressed;semantic parsing question;answering focused;normal conversation humans;answering focused long;question intents;semantic parsing;answering;problem conversational;background answering;normal conversation;important problem conversational;conversation humans;conversation humans effort;conversation;conversational;parsing;explore conversational", "pdf_keywords": "parsing question answering;sequenceential question answering;question answering tasks;question answering systems;question answering;interactive question answering;question answering development;task answering sequences;semantic parsing tables;sequential question answering;semantic parsing;question answering focused;semantic parsing question;semantic parsing end;realistic task answering;structured tables fromwikipedia;task answering;systems semantic parsing;answering systems;decompose multiple questions;semantic parsing dataset;questions neural;work semantic parsing;question sequences inquire;explore conversational qa;dataset semantic parsing;answering development computational;parsing tables;answering sequences;parsing"}, "9837207d3f4ee8c493375a97077c6f8b22cadac9": {"ta_keywords": "constitutional commentary court;constitutional commentarycombines;commentary court constitutional;present constitutional commentary;constitutional commentary;constitutional constitutional commentary;commentary constitutional commentary;commentary constitutional;constitutional commentary constitutional;form constitutional commentarycombines;court constitutional;conclusion court doctrinal;court doctrinal explanations;court constitutional constitutional;suprem court reasoning;constitutional commentarycombines despair;court reasoning;court doctrinal;commentary court ubiquitous;invalidity durational residency;article present constitutional;present constitutional;durational residency tests;conclusion court;court reasoning claim;constitutional;constitutional constitutional;durational residency;constitutional constitutional constitutional;despair suprem court", "pdf_keywords": ""}, "de9d3a28f9e112a248d097d72ba6ad41a71c8a78": {"ta_keywords": "learning clause logic;learning logic programs;clause logic programs;learning learning logic;learning clause;learning logic;cryptographic limits learning;limits learning clause;introductioncryptographic limits learning;logic programs examples;logic programs;problem learning single;logic programs methodsthis;single horn clause;learning single;limits learning;clause logic;clause focus generalizations;horn clause;depth determinate clauses;investigates problem learning;clauses;problem learning;logic programs active;learning single horn;clause;learning systems resultswe;clauses used practical;determinate clauses;generalizations language constant", "pdf_keywords": ""}, "bcde1ba141078cf37a69a691fd329d8fd7e70b9b": {"ta_keywords": "malignant neoplasm gastrointestinal;neoplasm gastrointestinal tract;neoplasm gastrointestinal;gastrointestinal tract malignant;tract malignant neoplasm;diagnosed malignant neoplasm;malignant neoplasm;patient malignant neoplasm;tract malignant;tract patient malignant;patient diagnosed malignant;diagnosed malignant;gastrointestinal tract patient;patient malignant;malignant;neoplasm;gastrointestinal tract;gastrointestinal;case patient diagnosed;patient diagnosed;present case patient;tract patient;diagnosed;case patient;article present case;tract;patient;article present;present case;case", "pdf_keywords": ""}, "83d6b4bfa8701578c291e55f5f1e5e6508aff313": {"ta_keywords": "patient autonomy;patient autonomy ability;patient autonomy model;focus patient autonomy;matter patient autonomy;autonomy model wellbeing;wellbeing technology users;autonomy;ethics consider computer;autonomy ability;wellbeing technology;medical ethics;autonomy model;autonomy ability set;medical ethics consider;model wellbeing technology;concepts medical ethics;technology impact wellbeing;technology users;medical;model wellbeing;wellbeing people use;ethics;technology users methodswe;ethics consider;wellbeing;thinking concretely technology;computer science ai;concepts medical;ai", "pdf_keywords": ""}, "1bc87dba9838b3028b636f456084252f2beac108": {"ta_keywords": "game encouraging energy;game building energy;efficient behavior occupants;behavior occupants distributing;lottery estimate occupants;results social game;energy efficient behavior;game building;non cooperative game;cooperative game;social game;backgroundsocial game building;social game encouraging;encouraging energy efficient;cooperative game estimated;behavior occupants;occupants distributing points;backgroundsocial game;building energy efficiency;encouraging energy;occupants distributing;game encouraging;efficient behavior;staackelberg game;building manager occupants;building energy;play non cooperative;game estimated utilities;energy efficiency incidentive;game", "pdf_keywords": ""}, "f016ac107259d6d222c9f52b37208fca4fa1d6bc": {"ta_keywords": "ads specifying requirements;specifying requirements ads;driving systems ads;requirements ads method;quality ads specifying;specifying requirements;requirements ads;notations specific ads;defections autonomous driving;ads specifying;needs terms notations;quality requirements;high quality requirements;quality requirements highly;driving systems;systems ads;ads method needs;terms notations specific;autonomous driving systems;complex traffic environments;traffic environments;terms notations;ads complex traffic;quality ads;systems ads result;requirements;autonomous driving;high quality ads;ads method;avoid defects need", "pdf_keywords": ""}, "36c770b79937db2e3416204b8cf177d0c9881f54": {"ta_keywords": "mri femoral femoral;imaging mri femoral;mri femoral;femoral femoral femoral;femoral femoral;femoral;magnetic resonance imaging;imaging mri;mri;resonance imaging mri;magnetic resonance;resonance imaging;imaging;magnetic;resonance", "pdf_keywords": ""}, "0a485fd94b2cb554e281d0f8d7e9f71db4891ce0": {"ta_keywords": "purposetoken pooling vision;novel token downsampling;token downsampling;pooling vision transformers;softmax attention;token downsampling method;images intermediate token;purposetoken pooling;pooling vision;token pooling efficiently;token representations;token pooling;intermediate token representations;softmax;assumptions softmax attention;vision transformers simple;vision transformers;softmax attention acts;exploiting redundancies images;called token pooling;mild assumptions softmax;token representations resultswe;pooling efficiently exploiting;purposetoken;pooling efficiently;representations resultswe mild;downsampling;token;intermediate token;assumptions softmax", "pdf_keywords": "novel token downsampling;token pooling improves;attention vision transformers;clustering algorithm attention;token downsampling method;token downsampling;computation vision transformers;vision transformers efficient;token pooling algorithms;new token downsampling;token downsampling principle;efficient vision transformers;downsample feature maps;tokens computational bottleneck;tokens weighted clustering;token pooling effective;vision transformers computational;data aware downsampling;bottleneck vision transformers;clustering tokens weighted;token pooling layers;redundancy features clustering;clustering tokens;demonstrate token pooling;attention major computation;algorithm attention layer;computational bottleneck vision;algorithm attention;method clustering tokens;feature maps pruning"}, "d1f32060e921b6e06badd7fdb2b750638b2d131c": {"ta_keywords": "deep beamforming networks;deep beamforming;beamforming feature extraction;beamforming networks;feature extraction acoustic;channel speech recognition;beamforming networks multi;multi channel speech;acoustic processing;acoustic modeling components;acoustic processing including;beamforming feature;acoustic modeling;stages acoustic processing;extraction acoustic modeling;including beamforming feature;reverberation captured speech;beam estimated network;speech recognition challenging;processing including beamforming;speech recognition;domain beam estimated;background deep beamforming;including beamforming;beamforming;captured speech signals;channel speech;extraction acoustic;represent stages acoustic;stages acoustic", "pdf_keywords": ""}, "98290fb02a844108df202e9a6dc3461e3f14ee32": {"ta_keywords": "nonconvex nonconcave min;nonconcave min max;nonconcave min;min max optimization;points nonconvex nonconcave;nonconvex nonconcave;stationary points nonconvex;optimal order stationary;max optimization small;max optimization;optimization small maximization;small maximization domain;maximization domain studied;convex concave approach;nonconcave;small maximization;min max problems;maximization domain;convex concave;optimization;concave approach;concave approach relies;max problems objective;points nonconvex;maximization;studied order optimal;optimization small;order stationary points;order optimal;assumed convex concave", "pdf_keywords": "nonconcave minimax optimization;nonconvex nonconcave minimax;non convex minimax;nonconvex concave minimax;nonconvex nonconcave min;convex minimax optimization;minimize nonconvex concave;nonconcave minimax;nonconvex concave optimizing;minimize nonconvex;convex minimax;minimization nonconvex;nonconvex nondifferentiable minimax;stationary points optimization;minimax optimization;nonconcave min max;non convex min;concave optimizing problems;nonconvex quadratic minimization;min max optimization;quadratic minimization nonconvex;nonconcave min;nonconvex quadratic optimization;optimization method nonconvex;concave minimax problems;optimization nonconvex;maximizer nonconcave;minimax optimization algorithms;maximizer nonconcave quadratic;solve minimize nonconvex"}, "ad26e5105b6019ff68404962e39ea3a1dfb5931d": {"ta_keywords": "temporaryral logic objectives;design temporaryral logic;temporaryral logic;optimal behavior incentives;agent finite decision;design temporaryral;agent optimal behavior;temporal logic formula;agent optimal;introductioninentive design temporaryral;optimal sequence incentives;agent agent optimal;temporal logic;behavior markov decision;expressed temporal logic;markov decision process;markov decision;finite decision;finite decision horizon;optimal behavior;objective expressed temporal;logic objectives;designing optimal;sequence incentives principal;behavior markov;temporaryral;sequence incentives;behavior incentives;behavior incentives realizes;incentives realizes principal", "pdf_keywords": "temporaryral logic objectives;optimal incentive design;incentive sequence minimizes;design incentive sequence;feasible incentive design;incentive sequence problem;agent sequence incentives;propose optimal incentive;incentive design agent;design temporaryral logic;ensuring optimal agent;modeled incentive design;agent optimal decision;incentives agent simple;incentive sequence;optimal incentive;generated optimal decision;process modeled incentive;optimal agent;described incentive design;synthesize sequence incentives;propose incentives agent;synthesize optimal policy;generate incentive design;agent sequential decision;incentive design problem;optimize agent policy;incentive design solves;incentive design described;sequence incentives minimizes"}, "89a8edbc0fe2ea8b9ee703ca37e5d5d6d34c571a": {"ta_keywords": "neural machine translation;source transcription pre;transcription pre training;speech translation e2e;source transcription;training automatic speech;machine translation nmt;leverage source transcription;translation nmt tasks;transcription;end speech translation;machine translation;knowledge distillation seq;transcription pre;knowledge distillation;speech translation;level knowledge distillation;sequence level knowledge;automatic speech;translation e2e test;source language text;translation e2e;training automatic;leverage source language;translation nmt;source language;distillation seq;speech recognition;automatic speech recognition;speech recognition ar", "pdf_keywords": "neural speech translation;training automatic speech;autoregressive speech source;speech translation e2e;speech source text;end speech translation;speech translation seqkd;automatic speech;methods automatic speech;paraphrased source transcriptions;neural machine translation;speech translation;source transcription;source transcription pre;transcription pre training;source transcriptions;autoregressive speech;speech source;performance bilingual translational;bidirectional neural speech;translation performance autoregressive;speech recognition ar;source transcriptions generated;training languages;translate foreign speech;boosted translation performance;multilingual automated automated;decoder improve paraphrasing;language model generate;speech fundamental step"}, "daa7e6af585d03e9cb05487413a6495f23400398": {"ta_keywords": "assignment wireless networks;learningbased resource assignment;assignment wireless;assignment problems wireless;resource assignment wireless;generating feasible assignment;feasible assignment solutions;feasible assignment;assignment solutions;network learns;binary assignment problems;binary assignment;sinkhorn neural network;networks identifies binary;resource assignment;network training;approach binary assignment;wireless networks identifies;deep learning;assignment problems;wireless networks;learning approach binary;network learns non;neural network learns;problems wireless networks;assignment solutions resultsto;learningbased resource;networks;network training strategies;neural network training", "pdf_keywords": "sinkhorn neural network;learning arbitrary assignment;minimization network;feasible assignment solution;neural network;described minimization network;deep neural network;feasible assignment;neural network learns;arbitrary assignment tasks;convex assignment problems;identifies feasible assignment;neural network proposed;assignment matrix;assignment solution;non convex assignment;deep learning;network learns;learns non convex;assignment problem described;deep neural;convex assignment;assignment solution output;proposed sinkhorn neural;joint optimizationwe propose;trained deep neural;network assignment problem;joint optimizationwe;minimization network cost;softmax function sinkhorn"}, "f11ed27f4640dd8785ea7c4aff9705ddaad2b24d": {"ta_keywords": "fake news detection;multimedia technology fake;technology fake news;content important fake;effects fake news;fake news case;fake news attempts;proliferation fake news;news detection social;videos attract mislead;news case presentationthis;social media promotes;technology fake;fake news caused;detection social media;important fake news;attempts utilize multimedia;multimedia content;images videos attract;news attempts utilize;news detection;multimedia content images;content images;utilize multimedia content;fake news;news attempts;content images videos;media promotes;news case;multimedia technology", "pdf_keywords": ""}, "ad4b09832454a821e925e45e96e769f0c01bd3d6": {"ta_keywords": "statistical topic models;topic models;topic models latent;documents topics;models latent dirichlet;latent dirichlet al;latent dirichlet;documents topics consequence;topics;summarize large document;representation documents topics;words representation documents;statistical topic;words documents results;words documents;document collections;large document collections;bag words representation;model visualize summarize;ity words documents;topics consequence;dirichlet al;dirichlet al location;introduction statistical topic;document collections completely;representation documents;results bag words;visualize summarize;bag words;words representation", "pdf_keywords": ""}, "1f133158a8973fb33fea188f20517cd7e69bfe7f": {"ta_keywords": "attention sublayer transformer;sublayer transformer encoder;transformer encoder efficient;transformer encoder;self attention sublayer;attention sublayer;encoder efficient;encoder efficient particularly;sublayer transformer;encoder;conversion self attention;transformer;self attention;attention;sublayer;conversion self;efficient smaller model;self;model;smaller model;efficient particularly efficient;conversion;efficient;particularly efficient;particularly efficient smaller;efficient smaller;efficient particularly;smaller model sizes;model sizes;smaller", "pdf_keywords": "attention sublayer transformer;attention sublayer;attention sublayers;self attention sublayer;attention model;attention sublayers simple;self attention sublayers;transform self attention;transformer encoder architectures;classification task fourier;attention model semantic;neural language learning;encoder architectures sped;sublayer transformer encoder;new neural networks;resulting neural networks;fourier transform fnet;fnet fourier transform;transform used neural;neural networks important;new neural;transform fnet fourier;neural;neural networks optimize;model self attention;neural language;neural networks;learning demonstrate fnet;efficient transfer learning;fast fourier transform"}, "c9ce3889c03fee2990b2277423bbc0fb4366df53": {"ta_keywords": "discriminative language modeling;learned word features;dis discriminative language;discriminative language;neural network embeddings;embeddings learned word;language modeling structured;modeling structured classification;language modeling;product feature representation;word features;word features extracted;structured classification;embeddings learned;feature representation;feature representation used;network embeddings learned;dis discriminative;structured classification problem;dot product feature;discriminative;function parameterized neural;log linear models;embeddings;classification problem log;features extracted;parameterized neural;network embeddings;background dis discriminative;automatically use convolutional", "pdf_keywords": ""}, "2de8019fd7d04e3d1305d5efaeeb591f0d966550": {"ta_keywords": "speech recognition ar;autoregressive transformer;non autoregressive transformer;autoregressive transformer structure;speech recognition;automatic speech recognition;deep transformers;short term memory;automatic speech;deep transformers outperformed;speech recognition production;recently deep transformers;structure automatic speech;term memory networks;memory networks;transformers outperformed conventional;large margin speech;transformer;transformer structure automatic;margin speech recognition;term memory;transformers outperformed;memory networks large;transformers;recognition ar chmm;autoregressive;recognition ar;transformer structure;ar chmm fmm;non autoregressive", "pdf_keywords": ""}, "ca2144b895cf6812eec535261df9294896417425": {"ta_keywords": "scientific relationship extraction;relation extraction classification;semantic relation extraction;relationship extraction selectively;relation extraction;relationship extraction;relation extraction model;end relation extraction;extraction classification scientific;introduction scientific relationship;incorporated concept embeddings;semantic relation;scientific relationship;concept embeddings;classification scientific papers;concept candidate embeddings;candidate embeddings result;extraction classification;candidate embeddings;concept embeddings aimto;relation;task semantic relation;scientific papers methodswe;classification scientific;embeddings result;scientific papers;embeddings;papers methodswe extend;end relation;semantic", "pdf_keywords": "relation classification task;relation extraction classification;semantic relation extraction;relation classification;relation extraction task;relation extraction;relation classification artificial;relation extraction model;end relation extraction;second relation classification;ranked relation classification;algorithm relation classification;task semantic relation;senerio relation extraction;classification task subtask;classification task subthe;semantic relation;extraction task subtask;concept candidate embeddings;information extraction;information human corpus;task task semantic;task semantic;candidate embeddings;encoding attention;models information extraction;information extraction proposing;extraction classification scientific;embedding resulting embeddings;resulting embeddings"}, "3638e5dfc79ba3fb757900f46ac0c7e7f6dadb05": {"ta_keywords": "technology photographic practices;photos sharing;cameraphone photos sharing;photos sharing methodsdevelopments;personal photography;experience personal photography;technology photographic;people use photos;cameraphone photos;image capture sharing;emerging technology photographic;photographic practices;personal photography designing;empirical study cameraphone;networked digital imaging;personal networked digital;photography designing technology;photographic;study cameraphone photos;photography;use photos;uses personal networked;personal networked;photographic practices vice;technology image capture;digital imaging promise;cameraphone;technology image;photography designing;capture sharing", "pdf_keywords": ""}, "86eb740bbc54a6d734242be28fccf76fd4d2c1ba": {"ta_keywords": "quadratic dynamic game;bound differential nash;known nash equilibrium;cooperative multi agent;algorithms non cooperative;learning algorithms non;finite time convergence;differential nash;differential nash support;dynamic game known;available learning algorithms;continuous game;dynamic game;nash equilibrium;time convergence bound;methods available learning;available learning;continuous game illustrates;learning algorithms;non cooperative multi;time convergence;analytical convergence guarantee;game known nash;cooperative multi;quadratic dynamic;nash support analysis;convergence guarantee linear;finite time;non cooperative;linear quadratic dynamic", "pdf_keywords": ""}, "0fdc3efc11526995d192f18e19f07fba062a76f7": {"ta_keywords": "programmatic weak supervision;labeling training;weak supervision paradigms;noisy supervision sources;introduction labeling training;labeling training data;weak supervision;various weak supervision;potentially noisy supervision;noisy supervision;synthesizing training labels;weak supervision pws;training labels;supervision sources methods;supervision paradigms programmatic;supervision sources;labeling bottleneck programmatically;labeling;manual labeling bottleneck;training labels multiple;manual labeling;supervision paradigms;easing manual labeling;introduction labeling;labeling bottleneck;supervision pws achieved;supervision;programmatically synthesizing training;supervision pws;labels", "pdf_keywords": "supervision weakly supervised;programmatic weak supervision;weak supervision models;use weakly supervised;approach weakly supervised;weakly supervised;weakly supervised models;weakly supervised learning;improving weak supervision;supervision weakly;implementation weakly supervised;designing weak supervision;effectiveness weakly supervised;development weakly supervised;weakly supervised classification;weak supervision weakly;weak supervision field;weak supervision sources;field weak supervision;weak supervision pw;potential weak supervision;weak supervision;state weak supervision;weak supervision article;status weak supervision;supervision models;supervision field weak;ability weakly supervised;labeling;supervised"}, "66340a93813d8f816a8c82354a8f39fa985de27f": {"ta_keywords": "domain question answering;arc challenge dataset;question answering;questions selected challenging;challenge dataset;question answering important;introduced arc challenge;challenge dataset contains;arc challenge;exams questions selected;selected challenging;multiple choice questions;retrieval methods corpus;questions authored grade;questions selected;answering important problem;science exams questions;exams questions;challenge;school science exams;choice questions authored;science exams;advances information retrieval;choice questions;answering;retrieval methods;retrieval;exams;questions;information retrieval", "pdf_keywords": ""}, "2ac6b8ade2a5e1ac89b99012ca6548eca4f8323f": {"ta_keywords": "topological layer based;landscapes based persistent;novel topological layer;topological layer;based persistent landscapes;layer based persistent;persistent landscapes based;topological layer general;persistent landscapes proposed;persistent landscapes;persistent homology arbitrary;persistent homology;introductionefefficient topological layer;persistent landscapes use;general persistent homology;models based persistent;layer general deep;novel topological;propose novel topological;inputs general persistent;general persistent;layer based;layer general;landscapes use robust;based persistent;layer;topological;persistent;proposed layer;landscapes based", "pdf_keywords": "topological layer based;novel topological layer;based topological layer;topological layer;topological features input;limitations topological layer;features complex persistence;exploit topological features;persistence landscapes robust;represent topological features;topological features effectively;exploit underlying topological;topological data;topological layer general;persistent homology arbitrary;new topological layer;persistent homology collection;introduce persistent homology;persistent homology;underlying topological features;represent topological;persistence landscapes efficiently;topological features complex;persistent homology multiscale;implement novel topological;using persistence landscapes;topological features;diagrams persistence landscapes;approach represent topological;complex persistence"}, "1fa02e5a5adffe82a41225f61f5f8ce86cf229d0": {"ta_keywords": "normalized cut;normalized cut meets;introduction normalized cut;mr normalized cut;cut meets malferrofluorofluorofluorofluorofluorodeoxyglucose;segmentation clustering model;segmentation;segmentation clustering;new segmentation;new segmentation clustering;normalized cut nc;propose new segmentation;meets malferrofluorofluorofluorofluorofluorodeoxyglucose mri;malferrofluorofluorofluorofluorofluorodeoxyglucose mri;malferrofluorofluorofluorofluorofluorodeoxyglucose mri important;markov random field;clustering;field mr normalized;clustering model;meets malferrofluorofluorofluorofluorofluorodeoxyglucose;normalized;random field mr;mr normalized;malferrofluorofluorofluorofluorofluorodeoxyglucose;random field;clustering model combines;cut meets;cut;introduction normalized;markov random", "pdf_keywords": ""}, "1cfd9b1db68fc320698da05fc6876dd0ea96fc9b": {"ta_keywords": "speech recognition ar;auditory speech recognition;speech recognition;automatic speech recognition;automatic speech;auditory speech;method auditory speech;end automatic speech;pruning method auditory;mobile embedded devices;model mobile embedded;mobile embedded;power energy consumption;energy consumption;task device computational;auditory;device computational power;energy consumption requirements;embedded devices challenging;method auditory;embedded devices;speech;devices challenging;devices challenging task;recognition ar model;device computational;computational power energy;ar model mobile;power energy;task device", "pdf_keywords": "networks speech recognition;training deep;training deep neural;loss layer pruning;neural networks speech;layer pruning stochastic;connectionist temporal classification;layer pruning;layer pruning layer;layer pruning present;networks speech;tuning speech recognition;pruning layer;deep neural network;pruning layer significantly;speech recognition;problem training deep;pruning stochastic depth;deep neural;layer pruning employ;reduce depth training;speech recognition proc;speech recognition ar;training pruning;layer pruning method;auxiliary loss layer;used layer pruning;layer pruning problem;neural network removing;training pruning method"}, "acbb4495dd698b3190db6899d7d35b0817e0a85e": {"ta_keywords": "minimax learning;trained minimax;performance trained minimax;trained minimax model;minimax learning problems;minimax optimization;minimax optimization algorithm;generative adversarial networks;problems generative adversarial;generative adversarial;success minimax learning;depend minimax optimization;adversarial networks;generalization performance trained;minimax;adversarial networks observed;adversarial;minimax model methodsto;minimax model;depend minimax;learning problems generative;success minimax;introductionthe success minimax;generative;observed depend minimax;generalization performance;underlying optimization;problems generative;properties underlying optimization;underlying optimization algorithm", "pdf_keywords": "concave minimax learning;minimax learning;gradientin general minimax;minimax learning objective;convex minimax objective;convex minimax;trained minimax;minimax optimization strongly;minimax learning problems;minimax objectives gd;performance trained minimax;general minimax;non convex minimax;minimax optimization algorithms;convex concave minimax;minimax optimization;concave minimax objective;depend minimax optimization;concave minimax objectives;gradient descent ascent;minimax objective optimal;strongly concave minimax;generalization risk convex;minimax objective method;descent ascent proximal;minimax objective;success minimax learning;trained minimax model;minimax optimization algorithm;analyze minimax optimization"}, "0053f75b7053f43b9787a9955426281e672b147b": {"ta_keywords": "outside recursive autoencoder;syntax simultaneously learns;recursive autoencoder;backgroundunsupervised latent tree;recursive autoencoder dor;recursive auto encoders;discovering syntax simultaneously;discovering syntax;latent tree induction;learns representations;tree induction deep;learns representations constituents;autoencoder;simultaneously learns representations;autoencoder dor fully;autoencoder dor;backgroundunsupervised latent;auto encoders;induction deep;latent tree;tree induction;predicts word input;induction deep inside;introduce deep inside;learns;outside recursive auto;auto encoders methodswe;simultaneously learns;method discovering syntax;inside outside recursive", "pdf_keywords": "outside recursive autoencoders;recursive autoencoders;inducing syntactic trees;generating sentence representation;inducing syntactic;syntax simultaneously learns;recursive autoencoders dora;syntactic trees representations;trained supervised parser;discovering syntax simultaneously;discovering syntax;capturing representing syntactic;autoencoders;neural language models;sentence representation;syntactic trees;language inference neural;learning sentences;representing syntactic information;reconstruct input word;representing syntactic;learns representations;supervised parser;generating sentence;processing sentences challenging;model processing sentences;supervised parser model;constituency parsing model;language models learning;nonlabeled constituency parsing"}, "95e8edd26744ecc2bc23996cfaa68fe6252442a9": {"ta_keywords": "fake news detection;automatic fake news;news claim methodsmost;evidence based fake;veracity news claim;news detection evidences;probe veracity news;fake news critical;perniciousness fake news;veracity news;news claim;based fake news;claim evidence interaction;fake news;evidence interaction based;capture claim evidence;detection evidences;news detection;claim evidence;evidence based;evidence interaction;automatic fake;news critical issue;prevalence perniciousness fake;veracity;news detection turn;news critical;detection evidences utilized;based fake;embed semantic", "pdf_keywords": "detecting fake news;evidence graphs;claim evidence graphs;fake news detection;graph based fake;automatic fake news;graph based semantic;graph based semantics;evidence aware fake;evidence based fake;aware evidence representations;models fake news;evidences design graph;graph structure learning;evidence unified news;claim evidence embeddings;evidence embeddings;news employ graph;news veracity propose;news veracity;evidence representations;prediction news veracity;aware fake news;detection based evidences;semantic structure boost;graph based;news detection model;employ graph structure;graph based models;based fake news"}, "452059171226626718eb677358836328f884298e": {"ta_keywords": "question answering;question answering qa;iterative attention;iterative attention process;tasks natural language;questions forms episodic;attention inputs;cast question answering;questions trigger iterative;condition attention inputs;trigger iterative attention;model condition attention;answering qa;answering qa problems;dynamic memory network;introduce dynamic memory;neural network architecture;neural;attention process;condition attention;attention inputs result;dynamic memory;generates relevant answers;natural language processing;attention;neural network;dn neural;qa problems language;episodic memories generates;questions forms", "pdf_keywords": "task based recurrent;tasks natural language;recurrent network implement;question answering;recurrent network;question answering directly;gated recurrent network;neural network recurrent;use gated recurrent;recurrent network application;answer episodic memory;recurrent neural network;network recurrent;recurrent neural;network recurrent network;generates answer episodic;language question answering;question answering problems;iterative attention;iterative attention process;episodic memory module;gated recurrent;model natural language;based recurrent neural;implement episodic memory;task episodic memory;answering problems language;language processing tasks;babii task based;architecture natural language"}, "c39ac49e2d3feec992e84868256cb0a0ff028346": {"ta_keywords": "distributed convex optimization;decentralized distributed convex;distributed convex;advances decentralized distributed;decentralized distributed;convex optimization;convex optimization described;non distributed setup;algorithms non distributed;distributed setup particular;distributed setup;distributed;theoretical advances decentralized;optimal algorithms;optimal algorithms non;advances decentralized;based optimal algorithms;optimization described paper;decentralized;optimization;optimal;convex;optimization described;non distributed;explained based optimal;based optimal;algorithms non;algorithms;setup particular provide;setup", "pdf_keywords": "decentralized distributed optimization;optimization decentralized distributed;distributed convex optimization;convex optimization decentralized;decentralized distributed convex;distributed optimization;convex optimization distributed;approach distributed optimization;learning distributed optimization;method distributed optimization;distributed stochastic optimization;decentralized optimization;decentralized optimization stochastic;distributed optimization based;rate distributed optimization;optimization distributed;method decentralized optimization;distributed optimization network;decentralized gradient descent;distributed optimization time;distributed optimization networks;optimization decentralized;decentralized optimization closed;distributed algorithms;method decentralized distributed;gap decentralized optimization;method distributed convex;decentralized distributed stochastic"}, "affb8d759af00540458c19696532220dd1c1373a": {"ta_keywords": "usingdn hmm models;word text recognition;models dn hmm;hmm models;approaches usingdn hmm;speech recognition lvcr;continuous speech recognition;usingdn hmm;hmm models explored;sub word text;vocabulary continuous speech;speech recognition ar;dn hmm achieved;hidden markov models;text recognition;speech recognition;explored text recognition;open vocabulary sub;recognition lvcr tasks;hidden markov;hybrid deep neural;introduction hybrid deep;vocabulary sub word;large vocabulary continuous;text recognition methods;deep neural network;word text;markov models dn;hybrid deep;network hidden markov", "pdf_keywords": ""}, "5c5bedaf66cadebbcd9116f38acd3df9ed43d816": {"ta_keywords": "pa wavelet transform;pa wavelet based;use pa wavelet;wavelet transform;wavelet transform wt;pa wavelet;optimized pa wavelet;wavelet based;wavelet;hydrocarbon detection using;hydrocarbon detection;wavelet based continuous;direct hydrocarbon detection;wt seismic data;wavet wt seismic;seismic data;entropy measurement;seismic data help;r\u00e9nyi entropy measurement;time frequency analysis;entropy measurement aim;analysis called entropy;based continuous wavet;entropy optimized pa;wt seismic;continuous wavet wt;frequency analysis called;continuous wavet;wavet wt;frequency analysis", "pdf_keywords": ""}, "bc247abf8180f583a42de392e4f7d2b2a41ad72d": {"ta_keywords": "systems text seql;users natural language;natural language interfaces;text seql technique;text seql;natural language questions;parsers;databases systems text;parser;novel parser;language interfaces databases;seql;natural language;databases using natural;query databases using;users query databases;query databases;using natural language;introductionin natural language;present novel parser;seql technique allows;recently parsers;parsers fall short;seql technique;allows users query;parsers fall;databases systems;databases;databases using;language interfaces", "pdf_keywords": "sql parser;text sql parser;users propose parser;sql parser user;sql parser uses;users natural language;queries using parser;natural language interfaces;systems text sql;text sql technique;parsers development;text sql process;text sql complex;parser independent interactive;semantic parsing development;sql parser modify;users text sql;parsers;complex text sql;parser integrated;databases systems text;natural language questions;queries novel approach;uses text sql;enhance text sql;built arbitrary parsers;text sql;propose parser;effectiveness parser integrated;parser user"}, "c5bcb690b0aa85ad0a5fd7e7aa4b8c468cd8c69a": {"ta_keywords": "dis discriminative training;mutual information mmi;discriminative training based;discriminative training;mobile information mpe;introduction dis discriminative;mobile information;maximum mutual information;information mmi objective;mutual information;error weighted objective;mobile mobile information;mmi objective function;expressed margin based;recognition error expressed;dis discriminative;margin based minimum;discriminative;error expressed margin;information mmi;mmi objective;mammi margin error;minimum phone errorror;mobile mobile;weighted objective function;mpe mammi margin;information mpe mammi;based minimum phone;phone errorror mpe;margin error space", "pdf_keywords": ""}, "f300a62d0522d9a623b62f1305052928d8d7170c": {"ta_keywords": "irony detection datasets;detection irony detection;detection existing irony;detection irony;irony detection existing;existing irony detection;structures irony detection;sensive irony detection;emoji sensive irony;irony detection;irony detection irony;irony detection important;emojis social media;cues emojis social;nonverbal cues emojis;ironic tweets;10 ironic tweets;datasets 10 ironic;cues emojis;emojis social;structures irony;emoji sensive;\u00e2shirt emoji sensive;existing irony;day \u00e2shirt emoji;emojis;\u00e2shirt emoji;role structures irony;sensive irony;emoji", "pdf_keywords": ""}, "50851e9e16b52e14c422b6e937cfd3ed063b6998": {"ta_keywords": "cross lingual encoders;lingual encoders;trained cross lingual;lingual encoders mbert;non resource languages;cross lingual;low resource languages;languages low resource;resource languages low;transfer learning non;enabling transfer learning;transfer learning;resource languages;lingual;contextual embed;learning non resource;resource languages success;languages low;non resource;encoders;languages;languages success;align contextual embed;encoders mbert devlin;encoders mbert;low resource;pre trained cross;languages success comes;pre trained;contextual", "pdf_keywords": "lingual transfer learning;multilingual language models;multilingual neural machine;lingual representation learning;cross lingual representations;better cross lingual;word embeddings leveraging;multilingual neural;cross lingual representation;multilingual contextualized word;crosslingual classification languages;parallel corpus;multilingual contextualized;crosslingual classification;implement multilingual neural;parallel corpus employ;approach crosslingual classification;language inference dataset;tool crosslingual classification;learning multilingualwe;modeling non english;contextual embeddings words;pair parallel corpus;embeddings leveraging attention;cross lingual;cross lingual transfer;integrate multilingual neural;monolingual approach mbert;language modeling non;use multilingual contextualized"}, "37e06f3622c17dc6194b547c944462b2a513b878": {"ta_keywords": "neural abstractive summarization;abstractive summarization systems;news summarization performance;generated abstractive summaries;news summarization;abstractive summarization;strategies news summarization;abstractive summaries;summarization performance;summarization systems dominated;summarization systems;summarization performance terms;generating incorrect facts;factual correction models;summarization;factual inconsistency generating;extractive strategies news;summaries;span fact suite;abstractive summaries face;suite factual correction;fact suite;trained neural abstractive;fact suite factual;factual inconsistency;introductionpre trained neural;neural abstractive;source text;suite factual;factual correction", "pdf_keywords": "summarization prediction response;improve factuality summaries;factuality generated summaries;summarization benchmarks demonstrate;factuality summaries generated;generated summaries generalizable;summaries predicting consistent;summarization prediction;summarization models;summarization models lower;summaries generalizable summarization;summarization models systemgenerated;generalizable summarization propose;summarization benchmarks;summarization systems huge;multiple summarization benchmarks;generalizable summarization;corrections generated summaries;abstractive summarization models;generated summaries predicting;facts generated summaries;summarization systems;generated summaries;method summarization prediction;scores summarization models;generated summaries span;summaries generated;generated summary answers;summaries generalizable;factc scores summarization"}, "e487f2508e5f62b2745a2e56ceb3c601c286d2e3": {"ta_keywords": "malignant neoplasm gastrointestinal;neoplasm gastrointestinal;neoplasm gastrointestinal tract;gastrointestinal tract malignant;tract malignant neoplasm;malignant neoplasm;diagnosed malignant neoplasm;tract malignant;diagnosed malignant;patient diagnosed malignant;malignant;neoplasm;gastrointestinal tract patient;case patient diagnosed;gastrointestinal;gastrointestinal tract;patient diagnosed;diagnosed;present case patient;tract patient;tract patient treated;patient treated combination;case patient;patient treated;tract;article present case;patient;combination;treated combination;combination combination", "pdf_keywords": ""}, "705e6b53f88ec733e3c186c6232c41b268248c01": {"ta_keywords": "computational social choice;behavioral social choice;social choice outcomes;choice contribute computational;social choice;approach social choice;social choice switch;make social choice;sampling approach social;social choice contribute;social choice important;social choice discuss;social choice literature;computational social;choice discuss behavioral;inference make social;choice outcomes;choice outcomes level;tool computational social;contribute computational social;behavioral social;choice literature ask;choice switch perspective;traditional sampling;choice literature;sampling;discuss behavioral social;choice contribute;away traditional sampling;consideration theoretical behavioral", "pdf_keywords": ""}, "740182c3aa9a3045fcd9370269d446455ae9f623": {"ta_keywords": "finite state transducers;finite state transducer;string transduction models;state transducers rational;state transducers;state transducer;neural finite state;transducers rational relations;string transduction;state transducer contrast;transducers rational;probability string pair;transducers;strings methodsthe probability;distributions pairs strings;transduction models;probability string;neural finite;transducer;family string transduction;state transducers fts;transduction models defining;introduce neural finite;strings;transduction;paths finite state;finite state;pairs strings;relations introduce neural;introductionneural finite state", "pdf_keywords": ""}, "ba159dbf205193d0cb7c9c18dd01f830d2f56eb8": {"ta_keywords": "linguistic annotation goal;automatic linguistic annotation;linguistic annotation;improving automatic linguistic;automatic linguistic;cdin27 shared task;natural language processing;annotation goal;annotation goal task;annotation;natural language;contemporary natural language;cdin27 shared;language processing tools;language processing;task improving automatic;linguistic;cdin27;shared task improving;task improving;improving automatic;task shared task;task shared;shared task;language;task improve;task improve quality;shared task shared;task;tools appl", "pdf_keywords": ""}, "b694472c13420acb599a5b1d25d5f2bd42eb8c1b": {"ta_keywords": "novo sequencing shot;efficiency novo dna;novo sequencing;novo dna assembly;sequencing shot gun;sequencing shot;novo dna;novo assembly algorithm;problem novo sequencing;dna assembly;dna assembly novel;sequencing;propose novo assembly;unknown sequence reconstructed;sequence propose novo;sequence reconstructed short;sequence reconstructed;data efficiency novo;assembly novel algorithmic;reconstructed short substrings;novo assembly;assembly algorithm;assembly algorithm requires;shot gun data;substrings sequence propose;gun data underlying;substrings sequence;short substrings sequence;dna;gun data", "pdf_keywords": ""}, "71d649dcb3dee2ca57d0775a9679cb68f82f22d5": {"ta_keywords": "sequencemarizing neural network;methods adn adaptation;introduction adn adaptation;adn adaptation technique;adn adaptation;vector extractor sn;summarizing neural network;sequencemarizing neural;sequence sequencemarizing neural;technique vector extractor;sequenceence summarizing neural;neural network sn;similarly vector extractor;summarizing neural;sn methods adn;extractor replaced sequenceence;vector extractor;extractor replaced sequence;replaced sequence sequencemarizing;extractor sn;adaptation technique vector;neural network;methods adn;vector extractor replaced;replaced sequenceence summarizing;introduction adn;sequencemarizing;sequence sequencemarizing;extractor sn produces;sn methods", "pdf_keywords": ""}, "0be998fffc5f44496042f7757fb2ffa8924e54cd": {"ta_keywords": "training adaptive scorer;adaptive scorer;adaptive scorer efficiently;training adaptive;learning model;adapts current learning;learning model potentially;trained better scorer;skill humans learn;current learning;scorer efficiently challenging;learning;training data;machine learning model;similarly machine learning;instance training adaptive;faster tutor based;humans learn;scorer efficiently;humans learn better;machine learning;current learning state;knowledge level;adaptive;training data instance;potentially trained better;tutor based;learn better faster;trained better;faster tutor", "pdf_keywords": "classification model dynamic;differentiable rewards training;training classification;optimizing training data;function efficiently training;problem training adaptive;training adaptive scorer;training adaptive;meta learning;efficiently training;training data;meta learning method;learning training scorer;efficiently training scorer;optimizing training;algorithm differentiable rewards;training classification model;adaptive scorer weights;adapts current learning;rewards training;image classification multilingual;usage differentiable rewards;training data usage;training scorer efficiently;learning training;differentiable rewards algorithm;method training classification;classification;classification neural;benchmarks image classification"}, "c5bb38b8e3ce21063670dfd81ac64dcb2ecf10b2": {"ta_keywords": "computation frequencies spectral;obtaining spectral notches;frequencies spectral;developer obtaining spectral;obtaining spectral;spectral notches;accurate virtualization sound;head related impulse;spectral notches prominent;impulse response hrr;head related transfer;fast computation frequencies;frequencies;hrr accurately methods;virtualization sound time;spectral;virtualization sound;computation frequencies;reconstruct head related;individualized head related;reconstruct head;response hrr accurately;individualized head;head related;background individualized head;impulse response;transfer functions hrfs;important reconstruct head;related impulse response;related transfer functions", "pdf_keywords": ""}, "8abd724b770348bd21b16b9aaf2ba0a77596b2ed": {"ta_keywords": "speaker diarization eend;introductionencoder decoderbased attractor;speaker diarization;speakers introduce encoder;approach speaker diarization;end neural diarization;decoderbased attractor calculation;neural diarization;decoderbased attractor;decoder based attractor;neural diarization eend;diarization eend;pipeline approach speaker;attractor calculation end;speakers methodsin;diarization eend promising;speaker overlap handling;diarization eend methods;introductionencoder decoderbased;number speakers methodsin;speaker overlap;attractor calculation module;attractor calculation;introductionencoder;based attractor calculation;terms speaker overlap;number speakers introduce;introduce encoder;speakers introduce;introduce encoder decoder", "pdf_keywords": ""}, "af787fda38ce6fa1d14ad2fb8568088faf973a21": {"ta_keywords": "icon based agreement;evaluating applicability icon;applicability icon based;icon based;applicability icon;agreement level design;mobile phone evaluated;concept intended mobile;concept aimed mobile;icon;agreement level;based agreement level;phone evaluated;intended mobile phone;based agreement;design concept;design concept aimed;evaluating terms agreement;design concept intended;agreement;mobile phone;design;phone evaluated method;aimed mobile phone;mobile;terms agreement level;intended mobile;aimed mobile;terms agreement;level design concept", "pdf_keywords": ""}, "be312e930f6739a709e60547aa0dfb9c3dc44497": {"ta_keywords": "multilingual lexicon encoding;neural machine translation;sde multilingual lexicon;lexicon encoding;encoding sde multilingual;lexicon encoding framework;multilingual lexicon;soft decoupled encoding;sde multilingual;word representations;machine translation;machine translation nm;multilingual;share lexical level;learning word representations;low resource languages;share lexical;introductionmultilingual neural machine;translation nm systems;efficiently learning word;decoupled encoding sde;designed share lexical;decoupled encoding;propose soft decoupled;word representations face;lexical level information;encoding;encoding sde;encoding framework specifically;introductionmultilingual neural", "pdf_keywords": "multilingual lexicon encoding;neural machine translation;multilingual neural neural;multilingual neural;lexical units multilingual;nonlingual translation challenging;multilingual nonlingual translation;lexical level transfer;lexical representation multilingual;units multilingual nonlingual;translation low resource;multilingual lexicon;machine translation low;representation multilingual nonlingual;nonlingual translation;lexicon encoding;word embeddings decode;multilingual sed corpus;improvements strong multilingual;units multilingual;word embedding preserving;sde multilingual lexicon;nonlingual language baseline;sharing lexical representation;strong multilingual nonlingual;method multilingual neural;representation multilingual;machine translation nm;nonlingual nonlingual nucleotide;multilingual nonlingual"}, "e961c8de1df75f70254656e98ca82f9d9fbd640c": {"ta_keywords": "recovering complex signal;signal crn intensity;complex signal crn;complex signal;intensity measurements form;measurement row vector;crn intensity measurements;intensity measurements;crn intensity;recovering complex;ai measurement row;problem recovering complex;signal crn;aix ai measurement;measurements form aix;measurement row;intensity;consider problem recovering;signal;ai measurement;measurements form;row vector methodswe;aix ai;row vector;form aix ai;problem recovering;crn;recovering;measurement;complex", "pdf_keywords": "compressive phase retrieval;algorithm compressive phase;phasecode algorithm compressive;approach compressive phase;compressive phase;general compressive phase;literature compressive phase;new approach compressive;decoding complexity phasecode;sparse graph coding;algorithm compressive;approach compressive;complexity phasecode provably;decoding phasecode graph;phase retrieval;phase retrieval signals;problem phase retrieval;phase phase retrieval;measurements phasecode robustified;efficient algorithm phasecode;phase retrieval fourier;phase retrieval method;sparse spectrum;phasecode robustified;signals having sparse;complexity phasecode;optimal sample decoding;decoding phasecode;phasecode robustified noise;phase retrieval problem"}, "e2a4e1a9f8e66baf12a49a3e5d8e33291f9347e7": {"ta_keywords": "text entity linking;entity linking;base entity linking;entity linking long;entity linking challenging;link reference knowledge;entity linking aims;linking challenging text;linking long text;reference knowledge base;aggregated semantic matching;short text entity;semantic matching;linking long;linking;task entity linking;reference knowledge;knowledge base entity;aggregated semantic;text entity;linking challenging;knowledge base;text fragments link;link reference;semantic;fragments link reference;short texts;framework aggregated semantic;base entity;short text", "pdf_keywords": ""}, "f951aad88e244182b37e4918c3d570560108c68c": {"ta_keywords": "image demonstrated adversarially;adversarially trained;adversarially;adversarially trained neural;perceptually aligned gradients;demonstrated adversarially;demonstrated adversarially trained;robust classifiers;property robust classifiers;classifiers;classifiers standard convolutional;robust classifiers standard;images uncannily resemble;trained neural;perceptually aligned;trained neural networks;optimization produces images;convolutional neural;gradients;aligned gradients general;gradients general;aligned gradients;standard convolutional neural;images uncannily;background perceptually aligned;resemble target class;robust;trained;uncannily resemble target;convolutional neural network", "pdf_keywords": "adversarial examples smoothed;improved smoothed neural;smoothed neural networks;adversarially trained neural;perceptually aligned gradients;adversarially trained;smoothed neural;optimizing smoothed neural;neural networks robustness;smoothed networks trained;adversarial;neural networks randomized;adversarially;smoothed neural network;neural network robustness;randomizedwe smoothed neural;image demonstrated adversarially;loss robustness neural;perceptuallyaligned gradients;adversarial examples;robustness neural networks;using adversarially trained;robustness neural;network robustness neural;method smoothed neural;using adversarially;adversarial defense;adversarial defense similar;targeted adversarial;gradients optimizing smoothed"}, "5eaa425af39339e0ae30202b348cc6e253813993": {"ta_keywords": "technology analysis forecasting;technological forecasting systems;retrieval technology analysis;technological forecasting;citation indexes developed;technology analysis;introductionan information retrieval;patent databases citation;information retrieval technology;citation indexes;research industry trends;grade technological forecasting;analyze research industry;analysis forecasting research;databases citation indexes;information retrieval;russian patent databases;forecasting research;forecasting research conducted;forecasting systems english;indexes developed;patent databases;analysis forecasting methodsexpert;grant proposals research;research industry;text corpus despite;text corpus;analysis forecasting;dependent text corpus;retrieval technology", "pdf_keywords": ""}, "0761a69310f7b8f4ab01495f31a30c6fe53d83b8": {"ta_keywords": "adaptive discriminative speech;change speech characteristics;discriminative speech modeling;speech modeling;conversation temporal changes;speech modeling cope;change speech;discriminative speech;conversation temporal;time incremental adaptations;multiscale adaptation;aim change speech;world conversation temporal;speech characteristics;adaptations multiscale adaptation;incremental adaptations multiscale;multiscale adaptation potential;incremental adaptations;adaptive discriminative;speech characteristics originated;introduction adaptive discriminative;temporal changes environments;temporal changes;adaptive;introduction adaptive;temporal changes dynamics;speech;single time incremental;adaptations multiscale;adaptation", "pdf_keywords": ""}, "225767ce707781d0114815068c355622869ee642": {"ta_keywords": "artificial cognitive systems;cognition living systems;artificial cognitive;cognitive systems;construction artificial cognitive;understanding cognition living;cognition living;understanding cognition;cognition;progress understanding cognition;living systems;cognitive;living systems new;computer science neuroscience;systems;systems new insights;neuroscience;systems new;researching construction artificial;neuroscience related fields;science neuroscience;artificial;neuroscience related;science neuroscience related;research computer;construction artificial;computer;research computer science;areas research computer;computer science", "pdf_keywords": ""}, "91184a2d40be8a0171b5c926b336666ed717ec6e": {"ta_keywords": "unfairness peer review;bias unfairness peer;peer review;unfairness peer;peer review methods;peer review results;peer review conclusion;bias unfairness;solutions bias unfairness;systemic challenges computational;computational solutions bias;unfairness;peer;bias;review methods jjjd;challenges computational;challenges computational solutions;review results jjjd;review results;solutions bias;review methods;systemic challenges;tutorial systemic challenges;computational;review conclusion;results jjjd 2021;review;2021 tutorial systemic;methods jjjd 2021;results jjjd", "pdf_keywords": ""}, "7d148b46f45e935765e56887d720492b2b716e55": {"ta_keywords": "connectivity collective dynamics;connectivity event timings;revealing physical connectivity;connectivity event;connectivity collective;network connectivity event;reconstructing network connectivity;network connectivity collective;networks reconstructing;collective dynamics generate;networked systems event;networks reconstructing network;physical connectivity;intrinsic collective dynamics;physical connectivity networked;connectivity networked systems;introductioninferring network connectivity;collective dynamics;event time series;collective dynamics typically;timings event space;reconstructing network;connectivity;connectivity networked;networks;event timings fundamental;neural networks reconstructing;systems event time;continuous time evolution;intrinsic collective", "pdf_keywords": "connectivity spiking neurons;synaptic connectivity spiking;connectivity collective dynamics;neurons collective network;neuronal network inter;connectivity spiking;spiking neurons fundamental;neurons collective;neurophysiology synaptic connectivity;spiking neurons;demonstrate synaptic connectivity;synaptic connectivity;synaptic connectivity fundamental;neuronal network;neuron determined events;generated neurons collective;intervals generated neurons;neuron network;collective dynamics typically;collective dynamics generate;synaptic connections neurons;mechanism synaptic connections;principle neuronal network;neuron pre postsynaptic;event space synaptic;connections neurons fundamental;connections neurons;intrinsic collective dynamics;connectivity collective;collective dynamics"}, "d3dd80269f2542cc173afb3a1df24b582a1e4af2": {"ta_keywords": "transformer recognizes parity;parity language bit;languages parity;using languages parity;language bit strings;languages parity language;recognizes parity;parity language;introduction transformers remarkably;transformers remarkably;regular languages struggle;looking regular languages;introduction transformers;transformers remarkably effective;regular languages;transformers;1s language bit;bit strings odd;constructing transformer recognizes;bit strings;transformer recognizes;recognizes parity perfect;language bit;transformer;constructing transformer;parity perfect accuracy;bit strings starting;languages struggle;parity;question constructing transformer", "pdf_keywords": "parity language bit;parity language;using languages parity;languages parity;languages parity language;language bit strings;languages transformers;constructions parity;1s language bit;different languages transformers;languages transformers recognized;bit strings;decisions constructions parity;recognition different languages;easy compute construction;transformer machine translation;looking regular languages;regular languages;language bit;entropy model string;bit strings odd;regular languages struggle;limitation using languages;machine translation;detecting word;1s language;parity;bit strings starting;number 1s language;machine translation method"}, "920257774e2caee8a8c74968c64c10bcb79a136c": {"ta_keywords": "transmission lines methodsin;core coaxial cable;coaxial cable;function transmission lines;transmission lines;genetic algorithm vector;propagation function transmission;hybrid genetic algorithm;genetic algorithm proposed;genetic algorithm;improved genetic algorithm;single core coaxial;method improved genetic;proposed approximation propagation;approximation propagation function;algorithm vector fitting;core coaxial;propagation function;vector fitting based;function transmission;coaxial;vector fitting;approximation propagation;lines methodsin;fitting based approach;better approximation propagation;propagation function proven;introductiona hybrid genetic;hybrid genetic;propagation", "pdf_keywords": ""}, "147b954ba0881d643706c918e017f7d66a15b827": {"ta_keywords": "concept human learning;discover cognitive models;human learning;learning agent sistud;cognitive model set;learning agent;cognitive models using;human learning article;human learning provide;cognitive model;representing cognitive model;knowledge components;cognitive models process;cognitive models;discover cognitive;development cognitive models;matically discover cognitive;search cognitive models;cognitive models depending;learning provide overview;knowledge components includes;construction cognitive models;quality cognitive models;search cognitive;knowledge components koedinger;representing cognitive;ways human students;manual construction cognitive;approach search cognitive;way representing cognitive", "pdf_keywords": ""}, "3426f000673aae995a55ade9273c842bb484ad18": {"ta_keywords": "phoneme segmentation;phoneme segmentation previously;multilingual phoneme recognizers;introductionunsupervised phoneme segmentation;automatic detection phoneme;detection phoneme boundaries;detection phoneme;phoneme recognizers;languages automatic phonemic;phoneme boundaries audio;automatic phonemic transcription;recordings unwritten language;recordings unknown language;phonemic transcription recordings;multilingual phoneme;introductionunsupervised phoneme;phoneme boundaries;use multilingual phoneme;phonemic transcription;automatic phonemic;transcription recordings unwritten;boundaries audio recordings;transcription recordings;unwritten languages automatic;unseen languages;previously unseen languages;documenting unwritten languages;segmentation previously unseen;linguists documenting unwritten;unknown language work", "pdf_keywords": ""}, "89c2cbdf1a5049a4068ca9215aa8859a1a97b1a3": {"ta_keywords": "contextual bandit learning;algorithm contextual bandit;practical contextual bandit;bandit learning algorithm;bandit learning;contextual bandit;bandit learning problem;statistically optimal regret;optimal regret guarantee;bandit;optimal regret;novel algorithm contextual;achieves statistically optimal;cost sensitive classification;algorithm contextual;context novel algorithm;supervised cost sensitive;learning algorithm approaches;regret guarantee;statistically optimal;regret guarantee kt;learning problem;learning algorithm;supervised cost;learning;fully supervised cost;learning problem presented;solving fully supervised;general policy classes;optimal", "pdf_keywords": "contextual bandit learning;algorithm contextual bandit;bandit learning algorithm;bandit learning;contextual bandit problem;algorithm contextual bandits;bandit problem optimal;bandit learning problem;bandit problem algorithm;contextual bandit;log contextual bandit;novel contextual bandit;contextual bandits;bandit problem;contextual bandits apply;optimal regret bounds;optimal regret bound;bandit;statistically optimal regret;learning algorithm contextual;optimal regret guarantee;regret bounds requiring;bandits apply fully;algorithm optimization policy;bandits;problem optimal regret;optimal regret;regret bounds;convex program used;optimization policy"}, "89f7db77a755d44d3aabdbcc7549b743d7debcc5": {"ta_keywords": "conditional preference networks;preference networks nets;preference networks;generalization conditional preference;conditional preference;uncertainty nets;uncertainty nets common;changes uncertainty nets;nets incorporates uncertainty;uncertainty methods nets;qualitative conditional statements;statements preferences;statements preferences set;conditional statements;qualitative conditional;nets formal;conditional statements cp;model qualitative conditional;generalization conditional;incorporates uncertainty;changes uncertainty;nets formal tool;cp statements preferences;common problem nets;networks nets;conditional;problem nets;uncertainty;preferences set objects;networks nets incorporates", "pdf_keywords": ""}, "18a82459d495fa3ad22a60bd7c9527df8bd55e1e": {"ta_keywords": "distributed regret learning;learning network games;distributed regret;algorithm network games;propose distributed regret;distributed learning network;distributed learning;regret learning algorithm;network games;strategy network games;network games highly;network games using;learning network;strategy network;network games paper;regret learning;communication graph learning;player optimizes global;games using primal;optimizes global objective;dual averaging;learning algorithm network;effective strategy network;paper propose distributed;dual averaging locally;propose distributed;distributed;graph learning;method dual averaging;algorithm network", "pdf_keywords": ""}, "341f6353547f4a58fdf11fbcc9de3a31083a619b": {"ta_keywords": "symptomatic plaque mouth;developed symptomatic plaque;symptomatic plaque;diagnosed symptomatic plaque;plaque mouth patient;dental plaques;dental plaques used;plaque mouth;plaque;mouth patient diagnosed;plaques;mouth patient;plaques used;plaques used tool;dental;patient developed symptomatic;patients difficult diagnose;patient diagnosed symptomatic;difficult diagnose case;diagnose case patient;difficult diagnose;difficult diagnose difficult;diagnosed symptomatic;diagnose difficult;patient diagnosed;diagnose difficult diagnose;diagnose case;developed symptomatic;diagnose;symptomatic", "pdf_keywords": ""}, "7a79099447bef9a3ea13b1dc409d04b3dff57320": {"ta_keywords": "automatic music composition;modeling music symbolic;task automatic music;sequence midii like;generative modeling music;composing music;composing music long;automatic music;music composition;musical scores serializing;modeling music;music composition entails;symbolic formats musical;composing;score sequence midii;music symbolic formats;music symbolic;sequence midii;art sequence models;especially composing music;serializing score sequence;task especially composing;especially composing;models self attention;sequence models self;sequence models;scores serializing;formats musical scores;formats musical;musical scores", "pdf_keywords": "sequence modeling music;automatic music composition;music composition models;dynamic model music;modeling music way;music modeling;modeling music;better music modeling;approach modeling music;music generative process;music way modeling;music generative;music synthesis;generating music;music composition using;music composition;music music synthesis;music compositions;music modeling improve;expressive music compositions;generate expressive music;approach generating music;music compositions introducing;music model using;model music approach;generative process music;framework automatic music;automatic music;generate music fundamental;generating music novel"}, "e9d8db4f5b5c106c43a268f635788c0a94b2916a": {"ta_keywords": "stochastic gradient descent;descent ascent methods;gradient descent ascent;ascent methods;ascent methods far;randomization distributed variants;distributed variants compression;gradient descent;variants compression methodsin;reduction coordinate randomization;stochastic gradient;variants compression;descent ascent;sampling variance reduction;variants arbitrary sampling;gd variants arbitrary;variety stochastic gradient;distributed variants;gd variants;classical gd variants;randomization;coordinate randomization;coordinate randomization distributed;variance reduction;randomization distributed;unified convergence analysis;compression methodsin;arbitrary sampling variance;large variety stochastic;propose unified convergence", "pdf_keywords": ""}, "b9913ddf94245c864509f0b94847bdbe77899b46": {"ta_keywords": "documentation speech recognition;jointly modelling phonemes;speech recognition;transcribing speech;tonal transcription language;speech recognition technology;modelling phonemes;transcribing speech important;modelling phonemes tones;connectionist temporal classification;phonemic tonal transcription;introduction transcribing speech;temporal classification loss;transcription language;transcription language documentation;phonemes tones;language documentation speech;phonemes tones versus;tonal transcription;phonemes;documentation speech;speech important language;harnessed aid linguists;transcription;architecture connectionist temporal;phonemic tonal;temporal classification;function phonemic tonal;neural network architecture;connectionist temporal", "pdf_keywords": ""}, "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a": {"ta_keywords": "words transformers image;transformers image recognition;recognition scale transformer;convolutional networks used;words transformers;cnns necessary;vision attention applied;vision attention;cnns necessary pure;cnns;convolutional networks;16x16 words transformers;methodswe reliance cnns;transformers image;components convolutional networks;recognition scale;reliance cnns necessary;convolutional;image recognition scale;limited vision attention;recognition;reliance cnns;components convolutional;transformer architecture facto;image recognition;transformer;transformer architecture;attention applied;computer vision;attention", "pdf_keywords": "convolutional networks used;cnns;convolutional networks;components convolutional networks;large dataset vision;dataset vision transformer;cnns necessary;convolutional networks keeping;recognition benchmarks;recognition task large;imagenet;cnns necessary pure;image recognition large;image classification tasks;trained public imagenet;recognition benchmarks lower;transfer learning;image recognition benchmarks;reliance cnns;neural transfer learning;recognition large;recognition large corpora;transformer large dataset;reliance cnns necessary;approach recognition large;dataset vision;imagenet 21k dataset;recognition benchmarks particular;transfer learning non;convolutional"}, "4533fd4cf13d2f4dd105edaf612934a1bd85ad5a": {"ta_keywords": "multi channel electroencephalogram;channel electroencephalogram addressed;channel electroencephalogram;electroencephalogram addressed;electroencephalogram;electroencephalogram addressed methodan;observed signal separated;noise removal singletrial;method noise removal;separated multiple signals;noise removal;spatial correlation prior;signal separated multiple;signal separated;observed signal;estimateion spatial correlation;correlation prior;multiple signals;potentials recorded multi;methodan observed signal;multi channel wiener;related potentials recorded;event related potentials;posteriori estimateion spatial;correlation prior paper;multiple signals multi;removal singletrial event;signals multi;potentials recorded;channel wiener filter", "pdf_keywords": ""}, "b62430b9f8810da4d9f28842ac0ca899aa66d422": {"ta_keywords": "diagnosis pulmonary artery;pulmonary artery disease;patients diagnosis pulmonary;pulmonary artery;diagnosis pulmonary;artery disease;pulmonary;patients diagnosis;management patients diagnosis;artery;evaluating effectiveness new;evaluating effectiveness;diagnosis;management patients;patients;importance evaluating effectiveness;effectiveness new approach;approach management patients;effectiveness new;effectiveness;evaluating;discuss importance evaluating;disease;importance evaluating;article discuss importance;new approach management;article;new approach;approach management;purpose article", "pdf_keywords": ""}, "d1c41eb99824e8f4752190da1b815378be23b4b9": {"ta_keywords": "catastrophic forgetting;mitigating catastrophic forgetting;forgetting scheduledd sampling;catastrophic forgetting scheduledd;neural machine translation;consolidation neural machine;forgetting;generated prefixes training;prefixes training;weight consolidation neural;consolidation neural;prefixes training process;scheduledd sampling elastic;machine translation aim;machine translation;forgetting scheduledd;scheduledd sampling;elastic weight consolidation;sampling elastic weight;model generated prefixes;weight consolidation;neural machine;sampling elastic;sampling;generated prefixes;neural;translation aim evaluate;prefixes;consolidation;translation aim", "pdf_keywords": "sequence tasks autoregressive;autoregressive models trained;generated prefixes training;model catastrophic forgetting;forgetting scheduled sampling;forgetting prefix generated;forgetting predict model;models trained scheduled;catastrophic forgetting prefix;trained scheduled sampling;sampling models trained;learns models;catastrophic forgetting predict;catastrophic forgetting scheduled;learns models ignore;forgetting predict;sequenceto sequence tasks;sequence tasks;scheduled sampling models;sequence model generated;generated prefixes neural;model generated prefixes;catastrophic forgetting neural;forgetting neural networks;forgetting prefix;models demonstrate scheduled;catastrophic forgetting;prefixes training process;model generated prefix;catastrophic forgetting lm"}, "175b58fe7e49bb5c0c771b73f8834bcff21b59c7": {"ta_keywords": "natural language inference;language inference natural;language inference nli;inference natural language;evaluation natural language;natural language hypothesis;determining natural language;inference nli task;inference nli;task natural language;language inference;natural language;natural language understanding;nli task determining;stress test evaluation;nli task;stress test;inference natural;test evaluation natural;benchmark task natural;stress;language hypothesis;language understanding existing;language hypothesis inferred;language understanding;nli;text extent models;evaluation natural;task determining natural;genres text", "pdf_keywords": "language inference nli;natural language models;models natural language;natural language inference;inference natural language;reasoning nli models;language inference natural;accuracy natural language;inference nli task;natural language data;entailment models;nl models;natural language hypothesis;inference nli;sentence encoder model;task natural language;language inference important;evaluation natural language;analysis textual entailment;language inference;entailment errors accuracy;nli models;problem entailment models;interpret natural language;language understanding nii;natural language systems;models understand semantic;language models;language inference use;developments natural language"}, "49edf7f0dbad8b8c101af9ef95c72f62f545591e": {"ta_keywords": "modeling topic embedding;correlated topic modeling;compact topic embeddings;topic embedding;topic embedding conclusionscorrelated;topic embeddings;conclusionscorrelated topic modeling;captures topic correlations;embedding conclusionscorrelated topic;topic modeling;topic embeddings captures;topic correlations;topic vectors;embeddings captures topic;learns compact topic;topic modeling topic;closeness topic vectors;backgroundefefficient correlated topic;topic correlations closeness;topic modeling limited;topic vectors method;correlated topic;modeling topic;compact topic;embedding conclusionscorrelated;captures topic;low dimensional embedding;conclusionscorrelated topic;correlations closeness topic;dimensional embedding", "pdf_keywords": "correlated topic modeling;topic correlation modeling;compact topic embeddings;topic vectors embedding;correlated topic model;learning topic correlation;topic embedding;topic embeddings;backgroundcorrelated topic modeling;topic embedding method;model topic correlations;topic correlations model;topic model correlated;captures topic correlations;topic document embeddings;embeddings captures topic;topic modeling;topic embeddings captures;compact topic vectors;representations latent topics;topic vectors cheap;topic models;model correlated topic;topic correlations;topic model;topic correlation;topic vectors;closeness topic vectors;topic modeling limited;topic correlations method"}, "db79a3e55690c5c86cfd0ec97712ed4ad1e47b3b": {"ta_keywords": "backgroundactive ranking pairwise;sequential active ranking;ranking pairwise comparisons;comparisons items ranked;ranking pairwise;item ranking;item ranking refers;backgroundactive ranking;active ranking;ranking set items;chosen item ranking;ranking;items ranked according;active ranking set;ranking refers;items ranked;ranking set;scores notion ranking;ranked according probability;ranking includes special;ranked according;ranking includes;ranking refers partitioning;noisy pairwise comparisons;pairwise comparisons items;according scores notion;ranked;comparisons items;notion ranking;scores notion", "pdf_keywords": ""}, "3c37b9ec2ff1828877575acc600b73c3bcde138f": {"ta_keywords": "systems departing bandits;bandits policy recommender;departing bandits methods;modeling attrition recommender;bandits policy;bandits methods;departing bandits;recommender systems departing;bandits methods traditionally;multi armed bandits;recommender influences rewards;attrition recommender systems;bandits;attrition recommender;armed bandits policy;traditionally recommender systems;recommender systems formalized;armed bandits;recommender systems;traditionally recommender;recommender;methods traditionally recommender;recommender influences;policy recommender;rewards;influences rewards accrued;influences rewards;policy recommender influences;rewards accrued;modeling attrition", "pdf_keywords": "algorithm departing bandits;bandits policy recommender;bandit learning;bandits protocol;bandits optimal;bandits optimal policy;departing bandits optimal;algorithm optimizing recommendations;optimal policy recommending;bandits protocol method;optimal recommendation;goal optimal recommendation;departing bandits protocol;optimizing recommendations;methodswethe departing bandits;bandit learning development;bandits policy;bandits goal optimal;multi armed bandits;role bandit learning;recommendation algorithm users;optimal recommendation algorithm;policies departing bandits;bandits;bandits problem;departing bandits problem;multi armed bandit;bandit setup;bandits problem local;bandit"}, "f6f4d30e4740bd92b31acd297a15872d490e7f11": {"ta_keywords": "constraints semi supervised;semi supervised learning;semi supervised;classifiers imposede declarrative;declarrative constraints semi;classifiers imposede declar;graphs classifiers imposede;supervised learning;classifiers imposede;declarrative constraints;supervised learning conclusionsusing;supervised;constraints semi;imposede declarrative constraints;graphs classifiers;conclusionsusing graphs classifiers;supervised learning methodsusing;classifiers;introductionusing graphs classifiers;supervised learning resultsusing;methodsusing graphs classifiers;resultsusing graphs classifiers;learning conclusionsusing graphs;learning methodsusing graphs;learning methodsusing;learning resultsusing graphs;constraints;learning conclusionsusing;learning;learning resultsusing", "pdf_keywords": "automatic learning query;learning query response;semi supervised learning;modeling semi supervised;constraints semi supervised;text classification;application semi supervised;classification tasks ssl;method semi supervised;learning query;semi supervised;text classification employ;text classification method;supervised classification;based text classification;constraints learned classifier;supervised classification tasks;classification tasks;information relation extraction;automatic learning;supervised learning approach;method automatic learning;supervised learning;traditional supervised classification;relation extraction;relation extraction task;data extraction labeled;supervised learning sl;classification;supervised learning natural"}, "dab261b25ff8ccd2c9144a5cb3a46b39ac0ac4bd": {"ta_keywords": "machine learning owes;key clinical messagethe;strength machine learning;clinical messagethe;key clinical;clinical;clinical messagethe current;machine learning;promoting clear scientific;empirical promoting clear;learning;rigorous research;scientific;theoretical empirical promoting;clear scientific;body rigorous research;strength machine;empirical promoting;learning owes large;clear scientific thinking;scientific thinking communication;research;communication community;theoretical empirical;machine;rigorous research date;thinking communication community;community sustain trust;scientific thinking;communication", "pdf_keywords": "misuse machine learning;machine learning consequences;machine learning discuss;machine learning common;writing machine learning;language misuse;misinterpreted explanation patterns;machine learning scholarship;machine learning use;misuse language;claims implications scientific;using language misuse;patterns mathematical claims;misuse language patterns;trending machine learning;gains misuse language;policymakers discuss misuse;confusion researchers policymakers;terminology machine learning;scientific papers misinterpreted;empirical gains misuse;language misuse machine;misuse language common;implications scientific communication;discuss misuse;mathematical claims implications;mathematical claims;discuss neural networks;papers misinterpreted explanation;misuse"}, "b79dcc5304e557ce200b161d2a884c0ff77f34ec": {"ta_keywords": "isolated misspellings;engineering isolated misspellings;models using spelling;toolkit spelling correction;misspelt tokens methods;misspelt tokens;naturally occurring misspelt;occurring misspelt tokens;misspelt token remedy;misspellings;source toolkit spelling;misspelt token;context misspelt token;using spelling errors;spelling correction;spelling errors context;toolkit spelling;spelling correction english;neuspell;context misspelt;introduce neuspell;spelling errors;background introduce neuspell;neuspell open source;correction english toolkit;errors context synthetically;neural models;misspelt;neural models using;english toolkit", "pdf_keywords": "spelling correction systems;models spelling correction;neural models spelling;spelling correction toolkit;spelling mistakes implement;neuspell adversarial misspellings;accuracy spelling correction;adversarial misspellings;spelling correction set;spelling mistakes tokens;performing spelling correction;improve spelling accuracy;adversarial misspellings evaluated;toolkit spelling correction;realworld spelling mistakes;generate missingpelt;spell checker improve;generate missingpelt correct;isolated misspellings;neuspell spelling correction;misspellings present toolkit;misspellings evaluated accuracy;spelling mistakes set;novel spell checker;common spelling correction;spelling correction significantly;engineering isolated misspellings;spelling correction developed;checker improve spelling;misspellings evaluated"}, "9b621c0bcd2029006b389bc51395fb6604f9a855": {"ta_keywords": "humans natural language;natural language;goal natural language;natural language ambiguous;natural language processing;question asking model;repair asking questions;visually grounded question;interactive;uncertainty humans;visually grounded;asking model capable;language processing;asking questions;question asking;questions;visually;building visually grounded;communicate seamlessly humans;questions seeking;uncertainty humans engage;asking model;repair asking;interactive process;machines communicate;language processing enable;cases uncertainty humans;grounded question asking;asking questions seeking;language", "pdf_keywords": "question generation;introduce question generation;visually grounded questionasking;questionasking model capable;question driven communication;questionasking model;question generation framework;questiondriven communication game;informative human answerers;human answerers model;generating useful questions;human answerers;questions improve communicative;human answerers demonstrate;produce informative questions;questionasking;grounded questionasking model;question meanings model;synthetic human answerers;humans natural language;question driven;informative questions unseen;pose questions questiondriven;questionanswer training;questiondriven communication;human answerers development;model asked questions;questions questiondriven communication;question meanings;questionanswer training data"}, "a84c319fef32b2514af9541576189a1735aac507": {"ta_keywords": "health problem world;language world;language world major;problem world world;public health problem;world major public;world wide world;problem world;major public health;health problem;world world;public health;language;world world wide;world major;world;health;wide world;wide world major;world wide;major public;problem;public;major;wide", "pdf_keywords": ""}, "f2de2c9d83b9058695e3272a4fbb7c30e04a1476": {"ta_keywords": "learning word second;learn second language;words harder learn;learning english second;second language duolingo;second second language;ease learning word;second language;second language using;users learning english;second language robust;language duolingo;harder learn second;words harder;makes words harder;learning word;word second;word second second;learn second;language robust factors;duolingo;harder learn;learning english;ease learning;english second;language using;english second second;language using large;factors difficult study;users learning", "pdf_keywords": "duolingo words similar;learning word second;word learning accuracy;language learning accuracy;duolingo words;status duolingo words;ease learning word;predictors language learning;language duolingo mobile;words similar language;learning word;english semantic density;word second language;word learning;easily words similar;language duolingo;language l2 learning;accuracy predicted words;learn word;ability learn word;language learning combines;second language duolingo;multilingual word network;word learning used;learned easily words;predictors language;main semantic predictors;word level predictors;linguistic semantic alignment;status word learning"}, "5b6c1e9dddc4b55036a5629227ae2cc7d49eb6d0": {"ta_keywords": "structured literaturee image;literaturee image finder;biomedical literature structured;structured literaturee;literature structured literaturee;image finder novel;structurally structured literaturee;literature structured;literaturee image;image finder;approach biomedical literature;biomedical literature;application biomedical literature;structured;finder novel approach;biomedical literature novel;structurally structured;novel approach biomedical;finder novel;data application biomedical;literaturee;structurally;approach biomedical;analysis primary data;analysis primary;finder;novel approach analysis;literature novel approach;primary data;primary data provides", "pdf_keywords": ""}, "a1babdf55a6bff96d533fd0c9bc44864283ec107": {"ta_keywords": "depositors creditors;banking institutions methods;depositors opening account;creditors behaviors depositors;banking institutions;depositors creditors neglected;management banking institutions;behaviors depositors creditors;stakeholders creditors behaviors;banking;neglected management banking;account particular bank;management banking;stakeholders creditors;decision factors depositors;depositors;particular bank;ans stakeholders creditors;stockholders corporation ans;depositors opening;stockholders;factors depositors;bank;stockholders corporation;factors depositors opening;creditors behaviors;corporate governance;pay attention stockholders;corporate governance standpoint;attention stockholders corporation", "pdf_keywords": ""}, "3af5e203368fa2c7959d035493571d181a8682af": {"ta_keywords": "multimodal music analysis;dataset multimodal music;visual analysis music;music performance dataset;music analysis;analysis music performances;multimodal music;music analysis challenges;audio visual analysis;analysis music;musical score midii;performance dataset multimodal;multitrack classicalal music;score midii format;music performance;midii format audio;classicalal music performance;provide musical score;individual tracks audio;recordings individual tracks;music performances;dataset multimodal;audio visual;music performances provide;musical score;midii format;dataset facilitating audio;tracks audio;performances provide musical;note level transcriptions", "pdf_keywords": "dataset multimodal music;music performance datasets;music performance dataset;multimodal music performance;music performance data;modal music analysis;analysis music data;music information retrieval;multimodal music;music audio datasets;music performance analysis;dataset provides musical;visual analysis music;music data;modality music performance;music ikala dataset;rochester multimodal music;music analysis;visual modality music;audiovisual features music;analysis music performances;music analysis synchronization;novel multimodal music;audio visual dataset;multi modal music;music performance urmp;audio visual analysis;visual information music;music performance tasks;audiovisual features"}, "8f43b63ca400a0ea1fdd272f8c83fd67f01d0182": {"ta_keywords": "gene mention tagging;mention tagging using;mention tagging;tagging using conditional;tagging using;crfs syntactic parsing;tagging;approach gene mention;syntactic parsing;gene mention;syntactic parsing taking;parsing;fields crfs syntactic;conditional random fields;parsing taking advantage;crfs syntactic;parsing taking;random fields crfs;random fields;information training data;syntactic;fields crfs;gene;using conditional random;training data;approach gene;information training;fields;conditional random;present approach gene", "pdf_keywords": ""}, "14a09a04c5c295a93ff25492516112cd86fa0114": {"ta_keywords": "labeled sequence transduction;decoders semi supervised;space variational encoder;variational encoder decoders;variational encoder;encoder decoders semi;sequence transduction;sequence transduction task;supervised labeled sequence;sequence transduction methods;encoder decoders;encoder;semi supervised labeled;model labeled sequence;decoders semi;semi supervised;labeled sequence;encoder decoders new;decoders;decoders new model;transduction task;multi space variational;transduction task transforming;introduction labeled sequence;supervised labeled;task transforming sequence;space variational;transduction methods;decoders new;transduction", "pdf_keywords": "decoders semi supervised;variational encoder decoder;variational encoder decoders;variationational encoder decoders;labeled sequence transduction;variational encoder;variationational encoder;languages decoder;space variational encoder;languages semisupervised model;model languages decoder;space variationational encoder;morphology generation models;task morphological reinflection;models machine translation;supervised labeled sequence;processing morphology generation;machine translation;morphological reinflection outperforming;latent embedding;morphology generation;neural machine translation;direction languages semisupervised;encoder decoders semi;employ latent embedding;semi supervised;machine translation systems;sequence transduction task;example morphological reinflection;encoder decoder"}, "5e63e47cb3386b032ec43a92ce5980466228c761": {"ta_keywords": "erasure coded distributed;coded distributed stored;distributed stored storage;data recovery erasure;recovery coded data;storage systems study;recovery erasure coded;challenges data recovery;cluster storing;stored storage systems;distributed stored;data recovery;coded distributed;erasure coded;stored storage;storage systems;storage;terabytes day cluster;day cluster storing;facebook warehouse cluster;cluster storing multiple;recovery coded;recovery erasure;coded data;reveals recovery coded;petabytes code;coded data results;network challenges data;warehouse cluster;storing", "pdf_keywords": "downloadefficient distributed storage;erasure codes network;distributed storage codes;coding distributed storage;distributed storage;efficient distributed storage;usage data centers;distributed storage systems;usage recovery;recovery operations data;data center;recovery operations erasure;data data center;cluster large downloads;data centers increasingly;data centers;facebook warehouse cluster;data centers used;stored cluster;erasure coded data;storage;large downloads recovery;stored cluster large;storage codes;data center network;warehouse cluster study;erasure codes reduce;downloadefficient distributed;data centers combat;bandwidth consumed recovery"}, "af44f5db5b4396e1670cda07eff5ad84145ba843": {"ta_keywords": "modeling textual compositionality;question answering;modeling textual;factoid question answering;textual compositionality;context text classification;input modeling textual;textual compositionality apply;question answering typically;text classification;tasks like factoid;introduce recursive neural;recursive neural network;textual;recursive neural;text classification methods;reason input modeling;words representations methods;question text;question text contains;neural network rnn;like factoid;words representations;rnn model;contains individual words;like factoid question;bag words representations;classification methods tasks;answering typically use;ineective question text", "pdf_keywords": ""}, "1bd43c91ecbf46098ef2b521c5367e849819960e": {"ta_keywords": "neural machine translation;monolingual data neural;backgroundback translation;machine translation nmr;translation nmr iteratively;monolingual data translate;backgroundback translation proven;translation nmr;data translate crucial;machine translation;translation improve model;performance selecting monolingual;selecting monolingual data;data translate;conducting translation improve;iteratively conducting translation;utilize monolingual data;monolingual data;selecting monolingual;translation improve;translation proven effective;utilize monolingual;data neural;conducting translation;method utilize monolingual;neural machine;synthetic data high;translate crucial;data neural machine;nmr iteratively", "pdf_keywords": "neural machine translation;insights intosummaryback translation;machine translation;data machine translation;machine translation proposed;machine translation developed;monolingual data neural;intosummaryback translation effective;machine translation nm;translation improve model;conducting translation improve;translation effective way;iterative translation;iterative translation measure;data train translation;translation model;machine translation annual;backgroundback translation;translation proposed data;domain adaptation low;iterative translation propose;strategy iterative translation;translation propose data;domain adaptation;effective domain adaptation;train translation model;translation improve;backgroundback translation proven;intosummaryback translation;train translation"}, "16f0c508aa54e26aa18e3b0f3c91b0c143c6a605": {"ta_keywords": "user retention minority;representation disparity minority;retention minority;retention minority group;disparity minority groups;speech recognizers;minority group shrink;disparity minority;models speech recognizers;speech recognizers usually;speakers contribute training;recognizers usually trained;affects user retention;user retention;minority groups;learning models speech;recognizers;minority;minority group;learning models;recognizers usually;group shrink time;machine learning models;retention;loss results representation;models speech;minority groups non;native speakers contribute;usually trained minimize;speakers contribute", "pdf_keywords": "fairness machine learning;models unfair mitigate;study fairness machine;fairness machine;fairness stability resultsour;fair models unfair;disparity amplification unfairness;approach fairness stability;risk max fairness;max fairness demographics;fairness demographics;models unfair;fair models;learning using loss;fairness stability;unfair mitigate;study fairness;empirical risk minimization;approach fairness;based approach fairness;models trained stochastic;max fairness;distributionally robust optimization;minimizing disparity disparity;fairness demographics repeated;loss minimization;machine learning models;losss minimization;losss minimization using;minimizing disparity"}, "5c159745fce2b87e8b00307b76f0948b9fa8b1d7": {"ta_keywords": "translation translation tasks;translation tasks;workshop asian translation;syntactic analyses translated;translation tasks methodsall;asian translation translation;asian translation;translated target language;translation input;translation translation;translation input sentence;translation;string f2s translation;analyses translated;f2s translation;f2s translation input;analyses translated target;syntactic parser forest;translated target;translated;syntactic parser;parsed using syntactic;2014 workshop asian;target language;using syntactic parser;target language addition;syntactic analyses;parser forest;possible syntactic analyses;forest possible syntactic", "pdf_keywords": ""}, "7a1a202e268ccc910e8044be556e56aa9eb5a94f": {"ta_keywords": "dependence plots pap;partial dependence plots;dependence plots;model partial dependence;paps ice plots;plots pap including;machine learning necessary;plots pap;applications machine learning;machine learning;instance specific paps;pap including instance;ice plots widely;plots widely used;partial dependence;plots widely;model current paps;ice plots;dependence;plots;specific paps ice;paps ice;learning necessary look;qualitative properties;specific paps;various qualitative properties;qualitative properties model;model partial;validate various qualitative;pap including", "pdf_keywords": ""}, "c674ce6454d69a87f00797f3ec90d1b38b451063": {"ta_keywords": "distributed convex optimization;method distributed convex;dual stochastic gradient;distributed convex;stochastic gradient oracle;primal dual stochastic;convex optimization;convex optimization problems;dual stochastic;stochastic gradient;gradient oracle method;method rate convergence;method distributed;oracle method distributed;gradient oracle;optimization problems networks;convergence terms duality;convex;networks methodswe proposed;introduce primal dual;method optimal;networks methodswe;duality gap probability;method optimal terms;primal dual;problems networks methodswe;proposed method optimal;bound distance iteration;distributed;optimization", "pdf_keywords": "distributed convex optimization;distributed optimization;method distributed optimization;distributed optimization problem;method distributed convex;dual stochastic gradient;stochastic convex optimization;stochastic dual oracle;stochastic dual oracles;formulation distributed optimization;distributed manner dual;stochastic dual;dual formulation distributed;distributed algorithm learning;problem stochastic dual;primal dual stochastic;case stochastic dual;distributed convex;stochastic convex programming;dual problem stochastic;method stochastic convex;dual stochastic;stochastic gradient oracle;gradient method distributed;distributed algorithm;convex optimization;approach distributed algorithm;convex optimization problems;stochastic convex;highly convex minimization"}, "2aecdd190066a57db8fea1e1143dc5fc288050e0": {"ta_keywords": "internet things privacy;things privacy consumers;tradeoff internet things;things privacy;collected internet things;smart grid;privacy consumers;privacy consumers methodswe;data collected internet;data pose privacy;internet things;privacy;internet things promises;internet things focus;smart grid application;backgroundthe internet things;pose privacy threat;quantifying tradeoff internet;privacy threat;pose privacy;privacy threat article;collected internet;tradeoff internet;monitoring physical;advantages control monitoring;data collected;focus smart grid;monitoring;monitoring physical systems;data", "pdf_keywords": ""}, "242cf2e991f0eed4b1309a2a9dff548e8b95900f": {"ta_keywords": "robust speech recognition;learning based beamforming;robust speech;designed robust speech;based beamforming methods;predicts beamforming;predicts beamforming weights;speech recognition;beamforming methods;beamforming methods specifically;estimation beamforming;speech recognition methods;beamforming weights generalized;based beamforming;beamforming;estimation beamforming weight;likelihood estimation beamforming;beamforming weights;network predicts beamforming;distortionless response beamforming;response beamforming;beamforming weights maximum;response beamforming weights;beamforming weight;speech;recognition methods;gc features neural;features neural network;recognition methods methods;neural network", "pdf_keywords": ""}, "b3bd90f630b2d19856ef031b3dddfcb9b041b243": {"ta_keywords": "learning strategies resultsthe;strategies learning solving;learning strategies;learning solving problems;tutor learning;tutored problem solving;tutor learning generalizing;learning strategies learning;hints tutor learning;resultsthe advantage tutored;strategies learning;learning tutored;compare learning strategies;learning tutored problem;showed learning tutored;advantage tutored;learning solving;outperformed learning strategies;advantage tutored problem;solving outperformed learning;feedback hints tutor;results showed learning;tutored problem;tutored;purposeto compare learning;tutor;hints tutor;learning;examples skills;solving problems feedback", "pdf_keywords": ""}, "dd3770b2dbc9668578fefdc078d37457ba9c0b9a": {"ta_keywords": "electrolaryngeal speech enhancement;prediction electrolaryngeal speech;speech enhancement results;approach electrolaryngeal speech;speech enhancement;electrolaryngeal speech;speech enhancement methods;excitation prediction electrolaryngeal;enhancement results electrolary;prediction electrolaryngeal;excitation feature prediction;unvoiced voiced prediction;statistical excitation prediction;evaluation excitation feature;removing micro prosody;voiced prediction hybrid;voiced prediction;excitation prediction;micro prosody;micro prosody low;improve statistical excitation;approach electrolaryngeal;excitation feature;hybrid approach electrolaryngeal;electrolary;introduction evaluation excitation;avoiding unvoiced voiced;results electrolary;electrolaryngeal;evaluation excitation", "pdf_keywords": ""}, "44cabe32482d4b622d9ca00bf23b3ee7950e2710": {"ta_keywords": "human making systems;consequential decisions;consequential decisions wide;introductionhbrid human making;human making;consequential;decisions;decisions wide range;empirical theoretical analyses;systems providing empirical;making systems;decisions wide;introductionhbrid human;body work;making systems increasingly;charge consequential decisions;work advanced understanding;body work advanced;systems increasingly;understanding systems;organization;human;analyses existing empirical;empirical theoretical;systems;goal work;needed organization;providing empirical theoretical;work advanced;making", "pdf_keywords": "human machine complementarity;human complementarity predictive;formalizing human complementarity;maternal ml complementarity;quantify complementarity human;human complementarity;sources human complementarity;human complementarity specific;investigate human complementarity;complementarity predictive decision;complementarity human;human machine predictive;human machine decision;human machine predictions;human machine decisions;predictions implications human;human maternal systems;human maternal ml;decisions human machine;human machine intelligence;human predictive;machine learning human;implications human machine;human machine learning;human predictive decision;classification setting complementarity;human machine contexts;complementarity predictive;machine complementarity;ml complementarity causes"}, "3ac59132297f4e50d5e83852555392f9ff05d8b4": {"ta_keywords": "optimization composites composites;composite optimization composites;optimization composites;composites composites optimized;optimized using composites;composites optimized using;composites optimized;composite optimization;methods composite optimization;using composites composites;composites composites composites;composites composites;using composites;composites;composite;new methods composite;optimization;methods composite;optimized using;optimized;new methods;development new methods;methods;using;report development;development new;report development new;development;new;report", "pdf_keywords": "decentralized distributed optimization;convex composite optimization;distributed optimization methodsin;distributed optimization;distributed optimization optimization;distributed optimization new;method composite optimization;composite optimization;method convex composite;stochastic convex optimization;composite optimization applied;case composite optimization;method stochastic convex;method decentralized stochasticwe;composite optimization problem;optimization applied decentralized;convex optimization;solving convex composite;method smooth decentralized;non smooth optimization;smooth decentralized distributed;gradientient sliding algorithm;smooth optimization problems;smooth optimization;composite optimization uses;convex composite;convex composite problem;method convex;convex optimization network;methods respectively convex"}, "ff3b83ef0a153ed376556057269f3a61da3a103a": {"ta_keywords": "instruments notes solo;keyboard different instruments;multiple instruments time;play multiple instruments;automatic instrumentation dynamically;assigning instruments notes;automatic instrumentation;multiple instruments;notes solo music;dynamically assigning instruments;keyboards allow musician;instruments notes;solo music performance;musician play multiple;different instruments;music performance addition;assigning instruments;notes solo;instrumentation dynamically;instruments;instruments time;allow musician play;music performance;solo music;instrumentation dynamically assigning;feasibility automatic instrumentation;introduction modern keyboards;instruments time assigning;pitch ranges keyboard;musician play", "pdf_keywords": "separation existing music;separation multitrack music;processing music context;input representation music;music perform separation;music input representation;generate music fundamental;model multitrack music;symbolic music generation;method music generation;processing music;process music;symbolic multitrack music;genres ensembles;multitrack music;learning separate parts;generate music;genres ensembles experiments;music datasets;algorithms voice separation;approach processing music;analysis music;music generation using;music employ data;networks symbolic music;automatic instrumentation learning;step music generation;representation music;ability generate music;music employ machine"}, "cb90d5ea3a95b4c6ec904f622f51d752f506636e": {"ta_keywords": "preferences ai;methodswhen preferences ai;reasoning preferences;reasoning preferences important;preferences ai expect;decision making machines;learning reasoning preferences;preferences decisions adhere;backgroundpreferences central decision;preferences decisions;consistent preferences decisions;preferences necessary understand;make decisions recommendations;consistent preferences;principles working preferences;ai;ai expect;preferences important;recommendations consistent preferences;decisions recommendations consistent;decisions recommendations;make decisions;preferences important area;preferences necessary;sciences methodswhen preferences;decisions adhere;decisions adhere certain;working preferences;ai expect make;central decision making", "pdf_keywords": ""}, "d4f5f1a196e203226e4a69d52a04d46823f32fb3": {"ta_keywords": "parallel corpora additionally;available parallel corpora;parallel corpora;parallel corpora collection;parallel sentences web;corpora tools;web combining corpora;crawled monolingual corpor;corpora additionally;corpora tools methods;combining corpora tools;web crawled monolingual;resultswe parallel sentences;corpora collection indic;sentence pairs web;combining corpora;crawled monolingual;corpora collection;corpora;million sentence pairs;parallel sentences;indic languages methodswe;publicly available parallel;resultswe parallel;sentences web combining;corpora additionally 37;available parallel;collection indic languages;sentences web;monolingual corpor", "pdf_keywords": "parallel corpora indic;indic languages parallel;indictrans trained multilingual;crawled monolingual corpora;available parallel corpora;indic monolingual corpora;indic languages resource;recent advances multilingual;advances multilingual representation;data indic languages;corpora indic languages;large indic monolingual;multilingual neural machine;indic languages analysed;indic languages using;parallel corpora;resource indic languages;crosslingual textual similarity;translation indictrans trained;corpora accurate multilingual;parallel corpora collection;character recognition indic;multiple indic languages;indic language pairs;multilingual language technologies;accurate multilingual representation;collection indic languages;indic monolingual;indic languages combining;indic languages"}, "d301054c2819e1a21480800fdabbe5ae909abe09": {"ta_keywords": "synthesis inferring programs;program synthesis;program synthesis inferring;learn program abstractions;effective program synthesis;program abstractions searchheuristics;inductive program synthesis;program synthesis depends;abstractions searchheuristics inductive;searchheuristics inductive program;finding programs solve;inferring programs examples;program abstractions;inferring programs;abstractions searchheuristics;finding programs;strategy finding programs;programs solve;programs examples desired;searchheuristics inductive;programs efficient;programs examples;functions build programs;inductive program;programs efficient search;learn program;program;build programs efficient;task introduce leaching;build programs", "pdf_keywords": "program abstractions neural;learn program abstractions;learning synthesis;synthesis inferring programs;programs synthesis;program synthesis;language learned synthesis;learning abstraction search;learning language abstraction;effective program synthesis;programs synthesis useful;models program learning;program synthesis inferring;inductive program synthesis;abstraction program search;learned synthesis algorithms;learning abstraction;learning synthesis remote;approach learning synthesis;language program learning;method learning synthesis;abstractions program search;program synthesis depends;program abstractions search;learned synthesis;learn use abstractions;program learning integrates;programs learned languageannotated;learned synthesis development;implementation program learning"}, "27b7489bd54dfd585edd2ba0da3920a31e7fd8b5": {"ta_keywords": "machine behavior beliefs;interacting machine humans;economic game theory;game theory;economic game;beliefs machine behavior;model interaction game;game theory results;discrete decision making;humans machines imperfect;machine behavior;perspective economic game;interaction humans machines;discrete decision;interacting machine;interaction game;machine humans naturally;machine humans;humans machines;machines imperfect information;introduction interacting machine;interaction game games;games investigated perspective;beliefs machine;beliefs affect interaction;results discrete decision;machines;natural model interaction;behavior beliefs;game games investigated", "pdf_keywords": ""}, "af38829cdb55ee7b71d49399f71397d975e40a95": {"ta_keywords": "question answering dataset;conditional answers dataset;answers dataset conditionalqa;questions conditional answers;answers dataset;answers dataset features;question answering;answering dataset contains;answering dataset;backgroundwe question answering;contains complex questions;conditional answers;questions conditional;combination extractive questions;dataset conditionalqa;extractive questions;reasoning combination extractive;dataset conditionalqa methodsin;extractive questions yes;complex questions conditional;yes questions questions;addition conditional answers;compositional logical reasoning;yes questions;questions yes questions;questions require compositional;logical reasoning combination;questions;questions questions;dataset contains complex", "pdf_keywords": "question answering dataset;conditional answers dataset;predicting answers questions;question answering variety;model question answering;question answering;answers dataset conditionalqa;predict answers conditions;background question answering;tool question answering;answers dataset features;answers dataset;question answering long;questions conditional answers;model predicting answers;challenging dataset conditionalqa;answers questions documents;policy domain corpus;conditionalqa contains questions;answers conditions model;answers dataset requires;conditional answers;contains questions conditional;answering dataset;predicting answers;answering long documents;answering dataset contains;predict answers;extractive questions;conditionalqa task challenging"}, "2c2234548de4694b6455a19cd0d85a9d6c473456": {"ta_keywords": "approximate search methods;measure search quality;tools measure search;approximate search;search methods spaces;search quality methodswe;metric spaces tools;search methods;new search methods;search quality;methods non metric;measure search;spaces tools measure;search methods non;metric;non metric metric;spaces tools;art approximate search;non metric;metric metric;benchmarks evaluate results;state art approximate;benchmarks;benchmarks evaluate;code run benchmarks;metric spaces;metric metric spaces;run benchmarks;tools measure;search", "pdf_keywords": "similarity search library;nearest neighbor search;similarity search methods;spaces nearest neighbor;similarity search;crossplatform similarity search;library fast similarity;search methods distances;useful nearest neighbor;searching generic metric;similarity nn search;sparse space distances;search metric spaces;search general metric;spatial search metric;evaluation similarity search;search metric;fast similarity;nearest neighbor;generalization nearest neighbor;search similarities;metric space library;fast similarity nn;nearest neighbor query;based nearest neighbor;distances spaces nearest;toolkit evaluation similarity;clusters search metric;method nearest neighbor;method search metric"}, "c00ba15810496669d47d2ed5b627e6c7d2b1f6aa": {"ta_keywords": "introductionpre training paraphrasing;training paraphrasing;multi document paraphrasing;training paraphrasing introduce;document paraphrasing;unsupervised multi lingual;document paraphrasing objective;paraphrasing;paraphrasing objective;paraphrasing introduce;texts languages conditioning;paraphrasing objective resultsmarge;reconstruction target text;lingual multi document;language modeling;pre trained sequence;masked language modeling;sequence model learned;language modeling paradigm;trained sequence sequence;multi lingual;related texts languages;multi lingual multi;introductionpre training;paraphrasing introduce marge;lingual multi;texts languages;target text;languages conditioning;trained sequence", "pdf_keywords": "learn paraphrasing paraphrasing;multilingual autoencoder retrieves;introduce multilingual autoencoder;multi document paraphrasing;multilingual autoencoder;unsupervised multi lingual;learn paraphrasing;neural machine translation;multilingual sequence sequence;machine translation powerful;document paraphrasing;document paraphrasing objective;paraphrasing paraphrasing;multilingual sequence;language models neural;news learn paraphrasing;machine translation;paraphrasing paraphrasing domain;neural language models;translation applications neural;paraphrasing objective described;cross attention decoder;machine translation important;generating documents languages;attention decoder;trained sequence sequence;paraphrasing objective;machine translation applications;paraphrasing;learn retrieval reconstruction"}, "4b890b6ded71f005414e55adb87c23efd437ef95": {"ta_keywords": "parametric speech synthesis;speech synthesis quality;synthetic speech quality;speech synthesis;concatenative speech synthesis;statistical parametric speech;speech synthesis including;generated speech parameter;synthetic speech;parametric speech;speech parameter;speech parameter trajectories;speech quality;backgroundthe synthetic speech;quality natural speech;speech quality good;generated speech;synthesis quality natural;quality statistical parametric;synthesis quality;observed generated speech;text speech;concatenative speech;good concatenative speech;synthesis including text;text speech 45s;based modulation spectrum;quality degradation smoothing;including text speech;modulation spectrum high", "pdf_keywords": ""}, "1006d191e9eb5b4dbc35fc0bb389328ddc75cba7": {"ta_keywords": "pre trained language;clinical messagemost widely;text language;language models operate;clinical messagemost;key clinical messagemost;token free models;trained language models;language models;trained language;process text language;tokens corresponding word;text;text preprocessing pipelines;characters benefits process;prone text preprocessing;text preprocessing;raw text;text bytes characters;sequences tokens;text bytes;raw text bytes;directly raw text;process text;messagemost widely used;bytes characters benefits;text language box;sequences tokens corresponding;clinical;tokens", "pdf_keywords": "neural machine translation;free language models;text text prediction;text prediction;neural language models;text prediction predictions;machine translation systems;token free language;machine translation important;language models increased;machine translation;linear language models;language models;translation systems;free language;byte level models;language models important;language models potential;text preprocessing tokenization;language mt5;byte level modeling;languages byt5 better;multilingual models robust;language technologies 2021;performance novel text;translation systems applications;models robust multilingual;robust multilingual models;language models field;sub word decoder"}, "041b2510e54b2504890cb9f58b9bbc5601f35e3e": {"ta_keywords": "teaching structured neural;assignments teaching structured;interactive assignments teaching;interactive assignments;assignments designed interactive;introduction interactive assignments;teaching structured;structured neural;assignments teaching;structured neural neural;nl presented assignments;neural architectures lstms;assignments;presented assignments;training methods fulll;assignments designed;neural architectures;neural network;modern neural architectures;presented assignments designed;architectures lstms;neural network network;approximate search training;neural neural network;search training;modern neural;parse trees logical;training methods;forms modern neural;trees logical forms", "pdf_keywords": ""}, "399ab2a0eddf7a7abf776241d5c0a2c4cd5bf313": {"ta_keywords": "discriminative model speech;model speech recognition;continuous speech recognition;speech recognition;speech signal searches;model continuous speech;speech recognition directly;speech model represented;speech recognition based;discriminative model;speech model;introduction discriminative model;model speech;proposes discriminative model;recognition decoder;parameters speech model;finite state transducers;recognition decoder given;discriminative model continuous;given input speech;paper proposes discriminative;state transducers methods;input speech signal;recognition based weighted;state transducers;introduction discriminative;speech signal;input speech;continuous speech;parameters speech", "pdf_keywords": ""}, "4b9bc5b5985bbfbc860039b98f233f8a43ad9171": {"ta_keywords": "pde especially uncertainties;neural network parameterise;especially uncertainties modelled;numerical partial differential;coefficients variability physical;numerical partial;pde captured features;equations pde especially;dimensionality commonly;neural network;variability physical quantities;uncertainties modelled;quantities derived pde;dimensionality commonly encountered;uncertainties modelled equations;using neural network;parameterise physical quantity;partial differential equations;using neural;coefficients variability;pde especially;dimensionality;modelled equations random;variability physical;random coefficients variability;physical quantity function;physical quantities;parameterise physical;propose using neural;pde", "pdf_keywords": "function nonlinear quantum;chain reaction pde;important parametric pdes;parametric polydease pde;quantum equation represented;nonlinear quantum;solution quantum equation;pde method feasible;polydease pde method;parametric pdes;quantum quantum equation;quantum equation;parametric pdes wide;equations pde especially;pde inhomogeneous coefficient;polydease pde;coefficients solution quantum;nonlinear quantum quantum;dimensional partial differential;schr odinger equation;reaction pde inhomogeneous;quantum equation case;quantities derived pde;pde method;equations pde;solution quantum;odinger equation;pde inhomogeneous;neural network solve;pde especially uncertainties"}, "302face5b5a0944cab13665a2d4e07ef3aaf5240": {"ta_keywords": "domain question answering;question answering;question answering task;question answering especially;answering task involves;answering;answering especially exploring;answering task;question answer pairs;answer paired disambiguated;answering especially;answer paper introduce;set question answer;single unambiguous answer;exploring new topics;introduce ambientgqa;answer pairs plausible;contextambigqa inherent open;unambiguous answer paper;questions single unambiguous;inherent open domain;answer paper;new topics;introduce ambientgqa new;question answer;disambiguated rewrite;contextambigqa;ambientgqa new;ambientgqa new open;paired disambiguated rewrite", "pdf_keywords": "domain question answering;identify ambiguous questions;question answering applications;question answering task;question answering;approach question answering;question disambiguation model;disambiguated questions;question disambiguated questions;ambiguous questions significantly;questions ambiguous entity;disambiguated questions differentiate;identify ambiguous question;develop question disambiguation;question annotated answers;question disambiguation;tool annotating questions;answers potentially ambiguous;providing disambiguated question;annotating questions;disambiguated question corresponding;elicit questions ambiguous;questions ambiguous;question ambiguity settings;annotating questions world;question providing disambiguated;ambiguous questions;oopen questions containing;answers open domain;potentially ambiguous open"}, "d86227948b6000e5d7ed63cf2054ad600b7994a0": {"ta_keywords": "text classification;sentiment analysis factoid;text classification aimto;methods text classification;question answering;composition rivals syntactic;outperforms models sentiment;sentiment analysis;question answering tasks;models sentiment;models sentiment analysis;simple deep neural;introductiondeep unordered composition;syntactic;syntactic methods text;syntactically ignorant;syntactic methods;deep neural;factoid question answering;unordered composition;sentiment;bag words;model syntactically ignorant;syntactically;simple deep;composition;neural network;neural;classification;rivals syntactic methods", "pdf_keywords": ""}, "8c4b187bdaf91bf068adfe005a0463c4f9c36387": {"ta_keywords": "pixel wise losses;backgrounddelineation curvilinear structures;backgrounddelineation curvilinear;deep architectures continued;deep architectures;powerful deep architectures;backgrounddelineation;deep learning;habitual pixel wise;computer vision;computer vision multiple;using habitual pixel;losses binary cross;habitual pixel;wise losses binary;deep learning focused;finding powerful deep;pixel wise;applications advent deep;advent deep learning;losses binary;cross entropy methodsin;vision multiple;problem computer vision;binary cross entropy;curvilinear structures;vision multiple practical;cross entropy;entropy methodsin;deep", "pdf_keywords": "refinement segmentation natural;segmentation natural;segment image natural;refinement segmentation;convolutional networks biomedical;recursive refinement segmentation;segmentation natural biomedical;convolutional networks;deep architectures continued;deep architectures;boost accuracy deep;biomedical image segmentation;networks biomedical image;accuracy deep;delineate segment image;powerful deep architectures;structure predicted image;deep network;image segmentation;accuracy deep network;natural images;convolutional model delineate;natural images neuronal;image natural biomedical;natural biomedical images;deep learning;concept convolutional networks;pixel wise losses;segmentation;segment image"}, "8147a495b9a933742f06458244f7c5df00767c4e": {"ta_keywords": "open information extraction;methodsopen information extraction;learning methodsopen information;improve open information;extracting open;ranking extractions;ranking extractions based;open information;learning methodsopen;information extraction;modeling ranking extractions;aware learning methodsopen;extracting open domain;information extraction iterative;task extracting open;open domain assertions;methodsopen information;information extraction task;assertions resultswe extraction;recall extracted assertions;open ii confidence;extraction likelihood confidence;improve open;extracted assertions;extraction iterative rank;extracted assertions resultswe;extraction task extracting;assertions natural language;extractions based estimated;extracting", "pdf_keywords": "open information extraction;modeling sequence labeling;recall extracted assertions;assertions natural language;reranking assertion generation;open confidence modeling;sequence labeling;open relation extraction;sequence labeling environment;quality assertions extracted;backgroundopen information extraction;assertions extracted;assertion generation;precision recall extracted;extracted assertions;information extraction task;information extraction;open domain assertions;supervised open systems;confidence modeling sequence;extraction task extracting;confidence using binary;relation extraction iteratively;assertion generation using;modeling ranking extractions;extraction likelihood confidence;confidence modeling ranking;information extraction important;assertions methodswe extraction;natural language sentences"}, "43e8e371449aaef34c2f43ae90f2157fd5a617bd": {"ta_keywords": "pricing socially optimal;pricing coordination dynamic;socially optimal feedback;pricing coordination;socially optimal cost;pricing socially;pricing mechanisms;control nash equilibrium;control strategy selfish;use pricing mechanisms;pricing mechanisms means;strategy selfish agents;coordination dynamic dynamics;introduction pricing coordination;feedback control nash;nash equilibrium agents;incurred pricing socially;selfish agents methodswe;socially optimal;coordination dynamic;optimal feedback control;nash equilibrium;equilibrium agents convex;selfish agents;optimal feedback;achieve socially optimal;control nash;strategy selfish;use pricing;feedback control strategy", "pdf_keywords": ""}, "830a396a4a77567caad1c155dd3b22597314e9f3": {"ta_keywords": "fairness institutionalal logics;fairness institutionalal;algorithmic fairness;algorithmic fairness poor;backgroundalgorithmic fairness institutionalal;model algorithmic fairness;backgroundalgorithmic fairness;recommendation non profit;institutionalal logics social;fairness poor;fairness;institutionalal logics;social technical systems;logics social choice;stakeholders multiple social;application personalized recommendation;non profit methodswe;personalized recommendation non;fairness poor match;algorithmic;social technical;multiple social technical;logics social;institutionalal;social choice;non profit;recommendation non;stakeholders multiple;personalized recommendation;application personalized", "pdf_keywords": ""}, "5083d9e25113a09faeba7d56b7808e2f77b5c15e": {"ta_keywords": "multihop inference rules;representations multihop inference;multi hop reasoning;multihop inference;hop templates neural;inference rules present;inference rules;symbolic knowledge base;hop reasoning using;large symbolic knowledge;multi hop templates;implementations second order;hop reasoning;hop templates;templates neural;templates neural model;symbolic knowledge;introductiondifferentiable representations multihop;rules present efficient;inference;representations multihop;order multi hop;efficient differentiable implementations;knowledge base;using large symbolic;second order multi;multi hop;reasoning using large;introductiondifferentiable representations;hop", "pdf_keywords": "semantic parsing;symbolic knowledge base;semantic parsing using;knowledge base completion;long reasoning chains;reasoning templates;reasoning templates formed;reasoning chains;results semantic parsing;step reasoning templates;parsing;downloadable based relation;reasoning chains use;approach knowledge base;large symbolic knowledge;knowledge base;multi step reasoning;reasoning chains accuracy;generating relations;logic long reasoning;parsing using;implement relation matrix;knowledge base kb;symbolic knowledge;parsing using second;method generating relations;relations bacterial downloadable;structure implement relation;semantic;implement relation"}, "4b762c0344f14bb00d590f5666c27b3aac7b0a7d": {"ta_keywords": "neural language models;rnn language model;recurrent neural language;introductiondependency recurrent neural;recurrent neural network;performance recurrent neural;senstence completion;neural language;language models senstence;senstence completion challenge;model incorporating syntactic;introductiondependency recurrent;models senstence completion;improve performance recurrent;completion challenge dependency;recurrent neural;performance recurrent;rnn language;language model incorporating;language models;incorporating syntactic dependencies;word predicted;language model;network rnn language;word predicted methodswe;neural network rnn;senstence completion aimthe;completion;incorporating syntactic;syntactic dependencies sentence", "pdf_keywords": "recurrent neural language;neural language models;language models neural;learning dependency parse;dependency language models;models neural language;dependency parsing model;novel language models;neural probabilistic language;language neural language;language neural;syntactic dependencies rn;recurrent neural networks;novel language model;dependent gram language;gram language models;sequential gram language;modeling language processing;recurrent neural;neural language;use recurrent neural;introductiondependency recurrent neural;language models;language modeling;modelsthe use recurrent;language model novel;novel dependency regression;gram language model;language modeling tothe;probabilistic language model"}, "bcd45c86e1bcf8d1411eb6704c4c58d0831b5b4f": {"ta_keywords": "words higher frequencies;models based multinomial;based multinomial distribution;multinomial distribution wide;based multinomial;text treat words;statistical models text;multinomial distribution;classification tasks;higher frequencies occurrence;frequencies occurrence sensible;multinomial;frequencies occurrence;classification;models text;models text treat;treat words higher;statistical models;words higher;occurrence;wide range classification;text;present statistical models;classification tasks classes;range classification tasks;occurrence sensible manner;text treat;occurrence sensible;treat words;statistical", "pdf_keywords": ""}, "0ad8284dbae11901a725cc71318a165c08852278": {"ta_keywords": "model voice conversion;approach voice conversion;voice conversion;methods voice conversion;voice conversion using;voice conversion studies;density model speaker;speaker model methods;speaker model voice;voice conversion aim;model speaker model;model speaker;model voice;speaker model;mixture model;source target speakers;target speakers widely;voice;mixture model gm;speaker;gassian mixture model;conversion studies based;speakers widely;conversion studies;probabilistic densities joint;joint density model;conversion;model methods voice;conversion using joint;gm probabilistic densities", "pdf_keywords": ""}, "b21b927c251c415b601b6d7f785a42cc5c292635": {"ta_keywords": "coreference clusters scientific;coreference scientific knowledge;relations coreference clusters;relations coreference scientific;coreference clusters;annotations tasks;scientific knowledge graph;coreference scientific;includes annotations tasks;annotations tasks develop;task identification entities;relations coreference;entities relations coreference;clusters scientific articles;sciie shared span;knowledge graph;coreference;annotations;articles create scierc;knowledge graph construction;introductionmulti task identification;identification entities relations;task identification;tasks develop unified;includes annotations;representations multi task;identifying entities relations;identifying entities;introduce multi task;dataset includes annotations", "pdf_keywords": "annotating scientific entities;extracting scientific entities;entity recognition;scientific entities relations;annotations tasks;entities recognition task;semantic scholar corpus;scholar corpus automatic;entity recognition relation;scholar corpus;coreference automated knowledge;annotation human agreement;entity relation extraction;scholar corpus corpus;annotating scientific;sentences dataset annotation;scientific information extractor;coreference clusters scientific;entities recognition;framework annotating scientific;relation extraction;scientific knowledge graph;improvement entity recognition;relations coreference clusters;recognition relation extraction;scientific knowledge graphs;annotations tasks develop;relation extraction mainly;annotation functional information;information extractor sciie"}, "34fb3e21a63fb2987f7a87f88ecf49aea53cff36": {"ta_keywords": "discuss role clinician;clinician;clinician development effective;management patient disease;role clinician;patient disease;clinician development;management patient;role clinician development;approach management patient;patient;disease;approach management;effective approach management;management;development effective approach;effective approach;approach;article discuss role;discuss role;development effective;role;effective;purpose article discuss;article discuss;development;purpose article;article;purpose;discuss", "pdf_keywords": ""}, "95cedaeb3178a4671703a05171a144e6b964a819": {"ta_keywords": "count based neural;neural language models;neural language model;language models;language model;language modeling;language model described;count based gram;language modeling exist;language models methodsa;backgroundgeneralizing hybridizing count;hybridizing count based;paradigm language modeling;based neural language;based gram models;hybridizing count;generalizing hybridizing count;gram models;gram models advantages;neural language;count based;modeling exist count;language;based gram;neural ls achieve;based neural;speed neural ls;exist count based;neural ls;major paradigm language", "pdf_keywords": "neural language models;neural networks language;language models;lstm language models;novel language models;modeling language modeling;language modeling;language modelling;gram language models;modeling language;language modeling exist;modeling probability word;language modeling generalizes;language models similarly;language models important;models probabilities grams;probabilities sequences words;language modeling approach;language modeling development;neural language;paradigms language modeling;networks language modelling;introduction language models;based gram models;gram neural network;gram models;language models ls;language models allowed;learning grams neurally;language models article"}, "a13d8400813743adb22ba0bd0570c49af2675a39": {"ta_keywords": "continuous speech separation;speech separation;task speech separation;speech separation aiming;speech separation cas;sentence level separation;perform separation independently;separation models task;partially overlapped recording;separation models;separation performance;overlapped recording straightforward;separation independently;perform separation;separation independently simple;segment long recording;overlapped recording;separating overlap free;separating overlap;separation;level separation models;blocks perform separation;separation aiming separating;separating;separation performance satisfactory;background continuous speech;level separation;separation aiming;continuous speech;long recording", "pdf_keywords": ""}, "54316d2861eb3d575a8c7d071f4cf7c2fc30be01": {"ta_keywords": "robustify pre trained;conventional adversarial training;adversarial training;adversarial training approach;queries randomized smoothing;robustification;applied robustification;conventional adversarial;adversarial;classifiers challenging;trained classifiers challenging;pre trained classifiers;robustify;robustify pre;classifiers;trained classifiers;demonstrate conventional adversarial;works applied robustification;ability robustify pre;randomized smoothing;classifiers challenging task;ability robustify;robustification process;queries randomized;applied robustification process;multiple queries randomized;randomized smoothing element;smoothing;randomized;pre trained", "pdf_keywords": ""}, "73bbd0b53044e9f518a3596a3607521bbce12fc2": {"ta_keywords": "regularized segmentmentation losses;segmentmentation losses simplicity;segmentmentation losses;regularized segmentmentation;dgress regularized segmentmentation;weakly supervised networks;neural networks loss;losses simplicity gradient;weakly supervised;training deeper;context weakly supervised;simplicity gradient descent;gradient descent;gradient dgress regularized;method training deeper;training deeper complex;networks loss functions;gradient descent gd;segmentmentation;supervised networks demonstrate;optimization context weakly;supervised networks;simplicity gradient;regularized;loss functions architectures;loss function alternative;loss functions;losses simplicity;networks loss;loss", "pdf_keywords": ""}, "a5b1d1cab073cb746a990b37d42dc7b67763f881": {"ta_keywords": "semantic parsing structured;like semantic parsing;nl understanding tasks;semantic parsing;nl questions structured;natural language nl;tasks like semantic;parsing structured;language nl understanding;pretrained language models;parsing structured data;natural language;models ls text;language models ls;form nl questions;language nl;text based natural;parsing;text suitable tasks;nl understanding;nl text;nl text suitable;form nl text;language models;like semantic;ls text based;questions structured tabular;pretrained language;based natural language;free form nl", "pdf_keywords": "semantic parsing tables;neural semantic parser;neural semantic parsing;strong semantic parser;semantic parser learning;supervised semantic parsing;semantic parsing results;novel semantic parser;semantic parsing structured;data semantic parsers;weakly supervised semantic;parsing tables;like semantic parsing;parser learning;useful semantic parser;semantic parser model;semantic parsers;semantic parser;semantic parsing;parser semantic;semantic parsing paradigms;parsing structured;semantic parser semantic;parsing structured data;data natural language;semantic semantic parsing;different semantic parsing;nl understanding tasks;semantic parsing important;parser learning representations"}, "7e870eb8d580fb1b8b7a8f97d94d67555a225635": {"ta_keywords": "intelligent message addressing;recipient contact intelligent;message addressing finding;addressing finding persons;intelligent auto completion;potential recipients message;expert search active;expert search;recipients message composition;specified recipients;message addressing;backgroundfind persons knowledgeable;intelligent message;task intelligent message;topic expert search;contact intelligent;contact intelligent auto;intended recipient contact;previously specified recipients;letters intended recipient;recipients message;recipient contact;persons knowledgeable given;intended recipient;auto completion;recipients;auto completion methods;finding persons;related task intelligent;message composition", "pdf_keywords": ""}, "267b94325028e0e2e6da1ae2cbe7f7a93284722e": {"ta_keywords": "search disambiguation email;similarity measures text;disambiguation email using;disambiguation email;similarity metrics documents;contextual search disambiguation;similarity measures;extended similarity metrics;context contextual search;similarity metrics;contextual search;metrics documents objects;information retrieval;search disambiguation;extended similarity;solving information retrieval;consider extended similarity;similarity;information retrieval problems;measures text;email using graphs;problems similarity measures;context contextual;documents non textual;retrieval;documents closely connected;non textual objects;disambiguation;closely connected documents;textual objects", "pdf_keywords": ""}, "a309ad4c4088843d230be1a85806960e633e1e46": {"ta_keywords": "deep learning;deep learning models;models learn;biases annotation artifacts;social biases annotation;models training;careful design datasets;development deep learning;learning models training;background nonliol community;biases annotation;training data;datasets deliver;design datasets deliver;models training data;nonliol community;datasets;training data lot;development deep;design datasets;patterns social biases;annotation artifacts algorithmic;models learn kinds;nonliol community currently;learning models;resources development deep;clear models learn;learning;deep;social biases", "pdf_keywords": "linguistics ai ethics;linguistics ai;computational linguistics ai;ai ethics;ai ethics interdisciplinary;parrots language models;computational linguistics;computational linguistics established;language models big;approach computational linguistics;language models;computational language knowledge;non linguistics;computational language technologies;linguistic;computational language research;process natural language;linguistics;computational linguistics bethe;linguistic conceptual reperfusion;computational language;language technologies;studying linguistic;language research;language technology;arguments training systems;language processing potential;association computational linguistics;development language models;language research article"}, "1be28ce9a1145c2cf4f78e6c494a4c15397fbac3": {"ta_keywords": "drug interactions datasets;detection adverse drug;drug interaction detection;adverse drug interaction;biomedical knowledge graphs;datasets large biomedical;large biomedical knowledge;adverse drug;drug drug interactions;interactions datasets large;drug interactions;drug interaction using;interactions datasets;biomedical knowledge;drug interaction;accurate detection adverse;gs adverse drug;detection adverse;biomedical gs adverse;knowledge graphs gs;drug drug;datasets large;knowledge graphs;gs adverse;interaction detection;interaction detection methods;knowledge gs;biomedical gs;machine learning models;datasets", "pdf_keywords": "drug interactions network;predicting drug interactions;drug interaction prediction;interaction prediction drugbank;predict drug interactions;prediction drug interactions;drug interaction algorithm;drug interaction predictions;drug interactions knowledge;drug prediction;methods drug prediction;drug di prediction;method predicting drug;predicting drug drug;interactions network drugs;approach predict drug;predicting drug;approach prediction drug;nodes drug interaction;prediction drugbank;analysis drug interactions;model predict drug;methods prediction drug;predict drug;predictive model drug;drug pair network;model drug interactions;drug prediction impact;prediction drug;network drugs interactions"}, "ce6143e24a455edc233f12933e9903426b963799": {"ta_keywords": "statistical topic models;topic models;topic models latent;models latent dirichlet;latent dirichlet;latent dirichlet al;summarize large document;large document collec;statistical topic;document collec;model visualize summarize;background statistical topic;summarize large;visualize summarize large;parallel implementations variational;document collec tions;electronic document collec;large document;models fast scalable;implementations variational;visualize summarize;important models fast;dirichlet;variational;models latent;summarize;models fast;dirichlet al location;topic;dirichlet al", "pdf_keywords": ""}, "51bf7a3aee6b1f61b902625f6badffedf200d31a": {"ta_keywords": "rules encoded deep;deep generative model;deep generative;generative model gaan;encoded deep generative;generative model;gaan learns model;model gaan learns;backgrounda deep generative;generative;rules encoded network;rules target distribution;generative model resultsto;learns model rich;encoded network rule;specific rules encoded;rules encoded;learns model;encoded deep;semantic physical rules;gaan learns;model gaan;rules target;network rule;physical rules target;distribution obscure rules;manipulation specific rules;obscure rules encoded;backgrounda deep;gaan", "pdf_keywords": "deep generative;rewriting deep generative;rewriting deep;backgrounda deep generative;deep generative model;rules encoded deep;encoded deep generative;method rewriting deep;image video synthesis;convolutional layer deep;video synthesis;visual editing;deep generator;layer deep generator;generative model increasingly;memory nonlinear convolutional;visual editing effects;gan generate images;generative;generative model;create visual editing;simple editing;layer deepwe generalize;images model efficiently;deepwe generalize idea;manipulating layer deepwe;generating adversarial;generating images;backgrounda deep;generalized gan"}, "990c7726dd31723f97a364828d5191080fe7ec2d": {"ta_keywords": "chiral topological superconductor;topological superconductor;topological quantum computing;topological superconductor decomposed;universal topological quantum;topological quantum;topological superconductor strongly;chiral topological;mode chiral topological;modes chiral topological;model braidings topologically;braidings topologically;braidings topologically assembled;majorana edge modes;correlated majorana edge;superconductor decomposed fibonacci;edge modes chiral;superconductor;edge mode chiral;universal topological;report universal topological;strongly correlated majorana;modes chiral;superconductor decomposed;superconductor strongly correlated;majorana edge mode;superconductor strongly;correlated majorana;quantum computing;mode chiral", "pdf_keywords": "topological superconducting quantum;topological superconductor quantum;topological quantum computing;topological quantum computation;topological superconducting tqc;topological superconducting;quantum gates superconductor;superconducting quantum;chiral topological superconducting;universal quantum circuit;superconductor quantum;universal topological quantum;superconducting quantum wires;topological quantum;topological superconductor;topological superconductor application;wave topological superconductor;chiral topological superconductor;topological qantum gates;universality quantum circuit;quantum computation fundamental;quantum computation;quantum quantum computation;quantum circuits fundamental;quantum computation using;universal quantum gates;quantum computing;design universal quantum;based quantum circuit;quantum quantum circuit"}, "6fae71765a5e86dfef2f93bbe03c4a2e20f827b5": {"ta_keywords": "english speech recognition;english automatic speech;speech recognition ar;forspoken language translation;language translation iwt;automatic speech;speech recognition foriwl;english automatic;speech recognition;automatic speech recognition;language translation;nationalist english speech;contribution english automatic;workshop forspoken language;translation iwt;forspoken language;speech processing;translation iwt annual;core speech processing;speech processing technologies;nationalist english;international workshop forspoken;english speech;translation;2015 international workshop;speech;naist contribution english;core speech;recognition ar;recognition foriwl 2015", "pdf_keywords": ""}, "b9e6c65aacfe8ecc1b7833b47803672273a918ec": {"ta_keywords": "prevention treatment diseases;prevention treatment;etiology disease;strategies prevention treatment;diseases;treatment diseases;prevention;new strategies prevention;disease;role etiology disease;etiology disease potential;strategies prevention;systematic review literature;disease potential;disease potential development;systematic review;results systematic review;treatment;etiology;present results systematic;results systematic;role etiology;systematic;review literature;review literature topic;knowledge role etiology;purpose article present;purpose article;article present;article", "pdf_keywords": ""}, "cf08bef866885edb8b001deb18e582eec94c51de": {"ta_keywords": "ted talk summarization;speech summarization open;automatic speech summarization;speech summarization;talk summarization paper;talk summarization;topics speaker speaker;topics speaker;automatic speech;domain ted talk;summarization open domain;domain ted talks;semantic acoustic features;diversity topics speaker;ted talks;messages free talks;summarization paper;ted talk;free talks methods;summarization open;problem automatic speech;extract topic related;semantic acoustic;extract topic;ted talks large;use semantic acoustic;summarization;free talks;summarization paper address;talks methods", "pdf_keywords": ""}, "b790c3e712c92065d596364af81a494adbc62c39": {"ta_keywords": "distributionally robust optimization;otimization distributionally robust;otimization distributionally;distributionally robust;robust optimization;neural generative models;neural generative;superbrust otimization distributionally;robust optimization dros;generative models;distributions uncertainty set;learning models able;distribulonally superbrust otimization;generative;data distributions uncertainty;distributions uncertainty;use neural generative;distributionally;worst case distribution;generative models characterize;distribution allowing flexible;uncertainty set methodsin;training machine learning;otimization;data distributions;learning models;robust;superbrust otimization;optimization;distribution allowing", "pdf_keywords": ""}, "43d5c00938bd2acb1aca8e81a7d220025eddbc23": {"ta_keywords": "patients diagnosis malignant;diagnosis malignant disease;diagnosis malignant;malignant disease;diagnosis treatment patients;physician diagnosis treatment;physician diagnosis;patients diagnosis;treatment patients diagnosis;malignant;diagnosis treatment;role physician diagnosis;patients;diagnosis;treatment patients;literature role physician;physician;role physician;disease;treatment;article present literature;literature role;article present;literature;present literature role;present literature;article;purpose article present;purpose article;role", "pdf_keywords": ""}, "6695d3b92e7cd7f2359f698a09c7b3dc37996329": {"ta_keywords": "label augmented pretraining;representation learning pretraining;modal representation learning;learning pretraining;visual language tasks;augmented pretraining;augmented pretraining model;pretraining model namedlamp;vision language pairs;vision language;backgroundmulti modal representation;learning pretraining increasing;quality vision language;visual language;pretraining model;representation learning;pretraining;various visual language;modal representation;visual;label augmented;pretraining increasing easy;language tasks;novel label augmented;backgroundmulti modal;language pairs highly;learning;language pairs;pretraining increasing;model namedlamp", "pdf_keywords": "visual linguistic pretraining;visual language pretraining;multimodal pretrained model;multimodal pretrained;visual language learning;representation learning pretraining;model multimodal representation;visual language tasks;multimodal learning;multimodal representation;performance multimodal learning;multimodal learning model;multimodal learning use;multimodal representation natural;vision natural language;modal representation learning;label augmented pretraining;labels visual language;linguistic pretraining model;linguistic pretraining;new visual linguistic;bert multimodal pretrained;images using pretrained;model multimodal;visual language;visual linguistic;novel multimodal learning;multimodal model use;language pretraining;learning pretraining"}, "887d84c1310c6e71a0f89874ef9985b65a44c855": {"ta_keywords": "acoustic feature space;train feature transforms;feature transforms;speech recognition;introductiondisperative feature transforms;generalm feature transform;feature transform;feature compensation techniques;speech recognition community;generalm feature compensation;feature transforms using;attracted speech recognition;methodstypically acoustic feature;feature compensation;acoustic feature;feature compensation performed;feature transform assigned;transforms using discriminative;gassian generalm feature;feature space modeled;techniques train feature;generalm feature;recently feature compensation;discriminative;train feature;recognition;discriminative criterion;feature space;model generalm feature;methodstypically acoustic", "pdf_keywords": ""}, "46bf4bece58764d22764acfd3d232b50fb7767f9": {"ta_keywords": "deep learning neuroimaging;networks classifying alzheimer;neuroimaging resultswe deep;learning neuroimaging resultswe;learning neuroimaging;classifying alzheimer disease;classifying alzheimer;learning techniques mri;3d deep convolutional;structural mri data;3d deep;deep convolutional neural;deep neural;deep learning;mri data;neuroimaging resultswe;mri data methodswe;mri scans;convolutional neural networks;convolutional neural;improve diagnosis neurological;techniques mri scans;alzheimer disease;deep neural networks;structural mri;deep convolutional;alzheimer disease based;neuroimaging;based structural mri;techniques mri", "pdf_keywords": ""}, "6f173939f6defe3ebae8fb12f19349ba96b7b5c4": {"ta_keywords": "training speaker counting;unsupervised clustering;introduce unsupervised clustering;speaker counting;training speaker;observed training speaker;speaker counting relies;unsupervised clustering process;diarization achieving;tuned conventional clustering;diarization achieving comparable;clustering;diarization;end diarization achieving;supervised;supervised learning;end diarization;conventional clustering;clustering based;conventional clustering based;clustering based methods;supervised learning methods;introduce unsupervised;end end diarization;number speakers;clustering process;unsupervised;introduction attractor based;counting relies supervised;attractor based", "pdf_keywords": "speakers training dataset;speech diarization;speakers training data;diarization speech flexible;speech diarization poorly;efficiently classified speaker;speaker efficiently classified;diarization speech;speech diarization field;prediction speech activities;prediction speech;attractors speaker efficiently;method diarization speech;diarization field speech;speech segments speakers;results based speaker;method prediction speech;etiology speech diarization;clustering etiology speech;speakers training;sequence acoustic features;dataset number speakers;speakers observed training;speaker efficiently;diarization achieving;speech recognition;recognition speech;speech recognition decade;local attractors speaker;end diarization achieving"}, "1fb88c130bedcd2e75fd205b70af2999c6a8c49d": {"ta_keywords": "neural network targeted;targeting neural network;targeted neural network;neural network targeting;network targeted neural;network targeting neural;targeting neural;targeted neural;neural network important;neural network;neural networks;network targeted;neural networks neural;network targeting;networks neural;networks neural network;neural;neural network recently;targeting;tool neural networks;shown neural network;important tool neural;tool neural;targeted;shown neural;networks;network;recently shown neural;network important;network important tool", "pdf_keywords": ""}, "6ad56b1b776a2c448fc90c543b50756941e5a119": {"ta_keywords": "incentives estimating consumer;backgroundenergy efficiency incentive;efficiency incentive design;efficiency incentive;resultselectrical efficiency incentive;incentive design utility;estimating consumer utility;designing incentives estimating;incentives estimating;utility learning;incentive design;consumer utility;consumer utility function;designing incentives;consumers revenue decoupling;utility learning methodsutilies;incentive;algorithm designing incentives;energy consumption patterns;patterns consumers revenue;design utility learning;incentives;consumers revenue;motivations modifying energy;modifying energy consumption;utility learning conclusionsutilies;energy consumption;utility company consumer;utility function resultselectrical;revenue decoupling demand", "pdf_keywords": ""}, "bb6317bbd2c4a81e94cf3d7eb1b73da246a022db": {"ta_keywords": "introductiongeneralization memorization nearest;neighbor language models;memorization nearest neighbor;introductiongeneralization memorization;neural language model;nearest neighbor language;language models introduce;language models;memorization nearest;trained neural language;trained lim embedding;neural language;language model linearly;language model;memorization;pre trained neural;nearest neighbors models;neighbor language;model nearest neighbors;embedding;lim embedding;neighbors models;introduce nearest neighbors;trained neural;models introduce nearest;nn model nearest;interpolating nearest neighbors;embedding space;neighbors models extend;lim embedding space", "pdf_keywords": "text learning knnlm;learning knnlm useful;web text learning;datasets language models;neural language models;learning knnlm;online language model;large datasets language;language models directly;text learning;language models;model large corpus;trained language model;memorizing nearest neighbor;language models using;neural language model;knnlm useful data;corpus user trained;knnlm useful;large corpus user;large corpus;knnlm useful tool;knn retrieval;scaling language models;language models important;language models applications;search knn useful;language models model;memory online language;model web text"}, "b31eb3428320342dfde042693ff2ca106dabed0d": {"ta_keywords": "abstractive summarization silics;summarization silics;abstractive summarization;sequence sequence learning;sequence learning framework;framework abstractive summarization;sequence learning;summarization;formulating text generation;text generation;summarization silics bridge;text generation reference;contrastive learning;assisted contrastive learning;contrastive learning methods;reference free evaluation;learning objective evaluation;generation reference free;formulating text;framework formulating text;learning objective;estimation assisted contrastive;assisted contrastive;generation reference;text;learning framework formulating;evaluation problem quality;sequence;free evaluation;sequence sequence", "pdf_keywords": "evaluation abstractive summarization;quality generated summaries;abstractive summarization systems;generating candidate summaries;abstractive summarization silcs;generated summaries;contrastive summarization framework;generate candidate summaries;model abstractive summarization;summarization achieves goal;contrastive summarization;summarization systems;able improve summaries;summarization framework aims;summarization framework;generated summaries summary;abstractive summarization seq2seq;abstractive summarization achieves;sequence sequence learning;sequence learning framework;summarization silcs;summarization achieves;summarization systems use;present contrastive summarization;abstractive summarization;quality extracted summaries;improve summaries;sequence learning;approach abstractive summarization;summarization seq2seq model"}, "5b1516c87818084dc5d195cc274e1ee8923210d2": {"ta_keywords": "clinical messagefor languages;bilingual word embeddings;translations based bilingual;messagefor languages annotated;word embeddings improve;translations based;transfer natural language;entity recognition ner;finds translations based;improve mapping lexical;word embeddings;bilingual word;based bilingual word;translations;finds translations;mapping lexical items;languages annotated;named entity recognition;mapping lexical;messagefor languages;word order languages;language processing models;entity recognition;languages annotated resources;lexical items languages;language processing;clinical messagefor;bilingual;based bilingual;key clinical messagefor", "pdf_keywords": "bilingual word embeddings;embeddings using bilingual;multilingual language models;lingual approach tagging;mapping translations words;words shared embedding;multilingual named entity;translations based bilingual;translations words shared;generating translations bilingual;models multilingual;models multilingual named;translation words languages;word embeddings english;train word embeddings;word embeddings improve;lexical mapping translations;translations bilingual word;embeddings english language;structural models multilingual;dictionary based translations;entity recognition ner;embeddings different languages;lingual knowledge transfer;word embeddings method;word embeddings;word embeddings using;translations bilingual;word embeddings challenging;dictionary based translation"}, "1ce0664989e0b28ceea223cab68f885ed18c39c4": {"ta_keywords": "speech modeling propose;speech modeling;sampling approach speech;speech dynamics;approach speech modeling;dynamics speech;speech dynamics dynamics;dynamics dynamics speech;gibbs sampling based;dynamics speech observed;propose gibbs sampling;gibbs sampling;modeling propose gibbs;scale mixture model;segmental utterance wise;segmental utterance;linguistic segmental utterance;property speech dynamics;acoustical linguistic segmental;mixture model;speech observed instance;scale property speech;speech observed;acoustical linguistic;multi scale mixture;time acoustical linguistic;utterance wise temporal;short time acoustical;mixture model m3;approach speech", "pdf_keywords": ""}, "2154bdb9ce841eb98b9fd13bf7bf0a42f11f89a6": {"ta_keywords": "distributed training;federated learning cloud;known distributed training;distributed training utilize;networks large datasets;federated learning;applications federated learning;learning cloud;training deep;networks large;training deep neural;neural networks large;dedicated clusters;large datasets accelerated;available dedicated clusters;deep neural networks;networks;high speed networking;networking available dedicated;speed networking;speed networking available;multiple compute nodes;deep neural;distributed;networking;large datasets;cloud;datasets accelerated;introduction training deep;known distributed", "pdf_keywords": "distributed training;distributed training efficient;efficient distributed training;decentralized training unreliable;distributed training alabile;protocol distributed optimization;distributed training compressed;method distributed training;methods distributed training;decentralized training heterogeneous;network training;distributed training ability;distributed optimization networks;training heterogeneous unreliable;distributed learning;sg distributed training;distributed learning results;distributed optimization strong;distributed optimization;network training instead;decentralized deep learning;training compressed communication;efefficient decentralized training;decentralized training;approach distributed optimization;method distributed optimization;unreliablefederated learning;optimization distributed;distributed machine learning;operate unreliablefederated learning"}, "d9212b207e49a3aa6806fb2ddadb303b7b1d47a8": {"ta_keywords": "command situated agents;task hierarchically;situated agents;situated agents works;command situated;tasks literature natural;nl command situated;particular task hierarchically;task hierarchically splitting;level tasks;natural language nl;actions hierarchies procedures;natural language;higher level tasks;sub tasks literature;language nl command;simple actions hierarchies;tasks;tasks literature;sub tasks;language nl;agents works;hierarchically;actions hierarchies;perform particular task;simple actions;agents;sequences simple actions;literature natural language;actions", "pdf_keywords": "programming language hierarchical;task hierarchically;natural language commands;navigate procedural actions;agent natural language;language hierarchical procedures;command situated agents;actions procedural;particular task hierarchically;language hierarchical;actions procedural actions;control situated agent;natural language command;procedural actions;language command situated;hierarchical procedural knowledge;described natural language;automatic control tasks;controlling situated agents;actionsthe process procedural;corresponding procedural actions;implementation hierarchical procedural;programs natural language;task hierarchically splitting;mapping natural language;complex natural language;level tasks;situated agents complex;actions agent development;language instructions actions"}, "8fcd012e8ed2ea8190163369c9f222178e70a19d": {"ta_keywords": "speech recognition ar;automatic speech recognition;speech recognition;automatic speech;introductioncontramental automatic speech;model hmm deep;hmm deep neural;model hmm;acoustic lexicon language;recognition ar;hidden markov model;recognition ar based;markov model hmm;based hidden markov;acoustic lexicon;phonetic context dependency;hidden markov;dictionary tokenization phonetic;linguistic resources pronunciation;neural network dn;pronunciation dictionary tokenization;phonetic;phonetic context;modules acoustic lexicon;pronunciation dictionary;tokenization phonetic;deep neural network;resources pronunciation dictionary;tokenization phonetic context;pronunciation", "pdf_keywords": ""}, "49418122bba375fa02907d38b0be80689f750b39": {"ta_keywords": "redundant computation decoding;computations using coding;redundant computations;redundant computation;redundant computations using;introduces redundant computation;computation decoding;codes emerging technique;computation decoding function;coding theoretic;codes emerging;coding;adding redundant computations;using coding theoretic;coding theoretic tools;encoding function proactively;called codes emerging;using coding;decoding;unavailabilities code;decoding function;encoding function;called codes;effects unavailabilities code;distributed compute;code;code consists encoding;unavailabilities code consists;encoding;redundant", "pdf_keywords": "learning codes learning;codes learning;computations learning codes;learning codes;coded computations learning;learning codes coded;codes learning codes;predictions learning code;learning codes developed;computations learned codes;decoding functions trained;learns codes;learned codes;learning encoding;neural networks encoding;decoding functions neural;approach learned codes;learned codes accurately;approach learns codes;learned code;learned code achieves;learning code;neural networks learn;architectures learning encoding;learning code effective;networks learn encoding;codes accurately reconstruct;learning code significantly;models learned code;networks encoding decoding"}, "49f657d704a1b80ce3dba0d8a9e5479ec1d703d4": {"ta_keywords": "transformers structure speech;deep transformers;speech recognition training;recently deep transformers;short term memory;non autoregressive transformers;autoregressive transformers;speech recognition;memory networks;autoregressive transformers structure;deep transformers start;term memory networks;memory networks large;structure speech recognition;recognition training input;recognition training;transformers;training input tokens;term memory;transformers structure;decoder;memory;training input;decoder randomly;novel non autoregressive;structure speech;transformers start;speech;non autoregressive;autoregressive", "pdf_keywords": "transformerbased automatic speech;automatic speech;structure automatic speech;machine translation automatic;speech manually;recognition language modeling;translation input speech;automatic speech recognition;machine translation input;translation automatic automatic;translation automatic;speech recognition language;speech manually manually;speech recognition;speech recognition production;machine translation decoding;speech recognition ar;neural machine translation;machine translation;utterance masked tokens;translation decoding;input speech manually;language modeling present;language modeling;shortterm memory networks;tokens utterance;recognition language;translation decoding natural;speech recognition important;masked tokens utterance"}, "a1321f4527559836509c27008329afaf11f8ea89": {"ta_keywords": "learning transfer order;transfer order problems;learning transfer;transfer order;problems presented students;solving problems interleaved;problems interleaved order;order problems presented;student switched problem;learning effectiveness previous;learning effectiveness;affects learning effectiveness;implications learning transfer;affects learning;interleaved order;backgroundproblem order implications;solving problems;order problems type;does solving problems;student switched;order problems;interleaved order results;presented students important;order implications learning;learning;completed student switched;students important;students important variable;variable affects learning;performance does solving", "pdf_keywords": ""}, "da46a0b5ddf0f4bf4caad9d29d6b4a93dd2eb2d2": {"ta_keywords": "simulations present bandit;bandit style problem;bounding regret insimulated;regret insimulated games;games methods preliminary;modeling games methods;games methods;bandit style;present bandit;bounding regret;present bandit style;initial heuristic algorithms;modeling games;bandit;heuristic algorithms;heuristic algorithms compare;based modeling games;initial heuristic;background bounding regret;heuristic;games important problem;insimulated games;provide initial heuristic;specific problem agent;regret insimulated;games;problem agent based;agent based modeling;insimulated games important;problem agent", "pdf_keywords": ""}, "8ff54aa8045b1e30c348cf2ca42259c946cd7a9e": {"ta_keywords": "sequenceential question answering;task answering sequences;question answering;purposesearch based neural;answering sequences;question answering focused;explore conversational qa;structured learning sequenceential;normal conversation humans;learning sequenceential;learning sequenceential question;realistic task answering;conversational qa;question sequences inquire;task answering;answering sequences simple;question sequences;neural structured;conversation humans methodsin;normal conversation;explore conversational;neural structured learning;conversational qa setting;conversation humans;based neural structured;answering focused long;answering;sequenceential;asked normal conversation;conversation", "pdf_keywords": ""}, "aaaff6b99684cb5b5e0a68e214bd8bbd4bf2e231": {"ta_keywords": "named entity recognition;entity recognition systems;biomedical named entity;entity recognition;combining biomedical named;evaluation biomedical named;biomedical named;fields syntactic parsing;random fields syntactic;data manually annotated;named entity;based hidden markov;parsing;manually annotated;syntactic parsing experiments;markov model based;conditional random fields;hidden markov model;syntactic parsing;evaluation biomedical;hidden markov;combining biomedical;parsing experiments;fields syntactic;introductionevaluating combining biomedical;evaluate combining biomedical;biomedical;parsing experiments used;markov model;concerned evaluation biomedical", "pdf_keywords": ""}, "956e096b1e8422c91989938b9508272b956d3070": {"ta_keywords": "directed graphs entities;walks directed graphs;random graph walks;graph walks directed;entity similarity methodsin;measure entity similarity;entity similarity;graph rerank;probabilities graph rerank;graph walks;lazy random graph;improve graph walk;graphs entities;graphs entities represented;graph walk performance;graph walk;directed graphs;entities represented nodes;similarity methodsin;walks directed;similarity methodsin paper;random graph;rerank;walk performance gradient;lazy random;nodes typed edges;nodes;supervised learning framework;similarity;represented nodes", "pdf_keywords": ""}, "a4a8e91995ae8c8b203dd857bdc0915facddeebe": {"ta_keywords": "estimating worker quality;estimate worker quality;worker quality algorithm;labels worker quality;modeling labels worker;algorithm estimate worker;worker quality noisy;noisy crowd sourced;estimating worker;jointly modeling labels;crowd sourced data;worker quality disagreement;rounds estimating worker;estimate worker;labels worker;quality noisy crowd;worker quality;quality algorithm estimate;algorithm jointly modeling;data alternating minimization;crowd sourced;jointly modeling;quality algorithm;worker quality establish;modeling labels;current estimate worker;noisy crowd;error bound models;models learned algorithm;quality establish generalization", "pdf_keywords": "crowdsourcing algorithm estimating;crowdsourcing improve data;recent years crowdsourcing;crowdsourcing improve;humans scalable crowdsourcing;crowdsourcing;crowdsourcing algorithm;crowdsourcing important tool;crowdsourcing scenario thousands;quality use crowdsourcing;scalable crowdsourcing;crowdsourcing scenario;approach crowdsourcing;use crowdsourcing improve;crowdsourcing important;new approach crowdsourcing;use crowdsourcing;crowdsourcing emerged;crowd sourced data;scalable crowdsourcing scenario;introduce crowdsourcing algorithm;estimate true labeling;introduce crowdsourcing;years crowdsourcing;crowdsourcing use;labels humans scalable;use crowdsourcing important;years crowdsourcing emerged;learning error labeling;learning noisy labels"}, "ca3535dcdda9849350ad7c991a60660b22844f2f": {"ta_keywords": "sequence tasks increasingly;sequence tasks like;compositionality learn searchable;approaches sequence tasks;sequence tasks;tasks like speech;speech translation systems;trained sub tasks;exploits compositionality learn;complex sequence tasks;compositionality learn;like speech translation;translation systems cascade;suggesting compositionality cascaded;searchable hidden representations;translation systems;compositionality cascaded systems;learn searchable hidden;representations intermediate stages;compositionality cascaded;hidden representations intermediate;cascade models trained;framework exploits compositionality;learn searchable;models trained;suggesting compositionality;like speech;models trained sub;sub tasks;superior suggesting compositionality", "pdf_keywords": "decomposing sequence tasks;decoder model speech;sequence tasks searchable;model speech translation;predicting speech translations;sub net decoder;pretraining speech translation;sequence tasks seek;decoder neural;model speech attention;decoder model translation;decomposable sequence tasks;neural machine translation;sequence tasks;decoder neural machine;models speech attentionwe;decoder models;predicting speech;sub net encoder;effective predicting speech;sequence tasks present;models speech;model speech;compositionality learn searchable;transcript sequences;transcript sequences ar;decoder models effective;net decoder;decompositions sequence tasks;multi decoder neural"}, "d56244c6abf3141900386d6911dd9097697a346b": {"ta_keywords": "classifier anchor extraction;web page classifier;page classifier anchor;page classifier;text categorization;text categorization systems;improve page classifier;classifier anchor;categories text categorization;methodsmost text categorization;anchor extraction link;extraction link analysis;categorization systems used;anchor extraction;categorization systems;link analysis;link analysis methodsmost;categorization systems use;link analysis resultsamong;extraction link;categorization;classifier;categories text;frequent categories text;simple web page;web page;improves simple web;anchor;improve page;simple web", "pdf_keywords": ""}, "42605c1ee030721cb38a3c225992d63297a6ace0": {"ta_keywords": "language documentation revitalization;technology language documentation;language documentation;documentation revitalization;documentation revitalization discussion;workshop language technology;documentation revitalization summary;language technology language;workshop language;language technology;summary workshop language;technology language;discussion workshop language;documentation;language;revitalization discussion workshop;revitalization summary workshop;revitalization summary;workshop;discussion workshop;summary workshop;revitalization discussion;introductiona summary workshop;revitalization;technology;introductiona summary;introductiona;discussion;summary", "pdf_keywords": "speech technology feasible;speech databases workshop;existing text speech;universal speech recognition;practical speech technology;speech databases;speech technology;developed universal speech;language revitalization technologies;language technologies;practical language revitalization;text speech databases;language documentation conservation;text speech;documentary linguists;research speech synthesis;speech recognition;new technologies language;documentary linguists technologists;feasible research speech;processing language technology;language technology;novel practical language;language technology application;technologies language processing;workshop concentrated speech;universal speech;speech synthesis;corpus developed;development language technologies"}, "c8d0e13de2eaa09a928eff36b99d63f494c2f5ec": {"ta_keywords": "parsing natural language;syntax target programming;semantic parsing;architecture powered grammar;semantic parsing paper;parsing natural;parsing;underlying syntax target;syntax prior knowledge;syntax target;parsing paper propose;consider problem parsing;natural language;capture target syntax;problem parsing natural;problem language generation;powered grammar model;natural language descriptions;language generation task;target syntax prior;powered grammar;syntax prior;target programming language;grammar model explicitly;language generation;work semantic parsing;programming language informed;problem parsing;programming language like;syntax", "pdf_keywords": "generation abstract syntax;generates abstract syntax;syntax generate;abstract syntax tree;syntax driven neural;syntax target programming;syntax driven;develop syntax driven;propose syntax driven;development semantic parsers;syntax driven approach;syntax driven algorithm;syntax tree;syntax trees;develop syntax;syntax generate sequence;underlying syntax target;grammar model syntax;poland syntax trees;semantic parsers;architecture powered grammar;natural language programming;semantic parsing complex;abstract syntax;syntax prior knowledge;syntax tree sequentially;parsing natural language;neural code generation;syntax tree ost;algorithm semantic parsing"}, "9c03d14520c897ca8536e165507f568d1980dabd": {"ta_keywords": "machine comprehension text;machine comprehension;introduction machine comprehension;lexical matching;develop lexical matching;lexical matching method;language processing machinecomprehension;comprehension text;lexical;comprehension text overarching;develop lexical;natural language processing;language processing;natural language;research natural language;machinecomprehension test;comprehension;processing machinecomprehension test;paper develop lexical;machinecomprehension;processing machinecomprehension;machinecomprehension test richardson;task consisting datasets;text overarching goal;multiple context;easy evaluate task;text;context;introduction machine;evaluate task", "pdf_keywords": ""}, "2ea226a7fadde6a45f537c714e0832e83136f861": {"ta_keywords": "biomedical event extraction;event extraction;approach biomedical event;event extraction using;biomedical event;structured prediction framework;structured prediction;based structured prediction;classification tasks;prediction framework sear;classification tasks models;search based structured;score bionl 2009;sensitive classification tasks;cost sensitive classification;bionl 2009 shared;extraction using search;approach biomedical;bionl 2009;biomedical;score bionl;inf score bionl;performance joint inference;prediction framework;sensitive classification;2009 shared task;develop approach biomedical;tasks models;bionl;tasks models learned", "pdf_keywords": ""}, "708f8c0eb5032edd6f31663a27febbb0529cbcf3": {"ta_keywords": "grounded neural syntax;neural syntax learner;neural syntax acquisition;visually grounded neural;syntax learner vg;syntax learner;syntax acquisition;learning syntactic;learning syntactic representations;neural syntax;syntax acquisition present;backgroundvisually grounded neural;grounded neural;approach learning syntactic;syntactic representations;syntactic representations structures;learner vg nl;captions vg nl;nl approach learning;parse trees texts;constituency parse trees;texts recursively composes;visually grounded;syntactic;reading paired captions;present visually grounded;generates constituency parse;parse trees;trees texts recursively;paired captions vg", "pdf_keywords": "grounded neural syntax;neural syntax learner;syntax learner visually;visual grounded phrases;visually grounded neural;visually grounded language;syntax learner;neural syntax;parse trees captions;learning syntactic;learning syntactic representations;syntax learner vg;syntax natural language;tree visual semantic;syntax learner vnl;visual semantic embedding;learner visually grounded;representations visual semantic;syntax structures humandefined;parse trees visual;grounded language structure;parse tree representations;novel visual semantic;syntactic representations;learning semantics supplementary;neural language models;syntactic representations structures;visual semantic;induces syntax structures;sentences development neural"}, "06e36261b21af2943e464a562c92c09dac292a82": {"ta_keywords": "professionals reverseengineering binaries;reverseengineering binaries;reverse compilation;reverseengineering binaries wild;security professionals reverseengineering;reverseengineering;professionals reverseengineering;attempts reverse compilation;binaries wild decompiler;language asc decompiler;compilation transforming binary;asc decompiler;decompiler decompiler;reverse compilation transforming;decompiler;decompiler decompiler attempts;decompiler attempts reverse;asc decompiler attempts;binaries;wild decompiler decompiler;wild decompiler;decompiler attempts;tool used security;binary;binary higher level;compilation transforming;programs providing useful;level language asc;ease reasoning programs;compilation", "pdf_keywords": "compilation decompilers;useful tool decompilation;reverse engineering binary;code efficient decompiled;efficient decompiled code;compilation decompilers able;reverseengineering binaries;decompiled variables decompiler;reverse engineering decompilers;decompiled code model;decompiled code reconstructed;reverse compilation;types decompiled variables;implement decompiler;data implement decompiler;written types decompiled;decompiled code ability;variable decompiler;types decompiled code;automated reverse engineering;improvements decompiler;decompiled variables;decompiled code efficient;tool reverseengineering binaries;future improvements decompiler;types names decompiled;types decoding decompiled;semantic types decompiled;implement decompiler variable;code reconstructed abstract"}, "54e7de06a97b4b6c41e185c0bee60c838a15265a": {"ta_keywords": "backgroundarticulatory controllable speech;controllable speech modification;controllable speech;speech modification;speech modification using;backgroundarticulatory controllable;sequenceential inversion production;gassian mixture models;backgroundarticulatory;mapping gassian mixture;sequenceential inversion;modification using sequenceential;using sequenceential inversion;gassian mixture;mixture models;inversion production;inversion production mapping;mixture models 16;mapping gassian;controllable;speech;sequenceential;production mapping gassian;using sequenceential;mixture;inversion;modification using;gassian;modification;production mapping", "pdf_keywords": ""}, "7771aa7badc3375a31bfac8dc47755ff5d5c7780": {"ta_keywords": "orthographic word types;cross linguistic comparison;cross linguistic diversity;linguistic comparison;cross linguistic;word types different;linguistic comparison following;subword tokenization;perform cross linguistic;different languages typological;linguistic diversity;languages typological characteristics;word types;distributions orthographic word;range cross linguistic;nonlinguistic;languages typological;nonlinguistic study language;nonlinguistic study;linguistic;orthographic word;levels subword tokenization;subword tokenization resultswe;types different languages;challenge nonlinguistic study;different languages;linguistic diversity major;major challenge nonlinguistic;characteristics different writing;challenge nonlinguistic", "pdf_keywords": "morphological complexity language;morphological word order;multilingual settings morphological;approach morphological complexity;subword representations multilingual;new approach morphological;useful morphological;morphological complexity;morphological segmentation;algorithm useful morphological;morphological segmentation bp;linguistic orthographic language;useful morphological word;approach morphological segmentation;subword tokenization;approach morphological;morphological segmentation based;use morphological segmentation;morphological;language processing;orthographic word types;non linguistic orthographic;use morphological;morphological word;corpus 47 languages;parallel corpus;subword distributions;linguistic orthographic;theoretic approach morphological;texts parallel corpus"}, "79c93274429d6355959f1e4374c2147bb81ea649": {"ta_keywords": "cross modality encoderrepresentations;modality encoderrepresentations transformers;vision language reasoning;modality encoderrepresentations;encoderrepresentations transformers;learning cross modality;encoderrepresentations transformers methods;vision language;encoderrepresentations;encoders object relationship;relationship encoder language;object relationship encoder;encoder language;propose lxmert learning;visual concepts language;lxmert learning;introduction vision language;relationship encoder;understanding visual concepts;encoders;encoders object;lxmert learning cross;encoder;model consists encoders;visual concepts;language reasoning;language reasoning requires;requires understanding visual;language semantics;understanding visual", "pdf_keywords": "prediction visual reasoning;visual question answering;cross attention layers;learn object prediction;cross modality encoderrepresentations;encoder attention;learning cross modality;image question answering;vision language reasoning;visual reasoning datasets;challenging visual reasoning;question answering image;crossmodality language modeling;vision language connections;object prediction visual;cross modality encoder;crossmodality encoder based;model visual reasoning;vision language task;present crossmodality encoder;novel crossmodality encoder;learn language vision;crossmodality encoder;attention layers;encoder cross modality;crossmodality encoder model;answering generalized multimodal;modality encoderrepresentations transformers;learning cross modal;encoders novel crossmodality"}, "03e4f33c0ccc4cb8c7e1589158a5377cdf5241d2": {"ta_keywords": "ethical priorityities ai;priorityities ai systems;priorityities ai;reasoning preferences ethical;modeling reasoning preferences;ethical priorityities;preferences ethical priorityities;model reasoning preferences;reasoning preferences;ai systems aimto;introductionmodeling reasoning preferences;ai systems methodsa;ai systems proposed;priorityities;ai systems resultsa;preferences ethical;ai systems;modeling reasoning;ai;aimto model reasoning;ethical;model reasoning;model modeling reasoning;systems aimto model;introductionmodeling reasoning;systems proposed conclusionsa;reasoning;proposed conclusionsa model;systems aimto;systems methodsa model", "pdf_keywords": ""}, "d5f22dbc8f4b9e99f62e6ecf886bc4b9a0372e4d": {"ta_keywords": "hierarchical classification entities;entities incomplete ontologies;classifying entities incomplete;entities incomplete ontology;incomplete ontologies;complete ontology classes;classification entities incomplete;incomplete ontology;ontology classes;classification entities;introduction classifying entities;complete ontology;hierarchical classification;classifying entities;hierarchical classification costly;ontologies;techniques hierarchical classification;challenges hierarchical classification;create complete ontology;ontology classes represent;incomplete ontology exponential;ontology;represent entities web;entities web;datasets class hierarchies;entities web need;class hierarchies;ontology exponential;hierarchies;hierarchical", "pdf_keywords": ""}, "680e61a17e27a1e8e121276c7ec53fc4fd40babb": {"ta_keywords": "linguistic acceptability research;linguistic acceptability;comprehension linguistic acceptability;language users utterances;language production explored;reading time acceptability;language comprehension linguistic;users utterances;comprehension linguistic;information density;predictions language comprehension;information density hud;hypothesis using reading;acceptability research;preference language;preference language users;acceptability data;posits preference language;utterances;linguistic;users utterances structured;language comprehension;utterances structured information;acceptability data resultswhile;time acceptability data;uniform information density;utterances structured;density hud hypothesis;makes predictions language;language users", "pdf_keywords": "uniformity language level;language model estimate;language users utterances;language fundamental;interpret language fundamental;language fundamental fundamental;language level linguistic;processing effort linguistic;language super linear;mean information rate;utterances structured information;information rate better;information density;linear language wide;information rate purpose;effort linguistic acceptability;super linear language;uniformity language super;information rate linear;perceived linguistic;ability interpret language;information rate;uniform information density;uniformity language;computational language modelling;effect linguistic acceptability;languages able fully;definition uniformity language;effort linguistic;predictions language comprehension"}, "34f8214cbaa0655794c2c9570898abf15649b079": {"ta_keywords": "recognition reverberant speech;noise reverberation research;reverberation prior recognition;noise reverberation;recognition reverberant;reverberant speech;problem recognition reverberant;reverberation research undertaken;reverberation research;reduce reverberation prior;reverberant speech received;reduce reverberation;reverberation;reverberation prior;method reduce reverberation;performance automatic speech;presence noise reverberation;automatic speech recognition;speech recognition;automatic speech;speech recognition severely;reverberant;undertaken noise robustness;noise robustness contrast;prior recognition preprocessor;recognition preprocessor;noise robustness;dereverberation method;recognition preprocessor remove;use dereverberation", "pdf_keywords": ""}, "3aba582b62d1abfcd95264e6c7b32aab4c9db4b8": {"ta_keywords": "introduction giiii giiii;introduction giiii;giiiii giiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii;giiii giiiii giiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii;giiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii;giiii giiii giiii;giiii giiii giiiii;giiii;giiii giiii;giiii giiiii;giiiii;introduction", "pdf_keywords": "neural text classifiers;text classifier predictions;inherently interpretable classifiers;interpretable classifiers;interpretability layers neural;classifier decisions selfxplain;text classifiers;neural classifiers;interpretable classifiers explain;perceive explanations generated;phrase based concepts;explain text classifier;existing neural classifiers;self explaining model;text classifier;concept computing relevance;influential concepts training;local interpretability layers;classifier predictions;integrating concepts training;predicting label;attention natural language;classifiers;interpretability layers;concepts training data;neural text;method predicting label;layers neural text;self explaining framework;concept model predictions"}, "e5a5888966be6b5f9c0e8a82facd604086a1ee4c": {"ta_keywords": "bioner paper corpus;trained annotated abstracts;paper corpus;syntax semi supervised;trained annotated;corpus demonstrate performance;learning trained annotated;paper corpus demonstrate;annotated abstracts;corpus;performance paper corpus;annotated abstracts papers;abstracts papers dataset;paper corpus evaluate;adding sentences papers;corpus evaluate performance;sentences papers;bioner highly automated;corpus demonstrate;cf syntax superior;semi supervised;cf syntax effective;genes bioner;genes bioner highly;syntax superior unseen;syntax superior;adding sentences;semi supervised learning;supervised;syntax effective", "pdf_keywords": ""}, "73271677da83a3f55523148d1b43a0501f0a35dd": {"ta_keywords": "zero sum games;online learning dynamics;online learning periodic;learning dynamics cyclic;game theory classicalal;online regret dynamics;learning dynamics;learning periodic zero;theorem online regret;regret dynamics converge;online learning;game theory;sum games methods;sum games;introduction online learning;theory classicalal learning;recurrent zero sum;regret dynamics;problem game theory;sum games important;learning periodic;classes online learning;recurrent zero;games methods;classicalal learning;converge equilibrium time;poincare recurrent zero;online regret;sum games resul;classicalal learning results", "pdf_keywords": "average strategies periodic;zero sum games;games recurrent periodic;average strategy periodic;strategy periodic zero;strategies periodic zero;games periodic zero;time average strategies;finite strategy periodic;sum games recurrent;games consider periodic;bilinear games continuous;sum games convergeperiodic;zero sum game;learning dynamics periodic;periodic evolving games;sum games periodic;games convergeperiodic zero;bilinear games converge;class games periodic;games converge average;game convergent time;time average strategy;game theory time;games convergeperiodic;periodic evolving game;online learning periodic;games periodic;learning periodic zero;strategy periodic"}, "74fb2834c820d2297b08201cb72de1c1d3d27f54": {"ta_keywords": "hiding speaker identity;client privacy;client control biometric;client privacy discuss;define client privacy;control biometric information;privacy;privacy discuss;hiding speaker;biometric information sent;ensure uploaded speech;speaker identity approaches;biometric information;control biometric;speaker identity;uploaded speech data;biometric;security concerns users;reduce certain security;certain security concerns;concerns users client;security concerns;private existing solutions;uploaded speech;certain security;users client;private existing;focus hiding speaker;users client control;security", "pdf_keywords": "anonymizing audio;approach anonymizing audio;tasks differential privacy;privacy downstream tasks;ensuring privacy speech;anonymizing audio employ;differential privacy;privacy speech;differential privacy suitable;hiding speaker identity;privacy guarantees formalized;differential privacy methods;privacy speech data;inherent differential privacy;private speech features;ensuring privacy downstream;transforms algorithms anonymize;privacy guarantees;client privacy approaches;server privacy guarantees;client privacy suitable;algorithms anonymize sensitive;privacy approaches;privacy;privacy approaches able;encrypted audio speech;privacy downstream;server privacy;privacy methods;ensuring privacy"}, "030d7d7ae48a9f81700b2c1f7cf835235777b8e7": {"ta_keywords": "question answering openqa;extracting answers passages;dmain question answering;question answering;reader extracting answers;answering openqa generally;extracting answers;answering openqa;corpus reader extracting;large corpus reader;complexity natural language;natural language questions;passages large corpus;corpus reader;large corpus;answers passages;natural language;finding candidate passages;corpus;grained vector representations;passages argue modeling;reader extracting;questions passages;openqa generally;openqa generally depend;answers passages recent;questions passages argue;candidate passages large;answering;openqa", "pdf_keywords": "question answering openq;domain question answering;question answering;question answering ability;question answering development;scalable neural retrieval;question answering present;effective openqa retrieval;question training;answering ability retrieve;field question answering;openqa retrieval;openqa retrieval proposed;answering openq;friendly approach retrieval;question training set;recent colbert retrieval;retrieval model colbert;answering ability;neural retrieval methods;colbert retrieval;approach retrieval;questions use retrieve;approach retrieval trivia;retrieval human language;answering development;retrieve information corpus;retriever demonstrate retrieval;colbert retrieval model;neural retrieval model"}, "7c655ef6f0de8c1a219cdb796c77f4ae3c389b82": {"ta_keywords": "intelligence aies 18;ai ethics society;ethics society aies;ai ethics;americani conference artificial;intelligence ethics artificial;aies 18 held;intelligence aies;artificial intelligence ethics;ethics artificial intelligence;association ai ethics;artificial intelligence aies;aies 18;society aies;americani conference;1st americani conference;aies;conference provided aaai;society aies 18;ethics artificial;intelligence ethics;ai;aies 18 order;conference artificial intelligence;conference artificial;just association ai;association ai;1st americani;ethics society;americani", "pdf_keywords": ""}, "b2b0fbf9033f1c36bea8bb11c173f14378c60db9": {"ta_keywords": "speech s2s translation;emphasis expressed language;speech speech s2s;s2s translation systems;s2s considering emphasis;speech s2s;languages conventionals2s;languages conventionals2s systems;different languages conventionals2s;emphasis word;s2s translation;systems ignore emphasis;translation systems;analyzed emphasis word;translation systems allow;emphasis;emphasis expressed;paralinguistic information;considering emphasis;ignore emphasis;analyzed emphasis;paralinguistic;considering emphasis important;speech speech;paralinguistic information order;emphasis important;factor paralinguistic information;expressed language;speech;expressed language languages", "pdf_keywords": ""}, "262c0e54370dfc03a7ad53d79930568d18dd448c": {"ta_keywords": "distributed machine learning;machine learning distributed;codes machine learning;learning distributed;learning distributed systems;machine learning algorithms;codes engineering;learning algorithms little;codes machine;performance distributed;machine learning;performance distributed machine;distributed machine;distributed systems;italic robustness;codes engineering applications;robustness italic;learning algorithms;use codes engineering;distributed systems paper;offer italic robustness;robustness italic italic;distributed;italic robustness italic;algorithms little;robustness;large scale systems;noise italic;affect performance distributed;cutting codes machine", "pdf_keywords": "coded distributed algorithms;distributed algorithms significantly;machine learning distributed;distributed algorithms;uncoded distributed algorithm;distributed machine learning;existing distributed algorithms;identically distributed computing;distributed computing time;distributed computing;distributed algorithms obtained;faster uncoded matrix;coded distributed algorithm;distributed learning algorithms;distributed algorithms introducing;distributed algorithms special;learning distributed;runtime distributed algorithms;distributed algorithm optimal;distributed algorithm suffers;uncoded distributed;learning algorithms straggler;distributed optimal coded;distributed algorithm;uncoded matrix multiplication;distributed learning;distributedly using coded;uncoded coded distributed;distributed algorithm achieve;coded matrix multiplication"}, "14a058a1e41459a30327bb5fb480d51430b6a096": {"ta_keywords": "framework geneid ranking;search framework geneid;geneid ranking;geneid finding studied;geneid ranking extended;geneid finding task;geneid finding;curation process geneid;framework geneid;database identifier gene;article geneid finding;geneid;process geneid finding;graph search;discussed article geneid;graph search framework;process geneid;article geneid;identifier gene;biocreative challenge hirschman;identifier gene discussed;introductiona graph search;biocreative challenge;experimentally biocreative challenge;finding database identifier;gene;gene discussed article;search;search framework;gene discussed", "pdf_keywords": ""}, "a6d505a6e46c15ef0d213b9a4349ce2f852be894": {"ta_keywords": "mixedture proportion estimationion;proportion estimationion pu;mixture proportion estimation;estimationion pu learning;positiveversus negative classifier;negative classifier;negative classifier formally;unlabeled examples positive;mixedture proportion;positive examples unlabeled;proportion estimation mpe;classifier;proportion estimationion;positive negative classes;estimate accurate positiveversus;proportion estimation;mixture proportion;pu learning;negative classes;mixedture;accurate positiveversus negative;estimationion pu;examples positive negative;unlabeled examples;accurate positiveversus;subtasks mixture proportion;examples positive;examples unlabeled data;examples unlabeled;examples unlabeled examples", "pdf_keywords": "approach classification mixture;estimating mixture coefficient;classification mixture;estimating mixture;method estimating mixture;best bin estimationion;objective pu learning;best bin estimation;discrimination learning combine;pu learning methods;leverages blackbox classifier;learning combine;mixture proportion estimation;estimation classifier;risk cvr objective;bin estimationion;classification mixture proportion;bin estimation;blackbox classifier;learning positive unlabeled;estimation classifier able;classifier learn;proportion estimation classifier;estimationion bbe conditional;estimation bbe conditional;cvr objective multidimensional;bin property classifier;pu training data;classifier make identifiable;gnoring risk cvr"}, "b08545e1281c1eb748e4474687eb61fd3b25d1a6": {"ta_keywords": "novelty lexical;class novelty lexical;york word innovation;word innovation types;contextual prediction novelty;novelty lexical derivation;annotated class novelty;word innovation;class novelty;york word;prediction novelty;innovation types dataset;novelty;novelty class;prediction novelty class;york york word;dialectal variation;variation dialectal;types dataset yorktwit;novel english words;dialectal variation dialectal;lexical;lexical derivation dialectal;variation dialectal variation;innovation types;dialectal variation blending;yorktwit collection;yorktwit;dialectal;words published new", "pdf_keywords": "novelty lexical;class novelty lexical;linguistic novelty;contexts linguistic novelty;linguistic novelty class;word innovation typess;existing english words;novelty lexical derivation;word innovation;word form neologisms;lexical;times word innovation;introductionneologisms common;introductionneologisms;word formation natural;words encountered context;dominant terminology technical;english words appeared;words encountered;past categories words;existing english;modern modern english;words appeared yorkt;categories words composed;form neologisms;modern english;annotation class novelty;contextual prediction novelty;novel words encountered;contexts linguistic"}, "43fae0a7af211d91557d115d2f82e3c46d8bf022": {"ta_keywords": "language generation tasks;natural language generation;generation automatic alignment;language generation methodsin;characterizing generation automatic;language generation;generation tasks;generation tasks based;automatic alignment prediction;generation automatic;change natural language;generation methodsin;alignment prediction models;models automatic alignment;characterizing generation;alignment prediction;evaluating natural language;automatic alignment;generation methodsin paper;natural language;_information alignment_;information change natural;introductioncompression transduction creation;generation;role characterizing generation;creation unified;_information alignment_ input;information change;tasks _information alignment_;output text", "pdf_keywords": "language generation evaluation;language generation tasks;quality nlg metrics;natural language generation;generation evaluation metrics;productive quality nlg;generation tasks quality;evaluation natural language;generation tasks aspects;generation tasks;nlg metrics;assessing accuracy summarization;generation evaluation;automatic evaluation natural;evaluation text summaries;quality nlg;language generation;evaluating accuracy summarization;language generation particularly;diverse tasks summarization;makes automatic evaluation;quality dialog tasks;conversation propose metrics;automatic evaluation;natural language development;summarization text evaluating;summaries significantly improved;language generation key;tasks propose metrics;information alignment prediction"}, "b0894f5c914cd90cc3b3e16b15bec11efe317b14": {"ta_keywords": "peer assessment;peer assessment tasks;promotions peer assessment;behaviour peer assessment;peer assessment task;peer assessment case;various peer assessment;peer grading;strategic behaviour peer;peer grading exams;including peer grading;assessment task competitive;behaviour peer;promotions peer;competitive students graded;detecting strategic behaviour;exams homeworks peer;behaviour various peer;hiring promotions peer;peer review;tasks including peer;homeworks peer review;assessment tasks;assessment task;assessment;homeworks peer;grading;peer review hiring;task competitive students;grading exams", "pdf_keywords": "manipulations peer grading;effectiveness peer grading;peer assessment;peer assessment tasks;various peer assessment;manipulations strategic reviewer;peer assessment task;peer grading;peer grading impact;effectiveness peer assesssment;peer grading fundamental;behaviour peer assesssment;discussed peer assessment;reviewer strategic agents;strategic behaviour reviewers;peer grading development;peer assesssment;peer grading non;peer grading demonstrate;submissions strategic reviewers;strategic behaviour reviewer;strategic reviewers formulate;reviewer strategic;strategic reviewers;peer grading setup;strategic reviewer strategic;student peer grading;detection strategic manipulations;strategic reviewer;development peer grading"}, "f481d6dea08e348cecd5eb23a813d47373e62a94": {"ta_keywords": "programming natural language;programming languages;programming languages highly;generate programming languages;programming;generate programming;programming ability;ways way programming;way programming ability;way programming;programming ability understand;programming natural;natural language;intersection programming natural;interact computers complex;intersection programming;understand generate programming;interact computers;languages;able interact computers;computers complex ways;language;languages highly;introduction computers;languages highly specialized;computers complex;introduction computers information;focus intersection programming;humans able interact;intersection", "pdf_keywords": ""}, "c7424d651d60ef9f052e91bff18efd88782225a3": {"ta_keywords": "election breaking ties;winner election parallel;control election breaking;election parallel universes;controlling result election;election parallel;control election;ties choose winners;ties strategically methods;breaking ties strategically;election breaking;breaking chair election;hard control election;deciding winner election;result election breaking;break ties choose;winners problem trivially;ties strategically;break ties;problem deciding winner;chair election;election asked break;breaking ties;winners problem;choose winners problem;ties study computational;winner election;election;deciding winner;result election", "pdf_keywords": "complexity controlling election;complexity voting rules;complexity voting;problem complexity voting;complexity control tie;tie breaking problem;discuss complexity voting;tie breaking rule;intractable twostage voting;controlling election breaking;voting rules manipulable;tie breaking control;rules control tie;control complexity;breaking control tie;rule tie breaking;control tie breaking;voting rules manipulation;complexity control breaking;winners problem trivially;control breaking ties;rule control tie;random tie breaking;ties choose winners;manipulation problem ties;complexity controlling;complexity controlling result;tie breaking functions;complexity control;election breaking ties"}, "19a6e362840d3a2d27d0fa5509eaa4d4597a2859": {"ta_keywords": "recombinant dna sequences;preserving recombinant dna;dna sequences single;single sequence dna;recombinant dna;preserving recombinant;dna sequences;sequence dna sequences;sequence dna;single sequence single;sequences single sequence;sequence single;sequences single;method preserving recombinant;sequence single sequence;recombinant;single sequence;dna;sequences;sequence;preserving;novel method preserving;novel method;single;method preserving;present novel method;method;novel;present novel;present", "pdf_keywords": ""}, "b145a46718f293429054f0a9a4cdd2de94813b37": {"ta_keywords": "analysis hyperlink structure;messagethe analysis hyperlink;analysis hyperlink;hyperlink structure web;hyperlink structure;hyperlink;web information retrieval;clinical messagethe analysis;successful link analy;describes successful link;information retrieval;information retrieval survey;structure web;retrieval survey;retrieval survey describes;link analy sis;clinical messagethe;web information;improvements web information;ignificant improvements web;key clinical messagethe;successful link;retrieval;messagethe analysis;link analy;improvements web;analy sis algorithms;web;sis algorithms;clinical", "pdf_keywords": ""}, "5ea3c08614e9673a109f581cf114af488f3aa601": {"ta_keywords": "automatic embryo staging;embryo staging;time lapse embryoscope;embryologists based time;method automatic embryo;lapse embryoscope videos;select embryos;embryoscope videos methods;lapse embryoscope;embryoscope videos;select embryos transfer;manually experienced embryologists;automatic embryo;strategically select embryos;embryoscope;embryo staging exploits;experienced embryologists;embryos;embryologists;embryos transfer;embryologists based;embryo;experienced embryologists based;embryos transfer common;structure time lapse;fertility clinics strategically;based time lapse;fertility clinics;lapse data;outcomes fertility clinics", "pdf_keywords": "embryo weakly supervised;learning embryo;predicting embryo stages;tool predicting embryo;approach predicting embryo;learning embryo typicaly;predicting embryo;prediction optimal embryos;prediction embryos;reinforcement learning embryo;predict embryo;stages embryo;embryo stages;algorithm identify embryo;algorithm embryonic embryos;predicting embryo morphokinetics;stages embryo present;embryo stages embryo;predict probability embryo;probability embryo stages;stages embryo use;morphokinetic prediction embryos;predict embryo morphokinetics;region corresponding embryo;used predict embryo;embryology;embryo;embryos demonstrate algorithm;identify embryo;select embryos transfer"}, "6ab36d2577f7c9487b28b2bcdf236191ba901aad": {"ta_keywords": "task oriented dialogs;end learning flowchart;dialogs;dialogs tod challenging;learning flowchart grounded;dialog;end learning task;dialogs tod;end end learning;learning task oriented;flowchart grounded task;dialog mimics troubleshooting;oriented dialogs;oriented dialogs tod;learning flowchart;end learning;dialogs tod dialog;dialog mimics;tod dialog;learning task;tod dialog mimics;grounded task oriented;problem field neural;task oriented;flowchart grounded;flowchart;introduction end end;grounded task;helps user diagnosing;introduction end", "pdf_keywords": "constructing troubleshooting dialogs;task oriented dialogs;collecting dialog data;task oriented dialog;approach dialog systems;dialog systems;collecting dialog;method collecting dialog;dialog data;approach dialog generation;task oriented dialogue;dialog fluomental dataset;dialogs;dialog systems simplest;oriented dialog dataset;dialog data discuss;dialogue development task;dialog dataset;dialog collect information;dialog generation;response based dialog;implementing dialog;dialogue systems development;user friendly dialog;friendly dialog process;based dialog history;conversation task exposes;oriented dialogue development;novel approach dialog;dialog"}, "1b114486d67252ff83fc90d4a8607636045c54ce": {"ta_keywords": "like code synthesis;code summarization data;natural language code;code synthesis natural;code synthesis;language code retrieval;language nl code;synthesis natural language;code summarization;retrieval code summarization;code retrieval;data natural language;code retrieval code;nl code;language code;like code;natural language nl;tasks like code;summarization data driven;grained alignments stacked;language nl;nl code fine;fine grained alignments;natural language;grained alignments;parallel data natural;retrieval code;alignments stacked overflow;stacked overflow promising;summarization data", "pdf_keywords": ""}, "518cb6d4247bdebf21e2811f296b0c7372602a0a": {"ta_keywords": "pretraining masked language;masked language models;pretraining masked;masked language;flaw pretraining masked;context masking;masking tokens uniformly;context masking tokens;masking tokens;mms minimize training;masking allows mms;bovant uniform masking;masking;uniform masking allows;pretraining inefficiency suboptimal;minimize training;language models mms;masking allows;pmi masking;propose pmi masking;uniform masking;masking principle;pmi masking principle;pretraining inefficiency;minimize training objective;signals leading pretraining;leading pretraining inefficiency;masked;language models;pretraining", "pdf_keywords": "optimizing masking vocabulary;masked language models;token words model;subword tokens;masking vocabulary;constructions masking vocabulary;subword tokens introduce;masked language model;gram corpus;tokens large corpora;token words;masking vocabulary size;multiple subword tokens;random token masking;multi token words;predicting single tokens;words model;pretraining masked language;nlms uniform masking;language models;generating vocabularies;vocabularies multi token;bidirectional language models;masking tokens uniformly;segmentation single tokens;token masking;corpus;multi token word;language model feasible;grams corpus"}, "aa9e0bf1e22563fca053578315b857688a0817cb": {"ta_keywords": "simulator based corpus;task oriented dialogue;dialogue systems;oriented dialogue systems;dialogue corpora;conventional dialogue corpora;dialogue corpora used;dialogue systems important;user simulator based;user simulator;reinforcement learners;oriented dialogue;learning reinforcement learners;based corpus;dialogue;build user simulator;reinforcement learners typically;environment conventional dialogue;separate corpus task;simulator based;simulator;corpus task specific;task specific annotated;annotated data;conventional dialogue;learning reinforcement;reinforcement learning reinforcement;annotated data building;corpus;specific annotated data", "pdf_keywords": "simulator operate dialog;dialogue policies using;optimize dialogue policies;user simulation;user simulators;based user simulation;dialogue model user;user simulation approach;dialogue management;dialogue model;dialogue policies;user simulator;models dialogue management;simulator user simulator;interact user simulator;user simulator operate;user simulator developed;agent user simulator;user simulator ability;level user simulator;user friendly simulation;dialogue systems;simulated users;use user simulators;using simulated users;literature user simulation;user simulator important;user simulator use;oriented dialogue model;dialogue systems challenging"}, "6b4ca249b3b28d3fee65f69714440c08d42cee64": {"ta_keywords": "gait gan training;general gait gan;gait gan;continuous unregularized gan;gan training convergent;unregularized gan training;gan training methodsin;gan training;general gait;problems general gait;training convergent furthermore;training convergent;unregularized gan;stability;absolutely continuous unregularized;stability problems general;gait;continuous unregularized;distributions absolutely continuous;stability problems;gan;training methodsin paper;training methodsin;knowledge stability problems;knowledge stability;current knowledge stability;requirement absolute continuity;distributions absolutely;absolutely continuous;continuous", "pdf_keywords": "convergent training dynamics;training dynamics regularization;regularized training dynamics;gan training dynamics;training dynamics convergent;training dynamics gans;globally convergent training;gamut training dynamics;convergent training;dynamics gans converge;convergent gan optimization;regularized gamut training;convergence rate gan;gradient descent fixed;training locally convergent;model training convergent;techniques stabilizing training;gan training;convergent gan;stabilizing training algorithm;gans training training;unregularized gait training;locally convergent gan;stabilizing training;regularized training;gans training;gans converge simple;gan gan training;continuous training dynamics;gan training unregularized"}, "993c184553c41ca9134f149a3eb71b5bfab298b5": {"ta_keywords": "view modularity clustering;narrative network information;multi view modularity;modularity clustering;view modularity;modularity clustering aimto;modularity clustering vmc;distinct narrative network;narrative network;sponsored information operations;information operations multi;broader digital campaign;called multi view;holistically determine coordinated;information operations;digital campaign;coordinated;network information maneuvers;operations multi view;groups accounts engaged;engaged distinct narrative;information operations distinct;state sponsored information;digital campaign resultsapplication;clustering vmc identify;multi view;information maneuvers;clustering;information maneuvers present;clustering vmc", "pdf_keywords": ""}, "6916118de98cb5293425c8f74919395a003e6076": {"ta_keywords": "methods text categorization;text categorization;classifying text;text categorization compare;text categorization task;introduction text categorization;categorization compare propositional;classifying text pre;task classifying text;categorization;classifying;categorization task classifying;categorization compare;categorization task;propositional rule learning;rule learning ripper;task classifying;immp methods text;eeectiveness immp methods;propositional analogs methods;categories;eeectiveness immp;rule learning;learning ripper;evaluate eeectiveness immp;propositional analogs;compare propositional analogs;learning ripper rst;deened categories;pre deened categories", "pdf_keywords": ""}, "affdfafb0293b44412ec99ff39b114de5e83eb98": {"ta_keywords": "hydraulic fracturing processes;post hydraulic fracturing;reservoirs hydraulic fractification;hydraulic fracturing;hydraulic fractification;hydraulic fractification process;tight reservoirs hydraulic;reservoirs hydraulic;fracturing processes unconventional;unconventional tight reservoirs;fracturing processes;study post hydraulic;significantly efficient hydraulic;hydraulic hydraulic hy;efficient hydraulic hydraulic;efficient hydraulic;tight reservoirs;post hydraulic;fracturing;hydraulic hy;reservoirs;hydraulic hydraulic;hydraulic hydraulic hydraulic;hydraulic;fractification process significantly;fractification process;processes unconventional tight;processes unconventional;fractification;process significantly", "pdf_keywords": ""}, "88347f9f12b50590f50aefce4cf71b3a3f0bd138": {"ta_keywords": "language grounding 3d;oriented language grounding;language grounding;language instructions autonomous;grounding 3d environments;instructions autonomous agents;environment natural language;grounding 3d;natural language instruction;trainable neural;autonomous agents need;trainable neural architecture;end trainable neural;task oriented language;meaningful representations language;semantically meaningful representations;neural architecture task;natural language instructions;linguistic perceptual knowledge;neural architecture;instructions autonomous;3d environments;representations language;language map visual;linguistic perceptual;autonomous agents;architecture task oriented;language instruction input;specified natural language;representations language map", "pdf_keywords": "visual language grounding;language grounding 3d;instruction multimodal fusion;visual language;attention multimodal fusion;instruction visual image;instruction multimodal;language instruction visual;language instructions autonomous;gated attention multimodal;task oriented visual;instruction representation convolutional;attention multimodal;language grounding;oriented visual language;language grounding challenging;natural language instruction;oriented language grounding;multimodal fusion verbal;linguistic visual inputs;perceptual knowledge trained;instruction image representation;fusion verbal visual;scenes using deep;language grounding raw;instructions autonomous agents;learning linguistic visual;instruction visual;language grounding methodswe;representations instructionwe propose"}, "5e10a61b34867c6e5b32ed7a1359bd47bbfb5e2d": {"ta_keywords": "multiple explanations training;explanation based learning;multiple inconsistent explanation;explanations training;explanations training instance;inconsistent explanation problem;learning ebi imperfect;ebi imperfect theories;produces multiple explanations;theories themultiple inconsistent;inconsistent explanation;applying explanation based;imperfect theories themultiple;imperfect theories;explanation problem occurs;correct domain theories;based learning ebi;multiple inconsistent;domain theories suffer;explanations;theories suffer multiple;themultiple inconsistent explanation;multiple explanations;explanation based;learning ebi;domain theories;applying explanation;problem multiple inconsistent;explanation problem multiple;ebi imperfect", "pdf_keywords": ""}, "e3862b1ff18dbb6a421b9efd1c0db22e09644b6d": {"ta_keywords": "caused mutation adolescent;mutation adolescent adolescent;mutation adolescent;occurrence new disease;adolescent adolescent ado;disease caused occurrence;disease caused mutation;adolescent ado;problem disease caused;disease caused;health problem disease;world disease caused;new disease world;new disease;adolescent adolescent;disease;problem disease;adolescent adolescent adolescent;disease world disease;caused mutation;world disease;disease world;adolescent;public health problem;mutation;health problem;caused occurrence new;disease world major;caused occurrence;public health", "pdf_keywords": ""}, "6f69fcacdf53a811ef18c5e9ac8ec58035dc43fc": {"ta_keywords": "network transducer rnn;transducer rnn;transducer rnn objective;neural network transducer;supervision recurrent neural;sequence time transduction;recurrent neural network;connectionist temporal classification;supervision recurrent;based supervision recurrent;objective recurrent neural;automatic speech;graph based supervision;temporal classification cc;time transduction graph;recurrent neural;speech recognition;transduction graph based;speech recognition systems;best automatic speech;automatic speech recognition;cc objective recurrent;objective recurrent;temporal classification;connectionist temporal;transduction graph;network transducer;similarly connectionist temporal;rnn objective;rnn", "pdf_keywords": "neural network transducer;rnn transducer decoding;network rnn transducer;transducer based neural;network transducer rnn;transducer rnn;rnn transducer;transducer decoding speech;demonstrate rnn transducer;linear loss neural;algorithm training transducer;transducer rnn objective;transducer based auditory;rnn transducer novel;loss neural network;training transducer based;classification transducer gc;classification transducer;continuous speech recognition;neural network outputs;decoding speech recognition;transducer decoding;neural network rnn;tempral classification transducer;training transducer;dependent neural network;speech recognition;recognition demonstrate rnn;neural network feasible;linear model neural"}, "31dc1e65d61a431964c75bf2eec167bcd9dca0fa": {"ta_keywords": "computer calculate accuracy;use computer calculate;accuracy method analysis;computer calculate;calculate accuracy method;calculate accuracy;accuracy method;method analysis data;analysis data;accuracy;method analysis;literature use computer;calculate;analysis;data;use computer;computer;method;literature use;provide overview literature;overview literature use;use;article provide overview;overview literature;purpose article;overview;provide overview;purpose article provide;literature;article", "pdf_keywords": ""}, "86471bf927401bf88af83626797228c2bf10a282": {"ta_keywords": "interpretations social attribution;faithful interpretations social;decisions causal attribution;causal attribution attribution;social attribution;attribution attribution human;causal attribution;interpretations social;social attribution methods;attribution human;interpretations faithful;faithful interpretations;attribution attribution;attribution;model interpretations faithful;attribution methods;aligning faithful interpretations;incomplete interpretation textual;interpretations;interpretations faithful vague;incomplete interpretation;interpretation;attribution methods requirement;interpretation textual;model interpretations;decisions causal;vague incomplete interpretation;causal chain decisions;chain decisions causal;causal", "pdf_keywords": "human explanations fundamental;human explanations;principle human explanations;attribution intent humans;concept human explanations;human explanations composed;attribution intent causal;predict explainers human;artificial intelligence explanations;decision process attribution;explanations classification;accurate attribution causality;human explanations inspired;explanations model;human explanation;sciences human explanations;steps causal attribution;intelligence explanations;attribution intent;causal attribution social;selection causal attribution;ai explanations;human provided explanations;causal attribution;attribution causality;explanations ai;attribution causality model;intent humans perceive;intelligence explanations redefine;attribution causal"}, "aa30949af5b59624224980e7d741ad8c084271ec": {"ta_keywords": "pandemic social media;trust vaccination users;trust vaccination;impact trust vaccination;vaccination users;propagating misinformation vaccination;social media platforms;misinformation vaccination;misinformation vaccination negatively;vaccination users typically;social media;social media play;multiple social media;vaccination;vaccination negatively impact;vaccinations;19 pandemic social;vaccination negatively;uptake vaccinations;pandemic social;vaccinations essential;vaccinations essential fighting;coronavirus disease 2019;high uptake vaccinations;uptake vaccinations essential;coronavirus;covid 19 pandemic;ongoing coronavirus;coronavirus sars cov;respiratory syndrome coronavirus", "pdf_keywords": ""}, "d35534f3f59631951011539da2fe83f2844ca245": {"ta_keywords": "training generative adversarial;generative adversarial networks;training generative;generative adversarial;adversarial networks jointly;learns latent codes;algorithm training generative;jointly learns latent;networks jointly learns;jointly learns;generative;learns latent;latent codes identities;latent codes generate;adversarial networks;generate diverse images;diverse images subject;diverse images;adversarial;latent codes;codes generate diverse;networks jointly;learns;portion latent codes;algorithm training;images subject;manifold subjects maintaining;photographs;codes identities individual;contingent aspects lighting", "pdf_keywords": "networks gans generate;training generative adversarial;networks gans;automatic encoder gan;gans generate;gans separate identity;generative adversarial networks;generative adversarial;encoder gan;algorithms sd gan;gan training scheme;images ability gans;gan training;neural networks gans;training generative;adversarial networks jointly;gans generate highquality;encoder gan effective;gan generates;algorithm training generative;sd gan training;gans;ability gans;gans separate;images adapt dagan;ability gans separate;gan effective tool;based inception resnet;generative;generating contrasting images"}, "61a07d1e4eaa831152e253b96b91808ef3a184b4": {"ta_keywords": "data annotation crowdsourcing;annotation crowdsourcing;annotation crowdsourcing shared;labeling public crowdsourcing;crowdsourcing;crowdsourcing shared;public crowdsourcing;crowdsourcing marketplaces present;public crowdsourcing marketplaces;crowdsourcing marketplaces;data labeling public;introduction data labeling;language data annotation;data annotation;data labeling;crowdsourcing shared leading;annotation;efficient natural language;natural language data;labeling;labeling public;language resource production;efficient label;efficient label collection;language data;natural language;language resource;world language resource;components efficient label;researchers engineers yandex", "pdf_keywords": ""}, "aead4418733b998792deb9cbf198a834449e00d2": {"ta_keywords": "sequenceto sequence models;sequence models;sequence models domain;evaluating generalization takes;distribution performance sequenceto;performance sequenceto sequence;evaluating generalization;generalizing systematically test;symbolic mathematical integration;integration requires generalizing;methodology evaluating generalization;symbolic mathematical;performance sequenceto;sequenceto;generalization;requires generalizing systematically;generalization takes;generalizing systematically;sequenceto sequence;requires generalizing;generalization takes advantage;generalizing;mathematical integration requires;problem symbolic mathematical;symbolic;consider problem symbolic;systematically test set;sequence;mathematical;mathematical integration", "pdf_keywords": "neural sequence integrator;neural sequence integrators;sequence integrator neural;data neural sequence;neural sequence method;integrator neural sequence;sequence results neural;prediction neural sequence;generalization symbolic mathematics;successful neural sequence;study generalization symbolic;performance sequencetothe neural;neural sequence important;sequence integrators robust;generalize complex problems;neural sequence;sequence integrator ability;sequence integrator highly;results neural sequence;functions model efficiently;generalization symbolic;sequencetothe neural;efficiently integrate problems;problems model efficiently;model efficiently integrate;problems neural sequence;sequencetothe neural sequence;demonstrate neural sequence;sequence method prediction;problems generalize"}, "9f7e317c6ef0bb15aacc9b19f0f0d00fee6c9a36": {"ta_keywords": "tail deep learning;deep learning algorithms;deep learning;concept long tail;long tailed;long tail deep;known long tailed;long tail;tail deep;learning algorithms provide;natural image;natural image data;known long;image data distributions;learning;concept long;learning algorithms;deep;long tailed significant;tailed;methodsfirst natural image;tail;long;informal known long;overview concept long;data distributions;distributions;image;distributions informal known;natural", "pdf_keywords": "memorization estimation deep;memorization estimates;useful estimating memorization;estimating memorization;memorization estimation;memorization influence estimates;memorization estimates sensitive;memorization large;estimating memorization values;utility memorization large;method memorization estimation;architectures memorization estimates;memorization accuracy;memorization value significantly;memorization accuracy learning;memorization large increase;memoryized examples significantly;accuracy learning memorization;memorization training data;example memorization influence;examples memorization influence;utility memorization significant;estimation deep networks;memorization result tuning;estimation deep;expected memorization label;memorization significant;empirically validating long;dataset memorization;learning memorization"}, "a1a8eeb64c0846070b10531061c18fed6d566f8c": {"ta_keywords": "tied candidates random;strategic voting tie;voting tie breaking;nondeterministic tie breaking;computing manipulating vote;breaking random vote;order tied candidates;candidates random vote;nondeterministic tie;vote compare nondeterministic;voting tie;manipulating vote;random vote compare;tied candidates;compare nondeterministic tie;strategic voting;random vote study;random vote;complexity manipulation tiee;manipulating vote different;tie breaking rule;candidates random;tiee breaking random;impact strategic voting;manipulation tiee breaking;vote compare;tie breaking;tie breaking means;voting;manipulation tiee", "pdf_keywords": ""}, "c4919feb50c514e32eb0f4131399180c6f9a0d7d": {"ta_keywords": "resource allocation procurement;allocation procurement costs;allocation procurement;online resource allocation;procurement costs methodswe;procurement costs;procurement cost total;cost total allocation;procurement cost;facing procurement cost;resource procurement;allocation multiple customers;resource allocation multiple;competitive ratioio online;assumptions resource procurement;resource allocation;customer facing procurement;procurement;resource procurement follows;total allocation;improved competitive ratioio;facing procurement;allocation;resources incoming customer;competitive ratioio;allocation assumptions resource;allocation multiple;procurement follows priori;allocate resources incoming;procurement follows", "pdf_keywords": "surrogate functions procurement;procurement cost functions;procurement cost function;optimization problem procurement;functions procurement costs;cost functions online;surrogate function design;cost function algorithm;optimization design surrogate;optimization problem surrogate;design surrogate function;procurement costs optimal;application surrogate function;allocation procurement costs;method online optimization;cost functions;allocation procurement;functions procurement;resource allocation procurement;surrogate function decisions;online optimization;use surrogate functions;online optimization framework;generalization procurement cost;functions use surrogate;cost function;surrogate functions;cost procurement;cost total allocation;technique surrogate function"}, "df56ccda14b5bc255a07fc061c50839e75563c5a": {"ta_keywords": "routing game parking;game parking traffic;routing games queue;queue routing game;parking urban mobility;traffic selects parking;mobility routing games;parking traffic selects;parking related traffic;parking traffic;parking classical routing;games queue flow;game parking;impact parking urban;queue routing;parking urban;routing games;create queue routing;games queue;selects parking;queuing game model;urban mobility routing;routing game;routing game model;queuing game;new routing game;selects parking zone;combine queuing game;classical routing game;parking related", "pdf_keywords": ""}, "4218563e1fe927440e00bf0abe5cb1e037deaf71": {"ta_keywords": "target domain accuracy;machine learning deployments;predicting target domain;method learns threshold;learns threshold;domain accuracy;methods predicting target;source training target;domain accuracy using;predicting target;accuracy using labeled;thresholded confidence;mismatches source training;machine learning;training target;learning deployments characterized;source training;training target test;labeled source data;unlabeled target data;practical method learns;data unlabeled target;thresholded confidence ac;average thresholded confidence;method learns;using labeled source;labeled source;world machine learning;target data methods;source data unlabeled", "pdf_keywords": "learns threshold model;softmax probability predict;learns threshold;prediction target threshold;method learns threshold;softmax;predicting distribution shifts;deep nets;softmax probability;confidence predicting accuracy;method leverages softmax;leverages softmax probability;leverages softmax;predicting target accuracy;threshold model confidence;model confidence predicting;predicting accuracy;predict accuracy;deep networks wide;threshold estimating;predicting distribution;approach deep nets;deep networks;thresholded confidence;confidence effective predicting;distribution shifts dataset;probability predict model;accuracy target distribution;deep nets mls;effective predicting target"}, "2d6d26c118f43f3ab314d07f58c20df6e89a13af": {"ta_keywords": "particles vaccine development;vaccine development influenza;development influenza virus;particles vaccine;influenza virus;like particles vaccine;development influenza;influenza virus like;article development influenza;virus like particles;influenza;vaccine development;vaccine;particles;like particles;virus;virus like;article development;development;like;article", "pdf_keywords": ""}, "92a8f7f09f3705cb5a6009a42220a6f01ea084e8": {"ta_keywords": "actionable knowledge embodied;knowledge embodied agents;extracting actionable knowledge;planners extracting actionable;actionable steps;embodied agents methodsin;embodied agents;knowledge embodied;learning explicit step;actionable knowledge;tasks expressed natural;extracting actionable;actionable steps open;natural language make;set actionable steps;tasks expressed;level tasks expressed;introduction language models;expressed natural language;shot planners extracting;learning explicit;language models;high level tasks;level tasks;natural language;step examples act;introduction language;embodied;agents methodsin;language models zero", "pdf_keywords": "actions large language;action plans language;actionable knowledge language;generating action plans;language models generate;plans language;extract actionable knowledge;actionable knowledge pre;plans language model;large language models;language models large;grounding actionable knowledge;embodied language models;demonstrate language models;language models learning;action phrase environment;learned language models;large language modelswe;trained language models;generate sensible action;language models additional;language models challenging;action translation proposed;trained large language;actionable knowledge propose;language models important;action plans improved;actionable knowledge;potential actionable knowledge;action plans"}, "59653e5cfa854a17c2ffcb86f2a454f27e12c716": {"ta_keywords": "translation decoding diversity;diversity machine translation;translation decoding;decoding diversity machine;decoding diversity;machine translation decoding;machine translation;introductiondecoding diversity machine;introductiondecoding diversity;diversity machine;translation;decoding;diversity;introductiondecoding;machine", "pdf_keywords": "gender pronouns recall;recall male pronouns;pronouns recall male;translating gender pronouns;pronouns beam search;translation neural;translation neural machine;bias translating gender;translating gender;frequent gender pronouns;machine translation neural;neural machine translation;pronouns recall;male pronouns beam;gender pronouns examine;gender pronouns;male pronouns;bias translating;machine translation;deep bidirectional;known bias translating;pronouns beam;role deep bidirectional;languages sampling;languages sampling results;translations;diversity non translation;gender pronoun;translation translations;machine translation demonstrate"}, "ac41e0ef30b6f9ee4930ac85dc46a9b50a1963d2": {"ta_keywords": "crowdsourced labels;quality crowdsourced labels;crowdsourced labels adversely;training data crowdsourcing;data crowdsourcing;learning quality crowdsourced;data crowdsourcing important;quality crowdsourced;crowdsourcing important machine;crowdsourced;crowdsourcing;crowdsourcing important;labeled training data;labeled training;need labeled training;labeled;labels;labels adversely;machine learning quality;training data;experts incentives;experts incentives workers;machine learning;workers experts incentives;need labeled;convey knowledge accurately;important machine learning;labels adversely affected;workers convey knowledge;learning quality", "pdf_keywords": ""}, "76f02d20e02c6baf39fee8f115cd94e4ceacf32b": {"ta_keywords": "reviewers conferences;burden reviewers conferences;reviewers conferences started;competent reviewers growing;submission history papers;competent reviewers;submission history;previous submission history;quality peer review;reviewers growing;peer review;reviewers;authors declare previous;met skepticism authors;skepticism authors raise;number competent reviewers;science conferences experiencing;reviewers growing slower;skepticism authors;modern machine learning;computer science conferences;submissions challenges quality;conferences experiencing;reduce burden reviewers;declare previous submission;science conferences;previous submission;encouraging requiring authors;conferences started encouraging;challenges quality peer", "pdf_keywords": ""}, "bf0beed35ea09aab56027d64c744098cc963fbde": {"ta_keywords": "change detection transient;detection transient dynamics;detect change quickly;detection transient;quickest change detection;change detection;detect change;alarm durations transient;transient phases observations;instantaneously series transient;durations transient;durations transient phases;transient dynamics;objective detect change;transient dynamics studied;transient;transient phases;series transient;series transient phases;persistent distribution;false alarm durations;arl false alarm;detect;final persistent distribution;happen instantaneously;happen instantaneously series;transient phases completely;dynamics studied change;observations different phases;does happen instantaneously", "pdf_keywords": "generalization dynamic cusum;cusum algorithm markov;change detection qcd;dynamic cusum;dynamics cusum algorithm;wd cusum algorithms;markov process cusum;dynamic cusum cusum;transient dynamics cusum;wd cusum algorithm;cusum algorithm performance;detection qcd transient;cusum algorithm depends;cusum algorithms;cusum algorithms established;process cusum algorithm;cusum algorithm regenerative;cusum algorithm;cusum algorithm new;cusum algorithm compared;regenerative cusum algorithm;called dynamic cusum;cusum algorithm present;cusum cusum algorithm;cusum algorithm applied;cusum algorithms respectively;algorithm regenerative cusum;cusum algorithm called;detection changes stochastic;detect change quickly"}, "f1005edfa1fbc4ea0d9a90345388bda8a01e69ed": {"ta_keywords": "introductionconfluent vessel trees;vessel trees;vessel trees common;introductionconfluent vessel;near capillary vasculature;arc construction minimizing;complex near capillary;multidirected arc construction;capillary vasculature;capillary vasculature thousands;near capillary;variants multidirected arc;unsupervised reconstruction;unsupervised reconstruction complex;capillary;multidirected arc;vessel;arc construction;phenomenon unsupervised reconstruction;vasculature thousands bifurcations;reconstruction complex near;reconstruction complex;topology geometry geodissolution;geometry geodissolution;geometry geodissolution physics;vasculature thousands;construction minimizing;arc;unsupervised methods;reconstruction", "pdf_keywords": "tubular graph explore;vessel trees directed;confluent vessel trees;tubular graphs directed;vessel trees method;vessel tree reconstruction;directed tubular graphs;tubular graphs minimizing;tubular graphs;directed tubular graph;vessel trees;confluent tree reconstruction;vessel trees using;confluent tubular graph;path tubular graph;geodesic tubular graphs;oriented vessels confluence;tubular graph;tubular graph use;practical graphbased reconstruction;circular arcs confluence;curves extend confluence;arcs confluence;arcs confluence fundamental;confluence confluent tree;vessel tree;trees directed tubular;confluence discrete paths;confluence approximates;tubular graph present"}, "794b0a1e9719d809ebdf2ef87ff84c2039bfdd52": {"ta_keywords": "machines wirelesshart protocol;wirelesshart protocol stack;introduction wirelesshart protocol;wirelesshart protocol;wirelesshart protocol mainly;state machines wirelesshart;wirelesshart data service;introduction wirelesshart;security wirelesshart;machines wirelesshart;reliability security wirelesshart;wirelesshart data;security wirelesshart data;wirelesshart;protocol stack designed;protocol mainly used;protocol;protocol stack;based qp event;time layer protocol;protocol composed;qp event;protocol mainly;machines delivered events;layer protocol;events communication;protocol composed state;layer protocol composed;series events communication;designed based qp", "pdf_keywords": ""}, "ae30f8fc5a969d2d14ae066db4cd07d86fadbf42": {"ta_keywords": "sulfones having agonist;sulfones useful analgesics;methionine5 enkephalin sulfoxides;receptors disclosed sulfoxides;agonist activity opiate;activity opiate receptors;addicting narcotic antagonists;opiate receptors disclosed;opiate receptors;narcotic antagonists anti;narcotic antagonists;enkephalin sulfoxides sulfones;sulfones useful;sulfoxides sulfones useful;enkephalin sulfoxides;sulfoxides sulfones;disclosed sulfoxides sulfones;activity opiate;antagonists anti diarrheal;analgesics non addicting;sulfoxides sulfones having;methionine5 enkephalin;opiate;non addicting narcotic;addicting narcotic;agonist activity;disclosed sulfoxides;sulfoxides;useful analgesics non;sulfones", "pdf_keywords": ""}, "ffe1416bcfde82f567dd280975bebcfeb4892298": {"ta_keywords": "architecture named transformer;neural networks rnn;rnn advantage architecture;neural network architecture;tosequence transformation tasks;networks rnn advantage;named transformer;sequence tosequence transformation;networks rnn;operation recurrent neural;named transformer used;transformer;rnn;recurrent neural;recurrent neural networks;transformer used;state art neural;sequential operation recurrent;tosequence transformation;neural network;rnn advantage;transformer used successfully;architecture fast iteration;art neural network;operation recurrent;neural;neural networks;transformation tasks;transformation tasks advantage;successfully sequence tosequence", "pdf_keywords": ""}, "c50f98961c951fe3fbdb6f375beb28e40a6b0581": {"ta_keywords": "incentives reviewers participate;backgroundperer reviewed publications;reviewers participate;lack incentives reviewers;incentives reviewers;reviewers participate expend;demand reviewers large;overwhelming demand reviewers;demand reviewers;backgroundperer reviewed;reviewers;reviewed publications;reviewed publications considered;submissions lack incentives;reviewers large;reviews methodsin;quality reviews methodsin;high quality reviews;reviewers large volume;reviews methodsin work;certifying disseminating ideas;quality reviews;reviews;certifying disseminating;large volume submissions;research community considers;submissions lack;submissions;disseminating ideas research;reviewed", "pdf_keywords": ""}, "3febb2bed8865945e7fddc99efd791887bb7e14f": {"ta_keywords": "contextualized word reppositions;contextualized word representation;deep contextualized word;word representation models;word reppositions introduce;purposedeep contextualized word;word representation;contextualized word;word reppositions;contexts model polysemy;linguistic contexts model;word use syntax;linguistic contexts;deep contextualized;semantics uses;contextualized;type deep contextualized;word use;vary linguistic contexts;purposedeep contextualized;polysemy methodswe introduce;linguistic;contexts;semantics;syntax semantics uses;contexts model;polysemy methodswe;polysemy;model polysemy;use syntax semantics", "pdf_keywords": "deep bidirectional language;learning word representations;contextualized word representations;tasks using deep;deep contextualized word;semantic role labeling;contextualized word representation;deep contextual;bidirectional language models;neural language models;word representations;word vectors learned;semantic rolele labeling;word representation models;deep bidirectional;deep contextual representations;learning textual entailment;deep contextualized;using deep contextual;contextual representations downstream;entailment semantic role;word representations fundamental;language models improve;neural models linguistic;bidirectional language model;word representations present;large text corpus;deep context dependent;polysemy word vectors;word representations linear"}, "aa2bd932a2ecb6e07c768bcf0dc119f0cd20f6e0": {"ta_keywords": "matching information integration;textual similarity measures;similarity measures matching;information integration;textual similarity;information integration methods;essential information integration;information integration objective;matching information;learning textual similarity;approximately duplicate database;adaptive matching information;matching results authors;identifying approximately duplicate;duplicate database records;similarity measures;similarity;duplicate database;matching results;combining learning textual;database records refer;introduction adaptive matching;results authors compare;adaptive matching;entity essential information;measures matching results;records refer entity;authors compare;measures matching;database", "pdf_keywords": ""}, "85a18aafcffdcc4eafcb9e5eda0abb8aa5cb8c3b": {"ta_keywords": "big data apachehadoop;apachehadoop provide ability;data apachehadoop;data apachehadoop provide;apachehadoop provide;apachehadoop;processing big data;distributed storage processing;storage processing big;process big data;big data hardware;distributed storage;platforms distributed storage;cluster commodity servers;processing big;commodity servers parallel;servers parallel;storage processing;big data;advances platforms distributed;massive amounts data;platforms distributed;data hardware;data hardware industry;cluster;using cluster;using cluster commodity;cluster commodity;commodity servers;data using cluster", "pdf_keywords": ""}, "d89f4534d1a87005cdf470ec5d8154998d5abdc7": {"ta_keywords": "prediction serving systems;models prediction serving;prediction serving;rates prediction serving;query rates prediction;serving systems queries;machines cluster;machine learning models;deploy models prediction;models prediction;queries return predictions;inference machine learning;high query rates;run machines cluster;predictions performing inference;machine learning;serving systems run;learning models;machines cluster settings;tail latency;cluster;prone slowdowns failures;cluster settings prone;prediction;return predictions performing;predictions performing;serving systems;performing inference machine;learning models primary;strict latency targets", "pdf_keywords": "serving systems parm;erasure coding;erasure coding machine;computation erasure coding;systems parm encodes;coded computation erasure;failures prediction serving;systems parm parity;encoders decoders parm;prediction parm;erasure codes recovering;computation erasure codes;slowdowns failures prediction;parm encodes;parm useful;implement parity;parity models useful;processing processing parm;techniques introducing parity;resilience distributed computation;processing parm;parm parity;coded computation protocols;erasure coding used;introducing parity models;decoders parm;coding machine learning;efficiently performs coded;parity model decoder;introducing parity"}, "e95a96dec775cc792b763f4eec13343c22e850e1": {"ta_keywords": "intelligence realization computer;systems study intelligence;intelligence realization;study intelligence realization;computer systems acm;systems acm presented;systems acm;realization computer systems;computer systems study;realization computer;computer systems;study intelligence;acm presented;computer systems scope;intelligence;acm presented scope;society computer systems;acm;systems study;scope study intelligence;american society computer;society computer;systems scope study;systems;realization;systems scope;computer;activity report;activity;presented scope study", "pdf_keywords": ""}, "49984bc327ef6952118c4b871eeef2a907f7a4ed": {"ta_keywords": "regularized learning games;convergence regularized learning;regularized learning algorithms;regularized learning;classes regularized learning;convergence regularized;learning games;bias achieve faster;backgroundfast convergence regularized;game uses algorithm;individual regret decays;learning algorithms form;algorithms form recency;faster convergence rates;classes regularized;regularized;faster convergence;regret decays sum;recency bias achieve;regret decays;learning algorithms;convergence rates approximate;normal form games;natural classes regularized;class individual regret;games methodswhen;learning;equilibria multiplayer;achieve faster convergence;backgroundfast convergence", "pdf_keywords": "regularized learning games;convergence smooth games;faster regret dynamics;game theory learning;dynamics faster regret;algorithms achieve regret;vanishing regret sequences;regret dynamics faster;algorithmic game theory;conventional regret dynamics;regret bounded variation;class regret dynamics;algorithm satisfies regret;regret sequences;dynamics conventional regret;regret sequences significantly;faster fixed strategies;regret bounds optimistic;smooth games;algorithmic game;strategies average regret;game theory computational;learning games;method regret player;faster regret;achieve regret bounds;regret bounded;regret rate player;learning games fundamental;learning equilibria"}, "c460fd4a0dc86bc518f9a8e982bc48faf1efb942": {"ta_keywords": "broadcasts timelines;broadcasts timelines primary;time broadcast;broadband broadcasts timelines;time broadcast scheduling;broadcast scheduling;audiences sprung startups;timing broadcasts;attention timing broadcasts;social platforms;online social platforms;social platforms today;broadcast;reach large audiences;large audiences spanning;startups social marketing;study time broadcast;like facebook twitter;broadcasts;broadcasts capture attention;broadcast scheduling problem;facebook twitter;marketplaces attention timing;facebook;online social;social marketing;startups social;twitter;audiences spanning;services like facebook", "pdf_keywords": "scheduling attention popular;strategies broadcast scheduling;broadcasting fewer posts;content maximise attention;twitter analysing attention;broadcast social network;attention social media;broadcast scheduling;scheduling attention;scheduling broadcasts;maximise attention received;attention maximize advertising;schedule optimizes attention;attention popular advertising;attention potential scheduling;fewer posts heuristic;popular heuristic schedules;time broadcast scheduling;time slots maximise;broadcast social;attention potential broadcasting;content producer time;publishing content maximise;posts heuristic strategies;like facebook twitter;optimize attention social;increases users microblogs;posts heuristic;maximise overall attention;posts time producer"}, "1a9c89cb2e57e06dadd4c2fab5fae1bfdbb3b6d5": {"ta_keywords": "conditional preference network;probabilityistic preference structure;probabilityistic conditional preference;preference network;preference network cp;net using probabilityistic;using probabilityistic preference;probabilityistic preference;conditional preference;preference structure methodsin;preference structure;collective reasoning tasks;perform collective reasoning;collective reasoning;cp net;net perform collective;probabilityistic conditional;network cp net;pcp net;results probabilityistic conditional;probabilityistic;cp nets;cp nets single;cp net using;network cp;pcp net perform;using probabilityistic;cp net result;collection cp nets;review results probabilityistic", "pdf_keywords": ""}, "f5a0c6593ba95d23c025608ce9280848da8b929f": {"ta_keywords": "gene mention recognition;gene mention task;biocreative gene mention;corresponding gene mentions;mention recognition;mention recognition methods;gene mention;results gene mention;sentences corresponding gene;gene mentions;identify substrings sentences;overview biocreative gene;biocreative gene;systems identify substrings;substrings sentences corresponding;identify substrings;substrings sentences;presented results gene;results gene;corresponding gene;mention task biocystic;overview biocreative;substrings;introduction overview biocreative;gene;biocreative;gene mentions variety;mention task;brief descriptions methods;task biocystic ii", "pdf_keywords": ""}, "92891a984b45df5fc764d81bf9bcd42e7e7ed1c7": {"ta_keywords": "pricing coordination open;nash equilibrium open;loop differential games;nash equilibrium;pricing coordination;loop differential game;differential game nonlinear;local nash equilibrium;differential games;game nonlinear dynamics;equilibrium open loop;game nonlinear;differential game;equilibrium global;introduction pricing coordination;differential games methodswe;coordination open loop;global equilibrium;equilibrium global equilibrium;pricing scheme;quadratic pricing;designing quadratic pricing;desired local nash;induced equilibrium global;equilibrium open;coordination open;quadratic pricing scheme;equilibrium;pricing scheme induce;local nash", "pdf_keywords": ""}, "3cc790174d138d7904189df997d5763f1793dedf": {"ta_keywords": "historical spelling normalization;annotator agreement normalization;spelling normalization methodsthis;spelling normalization;agreement historical spelling;common annotation;normalized wordforms;agreement normalization task;labels normalized wordforms;common annotation tasks;agreement normalization;inter annotator agreement;differs common annotation;evaluating inter annotator;labels normalized;inter annotator;annotations;annotation;annotator agreement historical;annotation tasks;annotator agreement;annotation tasks important;normalized;annotations match different;normalization task;annotations match;normalized wordforms open;normalization task task;backgroundevaluating inter annotator;class labels normalized", "pdf_keywords": ""}, "1d255aeabcb87929742280251007fd8c01bbe914": {"ta_keywords": "polyoxometalates cluster pmo11fe;cluster pmo11fe immobilized;catalytic exoxidation cyclooctene;fabrication pmo11fe cluster;exoxidation cyclooctene 32o2;pmo11fe immobilized nh2;exoxidation cyclooctene;polyoxometalates cluster;cyclooctene 32o2 oxygenidant;cluster pmo11fe;pmo11fe cluster;15 catalytic exoxidation;pmo11fe immobilized;iron substituted polyoxometalates;pmo11fe cluster supporting;substituted polyoxometalates cluster;nh2 functionalized sbe15;introductionthe fabrication pmo11fe;catalytic exoxidation;sibebese 15 catalytic;fabrication pmo11fe;polyoxometalates;32o2 oxygenidant methodsthe;pmo11fe;cyclooctene 32o2;immobilized nh2 functionalized;32o2 oxygenidant;exoxidation;functionalized sbe15;oxygenidant methodsthe transition", "pdf_keywords": ""}, "737f9a32d7f4007aa9526556c256ed4a182aec69": {"ta_keywords": "nash equilibrium robust;optimal nash equilibrium;socially optimal nash;nash equilibrium;equilibrium robust;linear quadratic game;optimal nash;quadratic game;equilibrium robust small;convex program;induce socially optimal;develop convex;socially optimal;game framed convex;develop convex conditions;convex conditions guarantee;equilibrium stable;equilibrium;quadratic game framed;convex;analysis develop convex;players feedback strategies;equilibrium stable respect;convex conditions;optimal;deviations players feedback;prices induce socially;convex program addition;perturbations linear quadratic;induced equilibrium stable", "pdf_keywords": ""}, "4aa72e4232ae809ea1a9fe142275da25ba930655": {"ta_keywords": "convergence nash equilibria;local convergence nash;linear quadratic games;policy gradient algorithms;convergence nash;counterexample policy gradient;quadratic games;quadratic games counterexample;policy gradient algoris;nash equilibria continuous;policy gradient;quadratic games classic;games counterexample policy;gradient algorithms guarantees;gradient play;equilibria continuous action;gradient algoris guarantees;context policy gradient;gradient play non;nash equilibria;convergence linear quadratic;analyze gradient play;guarantees convergence linear;guarantees local convergence;guarantees convergence;gradient algorithms;equilibria continuous;counterexample policy;algoris guarantees convergence;multi agent", "pdf_keywords": "games policy gradient;linear games policy;policy gradient algorithms;games convex game;linear quadratic games;algorithms policy gradient;convergence nash equilibria;games convex;convex game;counterexample policy gradient;nash equilibrium numerical;policy gradient converges;linear games;policy gradient;competitive game gradient;agent optimal control;single agent optimal;policy gradient general;gradient algorithms policy;linear quadratic game;policy gradient avoids;agent optimal;lq games convex;policygradient algorithm exists;quadratic game equilibria;local convergence nash;sum linear games;quadratic games;avoid nash equilibrium;convex game prove"}, "b62ce3135ed6065863c0dec26037fd07c081abba": {"ta_keywords": "counterfactually augmented data;counterfactually augmented;causality offers clarity;indirect causal effects;counterfactually;difference counterfactually augmented;causal effects;causal effects methods;causality;causal;spurious associations confounding;indirect causal;language causality;language causality offers;makes difference counterfactually;frameworks language causality;difference counterfactually;spurious associations;causality offers;direct indirect causal;confounding common cause;cause direct indirect;clarity spurious associations;associations confounding;augmented data aim;confounding;augmented data;associations confounding common;confounding common;augmented", "pdf_keywords": "regression nll learning;learning natural language;human annotators sentiment;natural language;sentiment models counterfactuallyaugmented;language causality offers;natural language inference;nll learning;regression nll;natural language processing;annotators sentiment;language causality;bias human language;nll learning specific;frameworks language causality;counterfactually revised data;human annotating text;models counterfactuallyaugmented data;models counterfactuallyaugmented;approach human sentiment;counterfactually revise documents;counterfactuallyaugmented data dataset;human annotators;train evaluate sentiment;human sentiment;annotators sentiment analysis;human annotating;counterfactuallyaugmented data;systems natural language;human language learning"}, "ca86a63362e51eea2e213ae2d3faed668ec1ad74": {"ta_keywords": "multi word commonsense;commonsense knowledge representation;commonsense concepts methods;commonsense knowledge;background commonsense knowledge;word commonsense;commonsense concepts;texts commonsense concepts;commonsense;texts commonsense;language texts commonsense;background commonsense;deconstructing natural language;natural language;natural language texts;sentiment analysis solutions;document auto categorization;effective multi word;opinion sentiment analysis;sentiment analysis;categorization web;auto categorization web;enhancement topic gisting;categorization web search;knowledge representation;opinion sentiment;knowledge representation reasoning;topic gisting social;multi word;level opinion sentiment", "pdf_keywords": ""}, "a1da1d600acd506b80c8870d293a756c70791683": {"ta_keywords": "bilingual lexicon induction;bilingual lexicons distribution;aligned bilingual lexicons;bilingual lexicons;bilingual lexicon;work bilingual lexicon;lexicon induction bi;depended aligned bilingual;lexicons distribution matching;aligned bilingual;lexicon induction;lexicons distribution;recent work bilingual;bilingual;work bilingual;embedding spaces empirically;spaces empirically;assumption weakens languages;assumption isometry embedding;spaces empirically assumption;isometry embedding spaces;isometry embedding;weakens languages;lexicons;languages;languages question increasingly;induction bi frequently;embedding spaces;lexicon;weakens languages question", "pdf_keywords": ""}, "f184908270fc934ab74438a0aaac7a43a5eae6d2": {"ta_keywords": "neural summarization models;neural summarization;summaries learn interpretable;single document summarization;generated summaries learn;art neural summarization;summarization models;document summarization;document generating summary;summaries learn;summarization models preserve;generated summaries;document summarization relied;quality generated summaries;summarization;generating summary contrast;generating summary;interpretable document representations;summarization relied;learn interpretable document;summaries;document representations;summarization relied modeling;document generating;document sequence tokens;structure document generating;document sequence;summary contrast;interpretable document;learn interpretable", "pdf_keywords": "structure aware summarization;neural summarization;summarization models learn;neural summarization family;neural abstractive summarization;summarization models suffer;abstractive summarization model;summarization models;abstractive summarization models;summaries structure good;summarization using structural;current summarization models;summarization model;summaries structure;summarization model able;summaries structure desirable;aware summarization;structure attention module;aware summarization employ;summarization aims compressing;summarization introduce novel;development neural summarization;document summarization;sentences neural network;generated summaries helps;novel abstractive summarization;structurally based summarization;model summaries structure;summarization effective;summarization employ novel"}, "99ac83b990af1fc591db5b676300a7c002905dae": {"ta_keywords": "multiclass semi supervised;semi supervised learning;semi supervised;hierarchical semi supervised;tackle semi supervised;view hierarchical semi;labels instances multiclass;multiple views;supervised learning;instances multiclass semi;supervised learning optimal;supervised learning information;multiclass semi;supervised;present multiple views;labels instances;multiple views paper;views;hierarchical semi;view hierarchical;sets labels instances;multiple views methodswe;views methodswe propose;multiple views resultsour;instances multiclass;labels;learning information datapoints;introductionmulti view hierarchical;multiclass;supervised learning presence", "pdf_keywords": ""}, "3f90668994d6e5949a530dfc84a10b492ff35cfa": {"ta_keywords": "shallow semantic parsing;semantic parsing especially;semantic parsing;shallow semantic;corpus appropriately rich;similar labeled sentences;semantic structures corpus;labeled sentences provide;grammatically similar labeled;labeled sentences;corpus appropriately;structures corpus appropriately;data shallow semantic;parsing especially;parsing;structures corpus;corpus;parsing especially limited;semantic;semantic structures;labeled data shallow;human labeled data;procedural scientific text;similar labeled;text human labeled;high coverage semantic;sentences provide high;scientific text human;scientific text;coverage semantic structures", "pdf_keywords": ""}, "fce19dd512a82693ab9070049ed426179eca8856": {"ta_keywords": "collaboration web textual;mass collaboration web;textual content analysis;web textual;collaboration web;web textual content;content analysis;collaboration web methodsin;content analysis means;introduction mass collaboration;textual content;mass collaboration;natural language processing;natural language;textual;discover linguistic;arising mass collaboration;linguistic structures data;means natural language;language processing aimto;language processing;discover linguistic structures;collaboration;traditional analysis tools;content web;analysis tools;content web grown;structured traditional analysis;analysis tools properly;handle discover linguistic", "pdf_keywords": ""}, "0d22ce72a62419086fd4860a4671991846cd492b": {"ta_keywords": "encryption standard anesthesiology;privacy internet things;life privacy internet;integrated life privacy;environments advanced encryption;encryption;advanced encryption standard;advanced encryption;encryption standard;introductionthe internet things;life privacy;privacy internet;cryptatic;cryptatic networks lightweight;cryptatic networks;developed simulating cryptatic;internet things national;internet things;things national security;privacy;security;security agency;simulating cryptatic;securing;anesthesiology suitable methodsthe;networks lightweight block;security agency developed;simulating cryptatic networks;standard anesthesiology suitable;standard anesthesiology", "pdf_keywords": ""}, "fb6ef2d6fbd1ea4905070077ab6c5b0108f2c38a": {"ta_keywords": "sarcasm detection twitter;sarcasm detection languages;sarcasm detection czech;research sarcasm detection;approach sarcasm detection;learning approach sarcasm;sarcasm detection;attempt sarcasm detection;detection twitter languages;presentation research sarcasm;detection twitter;research sarcasm;twitter languages;approach sarcasm;twitter languages english;detection czech language;sarcasm;detection czech;twitter;czech language results;attempt sarcasm;detection languages english;work attempt sarcasm;czech language;detection languages;languages english czech;language results;presents machine learning;english czech;english czech case", "pdf_keywords": ""}, "b8e49216e5b4a017342b0be5f6fbbd79e690a1c7": {"ta_keywords": "auctions deep learning;optimum auctions deep;auctions deep;optimal auction;optimum auctions;near optimal auction;auction;introduction optimum auctions;auctions;tools deep learning;deep learning intricate;deep learning;deep learning shaping;tools deep;results tools deep;optimal;design near optimal;learning intricate task;near optimal;optimum;learning;learning shaping powerful;deep;learning intricate;learning shaping;single item;introduction optimum;intricate task single;items;task single item", "pdf_keywords": "auctions using neural;approximation trained auction;trained auction;networks encode auction;models optimal auction;optimal auctions meaningfully;optimal auctions essentially;design optimal auctions;trained auction present;optimal auctions known;auction multi layer;modeling auction;auctions known optimal;assessing optimal auctions;approximating optimal auctions;creation optimal auctions;network optimal auctions;optimal auctions;optimal auction;rules optimal auctions;defined auctions;encode auction mechanisms;auctions optimal;optimize optimal auction;near optimal auctions;optimal auctions set;optimal auction design;optimize auction;general valuation auction;auction maximizes"}, "f297e939212780637705eba8798c9a386befd771": {"ta_keywords": "pivot target translation;target translation models;translation models;translation language pairs;translation process;introductionpot translation allows;translation models source;used translation process;translates combining source;translation language;translation accuracy conventional;target translation;high translation accuracy;translation accuracy;introductionpot translation;method translates;triangulation method translates;method translates combining;translation allows translation;translates combining;known high translation;allows translation language;high translation;information pivot phrases;allows translation;language pairs;translates;translation allows;combining source pivot;translation", "pdf_keywords": ""}, "8a902a848c3710290f04f2d59030f5670d3433f8": {"ta_keywords": "morphological complexity language;increases morphological complexity;morphological complexity;morphology increases morphological;morphology;nonlodol models morphphology;morphology objective;morphphology predictive errors;morphology increases;morphological;morphology objective evaluate;increases morphological;importance morphology;importance morphology increases;languages conjectures somewhat;languages conjectures;error analysis nonlodol;57 languages conjectures;ii importance morphology;analysis role morphology;models morphphology;analysis nonlodol models;models morphphology predictive;nonlodol models;complexity language methods;role morphology;role morphology objective;morphphology predictive;conjectures error analysis;language methods", "pdf_keywords": ""}, "ab48fb72541653f40523caa9fcaac9cb84bf3373": {"ta_keywords": "speaker automatic speech;multi speaker automatic;joint source separation;independent vector analysis;source separation;automatic speech;automatic speech recognition;multi speaker;channel multi speaker;speech recognition;source separation dereverberation;separation dereverberation based;speech recognition propose;speaker automatic;based independent vector;independent vector;frontend joint source;dereverberation based independent;speaker;separation dereverberation;vector analysis iva;vector analysis;end multi channel;multi channel multi;multi channel;separation;joint source;speech;channel multi;dereverberation based", "pdf_keywords": "multichannel speech separation;frontend multichannel speech;multichannel speech;multispeaker automatic speech;speech recognition independent;dereverberation multichannel speech;speech separation extend;speech separation;speech separation dereverberation;training multisensor speech;channel multispeaker;multi channel multispeaker;channel multispeaker automatic;multisensor speech;multisensor speech recognition;separation multichannel signals;source separation based;multispeaker;approach separation multichannel;separation multiple sources;multispeaker automatic;speech recognition;separation multichannel;separation dereverberation multichannel;speech recognition module;blind source separation;multichannel blind source;automatic speech recognition;decoding single speakers;separation based neural"}, "3c5d3bbb73aa0e3e969a25487a81b5b1f0c14044": {"ta_keywords": "knowledge graph identification;knowledge graph information;information knowledge graph;knowledge graph;knowledge graph propose;structure knowledge graph;propose knowledge graph;scale knowledge graph;knowledge graph utilizing;web scale knowledge;graph information entities;graph propose knowledge;information extraction;reasoning structure knowledge;leveraging ontological information;information extraction systems;confidences leveraging ontological;graph utilizing extraction;missing information knowledge;largescale information extraction;leveraging ontological;ontological information;extraction confidences leveraging;ontological information result;structure knowledge;information entities relationships;scale knowledge;jointly reasoning structure;information knowledge;web corpora", "pdf_keywords": ""}, "5ede529879d162d2779d410a5775d3f6cd6be3f4": {"ta_keywords": "distributionally robust optimization;player distributionally robust;robust optimization;distributionally robust;robust optimization dros;min max optimization;second player distributionally;max optimization;distributions uncertainty set;max optimization problem;modeling second player;data distributions uncertainty;player distributionally;distributions uncertainty;learning models able;uncertainty set methods;optimization;design uncertainty set;alternatives min max;learning models;training machine learning;optimization problem;optimization dros;machine learning models;uncertainty set critical;robust;optimization problem exactly;optimization dros provides;machine learning;uncertainty set", "pdf_keywords": "game inner maximization;distributionally robust optimization;constrained inner maximization;model adversary generative;adversary generative model;robust optimization;robust mediocre generative;inner maximization objective;generating robust models;generative model formulation;robust optimization development;stochastic optimization;inner maximization;adversary generative;parametric generative models;generative models;robust models synthetic;maximum simple robust;generative models propose;relaxation game inner;optimization problem amenable;parameter best robust;relaxation kl constrained;optimize model optimal;generative model able;maximization objective makes;learn robust models;generating model adversary;generating robust;maximization objective"}, "c6b462aaca52d0325db3118d2779865915b266c3": {"ta_keywords": "conquer rule learning;complexity rule induction;rule learning induction;rule learning systems;rule induction large;rule learning;introductionefefficient pruning methods;induction large training;rule induction;pruning methods separatee;introductionefefficient pruning;learning induction methods;pruning methods;learning induction;methods separatee conquer;separatee conquer rule;separate conquer rule;complexity rule;induction methods scale;pruning;large training sets;induction methods;conquer rule;learning problems;learning systems;asymptotic complexity rule;complexity;separatee conquer;induction large;learning problems methodsin", "pdf_keywords": ""}, "dfb35ebe4fd754f59053d27c78f555bb5e7ccbff": {"ta_keywords": "prior minimizing curvature;early vision framework;vision framework;minimizing curvature;early vision;optimization location detection;minimizing curvature center;prior minimizing;development early vision;vision framework methodswe;detection likelihoods prior;location detection;coordinate descent;likelihoods prior minimizing;location detection variables;coordinate descent develop;vision;minimizing;block coordinate descent;detection likelihoods;combining detection likelihoods;detection;curvature center lines;lower bound optimization;optimization algorithm;joint optimization;optimization;bound optimization algorithm;combining detection;detection variables effectively", "pdf_keywords": "view reconstruction;absolute curvature regularization;multi view reconstruction;edge vessel detection;curvature regularization;valued edge localization;prior minimizing curvature;edge localization;view reconstruction problem;edge detection;curvature regularization demonstrate;minimizing curvature;view reconstruction method;pixel edge detection;resulting curvature regularization;vision require estimation;edge detection real;edge localization method;minimizing curvature centerlines;detection 2d 3d;sub pixel localization;using curvature penalization;quadratic curvature regularization;pixel localization;curvature penalization;curvature regularization proof;edge detection regularwe;curvature regularization fundamental;edge localization use;valued orientation estimation"}, "cd5a9a0061de6a6841c63e60281133207b2d6763": {"ta_keywords": "neural description model;propose neural description;learning describing;neural description;learning describing given;context encoders description;phrase natural language;introduction learning describing;description model;natural language;describing;context encoders;describing given phrase;encoders description;global contexts;task propose neural;propose neural;local global contexts;global contexts results;description model consists;contexts;consists context encoders;contexts results;neural;contexts results solve;task describing;describing given;contexts important humans;context;description", "pdf_keywords": "phrase description datasets;description generation fromwikipedia;phrase description generation;contexts predict descriptions;description datasets corpus;aware phrase description;natural language descriptions;datasets corpus wordnet;generating natural language;description generation;based description generation;neural description model;description generation task;word phrase description;description datasets;generated descriptions;predict descriptions using;corpus wordnet;generated descriptions local;context aware phrase;wordnet urban dictionaries;description decoder;context predict description;propose neural description;global contexts decoder;description words text;descriptions unknown phrases;language descriptions;method wordnet oxford;generation fromwikipedia"}, "89b8153a86708b411bd21357c5b6006142104fc9": {"ta_keywords": "sed public speaking;spoken quote corpora;speaking quote corpora;quote corpora sed;reveal public speeches;public speeches;memorable speaking;public speeches retained;memorable speaking quote;introduction memorable speaking;quote corpora;public speaking important;public speaking;speaking important public;public speaking methods;speeches retained people;speeches;speeches retained;memorable spoken quote;corpora sed public;speaking quote;analysis memorable spoken;spoken quote;corpora sed;speaking methods;public awareness retained;memorable spoken;quotes interesting useful;public awareness;achieve public awareness", "pdf_keywords": ""}, "91ef95907dc637ad3c29ac3cc0e682b9c1985a37": {"ta_keywords": "simultaneous speech translation;speech translation;speech translation paper;segmentmentation strategies simultaneous;search optimal segmentation;optimizing segmentmentation strategies;optimizing segmentmentation;optimal segmentation strategy;segmentation directly maximizes;segmentation strategies simultaneous;segmentmentation strategies;optimal segmentation;introduction optimizing segmentmentation;segmentation strategy;machine translation;speech translation contrast;segmentation strategies;learning segmentation strategies;performance machine translation;segmentmentation;segmentation strategy resultsan;machine translation methodswe;segmentation directly;segmentation;strategies simultaneous speech;learning segmentation;finds segmentation;finds segmentation directly;simultaneous speech;algorithms learning segmentation", "pdf_keywords": ""}, "1f3c381eedfe8914b81e93070bfdb00cf86ac943": {"ta_keywords": "graph level representations;learning node graph;structural views graphs;graphs unlike visual;views graphs;node graph level;graph level;views graphs unlike;visual representation learning;graphs unlike;representation learning increasing;graphs;neighbors graph diffusion;node graph;visual representation;representation learning;learning node;neighbors graph;graph diffusion;unlike visual representation;level representations;graph;graph diffusion achieve;self supervised;order neighbors graph;introduce self supervised;level representations contrasting;approach learning node;representations contrasting structural;self supervised approach", "pdf_keywords": "graph representation learning;graph level representations;graph neural;representations graph feed;node representations graph;networks graph learning;view representation learning;graph learning;graph networks learning;graph encoders;representation learning node;learn node representations;encodings graph representation;networks learning graphs;graph convolution network;graph neural networks;views graphs;learning graphs;structural views graphs;representations graph;visual representation learning;graph supervised;novel supervised graph;graph representation;train graph encoders;views graphs including;graph encoders maximizing;node representations;graph learning discuss;graph supervised unsupervised"}, "4abdea830316d80ab0b29fb94ee0786216f6a1cd": {"ta_keywords": "phrase alignment extraction;joint phrase alignment;phrase alignment;model joint phrase;formulation memorizes phrases;alignment extraction;alignment extraction using;memorizes phrases generated;phrases granularities included;memorizes phrases;phrases granularities;phrases generated;unsupervised model joint;phrase table;phrases generated terminal;inversion transduction grammars;joint phrase;create phrase table;transduction grammars itgs;bayesian methods inversion;contribution phrases granularities;create phrase;introductionan unsupervised model;non parametric bayesian;transduction grammars;able create phrase;phrases;grammars itgs methodsthe;formulation memorizes;methods inversion transduction", "pdf_keywords": ""}, "d6b3effdeb3d38ac9ee43c3b8292b0937a295c30": {"ta_keywords": "hierarchical multitask learning;multitask learning ct;connectionist temporal classification;multitask learning auxiliary;multitask learning;ct based speech;improved hierarchical multitask;multitask learning context;hierarchical multitask;context connectionist temporal;decoder speech recognition;encoder decoder speech;introduction hierarchical multitask;temporal classification cc;deep encoder;layers deep encoder;speech recognition;speech recognition improved;speech recognition database;learning context connectionist;connectionist temporal;decoder speech;effect hierarchical multitask;speech recognition investigate;multitask;neural encoder;temporal classification;neural encoder decoder;based speech recognition;recognition improved hierarchical", "pdf_keywords": "speech hierarchical multitask;recognition hierarchical multitask;speech recognition hierarchical;telephone speech hierarchical;multitask loss softmax;hierarchical multitask training;multitask learning neural;multitask training telephone;learning hierarchical multitask;hierarchical multitask learning;multitask learning hierarchical;speech hierarchical;multitask learning auxiliary;training telephone speech;improved hierarchical multitask;learning neural speech;multitask learning;neural speech recognition;connectionist temporal classification;model neural speech;decoder speech recognition;hierarchical multitask;multitask learning context;encoder decoder speech;multitask learning used;present hierarchical multitask;models hierarchical multitask;deep encoder;neural speech;multitask learning improves"}, "3c78c6df5eb1695b6a399e346dde880af27d1016": {"ta_keywords": "adapting neural paragraph;neural paragraph level;multi paragraph readingcomprehension;paragraph readingcomprehension methods;paragraph readingcomprehension;question answering models;neural paragraph;paragraphs documents training;question answering;level question answering;individual paragraphs sample;sample multiple paragraphs;effective multi paragraph;answering models;individual paragraphs;paragraphs sample multiple;paragraphs sample;results individual paragraphs;paragraph level;multi paragraph;multiple paragraphs;multiple paragraphs documents;paragraph level question;paragraphs documents;paragraphs;readingcomprehension;readingcomprehension methods;readingcomprehension methods consider;answering models case;paragraph", "pdf_keywords": "pipelined question answering;reading comprehension model;question answering models;answering models;adapting neural paragraph;level question answering;reading comprehension systems;question answering;neural reading comprehension;answer model trained;neural paragraph level;question answering systems;comprehension model;ideas reading comprehension;comprehension systems;comprehension model uses;reading comprehension;paragraph level model;answering models case;comprehension systems employ;generating knowledge neural;neural paragraph;answering systems;answer model;modeling ideas reading;document level task;confidence paragraph level;answer text;multi paraagraph readingcomprehension;paragraph selection model"}, "7c3a2e953d2c07ff4f150865112e4ceec14090ea": {"ta_keywords": "electrolaryngeal speech enhancement;prediction electrolaryngeal speech;approach electrolaryngeal speech;speech enhancement;speech enhancement resultsan;electrolaryngeal speech;excitation prediction electrolaryngeal;speech enhancement methodswe;prediction electrolaryngeal;excitation feature prediction;evaluation excitation feature;enhancement resultsan electrolarynx;statistical excitation prediction;voiced prediction hybrid;unvoiced voiced prediction;excitation prediction;voiced prediction;removing micro prosody;electrolarynx;hybrid approach electrolaryngeal;approach electrolaryngeal;micro prosody;excitation feature;micro prosody low;improve statistical excitation;electrolaryngeal;evaluation excitation;resultsan electrolarynx;introductionan evaluation excitation;avoiding unvoiced voiced", "pdf_keywords": ""}, "73fe797b4f4f2d18784246bb74626426a8fe108e": {"ta_keywords": "graph neural networks;latent graph structure;graph neural;inferring latent graph;latent graph;context graph neural;graph structure;graphs assumed known;graphs methods;graphs methods introduce;static graphs;possible graphs methods;graphs;graph structure difficult;static graphs assumed;space possible graphs;graphs assumed;neural networks typically;possible graphs;networks;applied static graphs;graph;context graph;networks typically;neural networks;networks typically applied;inferring latent;neural;resort inferring latent;structure informed", "pdf_keywords": "pointer graph networks;graph neural networks;latent pointerbased graph;graph neural;learning latent pointerbased;model graph neural;abstract graph neural;graph networks;graph inference;graph inference mechanism;pointerbased graph;demonstrate graph inference;graph networks gns;learn algorithms relying;pointerbased graph using;pointer graph;learn algorithms;development graph neural;latent graph structure;state art graph;queries dynamic graphs;networks based gnn;present pointer graph;demonstrate pointer graph;graphs using latent;inferring latent graph;able learn algorithms;domain knowledge learning;sequence prediction;graphwe demonstrate pointer"}, "8b652c4d7a8d5836925ce0fe28a91dc661778524": {"ta_keywords": "neural language models;large neural language;language models;language models ls;neural language;results nonlinguistic tasks;nonlinguistic tasks;results nonlinguistic;large neural;introduction large neural;nonlinguistic tasks unclear;nonlinguistic;questions like country;language;authored comparative questions;neural ls;comparative questions like;art results nonlinguistic;ability neural ls;ls like bomann;human authored comparative;neural;models actually learn;ls entities human;comparative questions;like country older;entities human authored;country older;like bomann;models ls like", "pdf_keywords": "comparative question models;question models directly;comparative questions pretrained;language models respond;neural language models;question models;natural language challenging;large language models;comparative questions ability;generated comparative questions;extract comparative questions;learning natural language;questions ability learn;language models increasing;language models recent;analyzing comparative questions;question pretrained models;contextual reasoning entities;knowledge natural language;comparative question pretrained;questions fail learn;natural language systems;language models;questions pretrained models;language models finetuned;comparative questions scale;language models important;humanauthored comparative questions;comparative questions task;completing comparative questions"}, "f53aa1d2676689c94429944f6a69431f96e05ae1": {"ta_keywords": "topic models semi;topic indicative features;topic models;backgroundfrom topic models;models semi supervised;semi supervised learning;semi supervised;supervised learning biasing;topic indicative;entity entity clustering;entity clustering;learning biasing mixed;entity clustering methodswe;learning biasing;indicative features entity;latent variable models;biasing mixed membership;supervision mixed membership;membership latent variable;supervised;supervised learning;mixed membership models;bias models;membership latent;mixed membership latent;membership models;explorit topic indicative;features entity entity;features entity;backgroundfrom topic", "pdf_keywords": ""}, "57676e07d66b102f3335a5c538735ebff9076623": {"ta_keywords": "sex specific gene;model sex specific;sex specific;model sex;role sex specific;specific gene development;specific gene;new model sex;gene development;gene;literature role sex;gene development new;role sex;sex;specific;article;literature role;review literature role;purpose article;development;role;model;development new model;review literature;article present;literature;results review;purpose article present;development new;results review literature", "pdf_keywords": ""}, "ba1823889a80c231966a0f24e57c6cf4a569ff8c": {"ta_keywords": "multimodal fake news;fake news detection;improving fake news;multimodal fake;issue multimodal fake;text fake news;fake news text;multimodal models;developing multimodal models;diffusion text fake;diverse multimodal clues;news text images;news detection;news detection using;multimodal clues;news detection current;multimodal clues methodsafter;diverse multimodal;multimodal;multimodal models defective;backgroundim improving fake;fake news raising;improving fake;news text;text fake;issue multimodal;severe issue multimodal;developing multimodal;fuse diverse multimodal;contributions developing multimodal", "pdf_keywords": "multimodal fake news;correlations multimodal fake;multimodal textual;multimodal textual feature;multimodal fake;news measure multimodal;correlations multimodal fakethe;textimage correlations multimodal;enhanced multimodal fake;model multimodal content;modeling multimodal content;multimodal content;fusing multimodal information;multimodal content sufficiently;detection fake news;novel multimodal textual;multimodal feature extraction;multimodal fakethe;detecting fake news;fake news detection;multimodal features extraction;effective fusing multimodal;issue multimodal fake;image multimodal entity;novel multimodal fake;semantics stage multimodal;approach fusing multimodal;multimodal information propose;multimodal models;multimodal entity"}, "499ada382b7ce8f1cbd890e8c21500d95e20f2fe": {"ta_keywords": "holistic evaluation audiorepresentations;evaluation audiorepresentations;audiorepresentations audio embedding;evaluation audiorepresentations audio;audiorepresentations;audio embedding;audio representation provides;audio representation;audio embedding approach;purpose audio representation;audiorepresentations audio;general purpose audio;purpose audio;audio;2021 holistic evaluation;introductionhear 2021 holistic;tasks scenarios methodshear;embedding approach generalizes;hear 2021 neurips;holistic evaluation;embedding;embedding approach;introductionhear 2021;methodshear 2021;methodshear 2021 evalu;aim hear 2021;introductionhear;scenarios methodshear;downstream tasks variety;downstream tasks", "pdf_keywords": "audio learning task;hoious evaluation audiorepresentations;audio learning;evaluation audiorepresentations;practice audio representations;audioset semantic;evaluate audio representations;know audio representation;evaluation audio community;audio challenge rigorous;audio representations evaluated;pretrained audioset semantic;audio representations;task models audio;audioset semantic object;audio evaluation mlc;audio challenge;audio representations development;audio audio representations;hoistic evaluation audio;approach audio evaluation;audio representation;audio evaluation development;audiorepresentations;evaluation audiorepresentations hoyal;supervised learning speech;audio evaluation;audio representations variety;evaluation audio;tool evaluation audio"}, "aa2428e1c4ea6d6bb347cfa59beead8736e19c46": {"ta_keywords": "diagnosis malignant neoplasms;etiology malignant neoplasms;malignant neoplasms;malignant neoplasms poorly;diagnosis malignant;etiology malignant;model diagnosis malignant;neoplasms poorly understood;neoplasms;malignant;neoplasms poorly;diagnosis;new model diagnosis;model diagnosis;etiology;model;understood report;poorly understood report;understood report development;development new model;new model;report development new;poorly understood;report;report development;new;development new;understood;development;poorly", "pdf_keywords": ""}, "95ee674a03ad23eaaf4837121fc8aea30d885088": {"ta_keywords": "learning reasoning preferences;sets objects preferences;objects preferences;representing learning;reasoning preferences;decision making machines;learning measure;neural network;representing learning reasoning;reasoning preferences important;problem learning measure;preferences necessary understand;objects preferences user;introductionpreference central decision;learning reasoning;neural;introductionpreference;preferences important area;novel neural network;preferences user descriptions;preferences important;introductionpreference central;preferences;problem learning;distance sets objects;neural network address;sets objects;working preferences;preferences necessary;address problem learning", "pdf_keywords": "learning preference reasoning;conditional preference network;metric learning preference;preference representation knowledge;preference representations;learn metric network;structured preference representations;embeddings metric learning;preference network;learning reasoning preferences;metric learning neural;preference representation;preference representation combines;learning compact preference;structured preference representation;metric network;metric learning graph;learning preference;metric learning;compact preference representation;preference representations leverage;siamese network;metric learning compact;approach metric learning;graph embeddings metric;method metric learning;learning distance function;siamese network able;metric learning application;learn structured preference"}, "683bbb665bdaea8688834e97559d63842242ee1f": {"ta_keywords": "reinforcement learning sisyphean;combating deep reinforcement;deep reinforcement learning;curse reinforcement learning;sisyphean curse reinforcement;deep reinforcement;learning sisyphean curse;use deep reinforcement;combating deep;reinforcement learning wild;learning sisyphean;introduction combating deep;agent avoid catastrophic;reinforcement learning;algorithm doomed sisyphean;doomed sisyphean curse;sisyphean curse;deep network;doomed sisyphean;curse reinforcement;popular deep network;reinforcement learning use;approximation agents;learning use deep;function approximation agents;reinforcement;deep network algorithm;sisyphean curse owing;sisyphean;environments popular deep", "pdf_keywords": ""}, "13d9d24ff2ba69de4cedcebd8f59371a5c1de7ed": {"ta_keywords": "word sense disambiguation;sense disambiguation methodswe;sense disambiguation;sense disambiguation usefulness;contexts knowledge based;best contexts knowledge;contexts knowledge;disambiguation methodswe;disambiguation usefulness;knowledge based word;disambiguation;based word sense;disambiguation methodswe outline;individual context words;word sense;identifying best contexts;context words;identifying useful contextual;contextual cues knowledge;useful contextual;disambiguation usefulness individual;context words evaluated;contexts;contextual;based diverse lexico;best contexts;knowledge based methods;diverse lexico;usefulness individual context;different knowledge based", "pdf_keywords": ""}, "788aa828a194a6d6c4e5ab1d4b46fc5f987159b0": {"ta_keywords": "processing technology painting;technology painting paints;painting paints;painting paints demonstrated;painting;paints demonstrated studies;technology painting;paints demonstrated;paints;digital image processing;use digital image;image processing technology;paints demonstrated literature;application digital image;digital image;image processing;processing technology;use digital;application digital;processing;digital;image;studies application digital;demonstrated studies application;technology;application;studies application;demonstrated studies;demonstrated literature;literature", "pdf_keywords": ""}, "3df97e8237c7d98c7343fc025eacbbc2b96a10ae": {"ta_keywords": "exosomes small extracellular;exosomes;exosomes small;body exosomes;body exosomes small;extracellular vesicles secreted;development body exosomes;extracellular vesicles;small extracellular vesicles;vesicles secreted cells;vesicles secreted;vesicles;small extracellular;extracellular;secreted cells;secreted cells play;cells;cells play important;cells play;pathological processes body;physiological pathological processes;regulatory roles physiological;roles physiological pathological;physiological pathological;pathological processes;physiological;roles physiological;processes body;development body;processes body participate", "pdf_keywords": ""}, "a43d6fa0e96d56d0200e8d5e4407be8befc4e063": {"ta_keywords": "economic burden food;burden food increasing;health problem economic;food increasing economic;burden food;fast food industry;food industry;food increasing;economic economic burden;economic burden;problem economic economic;health problem;major health problem;problem economic;fast food;increasing economic;food industry major;increasing economic economic;industry major health;health;economic economic;major health;economic;food;burden;industry;problem;increasing;industry major;fast", "pdf_keywords": ""}, "b2fac3812885e3c8101cc729b6846f9108ac4d70": {"ta_keywords": "luce bl model;crowdsourcing use pairwise;ai bot tournaments;luce bl;bot tournaments;items bots teams;comparison data bradley;bot tournaments sports;bots teams students;peer grading crowdsourcing;bl model widely;grading crowdsourcing;teams students search;grading crowdsourcing use;comparison data;applications ai bot;sports peer grading;terry luce bl;pairwise comparison data;collection items bots;crowdsourcing use;maximum likel;bots teams;bl model;crowdsourcing;tournaments sports peer;used maximum likel;use pairwise comparison;bl model evaluate;bl", "pdf_keywords": "scale models mle;models mle;models mle widely;bikinetic model inverse;model mle;general model mle;mass scale models;model inverse model;scale models;model bikinetic model;model inverse;model inverse modelthe;consider model bikinetic;model widely;model bikinetic;inverse model;inverse modelthe method;modelthe method mass;model mle mle;mle widely;mle bias;inverse model inverse;inverse modelthe;mle analysis;model stretched mle;mle analysis instead;models;weighted mle;regression mle;mass weighted mle"}, "8e56db786a685b4b9c7f1b750f60a81baebff0b5": {"ta_keywords": "murmur normal speech;nonamdible murmur normal;methods nonamdible murmur;introductionnaudible murmur nam;nonamdible murmur;introductionnaudible murmur;murmur normal;statistical voice conversion;silent speech communication;speak making audible;murmur nam effective;voice conversion methods;voice conversion;murmur;normal speech;murmur nam;speech communication;statistical voice;normal speech nap;effective silent speech;silent speech;speech owing acoustic;speech communication allows;issue statistical voice;compared natural speech;making audible;speech nap;nam effective silent;speaker speak making;audible sound intelligibility", "pdf_keywords": ""}, "418349df9bf28e2b1290b758a4ebcf0d812c7288": {"ta_keywords": "blog classification ranking;political blog classification;classifying political blogs;blog classification;supervised political blog;blog network ranking;political blogs blog;political blogs;political blog;introductionthe multirank bootsstrap;multirank bootsstrap algorithm;classification ranking;multirank bootsstrap;algorithm classifying political;blogs blog network;link classification;blog network;seed blogs;link classification methodswe;multirank;ranking predicted classes;supervised link classification;blogs;blogs blog;classification ranking using;semi supervised link;seed blogs resultswe;introductionthe multirank;blog;ranking using semi", "pdf_keywords": ""}, "e00f0a9e184a9d2afd8bb344908ca25d8bdc9e04": {"ta_keywords": "computational linguistics;computational linguistics important;development computational linguistics;language processing systems;language processing;natural language processing;oriented natural language;linguistics;linguistics important goal;natural language;linguistics important;task oriented natural;goal computational processing;computational processing efforts;computational processing;goal computational;computational systems;computational;task oriented;formalisms;development computational;important goal computational;development various formalisms;formalisms prove useful;processing systems development;useful outside computational;outside computational;processing systems;various formalisms;design task oriented", "pdf_keywords": ""}, "692320cf5ae6980bc6b2b2d7bc48df961b545c22": {"ta_keywords": "speech enhancement videoconferncing;speech enhancement video;channel speech enhancement;speech enhancement single;enhancement video conferencing;multi channel speech;speech enhancement;enhancement videoconferncing methodstheconferencingspeech;enhancement single microphone;video conferencing challenge;ffield multi channel;introductionconferencingspeech challenge far;videoconferncing methodstheconferencingspeech 2021;enhancement videoconferncing;single microphone array;single microphone;channel speech;video conferencing;introductionconferencingspeech challenge;microphone array;microphone;videoconferncing methodstheconferencingspeech;far ffield multi;methodstheconferencingspeech 2021 challenge;microphone array focusing;challenge far ffield;videoconferncing;far field multi;conferencing challenge consists;enhancement video", "pdf_keywords": "multichannel speech enhancement;channel speech enhancement;speech enhancement video;performance speech enhancement;effective speech enhancement;speech enhancement improved;speech enhancement;speech quality recording;enhancement video conferencing;speech enhancement methods;quality multiple microphone;methods speech enhancement;microphone arrays speech;speech enhancement group;speech quality multiple;design speech quality;speech enhancement icsi;enhancement methods speech;field multichannel speech;speakers real meeting;conferencing performance speech;design microphone arrays;microphone arrays;microphone;design microphone;arrays speech quality;conferencing room recorded;evaluate multichannel speech;multi channel speech;circuit design microphone"}, "87d50fc84c71ed9860ed02b0149266b74c446c9c": {"ta_keywords": "linear regression hmms;hmms using variational;regression hidden markov;regression hmms using;regression hmms;hmm parameters widely;model hmm parameters;model hmm;hidden markov model;hmm parameters;markov model hmm;hidden markov;introductionlinear regression hidden;introductionlinear regression;speech processing;speech processing paper;markov model;training time series;using variational techniques;variational techniques;variational techniques methodsthis;likelihood linear regression;using variational;marginalized log likelihood;regression hidden;analytically derives variational;especially speech processing;log likelihood linear;hmms using;resultsby using variational", "pdf_keywords": ""}, "dc3adb99f682a11fe0507dcbc5dc2958199c5af1": {"ta_keywords": "aetiology understood aetiology;aetiology aetiology understood;aetiology understood;understood aetiology aetiology;understood aetiology;aetiology aetiology;shown aetiology aetiology;aetiology;study shown aetiology;shown aetiology;study;study shown;understood;new study;new study shown;new;shown", "pdf_keywords": ""}, "48685f26b32d199e6a4d80f6c61e62cc9738e403": {"ta_keywords": "2009 event extraction;event extraction;event extraction task;event extraction methodsfirst;bionl 2009 event;task event extraction;bionl 2009 shared;domain adapted parsing;bionl 2009;baselines bionl 2009;parsing;extraction task aimto;strong baselines bionl;support vector machine;extraction task;adapted parsing;2009 event;bionl;machine classifiers;classifiers;extraction methodsfirst;extraction methodsfirst implement;baselines bionl;vector machine classifiers;rule based component;adapted parsing replace;rule based approach;parsing replace rule;machine classifiers achieve;event", "pdf_keywords": ""}, "e107beee5e84cd11d6460f7040676687a51a378b": {"ta_keywords": "regularization inverse problems;driven regularization inverse;regularization inverse;end reconstruction operators;reconstruction operators;reconstruction operators based;posed inverse problems;driven regularization;ill posed inverse;posed inverse;data driven regularization;reconstruction meets data;regularization;variational framework iterative;wasserstein distance distributions;inverse problems propose;data ill posed;backgroundend end reconstruction;measurement space wasserstein;inverse problems;variational framework;classical variational framework;inverse problems methodsthe;wasserstein distance;inverse;unpaired training data;space wasserstein distance;end reconstruction;operators based unpaired;end end reconstruction", "pdf_keywords": "unrolled adversarial regularization;driven regularization adversarial;adversarial regularization algorithm;regularization adversarial;adversarial regularization;regularization adversarial learning;driven regularization;adversarial regularization uar;networks inverse imaging;neural networks inverse;regularization;regularization algorithm;shallow unrolled adversarial;converges unrolled reconstruction;data driven regularization;inverse imaging;end reconstruction optimal;reconstruction unrolled operator;end reconstruction operators;ill posed inverse;unrolled reconstruction converges;unrolled adversarial;regularization algorithm present;initialized reconstruction unrolled;iterative unrolling variational;regularization algorithm algorithm;reconstruction operators;algorithm unrolled adversarial;reconstruction converges unrolled;reconstruction optimal"}, "9f1d9dfb0b30d9fc5881d07b8e7f508815296c93": {"ta_keywords": "statistical parsing;statistical parsing concerns;parser generalizable;syntactic models corpora;good parsing model;learning probabilistic syntactic;probabilistic syntactic models;probabilistic syntactic;good parsing;parsing model generalizable;field statistical parsing;parsing;parsing model;parser;syntactic models;parsing concerns learning;domains good parsing;parsing concerns;senses parser generalizable;models corpora;different senses parser;corpora;syntactic;corpora considerable variation;senses parser;models corpora considerable;learning probabilistic;corpora considerable;concerns learning probabilistic;probabilistic", "pdf_keywords": ""}, "51ec4e93d8ae4c62453fdb34c6866696da0527b1": {"ta_keywords": "fake news detection;multimedia technology fake;technology fake news;content important fake;effects fake news;fake news case;fake news attempts;proliferation fake news;news detection social;videos attract mislead;news case presentationthis;social media promotes;technology fake;fake news caused;detection social media;important fake news;attempts utilize multimedia;multimedia content;images videos attract;news attempts utilize;news detection;multimedia content images;content images;utilize multimedia content;fake news;news attempts;content images videos;media promotes;news case;multimedia technology", "pdf_keywords": "visual content fake;detection fake news;detect fake news;multimedia fake news;fake news detection;content detect fake;fake multimedia news;news images fake;content fake;content important fake;content fake news;content effective fake;fake news images;features fake news;fake images provoke;exploiting visual content;detection false news;fake images;images fake news;multimodal information fake;fake news analyze;fake news datasets;analysis fake news;visual content exploit;fake real multimedia;detection fake;fake multimedia;use multimedia fake;detect fake;features multimedia fake"}, "a16cecbaf87d965e396e610f251f710a807b70ad": {"ta_keywords": "hearing impairment simulation;impairment simulation method;impairment simulation;personalization hearing impairment;perception hearing impaired;hearing impaired auditory;impairment simulation systems;impairment simulation effective;hearing people auditory;characteristics hearing impaired;introduction hearing impairment;hearing impairment;propose hearing impairment;hearing impaired vary;impaired auditory characteristics;people auditory perception;hearing impaired;impaired auditory;auditory characteristics individuals;measurement auditory characteristics;auditory perception hearing;auditory characteristics hearing;people auditory;normal hearing people;differences measurement auditory;auditory perception;hearing people;measurement auditory;perception hearing;accurately simulate individual", "pdf_keywords": ""}, "f49065750931c1c3c9edaf7d2f4bc8ea1342450a": {"ta_keywords": "neural autoregressive sequence;sequences define oversmoothing;autoregressive sequence models;oversmoothing neural machine;unreasonably short sequences;neural autoregressive;sequence models;autoregressive sequence;sequence models smear;short sequences define;probability unreasonably short;ones repetitive sequences;oversmoothing neural;probability possible sequences;repetitive sequences;short sequences;repetitive sequences work;background neural autoregressive;oversmoothing rate quantify;define oversmoothing rate;autoregressive;sequences define;high probability unreasonably;sequences including;oversmoothing rate;possible sequences including;sequences including degenerate;sequences work tackle;sequences;degree oversmoothing neural", "pdf_keywords": "machine translation oversmoothing;translation oversmoothing loss;neural machine translation;translation oversmoothing;machine translation rare;machine translation impact;sequences define oversmoothing;ratio improves translation;improves translation quality;improving translation quality;machine translation;autoregressive sequence models;machine translation useful;premature sequences;machine translation propose;abstractneuro autoregressive sequence;lower references neural;oversmoothing neural machine;sequence models;improves translation;improving translation;short sequences define;unreasonably short sequences;machine translation aim;translation quality;short sequences;reference sequences increase;translation quality despite;nature premature sequences;oversmoothing rate neural"}, "b0b1112b06898733faefc32f54940aa4e84bc383": {"ta_keywords": "translate paralinguistic information;speech s2s translation;speech speech s2s;speech s2s;paralinguistic information;usually translate paralinguistic;paralinguistic information focus;translate paralinguistic;paralinguistic information included;types paralinguistic information;s2s translation;various types paralinguistic;s2s translation gradually;paralinguistic;types paralinguistic;speech speech;languages limitations currents2s;input speech;speech various types;input speech various;speech;systems usually translate;included input speech;speech various;introduction speech speech;emphasis type information;language barrier bringing;usually translate;break language;information used convey", "pdf_keywords": ""}, "c55bc339122ad8cdba1ae74d1336be3d2f089699": {"ta_keywords": "optimal decentralized distributed;algorithms stochastic convex;stochastic convex optimization;decentralized distributed algorithms;distributed algorithms stochastic;convex optimization problems;convex optimization;distributed algorithms;convex optimization discussed;stochastic convex;optimal decentralized;decentralized distributed;consider stochastic convex;optimization problems affine;background optimal decentralized;primal dual approach;algorithms stochastic;problems affine constraints;optimization methods;affine constraints;optimization discussed article;distributed;methods consider stochastic;optimization methods propose;convex;affine constraints develop;optimization problems;optimization discussed;methods using primal;using primal dual", "pdf_keywords": "optimization convex optimization;stochastic convex optimization;convex optimization;interested convex optimization;convex optimization problems;distributed convex optimization;optimization convex;convex optimization equations;distributed optimization convex;parallel optimization;parallel optimization compute;convex minimization;convex minimization problems;non convex optimization;convex optimization problem;stochastic optimization;convex smooth objective;strongly convex minimization;stochastic optimization problems;convex optimization large;convex dual;approachwe interested convex;distributed optimization;convex optimization neural;stochastic composite optimization;affine convex problems;convex dual problem;stochastic composit optimization;primal functional convex;convex problems stochastic"}, "6cf3bdcdee6236f9f04e7773e3601dbbb8fbc61e": {"ta_keywords": "namedd entity recognition;named entity recognition;entity recognition datasets;entity recognition;entity recognition models;human annotated datasets;rely human annotated;human annotated;questions generateate namedd;annotated datasets requir;natural language questions;named entity;namedd entity;annotated datasets;simple natural language;generateate namedd entity;natural language;generates ner datasets;ner datasets;entities 004;automatically generates ner;domain entities 004;ner datasets 006;simple questions generateate;domain entities;datasets 006 asking;entities 004 work;entities;introduces ask generate;questions 007", "pdf_keywords": "entity recognition ner;named entity recognition;entity recognition natural;entity recognition;domain recognition ner;introductionnamed entity recognition;named domain recognition;entities natural language;natural language ner;extracting named entities;automated ner dataset;ner task extracting;domain manual annotations;language ner models;generating manually annotated;ner dataset generation;rely human annotated;human annotated datasets;sentences xtrain annotating;named entities;ner datasets gener;generates ner datasets;domains ner benchmarks;named entities dictionary;classification human entities;annotated datasets requiring;automatically generates ner;generate ner datasets;manually annotated data;annotated data manually"}, "a0f00d5ea3727151b1c2fc8c407404f0c6641051": {"ta_keywords": "introductionckylark robust parser;robust parsers genre;robust parsers;parser aimto kylark;robust parser;parser robust;parser robust parsers;parsers known;structure parser robust;parsers known achieve;parsers;parsers genre;phrase structure parser;robust parser aimto;la parsers known;parser;parsing;parsers genre methodspcf;la parsers;kylark computerfg;introductionckylark robust;structure parser;methodspcf la parsers;kylark computerfg la;performance parsing;parses;competitive performance parsing;parses generated ckylark;parses generated;parsing process", "pdf_keywords": ""}, "3b7321832ba109cf47bfd13595c3b58acd4cb080": {"ta_keywords": "symtolic dysfunction symtolic;symtolic dysfunction;dysfunction symtolic;dysfunction symtolic dysfunction;disease form symtolic;form symtolic dysfunction;patient developed malignant;developed malignant disease;malignant disease poorly;etiology malignant disease;malignant disease;malignant disease form;developed malignant;symtolic;etiology malignant;malignant;form symtolic;disease poorly understood;disease poorly;disease;disease form;patient developed;case patient developed;dysfunction;etiology;case patient;patient;understood case patient;poorly understood case;poorly", "pdf_keywords": ""}, "3400b8bf1ffde3ef3d35dfcea893e6506427aa21": {"ta_keywords": "multi speaker speech;utterances multiple speakers;recognition utterances multiple;speaker speech recognition;utterances multiple;directly decode multiple;multi speaker;speech recognition utterances;speech recognition;decode multiple;recognition utterances;speaker speech;speakers recognized mixture;multiple speakers recognized;growing multi speaker;multiple speakers;directly decode;alignments effective learning;speech;speaker;utterances;decode;sequence sequence framework;sequence framework directly;recognized mixture;speakers recognized;sequence framework;multi;recognized mixture promising;sequence", "pdf_keywords": "input speech mixture;speech mixture multiple;speech output label;speech recognizer generate;speech recognition generation;speech mixture;speech recognition multi;decoding speech content;speaker speech recognizer;speech sources training;multi speaker speech;recognition multi speaker;explicit speech separation;speaker speech recognition;speech data mixed;speech recognizer;decoding speech;convert input speech;mixed speech mixture;speech data;speech recognition explicit;end speech recognition;input speech output;speech separation;approach decoding speech;recognition explicit speech;decode speech content;end multispeaker speech;speech output;corresponding isolated speech"}, "3ce0f00d6c949192107f1bd6a167c03e1fb7393a": {"ta_keywords": "deterministic dependency parsing;directingional dependency parsing;dependency parsing algorithm;dependency parsing;deterministic parsing algorithms;dependency parsing methodswe;deterministic parsing;resultstraditional deterministic parsing;parsing algorithms;parsing algorithms based;parsing algorithm;parsing;non directingional dependency;directingional dependency;parsing algorithm attempts;deterministic dependency;novel deterministic dependency;dependency structure;parsing methodswe present;dependency structure non;dependency;parsing methodswe;easiest arcs dependency;framework traverse sentence;arcs dependency structure;traverse sentence;traverse sentence left;easy non directingional;arcs dependency;introductionanefefficient alginom easy", "pdf_keywords": "dependency parsing algorithms;transition based parsing;dependency parsing algorithm;deterministic dependency parsing;directional dependency parsing;deterministic parsing algorithms;dependency parsing;based parsing algorithms;greedy deterministic parsing;deterministic parsing approach;easy parser transition;nondirectional easy parsing;parsing algorithms;parser transition based;deterministic parsing;parsing algorithms based;parsing algorithm significantly;dependency parsing applications;category dependency parsing;parsing sentence data;later stages parsing;nondirectional easy parser;parsing algorithm;parsing algorithm restricted;deterministic parsers capable;easy parser;easy parsing;parser transition;left right parsing;parsing approach"}, "b9f5115b0353c268999fcc2f49c4b8e03a223994": {"ta_keywords": "based evolutionary causal;evolutionary causal matrices;evolutionary causal;predict long term;outcomes interventions;term outcomes interventions;long term outcomes;outcomes interventions necessary;interventions;simulations based evolutionary;causal matrices markov;predict long;interventions necessary educational;educational social policy;policy making;social policy making;causal;policy making processes;influence society long;simulations;computer simulations;term outcomes;causal matrices;interventions necessary;social policy;outcomes;simulations based;predictions based;society long term;used predict long", "pdf_keywords": "markov chain simulations;simulations markov chains;simulations markov;running simulations markov;markov chain prospective;evolutionary causality matrix;markov chains described;markov chains;based evolutionary causal;transitions markov_learning;markov chain used;markov_learning length simulation;simulation intervention outcomes;markov chains systems;causality matrix markov;evolutionary causal matrices;based evolutionary causality;simulation intervention;simulation outcomes interventions;based electronic markov;markov chain;causal matrices markov;program simulation intervention;using evolutionary causal;simulations based evolutionary;definition markov chains;markov;interventions based evolutionary;evolutionary causal;outcomes using evolutionary"}, "cc4cc594c7dd38482c46a2db440135b8f26ff54f": {"ta_keywords": "zn nanocubes highindex;skin pt78zn22 proposed;zn nanocubes;exchange membrane fuel;membrane fuel cells;concave zn nanocubes;develop new catalyst;membrane fuel;pt78zn22 proposed;new catalyst;pt78zn22 proposed highefficiency;fuel cells;skin pt78zn22;new catalyst high;faceted skin pt78zn22;catalyst high;fuel cells unique;catalyst;catalyst high performance;ptskin t78zn22;nanocubes highindex faceted;gas delivery materials;pt78zn22;t78zn22;nanocubes highindex;proton exchange membrane;t78zn22 kb;catalysis proton exchange;ptskin t78zn22 kb;proposed highefficiency catalysis", "pdf_keywords": ""}, "e2f015bbddd7bade7caca693e37f84c4cf70a7f5": {"ta_keywords": "matrix factorization mnmf;nonfacial matrix factorization;factorization mnmf multi;factorization mnmf;negative matrix factorization;matrix factorization deal;matrix factorization;mnmf multi channel;factorizes observation matrix;factorization deal;methodsmmf factorizes observation;factorization deal spatial;nonfacial nonfacial matrix;nonfacial matrix;methodsmmf factorizes;values methodsmmf factorizes;factorization;mnmf multi;non negative matrix;channel non negative;factorizes observation;channel extension nonfacial;mnmf;negative matrix;multi channel;introductionmulti channel non;channel non;factorizes;simultaneously hand mnmf;mnmf larger", "pdf_keywords": ""}, "f8d7b263e8d663583cd22d5988c8ea4a49ed2840": {"ta_keywords": "entity relation extraction;relation extraction;entities extract relations;relation extraction aims;relation extraction methodsend;extract relations simultaneously;end relation extraction;extract relations;relation extraction establish;named entities extract;joint entity relation;approach joint entity;entities extract;joint entity;jointly unifying structured;relations simultaneously;entity relation;relations simultaneously recent;subtasks jointly unifying;identify named entities;relations;named entities;models subtasks jointly;unifying structured prediction;learning shared representations;structured prediction framework;structured prediction;jointly unifying;entities;subtasks jointly", "pdf_keywords": "entity relationship extraction;predicting entities relations;entities extract relations;relation extraction;relationship extraction;relation extraction independently;relation extraction model;relation extraction datasets;relation extraction aims;entity recognition relation;neural relation extraction;relations relation extraction;relation extraction use;extraction relation extraction;relation extraction natural;entities relations learn;entity recognition;relation extraction relation;relationship extraction zexuan;end relation extraction;recognition relation extraction;predict entity relation;extract relations;entities sentence relationship;entities relations using;entities extract;improve entity relation;named entities extract;relations using entity;entities relations"}, "249b7517a746b1389991e10fd618cad62e66c4df": {"ta_keywords": "extracting word synonyms;synonyms corpus parsed;word synonyms corpus;synonyms corpus;graph based similarity;word synonyms;similarity measures;similarity measures task;based similarity measures;synonyms;corpus parsed text;specialized similarity measures;syntactic vector based;corpus parsed;corpus;similarity measures different;text constrained graph;learning specialized similarity;similarity;specialized similarity;based similarity;word types;graph walk variant;different word types;task extracting word;syntactic vector;extracting word;parsed text constrained;measures different word;learn graph based", "pdf_keywords": ""}, "0718237a30408609554a0e2b90d35e37d54b1959": {"ta_keywords": "word embedding;word embeddings;embedding vectors word;word embedding algorithms;word embeddings traditionally;known word embeddings;embeddings traditionally word;traditionally word embedding;onehot representations words;embedding learning;meaningful semantic granularity;representations words phrases;embeddings;perform embedding learning;embedding vectors;embedding algorithms treat;semantic granularity;granularity perform embedding;embeddings traditionally;embedding;semantic granularity perform;representations words;embedding algorithms;phrases lower dimensional;finest meaningful semantic;learning distinct embedding;perform embedding;vectors known word;embedding learning distinct;vectors word", "pdf_keywords": ""}, "771c1cb5fb161231e9aaa0a189caba672256a880": {"ta_keywords": "languages productive morphology;models generate words;language models generate;language models;vocabulary language model;fixed vocabulary character;word based models;vocabulary character based;productive morphology;language model;generate words;language model incorporates;words fixed vocabulary;generated linguistically;open vocabulary language;fixed vocabulary;generate words fixed;morphology;vocabulary character;problems language models;character based models;vocabulary language;word type generated;generated linguistically nave;structure word based;type generated linguistically;introduce open vocabulary;introduction languages productive;linguistic facts built;word based", "pdf_keywords": ""}, "7374494ee88608ef76f74b58a8f8c26ab06adfb9": {"ta_keywords": "clustering based diarization;diarization methodsa clustering;based diarization methodsa;overlapping speech frame;overlapping speech treating;overlapping speech;end diarization methods;diarization methods handle;diarization methods;diarization methodsa;handle overlapping speech;based diarization;based diarization method;diarization model;end diarization model;diarization;diarization method;speech frame;diarization method partition;end diarization;speech frame assigned;clusters number speakers;diarization model post;speech treating problem;end end diarization;frame assigned speaker;clustering;methodsa clustering based;methodsa clustering;clustering based", "pdf_keywords": "end speaker diarization;speaker diarization;speech separation recognition;speaker diarization useful;speaker diarization model;clustering based diarization;speech diarization;clusteringbased diarization using;diarization methods clustering;clusteringbased diarization;method clusteringbased diarization;background speaker diarization;clusteringbased results speakers;vector clustering hmm;speech diarization applications;diarization speakers using;clustering hmm resegmentation;speech diarization development;diarization results speakers;method diarization speakers;clustering hmm;state speech diarization;speaker datasets mixture;diarization speakers;speech separation;based diarization methods;diarization results evaluations;multi speaker datasets;performance speech separation;field speech diarization"}, "6276bbe6cc56234d430725a31a27939eeec88149": {"ta_keywords": "quotability identification;identification passage ranking;approach quotability identification;documents approach quotability;quotability identification passage;quotability identification given;identify passages quotable;quotation useful tools;passage ranking;passage ranking problem;quotability;based models quotation;quotable likely directly;task quotability identification;approach quotability;models quotation useful;passages quotable;passages quotable likely;quotable;models quotation;quotable likely;directly quoted;explore task quotability;content based models;documents approach;task quotability;directly quoted later;quotation;derived documents approach;bilectomic models rank", "pdf_keywords": "identification passage ranking;ranking identify passages;passage ranking identify;passage ranking;quotability identification identifying;passage ranking problem;quotability identification;documents approach quotability;approach quotability identification;use quotability identification;quotability identification problem;quotability identification given;identify passages quotable;quotability text;quotability text text;quotability identification passage;task quotability identification;corpthe quotability identification;source document measure;text identify passages;quotability quotability text;documents relative performances;library based quotability;identifying passages broader;passage text data;based quotability;given document predicted;sentence models;ranking identify;source text identify"}, "96d5e1f691397dfb51e8b818a21a2d11eee46a59": {"ta_keywords": "sparse linear codes;modern multicore processing;linear codes exploiting;exploiting modern multicore;propose use sparse;multicore processing;faster uncoded schemes;use sparse;sparse linear;multicore processing architecture;use sparse linear;faster uncoded;linear codes;sparse;modern multicore;processing architecture coding;times faster uncoded;optimal runtime;order optimal runtime;multicore;architecture coding solution;architecture coding;optimal runtime log;coding solution;coding solution achieves;codes exploiting modern;coding;codes exploiting;uncoded schemes;processing architecture", "pdf_keywords": ""}, "75fe6c3ffdea2608794b4f21119c5a4dec07663a": {"ta_keywords": "autoregressive sequence generation;sequence generation;sequence models autoregressive;sequence generation using;sequence sequence models;sequence models;autoregressive generate token;autoregressive seq2seq models;autoregressive sequence;models autoregressive generate;non autoregressive sequence;autoregressive generate;backgroundmost sequence sequence;seq2seq models generate;autoregressive seq2seq;sequence sequence;non autoregressive seq2seq;models generate tokens;sequence;seq2seq models;backgroundmost sequence;models autoregressive;models generate;efficiency parallel processing;parallel processing;model non autoregressive;generate token conditioning;generated tokens;generation using la;generate", "pdf_keywords": "generative flow;generative flow frame;generative flow elegant;based generative flow;models based generative;generation nonautoregressive translations;using generative flows;autoregressive sequence generation;neural machine translation;turn generative flow;generative flows;sequence model neural;generation using generative;called generative flow;generative flow turn;generating neural sequence;distributions using neural;flow turn generative;generative flows development;decoder posterior networks;density sequential latent;machine translation benchmark;neural sequence model;generative;machine translation;translation benchmark datasets;nonautoregressive generation;generating neural;machine translation development;generation using latent"}, "aa0b93501f79d57fe8542e72ccc8843ea50443c9": {"ta_keywords": "sequence speech recognition;multilingual sequenceence sequence;multilingual sequenceence;analyze multilingual sequenceence;sequenceence sequence speech;introductionanalysis multilingual sequenceence;seq2seq automatic speech;hmm systems sequence;sequence speech;speech recognition systems;speech recognition;automatic speech recognition;speech recognition ar;automatic speech;hmm systems;model hmm systems;model hmm;multilingual approaches developed;multilingual approaches;markov model hmm;applications various multilingual;various multilingual approaches;conventional hidden markov;analyze multilingual;sequence seq2seq automatic;multilingual;hidden markov;various multilingual;hidden markov model;sequenceence sequence", "pdf_keywords": ""}, "de971e50d70bc4d66f7debfab242942b0d1cae34": {"ta_keywords": "fusion speech summarization;speech summarization achieved;speech summarization;speech summarization speech;summarization speech summarization;summarization speech;approach speech summarization;speech summarization attractive;hypothesis fusion speech;summarization cascade;fusion speech;text summarization cascade;combining automatic speech;summarization cascade approach;summarization achieved combining;attention based multi;automatic speech;summarization achieved;ar text summarization;speech recognition ar;speech recognition;text summarization;automatic speech recognition;summarization;summarization attractive;multi hypothesis fusion;summarization attractive approach;attention based;background attention based;training datasets subtasks", "pdf_keywords": "speech summarization train;cascade speech summarization;speech summarization exploit;summarization speech model;task speech summarization;speech summarization model;summarization task speech;text speech summarization;speech summarization;speech summarization new;text summarization speech;summarization speech speech;speech summarization speech;speech summarization achieved;summarization application speech;speech summarization application;speech summarization corpus;approach speech summarization;method speech summarization;summarization speech;summarization speech corpus;approach summarization speech;summarizing speech speech;speech summarization employ;proposed speech summarization;analysis speech summarization;summarizing speech;approach summarizing speech;summarization performance auditory;summarization train"}, "e0ab89821af308f51647bfe872f114d775fd8818": {"ta_keywords": "multilingual conversations medical;speech translation medical;medical data multilingual;multilingual medical data;data multilingual speech;multilingual speech recognition;multilingual medical;multilingual speech;multilingual conversations;speech translation s2fast;development multilingual medical;speech speech translation;translation medical domain;translate spoken utterance;data multilingual;translation medical;speech translation;introduction multilingual conversations;multilingual;designed translate spoken;network based speech;speech recognition network;translate spoken;medical data network;recent development multilingual;introduction multilingual;translation s2fast designed;translation s2fast;s2fast designed translate;conversations medical domain", "pdf_keywords": ""}, "fba7ad8f63a42111b3618e51e3493ed70aafdcd0": {"ta_keywords": "influences word use;estimating influences speakers;word use speaker;speaker depends word;word distribution influences;word use polylogue;influences speakers conversation;influences word;people methodsin conversations;learning influences word;polylogue propose probabilistic;conversations people tend;methodsin conversations people;conversations people;use polylogue;influences speakers;conversation data;word distribution;estimating influences;word use;conversations;speakers conversation;model estimating influences;methodsin conversations;assume word use;general word distribution;depends word use;polylogue;speakers earlier word;use polylogue propose", "pdf_keywords": ""}, "1ce96d8dbf69199ebd043de6cfa25d7e48b8ab03": {"ta_keywords": "causal effects linguistic;effects linguistic properties;proxies linguistic properties;effects linguistic;linguistic properties predictions;linguistic properties;noisy proxies linguistic;proxies linguistic;data estimate causal;estimate causal effects;linguistic;linguistic properties methodsfirst;formalize causal;estimate causal;causal effects;formalize causal quantity;causal;causal quantity effect;causal quantity;predictions classifiers lexicon;methodsfirst formalize causal;classifiers lexicon;necessary identify observational;using observational;observational;observational data estimate;observational data;using observational data;identify observational;lexicon", "pdf_keywords": "causal effects linguistic;effects linguistic;effects linguistic properties;text based causal;effects latent linguistic;linguistic property information;linguistic properties observed;linguistic properties attributed;perceive linguistic property;linguistic properties;linguistic properties text;describing linguistic properties;confounding information text;linguistic;linguistic property;method describing linguistic;inferences using text;linguistics nonlinguisticlinguistics demonstrate;linguistics nonlinguisticlinguistics;noisy proxies linguistic;principle linguistics nonlinguisticlinguistics;proxies linguistic properties;describing linguistic;linguistic properties available;perceive linguistic;linguistic properties apply;information causal effects;attributed confounding information;words information;nonlinguisticlinguistics demonstrate method"}, "c37ecbccecab1774b545a5a5804b575718218f7d": {"ta_keywords": "emotional speech degrades;degrades emotional speech;speech degrades emotional;feature transformation methods;speech degrades auditory;speech degrades;feature transformation;influences speech degrades;emotional speech;speech acoustic model;degrades auditory;degrades auditory auditory;auditory model;auditory model ar;features cnn;auditory auditory model;speech acoustic;focus feature transformation;bottleneck features cnn;bottleneck features;acoustic model study;input speech acoustic;acoustic model;3d bottleneck features;mismatch input speech;auditory;auditory auditory;input speech;auditory auditory auditory;influences speech", "pdf_keywords": ""}, "ae5a34c20fee705ad7094c93a711d5f724d535f0": {"ta_keywords": "fairness aware tensor;tensor recommendation framework;recommender systems tensors;factorization methods recommender;aware tensor recommendation;traditional matrix factorization;matrix factorization methods;worsening fairness recommendations;matrix factorization;recommendation framework designed;recommendation framework;tensor recommendation;fairness recommendations;fairness recommendations aimsto;improved recommendation quality;factorization methods;sensitive latent factor;improving fairness methodsfour;recommendation quality;latent factor matrix;recommendation quality worsening;factor matrix;fairness aware;novel fairness aware;achieve improved recommendation;factorization;fairness methodsfour;aware tensor;improved recommendation;tensors achieve improved", "pdf_keywords": "recommendation tensor approaches;improving fairness recommender;tensor based recommendations;fairness recommender sensitive;recommendation tensor;tensor recommendation framework;recommendations based tensor;tensor based recommender;recommender systems tensors;fairness aware tensor;fairness recommender;aware tensor recommendation;tensor completion fairness;expert recommendation tensor;fairness implicit recommender;tensor factorization;sensitive attributes recommendations;fairness recommendations methodswe;use tensor factorization;fairness recommendation ethical;worsening fairness recommendations;tensor recommendation;sensitive attributes tensor;recommender sensitive;factorization methods recommender;recommender sensitive information;completion fairness recommendation;fairness recommendation;improved recommendation quality;tensor completion framework"}, "ff187722c5b5462ac2066a737ae97650ffa177ed": {"ta_keywords": "automatic pronunciation assessment;pronunciation assessment;improvement automatic pronunciation;automatic pronunciation;use automatic speech;pronunciation assessment noisy;students speech recording;automatic speech recognition;technologies detect pronunciation;speech recognition ar;automatic speech;speech recognition;speech recording acall;detect pronunciation errors;detect pronunciation;classroom utterances;acall classroom utterances;classroom utterances student;utterances student recorded;assessment noisy classroom;speech recording;pronunciation errors estimate;students speech;individual students speech;assisted language learning;language learning systems;utterances student;recording acall classroom;language education systems;pronunciation errors", "pdf_keywords": ""}, "0f61621206e363367db85b39e8e4325e425afcb4": {"ta_keywords": "singing voice conversion;convert singing voice;voice conversion based;voice conversion;statistical singing voice;voice conversion direct;method statistical singing;generated statistical singing;convert singing;possible convert singing;introduction statistical singing;statistical singing;voices generated statistical;quality voices generated;singing voice characteristics;voices generated;singing voice;voice characteristics source;variance methods diffsv;improve quality voices;quality voices;conversion direct waveform;voice;voice characteristics;characteristics source singer;target singer using;singer using;waveform modification global;direct waveform modification;conversion based direct", "pdf_keywords": ""}, "94c3fd8eea08008cecd98f4aace024cf63954ead": {"ta_keywords": "malicious sensors;secure remote estimation;unknown malicious sensors;malicious sensors inject;remote estimation;observations multiple sensors;sensors inject anomalous;remote estimation linear;measurements shared fusion;multiple sensors;multiple sensors considered;methods sensors;process observations;inject anomalous observations;sensors considered;methods sensors make;sensors make sequential;sensors;sensors considered framework;sensors make;process observations multiple;sequential measurements shared;cyber physical systems;shared fusion;secure remote;estimation linear gassian;systems internet things;applications methods sensors;sensors inject;sequential measurements", "pdf_keywords": "malicious sensor subset;attacks sensors attack;attack unknown sensor;detecting malicious sensor;attack detection sensors;attack safe sensor;detection attack propose;attacks sensors;malicious sensor;sensors attack;secure remote estimation;sensors attack time;secure estimation algorithm;detection attack;secure estimation;measure detection attack;fi attacks sensors;develop secure estimation;presence malicious sensor;detection localizing attack;attack detection;attack detection present;safe sensor subset;sensor subset detection;higher attack detection;attack detection probability;attack detection error;attack time algorithm;detection error free;method attack detection"}, "473021db54cbae9c4546597cd7e4b5d687a51c7f": {"ta_keywords": "training data crowdsourcing;data crowdsourcing;data crowdsourcing vital;workers crowdsourcing platforms;crowdsourcing;crowdsourcing platforms;crowdsourcing vital;resultsthe workers crowdsourcing;workers crowdsourcing;voting incentives crowddsourcing;incentives crowddsourcing methodsthe;incentives crowddsourcing;crowdsourcing platforms experts;crowdsourcing vital tool;crowddsourcing methodsthe;crowddsourcing;crowddsourcing methodsthe growing;backgroundappapproval voting incentives;incentives;voting incentives;systems incentives;labeled training data;current systems incentives;labeled training;training data;machine learning;developing machine learning;machine learning applications;backgroundappapproval voting;platforms experts making", "pdf_keywords": "incentive mechanism crowdsourcing;mechanism crowdsourcing;mechanism crowdsourcing simple;crowdsourcing approval voting;partial knowledge crowdsourcing;learning quality crowdsourced;mechanism crowdsourcing using;crowdsourcing mechanism attractive;developed mechanism crowdsourcing;training data crowdsourcing;crowdsourcing approval;crowdsourcing shown mechanism;develop mechanism crowdsourcing;crowdsourcing mechanism;labeled data crowdsourcing;crowdsourcing using approval;method crowdsourcing;setting crowdsourcing approval;knowledge crowdsourcing;data crowdsourcing mechanism;inexpensive method crowdsourcing;quality crowdsourced;crowdsourcing developed mechanism;crowdsourced labels;method crowdsourcing shown;quality crowdsourced labels;quality crowdsourcing;experts incentives;crowdsourced;crowdsourcing simple inexpensive"}, "042959b54176ad2c4f9d0966490ec407b6057527": {"ta_keywords": "federation learning fundamental;federation learning;learning framework;learning systems recently;learning systems;development learning systems;learning implementation;approach learning implementation;new approach learning;federation;development learning;approach learning;learning implementation context;learning;step development learning;context learning framework;learning fundamental;framework;learning fundamental step;implementation context learning;systems;development;fundamental step development;step development;systems recently;new approach;context learning;implementation context;implementation;identified new approach", "pdf_keywords": ""}, "90dd676184a796e3e5835c8e1f6a632985ce3e89": {"ta_keywords": "etiology etiology;etiology;etiology etiology poorly;etiology pathogenesis etiology;etiology pathogenesis;etiology poorly understood;pathogenesis etiology;development etiology pathogenesis;development etiology;etiology poorly;pathogenesis;model development etiology;development;development new model;model development;development new;poorly understood;new model development;poorly understood report;new;understood report development;understood report;report development new;report development;model;new model;report;understood;poorly", "pdf_keywords": ""}, "80cb8981af401d9e4df0096626553c514d9e6600": {"ta_keywords": "pyrochlore patient pyrochlore;case patient pyrochlore;patient pyrochlore like;patient pyrochlore;pyrochlore patient;pyrochlore like pyrochlore;like pyrochlore patient;pyrochlore like;pyrochlore;like pyrochlore;like pyrochlore like;report case patient;case patient;report case;patient;case;report;like", "pdf_keywords": ""}, "12b12ea73652da56023e0e4776211e4f4301f339": {"ta_keywords": "argumentation mining web;argumentation mining;introduction argumentation mining;argumentation mining function;annotation scheme argumentation;argumentation theory;argumentation theory applied;fits argumentation theory;scheme argumentation mining;argue annotation;argue annotation scheme;argumentation;paper argue annotation;introduction argumentation;fits argumentation;annotation studies;documents forums comments;methodsin annotation studies;web methodsin annotation;size fits argumentation;english documents forums;mining web information;annotation;annotation scheme;documents forums;editorials web;annotation studies experiment;task requirements corpus;requirements corpus;comments blogs", "pdf_keywords": ""}, "77b919c4f4f37415d8f1019b1b04191d46de426c": {"ta_keywords": "proximity queries labeled;recommendations retrieval;recommendations retrieval tasks;represented proximity queries;proximity queries;retrieval models based;retrieval models;walk based proximity;queries labeled directed;query execution retrieval;variety recommendations retrieval;based proximity measures;retrieval;queries labeled;random walk based;retrieval tasks represented;execution retrieval models;retrieval tasks;proximity measures recent;constrained random walks;random walks methodsa;nodes representing documents;labeled directed graph;random walks;backgroundfast query;proximity measures;walks methodsa;execution retrieval;based proximity;metadata labeled edges", "pdf_keywords": ""}, "e862e5f9a17938f1817017b2730e10463d94fb54": {"ta_keywords": "analyzing etiology disease;disease identify etiology;identify etiology disease;etiology disease identify;etiology disease;etiology disease able;identify etiology;disease form etiology;etiology disease recently;analyzing etiology;method analyzing etiology;etiology;form etiology disease;diagnosis disease;diagnosis disease disease;disease identify;disease;disease disease;disease disease form;disease able achieve;disease form;patient diagnosis disease;form etiology;diagnosis;patient diagnosis;disease recently shown;shown patient diagnosis;disease recently;disease able;achieve good outcome", "pdf_keywords": ""}, "2826ac3621fdd599303c97cb9e32f165521967b2": {"ta_keywords": "expert disagreements;high expert disagreements;disagreements patient diagnosis;disagreements human experts;expert disagreements particular;machine learning medicine;doctor disagreements;doctor disagreements patient;disagreements patient;machine learning;trained uncertainty;disagreements human;issue disagreements human;human experts ubiquitous;trained uncertainty scores;models trained uncertainty;disagreements particular identify;machine learning models;human experts;corresponds doctor disagreements;disagreements;disagreements particular;issue disagreements;result high expert;experts ubiquitous;work machine learning;experts ubiquitous machine;uncertainty scores;uncertainty;uncertainty scores data", "pdf_keywords": "uncertainty classification bias;useful predicting disagreement;predicting label disagreement;classification uncertainty;predicting disagreement;predict label disagreement;uncertainty prediction superior;classification uncertainty medical;uncertainty classification;predicting disagreement mixtures;uncertainty classification demonstrate;uncertainty classification uncertainty;models trained uncertainty;making uncertainty prediction;uncertainty prediction medical;uncertainty uncertainty classification;disagree outperform classifier;trained uncertainty;disagreewe machine learning;direct uncertainty prediction;classification bias;trained uncertainty scores;expert disagreement ability;expert disagreement direct;uncertainty medical opinions;uncertainty classification uvc;uncertainty prediction;high disagreement labels;medical opinion models;dataset direct uncertainty"}, "a7f30bae9303825adbc333a8df8a03398dea5151": {"ta_keywords": "rules sentiment classification;logic rules sentiment;rules senstiment classification;sentiment classification;sentiment classification models;senstiment classification;senstiment classification methodswe;logic rules senstiment;different sentiment classification;sentiment classification ineffective;rules sentiment;classification models syntactically;rules senstiment;instead logic rules;coding logic rules;logic rules;explicit logic rules;embeddings instead logic;sentences resultsthe distillation;inputs like sentences;performance different sentiment;sentiment;senstiment;instead logic;syntactically complex inputs;models syntactically complex;coding logic;classification;classification models;classification methodswe", "pdf_keywords": "human entity recognition;entity recognition reported;entity recognition;entity recognition previously;named entity recognition;logic rules sentimentwe;rules sentiment classification;word embedding model;sentiment classification complex;sentiment classification models;classification complex sentences;classification models syntacticallycomplex;sentiment sentences common;sentiment classification;rules sentimentwe;sentences lmo algorithm;sentiment sentences;sentiment classification systems;different sentiment classification;sentences common feature;word embedding;sentimentwe;logic rules sentiment;inputs like sentences;contextualized word embedding;learns logic;learns logic rules;rules sentimentwe present;feed contextualized embeddings;performance different sentiment"}, "203da29a37a983c487ce75a894b0d70698077bf5": {"ta_keywords": "disinformation recent electoral;problematic information contemporary;problematic content;problematic information;disinformation;spread problematic information;played disinformation;false content bad;lists problematic content;information contemporary media;content bad actors;detecting false content;media ecosystems attempts;disinformation recent;contemporary media ecosystems;played disinformation recent;contemporary media;false content;electoral processes intrinsic;media ecosystems;electoral processes;recent electoral processes;content bad;media;role played disinformation;spread problematic;content;widespread concern;detecting false;electoral", "pdf_keywords": ""}, "7f588b1d2a5b199a19a4c3bad6bd5154c7355817": {"ta_keywords": "premodification immunomagnetic beads;protein premodification immunomagnetic;performance immunomagnetic beads;immunomagnetic beads;immunomagnetic beads ims;premodification immunomagnetic;enrichment performance immunomagnetic;performance immunomagnetic;immunomagnetic;beads ims blood;blood proteins;environment blood proteins;ims blood samples;protein corona generally;controlled protein premodification;protein premodification;blood proteins endows;protein corona;formed protein corona;proteins endows composites;beads ims;situ formed protein;ims blood;proteins;beads ims diverse;formed protein;controlled protein;protein;blood samples;blood samples usually", "pdf_keywords": ""}, "44aa9a79cfc9eef9ac3f861cfa58a172cb863bd2": {"ta_keywords": "malignant neoplasm gastrointestinal;neoplasm gastrointestinal;neoplasm gastrointestinal tract;gastrointestinal tract malignant;tract malignant neoplasm;malignant neoplasm;diagnosed malignant neoplasm;tract malignant;diagnosed malignant;patient diagnosed malignant;malignant;neoplasm;gastrointestinal tract patient;gastrointestinal;case patient diagnosed;gastrointestinal tract;patient diagnosed;diagnosed;present case patient;tract patient;tract patient treated;patient treated combination;case patient;patient treated;tract;article present case;patient;combination;treated combination;combination combination", "pdf_keywords": ""}, "aeffb61024e5ccac5021ca0bf9d199d9196a0521": {"ta_keywords": "constraints population players;population players stochastic;congestion game framework;md congestion game;congestion game;constraints players population;coupled congestion costs;congestion costs;toll value constraint;congestion costs existing;dynamics coupled congestion;players stochastic dynamics;congestion;compute minimum toll;coupled congestion;markov decision process;enforcing tolls compute;minimum toll value;tolls compute minimum;players stochastic;players population distribution;distribution constraints population;md congestion;population distribution constraints;constraints population;process md congestion;satisfied enforcing tolls;markov decision;minimum toll;tolls compute", "pdf_keywords": "analysis congestion games;congestion games dynamics;congestion game;multidimensional congestion game;congestion pricing optimization;congestion games;mpm congestion game;process congestion games;congestion games unknown;congestion transportation networks;congestion game mpm;congestion transportation;congestion pricing;congestion game unknown;reducing congestion transportation;dynamics congestion costs;dynamics congestion;game unknown congestion;tolled mpm congestion;congestion minimizing;identical dynamics congestion;analysis congestion;driver congestion;increasing congestion costs;game tolled game;congestion costs method;oracle tolled game;approve congestion pricing;constraints multidimensional congestion;reduce driver congestion"}, "dc1d1f64503578d9c5d906da4556f631d4178b04": {"ta_keywords": "vehicle collision prediction;collision prediction;collision prediction algorithms;vehicle collision;large accident data;today vehicle collision;cnn;large collision data;developments deep learning;deep learning;cnn architectures;deep learning impossible;collision data real;modern cnn architectures;accident data;accident data set;collect large collision;based modern cnn;collision data;modern cnn;collect large accident;game namedgtrav;using accident data;video game namedgtrav;large collision;collision;efficient prediction;prediction;develop efficient prediction;large accident", "pdf_keywords": ""}, "de5057c1da9391269e926d4661d4558072db9f18": {"ta_keywords": "speech recognition parallel;fusion based attention;stream level fusion;information fusion;stream paradigm audio;information fusion previous;recognition parallel encoders;multi stream;multi stream paradigm;speech recognition;automatic speech;fusion based;recognition parallel;sources simultaneously;automatic speech recognition;introductionthe multi stream;stream paradigm;parallel encoders aim;parallel encoders;paradigm audio processing;fusion;fusion previous study;fusion previous;audio processing;level fusion based;end automatic speech;audio processing sources;sources simultaneously considered;processing sources simultaneously;paradigm audio", "pdf_keywords": "speech data multi;stream paradigm audio;streams acoustic arrays;speech recognition parallel;multi stream context;based streams acoustic;multi stream;multi stream paradigm;data speech;streams acoustic;multi stream end;optimizing speech data;data multi stream;improve multi stream;available data speech;speech data;stream level fusion;implementation multi stream;multi encoder multi;training optimizing speech;paradigm audio processing;stream context;introductionthe multi stream;analysis multi stream;multi encoder;encoder multi array;stream paradigm;fusion based attention;multi stream model;audio processing"}, "281605579936538ee92bc4b0baad1b83c683c076": {"ta_keywords": "language generation arrhythmia;parsing generation;representationation parsing generation;parsing generation task;meaning representationation parsing;generation task semeval;language generation;abstract meaning representationation;representationation parsing;generation task;meaning representationation;parsing;cast language generation;generation arrhythmia;generation arrhythmia arr;representationation;semeval 2017 task;task semeval;task semeval 2017;arrhythmia arr sequence;semeval 2017 abstract;arrhythmia;describes;arrhythmia arr;2017 task subtask2;task subtask2;language;subtask2;generation;semeval 2017", "pdf_keywords": ""}, "4ae0c4a511697e960c477ea3e37b3e11bf3e0e02": {"ta_keywords": "domain adaptation;benchmark domain adaptation;convolutional networks penalizing;domain adaptation tasks;training robust convolutional;convolutional networks;robust convolutional networks;training robust;gleaned local receptive;local receptive fields;receptive fields;receptive fields rely;representations learned;local receptive;local representations learned;robust convolutional;networks penalizing predictive;learned earlier layers;method training robust;networks;layers intuitively networks;intuitively networks;synthetic benchmark domain;convolutional;improved generalization;confers improved generalization;adaptation;penalizing predictive;receptive;adaptation tasks", "pdf_keywords": "adjversarial regularization;adversarial regularization;adjversarial regularization par;wise adjversarial regularization;patch wise adjversarial;wise adversarial regularization;adversarial regularization technique;adjversarial;domain invariance adversarial;training robust convolutional;patch wise adversarial;adversarial learning;convolutional networks penalizing;adversarial;invariance adversarial learning;invariance adversarial;domain generalization adaptation;robust convolutional networks;training robust;wise adjversarial;domain adaptation visual;generalization adaptation;adversarial learning apply;imagenet classification;improved regularization;efficiently improved regularization;global concepts classifying;wise adversarial;domain adaptation;classifying objects penalizing"}, "ce4db7a32724e0abc8afe27f74d33e32e099b8e6": {"ta_keywords": "porcine fit1 gene;porcine fit1 coding;fit1 gene;agonist porcine fit1;porcine fit1;upstream porcine fit1;fit1 gene interacting;fit1 gene human;role fit1 gene;diseases correlations pork;box myogenesis aims;fit1 coding;myogenesis aims;myogenesis aims gain;fit1 coding sequence;gene;novel agonist porcine;correlations pork meat;myogenesis;myod novel agonist;box myogenesis;gene human diseases;agonist porcine;canonical box myogenesis;fit1;gene interacting;gene human;correlations pork;porcine;upstream porcine", "pdf_keywords": ""}, "7506626f776f211afac2c2d1138aca0e0479e5c3": {"ta_keywords": "gans transform latent;contextgenerative adversarial networks;networks gans transform;contextgenerative adversarial;networks gans;images generated thegan;gans transform;generated thegan precisely;adversarial networks gans;generated thegan;thegan precisely recover;gan formulation;gans;original gan formulation;gan formulation gives;original gan;projecting images latent;images latent space;gan;thought original gan;thegan precisely;images latent;adversarial networks;adversarial;stochastic clipping;thegan;latent vectors visually;precisely recover latent;visually plausible images;transform latent vectors", "pdf_keywords": "networks gans transform;gans transform latent;networks gans;backgroundgenerative adversarial;backgroundgenerative adversarial networks;gans transform;adversarial network inverted;images generated gan;adversarial networks gans;generated gan precisely;gan precisely recover;generative adversarial network;cnn reconstructions;generative adversarial;discriminative cnn reconstructions;generated gan;noise generative adversarial;gan precisely;gans;robust noise generative;gan formulation;reconstructions images discriminative;gan formulation gives;original gan formulation;original gan;adversarial network;adversarial networks;adversarial;noise generative;gan"}, "712cd873d7370db280f4ceaaf000dc49f76b59fe": {"ta_keywords": "assessing robustness sequence;sequence sequence models;sequence models;sequence models perturbations;robustness sequence sequence;robustness sequence;alpha aggs considered;alpha aggs;aggs considered effective;aggs considered;aggs;background alpha aggs;sequence sequence;sequence;assessing robustness;robustness;example untarget;ignored evaluations growing;way assessing robustness;models perturbations indicate;largely ignored evaluations;models perturbations;weaknesses model change;indicate weaknesses model;ignored evaluations;input significantly legitimately;perturbations indicate weaknesses;weaknesses model;input significantly;using example untarget", "pdf_keywords": "generating adversarial perturbations;adversarial perturbations compared;adversarial perturbations word;generating adversarial;preserving adversarial perturbations;adversarial perturbations;improves robustness adversarial;adversarial training improves;evaluating adversarial;adversarial training;preserving adversarial;evaluating adversarial attacks;meaningful preserving adversarial;adversarial perturbations neural;method generating adversarial;method evaluating adversarial;adversarial;framework adversarial perturbations;evaluation framework adversarial;robustness adversarial;adversarial attacks beneficial;robustness adversarial attacks;beneficial robustness adversarial;adversarial attacks;abstractadversarial examples perturbations;assessing robustness sequenceto;target adversarial training;framework adversarial;adversarial attacks does;robustness sequenceto sequence"}, "1c8d9d5558dc43f3505fa37fc50247e3ce0d2f54": {"ta_keywords": "graph unobserved confounder;confounder average treatment;unobserved confounder;unobserved confounder average;confounding graph unobserved;treatment effect identifiable;confounder average;confounder;standard confounding graph;deconfounded data;causal graph;graph unobserved;properties causal graph;collect deconfounded data;practitioner collect deconfounded;confounding graph;standard confounding;deconfounded data run;effect identifiable estimate;treatment effect;causal graph render;effect identifiable;run clinical trial;estimate ate practitioner;average treatment effect;data run clinical;elucidate properties causal;unobserved;causal;clinical trial", "pdf_keywords": "graph unobserved confounder;confounders confounded deconfounded;unobserved confounder;confounders confounded;incorporating confounded data;confounders present;distribution confounders confounded;confounders;demonstrate confounded data;confounded data useful;confounders applied;unobserved confounder average;data demonstrate confounded;multiple confounders applied;multiple confounders;confounded data used;confounded data;confounded data present;confounded data necessarily;confounder;confounder revealing;applied multiple confounders;confounder average treatment;confounder apply methods;selectively deconfounded data;confounder apply;confounders offers maximal;distribution confounders present;fact confounded data;confounders present method"}, "4d86b32ea80e2d9df2283fac39892d6dbd87ea87": {"ta_keywords": "minimum error classification;pattern recognition minimum;minimum errorror classification;recognition minimum classification;minimum classification;discriminative training methods;classification errorror training;error classification;errorror classification;discriminative training;pattern recognition;recognition minimum;geometric margin minimum;minimum classification errorror;error classification various;errorror training especially;methods pattern recognition;large geometric margin;increased discriminative training;classification;errorror training;margin minimum errorror;training methods pattern;errorror classification recent;classification tasks derive;margin minimum;classification tasks;wide range classification;minimum error;range classification", "pdf_keywords": ""}, "8786ddc38ae0763e772337bf9331436252452918": {"ta_keywords": "detect fake news;news detect fake;fake news detection;fake news increasingly;existing fake news;entity bias real;fake news future;society fake news;entity bias;dissemination fake news;unintended entity bias;bias real world;model past news;past news detect;news detect;news detection aims;fake news;detect fake;news detection methods;news future;news increasingly;news pieces 2010;bias real;news detection;wide dissemination fake;example 97 news;news pieces;news increasingly threatening;dissemination fake;news future great", "pdf_keywords": "entity bias training;novel entity debiasing;entity bias existing;reduce entity bias;entity debiasing framework;entity debiasing;entity bias introduce;entity bias;entities news veracity;mitigates entity bias;propose entity debiasing;entity bias enhances;detect fake news;entity bias real;unintended entity bias;overlooked entity bias;entities perform debiased;news detect fake;introduce debiasing framework;debiasing framework fake;present debiasing framework;fake news detectors;detection false news;fake news detection;debiasing framework convenient;debiasing framework based;debiasing framework;framework fake news;fake news detector;designed debiasing framework"}, "9d0e4e9c9343b85311b1adff145fdbdfb69486ff": {"ta_keywords": "adolescent adolescent structural;adolescent structural;structure adolescent adolescent;structural features adolescence;adolescent structural features;structure adolescent;new structure adolescent;features adolescence adolescence;features adolescence;adolescent adolescent adolescent;adolescence adolescence;adolescent adolescent;adolescence;adolescence adolescence review;adolescent;adolescence review;adolescence review literature;structure;structural features;structural;structural features new;new structure;features new structure;literature;features;features new;new;review literature;review", "pdf_keywords": ""}, "cc74ef901219dfd26efbbb8b7b87d1b7b7d38634": {"ta_keywords": "historical texts neural;introductionnormalization historical texts;texts neural network;study historical texts;historical texts;historical texts presented;texts neural;introductionnormalization historical;neural network models;use neural network;introductionnormalization;modern modern methods;use neural;texts;modern methods;neural network;modern methods methodsa;models modern methods;texts presented;computerlinguistischer methodon;study historical;computerlinguistischer methodon nonlp;texts presented histoological;application computerlinguistischer methodon;extensive schreibvarianten gekennzeichnet;methodsa comprehensive;schreibvarianten gekennzeichnet application;network models modern;gekennzeichnet application computerlinguistischer;histoological documente", "pdf_keywords": ""}, "cd06dfa789bfe491130ac7440e55d9d407396a43": {"ta_keywords": "randomized coordinate descent;coordinate descent;coordinate descent rcd;acceleration strategy forrd;variants randomized coordinate;randomized coordinate;dimensions variants randomized;methods solving optimization;optimization;acceleration strategy;descent rcd;optimization problems big;method improves interpolates;extra directions sampled;coordinate directions spectral;type acceleration strategy;directions sampled;solving optimization;solving optimization problems;descent rcd paper;forrd based augmentation;variants randomized;optimization problems;improves interpolates;directions spectral;directions sampled rate;big dimensions variants;set coordinate directions;augmentation set coordinate;randomized", "pdf_keywords": "randomized coordinate descent;stochastic descent eigenvectors;descent eigenvectors stochastic;stochastic coordinate descent;stochastic descent methods;methods stochastic descent;descent methods optimal;spectral coordinate descent;stochastic spectral descent;method stochastic descent;optimal matrices rate;coordinate descent methods;stochastic descent;stochastic descent uniformly;optimal matrices uniform;stochastic descent obtains;descent methods uniform;rate stochastic descent;stochastic descent isond;optimal matrices;spectral descent;coordinate descent;descent eigenvectors;probabilities optimal matrices;descent methods;stochastic descent rateo;stochastic conjugate descent;eigenvectors stochastic;uniform distribution eigenvectors;matrix performance boost"}, "aacaad6ab396e085799052b1a667c965d6465e32": {"ta_keywords": "stabilized emulsions ethylmaleimide;protein stabilized emulsions;emulsions ethylmaleimide nezm;emulsions ethylmaleimide;stabilized emulsions;stabilized emulsions methods;emulsions methods investigate;emulsions;disulfide bonds sulfhydryl;emulsions methods;role disulfide bonds;blocked ethylmaleimide properties;sulfhydryl groups disulfide;disulfide bonds;sulfhydryl blocked ethylmaleimide;disulfide bonds different;groups disulfide bonds;ethylmaleimide properties;protein stabilized;ethylmaleimide properties different;different protein stabilized;disulfide;role disulfide;bonds sulfhydryl blocked;groups disulfide;ethylmaleimide nezm;bonds sulfhydryl;background role disulfide;blocked ethylmaleimide;ethylmaleimide", "pdf_keywords": ""}, "d5810f15cfdd59da549ffa648c5a05d806d94eb7": {"ta_keywords": "automated fact checking;fact checking platform;fact checking;textual evidence;automated fact;present automated fact;textual evidence document;relevant textual evidence;efforts combating misinformation;combating misinformation paper;combating misinformation;introduction factor checking;evidence document collection;factor checking essential;factor checking;retrieves relevant textual;journalism importance highlighted;essential task journalism;task journalism;journalism;journalism importance;textual;evidence supports refutes;evidence document;task journalism importance;relevant textual;verdict methods;verdict methods architecture;final verdict methods;evidence supports", "pdf_keywords": "fact checking platform;automated fact checking;evidence sentences retrieved;claims identifying sentences;fact checking dataset;fact checking essential;textual evidence;claim sentence embeddings;fact extraction;fact checking;evidence sentences;natural language inference;accuracy predicting evidence;evidence sentences claim;sentences providing evidence;textual evidence document;claim accuracy predicting;released fact checking;predicting evidence;background fact checking;predicting evidence supports;relevant textual evidence;retrieves relevant textual;evidence large document;evidence document collection;fact extraction virification;dataset fact extraction;selected evidence sentences;supporting refuting related;sentences retrieved"}, "a5148776955ef523de318a2fb45f8256e966b98e": {"ta_keywords": "distant labeling information;polysemy distant labeling;supervision information extraction;backgroundimproving distant supervision;labeling information extraction;distant supervision information;distant labeling;aimimifying distant supervision;nodes entity mentions;labeling information;label propagation lists;label propagation;using label propagation;label propagation graph;entity mentions;entity mentions mentions;distant supervision;information extraction;use polysemy distant;information extraction using;lists aimimifying distant;supervision information;information extraction leads;labeling;extraction using label;graph nodes entity;polysemy distant;propagation lists aimimifying;mentions mentions coupled;nodes entity", "pdf_keywords": ""}, "05c2bb89a5c42ad7932420bb39df2e566df6e1ec": {"ta_keywords": "annotation data;annotation data manually;annotated training data;annotators highly;annotators;annotated training;annotation;annotators highly timeconsuming;processing natural language;natural language staochastic;non natural language;natural language processing;annotated;manually team annotators;natural language;team annotators highly;language processing;lot annotated training;team annotators;language staochastic methods;data cases annotation;friendly editor;cases annotation data;user friendly editor;stochastic processing natural;cases annotation;heading stochastic processing;processing heading stochastic;language staochastic;lot annotated", "pdf_keywords": ""}, "2127bea25859ba9c5997e2d15e17899a75ef6cb3": {"ta_keywords": "code dagstuhl seminar;big code dagstuhl;programming big code;programming big;big code resultsa;program outcomes dagstuhl;introductionprogramming big code;seminar 15472 programming;code dagstuhl;big code methodsthis;15472 programming big;introductionprogramming big;big code;dagstuhl seminar;availability millions programs;dagstuhl seminar 15472;programs open source;outcomes dagstuhl seminar;millions programs;millions programs open;code methodsthis report;programming;code resultsa term;code resultsa;15472 programming;programs;program;open source repositories;source repositories;code methodsthis", "pdf_keywords": ""}, "3e2bac2abfb5b33a43fe56db5a868e17e38c616a": {"ta_keywords": "studentprojects teaching machine;projects teaching machine;teaching machine learning;teaching machine;student projects teaching;student projects;studentprojects;studentprojects teaching;projects teaching;classes master students;performance computing classes;students participate projects;learning high performance;role student projects;high performance computing;master students;classes students;machine learning high;computing approach teaching;students;performance computing role;approach teaching machine;classes master;performance computing;computing classes master;student;theoretical classes students;master students ural;introductionthe role studentprojects;role studentprojects", "pdf_keywords": ""}, "baf47cd0b471a9bb7b2230fec0b680fc9b3c4783": {"ta_keywords": "pragmatic models generation;pragmatic inference aids;introductionunified pragmatic models;explicit pragmatic inference;pragmatic models;pragmatic inference;natural language instructions;explicit pragmatic;introductionunified pragmatic;language instructions complex;pragmatic;objectivewe explicit pragmatic;instructions complex;instructions complex sequential;inference aids correctly;methodswe explicit pragmatic;generation following instructions;inference aids;following natural language;resultswe explicit pragmatic;natural language;complex sequential tasks;sequential tasks methodswe;models generation following;language instructions;models generation;sequential tasks;generation following;sequential tasks resultswe;generating following natural", "pdf_keywords": "pragmatic models generation;step linguistic tasks;instruction generation incorporating;semantic parsers attentional;pragmatic inference aids;language instructions complex;natural language instructions;sequential tasks pragmatics;instruction following generation;parsers attentional;instruction generation;unified pragmatic models;parsers attentional neural;pragmatic models;explicit pragmatic inference;models pragmatic reasoning;following instruction generation;instructions complex sequential;pragmatic inference;pragmatics improves performance;interrelated tasks instruction;generate instructions;pragmatics improves;previous pragmatic models;contexts pragmatics improves;pragmatic reasoning extended;capable reasoning sequential;multi step linguistic;pragmatic inference procedure;linguistic tasks"}, "00cc6deb3cf2c9281ddcf4875aad3ee14c92e52f": {"ta_keywords": "lingual named entity;translating entities matching;translating entities;subsequently translating entities;named entity recognition;approach cross lingual;entity recognition developed;entity recognition;cross lingual named;leveraging machine translation;translation systems;translation systems twice;machine translation systems;cross lingual;lingual named;machine translation;entities matching;entities matching entities;entity projection methods;entity projection;entities based orthographic;matching entities;systems twice translating;named entity;matching entities based;lingual;sentences subsequently translating;prior entity projection;translating sentences subsequently;orthographic phonetic similarity", "pdf_keywords": "lingual translation entities;leveraging machine translation;translation entities english;translation entities;translation annotated corpus;annotated corpus translation;lingual named entity;large annotated corpora;entities exploit corpus;translation improve annotationprojection;corpus translation annotated;named entity recognition;annotated corpus target;entity recognition;machine translation improve;annotated corpora;corpora named entity;machine translation systems;entity recognition methods;corpus target language;machine translation;machine translation non;mapping annotated corpus;neural machine translation;lingual noncoding ner;annotated corpora named;ner annotation projection;translation annotated;entities target language;ner annotation"}, "be360de73689dc4af56f7adcee7e38d7acfed1e1": {"ta_keywords": "aggregation ranked preferences;aggregation ranked;ranked preferences;list aggregation ranked;ranked preferences consider;orderings ranksings;set orderings ranksings;aggregate orderings;aggregate orderings maintain;aggregate ordering;set aggregate orderings;just aggregate ordering;ranksings smaller set;orderings ranksings smaller;ranksings;aggregate ordering work;certain properties fairness;fairness conclusions classical;introduction list aggregation;orderings maintain certain;properties fairness conclusions;set orderings;ranksings smaller;orderings;aggregation;list aggregation;properties fairness;ranked;fairness conclusions;smaller set aggregate", "pdf_keywords": ""}, "0f5bb9ae0c060b349597c0b2582bf271a5a2156a": {"ta_keywords": "supertagging lstms fundamental;supertagging lstms;supertagging parsing model;supertagging parsing;crg supertagging parsing;lstms pos tagging;tagging models encode;supertagging;crg supertagging;performance crg supertagging;tagging models;bidirectional lstms;bidirectional lstms pos;pos tagging models;lstms;lstms fundamental;parsing model outperforms;compete bidirectional lstms;lstms fundamental phenomenon;lstms pos;tagging;parsing model;neural models;pos tagging;neural;parsing;neural models demonstrate;performance neural models;models encode;performance neural", "pdf_keywords": ""}, "ce268e0942ce0d1f6942e4d7e7e2aa6464f1b577": {"ta_keywords": "automatically learn pronunciation;learn pronunciation lexicon;handcrafted pronunciation lexicons;pronunciation lexicon iterative;non native pronunciations;native pronunciations design;pronunciation lexicon;pronunciation lexicons;learn pronunciation;ar handcrafted pronunciation;pronunciation lexicons used;non native speech;handcrafted pronunciation;native pronunciations;automatic speech recognition;pronunciations design new;significantly native speech;pronunciations design;performance automatic speech;native speech;automatic speech;native speech differs;speech recognition ar;speech recognition;pronunciations;pronunciation;native speech resulting;non native;lexicon iterative;automatically learn", "pdf_keywords": ""}, "cb53f9558bd13c853026f97dce3bbe3d989ca97d": {"ta_keywords": "game learning argumentation;argumentation controversies culture;argumentation controversies;learning argumentation fallacies;introduction argumentation controversies;daily argumentation language;argumentation language;daily argumentation;deals daily argumentation;argumentation fallacies methods;argumentation fallacies;learning argumentation;argumentation;introduction argumentation;argumentation language requires;controversies culture language;argotario game learning;argotario game;controversies culture;game platform topic;controversies;game learning;fallacies methods examine;fallacies methods;game platform;argotario;successful game platform;platform topic selection;language requires substantial;deploying argotario game", "pdf_keywords": ""}, "57dd2bd5fb6677191f9b36b589c91bb171e217ff": {"ta_keywords": "genomes genomes genomes;web integral genome;genomes data;genomes genomes;analysis genomes data;analysis genomes;genomes;shown genomes genomes;genomes data recently;shown genomes;genome;tool analysis genomes;recently shown genomes;genome useful tool;integral genome;genome useful;integral genome useful;web integral;web;data;analysis;data recently;useful tool analysis;data recently shown;tool analysis;useful tool;tool;useful;recently shown;integral", "pdf_keywords": ""}, "a4dd375c18709b1554249cc5cb88d8ba6acfea10": {"ta_keywords": "use machine translation;machine translation finally;technology machine translation;machine translation;machine translation long;clinical messagethe use;use machine;translation finally;translation finally seeing;translation;clinical messagethe;key clinical messagethe;translation long;translation long held;technology machine;machine;clinical;key clinical;appearance computers;large scale use;advances technology machine;messagethe use machine;appearance computers years;computers;technology;computers years reached;scale use;forward machine;computers years;steps forward machine", "pdf_keywords": ""}, "a95400c70c4beb609c77cc500677b2f1ed852e8e": {"ta_keywords": "automated question generation;question generation improves;question generation;paraphrase detection generates;approach automated question;automated question;resolution paraphrase detection;answers using semantic;paraphrase detection;introductiongenerating questions multiple;coreference resolution paraphrase;language learners generating;choice answers using;sentences coreference resolution;multiple sentences coreference;sentences coreference;questions multiple choice;multiple choice answers;answers using;introductiongenerating questions;assessment perspective resultsour;semantic analysis text;questions multiple;learners generating;questions utilize specific;multiple choice questions;choice questions utilize;learners generating multiple;generating multiple choice;questions utilize", "pdf_keywords": ""}, "63d99a61e798d7cb714f336a8d581ae2b75672ee": {"ta_keywords": "speech representation learningcombining;deep cluster zerospeech;cluster zerospeech challenge;resource speech challenge;cc deep cluster;speech challenge 2021;speech challenge;zero resource speech;representation learningcombining mixing;predictive coding cc;coding cc deep;zerospeech challenge 2021;learningcombining mixing;conformer cc deep;contrastive predictive coding;deep cluster deep;cluster deep;representation learningcombining;speech representation;learningcombining mixing conformer;cluster zerospeech;introduction speech representation;zerospeech challenge;deep cluster;deep cluster prepare;cluster deep cluster;learningcombining;clustering outputs cc;predictive coding;cc deep", "pdf_keywords": "resource speech challenge;zero resource speech;speech challenge 2021;speech challenge;learning speech audio;supervised learning speech;speech audio;speech audio sample;learning speech;audio dataset;resource speech;model trained audio;phonetic metric reveals;unsupervised learning language;contraststive predictive coding;trained audio audio;phonetics lexicon syntax;phonetics lexicon;trained audio;metrics phonetics lexicon;using phonetic metric;phoneme discriminative representation;based speech audio;phonetic metric;learning language models;speech audio processing;speech recognition rapidly;phoneme discriminative;audio dataset 6k;deep cluster methodsin"}, "d4305b3bf233e5f192a5d17dde114b771b621d92": {"ta_keywords": "entrainment conversation key;lexical entrainment conversation;conversation key element;key element conversation;element conversation key;entrainment conversation;conversation key feature;conversation key;feature conversation key;lexical entrainment;key feature conversation;conversation;element conversation;feature conversation;lexical;key element;entrainment;key feature;element;key;feature", "pdf_keywords": ""}, "de3c3eb590065a6d78ec8566161f8236ab2a7435": {"ta_keywords": "workshop asian translation;asian translation wat2017;translation wat2017 comprehensive;patent translation subtasks;translation translation paper;asian translation;paper translation subtasks;translation wat2017 including;translation subtasks cj;translation paper presents;scientific paper translation;translation paper;tool translation translation;patent translation;paper translation;translation subtasks;translation translation;translation wat2017;ej patent translation;comprehensive tool translation;tool translation;translation subtasks hae;translation;workshop asian;4th workshop asian;jac scientific paper;wat2017 comprehensive comprehensive;wat2017 comprehensive;wat2017 including jae;domain subtasks je", "pdf_keywords": ""}, "ffb562d3ac7d86b5c527863f5a3e72e1aa22a809": {"ta_keywords": "prediction parametric agents;incentive mechanism prediction;information incentivize agents;agents private information;heterogeneous agents private;prediction parametric;parametric agents consider;incentivize agents;rational agents private;introductionparametric prediction parametric;incentivize agents different;introductionparametric prediction;agents private;parametric agents;incentive mechanism;prediction minimal cost;private information incentivize;prediction based opinions;elicit heterogeneous agents;information incentivize;incentive;agents consider problem;agents consider;prediction minimal;prediction based;mechanism prediction algorithm;private information making;prediction algorithm;prediction;opinions elicited heterogeneous", "pdf_keywords": ""}, "076b2ba158c35bd2941769864ce7455cf76ecd8e": {"ta_keywords": "peer review process;peer review;present peer review;review process;reviewing;biases present peer;review process design;reviewing papers;reviewing papers making;introductionper review;process responsible reviewing;responsible reviewing papers;responsible reviewing;cognitive biases;acceptance rejection decisions;cognitive biases important;introductionper review backbone;rejection decisions;various cognitive biases;rejection decisions given;human decision making;important understand biases;review backbone academia;review;biases important understand;human decision;making final acceptance;decisions given human;biases important;biases", "pdf_keywords": "review discussions herding;discussions herding behaviour;discussions herding;herding discussion phase;herding discussion;herding effect conference;discussion peer review;evidence herding discussion;reviewers participate discussion;reviewers begin discussion;herding outcome paper;evaluate effect herding;reviewers join discussion;peer review discussions;conference peer review;reviewers involved discussion;discussion reviewers;peer review discussion;reviewers discussions;herding outcome;random discussion increases;herding behaviour contribute;peer review process;herding behaviour;herding effect;effect herding outcome;evidence herding;review discussion reviewers;reviewers observations;reviewers discussions investigatethe"}, "80b747af8d86541cf53198519c8fa51109eed4f9": {"ta_keywords": "augmentation aud semisupervised;data augmentation aud;augmentation unsupervised data;data augmentation unsupervised;backgroundunsupervised data augmentation;augmentation aud;augmentation unsupervised;unsupervised data aumentation;aud semisupervised technique;aud semisupervised;data augmentation;backgroundunsupervised;backgroundunsupervised data;data aumentation gained;semisupervised;data aumentation;unsupervised;semisupervised technique applies;produced data augmentation;semisupervised technique;unsupervised data;aud;augmentation;aumentation;aumentation gained popularity;aumentation gained;classification;text classification open;observed unlabeled examples;text classification", "pdf_keywords": "augmentation text classification;classification sequence tagging;sequence tagging annotated;sequence tagging tasks;sequence tagging datasets;study sequence tagging;sequence tagging ability;datasets sequence tagging;augmentation unsupervised data;sequence tagging;data augmentation unsupervised;unsupervised data augmentation;augmentation unsupervised;random word replacement;word replacement surprisingly;tagging datasets reliable;randomly replace words;algorithm sequence tagging;tagging datasets demonstrate;tagging annotated text;words replacement;datasets use augmentation;augmentation text;tagging annotated;user sequence tagging;sequence tagging apply;naive data augmentation;word replacement;words words replacement;data augmentation methods"}, "b033400e9a80915a928f4603582e5e8bf7656a85": {"ta_keywords": "unimodal baselines multimodal;baselines multimodal;baselines multimodal domains;assessing performance multimodal;unimodal baselines;multimodal techniques results;performance multimodal;performance multimodal techniques;multimodal;modality performance visual;multimodal techniques;baselines argue unimodal;baseline single modality;visual navigation qa;modality performance;single modality performance;multimodal domains;performance visual navigation;strength unimodal baselines;unimodal approaches better;approaches better capture;visual navigation;baselines;visual;multimodal domains make;baseline;navigation qa demonstrate;unimodal approaches;performance visual;baseline single", "pdf_keywords": "multimodal navigation models;multimodal navigation;ability multimodal navigation;navigation unimodal environments;multimodal models demonstrate;multimodal counterparts;navigation unimodal;outperform multimodal counterparts;multimodal models;unimodal baselines multimodal;navigation models predict;multimodal;method navigation unimodal;models outperform multimodal;performance multimodal;baselines multimodal domains;outperform multimodal;assessing performance multimodal;using multimodal models;benchmarks unimodal models;visual navigation fidelity;baselines multimodal;performance multimodal techniques;visual navigation;navigation fidelity;ability multimodal;multimodal techniques;using multimodal;navigation models;datasets visual navigation"}, "c0099a15bd3251083c62ebd47c9705a16309b974": {"ta_keywords": "zero crossing image;multiscale zero crossing;crossing image representation;level early vision;crossing image;early vision;multiscale zero;zero crossing;image processing tasks;stable multiscale zero;multiscale;stable multiscale;optimize image processing;image processing;image representation;optimize image;used optimize image;image representation used;crossing;vision;level early;tasks level early;representation used optimize;processing tasks level;image;optimize;processing tasks;processing;used optimize;early", "pdf_keywords": ""}, "a16ae67070de155789a871cb27ecbf9eaa98b379": {"ta_keywords": "machine generated text;machine authored text;backgroundhuman evaluations typically;natural language generation;human machine authored;backgroundhuman evaluations;authored text gpt2;authored text;generated text;machine authored;language generation models;generation models fluency;machine generated;natural language;generated text objectivewe;language generation;assessing non experts;evaluations typically;standard natural language;evaluations;models fluency improves;evaluations typically considered;judge machine generated;experts ability distinguish;fluency improves evaluators;backgroundhuman;non experts ability;distinguish human machine;models fluency;generation models", "pdf_keywords": "humanlikeness generated text;assess humanlikeness text;humanlikeness text generated;different text evaluations;human authored text;text evaluations;usefulness generated text;authored text training;text generation experiments;authored text generate;human automatic evaluations;evaluators distinguish texts;natural language generation;models natural language;evaluators assess humanlikeness;human evaluation automatically;text world generative;text generated;machine generated text;human evaluations typically;language generation models;human evaluations studied;generated text important;human evaluations;generated text ability;evaluation automatically generated;article accuracy evaluators;human based text;evaluations non expert;assessments generated text"}, "ece56ab633f11d1592a3d4f9386412d3f48fcf95": {"ta_keywords": "argument reasoning comprehension;argument reasoning;reasoning comprehension task;reasoning comprehension;task argument reasoning;argument claim premise;introductionthe argument reasoning;objectivethe argument reasoning;challenging task argument;comprehension task;comprehension task new;comprehension task objectivethe;comprehension task methodsgiven;task argument;reasoning;argument;argument claim;comprehension;claim premise;task objectivethe argument;claims conclusiona solution;claim premise goal;conclusiona solution task;objectivethe argument;claims conclusiona;contradicting claims conclusiona;warrants plausible lexically;conclusiona solution;premise goal;challenging task", "pdf_keywords": "natural language argumentation;natural language arguments;natural language argument;reasoning natural language;language argumentation presented;arguments annotated;language argumentation;assessing argument reasoning;warrants natural language;argument reasoning user;argumentation common sense;arguments annotated stance;arguments annotated reasons;discussed argumentation;reasoning necessary argumentation;generated arguments annotated;reasoning comprehension crowdsourcing;arguments discussed argumentation;argumentation presented new;argument reasoning comprehension;argumentation presented;argumentation theory unsuitability;language argumentation neural;necessary argumentation common;argument reasoning fundamental;argument reasoning;language arguments discussed;argumentation common;argumentation scholars;discussed argumentation scholars"}, "2232808cf3161ca4c434126e35f47ee33c0c8219": {"ta_keywords": "explanations accuracy gains;explanations explanations teacher;backgroundevaluating explanations explanations;explanations teacher aid;explanations accuracy;explanations teacher;backgroundevaluating explanations;value explanations accuracy;explanations explanations;quantify value explanations;explanations;features aims explanations;explain predictions highlighting;student model trained;explanations serve;explain predictions;teacher model;explanations serve ought;aims explanations serve;predictions highlighting salient;aims explanations;simulate teacher model;predictions highlighting;student model;model trained;trained simulate teacher;value explanations;aid students methods;simulate teacher;teacher aid students", "pdf_keywords": "teaching explanations training;explanations learning student;use explanations learning;explanations training;learning learning explanations;explanations training attention;explanations learning;learning explanations;explanations learning use;teaching explanations;explanations classification tasks;learning explanations attention;learning use explanations;predictions student learner;prediction explanation generation;attention based explanations;explanations improve student;explanations question answering;students demonstrate explanations;methods teaching explanations;incorporating explanations results;explanations multitask learning;explanations attention;explanations classification;explanations student teacher;improvements incorporating explanations;producing explanations;explanations attention regularization;explanations accuracy gains;explanations student"}, "228f2efe7b06b6db3b2c6c0a61d7b33daee1d641": {"ta_keywords": "sense disambiguation based;word sense disambiguation;sense disambiguation methods;sense disambiguation;disambiguation based automatically;unsupervised word sense;disambiguation based;disambiguation methods;disambiguation methods given;disambiguation;sense target word;synset constituting sense;sentence synset;semantic similarity;introduction word sense;word sense;sense input word;unsupervised word;sentence synset constituting;semantic similarity given;nogoznal unsupervised word;given sentence synset;semantic;respect semantic similarity;constituting sense target;automatically induced synsets;chooses relevant sense;synset constituting;sense input;sense target", "pdf_keywords": ""}, "301352755a94d7524312b7c7f2fab7d3fd3d334d": {"ta_keywords": "conditional preferences probabilistic;preferences probabilistic uncertainty;qualitative conditional preferences;preferences probabilistic;conditional preferences;probabilistic uncertainty;probabilistic uncertainty introduce;qualitative conditional;model qualitative conditional;uncertainty introduce formalism;probabilistic;uncertainty;preferences;uncertainty introduce;pcp nets formalism;conditional;formalism model qualitative;model qualitative;qualitative;nets formalism model;nets formalism;introduce pcp nets;pcp nets;pcp;introduce pcp;formalism model;introduction introduce pcp;introduce formalism model;formalism;nets", "pdf_keywords": ""}, "035595ebf6821031a543ee1c30386a6230fc7a41": {"ta_keywords": "online speaker diarization;speaker diarization algorithm;speaker diarization;endnural diarization speaker;diarization speaker tracing;diarization speaker;online diarization;online speaker;methods online diarization;assign speaker regions;online diarization inherently;speaker tracing;speaker regions;diarization algorithm based;diarization algorithm;speaker permutation;assign speaker;speaker regions incorrectly;possibility assign speaker;speaker tracing buffer;novel online speaker;speaker permutation problem;presents speaker permutation;endnural diarization;diarization;end endnural diarization;diarization inherently;speaker;supervised self attention;diarization inherently presents", "pdf_keywords": "online speaker diarization;online diarization speech;diarization speech online;speaker diarization algorithm;speaker diarization;performance online diarization;speaker diarization using;approach speaker diarization;neural speaker diarization;method speaker diarization;online performing diarization;online online diarization;online diarization online;online diarization;recording wise diarization;propose speaker tracing;proposed speaker tracing;speaker diarization field;diarization online diarization;speaker tracing;online diarization used;speaker diarization implications;online diarization inherently;using speaker tracing;diarization speech;diarization online;called speaker tracing;heads online diarization;online diarization weighted;online speaker"}, "8328508dc12c295165f997e02d74d00a42971c01": {"ta_keywords": "semantic parsing context;parsing context methods;parsing context;parsing context received;typical context modeling;world semantic parsing;decoding semantic parser;semantic parsing;context modeling;semantic parser;context modeling methods;semantic parser adapt;context methods present;context methods;decoding semantic;parsing;recently semantic parsing;study context modeling;contextual;challenging complex contextual;complex contextual;parser;context received;context;based decoding semantic;contextual phenomena;world semantic;grammar based decoding;real world semantic;contextual phenomena previous", "pdf_keywords": ""}, "d36e39aedd802aea4be1ea303c70dc56e97dbc3c": {"ta_keywords": "abstractive summarization models;summarization models;evaluation metrics summarization;thefactual consistency summaries;consistency summaries methodspractice;consistency summaries;summaries methodspractice applications;abstractive summarization;summarization;summarization largely insensitive;summarization models limited;applications abstractive summarization;summarization largely;metrics summarization largely;summaries methodspractice;metrics summarization;summaries;frequent factual inconsistencies;existing automatic evaluation;automatic evaluation;factual inconsistencies respect;factual inconsistencies;introductionasing answering;kags automatic evaluation;automatic evaluation protocol;evaluate thefactual consistency;automatic evaluation metrics;thefactual consistency;frequent factual;limited frequent factual", "pdf_keywords": "accuracy summarization model;summarization models trained;summarization models;summarization develop quality;generated summaries summarization;summarization develop qgs;useful summarization model;model useful summarization;summarization model;summarization model develop;evaluation metrics summarization;evaluate accuracy summarization;metrics summarization model;summarization model useful;accuracy summarization;summarization natural language;model generated summaries;automatic summarization practical;abstractive summarization models;automatic summarization;evaluation tool summarization;automatic metrics summarization;tool automatic summarization;model abstractive summarization;summarization model abstractive;summarization datasets demonstrate;summarization metric measuring;approaches summarization;summarization models limited;abstractive summarization metric"}, "6c6975750207f787c318627ff7cb63a649165a8d": {"ta_keywords": "learn perceive visual;humans learn perceive;learn process displays;perceive visual displays;learn perceive;computers efficiently taught;grammar learner support;use statistical learning;visual displays;grammar learner;model humans learn;visual displays methods;efficiently taught understand;free grammar learner;learning;understand various displays;humans learn;learner support learning;learning model humans;efficiently taught;perceive visual;people learn read;people learn;understand use displays;humans learn process;statistical learning;existing probabilistic context;probabilistic context free;taught understand use;probabilistic context", "pdf_keywords": ""}, "1a671afdac8e7b759cf3b5ec7d03d485c76a989c": {"ta_keywords": "connectionist temporal classification;speech recognition ar;automatic speech recognition;end automatic speech;automatic speech;speech recognition;temporal classification cc;outputs connectionist temporal;temporal classification;connectionist temporal;recognition ar framework;recognition ar;autoregressive end end;non autoregressive end;autoregressive end;refining outputs connectionist;connectionist;outputs connectionist;classification cc;mask ct;novel non autoregressive;classification cc methods;end automatic;present mask ct;mask ct novel;autoregressive;end end automatic;non autoregressive;speech;sequence refining outputs", "pdf_keywords": "nonautoregressive speech recognition;masked speech recognition;autoregressive training decoding;training decoding mask;masked language model;speech recognition;target speech recognition;training decoding;automatic speech;decoding target speech;speech recognition model;speech recognition ar;recognition speech;nonautoregressive speech;tokens masked speech;mask decoding tasks;speech recognition task;connectionist temporal classification;recognition speech recognition;model masked language;trained predict masked;speech recognition fundamental;automatic speech recognition;predict masked tokens;developed nonautoregressive speech;language model masked;conditional masked language;end automatic speech;process speech recognition;masked speech"}, "38ff6cf441050a1db10df85ac0771ccc88dea748": {"ta_keywords": "conference peer review;peer review conference;strategyproof conference peer;peer review systems;day peer review;peer review;review conference setting;peer review methods;manipulate reviews strategic;consider peer review;reviews strategic manner;review conference;reviews strategic;strategyproof conference;conflicts reviewers manipulate;review systems;conference peer;setting conflicts reviewers;conflicts reviewers;review systems designed;reviewers manipulate;reviewers manipulate reviews;manipulate reviews;conflicts reviewers submissions;introduction strategyproof conference;submissions conflicts reviewers;reviewers submissions conflicts;rankings papers conclusion;reviewers submissions;influence final rankings", "pdf_keywords": "efficient peer review;peer review aggregation;strategyproofness peer review;strategyproof peer selection;peer review procedure;approach peer selection;conference peer review;peer selection order;peer selection;efficiency peer review;effectiveness peer selection;review aggregation algorithm;conferences peer review;review process peer;peer review propose;peer review;reviewers aggregating;representative peer review;partitioning papers reviewers;peer review process;strategyproof conference peer;approach peer review;peer selection general;strategyproof efficient peer;peer review conference;peer review fundamental;algorithm reviewer assignment;strategyproofness conference peer;reviewer assignment method;subset reviewers aggregating"}, "97ca917f66d60f5277651a74f233804b03cb5e3d": {"ta_keywords": "morphological segmentation russian;segmentation russian language;segmentation russian;morphological segmentation;task morphological segmentation;morphological;morpheme boundaries;russian language methodswe;russian language;morpheme boundaries beat;98 morpheme boundaries;task morphological;segmentation;morpheme;intelligenceligence natural language;convolutional neural networks;artificial intelligenceligence natural;russian;introduction artificial intelligenceligence;artificial intelligenceligence;problem 98 morpheme;language methodswe deep;98 morpheme;convolutional neural;deep convolutional neural;addresses task morphological;natural language present;language present paper;existing non neural;language present", "pdf_keywords": ""}, "49989dc4d77b9df775b284ab7682ba76c080be12": {"ta_keywords": "texture classification;implement texture classification;texture classification algorithm;texture classification using;texture classification methods;introduction texture classification;hidden markov models;hidden markov;course hidden markov;noncausal hidden markov;field texture classification;markov models hmms;models hmms;markov models;object classified classifiers;texture;classified classifiers;markov;classification;classifiers;markov models important;classification using noncausal;classification algorithms studied;markov models results;various classification algorithms;classification methods;classification algorithm;implement texture;classification algorithms;models hmms assume", "pdf_keywords": ""}, "51d735419392dbe961c60bff7eee95388b8d6d3d": {"ta_keywords": "labeled grammar induction;unsupervised grammar induction;grammar induction minimal;unlabeled dependency trees;labeled grammar;unsupervised grammar;grammar induction;grammar induction aims;trees gold speechtagged;induce unlabeled dependency;clean linguistic classes;minimal minimal supervision;work unsupervised grammar;clean linguistic;gold speechtagged text;speechtagged text clean;speechtagged text;unlabeled dependency;minimal supervision;minimal supervision reported;ago labeled grammar;dependency trees;dependency trees gold;text clean linguistic;gold speechtagged;induce unlabeled;speechtagged;induction minimal minimal;linguistic classes;unlabeled", "pdf_keywords": ""}, "b26ca2bb882c2d3526fb4ac7f544fb87c39ded62": {"ta_keywords": "gradient matching pursuit;matching pursuit method;pursuit method methods;pursuit method approximates;pursuit method approximation;kernel gradient matching;orthogonalization improved kernel;matching pursuit;pursuit method;kernel based classification;approximation technique kernel;vector orthogonalization improved;optimization kernel based;recognition developed kernel;kernel based probabilistic;sequential pattern recognition;efficient optimization kernel;basis vector orthogonalization;vector orthogonalization;technique kernel based;orthogonalization improved;pattern recognition;gradient matching;optimization kernel;pattern recognition developed;improved kernel gradient;conventional kernel gradient;orthogonalization;kernel based;technique kernel", "pdf_keywords": ""}, "2cd7c3ed5a06c461b259694376820dcfcfbe94a9": {"ta_keywords": "constituency parsing feasible;constituency parsing;results constituency parsing;external parsers decoding;parsers decoding tractable;decode directly generative;parsers decoding;discriminative neural models;parsers;external parsers;discriminative neural;directly generative models;parsing feasible search;introductiongenerative neural models;parsing feasible;used discriminative neural;parsing;directly generative;output external parsers;neural models recently;neural models;generative models;models enables decode;decoding;introductiongenerative neural;search used discriminative;neural;neural models enables;generative;decoding tractable", "pdf_keywords": "generative parser model;generative parser;decoding generative parser;constituency parsing feasible;generative dependency parsing;generative parser use;parsers decoding tractable;approach decoding generative;external parsers decoding;parser model;parser model able;constituency parsing;single model parser;decoding generative;parsing feasible search;results constituency parsing;decode directly generative;external parsers;model parser;parsers decoding;search structural lexical;parsers;model parser demonstrate;parsing feasible;directly generative models;efficiently search structural;lexical actions beam;directly generative;parser;output external parsers"}, "19a3af37df22c7c646cc99efad3af96cda6e80f0": {"ta_keywords": "multimodal translation task;forwm17 multimodal translation;multimodal machine translation;translation trained attentional;neural machine translation;nist forwm17 multimodal;multimodal translation;machine translation task;machine translation;nict nist forwm17;translation trained;hiero translation trained;translation task;translation task language;forwm17 multimodal;translation task aim;attentional encoder decoder;attentional encoder;nict nist wm;trained attentional encoder;nist wm 2017;nist forwm17;2017 shared multimodal;nist wm;nict nist;multimodal machine;based hiero translation;describes nict nist;decoder neural machine;nist", "pdf_keywords": ""}, "51546584aa394d159edcc08f2412ae30dd316f6c": {"ta_keywords": "prediction depth;effective prediction depth;prediction depth results;prediction depth given;understanding deep;understanding deep learning;relationships prediction depth;input effective prediction;deep learning employs;deep learning;effective prediction;deep;depth results extensive;depth;work understanding deep;depth results;prediction;making prediction;difficulty making prediction;prediction given input;simple relationships prediction;measures compress data;making prediction given;measures compress;depth given;learning;relationships prediction;compress data dependent;dependent information;measure computational difficulty", "pdf_keywords": "complexity deep learning;complexity deep;unrelated deep learning;learning prediction depth;deep learning phenomena;small prediction depths;deep models learn;understanding deep learning;difficult prediction depth;prediction depth simple;smaller prediction depths;prediction depths;deep learning potential;understanding deep;prediction depth highly;deep learning;prediction depths demonstrate;low prediction depth;effective prediction depth;deep learning fundamental;prediction depth;deep learning employs;networks deep;prediction prediction depth;architectures prediction depth;deep models;picture deep learning;neural networks deep;prediction depth gives;networks deep learning"}, "ca73cc17ca69fa0807e566c22c7c1711da916281": {"ta_keywords": "similarity searching;similarity searching vast;approximate search methods;approximate search;spaces approximate search;search methods considerably;proposed exact search;exact search suffer;background similarity searching;dimensional spaces approximate;similarity;exact search;searching vast range;search methods;efficient high dimensional;searching vast;spaces approximate;background similarity;high dimensional spaces;methods considerably efficient;complexity methods comprehensive;searching;search suffer;dimensionality applicable high;applicable high dimensional;complexity methods;search;methods proposed exact;results regarding complexity;dimensional spaces case", "pdf_keywords": ""}, "56501a3441c2074bbbbe31015d6d41c57d9d285b": {"ta_keywords": "paraphrastic sentence representations;paraphrastic sentence models;monolingual semantic similarity;lingual semantic similarity;semantic similarity cross;similarity cross lingual;semantic similarity;state art paraphrastic;sentence representations variety;sentence representations;sentence models;paraphrastic;cross lingual semantic;paraphrastic sentence;sentence models developed;lingual semantic;monolingual semantic;art paraphrastic;trained models english;art paraphrastic sentence;suite monolingual semantic;similarity cross;semantic;cross lingual;similarity;models english;models english arab;lingual;methods suite monolingual;trained models", "pdf_keywords": "paraphrastic sentence representations;learning paraphrastic sentences;paraphrastic sentence models;paraphrastic sentence embeddings;embeddings language paraphrase;learning paraphrastic;learning inference paraphrastic;inference training paraphrastic;generate sentence embeddings;sentence embedding models;multilingual language representations;sentence embeddings language;algorithm embed sentences;embed sentences;effective detecting paraphrases;training paraphrastic;training paraphrastic sentence;approach learning paraphrastic;embed sentences score;state art paraphrastic;sentence embeddings;detecting paraphrases;sentence representations variety;sentence embeddings way;predicting semantic similarity;generating paraphrases;sentence representations;paraphrastic sentences;paraphrastic sentences variety;evaluation paraphrastic sentences"}, "ce458be308f2c75edc53366272fa6e744fda7902": {"ta_keywords": "word sense disambiguation;sense disambiguation;sense disambiguation given;unsupervised word sense;sense disambiguation discussionin;sense disambiguation objectivein;mnogoznal unsupervised word;disambiguation;nogoznal unsupervised word;disambiguation given;disambiguation discussionin;disambiguation discussionin paper;mnogoznal unsupervised;word sense;unsupervised word;disambiguation given sentence;disambiguation objectivein;present mnogoznal unsupervised;disambiguation objectivein paper;nogoznal unsupervised;mnogoznal;present nogoznal unsupervised;present mnogoznal;unsupervised;nogoznal;sentence chooses relevant;sense;present nogoznal;word;paper present mnogoznal", "pdf_keywords": ""}, "9195186cf44876d0d1d03b87756c464b760a7f4e": {"ta_keywords": "e2e speech translation;knowledge distillation seqkd;level knowledge distillation;sequence level knowledge;knowledge distillation;speech translation specifically;speech translation;end e2e speech;distillation seqkd end;e2e speech;distillation seqkd;training data architecture;level knowledge;encoder;end end e2e;multi referenced seqkd;seqkd multiple teachers;conformer encoder;training data;conformer encoder multi;encoder multi;adopted conformer encoder;referenced seqkd multiple;referenced seqkd;distillation;end e2e;sequence level;seqkd end end;multiple teachers trained;seqkd end", "pdf_keywords": "speech translation track;speech translation data;offline speech translation;model speech translation;end speech translation;ms speech translation;speech translation using;translation track;domain speech translation;speech translation developed;segmentation alsoin speech;speech translation ms;speech translation;translation ms speech;speech text encoding;range speech processing;speech translation investigated;architecture audio segmentation;translation track methodsthis;speech translation st;evaluation speech text;speech processing;speech text;offline speech;translation data;speech translation important;wide range speech;machine translation wild;alsoin speech translation;audio segmentation data"}, "4fd6488e38043d680c592170bf7f651c079d0e98": {"ta_keywords": "mobility management heterogeneous;heterogeneous cellular networks;cell planning mobility;networks high mobility;management heterogeneous cellular;mobility management;planning mobility management;planning mobility;base stations mobile;small cell networks;cellular networks;high mobility users;cellular networks important;mobile moving users;stations mobile moving;heterogeneous tier network;heterogeneous cellular;cell networks high;cell networks;mobility users;mobility;macro base stations;mobile moving;cellular;high mobility;data rate mobile;stations mobile;micro base stations;mobility users results;rate mobile", "pdf_keywords": "heterogeneous cellular networks;networks high mobility;mobile mobile networks;mobile networks impact;mobile networks;feasible network mobile;mobile mobility;networks impact mobile;mobile mobility communication;network mobile;mobility mobile;small cell networks;cellular networks;mobile mobility mobile;tier poisson network;throughput mobile users;mobility communication;mean throughput mobile;mobility mobile mobile;mobile user hetnet;network model stochastic;throughput static mobile;cell networks high;networks popular mobile;throughput mobile;cell networks popular;networks practice handoff;cell networks;cellular networks article;heterogeneous network model"}, "4ab7b65e1a3b76eb3db064523c862f1325e04971": {"ta_keywords": "speech people parkinson;parkinson different speech;speech recognizers;recognition systems speech;speech recognition systems;speech recognition;different speech recognizers;automatic speech recognition;systems speakers parkinson;automatic speech;speech recognizers attention;performance automatic speech;speakers parkinson disease;systems speech;recognition systems speakers;systems speech people;speakers parkinson;parkinson disease methods;parkinson disease;parkinson;parkinson different;people parkinson different;systems trained language;people parkinson;art automatic speech;speech people;deep neural network;speech;recognition systems;systems trained", "pdf_keywords": ""}, "3f79b71b887d2ccb733926867a62f69902fcbdab": {"ta_keywords": "adaptive ontology mapping;ontologymappingseekstofindsemanticcorrespondences betweensimilarelements differentontolo;abstract ontologymappingseekstofindsemanticcorrespondences betweensimilarelements;ontologymappingseekstofindsemanticcorrespondences betweensimilarelements;adaptive ontology;generic adaptive ontology;introduction abstract ontologymappingseekstofindsemanticcorrespondences;abstract ontologymappingseekstofindsemanticcorrespondences;ontology mapping;ontologymappingseekstofindsemanticcorrespondences;semantic web;ontology mapping approach;semantic interoperability;achieve semantic interoperability;interoperability building semantic;building semantic web;semantic interoperability building;building semantic;ontology;semantic web paperproposes;semantic;achieve semantic;challenge achieve semantic;prior based propa;mapping approach called;mapping approach;theory information retrieval", "pdf_keywords": ""}, "7954b31ce1f6ad935808b7cf62c34bc118d20a9a": {"ta_keywords": "causal effect decision;causal inference;formalize causal inference;effect decision;decisions faced context;causal;causal effect;causal inference problem;methodswe formalize causal;formalize causal;different decisions;make different decisions;instance judges vary;large causal effect;doctors vary preference;large causal;preference start treatment;different decisions faced;maker large causal;decisions;judges vary leniency;start treatment certain;decision maker;context personal preferences;treatment certain types;patients methodswe formalize;treatment certain;offenses doctors vary;instance judges;patients methodswe", "pdf_keywords": "treatment decisions biased;agent specific biases;agent specific bias;decisions agent specific;bias agents;agent bias;bias agents demonstrate;agent bias defined;aggregates bias agents;agent bias written;relative agent bias;problem disagreement agents;agent specific statistical;assess treatment decisions;bias identifiable using;bias identifiable;disagreement agents;different agents objective;specific bias identifiable;decisions biased;estimating individual treatment;potential decisions agent;specific biases objective;decisions propose causal;agents heterogeneous preferences;agents objective identified;agents objective;biases objective;disagreement agents group;decisions agent"}, "4bf5084d21f681c09409bd890daa4bf1c4f9b691": {"ta_keywords": "platelet reactivity hpr;treatment platelet reactivity;periprocedural myocardial infarction;platelet reactivity;significance observed smoking;observed smoking;high treatment platelet;smoking cohort regarding;observed smoking cohort;treatment platelet;smoking cohort;role periprocedural myocardial;myocardial infarction;periprocedural myocardial;smoking play significant;myocardial infarction study;infarction common cardiac;infarction study;infarction study begun;reactivity hpr;platelet;myocardial infarction common;recently reported smoking;reported smoking;smoking play;smoking;infarction common;infarction;reactivity hpr apologetic;reported smoking play", "pdf_keywords": ""}, "c3490ec9b8f695bed2187fb4a4164b1509389ca8": {"ta_keywords": "sex specific sex;specific sex specific;sex specific;specific sex;role sex specific;literature role sex;role sex;sex;purpose article;literature role;specific;purpose article present;article present literature;present literature role;literature;article;present literature;purpose;role;article present;present", "pdf_keywords": ""}, "7d94d4c6b2db490e08beabd2661df009f1a06d6c": {"ta_keywords": "yar russnet project;yar russnet;noun synsets presently;large open wordnet;wordnet like thesausaurus;create noun synsets;wordnet;open wordnet;noun synsets;wordnet like;open wordnet like;russnet project started;russian means crowdsourcing;russnet project;synsets presently resource;russnet;like thesausaurus russian;thesausaurus russian;crowdsourcing;crowdsourcing stage project;means crowdsourcing;crowdsourcing stage;thesausaurus russian means;create noun;synsets presently;synsets;synsets 200 people;44k synsets;comprises 48k word;project create noun", "pdf_keywords": ""}, "02a757548da783d43ffcfd4b60f2cbb0ac71a4bc": {"ta_keywords": "fairness elicitation fairness;fairness elicitation;notion individual fairness;individual fairness proposed;individual fairness;fairness metric task;elicitation fairness indirectly;fairness indirectly specified;elicitation fairness;fairness proposed;framework fairness elicitation;fairness indirectly;fairness metric;completely specified fairness;specified fairness;specified fairness metric;consider framework fairness;framework fairness;fairness proposed dawork;fairness;individuals treated similarly;asks similar individuals;similar individuals treated;individuals treated;treated similarly;individuals;pairs individuals treated;sample pairs individuals;elicitation;similar individuals", "pdf_keywords": "fairness constrained learning;empirical fairness generalization;satisfying empirical fairness;empirical fairness;empirical fairness good;empirical fairness constraints;fairness empirical fairness;eliciting enforced fairness;fairness empirical;classifiers fairness;fairness generalization;generalization fairness empirical;fairness constraints empirical;fairness generalization fairness;constraints empirical fairness;fairness good generalization;elicited subjective fairness;good generalization fairness;randomized classifiers fairness;deriving fairness;fairness constraints data;generalization fairness;similarly deriving fairness;fairness constraints;fairness captured simple;based principle fairness;subjective fairness;generalization principle fairness;fairness loss generalization;analysis fairness"}, "f7247fefc9efb57ace33425a2981d6aba08da3b7": {"ta_keywords": "statistical dialogue management;statistical dialogue framework;based statistical dialogue;dialogue management;dialogue management using;statistical dialogue;method statistical dialogue;intention dependency graph;dialogue framework;hierarchical graph intentions;directed intention dependency;dialogue framework resul;graph intentions way;markov decision process;dialogue;graph intentions;using directed intention;observable markov decision;intention dependency;markov decision;directed intention;decision process pomd;intentions way combine;dependency graph;decision process;observable markov;information derived hierarchical;partially observable markov;transition probabilities model;process pomd framework", "pdf_keywords": ""}, "23e42bc79f10234bdceef31441be39a2d9d2a9a0": {"ta_keywords": "learning logical rules;logical rules knowledge;rules knowledge base;introductiondifferentiable learning logical;neural logic programminging;knowledge base reasoning;learning logical;neural logic;learning order logical;developed differentiable logic;framework neural logic;rules knowledge;base reasoning methodswe;knowledge base;differentiable logic;logical rules;logic programminging;differentiable logic calledtens;order logical rules;logical rules end;logic programminging combines;base reasoning;probabilistic order logical;reasoning methodswe;reasoning methodswe propose;base reasoning objectivewe;learning probabilistic;learning probabilistic order;logic calledtens;parameter structure learning", "pdf_keywords": ""}, "06064617f152f5032137204aec739c0c82dbb836": {"ta_keywords": "distant microphone automatic;introduction distant microphone;distant microphone;speech recognition ar;microphone automatic speech;speech recognition;automatic speech recognition;microphone;microphone automatic;chime challenge initiative;automatic speech;2nd chime challenge;chime challenge;reverberation;background sources reverberation;sources reverberation;reference 2nd chime;sources reverberation paper;anr systems;2nd chime;recognition ar;reverberation paper;reverberation paper intended;performance anr systems;chime;performance anr;distant;introduction distant;anr;anr systems real", "pdf_keywords": ""}, "14047a24b23d9e392776229f9d40bee9f8243e4c": {"ta_keywords": "optimal active sensing;sensing process tracking;complexity dynamic sensor;sensor networks cyberphysical;sensor activation tracking;dynamic sensor activation;active sensing process;active sensing;tracking time varying;sensor networks;dynamic sensor;process tracking proposed;sensing process;process tracking;activation tracking time;energy efficiency fidelity;minimizing time averaged;cyberphysical systems problem;tracking proposed;tracking time;introduction optimal active;time varying process;things sensor networks;sensor activation;internet things sensor;cyberphysical systems;networks cyberphysical systems;things sensor;efficiency fidelity problem;optimal active", "pdf_keywords": ""}, "0dd1b9ad5aeda250dc61f38cf7018e7a014e91c0": {"ta_keywords": "use traffic congestion;land use traffic;traffic congestion;traffic congestion based;congestion based gr;use traffic;traffic;land use;congestion;congestion based;impact land use;based gr model;gr model discussed;gr model;impact land;based gr;model discussed;land;gr;impact;model;use;based;discussed", "pdf_keywords": ""}, "a67face220a88b6b36f3343a6a017a3536562d5b": {"ta_keywords": "visual guessing games;guessing games methodsknowing;learned visual guessing;playing guessing games;methodsknowing games prototypical;guessing games;benefit playing guessing;visual guessing;guessing games later;playing guessing;visual question answering;methodsknowing games;neuralrepresentations learned visual;games methodsknowing;neuralrepresentations learned;games methodsknowing games;games prototypical instance;exploit playing guessing;artificial agent benefit;learning interacting paradigm;learning interacting;learned visual;games prototypical;power neuralrepresentations learned;neuralrepresentations;instance learning interacting;generalization power neuralrepresentations;agent benefit playing;question answering;investigates artificial agent", "pdf_keywords": "questioner learns games;visual guessing games;visual question answering;questioner learns;questioner agent training;gameplay questioner agents;tasks agent trained;generalise visual guessing;process questioner learns;agent trained guesswhat;trained guesswhat dialogues;agent generalise visual;instances questioner agent;play mechanism learning;play guessing games;questioner agents;learns games previously;learns games;learning general grounded;guessing games;game human dialogue;questioner agent;play guessing;visual guessing;agent trained;general grounded representations;grounded representations;observations gameplay questioner;grounded representations seed;algorithm self play"}, "970383c0a41d7ae1ec4b8abaa3033778203377b9": {"ta_keywords": "virtual assistants answer;assistants answer questions;question answering qa;answering qa tasks;question answering;factoid question answering;virtual assistants;recognition systems factoid;neural language processing;automatic speech;recognition speech;assistants answer;answering qa;inputs machine translation;noise automatic speech;speech recognition instance;speech recognition;neural language;character recognition speech;qa tasks;automatic speech recognition;tasks integrating confidences;speech recognition systems;qa tasks integrating;instance virtual assistants;unknown words empirically;decoding unknown words;recognition speech recognition;language processing;answer questions understanding", "pdf_keywords": "automatic text speech;automatic speech;models synthetic corpus;question answering tasks;synthetic corpus;method automatic speech;question answering task;accuracy automatic text;inputs machine translation;virtual assistants answer;recognition speech;improving question answering;character recognition speech;question answering;virtual assistants;supervised nonlinguistic tasks;speech recognition instance;text speech;question answering models;answering models used;recognition understanding text;noise automatic speech;automatic speech recognition;model question answering;answering models;synthetic corpus 500;speech compare accuracy;recognition systems factoid;recorded question datasets;accuracy sentence level"}, "3193766c0439ff29a0a3d176628f8144d6e77231": {"ta_keywords": "patients history;management patients history;patients history history;history disease;history history disease;retrospective analysis literature;retrospective analysis;results retrospective analysis;retrospective;present results retrospective;results retrospective;approach management patients;management patients;patients;disease;history history;history;new approach management;approach management;new approach;use new approach;article present;management;analysis literature use;article present results;present results;article;analysis literature;approach;use", "pdf_keywords": ""}, "b38ec68c8bab031138606a9b00e9d817be3e1d22": {"ta_keywords": "link modeling jointly;jointly modeling links;topic models;models topic models;entity link modeling;link modeling;topic models improve;modeling links text;modeling links;latent groups entities;text entities linked;links text entities;entities linked;entity link;protein interactions social;pairs entities frequently;mixed membership stochastic;jointly modeling;modeling jointly modeling;block models topic;identifying latent groups;model datasets protein;social networks methods;entity entity link;stochastic block models;groups entities observed;modeling jointly;latent groups;membership stochastic block;pairs entities", "pdf_keywords": ""}, "3f256b31d446015d8cd0f9f3996009cdf2034c5e": {"ta_keywords": "speech recognition language;recognition language independent;multilingual automatic speech;monolithic multilingual automatic;joint language identification;speech recognition;speech recognition model;model recognize speech;identification speech recognition;recognition language;automatic speech recognition;language independent neural;monolithic multilingual;multilingual automatic;develop monolithic multilingual;based speech recognition;language identification speech;automatic speech;language independent end;speech recognition objective;joint language;recognize speech;recognize speech 10;neural network architecture;architecture joint language;language identification;independent neural network;character based speech;multilingual;hybrid attention connection", "pdf_keywords": ""}, "c56aced0f0c5cfebefadb530cb08d736c3ac5c05": {"ta_keywords": "code summaries retrieval;code summary generation;developers code summary;code documentation software;code summaries;code code summaries;source code code;source code documentation;code summaries written;code summary;software documenting;developers code;mimic developers code;code documentation;source code;summaries retrieval database;summaries retrieval;documentation software development;relevant code summaries;developers recall;summary generation;lot source code;implementing software documenting;propose retrieval;developers recall parts;retrieves relevant code;introduction software developers;intrainsically developers recall;documentation software;summary generation behavior", "pdf_keywords": "code summaries retrieval;code generation retrieval;code generation summarization;generate code summaries;code summary generation;code summarization useful;code summarization implement;source code summarization;code summaries database;code summarization;summarizing source code;code summaries use;target code summaries;10 code summarization;code summaries;code code summaries;retrieved code summary;summarization framework code;help code generation;retrieval propose code;generation summarization implement;code summaries written;code documentation software;developers code summary;automated code generation;generation summarization programming;generation summarization framework;tool code generation;summary generation retriever;summarization programming languages"}, "a4ce6cd06bc73d81651f7888efa4337fd82a60f0": {"ta_keywords": "detecting unknown words;words spoken dialog;word detectingion based;introductionunknown word detectingion;word detectingion;spoken dialog systems;spoken dialog;unknown words spoken;unknown words disturbs;dialog systems;utterance unknown;dialog systems deals;words disturbs communication;detectingion based event;utterance unknown user;unknown words;detecting unknown;utterance;user covered vocabulary;dialog;user case utterance;words disturbs;case utterance unknown;unknown words user;detectingion based;systems deals words;related brain desynchronization;focus detecting unknown;brain desynchronization responses;brain desynchronization", "pdf_keywords": ""}, "04b364d56995de2228cb1acfb320a935cbcf4440": {"ta_keywords": "deteriorate weakly supervised;semantic segmentation expensive;weakly supervised;weakly supervised settings;low level segmentation;standard semantic segmentation;segmentation expensive;segmentation expensive requiring;semantic segmentation;labeled image level;priors pixel labels;requiring pixel labeled;level segmentation;segmentation;pixels labeled image;labeled image;pixel labeled;pixels labeled;pixel labels;level segmentation representing;segmentation representing;image level tags;regularized losses;segmentation representing geometric;unsupervised low level;supervised;shown regularized losses;unsupervised low;geometric priors pixel;labeled", "pdf_keywords": "weakly supervised segmentation;semantic segmentation neural;instance learning mil;networks semantic segmentation;multiple instance learning;losses weakly supervised;instance learning;weakly supervised semantic;semantic segmentation common;weakly supervised neural;network semantic segmentation;weakly supervised;limitations weakly supervised;accuracy weakly supervised;underestimated weakly supervised;semantic segmentation present;common approach deep;supervised semantic segmentation;supervised segmentation robust;iterations weakly supervised;segmentation neural;supervised segmentation increasing;semantic segmentation;deep neural;low level segmentation;approach weakly supervised;approach deep neural;approach deep;regularization losses;segmentation neural neural"}, "fa774368fcf51cc0fa1bfda59b6a606e163c64b1": {"ta_keywords": "exploit symmetry synthesize;systems counting constanttraints;dimensional systems counting;symmetry synthesize;symmetry synthesize provably;highly symmetrical counting;synthesizing high dimensional;correct controllers systems;systems counting;counting constanttraints general;counting constanttraints;symmetrical counting problems;symmetrical counting;provably correct controllers;synthesis;problems exploit symmetry;synthesis methods limited;correct construction synthesis;synthesis methods;counting problems exploit;controllers systems;exploit symmetry;construction synthesis;construction synthesis methods;high dimensional systems;synthesize provably correct;controllers systems tens;dimensional systems;synthesize provably;backgroundcontrol synthesizing", "pdf_keywords": "systems counting constraints;systems counting constraint;symmetric counting constraints;relaxed counting constraints;systems formulation counting;counting constraints propose;counting constraints proposed;counting constraints;constraints counting;satisfy counting constraints;counting constraints collection;counting constraints counting;systems counting;counting constraints applied;counting constraints linear;formulation counting constraints;counting constraint;different systems counting;method counting constraints;counting constraint satisfied;counting constraints apply;highly symmetrical counting;controllers satisfy counting;program integer constraints;exploit symmetry synthesize;counting problem feasibility;constraints counting problem;counting problems exploit;solvable linear programs;symmetrical counting problems"}, "9abf14d4f89bf6c297e1bbd637cd54e1a0335e71": {"ta_keywords": "compositor attribution clustering;attribution clustering pages;automatic compositor attribution;textual visual features;shakespeare compositor attribution;attribution clustering;compositor attribution folio;textual visual;describes textual visual;folio shakespeare compositor;introduction automatic compositor;compositor attribution;textual;attribution folio;describes textual;shakespeare compositor;bibliographic;bibliographic task;clustering pages historical;historical printed document;visual details printed;attribution folio shakespeare;jointly describes textual;pages historical printed;clustering pages;printed document;bibliographic task relies;printed document individual;details printed page;attribution", "pdf_keywords": "compositor identification texts;textual content;transcriptions textual content;ocr transcriptions textual;textual;textual content model;textual visual features;textualthe;transcriptions textual;identification texts corpora;describes textual;texts corpora;historical text model;textualthe authors propose;context historical text;compositor studies shakespeare;contextcompositor attribution clustering;textual visual;texts corpora using;describes textual visual;attribution clustering pages;jointly describes textual;textualthe authors;identification texts;predicts compositor attributions;spelling preferences texts;texts;compositor attributions;compositor preferences literature;integrates textualthe authors"}, "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4": {"ta_keywords": "soft prompts learned;frozen language models;learning frozen language;frozen language model;learning frozen;prompts learned backpropagation;mechanism learning frozen;soft prompts;clinical messagewe explore;language models perform;language models;clinical messagewe;key clinical messagewe;prompts learned;discrete text prompts;language model;frozen language;shot learning;text prompts;text;learned backpropagation;shot learning large;text prompts used;bygt soft prompts;learned approach;learned approach outperforms;learning;outperforms shot learning;effective mechanism learning;condition frozen language", "pdf_keywords": "pretrained language models;models prompt tuning;learning soft prompt;optimizing prompts model;prompts model easily;large language models;trained models prompt;learning continuous prompts;optimizing prompts;automatedwe prompt tuning;achieved optimizing prompts;model tuning prompt;language models perform;frozen language models;tuning prompt optimization;predict text generation;adapting language models;prompt optimization;prompt tuning improves;task text generation;language models increasing;propose prompt tuning;prompt tuning;trained language model;use prompt tuning;prompt tuning outperforms;prompts model ability;prompt tuning sufficient;method optimizing prompts;task based prompts"}, "1a3fcb1e2a416cbc79a011f1a1916aa53f7a2a09": {"ta_keywords": "meaning body postures;body postures essential;body postures;postures essential social;meaning body;body configurations;postures essential;purposecomprehending meaning body;postures;body configurations carry;possible body configurations;upper body;body;470 upper body;possible body;humans example important;humans example;fraction possible body;characterize;social organisms humans;understand glance people;characterize meaning;best way characterize;way characterize meaning;organisms humans example;way characterize;important understand glance;organisms humans;essential social organisms;social organisms", "pdf_keywords": ""}, "e63c9eb5b623baad0a7805e839e5d9fabad37fce": {"ta_keywords": "automated question answering;inference explanationregeneration tasks;question answering systems;answers natural language;human readable explanations;question answering;readable explanations answers;explanations answers;explanationregeneration tasks;retrieve answers natural;answering systems increasingly;explanations standardized elementary;explanations standardized;answering systems;explanationregeneration tasks participants;automated question;introduction automated question;readable explanations;gold explanations standardized;natural language questions;inference explanationregeneration;detailed gold explanations;explanations answers quite;able retrieve answers;retrieve answers;gold explanations;elementary science exam;science exam;natural language;questions ability", "pdf_keywords": ""}, "ab8174a1f1810c1122f90649276a552d2eb1ccd4": {"ta_keywords": "etiology malignant disease;etiology malignant;disease malignant;disease malignant disease;case malignant disease;malignant disease;malignant disease malignant;patient malignant disease;understood case malignant;disease patient malignant;malignant disease patient;case malignant;patient malignant;malignant;malignant disease poorly;disease;disease poorly understood;disease patient;etiology;disease poorly;patient;poorly understood case;understood case;poorly understood;case;understood;poorly", "pdf_keywords": ""}, "7618c65685c98fa88526555ae3f62cd5645066ad": {"ta_keywords": "researching relation entailment;relation entailment allows;relation entailment;relation entail;relation entailment existence;examine relation entailment;relation entail existence;relations words entities;entailment existence relation;semantic understanding text;relations words;relations meta relations;introduction relations words;relations relations meta;entail existence relation;relation hierarchies;semantic understanding;entailment allows;relation hierarchies enabling;relations meta;relations relations;entailment allows construct;meta relations;researching relation;existence relation entail;important semantic understanding;entailment;semantic;relations;construct relation hierarchies", "pdf_keywords": ""}, "6e78e32481218e9391a88e6d0e30c0062ae71bec": {"ta_keywords": "speech gesture animation;transfer speech gesture;animation multi speaker;embeddings speaker gestures;gesture animation multi;speaker gestures;style transfer speech;speech gesture;gesture animation;speaker gestures end;speaker conditional mmixture;style embeddings speaker;transfer speech;gestures;multi speaker conditional;embeddings speaker;style transfer;animation multi;gestures end;multiple speakers learning;mix stage learn;gesture;gestures end end;multi speaker;speaker conditional;animation;model multiple speakers;speakers learning;mix stage;background style transfer", "pdf_keywords": "gesture generation mix;speech gesture generation;generate speech gestures;audio gesture generation;embeddings speaker gestures;speech gestures content;speech gestures;gesture generation;speaker gestures;learns gesture style;performance speech gesture;drives speech gesture;gesture style audio;speaker gesture style;gesture style transfer;speakers speech gesture;speech gesture;speech gestures idiosyncratic;gesture style speaker;gesture animations;models conditioned gesture;cospeech gesture animation;gestures content audio;gesture generation setting;learns gesture;speaker gesture;style audio gesture;gesture generation demonstrate;multispeaker generation models;model corresponding gesture"}, "f3bca263a92b69c6da872a9a3268f260ba43f690": {"ta_keywords": "language model rnn;recurrent neural network;neural network language;model rnn lim;model rnn;predicted reference words;speech recognition;automatic speech;gram language model;language model;automatic speech recognition;speech recognition ar;rnn lim use;rnn lim;recurrent neural;introduction recurrent neural;discriminative training;language model effective;network language model;accomplishment automatic speech;unlike discriminative training;rnn;rn lim based;training criteria rn;word context gram;context gram language;gram language;rn lim;long word context;use long word", "pdf_keywords": ""}, "53880036fb85cc737103c480c613e1912c416010": {"ta_keywords": "wrapper induction extracting;structured wrapper induction;introductiona structured wrapper;wrapper learning systems;structured wrapper;restricted extraction language;wrapper learning;extraction language;wrapper learning encoded;induction extracting information;semi structured documents;allows wrapper learning;wrapper induction;restricted extraction;structured documents;induction extracting;extracting information semi;extracting information;bias wrapper learning;extraction language lo;associated restricted extraction;structured documents proposed;information semi structured;extracting;architecture allows wrapper;semi structured;introductiona structured;learning systems easily;learning encoded ordered;wrapper", "pdf_keywords": ""}, "a2f4731258830c76af7e3bdb96c4488823219585": {"ta_keywords": "frequency masking reverberant;time frequency masking;frequency masking;stereo input speech;masking reverberant environment;blind source separation;sparseness based underdetermined;underdetermined blind source;masking reverberant;input speech recognition;speech recognition ar;speech recognition;robust automatic speech;recognition using sparseness;using sparseness based;sparseness based time;automatic speech recognition;based underdetermined blind;speech recognition using;sparseness based;present noise robust;noise robust automatic;underdetermined blind;noise robust;using sparseness;frequency masking paper;ar using sparseness;input speech;source separation;automatic speech", "pdf_keywords": ""}, "341c72f55a89572915aa476db7f525c2e0b60eba": {"ta_keywords": "cluster similarity indices;evaluate clustering algorithms;similarity indices used;similarity indices;introductionthere cluster similarity;cluster similarity;evaluate clustering;used evaluate clustering;clustering algorithms choosing;clustering algorithms;pair counting indices;counting indices;introductionthere cluster;clustering;cluster;counting indices prove;theoretically verify indices;indices;indices used;verify indices satisfy;indices satisfy;indices satisfy methodsin;verify indices;indices prove meet;algorithms;similarity;indices used evaluate;pair counting;indices prove;investigate dozens pair", "pdf_keywords": "validity cluster similarity;evaluating cluster similarity;validation cluster similarity;cluster similarity indices;cluster similarity index;cluster similarity indexes;measuring cluster similarity;comparing clusterings literature;similarity clusters index;clusterings cluster similarity;analysis cluster similarity;similarity indices cluster;analyzing cluster similarity;indices cluster similarity;cluster similarity important;cluster similarity analysis;validation clusterings cluster;comparing clusterings;compare clustering quality;cluster similarity clusters;cluster similarity;evaluate clustering algorithms;similarity analysis cluster;method comparing clusterings;compare clustering;clustering comparisons used;evaluation clusterings;method cluster similarity;validation clusterings;evaluation clusterings method"}, "143183584a8ebaad93490f4550295a9cb6cf9817": {"ta_keywords": "relational learning slr;statistical relational learning;relational learning;logic machine learning;logic probabilistic;semantic parsing;probabilistic order logic;inference order logic;semantic parsing information;classification semantic parsing;logic probabilistic order;order logic probabilistic;statistical relational;natural language processing;parsing information extraction;background statistical relational;parsing information;text classification semantic;probabilistic inference;natural language;parsing;information extraction;language processing nl;probabilistic inference objectives;inference order;learning methods probabilistic;formulated inference order;information extraction coreference;inference objectives natural;classification semantic", "pdf_keywords": ""}, "fc78af26fd7644867af1abb8fbf2c37b47ad8257": {"ta_keywords": "lingual word embeddings;language english embedding;english embedding;english embedding space;hub learning multilingual;learning multilingual;word embeddings;lexicon induction evaluation;word embeddings severely;cross lingual word;evaluation dictionaryaries english;cross lingual;learning multilingual setting;lexicon induction;embeddings severely anglocentric;work cross lingual;lingual word;lingual;majority lexicon induction;multilingual;vast majority lexicon;dictionaryaries english;induction evaluation dictionaryaries;multilingual setting;dictionaryaries english language;language significantly impact;evaluation dictionaryaries;hub language significantly;embeddings severely;language significantly", "pdf_keywords": "lingual word embedding;learning multilingual;language english embedding;lexicon induction performance;multilingual translations methods;lingual translations;downstream lexicon induction;cross lingual translations;lexicon induction evaluation;multilingual translations;english evaluation benchmarks;hub learning multilingual;evaluation dictionaries english;english embedding;learning multilingual setting;word embedding mapping;downstream lexicon;lingual translations fundamental;language dictionaries;impact downstream lexicon;multilingual settings empirically;word embedding;resource language dictionaries;lexicon induction;benchmarks compared english;distant languages multilingual;dictionaries low resource;new dictionaries baseline;morphologically rich languages;learn cross lingual"}, "682660c7a014e806b924fdf1a2a3d999a9ac13cf": {"ta_keywords": "generation abstractive summarization;summarization using abstract;abstractive summarization;abstractive summarization using;neural language generation;guidance improves summarization;language generation abstractive;using abstract meaningrepresentation;work abstractive summarization;abstract meaningrepresentation;introductionguided neural language;summarization results;improves summarization results;abstract meaningrepresentation amr;improves summarization;abstract meaningrepresentation aimto;summarization using;summarization;generation abstractive;summarization results 10;introductionguided neural;meaningrepresentation amr neural;neural language;language generation;amr neural language;language generation stage;meaningrepresentation;meaningrepresentation amr;using abstract;meaningrepresentation aimto extend", "pdf_keywords": "generation abstractive summarization;anarto text generation;summarization propose guided;summarizing anatomical data;approach summarizing anatomical;abstractive senstence summarization;senstence summarization propose;novel approach summarizing;document model summarization;summarization model aim;summarization model;senstence summarization;text generation;summarizing anatomical;text generation propose;abstractive summarization;summarization propose;neural language generation;senstence summarization process;abstractive summarization using;model summarization;text summary anatomical;text generation using;summarization process;summarization process results;summarization using abstract;summarizing;summarization;introductionguided neural language;analysis summarization"}, "9d698e034d83eedc05237e629eaad1c0c4e5bbb9": {"ta_keywords": "learning recursive logic;recursive logic programs;learnable clause programs;logic programs polynomial;free recursive logic;recursive logic;clause learnable;learnable clause;logic programs;learning recursive;clause programs;clause programs consisting;clause learnable clause;logic programs common;determinate clause learnable;programs consisting learnable;programs polynomial time;single ary recursive;equivalence queries methodsin;free recursive;backgroundpac learning recursive;recursive constant depth;equivalence queries;ary recursive;polynomial time equivalence;function free recursive;recursive;programs polynomial;algorithms learn;ary recursive constant", "pdf_keywords": "recursive logic programs;free recursive logic;learnability recursive logic;learning recursive logic;generalized recursive logic;recursive clause learnable;learnable recursive clause;learnable clause programs;recursive logic learning;recursive logic;recursive logic program;logic programs polynomial;learnable recursive;logic programs described;recursive clauses learning;logic programs inductive;generalizing recursive clauses;learning recursive clause;relaxed ary recursive;able equivalence queries;ary recursive programs;inductive logic programming;learnability recursive programs;recursive programs implemented;algorithm clause programs;programs inductive logic;clause recursive logic;learning recursive programs;logic programs;learnability recursive"}, "bfe6d67ed1c9119f91774e62fe0f4f328830526e": {"ta_keywords": "neural conversation models;adaptation neural conversation;training neural conversation;speaker role adaptation;conversation models;task learning speaker;learning speaker role;conversation data model;conversation models building;persona based conversation;conversation agent;speaker specific conversation;based conversation agent;neural conversation;speaker roles modeled;conversation agent challenging;conversation models leverages;conversation data speakers;conversation data;specific conversation data;learning speaker;speaker roles;speaker role;leverages conversation data;data pertaining speaker;role adaptation neural;speaker speaker roles;conversation;multi task learning;specific conversation", "pdf_keywords": "neural conversation models;neural conversational models;generating conversational responses;training neural conversation;conversation introduce autoencoder;end conversation models;conversational models;conversation models;models conversational responses;conversational models conversational;driven conversational data;generating conversational;conversational personal data;novel conversational model;conversational model user;conversational models using;conversation data model;conversational data;conversation models leverages;conversational model;leverage conversational data;conversation models use;models conversational;general conversational model;tool generating conversational;persona based conversation;conversation data;conversational data users;specific conversation data;conversational data integrate"}, "ca879ec1c04b94de274954dfd09dddfde6cbb4f3": {"ta_keywords": "singing voice conversion;statistical singing voice;age statistical singing;perceptual age singing;singing voice age;voice conversion;voice conversion methods;voice quality control;age singing voice;statistical singing;voice conversion sv;voice timbre control;voice age;age singing;voice quality;singing voice;voice age singer;voice timbre;introduction voice quality;age singer perceived;conversion methods perceptual;voice;novel voice timbre;perceptual age statistical;singing;timbre control technique;age singer;singer perceived;timbre control;singer perceived listener", "pdf_keywords": ""}, "bd6c708a535af588d90025a0e6cf17407bf65434": {"ta_keywords": "deception detection models;interact deception detection;participants interact deception;deception detection;attributing predictions features;techniques attributing predictions;predictions machine learning;interact deception;conduct crowdsourcing;crowdsourcing study;predictions features deemed;crowdsourcing;crowdsourcing study participants;conduct crowdsourcing study;attributing predictions;predictions features;deception;attributions;predictions machine;paper conduct crowdsourcing;explain predictions machine;machine learning models;attributions claimed;important attributions;important attributions claimed;human understanding models;understanding models surprisingly;explain predictions;ex explain predictions;machine learning", "pdf_keywords": "explanations text classification;evaluating model explanations;model explanations evaluation;evaluate model explanations;explanations evaluation paradigm;model explanations text;explanations evaluation;detection models deception;models deception detection;model explanations introduce;explanations help participants;models explanations;simulate models explanations;models global explanations;examine explanations;deception detection models;explanations significantly edits;text classification tasks;model explanations;study examine explanations;explanations text;trust machine learning;highlighted explanations significantly;text classification;accuracy deception detection;deception detection task;use deception detection;explanations help humans;explanations reducing;models deception"}, "bf7481685e63b85ef2586de3f6098f1a5fbe0e2d": {"ta_keywords": "samplers ambient monitoring;sampler organic contaminants;active sampler organic;samplers ambient;free active sampler;active sampler;contaminants surface water;sampler organic;backgroundactive samplers ambient;monitoring trace contaminants;osmotic pump op;osmotic pump;trace contaminants surface;used op sampler;improved osmotic pump;sampler;op sampler;backgroundactive samplers;contaminants surface;samplers;trace contaminants;extraction spe cartridge;ambient monitoring trace;organic contaminants;surface water;phase extraction spe;extraction spe;coupling improved osmotic;contaminants resultsthe op;organic contaminants resultsthe", "pdf_keywords": ""}, "72579f6ce4a413585445c4ef8c8c2fa63ea1b8bc": {"ta_keywords": "privacy deep neural;learning stochasticrepresentations privacy;stochasticrepresentations privacy deep;stochasticrepresentations privacy;stochastic perturbations obfuscate;privacy deep;perturbations obfuscate private;private data;privacy;obfuscate private;stochasticity offline gradient;perturbations obfuscate;learning stochasticrepresentations;discovery stochasticity offline;obfuscate private data;stochasticrepresentations;approach learning stochasticrepresentations;discovery stochasticity;deep neural inference;stochasticity offline;private data sent;neural inference methodsthis;private;neural inference;deep neural;stochastic perturbations;formulating discovery stochasticity;optimal stochastic perturbations;stochasticity;stochastic", "pdf_keywords": "prediction privacy ability;prediction privacy;ability predict privacy;prediction privacy present;predict privacy;prediction privacy led;factor prediction privacy;soft constrained optimization;data privacy utility;predict privacy fundamental;importance prediction privacy;method optimization classifier;soft constrained;privacy utility;privacy deep learning;detect preserve privacy;optimization classifier;prediction services;outperforms shredder;privacy deep;method outperforms shredder;sophisticated algorithms;enables closureak minimize;problem soft constrained;outperforms shredder recent;optimization classifier based;storage privacy;sophisticated sophisticated algorithms;preserve privacy using;preserve privacy"}, "80edd01d46228fac7ec0cd14aea1666253b28f4d": {"ta_keywords": "agents manipulate vote;voter preferences computationally;agents vote choose;voter preferences;agents vote;preferences computationally complex;situations agents vote;preferences computationally;preferences group agents;outcome voting way;voting scenarios;information voter preferences;voting scenarios people;outcome voting;real world voting;better outcome voting;situations assumed voters;vote choose alternative;collective decision making;manipulate vote;introductionin collective decision;preferences real world;world voting scenarios;collective decision;manipulate vote achieve;agents manipulate;voting way does;voting way;vote achieve better;group agents manipulate", "pdf_keywords": "utility voting strategies;alternatives voting heuristics;voting strategies maximize;utility voting;voting heuristics;examine voting strategies;voting strategies;voting environments;voting heuristics study;voting strategies achieve;voter preferences computationally;expected utility voting;vote truthful strategic;particular vote truthful;voter preferences;vote strategies;decide vote strategies;agents manipulate vote;better outcome voting;agents vote choose;outcome voting;votes depending underlying;alternatives voting;effort alternatives voting;voting scenarios;outcome voting way;heuristic people vote;features voting environments;voting environments affect;utility candidate decide"}, "1da3a9c194a01c0bff7b6ecda79db9d673810bee": {"ta_keywords": "outperform recurrent neural;outperform recurrent models;outperform recurrent;works outperform recurrent;recurrent models;recurrent neural network;morphological inflection generation;recurrent models using;recurrent neural;word level nonpolymers;shown outperform recurrent;tasks morphological inflection;transduction tasks morphological;sequence sequence models;character level transduction;sequence models;text normalization works;text normalization;morphological inflection;recurrent;historical text normalization;nonpolymers character;contrast recurrent;models various word;nonpolymers character level;sequence models various;level nonpolymers character;inflection generation;word level;transduction tasks", "pdf_keywords": "task morphological inflection;performance morphological inflection;level translation tasks;conversion morphological inflection;translation tasks large;morphological inflection task;translation tasks;features morphological inflection;character level transduction;neural machine translation;machine translation important;translation applications neural;shared task morphological;morphological inflection;encoding features morphological;machine translation applications;conversion morphological;inflection task learning;machine translation;machine translation development;morphological inflection model;morphological inflection grapheme;translation development neural;machine translation field;outperform recurrent models;task morphological;translation field neural;language processing;level transduction tasks;character level tasks"}, "28e81f96eab94e99febcaaee00637825c8a3e664": {"ta_keywords": "interpretable machine learning;im interpretable machine;machine learning society;interpretable machine;machine learning grew;reasoning increasingly complex;increasingly complex models;building trust models;understand reasoning increasingly;trust models performing;trust models;reasoning increasingly;machine learning;interpretable;field im interpretable;emergence machine learning;model debugging generally;model debugging;im interpretable;models;inability understand reasoning;models field im;understand reasoning;informing real human;human decision making;complex models;people inability understand;increasingly complex;cases building trust;real human decision", "pdf_keywords": ""}, "faf494d0aa25a17aa25930ffb4c750fa59c44849": {"ta_keywords": "speaker embedding text;learning speaker verification;learning speaker embedding;representation learning speaker;corresponding speaker embedding;speaker embedding;speaker verification methods;speaker embedding work;speaker verification;text speech shot;text corresponding speaker;speaker text speech;embedding text speech;textt speaker;speaker text;learning speaker;multi speaker text;speech shot multi;target speaker voices;taccotron textt speaker;speech shot;corresponding speaker;speaker voices given;text speech;introduction learning speaker;textt reconstruction;textt reconstruction objective;speaker voices;text speech tact;effectiveness textt reconstruction", "pdf_keywords": "learning speaker verification;speech speaker verification;speaker encoder training;representation learning speaker;speaker embedding networks;learning speaker embedding;speaker embedding model;speech embedding;speaker encoder developed;speaker verification jointly;speaker verification;speaker encoder;speaker embedding supervised;text speech speaker;suitable speaker encoder;proposed speech embedding;speaker embedding;trained multispeaker encoder;extract speaker embedding;speech embeddings;encoder speaker encoder;corresponding speaker embedding;speaker embedding input;neural text speech;speaker user speech;speech recognizer;speech embeddings using;speaker embedding work;encoder speaker;dimensional speech embeddings"}, "7efb1788b5e0fa3b4d9932722286ba1753b42f91": {"ta_keywords": "taskspecific ontology schemata;task oriented dialogue;taskspecific ontology;contained taskspecific ontology;tasks convey semantics;context task oriented;tasks information conventionally;task oriented;tasks information;terms intents slots;slots contained taskspecific;oriented dialogue systems;intents uniform tasks;convention slots intents;ontology schemata;intents slots contained;ontology schemata schemata;propose schemata;slots intents;specified terms intents;intents slots;dialogue systems;dialogue systems required;taskspecific;contained taskspecific;context task;key information conversations;paper propose schemata;information conversations completion;terms intents", "pdf_keywords": "task oriented dialogue;dialogue state tracking;backgroundtask oriented dialogue;dialogue tod systems;description driven dialogue;dialogue modeling;dialogue modeling uses;driven dialogue modeling;dialogue models;approach dialogue modeling;dialogue models fundamental;dialogue modeling model;oriented dialogue tod;oriented dialogue development;schema guided dialogue;descriptions dialogue state;dialogue development;associate dialogue context;development dialogue models;associate dialogue;schema descriptions dialogue;named descriptiondriven diallogue;dialogue important goal;descriptiondriven diallogue;descriptions dialogue;dialogue state;dialogue internet;conversations completion given;conversations completion;descriptiondriven diallogue state"}, "9688671a573651955c26d710c12617de26715e78": {"ta_keywords": "codes distributed storage;introductionregenerating codes;introductionregenerating codes class;resultsregenerating codes possess;resultsregenerating codes;codes distributed;file resultsregenerating codes;developed codes distributed;developed codes;recently developed codes;distributed storage;codes class;distributed storage used;codes possess;introductionregenerating;codes class recently;storage used repair;file resultsregenerating;arbitrary nodes downloading;storage;data file resultsregenerating;resultsregenerating;codes;downloading data typically;nodes downloading data;codes possess addition;nodes;nodes downloading;distributed;data file", "pdf_keywords": "existence regenerating codes;nodes codes equivalent;regenerating codes construction;network codes achieve;regenerating codes achieving;multicast network coding;linear regenerating codes;systematic nodes codes;bound repair bandwidth;regenerating code distributed;regenerating codes;network coding;nodes codes;regenerating code overcomes;random network codes;regenerating codes case;network coding problem;regenerating codes introduced;exact regenerating code;repair regenerating code;regenerating code possess;network codes;repair network networks;regenerating codes 2k;storage regenerating codes;node repair bandwidth;concept regenerating codes;explicit regenerating code;systematic regenerating code;networks requirement repair"}, "148efaba70165d9faef0dac28d5fa2538cfa662d": {"ta_keywords": "cognitive biases ai;cognitive biases human;biases ai assisted;cognitive biases provide;api collaborative decision;cognitive biases;model cognitive biases;collaborative decision;collaborative decision making;ai assisted decision;account cognitive biases;biases human api;biases ai;assisted decision making;role cognitive biases;collaborative performance;assisted decision;biases human;ai assisted;human api collaborative;mathematically model cognitive;collaborative performance results;cognitive science;effects collaborative performance;model cognitive;api collaborative;field cognitive science;cognitive science account;cognitive;ai", "pdf_keywords": "bias human ai;bias ai assisted;biases human ai;biased ai assisted;ai collaborative decision;bias ai;anchoring bias ai;accuracy cognitive biases;human ai collaborative;ai assisted decision;collaborative accuracy cognitive;biased ai;cognitive biases decision;ai collaborative;human ai collaboration;biases decision support;modeling biased ai;ai human decision;human ai accuracy;collaboration human ai;making cognitive biases;ai collaboration human;agent cognitive biases;cognitive biases human;assisted collaborative decisionmaking;biases human assisted;biases decision making;ai assisted;accuracy human ai;human ai teams"}, "a6219725a9ad2079536c091f02fda2d4da6d62ac": {"ta_keywords": "reliability distributed storage;distributed storage systems;distributed storage;errorasure coding techniques;increase reliability distributed;errorasure coding;storage systems minimizing;systems minimizing storage;reliability distributed;storage systems;introduction errorasure coding;repair node failure;increase reliability;minimizing storage;minimizing storage overhead;bandwidth required repair;storage overhead bandwidth;errorasure;required repair node;node failure;repair node;used increase reliability;reliability;storage;storage overhead;node failure plays;failure plays crucial;coding techniques;failure;distributed", "pdf_keywords": ""}, "3c1001c04866647650216201feb54c927af3a05b": {"ta_keywords": "concept description language;description language learning;concept description;called concept description;introduction concept learning;concept learning;description language;language called concept;concept learning produces;description language explicit;describes learning;large concept description;learning systems concept;systems concept description;description language xed;language learning systems;learning set horn;learning systems;constrained language;language learning;concept;called concept;describes learning makes;paper describes learning;learning set;constrained language called;introduction concept;learning produces hypotheses;technique learning set;language explicit", "pdf_keywords": ""}, "60f0af1dbc2775a69f64e4351d969ac966659fb2": {"ta_keywords": "synset induction methods;based synset induction;synset induction;graph based synset;synonymy graph sparseness;clustering synonymy graph;extracted synsets methods;extracted synsets;synsets methods;quality extracted synsets;synsets methods paper;global clustering synonymy;clustering synonymy;input synonymy graph;induce synsets;synonymy graph methods;induce synsets performing;synsets;synsets performing global;synset;watset induce synsets;graph sparseness input;synsets performing;sparseness input dictionary;based synset;synonymy graph;graph sparseness;input dictionary substantially;global clustering;dictionary substantially reduce", "pdf_keywords": "synsets sparseness synonyms;synonymy dictionaryaries graph;clustering synonymy graph;synonymy graph sparseness;clustering synonymy;synset embeddings dataset;synset embeddings;global clustering synonymy;based synset embeddings;processing graph synonyms;incompleteness synonymy dictionaries;input synonymy graph;synonymy dictionaries proposed;graph based synset;input synonymy dictionaryaries;synset induction methods;synonymy graph methods;synonymy dictionaries;synonymy dictionaryaries;symmetric semantic relations;graph synonyms;language concept discovery;synonym dictionaries;synonyms analysis;synset induction;method augmentation synsets;based synset induction;dictionaryaries graph based;augmentation synsets using;extracted synsets"}, "22616702da06431668022c649a017af9b333c530": {"ta_keywords": "fact checking task;fact checking;research fact checking;misinformation stimulated research;misinformation stimulated;assessing truthfulness claim;misinformation;truthfulness claim research;increased focus misinformation;focus misinformation;claim research automating;truthfulness claim;assessing truthfulness;task assessing truthfulness;use inconsistent terminology;focus misinformation stimulated;claim research;research fact;databases journalism substantial;databases journalism;journalism;representation databases journalism;unaware use inconsistent;journalism substantial;journalism substantial progress;inconsistent terminology;use inconsistent;fact;natural language processing;inconsistent terminology impeding", "pdf_keywords": "fact checking task;automated fact checking;fact checking systems;fact checking journalistic;fact checking stimulated;use automated fact;fact checking process;fact checking methods;fact checking use;fact checking work;fact checking research;based fact checking;survey automated fact;fact checking field;fact checking conducted;fact checking;fact checking claims;research fact checking;automated fact;fact checking evidence;methods automated fact;goal automated fact;systems automated fact;fact checking paper;overview fact checking;fact checking important;fact checking increased;verification fact checking;fact checking highlight;use fact checking"}, "6b7f2f30840b0d72484784a15b3be670868a9f95": {"ta_keywords": "transfer distant languages;languages transfer;methods cross lingual;cross lingual transfer;languages transfer difficult;lingual transfer;lingual transfer distant;resource languages transfer;lingual transfer effective;background cross lingual;cross lingual;distant languages;languages especially annotated;low resource languages;lingual;syntactic analysis tools;typologically distant languages;distant languages propose;transferring typologically distant;build syntactic analysis;distant languages especially;transferring typologically;data parallel corpora;parallel corpora;difficult transferring typologically;parallel corpora available;resource languages;build syntactic;syntactic analysis;languages", "pdf_keywords": "cross lingual parsing;distant transfer parsing;lingual transfer learning;lingual parsing;prior dependency parsing;universal dependency treebanks;tagging dependency parsing;crosslingual transfer discriminative;transfer parsing;source corpus transfer;dependency treebanks;inducing syntactic structure;transfer distant languages;inducing syntactic;transferring distant languages;lingual parsing important;tagging language language;approach dependency parsing;dependency treebanks use;dependency parsing large;languages unsupervised adaptation;correlated dependency parsing;model crosslingual transfer;dependency parsing;terms dependency parsing;corpus transfer;dependency parsing propose;syntactic structure monolingual;languages study transfer;english source corpus"}, "eebc1811c55c2e5e8b3b78d0b0382ad50f22e32a": {"ta_keywords": "fact verification models;evidence verify claims;evidence sources change;claims evidence sources;require fact verification;written evidence verify;supporting evidence methods;typical fact verification;fact verification;factual changes results;evidence sources;evidence verify;differences supporting evidence;verify claims evidence;evidence methods;evidence methods present;claims evidence;supporting evidence;written evidence;retrieved written evidence;cases require fact;verification models discern;require fact;vitaminc benchmark;present vitaminc benchmark;verification models use;factual changes;evidence;slight factual changes;verification models", "pdf_keywords": "mining factual revisions;factual revisions corpus;trained factual revision;fact verification tasks;fact verification models;mining factual;factual changes results;method predicting factual;predicting factual;predicting factual nonfactual;report fact verification;fact verification classifier;generating factually consistent;factual revisions;fact verification paradigm;factual nonfactual revisions;factual revision flagging;generating factually;revisions corpus;evaluating fact verification;factual revision;method mining factual;fact verification viability;revisions corpus english;fact verification results;contrastive fact verification;training evaluating fact;models trained factual;fact verification task;factual change awikipedia"}, "6dd1e4d97dbdb370a36c25f82a9a9baaa16c836c": {"ta_keywords": "backgroundebola virus contains;backgroundebola virus;g2 subunits glycoprotein;glycoprotein subunit;subunits glycoprotein;subunits glycoprotein subunit;glycoprotein subunit possesses;virus contains single;binding membrane fusion;glycoprotein glycoprotein;membrane fusion proteolytically;single glycoprotein glycoprotein;glycoprotein glycoprotein responsible;single glycoprotein;virus contains;glycoprotein;fusion activity viral;glycoprotein responsible;proteolytically cleaved disulfide;virus;g1 g2 subunits;binding membrane;disulfide linked g1;g2 subunits;contains single glycoprotein;glycoprotein responsible receptor;receptor binding membrane;fusion proteolytically cleaved;cleaved disulfide linked;membrane fusion", "pdf_keywords": ""}, "7e122cc1a62e2f30951e14b91811896e1866dd7c": {"ta_keywords": "symbolic music generation;approach symbolic music;symbolic music;music generation;music generation uses;sequence quality samples;failures classifiers trained;failures classifiers;classifiers;music;detect failures classifiers;classifiers trained;sequence quality;transformer autoregressive model;observed sequence quality;transformer autoregressive;trained minimizing negative;uses transformer autoregressive;quality samples models;samples models;sampled sequences;trained distinguish real;trained minimizing;autoregressive;autoregressive model;real sampled sequences;autoregressive model commonly;real sampled;distinguish real sampled;classifiers trained distinguish", "pdf_keywords": ""}, "c7af06170f3d81ab761873a4c1fe0af2736eb0a2": {"ta_keywords": "affective communication automatic;prediction emotion triggers;social affective communication;automatic prediction emotion;prediction emotion;affective communication;responses television talk;communication automatic prediction;indonesian automatic prediction;triggers responses television;emotion triggers responses;social affective;emotion triggers;television talk;study social affective;responses television;television talk shows;communication automatic;talk shows resultsamong;talk shows methodsa;english indonesian automatic;automatic prediction;talk shows;affective;automatic prediction performance;indonesian automatic;communication;prediction;emotion;social", "pdf_keywords": ""}, "8dd3b88ac87372c9f4428029ac12288ff3405199": {"ta_keywords": "variability blood lipids;backgroundvariability blood lipids;blood lipids neutrophil;lipids affects neutrophil;lipids neutrophil lymphocyte;blood lipids affects;lipids neutrophil;blood lipids;neutrophil lymphocyte ratio;association variability blood;lipids affects;affects neutrophil lymphocyte;lymphocyte ratio assessed;lipoprotein cholesterol;affects neutrophil;density lipoprotein;density lipoprotein cholesterol;high density lipoprotein;neutrophil lymphocyte;lipoprotein;lymphocyte ratio patients;backgroundvariability blood;variability blood;lymphocyte ratio;neutrophil;lipids;cholesterol;percutaneous coronary;coronary intervention;percutaneous coronary intervention", "pdf_keywords": ""}, "36b6abfb32ea56208a2858b558acbdd001c965e9": {"ta_keywords": "neural machine translation;translation generation;machine translation generation;computationallinguistics ac 2018;machine translation;computationallinguistics;translation generation held;computationallinguistics ac;efficient neural machine;association computationallinguistics;second workshop neural;workshop neural machine;conference association computationallinguistics;association computationallinguistics ac;linguistic structure domain;efficient neural;structure domain adaptation;task efficient neural;workshop neural;domain adaptation;neural machine;linguistic structure;particular linguistic structure;linguistic;adaptation data augmentation;note particular linguistic;particular linguistic;domain adaptation data;neural;translation", "pdf_keywords": "neural machine translation;translation generation;translation generation conducted;machine translation generation;machine translation;translation generation wmc;improve translation speed;translation accurate efficient;machine translation wmt;task improve translation;systems nonlingual translation;translation organization;national translation organization;translation speed;improve translation;nonlingual translation accurate;translation organization nmt;translation speed accuracy;nonlingual translation;neural language language;national national translation;creating systems nonlingual;national translation;translation wmt decades;systems nonlingual;new neural language;workshop neural machine;translation wmt;language learning opennmt;neural language"}, "47b6023808002dfde031c17b34dcb1b522d3b326": {"ta_keywords": "tosshishiha rice cooker;rice cooker scc;new tosshishiha rice;rice cooker;tosshishiha rice;cooker scc;cooker scc promising;rice;cooker;tosshishiha;development new tosshishiha;new tosshishiha;care patients scc;patients scc;improve quality care;quality care patients;scc promising approach;quality care;care patients;scc promising;patients;scc;improve quality;promising approach;promising approach improve;quality;approach improve quality;care;promising;new", "pdf_keywords": ""}, "743d1aae44a12fb37b743ec947fad41cba9831b8": {"ta_keywords": "informative text generation;conditional text generation;text generation;text generation using;language generation;text generation improve;introductionpragmatically informative text;standard language generation;generation improve informativeness;generate output text;generation using techniques;techniques computational pragmatics;computational pragmatics;introductionpragmatically informative;formulate language production;computational pragmatics methodsthese;generation using;models conditional text;informative text;language production game;language production;pragmatics;conditional text;pragmatics methodsthese techniques;pragmatics methodsthese;text describes;generate output;grounded language learning;informativeness models conditional;introductionpragmatically", "pdf_keywords": "language generation tasks;natural language generation;language generation systems;informative text generation;language generation computational;language generation;conditional text generation;text generation fundamental;experiments language generation;text generation;formulate language production;methods text generation;pragmatics improve performance;text generation using;generation implementation pragmatics;generation tasks generation;theart language generation;pragmatics reconstructor;pragmatics reconstructor based;generation fundamental task;traditional language generation;generation meaning representations;generation tasks;language pragmatic methods;text generation implementation;computational pragmatics;task natural language;generation using techniques;language pragmatic;models pragmatics reconstructor"}, "ba5e3559a2d54bb0e8d7678c9905b4a77da63f71": {"ta_keywords": "incentivizes truthful responses;responses objective evaluations;commerce platforms eliciting;truthful responses objective;evaluations products;square root agreement;reward evaluation;reward evaluation answer;evaluations products services;obtaining evaluations products;agreement rule;mechanism incentivizes truthful;gets reward evaluation;responses objective;objective evaluations platforms;eliciting informative responses;challenge obtaining evaluations;objective evaluations;root agreement rule;evaluation answer matches;agent gets reward;commerce platforms;evaluations platforms;simple reward;agreement rule sr;simple reward mechanism;incentivizes truthful;agreement;responses absence verifiability;verifiability", "pdf_keywords": "incentivizing truthful responses;truthful behavior incentive;incentivizing truthful feedback;free incentive mechanism;incentivizes truthful responses;mechanism inentivizing truthful;incentive mechanism;mechanisms incentivizing truthful;peer agent evaluation;incentivize truthful behavior;mechanism incentivizes truthful;incentive mechanism single;truthful mechanisms incentivizing;incentivizing truthfulness mechanism;evaluated peer agent;incentivizing truthful behavior;mechanisms balance incentives;platforms truthfully elicits;inentivizing truthful feedback;behavior incentive;agreement rule inentivizing;rewards agreement popularity;rule incentivizing truthful;mechanism incentivizing truthfulness;platforms expected reward;incentive compatible respect;minimal free incentive;rewards agreement;equilibrium truthful strategy;agent evaluated peer"}, "52824fb6eb5d3b55fb6634c77dc80f5826964464": {"ta_keywords": "introductioninductive specification recovery;specification recovery;specification recovery understanding;introductioninductive specification;datalog specifications;used datalog specifications;method used datalog;software learning example;datalog specifications forc;software learning;datalog;understanding software learning;recovery understanding software;used datalog;understanding software;examples behavior inductive;technique instrumented code;behavior inductive learning;generating examples behavior;behavior inductive;learning example behaviors;behavior case study;inductive learning techniques;specifications forc code;specification;inductive learning;instrumented code;cases generating examples;instrumented code run;techniques used generalize", "pdf_keywords": ""}, "0cd693f1a1223f25e89c1f5efdedd7c3b7846691": {"ta_keywords": "traffic parking data;traffic parking;traffic congestion;urban traffic;introduction urban traffic;traffic searching parking;parking data;city level traffic;overall traffic congestion;traffic congestion end;urban traffic searching;parking data determine;traffic;level traffic parking;cruising curbside parking;queueing network;queueing network present;curbside parking;traffic searching;kind queueing network;overall traffic;searching parking;level traffic;curbside parking contributes;parking;queueing;congestion;data determine cruising;parking paper explore;contributes overall traffic", "pdf_keywords": "queue network model;parking queues network;queue network;network queues;queues network;queue network similar;queues network perform;network queues exogenous;queues analyze traffic;parking queues;curbside parking queues;capacity queues tasks;queues model;queue regular network;capacity queues model;queues exogenous arrival;used queue network;queues tasks;queues model provides;queue fundamental problem;finite capacity queues;capacity queues;queue search;queues analyze;node queue network;queue fundamental;queue available server;queues tasks arrive;queues exogenous;queue"}, "1f0524971c20a06d745ab784689eb8833435fde1": {"ta_keywords": "fact extraction virification;fact extraction;human written factoid;results fact extraction;evidence retrieved fromwikipedia;introductionthe fact extraction;written factoid claims;written factoid;factoid claims;factoid claims supported;results fact;using evidence retrieved;evidence retrieved;fever sharedd task;classify human written;factoid;using evidence;supported using evidence;virification fever sharedd;fromwikipedia;fever sharedd;present results fact;retrieved fromwikipedia;evidence;fromwikipedia resultswe;fromwikipedia resultswe received;fact;extraction virification fever;task challenged;task challenged participants", "pdf_keywords": "task natural language;process natural language;natural language inference;method natural language;natural language technologies;natural language;approaches natural language;approach natural language;development natural language;natural language world;fever shared task;emergence natural language;question answering text;teams fact extraction;fact extraction verification;extraction verification fever;teams task natural;task automatic information;language inference designed;developed fact extraction;enable natural language;question answering;assess validity humanannotated;language inference compare;results fact extraction;fact extraction;language inference approaches;research group fever;language inference;language inference used"}, "68731c68773b117250f04509103031109b222d27": {"ta_keywords": "introductioninformation extraction global;introductioninformation extraction;extraction global structure;extraction global;structure constraints extracting;knowledge bases supervision;extracting entities;constraints extracting entities;massive text corpora;framework distant supervision;external knowledge bases;knowledge bases;extracting entities relations;open ionization calledremine;constraints extracting;understanding massive text;extracting;open ionization;text corpora;bases supervision effectively;entities relations text;global structural signal;ionization calledremine;bases supervision;novel open ionization;massive text;ionization;global structural;extraction;context signal", "pdf_keywords": ""}, "fd8e176087335355ff5e81821a616d15ec8d3346": {"ta_keywords": "weak indirect supervision;generating labeled training;recent weak supervision;noisy supervision sources;weak supervision frameworks;weak supervision;indirect supervision;labeled training sets;indirect supervision wis;labeled training;potentially noisy supervision;supervision sources existing;noisy supervision;restricted supervision sources;supervision frameworks synthesize;supervision sources;synthesize training labels;generating labeled;training labels;supervision sources share;supervision frameworks;frameworks restricted supervision;introduction generating labeled;restricted supervision;labeled;weak indirect;training labels multiple;supervision;formulate weak indirect;supervision wis new", "pdf_keywords": "indirect labeling;leverage indirect labeling;indirect labeling functions;creating labeled training;predicting unseen labels;labels learned models;grained label relations;use indirect labeling;distinguishability indirect supervision;learned label based;label relations synthesize;models dependency labeling;generating label relation;weak indirect supervision;approaches label relations;probabilistic label relation;synthesizing training labels;supervision sources label;incorporating label relation;label relations;labeled training;training labels introduce;training labels distinguishable;explicitly incorporating label;labeled training sets;indirect supervision sources;creating labeled;based labeling;create training labels;training labels"}, "e34f9e9163b13de00707157feda6a8b853c5c82d": {"ta_keywords": "duplicates crowddsourced lexical;crowddsourced lexical resources;fuzzy duplicates crowddsourced;crowddsourced lexical;duplicates crowddsourced;collaboratively created lexical;created lexical resources;lexical resources methods;lexical resources trending;fuzzy duplicates;lexical resources;crowddsourced;resources methods collaboratively;created lexical;lexical;collaboratively created;constructing resource;elimination fuzzy duplicates;collaboratively;duplicates;constructing resource approach;resource approach;resources trending approach;methods collaboratively created;share knowledge;methods collaboratively;resources trending;resource;aim constructing resource;resource approach tends", "pdf_keywords": ""}, "e1d35deec12d18e53ca97a3cf4071526ad47968d": {"ta_keywords": "pretrained language models;transformer language models;pretrained language;large pretrained language;language models;learn linguistic knowledge;linguistic knowledge recent;ability learn linguistic;learn linguistic;language models affect;linguistic knowledge;linguistic knowledge encoded;kinds linguistic knowledge;language models led;transformer language;linguistic;examining kinds linguistic;language;modifications transformer language;knowledge encoded models;knowledge recent progress;knowledge encoded;kinds linguistic;ability learn;knowledge;encoded models computational;models acquisition;factors training;knowledge recent;various factors training", "pdf_keywords": ""}, "e2ebf18e0b88752bd3ff905d2fba74213dcd2c51": {"ta_keywords": "backgroundan electrolarynx speaking;backgroundan electrolarynx;electrolarynx speaking aid;electrolaryngeal el speech;electrolarynx speaking;laryngectomees produce electrolaryngeal;electrolarynx;sounds help laryngectomees;produce electrolaryngeal el;electrolaryngeal el;excitation sounds make;excitation sounds;naturally sounding el;mechanical excitation sounds;generate excitation sounds;sounding el speech;produce electrolaryngeal;electrolaryngeal;excitation sounds help;speaking aid device;help laryngectomees produce;laryngectomees;help laryngectomees;sounding el;laryngectomees produce;naturally sounding;el speech;produce naturally sounding;artificially generate excitation;excitation", "pdf_keywords": ""}, "595a79ca667258ca2a4f5e7775e95a0fb0a0f024": {"ta_keywords": "monotone games complexity;free gradient play;strongly monotone games;gradient play strongly;games complexity target;games complexity;play strongly monotone;monotone games;gradient play;free gradient;derivative free gradient;method unconstrained optimization;unconstrained optimization;unconstrained optimization result;complexity target accuracy;derivative free;efficiency estimate actually;efficiency estimate;known efficiency;known efficiency guarantee;reduces known efficiency;shows derivative free;complexity target;complexity;shows efficiency estimate;strongly monotone;optimization;play strongly;efficiency;method unconstrained", "pdf_keywords": "monotone games method;game algorithms converge;general game algorithms;strongly monotone games;game algorithms;monotone games;game strongly monotone;convex optimization bandit;free gradient play;players formulate gradient;algorithm effective game;optimization bandit;game algorithm effective;game algorithm;convergence nash equilibrium;function general game;convergence nash;gradient play strongly;approach game algorithm;computational approach game;nash equilibrium;gradient play players;game perturbed game;gradient play;monotone games influential;games method;gamethe problem smoothed;play strongly monotone;learning called bandit;nash equilibrium paper"}, "36f7827bc344f9c2198dcb29732c525c68dc637a": {"ta_keywords": "traveling salesperson game;traveling salesperson problem;modeled traveling salesperson;locations traveling salesperson;traveling salesperson;cost serve analysis;cost serve location;calculating cost serve;transferable utility game;salesperson game;cooperative transferable utility;cost serve;salesperson game tschg;tschg cooperative transferable;locations traveling;operationally transportation settings;modeled traveling;utility game agents;formally modeled traveling;strategically operationally transportation;introduction cost serve;operationally transportation;transportation settings;cooperative transferable;calculating cost;transportation;utility game;correspond locations traveling;transferable utility;transportation settings problem", "pdf_keywords": "shpley value cooperative;value cooperative game;fair allocation traveling;quantities cooperative game;schemes cooperative games;cooperative games giving;costs quantities cooperative;value coalitional game;shpley value coalitional;theory shpley allocations;cooperative game theory;principled fair allocation;allocation concepts cooperative;cooperative games;optimal tour shpley;principle cooperative game;cost coalition principle;cooperative game;coalitions computation shpley;game theory shpley;value cooperative;tour shpley value;shpley value competitive;game shapley value;shpley allocations highly;shpley allocations;fair allocation;quantities cooperative;value competitive games;asymmetrical allocation good"}, "ead1e044d284f3deecd32c2d5cc89fe513195a0a": {"ta_keywords": "synonymy graph augmentation;input synonymy graph;synonymy graph connectivity;introduction synonymy graph;approach synonymy graph;synonymy graph;word sense induction;graph based word;synonyms input;synonymy relation;potential synonyms input;synonyms;synonyms input synonymy;synonymy relation implies;synonymy graph conduct;sense induction;edges potential synonyms;graph augmentation;graph augmentation approach;input synonymy;synonymy;word sense;introduction synonymy;sense induction paper;property synonymy relation;graph connectivity;based word sense;potential synonyms;equivalence property synonymy;graph connectivity graph", "pdf_keywords": ""}, "7e63be5285e6596fbbc6c56bc89f7b6fd8bbe8c5": {"ta_keywords": "model explanations effective;model explanations;model debugging;model debugging methodsin;explanations effective diagnosing;explaining model prediction;errors model debugging;hoc model explanations;explanation methods proposed;explanations effective;diagnosing model errors;diagnosing model;explaining model;debugging;explanation methods;effective diagnosing model;explanations;challenge explaining model;debugging methodsin;resultswe categorize textitbugs;categorize textitbugs;debugging methodsin response;categorize textitbugs based;model errors model;textitbugs based;textitbugs based source;model prediction vast;textitbugs;model errors;errors model", "pdf_keywords": "model explanations effective;debuging models feature;model debugging effectiveness;debuging models;explanations effective diagnosing;methods debuging models;model explanations;models feature attributions;assess feature attributions;model debugging;defective models attributions;explanation methods assessed;feature attributions easily;model debugging tasks;explanation methods assess;explanation methods feature;explanations effective;errors model debugging;model bugs provide;feature attributions;bug categorization provide;explanations widely used;debugging effectiveness;overview bug categorization;explanations widely;effective debugging;bug categorization;diagnosing model errors;model explanation methods;models attributions present"}, "bd8922f8cc8284553dc9e6db529af309298451fe": {"ta_keywords": "anrar encoder decoder;encoder decoder anrar;decoder anrar encoder;anrar encoder;decoder anrar encode;anrar encode;end anrar encoder;decoder anrar;anrar;end anrar;end end anrar;introductionpretraining backtranslation end;introductionpretraining backtranslation;encoder decoder;backtranslation end;backtranslation end end;encoder;decoder;backtranslation;encode;introductionpretraining;end end;end", "pdf_keywords": ""}, "25ddddbd0bd1cfebf1548b2ee91bb1bbd05fdff1": {"ta_keywords": "episodic transformer multimodal;complex human instructions;challenges neural agents;language instructions dynamic;neural agents;instructions dynamic environments;transformer multimodal;multimodal transformer encodes;natural language instructions;multimodal;transformer encodes language;human instructions methods;instructions dynamic;language inputs episode;encodes language inputs;visual observations actions;long sequence subtasks;sequence subtasks;neural agents paper;human instructions;encodes language;multimodal transformer;transformer multimodal transformer;episodic transformer;synthetic instructions intermediate;propose episodic transformer;language inputs;challenges neural;sequence subtasks understanding;neural", "pdf_keywords": "propose attentionbased architecture;attentionbased architecture vision;attentionbased architecture;recurrent based architectures;improving vision language;pretrained language encoder;vision language action;vision language navigation;challenges neural agents;pretrain language encoder;language encoder pretraining;language encoder agent;vision language;learning propose attentionbased;language navigation tasks;optimize language attention;language attention;synthetic language pretrain;implement multimodal transformers;multimodal transformers;neural agents objectiveinteraction;long sequence subtasks;lstm based agent;episodic transformer multimodal;transformer multimodal;visual language navigation;encode multimodal transformer;significantly improved recurrent;embodied navigation agents;inputs encode multimodal"}, "0805cb1b26577f08f84190445992f7f0584e4742": {"ta_keywords": "end information extraction;information extraction;information extraction multiple;knowledge base;knowledge base domain;builds knowledge base;oera cru usc;cru usc isi;results builds knowledge;usc isi performs;builds knowledge;usc isi;end end information;end information;oera cru;media languages;reasoning answer questions;information;cru usc;multiple media languages;media languages english;isi;extraction multiple;knowledge;isi performs;hypothesis creation reasoning;isi performs end;oera;extraction multiple media;extraction", "pdf_keywords": ""}, "352ac73b7d92afa915c06026a4336927d550cec3": {"ta_keywords": "introduction graph neural;graph neural network;graph neural networks;graph neural;novel graph neural;relation extraction;parameters relation extraction;networks generated parameters;networks generated;neural networks generated;generated parameters relation;neural network generated;network generated parameters;relation extraction relatively;novel graph;parameters gp gns;propose novel graph;introduction graph;generated parameters gp;generated parameters;gns methods parameters;parameters propagation module;relation;gns methods;graph;parameters relation;gp gns methods;neural network;networks;sentences inputs", "pdf_keywords": "neural relation extraction;gnns useful relational;relation extraction;improving relational reasoning;relation extraction distantly;relation extraction text;relation extraction data;relational reasoning machine;relation extraction methods;relation extraction human;relation extraction using;models graph neural;relational reasoning natural;relational reasoning indispensable;parameters graph neural;relational reasoning relational;knowledge relational reasoning;graph neural;network classify relations;progress improving relational;task relation extraction;graph neural networks;reasoning relational;approach relation extraction;hop relational reasoning;novel graph neural;tasks relation extraction;useful relational reasoning;relational reasoning;framework graph neural"}, "b53689b8c28353106f327f0981b108eb67816053": {"ta_keywords": "based syntactic preprocessing;syntactic preprocessing;syntactic preprocessing syntax;preprocessing syntax based;preprocessing syntax;based machine translation;rule based syntactic;machine translation;machine translation methods;syntax based;syntactic information linguistically;preprocessing;machine translation bm;literature conducted preprocessing;syntax based machine;based syntactic;using syntactic;translation methods review;using syntactic information;technique using syntactic;conducted preprocessing;syntactic;syntactic information;preprocessing technique using;preprocessing technique;phrase based machine;machine translation aim;syntax;translation methods;conducted preprocessing technique", "pdf_keywords": ""}, "21d45b4923ad165fbb6612e08d06f9d786f9b4cc": {"ta_keywords": "training commonsense models;commonsense knowledge graphs;commonsense models work;commonsense models;commonsense models methodswe;commonsense knowledge;graphs train commonsense;train commonsense models;models author commonsense;training commonsense;commonsense models gone;author commonsense knowledge;commonsense;alternative machine corpus;humans author commonsense;practice training commonsense;train commonsense;machine corpus;corpus machine general;author commonsense;language models;machine corpus machine;corpus machine;general language models;human corpus machine;language models author;corpus machine humans;knowledge graphs train;corpus;human corpus", "pdf_keywords": "commonsense knowledge graphs;knowledge graphs commonsense;model commonsense corpus;training commonsense models;language models commonsense;transfer commonsense knowledge;models commonsense reasoning;graphs commonsense leveraging;curating commonsense knowledge;graphs train commonsense;model generate commonsense;commonsense model commonsense;commonsense knowledge sources;model commonsense;commonsense knowledge;knowledge graphs training;commonsense models work;commonsense models ability;commonsense models;commonsense leveraging;models commonsense;training commonsense;commonsense inference;graphs commonsense;quality commonsense knowledge;generate transfer commonsense;commonsense corpus;approach commonsense inference;commonsense knowledge fundamental;knowledge use commonsense"}, "53e0abebd9aef5915f72147d3674596a0051748c": {"ta_keywords": "backgrounddata protection ai;backgrounddata protection;protection ai services;data resulting privacy;ai based services;protection ai;ai services;privacy;ai services important;edge research privacy;research privacy;backgrounddata;user data;resulting privacy;privacy implications;personalization better support;support ai based;privacy implications facto;resulting privacy implications;user services;personalization better;personalization;support ai;require user data;better support ai;enhanced personalization better;ai based;enabling enhanced personalization;enhanced personalization;user data resulting", "pdf_keywords": "privacy personalized ai;ai services privacy;personalized ai services;data protection ai;privacy preserving ai;ai services personal;protection ai services;personalized ai;use personalized ai;personalized artificial intelligence;terminology personalized ai;privacy privacy personalized;privacy personalized;personalized ai models;approach personalized ai;personalization privacy;related personalized ai;principles personalized ai;privacy data protection;services use personalized;ai services data;services privacy;services personal data;personalization privacy paradox;services data protection;services privacy privacy;protection ai;services use ai;ai services use;protection approach ai"}, "b6145cc19acfbec31373446a2dba210cc9b1eb7f": {"ta_keywords": "supervised relation extraction;relation extraction;relation extraction jointly;relation extraction combine;extraction relation extraction;distantly supervised relation;supervised relation;extraction jointly learning;concept instance extraction;structured corpora;small structured corpora;instance extraction relation;structured corpora sections;extract additional relation;extraction relation;relation arguments distantly;extraction jointly;distantly supervised;distantly labeled examples;instance extraction;clinical messagewe propose;key clinical messagewe;performance distantly supervised;jointly learning;clinical messagewe;relation examples;correspond relation arguments;identified correspond relation;additional relation examples;relation", "pdf_keywords": "supervised relation extraction;mentions structured corpus;relation extraction;relation extraction jointly;distantly supervised relation;extraction relation extraction;useful structured corpus;relation extraction combine;classify relation mentions;structured corpus useful;extraction jointly learning;distantly supervised information;small structured corpus;mentions useful structured;supervised information extraction;mentions large corpus;distant labeling methods;label propagation mentions;information extraction;small structured corpora;structured corpus commonly;large corpus text;concept mentions structured;structured corpus;generating concept mentions;relation mentions useful;concept instance extraction;corpus target relation;distantlylabeled examples sections;examples structured corpus"}, "181e1d4b08dc62237277a6a743576facd8c5e572": {"ta_keywords": "voice activity detection;speaker voice activity;speaker diarization;vad speaker diarization;speaker diarization highly;results speaker diarization;target speaker voice;speaker diarization unknown;highly overlapped speech;overlapped speech;voice activity;target speaker;speaker voice;overlapped speech original;vad speaker;promising results speaker;extend vad speaker;results speaker;speech original model;speaker;known number speakers;voice;numbers speakers achieved;diarization highly overlapped;unknown numbers speakers;numbers speakers;speakers achieved steps;speakers achieved;speech;number speakers", "pdf_keywords": "speaker conversations diarization;voice activity detection;approach speaker diarization;speaker diarization;speech diarization based;approach speech diarization;speaker voice activity;speaker diarization highly;results speaker diarization;speech diarization;conversations diarization improved;diarization multi speaker;conversations diarization;voice activity;vad speaker diarization;recognition speech;speech recognition large;speaker conversations;speech recognition;speaker diarization unknown;effective detecting speaker;component speech processing;speech processing;speech recognition speech;speakers inference observed;multi speaker conversations;highly overlapped speech;speaker diarization article;recognition speech recognition;messagetarget speaker voice"}, "4236a5f650f5b7ced7512b5072a062b521220b31": {"ta_keywords": "predicting traffic;traffic speed prediction;predicting traffic speed;aim predicting traffic;large traffic data;traffic data;traffic data quite;transfer learning;propose transfer learning;speed prediction;traffic speed urban;transportation smart city;intelligent transportation smart;applications intelligent transportation;intelligent transportation;traffic;traffic speed;large traffic;speed prediction benefit;collecting large traffic;recent supervised machine;smart city recent;city recent supervised;pulmonary disease iot;transportation smart;speed urban;introduction traffic speed;prediction;machine learning approaches;speed urban areas", "pdf_keywords": ""}, "e25ce2a7b28699e1d57803ef977175937ce50923": {"ta_keywords": "annotation efefficient corpus;partial annotation efefficient;annotation efefficient;annotating particular words;based partial annotation;efefficient corpus construction;efefficient corpus;partial annotation;utilize corpus based;utilize corpus;corpus based techniques;annotation;annotating;corpus based;corpus construction;word based partial;corpus;effective natural language;annotating particular;natural language processing;corpus construction order;benefit annotating;linguistic resources necessary;order utilize corpus;creation linguistic resources;natural language;benefit annotating particular;words high information;linguistic resources;document sentence level", "pdf_keywords": ""}, "0e532d1489d7420cff7ff8aa211ded08e7d57fe9": {"ta_keywords": "online learning algorithms;learning strongly convex;algorithms strongly convex;strongly convex loss;learning algorithms strongly;convex loss functions;machine learning strongly;convex loss;learning strongly;analysis online learning;strongly convex;learning algorithms;online learning;algorithms strongly;loss functions analysis;loss functions;problems machine learning;machine learning;convex;large number optimization;optimization;analysis online;algorithms;learning;optimization problems;loss;optimization problems machine;number optimization problems;number optimization;strongly", "pdf_keywords": ""}, "1109f787fc8d51feb3bae9bf6e1945dc4a1191e7": {"ta_keywords": "improvements explanations ai;explanations ai;motivate explainable ai;explainable ai studies;explanations ai outperformed;improves ai explains;explainable ai;ai explains recommendations;tasks improves ai;ai studies showing;ai explains;showing human ai;human ai team;ai studies;human ai;improves ai;studies datasets ai;ai team performance;researchers motivate explainable;ai outperformed human;ai team;observed improvements explanations;improvements explanations;ai;datasets ai;ai accuracy;task explaining;datasets ai accuracy;ai accuracy comparable;ai outperformed", "pdf_keywords": "explanations ai recommendation;ai effectiveness explanations;ai explanations results;teams adaptive explanations;explain ai recommendation;explanations team performance;ai explanations complementary;explanations human ai;ai explanations;explanations improve team;human explanations improve;explanations ai;ai recommendation improve;explanations improved team;help ai explanations;ai explains suggestions;team performance ai;humans adaptive explanations;effectiveness explanations team;effect ai explanations;generated explanations ai;team performance explanations;ai team performance;ask ai explanations;ai explanations help;adaptive explanations improve;ai assistance improve;improved ai explains;team improved ai;strategy adaptive explanations"}, "328a9fe143639810d6413c2cc901ec3afa6aa607": {"ta_keywords": "molecular dynamics displacements;peridynamic solid models;surrogate molecular dynamics;peridynamic solid model;model surrogate molecular;molecular dynamics;solid model surrogate;discretized linear peridynamic;solid models;md data optimal;model allow peridynamic;dynamics displacements maximize;dynamics displacements;posedness conditions discretized;linear peridynamic solid;solid models sign;posedness resulting model;allow peridynamic influence;solid model;md data;extract md data;peridynamic solid;learnt model;peridynamic influence function;displacements maximize accuracy;model surrogate;optimal linear peridynamic;models;framework extract md;displacements maximize", "pdf_keywords": ""}, "350e5f5a89cbb3a23442c9d0d3e59fc50d665dbb": {"ta_keywords": "greece electricity market;greece electricity;aspects greece electricity;electricity market;electricity market based;model greece day;greece day;model greece;greece day ahead;simplified model greece;scheduling problem;scheduling;units load frequency;generating units load;scheduling problem constitutes;day ahead scheduling;electricity;main aspects greece;aspects greece;greece;ahead scheduling problem;load frequency related;objective function pricing;function pricing;units load;ahead scheduling;load frequency;pricing;ancillary services formulate;related ancillary services", "pdf_keywords": ""}, "2226f5a13e3e9faac2e228e95175d3e612b52395": {"ta_keywords": "group ai acm;ai acm;ai acm siagai;acm sigi;worldwide acm sigi;acm siagai presented;acm sigi recently;acm siagai;agencies worldwide acm;acm;acm special group;acm special;worldwide acm;report acm;report acm special;activity report acm;group ai;special group ai;sigi chapters professional;siagai presented membership;spm institute;cm sigi;spm institute science;cm sigi chapters;chapter spm institute;new cm sigi;ai;institute;institute science technology;institute science", "pdf_keywords": ""}, "c102ac8c779ee0a53dc8e4ee20b4088ac2c7e186": {"ta_keywords": "end speech recognition;speech recognition deep;attention based encoder;connectionist temporal classification;end end speech;ct attentionbased end;recognition deep cnn;joint ct attentionbased;deep cnn;end automatic speech;attentionbased end end;end speech;attentionbased end;speech recognition;deep cnn incoder;classification cc attention;joint connectionist temporal;speech recognition model;temporal classification cc;automatic speech recognition;ct attentionbased;recognition deep;encoder decoder network;automatic speech;cc attention based;characters joint connectionist;connectionist temporal;encoder;recognition model learn;decoder network encoder", "pdf_keywords": "speech recognition attention;attention based encoder;joint speech recognition;decoding combination attention;modeling speech recognition;recognition attention model;classification cc attention;networks integrated recurrent;speech recognition;networks speech recognition;attention based probabilities;attention model;neural networks speech;attention model based;joint decoding neural;data driven attention;encoder deep;cc attention based;automatic speech;automatic speech recognition;modeling speech;decoder networks joint;speech recognition ar;recognition attention;decoder networks;end automatic speech;combination attention based;deep cnn encoder;approach speech recognition;deep cnn"}, "1144cc3e86b1cc4160aedddb085d7861d4b528dc": {"ta_keywords": "softmax rn transducer;sampled softmax rn;sampled softmax;apply sampled softmax;softmax rn;rn transducer;softmax;rn transducer requires;speech recognition thern;backgroundrn transducer promising;speech recognition;automatic speech;automatic speech recognition;backgroundrn transducer;end automatic speech;subset vocabulary training;vocabulary training;recognition thern transducer;transducer promising architectures;thern transducer;transducer promising;strong accuracy streaming;thern transducer advantages;transducer requires;transducer;memory consumption training;accuracy streaming;accuracy streaming friendly;transducer advantages;transducer requires small", "pdf_keywords": "softmax rn transducer;sampled softmax rntransducer;softmax rntransducer requires;softmax rntransducer;sampled softmax rn;training sampled softmax;extend sampled softmax;sampled softmax propose;sampled softmaxthe memory;sampled softmax;softmax rn;implementation sampled softmax;distributions sampled softmax;rn transducer memory;softmaxthe memory;sampled softmaxthe;softmax propose new;sampled softmax approximate;softmax;softmax propose;apply sampled softmax;sampled softmax example;sampled softmax experimentally;experimentally sampled softmaxthe;sampled softmax gives;training rn transducer;softmax experimentally sampled;softmaxthe memory consumption;softmax gives huge;method sampled softmax"}, "69e9a040ef633c60533843442529cc68c5f12932": {"ta_keywords": "graph clustering method;graph clustering;power iteration clustering;scalable graph clustering;iteration clustering;clustering method;iteration clustering pic;effective cluster;clustering pic methodspic;simple scalable graph;scalable graph;clustering method called;clustering;effective cluster indicator;cluster;turns effective cluster;clustering pic;cluster indicator;cluster indicator consistently;similarity matrix data;matrix data embedding;spectral methods;graph;dimensional embedding dataset;power iteration normalized;wise similarity matrix;spectral methods ncut;power iteration;used spectral methods;data embedding", "pdf_keywords": ""}, "3dc4580a154df87f3a56aa3d16b00c5a935ebe15": {"ta_keywords": "citation reviewer;does citation reviewer;citation reviewer work;cite prospective reviewers;investigate citation bias;citation bias;citation bias actually;evaluation scientific impact;citations;citations play important;does citation;scientific impact anecdotes;prospective reviewers;citation;work investigate citation;investigate citation;cite prospective;citations play;evaluation scientific;fact cite prospective;prospective reviewers try;scientific impact;positive evaluation submission;impact anecdotes;background citations;cite;reviewer work submission;reviewers try obtaining;reviewers;positive evaluation", "pdf_keywords": "citation bias peer;investigated citation bias;citation bias peerthe;citation bias conducted;identifies citation bias;citation bias;citation bias venuescitation;bias peer review;citation bias present;suggest citation bias;bias peer reviewed;studies citation bias;citation bias create;conferences citation bias;test citation bias;citation bias strong;venues citation bias;citation bias genuine;indicator citation bias;causality citation bias;citations;reassessment citation bias;cite prospective reviewers;peer review evaluation;investigated citation;reviewed journals citation;peer review studies;analysis peer review;analysis identifies citation;genuinely present citations"}, "3cfdec4f1664fcdc20fd5a6d3f86e7b40cf19f70": {"ta_keywords": "length neural encoder;control length encoder;length encoder decoder;length encoder;sequence length neural;introductionneuro encoder decoder;encoder decoder outputs;neural encoder decoder;introductionneuro encoder;neural encoder;output sequence length;encoder decoder models;encoder decoder;encoder;controlling output sequence;length neural;decoder models control;decoder outputs;sequence length;decoder models;ability neural encoder;sequence generation;sequence generation tasks;models control length;output sequence;decoder models shown;decoder;control length;decoder outputs methodsin;like control length", "pdf_keywords": "generating gaword summaries;summarization generate concise;summarization generate;generate gaword summaries;text summarization generate;able generate summaries;length summary summarization;summaries development neural;generate concise summaries;generate summaries;sequence generation tasks;generate summaries various;sentence summaries learning;summaries desired length;gaword summaries propose;gaword summaries algorithm;sequence length neuralthe;generating short sentence;summarization systems able;summarization input source;summaries learning;gaword summaries;introductionneuro encoder decoder;sequence generation;length encoder decoder;summarization systems;length neural encoderdecoder;text summarization input;gaword word summaries;sequences decoding based"}, "7a070c558cdb9c525559d1ad48159551381750c9": {"ta_keywords": "answering big corpus;question answering big;big corpus methodswe;question answering;gram machine ngm;big corpus;gram machine;called gram machine;tasks question answering;symbolic meaning representations;corpus methodswe propose;knowledge intensive tasks;corpus methodswe;approach called gram;corpus;ngm babii tasks;gram;babii tasks;answering big;neural networks limited;success neural networks;knowledge intensive;meaning representations indexed;neural networks;complexity independent text;limited knowledge intensive;called gram;machine ngm babii;ngm babii;neural", "pdf_keywords": "question answering;trainable efficiently answering;linguistic knowledge storages;text reasoning tasks;knowledge storage;build knowledge storage;knowledge storages;knowledge storage crucial;knowledge storage generate;knowledge answer questions;text understanding models;information knowledge storage;efficiently answering;efficiently answering questions;answer questions gram;answer questions model;text knowledge;knowledge storage introduce;knowledge decoder;uses knowledge storage;answering questions large;neural network text;single knowledge storage;learning knowledge representations;tasks synthetic text;knowl text knowledge;knowledge storages programs;question answering provide;text knowledge store;text reasoning"}, "6516b800482100731f0eb348f678ad30799c839f": {"ta_keywords": "neologisms semantic neighborhoods;neologisms semantic;analysis neologisms semantic;semantic analysis neologisms;distributional semantic;born distributional semantic;distributional semantic analysis;words born distributional;neologisms;analysis neologisms;words emerge language;semantic neighbors;semantic sparsity;semantic sparsity frequency;phenomenon neology;words emerge;diachronic corpora english;phenomenon neology process;semantic neighborhoods;rates semantic neighbors;large diachronic corpora;new words emerge;semantic;neology process new;factors semantic sparsity;diachronic corpora;process new words;neology process;neology;growth rates semantic", "pdf_keywords": "semantic differences neologisms;word emergence significantly;words neologisms;words neologisms common;corpus neologisms likely;corpus neologisms;associated word emergence;emergence neologisms area;neologisms words;emergence neologisms;word emergence;corpus neologismneologisms;neologisms words neologisms;represent neologisms words;analyzing word emergence;corpus neologismneologisms common;neologisms area;neologisms differ terms;words neologisms course;differences neologisms;neologisms;words likely emerge;neologism set words;words emerge language;neologisms common;semantic space highly;semantically related words;neologisms common topic;correlated emergence neologisms;semantic space hypothesize"}, "ac03cf22e2a831ab030ae33b5ddf5f9864917a17": {"ta_keywords": "annual annual annual;annual annual;annual", "pdf_keywords": ""}, "0a7f95adbaf0e46c93b5f82c74a26f5874c861ac": {"ta_keywords": "based godunov numerical;model based godunov;governmentunov based dynamics;godunov based hybrid;godunov numerical;hybrid model forramp;isolated traffic ramp;traffic ramp metering;godunov numerical scheme;ramp metering problem;model forramp metersing;based godunov;model isolated traffic;dynamics model isolated;ramp metering;introductionanalysis godunov based;based dynamics model;introductionanalysis godunov;godunov based;godunov;forramp metersing robust;dynamics model;metersing robust feedback;traffic ramp;model forramp;based hybrid model;isolated traffic;robust feedback control;forramp metersing;weak solution shock", "pdf_keywords": ""}, "c5e4eafd85949e6aac9d8e98d5e03b2acf444046": {"ta_keywords": "adversarial datasets produces;objectivesin adversarial data;training adversarial datasets;adversarial datasets;adversarial data collection;objectivesin adversarial;training adversarial;adversarial data;adversarial;unclear training adversarial;datasets produces robust;challenging datasets rely;datasets rely;models trained challenging;trained challenging datasets;models trained;datasets rely superficial;produces robust models;challenging datasets;robust models;datasets produces;datasets;elicit incorrect predictions;trained challenging;data collection;incorrect predictions researchers;predictions researchers;robust;models;trained", "pdf_keywords": "adversarial sample performance;adversarial sample;using adversarial sample;training adversarial data;appeal adversarial datasets;adversarial datasets;training adversarial;adversarial datasets evaluate;adversarial datasets produces;adversarial data collection;non adversarial datasets;data collection adversarial;models adversarial;adversarial data;demonstrate training adversarial;challenging datasets rely;adversarial data leads;model adversarial;adversarial data evaluate;collection adversarial data;based models adversarial;adversarial;improved using adversarial;using adversarial;models trained challenging;created adversarially;adversarially;trained challenging datasets;fine tuned adversarial;backgroundin adversarial data"}, "575ac3f36e9fddeb258e2f639e26a6a7ec35160a": {"ta_keywords": "supervised parsing semantic;supervised syntactic parsing;parsing semantic;supervised parsing;usefulness supervised parsing;self supervised language;syntactic parsing necessary;syntactic parsing;supervised syntactic;semantic language understanding;higher level semantic;level semantic language;parsing necessary successful;supervised language modeling;parsing necessary;parsing;supervised language;semantic language;language modeling success;language understanding;language modeling;held supervised syntactic;level semantic;semantic;end neural models;models self supervised;neural models self;introductiontraditional nonprogramming;introductiontraditional nonprogramming long;language understanding lui", "pdf_keywords": "intermediate parsing training;supervised parsing semantic;supervised syntactic parsing;parsing semantic role;dependency parsing training;parsing semantic;parsing training;intermediate dependency parsing;intermediate parsing;supervised parsing;parsing training transformers;learn shallowly parsed;syntactic parsing necessary;syntactic parsing;methods intermediate parsing;usefulness supervised parsing;transformer based parser;dependency parsing;syntactic knowledge supervised;supervised syntactic;biaffine dependency parser;level semantic language;syntactic knowledge semantic;infusing structural language;self supervised language;dependency parser;parsing necessary;downstream language understanding;parsing;shallowly parsed corpora"}, "b437cc7c0ae672b188df078b5dd80f97e8dde978": {"ta_keywords": "lexical units;lexical units used;question lexical units;lexical;words fundamental unit;language processing;unsupervised learning;question lexical;unsupervised learning methods;natural language processing;acquiring unsupervised learning;answer question lexical;speech recognition;fundamental unit processing;unit processing;language processing systems;words fundamental;conventionally treat words;unit processing cases;treat words fundamental;units;natural language;machine translation conventionally;processing systems speech;acquiring unsupervised;applications acquiring unsupervised;recognition machine translation;unsupervised;units used;speech recognition machine", "pdf_keywords": ""}, "69ba64b20d0a1849ef08d63c39bfafbaac909087": {"ta_keywords": "online agent learns;agent learns;systems learn reward;online rewards guiding;agent learns set;behavioral constraints observation;learn reward;learn reward feedback;set behavioral constraints;behavioral constraints;learned constraints;ai systems learn;reward feedback actions;reward feedback;online rewards;uses learned constraints;online agent;rewards guiding;learns set behavioral;ai;introduction ai;novel online agent;rewards guiding criteria;agent;ai systems;reward;rewards;introduction ai systems;set behavioral;cases online rewards", "pdf_keywords": ""}, "40bbd3046f1fa86a50e526b3848b4f2bd3a1d873": {"ta_keywords": "soluble lithium salt;lithium salt preparing;lithium salt;designed soluble lithium;high capacity lithium;soluble lithium;lithium o2 batteries;lithium dendrite growth;lithium dendrite;capacity lithium;capacity lithium o2;lithium o2;conventional electrolytes;lithiated nafion ln;lithium;batteries induced lithium;induced lithium dendrite;lithiated nafion;applications lithiated nafion;conventional electrolytes hinder;electrolytes hinder practical;nafion ln sulfonic;electrolytes;immobilized perfluorinated;group immobilized perfluorinated;induced lithium;immobilized perfluorinated backbone;polarization low liquid;o2 batteries;sulfonic group immobilized", "pdf_keywords": ""}, "059f515bf53bcddeca031fd4a4071c911999a3c6": {"ta_keywords": "person identified identification;person clothing change;person identified;wearing similar clothes;persons appearance;person clothing;persons wearing similar;persons appearance rarely;deep learning;deep learning methods;different persons wearing;similar clothes;persons wearing;clothing change different;identification;appearance rarely changes;wearing similar;mall person clothing;clothing change;rise deep learning;identified identification;identification rei performance;different persons;persons;clothes;person;appearance rarely;clothing;window persons appearance;identified", "pdf_keywords": "person clothing features;apparel invariant feature;person identity recognition;learn apparel invariant;pedestrian representation;invariant pedestrian representation;person clothing variations;clothing features;person specific recognition;pedestrian representation using;identity recognition wild;consistent feature apparel;feature apparel;recognition people context;apparel invariant;wearing similar clothes;identity recognition;unsupervised apparel invariant;person clothing;person reidentification tasks;propose apparel invariant;invariant pedestrian;person reidentification;human recognition;learning individual identity;specific recognition people;person images;model human identity;invariant feature learning;clothing variations"}, "c96363c42bc8c465902c22b8c33c8704233f519e": {"ta_keywords": "languages code generation;generation code summarization;code summarization applications;benchmark code generation;code generation natural;languages code;language commands;natural language commands;multilingual dataset mconala;code summarization;code generation;natural programming languages;programming languages code;code generation code;generation natural language;language commands extending;mconala benchmark code;propose multilingual dataset;multilingual dataset;development languages;technology development languages;development languages propose;commands extending english;programming languages;benchmark code;generation code;dataset mconala benchmark;english centric create;natural language;languages propose", "pdf_keywords": "multilingual code generation;generating code multilingual;language code generation;languages code generation;code natural languages;generate multilingual code;natural language code;english monala database;demonstrate multilingual code;multilingual code;code multilingual;generation code summarization;testbed languagecomprehensive approaches;code summarization applications;generation natural language;annotated annotated code;code multilingual code;code generation natural;generate multilingual;natural language commands;language applications;propose multilingual dataset;annotated code;difficult generate multilingual;target natural languages;solid testbed languagecomprehensive;language code;code generation data;generation multiple languages;annotated code code"}, "70a321f12a655e305781e2de0ca9617d96e462c3": {"ta_keywords": "estimation strategic data;statistical estimation strategic;central data aggregator;interactions data market;strategic data sources;data aggregator;data sources competitive;data market methods;reporting estimates aggregator;strategic data;estimates aggregator;data aggregator design;data market;estimation strategic;estimates aggregator shown;producing reporting estimates;aggregator;central data;sources competitive;aggregator design;reporting estimates;data sources;aggregator design mechanisms;single central data;aggregator shown mechanisms;sources competitive settings;statistical estimation;market methods;users sources aversion;aggregator shown", "pdf_keywords": "incentives data buyers;incentives data;effort compensate data;based incentives data;buyer reward data;data buyers incentivized;problem data market;data buyers optimal;handle competition data;equilibria data market;competition data buyers;data market;data markets;data buyers equilibrium;data markets marketplace;data source incentivized;buyers propose data;problem data buyers;incentive mechanisms effectively;reward data source;data buyers propose;incentive mechanisms;willingness data sources;model data markets;truthfully data buyers;reward data;data source effort;data buyers prove;data market buyer;based data markets"}, "bee52c51cbd37d0e48c3ea5f71a08f177d2aff73": {"ta_keywords": "g2p conversion structured;regularization weight vectors;adaptive regularization weight;g2p conversion;regularization weight;conversion structured learning;introduction g2p conversion;methods adaptive regularization;adaptive regularization;regularization;introduction g2p;g2p;weight update method;conversion structured;weight vectors;infused relaxed algorithm;weight vectors arow;learning based marginalin;overfitting;weight update;binary classification;structured learning;aggressive weight update;noisy methods adaptive;relaxed algorithm;problem binary classification;conversion;relaxed algorithm mira;ofmira prone overfitting;structured learning based", "pdf_keywords": ""}, "7129b62be18487db5e9602e353bb10a4c79a9b92": {"ta_keywords": "stripped executables approach;engineering stripped executables;stripped executables diverse;stripped executables contain;available stripped executables;stripped executables;names stripped executables;executables contain debug;engineering stripped binaries;stripped binaries using;reverse engineering stripped;stripped binaries;executables diverse assembly;executables contain;executables;executables approach;reverse engineering;executables diverse;introductionneural reverse engineering;executables approach combines;procedure names stripped;compiler;debug information challenging;patterns arising compiler;problem reverse engineering;debug information;compiler optimizations present;assembly code patterns;arising compiler;arising compiler optimizations", "pdf_keywords": ""}, "f4cf4246f3882aa6337e9c05d5675a3b8463a32e": {"ta_keywords": "alred action learning;action learning realistic;action learning;long compositional tasks;actions household tasks;compositional tasks;instructions egocentric;learning realistic environments;language instructions egocentric;directives benchmark learning;learning realistic;instructions egocentric vision;tasks alred;vision sequences actions;sequences actions household;learning mapping natural;benchmark learning mapping;actions;learning mapping;benchmark learning;mapping natural language;compositional tasks non;household tasks alred;introductionalred benchmark forinterpretinggground;actions household;learning;tasks;benchmark forinterpretinggground instructions;natural language instructions;egocentric vision sequences", "pdf_keywords": "interactive agent trained;learn agents visual;action learning realistic;learning realistic environments;actions agent trained;world robots interactive;robots interactive;expert demonstration supervised;robots interactions generate;agent trained interact;interactive agent;generate expert demonstrations;trained interact environment;robots interactive visual;language instructions egocentric;agents visual inputs;trained interact;learn agents;alred action learning;real world robots;agents visual;experts composed agent;robots interactions;actions agent;instructions egocentric vision;demonstration supervised;actions demonstrate benchmark;mapping natural language;instructions egocentric;world robots"}, "8f11643b42976433fc3a2ec19feef486929527a1": {"ta_keywords": "prevention treatment diseases;disease development;disease development new;diseases;disease;evolution disease;treatment diseases;evolution disease discuss;prevention treatment;strategies prevention treatment;literature evolution disease;disease discuss;importance disease development;disease discuss importance;new strategies prevention;prevention;discuss importance disease;strategies prevention;importance disease;treatment;development new strategies;article present literature;evolution;development;purpose article present;literature evolution;present literature evolution;purpose article;article present;development new", "pdf_keywords": ""}, "d9e56aa9f69e18c9d37799b86b50d36709cbf711": {"ta_keywords": "human organism evolution;organism evolution human;evolution human human;overview evolution human;evolution evolution human;evolution human;organism evolution;organism evolution evolution;evolution evolution;human human organism;evolution;overview evolution;human organism;provide overview evolution;organism;human human;human;article provide overview;article;overview;provide overview;article provide;aim article;aim article provide;provide;aim", "pdf_keywords": ""}, "59a228f48a83eb0905391f7e454fde0eeb6680ee": {"ta_keywords": "model automatic speech;automatic speech recognition;speech recognition;training phoneme lattices;automatic speech;learn language model;acoustic model linguistic;speech recognition ar;continuous speech methods;language model automatic;phoneme lattices using;learning lexical;simultaneously learning lexical;generate phoneme lattices;language model;learning lexical units;speech methods proposed;speech methods;phoneme lattices;training phoneme;phoneme lattices simultaneously;directly continuous speech;lattices using acoustic;continuous speech;model linguistic;generate phoneme;lattices simultaneously learning;perform training phoneme;using acoustic model;acoustic model", "pdf_keywords": ""}, "8b7a8f9a27b8dc73a5b0b62ada14bbab047084fc": {"ta_keywords": "speech enhancement based;speech enhancement systems;silent speech enhancement;speech enhancement;speech enhancement results;additional speech enhancement;electrolaryngeal speech enhancement;statistical voice conversion;speech enhancement electrolaryngeal;voice conversion proposed;enhancement electrolaryngeal speech;voice conversion;voice conversion silent;voice conversion methods;conversion silent speech;statistical voice;silent electrolaryngeal speech;time statistical voice;conversion silent;electrolaryngeal speech;enhancement based real;implementation silent electrolaryngeal;enhancement based;enhancement systems based;silent speech;real time statistical;results additional speech;voice;enhancement systems;silent electrolaryngeal", "pdf_keywords": ""}, "4100256a125d7b56cac693a436bba2b00fae3fa3": {"ta_keywords": "audio captioning;techniques audio captioning;audio captioning aim;captioning;language descriptions acoustic;anrriasis techniques audio;captioning aim;captioning aim evaluate;natural language descriptions;techniques audio;language descriptions;audio;generate natural language;captioned;encoder transformer decoder;transformer decoder generate;transformer decoder;descriptions acoustic;decoder;descriptions acoustic signals;decoder generate natural;encoder;encoder transformer;transformer conformer encoder;decoder generate;conformer encoder transformer;convolution augmented transformer;natural language;employ convolution augmented;acoustic signals end", "pdf_keywords": ""}, "306c59458cebb35c2d520dd129f09d5c6cc2985f": {"ta_keywords": "paraphrase model associated;paraphrase model;function paraphrase model;structure function paraphrase;paraphrase;function paraphrase;associated structures;structure;model associated structures;authors structure;associated structures structure;authors structure function;model associated;structures;structures structure;model;structures structure function;structure function;associated;authors;function", "pdf_keywords": "generating paraphrase models;models paraphrase tasks;phrase paraphrase tasks;paraphrase models;paraphrase tasks improved;learning lexical paraphrasability;paraphrase database;generating word paraphrase;models short paraphrases;paraphrase models text;paraphrase tasks developed;word phrase representations;paraphrase tasks;paraphrase database ppb;generating paraphrase;paraphrases likely associated;paraphrases paraphrases likely;word paraphrase pairs;paraphrases ppb dataset;associated paraphrases paraphrases;paraphrase relationship annotated;likely associated paraphrases;paraphrase pairs;paraphrase relationship significantly;dataset paraphrase relationship;quality poor paraphrases;introductionthe paraphrase database;associated paraphrases;paraphrases common task;paraphrase tasks aim"}, "bba9b93ab8d9b98cd54001a5ba9673e513a35219": {"ta_keywords": "data recurrent neural;data recurrent;clinical medical data;neural networks nursesns;medical data;missing data recurrent;series observations patient;intensive care unit;recurrent neural networks;observations patient visit;recurrent neural;care unit icu;observations patient;medical data especially;intensive care;icu consist;patient visit episode;time series observations;icu consist multivariate;unit icu consist;icu;unit icu;episode sensor data;electronic health record;especially intensive care;recorded patient electronic;patient visit;recorded patient;care unit;results recorded patient", "pdf_keywords": ""}, "cc352ea39f820c3effc40ce09369cf59afe361df": {"ta_keywords": "electronic health technology;electronic health;introduction electronic health;health technology;health systems designed;health technology brought;traditional health systems;health systems;paper based medical;cic based systems;technologies cic based;information communication technologies;communication technologies cic;technologies cic;based medical practice;medical practice information;systems automatic management;automatic management storage;based medical;communication technologies;computers locally managed;management storage;methods traditional health;cic based;management storage processing;stovepipes dedicated networks;practice information communication;medical practice;based systems;based systems automatic", "pdf_keywords": "health applications cloud;healthcare cloud computing;cloud computing health;cloud based medical;healthcare cloud;healthcare healthcare cloud;applications cloud based;cloud basedtelemedicine practice;cloud implement;hosting health applications;practice management cloud;management cloud basedtelemedicine;cloud platform;cloud using;practical applications cloud;cloud basedtelemedicine;use cloud computing;cloud services;applicationthe use cloud;applications cloud;cloudbasedtelemedicine practice windows;cloud platform web;use cloud;computing applications healthcare;cloud computing;applications cloud computing;azure cloud implement;cloud computing applied;cloud cloud services;cloud cloud"}, "589e651c69251ee20a89e075d015eb03b35cf17d": {"ta_keywords": "introductionfast inference speed;inference speed;inference speed important;speech translation st;deployment speech translation;speech translation;decoding speed;translation st systems;introductionfast inference;text based translation;regarding decoding speed;translation generates target;decoding speed explored;translation generates;based translation generates;encoder decoder architecture;models based encoder;translation st;encoder decoder;effectiveness regarding decoding;decoder architecture;decoder;encoder;decoder architecture suitable;autoregressive nar;autoregressive nar methods;decoding;non autoregressive nar;speed;regarding decoding", "pdf_keywords": "machine translation speech;translation speech recognition;translation models;speech translation e2e;machine translation increasing;neural machine translation;neural network translation;speech translation;machine translation;end speech translation;translation speech;nonar decoder fast;translation models language;network translation models;speech translation st;decoding speech single;decoder fast token;machine translation application;decoder fast;decoding speech;automatic decoder adopt;deployment speech translation;automatic decoder;transcription translation;decoding speech singlewe;approach decoding speech;corresponding transcription translation;theart automatic decoder;field speech translation;introductionfast inference speed"}, "d5123ab81f511027cbe11dc92d99e116fd193158": {"ta_keywords": "sensing energy harvesting;harvesting source opportunistically;instant energy harvesting;energy harvesting;remote sensing energy;methodsthe energy harvesting;energy harvesting considered;sensing energy;energy harvesting source;time varying wireless;information remote sensing;time instant energy;harvesting source;harvesting source decides;remote sensing;node time varying;node time;link discrete time;harvesting considered methodsthe;minimization time averaged;source opportunistically samples;time instants sends;sensing;sink node time;harvesting;minimization time;instant energy;harvesting considered;wireless link discrete;varying wireless link", "pdf_keywords": "channel state optimal;channel states optimal;optimal policy channel;information energy harvesting;fading channel energy;channel state threshold;optimal source sampling;minimization time averaged;index energy harvesting;sensing setting considered;energy harvesting systems;time varying channels;sensor single source;single source monitoring;discrete time process;sensing setting single;time varying channel;channel energy;time varying wireless;channel energy generation;minimizing time averaged;sensing setting remote;energy harvesting;state optimal;optimal action sample;state optimal probing;electronic health sensor;harvesting systems fundamental;optimal sampling policy;modeled discrete time"}, "ddfd297531f56121b8383bd1eb2bb09189ab2e2b": {"ta_keywords": "mapping emphasis languages;speech translation systems;focus speech translation;translation systems;speech translation;languages utilizing conditional;traditional speech translation;linguistic content emphasis;transfer linguistic content;emphasis information languages;mapping emphasis;conditional random fields;transfer linguistic;emphasis languages;emphasis languages utilizing;paralinguistic information;linguistic content;translation systems transfer;languages utilizing condition;emphasis information;systems transfer linguistic;paralinguistic information recent;translation systems oblivious;linguistic;content emphasis information;focus speech;paralinguistic;utilizing conditional random;content emphasis;oblivious paralinguistic information", "pdf_keywords": ""}, "f0a498014c4ef67c0b72ceb18d95e0d25087fd57": {"ta_keywords": "machine translation binary;neural machine translation;translation binary code;translation binary;binary code prediction;predicting binary code;machine translation;machine translation systems;binary code word;introductionneural machine translation;predicting binary;translation systems;code prediction;translation systems methodsthe;binary code;based predicting binary;output layer neural;layer logarithmic vocabulary;code prediction paper;neural machine;layer neural machine;logarithmic vocabulary size;code word;binary;code word reduce;output layer logarithmic;layer neural;word reduce computation;calculating output layer;memory requirements output", "pdf_keywords": "neural machine translation;machine translation models;softmax binary code;machine translation method;softmax binary codes;machine translation;machine translation increased;machine translation systems;bidirectional translation tasks;translation models;evaluation machine translation;softmax binary;improve translation quality;words binary codes;softmax prediction binary;predicting binary code;binary code word;output words binary;binary code prediction;using softmax binary;prediction binary code;translation systems;translation quality proposed;codes viterbi decoding;translation quality;standard softmax binary;translation able improve;softmax;translation tasks;translation models indirectly"}, "88e2beccbc89b3e3dd793e2502b17c1fa551151d": {"ta_keywords": "distributed data repair;recover data distributed;distributed storage;propose distributed storage;distributed storage used;data repair bandwidth;storage node repair;distributed data;node repair bandwidth;data distributed data;tradeoff storage node;data distributed;storage node;repair bandwidth node;tradeoff storage;data repair;reduced node stores;storage used recover;repair bandwidth;storage;node repair;distributed;characterize tradeoff storage;used recover data;recover data;node stores;propose distributed;storage used;node stores slightly;bandwidth node", "pdf_keywords": ""}, "f4c8539bed600c9c652aba76a996b8188761d3fe": {"ta_keywords": "neural machine translation;machine translation grown;machine translation;introductionsearch neural machine;introductionsearch neural;translation grown rapidly;nm implementations new;nm implementations;vanilla nm implementations;neural machine;translation grown;translation;language data scenarios;language data;neural;demonstrated language data;introductionsearch;nm;shared task systems;gains vanilla nm;vanilla nm;production shared task;shared task;algorithmic improvements lead;regularly introduces architectural;architectural algorithmic improvements;task systems result;effectiveness demonstrated language;machine;task systems", "pdf_keywords": "neural translation algorithms;neural machine translation;neural translation effective;neural translation useful;neural translation powerful;neural translation;tool neural translation;optimum neural translation;translation algorithms;using neural translation;evaluate neural translation;machine translation grown;neural translation new;translation systems;translation algorithms using;translation algorithms data;machine translation new;translation powerful tool;machine translation systems;machine translation;translation useful tool;translation understood neural;neural translation understood;translation accuracy effectiveness;machine translation results;machine translation improved;machine translation limited;effective translation large;understood neural translation;improve translation accuracy"}, "3e3254bce9c321310d2e9825ed52b30da9879173": {"ta_keywords": "speech segment boa;boa speech segments;models speech segments;speech segments;arcs boa speech;speech segment;models speech;segments speech segment;speech segments speech;efficiently models speech;segments speech;speech segments strong;bag words model;feature representation bag;new feature representation;feature representation;representation bag arcs;words model;clinical messagethe proposed;speech;clinical messagethe;words model bow;boa speech;bag arcs boa;key clinical messagethe;bag arcs;segment boa simply;counts unique arcs;feature;arcs efficiently models", "pdf_keywords": ""}, "285c50d98dab741a82649b1abcaca8273cb8f253": {"ta_keywords": "regularization weight vectors;adaptive regularization weight;conversion structured learning;regularization weight;g2p conversion structured;methods adaptive regularization;discriminative training method;g2p conversion;method multiclass classification;multiclass classification;adaptive regularization;multiclass classification known;training method multiclass;discriminative training;introduction g2p conversion;online discriminative training;regularization;mira online discriminative;weight update method;weight vectors arow;weight vectors;online discriminative;overfitting;introduction g2p;aggressive weight update;training method;classification;ofmira prone overfitting;conversion structured;g2p", "pdf_keywords": ""}, "55bdc4ad158e272ccf796ae52b0ab7086a834352": {"ta_keywords": "student modeling;introduction student modeling;student model model;student model;decisions student modeling;affects automated tutoring;automated tutoring;automated tutoring systems;student modeling key;decisions student model;good student model;tutoring systems;tutoring;student behavior patterns;tutoring systems making;student behavior;student model matches;predict probability student;model matches student;student making errors;probability student making;systems making instructional;student;matches student behavior;instructional decisions student;making instructional decisions;student making;making instructional;learning task difficulty;learning related problems", "pdf_keywords": ""}, "c3177616ad35ef7850ea1e62da1fa3be36943e8b": {"ta_keywords": "neural network paraphrase;network paraphrase identification;paraphrase identification example;paraphrase identification;dialog retrieval;based dialog retrieval;dialog retrieval example;example based dialog;based dialog model;network paraphrase;dialog model;utilizing recursive neural;paraphrase;introductionrecursive neural network;vocabulary hov database;dialog;dialog model require;based dialog;recursive neural network;retrieval example based;recursive neural;retrieval example;introductionrecursive neural;handling vocabulary hov;handling vocabulary;retrieval;words sentence;vocabulary hov;comes handling vocabulary;handling interactions words", "pdf_keywords": ""}, "49a049dc85e2380dde80501a984878341dd8efdf": {"ta_keywords": "representations speech audio;powerful representations speech;representations speech;speech audio;speech input latent;transcribed speech outperform;simpler methods wav2vec;methods wav2vec;best semi supervised;wav2vec masks speech;semi supervised;tuning transcribed speech;learning powerful representations;quantization latent representations;speech outperform;semi supervised methods;speech outperform best;representations jointly learned;wav2vec;speech input;latent representations;speech audio followed;latent representations jointly;methods wav2vec masks;transcribed speech;fine tuning transcribed;supervised;representations;wav2vec masks;masks speech input", "pdf_keywords": "learning speech representations;contextualized speech representation;resource speech recognition;representations speech audio;phoneme recognition audio;processing speech representations;supervised learning speech;speech units contextualized;speech representations;speech representation;phoneme recognition;speech recognition fundamental;speech representations approach;speech units framework;speech representation inventory;audio data speech;data speech;speech recognition integrating;quantized speech representations;speech audio;phoneme recognition single;learning discrete speech;technologies speech recognition;speech recognition;recognition speech;contextualized speech;networks speech recognition;speech recognition lexicon;learning speech;setups speech recognition"}, "5f1bbc96a22a630d3662b3fceb3160091e4bd814": {"ta_keywords": "voice activity detection;background voice activity;robust voice activity;background voice;voice activity;noise robust voice;robust voice;weight normalization noise;pruning weight normalization;normalization noise robust;activity detection using;activity detection;voice;activity detection vad;normalization noise;kalman filter based;gassian pruning weight;weight normalization methodsthis;weight normalization;noise gassian;detection using frame;based gassian pruning;stationary noise gassian;kalman filter;noise robust;switching kalman filter;pruning weight;gassian pruning;normalization methodsthis paper;normalization methodsthis", "pdf_keywords": ""}, "bf0105bdd5b0dfc09580697739fb84590d031d0b": {"ta_keywords": "methodsthe mouse model;cognitive tutor;methodsthe mouse;cognitive tutor authoring;testing methodsthe mouse;mouse model;mouse model machine;learns cognitive;learns cognitive skills;block cognitive tutor;learning agent;build cognitive model;cognitive skills;agent learns cognitive;cognitive skills demonstration;generate cognitive model;tutor;automatically generate cognitive;mouse;tutor authoring tools;learning agent learns;generate cognitive;student using;build cognitive;cognitive model;training testing methodsthe;tutor authoring;learns;students data training;agent learns", "pdf_keywords": ""}, "8ec127925a8680928d546df7248963e772e07a5d": {"ta_keywords": "screening models employers;observe underlying skill;estimate worker skill;job candidates employers;candidates employers;bernoulli gaussian models;candidates employers rarely;employers rarely observe;screening models;models employers;job candidates;recruiting job candidates;worker skill;access worker skill;underlying skill level;models employers access;worker skill single;skill single noisy;bernoulli gaussian;interviews collate noisy;interviews;considering bernoulli gaussian;employers rarely;gaussian models;recruiting job;underlying skill;screening;noisy signals;introduction recruiting;employers", "pdf_keywords": "discriminate employees;optimal employer policy;model unfairness employment;unfairness employment;discrimination employment limited;discrimination employment;methods discrimination employment;discriminate employees regulatory;unfairness employment uses;discrimination;learning methods discrimination;employment uses randomized;ability discriminate employees;randomized threshold policy;optimal employer;analysis optimal employer;employer policy;threshold policy randomized;methods discrimination;employer able minimize;threshold policy greedy;policy randomized threshold;threshold policy;threshold policies;threshold policy assess;probability employer able;group candidates employer;model unfairness;probability employer;employment limited binary"}, "a34954d9e36ea6c57743f55124a6ae444b951c2c": {"ta_keywords": "explain predictions deep;activations training points;explaining deep neural;predictions deep neural;pre activation prediction;activation prediction neural;selection explaining deep;activation prediction;predictions deep;training points weights;deep neural;test point prediction;training points;point prediction resultsspecifically;prediction neural;point selection explaining;point prediction;representer points training;neural network pointing;deep neural networks;explaining deep;deep neural network;prediction neural network;points training set;points training;neural networks;activations training;combination activations training;neural network;neural", "pdf_keywords": "predictions deep neural;model deep neural;activation prediction neural;prediction neural;model deep;activation prediction;explain predictions deep;model learning;provide model deep;deep neural;neural networks demonstrate;training neural;predictions deep;training neural networks;corresponding model learning;learning neural;prediction neural network;introductionthe prediction neural;neural network pointing;activations training points;learning neural networks;prediction fundamental goal;pre activation prediction;method training neural;approach learning neural;training point feature;neural networks;model learning process;neural network;deep neural networks"}, "ce4eadb324026191c075f1af876403a847329d5b": {"ta_keywords": "learning trees ules;learning trees;introduction learning trees;feature vectors;feature vectors components;featurevector representation allows;feature set strings;length feature vectors;learning systems;featurevector representation;value feature set;feature set;useful learning systems;featurevector;learning systems examples;extension featurevector representation;propose extension featurevector;features inherently;features;systems learning systems;ed features inherently;possible set elements;vectors components;value feature;fixed length feature;length feature;extension featurevector;trees ules set;set elements;learning systems learning", "pdf_keywords": ""}, "63bc09c11a792abfcbb2d9e2809aa67929f09262": {"ta_keywords": "semantic relations hypernymy;distributional semantics;technique computing hypernyms;semantic relations;popularisation distributional semantics;computing hypernyms;inducing relations words;word embeddings inducing;relations hypernymy hyponymy;embeddings inducing relations;hypernymy hyponymy widely;computing hypernyms hyp;introductionthe semantic relations;hypernymy hyponymy;word embeddings;semantic;distributional semantics significant;semantics significant attention;semantics;applying word embeddings;embeddings inducing;hypernyms;relations hypernymy;introductionthe semantic;relations words methodsin;natural language processing;sense reasoning popularisation;hypernyms hyp;inducing relations;hyponymy widely", "pdf_keywords": ""}, "cfb1b39d1a6733f42cc5e8cfd60dc68cafa01d28": {"ta_keywords": "natural language processing;natural language;field natural language;processing machine learning;machine learning;language processing;language processing machine;literature field natural;processing;learning;language;overview current literature;field natural;processing machine;literature field;current literature;article;current literature field;overview;literature;field;machine;article provide overview;natural;aim article;aim article provide;provide overview;article provide;provide overview current;overview current", "pdf_keywords": ""}, "2b110fce160468eb179b6c43ea27e098757a56dd": {"ta_keywords": "controlled paraphrase networks;adversarial example generation;trained produce paraphrase;syntactically controlled paraphrase;generate adversarial examples;paraphrase networks;paraphrase networks cacs;produce paraphrase;controlled paraphrase;produce paraphrase sentence;example generation syntactically;generate adversarial;generation syntactically controlled;use generate adversarial;generation syntactically;adversarial example;adversarial examples;adversarial;introduction adversarial example;adversarial examples methods;example generation;sentence target syntactic;syntactically controlled;paraphrase;paraphrase sentence desired;paraphrase sentence;introduction adversarial;parse scpns trained;target syntactic;sentence target", "pdf_keywords": "automated paraphrase networks;generate paraphrases parse;controlled paraphrase networks;paraphrase generation parses;able generate paraphrases;controlled paraphrase generation;generating paraphrases;paraphrases adversarial;generate paraphrases;paraphrase networks;generate paraphrases generating;paraphrase generation systems;paraphrase generation syntactically;paraphrases adversarial examples;ability generate paraphrases;algorithm generate paraphrases;valid paraphrases adversarial;syntactically controlled paraphrases;method generate paraphrases;automated automated paraphrase;tool generating paraphrases;paraphrases generating paraphrases;automated paraphrase;trained produce paraphrase;driven paraphrase generation;generating paraphrases similar;paraphrases manually processed;paraphrase networks scpns;uncontrolled paraphrase generation;paraphrases generating"}, "cf0860ab99c63cb7cbd5317fca7cf1fe70e8fb63": {"ta_keywords": "corpus virtual knowledge;reasoning virtual knowledge;virtual knowledge base;virtual knowledge;introductiondifferentiable reasoning virtual;knowledge base;reasoning virtual;corpus virtual;traverses textual data;entities corpus;using corpus virtual;consider task answering;knowledge base consider;traverses textual;knowledge base particular;task answering complex;entities corpus step;task answering;mentions entities corpus;questions using corpus;introductiondifferentiable reasoning;textual data;textual data like;textual;hop questions using;corpus;knowledge;neural module;mentions entities;using corpus", "pdf_keywords": "corpus virtual knowledge;mentions entities corpus;entities corpus;complex query answering;mentions corpora sparse;query answering large;query answering;constructing mention representations;entities corpus step;hop question answering;filtering mentions efficiently;traverses textual data;mentions efficiently using;large text corpus;corpus encoded query;mention representations aggregation;mentions efficiently;question answering;mentions entities accomplished;new approach retrieval;linked text corpus;consider task answering;answering large text;aggregation mentions entities;answering natural language;mentions entities;new tool answering;aforementioned query computational;query computational;knowledge bases text"}, "024091a3c0223f27d6456b1a27db18fb08d41e5a": {"ta_keywords": "language model nml;model nml word;machine translation accuracy;vocabularies training njm;large vocabularies training;network language model;language model;machine translation;nml word source;vocabularies training;neural network language;gains machine translation;translation accuracy;nml word;translation accuracy problems;neural network joint;using large vocabularies;large vocabularies;word source context;njm augments neural;model nml;network language;vocabularies;joint model njm;contrastive estimation nce;njm noise contrastive;network joint model;contrastive estimation;word source;nml", "pdf_keywords": ""}, "629323c5b9f7c64afac9300212538e488569bd1e": {"ta_keywords": "embeddings ontology induction;word embeddings ontology;synonym dictionaries establishing;knowledge synonym dictionaries;synonym dictionaries;dictionaries word embeddings;dictionaries establishing semantic;ontology induction methodsthis;ontology induction;ontology induction approach;presents ontology induction;ontology induction aimto;introduce joining dictionaries;embeddings ontology;lexical knowledge synonym;introduction joining dictionaries;joining dictionaries word;establishing semantic relations;structured lexical knowledge;semantic relations;joining dictionaries;lexical knowledge;establishing semantic;semantic relations structures;word embeddings;semantic;dictionaries word;word embeddings projections;dictionaries establishing;using word embeddings", "pdf_keywords": ""}, "33aa6c70eac0e4b7eb28d8386e5e4113fdd55203": {"ta_keywords": "question answering qrc;modular automatic question;automatic question answering;question answering ncr;question answering;automatic question;answering qrc resultsa;qrc resultsa answers;answering qrc;based modular automatic;qrc resultsa;resultsa answers;modular automatic;questions world;use modular automatic;history entrance exam;exam;answers multiplechoice english;answering ncr 11;english questions world;exam questions preceded;purposeto use modular;exam questions;uima based modular;resultsa answers multiplechoice;answering ncr;questions;use modular;based modular;entrance exam", "pdf_keywords": ""}, "2e820673ca861a9ece8d36f2b93793b5d2c7e1da": {"ta_keywords": "advanced cryption standard;ciphers internet things;advanced cryption;current cryptanalysis implementation;cryptanalysis implementation;cryptanalysis implementation results;cryption standard;cryption standard ae;environments advanced cryption;current cryptanalysis;cryptanalysis;cryptic ciphers internet;simulating cryptic ciphers;cryptic ciphers;ciphers internet;ciphers;cryption;lightweight block cipers;rationale current cryptanalysis;lightweight block;standard ae;internet things speck;standard ae suitable;family lightweight block;speck family lightweight;ae suitable;developed simulating cryptic;securing applications constrained;securing;simulating cryptic", "pdf_keywords": ""}, "49d415cf593be38c6cd97a183dadc7d7b48bab72": {"ta_keywords": "ai innovation;ai innovation methodswe;data ai innovation;patent grants assess;patent grants;text patent grants;parse text patent;data ai;artificial intelligence ai;firms industries accurately;machine learning;machine learning algorithms;ai entered new;ai;artificial intelligence;intelligence ai entered;innovation;patent;firms industries;innovation methodswe address;labor firms industries;text patent;intelligence ai;innovation methodswe;labor firms;learning algorithms;level data ai;firms;industries accurately;rapidly advancing capabilities", "pdf_keywords": ""}, "2225950d1d3e02bc0d88a0c78325d00e0122b576": {"ta_keywords": "speech separation end;speech separation methods;speech separation;methods speech separation;separation methods deep;speech recognition ar;deep clustering;speech recognition;automatic speech recognition;network methods speech;methods deep clustering;end automatic speech;end deep network;advances deep learning;automatic speech;deep network;deep learning;separation;separation methods;ar methods speech;separation end;separation end end;deep network methods;deep learning resulted;deep clustering address;methods speech;recognition ar;advances deep;optimized isolation;recognition ar methods", "pdf_keywords": ""}, "05fb5a180214bf092eeda30baf9f16fb6bd15727": {"ta_keywords": "using native speech;speech modified;speech modified speech;generating corrected speech;correcting durational patterns;native speech language;modify durational patterns;native speech quality;speech language learning;non native speech;modified speech parameter;modified speech;native speech reference;dynamic time warping;native speech;modify durational;speech parameter sequence;attempts correcting durational;speech quality corrected;corrected speech modified;speech reference generating;flexibly modify durational;modifies speech parameter;speech language;quality corrected speech;durational patterns;correcting durational;time warping using;speech parameter;approaches modifies speech", "pdf_keywords": ""}, "649c1148439a4e875dab414ba67bf8c80350af4a": {"ta_keywords": "neural semantic parser;neural abstract syntax;semantic parsing code;abstract syntax parser;syntax parser semantic;parsing code generation;semantic parser;parser semantic;syntax parser;abstract syntax description;parser semantic parsing;semantic parser maps;based abstract syntax;semantic parsing;syntax description language;formal meaning representations;abstract syntax;parsing code;parser;parser maps natural;based neural semantic;transition based neural;maps natural language;neural semantic;transition based abstract;natural language nl;based neural abstract;syntax description;parsing;code generation", "pdf_keywords": "parses utterance adolescent;developed semantic parsing;natural language developed;utterance adolescent based;transition parses utterance;extensibility parser semantic;utterance adolescent;transition used parser;parser semantic;syntax parser generalizable;semantic parsing code;semantic parsing;abstract syntax parser;parsing code generation;semantic parsing fundamental;parser generalizable;parser semantic parsing;generalization natural language;parses utterance;natural language;generate present parser;present parser semantic;uses transition parses;syntax parser;algorithms semantic parsing;generalization extensibility parser;parser easily extended;based meaning representation;functional algorithms semantic;parser generalizable effective"}, "86d84c1c9b0a500f930696ab27c83a4b30477560": {"ta_keywords": "paraphrastic similarity parallel;effective paraphrastic similarity;paraphrastic sentence embeddings;paraphrastic similarity;learning paraphrastic sentence;learning paraphrastic;effective paraphrastic;para phrase corpora;simple effective paraphrastic;similarity parallel translations;paraphrastic;paraphrastic sentence;creating para phrase;sentence embeddings directly;parallel translations;parallel translations aims;methodology learning paraphrastic;phrase corpora methods;para phrase;sentence embeddings;cross lingual tasks;phrase corpora;lingual tasks outperforms;lingual tasks;cross lingual;similarity parallel;corpora methods;applied cross lingual;translations;corpora", "pdf_keywords": "semantic crosslingual tasks;paraphrastic cross lingual;learning sentences parallel;monolingual semantic similarity;paraphrastic sentence embeddings;performance semantic crosslingual;semantic crosslingual;embeddings natural language;learning paraphrastic;sentences parallel corpus;parallel corpus;learning paraphrastic sentence;lingual monolingual semantic;crosslingual tasks 2017;similarity cross lingual;sentence embeddings directly;monolingual semantic;embeddings development paraphrastic;cross lingual tasks;multilingual language representations;semantic similarity cross;performing sentence embeddings;sentence embeddings;cross lingual representations;lingual tasks outperforms;using parallel corpora;parallel corpus apply;sentence embeddings development;cross lingual monolingual;crosslingual tasks"}, "65c53ed3575e160eb1e7d0a516353ba52de7e7e5": {"ta_keywords": "auctions machine learning;leakage procurement auctions;bid leakage procurement;identifying bid leakage;leakage particular auction;detect bid leakage;bid leakage particular;bid leakage price;sealed bid auctions;bid leakage;2018 bid leakage;russian procurement auctions;procurement auctions 2014;bid auctions;identifying bid;detect bid;procurement auctions machine;auctions methods extract;procurement auctions;bid auctions methods;auctions machine;background identifying bid;leakage procurement;auctions 2014;approach detect bid;particular auction;auctions methods;auction;auctions 2014 2018;price sealed bid", "pdf_keywords": "classification bid leakage;identification estimation auctions;bid leakage procurement;leakage specific auction;leakage likely auctions;russian procurement auctions;bid leakage estimation;artificial auction dataset;estimation bid leakage;leaked bids probability;identify bid leakage;leakage bids;bid leakage bids;analyze bid leakage;leakage bids win;probabilities bid leakage;classification bid;leaked bids;probability bid leakage;bid leakage patterns;patterns random auction;identification estimation bid;procurement auctions;bid leakage bid;leakage bid;estimation auctions;bid leakage specific;bid leakage present;strategy bid leakage;leakage estimation winners"}, "a9e6222e71dd101d444b7192b3a0636c71edb0a4": {"ta_keywords": "corpus virtual knowledge;virtual knowledge base;corpus virtual;virtual knowledge;traverses textual data;using corpus virtual;entities corpus;corpus step module;entities corpus step;traverses textual;knowledge base;questions using corpus;mentions entities corpus;knowledge base kk;corpus;textual data like;textual data;using corpus;corpus step;consider task answering;task answering complex;textual;neural module;hop questions using;task answering;mentions entities;knowledge;particular neural module;relations mentions entities;neural module withdockit", "pdf_keywords": ""}, "6536f36648d39f0f9f6105562f76704fcc0b19e8": {"ta_keywords": "language robot actions;persistent spatial semantic;robot actions layers;robot actions;robot actions long;tasks robotic agents;specific robot actions;spatial semantic representation;robotic agents;spatial semantic;language robot;semantic representation;term tasks robotic;propose persistent spatial;actions layers abstraction;persistent spatial;persistent representations;robotic agents non;tasks robotic;semantic representation method;abstract specific robot;abstraction;robot;gap language robot;persistent representations resultswe;instructions abstract specific;level instructions abstract;semantic;specific robot;robotic", "pdf_keywords": "robot actions layers;language robot actions;tasks robotic agents;robot actions long;descriptions actions 3d;robot actions;tasks robotic;term tasks robotic;persistent spatial semantic;specific robot actions;learning instructions actions;semantic representation hierarchical;task descriptions actions;robot manipulation interactive;spatial semantic representation;robotic agents;temporal abstraction bridge;actions 3d;manipulation tasks;temporal abstraction;manipulation tasks state;level task actions;task planning near;implement spatial semantic;actions layers abstraction;mobile manipulation tasks;spatial semantic;language robot;mapping action;integrating hierarchical task"}, "2c0f2a03c3a427cc61359b5e2c31cfefe9850a31": {"ta_keywords": "extracting concept instance;information extraction;domain information extraction;clustering terms html;extracting concept;information extraction method;method extracting concept;concept names clusters;html corpus;pairs html corpus;html corpus methodsmost;concept instance pairs;concept names;terms html tables;assigning concept names;terms concept instance;concept instance;terms html;approach clustering terms;extraction method extracting;instance pairs html;names clusters usinghearst;clustering terms;method extracting;names clusters;extracting;similar terms concept;novel approach clustering;terms concept;corpus methodsmost", "pdf_keywords": "extraction technique websets;unsupervised information extraction;information extraction;corpus websets efficiently;websets extracts conceptinstance;information extraction unstructured;websets efficiently extract;extract tables corpus;domain information extraction;information extraction technique;text corpus websets;corpus websets;information extraction method;entities large corpus;named websets extracts;conceptinstance pairs unstructured;clustering terms html;extracts conceptinstance pairs;websets disambiguate;html corpus;extracting concept;help websets disambiguate;tables corpus;concept names clusters;extracting concept instance;html corpus developed;technique named websets;web unsupervised information;websets extracts;clusters generated websets"}, "ed2cc779c7eb0004bd6dd50538a2cafca092c94f": {"ta_keywords": "spelling normalization historical;normalization historical texts;normalization linguistic annotation;linguistic annotation historical;spelling normalization;normalization linguistic;introductionautomatic normalization linguistic;deals spelling normalization;annotation historical language;modern speech taggers;speech taggers methodsdifferent;normalization historical;historical language data;linguistic annotation;annotation historical;speech taggers;speech taggers paper;introductionautomatic normalization;historical german texts;historical language;set historical german;processing modern speech;normalization;german texts;taggers methodsdifferent;historical german;german texts 15th;taggers methodsdifferent methods;historical texts;language data", "pdf_keywords": ""}, "5bcd9117899bc2c91db83532dcf587b9d8f8888b": {"ta_keywords": "constitutional constitutional constitutional;constitutional constitutional;constitutional", "pdf_keywords": ""}, "3d1cfefdbe40f7535ada772c260c192bb63bb9fe": {"ta_keywords": "scientific document similarity;document similarity model;document similarity;cited papers related;papers citations;text papers cite;descriptions cited papers;relatedness provide textual;citations;cited papers;textual descriptions cited;paper relatedness provide;papers citations citations;paper relatedness;source supervision sentences;citations citations;papers cite;close paper relatedness;multiple papers citations;citations reflect;citations reflect close;citations citations reflect;similarity model;sentences text papers;source supervision;similarity model based;cite multiple papers;scientific document;papers cite multiple;descriptions cited", "pdf_keywords": "learning similarity citations;learning aspect matching;training citation sentences;similarity citations;citations corpus;cited papers corpus;predicting citations;learning framework citations;approach predicting citations;citation contexts closely;document similarity tasks;predicting citations citations;aspectconditional similarity tasks;grained aspects texts;citations citations corpus;corpus citation contexts;aspect matching models;papers corpus citation;papers citations cocitations;aspect similarity significantly;citations corpus present;citations cocitations;similarity citations apply;grained document similarity;science articles corpus;context cited papers;source supervision sentences;citations source language;citation contexts;textual descriptions cited"}, "5e74d4e041a25e7752a596e2891975df5ba65aa2": {"ta_keywords": "singlechannel speech enhancement;speech enhancement deep;mvr beamforming speech;beamforming speech;speech enhancement;channel mask prediction;beamforming speech analyze;singlechannel speech;use singlechannel speech;mask prediction networks;improvedd mvr beamforming;speech analyze mask;enhancement deep networks;beamforming;mask prediction;beamforming using single;mvr beamforming;noise spatial covariance;single channel mask;mask prediction affects;analyze mask prediction;beamforming using;enhancement deep;mvr beamforming using;background noise challenging;channel mask;noise challenging;noise spatial;removal background noise;steers mvr beamforming", "pdf_keywords": ""}, "a0ab4106dabd6bc067c7b3e4db06807e2c0f6036": {"ta_keywords": "language models pretrained;pretrained language models;models pretrained language;pretrained language;basis pretrained language;large general corpus;language models;models pretrained;corpus used task;general corpus;corpus;large scale pretraining;general corpus used;learning framework tlim;pretraining results;language models standard;pretraining results given;pretraining;corpus used;available basis pretrained;pretrained;introductionno data available;introductionno data;basis pretrained;train methodswe;efficient learning framework;labeled task data;learning framework;train methodswe propose;simple efficient learning", "pdf_keywords": "language models pretraining;pretraining language models;pretrained language models;contextpretrained language models;language model pretraining;language models efficient;large general corpus;general corpus learns;contextpretrained language;corpus jointly optimizes;large pretrained language;relying pretraining language;corpus learns retrieved;driven language modeling;driven language models;pretraining labeled task;corpus learns;pretraining language;task driven language;corpus labeled task;model natural language;pretrained language;novel language models;pretraining algorithms language;corpus task data;language models;objective language modeling;learning natural language;language modeling objective;language models nlls"}, "a4f2e6c38454c9e7b4068a456813d622b91f2663": {"ta_keywords": "speech diadochokinetic dk;speech diadochokinetic;reported speech diadochokinetic;diadochokinetic dk rates;diadochokinetic dk;diadochokinetic;dk rate measurement;dk rate routine;dk rate calculation;calculation dk rate;rate routine clinical;dk rates;dk rate;dk rates published;clinical assessment methodological;norms reported speech;routine clinical assessment;clinical assessment;dk measurement;rate measurement;rate routine;rate measurement identified;problems dk rate;rate calculation differences;calculation dk;collection dk rate;protocol dk measurement;rates published studies;rate calculation;differences norms reported", "pdf_keywords": ""}, "d408be961d0db8b97c0ca6b2fc7afd3c9dc914e7": {"ta_keywords": "transportation management platforms;transportation management platform;transportation management;introduction transportation management;platform mobility planning;mobility planning application;mobility planning;component transportation management;transportation demand management;management platform mobility;manageability modularity allows;required manageability modularity;manageability modularity;platform mobility;available mobility;localized transportation demand;integrate available mobility;transportation;localized transportation;introduction transportation;available mobility options;mobility options;transportation demand;mobility;options localized transportation;mobility options localized;component transportation;central component transportation;resiliently methods modularity;modularity allows", "pdf_keywords": ""}, "4fe70c172cc38c2eb15103f0f1eac4e6766c60e6": {"ta_keywords": "robust voice activity;noise robust voice;voice activity detection;voice activity;robust voice;voice activity using;voice activity patient;patient noise robust;noise robust;identify noise robust;detection patient noise;case noise robust;activity detection patient;activity detection model;voice;reestimation noise robust;identify noise;patient noise;activity detection;noise;used identify noise;activity patient model;robust;based reestimation noise;reestimation noise;activity using scale;case noise;activity patient;detection model used;report case noise", "pdf_keywords": ""}, "efe9fe804f34b18524708b18293508191bda78eb": {"ta_keywords": "faults employing redundancy;modular redundancy;triple modular redundancy;modular redundancy tr;redundancy analyzing;employing redundancy;redundancy;employing redundancy analyzing;redundancy tr;redundancy analyzing results;faults employing;redundancy tr historical;level reliability suffers;various kinds faults;reliability tr provides;desirable level reliability;reliability suffers high;level reliability;faults;program separate executions;kinds faults employing;separate executions program;reliability;reliability suffers;triple modular;reliability tr;levels reliability;levels reliability tr;modular;excellent levels reliability", "pdf_keywords": ""}, "395044a2e3f5624b2471fb28826e7dbb1009356e": {"ta_keywords": "paraphrastic sentence embeddings;supervision paraphrase database;paraphrase database;annotated textual similarity;sentence embeddings based;sentence embeddings;textual similarity datasets;supervision paraphrase;based supervision paraphrase;textual similarity;paraphrase database ganitkevitch;general purpose paraphrastic;paraphrastic sentence;paraphrastic;embeddings based supervision;annotated textual;evaluating annotated textual;paraphrase;purpose paraphrastic sentence;purpose paraphrastic;textual;embeddings;embeddings based;architectures evaluating annotated;similarity datasets;evaluating annotated;2013 compare compositional;compare compositional architectures;similarity datasets drawn;compare compositional", "pdf_keywords": "embedding sentences representations;paraphrastic sentence embeddings;learn word embeddings;textual similarity tasks;sentences representations improve;embedding sentences;word embeddings optimize;sentence embeddings;prediction textual similarity;sentence embeddings based;sentence embeddings propose;word embeddings thorough;supervision paraphrase database;textual similarity rn;word embeddings;universal sentence embeddings;sentences representations;text similarity tasks;baseline embedding sentences;sentence similarity;textual similarity datasets;model textual similarity;classification sentence similarity;tuning paraphrase relationships;annotated textual similarity;paraphrase database;similarity entailment tasks;textual similarity task;similarity textual similarity;similarity tasks entailment"}, "14551d2bf2584bb1ea7ad69f9a64419bab82bb6e": {"ta_keywords": "sound event detection;sound event;tool sound event;based sound event;bssed sound event;features audio data;local features audio;features audio;introductionconformer bssed sound;event detection simi;learning data augmentation;tool sound;audio data;important tool sound;conformer based sound;data augmentation methodsthe;audio data effectively;audio;data augmentation;semi supervised;employs conformer convolution;cnns global features;conformer convolution augmented;bssed sound;semi supervised learning;event detection;using cnns global;event detection paper;conformer convolution;cnns global", "pdf_keywords": ""}, "469ad889bd628e2cf46424f7097c4830719ec740": {"ta_keywords": "vowel space estimates;automatic vowel space;talker vowel space;estimation talker vowel;vowel space using;vowel space representations;vowel space;expanded vowel space;introductionrelating automatic vowel;talker vowel;automatic vowel;sparse expanded vowel;vowel;expanded vowel;estimates talker intelligibility;talker intelligibility;estimation talker;space estimates talker;automatic estimation talker;talker intelligibility methodsa;intelligibility scoring supervised;estimates talker;dimensional space intelligibility;space intelligibility scoring;intelligibility scoring;talker;intelligibility methodsa novel;space representations including;space representations;automatic estimation", "pdf_keywords": ""}, "d9944e13a38e5ca685985c9b5c050ec6d300e104": {"ta_keywords": "non verbal communication;verbal communication communication;verbal communication;communication communication;communication paradigm;non verbal;communication communication context;communication;aforementioned communication paradigm;communication context;problem non verbal;verbal;communication context aforementioned;aforementioned communication;context aforementioned communication;problem non;approach problem non;non;present new approach;article present;article present new;present new;purpose article present;new approach;present;new approach problem;approach;paradigm;context;approach problem", "pdf_keywords": ""}, "ba602ea9aaecab5a3ad243211f110ae7db4cc66a": {"ta_keywords": "deploying learning algorithms;strategic classification decision;behavior deploying learning;classifier data distribution;strategic classification;classification decision;classify closed loop;classification decision dependent;performative prediction;classifier;deploying learning;classification;prediction seek classify;learning algorithms;mapping classifier;decisions learner;classifier data;mapping classifier data;decisions learner work;decision dependent distributions;performative prediction seek;classifier underlying;learner work strategic;classifier underlying data;learning algorithms explicitly;work strategic classification;works performative prediction;properties mapping classifier;classify;seek classify closed", "pdf_keywords": ""}, "ef6a4d8bf248944ca1d0cfdc107d3bb107f57bff": {"ta_keywords": "diagnosis pulmonary infection;pulmonary infection;patients diagnosis pulmonary;diagnosis pulmonary;pulmonary;patients diagnosis;infection;management patients diagnosis;diagnosis;patients;management patients;approach management patients;approach management;new approach management;article discuss importance;importance new approach;management;new approach;article discuss;article;approach;aim article discuss;discuss importance new;aim article;discuss importance;new;importance new;importance;discuss;aim", "pdf_keywords": ""}, "6c68866e6486923d2e8b999de57d450c9d4febab": {"ta_keywords": "neural machine translation;translation pabm neural;machine translation nm;statistical machine translation;machine translation sm;machine translation phrase;machine translation pabm;machine translation;phrase based soft;based machine translation;translation phrase based;phrase based machine;soft forced decoding;phrase based sm;pabm neural machine;translation sm phrase;improving neural machine;sm phrase based;translation nm;pabm neural;forced decoding compared;translation pabm;neural machine;translation sm;forced decoding;decoding compared traditional;decoding compared;improving neural;soft forced;sm phrase", "pdf_keywords": ""}, "c2b22a18ea2ed444c6c1f5bb27ab55bda2b44567": {"ta_keywords": "speech translation systems;emphasis transfer speech;backgroundtraditional speech translation;transfer speech translation;speech translation using;emphasis transfer;model emphasis transfer;speech translation;transfer speech;translation systems;backgroundtraditional speech;conditional random fields;paralinguistic information;paralinguistic;paralinguistic information recent;neural networks resultsour;utilizing conditional random;model emphasis;translation using;speech;random fields;translation systems oblivious;backgroundtraditional;random fields crfs;conditional random;neural networks;translation using approach;oblivious paralinguistic information;systems oblivious paralinguistic;neural networks methodswe", "pdf_keywords": ""}, "b799d66c710dd82a1b925b9c31e55a0d2d99b624": {"ta_keywords": "hidden markov model;sharing hidden markov;hidden markov;people activities urban;markov model sshimm;markov model;modeling people activities;activities urban space;activities urban;dynamics human activities;model temporal dynamics;urban states;markov;urban states city;temporal dynamics human;population flows;model temporal;human activities;people activities;human activities concisely;temporal dynamics;state sharing hidden;extracts urban states;present state sharing;states city captures;urban space crucial;urban;urban space;state sharing;states city", "pdf_keywords": ""}, "0c39c0dc296a902e4a5eb85182209f7b9e6053b0": {"ta_keywords": "backgroundresearchal deep learning;deep learning dm;dataflow graph construction;dataflow graph;backgroundresearchal deep;repeated dataflow graph;dataflow;existing dataflow;existing dataflow based;deep learning;dataflow based programming;dataflow based;inefficient repeated dataflow;repeated dataflow;existing existing dataflow;vertex centric programming;backgroundresearchal;data network;programming models dm;learning dm models;learning dm;models dm inefficient;graph construction processing;data network structure;programming models;vertex centric;handling data network;dm models;cas vertex centric;trees graphs existing", "pdf_keywords": "dynamic dataflow graphs;dataflow graphs vertex;dataflow graphs;dataflow graphs optimization;computation dataflow graphs;dataflow graphs implement;graphs existing dataflow;best dataflow graphs;dataflow graph;dataflow graphwe;dataflow graphs implementation;data flow graphs;dataflow graphwe present;integrating dataflow graph;model graph computing;deep learning cas;graph computing;dataflow graphs evaluate;flowing computational graphs;repeated dataflow graphwe;computational dynamic dataflow;programming models deep;dynamic deep learning;efficient dynamic deep;flow graphs;dataflow graph user;graph execution engine;dynamic dataflow;learning cas vertex;flow graphs applications"}, "57026b2d45fa59c6326b5a1d2e27626403f083ba": {"ta_keywords": "ethics artificial intelligence;artificial intelligenceligence courses;ethics artificial;issues ai courses;ai courses instructors;ai courses;philosophical issues ai;ethical philosophical;moral ethical philosophical;intelligenceligence courses;intelligenceligence courses recent;moral ethical;ethical philosophical issues;students artificial intelligence;considerations artificial intelligenceligence;ethical;understand moral ethical;artificial intelligence society;ethical philosophical impacts;artificial intelligence practitioners;artificial intelligenceligence;intelligence society methodsin;ethics;impacts artificial intelligence;students artificial;intelligenceligence;issues ai;practitioners understand moral;prepares students artificial;address moral ethical", "pdf_keywords": "ethical issues ai;ethical issues ais;ethics artificial intelligence;ai research ethical;ethical ethics artificial;ethics artificial;ai educators;use ai educators;ethical issues artificial;ethical considerations machines;teaching ethical systems;computer ethics;intelligence design ethical;robot ethics described;philosophical issues ai;teaching ethical;study robot ethics;robot ethics;ethics education;ethical ethical;computer ethics course;ethical systems discussed;robot ethics implications;fiction computer ethics;ai educators alsothe;ethical ethical discussed;discussed ethical ethical;ethical philosophical;ethical discussed ethical;classroom ethical"}, "653add540adae12491fade7e18ec4e1e4288b4a7": {"ta_keywords": "selection advising career;course selection advising;selection advising;academic advising;support academic advising;advising career paths;academic advising lessons;advising career;decision theoretic advising;advising;advising lessons;advising support tool;advising lessons learned;theoretic advising support;advising support;theoretic advising;support tool undergraduates;concerning course selection;course selection;career paths;career paths results;support academic;tool support academic;students multiple majors;majors colleges surveys;tool undergraduates;undergraduates;majors colleges;tool undergraduates large;academic", "pdf_keywords": "course selection advising;automated advising students;online advising tools;software academic advising;advising students;students want advising;utility automated advising;development automated advising;automated advising support;advising tools;recommending courses students;selection advising;approach recommending courses;selection advising career;automated advising;recommendations students want;academic advising students;recommendations students;automated advising use;online advising;advising support tool;designing advising support;decision theoretic advising;advising career paths;recommendation students tertiary;advising students advisors;support academic advising;recommendation students;designing advising;advising support software"}, "6fe62b967376361d7cd55e1033ab968895841d67": {"ta_keywords": "focused concept miner;deep learning text;backgroundfocused concept miner;fcm interpretable deep;concepts text data;concept miner fcm;learning text mining;concept miner;learning text exploration;concepts text;text mining;focus discovery concepts;interpretable deep learning;text exploration;learning text;fm interpretable deep;concept miner fm;interpretable deep;level concepts text;corpus level concepts;focused concept;miner fcm interpretable;concepts highly;focus discovery;fcm interpretable;data focus discovery;text exploration methodswe;text mining algorithm;introduce focused concept;concept correlational importance", "pdf_keywords": ""}, "29001ac04e61dfffb8e24ffd3e351ece12ce44af": {"ta_keywords": "estimation phase mixture;frequency representation mixture;time frequency masks;frequency masks;representation mixture signal;phase estimation;mixture signal;estimation phase;phase estimation phase;frequency masks res;mask applied time;estimating magnitude target;target source estimating;time frequency representation;reconstructing estimated time;mixture used reconstructing;valued mask applied;frequency representation;phase mixture;lack phase estimation;phase mixture used;complex time frequency;magnitude target source;mixture signal lack;estimate complex time;valued mask;reconstructing estimated;applied time frequency;estimated time domain;source estimating", "pdf_keywords": "phase codebook softmax;softmax layer waveform;magnitude codebook softmax;mask inference networks;representations source separation;softmax;processing speech separation;learning phasebook layer;softmax activation magnitude;algorithm speech separation;task learning phasebook;source separation;deep learning speech;based outputs softmax;softmax output predicts;convex softmax output;softmax output;phase mask network;source separation source;outputs softmax;outputs softmax consider;softmax layer;jointly softmax;convex softmax activation;jointly softmax layer;softmax consider;train mask inference;learning phasebook;softmax activation;interpolated jointly softmax"}, "5dce0fd43a21825bebd8121fd0a28155d524c44c": {"ta_keywords": "patients history history;patients history;management patients history;history history;history history history;history;patients;management patients;approach management patients;approach management;management;new approach management;article discuss importance;article discuss;new approach;importance new approach;approach;article;discuss importance new;discuss importance;aim article discuss;discuss;aim article;importance new;importance;new;aim", "pdf_keywords": ""}, "4b2d583e22f378f9104814d9f63cda411ddd5825": {"ta_keywords": "lexical sememe prediction;lingual lexical sememe;sememe based linguistic;cross lingual lexical;sememe prediction ememes;automatically predict seme;lexical sememe;languages sememe based;introduction cross lingual;sememe prediction;predict seme;lingual lexical;prediction ememes;languages sememe;prediction ememes defined;linguistic knowledge bases;sememe prediction aiming;nonllogs languages sememe;cross lingual;linguistic knowledge;task cross lingual;based linguistic knowledge;lingual;sememe based;minimum semantic units;lexical;units human languages;sememe;linguistic;automatically predict", "pdf_keywords": ""}, "f1513d72cb5dd6d70541cce0da36b77467128d13": {"ta_keywords": "q4mre pilot task;2013 q4mre methodsthe;clef 2013 q4mre;2013 q4mre pilot;q4mre pilot;q4mre methodsthe core;q4mre methodsthe;2013 q4mre;q4mre;error rate training;2013 q4mre use;q4mre use;pilot task clef;introductionnaist clef 2013;exam pilot task;task clef 2013;rate training mert;q4mre use minimum;nology entrance exam;training mert;training mert train;task clef;exam;main task clef;entrance exam pilot;entrance exam;clef 2013;exam pilot;tech nology entrance;rate training", "pdf_keywords": ""}, "838fbfd9066dbbac6c10059c5b183046fb1cd9d1": {"ta_keywords": "deep bayesian active;bayesian active learning;active learning natural;active learning;bayesian active;deep bayesian;background deep bayesian;learning natural language;natural language processing;question aim supervised;deep learning natural;aim supervised learning;supervised;learning natural;language processing al;natural language;supervised learning;data dependence deep;deep learning;language processing;learning;language processing established;supervised learning practitioners;evaluating validation;selecting model al;dependence deep learning;validation;bayesian;methods evaluating validation;aim supervised", "pdf_keywords": "tagging tasks large;active learning al;deep learning natural;reduce deep learning;learning network sequences;learning natural language;tagging tasks;deep learning;tagging tagging tasks;datadependence deep;active learning;active learning classification;question supervised;question supervised learning;nonlack tagging;natural language processing;datadependence deep learning;open question supervised;approach reduce deep;learning al;tagging;nonlack tagging tagging;deep learning data;backgroundthe datadependence deep;learning network;applications active learning;budgets active learning;bayesian active learning;natural language;learning data"}, "5693c74eb8ffde1490ba480fdc963f008243906a": {"ta_keywords": "annotation framework sequencetagging;alcatag sequence tagging;crowdd annotation framework;sequence tagging;sequence tagging tasks;based crowdd annotation;crowdd annotation;sequencetagging methodswe introduce;sequencetagging;sequencetagging methodswe;suggesting annotations sampling;annotation framework alcatag;framework sequencetagging;suggesting annotations;dynamically suggesting annotations;annotations sampling informative;framework sequencetagging methodswe;annotation framework;tagging;active intelligent recommendation;data annotation framework;annotations sampling;data annotation;based data annotation;backgroundalpacatag active learning;annotations;annotation;named entity recognition;learning based crowdd;entity recognition ner", "pdf_keywords": ""}, "a8239258abded4f08d1bf270c2e86662f4dc1760": {"ta_keywords": "problem solving knowledge;solving knowledge acquired;student prior knowledge;solving knowledge;learning problem solving;problem solving skills;solving skills;sistudent learning outcomesthe;sistudent learning;learning called sistuddent;knowledge changes learning;prior knowledge;prior knowledge changes;prior knowledge learning;prior knowledge methodswe;measured sistudent learning;investigate learning;learning complex skill;investigate learning complex;impact prior knowledge;study investigate learning;learning called;knowledge acquired;learning process;knowledge;learning outcomesthe;specific prior knowledge;complex skill changes;knowledge learning;knowledge learning problem", "pdf_keywords": ""}, "a182a8a0678857df5c513d52469fa707c32e69ec": {"ta_keywords": "translation rules based;statistical machine translation;translation rules;appropriate translation rules;machine translation smt;machine translation;machine translation perform;space rule selection;based rule selection;syntax based statistical;rule selection;rule selection crc;rule selection methods;translation smt choose;rules based sentence;dependent rule selection;rule selection mers;translation perform context;entropy based rule;space rule;syntax based;continuous space rule;choose appropriate translation;translation smt;context dependent rule;crc model syntax;sentence context paper;model syntax based;selection crc model;discrete representations words", "pdf_keywords": ""}, "4e3935ef7da6bcbb202ec7f8b285c313cadcd044": {"ta_keywords": "question answering datasets;question answering;seeking question answering;questions question answering;answering datasets usually;question answering systems;answering datasets;questions generic factoid;answering specific questions;factoid type information;answering systems answer;systems answer questions;answering systems;answering specific;factoid type;existing information seeking;information seeking question;factoid;generic factoid;answer questions;information seeking;research papers read;academic research papers;answering;goal answering specific;generic factoid type;complex reasoning claims;answer questions make;read goal answering;usually contain questions", "pdf_keywords": "question answering datasets;question answering dataset;papers annotators asked;papers annotators;annotators interested papers;answering dataset academic;question answering systems;interested papers annotators;question answering;informationseeking question answering;questions question answering;answering datasets;answer questions papers;answering datasets usually;seeking question answering;annotators asked answer;answering dataset;answering systems;processing political events;annotators interested;questions papers;annotators asked;research papers dataset;answering specific questions;annotators annotators interested;data provide annotators;annotators;dataset annotators;answering systems answer;answers text paper"}, "85e148ac629b1b38556c5fe5f8d657f2eb01a701": {"ta_keywords": "differences sex differences;sex differences;sex differences sex;differences sex;differences;sex", "pdf_keywords": ""}, "a997d6e253f08a3e589432c611d6d2a3097d7629": {"ta_keywords": "collaborative online research;online research paper;research exchange;research exchange methods;online research tool;tool research exchange;introduction research exchange;online research;collaborativeative paper annotation;research paper tool;col collaborativeative paper;collaborativeative paper;research tool;research tool designed;issues collaborative online;paper tool research;research exchange col;efficacy online research;collaborative online;exchange col collaborativeative;research;research paper;paper annotation tool;tool research;col collaborativeative;research looks explore;paper annotation;research students;collaborativeative;research students getting", "pdf_keywords": ""}, "3ec37205c9201fc891ab51da200e361fdc34bfb3": {"ta_keywords": "word embeddings reading;embeddings reading compprehension;embeddings reading;trained word embeddings;study word embeddings;word embeddings;word embeddings representation;vocabulary tokens;reading compprehension;representation vocabulary tokens;reading compprehension tasks;vocabulary tokens test;embeddings representation vocabulary;pre trained word;reading;reading compprehension methods;embeddings;deep learning architectures;research reading compprehension;representation vocabulary;deep learning;study word;trained word;novel deep;comparativative study word;embeddings representation;vocabulary;research reading;novel deep learning;learning architectures", "pdf_keywords": "trained word embeddings;word embeddings 2nd;word embeddings;embeddings initializing word;models reading comprehension;trained embeddings beneficial;embeddings trained;neural models reading;embeddings neural;trained embeddings initializing;trained embeddings neural;using embeddings trained;pre trained embeddings;trained embeddings;trained embeddings limited;reading comprehension tasks;embeddings neural networks;embeddings trained right;embeddings generate high;embeddings beneficial;generating word vectors;embeddings generate;representation outof vocabulary;comprehension tasks primarily;machine comprehension use;use embeddings generate;embeddings limited fact;embeddings 2nd;initializing word vectors;dataset machine comprehension"}, "cfad4dc5f1f7fcaf7ca318acf672ad92d47f8413": {"ta_keywords": "inequality employment face;inequality employment;address inequality employment;workforce imbalance;federal contract compliance;contract compliance programs;workforce imbalance result;contract compliance;wells fargo labor;employment face hiring;compliance programs occ;fargo labor department;compliance programs;address workforce imbalance;hiring paradox;office federal contract;compliance;fargo labor;employment;hiring paradox failing;imbalance result legal;workforce;federal contract;labor department;labor department office;result legal conflict;legal conflict;microsoft wells fargo;employment face;failing address workforce", "pdf_keywords": "laws biases hiring;legal ethical hiring;biases hiring;bias aware legal;discrimination laws biases;biases hiring pipeline;ethical hiring goals;ethical hiring;legal solution hiring;legal algorithmic challenges;hiring algorithms;hiring algorithms highlight;legal algorithmic;algorithmic approaches employment;discrimination law new;discrimination law;algorithmic bias born;hiring paradox argue;unfairness employment;unfairness employment decision;tracks legal algorithmic;solve hiring paradox;addressing discrimination;solve hiring;discrimination;discrimination laws;evaluate candidates biasaware;hiring paradox;illegal discriminatory practices;candidates biasaware"}, "f889723a4427e914e4e32547dfd0ca4996170180": {"ta_keywords": "introductionin voice conversion;voice conversion;generate converted speech;voice conversion challenge;converted speech;voice conversion vc;converted speech paradigm;transcribe source speech;latest voice conversion;speech recognition ar;input text speech;automatic speech;use automatic speech;speech tacs generate;text speech;text speech tacs;automatic speech recognition;speech tacs;modeling prosody;ar model transcribe;speech recognition;source speech;model transcribe source;model transcribe;transcribe source;transcribe;voice;overlooks modeling prosody;source speech underlying;introductionin voice", "pdf_keywords": "generate converted speech;introductionin voice conversion;voice conversion systems;voice conversion;conversion systems voice;challenge voice conversion;voice conversion based;systems voice conversion;voice conversion challenge;speech synthesis prosody;new speech synthesizing;text speech synthesis;speech synthesizing;speech synthesis;speech tts generate;voice conversion vc;transcribe source speech;performance voice conversion;speech synthesizing synthesis;converted speech;application voice conversion;converted speech paradigm;training source prosody;latest voice conversion;generate natural speech;synthesis prosody transfer;source prosody transfer;text speech tts;input text speech;neural vocoder generate"}, "f4cca8ea79e26fa20a91c3d3b769c9f7b82a6207": {"ta_keywords": "microphones pinna spectral;pinna spectral notches;spectral notches median;spectral notches extracted;spherical microphone array;spherical array microphones;array microphones pinna;spherical microphone;microphones pinna;virtual spherical microphone;extraction pinna spectral;notches extracted computationally;spectral notches;microphone array;notches extracted head;notches extracted;array microphones;notches median plane;microphone array discussed;notches median;measured spherical array;microphone;general pinna spectral;microphones;pinna spectral;method extraction pinna;impulsee response measured;response measured spherical;notches;extraction pinna", "pdf_keywords": ""}, "8c38bffc058d558e7c734032ba63942865e05ae4": {"ta_keywords": "deductive reasoning answers;deductive reasoning;deductive reasoning proposed;method deductive reasoning;deductive;deductive closure;disagree deductive reasoning;exactly logical queries;logical queries;quantume systems;quantume systems disagree;method deductive;backgroundthe deductive closure;quantume;novel method deductive;reasoning answers;systems disagree deductive;reasoning answers require;logical queries kc;closure ideal knowledge;backgroundthe deductive;deductive closure ideal;answer queries;answer queries real;disagree deductive;failing answer queries;reasoning proposed;ideal knowledge base;answers require generalization;reasoning proposed experiments", "pdf_keywords": ""}, "e4a6bc3ac385b8982bbbe0a2a5ac0c79101ec979": {"ta_keywords": "malignant neoplasm gastrointestinal;neoplasm gastrointestinal;neoplasm gastrointestinal tract;gastrointestinal tract malignant;tract malignant neoplasm;malignant neoplasm;diagnosed malignant neoplasm;tract malignant;diagnosed malignant;patient diagnosed malignant;malignant;neoplasm;gastrointestinal tract patient;gastrointestinal;case patient diagnosed;gastrointestinal tract;patient diagnosed;diagnosed;present case patient;tract patient;tract patient treated;patient treated combination;case patient;patient treated;tract;article present case;patient;combination;treated combination;combination combination", "pdf_keywords": ""}, "9bbeb4f0e48032df19f9f6a08839da5d2e60e8eb": {"ta_keywords": "distant stereo microphones;using distant stereo;environment speech applications;backgroundreverberant noisy automatic;distant stereo;stereo microphones challenging;binary masking algorithm;noisy automatic speech;backgroundreverberant noisy;home environment speech;enhance binary masking;automatic speech recognition;speech applications scenario;masking algorithm;speech recognition;binary masking;masking algorithm using;speech recognition ar;stereo microphones;environment speech;microphones challenging;speech applications;sound sources environment;microphones challenging desirable;automatic speech;interference methodswe propose;sound sources;interference methodswe;using distant;ar using distant", "pdf_keywords": ""}, "8f963beca679cb1129df0a944c6de4b126e20fd5": {"ta_keywords": "sequenceence speech recognition;trained language model;term memory lstm;train seq2seq model;sequenceence sequenceence speech;state seq2seq decoder;seq2seq decoder;sequenceence speech;memory lstm;short term memory;train seq2seq;seq2seq decoder long;seq2seq model;language model;memory control sequenceence;memory lstm memory;lstm memory;lstm memory cell;seq2seq model integrate;speech recognition;language model proposed;trained language;pre trained language;lstm;state seq2seq;seq2seq;term memory;introductionlanguage model integration;hidden state seq2seq;introductionlanguage model", "pdf_keywords": "seq2seq lstm decoder;seq2seq model training;seq2seq lstm;term memory lstm;training seq2seq model;seq2seq model decoder;seq2seq model trained;trained language model;states seq2seq lstm;short term memory;training seq2seq;rnl seq2seq model;decoding seq2seq model;memory lstm;seq2seq decoder;state seq2seq decoder;potential training seq2seq;lstm decoder;memory cell seq2seq;lstm memory;train seq2seq model;seq2seq decoder long;lstm memory cell;memory lstm memory;train seq2seq;rnl seq2seq;methods decoding seq2seq;decoding seq2seq;seq2seq model integrated;method seq2seq decoder"}, "b2c47dd46bf7087b754aed45f06b6196cf2b1c28": {"ta_keywords": "imaging acute abdomen;diagnosis acute abdomen;acute abdomen;associated acute abdomen;acute abdomen chapters;acute abdomen difficult;abdomen chapters imaging;diagnostic imaging acute;imaging acute;abdomen;abdomen difficult;diagnosis acute;abdomen difficult difficult;radiographic sonographic computed;analysis radiographic sonographic;radiographic sonographic;diagnostic imaging;abdomen chapters;computed tomography findings;aspects diagnostic imaging;sonographic computed;sonographic computed tomography;tomography findings;tomography findings diseases;computed tomography;analysis radiographic;radiographic;image analysis radiographic;sonographic;diseases associated acute", "pdf_keywords": ""}, "46d87d4614d9353f1b7d527333073ef9109bfaea": {"ta_keywords": "label item ranking;pick correct labels;item labeling;likely label item;users pick label;label set candidates;labeling;applications item labeling;pick label;determine likely label;item ranking;item labeling methodswe;labels items;labels;label item;item ranking users;label;pick label set;labeling methodswe analyze;problem crowd sourced;correct labels items;labeling methodswe;fast algorithm;backgroundhitsndiffs fast algorithm;ranking;ranking users based;likely label;labels items start;candidates set items;label set", "pdf_keywords": ""}, "82ae0d4b41046ccedb435ece08a61f198cf77bb9": {"ta_keywords": "generating editing sentences;generation text;text content manipulation;controlledled generation text;editing sentences textual;generate sentence accurately;generate sentence;editing sentences;generating editing;generation text high;content manipulation;sentences textual attributes;text content;objective generate sentence;content manipulation objective;sentences textual;textual;text;textual attributes;writing style wording;textual attributes sentiment;setting text content;accurately describes content;style wording transitions;writing style;style wording;describes content;content record writing;progress generating editing;setting text", "pdf_keywords": "existing sentences exemplars;text generation;text generation focused;improving content fidelity;neural machine translation;sentences exemplars new;accurate content description;natural language generation;sentences exemplars;adaptively imitate style;data text generation;style imitation data;sentences exemplars allowing;adjversarial style transfer;weak supervisions content;existing sentences;integrating text style;improving content;attention copy;approach attention copy;use existing sentences;style processing exemplars;implementing style imitation;machine translation models;learning content;balances content fidelity;description content;style imitation;content fidelity style;encourage accurate content"}, "bc1832e8b8d4e5edf987e1562b578bd9aa5e18a9": {"ta_keywords": "sequencemazizing neural network;sequence sequencemazizing neural;speech recognizer;sequencemazizing neural;sequence summarizing neural;selection sequence sequencemazizing;acoustic conditions similarity;similarity acoustic conditions;data selection sequence;mismatch condition training;robustness speech recognizer;condition training data;similarity acoustic;respect similarity acoustic;sequence sequencemazizing;sequencemazizing;training data augmentation;speech recognizer deployed;summarizing neural network;improve robustness speech;training data;training test conditions;based sequence summarizing;neural network mismatch;neural network extracts;sequence summarizing;condition training;robustness speech;data selection;data augmentation", "pdf_keywords": ""}, "e212f788c701370af02b138d2a61e180cddfb138": {"ta_keywords": "target machine translation;translation multi synchronous;translating single source;simultaneously translating single;machine translation multi;language multiple target;simultaneously translating;source language multiple;translation multi;model disambiguate translations;single source language;language model translations;method simultaneously translating;multiple target languages;translating single;disambiguate translations;machine translation;model translations;language multiple;model translations associated;language model disambiguate;translations associated;weak language model;target languages;translations associated use;context free grammars;translations;free grammars methodswe;strong language model;multi synchronous context", "pdf_keywords": ""}, "c5dfc5fe7102fd8647edd1c9483aded82557e544": {"ta_keywords": "predictive models increasingly;predictive model vet;interpret predictive;predictive models;analyse interpret predictive;interpret predictive model;algorithms provide recourses;predictive model;predictive;discriminatory;dis discriminatory;recourses affected individuals;discriminatory deployed;non dis discriminatory;dis discriminatory deployed;context predictive models;provide recourses affected;discriminatory deployed real;models increasingly;recourses offers meaningful;recourses affected;ensure recourses;provide recourses;context predictive;thoroughly ensure recourses;recourses;affected individuals;developing algorithms provide;developing algorithms;models increasingly deployed", "pdf_keywords": "generating recourses datasets;actionable recourse summaries;models demonstrate recourse;counterfactuals individual recourse;interpretable summaries recourses;recourses datasets;recourses challenge framework;accurate summary recourses;recourse costs interpretability;generate recourse summaries;optimizing recourse accuracy;quantify recourse;analysis recourses challenge;actionable recourse;optimize recourse correctness;recourses analysis context;recourses challenge;generate recourses;analysis recourses individual;recourses offers meaningful;summary recourses;optimizing recourse;improved optimizing recourse;generating recourses;learn recourse costs;generalised recourse search;optimize recourse;generalised recourse;model analysis recourses;recourse summaries"}, "90ed32fa521b9e85f1c9efe356619814a2e79961": {"ta_keywords": "tetrahydromyces cerevisiae development;model tetrahydromyces cerevisiae;tetrahydromyces cerevisiae;role tetrahydromyces cerevisiae;cerevisiae development;cerevisiae development new;cerevisiae;new model tetrahydromyces;model tetrahydromyces;role tetrahydromyces;knowledge role tetrahydromyces;tetrahydromyces;development new model;new model;model;development new;development;current knowledge role;current knowledge;knowledge role;present current knowledge;knowledge;aim article present;aim article;new;role;aim;article;article present;current", "pdf_keywords": ""}, "a75869d69cc86f501939c237ae4711aa2885f6a6": {"ta_keywords": "translation meta learning;low resource translation;resource translation meta;translation meta;neural machine translation;agnostic meta learning;machine translation;resource translation;low resource neural;machine translation nmt;meta learning;meta learning algorithm;low resource languages;meta learning problem;multilingual high resource;high resource language;translation nmt methodswe;translation nmt;model agnostic meta;resource neural machine;resource neural;translation;resource language tasks;resource language;low resource;language tasks;agnostic meta;resource languages;multilingual high;2017 low resource", "pdf_keywords": "translation meta learning;multilingual translation meta;low resource translation;neural machine translation;translation low resource;resource machine translation;translation meta;resource translation metalearning;outperforms multilingual translation;translation useful neural;translation applications neural;resource translation low;agnostic metalearning multilingual;translation metalearning;languages meta learning;translation present meta;translation multilingual transfer;meta learning low;translation systems multilingual;machine translation nmt;machine translation;translation metalearning problem;multilingual translation multilingual;machine translation useful;metalearning multilingual;multilingual transfer learning;resource translation language;translate translation systems;target languages meta;processes multilingual translation"}, "18e70ad07561cf09a2d7f0da992a0e87a5e5c0a8": {"ta_keywords": "topic tracking language;tracking language;topic tracking;measure speech recognition;analyze speech recognition;tracking language new;useful analyze speech;analyze speech;tool measure speech;measure speech;speech recognition speech;recognition speech;speech recognition;speech recognition useful;recognition speech recognition;tracking;recognition useful analyze;speech;language new tool;language;topic;useful analyze;recognition useful;recognition;language new;analyze;tool measure;new tool measure;measure;useful", "pdf_keywords": ""}, "8cebfae7cd436241eb5c3442e687a913a75a5531": {"ta_keywords": "word sense induction;sense induction russian;induction russian language;contexts word bank;contexts given word;russian language methodsthe;sense induction;word sense;instance given word;language methodsthe participants;induction russian;contexts word;given word bank;russian language;set contexts word;contexts;given word;senses provided instance;language methodsthe;contexts given;task word sense;group contexts;group contexts given;word accordance senses;bank set contexts;word bank set;asked group contexts;word bank;language;word bank financial", "pdf_keywords": "disambiguation methods slavic;sense induction slavic;sense disambiguation induction;sense induction disambiguation;sense induction russian;word sense disambiguation;word sense induction;sense disambiguation text;russian associated corpus;word sense embeddings;induction slavic language;sense disambiguation;disambiguation induction;induction russian language;disambiguation text corpora;disambiguation induction german;russian language explore;processing word sense;induction disambiguation approaches;induction disambiguation methods;dictionary russian associated;corpus russian;russian explanatory dictionary;active dictionary russian;related words russian;methods slavic language;slavic language shares;corpus russianthe;words russian wiktionary;disambiguation methods"}, "c6bb04f3d8000b7e800f6359082de39548c7da79": {"ta_keywords": "context structural locality;utilizing structural locality;structural locality ubiquitous;structural locality;hierarchies source code;structural locality non;locality ubiquitous;locality non parametric;parametric language models;topical clusters text;text project hierarchies;source code repositories;locality ubiquitous feature;topical clusters;project hierarchies source;clusters text project;examples external source;language models generate;include topical clusters;locality;language models;non parametric language;code repositories;external source methods;local hierarchies examples;locality non;source methods propose;organized local hierarchies;code repositories paper;context structural", "pdf_keywords": "locality nonparametric language;nonparametric language models;localitythe relevance contexts;structural locality nonparametric;locality nonparametric;nonparametric language;learn locality information;locality non parametric;concept structural locality;language models improve;incorporating locality features;domains locality features;locality information models;language domain empirically;locality information using;language modeling benchmark;adding locality information;retrieval demonstrate locality;nonparametric retrieval;integrating locality features;locality features domains;localitythe relevance;learn locality;novel language models;text demonstrate locality;information nonparametric retrieval;parametric natural language;enhance locality information;incorporating locality;novel language modeling"}, "97846070369f66c3080a0803be58e96963dec581": {"ta_keywords": "website usage twitter;usage twitter covid;usage patterns tweet;twitter covid;twitter covid 19;usage twitter;cluster websites urls;cluster websites;multi view clustering;view clustering;twitter;clustering analysis website;website usage;view clustering technique;data cluster websites;analysis website usage;patterns tweet;views data cluster;clustering;patterns tweet text;urls based usage;clustering technique;clustering analysis;external website usage;websites urls based;tweet text occurs;clustering technique able;tweet text;cluster;patterns external website", "pdf_keywords": ""}, "08f6819e66318cd49cddefd5d690a752d1098da7": {"ta_keywords": "conceptualize claims;appear conceptualize claims;claim identification objective;conceptualize claims quite;claim methods;claim practical applications;claim methods perform;concepts claim methods;claim identification;claim practical;cross dmain claim;dmain claim;dmain claim identification;claims quite differently;claims;claim cross dmain;claim;different conceptualizations claim;concepts claim;conceptualizations claim;claim cross;conceptualizations claim practical;claims quite;essence claim cross;essence claim;qualitative analysis;context essence claim;qualitative;datasets appear conceptualize;identification objective aim", "pdf_keywords": "argumentation mining;argument mining;argument mining useful;tools argument mining;argument mining popular;processing argumentation mining;argumentation mining process;claim annotations computational;annotations computational argumentation;argumentation study corpora;analyze claim annotations;conceptualizations claims datasets;context argument mining;analysis claim annotations;argumentative components claims;identification argumentative components;analysis argumentative content;claim annotations tackled;investigate claim annotations;argumentation study;mining claims evidence;identification argumentative;argumentative content;compare conceptualizations claims;claim annotations;computational argumentation study;computational argumentation;conceptualizations claims assessed;conceptualize claims;argumentative components"}, "9f73c3f86026c21d0e5e55c70462952c6ada1175": {"ta_keywords": "accelerateing deep learning;training deep;deep learning;training deep neural;dnns prioritizing examples;accelerates training deep;introduction accelerateing deep;deep neural networks;selective backprop;networks dnns prioritizing;deep neural;high loss iteration;dnns prioritizing;accelerateing deep;introduces selective backprop;methods selective backprop;selective backprop technique;training example forward;deep learning focusing;backprop technique accelerates;loss iteration;backprop technique;backprop;neural networks dnns;networks dnns;selective backprop uses;loss iteration methods;examples high loss;backprop uses;neural networks", "pdf_keywords": "learning selective backprop;backpropagations reducing training;accelerating deep learning;accelerate deep learning;deep learning selective;training deep;deep learning selectively;deep learning reducing;computationally expensive backpropagation;expensive backpropagation;backprop variant efficiently;expensive backpropagation steps;iteration selective backprop;deep learning;training deep neural;reducing number backpropagations;deep learning important;accuracy selective backprop;backpropagations reducing;deep neural;selective backprop variant;loss selective backprop;accelerates training deep;neural networks prioritizing;backpropagations;selective backprop approach;approach deep;method deep learning;approach deep learning;learning selective"}, "dc984ea8be018a0244b40468d13f7b734ab55bac": {"ta_keywords": "introductionnural machine translation;machine translation nm;machine translation;translation lexicons efficiently;discrete translation lexicons;probability word translation;translation nm;discrete translation;translation lexicons;systems discrete translation;translations low frequency;translations low;translation nm makes;translating low frequency;encode translations low;word translation candidate;word translation;efficiently encode translations;low frequency words;using attention vector;encode translations;translations;attention vector;translation candidate using;lexicon probability;lexicon probability word;calculate lexicon probability;translating low;frequency content words;translating", "pdf_keywords": "attentional nonmetastatic translation;neural machine translation;introductionnural machine translation;lexical translation probabilities;machine translation;translation normal attentional;machine translation nm;translation lexicons efficiently;probability word translation;improving translation accuracy;machine translation new;machine translation important;machine translation field;utility improving translation;discrete translation lexicons;machine translation development;translation probabilities word;machine translation association;attentional nonmaturity models;translation probabilities;translation field neural;lexicons translation process;improving translation;integrating lexicons translation;translation accuracy;translation development neural;vectors attentional nonmaturity;translation accuracy reducing;translation lexicons;demonstrate translation probabilities"}, "0533ccdc4840eed0fe1769b5e78da912631be609": {"ta_keywords": "optical soiltons formed;equation optical soiltons;optical soiltons exist;optical soiltons;soiltons formed;like photonic crystal;schrodinger equation optical;photonic crystal;photonic crystal bres;soiltons exist various;equation optical;soiltons formed balance;systems like photonic;soiltons exist;dispersion self phase;soiltons;optical;photorefractive materials;like photonic;like photorefractive materials;photonic;materials like photorefractive;dispersion self;nonlinear schrodinger;equation nonlinear schrodinger;photorefractive materials photopolymers;nonlinear schrodinger equation;dispersion regime;materials photopolymers;dispersion", "pdf_keywords": ""}, "4b73f4956c31cd10994c73b21e2c38a60a68d03e": {"ta_keywords": "paper assignment problem;allocating papers referees;assignment problem sided;solving assignment problem;assign indivisible goods;assignment problem;allocating papers;conference paper assignment;sided matching problem;problem sided matching;problem allocating papers;sided matching;assignment problem using;weighted averages assign;papers referees conference;reviewers objects papers;preferences agents reviewers;problem preferences agents;averages assign indivisible;paper assignment;averages assign;matching problem preferences;mechanism solving assignment;papers referees;academic problem allocating;assign indivisible;conference paper;agents reviewers objects;reviewers objects;matching problem", "pdf_keywords": "maximal assignment computationally;assignment computationally efficient;optimal allocation pareto;assignment objective;owa assignment algorithm;assignment algorithm;allocation pareto optimal;assignment problem utilitarian;algorithms assignment problem;assignment utility;utilitarian assignment utility;egalitarian assignment objective;assignment computationally;assignment utility assignment;allocation pareto;assignment objective objective;assignment problem fundamental;assignment algorithm compute;assignment polynomial time;algorithms assignment;utilitarian assignment balance;optimal allocation;allocation optimal;allocation optimal allocation;maximal assignment;optimal allocation optimal;allocation owa assignment;welfare maximmal assignment;irrespective optimal allocation;assignment objective owa"}, "b131cf78363993e4126b2562a156bd9d046c8bc4": {"ta_keywords": "tree based translation;introductionpovot translation useful;based translation models;approach pivot translation;translation models combines;introductionpovot translation;translation models;pivot translation;pivot target translation;pivot translation used;translation useful method;target translation models;method translating languages;useful method translating;translation models source;translating languages;method translating;target translation;based translation;data intermediate language;phrase based tree;translating languages little;translating;translation useful;combines source pivot;based constituent words;source pivot;translation used;popular approach pivot;translation", "pdf_keywords": ""}, "18e5fb8cec55a75b288a499c57d77ede541dc049": {"ta_keywords": "shot question answering;question answering;answering commonsense tasks;language models training;question answering commonsense;commonsense tasks guided;commonsense tasks;pre training models;answering commonsense;language models;pre existing knowledge;answering;neuro symbolic framework;models training;existing knowledge resources;clinical messagewe propose;tasks guided;set language models;knowledge resources form;clinical messagewe;training models;key clinical messagewe;existing knowledge;neuro symbolic;tasks extending prior;knowledge;training models vary;novel neuro symbolic;tasks;effective pre training", "pdf_keywords": ""}, "5b1bb1f6ed091dfd53adf7ebbcda2c48a3b67c2c": {"ta_keywords": "semantic frame induction;unsupervised semantic frame;unsupervised frame induction;task unsupervised semantic;unsupervised semantic;contextualized word embeddings;task unsupervised frame;semantic frame;present semantic frame;frame induction;frame induction using;word embeddings;frame induction qasem;induction using contextualized;word embeddings methods;subtaskb;frame induction showed;semeval 2014 task;2014 task unsupervised;unsupervised frame;subtaskb finished;2019 task unsupervised;subtaska semeval 2014;subtaskb finished runner;performance subtaskb;best performance subtaskb;introduction hmm semeval;task unsupervised;semeval 2019 task;present semantic", "pdf_keywords": "semantic frame induction;grouping verbs frame;unsupervised semantic frame;subtasks grouping verbs;embeddings syntactical features;word sentence embeddings;embeddings enhanced syntactical;word context embeddings;sentence embeddings models;combining embeddings syntactical;embeddings syntactical;sentence embeddings;verb clustering role;task unsupervised semantic;verbs frame type;verb clustering approach;word2vec embedding sentence;verb clustering;verbs frame;unsupervised semantic;verb clustering using;semantic frame role;semantic frame;verbs frame specific;embeddings models word2vec;clusters grouping verbs;embedding sentence context;context embeddings;dependencies word2vec embedding;approach unsupervised semantic"}, "384bf224d91a1691c9e6384201483121e2e7ddab": {"ta_keywords": "introductionsubspace clustering;face clustering;clusters different subspaces;subspaces similarity measurements;face clustering goal;segmentation face clustering;introductionsubspace clustering celebrated;clustering;required reliable clustering;subspaces similarity;clustering goal;reliable clustering;reliable clustering unknown;clustering unknown;different subspaces similarity;motion segmentation face;clustering celebrated problem;clusters;motion segmentation;clustering celebrated;similarity measurements data;problem clusters;applications motion segmentation;similarity measurements;segmentation face;clustering goal problem;segmentation;clusters different;problem clusters different;points information theoretic", "pdf_keywords": ""}, "e01aa6f8ce625469b6f161d7ab9e61a60ac33798": {"ta_keywords": "streaming codes;streaming codes need;transmission streaming codes;setting streaming codes;streaming codes variable;streaming codes class;streaming communication;practice streaming codes;latency streaming communication;streaming communication settings;low latency streaming;latency streaming;live video streaming;streaming;transmission streaming;setting streaming;erasure codes;erasure codes specifically;video streaming;losses transmission streaming;video streaming methods;streaming methods;proposed setting streaming;service live communication;codes variable size;streaming methods practice;codes class erasure;practice streaming;live video;class erasure codes", "pdf_keywords": "latency streaming codes;quality streaming codes;streaming codes fundamental;streaming codes;streaming codes need;transmission streaming codes;latency streaming communication;streaming code;setting streaming codes;streaming communication;delay channel coding;low latency streaming;streaming codes variable;streaming codes class;coding low latency;streaming code used;practice streaming codes;latency streaming;streaming communication settings;online coding schemes;online coding scheme;low complexity streaming;online streaming;online streaming model;decoding delay lossless;offline coding scheme;high quality streaming;quality streaming;channel coding scheme;live video streaming"}, "6b7004138ee2de5ec52e500cae4e65390e961e16": {"ta_keywords": "partitioning kernel clustering;kernel clustering regularization;kernel clustering;regularization kernel cluster;regularization based segmentation;data partitioning kernel;clustering regularization;clustering regularization based;kernel cluster;partitioning kernel;segmentation;methodologies data partitioning;regularization kernel;data partitioning;based segmentation;clustering;explain regularization kernel;optimization spectral relaxation;partitioning;regularization based;formulation optimization spectral;segmentation addressing closely;methods explain regularization;optimization spectral;regularization;spectral relaxation;spectral relaxation versus;explain regularization;cluster;segmentation addressing", "pdf_keywords": ""}, "e0236106e51984e4ea6bbbd1fb5ce57abf3e4e5e": {"ta_keywords": "social distancing face;distancing face mask;mask detectingion social;distancing face;face mask detectingion;face mask;detectingion using deep;detectingion social distancing;deep learning important;social distancing;mask detectingion;deep learning;mask;mask detectingion using;using deep learning;introduction social distancing;using deep;deep learning methods;tools social distancing;distancing;face;detectingion social;learning methods covid;deep;learning;covid 19;covid;covid 19 pandemic;detectingion;coronarvirus", "pdf_keywords": ""}, "af92dd61340808f3008a84ae57803bb4aa57d03b": {"ta_keywords": "forecasting personalized avatar;dyadic residual attention;posee forecasting personalized;attention model;avatar dysadic conversations;visual posee forecasting;attention generate sequences;pose conditioned audio;residual attention model;attention generate;selective attention generate;attention model dram;residual attention;personalized avatar dysadic;attention;end visual posee;pose;body pose conditioned;sequences body pose;visual posee;pose conditioned;selective attention;personalized avatar;using selective attention;forecasting personalized;dram integrates intrapersonal;posee forecasting;body pose;interpersonal dyadic dynamics;integrates intrapersonal monadic", "pdf_keywords": "body pose conversation;pose conversation;pose conversation setting;body pose forecasting;human human interactions;predict behavior avatar;pose person specific;expressions body posture;jointly modeling interpersonal;facial interactions based;interactions human;human interactions;model human communication;conversation model;modeling interpersonal;pose person;audio body pose;body pose person;interactions human interlocutor;behaviours gestures facial;dynamics conversation model;human communication;pose forecasting;behavior avatar body;dyadic interactions human;dynamical interactions avatar;verbal behaviours gestures;communication context human;avatar human model;gestures facial expressions"}, "5b1c0152bbb12ece2a8817c727e33e6d5c503065": {"ta_keywords": "distributed learning algorithms;distributed machine learning;distributed learning;blocks distributed learning;algorithms data shuffling;data shuffling matrix;shuffling matrix multiplication;data shuffling;learning algorithms coding;shuffling matrix;algorithms coding;algorithms coding theoretic;distributed machine;distributed;machine learning algorithms;learning algorithms data;resultsin data shuffling;learning algorithms;shuffling;algorithms;machine learning;algorithms data;coding theoretic;blocks distributed;matrix multiplication resultsin;view distributed machine;coding;building blocks distributed;view distributed;matrix multiplication", "pdf_keywords": ""}, "c395595cf7be23f7d90cbca98d8c7861ebfd884d": {"ta_keywords": "disagreement deconvolution bringing;introductionthe disagreement deconvolution;disagreement deconvolution;machine learning performance;learning performance metrics;classifiers;machine learning classifiers;comment toxicity misinformation;classifiers human;learning classifiers human;methods machine learning;machine learning;classifiers human facing;learning classifiers;bringing machine learning;misinformation score highly;highly metrics roc;comment toxicity;performance metrics;toxicity misinformation score;tasks comment toxicity;metrics roc;misinformation score;metrics roc uu;learning performance;highly metrics;technical performance human;toxicity misinformation;performance human computer;technical performance", "pdf_keywords": "stable opinions annotator;annotators disagreement deconvolution;classifiers social computing;performance metrics classification;performance measures disagreement;social computing disagreement;opinions annotator;learning classification metrics;performance classifier significantly;classifiers social;disagreement deconvolution social;disagreement based performance;disagreementadjusted metrics;performance social computing;evaluating performance classifier;creating classifiers social;performance classifier tasks;stable opinions noise;opinions noise estimating;measure disagreement dataset;evaluating social computing;disagreement deconvolution useful;measures disagreement deconvolution;deconvolution social computing;noisy annotations;noisy annotations modeling;disagreement deconvolution valuable;optimal annotator classification;disagreement dataset;opinionsthe ability predict"}, "37074b2b9cebd89e4a92d20f41eec7360e11fe5a": {"ta_keywords": "speech recognition ar;automatic speech recognition;attention speech processing;speech recognition;automatic speech;autoregressive nar modeling;autoregressive models recognition;speech processing recent;non autoregressive models;autoregressive models realize;autoregressive nar;non autoregressive nar;wait completion speech;autoregressive models;speech processing;attention speech;based automatic speech;attention based automatic;compared autoregressive models;accuracy compared autoregressive;non autoregressive;autoregressive;completion speech;structure non autoregressive;background non autoregressive;compared autoregressive;state art attention;nar modeling gained;attention based;gained attention speech", "pdf_keywords": "streaming speech recognition;streaming nonar speech;nonautoregressive speech recognition;nonar speech recognition;streaming speech;end streaming speech;new speech recognition;nonarist speech recognition;speech recognition nonarist;speech recognition ar;speech processing recent;automatic speech recognition;integrating speech recognition;propose streaming speech;automatic speech;speech recognition model;speech recognition;recognition nonarist speech;speech recognition speech;speech speech processing;recognition speech;speech processing;recognition speech recognition;based automatic speech;blockwise attention encoder;speech recognition combining;attention speech processing;nonautoregressive speech;approach automatic speech;method nonautoregressive speech"}, "bd6018632a360cb567da8e50e1717ff526503845": {"ta_keywords": "stochastic beam search;beam search stochastic;poisson stochastic beam;search stochastic;stochastic beam;search stochastic process;beam search;conditional poisson sampling;poisson sampling;turning beam search;poisson sampling design;poisson stochastic;conditional poisson stochastic;beam search results;beam search methodswe;iteration sample candidates;stochastic;process conditional poisson;sampling design;sample candidates replacement;sampling;set iteration sample;iteration sample;stochastic process;conditional poisson;sample candidates;stochastic process conditional;according conditional poisson;beam;poisson", "pdf_keywords": "stochastic beam search;beam search sampling;beam search stochastic;search sampling;search stochastic beam;search sampling withoutreplacement;search stochastic;sampling methods neural;sampling strategies neural;neural sequence models;decoding algorithms sequence;decoding algorithm stochastic;algorithms sequence models;decoding algorithms;probabilities language generation;search inclusion probability;beam search;beam search comparison;modification beam search;stochastic beam;methods neural sequence;efficient sequence model;model estimate decoding;neural machine translation;turning beam search;unbiased inclusion probabilities;able estimate decoding;beam based decoding;analyzing stochastic beam;sampling set elements"}, "6494cd26511c076186673c9a636d21d1dfed8d5a": {"ta_keywords": "advanced auditory auditory;auditory auditory auditor;advanced auditory;auditory auditory auditory;advanced advanced auditory;auditory auditory;auditory auditor;auditory;distant talking advanced;talking advanced advanced;talking advanced;advances distant talking;distant talking;auditor;advanced advanced;recent advances;advances;recent advances distant;advanced;introduction recent advances;advances distant;introduction recent;talking;distant;introduction;recent", "pdf_keywords": ""}, "c8f9313ce8416a7be079935d1cbb637705f75182": {"ta_keywords": "individuality using translation;individuality spotken language;individuality writer speaker;improvements translation dictionary;transforming individuality using;translation dictionary language;transforming individuality spotken;translation dictionary;reflect individuality writer;using translation dictionary;translation dictionary method;transforming individuality;individuality writer;improvements translation;method transforming individuality;individuality using;spotken language text;using translation;language text speech;individuality spotken;introduction improvements translation;features dialogue systems;language text;dictionary language model;reflecting features dialogue;language model;dictionary language;features dialogue;spotken language;text speech various", "pdf_keywords": ""}, "8c7628641450203b0aa959b5a69729ff906760ff": {"ta_keywords": "speaker diarization eend;end neural diarization;speaker diarization;approach speaker diarization;introductionencoder decoderbased attractors;neural diarization methodsthis;neural diarization;neural diarization eend;cascaded approach speaker;diarization eend;diarization eend methods;decoderbased attractors end;decoderbased attractors;diarization methodsthis;diarization eend method;speaker overlap handling;diarization methodsthis paper;introductionencoder;end end neural;speaker overlap;introductionencoder decoderbased;end neural;diarization;approach speaker;terms speaker overlap;attractors end end;attractors end;speaker;attractors;number speakers", "pdf_keywords": "end speaker diarization;speech separation attractors;speech separation attractor;speaker diarization end;end neural diarization;diarization speech separation;speaker diarization;challenges speaker diarization;speaker diarization field;approach speaker diarization;speaker diarization method;speaker wise attractors;neural diarization methods;speech diarization;neural diarization model;speakerwise attractors;neural diarization;speakers using encoderdecoder;diarization speech;sequence speakerwise attractors;diarization speech speech;end speaker;approach speaker diarisation;speakerwise attractors calculated;end end speaker;speech separation implemented;input embeddings attractors;encoderdecoder based attractor;neural discriminate speakers;speech diarization speech"}, "5aea95e1ae78a66474051a330ded374e199b658c": {"ta_keywords": "learning graphs jumping;graphs jumping knowledge;representation learning graphs;introductionrepresentation learning graphs;jumping knowledge networks;learning graphs;learning graphs follow;graphs jumping;graph structure;graph structure analogous;graphs;nodes node representation;node representation draws;node representation;graphs follow neighborhood;jumping knowledge;graphs follow;neighboring nodes;graph;follow neighborhood aggregation;networks;knowledge networks;depends graph structure;learning approaches representation;neighborhood aggregation;networks methodsa;approaches representation learning;representation learning;neighboring nodes node;nodes", "pdf_keywords": "learning graphs jumping;graphs jumping knowledge;jumping knowledge networks;neighborhood aggregation models;representation learning graphs;follow neighborhood aggregation;simple neighborhood aggregation;neighborhood aggregation;learning graphs;aggregations layer representations;graph convolutional neural;backgroundrepresentation learning graphs;approach neighborhood aggregation;graphs training;art neighborhood aggregation;learning graphs follow;graphs training data;node representation learning;graph convolutional;graphs subgraphs diverse;graphs jumping;neighborhood aggregation procedure;subgraphs diverse local;representations particular graphs;subgraphs diverse;graph structure analogous;layer representations jump;graph clustering;graphs subgraphs;locality affected graph"}, "564dec6eab6115ecd604f22738ce0b47777f6e17": {"ta_keywords": "bayesian framework speech;clustering speech recognition;estimation clustering speech;backgroundvariational bayesian estimation;speech recognition vc;backgroundvariational bayesian;clustering speech;bayesian estimation clustering;speech recognition;framework speech recognition;total bayesian framework;vc total bayesian;speech recognition procedures;modeling speech classification;speech classification based;acoustic modeling speech;speech classification;modeling speech;bayesian framework;recognition vc total;framework speech;based vb posterior;bayesian estimation;backgroundvariational;total bayesian;recognition vc;bayesian framework generates;methodsthe total bayesian;recognition procedures acoustic;vb posterior distribution", "pdf_keywords": ""}, "8c5465eb110d0cab951ca6858a0d51ae759d2f9c": {"ta_keywords": "text classifier learns;text classifiers;classifiers make interpretable;text classifiers make;classifiers;text classifier;classifier learns;jointly training neural;neural network models;success neural networks;input text classifier;neural networks;focus text classifiers;interpretability focus text;neural networks comes;interpretability focus;training neural;short informative input;classifiers make;training neural network;informative input text;interpretability;make interpretable;neural network;classifier;informative input;neural;success neural;rationale predictions approach;problem jointly training", "pdf_keywords": "text classifiers;sentiment classification;method sentiment classification;text classification able;text classifiers make;text classification;text classification present;generating sentiment;attention natural language;sentiment classification apply;focus text classifiers;approaches text classification;generating sentiment data;attention layers;method generating sentiment;classifiers;attention layers based;tool text classification;contribute encodings classification;analyzing attention;classifiers make interpretable;learning classification;classification able extract;sentiment analysis;sentiment data;model natural language;learning classification propose;method analyzing attention;sparsely activated attention;latent regularization"}, "2660dbba723573266edb2a0a4929e6847ae83212": {"ta_keywords": "advancing rn transducer;rn transducers rnn;speaker adaptation language;transducers rnn;techniques rn transducers;speaker adaptation;transducer technology speech;changes speaker adaptation;rn transducer;rn transducers;transducers rnn ts;adaptation language model;architectural changes speaker;rn transducer technology;speech recognition;technology speech recognition;speech recognition investigate;adaptation language;language model fusion;conversational language 780;rnn ts;technology speech;hours conversational language;changes speaker;conversational language;rnn ts instrumental;rnn;language model;transducer;hours conversational italian", "pdf_keywords": "language model fusion;fusion language model;speaker adaptation language;techniques recurrent neural;recurrent neural network;speaker adaptation;neural network transducers;speaker adaptation model;speech recognition;adaptation language model;regularization speaker adaptation;fusion language;model fusion language;conversational language data;tensor memory;changes speaker adaptation;model regularization speaker;network transducers rn;loss gradient computa;memory output tensors;automatic speech;adaptation language;corpus conversational speech;recurrent neural;quality tensor memory;tensor size network;regularization speaker;corpus conversational;automatic speech recognition;fusion general training"}, "94f22d7a8b48784b3d8975616e20d8028a08162f": {"ta_keywords": "molars major component;major component molars;component molars molars;component molars;molars molars major;molars major;molars molars;molars;major component;major;component", "pdf_keywords": ""}, "d119cc4051ed1206a0dac963cd23a84acf77fea7": {"ta_keywords": "decisions threshold calibration;deployment forecasted probabilities;forecasted probabilities;rely probabilistic forecasts;probabilistic forecasts predict;probabilistic forecasts;introductionreliable decisions threshold;calibration methods decision;forecasts typically;rules deployment forecasted;perfect forecasts typically;accurate perfect forecasts;forecasts predict;probabilities calibrated;decisions threshold;forecasts;forecasted probabilities match;forecasts typically impossible;perfect forecasts;frequencies predicted losses;forecasts predict loss;threshold calibration methods;calibration typically;deployment forecasted;threshold calibration;true frequencies predicted;predicted losses accurate;average notion calibration;decision rules deployment;impossible probabilities calibrated", "pdf_keywords": "threshold decision forecasted;decision thresholds;prediction decision making;threshold calibrated forecaster;decision rules forecasted;decision threshold loss;decision rules threshold;threshold decision rules;loss threshold decisions;accurate decision loss;decision threshold;threshold decision losses;prediction decision;decision loss prediction;decision losses threshold;guarantee accurate decision;threshold decisions fully;gap decision thresholds;estimation threshold decision;guarantees accurate decision;rules threshold decision;threshold decisions;losses threshold decision;threshold decision threshold;threshold decisions whichthe;forecaster decision loss;consider threshold decision;compromising forecaster decision;threshold decision;uncalibthe decision maker"}, "6a9795853e5f39325deb0d916fe22d9e5a202a9f": {"ta_keywords": "printing milton areopagitica;printing milton;pamphlets 1640s article;pamphlets 1640s;100 pamphlets 1640s;pamphlets;article describes printing;attributes printing milton;areopagitica london printers;authors printing text;printing;authors printing;pieces 100 pamphlets;london printers matthew;1640s article;printing text;printers matthew;100 pamphlets;describes printing text;context authors printing;milton areopagitica london;describes printing;milton areopagitica;1640s article attributes;text context adolescent;printing text context;london printers;young adults article;printers matthew simmons;article attributes printing", "pdf_keywords": ""}, "5270b626feb66c8c363e93ba6608daae93c5003b": {"ta_keywords": "transformer based language;memorization generalization transformers;generalization transformers;generalization transformers widely;transformers forget specific;language models;introductionlarge transformer models;transformer models;transformer models achieved;natural language tasks;make transformers forget;facts memorize new;improving memorization generalization;transformers forget;based language models;facts memorize;language models shown;transformer;introductionlarge transformer;make transformers;improving memorization;transformers;memorization generalization;old facts memorize;transformers widely;memorize new ones;known make transformers;language tasks;performance natural language;transformer based", "pdf_keywords": "transformer based language;language models multitude;manually manually manually;language model;ability manually manually;manually manually manuallythe;manually manually;word language model;language models serve;language model trained;manually manuallythe ability;language model method;language models;language models like;based language models;ability language models;manually manuallythe;manuallythe;manually;issue neural systems;transformers results better;domain language model;transformers results;manuallythe ability;results language generation;language generation;ability manually;success transformer based;success transformer;ability language"}, "7b99c51d562e33309a46601c846abbe72a65c6a4": {"ta_keywords": "intermediate transfer learning;transfer gains nonlinguistic;gains nonlinguistic tasks;transfer learning;transfer learning methods;tasks intermediate transfer;nonlinguistic tasks;trained language models;pre trained language;gains nonlinguistic;trained language;nonlinguistic tasks abundance;intermediate transfer;language models infease;task fine tuning;culminate large transfer;tasks intermediate;best transfer setting;language models;combinations best transfer;best transfer;transfer;learning;beneficial tasks intermediate;datasets pre trained;large transfer gains;transfer gains;large transfer;intermediate task;pre trained", "pdf_keywords": "task learning highly;intermediate task trained;transfer learning considerably;intermediate tasks trained;trained intermediate tasks;transfer learning;models intermediate task;trained language models;intermediate task selection;task learning;transfer learning non;multi task learning;task trained;based transfer learning;tasks trained;selection intermediate tasks;pre trained language;intermediate tasks berta;intermediate tasks easily;task trained trained;language models intermediate;tasks trained target;optimize intermediate tasks;selecting intermediate tasks;trained language;trained target task;best intermediate tasks;intermediate task optimal;tasks best intermediate;intermediate tasks best"}, "c47c8c2527bf2ca8339c342f44db2218a0cbcbbd": {"ta_keywords": "extraction knowledge base;information extraction knowledge;knowledge base construction;constructing knowledge bases;deriving knowledge text;extraction knowledge;information extraction;desired knowledge base;knowledge bases sources;knowledge base;knowledge bases;knowledge text;introduction information extraction;semantics using ontological;using ontological constraints;problem constructing knowledge;constructing knowledge;using ontological;candidate extractions;candidate extractions solve;ontological constraints candidate;ontological constraints;deriving knowledge;challenge deriving knowledge;ontological;semantics using;knowledge text key;millions candidate extractions;problem turn semantics;facts eliminate errors", "pdf_keywords": ""}, "4eb22b488052c430170139c492674aa05512f7bf": {"ta_keywords": "die shape optimization;shape optimization design;shape optimization;forging shape;optimization design carried;controllingling forging shape;forging shape deforming;design controllingling forging;optimization design controllingling;optimization design;preform die shape;perform optimization procedure;perform optimization;optimization procedure;deforming force forging;optimization;optimization procedure res;shape deforming force;used perform optimization;controllingling forging;forging process;force forging process;force forging;shape deforming;die shape;material energy forging;based fluorodeoxyglucose fem;objective preform die;energy forging process;forging", "pdf_keywords": ""}, "f394c5101d7bfc3d8055f9391a83f7e2395dec4a": {"ta_keywords": "parallelization sequenceential programs;backgroundpredicting parallelization sequenceential;sequential programs parallelized;parallelization sequenceential;programs parallelized;programs parallelized using;sequenceential programs using;sequenceential programs;programs using supervised;annotated openmp parallelization;backgroundpredicting parallelization;parallelization;parallelized using;parallel benchmark;parallelized using dynamic;parallelized;parallelization produced;code collected runtime;parallel benchmark ncb;parallelization directives;sequential programs;nas parallel benchmark;openmp parallelization;openmp parallelization directives;regions sequential programs;benchmark ncb code;parallelization directives order;collected runtime;approximate parallelization;approximate parallelization produced", "pdf_keywords": ""}, "f41e6c832c9e0d5360b66ee7681d3b1ffd2d9c3d": {"ta_keywords": "hierarchical task learning;task learning sub;task learning language;hierarchical task;task learning;introduction hierarchical task;sub goal planning;decomposed task learning;goal planning scene;planning scene navigation;learning language instructions;architecture decomposed task;goal planning;introduction hierarchical;planning scene;learning sub problems;language instructions unified;hierarchical;paper hierarchical task;planning;learning language;decomposed task;task;learning sub;object manipulation;scene navigation;learning;navigation object manipulation;instructions unified transformers;sub goal", "pdf_keywords": "hierarchical tasks unified;hierarchical tasks;hierarchical task structure;hierarchical task modeling;hierarchical task learning;hierarchical task networks;hierarchical task;hierarchical task model;demonstrate hierarchical task;present hierarchical task;developed hierarchical task;task learning language;planning hierarchical;structure hierarchical task;stands hierarchical tasks;task learning awolfred;modeling planning hierarchical;learning decomposed task;form hierarchical task;tasks unified;task networks;planning hierarchical structure;task learning;look task learning;task structure;task learning decomposed;task networks widely;benchmark task learning;task modeling;task decomposition"}, "c54ad6e29f3e516eecf0a72bd1f95b80e8617116": {"ta_keywords": "compressive phase retrieval;complexity compressive phase;compressive phase;general compressive phase;phase uncertainty sparse;phase retrieval problem;phase retrieval methods;phase retrieval;low complexity compressive;complexity compressive;phasecode low complexity;approaching phasecode;uncertainty sparse complex;approaching phasecode low;capacity approaching phasecode;tackle general compressive;general compressive;sparse complex vector;phasecode;sparse complex;compressive;phase uncertainty;recover global phase;phasecode low;global phase uncertainty;uncertainty sparse;sparse;phase;linear measurements ax;global phase", "pdf_keywords": "compressive phase retrieval;phasecode algorithm compressive;algorithm compressive phase;efficient compressive phase;approach compressive phase;compressive phase;phase retrieval scheme;propose phase retrieval;general compressive phase;phase retrieval signal;method compressive phase;phase uncertainty sparse;based phase retrieval;asymmetric phase retrieval;phasecode algorithm recover;scheme compressive phase;phase retrieval method;phase retrieval;problem phase retrieval;phase retrieval problem;phase retrieval single;phasecode algorithm;algorithm irregular phasecode;phasecode algorithm introduced;layer phasecode algorithm;phase retrieval inner;algorithm compressive;compressive sensing;efficient compressive;irregular phasecode"}, "044b502e5a00b5eeff1dd078ea03f491ca2c37bf": {"ta_keywords": "recognizers structural classification;speech recognition;automatic speech recognition;structural classification;speech recognition ar;recognizers structural;structural classification approaches;structural classification methods;methods automatic speech;modeling acoustic linguistic;automatic speech;potential structural classification;linguistic aspects recognizers;decoding;recognizers;unified modeling acoustic;acoustic linguistic;decoding technique;synchronous pass decoding;pass decoding technique;computational efficiency decoders;aspects recognizers structural;decoders;acoustic linguistic aspects;efficiency decoders;pass decoding;recognition;classification methods automatic;aspects recognizers;modeling acoustic", "pdf_keywords": ""}, "b9057dce43181a30aa3e0435c8ffc4c0b6f8f127": {"ta_keywords": "generate inference graphs;argumentation supporting inference;inference graphs humans;automatically generate inference;supporting inference graphs;generate inference;inference graphs useful;reasoning constructing;inference graphs;humans inference graphs;inference graphs transfer;useful reasoning constructing;handcraft argumentation;handcraft argumentation supporting;reasoning mode reasoning;defeasible reasoning;supporting inference;literature handcraft argumentation;graphs humans inference;introduction defeasible reasoning;defeasible reasoning mode;humans inference;inference;graphs useful reasoning;logic literature handcraft;reasoning mode;cognitive science logic;reasoning conclusions overturned;logic literature;reasoning conclusions", "pdf_keywords": "inference graphs humans;generate inference graphs;humans inference graphs;supporting inference graphs;inference graphs useful;automatically generate inference;graphs humans inference;inference graphs defeasible;graphs defeasible inference;inference graphs;generate inference;inference graph;graphs useful reasoning;looked inference graphs;humans defeasible reasoning;defeasible reasoning humans;humans using inference;useful reasoning constructing;inference graphs transfer;inference graph helped;using inference graphs;reasoning important tool;semantically meaningful graphs;graphs semantically meaningful;generation defeasible reasoning;reasoning constructing;defeasible inference machine;defeasible reasoning model;defeasible inference;inference machine generated"}, "b143ee344fe3af4169bde8af8b682a2835dae4a4": {"ta_keywords": "multi agent collaboration;agent collaboration;agents learn collaborate;agent collaboration research;task furnmove agents;multi agent;centralized agent;new centralized agent;agents work;agents learn;centralized agent time;single agent;introductionautonomous agents learn;agents;agent time task;collaborate scalable;abilities multi agent;learn collaborate scalable;agent;agent abilities multi;outpaces single agent;collaborate;gridworld like environments;single agent abilities;agent abilities;agent time;introductionautonomous agents;novel task furnmove;collaborate scalable develop;collaboration", "pdf_keywords": "agent collaborative task;multi agent collaboration;multi agent embodied;agent collaboration;agent collaborative;multi agent collaborative;multi agent task;agents learn collaborate;agent embodied task;visual multi agent;collaborative task embodied;actions multi agent;multi agent action;coordination agents;embodied visual agents;task furnmove agents;agent task;visual agents multi;multi agent;close coordination agents;agents object communication;multi agent interactions;coordination agents develop;agent embodied;multi agent training;visual agents;expressive joint action;agents coordinate actions;multi agent communication;agents able communicate"}, "c3930cb34241a42e03ed02cbc83a3c87dddd60cc": {"ta_keywords": "automatically generating stories;story continuation systems;generating stories;makes story continuation;continuation story methodswe;outputs story continuation;generating stories focus;story continuation;sentence continuation story;story continuation interesting;generate sentence continuation;story methodswe seek;story methodswe;quality crowdsource annotations;continuation story;setting sentences story;quality automatically generating;overall quality crowdsource;outputs story;criteria outputs story;quality crowdsource;sentence continuation;generate sentence;task generate sentence;crowdsource annotations criteria;story provided task;sentences story;automatically generating;crowdsource annotations;stories focus", "pdf_keywords": ""}, "c9d7b1f9b13d6ea4ff45b908285cc65af959cc5b": {"ta_keywords": "probabilities word language;probability word language;assigns probability word;assigning probability word;word probabilities;grammar resultsa probability;define word probabilities;probability word basis;word probabilities word;probabilities word;probability word;method assigns probability;assigning probability;assigns probability;probability associated;word language considered;problem assigning probability;language considered methods;probabilities;languages generated grammar;word language;languages generated;language second method;probability;language considered;generated grammar resultsa;applied languages generated;method applied languages;generated grammar;probability associated production", "pdf_keywords": ""}, "3050735eb35af3527276aa1952f79eb2483df3f0": {"ta_keywords": "resource tagging utterance;tagging utterance level;tagging utterance;speaker intentions emotions;speakers intentions emotions;task conversational understanding;conversational understanding;conversational understanding task;analysis speaker intentions;context contextalized representations;objective conversational understanding;utterance level analysis;core task conversational;conversational understanding depending;context contextalized;contextalized;intentions emotions core;task conversational;contextalized representations;objective conversational;contextalized representations low;low resource tagging;aspects speakers intentions;dialog act affect;intentions emotions;categorical dialog act;speaker intentions;resource tagging;conversational;emotions core task", "pdf_keywords": ""}, "a556914c1b32372d47a36f2826cbe143ddae95ca": {"ta_keywords": "supervised taxonomy expansion;existing taxonomy expansion;taxonomy expansion;taxonomy expansion model;study taxonomy expansion;expand existing taxonomies;supervision existing taxonomy;taxonomies new concept;taxonomy expansion problem;self supervised taxonomy;existing taxonomies;new concept terms;taxonomies new;existing taxonomies new;concept terms propose;existing taxonomy;supervised taxonomy;taxonomies;concept terms;taxonomy;ontologies underpin numerous;knowledge ontologies;taxonomies used;ontologies;knowledge ontologies underpin;propose self supervised;important knowledge ontologies;ontologies underpin;daily basis taxonomies;taxonomies used practice", "pdf_keywords": ""}, "d85c0032d7bb0bd220eb2df8ba6d2130bc87e79e": {"ta_keywords": "semi supervised training;data joint speech;semi supervised;end neural diarization;present semi supervised;diarization eend;neural diarization eend;pseudo labeling end;neural diarization;speech wellt tuned;supervised training technique;overlapping speech wellt;overlapping speech;joint speech activities;pseudo labeling;diarization eend methodsthe;supervised training;joint speech;speech wellt;using pseudo labeling;speech activities speaker;labeling end;supervised;diarization;speech activities;labeling end end;activities speaker;labeled data;end end neural;end neural", "pdf_keywords": "semi supervised training;diarization training;neural speaker diarization;diarization training using;pseudo labeling end;semi supervised;speaker diarization;auditory diarization training;method speaker diarization;training using unlabeled;speaker diarization method;propose semi supervised;present semi supervised;pseudo labeling;end neural diarization;training method speaker;neural diarization based;clustering neural diarization;data neural diarization;speaker diarization demonstrate;iterative pseudo label;pseudo label based;using pseudo labeling;diarization requires labeled;labeling end;supervised training technique;training using pseudo;unlabeled data neural;label based training;pseudo label alternately"}, "571b4425498549c56c0828a824dc453ff6f482fc": {"ta_keywords": "mac protocols mind;mac protocols;design mac protocols;iot delay sensitive;iot delay;access control protocols;things iot delay;medium access control;hoc wireless networks;control protocols slotted;hoc wireless;contention access light;ad hoc wireless;protocols slotted framework;protocols slotted;iot;consider medium access;control protocols;protocols mind need;medium access;protocols;things iot;access light traffic;protocols mind;developing medium access;scheduled access heavy;contention access;internet things iot;need contention access;wireless networks", "pdf_keywords": ""}, "0823f2187eeed53be8fd452decf6ed9a6a6cd124": {"ta_keywords": "semantic parser;semantic parsing corpus;semantic parsing;semantic parser main;referred semantic parser;parsing corpus;parser;creation semantic parsing;spacebook referred semantic;language understandingcomponent spacebook;parsing;natural language understandingcomponent;parser main document;natural language;parser main;semantic;prototype natural language;language understandingcomponent;corpus;document creation semantic;referred semantic;creation semantic;understandingcomponent spacebook referred;understandingcomponent spacebook;document describes;functional domains provide;overview development functional;functional;spacebook referred;functional functional domains", "pdf_keywords": ""}, "5de24203bf98ae7f4c514bc0bd2a310caa47a047": {"ta_keywords": "train coordination;train coordination grid;efefficient train coordination;vehicle scheduling;solve vehicle scheduling;competition 2020 mapf;methodsthe flatland competition;flatland competition 2020;vehicle scheduling problem;coordination grid world;scheduling problem vrp;scheduling;flatland competition aimed;flatland competition;backgroundthe flatland competition;marl efefficient train;efefficient train;scheduling problem;coordination grid;approaches solve vehicle;train;2020 mapf;solve vehicle;competition aimed finding;coordination;competition 2020;world methodsthe flatland;competition;grid world;vrp resultsthe flight", "pdf_keywords": "virtual vehicle scheduling;vehicle scheduling;scheduling vehicles;scheduling vehicles disruptions;solve vehicle scheduling;vehicle scheduling problem;networks scheduling vehicles;solving virtual rescheduling;scheduling problem virtual;flatland competition implemented;scheduling trips traffic;scheduling problem edition;virtual rescheduling;virtual rescheduling problem;scheduling problem;scheduling;scheduling problem vrp;flatland competition methods;traffic networks scheduling;problem concerned scheduling;2020 flatland competition;flatland competition 2020;situations flatland competition;vrp virtual rescheduling;virtual vehicle;flatland competition use;rescheduling problem concerned;approaches virtual vehicle;trips traffic networks;rescheduling problem various"}, "254d1b8cf247ae8b19e017f7ba758d670207ddda": {"ta_keywords": "discriminative beamforming phase;dis discriminative beamforming;discriminative beamforming;networks speech enhancement;beamforming phase aware;neural networks speech;speech enhancement recognition;speech enhancement;speech recognition;predicts optimal beamforming;automatic speech recognition;beamforming phase;optimal beamforming;beamforming;networks speech;automatic speech;function automatic speech;speech recognition methods;phase aware neural;optimal beamforming parameters;beamforming parameters;recognition objectives network;beamforming parameters frequency;phase aware;enhancement recognition objectives;enhancement recognition;neural networks trained;neural networks;array signals predicts;networks trained", "pdf_keywords": ""}, "1f5a1e959147e989e12846a5bd1d20234ef667d7": {"ta_keywords": "anticoagulants management bleeding;oral anticoagulants methods;direct oral anticoagulants;oral anticoagulants;anticoagulants management;oral anticoagulants debated;concentration anticoagulants management;plasma concentration anticoagulants;oral anticoagulants complex;anticoagulants methods performed;anticoagulants methods;anticoagulants;concentration anticoagulants;management bleeding patients;management severe bleeding;anticoagulants complex use;anticoagulants debated;management bleeding;anticoagulants debated aim;bleeding events patients;bleeding patients treated;outcomes severe bleeding;severe bleeding events;severe bleeding patients;anticoagulants complex;bleeding patients;bleeding events;severe bleeding;prothrombin;prothrombin complex concentrates", "pdf_keywords": ""}, "148f055083666c72945eea79833a19494f5f57c0": {"ta_keywords": "synonymic dilations overlooked;synonymic dilations;sparsely synonymic dilations;patient solitary asymptomatic;solitary asymptomatic asymptomatic;solitary asymptomatic;asymptomatic solitary asymptomatic;dilations overlooked case;asymptomatic patient solitary;asymptomatic solitary;asymptomatic asymptomatic solitary;asymptomatic;asymptomatic asymptomatic;dilations overlooked;asymptomatic asymptomatic asymptomatic;dilations;asymptomatic patient;asymptomatic asymptomatic patient;patient solitary;case patient solitary;solitary;synonymic;sparsely synonymic;case patient;overlooked case patient;patient;sparsely;overlooked case;case;overlooked", "pdf_keywords": ""}, "924ce584acc148be29ef905c228fda7fe552c0c2": {"ta_keywords": "probabilistic logics answering;large knowledge bases;reasoning large knowledge;large knowledge base;probabilistic logics;probabilistic logics reasoning;logics reasoning large;knowledge bases;challenge probabilistic logics;logics answering queries;knowledge bases kbs;shared probabilistic logics;learning large knowledge;knowledge base;inference learning large;logics answering;large knowledge;reasoning large;grounding mapping propositional;knowledge base important;queries involves grounding;inference learning;propositional representation size;answering queries involves;introductionefefficient inference learning;answering queries;logics reasoning;learning large;mapping propositional representation;inference", "pdf_keywords": "probabilistic reasoning maintaining;prolog efficient tool;underlying database prolog;order probabilistic language;probabilistic order language;prolog order probabilistic;probabilistic language;stochastic logic programs;approximate pagerank;large knowledge base;pagerank process proof;probabilistic language suited;pagerank graph propr;prolog efficient;probabilistic language based;graph personalized pagerank;allow probabilistic reasoning;approximate pagerank nibble;database prolog;personalized pagerank graph;computing personalized pagerank;personalized pagerank;personalized pagerank process;probabilistic reasoning;tasks approximate pagerank;probabilistic language uses;pagerank nibble prove;pagerank;related personalized pagerank;pagerank graph"}, "8c4d1e81c277f71cd9e3c9a0af356203c7948dca": {"ta_keywords": "documentation automatic speech;automatic speech recognition;endangered language documentation;speech recognition ar;automatic speech;speech recognition;effective human transcribers;transcriber shortage;challenges endangered language;human transcribers transcriber;transcribers transcriber shortage;transcribers transcriber;human transcribers;transcribers;transcriber;endangered language;transcriber shortage main;language documentation automatic;hidden markov;unlike hidden markov;markov model anr;hidden markov model;documentation automatic;speech;language documentation;recognition ar suggested;recognition ar;effectiveness documentation end;documentation end;model anr", "pdf_keywords": "endangered language corpus;transcription challenge native;transcription inthe corpus;errors novice transcription;documentation automatic speech;novice transcription challenge;accuracy novice transcription;novice transcription;novice transcription inthe;end speech processing;novice transcription depth;novice transcription article;combine novice transcription;automated transcription;automated automated transcription;increasingly extensive lingual;transcription novice;language corpus yoloxo;novice transcription anar;developing speech recognizers;novice transcription combine;speech recognizers;alignment novice transcription;speech processing toolkit;speech recognizers endangered;novice transcription apply;novice transcription novice;automated transcription hm;development automated speech;rules novice transcription"}, "c5ed3d1a2ce418610a6fc9b5520a4f845279969a": {"ta_keywords": "prediction serving systems;models prediction serving;prediction serving;erasure coded resilience;parity models new;parity models;introduce parity models;prone slowdowns failures;coded resilience;machines cluster;parity;machine learning models;deploy models prediction;serving systems queries;models prediction;methods introduce parity;inference models prediction;slowdowns failures;tail latency methods;cluster settings prone;introduce parity;run machines cluster;cluster;approach enabling erasure;return predictions;machines cluster settings;tail latency;enabling erasure;enabling erasure coded;return predictions performing", "pdf_keywords": ""}, "657329c633709dd1ac34a30d57341b186b1a47c2": {"ta_keywords": "sparse attention patterns;dynamic sparse attention;sparse attention;attention patterns;attention patterns avoid;summaryself attention;sequence modeling;summaryself attention recently;attention;sequence modeling problems;learn dynamic sparse;self attention;memory attend content;self attention suffers;effectiveness self attention;attention recently;computation memory attend;attention suffers quadratic;range sequence modeling;attention suffers;memory attend;wide range sequence;dynamic sparse;sequence length;sequence length successful;patterns avoid allocating;sequence;memory;learn dynamic;patterns avoid", "pdf_keywords": "sparse recurrent attention;attention sparse;learn sparse attention;sparse attention model;sparse attention matrices;attention long sequences;sparse attention application;sparse attention patterns;propose sparse attention;sparse recurrent;dynamic sparse attention;approach sparse attention;sparse attention;approaches sparse attention;variant sparse attention;routeing sparse attention;based sparse attention;attention sparse attention;approach sparse recurrent;sparse attention sufficient;sparse attention fundamental;sparse attention optimal;attention model optimization;sequences local attention;natively sparse attention;sparse attention local;product attention sparse;complexity sparse attention;sparse attention better;sparse attention various"}, "ba4a34680e09e77984624c95f5245d91b54373f6": {"ta_keywords": "multilingual models benchmark;benchmarks limited english;multilingual models;increasing multilingual models;languages tasks;learning models non;models non coding;range languages tasks;domains driven benchmarks;models benchmark;multilingual;non coding;non coding domains;models benchmark enables;benchmarks evaluate models;diverse range languages;languages;machine learning models;languages tasks missing;range languages;increasing multilingual;coding domains driven;learning models;benchmark enables comprehensive;benchmarks limited;benchmark enables;benchmark;driven benchmarks;benchmarks;coding domains", "pdf_keywords": "generalization crosslingual benchmark;crosslingual generalization tasks;multilingual models benchmark;cross lingual retrieval;benchmark evaluating crosslingual;benchmark cross lingual;cross lingual models;evaluating crosslingual generalization;lingual transfer learning;generalization cross lingual;crosslingual generalization use;crosslingual benchmark evaluating;crosslingual benchmark;cross lingual generalization;cross lingual benchmark;perform crosslingual generalization;lingual transfer models;crosslingual generalization;cross language generalization;analyses cross lingual;lingual generalization cross;multilingual language models;evaluation cross lingual;multilingual representation learning;generalization crosslingual;useful multilingual translation;cross lingual text;multilingual models;evaluating cross lingual;lingual retrieval tasks"}, "927ff874d3ed9307356d256c31b79a0624b3c9d5": {"ta_keywords": "speech recognition chime;multi microphone conversational;jah multi microphone;microphone multi;speech diarization recognition;multi microphone multi;multi microphone;microphone multi speaker;distant multi microphone;speech recognition;microphone conversational speech;recognition chime challenge;microphone conversational;conversational speech diarization;speech diarization;automatic speech recognition;speaker automatic speech;multi speaker automatic;recognition chime;automatic speech;multi speaker;microphone;chime challenge distant;tracks chime challenge;diarization recognition everyday;chime challenge described;chime challenge;diarization recognition;challenge distant multi;tracks chime", "pdf_keywords": "speech activity detection;activity detection speech;fusion speech activity;activity speech diarization;speech separation recognition;detection measure speech;diarization speech activity;measure speech activity;detection speech;workshop speech processing;speech activity speech;chime speech separation;diarization speech recognition;speech speech processing;automatic speechthe dinner;evaluation speech activity;speech recognition everyday;combination automatic speechthe;speech activity;activity speech;speech processing;fusion speech;multispeaker automatic speech;speech processing everyday;automated speech;enhancement diarization speech;detection speech decoding;speech diarization;speech diarization speech;automatic speechthe"}, "c4efaeccd7f0d900b1df95dadf51bad74264f613": {"ta_keywords": "randomized rules assignment;randomized rules;computational complexity agent;prominent randomized rules;complexity agent manipulating;probabilistic serial ps;rules assignment problem;probabilistic serial;complexity agent;rule prominent randomized;assignment problem known;strategising ps rule;serial ps rule;introductionthe probabilistic serial;rules assignment;probabilistic;agent manipulating ps;computational complexity;manipulative behaviour agents;manipulative behaviour;agent manipulating;assignment problem;known superior fairness;fairness welfare properties;complexity;introductionthe probabilistic;study computational complexity;superior fairness;ps rule prominent;randomized", "pdf_keywords": "probabilityistic assignment objects;random assignment algorithm;allocation agents probabilityistic;optimal allocation agents;preserving allocations agents;agent allocation;agent allocation based;allocations agents;assignment problem probabilityistic;assignment algorithm;allocations agents work;approach random assignment;assignment algorithm consider;random assignment problem;allocation agents;allocation agent;optimal allocation;agents houses optimal;assignment objects;optimal optimal allocation;randomized rules assignment;probabilityistic assignment;outcome agent allocation;rule random assignment;allocation houses agents;preserve allocation agent;assignment objects fundamental;random assignment;allocation agent agent;equal probabilityistic assignment"}, "605bae6c397e4829dde7ff7b8ddb84782ec6e607": {"ta_keywords": "map influenza virus;influenza virus life;influenza virus;map influenza;influenza viruses;influenza viral;influenza viral infection;comprehensive map influenza;influenza viruses annual;defend influenza viral;influenza;caused influenza viruses;defend influenza;life cycle flumap;disease caused influenza;better defend influenza;caused influenza;viruses annual epidemics;cycle flumap;cycle flumap undertaking;virus life cycle;flumap;flumap undertaking;annual epidemics;backgroundinfluenza common infectious;annual epidemics cause;infectious disease;viruses annual;viral infection;epidemics", "pdf_keywords": ""}, "a18b49fae647ae08711c2384611b3537485e8408": {"ta_keywords": "automatic speech translation;speech translation systems;translation systems;translation systems translate;speech translation;translate real time;automatic speech;translation studies simultaneous;work automatic speech;translation studies;translations;studies simultaneous interpreters;translations small chunks;systems translate;simultaneous interpreters perform;simultaneous interpreters;including dividing translations;simultaneous interpreter;dividing translations;field translation studies;version simultaneous interpreter;simultaneous interpreter noticed;translations small;interpreters perform;interpreters;dividing translations small;systems translate real;interpreter;understand real time;translation", "pdf_keywords": ""}, "417259d40d0d8b3ca7ebdcf811aa9f7814d5c0c5": {"ta_keywords": "model parameters saxophone;saxophone presented methods;parameters saxophone;parameters saxophone presented;estimating tone hole;saxophone;estimating pitch applied;saxophone presented;estimating pitch;estimating tone;merely estimating pitch;jointly estimating tone;pitch applied fingering;reed model parameters;tone hole configuration;produce different pitches;fingering estimated solely;estimated solely spectral;fingering estimated;reed model;overblowing fingering estimated;hole configuration reed;configuration reed model;different pitches bugling;pitch applied;different pitches;tone hole;pitches bugling overblowing;pitches bugling;pitches", "pdf_keywords": ""}, "4302e981e3ec118b68e0b3fcf1820b3f6ecfa988": {"ta_keywords": "argumentation quality viewed;argumentation quality;argumentation theory practical;argumentation theory;differently argumentation theory;viewed differently argumentation;arguments practice;differently argumentation;argumentation;arguments practice correlate;arguments;comparisons arguments practice;comparisons arguments;empirically observations quality;relative comparisons arguments;theory practical assessment;quality phrased spontaneously;observations quality phrased;quality ratings;quality phrased;fact adequately represented;adequately represented theory;quality ratings based;assessment approaches;assessment;practical assessment;practical assessment approaches;assessment approaches paper;ratings based theory;clarify views learn", "pdf_keywords": ""}, "15251fa3a3bcf695bf153d0856886cab9a3145ea": {"ta_keywords": "text summarization;text summarization researchers;text summarization summaries;summarization researchers;problem text summarization;summarization researchers areas;summarization;view text summarization;summarization summaries;techniques reranking;techniques reranking stacking;reranking stacking approach;refer techniques reranking;reranking stacking;reranking;summaries;investigate problem text;text;stacking approach;unified view text;stacking approach problem;introductionalthough recent;commonly refer techniques;introductionalthough;stacking;complementarity;introductionalthough recent works;view text;frameworkrefactor provides unified;present new frameworkrefactor", "pdf_keywords": "neural text summarization;summarization stage learning;text summarization stage;summarization stage;effective summarization data;summarization systems apart;formulating text summarization;performance text summarization;formulate summarization stage;summarization systems;summary selecting sentences;summarization data large;summarization summaries combination;summarization systems formulating;effective summarization;summarization provides unified;summarization data;summarization using base;text summarization;view text summarization;text summarization summaries;text summarization provides;method text summarization;summarization provides;text summarization systems;method effective summarization;formulate summarization;text summarization using;new approach summarizing;summarization summaries"}, "f9e3b7c6ca7d534694148bd0c7c37c1ef896a784": {"ta_keywords": "automatic speech recognition;automatic speech;speech recognition ar;speech recognition;end automatic speech;sequence acoustic features;acoustic features direct;sequence sequence acoustic;sequence acoustic;acoustic features;recognition ar systems;recognition ar;word label sequence;speech;estimation character word;direct estimation character;features direct optimization;character word label;recognition;enables direct estimation;label sequence sequence;end automatic;acoustic;label sequence;estimation character;end end automatic;direct estimation;challenging task multi;word label;expressive power end", "pdf_keywords": ""}, "400e083a18ab94bbf45b0820693fb5035684dd7c": {"ta_keywords": "utterances goal computer;sentence algorithm used;meaning recognition spoken;spoken utterances goal;sentence algorithm;utterances goal;given sentence algorithm;recognition spoken utterances;spoken utterances;utterances;interaction natural language;natural language;recognition spoken;meaning recognition;natural language article;computer algorithm capable;sentence methodsthis article;sentence methodsthis;problem meaning recognition;algorithm capable;given sentence methodsthis;algorithm capable construct;article computer algorithm;goal computer algorithm;human computer interaction;computer algorithm;meaning description given;description given sentence;meaning description;language article experiments", "pdf_keywords": ""}, "71a85e735a3686bef8cce3725ae5ba82e2cabb1b": {"ta_keywords": "returned underspecified pipelines;underspecified pipelines;pipeline underspecified;underspecified pipelines treated;pipeline underspecified return;underspecified return predictors;methodsan pipeline underspecified;resultspredictors returned underspecified;performance predictors behave;training domain resultspredictors;failures methodsan pipeline;training domain performance;methodsan pipeline;pipelines treated;pipelines;underspecified return;returned underspecified;pipeline;underspecified;domain performance predictors;pipelines treated equivalent;performance predictors;domain resultspredictors returned;performance training domain;domain resultspredictors;identify underspecification;underspecification;underspecification key reason;predictors equivalently strong;predictors behave", "pdf_keywords": "underspecified predictors pipeline;model training underspecification;deep learning pipelines;underspecification machine learning;algorithm underspecified predictors;practical ml pipelines;ml pipelines poorly;underspecified predictors;pretraining underspecification supervised;learning pipelines;training domain underspecification;underspecification supervised language;underspecification performance predictors;modeling pipeline underspecified;pipeline underspecified practically;ml pipelines;underspecification supervised;predictors underspecification;predictors underspecification direct;ms based deep;predictors pipeline;optimal predictors underspecification;models demonstrate underspecification;underspecified extent trained;predictors underspecification present;deep learning predictors;models underspecified;model sensitive underspecification;underspecified pipelinein;models encode generalizable"}, "dd961bb9e2a70f3819a13b13402fe585ae384226": {"ta_keywords": "indivisible goods agents;fairness welfare properties;probabilistic serial;probabilistic serial ps;nash deviations;methodsfirstly nash deviations;assigning indivisible goods;serial ps rule;nash deviations ps;introductionthe probabilistic serial;randomized rule;indivisible goods;ps methodsfirstly nash;randomized rule assigning;fairness welfare;prominent randomized rule;good fairness welfare;known good fairness;ps rule cycle;methodsfirstly nash;goods agents;rule cycle;probabilistic;rule prominent randomized;welfare properties strategyproof;goods agents known;rule assigning indivisible;introductionthe probabilistic;fairness;equilibria ps methodsfirstly", "pdf_keywords": "nash equilibrium utilities;generalization probabilistic;generalization probabilistic serial;pure nash equilibrium;rule generalization probabilistic;game optimal behavior;serial rule generalization;nash equilibrium guaranteed;nash equilibrium;choice generalization probabilistic;manner nash equilibrium;generalization rule;methodsfirstly nash deviations;generalization generalization rule;understood optimal behavior;equilibrium understood optimal;cyclesthe generalization generalization;probabilistic serial mechanism;dependent manner nash;optimal behavior;indivisible goods agents;game optimal;nash deviations;majority equilibria equilibria;social choice generalization;randomized social choice;generalization rule problem;generalization generalization;probabilityistic serial rule;generalization"}, "86d55c5a098689438ceb1d52bdd768da3b47f55f": {"ta_keywords": "dynamic sensor subset;dynamic dynamic sensor;dynamic sensor activation;dynamic sensor;problem dynamic sensor;sensor subset selection;tracking time varying;active sensors fidelity;sensor activation tracking;backgroundoptimal dynamic dynamic;sensor subset;time varying stocochastic;selection tracking time;sensors fidelity;subset selection tracking;sensors fidelity increases;backgroundoptimal dynamic;sensors problem minimizing;active sensors;varying stocochastic process;dynamic dynamic;selection tracking;time varying process;sensor activation;active sensors problem;activation tracking time;dynamic;sensor;tracking time;varying stocochastic", "pdf_keywords": "decentralized tracking markov;distributed kalman filtering;filtering distributed kalman;kalman filtering distributed;kalman consensus filtering;stochastic sensor selection;distributed kalman filter;based kalman consensus;based distributed kalman;stochastic sensor querying;distributed kalman;stochastic approximation decentralized;stochastic sensor;selection decentralized tracking;method stochastic sensor;tracking markov chain;kalman consensus filter;methods kalman consensus;tracking markov;kalman consensus;decentralized tracking;consensus filtering;consensus sensor nodes;kalman filtering;dynamic sensor subset;distributed tracking;consensus sensor;algorithm consensus sensor;sampling stochastic approximation;based stochastic approximation"}, "2c0ebf5479db7f76c1e15512676c16b9032343fb": {"ta_keywords": "management malignant diseases;malignant diseases world;etiology malignant diseases;malignant diseases;management malignant;mechanism etiology malignant;etiology malignant;malignant diseases poorly;strategy management malignant;malignant;diseases world;diseases;diseases poorly understood;diseases poorly;etiology;mechanism etiology;development new strategy;strategy;strategy management;management;new strategy management;new strategy;mechanism;world;development;understood development;understood development new;poorly understood;development new;poorly understood development", "pdf_keywords": ""}, "0d360a1256ccdfca58cf98d12243df8407fd442d": {"ta_keywords": "users download weights;weights models pre;untrusted pre trained;download weights models;downloading untrusted pre;pre trained weights;weights pose security;pre trained models;download weights;models pre trained;downloading untrusted;trained models users;weights models;question downloading untrusted;trained weights;models users download;untrusted pre;trained models;pose security threat;pre trained;weights;models pre;security threat;large pre trained;security threat methods;security;pre trained large;trained large datasets;fine tune weights;untrusted", "pdf_keywords": "pre trained models;untrusted pre trained;models argue attacker;trained models;models pre trained;poisoned pre trained;weights models pre;attacker risk poisoning;agnostic meta learning;poisoned model far;spam weight poisoning;poisoning attacks;attack poison model;weights models;trained models users;download weights models;meta learning;poisoning fine tuning;threat methods;poisoning loss function;pre trained weights;security threat methods;non poisoned model;method poisoning loss;poisoning poisoned model;poisoned model;weights pose security;poisoning performance using;trained weights;users download weights"}, "c14254fd285706e549d0dcc57ae74680164c9afc": {"ta_keywords": "inverse reinforcement learning;sensitive inverse reinforcement;inverse reinforcement;problem inverse reinforcement;risk sensitivity reinforcement;sensitivity reinforcement learning;gradient based inverse;sensitivity reinforcement;reinforcement learning gradient;reinforcement learning markov;reinforcement learning;sensitive inverse;learning markov decision;reinforcement learning framework;introductionrisk sensitive inverse;reinforcement;markov decision processes;based inverse;agent risk;risk sensitivity;agent risk sensitive;markov decision;decision processes agent;model risk sensitivity;behavioral economics neuroscience;learning markov;inverse;risk;decision processes;risk sensitive", "pdf_keywords": "risk reinforcement learning;risksensitive reinforcement learning;risk reinforcement;risk sensitive reinforcement;modeling risk reinforcement;risk metric learn;risk learning;risksensitive reinforcement;learning human decision;risk learning established;risksensitivity loss aversion;reward probability risk;sensitive reinforcement learning;forward risksensitive reinforcement;learning risk;values agent policy;based inverse reinforcement;model human decision;inverse reinforcement learning;risk sensitive learning;risk metrics behavioral;learning risk sensitive;learn value;value function policy;learning value;derive reinforcement learning;approach learned value;approach learn value;inverse reinforcement;disadvantages reinforcement learning"}, "e10dba1d4a56a81429d6ec4c9b7bdc15ea75474b": {"ta_keywords": "secure remote estimation;malicious sensors;presence malicious sensors;remote estimation;malicious sensors inject;remote estimation linear;observations multiple sensors;sensors inject anomalous;measurements shared fusion;cyberphysical systems;relevant cyberphysical systems;process observations;estimation linear time;inject anomalous observations;multiple sensors;process observations multiple;gassian process observations;sensors make sequential;multiple sensors considered;sequential measurements shared;sensors considered framework;sensors considered;sensors;cyberphysical systems internet;framework relevant cyberphysical;cyberphysical;sequential measurements;sensors make;relevant cyberphysical;shared fusion", "pdf_keywords": ""}, "5403fd71810d098e572d9bd0f9ec10e96d6b6336": {"ta_keywords": "mobile network classically;problem mobile network;mobile network;classically dynamic programming;dynamic programming;network classically dynamic;dynamic programming techniques;iteration linear programming;networks large state;structure optimal policy;solving problem mobile;solve optimization problem;solve optimization;iteration policy iteration;complexity networks large;policy iteration linear;policy iteration;optimization problem;linear programming;optimal policy;optimization problem suffer;optimization;networks large;iteration policy;linear programming employed;value iteration policy;complexity networks;optimal policy exploited;network classically;complexity reduction", "pdf_keywords": ""}, "967b2d10b8b378f1da43fd4d9107826e540e1112": {"ta_keywords": "animations natural language;challenge language pose;linguistic concepts motion;language pose application;language pose;concepts motion animations;motion planning sentences;motion animations;human animation;concepts motion;animation robot motion;virtual human animation;motion animations methodsin;multimodal;animation;animations;map linguistic;modeling challenge language;animation robot;animations natural;planning sentences;movie script visualization;map linguistic concepts;human animation robot;motion;movie script;robot motion;natural language;language sentences finds;challenge language", "pdf_keywords": "language pose generation;language pose learn;pose sequence generation;embedding language pose;conditioned pose forecasting;pose image speech;language pose joint;modal pose forecasting;joint language pose;predict future pose;speech conditioned pose;joint languageto pose;pose forecasting;pose joint embedding;learns joint embedding;pose learn;pose generation;pose sequence;language pose;used pose forecasting;language descriptions pose;pose forecasting proposed;descriptions pose sequence;language pose orjl2p;pose learn joint;learn joint embedding;pose forecasting challenging;pose generation model;animation natural language;trained joint embedding"}, "0bdf1f3b79f4df5d5e11af1ea00379e1461e22fa": {"ta_keywords": "partial dependence plots;dependence plots pap;applications machine learning;class partial dependence;machine learning;machine learning necessary;dependence plots;partial dependence;validate various qualitative;various qualitative properties;features checking;qualitative properties;instance specific paps;dependence;features;qualitative properties monotonicity;features checking undesirable;monotonicity respect feature;pap including instance;specific paps;various qualitative;metrics test accuracy;feature combination features;combination features;respect feature combination;combination features checking;learning necessary look;respect feature;look standard metrics;metrics", "pdf_keywords": ""}, "d95973f0f0d86b758154e9a5f3d7434430d7856c": {"ta_keywords": "gradient free optimization;euclidean proximal operator;accelerated gradient free;gradient free method;free optimization methods;proximal operator;propose accelerated gradient;proximal operator described;accelerated gradient;free optimization;proximal operator associated;gradient free;optimization methods non;methods propose accelerated;euclidean proximal;non euclidean proximal;optimization methods;operator described methods;background accelerated gradient;gradient;operator associated norm;estimates rate convergence;convergence method low;method low noise;optimization;proximal;methods non euclidean;propose accelerated;method non euclidean;rate convergence method", "pdf_keywords": ""}, "194c5644c49e9e1b87990439fae05c98ba8b4fbb": {"ta_keywords": "annotating materials synthsynthesis;corpus annotating materials;procedural text corpus;annotating materials;synthesis procedures annotated;science procedural text;corpus annotating;text corpus annotating;materials science procedural;procedural text;text corpus;procedures shallow semantic;materials synthsynthesis procedures;semantics synthesis;corpus;semantics synthesis sentences;materials synthsynthesis;procedures annotated;annotating;express semantics synthesis;semantic structure;semantic structures objectivethe;synthesis sentences;semantic structures;synthesis sentences resultsthe;semantic structure methodswe;semantic;shallow semantic structures;shallow semantic;annotated", "pdf_keywords": "annotating materials syntheses;annotating materials;annotating scientific text;resource annotating materials;scientific information extraction;procedural text corpus;corpus materials;text corpus;corpus new resource;extraction systems corpus;annotating scientific;materials syntheses text;challenges annotating scientific;structure make corpus;annotation process;corpus materials synthesis;tool natural language;systems corpus materials;annotation;annotations;annotation agreements 21;annotation process present;corpus;annotating;science procedural text;annotations available;annotation agreements;language report annotation;text corpus new;systems corpus"}, "03b68259f9e70d2007d40e5331c9ff31f2bb46b9": {"ta_keywords": "acceleration based activity;activity modeling;activity modeling methodsthis;activity recognition method;based activity modeling;activity recognition;proposes activity recognition;acceleration sensor data;activity modeling aimto;activities using;user activities using;activities using labeled;user activities;training data;activity;acceleration sensor;data acceleration based;acceleration based;training data selection;introduction training data;characteristics data acceleration;selection user physical;data acceleration;based activity;physical characteristics data;user physical characteristics;evaluate training data;unlabeled acceleration sensor;activities;sensor data", "pdf_keywords": ""}, "e11d6a031d5f85f372b0fda3ab62ca4ce2d89f2c": {"ta_keywords": "socrates rule based;rule based expert;socrates rule;introduction socrates rule;rule generation;rules rule generation;rule generation module;expert optimizes combinational;based expert optimizes;expert optimizes;rule based;socrates;automatically encodes rules;choose rules applied;choose rules;introduction socrates;optimizes combinational logic;rules applied circuit;design control mechanism;based expert;combinational logic specific;combinational logic;design control;strategies choose rules;rules rule;rules applied;expert;encodes rules;transformation rules rule;rules inserts", "pdf_keywords": ""}, "90fbeb4c871d3916c2b428645a1e1482f05826e1": {"ta_keywords": "reviewer module caption;caption generation;caption generation methodswe;introductionencode review decode;module caption generation;decode reviewer module;review decode;decoder model reviewer;reviewer module;review decode reviewer;reviewer module improve;introductionencode review;decode reviewer;module caption;module reviewer module;model reviewer module;resultsthe reviewer module;caption;module reviewer;reviewer module performs;novel module reviewer;reviewer module generic;introductionencode;module improve encoder;resultsthe reviewer;encoder decoder learning;attention mechanism encoder;reviewer;model reviewer;decoder learning framework", "pdf_keywords": "encoders review network;rnn decoders cnn;cnn rn encoders;rn encoders review;cnns rns encoderswe;rnn decoders;decoders cnn rn;captioning using cnns;review networks;encoder learn thought;rn encoders;encoder learns thought;review information encoded;consider rnn decoders;decoders cnn;representation neural;encoders review;encoder learns;useful representation neural;encoder decoder learning;attentive encoder;rns encoderswe;encoder learn;review networks indicates;attention mechanism encoder;rns encoderswe present;model review network;attentive encoder decoders;cnns rns;captions based learned"}, "ddd74358d7e11535ee77e2c323dd662d115a0f20": {"ta_keywords": "learning robot policy;learning robot;robot policy;object locations instructed;robot policy follow;natural language instructions;trained augmented reality;problem learning robot;language conditioned object;introduce shot language;trained augmented;conditioned object grounding;learned map;shot language conditioned;learned map representation;robot;natural language;method trained augmented;shot language;follow natural language;present learned map;exemplars identify objects;object locations;augmented reality data;encodes object locations;conditioned object;representation encodes object;language instructions easily;identify objects;instructions present learned", "pdf_keywords": "learning robot policy;instructions physical robots;objects instruction;learning robot;learning objects;natural language instruction;action generation networks;objects instruction segments;object locations instructed;maps natural language;objects navigation corpus;learning objects seen;robots implemented environments;propose shot language;human robots present;demonstrate natural language;robot policy;trained augmented reality;physical robots;instruction semantics representation;instructions navigation corpus;expressions human robots;generate action generation;robots implemented;action generation;physical robots implemented;robots present;robot policy follow;models instruction followinging;natural language instructions"}, "97943a5dee3c6e36d01a6099acb9ec360ad0ee19": {"ta_keywords": "portmanteaus word formation;end trainable language;word formation;trainable language;neural sequence sequence;use additional phonetic;portmanteau generation;portmanteaus word;neural sequence;trainable language independent;additional phonetic information;portmanteau generation end;words combine new;word formation phenomenon;additional phonetic;phonetic information methods;background portmanteaus word;combine new word;phonetic;level neural sequence;character level neural;sequence s2s;phonetic information;sequence s2s methods;sequence sequence s2s;words combine;portmanteaus;task portmanteau generation;portmanteau;formation phenomenon words", "pdf_keywords": "prediction character sequence;neural model portmanteau;portmanteau data neural;data neural sequenceto;character language model;portmanteaus word formation;neural sequenceto sequence;language model generate;previous characters sequence;neural sequenceto;bidirectional encoder generate;neural sequence;character sequence;neural sequence tosequence;characters sequence use;characters sequence;additional phonetic information;encoder generate portmanteau;neural machine translation;model words english;character sequence conditioned;character language;generate prediction character;use additional phonetic;trainable language;toend trainable language;phonetic information methodswe;model words;character level neural;bidirectional encoder"}, "30f86d38f0660af5ea2e16d996434c72eee8c5ee": {"ta_keywords": "espnet spss software;espnet open source;processing named espnet;speech processing named;end speech processing;speech processing;espnet methods new;named espnet methods;espnet methods;espnet spss;spss software;spss software used;espnet;named espnet spss;anrria software platform;differentiate espnet;anrria software;open source anrria;named espnet;software platform;espnet open;source platform;open source;open source platform;source anrria software;differentiate espnet open;software;software used analyze;functionalities differentiate espnet;software used", "pdf_keywords": "end speech recognition;endto end neural;end neural network;end neural;endto end speech;end end speech;speech recognition toolkit;end toend speech;end speech;speech recognition recent;speech recognition neural;cc attention architecture;networks speech recognition;speech recognition legacy;toend speech processing;neural networks speech;attention architecture multiobjective;human speech recognition;conversational speech recognition;automated speech;attention based cc;attention architecture;speech recognition results;state art endto;automated speech recognition;new toolkit neural;development automated speech;speech recognition;hybrid cc attention;speech processing"}, "36bca9d41de386fce5dce06999a45a802a7c4f41": {"ta_keywords": "conditional preference networks;preference networks;conditional preference;preference networks pfs;modeling preferences;modeling preferences study;backgroundgenerating conditional preference;formalism modeling preferences;preferences study properties;preferences study;preference;preferences;conditional;problems naive generation;naive generation including;sampling bias;naive generation;backgroundgenerating conditional;including sampling bias;bias;networks pfs commonly;average needs generate;pfs equiprobable manner;generate pfs equiprobable;generation including sampling;base assumptions statistical;networks pfs;pfs equiprobable;pfs commonly;sampling", "pdf_keywords": ""}, "0c5bfa2d4bb351a479073cb358c3ae6f7ecf0476": {"ta_keywords": "natural language processing;implementation natural language;corpora annotations data;corpora annotations;natural language;language processing;reading corpora annotations;text representation;language processing requires;text representation extracting;constructs reading corpora;annotations data structures;corpora;annotations data;reading corpora;augment text representation;machine learning components;annotations;representation extracting;representation extracting features;represent language constructs;structures represent language;represent language;creating data structures;features training machine;data structures;text;learning components;introductionthe implementation natural;data structures applying", "pdf_keywords": ""}, "a2aa642db090b3aa28a44ccbc3c51fdb0be8335b": {"ta_keywords": "treebanks constituency parsers;benchmark treebanks constituency;neural parsers;constituency parsers;generalization neural parsers;results benchmark treebanks;benchmark treebanks;treebanks constituency;treebanks;neural parsers zero;constituency parsers degree;parsers;backgroundneural parsers obtain;neural par;backgroundneural parsers;non neural par;parsers obtain;training trees corpus;parsers zero shot;parsers degree generalize;trees corpus;parsers degree;parsers zero;parsers obtain state;trees corpus evaluating;corpora resultsfirst neural;corpus;corpora;par;results generalization neural", "pdf_keywords": "non neural parsers;parser pre trained;neural parsers wj;neural parsers;parsers generalization neural;neural parsers generalize;neural parsers zero;generalization neural parsers;neural parsers generalization;backgroundneural constituency parsers;tool neural parsers;constituency parsers increasingly;structure improves parsing;constituency parsers;attentive parser pre;self attentive parser;parsers increasingly;attentive parser rnng;parsers generalize comparably;parsers generalize;parsers wj test;parsers generalization;improves parsing;parsers;improves parsing performance;parser rnng bert;parser able generalize;parser pre;parsers increasingly used;attentive parser"}, "de5834305ea419c25b17f0c8d27bad6a5feb311a": {"ta_keywords": "chess commentary dataset;generate commentaryary chess;commentaryary chess games;commentaryary chess;chess commentary;chess commentary pairs;scale chess commentary;298k chess commentary;commentary individual moves;generate commentary;language descriptions chess;descriptions chess games;generate commentary individual;commentary dataset propose;commentary dataset;descriptions chess;learning generate commentaryary;generate commentaryary;methods generate commentary;chess games methodswe;moves chess game;commentary;moves chess;chess games large;chess games;large scale chess;commentary pairs;commentary individual;individual moves chess;chess game resultsthe", "pdf_keywords": ""}, "47234fca1b14666d72bc5df0e2d911ff7cdea688": {"ta_keywords": "hypergraph spectral clustering;hypergraph spectral;spectral clustering;spectral clustering celebrated;hypergraph;hypergraph spect;pairwise similarity information;based pairwise similarity;pairwise similarity;intended setting hypergraph;setting hypergraph;setting hypergraph spect;similarity measures available;similarity information;similarity measures;clustering;way similarity measures;italic way similarity;similarity information approach;clustering celebrated;clustering celebrated algorithm;way similarity;similarity;spectral;applications italic multi;algorithm partitions objects;algorithms;algorithm partitions;celebrated algorithm partitions;based pairwise", "pdf_keywords": "spectral clustering hypergraphs;hypergraph spectral clustering;clustering hypergraphs;clustering hypergraphs algorithm;hypergraph clustering;hypergraph spectra clustering;partition weighted hypergraph;hypergraph clustering algorithms;hypergraphs algorithm;improved weighted hypergraph;hypergraph spectral;develop hypergraph clustering;weighted hypergraph;hypergraph model propose;hypergraphs algorithm weakly;clustering hc hypergraph;graph clustering accuracy;graph clustering;algorithm hypergraph;model weighted hypergraph;algorithm hypergraph spectra;hypergraphs;algorithms random hypergraph;spectral clustering;spectral clustering local;weighted hypergraph model;new algorithm hypergraph;spectral clustering present;hypergraph;hypergraph model weighted"}, "9a41111cf881b052555985bd8cf304ef9fc4f6d5": {"ta_keywords": "labeling information extraction;information extraction;labeling information;introductiondtant labeling information;information extraction suffers;introductiondtant labeling;potential instance labels;labeling;instance labels;instance labels methodsa;coupling corpora;coupling corpora sections;example coupling corpora;exist corpora augmenting;corpora sections identified;likely label sections;corpora sections;noisy training data;labels methodsa;labels;label sections exist;label sections;corpora augmenting;sections exist corpora;section likely label;sections identified items;corpora augmenting large;associated distant identifying;corpora;distant identifying coupling", "pdf_keywords": "structured corpus extracts;structured corpus;information targeted corpora;structured corpora;labeling information extraction;smaller structured corpus;structured structured corpora;corpus extracts chunks;large target corpus;information extraction;targeted corpora;corpus smaller structured;target corpus;structured corpora limited;corpora extract;information extraction suffers;supervision label propagation;parse corpora extract;parse corpora;information extraction using;corpus extracts;large scale semantic;label propagation mentions;approach information extraction;curated knowledge base;corpus;curated structured structured;relations human corpus;labeling information;coupling corpora corpora"}, "c8a95217cde1bc893b230297250918818aa01dd7": {"ta_keywords": "mamps degenerate environments;systemic mapping backpack;backpack mobile mamps;analysis bacterial;mapping backpack;analysis bacterial bacterial;bacterial species;bacterial;mapping backpack mobile;bacterial bacterial;bacterial bacterial species;mamps degenerate;bacterial bacterial bacterial;tool analysis bacterial;mobile mamps degenerate;environments systemic mapping;degenerate environments systemic;mobile mamps;mamps;degenerate environments;degenerate environments useful;environments systemic;environments useful tool;environments;environments useful;algorithm systemic mapping;systemic mapping;backpack mobile;species;mapping", "pdf_keywords": ""}, "71cdf94d13cc6c497dcc2dcb20893fe64cfaf62e": {"ta_keywords": "guide text generation;text generation;generation desired topical;text generation desired;interactive writing assistants;interactive writing;language models aid;current interactive writing;based language models;language models;upcoming topics user;guide generation methods;upcoming topics;transformer based language;guide generation;topics user select;generation methods framework;generation methods;writing assistants;subset guide generation;topics;plausible continuations text;continuations text;writing assistants allow;continuations text written;human authors suggesting;text written far;topics user;candidate upcoming topics;suggesting plausible continuations", "pdf_keywords": "text generation models;generate sentences;topic generator;domain narrative generation;generate topics input;text generation;ability generate sentences;neural story generation;text generation desired;generate sentences containing;generate topics corpus;narrative generation;narrative generation based;using text generation;guide text generation;generate topics;generates topic options;topic generator mn;conditional text generator;ability generate topics;words continuation generation;narrative ability generating;novel text generation;tool annotating narrative;predict content text;interactive writing assistant;generation desired topical;generating text;generating text generated;text generator"}, "edb49aa423afc210facec998277923c4b75e4648": {"ta_keywords": "crse4 chains antiferromagnetic;ferromagnetic crse4 chains;ferromagnetic crse4;ions crse4 chains;crse4 chains crystallographic;coupling ferromagnetic crse4;cr3 ions crse4;ions crse4;magnetic interaction cr3;structural phase transition;crse4 chains;antiferromagnetic magnetic structure;cr3 ions;interaction cr3 ions;structural phase;chains antiferromagnetic;se2 ions;antiferromagnetic;phase transition mainly;crse4;phase transition;displacements se2 ions;chains antiferromagnetic magnetic;antiferromagnetic magnetic;magnetic structure;interaction cr3;se2 ions induced;structure spiral magnetic;chains crystallographic;magnetic structure spiral", "pdf_keywords": ""}, "4e749b2e0728044af44d50a708fc99d49359ea0b": {"ta_keywords": "sequence models attention;character level transduction;encode structural linguistic;sequence models;models attention;sequence sequence models;models attention focusing;structural linguistic knowledge;finite state models;unsupervised learning scenario;structural linguistic;level transduction problems;unsupervised learning;models designed encode;transduction problems solved;transduction problems;learning scenario;explored unsupervised learning;level transduction;encode structural;linguistic knowledge;learning;transduction;state models;linguistic knowledge underlying;attention;learning scenario compare;state models designed;linguistic;models", "pdf_keywords": "language sequence transduction;romanization decipherment task;decoding informal romanization;transduction natural language;romanization challenging;romanization decipherment;romanization challenging task;natural language sequence;text language processing;language processing;sequence transduction tasks;approach deciphering romanization;informal romanization challenging;generate text language;language sequence;romanization;language text language;deciphering romanization decipherment;character level seq2seq;text language priors;language priors character;text language;informal romanization;language processing important;translation task seq2seqwe;character level transduction;translation translation model;translation model based;translation model;script data romanized"}, "9700940262cd5e797ab81eee464c3b3a16295cba": {"ta_keywords": "adaptation speech enhancement;mismatch speech enhancement;robustness speech recognizers;increase robustness speech;speech enhancement;speech enhancement approaches;model adaptation speech;speech recognizers model;speech enhancement increase;robustness speech;speech recognizers;automatic speech recognition;speech recognition performs;dynamic mismatch speech;speech recognition;automatic speech;known automatic speech;recognizers model adaptation;adaptation speech;noise reverberation research;mismatch speech features;speech features acoustic;model adaptation effective;noise reverberation;features acoustic model;speech features;static mismatch speech;model adaptation;enhancement increase robustness;undertaken model adaptation", "pdf_keywords": ""}, "0d2a1c0724743de0cb74463466b075598ba36c45": {"ta_keywords": "medical equipment alarms;medical devices alarms;alarms medical equipment;equipment alarms related;devices alarms medical;equipment alarms;alarms medical;alarms related;devices alarms;alarms related problems;alarms;diagnosis diagnosis diagnosis;problems patient diagnosis;patient diagnosis diagnosis;patient diagnosis;diagnosis diagnosis;medical devices;medical medical devices;medical equipment;diagnosis;devices;equipment;related problems patient;problems frequently reported;medical medical;reported medical medical;related problems frequently;medical;related problems;frequently reported medical", "pdf_keywords": ""}, "f01f4808263ecfa221f856c34d3420166dbf5930": {"ta_keywords": "confusion status detector;confusion status detection;confusion level driver;driver confusion status;car navigation;driver using classifier;status detector car;detector car navigation;trained multimodal sensor;car navigation proactively;driver confused;detection using recurrent;estimating confusion level;driver confusion;classifier trained multimodal;driver;road driving traffic;trained multimodal;road driving;driving traffic;background driver;collected road driving;multimodal sensor;multimodal sensor data;driver using;classifier trained;driving traffic using;driving;background driver confusion;recurrent neural networks", "pdf_keywords": ""}, "ff7b5379641875be7357766af0b1e2bd55c74cc8": {"ta_keywords": "information retrieval;information retrieval accomplished;transformer information corpus;information corpus encoded;information corpus;retrieval;search index;differentiable search index;corpus encoded parameters;demonstrate information retrieval;learns text text;model answers queries;learns text;search index di;text model;retrieval accomplished;retrieval accomplished single;text text model;text model maps;corpus encoded;search;answers queries directly;words di model;introduce differentiable search;string queries directly;corpus;answers queries;string queries;directly relevant docids;paradigm learns text", "pdf_keywords": "indexing retrieval tasks;document retrieval;indexing retrieval;document retrieval task;integrates retrieval retrieval;novel indexing retrieval;studied document retrieval;retrieval retrieval tasks;retrieval retrieval;information retrieval;approach retrieval information;information retrieval accomplished;model integrates retrieval;unsupervised retrieval;approach retrieval;retrieval new;using retrieval;retrieval tasks encode;integrates retrieval;retrieval information;retrieval tasks single;retrieval tasks;novel indexing;retrieval augment generation;retrieval;generative indexing improving;retrieval information data;representations documents docids;friendly approach retrieval;unsupervised retrieval scaling"}, "3cd4ae1cac866f853bb3276d215cff18df371b67": {"ta_keywords": "spech recognition chime;robund spech recognition;spech recognition;approaches source separation;estimating clean signal;source separation;used source separation;recognition chime;recognition chime hallenge;clean signal given;clean signal;given noisy signals;recognition;robund spech;source separation community;signal given noisy;noense robund spech;noisy signals;background discreiminative methodes;estimating clean;noisy signals pursue;background discreiminative;signals;problem estimating clean;spech;signal;given noisy;signal given;separation;chime hallenge benmark", "pdf_keywords": ""}, "ceef266c59698999c9283a0cda852d8bc1ce27ea": {"ta_keywords": "pretrained language models;tuning pretrained language;pretrained language;language models;structural changes embedding;language models usually;embedding space changes;changes embedding space;improvements downstream tasks;changes embedding;fine tuning pretrained;tuning pretrained;isotropy embedding space;isotropy embedding;embedding space methods;pretrained;performance improvements downstream;embedding space;embedding;downstream tasks;improvements downstream;downstream tasks limited;extent isotropy embedding;language;changes fine tuning;brings performance improvements;performance improvements;fine tuning results;fine tuning;downstream", "pdf_keywords": "embedding space pretrained;pretrained linguistic domains;space pretrained linguistic;pretrained linguistic;word embeddings;improves isotropy embedding;word representations highlythe;fine tuned embedding;word embeddings association;word representations;embedding spaces significantly;rrs word representations;embedding space changes;context word embeddings;isotropy embedding spaces;isotropy embedding space;isotropy embedding;linguistic domains rrs;pre trained models;embeddings;encoded pre trained;app isotropy embedding;embedding spaces;embedding space;tuned embedding space;embedding spaces fine;evaluate isotropy embedding;spaces significantly improved;embeddings association computational;linguistic domains"}, "b719fc66b173f8e9e0624317bb00abf10a4d5606": {"ta_keywords": "temporal segmentation basketball;segmentation basketball;segmentation basketball game;basketball game video;video shot basketball;shots temporal segmentation;segmentation based video;detection video shot;video shots temporal;edge detection video;analysis video shot;game video shots;games edge detection;detection video;video image frame;image frame extraction;video image;game video;backgroundthe analysis video;based video image;temporal segmentation based;frame extraction;temporal segmentation;studying temporal segmentation;video shot active;video application;basketball games edge;segmentation based;analysis video;video shot", "pdf_keywords": ""}, "18ef33a6e040b49ba475e586202932cecbafba0d": {"ta_keywords": "event influencece generation;generate event influences;reasoning events tracking;influencece generation using;models generate event;language models generate;event influencece;influence distance reasoning;influencece generation;events tracking influences;introductioneigen event influencece;event influences;reasoning events;generate event;language models;trained language models;influences conditioned context;event influences conditioned;context nature influence;language models important;process reasoning events;pre trained language;introductioneigen event;events;conditioned context nature;events tracking;reasoning chain;conditioned context;event;trained language", "pdf_keywords": "event influence generation;generate event influences;reasoning events tracking;generation targeted events;generating event influence;event influence analysis;generated event influences;context reasoning events;event generate information;generate targeted influence;events tracking influences;influence generation framework;targeted events context;downstream question answering;events context association;event influences reference;source event generate;demonstrate event influence;generate information context;influence generation;evaluate event influences;context association computational;influence distance reasoning;event influence;influence nodes event;influence generation eigen;models generate event;event generate;better reference event;targeted influence nodes"}, "e5acad5bba23a8c3a9f7cd24f7694ab10357ebc7": {"ta_keywords": "speech separation recognition;speaker speech separation;multichannel multi speaker;separation recognition singlechannel;speech separation;multi speaker;multi speaker speech;applied multi speaker;recognition singlechannel multichannel;separation recognition;recognition singlechannel;singlechannel multichannel;singlechannel multichannel conditions;multichannel multi;reverberant noisy scenarios;singlechannel;speaker;reverberant conditions methods;multichannel;speaker speech;focus multichannel multi;multichannel conditions;reverberant noisy;anechoic reverberant conditions;reverberant conditions;gap anechoic reverberant;observed reverberant noisy;separation;focus multichannel;degradation observed reverberant", "pdf_keywords": "speech separation recognition;speech recognition multichannel;separation speech recognition;multichannel speech recognition;multichannel speech signal;recognition multichannel speech;based multichannel speech;speech signals multichannel;multichannel speech;multichannel multi speakerthe;approach multichannel speech;model multichannel speech;signals multichannel speech;speaker speech separation;acoustic model multichannel;separation recognition singlechannel;multi speaker;speech signal input;multi speakerthe problem;speech separation;speech enhancement performance;multi speakerthe;recognition speech enhancement;denoising separation speech;detection acoustic transfer;training dereverberation beamforming;applied multi speaker;training speech recognition;recognition singlechannel multichannel;multi speaker speech"}, "4b9b7240ef9b6bc442044684ed5646ef02897d87": {"ta_keywords": "stochastic planners;stochastic planners current;fast stochastic planners;probabilityistic planning competition;probabilityistic planning;planner based mimp;deterministic planner;international probabilityistic planning;deterministic planner based;leading deterministic planner;planning competition;planner;planner based;planning competition leading;planners;planning;planners current domains;mimp solvers able;based mimp solvers;mimp solvers;planners current;stochasticity multi valued;challenges leading deterministic;stochasticity multi;concurrent actions paper;fast stochastic;true stochasticity multi;stochastic;variables concurrent actions;showcase fast stochastic", "pdf_keywords": ""}, "c3aa698b562e91f78a042b938ffce1877b6e859c": {"ta_keywords": "awful german language;german language complex;awful german;german language;language complex;language complex complex;german;language;triad;awful;complex;triad triad;complex process characterized;triad triad triad;characterized;complex complex process;complex process;presence triad triad;complex complex;process characterized;characterized presence triad;presence triad;process;process characterized presence;characterized presence;presence", "pdf_keywords": ""}, "728a6850882a0d8ef5551949cc2baee1e1667cd8": {"ta_keywords": "bribery schemes voting;optimal bribery schemes;optimal bribery;finding optimal bribery;bribery schemes;voting combinatorial domains;voting combinatorial;bribery problem;introductionbribery voting combinatorial;bribery problem easy;bribery;cases bribery problem;schemes voting;schemes voting domains;cases bribery;methodswe cases bribery;voting domains;voting domains candidate;introductionbribery voting;candidate set cartesian;combinatorial domains easy;combinatorial domains;candidate set;complexity finding optimal;voting;preferences represented nets;agents preferences represented;combinatorial;computational complexity;computational complexity finding", "pdf_keywords": ""}, "df8ae2068d17d969db6ab2d27108776e99413975": {"ta_keywords": "backgroundnatural language inference;language inference nli;language inference;question answering nl;natural language processing;inference nli;inference nli fundamental;question answering;natural language;backgroundnatural language;textual information;fundamental natural language;semantic;textual;including semantic;answering nl problem;use textual information;inference;answering nl;language processing np;use textual;search question answering;classify given premise;language processing;semantic search;applications including semantic;methods use textual;nli fundamental natural;semantic search question;including semantic search", "pdf_keywords": "conceptnet textual entailment;natural language inference;inference natural language;language inference natural;backgroundnatural language inference;conseqnet entailment model;conceptnet textual;knowledge source conceptnet;graphs conceptnet textual;knowledge graphs conceptnet;wordnet using knowledge;text graph entailment;conseqnet entailment;conceptnet wordnet;language inference;language inference nli;textual entailment;entailment dataset;language inference important;conceptnet external knowledge;present conseqnet entailment;conceptnet wordnet using;textual entailment future;conseqnet framework neural;wordnet;rnn natural language;entailment models;conseqnet based text;bpedia conceptnet wordnet;question answering"}, "a7abd783de8d21d640e41d31ec89f2c1caec4e42": {"ta_keywords": "word sense disambiguation;sense disambiguation methods;sense disambiguation;sense disambiguation wd;disambiguation methods word;disambiguation wd knowledge;disambiguation methods;disambiguation;representing word senses;word sense;word senses hypernyms;interpretable word sense;unsupervised knowledge free;knowledge free interpretable;interpretable knowledge free;free interpretable word;disambiguation wd;senses hypernyms usage;unsupervised knowledge;representing word;knowledge free;word senses;methods word sense;tend interpretable knowledge;knowledge free counterparts;senses hypernyms;interpretable knowledge;elements representing word;present word sense;background unsupervised knowledge", "pdf_keywords": "word sense disambiguation;sense disambiguation;human interpretable disambiguation;sense disambiguation unsupervised;sense disambiguation bridges;interpretable disambiguation context;interpretable disambiguation;sense disambiguation wsd;disambiguation unsupervised interpretable;disambiguation context available;sense disambiguation wd;use word sense;available word sense;disambiguation context;sense induction disambiguation;disambiguate text;user disambiguate text;word sense;disambiguate text text;disambiguation;representing word senses;word sense induction;interpretable knowledge free;framework word sense;disambiguation unsupervised;disambiguate;word sense looking;uses user disambiguate;disambiguation wsd new;induced word sense"}, "58961f0ea3291ddab697fbe5be999a0793b0efaf": {"ta_keywords": "erasure codes provide;erasure codes;use erasure codes;erasure code encodes;erasure code;distributed storage;distributed storage systems;introduction distributed storage;failures erasure code;separable scalar codes;codes provide tolerance;storage systems;typically use erasure;storage systems typically;scalar codes commonly;symbols distributed nodes;use erasure;failures erasure;scalar codes;codes commonly used;node failures erasure;codeword symbols distributed;storage;distributed nodes;codes commonly;erasure;symbols distributed;encodes message codeword;distributed nodes maximum;distributed", "pdf_keywords": ""}, "f75e691daae9133941c9a083e319b39bd837d456": {"ta_keywords": "iterative entity alignment;entity alignment joint;entity alignment;joint knowledge embeddings;entities aswikipedia links;alignment joint knowledge;approach entity alignment;link entities counterparts;entity alignment aims;link entities;knowledge embeddings method;entities aswikipedia;knowledge embeddings;information entities aswikipedia;multiple knowledge graphs;entities counterparts multiple;knowledge graphs;iterative entity;entities relations;knowledge graphs kgs;introduction iterative entity;entities relations various;entities counterparts;encodes entities relations;aswikipedia links;aims link entities;counterparts multiple knowledge;joint knowledge;aswikipedia links require;entities", "pdf_keywords": ""}, "80a085a79ac6cee94f21d21ab8ca302458c4e131": {"ta_keywords": "cloud hosted inference;data compromising cloud;compromising cloud service;compromising cloud;hosted inference services;cloud service;hosted inference;cloud service ability;private privileged data;privileged data network;cloud;cloud hosted;cloud increasingly;communicated data compromising;inference services;digital network inference;content communicated data;relying cloud;increasingly relying cloud;services raises privacy;cloud perform huge;relying cloud perform;privileged data;cloud increasingly relying;communicated data;inference services raises;servers exposing service;cloud perform;servers exposing;data compromising", "pdf_keywords": "implementations privacy neural;privacy protecting neural;privacy neural networks;privacy preserving neural;privacy neural;privacy cloud;privacy cloud improve;privacy accuracy loss;optimizing privacy cloud;protect inference privacy;privacy preserving;privacy network;optimizing privacy;accuracy privacy;deep private feature;method optimizing privacy;inference privacy;optimize privacy;incorporating privacy accuracy;protecting neural networks;optimize privacy network;approach privacy preserving;datathe privacy;privacy accuracy;inference privacy using;implementations privacy;balance accuracy privacy;accuracy privacy formal;approach optimize privacy;private feature extraction"}, "464b47a6a395fa1338e230254965cf5f669e715c": {"ta_keywords": "neural machine translation;random babbling baseline;babbling baseline using;babbling baseline;based random babbling;machine translation;babbling baseline coatal;random babbling;machine translation techniques;translation techniques;ii random babbling;translation techniques developed;character based random;character trigrams;babbling;range neural machine;neural machine;using character trigrams;baseline using character;phrase based model;range neural;using character;standard phrase based;moses character based;moses character;runs standard phrase;neural;translation;evaluated range neural;character based", "pdf_keywords": ""}, "ca57443fcb87f03267fccee162a4924c56062c6f": {"ta_keywords": "imaging mri diagnosis;mri diagnosis treatment;mri diagnosis;imaging mri;magnetic resonance imaging;resonance imaging mri;mri;patients diagnosis malignant;resonance imaging;diagnosis malignant disease;use magnetic resonance;diagnosis malignant;magnetic resonance;malignant disease;imaging;patients diagnosis;malignant;diagnosis;diagnosis treatment patients;treatment patients diagnosis;diagnosis treatment;patients;importance use magnetic;disease;use magnetic;magnetic;treatment patients;resonance;treatment;article discuss importance", "pdf_keywords": ""}, "d3304b926cfcd91110bd5ba01db21d26ce5fca2d": {"ta_keywords": "paraphrastic sentence embeddings;paraphrastic senstence embeddings;learning paraphrastic senstence;learning paraphrastic;generate sentential paraphrases;sentential paraphrases translation;paraphrases translation bilingual;introduction learning paraphrastic;sentential paraphrases;paraphrase pairs ability;paraphrases translation;paraphrase pairs;paraphrastic senstence;neural machine translation;general purpose paraphrastic;paraphrastic;paraphrastic sentence;paraphrases;sentence embeddings;evaluate paraphrase pairs;embeddings translated bitext;purpose paraphrastic;embeddings translated;purpose paraphrastic sentence;bilingual sentence pairs;evaluate paraphrase;senstence embeddings translated;machine translation;pairs evaluate paraphrase;translation generate sentential", "pdf_keywords": "produce paraphrastic embeddings;introductionparaphrastic sentence embeddings;sentence embeddings paraphrastic;paraphrastic sentence embeddings;paraphrases neural machine;generate sentential paraphrases;embeddings paraphrastic sentence;paraphrastic embeddings;paraphrases training data;generated paraphrase corpus;effective paraphrase generation;train paraphrases learn;discovering paraphrases neural;training sentence embeddings;paraphrastic embeddings par;train paraphrases using;embeddings paraphrastic;paraphrases train paraphrases;neural machine translation;learning sentence embeddings;paraphrase generation;paraphrases learn semantic;paraphrase generation technique;neural machine translations;generating discovering paraphrases;paraphrases neural;sentence embeddings dataset;paraphrase datasets significantly;paraphrases training;improve generated paraphrase"}, "305a1251a68fb16835876d8c99de498472c0cd8f": {"ta_keywords": "locality codes defining;locality codes;computational locality;locality called computational;coded computation;called computational locality;computational locality locality;computation lens locality;introductioncoded computation emerging;locality;locality locality;coded computation lens;locality locality properties;view coded computation;introductioncoded computation;locality properties appropriately;new notion locality;distributed computing;scale distributed computing;locality properties;lens locality codes;locality called;coding theory;distributed computing applies;notion locality;code function computed;coded;coding;coding theory provide;computation emerging", "pdf_keywords": ""}, "3332dc72fbe3907e45e8a500c6a1202ad5092c0f": {"ta_keywords": "source separation deep;separation deep learning;embeddings segmentation separation;deep clustering;backgrounddeep clustering discriminative;deep clustering resultsafter;spectrogram input mixtures;called deep clustering;segmentation separation;backgrounddeep clustering;separation deep;clustering discriminative embeddings;discriminative embeddings segmentation;segmentation separation methodswe;source separation;spectrogram;clustering discriminative;embeddings segmentation;labels target spectrogram;target spectrogram;spectrogram input;target spectrogram input;clustering resultsafter training;region spectrogram;deep network;training deep network;deep learning framework;deep learning;frequency region spectrogram;predict segmentation labels", "pdf_keywords": "segmentation separation deep;separation deep learning;deep clustering;deep clustering directly;speech separation recognition;deep clustering complex;application deep clustering;segmentation separation;separation recognition highly;spectrogram embeddings discriminative;separation recognition;framework deep clustering;separation deep;embeddings discriminative partition;speech separation task;separation recognition single;effective speech separation;optimizing deep network;optimizing deep;approach segmentation separation;partition based training;speech separation source;speech separation;separate speech;model speech separation;approach speech separation;deep learning;deep network optimizing;separate speech signal;structure speech separation"}, "6e7cfed8815cce163efac9d17b1109849c050c6b": {"ta_keywords": "extract data web;disease understood use;etiology disease understood;disease;extract data;disease understood;data web pages;etiology disease;method extract data;data web;web pages;web;etiology;data;method extract;pages;extract;new method extract;method;use;new method;understood use;understood use new;use new;new;use new method;understood", "pdf_keywords": ""}, "7038b181f776e9cd587d4d61cb68692fdac8ec26": {"ta_keywords": "signalized traffic flow;control signalized traffic;dynamical mode decomposition;signalized traffic;algorithm dynamical mode;signalized intersection propose;signalized intersection;propose koopman operator;koopman operator theory;mode decomposition;problems signalized traffic;koopman operator;single signalized intersection;analysis control signalized;signalized traffic resultswe;traffic flow;traffic flow networks;control signalized;dynamical mode;mode decomposition dd;operator theory;operator theory related;algorithm dynamical;single signalized;signalized;traffic;related algorithm dynamical;flow networks methodsdd;study single signalized;complex oscillatory dynamics", "pdf_keywords": "dynamic mode decomposition;signalized traffic flow;properties signalized traffic;dynamical mode decomposition;signalized traffic study;koopman operator theory;signalized traffic traffic;study signalized traffic;signalized traffic;operator learned traffic;traffic operators;mode decomposition operator;decomposition koopman operator;mode decomposition;koopman operator;traffic intersection spectral;signalized traffic networks;koopman operator learned;sequence signalized traffic;traffic operators apply;structure koopman operator;signalized traffic provide;applications signalized traffic;koopman operator allows;mode decomposition control;using koopman operator;data dynamical mode;extract koopman modes;traffic flow dynamic;koopman modes provide"}, "1e58c9d1153d2f25d94b3a12b785bd7abe43bd1c": {"ta_keywords": "sequential data deep;sequenceential sensor data;data deep;data deep learning;sequenceential sensor;datasets sequences high;deep learning;effective classifiers sequential;classifiers sequential data;datasets sequences;classifiers sequential;introduction sequenceential sensor;large datasets sequences;sequenceential;sequences high quality;deep learning led;precise labelling segmentation;classifiers;limited precise labelling;class labels extremely;introduction sequenceential;effective classifiers;learning effective classifiers;high quality labels;precise labelling;class labels;labelling segmentation;sequential data;datasets;sequences high", "pdf_keywords": "sequences supervised autoencoder;auto encoders semisupervised;autoencoders supervised;semi supervised learning;sequences supervised;semi supervised;encoders semisupervised;supervised autoencoder;separate sequences supervised;supervised autoencoders;supervised learning sequential;autoencoders supervised autoencoders;autoencoder reconstruct unlabeled;supervised learning change;unlabeled data learned;supervised autoencoder baselines;change points learning;learning sequential data;challenge deep learning;supervised autoencoders supervised;encoders semisupervised setting;similarity constraints sequential;similarity constraints unlabeled;use semi supervised;sequential auto encoders;sequence extract embeddings;regularization exploit unlabeled;using supervised autoencoders;learning sequential;points semi supervised"}, "5bad092098ba7400e19468a06cb8b238c43b7637": {"ta_keywords": "patient malignant disease;management patient malignant;etiology malignant disease;malignant disease;patient malignant;etiology malignant;malignant disease poorly;malignant;disease poorly understood;disease;management patient;patient;disease poorly;approach management patient;etiology;new approach management;approach management;management;development new approach;approach;new approach;understood development new;poorly understood;understood development;development new;new;development;poorly understood development;understood;poorly", "pdf_keywords": ""}, "c81bb5ff79e8c7f65a3e28b7ba52d90deaa32fde": {"ta_keywords": "arabicwikipedia talk;code switching arab;case arabicwikipedia talk;arabwikipedia capture language;switching arab languages;arabicwikipedia talk pages;arabicwikipedia;task arabwikipedia;arabwikipedia;case arabicwikipedia;present task arabwikipedia;arab languages predictor;arabwikipedia capture;task arabwikipedia capture;code switching social;act case arabicwikipedia;arab languages;switching arab;code switching;capture language choice;code switching online;introduction code switching;languages predictor;language choice case;language choice;languages;language;arab;effect code switching;pages code switching", "pdf_keywords": ""}, "f26f17ec49f2593bcc926051394871480a80c0c2": {"ta_keywords": "bilingual word embedding;backgrounddensity matching bilingual;bilingual lexicon induction;matching bilingual word;lingual word similarity;bilingual lexicon;set bilingual lexicon;matching bilingual;data set bilingual;bilingual word;cross lingual word;induction cross lingual;set bilingual;word similarity;lingual word;cross lingual;word embedding;bilingual;word embedding methodsa;word similarity approach;lingual;lexicon induction cross;embedding methodsa benchmark;morphologically rich languages;lexicon induction;distant morphologically rich;rich languages resultsin;rich languages;backgrounddensity matching;distant morphologically", "pdf_keywords": "bilingual embedding;monolingual embedding;bilingual embeddings;vector monolingual embedding;bilingual word embedding;embeddings bilingual;method bilingual embeddings;bilingual word embeddings;bilingual embedding mapping;monolingual embedding space;method bilingual embedding;bilingual embeddings bilingual;feasible monolingual embedding;embeddings bilingual lexicon;monolingual embedding spaces;expresses monolingual embedding;word embeddings language;density matching bilingual;embeddings language;mappings word embeddings;word embedding mappings;language given embeddings;word embedding;integrating bilingual word;bilingual lexicon induction;unsupervised cross lingual;lingual mappings;word embeddings;lingual mappings word;bilingual lexicon"}, "28028458d75bf9281200389a880741eb6d06a3a4": {"ta_keywords": "reconstruct abstract datalog;software specifications inductive;abstract datalog specifications;abstract datalog;inductive logic programming;datalog specifications;specifications inductive logic;datalog specifications certain;datalog;software particular learning;database software examples;logic programming methodswe;understand large software;logic programming;software examples operation;software specifications;database software;type database software;specifications inductive;large software particular;software examples;inductive logic;large software;backgroundrecovering software specifications;real world software;software particular;programming;software;type database;machine learning", "pdf_keywords": ""}, "436380dd75d8ff3f2debb29913bd2fe8dde0b684": {"ta_keywords": "source separation factorize;phase source separation;separation factorize matrix;methods source separation;phase speech mixture;matrix factororization approach;separation factorize;methods phase speech;source separation;matrix factororization;factororization approach;complex matrix factororization;speech mixture generally;phase speech;speech mixture;factorize matrix spectral;factororization approach joint;source separation conclusioncontrary;phase included decomposition;reconstructing target speech;factororization;target speech signal;modeling magnitude phase;magnitudes spectral phase;magnitude phase source;spectral phase included;speech signal results;separation conclusioncontrary conventionalal;speech signal;matrix spectral magnitudes", "pdf_keywords": "matrix factorization speech;nonnegative matrix factorization;separation factorize matrix;negative matrix factorization;factorization speech processing;matrix factorization;matrix factorization method;source separation factorize;factorize matrix spectral;matrix factorization nf;matrix factorization mfbrian;nonnegative tensor factorization;factorization speech;factorize matrix;complex matrix factorization;matrix phase reconstruction;separation factorize;factorize complex matrices;factorization complex matrix;factorization method proposed;tensor factorization;factorization nf proposed;novel method factorize;efficient nonnegative tensor;factorization nf widely;factorization complex;factorization method;tensor factorization complex;factorization nf framework;factorization"}, "6992f54509c139455c3cffa9b0e4ae5c19ebff82": {"ta_keywords": "multilingual conversations medical;multilingual conversations;analogues multilingual conversations;new multilingual conversations;multilingual;new multilingual;multilingual analogues multilingual;described based multilingual;multilingual analogues;conversations medical domain;based multilingual analogues;analogues multilingual;based multilingual;conversations medical;medical domain described;conversations;medical domain;medical;domain described based;domain described;domain;analogues;described based;new;described;based", "pdf_keywords": ""}, "30c6be4c7f549a2ec7328d24ecc0a54fbf90d41c": {"ta_keywords": "optimal control policies;policies wireless networks;control policies wireless;policy structure solving;determine optimal policies;policies wireless;optimal policies;policy iteration;optimal control;develop optimal control;iteration policy;iteration policy iteration;methodsmarkov decision processes;value iteration policy;policy iteration employed;control policies;policy structure;solving mads large;solving mads;policies moderate complexity;methodsmarkov decision;optimal policies moderate;state space methodsmarkov;structure solving mads;wireless networks;wireless communication networks;communication networks goal;develop optimal;space methodsmarkov decision;mads large state", "pdf_keywords": ""}, "4da018847a0f44378e6a1ded93fee672a3c7c370": {"ta_keywords": "speech recognition ar;connectionist temporal classification;predict connectionist temporal;speech recognition;automatic speech recognition;mask predict connectionist;automatic speech;temporal classification cc;predict connectionist;connectionist temporal;based mask predict;temporal classification;mask predict;deployment automatic speech;anr based mask;classification cc mask;classification cc based;recognition ar;cc mask ct;connectionist;classification cc;recognition ar desired;mask ct;capable fast inference;cc based mask;mask ct achieves;fast inference;anr based;cc mask;recognition", "pdf_keywords": "speech decoder trained;sequences decoder trained;decoder isinferring speech;speech decoder;masked sequences decoder;decoder trained predict;inference speech decoder;isinferring speech processing;speech processing challenging;connectionist temporal classification;mask predict connectionist;predict connectionist temporal;predict masked sequences;decoder trained;speech recognition new;task speech processing;networks automatic speech;automatic speech;masked language model;model speech recognition;recognition speech;speech recognition;decoding algorithm speech;speech recognition speech;trained predict masked;speech recognition ar;recognition speech recognition;improved decoding;sequences decoder;temporal classification cc"}, "abaf39dc9d1b156ddf387230611f5102378d052c": {"ta_keywords": "sequence model attention;input attention averaging;single input correction;correction new decoder;multi input attention;oc post correction;input correction;attention averaging;input attention;post correction;evidence decoding sequence;repeated texts large;repeated texts;attention applied single;texts large corpora;optical crp oc;corpora source noisy;input correction new;decoder;decoding;decoding sequence;model attention applied;exploits repeated texts;decoding sequence sequence;optical crp;attention averaging developed;corpora;new decoder;evidence decoding;model attention", "pdf_keywords": "correction attention based;error correction attention;algorithm text correction;recognition ocr correction;ocr post correction;ocr correction;correction digital corpora;text correction;function recognition ocr;attention based sequence;text correction digital;sequence model attention;text processing;correction attention;accuracy corrected text;processing text;approaches recognition text;recognition text;ocr correction rui;correction performance corpora;implement unsupervised correction;processing text fundamental;attention based algorithm;corrected text improved;correction duplications bootstrapping;approach ocr post;unsupervised correction;texts large corpora;exploits repeated texts;ocr important tool"}, "63bfe58735f44b0af24da3c2cb6b1651b001b83c": {"ta_keywords": "complexity secret sharing;algorithm secret sharing;secret sharing networks;deterministic network coding;secret sharing network;communication complexity secret;secret sharing;sharing networks satisfy;distributed deterministic network;complexity secret;algorithm secret;network coding;efficient distributed deterministic;sharing networks;bounds communication complexity;network coding problem;communication efficient distributed;distributed deterministic;algorithm communication efficient;efficient distributed;admitting distributed deterministic;present algorithm secret;deterministic network;communication complexity;sharing network independent;networks satisfy propagating;lower bounds communication;satisfy propagating dealer;algorithm communication;distributed deterministic solution", "pdf_keywords": ""}, "5d69380565aa258bfa54005c9ba05e30675be227": {"ta_keywords": "hierarchical semi supervised;semi supervised classification;hierarchical classification;semi supervised;methods hierarchical classification;supervised classification tasks;classification tasks;flat classification task;flat classification;supervised classification;limited flat classification;semantic drift seeded;classification;learning methods hierarchical;introduction hierarchical semi;supervised;seeded classes explorratory;classification task;non hierarchical semi;hierarchical semi;classification task methodsthis;classification tasks incomplete;classes explorratory learning;drift seeded classes;semantic drift;classification tasks presence;exploratory learning methods;seeded classes;classes cause semantic;non hierarchical", "pdf_keywords": ""}, "62d6ccd01c2e022a385add5e689b4561b0fbfd88": {"ta_keywords": "speaker diarization method;based speaker diarization;speaker diarization;introduction speaker diarization;novel speaker diarization;speaker diarization important;network based speaker;overlapped speech methodsin;speech methodsin;overlapped speech;speech applications;based speaker;diarization systems;standard diarization systems;diarization method;step speech applications;speech methodsin paper;diarization systems achieve;speech applications aims;speaker;diarization;propose novel speaker;diarization method region;standard diarization;deal overlapped speech;diarization important pre;processing step speech;novel speaker;diarization important;region proposal network", "pdf_keywords": "speech segment proposals;segment proposals speaker;segment proposals audio;detecting speech segment;detect speech segments;diarization prediction audio;speaker classification boundary;proposals speaker embedding;based speaker diarization;speaker diarization;speaker diarization using;speech segment;speaker diarization speech;overlapped speech segment;speech diarization challenge;speech segments;introduction speaker diarization;novel speaker diarization;study speaker diarization;detecting speech;speaker diarization method;speaker embedding extraction;perform speaker classification;speech diarization better;effective detecting speech;speech diarization;proposals audio;diarization speech diarization;framework speech segment;speaker diarization important"}, "9ba545841b837fa077579290e252eb00351ebeb0": {"ta_keywords": "convex distributed learning;distributed learning;distributed learning heterogeneous;communication compression;communication compression strategy;novel communication compression;based compression gradient;heterogeneous datasets resultsmarina;compression gradient;learning heterogeneous;learning heterogeneous datasets;compression gradient differences;compression strategy based;communication efficient method;strategy based compression;compression strategy;based compression;datasets resultsmarina employs;heterogeneous datasets methodswe;new communication efficient;datasets resultsmarina;non convex distributed;convex distributed;communication efficient;heterogeneous datasets;compression;distributed;method non convex;learning;marina new communication", "pdf_keywords": "distributed learning compressed;distributed learning compression;nonconvex distributed learning;distributed learning faster;convex optimization compression;convex distributed learning;federated learning compression;new distributed optimization;distributed optimization methods;distributed optimization;learning compression diverges;learning compression feasible;learning compressed;distributed learning copression;distributed learning;distributed optimization communication;distributed learning described;distributed learning requires;distributed distributed learning;distributed learning algorithm;method distributed optimization;distributed learning derive;learning compressed data;distributed learning new;distributed learning present;learning compression non;distributed learning presented;distributed learning method;distributed learning 12;learning compression"}, "ea1f61270480a8dec54ec571c0e6ce116d096241": {"ta_keywords": "polyphonic sound event;sound event detection;sound event;model hybrid polyphonic;hybrid polyphonic sound;hybrid polyphonic;memory hidden markov;polyphonic sound;hidden markov;polyphonic;hidden markov model;propose polyphonic sound;propose polyphonic;backgroundconvolutional bidirectional long;short term memory;recurrent neural network;study propose polyphonic;network hidden markov;backgroundconvolutional bidirectional;memory recurrent neural;hybrid convolutional bidirectional;recurrent neural;event detection;convolutional bidirectional long;event detection methodsin;markov model hybrid;event detection method;based hybrid convolutional;memory recurrent;markov", "pdf_keywords": ""}, "598321d9c3eb5c035b449e19e539b6fa04b3802a": {"ta_keywords": "new platform web;platform web;platform web designed;web important resource;sites facilitate search;web important;search new sites;web designed;web;web designed facilitate;facilitate search new;designed facilitate search;facilitate search;new sites facilitate;sites facilitate;developed new platform;new sites;new platform;development new technologies;search new;new technologies;new technologies technologies;platform;search;technologies developed new;technologies technologies developed;technologies technologies;technologies;technologies developed;sites", "pdf_keywords": ""}, "8f1f43408baf1ccb0ec3e7985592326c83ee276d": {"ta_keywords": "chat oriented dialog;dialog methodsthis;statistical machine translation;dialog methodsthis paper;approaches chat oriented;oriented dialog methodsthis;based approaches chat;machine translation;machine translation smt;approaches chat;building chat oriented;dialog;translation smt based;approaches building chat;translation smt approaches;building chat;chat oriented;oriented dialog;dialog investigates combined;oriented dialog investigates;dialog investigates;example based statistical;translation smt;chat;compare example based;example based;compare example;statistical machine;based statistical machine;example based ebdm", "pdf_keywords": ""}, "07a9f47885cae97efb7b4aa109392128532433da": {"ta_keywords": "attention heads encoder;coded attention variant;attention based models;efficient attention based;efficient attention;hard coded attention;attention variant;attention variant learned;coded attention;attention based;attention heads;development efficient attention;learned self attention;self attention heads;attention achieving;attention;multi headed attention;headed attention achieving;heads encoder decoder;achieving high translation;self attention;attention achieving high;heads encoder;headed attention;high translation quality;encoder;translation quality push;transformation transformation fundamental;encoder decoder;transformation fundamental", "pdf_keywords": "translation use automated;highly efficient translation;neural machine translation;machine translation report;machine translation;machine translation challenging;attention improve translation;efficient translation quality;machine translation association;machine translation use;translation challenging task;efficient translation;translation systems;machine translation development;machine translation systems;machine translation number;attention important decoder;translation development neural;translation association computational;machine translation linear;improve translation quality;translation quality hard;machine translation impact;attention decoding speedup;good translation quality;hard coded attention;optimization multiheaded attention;attention decoding;translation systems article;improve translation"}, "8afbc4188be9e9452ce1fe868ebe217179d36793": {"ta_keywords": "speech recognition ar;recognition ar noise;speech recognition general;automatic speech recognition;speech recognition;performance automatic speech;speak background noise;automatic speech;stochastic approach speech;speech recognition systems;approach speech recognition;ar noise;noise speaker variations;ar noise popular;speakers speak background;background noise speaker;background noise;speaker variations;noise;noise speaker;condition speakers speak;predict kind acoustic;speaker variations great;speakers speak;recognition ar;acoustic condition;speak background;noise popular;speaker;kind acoustic condition", "pdf_keywords": ""}, "3426fadf73a5ce418486e640b26b3d2470d932b5": {"ta_keywords": "multilingual adversarial speech;multilingual adversarial;massively multilingual adversarial;adversarial speech recognition;adaptation multilingual end;adaptation multilingual;languages dimensions phonetics;adversarial speech;target pretraining languages;pretraining languages dimensions;trained 100 languages;pretraining languages;end speech recognition;speech recognition models;speech recognition important;dimensions phonetics phonology;massively multilingual;phonetics phonology;phonetics phonology language;speech recognition;dimensions phonetics;multilingual;phonetics;100 languages findings;speech recognition report;multilingual end;100 languages;languages dimensions;languages findings;report adaptation multilingual", "pdf_keywords": "transfer multilingual auditory;multilingual adversarial speech;multilingual auditory recognition;speech recognition multilingual;multilingual speech recognition;pretraining multilingual models;multilingual pretraining models;language adversarial pretraining;multilingual model speech;phoneme language adversarial;adversarial speech recognition;multilingual adversarial;adaptation multilingual endto;massively multilingual adversarial;multilingual auditory;massively multilingual pretraining;pretraining multilingual;multilingual pretraining useful;multilingual speech;adaptation multilingual;multilingual pretraining;multilingual knowledge transfer;paired language adversarial;transfer multilingual;use multilingual speech;development multilingual speech;multilingual pretraining important;knowledge transfer multilingual;transfer unseen languages;use pretraining multilingual"}, "8de431e0e62653711136836642af38179731c2f0": {"ta_keywords": "erasure coded distributed;secure errorasure codes;codes distributed storage;coded distributed storage;errorasure codes distributed;coding schemes repair;erasure coded;theoretically secure errorasure;operations erasure coded;coded distributed;secure errorasure;codes distributed;errorasure codes;distributed storage systems;distributed storage;repair algorithms;implementing repair operations;repair operations erasure;ensure security data;repair algorithms ensure;distributed storage important;coding schemes;security data presence;storage systems;implementing repair;security data;schemes repair algorithms;coded;operations repair operations;storage systems involve", "pdf_keywords": "secure distributed storage;security distributed storage;distributed storage codes;distributed storage eavesdropping;secrecy distributed storage;codes distributed storage;distributed storage erasure;storage codes distributed;adversaries secure codes;algorithms distributed storage;storage codes approach;constructing secure distributed;secure codes presented;secure codes;efficiently distributed storage;secure distributed;method distributed storage;distributed storage;storage erasure coded;stored passive eavesdroppers;ensuring security eavesdroppers;distributed storage special;distributed storage discussed;distributed storage systems;multicast networks secure;designing secure distributed;operations distributed storage;systems distributed storage;recovery distributed storage;approach distributed storage"}, "fdf1aec2da3597010c31138159574b1016019f73": {"ta_keywords": "computational social choice;group decision making;empirical computational social;social choice preference;preference reasoning uses;preference reasoning;computational social;group decision;choice preference reasoning;social choice;choice preference;humans empirical computational;voting resource allocation;topics group decision;preference;empirical computational;decision making;empirical computer scientists;bringing humans empirical;allocation methodsi argue;generally algorithms complexity;humans empirical;computational;voting resource;generally algorithms;allocation methodsi;areas including voting;science generally algorithms;algorithms complexity;resource allocation methodsi", "pdf_keywords": ""}, "538deb39d57bef62833c492a56c796a2bafa340f": {"ta_keywords": "machines active learning;active learning using;active learning;vector machines active;learning active learning;active learning active;active learning support;learning active;tool active learning;support vector machines;learning support vector;active learning objectivethe;backgroundactive learning;entity recognition;backgroundactive learning support;entity recognition treated;named entity recognition;use support vector;support vector;vector machines important;shallow parsing;learning using linear;vector machines;kernels methodsin experiments;tackled shallow parsing;learning support;learning using;function kernels;basis function kernels;machines active", "pdf_keywords": ""}, "63e7e3b16e03da62a2c535ac9cfccfa3ae48b292": {"ta_keywords": "ai advised decision;decision making ai;ai systems recommend;ai advised;known ai advised;argue accurate ai;ai;ai practitioners;ai systems;making ai;accurate ai;human expert responsible;actions human expert;human expert;ai team mate;accurate ai team;making ai practitioners;known ai;ai practitioners deploy;context known ai;healthcare ai;healthcare ai systems;ai team;expert responsible;advised decision making;expert responsible final;responsible final decisions;decisions context known;finance healthcare ai;advised decision", "pdf_keywords": "human ai teams;human ai teamwork;ai teamwork;ai teamwork fundamental;ai teamwork effective;ai teams;ai recommendations human;human ai effectiveness;ai effectiveness automated;ai best teammatethe;team utility human;humancentered ai accurate;human ai;ai human;ai accurate mental;decisions accurate ai;models human ai;humancentered ai;team efficiently optimized;ai teamwork established;accurate ai best;human human ai;ai teams use;ai best;accuracy automatic ai;ai recommendations;automatic ai;automated ai recommendations;useful human ai;ai community"}, "b2baf9e053c32abfb3c8658b9bc6d6790ae671cb": {"ta_keywords": "reading using eye;gaze based unknown;eye tracking features;gaze duration word;eye gaze based;reading using svms;detect unknown words;gaze based;word detectingion;using eye tracking;eye gaze;eye tracking;native language reading;unknown word detectingion;words natural reading;gaze duration;gaze;svms random forests;background eye gaze;natural reading using;approach utilizes gaze;language reading using;utilizes gaze duration;reading improve detection;utilizes gaze;language reading;natural reading;word rarity features;word detectingion non;using eye", "pdf_keywords": ""}, "15931520cce546bbf19b4cebeb4161c4debeabe7": {"ta_keywords": "multi winner approval;winner approval voting;decisions using voting;ballot consisting approvals;approval voting agent;collective decisions using;approval voting;collective decisions;employing voting;elections employing voting;winner approval;multiple winners multi;votes choosing candidates;employing voting rules;votes choosing;multi winner;voting agent;voting scenarios;winners multi winner;using voting scenarios;consisting approvals candidates;situations collective decisions;voting agent submits;voting scenarios committee;multiple winners;tallying votes choosing;ballot;approvals candidates;ballot consisting;agent submits ballot", "pdf_keywords": "approval voting heuristics;heuristics models voter;voting heuristics;voting heuristics included;decisions using voting;heuristics measure voter;decision making voting;voting environments;approval voting environments;approval voting scenarios;heuristics multi winner;voting environments challenging;winner approval voting;using voting scenarios;voting scenarios maintaining;approval voting paradigm;voting uncertainty;employing voting rules;models voter behavior;model voter behavior;approval voting uncertainty;employing voting;voting scenarios;accuracy heuristics model;model voter;voting uncertainty context;information voting scenarios;votes choosing candidates;multi winner approval;voting paradigm"}, "931cbd9d689e9fd6bd91f4e8e1dbdd7fbb6df9de": {"ta_keywords": "recognize speech mixtures;automatic speech recognition;speech recognition ar;speech recognition;automatic speech;speech features sequence;inserting speech separation;sequence speech features;speech mixtures inserting;speech mixtures;speech separation;speech separation mechanism;end automatic speech;mixtures inserting speech;extended recognize speech;recognize speech;speech features;sequence speech;inserting speech;maps sequence speech;output recognition;training decoding;training decoding pipelines;output recognition results;single neural network;allowing output recognition;neural network;systems extended recognize;recognition ar directly;extended recognize", "pdf_keywords": ""}, "a445adf335aa5212f929f67c1ca56a62c221b43a": {"ta_keywords": "discovery unknown unknowns;problem informed discovery;introductionpredictive models deployed;informed discovery unknown;unknown unknowns;models blind errors;informed discovery;mismatch training data;needed identify failures;introductionpredictive models;training data cases;unknowns;identify failures;instances high confidence;model incompleteness;errors unknown unknowns;model incompleteness typically;incompleteness;high confidence errors;incompleteness typically arise;incompleteness typically;discovery unknown;blind errors;discovery;problem informed;oracle needed identify;training data;arise mismatch training;models blind;rooted model incompleteness", "pdf_keywords": "discovering unknown unknowns;unknowns training data;approach discovering unknown;training data discovery;unknown unknowns predictive;recognize unknown unknowns;unknown unknowns semantically;unknown unknowns training;discovering unknown;unknowns predictive;unknowns predictive models;unknowns partitions leveraging;unknowns vary training;training data predictive;unknowns training;unknowns semantically meaningful;searching unknown unknowns;unknown unknowns formalize;unknowns semantically;unknown unknowns challenging;problem informed discovery;training data cases;leveraging feedback oracle;discovery unknown unknowns;mining knowledge;recognize unknown;cause unknown unknowns;unknowns challenging task;unknown unknowns use;approach discovering"}, "947750c717a5fbd17fc52758322d1ca201c4c6bc": {"ta_keywords": "safety information mining;safety people disaster;neurology nl disaster;mining nonlidiabetic neurology;information mining nonlidiabetic;safety information;people disaster stricken;information regarding safety;introduction safety information;disaster stricken;earthquake earthquake;people disaster;regarding safety people;earthquake;neurology;stricken area twitter;disaster stricken area;nl disaster aimthe;disaster aimthe;earthquake methodsspecifically created;earthquake earthquake methodsspecifically;nonlidiabetic neurology;nonlidiabetic neurology nl;regarding safety;information mining;safety people;create aid relief;neurology nl;nonlidiabetic researchers create;disaster", "pdf_keywords": ""}, "c45a23e7c565169c5a55898683aceac458c116bb": {"ta_keywords": "chime speech separation;speech separation recognition;3rd chime speech;3rd chime challenge;chime challenge using;recognition challenge chime;speech recognition;advanced speech recognition;extraction advanced speech;chime speech;speech separation;chime challenge;multi microphone signals;3rd chime;designed 3rd chime;challenge chime resultstwo;beamforming robust feature;multi microphone;si 3rd chime;combine multi microphone;microphone signals;challenge chime;speech recognition methodsthis;separation recognition challenge;microphone;separation recognition;challenge using beamforming;using beamforming robust;beamforming robust;microphone signals obtain", "pdf_keywords": ""}, "2b0aa68ef2c1773642ca91627a4fc03f536cc5fc": {"ta_keywords": "symptomatic malignant asymptomatic;malignant asymptomatic symptomatic;asymptomatic diagnosed malignant;malignant asymptomatic;asymptomatic malignant asymptomatic;asymptomatic malignant;asymptomatic symptomatic history;history symptomatic malignant;asymptomatic diagnosed;malignant asymptomatic malignant;symptomatic history asymptomatic;asymptomatic asymptomatic diagnosed;asymptomatic symptomatic;symptomatic malignant;asymptomatic asymptomatic;asymptomatic asymptomatic asymptomatic;diagnosed malignant;asymptomatic;history asymptomatic asymptomatic;diagnosed malignant neo;malignant neo;malignant;diagnosed;symptomatic;history asymptomatic;symptomatic history;present case patient;history symptomatic;patient history symptomatic;case patient", "pdf_keywords": ""}, "f762ce106b37728df1126375981a02a589e0497c": {"ta_keywords": "stochastic gradient descent;proximal stochastic gradient;theory stochastic gradient;stochastic gradient;gradient descent;gradient descent ggs;proximal stochastic;variants proximal stochastic;unified theory stochastic;gradient descent tt;theory stochastic;stochastic;gradient;intuitions convergence analyses;intuitions convergence;convergence analyses;different intuitions convergence;descent ggs;descent ggs described;descent;variants proximal;proximal;convergence;introduction unified theory;introduction unified;unified analysis;communities methods;convergence analyses different;unified analysis large;communities methods framework", "pdf_keywords": "stochastic gradient descent;stochastic gradient optimization;stochastic gradients sgds;methods stochastic gradient;method stochastic gradients;optimization sgd methods;constructing stochastic gradients;stochastic gradient optimum;stochastic gradient method;stochastic gradients framework;stochastic gradients method;stochastic gradients;gradient descent sgd;gradient optimization sgd;theory stochastic gradients;stochastic gradient gradient;gradients stochastic gradients;stochastic gradients stochastic;stochastic gradients arbitrary;based stochastic gradients;proximal stochastic gradient;analysis stochastic gradients;unbiased stochastic gradient;stochastic gradient;theory stochastic gradient;gradients stochastic;stochastic gradients present;evaluation stochastic gradients;gradient stochastic;gradients arbitrary sampling"}, "ec084dac14a069da2e924ff7f3d5d2fb75b9b39a": {"ta_keywords": "sentiment sufficient dimension;multinomial inverse regression;sentiment variables;sentiment variables research;multinomial inverse;methods multinomial inverse;dimension reduction text;connected sentiment variables;reduction text data;sentiment sufficient;multinomial;data methods multinomial;data connected sentiment;dimension reduction;methods multinomial;sentiment;connected sentiment;text data methods;sufficient dimension reduction;inverse regression;text data;inverse regression introduced;framework sentiment sufficient;marketing economics high;reduction text;economics high dimensional;straightforward framework sentiment;high dimensional;simplifying predictor sets;dimensional", "pdf_keywords": "sentiment sufficient dimension;multivariate sentiment text;text sentiment response;sentiment variables;multivariate sentiment;text given sentiment;approach multivariate sentiment;text sentiment;text sentiment analysis;approach text sentiment;text analysis multinomial;text regression;sentiment variables research;sentiment text;sentiment response;regression text analysis;multinomial inverse regression;text regression text;analysis sentiment text;text regression robust;sentiment analysis;sentiment response factor;method text sentiment;regression text;sentiment sufficient;sentiment text apply;rating inverse regression;framework text regression;sentiment text present;accuracy text regression"}, "bc251481aa5566b1e86a8dbd0417cdf858205e3b": {"ta_keywords": "patterns fake news;fake news detection;fake news researchers;fact based fake;fake news posts;defend fake news;based fake news;news posts claim;fake news;based fake;news detection model;methods defend fake;fact based;veracity considering patterns;news posts;claim veracity;claim veracity considering;news researchers;news detection;defend fake;verify claim veracity;news researchers developed;patterns fake;posts claim result;posts claim;veracity considering;shared patterns fake;model preference learning;sources verify claim;detection model preference", "pdf_keywords": "detecting fake news;detect fake news;detection fake news;fake news detection;patterns fake news;analyzing fake news;detecting false news;detection false news;prediction fake news;fake news fundamental;fake news posts;fact based fake;method fake news;characteristics fake news;fake news researchers;news posts claim;defend fake news;model fake news;fake news works;based fake news;joint fake news;detecting fake;news use pattern;fake news developed;news detection models;posts claim fact;news posts pattern;focusing fake news;detect fake;fact based models"}, "dc2f6f092fa04e334dfe2e8592b6d597e00b97ca": {"ta_keywords": "hypernymy extraction distributional;helpful extraction hypernyms;distributional semantic classes;hypernymy extraction;extraction hypernyms;backgroundimproving hypernymy extraction;extraction distributional semantic;distributionally induced semantic;extraction hypernyms methodswe;distributional semantics;distributional semantics using;distributional semantic;using distributional semantics;induced semantic classes;hypernymy relations resultsdenoising;noisy hypernymy relations;semantic classes;semantic classes helpful;aware semantic classes;semantic classes filtering;hypernyms;induced semantic;sense aware semantic;semantic classes using;using induced semantic;hypernymy relations;semantic classes paper;hypernyms methodswe present;extraction distributional;semantic", "pdf_keywords": "semantics denoising hypernyms;useful extraction hypernyms;extraction hypernyms;distributional semantics denoising;distributional semantics graph;hypernymy extraction;hypernymy extraction challenging;semantic class hypernyms;distributional semantics;inducing semantic classes;noisy hypernymy relations;extract hypernyms;using distributional semantics;inducing semantic;extracted lexical semantic;extraction hypernyms text;method extraction hypernyms;word sense induction;extract hypernyms single;sense aware semantic;extraction hypernyms textwe;words sense cluster;semantics denoising;hypernyms improved;existing lexical semantic;noisy hypernymy databases;extracted semantic classes;hypernymy cooccurrences lexical;lexical semantic;method inducing semantic"}, "02a83a01d6236149e4ead01e202b2453f9590e9e": {"ta_keywords": "group fairness notions;group fairness;introductiongroup fairness;fairness class fairness;introductiongroup fairness class;class fairness notions;fairness notions;majority group fairness;class fairness;group fairness resultswe;fairness class;fairness notions measure;offs group fairness;fairness resultswe;fairness resultswe observe;fairness;groups individuals treated;individuals treated differently;different groups individuals;groups individuals;according protected attributes;different groups;treated differently according;individuals treated;measure different groups;groups;treated differently;majority group;protected attributes;differently according protected", "pdf_keywords": ""}, "c263507db2c15a8b2e3c955bda7b3c29a1ebd106": {"ta_keywords": "intent detection wild;wild intent detection;intent detection systems;intent detection;detection wild intent;live chatbots diverse;chatbots diverse domains;live chatbots;created live chatbots;chatbots diverse;chatbots;wild intent;bar intent detection;intent;bar intent;intent unintended;intent unintended correlations;varying perception intent;detection wild;perception intent unintended;perception intent;complexities imbalanced datasets;raising bar intent;imbalanced datasets;diverse domains resultscompared;imbalanced datasets containing;detection systems real;diverse domains;detection;datasets", "pdf_keywords": "live chatbots diverse;chatbots diverse domains;live chatbots;chatbots diverse;chatbots;created live chatbots;web based dialogue;dialogue systems far;live chat bots;dialogue systems;based dialogue systems;chat systems models;live chat systems;chat systems;crowdsourced datasets;existing datasets crowdsourced;dataset intent detection;chat bots;datasets crowdsourced datasets;datasets crowdsourced;natural language far;natural language association;based dialogue;complexities imbalanced datasets;detection new datasets;crowdsourced datasets contain;chat bots period;imbalanced datasets;performance live chat;natural language"}, "9bd170248355047067f05349d57110cc8e4de5cf": {"ta_keywords": "web important resource;web resource web;web web search;web resource;resource web web;web web;web search useful;important resource web;web important;resource web;web search;resource web resource;web based web;web web based;based web web;web;based web;web based;search useful;important resource;resource;search useful user;search;useful;useful user;important;based;user", "pdf_keywords": ""}, "9952fe8cbd09e4fc89dc7d76595d138e36c7d7b5": {"ta_keywords": "neural ranking models;neural ranking;ranking models english;based neural ranking;models english datasets;evaluation transferability baffer;english datasets;english datasets methods;ranking models;transferability baffer;baffer based neural;ranking;training data important;systematic evaluation transferability;transferability baffer based;high annotation costs;training data;evaluation transferability;shot transfer;shot shot transfer;models english;high annotation;neural;shot transfer large;annotation costs;baffer;dataset small;baffer based;based neural;datasets", "pdf_keywords": "bertbased neural ranking;ranking models english;retrieval question answering;neural ranking models;neural ranker retrieval;question answering annotated;neural ranking;ranker retrieval;rank answers web;transferability bertbased neural;evaluation transferability bertbased;question answering;annotated queries;bertbased neural;query queries trained;queries trained;neural information retrieval;answering annotated data;ranker retrieval question;models english datasets;learning rank;ranking models;learning rank models;neural ranker;annotated queries fine;training human annotated;answering annotated;transferability bertbased;information retrieval;high annotation costs"}, "1f76ee8472ec3a41511540f62e4676317df14ea5": {"ta_keywords": "perceptual age singing;singing voice conversion;singing voice age;age singing voice;voice timbre control;singing voice;voice timbre;age singing;voice age;voice timbre varieties;voice conversion;timbre varieties voices;voice age singer;controlling prosody voice;sing expressively controlling;prosody voice timbre;methods singers sing;voice conversion sv;prosody voice;age singer perceived;methods singers;novel voice timbre;sing expressively;sv methods singers;singers sing expressively;voice;voices;singing;varieties voices;singers sing", "pdf_keywords": ""}, "5a5fb155b5fc518389a7fe67b55271e143ad695d": {"ta_keywords": "community recovery hypergraphs;community recovery graphs;segmentation community recovery;recovery hypergraphs studied;recovery graphs;recovery graphs extensively;recovery hypergraphs;recovery graphs popular;problem community recovery;community recovery;segmentation community;hypergraphs studied;hypergraphs;clustering;graphs extensively studied;engineering community recovery;hypergraphs studied methods;clustering core;graphs extensively;clustering core problem;image segmentation community;graphs;clustering received;data clustering;data clustering received;data clustering core;graphs popular approach;context data clustering;graphs popular;clustering received significant", "pdf_keywords": "recovery hypergraphs generalized;complexity exact recovery;community recovery hypergraphs;recovery hypergraphs minimal;complexity exists recovery;recovery hypergraphs;hypergraphs minimal sample;network clustering motion;minimal sample complexity;face clustering protein;clustering algorithm recover;community detection discussed;face clustering;community detection stochastic;clustering optimal;network clustering;minimum sample complexity;community detection;recovery algorithm;retrieval community recovery;clustering protein complex;contextcommunity recovery;exists recovery algorithm;hypergraphs generalized censored;hypergraphs minimal;sample complexity exact;sample complexity exists;sample complexity;clustering protein;hypergraphs generalized"}, "17c7c92db1f4ace842f9db6b44bfce264308b628": {"ta_keywords": "learned phrase vectors;phrase vectors deep;phrase vectors;labels using learned;recursive autoencoders;outside recursive autoencoders;induce span labels;recursive autoencoders dor;autoencoders;using learned phrase;meaningful constituent spans;vectors deep;learned phrase;vectors deep inside;span labels additionally;spans noun phrases;improve model labeling;span labels;cluster span representations;autoencoders dor;labeling;model labeling;phrases work effectively;identifying meaningful constituent;autoencoders dor methods;understanding text;constituent spans noun;constituent spans;span representations;latent code", "pdf_keywords": ""}, "753fd6952c9f06f3bbd46e37129acc3f7a984896": {"ta_keywords": "text generation systems;generating informative utterances;text generation;textual knowledge corpus;introduction text generation;generating informative;retrieve passages textual;like generating informative;corpus wikipedia providing;knowledge corpus;knowledge corpus wikipedia;retriever generator;wikipedia providing passages;generation tasks like;informative utterances conversations;generation tasks;tasks like generating;generator underperform retriever;passages textual knowledge;additional context generator;context generator;retriever generator underperform;textual knowledge;utterances conversations;informative utterances;corpus wikipedia;retrieve passages;textual;retriever retrieve passages;like generating", "pdf_keywords": "retrieval generator;retrieval generator trained;retrieval augmented generation;demonstrate retrieval generator;retriever generator retrieval;friendly text generation;generator retrieval augmented;generator retrieval;supervision retriever generator;text generation existing;better retriever generator;text generation;useful retrieval target;generator better retriever;generation tasks using;generator underperform retriever;passages corpus retriever;generator retriever;retrieve passages corpus;retriever generator;retriever generator underperform;generation task using;generation method retrieve;corpus retriever jointly;retrieval target output;generation tasks;generate utterances informative;retriever generator generator;output tasks generator;corpus retriever"}, "7fa3a5318ac45b2fd93a0130f0ceba9995ffa3c0": {"ta_keywords": "computing cross cultural;common cross lingual;cross cultural similarities;lingual natural language;slang terms languages;cross lingual;cultural similarities methods;cross cultural differences;cross lingual natural;cultural similarities;cultural differences similarities;cross cultural;entity understanding slang;knowledge cross cultural;people distinct cultures;understanding slang terms;language understanding especially;language understanding;background cross cultural;differences similarities common;similarities common;distinct cultures;lingual;cultures hold different;slang terms;natural language;natural language understanding;cultural differences;understanding slang;distinct cultures hold", "pdf_keywords": ""}, "8306e4a566e2b1279d5d67b40facc8e1e345c4e3": {"ta_keywords": "communication compression strategies;communication compression;enhanced communication compression;contractive compression operators;compression strategies;contractive compression;compression strategies based;distributed gradient based;compression operators existing;compression operators;application contractive compression;convergence distributed gradient;gradient based optimization;distributed gradient;error feedback proposed;compression;modern error feedback;optimization methods enhanced;optimization methods;based optimization methods;error feedback popular;whispers practical algorithmic;heuristic error feedback;error feedback;based optimization;enforcing convergence distributed;optimization;methods enhanced communication;gradient based;enhanced communication", "pdf_keywords": "distributed gradient descent;communication compression strategies;efficiently distributed gradient;communication compression;enhanced communication compression;stochastic gradient descent;bidirectional biased compression;compression strategies;contractive compression operators;convergence distributed gradient;gradient descent;contractive compression;biased compression contractive;distributed algorithm deep;distributed gradient based;reformulation stochastic gradient;gradient descent methods;compression operators;compression operators ability;convergence stochastic gradient;compression contractive;compression strategies based;biased compression;gradient based optimization;distributed gradient;gradient methods convergence;gradient descent rd;clientss compression f21;application contractive compression;compression method effective"}, "bad5d2d6d1f3282ebbcb602a6f3a5dd9488fd713": {"ta_keywords": "annotation based phonetic;phonetic annotation based;phonetic annotation;based phonetic annotation;recognition based phonetic;phonetic annotation incorporates;phoneme recognition;multilingual recognition;phoneme recognition using;multilingual recognition based;allosaurus method multilingual;annotation incorporates phonological;improving speech recognition;speech recognition;method multilingual recognition;speech recognition particularly;based phonetic;phonological knowledge language;phonological knowledge;phonetic;recognition using allosaurus;incorporates phonological knowledge;focus phoneme recognition;language dependent allophone;trained multiple languages;incorporates phonological;phonological;using allosaurus;multiple languages;multilingual", "pdf_keywords": "annotation based phonetic;phonetic annotation based;recognition based phonetic;phoneme recognition benchmark;based phonetic annotation;phonetic annotation;phoneme recognition;tuning universal phonetic;universal phonetic representations;phonetic representations efficient;phone recognition datasets;universal phonetic;phonetic representations;multilingual recognition;phonetic annotation evaluate;speech recognition great;phoneme recognition using;present phoneme recognition;phonetic representations method;phone recognition;multilingual recognition based;universal phone representations;phone recognizer task;speech recognition;curate phone recognition;phone representations;phone recognition developed;based phonetic;method multilingual recognition;focus phoneme recognition"}, "c64843e51f24773c895511ba9befa8a9bc4924a9": {"ta_keywords": "convert singing voice;singing voice characteristics;voice characteristics manipulating;statistical voice conversion;voice characteristics arbitrary;possible convert singing;control singing voice;arbitrary target singer;use statistical voice;statistical voice;varieties voices singers;voice conversion technique;convert singing;arbitrary source singer;intuitively control singing;singer arbitrary target;voice conversion;singer difficult intuitively;singing voice;controlling prosody vocal;sing expressively controlling;control singing;voice characteristics;voices singers produce;source singer arbitrary;voices singers;singers sing expressively;singer arbitrary;sing expressively;vocal timbre varieties", "pdf_keywords": ""}, "8ef0ca924ae88ac2bb2803c49589722b52efc5b4": {"ta_keywords": "affective interaction corpus;corpus social affective;affective interactions english;social affective interactions;social affective interaction;affective interaction;affective interactions;interaction corpus;interaction corpus english;introductionsocial affective aspects;presents corpus social;affective aspects interaction;introductionsocial affective;social affective;corpus social;corpus english indonesian;interactions english;interactions english india;development social affective;paper presents corpus;affective aspects;presents corpus;affective;english indonesian methodsthis;human communication;making human communication;corpus english;corpus;natural interaction;observing complex emotional", "pdf_keywords": ""}, "192fc995631e3443bb7f291a971089bd06e61017": {"ta_keywords": "ai2 reasoninging challenge;introduces ai2 reasoninging;ai2 reasoninging;reasoninging challenge associated;reasoninging challenge;knowledge reasoning;introduces ai2;reasoninging;ai2;types knowledge reasoning;knowledge reasoning required;work introduces ai2;challenge associated arc;annotation;annotation process;reasoning;arc dataset;associated arc dataset;annotation process used;questions easy set;challenge associated;types knowledge;questions easy;challenge set;labels annotation process;labels annotation;questions;arc;challenge set work;knowledge", "pdf_keywords": ""}, "39ffb5e9f2f36df42ef8ea010499e484c913e79e": {"ta_keywords": "historical language corpora;map historical wordforms;historical wordforms modern;historical wordforms;employ spelling normalization;language corpora projects;spelling normalization;language corpora;spelling normalization map;create corpora;growing historical language;historical language;corpora;normalization map historical;corpora projects;create corpora exist;wordforms modern ones;corpora exist;wordforms modern;projects create corpora;languages german scheible;slovene erjavec;slovene erjavec 2012;2010 slovene erjavec;corpora exist variety;slovene;wordforms;corpora projects create;languages german;2010 slovene", "pdf_keywords": ""}, "26f427d2b27828f2893e95344342570699e9c589": {"ta_keywords": "allow code conversions;code conversion introduce;code conversions;code conversion;introduce convertible codes;convertible codes;code conversions resource;code pairs allow;notion code conversion;code pairs;decodability constraint;pairs allow code;convertible codes new;md decodability constraint;formalize notion code;decodability constraint prove;separable md decodability;class code pairs;decodability;conversions resource efficient;conversion introduce convertible;allow code;md decodability;conversion introduce;new framework formalize;notion code;code;conversion;conversions resource;conversions", "pdf_keywords": ""}, "5c333f11431d1f0d04ced62b712c8d05ebac0891": {"ta_keywords": "speech enhancement algorithm;introduction speech enhancement;speech enhancement;speech enhancement critical;novel speech enhancement;lagging speech enhancement;speech signal diffusion;speech enhancement aim;observed noisy speech;noisy speech;speech synthesis;noisy speech signal;advances diffusion probabilistic;potential speech synthesis;enhancement algorithm incorporates;enhancement algorithm;synthesis lagging speech;diffusion probabilistic models;diffusion probabilistic;speech synthesis lagging;signal diffusion reverse;signal diffusion;lagging speech;diffusion reverse processes;advances diffusion;enhancement critical;diffusion reverse;speech signal;enhancement;recent advances diffusion", "pdf_keywords": "generative speech enhancement;generating speech enhancement;speech enhancement critical;speech enhancement;speech enhancement approaches;approach speech enhancement;speech enhancement problems;method speech enhancement;world speech enhancement;lagging speech enhancement;speech enhancement work;speech enhancement algorithm;novel speech enhancement;generative speech;speech enhancement using;compared generative speech;potential speech synthesis;speech synthesis;speech signal diffusion;speech enhancement time;speech enhancement telephone;process generating speech;synthesis lagging speech;generating speech;speech synthesis lagging;noisy speech;observed noisy speech;conditional diffusion reverse;conditional diffusion probabilistic;advances diffusion probabilistic"}, "c82854b8d4c715da141d34c73bf9bda67adf307c": {"ta_keywords": "political blogs;events political blogs;political blogs form;semisupervised latent variable;modelingpolarizing topics;modeling modelingpolarizing topics;modelingpolarizing topics important;multitarget semisupervised latent;semisupervised latent;latent variable model;news events political;present multitarget semisupervised;political discourse united;blogs;political discourse;multitarget semisupervised;topics;semisupervised;blogs form social;transformation political discourse;differently news events;latent variable;getting increasingly polarized;process political discourse;events political;discourse united states;increasingly polarized;news events;transformation political;process political", "pdf_keywords": ""}, "30a30781c66c758e8e59cdb00c368f3add99768b": {"ta_keywords": "unsupervised speaker representations;robust speaker recognition;unsupervised speaker;recognition models speaker;speaker recognition;speaker representations;speaker recognition models;speaker representations based;embeddings dissimilar utterance;train robust speaker;works unsupervised speaker;utterance embeddings dissimilar;utterance embeddings;separate speaker information;similar utterance embeddings;utterance embeddings similar;embeddings similar utterance;robust speaker;speaker information;encourage utterance embeddings;models speaker labels;models speaker;speaker information channel;contrastive learning;representations based contrastive;speaker labels research;based contrastive learning;speaker labels;separate speaker;difficult separate speaker", "pdf_keywords": "speaker embedding extractor;speaker embedding;effective speaker embeddings;augmentation adversarial training;training speaker discriminative;unsupervised speaker representations;speaker embeddings;present augmentation adversarial;embeddings speaker;explicitly train speaker;speaker discriminative environment;augmentation adversarial;embeddings speaker labels;invariant embeddings speaker;speaker embeddings self;unsupervised speaker;recognition models speaker;prevent speaker embedding;supervised speaker;self supervised speaker;propose augmentation adversarial;train speaker discriminative;learning speaker recognition;speaker recognition;robust speaker recognition;speaker representations;supervised speaker recognition;training speaker;learning speaker;encourage utterance embeddings"}, "84370f2fa3ac21ed3c6a30144fbdb377157b8853": {"ta_keywords": "conditional preferences probabilistic;model conditional preferences;preferences probabilistic uncertainty;preferences probabilistic;conditional preferences;conditional preferences multi;preferences multi agent;collection cytotoxic based;represent collection cytotoxic;probabilistic uncertainty methods;probabilistic uncertainty;antigen cp net;cytotoxic based antigen;cytotoxic based;nets multi agent;multi agent context;methods model conditional;probabilistic;based antigen cp;based antigen;collection cytotoxic;model conditional;uncertainty methods;used model conditional;multi agent;antigen;preferences multi;agent context compactly;antigen cp;conditional", "pdf_keywords": ""}, "d66110a315f5f216b42d99cfafec31e8e30a03ea": {"ta_keywords": "neural uncertainty estimation;target speaker extraction;using neural uncertainty;estimation target speaker;neural uncertainty;speaker extraction;uncertainty estimation performed;uncertainty estimation equipped;uncertainty estimation target;rn transducer device;uncertainty estimation;transducer equipped rn;speaker extraction equipped;rn transducer equipped;described neural uncertainty;equipped rn transducer;rn transducer;estimation equipped rn;transducer;transducer equipped;transducer device;target speaker;extraction equipped rn;uncertainty;transducer device described;speaker;estimation target;estimation equipped;using neural;estimation performed using", "pdf_keywords": ""}, "05d1e21f5f0c8209cc125f2e9ccd3a62d6479114": {"ta_keywords": "document similarity metrics;documents document similarity;document similarity;hoc document retrieval;document retrieval;similarity pairs documents;document retrieval text;information retrieval;information retrieval process;retrieval text classification;assessing similarity;similarity metrics used;assessing similarity pairs;similarity metrics;retrieval text;methods assessing similarity;classification summarization similarity;summarization similarity metrics;identification similarity;summarization similarity;identification similarity pairs;similarity pairs;similarity;pairs documents document;pairs documents;retrieval process;retrieval process complex;ad hoc document;retrieval;hoc document", "pdf_keywords": ""}, "f9409302c4d8201481fe65675bc6f0fa32e01df7": {"ta_keywords": "protect injury;injury essential development;protect injury essential;individual protect injury;development new therapies;new therapies;injury essential;therapies;ability individual protect;injury;individual protect;protect;ability individual;essential development new;ability;essential development;development new;development;essential;new;individual", "pdf_keywords": ""}, "59d3f6a14e20efdf54216188e227e58a351237e5": {"ta_keywords": "interpretable gan fingerprint;fake image attribution;gan fingerprint;gan fingerprint explain;obtain interpretable gan;interpretable gan;image attribution existinging;visual forensics malicious;image attribution mainly;image attribution;fake image;threats visual forensics;forensics malicious personation;gan;works fake image;malicious personation digital;generative models brought;generative models;visual forensics;attribution existinging works;attribution existinging;forensics malicious;generative;personation digital copyrightinfringement;existinging works fake;malicious personation;digital copyrightinfringement;attribution;digital copyrightinfringement promotes;attribution mainly", "pdf_keywords": "disentangle fingerprint gan;gan fingerprint disentangling;fingerprint gan generated;fingerprint gan;net gan fingerprint;analysis gan fingerprint;gan fingerprint withthe;gan fingerprint;propose gan fingerprint;qualified gan fingerprint;gan fingerprints data;gan fingerprint propose;novel gan fingerprint;fingerprint disentangling network;artificial gan fingerprints;gan fingerprints implications;gan fingerprints;gan fingerprint paper;fingerprint disentangling;gan network generalize;gan identify specific;analogue gan identify;identify specific gan;gan generated images;analysis gan;gan network;disentangle fingerprint generalized;gan identify;comprehensive analysis gan;gd net gan"}, "207c64b36fbd6accf7067366a251d071e8dd03a7": {"ta_keywords": "vascular flora mtvardoussia;flora mtvardoussia comprises;floristic affinity mtvardoussia;mtvardoussia neighbouring mountains;flora mtvardoussia;mountainous character flora;mtvardoussia comprises;vascular flora;thehemicryptophytes underlines mountainous;mtvardoussia neighbouring;flora;flora examination floristic;mtvardoussia comprises 1114;flora examination;mtvardoussia;thehemicryptophytes;affinity mtvardoussia neighbouring;character flora;thehemicryptophytes underlines;character flora examination;predominance thehemicryptophytes;predominance thehemicryptophytes underlines;affinity mtvardoussia;examination floristic affinity;floristic affinity;examination floristic;time predominance thehemicryptophytes;floristic;neighbouring mountains iti;comprises 1114 taxa", "pdf_keywords": ""}, "4f6d64eec6eaa38177ae45ad6315cf25d1535294": {"ta_keywords": "contextal question answering;encode conversation history;answer embedding;conversational search;answer embedding method;history answer embedding;conversation history position;question answering convq;encode conversation;question answering;setting conversational search;bovine text bert;method encode conversation;conversation history;conversation history understand;leverage conversation history;conversational search major;text bert;text bert natural;setting conversational;answering convq;bovine text;conversation;using bovine text;concrete setting conversational;conversational;bert natural way;positional history answer;answering;answering convq simplified", "pdf_keywords": "conversational question answering;conversational search previous;conversational retrieval;model conversational retrieval;conversation history modeling;modeling conversation history;conversational retrieval introduce;conversational search conversation;conversational ai;learning history attention;conversational search systems;search conversation history;conversational search;selection conversation histories;functional conversational search;history attention network;referred conversational search;history modeling conversation;encode conversation history;conversation histories model;conversation history using;intelligence conversational ai;approach conversation history;search conversation;handling conversation history;conversation histories;conversational ai begun;conversation histories qk;conversation models;conversation history"}, "e8135016ff3bd33ace936e50247fd650fcc58a7a": {"ta_keywords": "neural machine translation;machine translation model;translation model;machine translation;machine translation nm;translation nm models;discrete translation lexicons;use discrete translation;discrete translation;translation lexicons improve;translation lexicons;translation model make;lexicons improve probability;translation nm;translation;risk training optimize;standard neural machine;minimum risk training;neural machine;knowledge neural machine;knowledge neural;standard neural;use minimum risk;risk training;addition standard neural;minimum risk;lexicons improve;training optimize beu;improve probability estimates;current knowledge neural", "pdf_keywords": "translation based attentional;neural machine translation;machine translation;machine translation systems;model improve translation;machine translation nmt;improve translation accuracy;translation systems;translation accuracy;attentional neural machine;translation lexicons improve;translation systems developed;translation nmt models;translation accuracy present;neural mrna algorithm;translation accuracy implement;neural mrna model;improve translation;approach neural mrna;translation lexicons;translation track;translation systems article;discrete translation lexicons;english translation track;translation based;translation track 2016;attentional neural;mrna model useful;translation nmt;evaluation attention"}, "7847419becbc04596b79f804f844cf9719e875ea": {"ta_keywords": "unannotated demonstrations parsing;learning hierarchical policies;demonstrations parsing;hierarchical policies demonstrations;demonstrations parsing demonstration;level actions train;actions train;sparse natural language;learning hierarchical;level subtask descriptions;actions train model;action sequences goals;unannotated demonstrations;subtask descriptions descriptions;low level actions;subtask descriptions;level actions;primarily unannotated demonstrations;model action sequences;policies demonstrations using;parsing demonstration;skills autonomous;reusable skills autonomous;generative model action;action sequences;descriptions generate;descriptions generate sequences;high level subtask;skills autonomous decision;level subtask", "pdf_keywords": "learning hierarchical policies;hierarchical policy learning;training hierarchical policies;natural language supervision;policies trained sparse;policy learning;hierarchical policies demonstrations;language supervision learning;language supervision;train hierarchical policies;sparse natural language;policy learning approaches;hierarchical policies predict;demonstrations sparsely annotated;language policy representation;policies trained;supervision development hierarchical;based policy learning;training hierarchical;learning hierarchical;supervised training hierarchical;supervision learning;hierarchical policy;language supervision introduce;hierarchical policies using;description hierarchical;overview hierarchical learning;hierarchical policies;hierarchical policies outperform;policies predict sequence"}, "ad734a3f530a6c338af6bf2bf678e5af05477c1a": {"ta_keywords": "distributed secret sharing;algorithm secret sharing;secret sharing general;secret sharing;introductionefefficient distributed secret;secret sharing dealer;distributed secret;problem secret sharing;sharing dealer;distributed algorithm secret;sharing dealer does;satisfy propagating dealer;sharing general networks;algorithm secret;propagating dealer condition;consider problem secret;propagating dealer;networks satisfy propagating;dealer participants;dealer condition derive;efficient distributed algorithm;derive information theoretic;efficient distributed;sharing;participants instead dealer;present efficient distributed;problem secret;cryptographic protocols;sharing general;distributed algorithm", "pdf_keywords": ""}, "11154216ca898590e7b2f0339587e3378c2c646c": {"ta_keywords": "psychology driving confidence;introduction driving confidence;driving confidence;important driving psychology;drivers driving psychology;driving psychology;driving psychology driving;driving confidence connected;driving confidence guide;psychology driving;driving psychology needs;driving behavior;confidence connected vehicle;driving behavior data;based driving behavior;affect drivers driving;confidence guide drivers;drivers driving;drivers operate calmly;important driving;introduction driving;driving;traffic environment important;technology affect drivers;efficiency based driving;connected vehicle environment;based driving;confidence connected;confidence guide;explored important driving", "pdf_keywords": ""}, "87951cea6573eed827986371a35025e478d3c184": {"ta_keywords": "max optimization variational;optimization variational inequalities;stochastic gradients mini;optimization variational;mini batching convergence;sampling stochastic gradients;stochastic gradients;variational inequalities problems;batching convergence guarantees;variational inequalities;min max optimization;sum variational inequalities;gradients mini batching;variational inequalities possibly;batching convergence;convergence properties sg;max optimization;convergence guarantees monotone;finite sum variational;sum variational;gradients mini;convergence guarantees;variational;optimization;mini batching;including sampling stochastic;sampling stochastic;min max;solving min max;stochastic", "pdf_keywords": "max optimization variational;sampling stochastic gradients;stochastic gradients mini;optimization variational inequalities;stochastic estimator stepsizes;optimization variational;stochastic gradients;optimal methods stochastic;stochastic extragradient methods;stochastic estimator;mini batching convergence;optimization generalization expected;analysis convergence stochastic;methods stochastic extragradients;convergence stochastic;optimization generalization;stochastic extragradient;stochastic programming;variants variational inequality;stochastic extragradients;variational inequalities method;general variational inequality;stochastic programming aim;paradigm convergence stochastic;min max optimization;sum variational inequalities;variational inequalities problems;stochastic extragradients context;gradients mini batching;variational inequalities"}, "dfa7bdea128b899d348ed32a84a7ccb1da4340e4": {"ta_keywords": "dialog response retrieval;generate dialog response;dialog response based;response retrieval generation;robust dialog systems;generate dialog;dialog systems;retrieve generate dialog;dialog response;building robust dialog;approaches dialog response;response retrieval;dialog systems using;robust dialog;paraphrase identification retrieval;dialog;retrieval generation;neural networks retrieve;paraphrase identification;approaches dialog;pooling determine sentences;retrieval generation work;uses paraphrase identification;employing recursive autoencoders;sources retrieval task;network approaches dialog;recursive autoencoders;recursive autoencoders dynamic;sources retrieval;response based existing", "pdf_keywords": ""}, "3675958405f3ad1633d565efa36b4eb3004bcf59": {"ta_keywords": "live streams watched;live video streaming;traditional live streaming;streaming live streams;live streams;social live video;live streaming;live streaming live;streaming live;stream real time;facebook live youtube;viewers watch live;viewers different delays;different delays viewers;delays viewers;delays viewers watch;youtube live twitch;live twitch;live stream real;platforms facebook live;streaming applications increasingly;facebook live;video streaming;youtube live;video streaming applications;streams watched viewers;live video;live stream;streaming applications;live youtube", "pdf_keywords": ""}, "0ca2a7465fe88f1f4912b8dd7b4b0db69a268b0b": {"ta_keywords": "language modeling;language models;new language modeling;language modeling paradigm;language models used;introductionnon language models;optimize information flow;information flow multiple;information flow;flow multiple granularities;granularities methodsthese models;sequence probabilities optimize;sentence marginalize lattice;multiple granularities;models construct lattice;multiple granularities work;granularities;language;introductionnon language;granularities work;flow multiple;new language;propose new language;optimize information;granularities work propose;sequence probabilities;modeling paradigm;models used optimize;moderation information flow;multiple granularities methodsthese", "pdf_keywords": "neural lattice language;language neural lattice;lattice language models;neural lattice languages;lattice language model;neural language model;recurrent neural net;lattices neural framework;incorporating lattices neural;word lattice model;embeddings word lattice;using neural lattice;neural language;lattice language;language neural;multitoken phrases mental;embeddings neural lattice;multitoken phrases;propose neural language;multitoken lexical;idea neural lattice;analysis language neural;formulation language modeling;word language modeling;language modeling;using multitoken lexical;new language modeling;lattice languages aiming;language models generalize;use multitoken lexical"}, "4786e10003655be97feee21b9d9894a88a62885f": {"ta_keywords": "multilingual transfer nonlack;nonlack multilingual transfer;multilingual transfer nl;transfer nonlack multilingual;massively multilingual transfer;multilingual transfer massively;multilingual transfer;transfer massively multilingual;transfer nl models;introductionmultilingual ner transfer;ner transfer low;low resource languages;ner transfer;nonlack multilingual;target language;resource target language;languages applied low;nl models;transfer nl;models source languages;transfer nonlack;massively multilingual;transfer low resource;transfer low;multilingual;target language contrast;problem massively multilingual;nl models source;resource languages;resource languages important", "pdf_keywords": "multilingual transfer models;multilingual transfer outperforms;model multilingual transfer;multilingual transfer model;shot multilingual transfer;ner corpus model;multilingual transfer techniques;zero shot multilingual;ner corpus;unsupervised transfer supervised;applied ner corpus;low resource supervised;multilingual transfer better;multilingual annotation multisource;ensembling method multilingual;resource supervised baseline;transfer supervised;ner model trained;new multilingual transfer;language multilingual transfer;multilingual transfer;corpus model highly;method multilingual transfer;supervised transfer;multilingual transfer presented;based multilingual transfer;useful multilingual annotation;model multilingual;transfer supervised transfer;multilingual annotation"}, "4a06bfa86cbccdf5e55dcec3505cdc97b8edb288": {"ta_keywords": "categorial grammars generalized;combinatory categorial grammars;grammars generalized composition;introductionnormal form parsing;categorial grammars;grammars generalized;form parsing combinatory;parsing combinatory categorial;parsing combinatory;grammatical type raising;generalized composition type;form parsing;grammatical type;composition type raising;grammars;generalized composition;parsing;combinatory categorial;composition type;deal grammatical type;type raising;introductionnormal form;account generalized composition;type raising methodswe;generalized composition bounded;composition bounded degree;extension deal grammatical;composition;normal form;combinatory", "pdf_keywords": ""}, "622f980030f766e5eb3989f36eea4459ccc948bf": {"ta_keywords": "adaptation techniques speech;temporal changes speaker;incremental adaptation;novel incremental adaptation;inremental adaptation techniques;incremental adaptation framework;inremental adaptation;changes speaker speaking;introduction inremental adaptation;changes speaker;acoustic models quickly;adjusting acoustic models;time variant acoustic;acoustic characteristics temporal;adaptation framework;adaptation techniques;adaptation framework based;speech recognition;acoustic models;distributions acoustic model;speech recognition aimed;adaptation;variant acoustic characteristics;time evolution models;posterior distributions acoustic;techniques speech recognition;variant acoustic;updating posterior distributions;models time variant;acoustic model parameters", "pdf_keywords": ""}, "ae06bc1e8e67c27b89329ebcfe61b71625d853f6": {"ta_keywords": "saliency scores words;model compute saliency;saliency scores;saliency;compute saliency;compute saliency scores;learning introduced models;machine learning introduced;words input instance;trained model;provided trained model;models predictions;models approach human;machine learning;new explainability techniques;explainability techniques;words input;trained model compute;explainability techniques provided;approach human performance;developments machine learning;scores words input;explainability;scores words;new explainability;models predictions transparent;techniques provided trained;learning;models;human performance", "pdf_keywords": "annotated saliency explanations;saliency explanations predict;tasks explainability methods;explainability techniques human;explanations best models;tasks explainability;saliency explanations;evaluating explainability methods;evaluating explainability;explainability methods better;new explainability techniques;evaluate explainability techniques;explainability methods diagnostic;predicted words saliency;explainability techniques text;explainability methods include;model demonstrate explainability;explanations predict inference;explainability techniques diagnostic;evaluate explainability;explainability techniques useful;explainability techniques propose;explainability methods;dataset explainability techniques;explainability techniques;explanations data;explainability techniques used;results explainability methods;explainability techniques downstream;explanations predict"}, "43896ea7d488100d135645fbb4be6e7eb2e7f4e2": {"ta_keywords": "set valued features;learning trees rules;feature set strings;value feature set;feature vectors;learning trees;length feature vectors;introduction learning trees;feature set;learning systems;feature vectors components;valued features;feature vector representation;learning systems examples;feature vector;value feature;trees rules set;vector representation allows;systems learning systems;rules set valued;fixed length feature;length feature;features;valued features important;extension feature vector;possible set elements;feature ve;set elements;tools learning systems;learning systems learning", "pdf_keywords": ""}, "adb80a4190fdb6a019d57f18ae072ca93f494b7e": {"ta_keywords": "dnn acoustic models;noisy speech recognition;neural network acoustic;network dnn acoustic;speech recognition;speech recognition sequence;discriminative training low;dis discriminative training;neural network dnn;dnn acoustic;rank deep neural;deep neural network;discriminative training;acoustic models outperform;acoustic models methodsdeep;network acoustic models;low rank deep;acoustic models;effectiveness noisy speech;deep neural;network dnn;noisy speech;reduce dn model;network acoustic;models methodsdeep neural;detection dis discriminative;training low rank;gamm reduce dn;methodsdeep neural network;rank deep", "pdf_keywords": ""}, "f3762141fd64bee8d09e55ad4c83057cd4e002d4": {"ta_keywords": "estimate correlations agents;correlations learning utilities;correlations learning;correlations agents;cooperative agents competing;correlations agents using;correlation utility;leveraging correlations learning;leveraging correlations;cooperative agents;correlation utility function;generate correlation utility;non cooperative agents;approaches leveraging correlations;utility function agents;competing game methods;game methods estimate;estimate correlations;agents competing game;utility function agent;methods estimate correlations;estimated correlations;estimated correlations generate;generate correlation;agents competing;correlations generate correlation;agents using constrained;use estimated correlations;game methods;correlations generate", "pdf_keywords": ""}, "388d41b99c9c0867301f345c65877a2796225ead": {"ta_keywords": "transcribes target speaker;target speaker utterances;function target speaker;target speaker speech;speaker speech recognition;speaker automatic speech;losss target speaker;sample target speaker;target speaker automatic;speaker losss target;target speaker;introductionaxiliary interference speaker;multiple speakers speech;target speaker proposed;automatic speech;speech recognition ar;automatic speech recognition;interference speaker losss;speakers speech;speaker utterances monaural;speaker utterances;speech recognition;interference speaker;speaker speech;speaker automatic;speaker losss;speakers speech given;speaker proposed auxiliary;novel auxiliary loss;proposed auxiliary loss", "pdf_keywords": "function target speaker;talker speech recognition;speaker speech recognition;target speaker aneurysm;speaker aneurysm accuracy;accuracy multi speaker;speaker auditory speech;speaker speech transcription;speakers speech recognition;target speaker utterances;target speaker anova;target speaker speech;multi speakers speech;speaker automatic speech;auditory speech separation;speech recognition challenging;neural networks acoustic;multiple speakers speech;speaker anova improve;sample target speaker;speech separation;speaker aneurysm;conversational speech recognition;improve target speaker;multi speaker;ability targeted speaker;transcribes target speaker;targeted speaker;method speech separation;speech recognition ar"}, "7a8f8109e65ed9a6048859681a825eb5655e5dd2": {"ta_keywords": "word embeddings training;embeddings training random;trained word embeddings;sentence embeddings;modern sentence embeddings;sentence embeddings gain;word embeddings;sentence representations;embeddings training;embeddings gain random;encoders senstence classification;computing sentence representations;aim sentence embeddings;sentence embeddings solid;sentence representations pre;random encoders senstence;exploring random encoders;random encoders;embeddings;training random parameterizations;encoders senstence;representations pre trained;senstence classification;embeddings gain;encoders;senstence classification methodswe;training random;methods computing sentence;random parameterizations aim;random parameterizations", "pdf_keywords": "pretrained word embeddings;sentence representations pretrained;word embeddings training;trained word embeddings;word embeddings useful;word embeddings;sentence embeddings;word embeddings use;sentence embeddings gain;sentence representations;sentence representations pre;trained sentence encoders;sought sentence embeddings;embeddings training random;representations pretrained word;embeddings training;dimensionality sentence embeddings;evaluation sentence representations;word embeddings reflected;computing sentence representations;modern sentence embeddings;random sentence encoders;sentence embeddings solid;language ability embeddings;generate sentence representation;aim sentence embeddings;embeddings useful classification;embeddings use random;embeddings gain random;embeddings useful"}, "03058f9a39d37a8bee635969eed227d59bbc8152": {"ta_keywords": "method stochastic differential;stochastic differential equations;stochastic differential;adjoint sensitivity method;derive stochastic differential;stochastic differential equation;solution gradient memory;memory computation gradients;gradient memory efficient;gradient memory;adjoint sensitivity;generalize method stochastic;sensitivity method scalably;adaptive solvers;derive stochastic;order adaptive solvers;method stochastic;computation gradients;scalably computes gradients;high order adaptive;adaptive solvers resultsspecifically;solution gradient;computes gradients solutions;equation solution gradient;algorithm caching noise;resultsspecifically derive stochastic;computation gradients high;differential equations generalize;sensitivity method;backgroundthe adjoint sensitivity", "pdf_keywords": "gradientbased stochastic;stochastic differential equations;backward sde stochastic;sde stochastic adjoint;stochastic differential;method stochastic differential;differential equations stochastic;equations stochastic adjoint;backward stratonovich sde;stochastic adjoint framework;method stochastic adjoint;gradientbased stochastic variational;method stochastic differentialial;adjoint framework stochastic;latent stochastic differential;stochastic adjjoint method;derive stochastic differential;framework stochastic differentialial;stochastic adjoint approach;stratonovich sde diffusion;stochastic differential equation;using stochastic adjoint;stochastic differentialial;stochastic adjjoint;stochastic adjoint process;stochastic adjoint;sde using stochastic;applied stochastic differential;using stochastic adjjoint;backward stochastic derivation"}, "4eb62ee328ceac9976c72bca65570d73ca0e8b64": {"ta_keywords": "structurally stable marriage;behavioral structurally;behavioral structural structural;behavioral structural;behavioral structurally stable;structural biology;structural structural biology;structural biology context;stable marriage;behavioral;biology context behavioral;structural structural;importance behavioral structural;structural;structurally;context behavioral structurally;marriage;structurally stable;importance behavioral;context behavioral;discuss importance behavioral;biology;biology context;stable;context;purpose;purpose article;discuss;purpose article discuss;importance", "pdf_keywords": ""}, "30a6a5614727017e7d7981f87df57d17713501a0": {"ta_keywords": "bandit framework matching;personalized incentives recommendations;incentives users preferences;matching incentives users;bandit framework;armed bandit framework;incentives recommendations improve;incentives recommendations;personalized incentives;incentives users;matching incentives;design personalized incentives;framework matching incentives;bandit;multi armed bandit;greedy matching paradigm;incentives;greedy matching;user engagement gaining;ideas domains greedy;greedy;domains greedy matching;recommendations improve user;improve user engagement;domains greedy;user engagement;armed bandit;recommendations improve;matching paradigm;confidence bound algorithm", "pdf_keywords": "bandit framework matching;bandit algorithm matching;bandit approach combinatorial;algorithm incentive matched;propose bandit algorithm;algorithm matching incentives;conventional bandit approach;extending conventional bandit;bandit approach online;armed bandit algorithm;algorithm incentive;bandit algorithm;greedy matching feasible;matching incentives users;competitive algorithm incentive;bandit approach;bandit framework;personalized incentives recommendations;matching incentives;bandit algorithm capacitated;bandit algorithm repeatedly;develop bandit algorithm;incentive matched;incentive matched competitive;greedy matching paradigm;armed bandit framework;algorithm empirical reward;algorithm incentive design;matches agents incentives;propose bandit"}, "357ff26120dd220d7132f8083697d54b007ef260": {"ta_keywords": "self supervised learning;speech processing universal;self supervised;supervised learning sl;background self supervised;speech processing;learning sl;introduce speech processing;universal performance benchmark;supervised;processing nl;nl computer vision;language processing nl;processing universal performance;learning;cv paradigm pretrains;supervised learning;paradigm pretrains shared;pretrains shared model;learning sl proven;state art sota;tasks minimal adaptation;processing universal;minimal adaptation methods;performance benchmark superb;benchmark;minimal adaptation;paradigm pretrains;benchmark superb;pretrains shared", "pdf_keywords": ""}, "9f24b8f93ed00a1592e02fdb0edf5ebf0d8752ff": {"ta_keywords": "fairness peerreview algorithm;guarantees fairness peerreview;fairness peerreview;reviewers conference peer;assignment papers reviewers;conference peer review;peer review;peerreview algorithm;review focus fairness;papers reviewers;peerreview algorithm establish;fairness statistical accuracy;papers reviewers conference;accuracy results fairness;results fairness objective;approximation guarantees fairness;peerreview algorithm section;maximize review quality;peer review focus;reviewers conference;review quality disadvantaged;fairness statistical;results fairness;automated assignment papers;fairness objective maximize;fairness objective;focus fairness statistical;reviewers;guarantees fairness;conference peer", "pdf_keywords": ""}, "6a4deeb40aed8a4d56c8d9401c94b6c7a769e8c3": {"ta_keywords": "faceted query example;faceted query;information retrieval;known information retrieval;query document;information retrieval task;task faceted query;retrieval task document;retrieval;retrieval task;search query;input query document;search query goal;query example known;query example;introduction query example;user search query;introduction query;search;retrieve relevant documents;query;collection document covers;input query;user search;query goal;relevant documents large;document covers;example known information;documents large collection;introduce task faceted", "pdf_keywords": "faceted search based;webthe faceted search;quantitatively faceted search;search quantitatively faceted;faceted search;search faceted search;faceted search quantitatively;faceted search recommendation;faceted search paradigm;faceted search report;queries annotated facets;use search faceted;documents quantitatively faceted;dataset faceted search;challenges faceted search;search faceted;search based;faceted search review;literature search tools;based faceted web;approaches search search;discovery literature search;faceted web web;search tools;search search information;scientific literature search;tool search provide;searching searching;search information;search search"}, "356ea9b29101ec6974a7a97b62266b0e7e58d6bf": {"ta_keywords": "srrna mediated asrrna;asrrna mediated asrrna;mediated asrrna mediated;srrna mediated;asrrna mediated;case srrna mediated;mediated asrrna;asrrna mediated asr;srrna;case srrna;report case srrna;asrrna;mediated asr;asr;mediated;report case;report;case", "pdf_keywords": ""}, "9e0d3161b13481418b7e85e3a691d23d67cf1e68": {"ta_keywords": "images leak privacy;privacy leaking image;automatically identify images;leaking image detection;identify images leak;sharing images social;object detectors;privacy leaking;images social media;data available gene;identify images;leak privacy;automatically identify;images social;privacy leakage;pretrained object detectors;object detectors modeling;sharing images;leak privacy recent;images leak;issue privacy leaking;privacy;issue privacy leakage;leaking image;practice sharing images;detectors modeling correlation;privacy leakage address;privacy recent advance;privacy recent;image detection", "pdf_keywords": "convolutional regions privacy;privacy aware image;privacy leak image;images leak privacy;privacy leaking image;leak image detection;leaking image detection;identify private images;aware graph neural;image privacy;neural network privacy;model image privacy;regions privacy leak;sharing images social;identify images leak;image privacy picalert;regionaware graph convolutional;privacy leaking;private images;graph convolutional network;privacy leak;leak privacy;novel privacy leak;aware graph;privacy aware;privacy leakage;region aware graph;private public images;leak image;graph convolutional"}, "a3c9d1c5e403f35e5694778b86832f0f9a7d87e6": {"ta_keywords": "fast finding speech;search large speech;backgroundfast similarity search;large speech data;acoustically similar query;similarity search large;similarity search;finding speech model;neighborhood graph indexing;backgroundfast similarity;speech data;model acoustically similar;speech data set;speech model acoustically;finding speech;speech model set;speech models results;speech models;graph indexing;speech model;fast finding;set speech models;acoustically similar;large set speech;graph indexing methodsthis;models results speech;results speech model;problem fast finding;model acoustically;large speech", "pdf_keywords": ""}, "77910e51a40d17157fc798325d06edfa6cff18d6": {"ta_keywords": "nl code generation;code generation methodsopen;code generation automatically;natural language code;code generation;code generation aims;language code generation;generate code general;knowledge nl code;domain code generation;training natural language;generate code;language python natural;python natural language;external knowledge nl;writing code explore;aims generate code;generation methodsopen;code general purpose;generation methodsopen domain;general purpose programming;purpose programming language;purpose programming;code explore effectiveness;code explore;python natural;external knowledge pre;language nl intents;programming language python;generation automatically", "pdf_keywords": "knowledge code generation;nl code generation;external knowledge code;code generation models;programming language api;knowledge nl code;code generation automatically;code generation;language api documentation;domain code generation;annotated data mined;code pairs api;generate code general;retrieve nl code;language api;writing code explore;knowledge retrieve nl;external knowledge retrieve;automatically external knowledge;generate code;code generation aims;existing api documentation;state art syntax;knowledge code;python natural language;code code emulate;api documentation propose;nl code pairs;generated code;stackoverflow programming language"}, "948b68677c4f3bcbb1bae7f1d4e1fd5a103f03d4": {"ta_keywords": "optimize speech enhancement;speech enhancement systems;speech enhancement automatic;speech enhancement application;speech enhancement;enhancement automatic speech;enable optimize speech;enhancement systems denoise;optimize speech;jointly optimize speech;enhancement automatic;speech recognition ar;speech recognition;automatic speech recognition;signal reconstruction objectives;enhancement application;enhancement systems;automatic speech;based signal reconstruction;signal reconstruction;enhancement;enhancement application oriented;denoise dereverberate;neural methods enable;systems denoise dereverberate;optimized based signal;systems denoise;denoise dereverberate distorted;anr error minimization;end neural methods", "pdf_keywords": ""}, "5edaab1fa078a5c468e3fb26d267ca49be32e70e": {"ta_keywords": "preference elicitation aggregation;preference elicitation;amazon mechanical turk;elicitation aggregation;estimate cost answering;elicitation aggregation plackett;mechanical turk;mechanical turk use;cost answering;cost effective elicitation;turk use estimate;framework preference elicitation;cost answering different;aggregation plackett luce;effective elicitation questions;elicitation questions order;effective elicitation;elicitation questions;elicitation;aggregation;aggregation plackett;turk use;luce model;propose cost effective;group decision methods;better group decision;group decision;preference;estimate cost;propose cost", "pdf_keywords": "preference elicitation aggregation;effective preference elicitation;preference elicitation using;agent preference aggregation;elicitation adapted ranking;preference aggregation;preference elicitation;group eliciting preferences;preference elicitation adapted;eliciting preferences;agents preferences experiments;amazon mechanical turk;adapted ranking model;predicting agent alternatives;adapted ranking;choice eliciting preferences;eliciting preferences regular;agent rank choices;preference aggregation group;preferences experiments;elicitation aggregation;preferences agents;prediction agent preference;preferences experiments method;preferences agents probability;dataset cost elicitation;framework preference elicitation;deterministic preferences agents;group agents preferences;ranking model"}, "98caf4eb79208cf4bbfe20bde37bc1b6ded6d6de": {"ta_keywords": "named entity recognition;entity recognition models;entity recognition english;entity recognition;method soft gazetteers;exhaustive entity gazetteers;gazetteers exist languages;gazetteers lists entities;entity gazetteers exist;soft gazetteers;models use gazetteers;entity gazetteers;use gazetteers lists;gazetteers exist;use gazetteers;named entity;contexttraditional named entity;gazetteers lists;recognition english data;lists entities;utility named entity;lists entities features;entities features improve;english data;recognition english;gazetteers;low resource languages;entities features;english data designing;resource languages challenging", "pdf_keywords": "gazetteers useful neural;named entity recognition;entity recognition models;useful neural ner;entity recognition english;entity recognition;features lowresource languages;entity recognition second;languages soft gazetteer;features lowresource ner;neural ner;low resource languages;sentence soft gazetteer;neural neural ner;low resource language;neural ner low;soft gazetteer features;gazetteer features lowresource;features improve ner;features word span;features low resource;language dataset;entity mentions character;lowresource languages;languages using lowresource;resource language dataset;exhaustive entity gazetteers;resource languages soft;entity recognition year;soft gazetteers useful"}, "55b61befce42280c3d57331121c7d349dd8be4cf": {"ta_keywords": "simultaneous speech translation;speech translation technology;speech translation;speech translation beginning;translation technology;inherent speech translation;simultaneous speech;translation technology attempts;introduction simultaneous speech;delay inherent speech;translation beginning translation;translation end explicit;reduce delay inherent;beginning translation end;translation beginning;reduce delay;speech;delay inherent;translation;translation end;beginning translation;introduction simultaneous;simultaneous;delay;speed accuracy systems;speed accuracy;systems delay achieving;sentence boundaries;delay achieving;systems delay", "pdf_keywords": ""}, "a949ba38194ad43c86925acec6705b434d5a920f": {"ta_keywords": "sex specific mutation;sex specific disease;mutation aetiology sex;diagnosis sex specific;diagnosis sex;aetiology sex specific;model diagnosis sex;aetiology sex;occurrence sex specific;specific mutation aetiology;mutation aetiology;sex specific;occurrence sex;specific mutation;specific disease;mutation;disease;specific disease major;disease major cause;disease major;diagnosis;major cause death;aetiology;new model diagnosis;sex;death world report;model diagnosis;cause death world;cause death;death world", "pdf_keywords": ""}, "22b6e88a2f234fc5646f6239f9040a776e841a97": {"ta_keywords": "bilingual lexicon corpus;induction bilingual lexicon;bilingual lexicon;bilingual lexicons;introductioninducing bilingual lexicons;bilingual lexicons small;introductioninducing bilingual;induction bilingual;investigate induction bilingual;performs monolingual segmentation;monolingual segmentation followed;aligned english translations;corpus phonemic transcriptions;lexicon corpus phonemic;monolingual segmentation;bilingual;transcriptions sentence aligned;phonemic transcriptions sentence;aligned phonemic transcriptions;corpus phonemic;performs monolingual;sentence aligned phonemic;translations;transcriptions sentence;phonemic transcriptions;corpus;lexicon corpus;phonemic transcriptions methodswe;translations evaluate existing;monolingual", "pdf_keywords": ""}, "9abd13caa32b1a90e32462a884a512f8666e80cc": {"ta_keywords": "dependent semantic parsing;independent semantic parsing;language queries contextual;introductioncontext dependent semantic;semantic parsing propose;semantic parsing;natural language queries;queries contextual;context independent semantic;queries contextual information;dependent natural language;independent semantic;parsing;context dependent natural;semantic parsing proven;dependent semantic;language queries;restate context dependent;advances context independent;context dependent;parsing propose;introductioncontext dependent;contextual information accomplish;contextual information;follow query analysis;context independent;restate context;natural language;contextual;semantic", "pdf_keywords": "context independent parsers;contextindependent semantic parsing;independent semantic parsers;dependent semantic parsing;parser independent seamlessly;independent parsers provide;independent parsers;context independent semantic;semantic parsing;advances contextindependent semantic;semantic parsers;friendly semantic parser;semantic parser;language queries contextual;semantic parsing propose;queries phases parser;context dependent semantic;queries contextual;queries contextual information;natural language queries;queries employ context;semantic parsers conduct;parser independent;independent semantic;dependent natural language;natural language processing;contextual information query;parsers provide;parsing;contextindependent semantic"}, "3c8853d4ae3ad2633c47e840a48951d62b64a5b4": {"ta_keywords": "view graph learning;multi view graph;graph learning adaptive;graph learning;graph learning approach;view graph;factor extraction graph;graph sparsification label;extraction graph sparsification;label propagation unified;adaptive label propagation;backgroundmulti view graph;graph sparsification;factors multi view;latent factor extraction;propagation semi supervised;label propagation semi;label propagation;learning adaptive label;semi supervised;label propagation objectivethe;semi supervised classification;multi view data;shared latent factors;independent data representations;adaptive label;multi view;data view independent;latent factors multi;extraction graph", "pdf_keywords": ""}, "df29486c04eafd004f2f0816e84c798783802cdf": {"ta_keywords": "lingual morphological inflection;morphological inflection languages;inflection cross lingual;cross lingual morphological;introductiontransliteration cross lingual;transliteration related languages;morphological inflection cross;lingual morphological;morphological inflection;inflection languages share;inflection languages;lingual transfer typologically;related languages grapheme;transliteration;transliteration related;languages grapheme phoneme;languages grapheme;cross lingual transfer;use transliteration related;lingual transfer;cross lingual;use transliteration;task morphological inflection;typologically related languages;related languages;explore use transliteration;lingual;grapheme phoneme conversion;related languages proven;morphological", "pdf_keywords": ""}, "298a68153859303ee70b3ef1525ee9c7031e32f5": {"ta_keywords": "chit chat conversation;chit chat dialogue;chat dialogue systems;chat dialogue;chit chat;consistency chit chat;chat conversation;dialogue systems;quality chit chat;chat conversation motivated;conversation;dialogue systems majority;interlocutors research cognitive;human like responses;conversation motivated;dialogue;modeling understanding interlocutors;engagingness consistency chit;chat;modeling understanding;receiver based framework;chit;conversation motivated propose;cognitive;receiver based;cognitive science;like responses;improve engagingness consistency;research cognitive science;responses leaving understudied", "pdf_keywords": "building personalized chatbots;personalized chatbots;persona perception bot;chatbots;personalized chatbots deliver;chatbots deliver engaging;bot approach human;development personalized dialogue;persona chat;persona chat improve;personalized dialogue generation;dataset persona chat;chat dialogue systems;experiments personachat dataset;generate dialogues user;personalized dialogue systems;interaction persona dialogue;conversations gain user;personachat dataset demonstrate;experiments personachat;personalized dialogue;perception experiments personachat;achieve personalized dialogue;dialogue generation experiments;chatbots deliver;supervised dialogue;persona dialogue results;supervised dialogue generation;perception bot;chat dialogue"}, "251a80dd4126fed3d6ae64f00dc24479f0ba5662": {"ta_keywords": "23 tutorials hosted;web conference 2021;tutorials hosted;tutorials hosted web;web conference;lecture style tutorials;review web conference;hosted web conference;tutorials report;23 tutorials;2021 hosted lecture;tutorials;summarizes 23 tutorials;showed web conference;tutorials report summarizes;style tutorials;hosted lecture style;hosted lecture;2021 lecture style;conference 2021 hosted;tutorials 14;conference 2021 lecture;style tutorials 14;review web;hands tutorials report;2021 lecture;hands tutorials;conference 2021;2021 showed web;web", "pdf_keywords": ""}, "46dab5eb9c11bd49893e2dafa7d1b720a0aa2b3d": {"ta_keywords": "ord orc haracters;orc haracters;introductionw ord orc;haracters fr physicians;orc haracters fr;ord orc;physicians;introductionw ord;physicians physicians;physicians physicians physicians;fr physicians physicians;ord;orc;fr physicians;haracters;haracters fr;introductionw;fr", "pdf_keywords": ""}, "e92677eb974a2814d57de54e2c3733cbd92e2c00": {"ta_keywords": "introductioncodinging distributed computing;distributed computing;distributed computing supports;distributed computing systems;world distributed computing;hierarchical coding scheme;low latency computation;hierarchical computational;introductioncodinging distributed;propose hierarchical coding;consider hierarchical computational;hierarchical coding;latency computation relieving;hierarchical computational structure;computing supports low;computation relieving burden;latency computation;simple master worker;work propose hierarchical;coding scheme;computing systems;distributed;coding;coding scheme model;computational;computing supports;computing;master worker model;computation relieving;straggling workers", "pdf_keywords": "computing coded computing;coded computing;distributed computing;coded computation homogeneous;coded computation;scheme distributed computing;distributed computing consider;coded distributed;data coded distributed;coded computing based;coded computation introduced;coded computing coded;distributed computing systems;coded matrix multiplication;coding scheme distributed;generating coded computation;hierarchical coding scheme;data multi rack;computing coded;multi rack systems;notion coded computation;coded distributed n2;hierarchical computing;coded matrix;propose hierarchical coding;racks propose hierarchical;group coding scheme;distributed n2 racks;cost coding schemes;coding matrix"}, "0e9e334e2647307f8fa7f9937d93f3ca9095e351": {"ta_keywords": "extragradient method korpelevich;extragradient method;iterate optical convergence;optical convergence;saddle point variational;optical convergence rate;point variational inequalities;variational inequalities problems;extragradient;variational inequalities;derive iterate optical;methods solving saddle;convergence paper;iterate optical;questions convergence paper;solving saddle point;point variational;convergence paper resolve;optimization;solving saddle;method korpelevich 1976;convergence rate;saddle point;method korpelevich;inequalities problems vip;variational;convergence;questions convergence;optical;open questions convergence", "pdf_keywords": "convergence rate extragradient;extragradient method practical;method convex optimization;extragradient method;extragradient method based;iterate optical convergence;extragradient method popular;method convex;inequalities based progradient;applications extragradient method;rate extragradient method;gd extragradient method;extragradient operator;variational inequalities method;convex optimization method;method monotone variational;learning method convex;extragradient operator linear;optical convergence;optical convergence rate;extragradient method federated;progradient method;technique rate convergence;cocoercivity extragradient operator;progradient method paper;cocoercive gradient convex;convex optimization;iterate convergence monotone;iterate convergence rate;variational inequalities introduced"}, "75ba422d90c488b1388345865e0525208331bb3d": {"ta_keywords": "differentially private learning;private learning;learning non private;private learning non;privacy guarantees differentiallyprivate;differentially private;strategies differentially private;preserving privacy training;privacy training;guarantees differentiallyprivate stochastic;privacy training modern;private tasks methods;stochastic gradient descent;private tasks;differentiallyprivate stochastic gradient;non private tasks;privacy guarantees;gradient descent dpg;stricter privacy guarantees;privacy;differentiallyprivate stochastic;preserving privacy;modern neuronl models;know stricter privacy;private;methods preserving privacy;stricter privacy;neuronl models;non private;stochastic gradient", "pdf_keywords": "introductionpreserving privacy training;privacy training;differentially private training;privacy training modern;wikiann speech tagging;speech tagging;non nlp tasks;privacy guarantees differentiallyprivate;entity recognition ner;named entity recognition;introductionpreserving privacy;non nl datasets;entity recognition;explored differentially private;differentially private;sequence tagging tasks;learning processing natural;privacy;natural language processing;language models;word classification;processing natural language;modern neural models;speech tagging pos;non nlp;computational language models;know stricter privacy;word classification problems;stricter privacy guarantees;language inference dna"}, "9d3e33875ec39001e72313fb919f66242ee97880": {"ta_keywords": "discovery linguistic units;words language orthography;orthographic transcriptions images;language orthography;discovery linguistic;orthographic transcriptions;linguistic units subwords;replacement orthographic transcriptions;surrounding discovery linguistic;transcriptions images translated;discovery raw speech;language orthography study;transcriptions images;linguistic units;subwords words language;units subwords words;words language;transcriptions;unsupervised discovery;orthography;subwords words;language help unsupervised;images translated text;unsupervised discovery raw;orthographic;speech study replacement;units subwords;replacement orthographic;linguistic;help unsupervised discovery", "pdf_keywords": "discovery linguistic units;building linguistic units;discovery linguistic;unit discovery speech;units language orthography;symbolic units speech2image;linguistic units language;surrounding discovery linguistic;generate speech tasks;discovery speech synthesis;linguistic units;building linguistic;linguistic units sub;units speech2image;units speech2image andthe;language orthography;speech tasks;discover spoken term;words language orthography;learning language;speech children learning;discovery raw speech;children learning language;linguistic units concentrated;field speech development;speech2image andthe project;speech tasks image2speech;speech development;learning language information;language orthography study"}, "7d7469e059c6890c24d42931c697df835329f26a": {"ta_keywords": "estimating noise mixture;noise suppression objectiveto;noise mixture model;noise model estimation;method noise mixture;noise suppression;estimationion method noise;estimating noise;estimation noise noise;noise mixture;model noise suppression;backgrounda robust estimationion;mixture model noise;estimation noise;way estimating noise;estimate noise;mmze estimation noise;estimate noise methodsby;mmze estimate noise;robust estimationion method;noise methodsby;robust estimationion;noise noise model;backgrounda robust;noise model;noise methodsby iterating;mmze estimation;suppression objectiveto solve;method noise;iterating mmze estimation", "pdf_keywords": ""}, "f7f6160d4e9e3bf7f36bacbc9f15e916a6f226de": {"ta_keywords": "multichannel speech enhancement;enhancement speech recognition;speech enhancement;does speech enhancement;speech enhancement work;speech enhancement speech;enhancement speech;end speech recognition;multichannel speech;speech recognition architecture;speech recognition ar;automatic speech recognition;speech recognition single;components multichannel speech;speech recognition;end automatic speech;utility automatic speech;automatic speech;end speech;novel multichannel end;novel multichannel;multichannel end;end end speech;proposed novel multichannel;enhancement work end;analysis multichannel end;multichannel end end;analysis multichannel;multichannel;recognition architecture integrates", "pdf_keywords": ""}, "99c87e16c56b8a113124779734951f11bd662d5d": {"ta_keywords": "encouraging energy efficient;energy efficient behavior;encouraging energy;designed encouraging energy;energy consumption building;social game designed;lighting level win;social game;behavior building occupants;vote desired lighting;consumption building;game designed encouraging;consumption building methodsoccupants;energy consumption;occupants utility maximizers;efficient behavior building;winning points comfort;utility maximizers;utility maximizers utility;energy efficient;efficient behavior;overall energy consumption;methodsoccupants vote desired;building occupants aim;behavior building;building methodsoccupants vote;reducing overall energy;energy;backgroundwe social game;maximizers utility functions", "pdf_keywords": "energy consumption building;game improving energy;occupants behave energy;energy efficient behaviors;game energy savings;convex constraints nash;occupants continuous game;game energy;energy efficient behavior;energy consumption;optimization utility learning;encouraging energy efficient;utility estimation;formulate utility estimation;occupants modeled utility;utility estimation problem;modeled utility maximizers;model utility estimation;social game energy;occupants utility maximizers;utility maximizers;behave energy efficient;utility learning;consumption building;cooperative game occupants;utility maximizers utility;game convex constraints;utility learning described;maximizers utility functions;utility maximizers engage"}, "4697ef43450f173e12b1e22b77e976dc56fdf5fe": {"ta_keywords": "compressive sensingbased adaptive;compressive sensing;novel compressive sensing;existing compressive sensing;defence versus adversarial;compressive sensing based;compressive sensingbased;context compressive sensingbased;sensingbased adaptive defence;methods existing compressive;novel compressive;adversarial images;adversarial;adversarial images considered;adaptive defence;compressive;context compressive;adaptive defence cad;adversarial perturbations usually;versus adversarial images;adversarial perturbations;based adaptive defence;sensing based defence;proposes novel compressive;assume adversarial perturbations;versus adversarial;existing compressive;combats distortion frequency;assume adversarial;adaptive defence versus", "pdf_keywords": "compressive sensing adversarial;combat adversarial attacks;attack compressanti adversarial;combat adversarial;sensing adversarial training;combating adversarial images;sensing adversarial;compressanti adversarial attacks;attacks based compressive;combating adversarial;adversarial training propose;adversarial attacks commonly;adversarial attacks present;method combating adversarial;quantify adversarial images;adversarial training;approach combat adversarial;adversarial attacks;bandit techniques adversarial;adversarial attacks based;algorithm adversarial image;adversarial images;analysis adversarial robustness;adversarial image;analysis adversarial;adversarial attacks use;attack present compressive;adversarial images using;adversarial;attacks gradient based"}, "ce3d6673b7eebdd0198940316d18e383e9597c9a": {"ta_keywords": "learning contextual bandits;contextual bandits;contextual bandits help;bandits help loss;bandits;bandits help;adversarial versus stochastic;loss predictors;learning contextual;problem learning contextual;loss predictors provide;various settings adversarial;adversarial;adversarial versus;settings adversarial;settings adversarial versus;problem learning;loss predictors address;help loss predictors;learning;study learning contextual;address problem learning;stochastic environments known;loss;stochastic environments;contextual;predictors provide complete;predictors;predictors provide;versus unknown", "pdf_keywords": "predictors contextual bandits;learning contextual bandits;contextual bandits optimal;bounds contextual bandits;regret contextual bandits;guarantee contextual bandits;contextual bandits extent;analyzing contextual bandits;contextextual bandit optimal;contextual bandits good;learning contextextual bandit;contextual bandits;algorithm contextextual bandit;stationary contextual bandits;contextual bandits continuous;contextual bandits help;contextextual bandit algorithm;bandits optimal regret;contextual bandits present;probability contextextual bandit;analysis contextextual bandit;adaptive regret bounds;regret bounds adversarial;contextextual bandit method;bandit prediction;bandit prediction action;contextextual bandit;bandits good predictors;regret bounds contextual;optimal regret bounds"}, "29b3a609f2b5cb10cffb80a6aaf96413a4a9998e": {"ta_keywords": "compression using normalizing;lossless compression using;modular scalee transform;lossless compression;scalee transform mt;scalee transform;discuss lossless compression;compression ratios;compression ratios methodswe;high compression ratios;compression using;unform base conversion;achieving high compression;base conversion;transform;invertible flow transformations;transform mt;using normalizing flows;compression;modular scalee;propose modular scalee;transformations based onmt;normalizing flows;flow transformations based;base conversion bc;normalizing flows demonstrated;flow transformations;numerically invertible flow;using normalizing;high compression", "pdf_keywords": "neural compression codec;fast codelength compression;coding data compression;neural compression algorithms;effective neural compression;application neural compression;efficient lossless compression;state art compression;compression codec;based neural compression;improved compression performance;neural compression;lossless compression;socalled neural compression;data compression;compression performance approaches;compression ratios coding;codelength compression;improved compression;technique neural compression;compression codec bridges;neural compression garners;lossless compression alg;method lossless compression;compression technique neural;neural compression technique;compression state art;minimal value compressionthe;compressionthe data compression;compression algorithms significantly"}, "24ee54c8d5a01197e015d40be4277cfbb727394f": {"ta_keywords": "planning bus routes;plan bus routes;travel planning bus;routes consideration sharingbikes;bus routes;bus routes consideration;planning bus;driven plan bus;plan bus;consideration sharingbikes;sharingbikes;sharing bikes;emergence sharing bikes;bus;daily travel planning;extend bus;travel planning;sharing bikes exerts;bus routes current;approach extend bus;travel flow coverage;consideration sharingbikes animing;maximize travel;sharingbikes animing maximize;sharingbikes animing;maximize travel flow;daily travel;citizens daily travel;bikes;data driven plan", "pdf_keywords": ""}, "ea5cfce90444b17b36da07840b2f0cafb54ab0a7": {"ta_keywords": "automatic deception detection;deception detection;automatic deception;deception detection attempts;detect deception;attempt detect deception;detect deception perform;deception asking questions;deceptive conversational partner;deceptive conversational;deception asking;deception perform actions;work automatic deception;learn signs deception;deception;signs deception asking;telltale signs deception;catch potential liar;deception perform;unveil deceptive conversational;dialogue systems asks;signs deception;action envisioning dialogue;envisioning dialogue;potential liar;dialogue systems;envisioning dialogue systems;dialogue;deceptive;attempt unveil deceptive", "pdf_keywords": ""}, "e3a1b2a19356dc685d78630ae2a8852ad6c86200": {"ta_keywords": "proximity aware ranking;indexing word pairs;retrieval enterprise search;pruning proximity information;indexing word;moderately loaded retrieval;aware ranking functions;mild pruning proximity;pruning proximity;aware ranking;use indexing word;close word pairs;retrieval;enterprise search;loaded retrieval enterprise;retrieval enterprise;enterprise search engine;indexing;pairs words frequent;loaded retrieval;proximity information;occurrences close word;ranking functions;proximity information appropriate;search engine methods;use indexing;ranking;ranking functions use;proximity aware;proximity", "pdf_keywords": ""}, "5e24aa9fdf5466e96d314dfcde973fccec02995d": {"ta_keywords": "batch learners;massive data streams;memory online learning;data streams;online learning methods;batch learners methodsin;data streams essential;stream processing practice;online learning;stream processing;streams;streams essential;suited stream processing;training line learner;inference learning methods;training data;learn concepts massive;passes training data;learning methods;learning methods perceptron;memory online;art batch learners;learning;inference learning;concepts massive data;learning methods operate;stream;line learner;streams essential design;massive data", "pdf_keywords": ""}, "e2854bf66ed86a5dc74183bae5fde18e65699833": {"ta_keywords": "hybrid bbstm hmm;polyphonic sound event;sound event detection;bbstm hmm;speech recognition;memory hidden markov;speech recognition multi;bbstm hmm methodswe;hidden markov;hidden markov model;sound event;hybrid bbstm;short term memory;polyphonic;polyphonic sound;model hybrid bbstm;field speech recognition;performance field speech;bbstm;method polyphonic sound;recognition multi label;markov model hybrid;event detection;method polyphonic;event detection based;new method polyphonic;hybrid model neural;markov;duration model output;markov model", "pdf_keywords": ""}, "21c9c624bc328686cef4bb1f80a786a5027d8886": {"ta_keywords": "scientific information extraction;information extraction sciie;information extraction;information scientific documents;extraction sciie consider;raw scientific text;considered extracting document;sciie consider extraction;extracting document;extraction sciie;extracting document level;scientific documents;scientific text;key information scientific;extracting key information;scientific documents potential;scientific information;scientific text improve;works scientific information;considered extracting;improve literature search;extracting;consider extraction;work considered extracting;extracting key;extraction;document level entity;entity clusters;raw scientific;entity clusters relations", "pdf_keywords": "citation aware scientific;information extraction citationice;extraction citation graph;citationie tasks;citationie tasks mention;citation aware;extraction citationice model;extraction citationice;new citation aware;extraction citation;content citation graph;citation graph information;relation extraction citation;methods citationie tasks;citation graph helps;information citation graph;scientific information extraction;citation graph content;information extraction sciie;citation graph embeddings;extraction use citation;task model citationio;classification using citationii;citation information;textual content citation;citation information mention;nodes citation;cited documents information;cited cited documents;cited documents"}, "50c651e9f94f9d4927a726af0ef44818179d87da": {"ta_keywords": "geoquery corpus parser;multilingual geoquery corpus;geoquery corpus;structured meaning representation;semantic parser;adapted semantic parser;corpus parser;experiments multilingual geoquery;machine translation components;semantic parser experiments;machine translation;multilingual geoquery;straightforward machine translation;standard machine translation;representation natural language;parser experiments multilingual;corpus parser competitive;machine translation task;natural language utterance;parser;translation task;natural language;translation components;deriving structured meaning;translation components adapted;parser competitive;corpus;parser experiments;adapted semantic;language utterance methodswe", "pdf_keywords": ""}, "cd0702deabaa8b7ccfba077f89dcc24e48ae1d47": {"ta_keywords": "retrieval methods subtopic;retrieval subtopic retrieval;retrieval subtopic;methods subtopic retrieval;subtopic retrieval problem;subtopic retrieval;retrieval problem subtopic;problem subtopic retrieval;subtopic retrieval subtopic;traditional retrieval problem;non traditional retrieval;subtopics query topic;traditional retrieval methods;retrieval methods;ranking dependent documents;dependent documents ranking;retrieval poses challenges;document ranking;traditional retrieval;documents ranking;document ranking dependent;assumed traditional retrieval;different subtopics query;utility document ranking;retrieval;subtopics query;retrieval problem concerned;documents ranking violating;retrieval problem;subtopic retrieval poses", "pdf_keywords": ""}, "c1d0e73ec3aaf7ffdcbe41835d649d638cbc2f2d": {"ta_keywords": "accelerating inference;accelerating inference large;confidently accelerating inference;accelerated inference confident;inference confident adaptive;accelerated inference;introductionconsistent accelerated inference;inference large expensive;confident adaptive transformers;inference large;introductionconsistent accelerated;confident adaptive;inference confident;adaptive transformers;adaptive transformers case;language processing nl;processing nl;adaptive transformers simultaneously;natural language processing;adaptive;language processing;approximate computational;cats confident adaptive;computational methods increase;transformers ubiquitous natural;nl amortized approximate;inference;processing nl amortized;expensive multilayer transformers;ubiquitous natural language", "pdf_keywords": "confident adaptive prediction;adaptive prediction minimal;inference confident adaptive;prediction accommodate adaptive;adaptive prediction;accommodate adaptive prediction;accelerateerated inference confident;adaptive prediction develop;confident adaptive;approach confident adaptive;accelerating inference multilayered;method consistent prediction;consistent prediction;accuracy prediction consistency;prediction consistency;predictions methodsamortized approximate;approach consistent prediction;adaptive adaptive;prediction automatic models;method multilayered adaptive;predictions methodsamortized;confident adaptive transformers;multilayered adaptive adaptive;automatic prediction;consistent prediction present;accelerateerated inference;adaptive adaptive network;automatic automatic prediction;adaptive;prediction automatic"}, "77cd3ae8a0b9ef6865d5324a4d62280e6f7a1053": {"ta_keywords": "resultse2e automatic speech;end speech recognition;speech recognition distant;data augmentation methods;data augmentation;augmentation methods end;chime challenge datasets;backgrounddata augmentation methods;automatic speech;speech recognition models;speech recognition;recognition distant talk;automatic speech recognition;backgrounddata augmentation;augmentation methods;trained series chime;use augmentation methods;chime challenge;recognition models trained;end end speech;end speech;use augmentation;propose use augmentation;series data augmentation;distant talk scenarios;augmentation;recognition distant;models trained;spontaneous speech;noisy spontaneous speech", "pdf_keywords": "training data speech;data speech enhanced;2e speech recognition;speech recognition e2e;data speech;electronic 2e speech;speech enhanced noisy;talk transcription;talk transcription using;automatic speech;speech enhanced;speech recognition task;recognition task speech;2e audio auditory;augmentation distant talk;distant talk transcription;automatic speech recognition;speech recognition distant;task speech recognition;recognition context speech;speech recognition;context speech recognition;challenge field speech;end automatic speech;fluorescence based speech;data augmentation methods;speech processing;discriminator cycle gan;tool speech communication;speech recognition important"}, "3b3a5a9c7352b74e8377ce3182ea646b0bed5b4c": {"ta_keywords": "semantic parsers achieved;semantic parsers;semantic parsing;based semantic parsers;introduction semantic parsing;semantic parsing considers;parsers achieved;parsers;parsing;parsers achieved impressive;transducing natural language;nl utterances machine;natural language nl;parsing considers task;utterances machine executable;utterances machine;semantic;introduction semantic;nl utterances;executable meaning representations;natural language;parsing considers;language nl utterances;based semantic;language nl;meaning representations;utterances;network based semantic;representations rs neural;rs neural", "pdf_keywords": ""}, "89d15c9de3608157ff746af7368556149b50e037": {"ta_keywords": "words language modeling;language modeling;language model sdlim;driven language model;atomic semantic units;language modeling named;language model;atomic semantic;language modeling methods;words atomic language;sememes minimum semantic;seme driven language;necessarily atomic semantic;introduction language modeling;atomic language units;semantics words language;atomic language;minimum semantic units;semantic units inspired;semantic units;words language;modeling named seme;semantics words;driven language;implicit semantics words;minimum semantic;sequential patterns words;semantic;units human languages;represent implicit semantics", "pdf_keywords": "words language modeling;neural language models;model word prediction;neural language model;language model predict;driven language model;language models;lexical sememe prediction;prediction words sememe;language modeling;predicting words;language modeling model;language models present;language models developed;specifically predict word;effective predicting words;new language modeling;language models important;language model;word prediction;prediction words;predict word sdlim;words sememe predictions;novel language model;language model sdlim;linguistic models;language modeling named;common language modeling;predict word;word level language"}, "9dbd86f089c2132dc46d316750d9786d60d5d720": {"ta_keywords": "cora web based;based annotation tool;web based annotation;manual annotation;annotation tool;annotation;annotated data;annotation process supports;cora web;based annotation;annotation process;annotation tool manual;manual annotation historical;newly annotated data;annotated;cora;newly annotated;taggers newly annotated;tool manual annotation;annotation historical;language data;annotation historical non;language data allows;standard language data;token boundaries annotation;boundaries annotation process;boundaries annotation;retraining taggers;retraining taggers newly;immediate retraining taggers", "pdf_keywords": ""}, "6f1ca0249eafa36a5762ac53f6ba2a4ee2133456": {"ta_keywords": "speech recognition corpus;english speech recognition;transcribed audio collected;transcribed audio;hours transcribed audio;speech recognition;domain english speech;recognition corpus 10;000 hours transcribed;audio suitable supervised;recognition corpus;tramnscribed audio objectivethis;corpus 10 000;corpus;hours tramnscribed audio;corpus 10;labeled audio;audio collected;hours transcribed;tramnscribed audio;audio collected audiobooks;audio objectivethis;collected audiobooks;audio objectivethis paper;hours total audio;labeled audio suitable;multi domain english;introduces gigaspeech;gigaspeech;audio", "pdf_keywords": "corpus audio transcripts;generating audio transcripts;transcribed audio collected;audio transcripts implement;audio transcripts process;transcribed audio;corpus audio data;gigaspeech corpus audio;audio transcripts various;audio transcripts;creating corpus audio;speech recognition corpus;corpus audio;creating corpus;new corpus based;gigaspeech corpus;transcribed audio used;corpus speech recognition;speech recognition toolkits;text processing speech;recognition new corpus;new corpus;processing speech;audio data generated;corpus speech;development corpus speech;transcripts implement;highly diverse corpus;domain speech recognition;end gigaspeech corpus"}, "22b7a7c9faa8f340520ae1418c9cf8d960aaeec0": {"ta_keywords": "neuro symbolic reasoner;symbolic reasoner neuro;reasoner neuro symbolic;neuro symbolic knowledge;symbolic reasoner closely;symbolic reasoner;knowledge based reasoning;dataflow query language;symbolic knowledge based;neuro symbolic;based dataflow query;reasoner neuro;reasoning based set;set based dataflow;dataflow query;reasoning based;based reasoning based;based dataflow;symbolic knowledge;reasoner closely integrated;based reasoning;dataflow;variants neuro symbolic;neural language;neuro;reasoner closely;modern neural language;knowledge based;query language;reasoner", "pdf_keywords": ""}, "eeec05fc11b2e0b40b3b0800bc50930e240cafeb": {"ta_keywords": "prerequisite structure corpus;textual information sources;corpus semantic;corpus semantic impact;textual information;structure corpus semantic;structure corpus;corpus;domain textual information;online technical material;textual;access technical publications;semantic impact documents;technical material using;comprehend complex technical;prerequisite structure;domain textual;open domain textual;information sources;information sources means;technical material;semantic impact;technical publications;complex technical material;semantic;material using statistical;documents individual reader;model prerequisite structure;impact documents;using statistical methods", "pdf_keywords": ""}, "525b7f73744f5650391be4678d6d51ddaf23ed72": {"ta_keywords": "measured errorror models;measuring errorrors;measuring errorrors article;methods measuring errorrors;measuring measurement errorror;errorror models important;errorror nonlinear models;measurement errorror nonlinear;validity measured errorror;errorror models;measured errorror;measurement errorror;validity measured;errorrors;method measuring measurement;methods measuring;errorror nonlinear;method measuring;nonlinear models;new methods measuring;errorrors article present;errorrors article;measuring measurement;new method measuring;measuring;measurement;measured;models;validity;models important", "pdf_keywords": ""}, "d338bcd1e34a8259e123465203b05c5bf21aa12a": {"ta_keywords": "correction preserving speaker;preserving speaker individuality;japanese speech synthsynthesisbased;ja speaker framework;speaker individuality ja;japanese voices effective;english speech synthe;introductionprosody correction preserving;captures speaker individuality;read japanese voices;english acoustic model;model speaker dependent;japanese voices;speech synthsynthesisbased humanmm;speaker individuality english;read japanese speech;speaker individuality;preserving speaker;individuality ja speaker;speaker framework;speaker framework using;ja speaker;speech synthe;model speaker;directly model speaker;japanese speech;introductionprosody correction;speaker dependent acoustic;build english acoustic;model captures speaker", "pdf_keywords": ""}, "095bc69eddbf73fabf58a929d2be9a99c1b533a6": {"ta_keywords": "preflib library preferences;preflib library;introduce preflib library;www preflib;http www preflib;preflib preflib preflib;preflib preflib;www preflib org;preferences;introduce preflib;library preferences online;library preferences;preferences online;preflib;preflib org preflib;org preflib preflib;preferences online resource;preflib html;org preflib;preflib org;html introduce preflib;preflib html introduce;org preflib html;library preferences http;preferences http;library;preferences http www;resource;www;online resource", "pdf_keywords": ""}, "85099e075880a4844f3de77006a80c73daf99a4c": {"ta_keywords": "adapting simplenlg german;german based simplenlg;simplenlg german;introduction adapting simplenlg;based simplenlg;adapting simplenlg;syntax german implementation;paper simplenlg german;simplenlg german surface;simplenlg;syntax german;simplenlg german aim;features syntax german;paper simplenlg;realisation engine german;syntax;based simplenlg gatt;simplenlg gatt;word order;word order phenomena;grammatical coverage demonstrated;german implementation;german surface realisation;phenomena grammatical coverage;realisation engine;simplenlg gatt andreiter;grammatical coverage;surface realisation engine;focus word order;features syntax", "pdf_keywords": ""}, "f7ce4c7ec30c846cc122393deee98f1eacd24049": {"ta_keywords": "dialogue state tracker;term memory lstm;short term memory;dialogue state;lstm;memory lstm neural;construct lstm;memory lstm;lstm neural;lstm neural networks;utterances dialogue;lstm network;receives utterances dialogue;dialogue state current;utterances dialogue participants;utterances separated vectors;outputs dialogue state;methods construct lstm;dialogue;dialogue participants input;construct lstm network;tracker based long;dialogue participants;propose dialogue state;outputs dialogue;state current utterance;utterances;lstm network receives;state tracker based;input utterances", "pdf_keywords": ""}, "bd24b47165407a8b2d32016645ca71f7c9213636": {"ta_keywords": "classify email speech;email speech acts;classify email;learning classify email;useful classify email;classify email according;email speech;typical email speech;classify email fashion;describing typical email;acts useful classify;email according intent;speech acts;speech acts useful;email according;email fashion class;email;learning classify;classify;intent sender propose;speech;useful classify;typical email;introduction learning classify;intent sender;email fashion;class corresponds verbnoun;taken predefined ontology;acts useful;according intent sender", "pdf_keywords": ""}, "b54d49cdf57abf7f7e7bcc0e946f450fd807f829": {"ta_keywords": "inverse reinforcement learning;inverse reinforcement;problem inverse reinforcement;reinforcement learning irl;reinforcement learning;estimating reward;agent attempting maximize;estimating reward function;maximize cumulative rewards;simple reward;reinforcement;learning irl;represented simple reward;simple reward combined;focus estimating reward;reward function;reward function best;agent policy demonstrated;approaches problem inverse;expert agent policy;reward;attempting maximize;learning irl focus;control task;behavior control task;cumulative rewards;attempting maximize cumulative;inverse;agent policy;constraints setting agent", "pdf_keywords": "inverse reinforcement learning;inverse reinforcement;problem inverse reinforcement;method inverse reinforcement;demonstrations optimal constraint;demonstrations optimal;constraints behavior agents;demonstrations use constraints;inferring probability demonstrations;reinforcement learning interfering;optimal control task;agent attempting maximize;constraints observed behaviors;maximum entropy inferring;probability demonstrations experts;policy optimal;policy policy optimal;estimating reward;estimating reward function;demonstration likelihood maximization;set demonstrations optimal;maximum entropy;model human demonstrations;action feature constraints;principle maximum entropy;reinforcement learning;constraint inference synthetic;constraint inference;reinforcement;model explain demonstrations"}, "1a2410823486613e327892f05b38d3070f2d712c": {"ta_keywords": "local localization world;local local localization;local localization local;local localization;localization local local;local local local;localization local;local local;localization world;localization;role local local;local;literature role local;role local;article provide overview;article;overview;world;purpose article;article provide;literature role;literature;purpose article provide;overview literature;provide overview;provide overview literature;overview literature role;role;purpose;provide", "pdf_keywords": ""}, "3bb1e24eb3429f807397833105d1e137d9927767": {"ta_keywords": "active sequence labeling;sequence labeling tasks;sequence labeling resultsour;sequence labeling methods;sequence labeling;resource sequence labeling;backgroundactive learning;leveraging human annotations;backgroundactive learning important;labeling tasks;labeling tasks current;improve label efficiency;label efficiency active;human annotations;effective data augmentation;labeling methods;labeling;active sequence;human annotations methodswe;method improve label;improve label;labeling methods use;efficiency active sequence;labeling resultsour;data augmentation;annotations;backgroundactive;labeling resultsour method;label efficiency;data augmentation method", "pdf_keywords": "sequence labeling tasks;sequence labeling datasets;sequence labeling dataset;facilitate sequence labeling;active sequence labeling;sequence labeling framework;augmentation sequence labeling;sequence labeling methods;employ sequence labeling;sequence labeling;approach sequence labeling;labeling sequences dataset;sequence labeling task;sequence labeling active;sequence labeling using;sequence labeling model;sequence labeling lowresource;sequence labeling resultsour;use sequence labeling;model sequence labeling;using sequence labeling;sequence labeling methodsby;framework sequence labeling;sequence labeling method;labeling sequences;resource sequence labeling;labeling active sequence;labeling using sequence;leveraging human annotations;labeled sequences queried"}, "17c9a0f1a287c08bb2c1c1df47fa51ce1e428c4e": {"ta_keywords": "speech recognition new;recognition speech recognition;speech recognition speech;speech recognition;approach speech recognition;recognition speech;speech recognition important;tool speech recognition;recognition new approach;new approach speech;recognition new;tool speech;recognition important tool;approach speech;important tool speech;recognition;speech;recognition important;new approach;important tool;tool;approach;new;important", "pdf_keywords": ""}, "a064010cf6fe594b2506a8fecd16dc0040211daa": {"ta_keywords": "neural machine translation;multilingual transfer nm;multilingual transfer;effectiveness multilingual transfer;machine translation;machine translation nmt;high resource language;low resource languages;nm models translate;translation nmt low;models translate;models translate lall;improve effectiveness multilingual;multilingual;resource languages lrl;translation nmt;resource language;transfer nm models;decoder word embedding;languages lrl effective;resource languages;effectiveness multilingual;resource language hrl;language hrl;languages lrl;nmt low resource;word embedding;performance neural machine;improve performance neural;performance neural", "pdf_keywords": "integrating multilingual translation;neural machine translation;improves translation accuracy;integrating multilingual neural;multilingual neural machine;translation multilingual translation;machine translation improves;multilingual translation method;multilingual translation multilingual;multilingual translation;translation multilingual;multilingual neural;machine translation decoupled;assist multilingual transfer;massively multilingual neural;translation nmr low;improves translation;multilingual transfer;translation improves translation;machine translation nmr;translation accuracy low;transformation assist multilingual;low resource languages;nonlingual decoders;machine translation;translation decoupled encoding;systematic decoders nonlingual;introduce multilingual target;nonlingual decoders use;multilingual target word"}, "2fbb75d7947808698f1554e4d400ec5ecb5ef998": {"ta_keywords": "multiple choice readingcomprehension;choice readingcomprehension long;choice reading comprehension;multiple choice reading;choice readingcomprehension;reading comprehension particular;reading comprehension;readingcomprehension long documents;readingcomprehension long;documents weighted global;normalization multiple choice;long documents weighted;readingcomprehension;answer selection;improving answer selection;attention multiple choice;choice reading;weighted global normalization;answer selection long;global normalization multiple;documents weighted;normalization multiple;introductionweighted global normalization;global normalization;selection long documents;normalization;attention multiple;span prediction;reading;span prediction models", "pdf_keywords": "reading comprehension model;reading comprehension challenge;reading comprehension dataset;reading comprehension longer;improving answer selection;answer selection task;question answering long;answer selection helps;challenging reading comprehension;comprehension model paragraph;answer selection aids;narrativerativeq answer selection;question answering;adapted answer selection;answer selection;reading comprehension;answering long context;answer selection long;dataset answer selection;apply reading comprehension;reading comprehension applying;comprehension longer contexts;choice reading comprehension;comprehension model;comprehension dataset;difficulty reading comprehension;rank difficulty reading;scores answer candidates;comprehension longer;long documents weighted"}, "c7c93601b52b1bcc68ec1f8b2c77c54f1b358ab9": {"ta_keywords": "testing pairwise comparison;pairwise comparison data;sample testing pairwise;pairwise comparison;comparison data;ratings converted comparisons;testing pairwise;comparison data provided;analysis peer grading;question comparison data;comparisons examples;comparisons;comparison data instance;converted comparisons;converted comparisons examples;comparisons examples include;comparison data role;similar ratings converted;introductiontwo sample testing;sample testing;peer grading;comparison;data analysis peer;distributed similar ratings;analysis peer;peer grading paper;data instance crowdsourcing;crowdsourcing;instance crowdsourcing;question comparison", "pdf_keywords": ""}, "e59adee86b666ad76164b3446cfee5068a15e5c9": {"ta_keywords": "network inference gpus;inference gpus emerging;inference gpus;neural network deployment;fault tolerance abf;fault tolerance;based fault tolerance;tolerance abf neural;gpus emerging strategy;deployment propose adaptive;neural network inference;network inference exploits;algorithm based fault;gpus emerging;gpus;emerging deployment scenarios;abf neural network;af schemes best;af schemes;inference exploits;network deployment propose;guided af propose;network inference;af propose intensity;investigate af schemes;emerging deployment;inference exploits untapped;neural network;abf neural;tolerance abf", "pdf_keywords": "inference optimized gpus;logic inference gpus;network inference gpus;algorithm inference gpus;inference gpus promising;fault tolerance nns;inference gpus propose;inference gpus;fault tolerance nn;computations improve fault;ns inference gpus;fault tolerance methods;fault tolerance neural;algorithmthm based fault;inference gpus focus;improve fault tolerance;fault tolerance convolutional;guided fault tolerance;tolerance nn inference;fault tolerance practical;approaches detecting faults;inference gpus demonstrate;detecting faults;computational capabilities gpus;gpus high compute;algorithm based fault;approach fault tolerance;based fault tolerance;error detection;driven fault tolerance"}, "e6fa88f4af68aa7be4ae91940892eee52571997c": {"ta_keywords": "shot object detection;detecting rare objects;object detection task;crucial shot object;simple shot object;meta learning promising;detectors rare classes;shot object;object detection;classes crucial shot;detection detecting rare;meta learning;simple shot;object detection detecting;introductionfrustratingly simple shot;attention fine tuning;detectors rare;detecting rare;detection task;detection task simple;rare objects;rare objects examples;detection;objects examples emerging;detectors;existing detectors rare;detection detecting;attention;attention fine;works meta learning", "pdf_keywords": "detection benchmarks coco;object detection benchmarks;object detecting coco;object detection large;novel object detection;shot object detection;object detector balanced;substantially improve detection;improve detection accuracy;improving detection accuracy;object detector;layer object detector;benchmarks coco new;object detection new;benchmarks coco;effective improving detection;crucial shot object;object detection network;detecting novel objects;neural networks shot;improving detection;object detection adopt;detecting coco;object detection generalized;detecting shot object;outperforms meta learning;attention fine tuning;novel objects datasets;object detection reported;detection accuracy outperforming"}, "db0a3ce9f315f650fe5220101c5677778de39fee": {"ta_keywords": "machine translation reordering;parser machine translation;translation reordering using;discriminative parser optimize;translation reordering;discriminative parser machine;introductioninducing discriminative parser;learning discriminative parser;discriminative parser;inducing discriminative parser;translation reordering important;machine translation;parser optimize;use machine translation;parser;parser machine;parser optimize use;parallel text;aligned parallel text;methods inducing discriminative;method learning discriminative;inducing discriminative;learning discriminative;introductioninducing discriminative;reordering using;automated methods inducing;discriminative;automated automated methods;automated methods;automated automated", "pdf_keywords": ""}, "06f4de06fc37576e1e381cd76e375d57852047b9": {"ta_keywords": "translation multi task;neural machine translation;machine translation multi;backgroundimproving robustness neural;robustness neural;robustness neural machine;translation multi;multi task learning;machine translation;multi task;task learning;task learning pausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepausepause;backgroundimproving robustness;robustness;translation;neural machine;neural;task;backgroundimproving", "pdf_keywords": ""}, "cd3595f65519e4af6bcd073790ac32acdafadf55": {"ta_keywords": "target domains demonstrate;generating high quality;target domains;source target domain;target domains including;different target domains;high quality results;domains demonstrate effectiveness;target domain;effectiveness algorithm generating;quality results;algorithm generating;source target;domains demonstrate;algorithm generating high;quality results different;results different target;dissimilarity source target;domains;method generating;method generating high;generating;domains including;domains including extremely;target;demonstrate effectiveness algorithm;generating high;effectiveness algorithm;different target;algorithm", "pdf_keywords": "approach generating gans;shot image generation;generative model pretrained;generating gans;adapt pretrained generative;pretrained generative model;unconditional image generation;pretrained generative;image generation;generative adversarial networks;generating gans real;image generation seeks;generative model learned;architecture generative adversarial;generative adversarial;adapt weights generative;weights generative model;weights generative;pretraining human faces;image generation low;learned source domain;source domain pretraining;artistic domains;generative;generative model;gans;method artistic domains;generative model new;generator architecture generative;pretrained source domain"}, "6887537de3655a25c75bf4d0833f51e72331bdad": {"ta_keywords": "signal extended audio;extended audio signal;noisy signal extended;converts noisy audio;enhanced audio signal;signal enhanced audio;extended audio;audio signal enhanced;detecting noisy audio;converting noisy signal;noisy audio signal;audio signal processed;audio signal environment;noise signals processed;audio signal;enhanced audio;audio signal proposed;signal extended;environment noisy audio;signal processed extension;signal detecting noisy;noisy audio;audio signal detecting;signals processed extension;noise signals;converts noisy;detecting noisy;extension network network;converting noisy;method converting noisy", "pdf_keywords": ""}, "a8fc183c089bd596ccc48b3d666f8814e1b41e55": {"ta_keywords": "trained generate code;generate code;program synthesis;program synthesis left;descriptionincoder trained generate;generate code files;perform program synthesis;introductiondecode;case descriptionincoder trained;case descriptionincoder;introductiondecode seldom written;infill case descriptionincoder;descriptionincoder;right generation editing;introductiondecode seldom;descriptionincoder trained;generation editing;code files;trained generate;unified generative;incoder unified generative;generation editing infill;code regions;code;generative;licensed code;code regions code;unified generative model;generate;generative model", "pdf_keywords": "generative code models;trained generate code;generative code;code generation;neural code generation;masked code models;language guided synthesis;code training inference;causal masking generate;generating code;infilling generation code;code generation task;causal masking contextwe;contexts zero shot;generate code;generation editing infilling;code infilling editing;corpus generative;code able generate;automatically generate masked;code models training;large corpus code;generate code files;masked language models;generate causal masked;comparable generative code;code zero shot;code infilling;generation editing;domains code training"}, "9a43dda4b01dde5d513c431564098e4d8794a7a5": {"ta_keywords": "sentiment datasets;document level sentiment;large sentiment datasets;sentiment analysis;level sentiment analysis;sentiment analysis methodswe;sentiment datasets czech;method large sentiment;semi supervised method;word cluster features;semi supervised;new semi supervised;level sentiment;large sentiment;word cluster;sentiment;movie reviews;adding word cluster;clusters words;english movie reviews;clusters words represented;movie product reviews;supervised method document;supervised method;exploit clusters words;product reviews english;reviews english movie;product reviews;cluster features;cluster features features", "pdf_keywords": ""}, "2f201c77e7ccdf1f37115e16accac3486a65c03d": {"ta_keywords": "adversarial examples care;adversarial examples;adversarial examples inspiration;adversarial;vulnerable adversarial examples;guard adversarial examples;vulnerable adversarial;wild guard adversarial;guard adversarial;known vulnerable adversarial;game adversary;game adversary model;adversary model;adversary;sum game adversary;adversary model general;general games optimal;games optimal;games optimal strategy;deep learning systems;zero sum game;model general games;minimax zero;deep learning;images imperceptible humans;learning systems wild;optimal strategy;real images imperceptible;optimal strategy players;game theory", "pdf_keywords": "training adversarial;adversarial defense;adversarial approach pretrained;adversarial training;effective adversarial;additional training adversarial;guard adversarial;adversarial training yields;training adversarial setting;protect networks adversarial;effective adversarial examples;adversarial;pruning neural networks;method adversarial defense;guard adversarial examples;adversarial defense supervised;method adversarial;adversarial examples care;networks adversarial examples;networks adversarial;apply adversarial;adversarial examples;adversarial examples computational;adversarial examples present;vulnerable adversarial;adversarial examples inspiration;adversarial setting;problems apply adversarial;adversarial examples algorithm;adversarial examples density"}, "136235d2a3dc4f1c995eaf977aec9c42114da850": {"ta_keywords": "systems predictions conclusionstranslation;predictions conclusionstranslation;language families evaluate;error analysis systems;analysis systems predictions;error analysis;language families;systems predictions;extensive error analysis;systems predictions resultsthe;13 language families;languages;languages 13 language;32 languages;32 languages 13;data 32 languages;languages 13;conclusionstranslation;language;systems predictions methodsthe;13 language;systems new data;predictions resultsthe authors;equally systems new;predictions methodsthe authors;systems new;analysis systems;systems;contributed equally systems;predictions resultsthe", "pdf_keywords": ""}, "8a09c90f6e9a3f6c3b172e5059c7af47f528f66b": {"ta_keywords": "reinforcement artistic typography;text visually appealing;thematic reinforcement artistic;memorable semantic reinforcement;artistic typography;text visually;appealing memorable semantic;make text visually;semantic reinforcement;visually appealing memorable;semantic reinforcement called;semantic reinforcement use;reinforcement artistic;troeat thematic reinforcement;artistic typography results;approach semantic reinforcement;thematic reinforcement;memorable semantic;typography;text;make text;reinforce message;reinforcement use visual;reinforce message google;visually appealing;theme word;artistic;use visual cues;approach make text;used reinforce message", "pdf_keywords": "reinforcement artistic typoography;word recognition theme;memorable semantic reinforcement;text visually appealing;recognition theme;visual depiction text;depicted theme recognition;theme recognition novel;appealing memorable semantic;based text clipart;theme recognition;learn representation captures;text visually;recognition theme recognition;word depicted theme;approach text theme;thematic reinforcement artistic;theme text cliparts;make text visually;depiction text;visually appealing memorable;artistic typoography given;visual similarities cliparts;theme recognition subject;artistic typoography;generating text;depiction text using;approach generating text;theme based cliparts;representation captures"}, "1022696090666eab5c82ebc07d63c0de2fca2521": {"ta_keywords": "text classification;text classification using;generalize text classification;classification using iridol;joins generalize text;classification using;introduction joins generalize;classification;introduction joins;joins generalize;generalize text;joins;iridol wir;iridol wir wir;iridol;using iridol wir;text;using iridol;wir;generalize;wir wir;introduction;wir wir wir;using", "pdf_keywords": ""}, "71124b00b873e85aa55b07100cd5b492e5b1d73d": {"ta_keywords": "quantum key distribution;based quantum key;quantum key;encryption secret sharing;introductionthe quantum key;secret sharing experimentally;authenticated secret sharing;secret sharing;secret sharing scheme;pad encryption secret;key distribution network;encryption secret;secret sharing powerful;shakemirs secret sharing;pad encryption;secure contacts distributed;encryption;scheme based quantum;authenticated secret;sharing powerful security;key distribution;based quantum;password authenticated secret;distributed storage;network shakemirs secret;distribution network vernam;theoretically secure;quantum;introductionthe quantum;contacts distributed storage", "pdf_keywords": "ionizing message authentication;integrity protection scheme;dispute data integrity;proposing integrity protection;integrity protection;secure using quantum;verification distributed storage;integrity data verify;data verify integrity;data integrity;transmission data integrity;integrity protection realized;timestamp integrity protection;secret sharing implement;data integrity developed;verification timestamp integrity;verify integrity data;secure data transmission;authenticated secret sharing;secure data;cryptography quantum;implementing message authentication;verify integrity;verification implement shared;cryptography quantum cryptography;data introduce trusted;integrity data;quantum cryptography;arbitrary secret sharing;theoretically secure data"}, "eb7a64195ef4a268f79fa6740f128387f2696c65": {"ta_keywords": "inverse reinforcement;inverse reinforcement learning;uses inverse reinforcement;reward environment learn;ai agents ethical;demonstrations reinforcement learning;teaching ai agents;agents maximize reward;demonstrations reinforcement;agents ethical values;reinforcement learning learn;maximize reward environment;teaching ai;constraints demonstrations reinforcement;reward environment;agents ethical;ai agents;learn maximize environmental;reinforcement;maximize environmental rewards;reinforcement learning;introduction teaching ai;learning learn maximize;environment learn;allow agents maximize;agents maximize;maximize reward;environment learn follow;learn maximize;environmental rewards", "pdf_keywords": ""}, "6cddfbed35c46937588bd9d6b846ca2855953cea": {"ta_keywords": "inputs word lattices;input neural sequence;representing inputs word;speech recognizer stream;neural sequence sequence;sequence sequence model;neural sequence;sequence model;recognizer stream models;input neural;tagger speech recognizer;word segmenter speech;stream word segmenter;segmenter speech tagger;inputs word;speech recognizer;introductionthe input neural;segmenter speech;sequence model determined;word lattices allows;word segmenter;alternative sequences posterior;word lattices;stream models;sequences posterior probabilities;determined stream word;sequences posterior;capturing alternative sequences;speech tagger;stream word", "pdf_keywords": "attentional lattice sequence;words lattice models;inputs word lattices;use attentional lattice;attentional lattice;speech translation lattice;lattice information speech;lattice translation speech;output words lattice;lattice tosequence model;sequential auditory auditory;sequential lattice inputs;sequential auditory;word lattice information;lattice sequence model;decoder lattice encoder;lattices assign word;tuned sequential auditory;algorithm decoder lattice;lattice encoder;speech recognizer stream;decoder lattice;word lattices allows;word lattice;lattice transcripts;lattice training;use word lattice;takes word latticethe;translation lattice models;lattice tosequence"}, "58a2e825884bc86e650fffafb86a2833117852c5": {"ta_keywords": "pre trained models;model hubs pre;hubs pre trained;model hub;model hubs;introduction model hubs;hubs pre;provided model hub;model hub popularity;trained models pts;cornerstone deep learning;pre trained;trained models;deep learning built;models pts;deep learning;models;models pts cornerstone;hub;pt provided model;model;hubs;learning built high;learning;learning built;pre;hub popularity;cornerstone deep;pts cornerstone deep;trained", "pdf_keywords": "models pre trained;trained models pre;pre trained models;optimizing pre trained;ranking pre trained;pre trained model;model hubs ranking;prediction pre trained;hubs ranking tuning;optimal pre trained;trained models fast;imagenet pre trained;tuning pre trained;data pre trained;trained models ranking;ranking tuning pre;metric pre trained;exploiting model hubs;supervised pre trained;hub optimizing;hub optimizing cluster;trained models sequential;knowledge large hub;learning large scale;use pre trained;paradigm optimizing hub;optimizing hub;optimizing network hub;models development deep;optimizing hub optimizing"}, "e6cec3044688f1701b4b72b4b2189f215abc3759": {"ta_keywords": "eliciting truthful responses;challenge faced crowdsourcing;truthful responses agents;tasks like labeling;crowdsourcing;messageinentivizing effort eliciting;effort eliciting truthful;crowdsourcing types evaluation;evaluation tasks like;responses agents;like labeling;like labeling images;clinical messageinentivizing effort;crowdsourcing types;evaluation tasks;faced crowdsourcing;effort eliciting;new reward mechanisms;reward;reward mechanisms settings;messageinentivizing effort;propose new reward;faced crowdsourcing types;labeling;eliciting truthful;reward mechanisms;eliciting;labeling images grading;key clinical messageinentivizing;verifiability major challenge", "pdf_keywords": ""}, "549df5fc83c382cbdf633dc782fa67bf2f983f2c": {"ta_keywords": "neural networks private;networks private mixtures;private mixtures training;activations differentially private;gradient descent random;staochastic gradient descent;private mixtures;differentially private;descent random mixtures;differentially private train;random mixtures gdm;propose staochastic gradient;staochastic gradient;networks private;mixtures gdm simple;private train nonlinear;mixtures gdm;random mixtures;threats gdm converges;gradient descent;nonlinear neural networks;networks linear activations;neural networks linear;mixtures training data;train nonlinear neural;descent random;mixtures training;nonlinear neural;protecting data;deep neural", "pdf_keywords": ""}, "f61886d138497431cbeaa7bb73051bfb7a745026": {"ta_keywords": "scene graph generation;graph generation holistic;scene graph;holistic contextual supervision;linguistic structures captions;contextual supervision;contextual supervision intuitively;captions benefit scene;information scene graph;structures captions;graph generation;categorical supervision;graph generation results;graph generation requires;structures captions benefit;generation holistic task;holistic contextual;captions benefit;task holistic contextual;work scene graph;benefit scene graph;contextual;requires categorical supervision;captions;categorical supervision level;supervision intuitively improve;generation requires categorical;subjects objects predicates;supervision intuitively;objects predicates relate", "pdf_keywords": "caption supervision scene;leverages caption supervision;caption supervision;scene graph generation;image level supervision;supervision scene graph;weakly supervised scene;supervised scene graph;generation weakly supervised;improve scene graph;holistic contextual supervision;generation scene graph;generate scene graphs;weakly supervised object;contextual supervision;visual objects linguistic;visual entities predict;graph generation scene;supervised scene;scene graphs;scene graph;contextual supervision intuitively;captions benefit scene;develop scene graph;generate scene;based scene graph;localize visual entities;introduce weakly supervised;recognition visual entities;visual entities relations"}, "275aaa20ba853c40a461f224eefbf06730bf03a9": {"ta_keywords": "nonconvex optimization;nonconvex optimization paper;topic nonconvex optimization;optimization paper propose;algorithm smooth function;gradient based algorithm;optimization paper;propose simple gradient;nonconvex;optimization;simple gradient based;gradient;gradient based;topic nonconvex;simple gradient;epidemiologically stochastic setting;algorithm smooth;research topic nonconvex;smooth function;epidemiologically epidemiologically stochastic;epidemiologically stochastic;stochastic setting;based algorithm smooth;stochastic;stochastic setting central;algorithm;introductionepidemiologically epidemiologically epidemiologically;introductionepidemiologically epidemiologically;smooth;epidemiologically", "pdf_keywords": "accelerated gradient descent;accelerated gradient descents;gradient descent;gradient descents accelerated;perturbed gradient descent;gradient descents provide;descents accelerated gradient;algorithm escape saddle;optimization accelerated gradient;gradient descents;gradient descent method;simple gradient descent;gradient descent algorithm;method accelerated gradient;gradient descent perturbed;gradient descent useful;convergent gradient descent;application gradient descent;saddle point method;accelerated gradient;gradient descent based;escaping saddle points;neighborhood saddle point;perturbed accelerated gradient;perturbation algorithm robust;gradient point;gradients nonconvex optimization;escape saddle points;gradient algorithm;using gradient descent"}, "e6924d247b56980260e4c68dbc51b947409e4764": {"ta_keywords": "synthesis new asymmetric;new asymmetric asymmetric;asymmetric asymmetric asymmetric;new asymmetric;asymmetric asymmetric;asymmetric;synthesis;synthesis new;new", "pdf_keywords": "stochastic gradient descent;stochastic gradient estimators;stochastic gradient methods;gradient estimators convex;local stochastic gradients;distributed stochastic gradient;stochastic gradient algorithms;local stochastic gradient;stochastic gradient local;stochastic gradients local;stochastic gradients convex;gradient known stochastic;known stochastic gradient;gradient stochastic gradients;method distributed optimization;stochastic gradients;gradients local stochastic;stochastic gradient known;method stochastic gradient;stochastic gradients used;stochastic gradients sgs;local gradient estimators;analysis stochastic gradients;stochastic gradients described;distributed optimization;gradient descent;stochastic gradient new;gradients stochastic;stochastic gradients stochastic;gradient stochastic"}, "a1fc0041ef89ed5371317c8e2cc5effa8f38ae48": {"ta_keywords": "causal discovery methods;causal discovery;bayesian network structure;introduction causal discovery;network structure learning;research bayesian network;bayesian network;discovery methods rely;discovery methods;structure learning;large graphs;causal;assumption exact search;structure learning focuses;discovery;research bayesian;network structure;introduction causal;scale large graphs;bayesian;graphs;exact search methods;methods rely;methods rely faithfulness;search methods;line research bayesian;learning;search;learning focuses weakening;exact search", "pdf_keywords": "reliable causal discovery;networks causal discovery;causal discovery relying;discovery causal structures;causal discovery methods;causal discovery;causal discovery assumptions;causal discovery procedure;discovery causal;causal discovery method;networks causal;inferring causal structure;based causal discovery;bayesian network structure;discovery relying parsest;approach discovery causal;learn causal models;causal networks;backgroundthe causal discovery;generating causal models;causal networks causal;modeling causal networks;network structure learning;inferring causal;discovery relying;learn causal;generating causal;graphs search;discovery method structure;discovery procedure faithfulness"}, "0a227a21172f7344ad911aeefc40ae4ec82d7cac": {"ta_keywords": "metaphor natural language;metaphor important research;makes metaphor important;metaphor natural;metaphor;makes metaphor;metaphor important problem;metaphor important;experiments makes metaphor;cognitive linguistics automatic;cognitive linguistics;computational cognitive linguistics;language ai;semantics oriented nonlinguistic;natural language ai;semantics;linguistics automatic;linguistics;human reasoning;work metaphor natural;nonlinguistic application;language ai started;linguistics automatic identification;natural language;interpretation indispensable semantics;work metaphor;semantics oriented;indispensable semantics;oriented nonlinguistic application;nonlinguistic application work", "pdf_keywords": ""}, "178f424d0f156cbf5b35eb241fc00b27a0a3808b": {"ta_keywords": "lsm based speech;speech enhancement se;speech enhancement recognition;speech enhancement;performance speech enhancement;based speech enhancement;short term memory;effective modeling speech;speech recognition;speech recognition ar;noise robust speech;term memory lsm;robust speech recognition;lsm recurrent neural;modeling speech achieved;memory lsm recurrent;modeling speech;robust speech;recurrent neural network;recurrent neural networks;memory lsm based;automatic speech recognition;memory recurrent neural;term memory recurrent;automatic speech;memory lsm;lsm recurrent;learning long short;speech achieved;task learning long", "pdf_keywords": ""}, "317ed59456d76b500a7eb63b181df9e8b795976b": {"ta_keywords": "framework urban parking;observe queuing game;urban parking;parking urban;model parking urban;nash induced welfare;queuing game framework;queuing game;parking urban centers;urban parking methodswe;parking methodswe model;parking;queues overlay game;methodswe model parking;maximizers consider games;model parking;parking methodswe;observe queuing;socially optimal welfare;optimal welfare;queuing;queue;welfare socially optimal;game theoretic;queues;observe queue;game theoretic structure;utility maximizers consider;utility maximizers;nash induced", "pdf_keywords": "optimal payment parking;management traffic congestion;optimal allocation parking;parking options optimal;queues congestion;network queues congestion;traffic congestion;optimal allocation traffic;allocation traffic traffic;allocation traffic;allocation parking;queues congestion occupancy;congestion versus utilization;queuing game flow;traffic networks game;parking options;parking parking options;traffic fundamental principle;allocation parking parking;reduced allocation parking;limit congestion;impact traffic congestion;rate limit congestion;simulation traffic traffic;flow network queues;traffic congestion urban;based queueing game;connect queuing game;network queues;queuing game"}, "56823e326f2515f73662b176054fbee0895e0c44": {"ta_keywords": "thousands forms navigate;search navigate;forms navigate;forced search navigate;navigate familiar tasks;navigate;navigation;users faced navigation;forms navigate familiar;request large sap;search;particular update request;faced navigation problem;navigation problem finding;tasks users various;complex tasks users;familiar tasks users;accomplish update request;update request ww;sap;navigate familiar;forced search;forms;users forced search;sap 10;update request;accomplish update;particular update;tasks users;faced navigation", "pdf_keywords": ""}, "7891ec1d8ba2abf238326dc6e8862cc4431a6f5c": {"ta_keywords": "lattice path markov;random lattice path;deployment multihop wireless;wireless network deployment;impromptu deployment multihop;steps random lattice;network deployment operative;network deployment;path markov evolution;path markov;multihop wireless network;random lattice;impromptu deployment;probabilities path continue;multihop wireless;markov evolution;steps random;lattice path;probabilities path;deployment multihop;markov evolution methods;various probabilities path;wireless network;problem impromptu deployment;markov;step various probabilities;deployment;deployment operative steps;deployment operative;network", "pdf_keywords": ""}, "cdf5eb63e9c2434073e811aba50ae80ede9d15f6": {"ta_keywords": "focused retrieval methodsthe;retrieval methodsthe document;collection focused retrieval;relevance using crowdsourcing;retrieval methodsthe;documents judged relevance;documents highly ranked;relevance using;focused retrieval;document corpus category;queries educational domain;judged relevance using;crowdsourcing resultsall sentences;methodsthe document corpus;highly ranked query;queries educational;document corpus;crowdsourcing resultsall;using crowdsourcing resultsall;retrieval;ranked query highly;fory queries educational;corpus category clueweb12;web based collection;based collection focused;method judged relevance;ranked query;clueweb12 collection;relevant documents judged;query highly effective", "pdf_keywords": ""}, "1afe82d34c182d43cbcc365d26e704058aa32351": {"ta_keywords": "voice conversion;voice conversion using;corpus voice conversion;based voice conversion;integration based voice;voice conversion miv;voice conversion vc;density model speaker;corpus voice;parallel corpus voice;model speaker model;model speaker;speaker model;voice;gaussian mixture model;speaker model mitigate;mixture model gmm;mixture model;based gaussian mixture;combines parameter generation;conversion miv methodswe;model integration based;conversion using dynamic;conversion vc based;gaussian mixture;conversion miv;parameter generation algorithm;conversion;speaker;based voice", "pdf_keywords": ""}, "2c871df72c52b58f05447fcb3afc838168d94505": {"ta_keywords": "knowledge neurons;activation knowledge neurons;concept knowledge neurons;knowledge neurons methodswe;knowledge stored pretrained;activation knowledge;factual knowledge stored;neurons express fact;knowledge;knowledge stored;concept knowledge;factual knowledge;fact propose knowledge;introducing concept knowledge;knowledge attribution;motors introducing concept;studies factual knowledge;pretrained motors;knowledge attribution method;resultswe activation knowledge;propose knowledge;motors;propose knowledge attribution;relational fact;pretrained motors introducing;stored pretrained motors;relational fact propose;neurons express;neurons;given relational fact", "pdf_keywords": "knowledge neurons pretrained;leveraging knowledge neurons;knowledge neurons;analysis knowledge neurons;leverage knowledge neurons;concept knowledge neurons;knowledge neurons edit;knowledge neurons accordingly;knowledge neurons express;activation knowledge neurons;knowledge neurons relational;knowledge neurons fundamental;pretraining factual knowledge;utilize knowledge neurons;knowledge neurons significantly;identifying knowledge neurons;knowledge neurons update;activate knowledge neurons;knowledge neurons activate;shows knowledge neurons;method knowledge neurons;knowledge neurons identified;transformers knowledge neurons;knowledge neurons methodswe;knowledge neurons tend;knowledge neurons activated;identify knowledge neurons;neurons activate knowledge;neurons identified knowledge;knowledge stored pretrained"}, "26c2aad87810418b09e0f5b80352dd4d2536afe3": {"ta_keywords": "social skills training;improve social skills;social skills attempt;acquire social skills;social skills;procedure social skills;train improve social;anxiety discomfort social;discomfort social interaction;computer based training;decrease human anxiety;discomfort social;social interaction;improve social;social interaction acquire;social skills resemble;interaction acquire social;human anxiety;skills training;social;human anxiety discomfort;skills attempt automate;human instructors;acquire social;skills resemble training;skills training st;used human instructors;procedure social;based training;human instructors article", "pdf_keywords": ""}, "07cedc7899497f2f4ee6f4736e03b78accb47b74": {"ta_keywords": "semi supervised learning;relational neighbor classifier;semi supervised;neighbor classifier;learning labeled;supervised learning network;learning labeled unlabeled;baseline semi supervised;labeled training data;neighbor classifier wvrn;learning network data;relational neighbor;required learning labeled;supervised learning;supervised learning methods;reduce labeled training;vote relational neighbor;learning network;weighted vote relational;goal semi supervised;supervised;methods reduce labeled;labeled training;labeled unlabeled instances;labeled unlabeled;reduce labeled;labeled;classifier;data required learning;learning methods reduce", "pdf_keywords": ""}, "d4d26ccbf1e64e725b5bffc08ab28a72e271facb": {"ta_keywords": "cross database context;database context dependent;dependent text sql;context dependent text;database context;context independent questions;text sql;sql queries biases;text sql xdt;context dependent;queries biases;cross database;context independent;introductionthe cross database;queries biases conceal;easy sql queries;easy sql;sql;dependent text;sql xdt;sql queries;independent questions high;queries;proportion context independent;sql xdt problem;high proportion context;biases existing datasets;context;existing datasets xdt;datasets xdt", "pdf_keywords": ""}, "26d5c7ad2778c77a1b8734dceb34fe38a1179e2f": {"ta_keywords": "malicious application dynamic;malicious applications app;malicious application detect;malicious application detection;novel malicious application;real time malicious;malicious application;malicious applications;background malicious application;time malicious application;various malicious applications;application detection;application detection model;model directly malwares;directly malwares;malwares limitation application;propose novel malicious;application detect;analysis various malicious;real time api;novel malicious;background malicious;directly malwares limitation;malwares;android devices real;malwares limitation;rt med android;dynamic dynamic detectingion;detectingion real time;dynamic detectingion real", "pdf_keywords": ""}, "9045bf2a9c1e2b9621c69c57f991d10880e91f18": {"ta_keywords": "hardness host statistical;computational hardness;infer computational hardness;algorithms computational hardness;computational hardness host;computational hardness variant;clique problem studied;planted clique problem;hardness variant planted;polynomial time algorithms;clique problem used;clique problem;variant planted clique;computational efficiency existence;host statistical problems;planted clique;algorithms computational;infer computational;problems equating computational;hardness variant;algorithms;computational efficiency;equating computational efficiency;computational phenomena associated;polynomial time;interesting computational;hardness host;clique;computational phenomena;interesting computational phenomena", "pdf_keywords": "problem complexity statistical;complexity statistical problems;randomized logspace hardness;computational hardness;computational hardness variant;complexity clique random;complexity statistical;algorithms computational hardness;efficiently studied complexity;infer computational hardness;hardness host statistical;complexity clique;computational hardness host;logspace algorithms statistical;logspace hardness reduction;transfer computational hardness;complexity class statistical;randomized logspace algorithms;studied complexity hypergraph;complexity clustering cliques;algorithms planted clique;logspace hardness;graph complexity clique;worst case complexity;complexity known planted;logspace algorithms randomized;reductions reasonable complexity;algorithms randomized logspace;randomized logspace reductions;clique problem computationally"}, "24a2f68cf81ba3ee55e7a87d0770374ab8e99858": {"ta_keywords": "learnability recursive logic;learnability recursive;recursive logic programs;pac learnability recursive;eecient learnability recursive;learnability function free;free recursive logic;learnability function;analyze learnability function;recursive logic;logic programs models;introductionthe pac learnability;polynomial predictability identiication;encoding analyze learnability;logic programs;learnability;programs models polynomial;pac learnability;analyze learnability;polynomial predictability;logic programs res;eecient learnability;models polynomial predictability;predictability identiication equivalence;boundaries eecient learnability;logic programs important;free recursive;predictability identiication;programs models;function free recursive", "pdf_keywords": ""}, "5ed4b17a4b7932619f0969e1f5acae76e90f7bdd": {"ta_keywords": "neural semantic parsers;recursive semantic parsing;semantic parsing;novel recursive semantic;utterances nested sql;semantic parsers;semantic parsing framework;semantic parsers usually;recursive semantic;introduction neural semantic;parsing framework;parse long complicated;parsers;parsing;query generation;neural semantic;parse;complicated utterances nested;parsers usually;utterances nested;query generation problems;sq query generation;nested sql queries;sql query layer;fail parse long;query generation problem;long complicated utterances;parsing framework called;fail parse;parse long", "pdf_keywords": ""}, "feb403bb5a064ab68b2db655b80a7417f7cfc9f3": {"ta_keywords": "relational learning;relationshipal learning hidden;relation tree based;novel relational learning;relational learning approach;relationshipal learning;introductionefefficient relationshipal learning;relation tree;features modeling relational;modeling relational;relational data markov;modeling relational data;called relation tree;relational data;novel relational;markov networks incorporate;relational;relational malformations;propose novel relational;data markov networks;markov networks;restricted class relational;class relational malformations;relational data flexibility;class relational;relational malformations called;learning hidden variableable;relation;learning hidden;introductionefefficient relationshipal", "pdf_keywords": ""}, "2fb44f1317bc51a1e011a5a44d817ad9104e29e8": {"ta_keywords": "differential privacy;introduction differential privacy;nl differential privacy;differential privacy various;differentially private;differentially private auto;applications differential privacy;differential privacy provides;differential privacy meets;analysis differentially private;privacy meets nonlodosynaptic;privacy provides formal;formal approach privacy;private auto encoder;privacy various;nonlodosynaptic nl differential;privacy;privacy individuals;privacy provides;privacy various scenarios;approach privacy individuals;privacy individuals applications;formal analysis differentially;approach privacy;differentially;privacy meets;individuals applications differential;analysis differentially;introduction differential;private auto", "pdf_keywords": "differential privacy;differentially private data;method differential privacy;differentially private autoencoder;applications differential privacy;differential privacy various;differential privacy provides;differential privacy dep;analysis differentially private;differentially private;differentially private determines;retrieve differentially private;formal approach privacy;privacy provides formal;fact differentially private;privacy opposed formal;private autoencoder;private autoencoder text;new approach privacy;private data;privacy enhanced method;algorithm breaches privacy;private data apply;approach privacy enhanced;individual privacy;privacy enhanced;privacy;individual individual privacy;randomization protect;privacy various"}, "b3848d32f7294ec708627897833c4097eb4d8778": {"ta_keywords": "language models dialog;models specialized dialog;models dialog applications;neural language models;transformerbased neural language;dialog data;dialog applications;language models;lamd language models;words public dialog;models dialog;public dialog data;language models specialized;dialog;specialized dialog;dialog data web;dialog applications case;public dialog;text model;neural language;specialized dialog 137b;web text model;text model scaling;dialog 137b;dialog 137b parameters;transformerbased neural;fine tuning annotated;present lamd language;lamd language;trained 56t words", "pdf_keywords": "improving groundedness perform;safety groundedness metrics;groundedness models;significantly improving groundedness;automated automated automated;groundedness using crowdworkers;groundedness metrics;tool analysis groundedness;automated automated;groundedness metrics use;safety groundedness models;improving groundedness;groundedness measure attributes;analyze responses groundedness;groundedness models article;improvements safety groundedness;automated;improves safety groundedness;model safety groundedness;analysis groundedness;grounding reasoning model;groundedness far human;use automated automated;dataset results groundedness;grounding reasoning;groundedness perform;results groundedness;groundedness using;quality safety groundedness;evaluate dialogs groundedness"}, "17c5e16d16585a01fbfd90ff39f6799952675b21": {"ta_keywords": "conversational bilingual speech;conversational bilingual;bilingual speech recognition;bilingual speech;introduction conversational bilingual;comprise bilingual speech;bilingual speech encompasses;tasks comprise bilingual;monolingual code switch;monolingual sub tasks;utterances purely monolingual;bilingual;monolingual code;likelihoods monolingual code;comprise bilingual;monolingual types;monolingual;monolingual types intra;methods defining monolingual;defining monolingual;purely monolingual;purely monolingual types;monolingual sub;defining monolingual sub;likelihoods monolingual;model likelihoods monolingual;speech recognition;sententially code switched;intra sententially code;types utterances", "pdf_keywords": "bilingual speech recognition;contextal bilingual;contextal bilingual speech;monolingual bilingual;bilingual speech;monolingual bilingual systems;switching monolingual bilingual;rn transducer monolingual;bilingual bilingual sequence;model decomposing bilingual;decomposing bilingual bilingual;bilingual bilingual;monolingual sub tasks;bilingual speech encompasses;bilingual bilingual bilingual;tasks comprise bilingual;switch language modeling;improved explicit monolingual;switching monolingual;monolingual code switch;decomposing bilingual;bilingual systems improvements;switch language models;conditional rn language;bilingual;code switching monolingual;switching language models;bilingual systems;task monolingual;recognition defining monolingual"}, "c204d40384d39c59cd7249bde4cd8615972acaac": {"ta_keywords": "robustness machine translation;machine translation robustustness;translation robustustness;current machine translation;machine translation systems;task machine translation;translation systems;translation robustustness methods;machine translation task;machine translation;translation systems ability;translation task;translation task aims;task improving robustness;improving robustness machine;robustness machine;2020 shared task;improving robustness;translation;robustness;shared task improving;wm 2020 shared;robustustness;non standard texts;social media results;findings wm 2020;world domain diversity;challenges facing models;domain diversity;domain diversity non", "pdf_keywords": ""}, "024aa0b78e2a29d07533ee1c6e3b2e875ae45618": {"ta_keywords": "influences fromword use;learning influences fromword;word use speaker;speaker depends word;estimating influences speakers;word distribution influences;influences fromword;influences speakers conversation;fromword use polylogue;people methodsin conversations;methodsin conversations people;polylogue propose probabilistic;conversations people tend;conversations people;word distribution;use polylogue;influences speakers;word use;conversation data;general word distribution;methodsin conversations;speakers conversation;assume word use;conversations;speakers earlier word;estimating influences;depends word use;word use previous;fromword use;model estimating influences", "pdf_keywords": ""}, "ffc211476f2e40e79466ffc198c919a97da3bb76": {"ta_keywords": "learning joint action;agents centralised;train agents centralised;exploit centralised learning;centralised learning;agents coordinate behaviour;agents;global state information;team agents;learning best strategy;team agents coordinate;train agents;settings team agents;exploit centralised;way exploit centralised;centralised learning best;agents coordinate;possible train agents;learning joint;strategy extract;joint action values;learning;best strategy extract;action values conditioned;behaviour acting;behaviour acting decentralised;joint action;agents centralised fashion;lifted learning joint;learning best", "pdf_keywords": "learning multi agent;deep multi agent;multi agent reinforcement;multiagent reinforcement learning;multi agent simulation;learning agents;cooperative multi agent;agent reinforcement learning;multiagent reinforcement;multi agent;multi agent multi;agent multi agent;development multiagent reinforcement;agent multi;role multiagent reinforcement;agent reinforcement;implement multi agent;multi agent systems;learning agents controls;agent simulation;learning joint actions;development multi agent;individual agents;agent reinforcement reinforcement;individual agents fundamental;problem learning agents;learning decentralised policies;independent quantum learning;agents centralised;value function agents"}, "a6b431df3b3d40c98d8d623cab559a9cddd41662": {"ta_keywords": "dialog research neural;dialog policies training;dialog policy explicitly;dialog systems unseen;models implicitly memorize;dialog systems;implicitly memorize task;dialog policy;task specific dialog;challenge dialog research;dialog policies;dialog;dialog research;adapt dialog systems;specific dialog policy;unseen tasks domains;specific dialog policies;shot transfer learning;memorize task;challenge dialog;memorize task specific;specific dialog;implicit memorization;transfer learning;transfer learning end;neural models implicitly;implicit memorization precluded;implicitly memorize;major challenge dialog;schema guided paradigm", "pdf_keywords": "dialog policy neural;shot dialog development;zero shot dialog;dialog policies training;generalization taskoriented dialog;data driven dialog;shot dialog;task action prediction;dialog task;dialog development novel;driven dialog systems;dialog development;shot generalization taskoriented;plan based dialog;dialog task meaning;challenge dialog research;driven dialog;generate dialog structures;batches dialog task;task zero shot;dialog structures data;dialog schema guided;generate dialog;task specific dialog;taskoriented dialog schema;approach dialog history;dialog systems propose;dialog structures;zero shot task;action prediction"}, "62763dbdd47f144c73663b6c6b5d95caeb318e43": {"ta_keywords": "noisy matrix completion;matrix completion;matrix completion standard;noisy matrix;matrix low rank;problem noisy matrix;low rank matrix;permutation rank matrices;low rank approximated;approximated low rank;rank matrices;rank matrix methodsin;rank matrices considered;rank approximated low;rank matrix;rank approximated;backgroundlow permutation rank;approaches underdetermined inverse;permutation rank;underdetermined inverse problem;underlying matrix low;negative rank model;matrix low;negative rank;rank model;underdetermined inverse;non negative rank;underlying matrix;rank model enforces;assuming underlying matrix", "pdf_keywords": "rank matrix completion;estimation permutation rank;noisy matrix completion;permutation rank decomposition;rank matrix permutation;rank matrices algorithms;matrix permutation rank;permutation rank matrices;permutation rank generalization;permutation rank matrix;permutation rank models;rank generalization permutation;generalization permutation rank;permutation rank analysis;matrix completion;decomposition permutation rank;approximated low rank;permutation rank approach;rank decomposition;matrix low rank;matrix completion problem;low rank approximated;permutation rank model;matrix completion goal;low rank matrix;approach matrix completion;rank matrix algorithm;permutation rank conditions;rank matrix paper;rank permutation rank"}, "bd49e66af9755e6138967eba6aeb37d8190d2b4f": {"ta_keywords": "developed optimize data;optimize data method;optimize data;analysis data method;method developed optimize;data method developed;developed optimize results;data method;analysis data;optimize results;optimize results method;optimize;approach analysis data;analysis data required;developed optimize;data required analysis;simple approach analysis;approach analysis;required analysis data;analysis;data;required analysis;method developed;method method developed;data required;simple approach;results method;results method method;approach;method", "pdf_keywords": "relation extraction tasks;relation extraction datasets;classifier textual descriptions;relation extraction;natural language explanations;language explanations capture;extraction relation extraction;relations classifier;model textual descriptions;explanations natural language;language explanations improve;language explanations natural;relation extraction relation;datasets explanations improve;relation extraction use;natural language processing;potential relations classifier;natural language learning;datasets natural language;extraction datasets explanations;performance relation extraction;relations apply classifier;natural language;ofconcept relation extraction;method relation extraction;classifier textual;textual descriptions potential;implementation natural language;explanations improve model;language explanations allow"}, "5e51edfcef2b28594c63cce97c08752dfd438af0": {"ta_keywords": "discriminative models grapheme;online discriminative;models grapheme phoneme;online discriminative learning;structured online discriminative;discriminative models;generative discriminative models;discriminative learning;discriminative learning methods;grapheme phoneme g2p;conventional generative discriminative;models grapheme;grapheme phoneme;generative discriminative;phoneme g2p conversion;discriminative;outperform conventional generative;grapheme;phoneme g2p;conversion task methods;learning methods;generative;conventional generative;phoneme;g2p conversion task;learning;learning methods using;conversion task;models;g2p conversion", "pdf_keywords": ""}, "eebfece29b7a5c2202f1ec53ef49d6fdb75ce0ea": {"ta_keywords": "presynaptic neurons integrated;inputs presynaptic neurons;temporal functions learned;synaptic connectivity intrinsic;electrophysiological properties synaptic;computations depend synaptic;synaptic connectivity;presynaptic neurons;neurons integrated;backpropagation time;properties synaptic connectivity;depend synaptic connectivity;neural computations depend;online backpropagation time;synaptic connectivity determines;connectivity intrinsic electrophysiological;backpropagation time relying;neural computations;neurons integrated cellular;determines inputs presynaptic;complex temporal functions;temporal functions;complex temporal;synaptic;learned online backpropagation;neurons;online backpropagation;intrinsic electrophysiological;properties synaptic;inputs presynaptic", "pdf_keywords": "rule spiking neurons;learning rule spiking;spiking neurons;networks spiking neurons;spiking neurons ability;spiking neurons complex;spiking neural networks;spiking neural;apply spiking neurons;neural networks spiking;spiking neurons apply;neurons apply spiking;neuronal computations;neural circuitry spiking;neuron use event;circuitry spiking neurons;neuron spike;neuronal computations depend;computation student neuron;neuron efficiently;rule spiking;student neuron;spiking neurons extend;development spiking neural;networks spiking;student neuron use;electrophysiological properties synaptic;neuron;spike pattern recognition;neuron perform gradient"}, "e31efa7295e5d6681607ed8ef9c45300d64227aa": {"ta_keywords": "decisions using voting;voting agent vote;outcome voting;agent manipulate vote;winner approval voting;agent vote candidates;agent vote;multi winner approval;approval voting agent;voting agent;votes agent manipulate;votes choosing;using voting;voting scenarios;using voting scenarios;approval voting;collective decisions using;better outcome voting;votes choosing candidates;votes agent;receiving votes agent;collective decisions;voting rules;voting;candidates receiving votes;winner approval;receiving votes;voting scenarios committee;tallying votes choosing;board elections", "pdf_keywords": "complexity voting;complexity voting behavior;making complexity voting;decisions using voting;complexity voting rules;heuristics voting multiwinner;voting behavior multi;voting preferences cooperative;preferable strategies voting;winner approval voting;heuristic voting;heuristics voting;voting multiwinner approval;voting strategies;multiwinner approval voting;voting rules manipulation;strategic voting preferences;strategies voting;multi winner approval;votes choosing candidates;votes choosing;participants uniformly vote;best heuristic voting;votes decision decision;agent vote candidates;voting candidates preferable;heuristic voting candidates;evaluate voting;role heuristics voting;empirical study voting"}, "99c4007b1f6cb905788479db7fc886168f05e57c": {"ta_keywords": "robust speech recognition;networks robust speech;robust automatic speech;speech recognition ar;recognition ar recurrent;recurrent deep neural;robust speech;speech recognition;backgroundrecurrent deep neural;backpropagation time bptt;speech recognition proposed;ar recurrent connections;backpropagation time;automatic speech recognition;deep neural networks;new backpropagation time;new backpropagation;automatic speech;backpropagation;representations new backpropagation;temporal dependency deep;recurrent connections;neural networks robust;propose recurrent deep;recurrent deep;deep neural;backgroundrecurrent deep;dependency deep representations;neural networks dns;deep representations", "pdf_keywords": ""}, "c783e1fb3ce8514f981925ee590c00884660ee4e": {"ta_keywords": "masked language models;causal masked language;masked language;text image tokens;causal masked;common causal masked;causally masked;image tokens;new causally masked;corpus structured;token spans;large corpus structured;causally masked approach;language models trained;trained large corpus;casual masking object;token spans generated;contain text image;language models;tokens new causally;large corpus;text;modal documents;corpus;masked approach generates;masked approach;masking object provides;masked;text image;contain text", "pdf_keywords": "causally masked multimodal;modal multimodal tasks;large corpus structured;masked multimodal modeling;trained large corpus;masked multimodal;multimodal tasks;masked language models;gan tokens causally;summarization causally masked;masked language modeling;hypertext language image;modal multimodal;causally masked language;masked generative models;causal language modeling;large corpus;text image tokens;multimodal;causal masked languagethe;images hypertext;corpus structured;uni modal multimodal;masked enabling generative;language image model;cross modal tasks;introduce masked generative;masked generative;multimodal modeling;bidirectional context extensive"}, "80b92f762e116d4513da27792822897ca3915247": {"ta_keywords": "privacy preserving graph;graph convolutional networks;networks text classification;preserving graph convolutional;graph convolutional;convolutional networks text;networks text;classification aim privacy;preserving graph;privacy preserving;networks;context privacy preserving;convolutional networks;privacy;graph;text classification;text classification methods;context privacy;aim privacy preserving;http www biomedcentral;biomedcentral;www biomedcentral;biomedcentral com;text classification aim;www biomedcentral com;classification;classification methods data;classification methods;biomedcentral com 1471;aim privacy", "pdf_keywords": "graphs privacy;graphs privacy individual;privacy graphs privacy;privacy graphs;privacy utility graph;approach privacy graphs;graph neural networks;privacy preserving gnwe;private stochastic gradient;differential privacy;models privacy utility;differential privacy widely;differentially private training;graph neural;methods differential privacy;new algorithm privacy;privacy preserving algorithms;novel algorithm privacy;privacy preserving methods;models privacy;network models privacy;approach graph neural;privacy individual nodes;method privacy preserving;graph model training;differentially private stochastic;optimize privacy;algorithm privacy;strong privacy;differentially private using"}, "3d5b51fc30ffacdcc8424618555accb36756ccc9": {"ta_keywords": "randomized derivative free;randomized derivative;algorithm stochastic points;novel randomized derivative;derivative free algorithm;unconstrained minimization;minimization problem smooth;free algorithm stochastic;consider unconstrained minimization;stochastic points;unconstrained minimization problem;algorithm stochastic;minimization;smooth function mathbbrn;random search direction;free algorithm;random search;stochastic points spp;iteration complexity methods;iteration create random;randomized;novel randomized;minimization problem;create random search;complexity methods iteration;smooth function;stochastic;function mathbbrn;design novel randomized;analyze iteration", "pdf_keywords": "algorithm stochastic points;search method convex;stochastic point method;convergence random search;method stochastic points;stochastic points method;point method stochastic;randomized coordinate descent;optimizing convex functions;random search method;method optimizing convex;free algorithm stochastic;method convex minimization;free optimization complexity;random search direction;convex problems complexity;algorithm stochastic;free optimization;points method convex;random based methods;methods convex problems;method convex problems;unconstrained minimization;optimizing convex;generate random direction;function random direction;method stochastic;generate random search;random directions based;optimization complexity"}, "845aad7b99f48526fe003c775836091521624471": {"ta_keywords": "wiktionary collaborative lexicography;lexicography projects wiktionary;russian wiktionary collaborative;wiktionary collaborative;predicting word;predicting words;aswikipedia expert;collaborative lexicography;collaborative lexicography projects;predicting words week;crowdsourcing phenomena;projects wiktionary;aswikipedia expert built;fuzzy nature crowdsourcing;aswikipedia;crowdsourcing;wiktionary;just aswikipedia expert;lexicography projects;crowdsourcing phenomena methods;week russian wiktionary;russian wiktionary;projects wiktionary strong;lexicography;words week russian;attention predicting words;focuses predicting word;just aswikipedia;resources just aswikipedia;wiktionary strong competitors", "pdf_keywords": ""}, "a3cd9c4f8fa52c5e23885c2f82931d7e0f7d4b45": {"ta_keywords": "2d barcode;barcode symbology 2d;2d barcode 2d;barcode 2d;symbology 2d barcode;barcode 2d barcode;employing dimensional barcode;barcode symbols developed;barcode symbology;using dimensional barcode;dimensional barcode symbology;dimensional barcode;dimensional barcode symbols;2d barcode high;barcode symbols;barcode;barcode high density;barcode high;dispensing powder drugs;checking dispensing powder;new checking dispensing;drugs employing dimensional;automatic dispenying checking;checking dispensing;dispenying checking using;using linear bar;powder drugs employing;dispensing powder;dispenying checking;linear bar", "pdf_keywords": ""}, "697e6eecb0e77ba56c685bb99b221d959739d13b": {"ta_keywords": "automatic geo tagging;geo tagging images;geo tagging;tags automatic geo;latent dirichlet allocation;location models;location models providing;3d location models;tagging images;dirichlet allocation;providing location based;model user tags;dirichlet allocation la;tagging images methodsin;tagging;user tags;user tags automatic;propose latent dirichlet;location based;automatic geo;models providing location;latent dirichlet;location based services;tags;tags automatic;detailed 3d location;3d location;providing location;dirichlet;geo", "pdf_keywords": ""}, "e42b3ead5ff04adfa95c87e0180561f0c3ba4af4": {"ta_keywords": "safe reinforcement learning;learning control affine;optimization problem policy;reinforcement learning control;learning control;systems vertex networks;vertex networks challenging;introduction safe reinforcement;learned policy approach;vertex networks;constraint satisfaction safety;safe reinforcement;ensure constraint;seeking ensure constraint;control affine systems;control affine;ensure constraint satisfaction;affine systems vertex;learned policy;problems hard constraints;reinforcement learning;hard constraints previous;hard constraints;solving optimization;constraint satisfaction;constraint;constraints;requires solving optimization;step learned policy;constraints previous", "pdf_keywords": "vertex policy network;constraints network;policy optimization algorithms;geometry constraints network;allowing policy optimization;policy optimization;policy network architecture;novel vertex policy;policy network;optimization problem policy;constraints network architecture;vertex policy;safety constraint;obey safety constraint;vertices allowing policy;safe reinforcement learning;policy network vn;novel policy network;safety constraint early;safety region vertex;vanilla policy network;policy network encodes;constraints especially state;constraint satisfaction safety;feasible systems vn;algorithm based safety;seeking ensure constraint;vertices safety region;ensure constraint;vn feasible feasible"}, "e54a4e49917eb3da18c2f239be70a68fbd3274c3": {"ta_keywords": "debt peer review;technical debt peer;debt present reviews;technical debt;technical debt present;investigate technical debt;types technical debt;ropensci reviewers review;ropensci reviewers;packages reviewed;review documentation packages;debt peer;peer review documentation;editors ropensci reviewers;157 packages reviewed;approved published ropensci;packages reviewed approved;debt present;debt;peer review;ropensci manually analyzed;review documentation;comments 157 packages;documentation packages;comments posted package;published ropensci manually;ropensci methodswe collected;present reviews;investigate technical;packages", "pdf_keywords": "software peer review;documentation peer reviews;scientific software reviews;analyzed documentation peer;documentation peer review;peer review documentation;peer reviewed documentation;software reviews used;peer review packages;peer reviews software;software reviews;software review process;debt scientific software;reviews software software;software review;scientific software disposition;documentation peer;debt documentation peer;documentation types peer;reviews software;scientific software investigated;review packages documentation;review documentation definitions;developer documentation debt;peer review;peer review process;debt software engineering;packages documentation debt;analyzed documentation;peer reviews distribution"}, "59d225fcb08ce66935e0285a9936ee158c4fdb97": {"ta_keywords": "explanations form entailment;answers withentailment trees;approach generate explanations;explaining answers withentailment;explaining answers;introduction explaining answers;generate explanations;entailment trees;entailment trees tree;entailment steps facts;generate explanations form;tree multipremise entailment;entailment steps;form entailment trees;textual evidence rationale;explain answers showing;entailment;fragment textual evidence;textual evidence;explanations form;multipremise entailment steps;explain answers;form entailment;explanations;evidence rationale methods;multipremise entailment;reasoning known;answers withentailment;reasoning;known intermediate conclusions", "pdf_keywords": "entailment trees corpus;authoring entailment trees;automated evaluation entailment;automatic evaluation entailment;explanations web based;generate entailment trees;approach entailment trees;evaluation entailment trees;tool generate entailment;generate explanations;entailment trees user;generate entailment;systematic explanations web;entailment trees questionanswer;explanations form entailment;models trained entailedmentbank;entailedmentbank corpus;novel approach entailment;generate valid entailment;trees created entailedmentbank;entailment trees;entailedmentbank large dataset;sentences entailentailedmentbank web;entailment trees created;tree steps entailedmentbank;entailment steps entailedmentbank;entailment tree;allows authoring entailment;generate explanations form;steps entailedmentbank corpus"}, "deedb9b61a01d686b28e6034770fccc142e77fab": {"ta_keywords": "performance natural language;natural language processing;language processing tasks;language processing model;judgments natural language;language processing;domains natural language;natural language;language processing research;models possible experimental;plausible judgments natural;gaining plausible judgments;combinations tasks languages;languages domains natural;training testing model;model perform experimental;actually training testing;plausible judgments;computationally;tasks languages;tasks given complexity;training testing;processing research computationally;complexity combinations tasks;processing tasks;performance natural;build regression;backgroundpredicting performance natural;tasks languages domains;language", "pdf_keywords": "improve prediction lexicon;prediction lexicon induction;optimizing prediction lexicon;prediction lexicon;english improve prediction;translation english prediction;bilingual lexicon induction;lexicon induction results;language learning results;lexicon induction translation;english prediction model;english prediction;lexicon induction;prediction multilingual word;informative predicting evaluation;lexicon induction present;predicting evaluation;language processing nl;prediction multilingual;domains natural language;natural language processing;language learning;language useful tool;computational language learning;models predict evaluation;predicting evaluation scores;predict performance unseen;neural machine translation;nl research computationally;processing nl research"}, "4cfbd97a5b42695697f70a9f28ee29711f6ca433": {"ta_keywords": "changes environment adversarial;environment adversarial;learning driven safety;adversarial attacks unintentional;adversarial;safety critical autonomous;environment adversarial attacks;trustworthy prediction;make trustworthy prediction;deep learning driven;trustworthy prediction ability;adversarial attacks;trained model critical;driven safety critical;driven safety;novelty new input;deep learning;critical autonomous systems;self driving cars;autonomous systems self;safety critical;trained model;novel inputs;detect situations trained;systems self driving;autonomous;critical autonomous;novel inputs changes;trained model able;self driving", "pdf_keywords": ""}, "10e88416035a8a3cbef0e65f8967df650abd0a00": {"ta_keywords": "word sense disambiguation;sense disambiguation resourced;sense disambiguation;sense disambiguation given;unsupervised word sense;disambiguation resourced languages;disambiguation;disambiguation resourced;disambiguation given;disambiguation given sentence;unsupervised word;semantic similarity;sense target word;semantic similarity given;introductionan unsupervised word;watasense unsupervised word;word sense;sense input word;semantic;synset constituting sense;sentence synset;respect semantic similarity;sentence synset constituting;constituting sense target;given sentence synset;resourced languages;resourced languages presented;constituting sense;sense target;chooses relevant sense", "pdf_keywords": "sense disambiguation language;disambiguation language;approach disambiguation words;disambiguation words;disambiguation language based;word sense disambiguation;sense disambiguation method;disambiguate ambiguous words;disambiguation knowledge free;sense disambiguation especially;sense disambiguation methodsgiven;sense disambiguation;disambiguation method;sense disambiguation knowledge;disambiguation words sentence;disambiguation knowledge;new approach disambiguation;disambiguation language german;disambiguation especially difficult;unsupervised word sense;disambiguation;language ability disambiguate;disambiguation especially;disambiguation method watasense;disambiguate;disambiguation methodsgiven;disambiguate ambiguous;approach disambiguation;detect sense words;morphology russian language"}, "4fffa5245d3972077c83614c2a08a47cb578631e": {"ta_keywords": "self supervised speech;supervised speech representation;speech representation learning;supervised speech;approaches speech representation;speech representation;hidden unit bert;supervised approaches speech;unit bert;units input utterance;self supervised;self supervised approaches;approaches speech;approach self supervised;bert;utterance lexicon input;representation learning challenged;input utterance lexicon;unit bert hubert;lexicon input sound;utterance lexicon;sound units variable;sound units;sound units pre;input sound units;input utterance;speech;representation learning;representation learning utilize;bert hubert approach", "pdf_keywords": "self supervised speech;clustering masked;masked clustering;using masked clustering;masked prediction auto;model unsupervised speech;supervised speech;unit masked prediction;supervised speech representation;hierarchical clustering masked;bert like prediction;clustering masked frames;hidden unit bert;improve representation audio;achieved masked prediction;approach unsupervised speech;masked clustering algorithm;unsupervised speech recognition;self supervised learning;cluster assignments masked;trained model speech;audio clusters efficiently;audio use cluster;masked prediction;trained speech recognition;learn self supervised;self supervised;prediction loss masked;audio audio clusters;automated speech"}, "520e82c0f35a14ecf78b93de3673bb8b2a3212fc": {"ta_keywords": "inference timeline extraction;timeline extraction goal;timeline extraction;backgroundtimeline extraction;backgroundtimeline extraction using;trained temporal linking;joint inference timeline;entity involved timeline;timelines documents;temporal linking;inference timeline;involved timeline lack;backgroundtimeline;timeline lack explicitly;involved timeline;distantly supervised approach;using distant supervision;temporal linking systems;timelines documents res;timelines;timeline lack;heuristically aligning timelines;propose distantly supervised;timeline;distantly supervised;distant supervision joint;aligning timelines documents;distant supervision;extraction using distant;aligning timelines", "pdf_keywords": ""}, "0115d5d37f7cdc7b8d2147c0bb348e714432e899": {"ta_keywords": "speech video domain;language recognition evaluation;blstm enhancement language;speech video;noisy speech video;language recognition;enhancement language identification;2017 language recognition;channel blstm enhancement;single channel blstm;channel blstm;models telephone speech;telephone speech noisy;knowledge audio;telephone speech;blstm enhancement;language identification;recognition evaluation lre17;videos addition telephone;knowledge audio domain;requires knowledge audio;speech noisy speech;speech noisy;noisy speech;audios videos;blstm;audio;audios videos addition;recognition evaluation;language identification aimthe", "pdf_keywords": ""}, "cc2c3df6b09166c54e670d347bfe26dae236ac73": {"ta_keywords": "multiview semi supervised;view exploratory learning;exploratory learning akibb;semi supervised exploratory;multi view exploratory;automatic knowledge base;semi supervised learning;supervised exploratory learning;supervised exploratory;exploratory learning;semi supervised;automatic knowledge;knowledge base construction;abstract problem multiview;knowledge base;multi view;supervised learning incomplete;learning akibb;supervised learning;learning akibb problem;view exploratory;supervised;multiview semi;learning incomplete;argue automatic knowledge;learning incomplete class;problem multiview semi;separately viewed instances;multiview;abstract task summarize", "pdf_keywords": ""}, "f7979c6690562c5f8bf700e3fd184c4d1df0a54c": {"ta_keywords": "lingual entity linking;cross lingual entity;introductioncross lingual entity;lingual entity;entity linking;bilingual lexical resources;lexical resources bridge;entity linking maps;introductioncross lingual;cross lingual;target languages resources;source target languages;entity linking assume;mention source language;bilingual lexical;target language;linking maps entity;shot cross lingual;heavily bilingual lexical;entity mention source;different target language;target languages;lingual;target language previous;source language corresponding;low resource languages;entity mention;source language;language corresponding entry;linking", "pdf_keywords": "lingual entity linking;entity linking neural;english link entities;entity linking language;cross lingual entity;english entity link;lingual entity;bilingual lexicon higherresource;corresponding english entity;linking language;bilingual lexical resources;propose entity linking;backgroundcross lingual entity;lexical resources bridge;entity linking;coding bilingual lexicon;parallel entities english;machine translation entity;cross lingual coding;bilingual lexicon;bilingual lexicons;entity linking uses;lingual coding bilingual;linking link entities;translation entity;link entities source;entity discovery linking;linking language families;english entity;resource entity linking"}, "3b0a1a10d8f7496226635c5c3b8475fcd10d890d": {"ta_keywords": "latency serving requests;systems latency serving;sending redundant requests;redundant requests;latency serving;requests way distributed;redundant requests request;serving requests;serving requests potentially;distributed storage;data systems latency;distributed storage systems;way distributed storage;faster execution request;serve requests;serve requests way;requests potentially reduced;request sent servers;systems latency;reduced sending redundant;storage systems;flexibility serve requests;servers;latency;requests way;requests potentially;sending redundant;requests;distributed;servers needed", "pdf_keywords": ""}, "f826381aea632791b6007e427a9587c11b239b6a": {"ta_keywords": "dialog policy learning;exploration dialog policy;policy learning deep;policy learning;exploration dialog;task oriented dialogue;rewards sparse action;dialog policy;dialogue systems;introductionefefficient exploration dialog;greedy exploration inefficient;greedy exploration;dialogue systems primary;oriented dialogue;oriented dialogue systems;learning deep bbq;dialog;dialogue;spiking rewards sparse;rewards sparse;sparse action;exploration inefficient;epsilon greedy exploration;deep bbq networks;learning epsilon greedy;buffer spiking rewards;bbq networks replay;networks replay;deep bbq;sparse action spaces", "pdf_keywords": ""}, "f07a326e21395f025a87b2d77cac7e8ca502f002": {"ta_keywords": "complex domain disease;medical domain complex;disease characterized complex;specialized medical domain;characterized complex domain;domain complex;domain complex domain;domain disease disease;domain disease;complex domain;medical domain;disease disease characterized;disease characterized;disease disease;characterized complex;disease;introduction specialized medical;domain;specialized medical;complex;medical;characterized;introduction specialized;specialized;introduction", "pdf_keywords": "natural language inference;entailment natural language;entailment tailored medical;natural language understanding;entailment deep neural;language understanding models;approach textual entailment;textual entailment natural;entailment dataset;networks natural language;approach textual inference;abbreviations large corpora;expand abbreviations premise;entailment deep;natural language;question entailment dataset;approach expand abbreviations;textual entailment;expanded abbreviations;entailment dataset quora;entailment datawe demonstrate;expand abbreviations;language inference;processing expand abbreviations;entailment datawe;textual inference;methods expand abbreviations;expanded abbreviations post;entailment patient;question entailment deep"}, "d95aafa571e9cb6795cc28ecf257ead123664e3c": {"ta_keywords": "regularization energies markov;clustering regularization models;clustering regularization;common regularization energies;regularization energies;regularization models;normalized cut;standard pairwise clustering;regularization models widely;segmentation model;new segmentation model;ale clustering regularization;segmentation;regularization;segmentation model combining;like normalized cut;common regularization;new segmentation;pairwise clustering;combining common regularization;clustering;optimization spectral relaxation;propose new segmentation;optimization spectral;energies markov;energies markov random;association ale clustering;normalized cut nc;pairwise clustering criteria;spectral relaxation", "pdf_keywords": "clustering mf regularization;clustering joint regularization;standard pairwise clustering;regularization energies markov;segmentation using regularization;segmentation functionals;segmentation image clustering;general pairwise clustering;mf segmentation energies;segmentation energies;image clustering mf;image segmentation clustering;common regularization energies;generalizations pairwise clustering;pairwise clustering probabilities;common regularization;regularization general pairwise;clustering segmentation image;common regularization functionals;combining common regularization;standard regularization algorithms;segmentation functionals demonstrate;standard clustering;pairwise clustering high;high order segmentation;mr regularization;image clustering;image clustering clustering;pairwise clustering objectives;prior clustering"}, "250e4a8f5155f1f9f60b2dee3e8da8024338db4d": {"ta_keywords": "dirichlet distribution sentiment;document level sentiment;distribution sentiment labels;context sentiment target;sentiment labels;sentiment target movie;sentiment target;sentiment labels reviews;level sentiment analysis;global context sentiment;sentiment analysis;sentiment analysis rely;distribution sentiment;level sentiment;sentiment labels use;context sentiment;labels reviews target;assume sentiment labels;consistency dirichlet distribution;consistency dirichlet;reviews target consistent;reviews target;labels reviews;model consistency dirichlet;product assume sentiment;sentiment;dirichlet;dirichlet distribution;incorporating global context;assume sentiment", "pdf_keywords": ""}, "8eda71ecad19cdef6092e76276eba48312ec7063": {"ta_keywords": "dense retrieval;dense retrieval rr;retrieval;document query encoders;results stage retrieval;query encoders based;retrieval little known;retrieval rr;query encoders;retrieval little;specifically discretize embeddings;retrieval rr reaches;discretize embeddings;discretize embeddings output;based discrete representations;embeddings output;embeddings output document;embeddings;discrete representations;document query;encoders based discrete;stage retrieval;output document query;encoders based;representations;encoders;rr models;rr models specifically;stage retrieval little;models specifically discretize", "pdf_keywords": "query document representations;document representations queries;representations queries documents;dense retrieval;document representations;information retrieval;retrieval systems;specifically discretize embeddings;retrieval methods;information retrieval methods;representations queries;document representations continuous;discretize embeddings;dense retrieval rr;retrieval methods important;document query encoders;discretize embeddings output;information retrieval systems;automated information retrieval;retrieval systems article;retrieval;embeddings;representations continuous embedding;measure word importance;embeddings output;query encoders based;retrieval little known;embeddings output document;queries documents reclassify;embedding"}, "e0c66240239263f16159eef166a391d3939ae2d5": {"ta_keywords": "reading comprehension examples;reading comprehension;comprehension examples consist;comprehension examples;passage tuples physicians;address reading comprehension;questions passages predict;comprehension;tuples physicians combine;tuples physicians;physicians combine information;question passage tuples;passages predict corresponding;passages predict;reading;passage tuples;information questions passages;predict corresponding answers;questions passages;examples consist question;corresponding answers intense;tuples;combine information questions;physicians;corresponding answers;physicians combine;examples consist;question passage;passages;papers address reading", "pdf_keywords": "question answering;stanford question answering;question answering dataset;reading comprehension examples;reading comprehension;memory networks stanford;comprehension examples consist;address reading comprehension;learning tasks results;predict corresponding answers;questions passages predict;natural language inference;value memory networks;comprehension examples;learning tasks;answering dataset;language inference;learning major challenge;challenging task report;learning memory;language inference current;natural language processing;answering;learning memory qa4mre;challenging task rc;predicting correct answer;memory networks;field learning tasks;benchmarks remain unanswered;field natural language"}, "3105b5863d4597058bf51aeda40db53394075784": {"ta_keywords": "manipulation bribery discussed;bribery discussed article;bribery manipulation bribery;bribery discussed;bribery manipulation;manipulation bribery;manipulation bribery manipulation;bribery;manipulation;discussed article;article;discussed", "pdf_keywords": ""}, "446efa0bcf3528b51332a12495cb56784dd8bad3": {"ta_keywords": "deep transfer learning;deep transfer;transfer learning;transfer learning approaches;latent relational graphs;modern deep transfer;vectors task transferable;embeddings language pretrained;generic latent relational;language pretrained convolutional;structured graphical representations;relational graphs capture;transfer unary features;graphical representations;latent relational;pretrained convolutional features;embeddings;learning generic latent;graphs capture dependencies;generic feature vectors;graphical representations methodsthis;tasks word embeddings;embeddings language;pretrained convolutional;learning generic feature;feature vectors;feature vectors task;word embeddings;transferable tasks word;relational graphs", "pdf_keywords": "graph predictor learn;graph predictor trained;predictor learn graphs;latent graph learning;attention predictive unsupervised;relational graph learning;unsupervised latent graph;learn graphs features;learned graphs;learn graphs;graphs downstream tasks;latent graphs transfer;latent relational graphs;graph learning called;models learned graphs;graph learning;predictor extract graphs;learned graphs generic;classification learned graphs;learning generic graphs;graph predictor extract;latent relational graph;graphs capture dependencies;learning rich graph;novel graph predictor;graph learning use;tasks employ graph;relational graphs capture;output latent graphs;latent graphs"}, "549dae68d04eefad88885c64a4d946205e524b79": {"ta_keywords": "persistent homology basedrepresentations;homology basedrepresentations methods;homology basedrepresentations;document embedding;word embeddings;geometry word embeddings;study persistent homology;vectors document embedding;embeddings;persistent homology;embedding;document classification;topology text data;word embeddings help;document classification case;homology;help document classification;embeddings help;embeddings help document;basedrepresentations methods investigate;text data analysis;basedrepresentations methods;isometric invariant mappings;methods algebraic topology;topology text;mappings;classification;basedrepresentations;vectors document;algebraic topology text", "pdf_keywords": "persistent homology embeddings;embeddings generate persistence;study persistent homologybasedrepresentations;homology embeddings generate;homology embeddings;uses persistent homology;persistent homology;geometry word embeddings;persistent homologybasedrepresentations;persistent homologybasedrepresentations methodswe;word embeddings;word embeddings helps;document embedding;topology text data;embeddings helps;embeddings;embeddings helps document;vectors document embedding;persistence diagrams useful;persistence diagrams;document embedding stable;embeddings generate;homologybasedrepresentations methodswe investigate;embedding;text document clustering;document clustering;persistence diagram;generate persistence diagram;helps document classification;classification algorithms persistence"}, "35c710f5fdacc71a675832f6beaa2dbfe301d0ce": {"ta_keywords": "example based dialog;based dialog systems;dialog systems efficiently;dialog systems;based dialog;propose active learning;construction dialog systems;based dialog popular;dialog systems creating;active learning;active learning framework;human annotators create;dialog;human annotators;dialog popular;present human annotators;dialog popular option;construction dialog;annotators create responses;option construction dialog;annotators;strategies selecting inputs;annotators create;propose uncertainty sampling;creating example;systems creating example;example based;sampling strategies selecting;uncertainty sampling strategies;create responses selected", "pdf_keywords": ""}, "25efc17ba82ba4af29f2e03868de74e1ea66d025": {"ta_keywords": "multilingual multimodal embeddings;multilingual text video;contextual multilingual multimodal;multilingual multimodal;learns contextual multilingual;purposemultilingual multimodal pre;multimodal embeddings zero;purposemultilingual multimodal;multimodal pre training;transfer vision language;lingual transfer vision;vision language models;contextual multilingual;multimodal embeddings;learns contextual;embeddings zero shot;specifically focus multilingual;cross lingual transfer;multimodal pre;shot cross lingual;multilingual text;multilingual;lingual transfer;focus multilingual;focus multilingual text;multimodal;text video search;cross lingual;model learns contextual;language models specifically", "pdf_keywords": "learning multilingual multimodal;multilingual text video;multilingual multimodal embeddings;multilingual multimodal pretraining;multilingual textvideo search;videos multilingual;multilingual video text;multilingual textvideo collection;multilingual textvideo;multilingual video;transferability multilingual textvideo;video dataset multilingual;multilingual multimodal representations;search multilingual video;dataset multilingual multimodal;multilingual text tovideo;contextualized multilingual multimodal;proposed multilingual textvideo;textvideo search multilingual;queries videos multilingual;learns contextualized multilingual;largest multilingual textvideo;contextual multilingual multimodal;transfer multilingual multimodal;videos shared multilingual;training english video;multimodal embeddings zero;multilingual multimodal;multilingual multimodal pre;present multilingual multimodal"}, "e60b88313fad52c1ef8dd02b482785651d09ad66": {"ta_keywords": "size depth neural;neural networks exponentially;depth neural networks;depth neural;weights approximate approximated;bounded weights approximate;poly size depth;networks exponentially bounded;number neurons;weights approximate;approximated low degree;neurons;d3x number neurons;networks exponentially;approximate approximated;size depth;approximate approximated low;bounded weights;neural networks;approximated;approximated low;neural;exponentially bounded weights;networks;approximate;polynomial sin pi;poly size;polynomial sin;depth;degree polynomial sin", "pdf_keywords": "expressive power deep;power deep networks;neural networks approximate;depth network;expressive power neural;implemented depth network;deep networks beenthe;deep networks studied;depth network sphere;deep networks;power neural networks;networks approximate;bounds neural networks;small depth network;function implemented depth;networks approximate non;power deep;estimation bounds neural;bounds neural;implemented depth;depth network width;neural networks studied;approximated small depth;ability neural networks;neural networks;networks beenthe function;density relu network;power neural;dynamics relu network;sphere functions legendre"}, "a1c5af2a531c64f1c06e806d7986cd878ec3c33a": {"ta_keywords": "explainable ai methods;generate model explanations;explainable ai;new explainable ai;model explanations;model explanations having;robustness explanations seldom;explanations chosen;robustness explanations;explanations seldom evaluated;assessment explanations chosen;ai methods designed;explanations having specific;explanations;assessment explanations;fidelity robustness explanations;explanations seldom;tasks assessment explanations;proposing new explainable;explanations chosen fact;ai methods;ai;explanations having;new explainable;model end users;decision making tasks;model;explainable;fidelity robustness;robustness", "pdf_keywords": "explanations human ai;explainable ai methods;evaluation explanations experimentation;development explainable ai;human interpretability explanations;explanations utility;explanations end users;explanations information model;showing explanations human;explainable ai;explanations significantly improves;explanations human;new explainable ai;score explanations human;evaluation explanations;compare explanations utility;explainable ai xi;explanations utility different;assess compare explanations;generate model explanations;tasks assessment explanations;explanations human decision;explanations experimentation;assessment explanations chosen;explanations information;interpretability explanations;users value explanations;explanations agreement explainers;explanations heavily determined;explanations seldom evaluated"}, "4fa4e39ade763085a75146392b997b7d4da49725": {"ta_keywords": "backgroundweakly supervised text;backgroundweakly supervised;supervised text;supervised text classification;text classification;labels context free;seed words;pseudo labels context;text classification based;seed words recently;labels context;classification based user;context free;supervised;context dependent;provided seed words;classification;context free manner;backgroundweakly;classification based;pseudo labels;generate pseudo labels;context dependent nature;ambiguous context dependent;matching ambiguous context;text;human language;labels;context;nature human language", "pdf_keywords": "contextualized documents train;seed words contextualize;words contextualize corpus;classification document contextualization;words contextualize;supervised text;backgroundweakly supervised text;contextualized corpus predicted;contextualize corpus;contextualize corpus contextualized;unlabeled contextualized documents;contextualized corpus;cluster contextualized;contextualized weakly supervised;document contextualization;contextualized documents;supervision text classification;networks contextualized corpus;cluster contextualized representations;contextualized corpora;contextualized weak supervision;interpretations based contextualized;corpus contextualized;approach contextualized corpus;supervised text classification;contextualized representations clustering;contextualize;unlabeled contextualized;contextualized;contextualized corpus iterations"}, "462e36e5e296900c80dcd36173340f9c29e36c80": {"ta_keywords": "hidden markov models;hmms practical bayesian;hmm based variationational;hidden markov;st hmm based;mammalian st hmms;state hidden markov;sharedstate triphone human;based variationational bayesian;hmm based;markov models based;triphone human mammalian;markov models;variationational bayesian approach;variationational bayesian;bayesian model selection;triphone human;constructing sharedstate triphone;derived st hmm;bayesian framework methodsin;bayesian approach paper;models based bayesian;st hmms practical;bayesian framework;markov;sharedstate triphone;practical bayesian framework;based bayesian;method bayesian model;based variationational", "pdf_keywords": ""}, "f05741b65a1d644f2fae4c654dae315a7451ee85": {"ta_keywords": "text network exploration;text networks hyperlinked;topics text network;citation networks;academic citation networks;text networks;networks hyperlinked webpages;network exploration heterogeneous;proliferation text networks;network exploration;network exploration paper;text network text;text network;citation networks led;new text network;text network refers;web topics text;heterogeneous web topics;context text network;exploration heterogeneous web;networks hyperlinked;documents represented edges;network text network;network text;hyperlinked webpages academic;webpages academic citation;web topics;associated text document;vertex associated text;webpages academic", "pdf_keywords": "linked word topics;text network exploration;word topics doctopics;heterogeneous topic web;text networks heterogeneous;forheterogeneous topic web;model forheterogeneous topic;topic web heterogeneous;content similarity documents;topics doctopics serve;explore text networks;topic models introduce;documents wordtopics propose;topic models discuss;relevant documents wordtopics;topic models far;topic models;word topics results;topics represented words;documents wordtopics;existing topic models;topics doctopics;text networks scientific;documents wordtopics method;similarity documents methods;networks text networks;heterogeneous linked word;text networks hyperlinked;relation wordtopic doctopic;intermediaries wordtopics uncover"}, "fe54832083f65eade8e2847627d330a24df22488": {"ta_keywords": "methodsanalysis electroencephalograms;electroencephalograms;electroencephalogram;multi channel electroencephalogram;channel electroencephalogram;matrices methodsanalysis electroencephalograms;background noise removal;backgroundremoving noise event;methodsanalysis electroencephalograms egg;noise removal;noise event related;electroencephalograms egg usually;noise removal single;noise event;electroencephalograms egg;backgroundremoving noise;potentials erps recorded;background noise;event related potentials;method background noise;related potentials erps;potentials erps;potentials using probabilistic;erps recorded;erps recorded multi;grouped covariance;noise;suffers variety noises;noises paper propose;probabilistic generative model", "pdf_keywords": ""}, "d7a7ebd1565c3795bc2bcdec4334d42a65ad17c5": {"ta_keywords": "text generation;plms text generation;text generation important;neural generation models;background text generation;neural generation;text generation resultsamong;language models plms;pretrained language models;field neural generation;topic plms text;plms text;generation models;text;generation models especially;tasks natural language;achieved topic plms;natural language;natural language processing;language models;paradigm pretrained language;deep learning greatly;deep learning;pretrained language;background text;generation;neural;generation important challenging;resurgence deep learning;language processing np", "pdf_keywords": "text generation tasks;text generation preliminaries;generation text;text generation;text generation advance;text generation core;generation text generation;provide text generation;models text generation;model text generation;text generation text;text generation discuss;text generation difficult;text generation development;text generation important;development text generation;text generation researchers;text generation applications;plms text generation;used text generation;different text generation;text generation review;text generation organize;strategies text generation;text generation article;field text generation;technologies text generation;natural language generation;text generation relevance;methods text generation"}, "a2221b03211408ac2db0559b9a54c1d72b5f560c": {"ta_keywords": "music annotation tasks;learning music annotation;music annotation;music information retrieval;self supervised music;supervised learning music;context music annotation;music annotation critical;supervised music;supervised music acoustic;music acoustic representation;music information;field music information;learning music;acoustic representation learning;music acoustic;annotated training data;context music;topics field music;annotation tasks supervised;self supervised;annotated training;new self supervised;field music;supervised machine;acoustic representation;music;annotation;annotation tasks;use supervised learning", "pdf_keywords": "music acoustic encoder;music acoustic representation;generate music classification;acoustic music representation;encoder named musicoder;music annotation tasks;music representation;self supervised music;supervised music acoustic;supervised learning music;representation unlabeled music;music acoustic data;learning music annotation;music representation unlabeled;music classification task;supervised music;representation learning music;music annotation;music representation ability;algorithm music classification;powerful music representation;context music annotation;music classification algorithm;music classification;novel music classification;acoustic encoder based;classification algorithm music;music information rectrieval;acoustic encoder;features music extracted"}, "74e9053d6f44f4507bd40bbea999ee65f0cbefb2": {"ta_keywords": "interpretation methods neural;neural model predictions;way interpret neural;interpret neural model;interpret neural;neural models make;model predictions word;neural models;models make interpretations;model predictions;input reduction iterative;neural model;predictions word determined;methods neural model;existing interpretation methods;predictions word;neural;methods neural;interpretation methods;pathologies neural models;input reduction;make interpretations diffility;model predictions highlight;interpretations diffility way;features existing interpretation;make interpretations;decrease model confidence;use input reduction;word determined input;input perturbation", "pdf_keywords": "neural models predict;models interpretable input;adversarial examples confidence;interpret neural models;neural models overconfidence;neural models explain;interpretable input reduction;neural machine translation;answer ability predict;neural model sensitive;input output neural;neural models important;interpretation methods neural;models make predictions;predictions high confidence;neural network prediction;explain model predictions;predict answer ability;output neural model;output neural;improve interpretation ability;neural models shown;predictions despite short;neural models;able predict accuracy;answering models retain;answering models;networks implications neural;model predictions;models interpretable"}, "01a21d74fb7414404851872f23cdca42243ab6a8": {"ta_keywords": "person identity identification;transfer learning;person identity;approach person identity;pre trained feature;used transfer learning;transfer learning approach;trained feature extraction;trained feature;identity identification;identity identification rei;instead training model;feature extraction model;training model;model fine tuning;model target;feature extraction;tuning pre trained;identification;identification rei applications;person;identity;extraction model target;training model scratch;feature;pre trained;scenario instead training;model;model fine;trained", "pdf_keywords": "dataset human persons;batch related convolutional;large dataset human;dataset human;transfer learning network;person identification;approach person identification;mini batch training;person identification vitro;batch training methods;transfer learning;used transfer learning;mini batch dataset;batch training;propose transfer learning;deep learning;related convolutional cell;network image classification;convolutional cell;trained model;transfer learning based;learning network v2;transfer learning method;convolutional cell conv;convolutional cell v2;learning network;related convolutional;transfer learning approach;batch dataset;trained model target"}, "c2dd1c332f65fea3a66f4a982428f31ce1a9dc70": {"ta_keywords": "web based information;information sources pre;diierent information sources;knowledge integration systems;information sources common;information sources;search engines;knowledge integration;processed web based;sources common database;processing knowledge integration;based information systems;pre processed web;information systems varies;common database representation;information systems;based information;database representation;web based;processed web;search engines like;diierent information;degree information sources;integrate diierent information;information;common database;database;greatly search engines;sources pre processed;web", "pdf_keywords": ""}, "eca07d2b351d81719b33c913a87c63d6930ee7f5": {"ta_keywords": "diagnosis treatment;patients diagnosis disease;diagnosis treatment patients;diagnosis disease;approach diagnosis treatment;patients diagnosis;treatment patients diagnosis;new approach diagnosis;diagnosis;approach diagnosis;treatment patients;disease;patients;treatment;new approach;approach;article;purpose article;importance new approach;purpose article discuss;article discuss;article discuss importance;new;purpose;discuss importance new;discuss importance;importance;discuss;importance new", "pdf_keywords": ""}, "3a95fab610d5ff49fbdb7a4d8760b02c51df0013": {"ta_keywords": "anonymizing clinical notes;privacy clinical notes;embeddings improve privacy;anonymizing clinical;token clinical notes;improve privacy clinical;technique anonymizing clinical;clinical notes guarantees;privacy clinical;clinical notes neighboring;clinical notes;notes guarantees private;utilizing word embeddings;clinical notes objectives;word embeddings;privacy technique anonymizing;word embeddings improve;private health information;improve privacy;replacing token clinical;notes neighboring word;sensitive data family;health information secured;technique anonymizing;introduce privacy;introduce privacy technique;anonymizing;privacy technique;including sensitive data;sensitive data", "pdf_keywords": "anonymizing clinical notes;anonymized text embeddings;anonymized text;sensitive information text;performance anonymized text;anonymizing clinical;technique anonymizing clinical;token clinical note;refactoring clinical notes;techniques clinical privacy;text clinical notes;clinical notes deidentify;deidentify clinical records;masking sensitive information;clinical privacy literature;sensitive data names;anonymization technique clinical;clinical privacy;clinical note random;notes deidentify clinical;clinical notes using;free text clinical;novel anonymization technique;novel anonymization;clinical notes guarantees;clinical notes use;sensitive data information;replace token clinical;use clinical notes;growing clinical notes"}, "76b95833fd0e242896d231abdea8dc01a167c7a6": {"ta_keywords": "process prior drift;estimating drift functions;process inference drift;gassian process inference;prior drift function;prior drift;approach estimating drift;estimating drift;drift function stochastic;inference drift function;drift functions systems;drift function state;drift functions;gassian process prior;inference drift;systems stochastic differential;unobserved latent;stochastic differential;drift function;stochastic differential equations;deal unobserved latent;algorithm deal unobserved;nonparametric approach estimating;systems stochastic;stochastic;function stochastic differential;incomplete observations state;functions systems stochastic;unobserved;process prior", "pdf_keywords": "bayesian estimate drift;estimating drift functions;posterior prediction drift;process prior drift;prior drift function;estimate drift;estimate drift function;estimating drift;approach estimating drift;nonparametric bayesian estimate;estimation diffusion;approaches estimation diffusion;drift approximation;unobserved latent dynamics;general drift functions;prior drift;feasible general drift;method linear drift;estimation diffusion processes;drift method feasible;functions linear drift;parts drift approximation;models linear drift;prediction drift function;drift functions linear;drift functions systems;kernel coefficient drift;prediction drift;drift function state;drift functions"}, "a660429b77e932af1c1d7d3f0554f4b17c044082": {"ta_keywords": "similar terror groups;groups participating terrorist;clusters similar terror;terror groups using;terrorist actions investigating;terrorist attacks occurred;data terrorist attacks;terror groups;terrorist attacks;participating terrorist actions;terrorist actions;attacks occurred worldwide;complex networks approach;complex networks;latent clusters similar;latent clusters;access data terrorist;participating terrorist;data terrorist;networks approach;terrorist;networks approach approach;heterogeneity actors groups;goal complex networks;groups using information;clusters similar;networks;attacks occurred;terror;allow latent clusters", "pdf_keywords": "similarity terrorist groups;cluster analysis terrorism;similar terrorist groups;clusters terrorist groups;clusters similar terrorist;patterns terrorist groups;analyze terrorist groups;similar terror groups;terrorist groups related;latent clusters terrorist;network terrorist groups;terrorist groups complex;terrorist groups calculated;similarity terrorist;coefficient similarity terrorist;clusters similar terror;terrorist groups using;terrorist groups behavioral;terrorist terrorist groups;networks occurrence terrorism;clusters terrorist;terrorist groups belonging;terrorist groups;terrorist groups expanding;includes terrorist groups;patterns terrorist;terror groups using;terror groups;analyze global terrorist;terrorist groups way"}, "8512718bafa447f9b433da9e809215dfc28b6b28": {"ta_keywords": "performance prediction nonlodos;improving performance prediction;backgroundperformance prediction task;backgroundperformance prediction;performance prediction;performance predictors;performance predictors holistic;estimating performance performing;task estimating performance;estimating performance;prediction nonlodos;prediction nonlodos examine;nonlodos examine performance;examine performance predictors;prediction task estimating;backgroundperformance;improving performance;prediction task;performance;performance performing;grained performance;fine grained performance;examine performance;datasets languages tasks;performance performing experiments;task estimating;languages tasks models;prediction;holistic measures accuracy;contributions improving performance", "pdf_keywords": "performance prediction tasks;improving performance prediction;performance predictors holistic;grained performance prediction;predict performance prediction;prediction tasks;prediction models tasks;performance prediction;performance prediction results;performance prediction performance;predicted performance accuracy;prediction performance prediction;performance prediction models;introduce performance prediction;predict performance;predicted performance;task estimating performance;prediction tasks proposed;performance predictors;prediction task estimating;performance prediction using;grained evaluation metrics;scoring models speech;consider performance prediction;performance prediction non;backgroundperformance prediction task;performance prediction methodological;prediction performance;performance prediction scenarios;estimating performance performing"}, "84702b091af8842b6bbe457e5435c343a9824693": {"ta_keywords": "symptomatic asymptomatic asymptomatic;diagnosis symptomatic asymptomatic;asymptomatic asymptomatic asymptomatic;asymptomatic asymptomatic;symptomatic asymptomatic;asymptomatic asymptomatic asymptom;asymptomatic;asymptomatic asymptom;asymptom;physician diagnosis;patient diagnosis symptomatic;patient diagnosis;diagnosis management patient;diagnosis symptomatic;diagnosis;physician diagnosis management;role physician diagnosis;symptomatic;management patient diagnosis;physician;diagnosis management;role physician;management patient;patient;literature role physician;article;role;management;article present;purpose article", "pdf_keywords": ""}, "54e7209e692ca4f5c85f0e68df34040b3cfa8bad": {"ta_keywords": "computation scheme distributedly;distributedly computing;coded computation scheme;coded computation;stragglers encode data;scheme distributedly computing;new coded computation;stragglers encode;computes using distributed;distributedly computing presence;presence stragglers encode;distributed processors;using distributed processors;distributed processors waits;encoded matrix computes;finish computations decodes;computing presence stragglers;computation scheme;scheme distributedly;coded;encoded matrix;computations decodes;distributedly;computing;waits subset processors;processors finish computations;decodes partial computation;distributed;using distributed;stragglers", "pdf_keywords": ""}, "1acbfc7d3e245bd3146e9e24eae7550aa2d03482": {"ta_keywords": "understanding batch normalization;batch normalization;batch normalization key;introductiontheoretical understanding batch;normalization;understanding batch;train deep neural;deep neural;effectively train deep;batch;normalization key;normalization key component;deep neural networks;neural networks empirical;informationleveraging tools markov;train deep;novel informationleveraging;rank pre activation;informationleveraging;neural networks;neural;relevance novel informationleveraging;novel informationleveraging tools;tools markov;deep;networks;tools markov chain;networks empirical;activation;networks empirical evidence", "pdf_keywords": "initialized deep networks;initialization deep networks;random initialization deep;initialization deep neural;randomly initialized deep;batchnormalized networks;batchnormalized networks actually;deep linear neural;initialized neural networks;deep networks gradient;initialization deep;largest batchnormalized networks;initialized deep;rank deep linear;randomly initialized neural;deep networks spectral;batchnormalized networks able;networks crucial gradient;batchwe rank deep;deep linear;optimizing deep networks;rank deep;batchesnorm networks;optimizing deep;randomly initialized networks;deep neural;learning batch normalization;hidden layer activations;learning rank crucial;rank hidden representations"}, "796f29cee975603c7a1469df1eb21ed5142ecff5": {"ta_keywords": "literary evidence retrieval;78k literary quotations;literary quotations surrounding;excerpt literary analysis;literary quotations;literary analysis;literary evidence;task literary evidence;literary analysis surrounding;literature;novel form quotations;quotations surrounding critical;excerpt literary;quotations surrounding;relsic 78k literary;given excerpt literary;quotations work collect;literature novel form;work literature;task literary;literature novel;novel task literary;quotations;introductionhumanities scholars commonly;literary;evidence retrieval;masked quotation;evidence retrieval models;make work literature;78k literary", "pdf_keywords": "database literary evidence;retrieval dataset literary;literary evidence retrieval;retrieval literary evidence;dataset literary;database literary literary;retrieval literary;database literary;retrieve literary evidence;research repository literary;dataset literary domain;sources literary analysis;literary research repository;present database literary;embedding scholarly claims;repository literary literary;method retrieval literary;tool retrieval literary;hathitrust database literary;sources literary;collect quotations literary;repository literary;textual similarity model;trained literary domain;reic embedding scholarly;retrieve literary;quotation literature literature;textual similarity;literary quotations pretrained;primary sources literary"}, "d10e410765699a75628a1437b93f0d0fc3dc0aa6": {"ta_keywords": "supervised classification graph;semi supervised learning;semi supervised classification;semi supervised;introductionaccurate semi supervised;classification graph data;classification graph;performing semi supervised;labeled instances training;propagating labels labeled;learning propagating labels;labeled instances;labeled seed instances;labels labeled seed;supervised;supervised learning;supervised classification;labels labeled;labeled;supervised learning propagating;graph data machine;easy obtain labels;algorithms require labeled;labeled seed;obtain labels;unlabeled instances;propagating labels;require labeled instances;labels;graph data", "pdf_keywords": ""}, "48aa33ad92566cb60ef348ffa438e4712f618b03": {"ta_keywords": "swir transillumination reflectance;images lesions tooth;reflectance occlusal transillumination;transillumination images lesions;swi reflectance occlusal;tooth proximal occlusal;lesions tooth proximal;infrared swi reflectance;reflectance occlusal;transillumination reflectance probe;occlusal transillumination images;lesions tooth;transillumination reflectance;reflectance probe;interproximal occlusal lesion;swi reflectance;reflectance probe methods;occlusal lesion severity;lesion severity dual;occlusal transillumination;developed clinical probe;clinical probe;clinical probe capable;dual swir transillumination;infrared swi;wavelength infrared swi;images lesions;infrared;swir transillumination;reflectance", "pdf_keywords": ""}, "3fb78bee6cb39588a1a4cbb4e0abce5e362aa130": {"ta_keywords": "bounds adversarial bandits;improved regret bounds;dependent regret bounds;dependent bounds adversarial;regret bounds;regret bounds obtained;adversarial bandits;bounds adversarial;adversarial bandits line;regret bounds important;data dependent regret;regret bounds scribe;data dependent bounds;bounds obtained loss;dependent bounds;bandits;bandits line work;sequence completely adversarial;bandits line;adversarial;adversarial words;completely adversarial words;completely adversarial;improved regret;adversarial words classical;dependent regret;bounds important tools;bounds important;bounds;algorithms exp", "pdf_keywords": ""}, "e6ffeb4b9d808d6c9b8d388a7cbb431ac96bf194": {"ta_keywords": "patient history thrombocytopenia;hospital history thrombocytopenia;history thrombocytopenia patient;thrombocytopenia patient;thrombocytopenia admitted hospital;thrombocytopenia patient diagnosed;thrombocytopenia admitted;thrombocytopenia patient treated;diagnosis thrombocytopenia patient;history thrombocytopenia admitted;thrombocytopenia suffered;thrombocytopenia suffered history;suffered history thrombocytopenia;thrombocytopenia;history thrombocytopenia suffered;history thrombocytopenia;diagnosis thrombocytopenia;diagnosed diagnosis thrombocytopenia;treated aspirin discharged;aspirin discharged day;patient treated aspirin;aspirin discharged;treated aspirin;patient discharged;patient discharged day;20 patient discharged;patient diagnosed;patient diagnosed diagnosis;patient treated;patient history", "pdf_keywords": ""}, "99053e3a708fc27709c9dab33110dc98b187c158": {"ta_keywords": "financials robust numerical;large corpus financial;corpus financial documents;robust numerical reasoning;financial data;financial data aiming;questions financial data;corpus financial;deep questions financial;financial documents;financial documents contrast;financial statements;financials robust;analyze business financials;numerical reasoning;numerical reasoning likewise;finance;business financials robust;general domain finance;numerical reasoning understanding;financial statements makes;financials;finance domain;complex numerical reasoning;questions financial;domain finance;volume financial statements;business financials;financial;robust numerical", "pdf_keywords": "reasoning systems financial;financial data aiming;generating reasoning programs;financial data;questions financial data;financial reports;fintabnet dataset annotated;reasoning programs;financial reports generate;deep questions financial;financial data important;facts financial reports;representationsthe financial data;financial sentiment analysis;financial report;financial statements;supporting facts financial;numerical reasoning systems;financial reports use;large corpus financial;financial documents;facts financial;analyze business financials;corpus financial documents;tool financial analysis;financials robust numerical;gold programs dataset;general domain finance;generating reasoning;fintabnet dataset"}, "0acbdcac9edf74cc2c1e98bd59e301c9300977d0": {"ta_keywords": "annotation framework sequencetagging;alcatag sequence tagging;crowdd annotation framework;sequence tagging;sequence tagging tasks;based crowdd annotation;crowdd annotation;sequencetagging methodswe introduce;sequencetagging;sequencetagging methodswe;suggesting annotations sampling;annotation framework alcatag;framework sequencetagging;suggesting annotations;dynamically suggesting annotations;annotations sampling informative;framework sequencetagging methodswe;annotation framework;tagging;active intelligent recommendation;data annotation framework;annotations sampling;data annotation;based data annotation;backgroundalpacatag active learning;annotations;annotation;named entity recognition;learning based crowdd;entity recognition ner", "pdf_keywords": ""}, "ee2e171d6a897ee5d0b0bde2d5f2548b52d3a840": {"ta_keywords": "cognitive scaffolding learning;meta cognitive scaffolding;equations tutoring teachable;tutoring teachable agent;cognitive scaffolding;scaffolding learning teaching;tutoring teachable;tutoring;equations tutoring;learning teaching methodsstudy;algebraic equations tutoring;provide meta cognitive;teaching methodsstudy learning;learning teaching;teaching methodsstudy;meta cognitive;scaffolding learning;meta cognitive help;teach quiz sistudent;methodsstudy learning solve;online learning environment;learning solve algebraic;learning environment;teach quiz;students teach quiz;using online learning;teachable agent;problems students teach;learning environment called;online learning", "pdf_keywords": ""}, "9633928f72cda45d102fb6740291d47137d0a5ca": {"ta_keywords": "attack federated learning;federated learning systems;federated learning;clients attack federated;attack federated;backdoor samples training;malicious data backdoor;subset data backdoor;attacks aggregation stage;data backdoor samples;data backdoor patterns;attacks aggregation;data backdoor;malicious data;compromised global model;introductionmalicious clients attack;systems using malicious;detect attacks aggregation;clients attack;training phase compromised;detect attacks;using malicious data;backdoor patterns;compromised global;aggregation stage training;malicious;using malicious;backdoor samples;tried detect attacks;validation dataset designed", "pdf_keywords": "attacks federated learning;attack federated learning;federated learning pruning;backdoor attacks training;backdoor attacks pruning;learning mitigate backdoor;backdoor attacks federated;federated learning algorithm;federated backdoor attack;attacks pruning;attacks pruning guarantee;federated pruning methods;federated learning mitigate;learning federated learning;federated learning simplify;eliminate backdoor attacks;method federated learning;based federated backdoor;federated learning effective;federated learning powerful;backdoor attacks limiting;federated pruning;tool federated learning;federated learning propose;mitigate backdoor attacks;method federated pruning;federated learning;mitigating backdoor attacks;federated learning implement;federated pruningthe"}, "7731e3dec97c48498b585408d44615346ade144a": {"ta_keywords": "characterizing language variation;variation senses words;language variation;different sense clusters;reddit communities methods;language variation internet;sense clusters community;senses words analyzing;variation internet social;characterize variation senses;reddit communities;communities methods specificity;words analyzing;words used groups;senses words;characterizing language;sense clusters;variation senses;internet social groups;combined specificity community;community combined specificity;communities methods;specificity community;communities;analysis characterize variation;clusters community combined;clusters community;characterize variation;social groups focused;types words", "pdf_keywords": "semantic variation online;detect semantic variation;language community specific;slang detection;community specific language;identify words community;language reddit communities;language community community;slang detection apply;language association computational;language online communities;language community;words community described;language involved community;semantic variation majority;specific language community;community reflected language;language variation internet;characterizing language variation;community influenced language;incorporate semantic variation;words community;detect semantic;semantic variation;computational language association;association computational language;detect community specific;lexical variation semantic;variation online communities;percentile semantic variation"}, "b116e5044fe047fc48307795af1f3e11b3a9401c": {"ta_keywords": "statistical machine translation;machine translation;machine translation methodswe;word alignments statistical;machine translation resultswe;variational bayes gza;alignments statistical machine;computes word alignments;gza using variational;bayes gza widely;word alignments;bayes gza;variational bayes improves;translation methodswe;variational bayes highly;alignments statistical;using variational bayes;performance gza improving;variational bayes;performance gza using;translation methodswe apply;translation;bayesian technique variational;improves performance gza;gza improving overall;gza improving;translation resultswe;performance gza;software computes word;bayes improves performance", "pdf_keywords": ""}, "4b18303edf701e41a288da36f8f1ba129da67eb7": {"ta_keywords": "zero shot learning;shot learning approach;approach zero shot;shot learning;shot learning embarrassingly;zero shot;shot learning aim;learning;learning embarrassingly simple;paper zero shot;learning approach;learning approach implemented;learn recognise new;learn recognise;datasets;learning embarrassingly;learning aim learn;recognise new concepts;approaches standard datasets;learning aim;simple approach zero;learn;aim learn;aim learn recognise;approach zero;shot;new concepts just;concepts just;standard datasets;recognise new", "pdf_keywords": "zero shot learning;zeroshot learning;method zeroshot learning;zero shot recognition;learning predicted attributes;zeroshot learning predicted;shot learning;shot learning approach;shot learning boolean;shot learning generalise;shot learning method;shot learning consists;shot recognition;prediction attributes;transfer learning domain;recognition unreliable attributes;learning domain adaptation;zero shot;prediction attributes based;introduction zero shot;classify attribute;classify attribute signatures;learning domain;transfer learning;use zero shot;predicted attributes;approach zero shot;domain adaptation;shot recognition unreliable;zeroshot"}, "b29dd2c50da0dc4589eafac58007f6be7e13c501": {"ta_keywords": "indoor scene understanding;helpful indoor scene;indoor scene;indoor environments;geometry indoor environments;objects helpful indoor;indoor environments bedrooms;understanding geometry indoor;identifying objects scene;environments bedrooms;geometry indoor;model enclosing room;helpful indoor;environments bedrooms simultaneously;bedrooms simultaneously identifying;indoor;couches doors methodswe;objects scene beds;objects scene;scene understanding;identifying objects;room box frames;simultaneously identifying objects;doors methodswe focus;doors methodswe;integrates camera model;scene beds couches;camera model;location specific objects;scene understanding resultswe", "pdf_keywords": ""}, "873b83326ad1f98549beb85bdb130a40a61e1f9b": {"ta_keywords": "large language models;information alternative scoring;language models;probability ranking string;ranking string probability;alternative scoring;probability ranking;language models shown;highest probability ranking;choice tasks;zero shot;choice tasks simply;multiple choice tasks;results zero shot;pointwise mutual information;alternative scoring function;tasks simply conditioning;conditional pointwise mutual;ranking string;domain conditional pointwise;scoring function directly;mutual information alternative;large language;ranking;mutual information;language;zero shot settings;introduce domain conditional;form competition;domain conditional", "pdf_keywords": "language models classification;language models efficient;language models learning;language models identify;language models ability;models classification tasks;large language models;classification tasks;generative language models;language models;classification tasks argue;language models increasing;language models generalized;novel language models;computational language models;language models important;generative language;language models report;ability generative language;neural conversation models;conversation models;language processing;language probability;process natural language;models classification;granularities sentiment classification;classification;recognizing textual;natural language processing;models learning"}, "9fe579e54712ba82c4f1c93e46409613f592df16": {"ta_keywords": "matrix adducts adducts;matrix adducts;matrix matrix adducts;adducts adducts adducts;adducts adducts;adducts;analysis matrix matrix;matrix;analysis matrix;matrix matrix;approach analysis matrix;present new;present;present new approach;new;new approach analysis;new approach;approach analysis;analysis;approach", "pdf_keywords": ""}, "178d51c35c03e3ccaae2409c32a3c2001cefe7eb": {"ta_keywords": "incremental model adaptation;incremental estimation;model adaptation approach;incremental model;model adaptation;derive incremental estimation;incremental estimation algorithm;new incremental model;posterior refinement modeled;adaptation approach;adaptation approach based;analytically derive incremental;process posterior refinement;based posterior distributions;derive incremental;incremental;new incremental;posterior distributions model;posterior refinement;bayesian approaches;refinement modeled analytically;posterior distributions process;adaptation;conventional bayesian approaches;approach based posterior;refinement modeled;estimation algorithm based;propose new incremental;estimation algorithm;based posterior", "pdf_keywords": ""}, "415d4231cab5ddee73e2ed536d033d5c31f24b4a": {"ta_keywords": "bootsstrapping biomedical ontologies;information extraction biomedical;extraction biomedical text;biomedical ontologies scientific;biomedical ontologies;ontologies scientific text;open information extraction;information extraction;biomedical text based;extraction web text;ontologies scientific;biomedical text;extraction web;designed extraction web;introduction bootsstrapping biomedical;extraction biomedical;semi supervised bootstrapping;bootsstrapping biomedical;supervised bootstrapping approach;ontologies;supervised bootstrapping;scientific text;scientific text using;bootstrapping approach learn;text based nonell;web text;bootstrapping approach;extraction;2010 designed extraction;biomedical", "pdf_keywords": ""}, "8163c4010fc103343518d49db5974577593972f6": {"ta_keywords": "automatic deception detection;deception detection;automatic deception;deception detection attempts;detect deception;attempt detect deception;detect deception perform;deception asking questions;deception perform actions;deception asking;deceptive conversational partner;deceptive conversational;potential liar method;work automatic deception;learn signs deception;signs deception asking;deception;telltale signs deception;catch potential liar;liar method;deception perform;unveil deceptive conversational;dialogue systems asks;signs deception;action envisioning dialogue;envisioning dialogue;dialogue systems;potential liar;envisioning dialogue systems;dialogue", "pdf_keywords": ""}, "fa5c7406d09af3f06a3a7ead49975e3ee90ed584": {"ta_keywords": "sharedd autonomy human;share autonomy human;autonomy human robot;introductionbalancing sharedd autonomy;sharedd autonomy;humanrobot language;human robot communication;robot communication objectiveroboic;agents share autonomy;viewing humanrobot language;communicating robot;autonomy human;share autonomy;robot communication;viewing humanrobot;autonomy;humanrobot;communication objectiveroboic agents;communicating robot taxing;human robot;communication objectiveroboic;robot;autonomy human leverage;lost communicating robot;objectiveroboic agents share;human domain knowledge;robot taxing;paper viewing humanrobot;leverage human domain;robot taxing unnatural", "pdf_keywords": "communicate robots task;language commands robots;humanrobot language;human robot collaboration;robot collaboration;implemented robot task;language communicate robots;communicate robots human;humanrobot language used;study humanrobot language;viewing humanrobot language;communicating robots;communicating robots trained;robots task;robot oriented language;human users robots;robot collaboration developed;interaction humans robots;interact human robots;commands robots;robot task;robot oriented languagethe;robot useful tool;way communicating robots;language preferences robot;communicate robots;human robot autonomy;robot useful;human robot useful;human robots important"}, "9527352b925f9fa36c40966ed755afd22301b0aa": {"ta_keywords": "decision net formalism;decision based preferences;qualitatively model preferences;model preferences outcomes;decision makers properties;preferences optimization criteria;preferences outcomes;decision net;preferences outcomes wish;constraints decision context;based preferences optimization;net formalism;preferences providing effective;model preferences providing;net formalism convenient;based preferences;based constraints decision;decision context;quality decision based;criteria decision makers;decision context means;moral norms based;model preferences;preferences optimization;based constraints;decision based;constraints decision;impact decision net;way model preferences;ethical moral norms", "pdf_keywords": ""}, "74c80622b91894efbe4ae9ce1428e4d699b05516": {"ta_keywords": "distributed convex optimization;methods distributed convex;stochastic gradient oracle;dual stochastic oracle;distributed convex;dual stochastic gradient;convex optimization;gradient oracle methods;convex optimization problems;primal dual stochastic;oracle methods distributed;gradient oracle;stochastic oracle propose;primal dual oracles;stochastic oracle;stochastic gradient;additionally dual stochastic;dual stochastic;methods distributed;dual oracles;optimization problems networks;dual oracles additionally;oracles additionally dual;distributed;networks methodswe;convex;networks methodswe introduce;problems networks methodswe;introduce primal dual;networks proposed methods", "pdf_keywords": ""}, "c00e4564ea054c14c83cb564af6c37e47c8ab367": {"ta_keywords": "active tracking markov;tracking markov chain;tracking markov;centralized active tracking;sensor subset tracking;observations markov chain;active tracking;tracking discrete time;active sensor subset;making observations markov;selection active sensor;subset tracking discrete;observations markov;active sensor;state markov chain;markov chain subset;subset tracking;tracking discrete;finite state markov;state markov;markov chain;chain subset sensors;markov chain unknown;sensor subset;tracking;markov chain having;markov;subset sensors;estimation process;transition probability matrix", "pdf_keywords": "sensing track markov;active tracking markov;tracking markov;tracking markov chain;centralized tracking markov;detecting tracking markov;track markov chain;tracking markov chains;track markov;estimation sensor activation;making observations markov;active sensing track;observations markov chain;sensor subset tracking;hidden markov model;observations active sensing;algorithm active sensing;state estimation achieve;hidden markov;optimal sensor activation;observations markov;active sensing;state estimation;active tracking;estimation sensor;active sensor selection;active sensor;like state estimation;designed track markov;active sensor subset"}, "98ef0db84e62aef969629264c9de1f4d0013f3b9": {"ta_keywords": "knowledge multiple tasks;tasks methodsfirst knowledge;learn task specific;learn task;multiple tasks;adapters encapsulate task;multiple tasks methodsfirst;stage learn task;forgetting difficulties dataset;knowledge multiple;incorporate knowledge multiple;multiple tasks suffer;dataset balancing;tasks methodsfirst;tasks;leverages knowledge multiple;difficulties dataset balancing;task specific information;task specific;encapsulate task;encapsulate task specific;methodsfirst knowledge extraction;knowledge extraction stage;task specific parameters;information combine adapters;knowledge extraction;extraction stage learn;tasks suffer catastrophic;aiming incorporate knowledge;suffer catastrophic forgetting", "pdf_keywords": "tasks sentiment analysis;tasks knowledge extraction;sentiment classification tasks;task learning adapterfusion;multi task learning;paraphrase detection rec;task learning propose;diverse nonlinguistic tasks;reasoning paraphrase detection;paraphrase detection;task learning;knowledge multiple tasks;learn task;nonlinguistic tasks sentiment;multiple tasks knowledge;learning natural language;knowledge composition classifier;learn task specific;tasks sentiment;transfer learning learns;ognizing textual entailment;natural language inference;learning learns parameterized;classification tasks long;natural language processing;task learning complete;learns parameterized;learning learns;classification tasks;extraction knowledge composition"}, "43953a051b6518f32fc37734cfc49942baeac5a1": {"ta_keywords": "spectral parameters utterances;speaker spectral parameter;statistical voice conversion;conversion statistical voice;voice conversion;introductionintra speaker spectral;speaker spectral;metrics speaker utters;parameter variation utterances;voice conversion technologies;training metrics speaker;statistical voice;parameters utterances used;parameters utterances;metrics speaker;spectral conversion statistical;sentence spectral parameters;prediction spectral conversion;utters sentence spectral;variation utterances;utterances used evaluation;sentence prediction spectral;spectral conversion;speaker utters;sentence spectral;utterances;utterances sentence prediction;voice;variation utterances sentence;spectral parameters", "pdf_keywords": ""}, "4077c1986f32817801b3082ce8dde514424f71a1": {"ta_keywords": "synset cleansing thesaurus;quality synsets thesaurus;synsets thesaurus created;synsets thesaurus;cleansing thesaurus crucial;thesaurus created collaboratively;cleansing thesaurus;crowddsourcing synset cleansing;confirm crowddsourcing synset;remove confirm crowddsourcing;thesaurus crucial;thesaurus crucial resource;thesaurus created;crowddsourcing synset;non expert annotators;thesaurus;expert annotators;expert annotators aim;confirm crowddsourcing;annotators;synset cleansing;collaboratively non expert;crowddsourcing;annotators aim;add remove confirm;natural language processing;quality synsets;sense reasoning highly;common sense reasoning;collaboratively non", "pdf_keywords": ""}, "2344cca985dd4e2e2519838b2353b5c295e73036": {"ta_keywords": "ai2 reasoninging challenge;introduces ai2 reasoninging;ai2 reasoninging;reasoninging challenge associated;reasoninging challenge;knowledge reasoning;introduces ai2;types knowledge reasoning;ai2;reasoninging;questions easy challenge;knowledge reasoning required;challenge associated arc;easy challenge sets;2018 introduces ai2;challenge sets;reasoning;challenge associated;arc dataset;associated arc dataset;questions easy;challenge sets paper;types knowledge;easy challenge;questions;domain complex science;complex science questions;challenge;complex science;knowledge", "pdf_keywords": "question annotators;reasoning challenge associated;questions question annotators;reasoning labeling reliably;reasoning labeling;question annotators high;ai2 reasoning challenge;knowledge reasoning types;reasoning techniques solutions;question answering;knowledge reasoning type;introduces ai2 reasoning;questions retrieved knowledge;method question answering;questions provide annotations;knowledge reasoning;label knowledge reasoning;reasoning types provide;reasoning types;complex reasoning techniques;question analysis;solve questions;types knowledge reasoning;used question analysis;reasoning techniques;understand questions;ai2 types knowledge;reasoning type labels;classification questions systematic;reasoning challenge"}, "05fb1eea6381ccd21bde53495c7707546aa234c7": {"ta_keywords": "named entity recognition;entity recognition shared;namedd entity recognition;entity recognition social;recognizing emerging named;emerging named entity;entity recognition;recognition social media;entity social media;emerging namedd entity;social media messages;named entity social;named entity;task mnl 2017;recognizing emerging;channel neural;channel bilstm cf;emerging namedd;multi channel neural;namedd entity;mnl 2017 workshop;social media methodsin;emerging named;channel bilstm;channel neural architecture;architecture recognizing emerging;task mnl;introductionmulti channel bilstm;mnl 2017;shared task mnl", "pdf_keywords": ""}, "3132a18a441ab6067066e4d4d85608b058c9ed33": {"ta_keywords": "backgroundchange point detection;subtle changes autocorrelation;autoencoders learn;changes autocorrelation;autoencoder based;backgroundchange point;autoencoders;used autoencoders learn;autoencoder;changes autocorrelation statistics;backgroundchange;used autoencoders;autoencoder based methodology;employ autoencoder based;changes time series;employ autoencoder;deep learning;deep learning techniques;autocorrelation;autocorrelation statistics signal;issues employ autoencoder;autocorrelation statistics;function used autoencoders;abrupt property changes;locate abrupt property;using deep learning;identify subtle changes;false alarm rate;locate abrupt;point detection", "pdf_keywords": "change point detection;detecting changes autocorrelation;detecting changes time;continuous change detection;change detection;detects abrupt changes;effective detecting changes;detecting changes;identifying change points;detecting changes spatial;detecting outliers change;detection time series;backgroundchronic change point;autoencoder based change;subtle changes autocorrelation;time invariant features;abrupt changes temporal;changes time series;change detection cmp;detection time domain;approach detecting changes;outliers change points;identifying change;change point detected;change points time;timeinvariant features aimed;abrupt changes;identify subtle changes;method detecting changes;extract features change"}, "91e605a125f64207a242693d0dc1c862080f6c27": {"ta_keywords": "automatic transcription word;automatic phoneme recognition;manual phonemic transcription;phoneme recognition;improve automatic phoneme;automatic phoneme;automatic transcription;lexicon manual phonemic;translations improve automatic;phoneme recognition res;phonemic transcription;transcription word level;translations minority language;manual automatic transcription;phonemic transcription possible;transcription word;speech manual automatic;language easily;language documentation;gathering speech manual;lexicon manual;method harness translations;harness translations;translations;speech manual;translations improve;harness translations improve;translations minority;prior lexicon manual;hand translations minority", "pdf_keywords": ""}, "b953a582cc79c33054b295c20c1201e8d5bd8243": {"ta_keywords": "affects automated tutoring;automated tutoring systems;automated tutoring;tutoring systems;student model model;student model;tutoring systems making;good student model;tutoring;quality student model;student model built;built student model;systems making instructional;model built student;students good student;making instructional decisions;student behavior;human students;instructional decisions quality;ways human students;human students good;making instructional;students;instructional decisions;student behavior patterns;learning task difficulty;students good;student model matches;model matches student;decisions quality student", "pdf_keywords": ""}, "d9d0d908e3f652ee350f4919d4c2ab972ada1ca4": {"ta_keywords": "coreference dataset entertains;coreference dataset annotation;new coreference dataset;coreference dataset;coreference models;coreference models identify;identify coreference models;text data coreferences;coreferences;wheelss coreference dataset;develop new coreference;coreference models methodswe;data coreferences;new coreference;models identify coreference;coreferences named;coreference;coreferences named entities;identify coreference;objectiveto identify coreference;data coreferences named;training wheelss coreference;annotation framework;wheelss coreference;dataset annotation;dataset annotation framework;references quiz bowl;annotation;humans use quiz;denser references quiz", "pdf_keywords": ""}, "f18ec4e0bce2e4d847954c9692959d88ba8a9b66": {"ta_keywords": "vehicles communicated wireless;vehicle v2x networks;sensing systems vehicle;remote estimation gass;safety sensing systems;secure remote estimation;individual vehicles communicated;vehicles communicated;compromised vehicles goal;safety sensing;malicious compromised vehicles;compromised vehicles;evaluate safety sensing;systems vehicle v2x;remote estimation;estimation gass markov;systems vehicle;vehicles considered measurements;measurements set vehicles;collected individual vehicles;vehicles goal;sensing systems;individual vehicles;vehicle v2x;estimation gass;set vehicles;v2x networks;vehicles;set vehicles considered;v2x networks methodsin", "pdf_keywords": ""}, "5e27712db641bc8f16c510292f7fd5440acd563d": {"ta_keywords": "collective classification widely;collective classification classes;collective classification;stacked graphical learning;separately collective classification;collective classification usually;used collective classification;context collective classification;graphical learning meta;meta learning scheme;classification relational;classification relational datasets;meta learning;learning meta learning;graphical learning;instance separately collective;used classification relational;methods propose stacked;propose stacked graphical;classification widely;stacked graphical;relational datasets inference;separately collective;classification;learning meta;related instances predicting;classes predicted simultaneously;classification classes;classification classes predicted;relational datasets", "pdf_keywords": ""}, "73e868f74376814a4c08eca6ce043fe7c7aefeed": {"ta_keywords": "similarity measures nodes;tree structured markov;fields personalized pagerank;pagerank;personalized pagerank;nodes based graphwalks;probabilities tree structured;personalized pagerank widely;pagerank widely;similarity measure graph;pagerank widely used;measure graph nodes;graph nodes based;structured markov random;based graphwalks;based graphwalks particular;measures nodes graphs;graph nodes;connection inference tree;markov random fields;nodes graphs;inference tree structured;probabilities tree;graphwalks particular;marginal probabilities tree;graphwalks;structured markov;inference tree;nodes based;tree structured", "pdf_keywords": ""}, "f6160c3196288b9e435dc6f86024f56e6b5ab722": {"ta_keywords": "fair utilitarian allocations;computing fair utilitarian;computing allocations fair;utilitarian allocations indivisible;fair maximize utilitarian;utilitarian allocations;computing fair;allocations fair;allocations indivisible goods;allocations fair maximize;fairness concepts envy;fair utilitarian;tractable fairness concepts;tractable fairness;allocations indivisible;indivisible goods;focus tractable fairness;maximize utilitarian;indivisible goods methods;computing allocations;maximize utilitarian social;fairness concepts;envy freeness item;complexity computing allocations;allocations;utilitarian social welfare;welfare sum utilities;envy freeness;fair maximize;concepts envy freeness", "pdf_keywords": ""}, "579095d50eab27ace24a1ea0e97af2f70191dc7c": {"ta_keywords": "information telxt graphiograph;graphiograph challenging task;telxt graphiograph;graphiograph;telxt graphiograph challenging;subcellular location image;graphiograph challenging;text images journal;mining data text;biology combination text;information text image;images journal articles;image finder extracts;text images;combination text images;associating information text;images journal;finder extracts information;text image;extracts information;introductionassociating information telxt;image finder;location image finder;matching sub figures;extracts information particular;mining data;articles associating information;associating information;subcellular location;lif subcellular", "pdf_keywords": ""}, "0c61265a4325df4b97389f92e5e4f5df412f8e97": {"ta_keywords": "localization partial discharge;partial discharge location;location power transformer;discharge location power;method partial discharge;transformers multi path;discharge location;acoustic electrical joint;diffraction ultrasonic signal;diffraction ultrasonic;power transformers;operation power transformers;ultrasonic signal;acoustic electrical;refraction diffraction ultrasonic;power transformer;proposes acoustic electrical;power transformers multi;transformer;accurate localization partial;ultrasonic;partial discharge;introductionefefficient accurate localization;localization accuracy methodsthis;electrical joint method;ultrasonic signal add;localization partial;accurate localization;transformers multi;localization process", "pdf_keywords": ""}, "46a2960e409c39901c1efd07a6adfc5f26e22ee8": {"ta_keywords": "unintended recipient widespread;mistakes addressing email;introductioninformation leaks suggestions;analytic leaks accidentally;introductioninformation leaks;unintended recipient;mistake analytic leaks;mozilla thunderbird methodspeople;analytic leaks;leaks accidentally sending;thunderbird methodspeople;recipient widespread problem;message unintended recipient;thunderbird methodspeople make;make mistakes addressing;messages type costly;addressing email messages;thunderbird;mozilla thunderbird;mistakes addressing;leaks suggestions case;recipient likely source;email messages;costly mistake analytic;accidentally sending;addressing error forgetting;addressing email;recipient;leaks suggestions;type costly mistake", "pdf_keywords": ""}, "d6a7d2e9f2caf3e8eb615580f4ee8329ff9a271d": {"ta_keywords": "queue available parking;curbside parking network;network interdependent queues;parking network interdependent;parking network;available parking space;interdependent queues;capacity queues drivers;parking space;adjacent queue;model curbside parking;curbside parking;adjacent queue available;queue available space;available parking;queues drivers;capacity queues;interdependent queues introduce;parking;parking space continue;queues drivers arrive;finite capacity queues;drivers cruising neighborhood;search adjacent queue;queues;queue;networks finite capacity;proportion drivers cruising;queues introduce;join queue", "pdf_keywords": ""}, "4d01d6b445077ad0f1c9d85af93f9ed9239f3c33": {"ta_keywords": "tagging historical texts;pos tagging historical;ofspeech tagging historical;tagging historical;tagging historical data;introduction pos tagging;tagging accuracy manuscript;tagging accuracy;accuracy tagging;pos tagging;ofspeech tagging;historical texts sparse;accuracy tagging accuracy;method ofspeech tagging;evaluate accuracy tagging;tagging;historical texts;corpora historical german;texts sparse;texts sparse training;corpora historical;historical german;historical german 15th;different corpora historical;german 15th;german 15th 18th;manuscript 15th century;texts different corpora;data evaluates texts;texts", "pdf_keywords": ""}, "189e6bb7523733c4e524214b9e6ae92d4ed50dac": {"ta_keywords": "introductiontransgression learning sequencetagging;learning sequencetagging hierarchical;learning sequencetagging;neural sequence taggers;sequencetagging hierarchical recurrent;sequence taggers;sequencetagging hierarchical;sequence taggers source;tagging pen treebank;sequencetagging;annotations pos tagging;taggers source task;hierarchical recurrent networks;pos tagging;recurrent networks attractive;learning neural sequence;hierarchical recurrent;recurrent networks;tagging;treebank;pen treebank;annotations pos;introductiontransgression learning;problem transfer learning;plentiful annotations pos;treebank used improve;task plentiful annotations;treebank used;available annotations pos;pos tagging pen", "pdf_keywords": "neural sequence tagging;sequence tagging tasks;tagging using neural;neural architectures corpora;sequence tagging;approach sequence tagging;sequence tagging design;sequence tagging using;corpora transfer learning;different sequence tagging;sequence tagging important;learning neural architectures;entity recognition;approach neural sequence;tagging design neural;natural language processing;named entity recognition;transfer learning neural;deep neural;tagging tasks;approach neural architectures;neural networks jointly;entity recognition applications;language processing;neural sequence;neural architectures employ;neural architectures;new approach neural;transfer learning;neural network transfer"}, "58b628792d3eb22a034a871ed3cf373afe591928": {"ta_keywords": "codes hadoop hdf;erasure codes efficiently;new codes hadoop;hadoop hdf compare;codes hadoop;erasure codes;hadoop hdf;codes efficiently repairable;family erasure codes;hdf compare currently;hdf compare;reed solomon codes;hdf module uses;hdf module;solomon codes;solomon codes analytically;hadoop;codes efficiently;currently deployed hdf;deployed hdf;hdf;deployed hdf module;codes analytically codes;codes optimal;codes optimal recently;analytically codes optimal;tradeoff locality minimum;codes analytically;erasure;uses reed solomon", "pdf_keywords": "codes efficiently repairable;locally repairable codes;repairable codes optimal;repairable codes marginally;erasure codes efficiently;locally repairable code;repairable codes;repairable codes logarithmic;codes logarithmic locality;efficiently repairable;locality coded blocks;repairable code implemented;erasure codes;code block locality;repairable codes lrcs;prove locally repairable;locally repairable;data recovery clusters;recently erasure codes;erasure codes used;reliable replication scheme;codes efficiently;repairable code;lrcs efficiently repairable;block locality;scheme reliable replication;erasure codes called;logarithmic locality coded;reliable replication;recovery clusters"}, "58174f5bb9f9815b52a99fa03ec42f2b44f2d550": {"ta_keywords": "set instance extraction;instance extraction;instance extraction using;introductionautomated set instance;automatic set instance;semantic lexicons;production semantic lexicons;semantic class;semantic class input;semantic lexicons large;takes semantic class;automatic set;extraction using web;production semantic;lexicons large corpus;lexicons;problem production semantic;set instance;introductionautomated set;semantic;aniana automatic set;lexicons large;takes semantic;corpus;instance;large corpus;named aniana automatic;extraction using;acquirer takes semantic;class input car", "pdf_keywords": ""}, "2bafaffe45ba66685c87e2d0a598222a9a68ae13": {"ta_keywords": "russian linguistic resources;russian language processing;russian linguistic;resources russian language;community russian linguistic;russian language;tools resources russian;catalogue linguistic resources;linguistic resources presented;catalogue community russian;resources russian;catalogue linguistic;linguistic resources assist;linguistic resources;specialized catalogue linguistic;language processing;community russian;russian;survey nlub catalogue;methodsin survey nlub;linguistic;nlub catalogue community;nlub catalogue;language processing significant;survey nlub;language;resources presented compared;nlub;resources presented;specialized catalogue", "pdf_keywords": ""}, "108ec3512cdf2e89ba3067f5b10eaa4a96df9347": {"ta_keywords": "auditory model adaptation;transfer vectors acoustic;vectors acoustic modeling;introduction auditory model;auditory model;training transfer vectors;introduction auditory;vectors acoustic;acoustic modeling;acoustic modeling using;model adaptation based;model adaptation;reformulate adaptation;transfer vectors using;adaptation based coarse;reformulate adaptation scheme;transfer vectors;auditory;directional statistics methods;adaptation scheme;transfer vector;adaptation based;training transfer;directional statistics;paper reformulate adaptation;adaptation scheme coarse;fine training transfer;adaptation;cf transfer vectors;using directional statistics", "pdf_keywords": ""}, "b575d272036740e03fcf67d64db969557843f629": {"ta_keywords": "representations tweets learning;space representations tweets;representations tweets;tweets learning complex;composition model tweet2vec;tweets learning;model tweet2vec;tweet2vec;tweet2vec finds vector;tweet2vec finds;model tweet2vec finds;text social media;characters commonplace posts;character composition model;text social;character composition;propose character composition;character sequences;context text social;dependencies character sequences;informal language spelling;tweets;word level approaches;informal language;fail informal language;abbreviations special characters;social media provides;vector space representations;characters commonplace;special characters commonplace", "pdf_keywords": "vectorspace representations tweets;representations tweets learning;tweet embedding algorithm;posts hashtags embeddings;generate tweet embedding;tweets learning;tweet embedding;representations tweets;predicting hashtags;hashtags embeddings;predicting hashtags post;tweets learning complex;task predicting hashtags;tweet2vec outperforms word;present tweet2vec character;hashtags embeddings close;tweet2vec character;prediction tweet tweet2vec;tags tweet2vec outperforms;tweet2vec outperforms;predicting prediction tweet;tags tweet2vec;text prediction words;post text prediction;social media text;hashtags post latent;composition model tweet2vec;associated tags tweet2vec;text social media;tweet2vec novel useful"}, "d34712c217046ccf8063efe083fbb1e6cbfc0340": {"ta_keywords": "backgroundcontent delivery networks;clusters large cdn;delivery networks ccns;backgroundcontent delivery;networks ccns deliver;performance large distributed;delivery networks;ccns deliver world;distributed;logs 2000 clusters;clusters deployed;distributed challenging;clusters deployed edges;thousands clusters deployed;large distributed;ccns deliver;networks ccns;deliver world web;large distributed challenging;cdn study patterns;large cdn study;clusters large;cdn redundancy;clusters;thousands clusters;users thousands clusters;presentationfor cdn redundancy;large cdn;internet maintain;2000 clusters large", "pdf_keywords": ""}, "f8e580fcf34ee6da50989bbde685634018cbe224": {"ta_keywords": "dialog state tracking;track dialog state;dialog utterance tracker;track dialog;human dialog utterance;human dialog;human human dialog;dialog state human;dialog utterance;advanced dialog state;dialog state;introduction dialog state;5th dialog state;sequence sequence learning;attention based sequence;dialog;sequence learning;advanced dialog;utterance tracker;present advanced dialog;dc5 track dialog;5th dialog;sequence learning methods;state tracking attention;tracking attention based;designed 5th dialog;introduction dialog;tracking attention;state tracking challenge;utterance tracker emits", "pdf_keywords": ""}, "8a6d2e134b3b2df6291af8e36e126ae55d50649c": {"ta_keywords": "recurrent networks paraphrastic;paraphrastic sentence embeddings;paraphrastic senstence embeddings;networks paraphrastic senstence;networks paraphrastic;sentence embeddings;sentence embeddings revisiting;paraphrastic senstence;introductionresiting recurrent networks;recurrent networks underperform;general purpose paraphrastic;paraphrastic;recurrent networks;lm recurrent networks;paraphrastic sentence;training sentence pairs;embeddings revisiting;purpose paraphrastic;purpose paraphrastic sentence;introductionresiting recurrent;sentence pairs;underperform word averaging;embeddings;senstence embeddings;embeddings revisiting setting;networks underperform word;2016b lm recurrent;lm recurrent;word averaging present;recurrent", "pdf_keywords": "recurrent networks paraphrastic;embeddings word sequences;recurrent networks;paraphrastic senstence embeddings;introductionresiting recurrent networks;word sequences challenging;paraphrastic sentence embeddings;sentence embeddings development;networks paraphrastic;sentence embeddings;word sequences experimental;networks paraphrastic senstence;processing phrase model;independent embeddings word;ltm supervised textual;paraphrastic textual similarity;word sequences;predict semantic similarity;embeddings word;lexical sentential tasks;supervised textual;predict semantic;performance paraphrastic textual;domain independent embeddings;model lexical sentential;phrase model;tasks phrase model;textual similarity tasks;text similarity tasks;ltm textual similarity"}, "3911b13a61a3a57674cc8c70c760f545de8aeea2": {"ta_keywords": "honest responses agents;eliciting honest responses;mechanism strong incentive;incentive;online platforms labeling;labeling images gradeing;reviews online platforms;strong incentive properties;reward;incentive properties;new reward mechanism;strong incentive;incentive properties applicable;reward mechanism;propose new reward;large scale evaluations;responses agents;images gradeing online;reward mechanism strong;platforms labeling;intuitive output agreement;eliciting honest;gradeing online courses;platforms labeling images;images gradeing;evaluations product service;gradeing online;courses eliciting honest;service reviews online;evaluations product", "pdf_keywords": ""}, "f335e2256b31d9458c10c61e60bb8bed9dcaf1d9": {"ta_keywords": "vision language navigation;learning navigate visual;learning vision language;language navigation learning;learning navigate;vision language;navigation learning navigate;navigation learning;view learning vision;introductionmulti view learning;navigate visual;view learning;language navigation;learning vision;instructions different views;language ambiguity improve;natural language instructions;language instructions challenging;navigate visual environment;navigation;following natural language;navigate;language instructions highly;task natural language;training paradigm learn;paradigm learn;view;visual;ambiguity improve;natural language", "pdf_keywords": "learns navigate based;approach learns navigate;learns navigate;learning agent navigate;instructions approach learns;instructions ability navigate;introductionlearning navigate visual;visual navigation;introductionlearning navigate;navigate way adaptively;view learning agent;navigate visual;view learning;view learning present;view learning provide;approach learns;navigate visual environment;virtual lon learning;approach learns morethe;agent navigation;agent navigate;model visual navigation;view agent navigation;ability navigate;novel approach navigation;approach navigation;agent able navigate;visual navigation real;approach navigate;instructions different views"}, "5f23482a8c06ca1ae3e4577e3fdd9213884dac85": {"ta_keywords": "physician management patient;role physician management;physician management;role physician;literature role physician;management patient disease;physician;patient disease;management patient;patient;disease;literature role;management;article present literature;role;present literature role;article present;literature;article;purpose article;purpose article present;present literature;purpose;present", "pdf_keywords": ""}, "1e638d235a512cc76d00713639259540342c6fbe": {"ta_keywords": "neural relationship extraction;semantic relation extraction;relation extraction classification;relation extraction;relation extraction model;relationship extraction;relationship extraction model;end relation extraction;semantic relation;extraction classification scientific;neural relationship;concept embeddings;incorporated concept embeddings;task semantic relation;relation;extraction model selectively;2018 task neural;extraction classification;classification scientific papers;task neural relationship;concept embeddings aimto;papers methodsour model;semantic;end relation;scientific papers methodsour;extraction model;task neural;shared task semantic;papers methodsour;embeddings", "pdf_keywords": ""}, "c07fdc95bbf533f8709f8e39c069c1e22b73a7dc": {"ta_keywords": "new safe polyelectrolyte;safe polyelectrolyte polyelectrolyte;safe polyelectrolyte;polyelectrolyte polyelectrolyte;polyelectrolyte polyelectrolyte able;polyelectrolyte able used;polyelectrolyte able;polyelectrolyte;battery related defects;etiology battery;etiology battery related;battery related;battery;used battery;able used battery;related defects;defects;defects poorly understood;related defects poorly;defects poorly;new safe;development new safe;safe;new;poorly understood;poorly understood development;development new;understood development new;etiology;able used", "pdf_keywords": ""}, "26c65dad79da20aa67df21a6c10e509a964f0841": {"ta_keywords": "tckbp englishentity linking;englishentity linking slot;2013 tckbp englishentity;englishentity linking;reference knowledge base;tckbp englishentity;entities linking;knowledge base slot;entries reference knowledge;entity mentions correspond;knowledge base;illinois wikifier;illinois wikifier additional;entity mentions;wikifier additional functionality;wikifier;version illinois wikifier;wikifier additional;reference knowledge;systems entities linking;cluster entity mentions;entities linking integrates;correspond entries reference;entity;slot filler validation;entities;mentions correspond entries;mentions correspond;linking slot;submission 2013 tckbp", "pdf_keywords": ""}, "d4d25eaa373087ac80810d79afff863ef1bae3c3": {"ta_keywords": "pressure offloading wheelchair;offloading wheelchair users;offloading wheelchair;weight wheelchair users;weight wheelchair;stationary sitting periods;background weight wheelchair;increased loading ischial;onset pressure ulcers;wheelchair users;periods stationary sitting;wheelchair users experience;pressure ulcers;pressure ulcers experts;wheelchair;pressure ulcers cause;stationary sitting;loading ischial tuberosities;periodic pressure offloading;prolonged periods stationary;sitting periods accompanied;sitting periods;wheelchair users remember;pressure offloading;pressure offloading recommended;loading ischial;needed pressure offloading;sepsis periodic pressure;ulcers experts recommend;ulcers cause complications", "pdf_keywords": ""}, "a4cd428d196bf041c22592216f15246b98b91915": {"ta_keywords": "risk developing malignant;occurrence malignant disease;developing malignant diseases;malignant diseases;malignant diseases disease;malignant disease world;cause morbidity mortality;malignant disease;occurrence malignant;major cause morbidity;diseases disease caused;developing malignant;world disease caused;diseases;disease caused;cause morbidity;morbidity mortality world;mortality world disease;disease caused increased;diseases disease;morbidity mortality;world disease;disease;malignant;caused increased risk;disease world;risk developing mal;increased risk developing;mortality;mortality world", "pdf_keywords": ""}, "8a94106364576f0aa79dccfb30f0536514408249": {"ta_keywords": "feature based ranking;based ranking functions;noisy relevance judgment;learning eective feature;labeled document pairs;based ranking;ranking;eective feature based;ranking functions fundamental;eective feature;relevance judgment single;noisy relevance;ranking functions;based pairwise preference;pairwise preference framework;learning eective;document pairs;pairwise preference;introduction learning eective;relevance judgment;process noisy relevance;labeled document;document pairs used;feature based;preference framework;mis labeled document;relevance;judgment single document;search engines;feature", "pdf_keywords": ""}, "abcaec70b463ed925c29180437ed581c971952cf": {"ta_keywords": "dialogue modelsing ebdm;dialogue quality dialogue;quality dialogue corpora;dialogue modeling;dialogue example databases;example based dialogue;dialogue modeling important;dialogue modelsing;dialogue systems dialogue;dialogue corpora plural;spoken dialogue systems;systems dialogue modeling;based dialogue modelsing;quality dialogue;improving dialogue quality;dialogue systems;dialogue example;improving dialogue;dialogue quality;response utterances examples;dialogue corpora;response utterances;spoken dialogue;systems dialogue;build dialogue example;methods build dialogue;responses utterance;dialogue;responses utterance merges;introduction spoken dialogue", "pdf_keywords": ""}, "f46a3a5dc70a70292175e6c7ad505b8206cb070c": {"ta_keywords": "speech recognition ar;microphone automatic speech;introductiondient microphone automatic;performance automatic speech;speech recognition;speech recognition systems;chime challenge initiative;automatic speech recognition;automatic speech;microphone automatic;2nd chime challenge;chime challenge;introductiondient microphone;microphone;background sources reverberation;2nd chime;sources reverberation;reverberation;results 2nd chime;recognition ar;reverberation paper reports;sources reverberation paper;chime;recognition systems real;recognition systems;reverberation paper;speech;recognition;recognition ar remains;datasets baseline systems", "pdf_keywords": ""}, "1756376bf7cf0d0a7bec881d663b57907a361ecf": {"ta_keywords": "incremental editing structured;editing structured;model incremental editing;incremental editing;editing sequential;model single editing;editing structured data;models editing processes;models editing;data structural edits;structural edits;proposed models editing;structural edits methods;editing sequential data;single editing;editing;neural generative models;neural generative;generative models generate;generative models;editing processes;generative;edits methods;editor;introduction neural generative;structured;single editing pass;edits methods editor;editor learn;model incremental", "pdf_keywords": "incremental editing structured;incremental editing tree;editing tree structured;editing structured;model incremental editing;incremental editing;editing tree process;editing tree;learn editing tree;incrementally modify tree;editing structured data;editing tree use;partially edited tree;editor editing tree;bythe editing tree;sequence editing;editing sequential;models editing processes;propose neural editor;modify tree structured;proposed models editing;algorithm structural edits;edits propose neural;sequence editing actions;edits tree;edit representations tree;neural editor;editing existing trees;model single editing;tree edit"}, "5eba3e525056cac6112cf0b13b62d86ba66661d9": {"ta_keywords": "tagging active learning;speech tagging active;oof speech tagging;speech tagging;active learning oof;tagging active;confusion active learning;active learning;resource syntactic analyzers;syntactic analyzers;speech pos taggers;syntactic analyzers speech;active learning uses;tagging;minimize annotation cost;samples minimize annotation;minimize annotation;annotation cost essential;annotation;analyzers speech pos;taggers;annotation cost;low resource syntactic;learning oof speech;resource syntactic;analyzers speech;taggers existent;syntactic;training instances annot;pos taggers existent", "pdf_keywords": "speech tagging;method speech tagging;learning useful annotating;speech tagging demonstrate;annotations target language;human annotation using;accuracy existing annotations;minimize annotation cost;data annotation representative;human annotation;training instances annotating;newly acquired annotations;annotating;useful annotating;annotation representative;800 labeled sentences;existing annotations propose;perform human annotation;acquired annotations;annotations;annotations propose;labeled sentences;existing annotations;annotations propose method;annotation using;tagging demonstrate shortcomings;minimize annotation;data annotation;annotation;selecting data annotation"}, "84bc74d875e748aa0f11ac0c5e3000b16484b053": {"ta_keywords": "backgrounddiabetes common problem;backgrounddiabetes;backgrounddiabetes common;major problem world;world major problem;common problem world;problem world major;major problem;common problem;problem world;world problem;world problem world;problem world problem;problem;world major;world;common;major", "pdf_keywords": "annotators natural language;natural language inference;natural language adversaries;annotators natural;constructions annotators;annotators human annotators;annotators;human annotators natural;human annotators;adversarial examples generated;existing instances adversarial;use annotators described;adversarial instances discussed;generate adversarial instances;annotators described methods;use annotators;factoid sentences supported;constructions annotators used;described use annotators;fever2 shared task;adversarial instances generated;annotators described;fact extraction verification;generating probable sentences;new neural models;developments natural language;fact extraction;resulting adversarial instances;annotators human;algorithm natural language"}, "ecab8208e5182d4b3b0d6183928e816301d2366d": {"ta_keywords": "net regularization sparse;elastic net regularization;regularization sparse linear;regularization sparse;sparse linear models;net regularization;sparse linear;regularization;introductionefefficient elastic net;efficiently training linear;training linear;sparse;training linear model;elastic net;introductionefefficient elastic;zero features lazily;efficiently training;zero features;linear models;linear models methodswe;elastic;features lazily;non zero features;linear;features lazily bring;linear model;updates l1 l1;updates l1;models methodswe extend;l2 norm", "pdf_keywords": "efficient training sparse;training sparse;training sparse linear;regularization decreasing learning;elastic net regularization;net regularization algorithms;net regularization;lazy l2 regularization;regularization algorithms;net regularization experiments;regularization algorithms applicable;fast training linear;regularized elastic net;regularization;stochastic gradient descent;gradient descent optimize;regression regularized;l2 regularization;net regularization derive;gradient descent;net regularization methodsextending;regression regularized elastic;stochastic gradient updates;online optimization update;regularized;regularization methodsextending previous;online optimization;descent optimize;regularization methodsextending;regularization derive"}, "02cc92287c6614b6a2aa982007471f16b3450013": {"ta_keywords": "human robots;human robots recently;human robot;human human robot;human robot useful;principle human robots;tool human robots;robot;robots;talk machines;robot useful;machines build speak;know talk machines;robot useful tool;talk machines build;human genome;state human genome;human genome fundamental;robots recently;recognition synthesis machine;useful tool human;artificial intelligence;autonomous driving;speech recognition synthesis;autonomous;processing artificial intelligence;speak natural language;natural language make;component technologies speech;natural language instructions", "pdf_keywords": ""}, "ddc502b6c0d08fefe5b77639e4737cd8c7bce25c": {"ta_keywords": "structure html documents;structure html;html documents methods;html documents;web pages structure;types structure html;textual similarity;textual similarity developed;html;notion textual similarity;similarity developed information;information retrieval;meaningful structure used;pages structure ranked;similarity developed;meaningful structure;pages structure;recognizing certain types;documents methods;developed information retrieval;similarity;documents methods implemented;textual;structure ranked;soft logic incorporates;information retrieval community;methods recognizing;structure used;structure;methods recognizing certain", "pdf_keywords": ""}, "ab42ad9698386cc15a30a8c7885fa82b260f537b": {"ta_keywords": "curbside parking demand;parking demand;parking demand propose;curbside parking;parking performance;parking performance based;search parking performance;properties curbside parking;cruising search parking;parking;search parking;pricing policies;performance based pricing;administer pricing policies;pricing policies resultsspecifically;based pricing;seattle department transportation;congestion caused drivers;administer pricing;understanding administer pricing;pricing schemes;properties curbside;based pricing schemes;transportation considering aforementioned;pricing;demand propose methods;transportation considering;department transportation considering;temporal properties curbside;mitigate congestion", "pdf_keywords": ""}, "3abcd0ffc54c3a16c9dc5e5d3ea59eaa43070127": {"ta_keywords": "machine learning algorithms;standard machine learning;machine learning approaches;machine learning;learning algorithms used;learning algorithms;introduction machine learning;quality life algorithms;learning approaches;algorithms exhibit undesirable;algorithms used;learning approaches burden;algorithms;life algorithms;algorithm computer;learning;algorithm computer scientist;life algorithms exhibit;algorithms exhibit;computer scientist methods;algorithms used increasing;diagnoses standard machine;algorithm;user algorithm;user algorithm computer;medical;loss delaying medical;approaches burden avoiding;delaying medical diagnoses;burden avoiding harmful", "pdf_keywords": "algorithm predict students;machine learning predict;regression classification algorithms;education gp prediction;designing machine learning;learning algorithms create;learning predict;ensuring machine learning;machine learning prediction;algorithm predict;algorithm predicting;learning algorithm undesirable;challenging machine learning;regression classification;predict undesirable behavior;algorithm predicting students;behaved machine learning;gp prediction;safe machine learning;method predicting grade;learning algorithms safe;learning algorithms called;automated algorithms clinical;classifiers;regression algorithm predict;predicting grade;gp prediction present;regression algorithms enforce;constraints ability predict;learning prediction"}, "37241cdc693b9c2daf49557f18c1ad6a15247239": {"ta_keywords": "document binarization;present document binarization;document binarization scheme;binarization;binarizing;used prominent binarization;binarization scheme;color document images;prominent binarization;consistently binarizing;binarizing range;binarization competition;binarization scheme intended;thresholding;consistently binarizing range;prominent binarization competition;niblack thresholding method;binarizing range degraded;niblack thresholding;popular niblack thresholding;thresholding method;degraded color document;based segmentation applied;based segmentation;document images;document images proposed;color document;binarization competition subjected;intended consistently binarizing;segmentation", "pdf_keywords": ""}, "f66a17836380c0c79c1b42a9219cf8fde6524287": {"ta_keywords": "question answering systems;answers unstructured text;precise answers unstructured;question answering;answers unstructured;modern question answering;unstructured text specialized;answering systems;unstructured text corpora;answering systems rely;text specialized domains;qa requires deep;kbs unstructured text;qa using text;knowledge bases kbs;corpora sources answers;specialized qa using;unstructured text;rely knowledge bases;knowledge bases;domain expertise kbs;sources answers kbs;deep domain expertise;expertise kbs;text corpora sources;text specialized;text corpora;expertise kbs lacking;software engineering qa;answers kbs", "pdf_keywords": ""}, "a817785f0100f3fadc5c1203974d151d5b093310": {"ta_keywords": "parallel programs;implement parallel programs;parallel programs multiple;parallel virtual machine;programs multiple computers;pvm message passing;parallel virtual;libraries parallel virtual;interface mpi;virtual machine pvm;single computation programs;interface mpi provide;machine pvm message;mpi provide common;independent computers;implement parallel;programming interface;common application programming;application programming interface;passing libraries parallel;passing interface mpi;pvm message;virtual machine;computation programs;message passing libraries;libraries parallel;message passing interface;multiple computers libraries;computation programs written;programs multiple", "pdf_keywords": ""}, "9336a2ff833d0b4bc914e2282ad04e19d27bc2be": {"ta_keywords": "substation grounding impedance;substation grounding;220kv substation grounding;grounding impedance ground;grounding grid 220kv;grounding impedance;grid 220kv substation;impedance ground potential;injected grounding grid;grounding grid;impedance ground;grounding grid affect;grounding grid cause;connected ground grid;220kv substation;substation;ground grid locally;ground grid;substation shell secondary;grid cause ground;intelligent substation;current injected grounding;ground potential;cause ground potential;aiming grounding grid;points grounding grid;ground potential rise;backgroundin intelligent substation;substation shell;equipment connected ground", "pdf_keywords": ""}, "4d5f9a0aba65ba6294c543ba5e6108e6d690f133": {"ta_keywords": "supervised domain adaptation;domain adaptation;domain adaptation work;extracting protein mentions;semi supervised domain;supervised domain;features semi supervised;semi supervised;protein mentions academic;mentions academic publications;protein mentions;distinct source domains;transfer knowledge methods;source domains;researchers labeled examples;labeled data related;source domains seemingly;labeled examples desired;labeled data;researchers labeled;supervised;extracting;encountered researchers labeled;transfer knowledge;labeled examples;way transfer knowledge;extracting protein;document structural frequency;document structural;labeled", "pdf_keywords": ""}, "c69da8266e2f3f67febf22b8f2bf91623346d283": {"ta_keywords": "increased concerns twitter;websites shared twitter;concerns twitter;twitter;shared twitter;information sources agenda;advancing antivaccination messages;shared twitter turn;twitter turn;concerns twitter role;twitter turn uncover;pandemic consequent infodemic;content dynamics information;twitter role advancing;antivaccination messages vaccine;antivaccination messages;tracking links websites;uncover content dynamics;infodemic increased concerns;messages vaccine available;messages vaccine;advancing antivaccination;content dynamics;tracking links;role advancing antivaccination;sources agenda;twitter role;information sources;links websites shared;infodemic", "pdf_keywords": ""}, "cd9bfa6266cab4bf4b04c82746a5b650f83b57e4": {"ta_keywords": "non convex optimization;algorithms non convex;guarantees optimization algorithms;convex optimization methods;convex optimization;non convex problems;guarantees optimization;optimization algorithms non;performance guarantees optimization;general non convex;non convex;neural networks optimization;convex optimization application;increased optimization algorithms;recent increased optimization;networks optimization;convex problems;networks optimization problems;optimization algorithms;optimization methods;training deep;optimization application training;convex problems solved;optimization methods start;optimization;training deep neural;increased optimization;optimization problems;deep neural networks;deep neural", "pdf_keywords": "non convex optimization;non convex optimizing;optimization non convex;convex optimization non;algorithms non convex;convex optimization described;convex optimizing methods;convex optimization discussed;convex optimization propose;non convex minimization;convex optimization;convex optimization problems;convex optimization applications;convex stochastic optimization;nonconvex stochastic optimization;guarantees optimization algorithms;convex optimizing;algorithm convex non;convex smooth optimization;convex optimization recover;convex optimization implications;convex optimization method;stochastic optimization non;convex optimization general;convex optimization start;optimization algorithms non;non convex objective;unconstrained optimization;convex optimization based;advances non convex"}, "d385d8563192569b229bde762fcd4d57ce2b3ee2": {"ta_keywords": "learn definitions software;automatically generating definitions;generating definitions technical;definitions software entities;definitions software;learn definitions;definitions technical terms;define terms software;resultsspecifically learn definitions;terms software domain;learning define terms;text technical domain;technical terms reading;domain specific terms;definitions technical;terms software;domain resultsspecifically learn;software entities;technical domain;technical terms;define terms;knowledge domain;knowledge domain ask;software domain;technical domain resultsspecifically;text technical;terms reading text;entities large corpus;reading text technical;terms reading", "pdf_keywords": ""}, "63d7e40da7f0d37308b8e97fca4a14a26a6b52ea": {"ta_keywords": "data quality;conservation data quality;data data cascades;data cascades high;data valued;data cascades;paradoxically data valued;data quality carries;data;high stakes ai;data data;ai heightened downstream;paradoxically data;stakes ai heightened;ai heightened;ai;stakes ai;health conservation data;data valued glamorised;conservation data;ai methods;reported data data;reported data;impacting predictions;ai methods paper;impacting predictions like;allocations paradoxically data;impact impacting predictions;predictions like cancer;wildlife poaching loan", "pdf_keywords": "data excellence ai;ai data provide;data critical infrastructure;data practices high;data high stakes;ai data;data critical;ai systems data;ai critical data;data work broadly;data practices challenges;data viewed ai;data practices;data excellence high;data development;development data science;build artificial intelligencehigh;data excellence;build artificial intelligence;data development artificial;systems data critical;ai practitioner development;study data practices;ai practitioners;practices shaping data;data provide;data ethics;ai ways data;data human;stakes ai data"}, "7707af52b3e19bfd3fc07c2be5aed044e5d7953a": {"ta_keywords": "homology improvedd locality;persistent homology method;persistent homology;use persistent homology;homology method;homology;homology improvedd;introductionpersistent homology improvedd;homology method compare;topology data;introductionpersistent homology;deep networks delineate;networks delineate;topology data sets;improvedd locality;images neuronal;compare topology data;delineate road networks;improvedd locality information;neuronal processes microscopy;networks delineate road;images neuronal processes;aerial images neuronal;locality information effective;networks;topology;locality information;training deep;microscopy;microscopy scans", "pdf_keywords": "persistent homology train;homology train deep;effective detecting topological;segmentation using persistent;persistent homology useful;demonstrate persistent homology;detecting topological;persistent homology;using persistent homology;use persistent homology;counting persistent homology;topology data;method detecting topological;persistent homology method;detecting homology;deep networks segment;detecting topological differences;homology train;homology method detecting;deep networks delineate;persistence diagram deep;connectivity segmentation;method detecting homology;images demonstrate persistent;detecting homology classes;networks delineation crowd;deep networks delineation;connectivity metrics segmentation;connectivity segmentation masks;thresholding persistence"}, "f74ccbc8988b7f0b847c480d4e8bea3082f4f931": {"ta_keywords": "efficient deep reinforcement;deep reinforcement learning;deep reinforcement;search planning rl;adversarial tree search;sample efficient deep;generative adversarial tree;efefficient deep rl;reinforcement learning algorithm;deep rl generative;reinforcement learning;sample efefficient deep;rl generative adversarial;tree search gats;search planning;effective search planning;adversarial tree;caro tree search;tree search mctes;planning rl sampleinefficient;efficient deep;planning rl;tree search;known effective search;deep rl;efefficient deep;rl generative;learning algorithm mont;effective search;reinforcement", "pdf_keywords": ""}, "3f5b7fcb6fc50ba80318ab959f3d63253cd0ef6b": {"ta_keywords": "acoustic event detection;backgroundacoustic event detectingion;anesthetic event detection;acoustic event;backgroundacoustic event;proposes acoustic event;event detectingion classifier;event detection;backgroundacoustic;event detectingion;detectingion classifier chains;detection event;event detection ad;classifier based probabilistic;detectingion classifier;classifier chains;binary detection event;classifier chains new;detection ad classifier;classifier;anesthetic event;ad classifier chains;chains new classifier;classifier chains consists;new classifier based;acoustic;classifier based;proposed anesthetic event;proposes acoustic;new classifier", "pdf_keywords": "acoustic event detection;sound event detection;classification sound event;recurrentacoustic event detection;backgroundacoustic event detection;event detection classifier;classification sound;acoustic scene classification;classifier based probabilistic;event detection;detection classifier chains;events proposed classifier;classifier chain activity;sound event;proposed classifier chains;acoustic event;classifier chains proposed;classifier chains based;proposes acoustic event;propose acoustic event;scene classification sound;detection classifier;detection multiple events;event ad classification;backgroundacoustic event;gated recurrentacoustic event;event detection article;chains proposed classifier;using classifier chains;classifier chain"}, "e6beab7c192d7fb04c8bfb0886464fd719cd3421": {"ta_keywords": "target domain accuracy;machine learning deployments;predicting target domain;method learns threshold;learns threshold;domain accuracy;methods predicting target;source training target;domain accuracy using;predicting target;accuracy using labeled;thresholded confidence;mismatches source training;machine learning;training target;learning deployments characterized;source training;training target test;labeled source data;unlabeled target data;practical method learns;data unlabeled target;thresholded confidence ac;average thresholded confidence;method learns;using labeled source;labeled source;world machine learning;target data methods;source data unlabeled", "pdf_keywords": ""}, "d25c4bf23b4b951f2417e4a8a44574c99608e9d7": {"ta_keywords": "seismic time frequency;linear chirplet transform;time frequency analysis;robust seismic time;time frequency representation;chirplet transform;seismic time;linear chirplet;general linear chirplet;chirplet transform gtc;frequency analysis widely;frequency analysis;frequency representation;time frequency;seismic processing;seismic processing interpretation;form linear chirplet;frequency representation preferably;frequency analysis method;robust seismic;background time frequency;used seismic processing;detect hydrocarbon reservoir;distribution detect hydrocarbon;good time frequency;introduces robust seismic;seismic;widely used seismic;detect hydrocarbon;chirplet", "pdf_keywords": ""}, "4a160efbe80c38cd5eb2f92c7c095b49b113397d": {"ta_keywords": "code generation retrieval;code generation;learning methods code;generated code;retrieval natural language;methods code generation;concept code especially;natural language queries;software development;software development involves;concept code;language queries;apis unfamiliar libraries;turning concept code;language queries primarily;generation retrieval natural;great software development;apis unfamiliar;programming;apis;generation retrieval;natural language;code;based retrieval;especially dealing apis;dealing apis unfamiliar;machine learning methods;major difficulty programming;code especially dealing;purely based retrieval", "pdf_keywords": "code natural language;code generation retrieval;natural language code;natural language software;nl2code generation retrieval;code generation retrievalthe;natural language programming;code retrieval engine;code nl2code generation;generation code retrieval;retrieval natural language;development natural language;nl2code assistance methodsour;succeed code retrieval;code retrieval;language code nl2code;implement natural language;natural language implementation;code nl2code;natural language queries;results code retrieval;code retrieval results;help nl2code ide;natural language search;nl2code generation;snippets natural language;useful code generation;language programming development;provides natural language;code search generate"}, "e7ce1b01d2928514710bba044ac2af758c975d99": {"ta_keywords": "multicommodity selfish routing;selfish routing game;selfish routing;multi commodity routing;commodity routing networks;congestion costs multiplicative;routing game types;routing game;commodity routing;equilibrium quality multicommodity;quality multicommodity selfish;estimates congestion costs;congestion costs;equilibrium quality;routing networks;routing;multicommodity selfish;estimates congestion;routing networks important;type estimates congestion;uncertainty user type;costs multiplicative;study equilibrium quality;equilibrium;congestion;uncertainty user;model uncertainty user;multi commodity;quality multicommodity;objectivewe study equilibrium", "pdf_keywords": "selfish routing game;multicommodity selfish routing;congestion multicommodity selfish;selfish routing;commodity routing game;equilibrium cost uncertainty;congestion costs multiplicative;transportation networks cautious;costs congestion multicommodity;multi commodity routing;cost uncertainty social;cautious estimate congestion;uncertainty social cost;routing game linearly;social cost uncertainty;routing game complex;commodity routing networks;equilibrium congestion;equilibrium social costs;equilibrium congestion urban;commodity routing strategy;routing game significantly;improvement equilibrium congestion;congestion cost cruising;commodity routing;equilibria social cost;twocommodity routing game;assess equilibrium congestion;routing game;estimates congestion costs"}, "caf40157a7a1d72ae3a6946169c992d8c973b743": {"ta_keywords": "recession gingival inflammation;gingival inflammation areas;gingival inflammation;periodontics faced;periodontics;gingival recession gingival;periodontics faced challenge;recession gingival;attached gingiva;periodontium providing therapy;gingival recession;problems present periodontium;gingiva defined;attached gingiva defined;present periodontium;gingiva defined mucogingival;periodontium;case gingival recession;present case gingival;abrasion cervical caries;gingival;periodontium providing;gingiva;present periodontium providing;band attached gingiva;cervical caries;case gingival;cervical caries usually;cervical abrasion;abrasion cervical", "pdf_keywords": ""}, "1b57ffe73ae95f339015c174ec574b59f99ea553": {"ta_keywords": "prediction privacy;preserving prediction privacy;prediction privacy receiving;based perturbation maximization;maximization method discovers;perturbation maximization;perturbation maximization method;features preserving prediction;discovering essential features;gradient based perturbation;machine learning services;features equal discovering;privacy;machine learning;problem gradient based;gradient based;preserving prediction;receiving machine learning;features;necessary target prediction;maximization method;target prediction;features preserving;features necessary target;features fact;learning services cloud;privacy receiving machine;method discovers;privacy receiving;maximization", "pdf_keywords": ""}, "c6048cd0b1368be0e62633ef723f9d691323102c": {"ta_keywords": "model speaker clustering;mixture model speaker;speaker clustering;scale mixture models;mixture models;gaussian mixture model;gm speaker modeling;speaker modeling speech;speaker clustering noisy;modeling speech gams;speaker modeling;scale mixture model;mixture model;modeling speech;gaussian mixture;gibbs sampling based;mixture model methodsthe;mixture models assume;mixture model gm;represented gaussian mixture;multi scale mixture;mixture represented gaussian;speech gams;introductionblocked gibbs sampling;gibbs sampling;clustering noisy;model gm speaker;mixture represented;speech gams represents;component mixture represented", "pdf_keywords": ""}, "40e292d16168fcb8ac87c20682b827ad17a999dd": {"ta_keywords": "app usage learning;usage learning semantic;semantic aware spatio;app usage depresentation;app usage;spatio temporal app;temporal app usage;depresentation graph convolutional;usage learning;semantic aware;aware spatio;aware spatio temporal;usage depresentation graph;learning semantic aware;model app usage;graph convolutional network;temporal app;graph convolutional;app;spatio temporal;learning semantic;depresentation graph;spatio;semantic;model app;convolutional network;convolutional network promising;usage depresentation;aware;depresentation", "pdf_keywords": ""}, "6332d5bb0e6af89471ffc6157e3816c029b3ae83": {"ta_keywords": "durability peff;associated durability peff;durability peff major;parameters associated durability;durability;associated durability;dynamic load cycle;load cycle operating;load cycle;cycle operating parameters;peff;peff major issue;dynamic load;cycle operating;reported dynamic load;operating parameters;peff major;operating parameters associated;dynamic;cycle;load;reported dynamic;operating;recently reported dynamic;parameters;parameters associated;issue;issue world;issue world recently;major issue", "pdf_keywords": ""}, "8b3c0dd95167d4d63161038493a691ee5cdc76b3": {"ta_keywords": "monolingual sentence alignment;monolingual sentence matching;matching text simplification;alignment text simplification;sentence alignment;text simplification objectivethis;sentence alignment text;sentence matching text;sentence matching;improves monolingual sentence;text simplification specifically;simplification specifically text;text simplification;parallel corpora;similarity sentences limitation;similarity sentences;parallel corpora model;model similarity sentences;available parallel corpora;improves monolingual;text standard simplewikipedia;corpora;background monolingual sentence;corpora model;matching text;corpora model trained;alignment text;monolingual sentence;background monolingual;simplewikipedia", "pdf_keywords": "gram convolutional;text simplification task;text simplification specifically;gram convolutional operation;text simplification text;features gram convolutional;text simplification method;simplification specifically text;text simplification operation;text simplification;monolingual sentence alignment;simplification text simplification;alignment text simplification;semantic similarity increase;gram contextual features;gram contextual;similarity sentences limitation;simplification text;sentence alignment;word gram contextual;method text simplification;contextual features gram;application text simplification;simplification method text;improves monolingual sentence;improved word embedding;structural semantic similarity;humanreadable text grammar;increase structural semantic;alignedwikipedia sentence pairs"}, "e4de1009eb7b3524bf7d19bdcebced80035a47cf": {"ta_keywords": "asynchronous neighbor discovery;neighbor discovery protocol;asynchronous neighbor;based asynchronous neighbor;noncoherent neighbor discovery;backgroundasynchronous noncoherent neighbor;neighbor discovery;neighbor discovery obturation;apply neighbor discovery;sparse graph codes;discovery protocol;asynchronous group testing;noncoherent neighbor;discovery protocol internet;novel asynchronous group;asynchronous group;energy based asynchronous;synchronization;sparse graph;neighbor;using sparse graph;scheme apply neighbor;backgroundasynchronous noncoherent;iot solution;backgroundasynchronous;things iot solution;iot solution relax;discovery;asynchronous;protocol internet things", "pdf_keywords": ""}, "dbb159b288930c6be32c2d5b91373ca1e341e633": {"ta_keywords": "enhancement using dictionary;noisy speech feature;feature enhancement;based feature enhancement;noisy speech recognition;dictionary learning proposed;feature enhancement using;dictionary learning;speech feature;speech feature representation;vocabulary noisy speech;using dictionary learning;stereo based feature;speech recognition;alternative noisy speech;sparse weight vectors;scale speech processing;speech processing;speech recognition task;large scale speech;noisy speech;speech processing methods;use sparse;vocabulary noisy;recognition;feature representation;use sparse weight;stereo based;sparse;recognition task based", "pdf_keywords": ""}, "2f3ec666ba50c6a9ce74abad6a5127ea38a05bca": {"ta_keywords": "data replication codes;data replication;replication codes;replication;comparison data replication;replication codes perform;storing data network;data network;data network comparison;storing data;repair failed node;nodes partitioned;means storing data;file downloaded repair;node;nodes partitioned types;nodes;encoded using codes;network comparison data;failed node;failed node paper;framework nodes partitioned;partitioned types encoded;file;storing;precisely framework nodes;downloaded repair;node paper;encoded;framework nodes", "pdf_keywords": "codes distributed storage;regenerating codes distributed;data replication codes;erasure codes efficient;replication codes;node repair data;efficient node repair;regenerating codes;code recovered erasure;recovered erasure decoding;erasure codes;erasure decoding code;powerful regenerating codes;data replication;repair data reconstruction;use erasure codes;codes distributed;termed regenerating codes;replication codes perform;regenerating codes framework;encoding algorithm twin;erasure codes termed;proposed distributed storage;regenerating codes recently;repair twin code;codes termed regenerating;repair data;data reconstruction node;distributed storage;node repair twin"}, "8f2182846d5d4cfbc216b5e4c00411e021dc4776": {"ta_keywords": "learning diagnose lstm;diagnose lstm recurrent;diagnose lstm;lstm;lstm recurrent neural;lstm recurrent;recurrent neural networks;clinical medical data;learning diagnose;intensive care unit;medical data;care unit icu;intensive care;recurrent neural;series observations patient;electronic health record;observations patient visit;introduction learning diagnose;observations patient;health record;medical data especially;results recorded patient;especially intensive care;health record eh;recorded patient electronic;recorded patient;patient visit episode;icu consist;icu;patient electronic health", "pdf_keywords": "lstms classify diagnoses;networks rnns;recurrent neural networks;data recurrent neural;neural networks clinical;networks rnns powerful;learning sequence data;neural networks rnns;multi task neural;rnns;lstms classify;learning clinical data;data recurrent;recurrent neural;using lstms classify;deep learning;deep learning new;missing data recurrent;multilabel classification clinical;models learning sequence;learning sequence;lstms;classify diagnoses;rnns powerful increasingly;training better generalization;neural networks diagnosis;diagnostic classification critically;task neural networks;neural networks treatment;sequence data effectively"}, "bcffee102a99f726ddfe765906babb01b8226269": {"ta_keywords": "targeting mutation rrna;rrna gene targeting;mutation rrna gene;mutation rrna;rrna gene rrna;rrna gene;gene rrna gene;gene rrna;mutation rna gene;mutation rna;rna gene targeting;gene targeting mutation;targeting mutation;targeting mutation critical;rna gene;step targeting mutation;rna;rrna;gene targeting;mutation critical step;mutation;mutation critical;gene;critical step targeting;step targeting;targeting;critical step;step;critical", "pdf_keywords": ""}, "322ef476e90a487c8f9797bece7799b69af9e5c1": {"ta_keywords": "coded matrix multiplication;coded matrix;product coded matrix;matrix multiplication framework;coded computation scheme;dimensional coded matrix;large matrix multiplication;coded computation;coded computation involving;new coded computation;study coded computation;multiplication matrices large;matrix multiplication analysis;matrix multiplication;high dimensional coded;matrix multiplication matrices;multiplication matrices;matrices large propose;redundancy distributed computing;distributed computing;matrices large;multiplication framework;dimensional coded;large matrix;multiplication framework providing;multiplication analysis reveals;coded;distributed computing systems;propose new coded;multiplication analysis", "pdf_keywords": ""}, "822395760906f4940df68aa33925b6bf9123bac2": {"ta_keywords": "data science competitions;data science community;data data science;data science;tool data science;new technologies;new technologies add;new technologies important;development new technologies;competitions popular tool;science competitions;science competitions popular;popular tool data;technologies important tool;arises new technologies;technologies;platforms like codalab;data science problem;data data;data;technologies add;technologies important;important tool data;open source platforms;source platforms like;spur new research;tool data;competitions;platforms like;open source", "pdf_keywords": ""}, "24d28783f6061bd1e91fb60417ac8b3646305a49": {"ta_keywords": "information extraction systems;information extraction;information extraction include;systems information extraction;scale information extraction;classifiers extractors;large scale information;classifiers extractors experience;extraction systems described;extraction systems;different classifiers extractors;information components;containing learned components;information components flexibly;extractors;learned components;architecture large scale;extraction include;extractors experience building;classifiers;context large scale;extraction;systems containing learned;share information components;learned components important;architecture large;extractors experience;extraction include different;systems information;scale systems information", "pdf_keywords": ""}, "c8648d04f52e49167d1a4443a4830709cf3331ff": {"ta_keywords": "computing game theoretic;game theoretic solutions;algorithms assume defender;algorithms computing game;strategy called staackelberg;game theoretic;computing game;defender security;international airport algorithms;assume defender security;airport algorithms;real world security;defender security personnel;mixed strategy;airport algorithms assume;strategy;placement checkpoints;mixed strategy called;algorithms;strategy called;world security;checkpoints;placement checkpoints canine;defender;algorithms assume;assume defender;world security applications;staackelberg model;called staackelberg model;security", "pdf_keywords": "stochastic strategies security;asymmetric strategies security;games polynomial time;allocation games polynomial;strategy optimal twoplayer;complexity twoplayer nash;games strategy optimal;stochastic asymmetric strategies;strategy spaces problem;computing strategy twoplayer;resource allocation games;large strategy spaces;strategy optimal;twoplayer nash equilibrium;polynomial time optimal;strategy twoplayer normal;strategy corresponds allocations;time optimal strategy;optimal strategy;strategy twoplayer;games polynomial;security resource allocation;stochastic strategies;allocation games;optimal twoplayer;computation stochastic strategies;optimal strategy implemented;optimal stochastic asymmetric;strategy spaces;asymmetric strategies"}, "651468a69da74dab716cebbd179a5cbb8e672c14": {"ta_keywords": "imitation learning demonstrations;learning demonstrations lfd;exploration learning demonstrations;learning demonstrations;learning demonstrations challenging;demonstrations rarely optimal;self imitation learning;imitation learning;sophisticated exploration learning;exploration learning;demonstrations challenging task;demonstrations lfd;agent exploration states;agent exploration;guiding agent exploration;self imitation;control influence demonstrations;quality demonstrations rarely;sophisticated exploration;demonstrations rarely;requires sophisticated exploration;demonstrations challenging;demonstrations lfd remedies;background self imitation;quality demonstrations;imitation;demonstrations;influence demonstrations;exploration;exploration states experienced", "pdf_keywords": "imitation learning demonstrations;learning demonstrations;exploration learning demonstrations;learning demonstrations silfd;self imitation learning;learning demonstrations field;imitation learning formulate;imitation learning;reinforcement learning demonstrations;learning demonstrations lfd;requireself imitation learning;imitation learning sil;demonstrations self imitation;learn demonstrations fundamental;learn demonstrations;demonstrations agent improves;user learn demonstrations;demonstrations rarely optimal;demonstrations sparse environments;exploration learning;learn experimental demonstrations;sophisticated exploration learning;algorithms requireself imitation;integrating demonstrations sparse;effectiveness demonstrations sparse;demonstrations field neural;demonstrations algorithms;deep reinforcement;method deep reinforcement;expert demonstrations alleviating"}, "1cb7015c0a8015c65844876459809ecac917ec02": {"ta_keywords": "multi channel acoustic;speech recognition single;multi channel trained;trained multi channel;channel acoustic model;input multi channel;talker speech recognition;channel trained multi;multi channel input;multi talker speech;single channel trained;single multi channel;acoustic model single;auditory modeling distant;distant multi talker;multi talker;channel acoustic;introduction auditory modeling;auditory modeling;multi channel;speech recognition;heterogeneous input multi;channel input branches;single channel;multi channel branches;acoustic model;channel input;channel trained;recognition single multi;trained multi", "pdf_keywords": ""}, "899055ad2f0863cf1931c41f04da8b1dd7382607": {"ta_keywords": "a1c lipid variability;visit variability lipid;lipid variability;variability lipid profiles;variability lipid;hemoglobin a1c lipid;lipid profiles patients;a1c lipid;lipid variability methodswe;lipid profiles;backgroundhemoglobin a1c levels;relationship hemoglobin a1c;hemoglobin a1c;a1c levels associated;coronary intervention;percutaneous coronary intervention;backgroundhemoglobin a1c;coronary intervention pci;elective percutaneous coronary;a1c levels;lipid;percutaneous coronary;visit variability;conducted retrospective cohort;retrospective cohort study;visit visit variability;retrospective cohort;cohort study;explore relationship hemoglobin;cohort study explore", "pdf_keywords": ""}, "173c73077a421680f12576524e85dff4b890c17e": {"ta_keywords": "wasserstein distance sample;projected wasserstein distance;wasserstein distance methodswe;kernelel projected wasserstein;kernel projected wasserstein;wasserstein distance;distance projected distributions;test kernelel projected;projected wasserstein;sample test kernelel;works projected wasserstein;projected distributions contrast;projected distributions;test kernelel;distance sample test;wasserstein;kernelel projected;distance sample;kernel projected;develop kernel projected;distance projected;distributions contrast existing;distributions contrast;samples determine distribution;introductiontwo sample test;maximizes distance projected;distance methodswe;kernelel;statistics machine learning;distributions", "pdf_keywords": "dimensional sample tests;wasserstein distance sample;kernel test goodness;distance projected distributions;projected wasserstein distance;kernel projected wasserstein;results kernel test;distance wasserstein;kernel test;tests use projection;weighted distance wasserstein;distance wasserstein distance;weighted wasserstein distance;nonparametric tests;wasserstein distance investigated;distance functions empirical;wasserstein distance;high dimension testing;distance sample test;present kernel test;wasserstein distance fundamental;high dimensional sample;original wasserstein distance;reproducing kernel hilbert;distributions samples;projected samples;distance task sample;training testing datasets;projected distributions;distributions samples samples"}, "4ae15dbb068cc962b39dca07d87b22fe5dcd5f6a": {"ta_keywords": "grid operations privacy;smart grid operations;smart grid;tradeoff smart grid;operations privacy consumers;privacy consumers;operations privacy;smart meters;privacy consumers methodswe;smart meters come;grid operations data;privacy;grid installation smart;electrical grid;data pose privacy;installation smart meters;pose privacy threat;modernization electrical grid;privacy threat paper;thermostatically controlled loads;privacy threat;electrical grid installation;advantages control monitoring;pose privacy;direct load control;control monitoring;operations data collected;grid operations;using thermostatically controlled;using thermostatically", "pdf_keywords": "utility privacy tradeoff;energy consumption privacy;utility privacy;grid operations privacy;balance utility privacy;controller quantify privacy;electricity consumers privacy;aggregation privacy control;metric inferential privacy;privacy metric inferential;inferential privacy;privacy control fully;present framework privacy;inferential privacy privacy;privacy control;introduce privacy metric;privacy metric assumes;aggregation privacy;new privacy metric;framework privacy inferentiality;privacy inferentiality;consumers privacy metric;privacy tradeoff;privacy metric computing;quantify privacy;privacy inferentiality using;privacy metric;privacy tradeoff use;framework privacy;consumption privacy"}, "513552a56668279d6cd0857a4399fe8a63d92145": {"ta_keywords": "reversible etiology disease;etiology disease understood;disease characterized reversible;etiology disease;characterized reversible etiology;reversible etiology;new model disease;model disease characterized;model disease;disease;disease characterized;etiology;disease understood development;disease understood;characterized reversible;reversible;model;development new model;characterized;understood development;new model;development;understood development new;development new;new;understood", "pdf_keywords": ""}, "8113f05360c6483e52b3e261fc9efce671e0aaa6": {"ta_keywords": "separation speaker diarization;speech separationion diarization;speech separation speaker;integration speech separationion;speech separation;speaker diarization automatic;speech separationion;dealing speech separation;meeting transcription automatic;separation speaker;speaker speech recognition;speaker diarization;diarization automatic speech;multi speaker meetings;recognition multi speaker;speech recognition unsegmented;separationion diarization recognition;speaker meetings;speaker meetings challenge;meeting transcription;speech recognition ar;automatic speech;automatic speech recognition;methodsmulti speaker speech;speech recognition;transcription automatic subtitle;recognition unsegmented recordings;unsegmented recordings diverse;separationion diarization;unsegmented recordings", "pdf_keywords": "separation diarization audio;separation recognition audio;separation speaker diarization;speech separation;speech separation speaker;dealing speech separation;unsegmented recordings diverse;separated audio;methods separated audio;recording best separation;recognition unsegmented recordings;speech recognition unsegmented;unsegmented recordings;speaker diarization automatic;diarization automatic speech;separation diarization recognition;speaker recognition;beamforming speech recognition;automatic speech;diarization audio maskbased;diarization audio;speaker speech recognition;separation diarization auditory;speaker recognition challenge;transcription automatic subtitle;separated audio cpwer;meeting transcription automatic;speech speech processing;separation diarization pipeline;recognition audio"}, "29437d98b9e6f45bef7029f3ce1237b8b284464f": {"ta_keywords": "text generation style;text generation;text generation focused;data text generation;generation style mitation;improving content fidelity;control writing styles;content fidelity lacking;content fidelity;generation style;quality templates;high quality templates;style mitation;quality templates difficult;writing styles;text manual automatic;writing styles word;content fidelity does;text;approaches data text;neural approaches data;harm content fidelity;control writing;templates;neural approaches;text manual;styles word;generation focused improving;sentence structures traditional;templates difficult", "pdf_keywords": ""}, "d63edef60d674408819bb015b64b7f42470e151b": {"ta_keywords": "molecular generative model;design molecular generative;novo molecule design;moecule design molecular;design molecular;molecule design molecular;molecular generative;molecule design;protein binding pocket;novo molecule;sites novo molecule;binding sites novo;protein binding;novel generative model;3d information protein;moecule design;generative model;model conditioned 3d;protein binding sites;generative model gaining;structural information protein;generative model proposed;information protein binding;generative model conditioned;molecular;rn crnn model;drug like molecules;introductiondenovo moecule design;molecule;model control generation", "pdf_keywords": "model protein binding;generate protein binding;generative model protein;molecular generative models;protein binding pocket;molecular generative model;design molecular generative;model composition protein;model protein;generating models binding;molecule generating models;protein binding;generate structures binding;targeting protein binding;models binding pocket;composition protein binding;binding pocket model;model molecule based;binding affinity molecules;protein binding sites;generative model generate;models binding;generative models generate;molecular generative;developed molecular generative;thirdprotein binding pocket;novo molecule design;model molecule;controlled molecular generative;model generate structures"}, "1ddf9d306ae27113f55ea3d4eee12c8441235656": {"ta_keywords": "workshop asian translation;asian translation wat2016;translation wat2016 including;3d workshop asian;translation wat2016 includes;patent translation subtasks;paper translation subtasks;asian translation;translation wat2016;translation subtasks jk;jk patent translation;patent translation;paper translation;workshop asian;translation subtasks newswire;scientific paper translation;translation subtasks;translation;overview 3d workshop;tasks 3d workshop;3d workshop;wat2016 including ja;tasks 3d;workshop;wat2016 includes detailed;overview 3d;subtasks jk patent;including ja scientific;ja scientific paper;shared tasks 3d", "pdf_keywords": ""}, "b03feec6f5b898484fdfdc3cd12f084afbe77036": {"ta_keywords": "feature based ranking;based ranking functions;noisy relevance judgment;ranking functions based;ranking functions;ranking performance;labeled document pairs;ranking;pairwise preference framework;ranking performance methods;based ranking;based pairwise preference;noisy relevance;overall ranking performance;pairwise preference;overall ranking;learning feature based;relevance judgment single;document pairs;process noisy relevance;algorithms learning feature;deteriorate overall ranking;document pairs jeopardize;learning feature;preference framework;relevance judgment;labeled document;document pairs used;mis labeled document;feature based", "pdf_keywords": ""}, "f388c2be45e4415fcb59cf43a3b29463cf7e7940": {"ta_keywords": "journalists verifying claims;fact checked journalists;fact checking assessment;journalists verifying;fact checking;manually journalists verifying;statements fact checked;truthfulness claim task;claims public figures;checked journalists;verifying claims public;using statements fact;assessment truthfulness claim;task fact checking;assess truthfulness;checking assessment truthfulness;performed manually journalists;need assess truthfulness;statements fact;truthfulness claim;verifying claims;manually journalists;checked journalists available;assess truthfulness increasing;claims public;fact checked;journalists;assessment truthfulness;journalists available online;journalists available", "pdf_keywords": ""}, "9352dfd127dcfce8013eb350e0229cc72b9bd203": {"ta_keywords": "improved super resolution;super resolution methods;super resolution;attention augmented wan;super resolution model;current super resolution;backgroundpanchromatic images;backgroundpanchromatic images contain;backgroundpanchromatic;resolution model;resolution sensor;earth observation;useful earth observation;resolution methods based;resolution;reconstruct edge details;resolution methods;edge details images;resolution model involves;suffer low resolution;resolution sensor limitation;self attention augmented;low resolution sensor;attention augmented;low resolution;large scale view;earth observation suffer;abundant spatial information;images contain abundant;augmented wan", "pdf_keywords": ""}, "4104d632d1cff0c9314cde344e2b1da06e662c5b": {"ta_keywords": "sequence automatic speech;automatic speech;semi supervised training;automatic speech recognition;speech recognition ar;unsupervised semi supervised;speech recognition;sequenceroto sequence automatic;leverage unpaired speech;semi supervised;unpaired speech;improvements semi supervised;sequenceroto;introduction sequenceroto;sequence automatic;unsupervised semi;sequenceroto sequence;supervised training models;supervised training;training models;introduction sequenceroto sequence;training using cycle;supervised training using;supervised;speech;training models work;recognition ar models;recognition ar;unsupervised;training using", "pdf_keywords": ""}, "7e43dad7fbae3a7db47adc6b89c76acbd2fb225f": {"ta_keywords": "hierarchical knowledge;hierarchical knowledge base;domain hierarchical knowledge;hierarchical knowledge critical;procedures inherently hierarchical;knowledge base;hierarchical;hierarchical make videos;domain hierarchical;inherently hierarchical;open domain hierarchical;budget hierarchical knowledge;knowledge critical reasoning;reasoning complex procedures;modeling parent;modeling parent child;child relation methods;knowledge;inherently hierarchical make;knowledge base kb;structures modeling parent;knowledge critical;hierarchical make;parent child relation;procedures inherently;make videos;complex procedures existing;videos;make videos need;procedures existing", "pdf_keywords": "hierarchical knowledge base;domain hierarchical knowledge;construct hierarchical knowledge;hierarchical knowledge critical;hierarchical knowledge;propose hierarchical knowledge;knowledge base procedures;task demonstrate hierarchical;retrieval video oriented;encode hierarchies goals;automatic human evaluation;goals wikihow method;relevance goal video;knowledge base;linkable step goal;steps linked goals;generating hierarchies goals;demonstrate hierarchical hierarchical;highly query video;using deeper hierarchies;task relevance goal;video important retrieval;important retrieval video;procedures based wikihow;task linking steps;video oriented tasks;hierarchical make videos;query video retrieval;video highly query;effectively construct hierarchical"}, "15a6c3d32ae1daefba3c4b40146de8efdf16ec8d": {"ta_keywords": "patient history;patient history history;occurrence new disease;case patient history;disease;disease world;new disease world;mortality world case;world case patient;new disease;major cause morbidity;cause morbidity;disease world major;mortality world;mortality;history history;morbidity mortality world;cause morbidity mortality;history;case patient;history history history;morbidity mortality;morbidity;patient;occurrence;world case;occurrence new;world major cause;major cause;world", "pdf_keywords": ""}, "950c2c041db52c416e49fb0945078f6463c501b8": {"ta_keywords": "utility learning energy;energy disaggregation utility;learning energy disaggregation;utility learning;incentives estimating consumer;estimating consumer utility;energy disaggregation;utility function incentives;consumer utility function;consumer utility;consumers revenue decoupling;learning energy;design utility learning;energy consumption patterns;modifying energy consumption;designing incentives estimating;revenue decoupling demand;decoupling demand;disaggregation utility company;utility company consumer;designing incentives;incentives estimating;consumption patterns consumers;algorithm designing incentives;incentives designed;energy consumption;decoupling demand response;incentives designed using;demand response programs;patterns consumers revenue", "pdf_keywords": "utility learning energy;utility learning;incentives estimating consumer;incentives consumers estimating;designing incentives utility;consumers estimating utility;learning energy disaggregation;design utility learning;consumer aggregated power;utility company incentive;estimate consumer utility;estimating utility functions;incentives utility;consumer utility function;aggregated power consumption;incentives consumers;designing incentives estimating;estimating utility;incentives utility companies;design incentives consumers;designs incentive based;knows power consumption;aggregate power consumption;learning energy;incentive induces consumer;incentives estimating;incentive design;incentive compatible demand;power consumption signals;incentive based"}, "f637d061704579531a8b8e03ef6e8331ba117490": {"ta_keywords": "aggregating pairwise comparison;inline formula tex;tex math inline;probabilities future comparisons;stochastic transitivity;pairwise comparison data;comparisons working flexible;pairwise comparison;inline formula items;strong stochastic transitivity;methods aggregating pairwise;formula tex;math inline;inline formula;collection inline formula;math inline formula;future comparisons;comparison data;tex math;latex tex math;latex tex;comparison data collection;aggregating pairwise;comparisons;methods aggregating;tex;latex;formula items;study methods aggregating;tex math notation", "pdf_keywords": ""}, "7626f73c3b013b5b7bf293c1cc22d2835b6579b3": {"ta_keywords": "speech synthesis fundamental;synthesis speech fundamental;step speech synthesis;speech synthesis;speech synthesis speech;speech fundamental step;synthesis speech;speech synthesis addition;synthesis addition speech;fundamental step speech;addition speech synthesis;speech fundamental;synthesis fundamental step;synthesis fundamental;step speech;synthesis;fundamental step;synthesis addition;speech;addition speech;fundamental;step;addition", "pdf_keywords": ""}, "bf0b66e0e328df1df42b075422c8fecdd95736c0": {"ta_keywords": "sistuddent teachable peer;learning agent sistuddent;teachable peer learner;learning teaching sistuddent;agent sistuddent teachable;sistuddent teachable;teachable peer;teaching sistuddent;learning agent;peer learner allows;cognitive tutor;peer learner;cognitive tutor methodsthis;teaching sistuddent initial;learn teaching sistuddent;comparativing cognitive tutor;sistuddent initial classroom;teaching sistuddent integrated;learning teaching;learning environment;tutor;learner allows student;introduction learning teaching;game like learning;machine learning agent;allows student learn;tutor methodsthis;learner allows;like learning environment;teachable", "pdf_keywords": ""}, "4ffca5d623950e2396089e7fc1621b4a477436cb": {"ta_keywords": "text generation;text generation focused;data text generation;improving content fidelity;content fidelity lacking;quality templates;control writing styles;content fidelity;quality templates difficult;high quality templates;styles sentence structures;text manual automatic;neural approaches data;approaches data text;text;neural approaches;content fidelity does;determine realization text;writing styles;harm content fidelity;sentence structures word;control writing;writing styles sentence;recent neural approaches;improving content;sentence structures;data text;templates;text manual;realization text", "pdf_keywords": ""}, "97e033d79b6aebab1927ab9232afa8268e198481": {"ta_keywords": "distributed caching;based distributed caching;distributed caching architected;cache content placement;content placement cache;cache content;caching;placement cache;vod based distributed;placement cache users;cache;video demand vod;video demand;scale video demand;caching architected;distributed algorithm;cache users;caching architected built;vod based;fully distributed algorithm;demand vod based;problem cache content;codes optimization;distributed;network codes;large scale video;codes optimization design;based distributed;distributed algorithm jointly;demand vod", "pdf_keywords": ""}, "f5ca46585818771e64ee9449c930748fbee35cba": {"ta_keywords": "language feedback explanations;structured explanations;interactive refines explanations;natural language feedback;form structured explanations;feedback explanations class;structured explanations happens;feedback explanations;models reasoning tasks;interactively correct explanation;explanations class explainable;explanations class;refines explanations;modeling models reasoning;language feedback;correct explanation structures;language feedback methodswe;explanation structures;explanation structures natural;explanations happens supporting;reasoning tasks;models reasoning;reasoning tasks support;getting human feedback;refines explanations given;natural language;performance natural language;structures natural language;explanations given reasoning;explainable nonli modeling", "pdf_keywords": "inference queriesdefeasible reasoning;generate defeasible reasoning;interactive refines explanations;natural language feedback;defeasible inference queriesdefeasible;improves explanation structure;reasoning useful tool;language feedback explanations;feedback natural language;queriesdefeasible reasoning;propose inference graph;inference queriesdefeasible;inference graph provides;inference graphs;defeasible reasoning present;graphs defeasible reasoning;queriesdefeasible reasoning useful;graphs defeasible inference;defeasible inference;inference graphs defeasible;defeasible reasoning develop;feedback explanations introduce;generating inference graphs;inference graph;refines explanations;inference mode reasoning;defeasible reasoning;automate human feedback;inference graphs nodes;feedback explanations"}, "97db55b196cf0c768644a392a7e6c79d1c65207e": {"ta_keywords": "online speech enhancement;backgroundframe online speech;speech enhancement systems;speech enhancement;latency allows enhancement;frame online systems;latency allows;algorithmic latency allows;transform stft;stft domain usually;stft;stft domain;backgroundframe online;frame online;current frame online;latency equal window;fourier transform stft;transform stft domain;enhancement systems short;online speech;algorithmic latency;contextual information length;latency;usually algorithmic latency;short time fourier;itf algorithmic latency;backgroundframe;allows enhancement models;enhancement systems;algorithmic latency equal", "pdf_keywords": "overlapped frame prediction;speech overlapped frame;speech enhancement overlapped;online speech enhancement;useful speech enhancement;speech enhancement task;architecture speech enhancement;speech enhancement context;speech enhancement;speech enhancement systems;predicted signal frame;backgroundframe online speech;single frame prediction;speech enhancement speaker;monaural speech enhancement;speech enhancers;accuracy speech separation;frame prediction technique;frame prediction partial;frame prediction;frame online speech;frame prediction useful;algorithm overlapped frame;target speech overlapped;speech overlapped;enhancement overlapped frame;frame prediction use;frame prediction observe;frame prediction article;enhancement speaker separation"}, "45ce9fce4a4eea9f72688885182aee0c84786fab": {"ta_keywords": "translate musical domains;domain wavenet autoencoder;musical domains;method translating music;translating music musical;wavenet autoencoder;musical domains seen;translating music;wavenet autoencoder shared;musical instruments genres;instruments genres;allows translate musical;music musical instruments;translate musical;domain wavenet;instruments genres styles;musical instruments;domain independent encoder;autoencoder;multi domain wavenet;instruments;autoencoder shared encoder;autoencoder shared;encoder allows translate;wavenet;music;genres;music musical;independent encoder allows;encoder allows", "pdf_keywords": "trainingin audio translation;translate musical domains;music domains train;audio translation;domain wavenet autoencoder;musical domains train;generate audio domain;audio context domain;audio translation translations;trainingin audio;wavenet autoencoder;method translating music;processing audio music;processing audio context;audio context music;audio domain;translation music;music apply decoder;translating music;wavenet autoencoder shared;ability translate audio;audio domain ability;domain independent encoder;translating music musical;translate audio;musical domains;translate audio audio;audio music;music domains;context music"}, "782a50a48ba5d32839631254285d989bfadfd193": {"ta_keywords": "entity representations human;entity representations;creating entity representations;processing entities text;language processing entities;entities text;entities text typically;natural language processing;entity related tasks;embeddings produced way;models embeddings produced;entities;embeddings produced;processing entities;trained models embeddings;entity;embeddings;models embeddings;representations human readable;entity related;creating entity;approach creating entity;natural language;high performance entity;representations human;spaces pre trained;pre trained models;language processing;methodology natural language;text typically embedded", "pdf_keywords": "train entity typing;entity representations human;entity typing datasets;entity representations;creating entity representations;entity typing models;interpretable entity representations;entity typing;labeled entity typing;approach entity typing;language processing entities;typing models contextual;processing entities text;entities user friendly;contextual embeddings;models contextual embeddings;processing natural language;contextual embeddings use;natural language processing;train entities;interpretable na embedding;entities text;train entity;typing models;fromwikipedia train entities;entities text typically;natural language technologies;creating interpretable entity;natural language learning;trained models embeddings"}, "d26683135c70d7b2a61ce5f70fb49b4fa22cf9c4": {"ta_keywords": "mallows models pairwise;models pairwise preference;sampling learning mallows;learning mallows models;pairwise preference data;conditional mallows models;sampling arbitrary ranking;pairwise preference;mallows models mixtures;models pairwise;models particular sampling;introductioneffective sampling learning;sampling learning;mallows models;mallows models particular;introductioneffective sampling;repeated insertion model;algorithms learning mallows;preference data;learning mallows;sampling arbitrary;sampling;insertion model;pairwise comparison data;generalized repeated insertion;allows sampling arbitrary;arbitrary ranking distributions;conditional mallows;preference data methodswe;arbitrary ranking", "pdf_keywords": "ranking preference distributions;pairwise preferences learning;preference distributions observed;preference distributions;preference distributions critical;learning ranking preference;preferences empirically provides;pairwise preference data;preference profiles evidence;preference data training;preferences empirically;preferences learning effective;partial preferences evidence;partitioned preferences empirically;preferences learning;preferences evidence;larger preference data;revealed preference methods;learning population preferences;preference data derive;preference data;preference methods extend;predicting missing preferences;arbitrary pairwise preferences;backgroundlearning preference distributions;pairwise preference;sampling preferences mallows;sampling preferences;ranking preference;models sampling rankings"}, "205d67dfe0112df846bc4b221fa2665b0434d441": {"ta_keywords": "etiology malignant disease;etiology malignant;disease malignant;malignant disease;disease malignant disease;malignant disease malignant;disease patient malignant;malignant disease patient;patient malignant disease;history malignant disease;patient malignant;malignant;malignant disease poorly;history malignant;woman history malignant;disease patient;disease;disease poorly understood;etiology;disease patient treated;case young woman;disease poorly;patient treated;patient treated combination;report case young;young woman;report case;patient;woman;understood report case", "pdf_keywords": ""}, "7df95dceaba3f4fb45e2b9de29caf7fbce20e25c": {"ta_keywords": "results comparative analysis;comparative analysis literature;comparative analysis;method analyze data;analyze data;analysis literature;results comparative;present results comparative;analysis literature use;analysis;comparative;method analyze;new method analyze;data;analyze;article present results;literature use;present results;results;aim article;aim article present;literature use new;article;literature;method;article present;new method;use;aim;use new method", "pdf_keywords": ""}, "83a2582b94aeaaa97b2f52af8d827d28dc4690bf": {"ta_keywords": "stauttering speech;fluency speech;person stauttering speech;communicating person stauttering;stauttering speech disorder;fluency speech individuals;normal fluency speech;stauttering;speech disorder;person stauttering;speech individuals disorder;speech disorder affects;speech;speech delivering information;effectiveness speech;effectiveness speech delivering;speech delivering;speech individuals;fluency;measures effectiveness speech;speak fluently silent;means communication;communication;disorder difficult speak;communicating;fluently silent pauses;silent pauses;silent pauses repetitions;communicating person;speak", "pdf_keywords": ""}, "930445d9cda71d6ff857e69aa5bb4b1bef7d31e5": {"ta_keywords": "reinforcement learning drifting;discounted reinforcement learning;emphdynamic regret bounds;backgroundreforcement learning drift;learning drifting;learning drifting non;bounds discounted reinforcement;learning drift challenging;learning drift;discounted reinforcement;reinforcement learning;regret bounds discounted;regret bounds;drift challenging task;confidence bound rein;backgroundreforcement learning;drift challenging;emphdynamic regret;stationarity reward functions;drift;reinforcement;bounds discounted;non stationarity reward;drifting;reward functions state;drifting non stationarity;stationarity reward;state transition distributions;drifting non;contributions tuned sliding", "pdf_keywords": "discounted reinforcement learning;dynamic regret policy;dynamic regret algorithm;learning dynamic regret;dynamic regret asymmetric;dynamic regret bounds;discounted reinforcement;bandit reinforcement learning;regret policy algorithm;policy dynamic regret;consider discounted reinforcement;bandit reinforcement;modeling sequential decision;sequential decision making;algorithm dynamic regret;novel bandit reinforcement;analysis dynamic regret;dynamic regret round;dynamic regret bound;dynamic regret;challenge dynamic regret;low dynamic regret;dynamic regret widening;dynamic regret non;regret asymmetric algorithm;sequential decision;proof dynamic regret;unfavorable dynamic regret;learning markov decision;temporal drifts reward"}, "d3231772937a2182b2377d028417245c49868dd1": {"ta_keywords": "neural machine translation;neural sequence models;machine translation nm;machine translation;model errors neural;errors neural machine;translation nm methodswe;model entirewmt15 english;neural sequence;procedure neural sequence;translation nm;errors neural;neural machine;search errors model;inference procedure neural;sequence models;procedure neural;entirewmt15 english;beam search fails;entirewmt15 english german;sequence models based;transformer base model;neural;base model entirewmt15;beam search;model scores transformer;translation;search depth search;surprily beam search;search depth", "pdf_keywords": "neural machine translation;translation systems neural;machine translation nm;neural sequence models;decoding neural models;machine translation systems;translation investigations length;machine translation;translation systems;decoding neural;search length normalization;non neural mrna;model prefers translation;decoding strategy decoder;decoder efficiently;neural mrna nmr;driven decoding strategy;method decoding neural;strategy decoder efficiently;neural mrna;decoding algorithms;exact search length;nm problem translations;translation systems important;driven decoding;translation nm linked;neural sequence;decoding algorithms model;decoding;decoder efficiently reduce"}, "6b2b5d3d9a2ca4bc4fbd81551a62370be2fbff1b": {"ta_keywords": "scaling laws resultswe;scaling laws;connects scaling laws;power law scaling;law scaling;scaling test;limited scaling behavior;scaling behavior;limited scaling;scaling test error;test loss trained;awe scaling test;total scaling regimes;variance limited scaling;scaling fundamental;scaling;scaling follows simply;limited scaling follows;scaling regimes;parameters scaling fundamental;law scaling relations;parameters scaling;size training dataset;scaling regimes variance;scaling behavior dataset;trained neural networks;size total scaling;awe scaling;scaling follows;neural networks follows", "pdf_keywords": "losseswe neural scaling;scaling test loss;neural scaling;neural scaling laws;laws neural scaling;scaling laws neural;dataset scaling mild;scaling learning;demonstrate scaling loss;test loss trained;model scaling losses;wellwe scaling test;prediction scaling laws;suggest neural scaling;scaling loss;regularization strength trained;deep models standard;dataset scaling effect;scale learning;scaling losses resolution;dataset scaling;depth dataset scaling;trained mass convergence;experimental results deep;scaling learning rate;models scaling test;losseswe neural;scale learning dynamics;scaling regions neural;scaling losses"}, "4e1b16fd719354b0a9e92075be66c85d4b95082c": {"ta_keywords": "circuitry neural circuits;circuits neural circuitry;neural circuitry neural;neural circuitry;neural circuits neural;circuitry neural;circuits neural;neural circuits;neural circuits complex;component neural circuits;shown neural circuitry;neural circuitry key;circuits complex;neural;circuits;circuitry;component neural;circuitry key component;key component neural;circuits complex process;circuitry key;shown neural;recently shown neural;key component;component;complex process;complex;complex process difficult;key;process", "pdf_keywords": ""}, "713844009469478141671c53a3b73cd12caf9df0": {"ta_keywords": "diagnosis pulmonary infection;pulmonary infection;patients diagnosis pulmonary;diagnosis pulmonary;pulmonary;patients diagnosis;infection;management patients diagnosis;diagnosis;patients;management patients;approach management patients;approach management;new approach management;article discuss importance;importance new approach;management;new approach;article discuss;article;approach;aim article discuss;discuss importance new;aim article;discuss importance;new;importance new;importance;discuss;aim", "pdf_keywords": ""}, "810420af4fa5f3ed932724aea5f7b66d3bd592b2": {"ta_keywords": "links known documents;learning query web;documents collect links;collect links known;learning query;topic keeping resource;resource directories documents;online documents methodswe;collect links;documents methodswe propose;introduction learning query;world wide web;known documents specific;www filled resource;directories documents;query web;query web world;documents methodswe;web world wide;directories documents collect;machine learning methods;machine learning;links known;learning methods address;learning methods;web world;wide web www;resource directories;using machine learning;online documents", "pdf_keywords": ""}, "e5e74d312679eae8f2a2943e16f2efebcb5cc50f": {"ta_keywords": "area wind wind;area wind;changes area wind;wind wind wind;wind speed changes;wind speed;wind wind;wind;speed changes area;changes area;speed changes;area;speed;changes", "pdf_keywords": ""}, "e35357ac461a669fe7e4b877ee1fad0dfda26303": {"ta_keywords": "style transfer languages;style transfer task;style transfer using;style transfer;inference style extraction;style extraction;transfer languages style;style labelled corpus;setting style transfer;sentences inference style;shot style transfer;style extraction methods;sentence target style;background style transfer;inference style;sentences inference;style labelled;style labelled corpor;transfer languages;corpus recent work;310 sentences inference;style approximately preserving;corpus;languages style labelled;shot style;languages style;access large style;large style labelled;labelled corpus;rewriting sentence target", "pdf_keywords": "multilingual style transfer;translation style transfer;multilingual style transferwe;style controlled translation;style transfer development;shot style transfer;style transfer;style transfer training;language transfer translation;useful translation style;style transfer using;style transfer fundamental;controlling style transfer;style transfer new;multilingual language transfer;paraphrasing leads stylistic;translation style;concept style transfer;style transfer systems;style transfer able;style transfer use;language transfer approach;style transfer low;style transfer seven;style transfer details;formality style transfer;multilingual style;style examples inference;learning effective style;transfer resulting style"}, "e97c5b206c1f308b821917bc2f584b5f1faad547": {"ta_keywords": "miscalibrations popular approach;people miscalibrations typically;poorly people miscalibrations;miscalibrations typically;miscalibrations popular;people miscalibrations;miscalibrations typically far;simplistic models miscalibration;miscalibrations;miscalibration linear biases;suffer miscalibrations;biases bias scores;models miscalibration;miscalibration;bias scores;suffer miscalibrations popular;bias scores approach;known suffer miscalibrations;models miscalibration linear;biases bias;cardinal scores numeric;biases;cardinal scores;miscalibration linear;scores numeric ratings;bias;background cardinal scores;numeric ratings collected;scores approach fares;numeric ratings", "pdf_keywords": "cardinal ranking estimator;estimator cardinal scores;estimators rely ranking;estimator ranking scores;estimator ranking;ranking estimator;best estimator ranking;estimator superiority cardinal;cardinal estimator rank;information cardinal scores;cardinal ranking;based cardinal ranking;ranking design estimators;rankings estimator;ordinal rankings estimator;scores induced ranking;rankings estimator allows;cardinal scores induced;cardinal scores;ranking arbitrary;issues cardinal scores;success cardinal estimator;cardinal scores necessarily;induced ranking design;ranking scores;tradeoff cardinal scores;induced ranking;cardinal scores argue;superiority cardinal data;ranking arbitrary based"}, "2d3fcbaf28e650471b942f221c5fa3c178b1b72a": {"ta_keywords": "accelerated directional search;convex unconstraint optimization;directional search method;unconstraint optimization;propose accelerated directional;directional search;convex unconstraint;accelerated directional;unconstraint optimization problem;optimization problem mathbbrn;methods consider convex;optimization;accelerated nesterov;consider convex unconstraint;standard accelerated nesterov;prox structure methods;advance norm solution;paper propose accelerated;search method;optimization problem;convex;unconstraint;propose accelerated;search method non;method non euclidian;euclidian prox structure;structure methods;consider convex;prox structure;advance norm", "pdf_keywords": ""}, "b345057638e60eee581fea6c7110a98e3b9ebe61": {"ta_keywords": "emerging topics text;topics text streams;detecting emerging events;emerging events text;emerging topics;topic modeling;streams using topic;topics text;topic modeling methods;using topic modeling;locally emerging events;events text streams;emerging events;detecting emerging;emerging events massive;proposed detecting emerging;streams paper semantic;massive text streams;topics;events massive text;text streams paper;text streams highly;text streams;text streams using;events text;using topic;health interventions discovering;paper semantic;locally emerging;emerging", "pdf_keywords": "topics text streams;detect emerging topic;topic modeling important;novel topic modeling;topic model analyze;topic modeling;topic modeling online;contrastive topic modeling;emerging topics text;captures emerging topic;localized emerging topics;traditional topic modeling;use topic modeling;contrastive topic model;streams semantic scan;topic modeling methods;topic model;topic modeling approach;topic assignment spatial;present semantic scan;topic model designed;semantic scan detecting;emerging topic results;propose semantic scan;foreground topics;semantic scan;topics text;semantic scan novel;spatial event detection;semantic scan approach"}, "2038086c604f1f8841d086cd5cc6052e546ffc24": {"ta_keywords": "srrna mediated asrrna;asrrna mediated asrrna;mediated asrrna mediated;srrna mediated;asrrna mediated;case srrna mediated;mediated asrrna;asrrna mediated asr;srrna;case srrna;report case srrna;asrrna;mediated asr;asr;mediated;report case;report;case", "pdf_keywords": ""}, "9850d2b41c6c5be039649d6422306121b760169d": {"ta_keywords": "updating data distributed;distributed storage systems;data distributed storage;distributed storage;distributed computer systems;nodes need update;storage systems nodes;update protocols;distributed computer;codes update protocols;data distributed;truly distributed computer;update protocols stale;protocols stale node;online nodes need;online nodes;downloading data updated;nodes intermittently offline;protocols stale;node updated downloading;stale node updated;systems nodes intermittently;distributed;stale node;storage systems;updated downloading data;systems nodes;node updated;updating data;protocols", "pdf_keywords": ""}, "71bcdfe5b6be3a0d08ce4bde45acdfd0f738e2f7": {"ta_keywords": "inverse reinforcement learning;based inverse reinforcement;inverse reinforcement;problem inverse reinforcement;risk sensitivity reinforcement;sensitivity reinforcement learning;reinforcement learning markov;learning markov decision;reinforcement learning algorithm;reinforcement learning;reinforcement learning framework;gradient based inverse;markov decision processes;sensitivity reinforcement;markov decision;learning markov;risk sensitivity;models human decision;model risk sensitivity;agent risksensitive;reinforcement;risk;agent risksensitive particular;risksensitive particular model;based inverse;processes agent risksensitive;learning algorithm minimizes;decision processes agent;model risk;minimizes loss function", "pdf_keywords": ""}, "1843c91e9692484b574ef40961f1d0443a56ddf4": {"ta_keywords": "peer reward;reward answer evaluation;peer reward inversely;objective feedback platforms;simple incentive mechanism;incentive mechanism;matches peer reward;incentive mechanism obtaining;eliciting informative feedback;propose simple incentive;commerce platforms eliciting;feedback platforms;obtaining objective feedback;informative feedback;objective feedback;evaluation matches peer;feedback platforms methodsin;simple incentive;evaluations products services;incentive;evaluations products;reward answer;agent gets reward;obtaining evaluations products;reward;gets reward answer;gets reward;platforms eliciting informative;platforms eliciting;challenge obtaining evaluations", "pdf_keywords": ""}, "4c0a915b9389e6489753a968085ee12833131d0a": {"ta_keywords": "problems peer review;challenges peer review;peer review;peer review consequences;matthew effect rich;peer review process;scholarly;scholarly research;scholarly research overwhelming;communication scholarly research;communication scholarly;review process highly;prevalence matthew effect;matthew effect;essential communication scholarly;review consequences;key challenges peer;challenges peer;richer academia;improvement problems peer;problems peer;review consequences outcome;rich richer academia;majority researchers;prevalence matthew;researchers;review process;vast majority researchers;richer academia tutorial;matthew", "pdf_keywords": ""}, "df53aabeca68a8c0076f7e110f2cc7df7d010e7a": {"ta_keywords": "talker speech recognition;multi source localization;localization source splitting;multi talker speech;source localization;multi talker;speech recognition discussed;speech recognition;source localization source;localization source;effectiveness multi talker;talker speech;source splitting;multi source;source splitting effectiveness;talker;localization;based multi source;learning based multi;deep learning;deep learning based;speech;splitting effectiveness multi;splitting;recognition;recognition discussed;based multi;splitting effectiveness;learning;multi", "pdf_keywords": "localization speech sources;localizing speaker multi;speaker localization promising;localization speaker multi;speaker localization;speech localization;sound source localization;approach localizing speaker;localizing speaker;localization speaker;based speaker localization;source localization classify;estimation frontend speech;speech localization prior;localization speech;deep clustering speaker;source localization multi;multi source localization;mimo speech localization;useful localization speech;localization multi source;tool localization speaker;localization using deep;source localization;speech sources;source localization model;multi source doa;speech recognition fea;speech sources work;backgroundmulti source localization"}, "b03c7ff961822183bab66b2e594415e585d3fd09": {"ta_keywords": "multi headed attention;neural models focus;attention;attention powerful ubiquitous;nonlinguistic models;headed attention;attention powerful;introduction attention powerful;nonlinguistic models transformer;attention driving force;attention driving;headed attention driving;introduction attention;focus particular salient;neural models;neural;state art nonlinguistic;nonlinguistic;art nonlinguistic models;models focus;particular salient;allowing neural;models focus particular;salient;salient pieces information;allowing neural models;predictions particular multi;salient pieces;focus;average making predictions", "pdf_keywords": "multi head attention;attention heads;masking attention heads;head attention;attention heads verify;attention heads use;decoder attention;attention layers reliant;attention layers;masking attention;majority attention heads;decoder attention layers;multi headed attention;method masking attention;head self attention;16 attention heads;head attention mha;attention;neural models focus;attention heads removed;self attention layers;attention layers development;attention development multi;majority attention;encoder decoder attention;score important head;attention development;headed attention;set majority attention;self attention"}, "315432474166ff7abbc6351e8ff07fcccbd68458": {"ta_keywords": "urls high quality;tweets containing urls;urls shared twitter;low quality urls;urls low quality;quality urls;quality urls shared;misinformation terms tweets;misinformation websites shared;quality misinformation websites;higher rate tweets;quality misinformation sources;quality health sources;quality health information;rate tweets;misinformation websites;containing urls high;tweets;urls high;twitter;rate tweets containing;misinformation sources resultswe;tweets containing;containing urls low;low quality misinformation;shared twitter;quality misinformation;health information websites;misinformation sources;terms tweets", "pdf_keywords": "twitter conversation epidemic;misinformation websites shared;pandemic share news;shared misinformation sources;social media documented;misinformation terms tweets;news social media;sources shared twitter;health information prevalent;spread social media;world social media;quality misinformation websites;information prevalent covid;health epidemic world;social media news;misinformation websites;current outbreak pandemic;quality health information;quality information spread;social media sources;social media public;higher rate tweets;outbreak pandemic share;urls shared twitter;health epidemic;pandemic people linking;media public health;information sources misinformation;social media investigate;webpages shared misinformation"}, "92acaf505a9c738e56ed70759e8d0062f3c520d6": {"ta_keywords": "audio segmentation ar;20e2e automatic speech;segmentation audio;audio segmentation;segmentation audio deal;automatic speech recognition;automatic speech;recording audio segmentation;speech recognition;speech recognition systems;practical automatic speech;issue segmentation audio;segmentation ar model;speech recognition based;long recording audio;segmentation ar;input long recording;audio;introduction neural end;recording audio;long recording;neural end end;neural end;audio deal streaming;streaming input long;end end models;streaming input;end models;segmentation;deal streaming input", "pdf_keywords": "nonautoregressive automatic speech;concatenate audio segmentation;audio segmentation nonautoregressive;segmentation nonautoregressive auditory;audio segmentation auditory;joint audio segmentation;audio segmentation decoding;audio segmentation non;speech recognition ar;speech recognition integrated;combining audio segmentation;audio segmentation;audio segmentation introduce;speech recognition;automatic speech;audio segment processed;automatic speech recognition;propose concatenate audio;improve audio segmentation;processing speech;segmentation auditory auditory;processing speech recognition;segmentation auditory;autoregressive auditory;attention segment audio;audio segment;sequences human speech;autoregressive auditory auditory;concatenate audio;approach audio segmentation"}, "f75d05e759447c2aedb7097728f29f9a520d9bc1": {"ta_keywords": "range language models;long range language;transformer language models;long range context;range language;range transformer language;language models;language models actually;longer sequences models;transformer language;language models process;introduction long range;range context;range context remain;long range;long range transformer;range context additional;use long range;sequences models;language;sequences models past;longer sequences;efficiency self attention;introduction long;attention;advantage long range;range transformer;models process longer;attention led;models advantage long", "pdf_keywords": "range language models;long range language;introductionlinguistic models generally;introductionlinguistic models;transformer language models;context long range;long range context;language models;sequence distant context;language models studied;language models include;range language;longer sequences models;context novel sequence;range context long;discourse level information;sequence level tasks;longrange context;context improve predictions;machine translation models;context long;sequence level predictions;translation models;sequence level prediction;memorize sequence distant;long term context;translation models important;discourse level;context token sequencelevel;introductionlinguistic"}, "1dfa71ecab0c25c5fdd6b2df83a41e944ffa5d58": {"ta_keywords": "words higher frequencies;models based multinomial;based multinomial distribution;multinomial distribution wide;based multinomial;text treat words;statistical models text;multinomial distribution;classification tasks;higher frequencies occurrence;frequencies occurrence sensible;multinomial;frequencies occurrence;classification;models text;models text treat;treat words higher;statistical models;words higher;occurrence;wide range classification;text;present statistical models;classification tasks classes;range classification tasks;occurrence sensible manner;text treat;occurrence sensible;treat words;statistical", "pdf_keywords": ""}, "0c47eb31b2dd76d8dc986173a1d3f00da1c9c74d": {"ta_keywords": "neural language models;parametric neural language;models nls learn;language models;memorizing training datapoints;language models nls;explicitly memorizing training;learn explicitly memorizing;learn predictive distributions;models require retrieval;nls learn predictive;datastore allows learn;models nls;explicitly memorizing;non parametric nonls;memorizing training;predictive distributions text;neural language;learn predictive;retrieval;parametric nonls practical;parametric nonls;parametric neural;learn explicitly;distributions text utilizing;allows learn explicitly;retrieval large datastore;memorizing;non parametric;retrieval large", "pdf_keywords": ""}, "83cbe142d445a521aefa11acbd184e176085e7c7": {"ta_keywords": "behavior observed voters;trusting susspicious voter;voters negative information;observed voters negative;opinion candidate behavior;biased communications trusting;observed voters;candidate behavior;susspicious voter voters;voters methodsin;candidate behavior appears;voters negative;voter voters methodsin;prior positive opinion;communications trusting susspicious;voters methodsin recent;effect biased communications;political decision making;susspicious voter;biased communications;positive opinion candidate;political decision;voter;negative information candidate;voter voters;communications trusting;voters;decision making interpreted;candidate strengthens weakens;effect biased", "pdf_keywords": "distrusting voters computationally;trusting voter model;model suspicious voter;voter model biases;models trusting voter;suspicious voter model;trusting voter anomalous;voter model;trusting suspicious voter;behavior distrusting voters;voter model based;voter bias biases;voter model simple;bias voting;voter known bias;bias voter bias;voter bias;distrusting voters;trusting voters behave;trusting voter;trusting voters;voters behave intuitively;probability suspicious voter;voters computationally;bias voter;voter model effect;bias political decisions;voters analyzed model;introduce suspicious voter;easy trusting voters"}, "6a79ff7465d8249d9c8a50fa5f2e0a3e308b436d": {"ta_keywords": "generative pseudo labeling;unsupervised domain adaptation;domain adaptation;domain adaptation method;query generator pseudo;generator pseudo labeling;novel unsupervised domain;generative pseudo;encoder representative domain;pseudo labeling;adaptation method generative;unsupervised domain;query generator;dense retrieval approaches;labeling cross encoder;retrieval approaches;retrieval approaches overcome;method generative pseudo;domains paper propose;pseudo labeling gl;pseudo labeling cross;dense retrieval;generative;significantly improved search;improved search;improved search results;retrieval;labeling;generator pseudo;combines query generator", "pdf_keywords": "retrieval domain adaptive;phrase retrieval domain;efficient domain adaptation;domainspecific text retrieval;domains large training;retrieval approaches domains;approaches dense retrieval;retrieval domain;dense retrieval dataset;dense retrieval approaches;datasets domain adaptation;domain adaptive training;functional retrieval domain;unsupervised domain adaptation;domain adaptation easy;adaptation dense retrieval;domain adaptation significantly;domain adaptation;retrieval tasks domain;propose domain adaptation;dense retrieval models;dense retrieval pre;tasks domain adaptation;domain adaptation effective;dense retrieval;domain adaptation dense;present domain adaptation;dense retrieval use;training domain adapted;usage dense retrieval"}, "3e3f55cb25b919c4e8158195fd3ce2f23cfa7723": {"ta_keywords": "counterfactual inference;novel counterfactual inference;counterfactual inference framework;reduce language bias;language bias;capture language bias;language bias model;language bias direct;language bias subtracting;bias direct causal;counterfactual;propose novel counterfactual;answers result causal;novel counterfactual;development language bias;effect questions answers;answers reduce language;causal effects;causal effects propose;causal effect questions;language effect;causal effect;direct causal effect;result causal effects;causal;causal effect method;direct language effect;total causal effect;background causal effect;effect total causal", "pdf_keywords": "visual question answering;language bias visual;backgroundvisual question answering;answers mitigate bias;mitigate language bias;reduce language bias;bias visual quality;bias visual;effect language inference;language improve visual;rely language bias;language bias methodsin;language bias direct;quality visual language;language bias;language bias vq;counterfactual inference introduce;bias direct causal;vision language improve;inference counterfactual inference;encoding visual knowledge;language bias virtual;improving visual attention;bad language bias;formulate language bias;language bias realized;improve visual attention;reduction language bias;vision propose causal;effect questions answers"}, "7b29f45df975ed1e4c3864b6ab4483f11086aa76": {"ta_keywords": "neural machine translation;trained word embeddings;machine translation nmt;machine translation;word embeddings;parallel corpora;performance natural language;word embeddings proven;scale parallel corpora;translation nmt systems;parallel corpora obtained;language analysis tasks;performance neural;performance neural machine;translation nmt;corpora;embeddings;pre trained word;natural language analysis;large scale parallel;neural machine;embeddings proven;introductionthe performance neural;language analysis;improving performance natural;embeddings proven invaluable;corpora obtained pre;corpora obtained;trained word;natural language", "pdf_keywords": "trained embeddings translation;embeddings translation useful;improves accuracy translation;bilingual translation tasks;neural machine translation;training translation translation;embeddings translation;pre training translation;pretrained word embeddings;training translation;machine translation new;translation tasks;training word embeddings;human machine translation;machine translation nm;machine translation;translation translation multilingual;embedding alignment translation;translation multilingual;translation useful tool;produce reasonable translations;machine translation applications;word embeddings effective;pretrained embeddings effective;translation bilingual;bilingual translation;machine translation nmr;translation bilingual translation;word embeddings beneficial;translation multilingual context"}, "7a684045afae2ccf40338ff07b8fa429bad93a57": {"ta_keywords": "web corpus;web corpus 10;pages web corpus;commoncrawl billion crawled;web corpora;introductionlarge web corpora;billion crawled;web corpora containing;corpus 10 billion;billion crawled urls;crawled;corpora containing documents;million pages web;corpus;highly scalable hadoop;hadoop based framework;hadoop based;corpora containing;scalable hadoop;scalable hadoop based;50 languages extracted;languages extracted commoncrawl;corpora;corpus 10;introductionlarge web;million pages;hadoop;crawled urls;12 million pages;languages extracted", "pdf_keywords": ""}, "fc5d79301a0876201c95954a764ec374b8eb236e": {"ta_keywords": "translation bylexicicon induction;machine translation bylexicicon;neural machine translation;domain adaptation neural;machine translation nm;translation bylexicicon;domain adaptation;machine translation;introduction domain adaptation;translation nm sensitive;translation nm previously;translation nm;words lack supervision;bylexicicon induction;sensitive domain shift;lexicalized nature nm;induction problem neural;domain shift;adaptation neural;adaptation neural machine;effect highly lexicalized;bylexicicon induction problem;noted neural machine;highly lexicalized nature;highly lexicalized;failure sentences large;previously noted neural;lack supervision domain;supervision domain;failure sentences", "pdf_keywords": "domain parallel corpus;domain word translation;extraction lexicons parallel;neural machine translation;parallel corpus domain;lexicon improve adaptation;improving translation accuracy;seed lexicon alignment;domain unaligned corpus;lexicon alignment;translation domain monolingual;lexicons parallel domain;translation accuracy domain;machine translation association;improve translation accuracy;reverse translation domain;generated lexicon domain;machine translation nm;supervised seed lexicon;unknownneural machine translation;machine translation models;domain adaption lexicon;lexicon induction mitigating;parallel corpus;pseudo parallel corpus;machine translation;effective improving translation;lexicons parallel;improving translation;manually generated lexicon"}, "75abecb4568366d89e89c3c9d39574b9c1c028a5": {"ta_keywords": "nl biomedical text;curators navigate article;curators work informed;observing curators work;biomedical text;curators work;natural language processing;flybasee curators navigate;observing curators;flybasee curators;curators navigate;way flybasee curators;processing nl biomedical;biomedical text technology;discuss observing curators;text technology facilitate;curators;nl biomedical;natural language;language processing nl;processing nl;applying natural language;text technology;facilitate tasks database;text;tasks database curation;methodspaperbbrowser non nl;facilitate tasks;language processing;database curation", "pdf_keywords": ""}, "ec9367ab933a142124eecd3232fe2d933d93a144": {"ta_keywords": "navigation patient history;successful navigation;navigate;navigation patient;navigate manner;navigation;approach navigation patient;successful navigation novel;essential successful navigation;navigate manner easy;navigation novel approach;ability navigate;navigation novel;ability navigate manner;approach navigation;novel approach navigation;patient history history;patient history;history history;history history history;history;novel approach;approach;patient;novel;manner easy;manner easy perform;manner;ability;perform", "pdf_keywords": ""}, "51c2321244b0a489970e1b52c59b049fdcc5cd46": {"ta_keywords": "question answering systems;machine translation mt;machine translation;bases question answering;clarified machine translation;answer questions language;answering systems;question answering;systems answer questions;questions language based;translation mt tool;questions language;answering systems come;using knowledge bases;qa clarified machine;translation mt;cross lingual qa;knowledge bases question;knowledge bases;lingual qa;qa systems;qa systems answer;language based information;lingual qa clarified;clarified machine;build qa systems;topics knowledge bases;answer questions accurately;knowledge bases limited;systems answer", "pdf_keywords": ""}, "bb6c2a64ecb6e4c9f3f5720d53cca76a2c37505d": {"ta_keywords": "linguistic communication;successful linguistic communication;introduction successful linguistic;text corpora deeply;successful linguistic;linguistic communication relies;linguistic;text corpora;contextual foundations language;large text corpora;language methods;corpora deeply;contextual social;representation learning approaches;social nature language;contextual;foundations language methods;corpora;pioneered objective contextual;corpora deeply enriched;contextual foundations;representation learning;language;foundations language;language methods posit;objective contextual;trained large text;contextual social nature;language results;objective contextual foundations", "pdf_keywords": "contextualized pretrained representations;pretrained representations representations;pretrained representations;representations deep;learning enabled representations;fuzzy representations representations;representations deep models;neural language models;linguistic learning;unstructured fuzzy representations;learning non linguistic;neural language;representation learning;word vectors contextualized;dense word vectors;fuzzy representations;representation learning approaches;language models learning;trained large text;non linguistic learning;linguistic learning non;vectors contextualized pretrained;language models important;new language learning;enabled representations deep;word learning;contextualization language human;neural language processing;language models;development large corpus"}, "6b98bef930182a848c027dece1bfb58ca706449d": {"ta_keywords": "pronunciation assisted sub;sub word extraction;sub word segmentation;sub word modeling;characters sub words;end speech recognition;speech recognition output;speech recognition;word modeling pam;word segmentation;word extraction method;word extraction;speech recognition systems;sequence characters sub;pam sub word;pronunciation information;leverages pronunciation information;assisted sub word;word segmentation lead;pronunciation assisted;erroneous speech recognition;modeling pam sub;word modeling;extraction consider character;sub words;word extraction consider;method leverages pronunciation;character sequence frequencies;propose pronunciation assisted;characters sub", "pdf_keywords": "subword models speech;sub word segmentation;sub word extraction;pronunciation assisted sub;sub word modeling;end speech recognition;word segmentation;speech recognition output;speech speech processing;speech recognition corpus;models speech recognition;speech recognition;subword models;speech processing;characters sub words;speech recognition increasingly;models speech;word segmentation lead;sub word methods;speech recognition systems;use subword models;pronunciation information;word extraction;methods speech recognition;pronunciation information word;method learns phonetically;word extraction method;pronunciations experiments;use sub word;sub words"}, "8d7db1b1290e5d6f802e9f1075ef197cb55d754f": {"ta_keywords": "speech recognition interwordpress;recognition interwordpress;english speech recognition;recognition interwordpress wlc;speech recognition;contrastive english speech;interwordpress;naist contrastive english;purposethekit naist contrastive;interwordpress wlc;purposethekit naist;interwordpress wlc wr;speech;2012 evaluation campaign;contrastive english;english speech;evaluation campaign;ar track sed;task developed karlsruhe;naist contrastive;technology naist teams;collaboration interact project;technology naist;evaluation campaign methodsin;2012 evaluation;naist teams collaboration;evaluation;track sed;sed task developed;wr 2012 evaluation", "pdf_keywords": ""}, "acbf4f9a4457cf2884e6018e4653519beef2833a": {"ta_keywords": "congenital cytomegalovirus cmv;guinea pig cytomegalovirus;congenital cytomegalovirus;background congenital cytomegalovirus;cytomegalovirus cmv;pig cytomegalovirus;cytomegalovirus cmv infection;cytomegalovirus;human cmv infection;cmv infection;cmv infection endothelial;complex pentamer glycoproteins;pentamer glycoproteins;cmv infection major;potent vaccine antigen;pentamer glycoproteins required;cells potent vaccine;vaccine antigen;potent vaccine;human cmv;vaccine;vaccine antigen case;antigen;demonstrated pentameric complex;pentameric complex pentamer;studies demonstrated pentameric;demonstrated pentameric;required human cmv;pentameric;cmv", "pdf_keywords": ""}, "790d3503fa95ec32f04c280bd9a52fef6bf1e874": {"ta_keywords": "upwind forward differencing;finite difference schemes;backward euler upwind;euler upwind forward;models traffic flow;difference schemes including;difference schemes;forward differencing;forward differencing lax;various finite difference;finite difference;differencing lax friedrichs;traffic flow;lax friedrichs methods;friedrichs methods methods;backward euler;including backward euler;friedrichs methods;upwind forward;euler upwind;method consistent hydrodynamic;finite differing method;traffic flow authors;models traffic;methods methods finite;methods finite differing;macroscopic models traffic;hydrodynamic macroscopic models;schemes including backward;differencing lax", "pdf_keywords": ""}, "0fcfa0ef253a81c103854e1dc123d90e7310a0e1": {"ta_keywords": "private deep learning;privacy specifically sgd;differentially private deep;differential privacy;differential privacy specifically;advances differentially private;application differential privacy;differentially private;privacy;private deep;sgd algorithm disparate;privacy specifically;sgd vspate disparate;specifically sgd;sgd;sgd algorithm;introduction sgd;sgd vspate;specifically sgd algorithm;private;introduction sgd vspate;deep learning demonstrated;deep learning;model accuracy recent;recent advances differentially;advances differentially;model accuracy;differentially;deep;accuracy recent", "pdf_keywords": "privacy deep learning;differentially private learning;private deep learning;differentially private deep;differential privacy;differential privacy specifically;differential privacy promising;use differential privacy;private learning compare;private learning;advances differentially private;private learning algorithm;differentially private;privacy deep;differential privacy dd;compare differentially private;application differential privacy;methods differentially private;survey privacy deep;private deep;differences privacy significantly;parity differences privacy;privacy dd widely;privacy specifically dd;privacy;privacy significantly higher;privacy significantly;differences privacy;privacy dd;privacy specifically"}, "634bbe75c34b82e664f1e9f083314b5bdb6ba187": {"ta_keywords": "analyze electroencephalogram event;analyze electroencephalogram;attempting analyze electroencephalogram;electroencephalogram event related;electroencephalogram;electroencephalogram event;artifacts eye blinks;artifacts eye;contamination ocular artifacts;backgrounddata contamination ocular;artifact removal methods;ocular artifacts;ocular artifacts eye;related potential erp;potential erp data;artifact removal;multiple signals elicited;signals elicited psychological;signals elicited;potential erp;observed signal;number artifact removal;erp data;contamination ocular;blinks eye movements;assumes observed signal;backgrounddata contamination;event related potential;artifacts;eye movements", "pdf_keywords": ""}, "79a6f290cfe8652575e7bb65cfed519bca8f3bd3": {"ta_keywords": "systematic review literature;systematic review;results systematic review;systematic;review literature;present results systematic;results systematic;review literature topic;literature topic;purpose article;purpose article present;article;article present results;literature;article present;results;present results;topic;review;purpose;present", "pdf_keywords": ""}, "9cda754187545c3cc8f9e9f134c08707269d0fae": {"ta_keywords": "attention models pre;text pre training;language pre training;unsupervised language pre;unsupervised pre training;self attention models;speech text pre;attention models;unifying speech text;models pre trained;unsupervised language;speech understanding self;unsupervised pre;introduction unsupervised pre;text speech understanding;language pre;text pre;approach text speech;attention;universality unsupervised language;text speech;self attention;speech text;speech understanding;understanding self attention;step unifying speech;pre trained;pre training predominant;unifying speech;pre training", "pdf_keywords": "multimodal self supervised;multimodal training speech;self trained multimodal;trained multimodal speech;useful multimodal training;multimodal pre trained;multimodal speech;trained multimodal;multimodal speech models;multimodal model speech;multimodal training;training speech text;perform multimodal training;multimodal encoders predict;predict speech text;useful multimodal;multimodal encoder;multimodal encoder extract;pretraining speech translation;supervised learning speech;approach multimodal self;text self supervised;multimodal training use;encoders useful multimodal;multimodal bilingual pretraining;multimodal models;multimodal encoders multimodal;attention use multimodal;learning speech text;encoders multimodal models"}, "b50d03ecd9f2055b32451e3c04138a0da07b0f69": {"ta_keywords": "meeting recognition;time meeting recognition;real time meeting;meeting recognition understanding;meeting analyzer;meeting analyzer monitoring;group meeting;time meeting analyzer;ongoing group meeting;online manner meeting;automatically speaking online;meeting assistance methods;meeting;monitoring conversations;recognize automatically speaking;meeting assistance;captures utterances face;monitoring conversations ongoing;conversations ongoing group;poses speaker using;face poses speaker;manner meeting assistance;continuously captures utterances;time meeting;poses speaker;distant microphones;group meeting goal;captures utterances;utterances face poses;using distant microphones", "pdf_keywords": ""}, "2bd54adb3b5588281396a4b5dae7db09496b2c61": {"ta_keywords": "text baseline experimental;results provide description;analysis baseline experimental;baseline experimental results;experimental results;text results provide;baseline experimental;description text results;study provide description;text results;results provide;description text baseline;experimental;analysis baseline;text baseline;description text;experimental results gap;results;provide description;thorough analysis baseline;text;providing description thorough;description thorough;description;baseline;provide description text;description thorough analysis;aim study provide;results gap;providing description", "pdf_keywords": "neural question answering;reading comprehension models;comprehension models analyze;question answering;reading comprehension dataset;comprehension models;neural reading comprehension;question answering question;research multilingual qa;multilingual qa;german reading comprehension;comprehension task german;comprehension dataset;reading comprehension task;reading comprehension;multilingual qa ability;questions training;comprehension models rajpurkar;start question word;crowdsourcing lexical;answering question answering;research multilingual qr;questions analyze;questions analyze data;comprehension dataset created;questions using text;solve questions paragraphs;analyze answers dataset;multilingual qr facilitates;crowdsourcing lexical overlap"}, "5e657bc8097c12649d027ca3c16ff7d37df1354d": {"ta_keywords": "training multilingual machine;multilingual machine translation;training multilingual;multilingual machine;machine translation mri;languages training data;translation mri models;translate multiple languages;introduction training multilingual;machine translation;translation mri;languages faced imbalanced;multilingual;mri models translate;languages training;translate multiple;sample resourced languages;models translate;models translate multiple;training sets languages;sets languages training;multiple languages;multiple languages faced;resourced languages increase;languages increase representation;languages increase;imbalanced training sets;resourced languages;translation;translate", "pdf_keywords": "multilingual machine translation;optimize multilingual data;optimize performance multilingual;optimize multilingual;model training multilingual;training multilingual machine;translation models trained;methods multilingual translation;multilingual training method;multilingual model optimal;performance multilingual model;multilingual translation;multilingual models;training useful multilingual;multilingual models increasingly;optimizes usage multilingual;multilingual data best;multilingual models effective;methods multilingual training;multilingual translation variety;translation models;useful multilingual training;methods multilingual models;machine translation models;training multilingual;adaptive methods multilingual;performance multilingual;translation single language;multilingual data;language multilingual training"}, "302ae0d991d62dee82b63530b487a50469810af4": {"ta_keywords": "learning interpretable spatial;mapping natural language;interpretable spatial operations;spatial actions 3d;interpretable spatial;complex spatial pragmatic;instructions complex spatial;natural language instructions;spatial pragmatic interpretations;spatial actions;spatial pragmatic;natural language descriptions;complex spatial actions;3d spatial operations;3d spatial;actions 3d blocks;spatial operations rich;language instructions complex;natural language;actions 3d;spatial operations;learning interpretable;3d blocks world;spatial;language descriptions;descriptions require complex;complex 3d spatial;rich natural language;3d blocks;operations rich 3d", "pdf_keywords": "language spatial reasoning;spatial reasoning action;model language spatial;spatial reasoning realism;robotics language;mapping natural language;language spatial;spatial reasoning;robotics language seen;spatial actions 3d;language interactive;attributes spatial reasoning;natural learning interactive;spatial actions;language interactive learning;learning interactive;programming language interactive;intersection robotics language;spatial pragmatic interpretations;complex spatial actions;actions 3d;spatial pragmatic;interactive learning;visual attributes spatial;natural language descriptions;model language;complex spatial pragmatic;3d spatial;spatial operations rich;interactive"}, "a73d83e50b5687455336a2adce32a069c77ba163": {"ta_keywords": "tools analyze data;tools analyze;new tools analyze;systematic review literature;results systematic review;results systematic;present results systematic;analyze data;systematic review;analyze data identify;data identify optimal;tools;new tools;systematic;use new tools;data identify;review literature;optimal strategies use;review literature topic;analyze;strategies use;identify optimal;data;present results;article present results;identify optimal strategies;results;strategies use new;optimal;strategies", "pdf_keywords": ""}, "13608821aa3b369526221182dfbd3a8842549652": {"ta_keywords": "wireless relay placement;relay placement optimal;introductiondeploy wireless relay;duplex radios relays;relay channel;radios relays;multi relay channel;relay placement;wireless relay;relay channel study;relay channel model;radios relays decode;deployment relay;deployment relay nodes;relay nodes achievable;multi relay;formulas multi relay;relay nodes;forward relaying;relaying;using multi relay;decode forward relaying;relay;relays decode;relays decode forward;duplex radios;introductiondeploy wireless;formulas duplex radios;problem deployment relay;relays", "pdf_keywords": "relay channel optimal;expression optimal relay;relay networks optimal;optimal relay placement;optimal relay;relay optimal placement;relay optimal;multi relay optimal;relays optimal;optimal relay location;relay optimal distance;algorithm wireless relay;relay relays optimal;problem optimal relay;optimal distance relay;wireless relay networks;optimal single relay;relay placement wireless;wireless relay placement;place relay optimal;relays optimal single;radios relays;relay channel;relay networks article;deployment relay nodes;relay networks consider;relay nodes achievable;multi relay channel;relay network deployment;deploying relays walks"}, "81e684d01bbfb1f4143bb2ffea36cc4791f0530c": {"ta_keywords": "reverberant automatic speech;refverberant voice enhancement;speech enhancement preprocessing;channel speech enhancement;speech enhancement;voice enhancement recognition;voice enhancement;refverberant voice;challenge includes reverberant;reverberant automatic;includes reverberant automatic;reverberant environments methodsthe;reverberant environments;reverberant;includes reverberant;released refverberant voice;speech recognition;automatic speech recognition;various reverberant environments;approach various reverberant;automatic speech;multi channel speech;enhancement recognition benchmark;various reverberant;dereverberation feature transformation;speech recognition techniques;speech recognition task;dereverberation feature;enhancement recognition;channel speech", "pdf_keywords": ""}, "14dddd1d8cb2e8c5f4e9998fef84e715cb321ac9": {"ta_keywords": "trust ai;interpersonal trust;cognitive mechanism trust;trust ai prerequisites;identical interpersonal trust;interpersonal trust respect;mechanism trust promote;nature trust ai;mechanism trust;trust;introduction trust;trust inspired;trust inspired identical;trust respect people;introduction trust central;trust promote assess;model trust;model trust inspired;discuss model trust;precisely nature trust;trust promote;trust respect;people artificial intelligence;trust central component;nature trust;artificial intelligence;ai;interpersonal;interaction people artificial;intelligence precisely nature", "pdf_keywords": "human ai trust;trust ai;argue trust ai;trust artificial intelligence;ai trust;formalization human trust;ai trust discuss;trust ai believe;human trust artificial;trust ai work;trust consider ai;ai trust ability;formalizations interpersonal trust;trust ai models;trust humans artificial;trust artificial;anticipated trust ai;evaluating trust ai;warranted trust ai;human machine trust;trust ai developer;human trust humans;trust humans;interpersonal trust trust;conceptualization human trust;human trust;trust defined sociologists;overview human trust;trust relevant human;trust people defined"}, "941e42ee75fc2bf07078bcfbd14bdf9ca7fe99ff": {"ta_keywords": "lingual word embeddings;training language models;bilingual lexicons;languages bilingual lexicons;language models support;bilingual lexicons available;word embeddings low;word embeddings;language modeling;language models;introductioncross lingual;language modeling languages;documentary linguistics;introductioncross lingual word;resource language modeling;lingual word;task documentary linguistics;lingual;missing languages bilingual;modeling languages;embeddings low resource;training language;modeling languages established;languages bilingual;data missing languages;embeddings low;available creating lexicons;textual data;low resource language;creating lexicons", "pdf_keywords": ""}, "981152cb3f1e11c9cee6af2275b57ef79c621934": {"ta_keywords": "text generation suffers;text generation;based text generation;generation suffers text;text degeneration;search based decoding;repetition sampling nucleus;nucleus sampling outperform;decoding;network based text;suffers text degeneration;text degeneration issue;based decoding;text;outperform beam search;contain tedious repetitive;nucleus sampling;based decoding methods;based text;sampling outperform beam;sampling nucleus;tedious repetitive;backgroundthe neural;repetition sampling;sampling nucleus sampling;decoding methods focus;backgroundthe neural network;decoding methods;suffers text;tedious repetitive candidates", "pdf_keywords": "neural text generation;fluency generated text;repetitive words model;text generation suffers;text generation;repetition higher diversity;higher diversity repetition;distribution fluency generated;repetitive words;diversity repetition;neural text;distribution filtered vocabulary;diversity repetition compared;repetition traditional stochastic;generate samples necessarily;context repetitive words;trained language models;fluency generated;discouraging sampling;diversity emphasizing probable;generation suffers text;repetition;generate fluent samples;samples higher diversity;generated text;generate samples;vocabulary methodstraditional stochastic;discouraging sampling according;decoding vocabulary behavior;stochastic sampling"}, "1ea337ac24503d9da8dd9bbf98aac0bfd5920834": {"ta_keywords": "tospeaking style transformation;translation approach tospeaking;statistical machine translation;approach tospeaking style;tospeaking style;style transformation monoton;style transformation;style transformation monotonic;transformation monotonic statistical;approach tospeaking;machine translation;machine translation approach;translation approach;tospeaking;monotonic statistical machine;statistical machine;transformation;transformation monoton;translation;statistical;monotonic statistical;transformation monotonic;style;introductiona monotonic statistical;machine;monoton;monotonic;approach;introductiona monotonic;introductiona", "pdf_keywords": ""}, "b8b5b95a0471e0553a0e6cd5086f384cf0f4d4d8": {"ta_keywords": "translate paralinguistic information;speech translation s2t;translated paralinguistic information;speech translated paralinguistic;method translate paralinguistic;translate paralinguistic;translated paralinguistic;speech translation;speech speech translation;translates speech;paralinguistic information conveying;technology translates speech;speech emotion emphasis;paralinguistic information;translation s2t technology;translates speech languages;translation s2t;speech translated;s2t systems linguistic;emphasis speech speech;emphasis speech;meaning speech translated;linguistic meaning speech;paralinguistic;speech emotion;s2t technology translates;level emphasis speech;lingual communication conventional;cross lingual communication;lingual communication", "pdf_keywords": ""}, "0f2ea810c16275dc74e880296e20dbd83b1bae1c": {"ta_keywords": "attention readers text;gated attention readers;attention ga reader;attention readers;introduction gated attention;model gated attention;recurrent neural network;answering cloze style;readers text;recurrent neural;gated attention ga;gated attention;attention mechanism based;readers text compprehension;attention;query embedding;reader;document reader;novel attention mechanism;attention mechanism;novel attention;architecture novel attention;query embedding intermediate;attention ga;neural network document;questions documents;answering cloze;states recurrent neural;readers;questions documents methods", "pdf_keywords": "attention reader answering;reader answering;attention reader;reader answering cloze;gated attention reader;attention reader read;comprehension focused text;questions document comprehend;attention reader use;focused text comprehension;use gated attention;text comprehension focused;gated attention implemented;document comprehend;attention large dataset;attention implemented;model language comprehension;model gated attention;based gated attention;language comprehension optimal;query contextual embeddings;attention;questions documents model;answering cloze style;text comprehension;document comprehend paper;machine reading algorithms;attention ga reader1;understanding text comprehension;learning understanding text"}, "e58edbeb41f3d2d24832e6e3abb94baac754e3f7": {"ta_keywords": "evaluation text summarization;summarization automated evaluation;evaluation summarization papers;evaluation summarization;standard evaluation summarization;text summarization automated;summarization automated;text summarization;text summarization field;evaluating evaluation text;summarization field progressed;evaluation text;summarization papers methodin;summarization field;summarization papers;summarization;tasks text summarization;automated evaluation metrics;text generation tasks;text generation;manual evaluation;evaluation metrics stand;automated evaluation;manual evaluation essential;evaluation metrics;development text generation;generation tasks text;introductionre evaluating evaluation;evaluation;evaluating evaluation", "pdf_keywords": "evaluation summarization;standard evaluation summarization;summaries metrics;comparing summaries metrics;summarization summarymares valuable;metrics human texts;text summarization summarymares;summarization summarymares;evaluation summarization papers;scoring summaries semantic;summaries metrics poor;summaries semantic content;text summarization;text summarization field;summarization field progressed;summarization field;summarization report;method text summarization;summarization report new;tasks text summarization;summaries semantic;tool summarization report;tool summarization;summarization;valuable tool summarization;scoring summaries;references scoring summaries;comparing summaries;summary based semantic;metrics human judgements"}, "0180c56bfbfb21243f8605e4c6f6aab2779d3ef0": {"ta_keywords": "natural language explanations;language explanations optimal;markov decision process;trust user explanation;explanations optimal action;introductiona markov decision;markov decision;explanations optimal;decision process md;natural language;interacts md policy;language explanations;md policy methodswe;markov;process md policy;language explanations order;rely natural language;decision process;user interacts;user interacts md;reward accrual time;policy methodswe;state action preferably;explanations order;time natural language;explanations order build;introductiona markov;expected reward;policy methodswe rely;expected reward accrual", "pdf_keywords": ""}, "197fcdfe05d0892ee7b4a98ef6fa74dfbcd14b48": {"ta_keywords": "computation crowdsourcing;computation crowdsourcing involves;introductionhuman computation crowdsourcing;crowdsourcing;crowdsourcing involves joint;convex methodsin machine;crowdsourcing involves;convex function objective;existing methods convex;methodsin machine learning;methods convex;convex methodsin;inference ground;introductionhuman computation;methods convex methodsin;know objective functions;statistics convex;inference ground truth;applied statistics convex;truth answers worker;joint inference ground;objective functions existing;machine learning;objective functions;objective function;objective function assumed;function objective;optimizing objective function;answers worker;know objective", "pdf_keywords": "convex inference crowdsourcing;inference crowdsourcing;inference crowdsourcing systems;convexity human computation;computation crowdsourcing;inference techniques crowdsourcing;natural assumptions crowdsourcing;human computation convex;backgroundhuman computation crowdsourcing;functions crowdsourcing;inference human computation;crowdsourcing models satisfy;computation crowdsourcing involves;models crowdsourcing;assumptions crowdsourcing;functions crowdsourcing present;bayesian model crowdsourcing;guaranteeing convex inference;objective functions crowdsourcing;models crowdsourcing models;crowdsourcing field computational;crowdsourcing models;existing models crowdsourcing;assumptions crowdsourcing model;crowdsourcing model allows;model crowdsourcing;crowdsourcing model guarantee;convex existing inference;algorithm crowdsourcing;human computation axiomatic"}, "18c00a9b1e6fde799ec5100cf0b1f37c306d061f": {"ta_keywords": "approach web search;regularity web search;web search discussed;information retrieval;web search;web queries;traditional information retrieval;databases specific topics;web queries posed;web search build;web comprehensive databases;queries posed topic;retrieval assumed queries;search discussed;web comprehensive;information retrieval assumed;information web comprehensive;search;queries;centric approach web;comprehensive databases;comprehensive databases specific;retrieval;fraction web queries;specific topics case;information web;data centric approach;topics case;data centric;specific topics", "pdf_keywords": ""}, "5c283474bbb4838160410e24d33ce89ebaf32c07": {"ta_keywords": "speaker clustering methods;speaker clustering clustering;using speaker clustering;speaker clustering;suitable speaker clustering;bayesian models speech;inference multi mixture;gassian mixture model;mixture gassian model;models speech;models speech processing;multi mixture gassian;evaluation using speaker;mixture model;mixture model suitable;speech processing;multi mixture;mixture gassian;clustering methods;clustering clustering accuracy;bayesian inference multi;gassian model evaluation;clustering accuracy;gassian mixture;scale gassian mixture;clustering methods study;model suitable speaker;fully bayesian inference;multi scale gassian;parametric fully bayesian", "pdf_keywords": ""}, "2acc25a01a7ab7cd6b1a75d534ad29ea7d26f92d": {"ta_keywords": "retrieval methods subtopic;retrieval subtopic retrieval;retrieval subtopic;methods subtopic retrieval;retrieval problem subtopic;subtopic retrieval problem;problem subtopic retrieval;subtopic retrieval;subtopic retrieval subtopic;traditional retrieval problem;subtopics query topic;non traditional retrieval;traditional retrieval methods;retrieval methods;traditional retrieval;document ranking;ranking dependent documents;documents ranking;dependent documents ranking;different subtopics query;retrieval poses challenges;subtopics query;document ranking dependent;retrieval;assumed traditional retrieval;utility document ranking;retrieval problem concerned;retrieval problem;documents ranking violating;subtopic retrieval poses", "pdf_keywords": ""}, "0d3baef146655c5727452ccc0dd680d21d92ae4e": {"ta_keywords": "behaviors marketing variables;consumer behaviors relevant;consumer behaviors;consumer behaviors marketing;model consumer heterogeneity;relationship consumer behaviors;model consumer behavior;consumer behavior;consumer heterogeneity;consumer behavior separately;behaviors marketing;consumer heterogeneity great;marketing variables;appropriately model consumer;marketing variables challenging;model consumer;understanding relationship consumer;sparse model consumer;relationship consumer;collections consumer behaviors;behaviors relevant data;consumer;behaviors relevant;data sparse model;marketing;behaviors;relevant data associated;individual level data;sparse model;large collections consumer", "pdf_keywords": ""}, "0735fb79bf34698c1df4461a05ed51c232c412e4": {"ta_keywords": "transformer recurrent neural;transformer recurrent;model transformer recurrent;like transformers computational;transformers computational;trained models transformers;transformers computational model;model transformer encoder;transformer encoder;recurrent neural networks;models transformers familiar;models transformers;computational model transformer;transformer encoder form;thinking like transformers;recurrent neural;transformers;model transformer;transformer;transformers familiar parallel;like transformers;state machines;neural networks direct;encoder;neural;neural networks;transformers familiar;encoder form programming;finite state machines;machines", "pdf_keywords": "neural transformer pyon;generate transformer neural;task neural transformer;transformer pyon computation;neural transformer;transformer neural;transformer neural version;transformer pyon;processing language transformer;program represented transformer;transformer encoder attention;pyon computation sequences;sequence processing language;pyon computation task;attention patterns transformer;application pyon computation;processing neural network;sequence application pyon;simple sequence processing;pyon computation;attention sublayer transformer;attention patterns transformers;pyon computation fundamental;transformer encoder;language transformer;transformer examples programs;processing language transformerencoder;sequence processing;processing transformer;transformer used program"}, "6fea118a29d78340ae26c465ff06e80e55efbe3b": {"ta_keywords": "reading question answering;question answering;question answering methodscurrent;question answering models;context question answering;answering models;answering models reason;continual learning learn;question answering performs;learning learn incrementally;continual learning;learn incrementally;information incrementally understand;answering performs goal;answering;incrementally understand;incrementally understand goals;reading question;information incrementally;directed continual learning;learn incrementally process;passage incrementally;goal directed continual;art question answering;continual;entire passage incrementally;incrementally;reading;passage incrementally na;answering performs", "pdf_keywords": "incremental question answering;comprehends text incrementally;question answering tasks;question answering;models text comprehension;models question answering;questions answering slice;comprehension introduce incremental;classification question answering;questions answering;question answering introduce;incremental models text;answering tasks like;incremental reading;answering slice;question answering use;incrementally learn;incrementally learn incrementally;answer questions answering;short answer questions;answering tasks;text incrementally;text comprehension introduce;text comprehension;learn incrementally;learn incrementally learn;reads comprehends text;text incrementally possible;introductionhumans learn incrementally;context sequence incrementally"}, "92731a953ad063eab1bc90dc541fb956f147a6ba": {"ta_keywords": "food preferences;food preferences motivating;personal preference driven;preference based software;used food preferences;preference driven;preference driven presentation;preference handling;preference based;preference handling used;potential preference based;preferences motivating examples;based software restaurants;preferences motivating;software restaurants;preference;software restaurants believe;menus seating;problem personal preference;restaurants believe application;potential preference;preferences;menus seating require;restaurants;menus;presentation menus seating;commercial potential preference;foodies;driven presentation menus;restaurants believe", "pdf_keywords": ""}, "0791fe161d947d1e4d3af279b261155b88bc9ddf": {"ta_keywords": "postulate fine temporal;contain sequences events;visits different timestamp;predictions based data;data motivated intuition;records contain sequences;sequences events;temporal;models analyzing;sequences events place;predictions based;electronic records;fine temporal;data motivated;propose models analyzing;context electronic records;fine temporal series;succession alter predictions;based data;timestamp postulate;data;temporal series blood;different timestamp postulate;propose models;based data motivated;temporal series;electronic records contain;events;multiple visits different;multiple visits", "pdf_keywords": "sequence data augmentation;training data augmentation;patient sequence data;data augmentation cluster;propose data augmentation;healthcare time series;efficient data augmentation;postulated temporal clustering;sequence learning;temporal clustering;temporal clustering data;proposed data augmentation;data data augmentation;data augmentation;data augmentation tasks;time series clinical;demonstrating temporal clustering;sequence learning algorithms;time series coarsening;data augmentation techniques;data augmentation robust;data augmentation improve;variations temporal clustering;using data augmentation;optimized data augmentation;data augmentation patients;data augmentation allows;popular sequence learning;method patient sequence;use data augmentation"}, "8d35230fec724398bed3f5939e9fa6a94f55a785": {"ta_keywords": "mining sensitive data;data privacy;protect patient privacy;patient privacy;sensitive data;sensitive data methodsfor;privacy;patient privacy way;populations disclosing private;data privacy preserved;disclosing private information;information data privacy;private information;private information individuals;concealing information hard;information protect patient;machine learning;concealing information;machine learning extract;privacy preserved;privacy way;mining sensitive;privacy preserved concealing;populations disclosing;disclosing private;useful information protect;privacy way resolve;information protect;extract useful information;objective machine learning", "pdf_keywords": "differentially private learning;learning differential privacy;differentially private data;differential privacy data;differentially private algorithms;differential privacy;mining differential privacy;differential privacy smooth;differentially private algorithm;differential privacy possible;algorithms differentially private;differential privacy fundamental;differential privacy present;differentially private naive;preserving privacy differentially;differential privacy method;guarantee differential privacy;private machine learning;differentially private unsupervised;ensure differential privacy;ensuring differential privacy;analysis differentially private;private datasets;privacy differentially private;data private datasets;differential privacy discuss;propose differentially private;privacy differentially;differential privacy important;differentially private"}, "6eae6230ae277b6915706ec05241c8db6b9fab86": {"ta_keywords": "similarity search library;similarity search;new similarity search;similarity;present new similarity;new similarity;parallelization;benchmarks;realistic benchmarks;parallelization focus;high degree parallelization;realistic benchmarks end;parallelization focus realistic;degree parallelization;producing realistic benchmarks;benchmarks end;algorithms;degree parallelization focus;algorithms pursue;search library;important design algorithms;performance methods;design algorithms;algorithms pursue goal;performance methods measured;search library discuss;design performance;search;design algorithms pursue;design performance issues", "pdf_keywords": ""}, "8872e32284467fcbeadd1edd2f11aff077de4ccf": {"ta_keywords": "existing rule learning;backgroundfasteeective rule induction;rule learning existing;proposed rule learning;rule learning systems;rule learning algorithm;rule learning;field rule learning;learning existing rule;rule induction important;rule induction;backgroundfasteeective rule;learning algorithm irep;learning algorithm;large noisy datasets;existing rule;noisy datasets;learning systems computationally;noisy datasets paper;learning systems;benchmark;rule;benchmark problems;algorithm irep large;problem field rule;field rule;learning;datasets;algorithm irep;backgroundfasteeective", "pdf_keywords": ""}, "47442ea4c28d631a9d46a9c23454684b834e49ea": {"ta_keywords": "coreference trigger word;coreference trigger;coreference;resolving coreference trigger;approach resolving coreference;resolving coreference;dataset biomedical language;biomedical language processing;corpus vocabulary captures;essential distributional semantic;distributional semantic;corpus vocabulary;biomedical language;language processing;corpus;trigger word refers;trigger word;original corpus vocabulary;vocabulary captures;semantic;word refers;rich baseline models;vocabulary captures essential;25 original corpus;neural;original corpus;language processing 25;feature rich baseline;new dataset biomedical;word refers multiple", "pdf_keywords": ""}, "df873bde0b44e543634d109a7a8b1ba7dfaa8187": {"ta_keywords": "learns latent ontological;existing knowledge bases;knowledge bases;latent ontological;knowledge bases ks;predefined ontology;predefined ontology methods;ontological;ontology methods;ontology;fixed ontology;ontology given input;ontology methods propose;relying fixed ontology;match predefined ontology;fixed ontology given;ontology given;categories relations extracts;hierarchy categories relations;introduction existing knowledge;categories relations;existing knowledge;knowledge;hierarchy categories;relations extracts facts;learns latent;categories;propose unsupervised model;jointly learns latent;data cataloged", "pdf_keywords": ""}, "22dd93fe1a0b8e9cb83eaff6e2ecca0cd6693294": {"ta_keywords": "voice activity detection;integrates voice activity;voice activity;speech recognition integrated;based voice activity;speech recognition;withcc based voice;automatic speech recognition;automatic speech;end automatic speech;recognition online speech;speech recognition online;temporal classification cc;integrates voice;speech interface;online speech interface;recordings resultsagent temporal;recognition integrated withcc;audio recordings resultsagent;speech interface transcribing;long audio recordings;resultsagent temporal classification;temporal classification;activity detection vad;voice;recordings resultsagent;long audio;cc attention architectures;recognition integrated;extension cc attention", "pdf_keywords": "voice activity detector;voice activity detection;recognition voice activity;detect speech activity;integrating voice activity;integrate voice activity;audio segmentation;audio segmentation audio;automatic auditory activity;segmentation audio;application speech recognition;segmentation audio recordings;voice activity;automated audio segmentation;speech recognition;automatic speech recognition;auditory activity recognition;speech recognition online;detect speech;automatic speech;detecting speech;recognition online speech;automated audio;speech recognition novel;recognition application speech;able detect speech;detect speech speech;online speech interface;speech recognition voice;detection automatic auditory"}, "9d332ad27bfce66ee725b413aa07bd93c355efdf": {"ta_keywords": "natural language augmentation;language augmentation framework;language augmentation;present nl augmenter;processing nl enhancing;models natural language;nl augmenter;pythonbased natural language;participatory pythonbased natural;nl augmenter new;augmentation framework;augmentation framework supports;augmenter new participatory;language processing nl;new participatory pythonbased;natural language processing;participatory pythonbased;augmenter;augmentation;nl enhancing diversity;nl enhancing;processing nl;backgroundnon functional models;augmenter new;language processing;evaluation models natural;natural language;functional models;pythonbased natural;robustness evaluation models", "pdf_keywords": "natural language augmentation;natural language tasks;tasks natural language;language augmentation framework;natural language models;models natural language;tool natural language;language models nlrs;natural language processing;natural language aumentation;natural language technologies;present nl augmenter;pythonbased natural language;language augmentation;status natural language;syntactic changes robustness;lexicals filter allows;nl augmenter new;tests natural language;models nlrs powerful;natural language generation;language inference tasks;nlrs powerful tool;augmentation library nlpaug;text transformations filters;sensitive natural language;machine translation robustness;corpus powerful tool;nonlinguistic tasks labeling;language processing new"}, "8057a5e7bcb0be7059a6e632124bc861b533c794": {"ta_keywords": "model robust speech;matrix adaptation evolution;robust speech recognition;robust speech;language model robust;evolution strategybased neural;speech recognition;optimization lstm language;network optimization lstm;optimization lstm;adaptation evolution strategy;matrix adaptation;covariance matrix adaptation;neural network optimization;lstm language model;evolution strategy cca;speech recognition bottle;strategybased neural network;adaptation evolution;lstm language;language model;introduction evolution strategybased;strategybased neural;lstm;developing neural network;evolution strategybased;evolution strategy;neural network;neural network based;tuning meta parameters", "pdf_keywords": ""}, "88167f36dced91c279162d68af7225f2b4e2091c": {"ta_keywords": "masked language models;language models gue;training language model;tun language models;training language;language models;language models corpora;models pre trained;language model human;introductionpre training language;language model;model human language;pre training data;tun language;masked language;based masked language;fine tun language;human language possible;models gue benchmarks;models corpora;pre trained;human language;corpora certain features;pre training;training different transformer;pre training different;gue benchmarks;language;language possible;models pre", "pdf_keywords": "language models pretraining;pretraining natural language;pre trained linguistic;language model transfer;models pretraining structured;transformer language models;trained transformer language;pretraining structured data;tasks pretraining structured;encoding grammatical structure;trained linguistic;se5we pretraining structured;trained linguistic language;neural language models;pretraining structured;models pretrained data;natural language tasks;language tasks;models pre trained;language models;models pretrained;pretrained data;grammatical structure l1s;language processing gmiv;l1s language model;novel language model;pre trained data;models pretraining;analyze encoding grammatical;language model report"}, "f752bf6f8c1502b8cb58aa1483ef598f9fc0d44c": {"ta_keywords": "generating cytotoxic cytotoxic;cytotoxic cytotoxic cytotoxic;generating cytotoxic;method generating cytotoxic;cytotoxic cytotoxic;cytotoxic;method generating;generating;method", "pdf_keywords": ""}, "5ebe542ee1a7eab7aad8e36ed53dbdd7ebd98c8d": {"ta_keywords": "scene parsing understanding;scene parsing;scene understanding core;scene understanding;using deep neural;deep neural;importance scene understanding;statement scene parsing;core computer vision;convolutional neural networks;deep neural network;computer vision;convolutional neural;architectures usually convolutional;computer vision problem;using deep;usually convolutional neural;parsing understanding context;importance scene;neural networks surpassing;vision;vision problem;neural;convolutional;neural networks;neural network;context relations objects;scene;parsing understanding;problem statement scene", "pdf_keywords": ""}, "73484141ca58d9714ac592e3667de416322b51eb": {"ta_keywords": "wavelet transform;wavelet transform practical;wavelet;multiresolution analysis;multiresolution analysis methods;method multiresolution analysis;multiresolution;practical method multiresolution;method multiresolution;backgroundthe wavelet transform;describing transformed signal;reconstruction original signal;transformed signal;transformed signal using;signal zero crossing;methods describing transformed;original signal zero;zero crossing information;backgroundthe wavelet;zero crossing representation;transform practical method;signal using zero;information zero crossing;signal zero;describing transformed;using zero crossing;studied zero crossing;transform;crossing representation studied;information clear reconstruction", "pdf_keywords": ""}, "29263fa3632951be0ca617988d7c9ce651e74393": {"ta_keywords": "machine translation model;translation model;multilingual training;decoder machine translation;multilingual training essenth;machine translation;introduction multilingual training;translation model different;multilingual settings learning;machine translation mc;multilingual;translation mc systems;different multilingual;multilingual settings;different multilingual settings;introduction multilingual;effects different multilingual;ingredient machine translation;translation mc;encoder decoder machine;translation;encoder decoder;encoder;decoder machine;decoder;expose encoder decoder;expose encoder;settings expose encoder;different data distributions;settings learning training", "pdf_keywords": "multilingual translation models;translation models multilingual;encoders multilingual models;models multilingual training;improving multilingual translation;translation model multilingual;decoders multilingual models;fine tuning multilingual;encoders multilingual;tuning multilingual model;model multilingual training;data multilingual training;encoder multilingual;decoder learned multilingual;encoders decoders multilingual;models multilingual decoder;tuning multilingual;multilingual models load;multilingual decoder advantage;multilingual translation improve;improve multilingual translation;improve translation models;improving multilingual;encoder multilingual decoder;learned multilingual training;beneficial encoders multilingual;multilingual models;multilingual translation;multilingual neural machine;multilingual models beneficial"}, "8b1be80cc1fabcd9ccea76d9a8830e2b07e71f0c": {"ta_keywords": "review pubmed review;pubmed review produced;pubmed review;review pubmed;mining patterns reviews;review produced;review fruit;reviews barflies beeradvocate;hallucinate review fruit;review;patterns reviews;reviews;programmed know words;english syntax mining;fruit vegetable beer;instructed hallucinate review;words obey rules;reviews barflies;pubmed;vegetable beer;recurrent neural network;recurrent neural;hallucinate review;barflies beeradvocate;beeradvocate;syntax mining;patterns reviews barflies;called recurrent neural;neural;review produced computer", "pdf_keywords": ""}, "76b36a059c0d8d66a1bf910de32b34dba19482fa": {"ta_keywords": "blockwise synchronous decoding;decoder automatic speech;synchronous decoding;synchronous decoding algorithm;encoder decoder automatic;streaming style inference;style inference encoder;encoder decoder;endpoint prediction compute;inference encoder decoder;speech recognition;endpoint prediction;encoder;decoder automatic;automatic speech;automatic speech recognition;endpoint prediction endpoint;speech recognition systems;decoding;decoding algorithm hybrid;methods endpoint prediction;prediction endpoint;inference encoder;decoder;combines endpoint prediction;decoding algorithm;prediction endpoint post;streaming style;prediction compute expectation;endpoint post determination", "pdf_keywords": "decoder automatic speech;streaming style inference;end speech recognition;style inference encoder;token detection decoder;tokens decoding;automated speech;blockwise synchronous decoding;automatic speech;synchronous decoding;encoder decoder automatic;new approach decoding;inference encoder decoder;speech recognition;speech recognition ar;repeated token detection;development automated speech;tokens decoding process;decoder automatic;synchronous decoding algorithm;speech recognition rbs;encoder;encoder decoder;decoding;encoder output blocks;automated speech recognition;detection decoder;decoder;inference encoder;decoding decoding"}, "caabc3d0c5ece9d44fb2216a347362d4609934c1": {"ta_keywords": "introductionlarge language models;code natural language;introductionlarge language;language models ls;language models;models ls code;language descriptions current;language descriptions;natural language descriptions;synthesizing code natural;code synthesizing;code synthesizing code;existing models codex;synthesizing code;natural language;completing code synthesizing;state art code;code ls;ls code;introductionlarge;art code ls;models codex;models ls;code natural;models codex gap;ls code recently;existing models;code;largest existing models;art code", "pdf_keywords": "code natural language;language models code;natural language code;code models natural;code models;code modeling;programming language model;novel programming language;language code training;code training corpus;various programming languages;programming languages evaluate;programming languages dataset;code various programming;new language models;evaluation large language;programming languages;language code;code synthesizing code;code synthesizing;programming language;language models report;use code models;novel language models;large language models;synthesizing code natural;code modeling design;multiple programming languages;language descriptions current;programming languages codex"}, "3a8129e6fe3ad9bc3a51e44da32424e38612e4cc": {"ta_keywords": "knowledge bases;introductionlarge knowledge bases;probabilistic deductive database;knowledge deep gradient;deductive database calledtensorlog;deductive database;knowledge bases ks;database calledtensorlog reasoning;knowledge deep;probabilistic deductive;sort knowledge deep;introductionlarge knowledge;problem probabilistic deductive;learning systems;knowledge;deductive;gradient based learning;sort knowledge;calledtensorlog reasoning uses;calledtensorlog reasoning;deep gradient based;learning systems address;reasoning uses;database calledtensorlog;gradient based;deep gradient;learning;logical theory;logical theory converted;based learning", "pdf_keywords": "probabilistic logic models;reasoning probabilistic logic;probabilistic logic;probabilistic deductive database;methods probabilistic logic;probabilistic deductive databases;logic models;soft reasoning probabilistic;knowledge gradient based;introductionlarge knowledge bases;logic models introduce;knowledge bases;reasoning probabilistic;probabilistic similarity logic;knowledge bases ks;knowledge gradient;differentiable soft reasoning;database calledtensorlog reasoning;probabilistic learners;deductive databases;model probabilistic deductive;deductive database;soft reasoning;order probabilistic learners;probabilistic deductive;logic models apply;probabilistic learners learning;deductive databases tuple;logical theories containing;deductive database calledtensorlog"}, "891fd2690a21f29b2ab54ee2249261d93c8cbc5c": {"ta_keywords": "crowdsourced labels suffer;crowdsourced labels;obtain crowdsourced labels;improving quality labeled;crowdsourced;quality labeled data;fast obtain crowdsourced;labeled data goal;obtain crowdsourced;labeled data seek;labeled data;crowddsourcing;labels suffer;quality labeled;large amounts labeled;crowddsourcing popular means;crowddsourcing popular;labeled data modern;amounts labeled data;labeled;background crowddsourcing;machine learning tasks;background crowddsourcing popular;downstream machine learning;labels;labels suffer significant;machine learning;modern machine learning;data goal improving;machine learning methods", "pdf_keywords": ""}, "0ce6db2fb8c691ff8a89bd01f379ce92b1d248d0": {"ta_keywords": "topical retrieval classification;non topical classification;informative words frequent;topical classification;topical classification soft;words frequent methods;informative words rare;retrieval classification;appropriate topical retrieval;topical retrieval;retrieval classification tasks;sentiment author informative;modeling text;informative words;words frequent;words rare;modeling text implicitly;classification soft clustering;classification tasks;classification soft;author informative words;statistical learning tools;statistical learning;non topical;classification;frequent methods paper;approaches modeling text;classification tasks non;soft clustering;relate sentiment", "pdf_keywords": ""}, "6994b9860248aea10f8b8bac74e87afd3fcdc842": {"ta_keywords": "web set expansion;sets named entities;expansion namedd entities;set expansion namedd;named entities approach;expanding sets named;named entities;web google sets;expansion using web;namedd entities;set expansion;semi structured documents;set expansion refers;namedd entities using;structured documents;set expansion using;google sets;expansion namedd;structured documents written;does set expansion;method expanding sets;markup language;expanding sets;sets named;entities using web;google sets paper;partial set objects;written markup language;documents written markup;entities using", "pdf_keywords": ""}, "49f9afa4d0405019d01b55529ce4167380acc103": {"ta_keywords": "controllable speech modification;mapping articulatory speech;speech modification based;speech modification capable;developed speech modification;speech production mapping;performing speech articulatory;speech articulatory inversion;speech modification;articulatory speech production;purposearticulatory controllable speech;controllable speech;speech articulatory;articulatory speech;modification based gassian;speech production;articulatory inversion mapping;inversion mapping articulatory;manipulating unobserved articulatory;sequentially performing speech;mapping articulatory;gassian mixture models;mapping based gassian;waveform modification using;waveform modification;articulatory inversion;direct waveform modification;gassian mixture model;articulatory movements sequentially;mixture models direct", "pdf_keywords": ""}, "e1bb329621de73d08c47beae9b5439a1c244eb1a": {"ta_keywords": "novelty detection;novelty detection identifying;introduction novelty detection;suited novelty detection;novelty detection designing;representation suited novelty;novelty;introduction novelty;detection identifying;detection identifying given;score based representation;attempts learning representation;detection designing score;outside training distribution;training distribution;contrasting shifted instances;suited novelty;detection;shifted instances;reliable machine learning;learning representation;learning representation suited;training distribution essential;shifted instances csi;machine learning end;identifying given sample;machine learning;detection designing;based representation methods;identifying", "pdf_keywords": "novelty detection adversarial;generate novelty detection;novelty detection;novelty detecting;novelty detection supervised;backgroundadditional novelty detection;novelty detection identifying;suited novelty detection;novelty detection important;present novelty detecting;learning distributionally shifted;probabilistic novelty detection;contrastive learning distributionally;novelty detection designing;novelty detecting algorithm;novelty detection article;contrastive learning distribution;learning distributionally;learning detection;backgroundadditional novelty;distribution samples adversarial;detecting shifted instances;generating probabilistic novelty;detection using contrasting;simple contrastive learning;representation suited novelty;classifier detect distribution;detection supervised contrastive;learning distribution;detection shifting"}, "931a103258c96a1230dc5c7e38a1cd0b095b9d62": {"ta_keywords": "learning language model;learn language model;model continuous speech;language model construction;introductionlearning language model;language model continuous;language model;language model text;language model noisy;continuous speech phoneme;robustly learn language;model construction learning;speech phoneme lattice;approach language model;learning language;introductionlearning language;speech phoneme;construction learning language;using acoustic model;model text directly;model text;acoustic model;continuous speech;model scores bayesian;model construction;directly continuous speech;language development;acoustic model scores;language development paper;model noisy input", "pdf_keywords": ""}, "a5690b0a514a7cbc913871e41e54c9ad4f6362db": {"ta_keywords": "robustness machine translation;machine translation mt;machine translation;translation mt task;improve models robustness;improving robustness machine;task improving robustness;models robustness noisy;robustness machine;models robustness;translation mt;improving robustness;robustness noisy;focus language pairs;language pairs;pairs english;robustness;language pairs english;translation;robustness noisy input;japanese submitted systems;systems evaluated blind;focus language;french english japanese;blind test;improve models;blind test set;pairs english french;language;english french english", "pdf_keywords": "robustness machine translation;improve robustness translation;neural machine translation;robustness translation systems;setneural machine translation;translation systems;robustness translation;machine translation mt;ofneural machine translation;machine translation impact;machine translation;machine translation results;machine translation important;machine translation association;monolingual data improve;machine translation msc;translation systems european;translation mt task;translation impact neural;data augmentation backtranslation;domain monolingual data;machine translation article;data domain monolingual;translation results;translation agency;domain sensitive training;european translation agency;translation association computational;task translating;handling mtt translation"}, "6aecc93c2d61da073b70dec19795172ca1ff3405": {"ta_keywords": "spurious correlation;spurious correlations;informal spurious correlation;spurious correlation dependence;check spurious correlations;spurious correlations stress;spurious;informal spurious;check spurious;backgroundin informal spurious;correlation dependence;output check spurious;correlations;correlation;correlations stress;correlation dependence model;dependence;sentence sentiment predictor;sentiment predictor;machine learning know;sentence sentiment;sentiment;machine learning;sentiment predictor output;analyst thinks matter;dependence model;stress;dependence model aspect;thinks matter;matter machine learning", "pdf_keywords": "counterfactual invariance techniques;counterfactual invariance training;enforcing counterfactual invariance;counterfactually invariant predictors;predictors general counterfactually;establishing counterfactual invariance;counterfactual invariance dataset;counterfactually invariant predictor;counterfactual invariance practice;implications counterfactual invariance;counterfactual invariance;establishing counterfactual;enforcing counterfactual;counterfactual invariance present;obtaining counterfactually;counterfactual;problem counterfactual invariance;general counterfactually invariant;method establishing counterfactual;obtaining counterfactually invariant;method enforcing counterfactual;counterfactually invariant;implications counterfactual;predictors used causal;affects implications counterfactual;causal inference formalize;general counterfactually;causal structure predictor;counterfactually"}, "dfd4beb1ecf70b07eb4a52e6ae58f3357e66f478": {"ta_keywords": "augmentation artificially noised;noised examples training;data augmentation artificially;advances speech recognition;data augmentation;speech recognition;perform data augmentation;rapid advances speech;models background noise;advances speech;artificially noised examples;speech recognition current;noised examples;augmentation artificially;recognition current models;augmentation;artificially noised;background noise practitioners;recognition;examples training;amounts noise;examples training set;noise destroy;models background;background noise;noise practitioners;noise destroy performance;harden models background;noised;noise", "pdf_keywords": "recurrent speech models;signals invariantrepresentation learners;invariantrepresentation learners;invariantrepresentation learners rls;speech recognition noise;noise robust speech;robust speech;data augmentation models;robust automatic speech;robust speech recognition;method recurrent speech;speech models;noised examples training;noise invariant representations;noise speech recognition;speech models present;recurrent speech;trained noise augmented;model trained noise;goal speech recognition;representation noisy noise;speech models apply;noisy signals invariantrepresentation;speech recognition powerful;augmentation models;representation noisy;noise speech;data augmentation;recognition noise robust;tool automatic speech"}, "a427334e296b6be27c3a9c7d6b942d6468e487b8": {"ta_keywords": "deployment wireless sensor;sensor networks situations;measurementmentbased dedeploying connected;relay networks important;sensor networks;wireless sensor networks;wireless relay networks;relay networks;deployment agent walks;measurementmentbased;wireless sensor;station deployment agent;impromptu deployment wireless;deployment wireless;measurement;measurementmentbased dedeploying;relay;wireless relay;sensor;station deployment;connected wireless relay;dedeploying connected wireless;deployment agent;base station deployment;measurement needed;networks situations;networks important;networks situations starting;networks important situations;node base station", "pdf_keywords": ""}, "823956ee7b994735f3605f426a71e7f85d86f1d4": {"ta_keywords": "semantic frame induction;unsupervised semantic frame;context unsupervised semantic;semantic frame;perform unsupervised semantic;unsupervised semantic;web scale corpus;frame induction;triclustering problem generalization;frame induction using;clustering triadic;generalization clustering triadic;frame induction problem;corpus perform unsupervised;induction using triclustering;clustering triadic data;context unsupervised;scale corpus;triclustering;approach triframes;based approach triframes;using triclustering;corpus;using triclustering methods;induction problem triclustering;frame induction cast;approach triframes shows;cast frame induction;triclustering methods;semantic", "pdf_keywords": "semantic frame induction;unsupervised semantic frame;perform unsupervised semantic;unsupervised semantic;framework clustering verbs;semantic structures extracted;semantic frame;unsupervised frame induction;syntactic triples dataset;frame induction algorithms;web scale corpus;corpus perform unsupervised;syntactically analyzed corpus;triclustering unsupervised;semantic domains;semantic structures;rise semantic structures;evaluations frame induction;semantic domains vo;clustering verbs;extracted syntactically analyzed;frame induction task;corpus independent evaluations;triclustering unsupervised frame;corpus;clustering verbs triadic;analyzed corpus;association computational language;subject verbobject triples;generate syntactic triples"}, "6eb5029dabd60eb47fddebb5919c613d399fddc6": {"ta_keywords": "introductionlinear discriminant analysis;discriminant analysis lda;introductionlinear discriminant;discriminant analysis;speech recognizer;sequential discriminative criterion;speech recognizer methodsthis;discriminative criterion;discriminative criterion consists;sequential discriminative;improve discriminability;errors speech recognizer;consider sequential discriminative;improve discriminability maximizing;analysis lda;discriminant;aims improve discriminability;discriminability;recognizer methodsthis paper;discriminability maximizing;analysis lda simple;recognizer methodsthis;discriminability maximizing ratio;discriminative;lda simple effective;lda simple;lda;reducing errors speech;feature transformation technique;effective feature transformation", "pdf_keywords": ""}, "edca37b2004861513c54e7e97b64d4e00e72003f": {"ta_keywords": "learning learning logic;learning logic;learning logic programs;formally problem learning;logic programs;logic programs examples;clauses logarithmic depth;horn clause methods;logarithmic depth learnable;depth determinate clauses;problem learning single;clause methods;clauses used practical;single horn clause;problem learning;clauses logarithmic;learnable learning;horn clause;determinate clauses logarithmic;clauses;learning systems results;clause methods focus;generalizations language constant;clause;learning single;practical learning;determinate clauses;learning systems;depth learnable learning;practical learning systems", "pdf_keywords": ""}, "49775f20431c4a605c5dcc7111c9fe785bf00c62": {"ta_keywords": "risk aversion reinforcement;surrogate policy gradient;markov coherent risk;optimizing coherent risk;risk difficult markov;optimize coherent risk;aversion reinforcement learning;coherent risk functionals;risk aversion;model risk aversion;consistent surrogate policy;coherent risk mr;risk cvar optimizing;markov decision processes;risk functionals;policy gradient;coherent risk;conditional valueat risk;reinforcement learning emerging;markov decision;reinforcement learning;risk functionals class;difficult markov decision;coherent risk difficult;policy gradient updates;aversion reinforcement;risk mr;risk mr time;risk cvar;risk", "pdf_keywords": "optimizing risk functional;coherent risk optimality;markov coherent risk;coherent risk functionals;optimizing coherent risk;risk gradient;risk difficult markov;optimize coherent risk;coherent risk objective;risk functional;surrogate policy gradient;risk functionals;risk mr optimality;risk sensitive optimal;probability policy gradient;risk optimality;policy learning stochastic;model policy gradient;optimizing risk;discounted markov decision;risk aversion reinforcement;risk general gradient;method optimizing risk;risk functionals class;risk functionals convergence;risk gradient demonstrate;markov decision processes;model risk aversion;parameterized policies gradient;risk aversion fundamental"}, "4350ce87dd3ec067f1e583ad415f71ef4ba6075e": {"ta_keywords": "training dependency parsers;based dependency parser;dependency parsers;dependency parser;partially annotated corpora;parsers partially annotated;dependency parsers partially;dependency parser ja;annotated corpora;trained partially annotated;word based dependency;annotated corpora introduce;annotated corpora allowing;parser ja trained;partially annotated;approach training dependency;domain adaptation;linguistic resources reduction;parsers;available linguistic;parsers partially;training dependency;corpora allowing effective;parser;use available linguistic;dependency tree;evaluate domain adaptation;parser ja;based dependency;corpora", "pdf_keywords": ""}, "598e2d69d3573adae1f0e3bbe54d10c43f48e0b0": {"ta_keywords": "concept drift stochastic;drift stochastic tracking;concept drift;drift stochastic;convex function evolving;minimizing convex function;tracking performative prediction;minimizing convex;possibly stochastic dynamics;stochastic dynamics evolving;stochastic tracking;stochastic dynamics;problem minimizing convex;stochastic tracking performative;stochastic dynamics depend;stochastic;unknown possibly stochastic;possibly stochastic;performative prediction;learning signal;asymptotic convergence guarantees;non asymptotic convergence;novel non asymptotic;performative prediction methodswe;literature concept drift;convex function;prediction methodswe;machine learning signal;dynamics evolving;prediction", "pdf_keywords": "online stochastic gradient;proximal stochastic gradient;stochastic gradients proximal;stochastic gradient method;novel stochastic gradient;stochastic gradient;online proximal stochastic;minimizer drift gradient;stochastic gradients;based stochastic gradients;stochastic optimization;dynamic stochastic approximation;stochastic approximations linear;error proximal stochastic;analyzing stochastic gradient;optimization stochastic optimization;stochastic optimization time;stochastic gradient type;stochastic approximation;stochastic approximations;stochastic optimization stochastic;stochastic approximation stochastic;approximation stochastic approximations;guarantees stochastic optimization;method proximal stochastic;efficiency stochastic optimization;optimization stochastic;method stochastic approximation;minimizer drift;points stochastic optimization"}, "22f93927c487e0e0e0d2844489423bcb5d21b45c": {"ta_keywords": "malignant disease patient;patient diagnosed malignant;patient treated combination;diagnosed malignant;diagnosed malignant disease;malignant disease;disease diagnosed malignant;case patient diagnosed;malignant disease diagnosed;malignant;patient diagnosed;disease patient;diagnosed;present case patient;disease diagnosed;case patient;disease patient treated;treated combination;treated combination combination;combination;combination combination;patient treated;disease;combination combination combination;patient;article present case;article present;article;purpose article present;purpose article", "pdf_keywords": ""}, "19803adec3b97fb2e3c8097f17bf33fabf311795": {"ta_keywords": "pre trained language;weak supervision labeled;trained language models;tuning pre trained;using weak supervision;pre trained ls;tunedd pre trained;trained language;supervision labeled data;supervision labeled;fine tuning pre;language models ls;weak supervision;pre trained;fine tunedd pre;language models;trained ls using;require excessive labeled;language processing np;language processing;labeled data fine;natural language processing;success natural language;tuning pre;trained ls;tunedd pre;data fine tuning;fine tuning;excessive labeled;natural language", "pdf_keywords": "weakly supervised language;supervised language;semi supervised;training natural language;extended semi supervised;approach weakly labeled;semi supervised learning;weak supervision labeled;supervised language models;supervision labeled data;require excessive labeled;weakly labeled;generate weak labels;weakly supervised;use weakly supervised;weakly labeled unlabeled;labeled data finetuning;trained language models;weakly supervised models;approach weakly supervised;weakly supervised leaning;using weak supervision;automatic automatic automatic;weak supervision framework;method automatic automatic;automatic automatic automaticwe;supervised;pre trained language;automatic automatic;models weak supervision"}, "53dc99155c52979e311a403571f1b1d57ff73b48": {"ta_keywords": "biological tissue modeling;tissue modeling;tissue modeling aims;tissues digital image;model biological tissues;biological tissues digital;tissues digital;predict displacement field;guided neural operator;neural operator learning;biological tissue;digital image correlation;image correlation measurements;biological tissues;workflow biological tissue;image correlation;guided neural;introductionaphysics guided neural;displacement field based;predict displacement;image correlation disc;operator learning;operator learning approach;digital image;material database;tissue;tissues;based digital image;neural operator;neural", "pdf_keywords": "tissue modeling;fourier neural;biological tissue modeling;fourier neural operator;tissue data driven;modeling soft tissue;implicit fourier neural;tissue modeling aims;physicsguided neural operator;tissue data;image correlation biomechanical;integral neural operator;learn material model;material modeling;components neural operator;tissue tissue dynamics;material identification modeling;material modeling tasks;prediction displacement field;neural operator model;tissue dynamics;relevant tissue data;neural operator learning;modeling mechanical;operator learning models;operator learning methods;displacement field based;physicsguided neural;operator learning method;neural operators"}, "6bca949b7ce69d6a43120d75e65f43d4c5a80ed4": {"ta_keywords": "incentive design;incentive design clear;research incentive design;incentive;research incentive;area research incentive;intelligent infrastructure commerce;designing mechanisms;designing mechanisms help;coupling humans operations;self interested parties;infrastructure commerce;humans operations domains;intelligent infrastructure;exogenous uncertainties dynamic;uncertainties dynamic environments;mechanisms;exogenous uncertainties;mechanisms help;uncertainties dynamic;interested parties avoiding;kit designing mechanisms;humans operations;infrastructure;parties avoiding unexpected;coupling humans;tight coupling humans;avoiding unexpected outcomes;coordinate self interested;infrastructure commerce led", "pdf_keywords": ""}, "0ee468b9b709a2610c4b574d67218e7960350224": {"ta_keywords": "augmentation text based;data augmentation text;augmentation text;data augmentation strategy;data augmentation;data augmentation policy;simple data augmentation;methods data augmentation;augmentation strategy;augmentation strategy nm;augmentation policy;neural machine translation;augmentation policy desirable;augmentation;existing augmentation;subsumes existing augmentation;existing augmentation schemes;augmentation schemes;design data augmentation;machine translation;machine translation nm;augmentation schemes leads;replacing words;randomly replacing words;text based tasks;translation nm methodswe;translation nm;tasks neural machine;neural machine;tasks neural", "pdf_keywords": "augmentation text based;neural machine translation;augmentation text;data augmentation text;augmentation neural networks;effective data augmentation;improvements translation tasks;augmentation data augmentation;data augmentation word;augmentation neural;data augmentation policy;data augmentation algorithms;data augmentation;augmentation algorithms generate;augmentation word;augmented sentences sampled;augmentation data;using data augmentation;augmentation word dropout;augmentation policy useful;method augmentation neural;augmentation algorithms;data augmentation demonstrate;data augmentation data;augmentation policy propose;augmentation policy augmentation;data augmentation distribution;subsume existing augmentation;augmentation policy;augmentation"}, "4b22503d6da9ff3222d94106cc7425ea4fea43af": {"ta_keywords": "backgrounddependency parsing;backgrounddependency parsing core;parsers trained;parsers trained traditional;big challenge parsers;parsers;challenge parsers trained;parsing;challenge parsers;traditional newswire corpora;newswire corpora typically;newswire corpora;parsing core;extraction question answering;parsing core task;trained traditional newswire;machine translation era;corpora typically suffer;question answering machine;machine translation;answering machine translation;corpora typically;information extraction;question answering;corpora;traditional newswire;social media data;poorly social media;era social media;task nonlack widely", "pdf_keywords": ""}, "1fa69608666e66452df56b1f71282def7ac16035": {"ta_keywords": "choice manipulation bribery;manipulation bribery methodsthis;voting rules theoretically;bribery methodsthis;manipulation bribery;tampering election computationally;hard resultsthe bribery;voting rules;resultsthe bribery problem;resultsthe bribery;bribery problem;bribery;voting means preference;bribery methodsthis dissertation;election computationally;properties voting rules;election computationally hard;bribery problem asks;various properties voting;tampering election;social choice manipulation;make tampering election;properties voting;choice manipulation;voting;voting means;results social choice;decision making uncertainty;focuses voting;dissertation focuses voting", "pdf_keywords": ""}, "2f7c03f0d3c6f51728e925a874c49a25559cc6b3": {"ta_keywords": "dochopper answer compositional;retrieval method dochopper;long documents methods;answer compositional questions;questions long documents;document mixes retrieved;propose multihop retrieval;answering complex questions;answer compositional;predicting answers paper;sentence embedding document;multihop retrieval;multihop retrieval method;dochopper retrieves paragraph;embedding document mixes;introduction answering complex;retrieval method;retrieval;documents requires aggregating;predicting answers;long documents;evidence predicting answers;method dochopper answer;documents methods;document mixes;compositional questions long;embedding document;answers paper;method dochopper;paragraph sentence embedding", "pdf_keywords": ""}, "c13c400d1f481863d57ec265d296b0a08ec77876": {"ta_keywords": "segmentation input audio;audio segmentation automatic;integrate audio segmentation;segmentation automatic speech;audio segmentation;speech recognition segmentation;automatic speech recognition;speech recognition systems;speech recognition;automatic speech;recognition segmentation input;input audio;realizing automatic speech;practical automatic speech;segmentation input;scenario integrate audio;integrate audio;segmentation automatic;input audio ideal;audio;audio ideal scenario;segmentation;audio ideal;recognition systems run;recognition segmentation;neural network;recognition systems;quantization compression;single neural network;neural network techniques", "pdf_keywords": ""}, "0a2ba7b1c05062d2bb7cd35e218fe08d6ea29488": {"ta_keywords": "graph documents meeting;representing email meeting;graph walks framework;email meeting information;meeting information joint;utilizing graph walks;meeting information;graph walk paradigm;graph documents;email meeting;timeline structural graph;introductionan email meeting;email meeting assistant;graph walks;graph graph documents;lazy graph walk;assistant utilizing graph;walks framework representing;walks framework;meeting descriptions connected;structural graph;information joint graph;networks timeline structural;graph walk;structural graph exexamination;framework representing email;social networks timeline;graph exexamination framework;representing email;utilizing graph", "pdf_keywords": ""}, "fd54706252a094d592feadf53a0a3ffed4af9295": {"ta_keywords": "speech separation recognition;speech recognition;speech recognition technology;speech recognition real;3rd chime challenge;speech separation;automatic speech recognition;introductionthe speech separation;processing automatic speech;chime challenge;automatic speech;performance automatic speech;field speech recognition;speech recognition paper;chime challenge targets;far field speech;separation recognition challenge;3rd chime;field speech;separation recognition;recognition challenge;recognition challenge dataset;chime;outcomes 3rd chime;recognition technology promoting;recognition technology;person talking;speech;scenario person talking;baselines thechime challenge", "pdf_keywords": ""}, "142407d3cb61067e88d385f95ae238c74b19d554": {"ta_keywords": "facebook analyze data;data facebook analyze;significantly different facebook;facebook analyze;facebook posts data;data population facebook;population facebook posts;data facebook;posts data significantly;population facebook;different facebook;used data facebook;facebook posts;different facebook posts;data significantly different;facebook;data significantly;posts data;posts data available;significantly different;analyze data population;analyze data;data population;data;www biomedcentral com;biomedcentral com 1471;used data;http www biomedcentral;www biomedcentral;biomedcentral com", "pdf_keywords": ""}, "75983c55a489d526427fe399ce2670376168a2f0": {"ta_keywords": "generate paraphrases japanese;ja paraphrase resources;japanese japan paraphrases;japan paraphrases;paraphrases japanese;paraphrases japanese japan;extract generate paraphrases;paraphrase resources cover;generate paraphrases;paraphrase categories domains;japan paraphrases number;paraphrase resources;ja paraphrase;paraphrase categories;ja ja paraphrase;statistical machine translation;paraphrases;phrases domains;phrase based statistical;paraphrases number paraphrase;paraphrases number;corpora alignment techniques;machine translation;machine translation extract;parallel corpora;parallel corpora alignment;used parallel corpora;paraphrase;number paraphrase categories;corpora alignment", "pdf_keywords": ""}, "5bc188b4ab7b27649236fad6a686b2cfe6368219": {"ta_keywords": "document topic modeling;topic modeling;topic modeling presented;topic modeling technique;discovering abstract topics;background topic modeling;topics occur collection;topics occur;paper commonsense knowledge;abstract topics occur;document topic;topics;commonsense knowledge based;text auto categorization;abstract topics;opinion mining;categorization opinion mining;commonsense knowledge;paper commonsense;methods paper commonsense;opinion mining methods;depend word occurrence;knowledge based algorithm;probabilistic models proposed;topic;word occurrence;probabilistic models;auto categorization opinion;algorithm document topic;mining methods paper", "pdf_keywords": ""}, "d8d49cc56b303d6ed0e821f8593e2f7acd1b4fb4": {"ta_keywords": "sound event detection;supervise soodore sensor;dcace2020 task4 sound;task4 sound event;backgroundconvolation augmentated;backgroundconvolation augmentated transformer;sound event;backgroundconvolation;audio feature;soodore sensor technical;audio feature sequence;soodore sensor;context information audio;task4 sound;detection separation domestic;event detection separation;information audio feature;convolution networks efficiently;detection separation;audio;convolution networks;transformer semi supervise;sensor;supervise soodore;information audio;depth wise convolution;sensor technical;dcace2020 task4;transformer;event detection", "pdf_keywords": ""}, "a61ef7be5b5c9fbc6654f7c17fa595976652416b": {"ta_keywords": "transplants performed australia;kidney transplants;donations deceased patients;kidney transplants performed;transplants performed;85 kidney transplants;introductionmatching donations deceased;tissue authority australia;transplants;data provided organ;organ tissue authority;donations deceased;deceased patients patients;introductionmatching donations;deceased patients;australia methodswe propose;provided organ tissue;account 85 kidney;provided organ;donations;australia simple mechanism;consideration organ tissue;mechanisms perform matching;kidney;currently consideration organ;patients waiting list;patients waiting;patients patients waiting;australia methodswe;performed australia methodswe", "pdf_keywords": ""}, "ff1a1e39a94b9ca31e6013d12bc2d27f7a31567c": {"ta_keywords": "head decoder end;introductionmulti head decoder;end speech recognition;head attention model;head decoder;multi head attention;corpus spontaneous japan;integrated single attention;multiple attentions calculated;speech recognition aimto;speech recognition;head attention;multiple decoder;model multiple attentions;attention model multiple;attentions calculated integrated;multiple attentions;end end speech;end speech;decoder end end;uses multiple decoder;decoder end;attention model;attentions calculated;attention level proposed;corpus;using corpus spontaneous;evaluation using corpus;single attention;integration attention level", "pdf_keywords": "head attention layer;decoders attention integrated;multiple decoders attention;attention layer;attention model multi;attention layer wise;head attention model;multi head attention;attention based encoder;speech attention based;multi head decoder;attention model multiple;model multiple attentions;decoders attention;decoders attention integrates;attention integrated outputs;end speech recognition;attention functions head;head decoder architecture;context speech attention;attention model;integrated single attention;multihead decoder end;multihead decoder;head decoder end;attention model instead;head decoder;multiple attentions;speech attention;multiple decoders head"}, "3b4b5e72a2f84d079d0d1d825309c2f6ded76539": {"ta_keywords": "auditory model adaptation;speaker adaptation;speaker adaptation task;application speaker adaptation;training transfer vectors;transfer vector estimation;model adaptation;adapted model transfer;model adaptation based;vectors application speaker;adaptation task methods;model transfer vector;novel adaptation technique;introduction auditory model;adaptation technique;adaptation technique based;transfer vectors;adaptation based;auditory model;adaptation task;focus transfer vector;transfer vector;adaptation based coarse;adaptation;novel adaptation;transfer vectors results;introduction auditory;propose novel adaptation;training transfer;transfer vectors application", "pdf_keywords": ""}, "77e5c4fa595466aa51d29327a60f9d4af4436876": {"ta_keywords": "learns incrementally;explanation based learning;learns incrementally method;knowledge intensive inductive;concept learner;inductive learning algorithm;intensive inductive learning;inductive learning;eb learns incrementally;alternative learning algorithm;concept learner disadvantage;learning algorithm called;based learning eb;background knowledge improve;learns;based learning;learning algorithm calledia;learning algorithm;abductive explanation based;eb learns;alternative learning;performance concept learner;learning eb;learning;aimto alternative learning;algorithm called abductive;knowledge improve;knowledge intensive;uses background knowledge;described knowledge intensive", "pdf_keywords": ""}, "3c0d4dc4237934e37467f4ede3af859bcb140abf": {"ta_keywords": "context crowd counting;crowd counting methodsspecifically;crowd counting;crowd counting problem;progress crowd counting;global context crowd;context crowd;cnns indicates global;cnns;context convolutional;larger context convolutional;context convolutional neural;global scene context;networks cnns;cnns indicates;crowd;networks cnns indicates;global scene;scene context;counting;scene context essential;neural networks cnns;convolutional neural networks;convolutional neural;counting problem achieved;convolutional;features global;counting problem;indicates global scene;progress crowd", "pdf_keywords": "crowd counting spatially;benchmark crowd counting;crowd counting datasets;supervised crowd counting;effective crowd counting;developments crowd counting;improve prediction crowds;detecting crowds images;crowd counting feasible;effective detecting crowds;model crowd counting;crowd based datasets;crowds images;crowd counting applications;counting large spatially;prediction crowds;point supervised crowd;people crowded scenes;crowd counting facilitate;token attention regression;detecting crowds;token attention;counting crowd based;crowded scenes;approach crowd counting;supervised crowd;proposed token attention;demonstrate crowd counting;crowds images diverse;crowd counting large"}, "9f49ed155d7575d181d16dd5bc92b754cae0bea9": {"ta_keywords": "blind subspace identification;minimization blind subspace;norm minimization blind;blind subspace;subspace identification;nuclear norm minimization;subspace identification n2bpsi;blind identification problem;minimization blind;known blind identification;blind identification;norm minimization;estimate inputs dynamics;identification feasible;nuclear norm;simultaneously known blind;identification problem paper;identification problem;identification feasible measure;estimate inputs;desirable estimate inputs;applications identification feasible;known blind;identification practical applications;minimization;identification n2bpsi useful;subspace;identification n2bpsi;practical applications identification;feasible measure inputs", "pdf_keywords": ""}, "91a2d496553cfee2b66906f704b8e3d081e2d1bf": {"ta_keywords": "software fault prediction;fault prediction methods;methods predicting fault;inductive logic programming;predicting fault density;fault prediction;predicting fault;fault density classes;logic programming ilp;programming ilp methods;logic programming methods;programming methods software;study inductive logic;programming ilp;methods software fault;ilp methods predicting;evaluate inductive logic;programming methods;software fault;fault density;inductive logic;logic programming;methods evaluate inductive;faults errors;itive ii faults;methods software;ilp methods;faults;study inductive;programming", "pdf_keywords": ""}, "2bbb33ab8124e5078ec39e821a25c24c20a31b9b": {"ta_keywords": "crowdsourcing technology;crowdsourcing;make crowdsourcing technology;make crowdsourcing;crowdsourcing technology users;crowdsourcing transition;helping crowdsourcing;helping crowdsourcing transition;workshop crowdd science;crowdsourcing transition art;crowdd science organized;face make crowdsourcing;crowdd science;second workshop crowdd;goal helping crowdsourcing;workshop crowdd;2021 workshop;2021 workshop second;second workshop;workshop second;workshop;conference large data;crowdd;vlb 2021 workshop;science organized;researchers;art science;science tackles research;art science tackles;produces results researchers", "pdf_keywords": ""}, "0e96925c57b3325e7e37c1964b518e9276024cbf": {"ta_keywords": "natural language processing;methods natural language;natural language;human disease;human disease understood;etiology human disease;language processing;etiology human;disease understood article;language processing mnp;disease understood;disease;emimical;etiology;2016 conference emimical;processing;language;emimical methods natural;human;conference emimical methods;discuss results;conference emimical;present results 2016;results 2016;emimical methods;results 2016 conference;2016 australia discuss;mnp 2016 australia;methods natural;present results", "pdf_keywords": ""}, "7c2ff8fa0d24ed712e4bc2dbdb370a1cd62c965b": {"ta_keywords": "thoracic echocardiography;using thoracic echocardiography;patient diagnosed heart;disease diagnosed heart;diagnosed heart disease;heart disease diagnosed;diagnosed heart;echocardiography;disease using thoracic;heart disease;case patient diagnosed;heart disease using;patient diagnosed;thoracic;using thoracic;disease diagnosed;case patient;diagnosed;heart;patient;disease;disease using;case;using", "pdf_keywords": ""}, "3d846cb01f6a975554035d2210b578ca61344b22": {"ta_keywords": "semi supervised learning;learning graph embeddings;supervised learning graph;semi supervised;graph embeddings;present semi supervised;introductionresiting semi supervised;graph embeddings present;based graph embeddings;graph embeddings given;embeddings given graph;learning graph;graph instances train;graph develop transductive;embeddings present semi;instances train embedding;embedding instance jointly;train embedding instance;graph instances;supervised learning framework;supervised learning;given graph instances;train embedding;neighborhood context graph;supervised;class label neighborhood;embeddings;predict class label;embeddings present;label neighborhood context", "pdf_keywords": "learning graph embeddings;learning graph embedding;semi supervised embedding;algorithm learns embeddings;graph embeddings;graph embeddings neural;graph embeddings embeddings;supervised learning graph;graph embedding;learns embeddings;based graph embeddings;supervised embedding;graph embeddings given;based graph embedding;training classification graph;learned embeddings;semi supervised learning;classification graph context;embeddings given graph;learning semi supervised;supervised embedding ability;learning graph;revisiting semi supervised;classification graph;introduce semi supervised;determined learned embeddings;learned embeddings input;graph instances train;semi supervised;learns embeddings neural"}, "b6de9d0ca42a03967287aa7abfd59479e086a35a": {"ta_keywords": "structured knowledge bases;knowledge bases;constructing structured knowledge;knowledge bases ks;structured knowledge;hierarchical semi supervised;automatically constructing structured;automatically constructing;semi supervised learning;semi supervised;constructing structured;research automatically constructing;generating facts populate;supervised learning;generating facts;hierarchical semi;supervised;supervised learning algorithm;structured;hierarchical;knowledge;fin hierarchical semi;learning algorithm;constructing;facts populate;lo fin hierarchical;fin hierarchical;facts;research automatically;facts objective address", "pdf_keywords": ""}, "d39478dd8d825bbd6c963d6a5ef2cee6857f6c21": {"ta_keywords": "argumentation multifaceted communication;argumentation prevalent;argumentation multifaceted;view argumentation prevalent;argumentation prevalent textbooks;argumentation;instead argumentation;world instead argumentation;instead argumentation multifaceted;view argumentation;classical view argumentation;view argumentation multifaceted;use common sense;common sense emotions;emotions social context;communication tool;common sense;sense emotions social;communication;multifaceted communication;multifaceted communication tool;communication tool built;emotions social;sense emotions;context methodsthe classical;context resultsamong humans;social context methodsthe;social context;context methodsthe;methodsthe classical view", "pdf_keywords": ""}, "358d7d6333d3edd530e37efd8004cb9da8cfd5d4": {"ta_keywords": "knowledge extraction cooking;extraction cooking videos;analysis cooking videos;procedural knowledge extraction;cooking videos methodsobservational;structured procedural knowledge;cooking videos presented;cooking videos;procedural knowledge;learn procedures video;methodsobservational analysis cooking;analysis cooking;procedures video captioning;evaluation multimodal models;benchmark structured procedural;overall evaluation multimodal;evaluation multimodal;extraction cooking;structured procedural;automatically collecting knowledge;videos used learn;videos methodsobservational;knowledge extraction;video captioning;videos methodsobservational analysis;learn procedures;multimodal models;instructional videos;instructional videos used;used learn procedures", "pdf_keywords": "parsing video action;semantic parsing video;videos extract actions;parsing video;sentences video videos;video action recognition;procedures video captioning;extract knowledge video;annotated information video;knowledge video clips;clips sentences video;learning instructional videos;knowledge given video;video captioning;video context challenge;learn procedures video;video captioning way;video action;sentences video;action recognition;extract information video;video transcript heuristic;procedural knowledge video;action recognition propose;information videos extract;video transcript;extract actions objects;given video transcript;instructional videos method;video context"}, "adf726bdcdddacee1c70d911b8f84b6a16841a32": {"ta_keywords": "customer reviews products;reviews products service;summarizing customer reviews;reviews products;review aspects product;customer reviews;prominent review aspects;review aspects conventionalally;review aspects;analyzing summarizing customer;products service based;conventionalally prominent review;summarizing customer;products service;prominent review;number prominent review;reviews;review;services amazon com;systems analyzing summarizing;analyzing summarizing;aspects product type;product types;com yelp com;aspects product;services amazon;types new products;yelp com large;product types new;product type", "pdf_keywords": ""}, "0dff2b00fd6e8e7b5f3c0707b0e51e3628988420": {"ta_keywords": "sensitive information publishing;sensitive information;propose new privacy;protect data;way protect data;new privacy;privacy;decisions based text;biased decisions automatic;information publishing data;trails sensitive information;publishing data;protect data providers;fair decisions based;information publishing;publishing data methods;decisions automatic systems;biased decisions;background biased decisions;decisions automatic;make fair decisions;human decision makers;based text;systems human decision;systems make fair;based text instead;concerns research communities;decision systems human;fair decisions;decisions based", "pdf_keywords": "privacy aware translation;privacy aware translations;privacy aware text;rewriting propose privacy;annotators developed privacyaware;privacy risk adversarial;sensitive information publishing;sensitive information annotators;privacy aware translatorwe;optimal model privacy;prediction sensitive information;reducing discrepancy privacy;sensitive information fairness;privacyaware model human;sensitive information ethical;leakage sensitive information;sensitive information;use sensitive information;privacy aware models;sensitive information explore;privacyaware model;politics privacy aware;developed privacyaware model;new model privacy;obfuscation human annotators;aware text rewriting;model privacy;propose privacy aware;privacyaware;formulates privacy"}, "694c0c5a4d0176e29bb85e1b9ca8ea84075fbbbb": {"ta_keywords": "models hmms neural;mammalian models hmms;hmms neural networks;rns architectural transformations;models hmms;hmms neural;networks rns architectural;algorithm humanmms;welch algorithm humanmms;algorithm humanmms special;algorithm used neural;neural networks rns;hidden representations sequential;rns architectural;learn hidden representations;networks rns;human mammalian models;rns learn hidden;representations sequential data;neural;neural networks;hidden representations;backward computation baum;hmms rns learn;mammalian models;representations sequential;architectural transformations;commonality hmms rns;architectural transformations described;used neural", "pdf_keywords": ""}, "194a1e5f9af0ea00b22def879d90b926187fbb64": {"ta_keywords": "etiology understood etiology;etiology etiology understood;understood etiology etiology;etiology understood;etiology etiology;understood etiology;etiology understood recently;etiology;reported etiology etiology;reported etiology;recently reported etiology;understood recently reported;understood;understood recently;reported;recently reported;recently", "pdf_keywords": ""}, "c02da00857c33fa39b115c0eb6c655ff6cf96878": {"ta_keywords": "world world world;world world;world", "pdf_keywords": ""}, "ca201db9980e49647feedf39eb30b19f074bf68a": {"ta_keywords": "speech text st;speech text;english speech text;learning task acoustic;task acoustic models;arab characters necessary;uncased arab characters;task acoustic;text st machine;st machine learning;capitalization punctuation denormalization;arab characters;trained uncased arab;necessary orthography capitalization;acoustic models;orthography capitalization punctuation;orthography capitalization;acoustic models conventionally;characters necessary orthography;capitalization punctuation;punctuation denormalization;uncased arab;models conventionally trained;speech;english speech;text;text st;machine learning task;information present acoustic;capitalization", "pdf_keywords": "professional transcriptional corpus;corpus automatic speech;transcriptional corpus;corpus transcribed telephone;verbatim transcription;transcribed telephone conversations;corpus spontaneous speech;end transcription english;large corpus transcribed;label verbatim transcription;corpus transcribed;speech text task;speech large corpus;automatic speech;trained corpus;transcription english;speech text;speech processing corpus;corpus speech;new speech text;professionally transcribed;corpus speech speech;models trained corpus;audio composition corpus;transcribed verbatim;transcribed verbatim transcribed;development corpus speech;development speech text;transcription fully formatted;transcriptional corpus spontaneous"}, "48530f3d6425f2f150f07ccdd61ba951951a0a7d": {"ta_keywords": "adaptation nmetry resultsour;adaptation nmetry;approach adaptation nmetry;neural machine translation;adaptation neural machine;adaptation neural;machine translation;approach adaptation neural;efficient approach adaptation;machine translation methodswe;adaptation;approach adaptation;trained model lightweight;translation methodswe propose;injecting tiny task;nmetry;nmetry resultsour proposed;nmetry resultsour;translation methodswe;translation;tiny task specific;neural machine;tiny task;pre trained model;trained model;layers pre trained;adapt;adapt model;injecting tiny;trained", "pdf_keywords": "adaptation nonlingual translation;translation models large;neural machine translation;massively multilingual translation;machine translation models;translation models;multilingual translation massively;translation applications neural;translation massively multilingual;scalable adaptation neural;multilingual translation task;domain adaptation nonlingual;massively multilingual model;improve translation performance;translation performance propose;translation models demonstrate;train massively multilingual;translation high resource;translation translation performance;adaptation nonlingual;multilingual translation high;translation performance;multilingual translation translation;translation multilingual approach;translation multilingual;machine translation new;multilingual translation;multilingual model;translation performance domains;translation translation multilingual"}, "e3a85c5defe60f1f394fc4e7245fc071a249cf5b": {"ta_keywords": "behavioral models occupants;building occupants energy;occupants energy efficient;incentivize building occupants;occupants building sustainable;learn behavioral;occupants energy;learn behavioral models;models occupants building;building sustainable automation;building occupants;behavioral;occupants building;game incentivize building;sustainable automation methodswe;sustainable automation objectivethe;social game;goal social game;sustainable automation;social game incentivize;learning parameters behavioral;efficient learn behavioral;incentivize building;introductionthe goal social;objectivethe goal social;behavioral models;energy efficient learn;automation objectivethe goal;goal social;models occupants", "pdf_keywords": ""}, "c99e050b83360e5cbeee8fd2957aaab5b31aa638": {"ta_keywords": "accurate language models;generative sequence model;language models;language models capture;language models including;discrepancies generative sequence;building accurate language;term discrepancies generative;natural language processing;term dependencies;generative sequence;long term dependencies;language processing;sequence model;discrepancies generative;accurate language;challenge natural language;language processing end;sequence model true;term dependencies core;natural language;long term discrepancies;generative;dependencies;dependencies core challenge;term discrepancies;discrepancies improve model;art language models;use discrepancies improve;discrepancies improve", "pdf_keywords": "language models entropy;accurate language models;probabilityistic language model;language model entropy;generations language models;memory language models;language models;language models assessed;language models recently;language models capture;neural language models;language models demonstrate;language model long;popular language models;language models important;language model results;language models function;language models aim;language models applications;probabilityistic language;language modelsthe;language model;building accurate language;term memory language;language models true;term generations language;language modelsthe aim;generative sequence model;language models article;fix language modelsthe"}, "7ad913d1c6eddbdad1ab4571ab91f00f055ab735": {"ta_keywords": "fast recurrence attention;recurrence attention;recurrence attention mechanism;attention better parallelized;sequence transduction tasks;speech recognition ar;automatic speech;speech recognition;recognition ar attention;automatic speech recognition;attention mechanism far;including automatic speech;architecture sequence transduction;attention mechanism;ar attention mechanism;sequence transduction;ar attention;attention;attention mechanism excels;attention better;parallelized regular rn;built solely attention;fast recurrence;rn novel network;solely attention better;transduction tasks;transduction tasks including;solely attention;transformer architecture;architecture sequence", "pdf_keywords": "automatic speech;including automatic speech;automatic speech recognition;architecture serine recurrent;model longform speech;speech recognition ar;sequence modeling recurrent;speech recognition;recurrent neural network;modeling recurrent;recognition developments speech;recurrence srul encoder;developments speech recognition;attention fast recurrence;speech recognition developments;corpus speech recognition;speech recognition article;speech recognition important;amplification based encoder;methods speech recognition;sequence modeling;neural network rna;training decoding;fast recurrence srul;recurrent neural;longform speech;sequence modeling work;encoder audio;sequence transduction tasks;neural network"}, "40947612162cc4644f9489721ec1ca94fe7e765c": {"ta_keywords": "semantic relatedness terms;semantic relatedness;judgements semantic relatedness;semantic relatedness extensively;introduction semantic relatedness;systems semantic relatedness;relatedness terms;relatedness terms represents;semantic relatedness hand;judgements semantic;semantic;relatedness extensively studied;terms represents similarity;relatedness extensively;lexical databases evaluation;make judgements semantic;relatedness;similarity meaning;lexical databases;introduction semantic;processing systems semantic;lexical;generated lexical databases;systems semantic;represents similarity meaning;similarity;similarity meaning numerical;datasets generated lexical;information useful language;represents similarity", "pdf_keywords": "russian semantic relatedness;semantic similarity relatedness;computing semantic relatedness;semantic relatedness using;semantic relatedness list;semantic relatedness human;semantic relatedness presented;semantic similarity;russian semantic;predicting semantic relatedness;semantic relatedness;semantic relatedness terms;semantic similarity measures;evaluation semantic similarity;semantic relatedness present;semantic relatedness 105;semantic relatedness report;use semantic similarity;semantic relatedness datasets;study semantic relatedness;wordj similarityij designed;relatedness terms language;language distributional thesaurus;thesaurus wordvectors org;datasets semantic relatedness;purpose russian semantic;lexical computational semantics;thesaurus wordvectors;judgments semantic relatedness;wordj similarityij"}, "69379f55de081938ae9d8b91ef549542ed78f5f0": {"ta_keywords": "speakers dirization used;speakers dirization;shown speakers dirization;speakers improve ability;speakers improve;quality speakers improve;dirization used improve;improve quality speakers;study shown speakers;dirization used;quality speakers;dirization;speakers;communicate patients;ability communicate patients;shown speakers;improve ability communicate;ability communicate;used improve quality;communicate;patients;improve quality;used improve;improve;improve ability;quality;new study;new study shown;study shown;study", "pdf_keywords": "classification speaker diarization;classifications speech diarization;recognition speaker diarization;speaker diarization speech;speaker diarization;datasets speaker diarization;recognition speech diarization;speaker diarization based;challenges speaker diarization;diarization speaker diarization;speaker diarization techniques;methods speaker diarization;speaker diarization considered;speaker diarization fundamental;speaker diarization development;based speaker diarization;diarization speech recognition;network speaker diarization;speaker diarization evaluations;speech diarization;speaker diarization systems;speaker diarization algorithms;speaker diarization challenging;speech recognition speaker;speaker diarization investigated;speech diarization new;study speech diarization;classifications classification speaker;approach speaker diarization;techniques speech diarization"}, "d74c5b5ed8eb467dc7f313b70a08880fcd74c39d": {"ta_keywords": "technology acceptance model;technology acceptance;acceptance model;web questionnaire survey;using web questionnaire;web questionnaire;information systems websites;acceptance model tacm;user assessment electronic;electronic filtering payment;extension technology acceptance;using web;users use information;information using web;information systems;systems websites local;findings user assessment;tax survey;target information systems;user assessment;national tax survey;websites local governments;intentions users use;intentions users;users use;acceptance;analyze intentions users;survey tax;use information using;tax survey tax", "pdf_keywords": ""}, "625764f8e3e1334ffbfe5b3139e555499e6df4d5": {"ta_keywords": "website update requests;site update requests;requests update information;update requests;update requests semi;web site update;update requests important;requests update;understanding requests update;processing nl requests;website update;requests information studied;nl requests information;requests information;requests posted email;natural language website;site update;analyze requests posted;understanding requests;language website update;nl requests;update information;analyze requests;requests semi automatically;natural language processing;process natural language;understand web site;requests posted;requests;systems natural language", "pdf_keywords": ""}, "b5241fcbfbf30f6fd8ff1ae19d947dd2ca23244f": {"ta_keywords": "emotion provoking events;dictionary emotion provoking;experiences emotion provoking;backgroundaccquiring dictionary emotion;provoke particular emotion;dictionary emotion;emotion provoking;dictionary events survey;aggregating events web;emotion person experiences;acquiring aggregating events;experiences emotion;person experiences emotion;dictionary events;aggregation events provoke;provoking events;events provoke;particular emotion;events provoke particular;events web;particular emotion person;events web data;constructed dictionary events;aggregating events;aggregation events;events aimto aggregation;provoking events aimto;events;emotion;emotion person", "pdf_keywords": ""}, "21c39ce886dc38dd2006ea25d6bd1eff4cdba0b8": {"ta_keywords": "latent dirichlet allocation;political blogs;political blog posts;online political blogs;topic models;response political blog;posts topic models;political blog;political blogs extend;topic models paper;blogs extend latent;discussions online political;dirichlet allocation;latent dirichlet;blogs;blog community verbal;extend latent dirichlet;contents blog community;blog posts topic;blog posts;blog community;blog;blogs extend;introductionpredicting response political;contents blog;online political;dirichlet allocation blei;model discussions online;optionally contents blog;discussions online", "pdf_keywords": ""}, "51321a60f5ec2c80253394ef86e8b5fcc768f52a": {"ta_keywords": "entities integrating databases;integrating databases;data integration;integrating databases web;learning match cluster;information extraction methods;information extraction;data integration methodsparticipants;using information extraction;data integration determining;techniques clustering;sets data integration;databases web;databases web obtained;clustering;entities integrating;match cluster;match cluster large;databases;data sets data;high dimensional data;dimensional data sets;objects different databases;world entities integrating;data sets;cluster large;similarities textual names;cluster;different databases;determining sets identifiers", "pdf_keywords": ""}, "6954a6bb9d6f3e365b26b694c963ae1d62a03444": {"ta_keywords": "introductiontranslation powerful;text understanding inefficient;introductiontranslation powerful model;introductiontranslation;fastformer efficient transformer;text understanding;fastformer efficient;propose fastformer efficient;attention resultsin fastformer;transformer acceleration inefficient;powerful model text;inefficient long sequences;methods transformer acceleration;text;efficient transformer;paper propose fastformer;transformer acceleration;based additive attention;model text understanding;additive attention;model text;fastformer;efficient transformer model;transformer;long sequences effective;additive attention resultsin;input sequence length;long sequences;length methods transformer;sequence length methods", "pdf_keywords": "text understanding inefficient;effectively capture contextual;attention mechanism summarize;additive attention based;additive attention handle;additive attention uses;long text modeling;based additive attention;context information efficient;uses additive attention;propose additive attention;additive attention;fastformer natural language;contextual information fastformer;contexts token representations;additive attention resultsin;learn contextual;attention uses additive;learn contextual information;attention based transformation;performance long text;attention resultsin fastformer;additive attention mechanism;short text modeling;attention handle long;capture contextual information;attention based;capture contextual;effective text modeling;text understanding"}, "03006aefccdd0c5c6736ab11ed574d02ba1cc086": {"ta_keywords": "neural machine translation;resource machine translation;machine translation models;translation models;machine translation nm;machine translation;augmentation methods translation;translation models language;alleviate issues translation;monolingual data;machine translation aim;syntactic divergence low;translation nm standard;translation nm;monolingual data help;use monolingual data;translation make possible;translation fails;translation fails extreme;issues translation fails;divergence low resource;models language pairs;handling syntactic divergence;language pairs data;translation;issues translation;use monolingual;data augmentation;data augmentation methods;translation make", "pdf_keywords": "neural machine translation;monolingual target corpus;resource machine translation;monolingual english corpus;machine translation;monolingual target sentences;machine translation wom17;parallel corpus;machine translation wm17;target language sentences;sentences bilingual lexicon;machine translation models;machine translation development;sourceordered target sentences;bilingual lexicon propose;bilingual lexicon;pseudo parallel corpus;conference machine translation;machine translation artificially;translation low resource;parallel corpus method;english corpus;language sentences ordered;reordered sentences source;target corpus;translation models;effective translation;sentences bilingual;reordering monolingual;source order monolingual"}, "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269": {"ta_keywords": "protein language model;language models code;general protein language;protein language;synthesizing programs docstrings;programs docstrings model;large language models;language models;messageevaluating large language;correctness synthesizing programs;language model finetuned;docstrings model;language model;codex general protein;docstrings model solves;docstrings;programs docstrings;study python code;models code challenging;large language;github copilot humaneval;synthesizing programs;models code;functional correctness synthesizing;python code writing;github copilot;code github study;correctness synthesizing;github study python;python code", "pdf_keywords": "code generation models;automated code generation;code generation tools;code generation;code generation systems;code generation model;code generation provide;code github models;code generation associated;ways code generation;capable generating code;language models generate;code synthesis generation;docstringconditional code generation;perform code generation;use code generation;accurate code samples;systems code generation;development large language;challenges code generation;generate code fundamental;code generation development;code codex powerful;codex generate code;meaningful source code;software code writing;code generation report;development code generation;learning code wild;using code generation"}, "b5a667bf189a0cfda22bac702d97b601ae6adb6f": {"ta_keywords": "quantum quantum optimization;quantum optimization;quantum optimization emerging;objectives quantum quantum;optimistic gradient descent;objectives quantum;background objectives quantum;free optimistic gradient;optimistic gradient;quantum quantum;gradient free optimistic;gradient descent ascent;quantum;gradient descent;algorithm solving convex;ascent algorithm solving;descent ascent algorithm;ascent algorithm;reshuffling based gradient;convex concave min;optimization emerging;descent ascent;concave min max;robustness strategically adversarially;based gradient free;optimization;concave min;solving convex concave;algorithm enjoys convergence;optimization emerging key", "pdf_keywords": "optimistic gradientient descent;gradientient descent ascent;optimistic gradient descent;gradientient ascent descent;distributionally robust optimization;gradientient ascent;gradient descent ascent;free optimistic gradient;learning distributionally robust;optimistic gradientient;gradientient descent;method optimistic gradientient;studied optimistic gradientient;optimistic gradient;distributionally robust learning;omistic gradientient ascent;gradient free optimistic;robustness strategically adversarially;learning problem robustified;gradient descent;decisiondependent distributional robust;max optimization emerging;learning distributionally;descent ascent algorithm;ascent algorithm finite;literature robust optimization;ascent descent algorithm;ascent algorithm;robust optimization;min max optimization"}, "26cc9e13a7a76e3cf5f9885d08cdafabd6fbd7ec": {"ta_keywords": "computational social choice;tournaments fundamental computational;tournament solution sets;reasoning tournaments;tournaments theoretical;sets reasoning tournaments;tournament solution;reasoning tournaments theoretical;tournaments theoretical results;study tournaments fundamental;tournaments fundamental;published tournament solution;tournaments;study tournaments;computational social;tournament;fundamental computational social;science study tournaments;published tournament;results published tournament;social choice;background computational social;social choice results;economics social choice;social choice political;social choice comsoc;choice political science;computational;computer science economics;choice political", "pdf_keywords": "tournaments statistical modeling;optimum prediction tournaments;random tournament model;prediction tournaments;tournaments statistical;tournament seedings studied;prediction tournaments difficult;world tournaments statistical;tournament model;tournaments theoretical results;tournaments accuracy model;random tournament;tournament datasets;tournament model support;tournaments theoretical;tournament seedings;tournaments probability winning;pairwise tournament datasets;tournament seedings particular;tournament problem social;manipulation tournament seedings;random tournament problem;seeding knockout tournaments;tournament solution;generate synthetic tournaments;tournament solution sets;tournaments probability;study tournaments fundamental;unform random tournament;tournaments fundamental"}, "ac46562e61cfef6213a915bbb80d1a1a2901542a": {"ta_keywords": "recommendation service researchers;consider recommendation service;technical paper recommendation;recommendation service;multiple information sources;paper recommendation;online data sources;paper recommendation ask;information sources;information sources specifically;data sources;pointers journal papers;instance consider recommendation;daily pointers journal;proliferation online data;recommendation ask extended;technical paper;papers fields survey;pointers journal;online data;consider recommendation;journal papers fields;problem technical paper;service researchers;journal papers;papers fields;data sources opening;daily pointers;recommendation;sources opening", "pdf_keywords": "generating better recommendations;generating reviewer interests;sources reviewer interests;reviewer interests analyze;reviewer interests using;information retrieval;information sources reviewer;reviewer paper sources;recommendations method generating;information retrieval ltering;make recommendations reviewer;electronic information retrieval;information retrieval content;sources recommendation process;interests analyze reviewer;information source reviewers;information formulating recommendation;reviewer interests;recommendation systematically incorporate;improve performance recommendation;recommendation systematically;information retrieval methods;information sources recommendation;query consensus reviewer;recommendation results significantly;data describing reviewers;reviewers make recommendations;data involving reviewer;based information retrieval;better recommendations method"}, "559fdae33f0b7733b80a7dbcb902c79598a0d26e": {"ta_keywords": "transformers strong treatment;introductioncan transformers strong;introductioncan transformers;transformer architecture;treatment effect estimation;transformer;based transformer;based transformer architecture;transformers;transformers strong;framework based transformer;effect estimation problems;estimation problems resultsour;effect estimation;methods applicable covariates;estimation problems methodswe;transformer architecture address;estimation problems paper;estimation problems;strong treatment effect;applicable covariates tabular;resultsour methods applicable;estimation;applicable covariates;resultsour methods;treatment effect;methods applicable;challenging treatment effect;problems resultsour methods;covariates tabular", "pdf_keywords": "treatment effect estimateer;sentence estimated transmembrane;treatment effect estimation;effect estimation tasks;estimation treatment;domain adaptation propensity;adaptation propensity score;domain adaptation;method estimation treatment;transtrained language models;bias improving estimation;probabilistic treatment regularization;treatment effect estimates;treatment effect estimators;estimates effective treatment;estimation treatment effect;effect sentence estimated;estimation cross attention;estimation tasks propose;bias matching weighting;bias corroborates expressiveness;bias matching;sentence estimated;effect estimation patient;training propensity modeling;covariate adjustment learned;estimateer strong treatment;improve domain adaptation;adversarial domain adaptation;bias improving"}, "bf63276c90a803fe0d069ce0a3a4a8236e756363": {"ta_keywords": "inferences panels comic;comic book narratives;artwork dialogue comic;comic book panels;narratives conveyed stylized;stylized artwork dialogue;panels comic;panels comic book;dialogue comic;dialogue comic book;artwork dialogue;thegutter drawing inferences;drawing inferences panels;conveyed stylized artwork;closure driven narratives;comic book;mysteries thegutter drawing;thegutter drawing;narratives conveyed;comic;artwork;drawing inferences;conveyed stylized;content natural images;book narratives;inferences panels;driven narratives conveyed;book narratives objectiveto;narratives objectiveto follow;stylized artwork", "pdf_keywords": "comics interactions text;comics interactions;narrative flow comics;dialogue comics narratives;comics narratives;comics narratives improved;details comics movements;comic comics interactions;image dialogue comics;predicting narrative character;dialogue comics;text image comics;contextvisual narrative combination;contextvisual narrative;style tasks comics;details comics;text artwork comic;understanding narrative flow;dataset comic books;flow comics effectively;predicting narrative;comic books models;narrative flow;understand narratives characters;characterize narratives;crucial predicting narrative;multimodal inputs comic;characterize narratives fundamental;comics effectively;tasks comics explore"}, "83165cf62e62a013c2bad61c98120ccb9a0087ae": {"ta_keywords": "bias peer review;fairness bias peer;peer review sociootechnical;bias peer;peer review;fairness bias;tutorial fairness bias;systems peer review;challenges pertaining bias;peer review resultspeer;review sociootechnical intelligent;peer review methodsthe;2020 tutorial fairness;tutorial fairness;review sociootechnical;bias;intelligent systems peer;sociootechnical intelligent;pertaining bias;sociootechnical intelligent systems;fairness;review resultspeer review;review resultspeer;peer;resultspeer review;systems peer;scholarly research;review backbone scholarly;scholarly research faces;scholarly", "pdf_keywords": ""}, "e8deeebc7ff6315115f01fd70a343d62db202888": {"ta_keywords": "specific lexical resource;domain specific lexical;lexical resource;domain specific glossaries;specific glossaries sophisticated;glossaries sophisticated;glossaries terms;lexical resource useful;glossaries terms provided;glossaries sophisticated general;glossaries;specific glossaries;specific lexical;lexical;form glossaries terms;natural language processing;natural language;performance natural language;form glossaries;introductionenabling domain specific;language processing resources;introductionenabling domain;language processing;represented form glossaries;terms provided sense;individual terms;genus;thesuari highly topical;individual terms paper;terms", "pdf_keywords": ""}, "b778a7c4001898a1c3888577154d747522f16db4": {"ta_keywords": "adversarial loss functions;adversarial losses;adversarial losses training;various adversarial losses;adversarial loss;understanding adversarial losses;adversarial losses decoupling;valid adversarial loss;losses training generative;training generative adversarial;adversarial networks remains;adversarial networks;generative adversarial networks;generative adversarial;understanding adversarial;adversarial;various adversarial;deeper understanding adversarial;functions valid adversarial;valid adversarial;training generative;proposed various adversarial;loss functions loss;loss functions;functions loss functions;losses training;loss functions perform;functions loss;generative;component functions regularization", "pdf_keywords": "evaluate adversarial losses;adversarial losses supervised;adversarial losses apply;adversarial losses empirical;adversarial losses;adversarial losses present;analysis adversarial losses;different adversarial losses;adversarial losses discuss;adversarial losses fundamental;studies adversarial losses;adversarial losses recent;common adversarial losses;evaluating adversarial losses;adversarial losses different;adversarial losses propose;adversarial losses reveals;adversarial losses using;loss functions adversarial;adversarial loss;model adversarial losses;adversarial losses regularization;functions adversarial losses;adversarial losses discriminative;adversarial losses sufficient;use adversarial losses;adversarial loss divergence;method adversarial losses;adversarial loss provide;adversarial losses datasets"}, "0453bab552e83f19dd6ba12061949f128fa9b045": {"ta_keywords": "multiclass semi supervised;semi supervised multiclass;supervised multiclass learning;supervised multiclass;known semi supervised;semi supervised learning;multiclass learning;semi supervised;multiclass learning methods;introductionin multiclass semi;multiclass semi;supervised learning;supervised learning sl;learning methods robust;introductionin multiclass;supervised;multiclass;known labeled examples;classes;classes present data;learning methods;classes particular;known labeled;labeled examples;data known labeled;classes present;classes paper;extension expectation maximization;expectation maximization;examples provided classes", "pdf_keywords": "multiclass semi supervised;semi supervised multiclass;outperforms semi supervised;known semi supervised;supervised multiclass;supervised multiclass learning;seeded semi supervised;semi supervised learning;semi supervised;semi supervised setting;semi supervised variant;extra classes clustering;classes clustering;classification propose;multiclass learning;class exploratory learning;classification;clustering classification tasks;classification propose exploratory;multiclass learning methods;classification tasks propose;classes clustering data;introduction multiclass semi;fully labeled multiclass;seeded semi supervisedwe;classification tasks;labeled multiclass;semi supervisedwe;clustering classification;classes known"}, "b36dc8db9930a785edd55ca30328ace2896523e6": {"ta_keywords": "standardized semantic corpora;semantic corpora;semantic corpora development;corpora development semantic;standardized semantic;standardized annotated corpus;semantic analysis systems;cooperation standardized annotated;corpora development;corpora;corpus allows share;semantic analysis;annotated corpus;way standardized semantic;semantic;annotated corpus corpus;corpus corpus allows;development semantic analysis;semantics;standardized annotated;corpus corpus;corpus;science cooperation;development semantic;institutions semantics;corpus allows;progress science cooperation;different institutions semantics;science cooperation advantageous;semantics basic", "pdf_keywords": ""}, "87eece8d39d1e25ba87550be8b01af32738cbf2c": {"ta_keywords": "talker speech recognition;single talker end;multi talker speech;multi talker automatic;recognize overlapped speech;talker automatic speech;multi talker;multi talker single;single talker speech;channel multi talker;monaural multi talker;talker single talker;single talker;speech recognition architecture;speech recognition large;gap multi talker;talker end end;talker automatic;talker end;speech recognition;overlapped speech resultsthe;overlapped speech;speech recognition systems;automatic speech recognition;resultsthe single talker;automatic speech;talker;talker speech;talker single;speech resultsthe single", "pdf_keywords": ""}, "e2ece7ea0924b4f95f65587973118bea9a44a3d2": {"ta_keywords": "relationships software entities;term relationships software;software entities;relationships entities software;term relationships entities;software entities methodswe;software domain refer;text entities technical;associated text entities;entities software;term relationships;entities software domain;relations examining corpus;relationships software;entities technical domains;coordinate term relationships;entities technical;discovery coordinate term;text entities;software domain;relationships entities;entities suggesting coupling;named entities suggesting;detection coordinate term;associated text;named entities;objects named entities;entities;examining corpus;technical domains", "pdf_keywords": "term relationships software;detection coordinateterm relationships;relationships software entities;text entities underlying;term discovery;relationships entities software;entities classification words;entities software;coordinate term discovery;coordinate term classification;entities underlying;term discovery development;code taxonomy software;software entities;analyze text entities;entities classification;software entities improved;taxonomy software;code taxonomy;relations examining corpus;methods coord corpus;structured language domain;text entities technical;term classification;text entities classification;term relationships;interesting code taxonomy;text entities representingjava;coordinateterm relationships entities;coordinatee term relationships"}, "bbc7e533e5bfb388af1afd85bfb7ba17330cae76": {"ta_keywords": "shore power supply;cascadeh bridge inverter;voltage shore power;high voltage shore;dc bias transformer;bridge inverter current;bias transformer dc;bridge inverter;transformer dc;inverter current eliminated;dc bias suppression;supply based dc;high cascadeh bridge;voltage shore;diabetes bias transformer;power supply based;transformer dc component;bias transformer;inverter current;affected dc bias;based dc bias;current high voltage;shore power;transformer;bias transformer lead;power supply fundamental;dc component high;cascadeh bridge;high high cascadeh;distortion excitation current", "pdf_keywords": ""}, "92cee1e209f2d9a311416b0d9fd8a49b0fbe7df2": {"ta_keywords": "filters learned users;learned information filterers;transfer learned filters;filters learned;learned users collaborative;learned filters;retraining learned information;collaborative learning;transfer learned;learned filters setting;direct transfer learned;learning methods direct;exploit filters learned;learning methods use;learned users;symbolic learning methods;case collaborative learning;retraining learned;collaborative learning resultswe;introductiontransring retraining learned;information filterers;learned information;information filterers aimto;different learning methods;learning methods;symbolic learning;learning;different learning;conclude symbolic learning;introductiontransring retraining", "pdf_keywords": ""}, "956aa64b0d5f5802b98bf551d5bab8993b114fd0": {"ta_keywords": "backgrounddata integration data;backgrounddata integration;data integration soft;good similarity functions;context sensitive similarity;similarity functions crucial;similarity function tff;data integration;integration soft joins;widely used similarity;used similarity function;similarity metrics good;sensitive similarity metrics;similarity metrics;integration data sources;similarity functions;metrics good similarity;used similarity;sensitive similarity;soft joins;soft joins data;backgrounddata;similarity function;similarity;good similarity;integration data;subtasks data integration;data sources;joins data deduping;data sources using", "pdf_keywords": ""}, "f52f7964febd6d6d72aa23505b50d33e1d4ce0aa": {"ta_keywords": "introductionweakly supervised learning;weakly supervised learning;interactive weakly supervised;introductionweakly supervised;weakly supervised;novel labeling rules;labeling rules data;discovering novel labeling;labeling rules;supervised learning wl;labeling rule set;labeling rule;supervised;supervised learning;labeling;novel labeling;label scarcity nonprogramming;supervised learning problem;quality labeling rule;high quality labeling;iteratively automatically discovering;automatically discovering;rules data improve;quality labeling;named prbooost achieves;addressing label scarcity;named prbooost;label;prbooost achieves;learning wl", "pdf_keywords": "rule discovery boosting;prbooost achievesweakly supervised;weakly supervised fully;introduce weakly supervised;weakly supervised text;wistarweakly supervised learning;labeling rules enhance;interactive weakly supervised;fully supervised approaches;generate weakly labeled;weakly supervised baselines;labeling rules data;weakly supervised;labeling rules generate;annotating rules iterative;backgroundweakly supervised learning;weakly supervised learning;novel labeling rules;interactive wistarweakly supervised;weakly labeled data;annotating rules reducing;wistarweakly supervised;rules boosting;rules boosting based;supervised fully supervised;standard weakly supervised;labeling rules;rules interactive weaklysupervised;backgroundweakly supervised;fully supervised"}, "60f3e69e4f18e8e8e7dcc4ba66c1e216b49ad982": {"ta_keywords": "common sense game;common sense knowledge;collect common sense;sense game designers;understanding common sense;common sense;sense knowledge acquisition;common sense consists;engine common sense;sense knowledge;sense game;knowledge acquisition gecka;games methods gecka;background commonsense knowledge;game engine common;commonsense knowledge representation;commonsense knowledge;task game engine;knowledge acquisition;game engine;language understanding common;knowledge;reasoning key tasks;task game;game designers;understanding common;knowledge representation;tasks natural language;knowledge representation reasoning;sense consists information", "pdf_keywords": ""}, "09b87b6e7bfbf66d355574d292586595e0185d6e": {"ta_keywords": "typingological knowledge bases;typingological knowledge;introduction typingological knowledge;introduction typingological;typingological;typological kbs;knowledge bases ks;information linguistic properties;lingual transfer learning;linguistic probing;information linguistic;populated sense languages;linguistic properties;linguistic probing major;adoption typological kbs;linguistic properties world;languages shown useful;including cross lingual;contain information linguistic;lingual transfer;linguistic;sense languages;lingual;learning linguistic probing;knowledge bases;transfer learning linguistic;learning linguistic;cross lingual transfer;languages;cross lingual", "pdf_keywords": "predicting typological features;typological feature prediction;prediction typological features;typoological knowledge bases;predicting typological;task prediction typological;typological features multilingual;concerned predicting typological;prediction typological;typological features world;typoological knowledge;world languages task;task typological feature;development typoological knowledge;features world languages;knowledge bases;knowledge base;typological features;typological feature;current knowledge base;shared task typological;predict accuracy language;language models;task typological;knowledge base development;features multilingual language;languages world;task concerned predicting;language modelling;features multilingual"}, "5b16d138bf16762d43b55b6e21d9b0b61021180e": {"ta_keywords": "etiology malignant tumors;tumors understood etiology;understood etiology malignant;etiology malignant;malignant tumors understood;reported etiology malignant;malignant tumors;tumors poorly understood;tumors understood;tumors;malignant tumors poorly;malignant;tumors poorly;etiology;understood etiology;recently reported etiology;reported etiology;poorly understood;understood recently reported;understood;poorly understood recently;understood recently;reported;poorly;recently reported;recently", "pdf_keywords": ""}, "b15ea460c77a4ee8aa159a30ab0331deedfcf392": {"ta_keywords": "layer large language;large language models;language models greatly;experts base layer;training inference routing;language models;layers methods sparse;methods sparse layers;sparse layers methods;learn balanced routing;sparse layers;large language;assignment experts base;capacity sparse layers;training inference;experts base;inference routing;expert modules;specialized expert modules;use available experts;methods sparse;sparse layers dramatically;expert modules contain;sparse;parameters difficult learn;efficiency training inference;available experts existing;layers;high capacity sparse;experts existing approaches", "pdf_keywords": "sparse experts language;training large sparse;layer large language;efficient sparse experts;sparse experts efficient;large language models;use sparse experts;large sparse experts;performance sparse experts;training inference routing;sparse experts highly;sparse experts effective;experts efficient sparse;effective sparse experts;sparse experts;experts language models;sparse experts morein;sparse experts limited;large sparse models;language models greatly;sparse experts sparse;languages use sparse;experts effective sparse;capacity sparse layers;efficient training large;experts sparse;learning language models;limited complexity training;train large languages;sparse models future"}, "bbb7eb10c45cabaee6e427242fce7180c0217ef1": {"ta_keywords": "represent program using;represent programs way;abstract syntax tree;learning programs represent;syntax tree;represent program;syntax tree ost;represent programs;programs represent programs;introductionpredicting program properties;learning programs resultsthe;learning programs;idea represent program;programs represent;paths abstract syntax;program using paths;program properties;introductionpredicting program;program using;programming;abstract syntax;program properties names;path based representation;programs way;programs way facilitates;program;syntax;challenge learning programs;representation learning programs;programs", "pdf_keywords": ""}, "cbf941fef87830efa4de98455cfe943917909b66": {"ta_keywords": "superlinear convergence classical;quasi newton methods;local superlinear convergence;superlinear convergence;rates superlinear convergence;methods convex broyden;newton methods convex;classical quasi newton;analysis local superlinear;newton methods described;convergence classical quasi;methods convex;quasi newton;newton methods;ofhessian approximation;trace inversehessian approximation;inversehessian approximation;introduction rates superlinear;determinant ofhessian approximation;local superlinear;broyden class methods;convex broyden;ofhessian approximation trace;convex broyden class;approximation trace inversehessian;convergence classical;rates superlinear;superlinear;classical quasi;approximation trace", "pdf_keywords": "methods superlinear convergence;method superlinear convergence;methods converge superlinearly;superlinear convergence faster;superlinear convergence quasi;newton methods superlinear;quasi newton methods;newton method superlinear;superlinear convergence scheme;superlinear convergence achieved;rate superlinear convergence;superlinear convergence rate;convergence quasi newton;superlinear convergence good;superlinear convergence significantly;quasi newton method;superlinear convergence;generalization quasi newton;calculation superlinear convergence;superlinear convergence fundamental;superlinear convergence standard;method quasi newton;rates superlinear convergence;gradient method superlinear;methods superlinear;methods rate superlinear;method generalization quasi;converge superlinearly corresponding;classical quasi newton;analysis quasi newton"}, "d8551a4b49aa547ad8884ba9f545480860fcadd1": {"ta_keywords": "predict material response;material modeling;predict material;material modeling work;mechanical responses materials;measurements predict material;implicit fourier neural;problem material modeling;fourier neural;material response using;material response;fourier neural operator;deep neural operator;neural operator architecture;modulating mechanical responses;deep neural;material heterogeneity defects;responses materials fundamental;novel deep neural;mechanical responses;neural operator;materials fundamental;modeling directly;driven modeling directly;responses material heterogeneity;modulating mechanical;responses materials;material heterogeneity;driven modeling;neural", "pdf_keywords": "materials modeled learning;material response modeling;learning material response;materials modeled;materials learning;heterogeneous material modeling;predicting material responses;material responses prediction;material modeling;learning material responses;materials learning task;predict material responses;modeling heterogeneous material;paradigm material modeling;function material response;material model solution;predicting material;material responses driven;material modeling integrates;network fourier neural;response materials modeled;predict material;heterogeneous material responses;model heterogeneous material;material model;implicit fourier neural;fourier neural operators;loading neural operators;structural response materials;conditions predict material"}, "cee25a535ec7165eae38f498a391050077ad9f65": {"ta_keywords": "speaker clustering sampling;speaker clustering using;based speaker clustering;speaker clustering;sampling based speaker;clustering using utterance;utterance oriented dirichlet;dirichlet process mixture;utterance oriented speaker;clustering sampling;clustering sampling based;infinite mixture model;mixture model evaluation;model based speaker;introductiona sampling based;process mixture model;mixture model;mixture model applied;introductiona sampling;non parametric bayesian;parametric bayesian modeling;dirichlet process;using utterance oriented;methodsan infinite mixture;bayesian modeling implemented;sampling based;oriented speaker;bayesian modeling;speakers purpose framework;estimate number speakers", "pdf_keywords": ""}, "7668b23aadf43bebe5e2d3abf37938b44bd16200": {"ta_keywords": "visual question answering;language groundable visual;question answering vq;webqa challenging;webqa challenging new;visual representation learning;representation learning knowledge;introduce webqa challenging;question answering;visual representations;visual representation;learning knowledge aggregation;groundable visual representations;knowledge aggregation language;aggregation language generation;visual representations novel;advances visual representation;visual;knowledge aggregation;web searches;advances visual;answering vq;challenging new benchmark;fundamental advances visual;aggregation language;introduce webqa;web searches requires;visual question;learning knowledge;representation learning", "pdf_keywords": "rtasatusruitk wtanhaibcatha festival;wtanhaibcatha festival fes;winter trees festival;trees festival japan;festival syonan decorated;wtanhaibcatha festival;tanabata festival masskruege;festival fes winter;oktoberfest tpiinlvaacledss aoirnemjhapeplldaan;oktoberfest tpiinlvaacledss;tanabata festival;sendai tanabata festival;festival japan;festival japan large;oktoberfest related;trees festival castle;hiratsuka oktoberfest related;trees festival;oktoberfest related workthe;tanabata fes winter;festival masskruege mugs;background oktoberfest tpiinlvaacledss;colorful streamers decorated;oktoberfest hiratsuka oktoberfest;hiratsukatanabata decorated lights;oktoberfest hiratsuka;syonan hiratsukatanabata decorated;decorated thousandisn summer;festival syonan;festival masskruege"}, "c688e187cede868e35fc1b53913e0fbbe6e38ea0": {"ta_keywords": "structured prediction paradigm;structured prediction;structured prediction algorithm;framed structured prediction;based structured prediction;imitation learning paradigm;imitation learning;search based structured;introductionin imitation learning;paradigm algorithms learn;learning paradigm algorithms;structured;algorithms learn;prediction algorithm searn;dataset aggregation algorithm;prediction paradigm;learn expert demonstrations;introductionin imitation;proposed dataset aggregation;2009 framed structured;prediction paradigm developed;based structured;natural language processing;dataset aggregation;algorithm searn;algorithms learn expert;learning paradigm;prediction algorithm;language processing;expert demonstrations order", "pdf_keywords": ""}, "2448e63a7bb626d09001fe37e60befdb2919f6e6": {"ta_keywords": "commonsense knowledge;introduction commonsense knowledge;commonsense knowledge useful;sense knowledge web;knowledge web scale;knowledge web;readable knowledge bases;common sense knowledge;machine readable knowledge;commonsense facts world;knowledge bases;extract common sense;sense knowledge;commonsense facts;graph based markov;readable knowledge;basic commonsense facts;knowledge useful;language models sources;basic commonsense;web search;knowledge useful making;language models;knowledge bases lack;introduction commonsense;web scale language;lack basic commonsense;commonsense;making web search;knowledge", "pdf_keywords": ""}, "cee96ee69adacfdeb648c230d2c9b01011724724": {"ta_keywords": "etiology disease patient;diagnosed etiology;patient diagnosed etiology;diagnosed etiology disease;etiology disease;patient developed etiology;developed etiology disease;etiology disease poorly;etiology;disease poorly understood;disease patient diagnosed;developed etiology;disease patient;disease;patient diagnosed;diagnosed;case patient developed;patient developed;disease poorly;case patient;understood case patient;patient;poorly understood case;understood case;case;poorly understood;developed;understood;poorly", "pdf_keywords": ""}, "007371feab4af758b74580c43e74827b3500c67e": {"ta_keywords": "distributed video demand;distributed video;video demand streaming;framework distributed video;demand streaming;video demand;streaming theory design;streaming;demand streaming theory;demand streaming formulating;distributed implementation;streaming formulating optimization;distributed implementation highly;highly distributed implementation;framework distributed;streaming theory;distributed;bandwidth node;link bandwidth node;bandwidth;general framework distributed;streaming formulating;node resource constraints;link bandwidth;implementation highly scalable;highly scalable resilient;disk space network;scalable resilient;highly scalable;highly distributed", "pdf_keywords": ""}, "2a0cb1a1e78b77fe9981e4935410cf3ea900e370": {"ta_keywords": "speech recognition japan;speech recognition;development speech recognition;speech recognition challenging;recognition japan discuss;recognition japan;development speech;speech;japan discuss results;recognition;japan discuss;recognition challenging task;recognition challenging;japan;article development speech;workshop;discuss results workshop;results workshop;task article development;development;challenging task article;task article;task;challenging task;article development;discuss results;discuss;article;challenging;results", "pdf_keywords": ""}, "d60b4594fb0404329d9ebf6fd88702ca3479e904": {"ta_keywords": "extraction abbreviations;extraction abbreviations biomedical;based extraction abbreviations;hmm matching abbreviations;extracting abbreviation;extracting abbreviation definitions;abbreviations biomedical text;algorithm extracting abbreviation;matching abbreviations;matching abbreviations definitions;abbreviations biomedical;abbreviations;abbreviation definitions biomedical;hmm based extraction;abbreviations definitions report;alignment hmm based;alignment hmm matching;abbreviations definitions;based alignment hmm;biomedical text;abbreviation definitions;biomedical text approach;definitions biomedical text;abbreviation;biomedical text methods;alignment hmm;approach based alignment;objectives alignment hmm;recall standard data;definitions biomedical", "pdf_keywords": ""}, "99546b4d1f2547095bb15eec36e03f64b74a78d4": {"ta_keywords": "china markets following;china markets;squeeze china markets;markets following;markets following event;stock trading injanuary;trading injanuary 2021;high risk options;markets;stock trading;backgroundwallstreetbets wb reddit;risk options stock;discusses high risk;options stock trading;users activity increased;trading;trading injanuary;risk options;options stock;attracted worldwide;2021 attracted worldwide;high risk;stock;risk;backgroundwallstreetbets wb;reddit community primarily;short squeeze china;activity increased exponentially;activity increased;backgroundwallstreetbets", "pdf_keywords": ""}, "5e3d1bece9dd2356fd2b31312bd62c8f7126882d": {"ta_keywords": "utility privacyy policy;privacyy policy policy;policy policy policy;policy policy;privacyy policy;utility privacyy;policy;utility;privacyy", "pdf_keywords": ""}, "b990331a5394f3642a1fd1791d70bfa2d85d9d1d": {"ta_keywords": "tweets discussed vaccination;tweets posted covid;vaccine related content;twitter covid;twitter covid 19;vaccine hesitancy narratives;shared twitter covid;tweeted web domains;covid 19 conversations;content including tweeted;tweets discussed;conversations include tweets;include tweets discussed;tweets posted;content dynamics conspiracy;posted covid 19;tweeted web;screened million tweets;discussed vaccination identified;including tweeted web;include tweets;discussed vaccination;million tweets posted;background vaccine related;spread anti vaccine;tweets;domains urls tweeted;posted covid;background vaccine;vaccine related", "pdf_keywords": ""}, "8b98f7ff3bb1b199db85fc219a5c27b355adf1be": {"ta_keywords": "laser osseous crown;osseous crown lengthening;technique osseous crown;alternative osseous crown;crown lengthening typically;crown lengthening;outcome erbium laser;using laser osseous;laser enables clinician;crown lengthening includes;erbium laser enables;osseous crown;laser osseous;crown lengthening negating;erbium laser;advantages using laser;using laser;present technique osseous;laser enables;aesthetic outcome erbium;surgery procedure;invasive alternative osseous;surgery procedure frequently;involves flap surgery;flap surgery procedure;laser;lengthening includes hemostasis;crown;technique osseous;flap surgery", "pdf_keywords": ""}, "a604ad4654f31d325b888806e276123a704cb5c8": {"ta_keywords": "minimum error classification;support vector machine;classification robustness conventionalmce;increase classification robustness;classification robustness;error classification;machine svm methods;svm methods;svm methods realize;vector machine svm;support vector;error classification various;margin maximization support;machine svm;svm;maximization support vector;vector machine;classification;classification tasks derive;geometric margin maximization;classification tasks;errorror mce training;increase classification;classification various;wide range classification;margin maximization;achieve minimum error;minimum errorror mce;high robustness wide;minimum error", "pdf_keywords": ""}, "652e3c774da47c0c8788111ec886a00d3b8fc637": {"ta_keywords": "maximise tumour removal;navigation maximise tumour;partial tumour removal;tumour removal simultaneously;location remaining tumour;tumour removal continuing;tumor resection requires;tumour removal;tumor resection;background tumor resection;maximise tumour;remaining tumour partial;remaining tumour;meshless total lagrangian;tumour partial;lagrangian explicit dynamics;tissues neurosurgeons need;parenchyma tumour cerebrospinal;includes parenchyma tumour;tissues neurosurgeons;tumour cerebrospinal fluid;tumour partial tumour;explicit dynamics solver;tumour cerebrospinal;removal continuing resection;dynamics solver problem;partial tumour;neurosurgeons need;neurosurgeons;dynamics solver", "pdf_keywords": ""}, "d0ea87ce3bcd86428d379fd478c365c64f870200": {"ta_keywords": "dialogue state tracking;dialogue systems linguistic;schemas evaluate dialogue;dialogue systems;robustness dialogue systems;dialogue state;measuring robustness dialogue;schemaa guddialdial dataset;evaluate dialogue state;linguistic variations schemas;robustness dialogue;dialogue;clinical messagethe schemaa;systems linguistic;evaluate dialogue;systems linguistic variations;messagethe schemaa guddialdial;messagethe schemaa;schemaa guddialdial;guddialdial dataset introduced;generalizes schema variations;guddialdial dataset;schema variations measured;schema variations;generalizes schema;training use schemas;schemaa;linguistic;linguistic variations;schema", "pdf_keywords": "schemaguided dialogue models;dialogue models schema;dialogue state tracking;dialogue systems schema;dialogue modeling pipeline;schema guided dialogue;dialogue model pipeline;robustness dialogue systems;measuring robustness dialogue;improving schema robustness;guided dialogue modeling;schema guided dialog;dialogue systems linguistic;schemaguided dialogue;dialogue models;robust schema variations;dialogue systems development;robust schema changes;models robustness dialogue;guided dialogue model;dialogue modeling;evaluating dialogue systems;augmenting dialogues human;dialogue systems;topperforming dialogue state;database schemaguided dialogue;based augmenting dialogues;linguistic variations schemas;evaluation dialogue systems;based schema augmentation"}, "429b65937d4922578a81e1f0ef5aeab7361ae36b": {"ta_keywords": "n2bash corpus semantic;parser natural language;n2bash corpus;semantic parser natural;corpus semantic parser;semantic parser;sentences bash commands;bash commands n2bash;natural language interface;semantic parser methods;commands n2bash;parser;data semantic parser;parser natural;scripting;natural language;commands n2bash long;specific scripting simply;specific scripting;scripting simply;bash commands;commands;scripting simply stating;parser methods;corpus semantic;sentences bash;language interface;n2bash;mapping english sentences;bash", "pdf_keywords": "programming natural language;processing command commands;sentences bash commands;based approach nl2bash;process natural language;natural language bash;bash commands nl2bash;command structures decoder;approach nl2bash;natural language code;commands nl2bash;natural language programming;utilities natural language;text command pairs;command structures;commands user friendly;command command significantly;command structure;sentence command;commands nl2bash methodswe;language programming;language bash mapping;description sentence command;language language processing;command commands;semantic parsing;task bash programmers;language processing;scripting;parallel natural language"}, "ba45a346690f3c5b6f8c371b5c6cf1d7cce5619d": {"ta_keywords": "backgroundblind identification lifting;constrained rank minimization;backgroundblind identification;rank minimization;rank minimization problem;backgroundblind;constrained rank;identifying arx model;identification lifting;arx model output;task identifying arx;identification lifting known;identifying arx;arx model;relaxed convex formulation;minimization;minimization problem present;minimization problem;convex formulation approximate;convex formulation;posed problem assumptions;phrase constrained rank;relaxed convex;identification;arx;problem posed assume;formulation approximate solution;present relaxed convex;model output measurements;problem posed", "pdf_keywords": "known subspace input;input known subspace;subspace input known;identification arx models;identification input nonlinearity;constrained rank minimization;method blind identification;identification arx model;blind identification input;framework blind identification;rewritten rank minimization;known subspace desired;approach blind identification;rank minimization;method identification nonconvex;rank minimization problem;method applied identification;known subspace;subspace input;applied identification problem;identification nonconvex solution;identification input;blind identification arx;known subspace present;regressive exogenous input;subspace desired input;subspace present method;optimal solution matrices;identifying auto regressive;problem convex relaxation"}, "65c2a39f1579a947926ac5746888445ea4afdf6e": {"ta_keywords": "free grammar cfg;grammar cfg based;context free grammar;grammar cfg;grammar induction focus;methods grammar induction;grammar induction;free grammar;grammar induction benefit;modeling lexical dependencies;formalisms lexicalized pcfgs;lexicalized pcfgs;cfg based methods;syntactic formalisms lexicalized;disparate syntactic formalisms;lexical dependencies;based methods grammar;grammar;syntactic formalisms;current methods grammar;methods grammar;lexicalized pcfgs plagued;context free;cfg based;discovering constituents dependencies;disparate syntactic;modeling lexical;cfg;formalisms lexicalized;syntactic", "pdf_keywords": "context free grammars;unsupervised grammar induction;grammar induction model;dependency structure grammar;context free grammar;based grammar induction;grammar induction discussed;free grammars;development unified grammar;grammar unified grammar;free grammar cfg;unified grammar;free grammars parameter;formalism unified grammar;grammar induction focus;grammar induction;grammar unified;generating syntactic;unified grammar unified;grammar induction propose;grammar cfg based;constituent based grammar;methods grammar induction;free grammar;lexicalized grammar rules;unified grammar propose;neural constituency parsing;unsupervised grammar;unified grammar fundamental;modeling lexical dependencies"}, "562f33611cdc0d8ed6609aa09f153e6238d5409e": {"ta_keywords": "missing data sequences;clinical time series;missing data sequential;modeling missing data;classification clinical time;data sequences rns;multivariate time series;missing data;data sequences;cope missing data;modeling missing;classification diagnoses;data sequential;improvedd classification clinical;classification clinical;classification diagnoses given;sequences rns improvedd;time series observations;time series;data sequential inputs;rns improvedd classification;sequences rns;clinical time;multilabel classification diagnoses;consists multivariate time;intensive care unit;time series resultscolected;medical center data;given clinical time;multivariate time", "pdf_keywords": "learn missing data;training models missingness;multilabel classification diagnoses;classification diagnoses;missing data indicators;sequence missingness indicators;classification diagnoses use;missing data sequential;classification diagnoses given;models missingness indicators;missing data features;provide diagnosis classification;diagnosis classification;missingness indicators improve;missingness indicators;predicting clinical;data missingness;diagnosis classification performance;data missingness characteristics;missingness patterns temporally;observations missingness patterns;missingness indicators demonstrated;missing data information;diagnostic assistant predict;automatically detect missing;present data missingness;predicting clinical decisions;detect missing values;missing data;detect missing"}, "a36f7d5d8f724168e534925edff97b3680e545c9": {"ta_keywords": "tensor contractions neural;contractions neural network;incorporate tensor contractions;contraction layers useful;contraction layers;use tensor contractions;tensor contractions;10sor contraction layer;introductiontensor contraction layers;activation tensors;tensor contractions end;deep nets;deep nets paper;contraction layer;parsimonious deep nets;neural network layers;activation tensors methodsspecifically;apply activation tensors;end trainable neural;contractions neural;tensors;trainable neural network;contraction layer cl;trainable neural;incorporate tensor;tensor;use tensor;neural network;10sor contraction;network layers", "pdf_keywords": "activation tensor network;complexity activation tensor;tensor contraction layer;layers tensors neural;activation tensors;tensor contraction activation;tensors neural network;tensor network;activation tensor;contraction activation tensor;activation tensor improve;layers tensors;tensor contractions neural;tensors neural;dimensionality activation tensor;tensor network demonstrate;apply activation tensors;activation tensors methodswe;connected layers tensors;tensor contraction convolutional;structure activation tensor;contractions neural network;layer performs tensor;activation tensor yield;incorporate tensor contractions;introducing tensor contraction;tensor factorization neural;representation learning tensor;neural networks end;adding tensor contraction"}, "c2ff76c75acc777e005360e9d4c4d928d95c0432": {"ta_keywords": "reliability regenerating codes;reliability redundant storage;regenerating codes;failure redundant storage;regenerating codes including;overview regenerating codes;redundant storage;redundant storage data;regenerating codes new;storage nodes;storage data distributed;codes minimize repair;individual storage nodes;storage nodes prone;ensure reliability regenerating;reliability regenerating;minimize repair bandwidth;introductionin storage;storage individual storage;ensure reliability redundant;nodes ensure reliability;storage data;repair bandwidth retaining;individual storage;storage;reliability redundant;introductionin storage individual;data distributed;repair bandwidth;data distributed manner", "pdf_keywords": ""}, "6c3b8e65dc45cb62172f9425dcff4c48055d47eb": {"ta_keywords": "food social media;language food social;language food;food related posts;twitter demonstrate latent;anyzing language food;social media methods;food social;power language food;food related;corpus million food;posts twitter;related posts twitter;twitter;million food related;social media objectives;social media;predicted data overweight;language based models;food;posts twitter demonstrate;latent population;million food;demonstrate latent population;language;latent population characteristics;predictive power language;latent;twitter demonstrate;predictive", "pdf_keywords": "food related tweets;food tweets;food social media;food tweets identify;using tweets predict;tweets use prediction;twitter data;twitter data people;tweets predict;tweets predict latent;analysis twitter posts;analyze data twitter;tweets model;twitter posts data;twitter dataset;tweets region predictive;collected tweets;analyzes tweets;analyze tweets;themes food tweets;tweets locatedthe language;analyze social media;twitter study;analysis twitter;analysis data twitter;data obtained twitter;world using tweets;data twitter dataset;collected tweets locatedthe;analyze tweets twitter"}, "5e00596fa946670d894b1bdaeff5a98e3867ef13": {"ta_keywords": "vision language pretraining;modeling visual textual;visual textual representations;language pretraining;language pretraining vlp;textual representations vision;representations vision language;image captions regional;captions regional labels;captions regional;multimodal downstream tasks;visual textual;annotations including;expensive annotations;annotations;expensive annotations including;vision language;textual representations;image captions;clean image captions;multimodal downstream;captions;requirement expensive annotations;multimodal;annotations including clean;performance multimodal downstream;impressive performance multimodal;performance multimodal;pretraining vlp;modeling visual", "pdf_keywords": "vision language pretraining;visual language tasks;visual language model;minimalist pretraining framework;vision language models;vision language learning;visual language learning;language pretraining model;visual language;simple visual language;visual language representation;joint visual language;language pretraining challenging;language pretraining;vision language representation;development vision language;visual text language;pretain vision language;vision language;language understanding visual;approach visual language;framework vision language;demonstrate vision language;minimalist pretraining;visual language understanding;vision language context;simplifies visual language;pretraining framework;present minimalist pretraining;visual language processing"}, "96bb4b49f69419c31857e928969fcaa137e15060": {"ta_keywords": "review based qa;selecting relevant reviews;available product reviews;product reviews;product reviews propose;review based;questions amazon product;post questions amazon;informational retrieval;task review based;questions amazon;reviews propose task;relevant reviews;reviews;review;informational retrieval techniques;knowledgeable customer answer;combines informational retrieval;retrieval techniques selecting;retrieval techniques;amazon product pages;based qa objective;based qa;qa;task review;amazon product;customer answer question;amazon;customers post questions;reviews propose", "pdf_keywords": "question answering dataset;question answering datasets;community question answering;user review dataset;question answering task;question answering;large question answering;answering user review;review text data;question answering provide;based question answering;answerable based reviews;review dataset;review dataset propose;answering dataset interesting;answering datasets compare;answering datasets;question review text;review snippets generate;selecting relevant reviews;natural language answers;review snippets;world question answering;learning answer domainthe;answering dataset;relevant review snippets;review snippets question;query relevant review;question answering user;question review"}, "2873f78efd7adcb118a70f8ea3ca7fa1501e320a": {"ta_keywords": "shot relation classification;fewrel dataset;relation classification;relation classification models;build fewrel dataset;fewrel dataset adding;construct fewrel;fewrel challenging task;methodsto construct fewrel;detect nosta relations;nosta relations methodsto;nosta relations;fewrel challenging;present fewrel challenging;fewrel;fewrel build fewrel;relations;present fewrel;shot relation;relations methodsto;build fewrel;relations methodsto construct;construct fewrel build;relation;fewrel build;aspects shot relation;introductionwe present fewrel;domain handful instances;investigate aspects shot;classification", "pdf_keywords": "shot relation classification;shot detection fewshot;classes shot detection;method shot learning;shot learning;use shot learning;challenging shot relation;shot learning based;shot relation;shot learning shot;learning shot learning;shot learning emerged;learning shot;used shot learning;fewshot nota queries;relation classification;detection fewshot;shot learning model;relation classification task;relation classification models;detection fewshot nota;years shot learning;buildin shot learning;classes shot;shot detection;shot learning demonstrate;shot learning original;fewrel challenging shot;nonprogramming propose fewrel;relations construct fewrel"}, "3f311aee9d25b0284d21274cfc8706d6f0277f87": {"ta_keywords": "quantization quantizing bitwidths;bitwidths operations deep;deep quantization quantizing;power deep quantization;deep quantization;quantizing bitwidths;quantizing bitwidths maintaining;operations deep neural;bitwidths maintaining accuracy;bitwidths operations;bitwidths achieve accuracy;algorithmic insights bitwidths;quantization quantizing;quantization;bit weights activations;insights bitwidths operations;lower bitwidths;bitwidths;bitwidths maintaining;lower bitwidths achieve;insights bitwidths;bitwidths achieve;neural networks reduced;quantizing;operations deep;bit weights;deep neural networks;accuracy use bit;cases lower bitwidths;use bit weights", "pdf_keywords": "deep quantization activations;deep quantization neural;deep quantization;effectiveness deep quantization;effort deep quantization;algorithm deep quantization;quantizing neural networks;deep quantization domain;approach deep quantization;quantization neural networks;underlying quantized training;deep quantization apply;quantizing neural;framework deep quantization;quantization convnets;quantization neural;approach quantizing neural;quantized training;quantization convnets using;explore quantization hyperparameter;precision quantization convnets;layers results quantization;quantization activations;training technique quantization;deep quantization lead;optimization explore quantization;approach quantization neural;quantization hyperparameter;quantized training technique;quantization hyperparameter space"}, "bcd6cd7bdd661bd86c58b7251ae4633a6ba9979e": {"ta_keywords": "automate conference reviewing;conference reviewing process;conference reviewing;conference paper submissions;assigning conference paper;attempts automate conference;automate conference;reviewers rate keywords;submissions suitable reviewers;conference paper;assigning conference;technical paper recommendation;task requires reviewers;paper recommendation cases;requires reviewers rate;paper recommendation;suitable reviewers;paper submissions;rate keywords;reviewers rate;suitable reviewers viewed;requires reviewers;paper submissions suitable;reviewing process typically;reviewers;reviewing;direct papers greatest;problem assigning conference;reviewing process;conference", "pdf_keywords": ""}, "705794a57cca12c2e58b2d77ac32bd4f92ed31ab": {"ta_keywords": "introductionenhancing russian wordnets;russian wordnets utilizing;russian using crowdsourcing;russian wordnets;thesaurus russian using;crowd yarn russnet;readable thesaurus russian;thesaurus russian;wordnets;wordnets utilizing;using crowdsourcing;crowdsourcing;machine readable thesaurus;wordnets utilizing force;crowd yarn;crowdsourcing aimto;crowdsourcing aimto project;thesaurus;using crowdsourcing aimto;yarn russnet project;introductionenhancing russian;readable thesaurus;russnet project aims;yarn russnet;force crowd yarn;russnet project;russian using;russnet;yarn;russian", "pdf_keywords": ""}, "fd306df2809c7acc19dd1994e8ecb11caa33290d": {"ta_keywords": "approach communication communication;communication context social;new approach communication;communication communication;approach communication;communication;social social interactions;communication communication context;social interactions;communication context;social social;context social social;social;context social;interactions;approach;present new approach;new approach;purpose article present;article present;article present new;present new;new;present;purpose article;context;article;purpose", "pdf_keywords": ""}, "99e56ebc2f3739dfca93d5a92ebc1e6e2a3050d2": {"ta_keywords": "peergrading students grade;peergrading students;employing peergrading students;employing peergrading;peergrading;grading;courses student evaluation;student evaluation;grading technology;students grade;student evaluation large;auto grading;grading technology feasible;students grade number;push employing peergrading;online courses student;students;auto grading technology;present auto grading;large number students;open online courses;number students;students massive open;grade number;online courses;grade assignments present;students massive;grade assignments;numbers students;increasing numbers students", "pdf_keywords": ""}, "65b226f71faaac9b8a4d63445c85601a16635464": {"ta_keywords": "stochastic gradient descent;gradient descent;gradient descent sg;backgroundgradient descent;backgroundgradient descent gd;descent gd stochastic;gd stochastic gradient;methods convex optimization;convex optimization;stochastic gradient;nonconvex optimization;involved nonconvex optimization;performance methods convex;convex optimization problems;nonconvex optimization gap;learning involved nonconvex;backgroundgradient;methods convex;machine learning classical;scale machine learning;descent gd;gd stochastic;optimization gap arisen;gradient;optimization problems notable;optimization;optimization gap;machine learning;descent sg workhorses;machine learning involved", "pdf_keywords": "nonconvex algorithm learning;stochastic gradient descent;gradient descent sgd;minima gradient descent;gradient descent;gradient descent stochastic;gradient descent efficient;gradient descent achieves;gradient descent present;descent stochastic gradient;method nonconvex optimization;nonconvex algorithmic algorithms;algorithms nonconvex;gradient descent order;nonconvex algorithmic;non convex optimization;nonconvex optimization non;algorithms nonconvex algorithm;gradient descent escape;nonconvex optimization gap;nonconvex optimization;nonconvex optimization machine;method nonconvex algorithmic;nonconvex algorithm;algorithmic algorithms nonconvex;gradient descent approach;involved nonconvex optimization;optimization non convex;methods convex optimization;use nonconvex optimization"}, "c6bb9e4a9eaa0f0f8309597af2cefe03bd3f1bb5": {"ta_keywords": "underlying attractor chaotic;attractor chaotic systems;attractor chaotic;distributions chaotic systems;chaotic systems;probability distributions chaotic;distributions chaotic;generative nature chaos;chaos like probability;chaotic systems repeatedly;information underlying attractor;fractal;strange attractors;chaotic systems pose;geometry strange attractors;chaotic;striking fractal;fractal geometry;attractors;chaos;underlying attractor;striking fractal geometry;nature chaos;nature chaos like;attractor;fractal geometry strange;strange attractors underscores;attractors underscores generative;chaos like;backgroundthe striking fractal", "pdf_keywords": "dataset chaotic dynamics;predicting chaotic dynamics;dataset chaotic systems;predicting chaotic;chaotic systems data;known chaotic dynamicsal;developed dataset chaotic;chaotic dynamics resource;dimensional chaotic;dataset chaotic;dimensional chaotic systems;chaotic dynamicsal;low dimensional chaotic;known chaotic;quantify chaos;rarity chaotic attractors;chaotic dynamics used;chaos systems biology;data chaotic systems;chaotic dynamics dynamical;present dataset chaotic;training data chaotic;chaotic attractors;sought quantify chaos;distributions chaotic systems;chaotic attractors space;chaotic dynamicsal systems;method predicting chaotic;data chaotic;chaos systems"}, "ede108538033ae00d1667685afbd488380020613": {"ta_keywords": "sar cov omicron;coronavirus sars cov;sars cov omicron;respiratory syndrome coronavirus;antiviral drugs sar;drugs sar cov;syndrome coronavirus sars;coronavirus sars;antibodies antiviral drugs;antibodies antiviral;cov omicron subvariants;cov omicron subvariant;neutralizing antibodies antiviral;syndrome coronavirus;coronavirus;sar cov;omicron subvariants ba;omicron subvariant ba;sars cov;antiviral drugs;omicron subvariant;omicron subvariants;earlier omicron subvariant;antiviral;subvariant ba variants;cov omicron;efficacies neutralizing antibodies;neutralizing antibodies;acute respiratory syndrome;subvariants ba ba", "pdf_keywords": ""}, "f9f862f48599526147bbb110ba986ff6872ef4b0": {"ta_keywords": "uncertainty indoor trajectories;movement uncertainty indoor;indoor trajectories mobile;modelling movement uncertainty;indoor trajectories;indoor trajectories significantly;trajectories mobile sensing;uncertainty indoor;movement uncertainty;demonstrate movement uncertainty;mobile sensing;mobile sensing data;modelling movement;trajectories mobile;sensing data;data demonstrate movement;approach modelling movement;uncertainty;sensing data demonstrate;trajectories;trajectories significantly improved;sensing;trajectories significantly;movement;indoor;demonstrate movement;mobile;modelling;approach modelling;new approach modelling", "pdf_keywords": ""}, "be0c64252a2c3071236d88feeab47d06ef6e0fb7": {"ta_keywords": "recipient recommendation systems;systems suggesting recipients;recipient recommendation;suggesting recipients message;suggesting recipients;recipients appendibular;recipients appendibular important;recipients methodsthis;email systems enhanced;backgroundrecommending recipients appendibular;backgroundrecommending recipients;recipients methodsthis valuable;recipients;recipients message;specified recipients methodsthis;email systems;specified recipients;work recipient recommendation;recipients message message;previously specified recipients;recipient;addition email clients;email clients;email clients particularly;enhanced work recipient;work recipient;investigate email systems;valuable addition email;addition email;email", "pdf_keywords": ""}, "b661520bf0061b7d96ccf12016e351dd3a6ee780": {"ta_keywords": "importance weighted risk;learning effect importance;importance weighted;importance weighting;weighted risk minimization;importance weighting characterized;effect importance weighting;weighted risk;imbalance policy reinforcement;impacts parameterized deep;risk minimization;context importance weighted;parameterized deep neural;class imbalance policy;parameterized deep;learning algorithms causal;imbalance policy;effect importance;deep neural;inference domain adaptation;policy reinforcement learning;neural networks aim;reinforcement learning effect;learning effect;domain adaptation;adaptation class imbalance;risk minimization key;importance;reinforcement learning;deep neural networks", "pdf_keywords": "deep networks importance;weighting deep learning;importance weighting deep;importance weighting neural;impacts deep nets;neural networks importance;deep learning effects;importance weights restoring;importance weighting regularization;importance weighting diminishes;impacts overparameterized deep;deep learning known;weighting neural networks;overparameterized deep neural;importance weights;weighting useful deep;deep learning differences;weighting diminishes epochs;weighting neural;deep nets early;importance weightings significantly;useful deep networks;overparameterized deep;importance weighting models;gse57979 importance weighting;importance weighting does;importance weightings;importance weighting;networks importance weighting;gradient descent sgd"}, "2cd2df06d488565063e0600ff840d293be2eaf31": {"ta_keywords": "game regret matching;adaptive procedure playing;playing game regret;game regret;regret matching methodsin;regret matching;proportional measures regret;play probabilities;play probabilities proportional;measures regret;current play probabilities;adaptive procedure;adaptive;procedure playing game;adaptive procedure guarantees;simple adaptive procedure;play converge;regret;measures regret having;guarantees probability empirical;strategies past;simple adaptive;shown adaptive procedure;procedure playing;adaptive procedure leading;guarantees probability;play;strategies;procedure guarantees probability;used strategies past", "pdf_keywords": "game regret matching;adaptive procedure playing;correlated equilibria game;strategies correlated regrets;game regret;strategies correlated;playing game regret;aequilibrium game decision;different strategies correlated;based aequilibrium game;strategy shown empirical;adaptive procedure;equilibria game;adaptive procedure guarantees;player strategy;adaptive;regret matching results;distributions ofthe adaptive;aequilibrium game isthe;regret matching;adaptive procedures;strategy player strategy;aequilibrium game;simple adaptive procedure;adaptive procedure called;game method;adaptive procedure simple;procedure adaptive;transition probabilities players;ofthe adaptive procedure"}, "372657f609f5a95b378a1aad7b08deb9b9b510c0": {"ta_keywords": "policy optimization unsupervised;unsupervised model adaptation;based policy optimization;optimization unsupervised model;modeling based policy;model based policy;policy optimization;methodsmodel based policy;model adaptation described;model adaptation;resultsmodel based policy;unsupervised model;optimization unsupervised;unsupervised;adaptation described paper;based policy;model based;modeling based;adaptation;introduction model based;adaptation described;resultsmodel based;methodsmodel based;paper resultsmodel based;paper methodsmodel based;introduction model;policy;described paper resultsmodel;described paper methodsmodel;modeling", "pdf_keywords": "batch reinforcement learning;adaptation policy optimization;model adaptation policy;model estimation policy;based modeling mrna;policy optimization unsupervised;estimation policy optimization;modeling mrna;policy optimization;free reinforcement learning;modeling reinforcement learning;policy optimization compare;mrna based modeling;model free reinforcement;reinforcement learning mlr;improve model adaptation;simulated model adaptation;dynamics model learn;based policy optimization;policy optimization introducing;modeling mrna distribution;actor model adaptation;modeling policy;adaptation model learning;distribution bias mrna;model adaptation;improving model learning;deep mrna based;model adaptation seamlessly;policy optimization apply"}, "a0511f02a867bf19e2fa01e6cbd3663f4bd1b953": {"ta_keywords": "relationship family family;relationship family;literature relationship family;family family article;family article discusses;family family;family article;family;literature relationship;authors literature relationship;relationship;current literature relationship;authors literature;literature;current literature;authors;article;discusses current literature;article discusses;discusses;article discusses current;current;discusses current", "pdf_keywords": ""}, "1ae1850bcfa3c31d7bc828cc33f7dd3926cee26f": {"ta_keywords": "entity discovery linking;discovery linking english;discovery linking;automated entity discovery;entity discovery;knowledge base populationentity;knowledge base;populationentity discovery linking;probabilistic logics;automated entity;triaccacc knowledge base;linking english language;potential probabilistic logics;linking english;probabilistic logics complex;base populationentity discovery;discovery linking dl;performing automated entity;linking;logics complex;naturally encoded relations;populationentity discovery;logics complex large;data resources naturally;2014 triaccacc knowledge;entity;relations methodsin pursuit;encoded relations methodsin;relations methodsin;encoded relations", "pdf_keywords": ""}, "30cf652bd33049aaf111a5f84eb262a87c045bdb": {"ta_keywords": "fact checking effort;manual fact checking;human fact checkers;detect sentences;detect sentences contain;fact checking;fact checkers;sentences contain claim;fact checkers benefit;aims detect sentences;contain claim verified;timeconsuming human fact;manual fact;human fact;false claims online;sentences contain;document aims detect;lot manual fact;checking effort;checkers benefit tools;contain claim;claim verified;proliferation false claims;false claims;given input document;sentences;detect;input document aims;claims online;checking", "pdf_keywords": "fact checking effort;sentences containing claim;sentences contain claim;verified claim retrieval;identifying sentences verified;human fact checkers;claim retrieval algorithm;claim retrieval;fact checkers;formulation claimfocused task;fact checking application;claimfocused task formulation;assist fact checkers;sentence verified claim;task identifying sentences;fact checkers journalists;manual fact checking;sentences verified using;factchecked claims;task formulation claimfocused;fact checking;end fact checking;claims input sentence;claims text;detect sentences;fact checked claims;fact checkers benefit;identify relevant claims;journalists focuses claims;claims text database"}, "31412f9b23511e212895305927d9ccddb445bcbc": {"ta_keywords": "voice timbre control;control voice timbre;statistical voice timbre;multiple voice timbre;parameters statistical voice;timbre control methodsmulti;voice timbre;described voice timbre;statistical voice;timbre control;control voice;voice timbre axes;gassian mixture models;corresponding multiple voice;voice timbre expression;regression gassian mixture;multiple voice;mixture models gm;speakers voice;target speakers voice;mixture models;allow control voice;control parameters statistical;voice;control methodsmulti regression;methodsmulti regression gassian;timbre;timbre expression words;regression gassian;described voice", "pdf_keywords": ""}, "633ee881c594cface387557359ef13613d8eaef0": {"ta_keywords": "egalitarianism random assignment;random assignment mechanisms;egalitarian welfare random;ordinality envy freeness;welfare random assignment;optimal egalitarian;egalitarianism random;optimal egalitarian value;approximate optimal egalitarian;assignment mechanisms;background egalitarianism random;ordinality envy;assignment mechanisms agents;assignment mechanisms approximate;assignment mechanisms common;egalitarian value oev;welfare random;egalitarian welfare;like ordinality envy;random assignment;envy freeness truth;consider egalitarian welfare;egalitarian value;different random assignment;envy freeness;egalitarian;mechanisms approximate optimal;consider egalitarian;agents unrestricted cardinal;egalitarianism", "pdf_keywords": ""}, "1ccd031f28dccfb226f6c0c588c93a97a50bf95f": {"ta_keywords": "benchmark code generation;code generation performance;evaluating code generation;code generation results;code generation;apps benchmark code;benchmark code;introduce apps benchmark;apps benchmark;evaluating code;write code;programming broadly;generation performance accurate;work evaluating code;generation performance;generation results;benchmark;learning models write;programming;programming broadly applicable;models write code;generation results like;code;introduction programming broadly;machine learning;machine learning models;code despite importance;introduction programming;challenge introduce apps;art machine learning", "pdf_keywords": "benchmark code generation;evaluating code generation;code generation results;code generation performance;code natural language;code generation;code generation task;synthesis natural language;program synthesis natural;program synthesis;program synthesis advancements;programming programming apps;coding challenge apps;language models apps;apps benchmark code;tool generating code;automateded programming progress;programming apps;generating code;python language models;benchmark code;generating code natural;automateded programming;able generate code;generate code;language aforementioned benchmark;task program synthesis;generate python code;models apps benchmark;software tool programming"}, "ac5e7f9bbc5d46bebc4ec5616aba9d014a6d237f": {"ta_keywords": "purposethe undergraduate computer;undergraduate computer science;computer science curriculum;science fiction reviews;reviews results students;undergraduate computer;research reviews;fiction reviews used;purposethe undergraduate;research field learn;methodswe science fiction;literature methodswe science;computer science;navigate research literature;students exposed research;fiction reviews;research literature methodswe;science curriculum generally;research reviews results;science curriculum;research topic stirs;learn navigate research;literature methodswe;research field;gateway research reviews;tools students;critically compare technical;navigate research;tools students exposed;technical papers topic", "pdf_keywords": ""}, "615b823d1fc9548ce384f1bb4f544445175e8537": {"ta_keywords": "pea starch alcohol;treatment pea starch;starch alcohol solution;starch alcohol;pea starch;drug treatment pea;alcohol solution;starch;alcohol solution highly;treatment pea;solution highly potent;effective drug treatment;drug treatment;highly potent effective;potent effective drug;alcohol;effective drug;pea;potent effective;treatment;highly potent;drug;potent;solution highly;solution;effective;highly", "pdf_keywords": ""}, "556a4a0b5fcda4d9f9fad637f2655aeb1b1a00b2": {"ta_keywords": "paralinguistic information;paralinguistic features;possible paralinguistic features;information sensitive paralinguistic;paralinguistic information sensitive;introduction paralinguistic information;paralinguistic;sensitive paralinguistic information;paralinguistic features handle;introduction paralinguistic;features input speech;paralinguistic information different;different possible paralinguistic;possible paralinguistic;language duration power;output speech;input speech;sensitive paralinguistic;speech output speech;speech output;output speech continuous;speech continuous space;input speech output;language duration;speech continuous;source language duration;speech;duration power information;duration power;translate features input", "pdf_keywords": ""}, "d6fc0fcf0764065f6e58c57ca850abfdd918504b": {"ta_keywords": "sentence features thresholded;sentence features;errorror rate training;introductioninter sentence features;intersentence features features;rate training naist;training naist clef;intersentence features;log linear scoring;features thresholded;features;linear scoring;minimum errorror rate;linear scoring model;features features;features thresholded minimum;errorror rate;rate training;scoring model;training naist;naist clef 2013;intra intersentence features;thresholded minimum errorror;2013 q4mre methodsthe;training;features receives;minimum errorror;q4mre methodsthe core;task clef 2013;sentence", "pdf_keywords": ""}, "7de4a82edf68b69a9c007fe8e840edf4ade1171c": {"ta_keywords": "kinetics ficin arginine;ficin sulphydryl enzyme;kinetics ficin;ficin arginine derivatives;recently kinetics ficin;ficin arginine;ficin sulphydryl;enzyme sulphydryl enzyme;sulphydryl enzyme;enzyme sulphydryl;sulphydryl enzyme sulphydryl;ficin;enzyme recently kinetics;sulphydryl enzyme recently;electrophoretic ultracentrifugal behaviour;electrophoretic ultracentrifugal;enzyme;arginine derivatives studied;arginine derivatives;communication electrophoretic ultracentrifugal;ultracentrifugal behaviour described;enzyme recently;ultracentrifugal behaviour;ultracentrifugal;arginine;electrophoretic;sulphydryl;kinetics;conditions activity stability;activity stability established", "pdf_keywords": ""}, "2cae732250b59f9e2238626d8d7e0064b97de3c9": {"ta_keywords": "crossing representation wavelet;representation wavelet transform;wavelet transform;representation wavelet;feature signal reconstruction;wavelet transform domain;signal reconstruction;signal reconstruction problem;algorithm signal reconstruction;signal reconstruction resultswe;wavelet;image representation early;zero crossing representation;extension image representation;image representation;representation early vision;early vision methodswe;reconstruction problem reduces;iterative algorithm signal;stabilized zero crossing;reconstruction problem;minimum norm optimization;vision methodswe present;zero crossing;vision methodswe;early vision;feature signal;algorithm signal;salient feature signal;norm optimization", "pdf_keywords": ""}, "616c15dd765c36c21efc75c7ed52e5af81c21053": {"ta_keywords": "ai technologies implications;ai provided overview;ai technologies;report artificial intelligence;state ai technologies;current state ai;intelligence ai provided;artificial intelligence ai;intelligence ai;ai;state ai;recent report artificial;artificial intelligence;ai provided;technologies implications public;technologies implications;report artificial;technologies;overview current state;intelligence;implications public;overview;recent report;artificial;provided overview;current state;provided overview current;implications;overview current;report", "pdf_keywords": ""}, "bf9b069242f0af129c2aad8430a52454b008c327": {"ta_keywords": "learning rate gradually;learning rate decay;tuning learning rate;learning rate;learning rate including;initial learning rate;introductionthe learning rate;rate including learning;learning rate single;including learning rate;stochastic nonconvex optimization;effect learning rate;tuning learning;large initial learning;broadly stochastic nonconvex;techniques tuning learning;rate decay;stochastic nonconvex;rate gradually;rate decay starts;training neural;neural networks broadly;nonconvex optimization;broadly stochastic;networks broadly stochastic;rate gradually decreased;initial learning;training neural networks;stochastic;learning", "pdf_keywords": "stochastic gradient langevin;stochastic gradient descent;stochastic optimization nonconvex;gradient descent stochastic;stochastic nonconvex optimization;descent stochastic gradient;stochastic optimization dependent;adaptive stochastic gradient;stochastic gradient;stochastic optimization;learning rate nonconvex;rate stochthe learning;stochastic gradient algorithms;performance stochastic optimization;learning rate stochthe;learning rate convex;stochthe learning rate;optimization dynamics stochastic;broadly stochastic nonconvex;gradient langevin dynamics;dynamics stochastic optimization;learning rate gradually;rate nonconvex optimization;approach stochastic gradient;stochastic nonconvex;gradient langevin;method stochastic gradient;learning rate decay;descent stochastic;learning rate completely"}, "250f8f71f7cff972a70482229ca9053b356217cd": {"ta_keywords": "online unsupervised voice;unsupervised voice activity;voice activity detecting;online voice activity;voice activity detection;method online voice;online variational bayes;voice activity;unsupervised voice;online voice;online variational;use online variational;variational bayes framework;variational bayes method;comparison variational bayes;variational bayes;online unsupervised;bayes method online;bayes framework online;voice;model comparison variational;activity detecting;online model comparison;activity detection;detecting vad unsupervised;activity detection methods;framework online unsupervised;activity detecting vad;online model;comparison variational", "pdf_keywords": ""}, "3d2ceea5dea234ae9a20f8e1c9e558735757e90e": {"ta_keywords": "multilingual acoustic models;languages multilingual acoustic;multilingual acoustic;languages multilingual;training languages;models improve language;language corresponding phones;training languages identically;multilingual;spoken language independent;introductionmultilingual models improve;parameters languages multilingual;difference phonemes sounds;improve language processing;identically annotated phonemes;spoken language;language processing particularly;improve language;language processing;annotated phonemes;language corresponding;sounds support lexical;languages;actually spoken language;phonemes sounds support;language independent;languages identically annotated;languages identically;phonemes sounds;difference phonemes", "pdf_keywords": "multilingual phoneme prediction;multilingual acoustic models;multilingual acoustic modeling;phonemes multilingual acoustic;multilingual acoustic;phone predictor language;phones phonemes multilingual;languages multilingual acoustic;multilingual phoneme;phoneme prediction using;phonemes multilingual;approach multilingual phoneme;tool multilingual acoustic;phoneme prediction;annotated phonemesbackgroundmultilingual acoustic;language corresponding phones;phonemesbackgroundmultilingual acoustic models;languages training data;phonemes sounds support;speech recognition toolkit;shared phoneme model;modeling speech world;annotated phonemesbackgroundmultilingual;languages integrating allophones;identically annotated phonemesbackgroundmultilingual;allophones phone phoneme;recognition languages integrating;phonemesbackgroundmultilingual acoustic;phoneme mappings;acoustic modeling speech"}, "8234049255a0e03fc745457de456634d1aab214b": {"ta_keywords": "web page structure;structure web page;structure web;machines understand pages;page case descriptionwe;page structure;understand pages;ways structure web;understand pages specifically;recognizing using structure;hyperlinks pages;hyperlinks;pages html;defined hyperlinks pages;discover web page;html;typically defined hyperlinks;hyperlinks pages html;page structure conclusionthese;introductionbecause information web;defined hyperlinks;discover web;web presented;web page;learn discover web;descriptionwe;information web presented;pages html formatting;information web;using structure structure", "pdf_keywords": ""}, "e11b4750e288785134f042c144f057a11dc0180a": {"ta_keywords": "introductionflexible representative democracy;interactive democracy;representative democracy frd;representative democracy;hybrid representative democracy;interactive democracy model;representative democracy rd;direct democracy;liquid democracy fd;democracy fd uses;rd direct democracy;democracy fd;direct democracy dd;liquid democracy;unlike liquid democracy;democracy frd;literature interactive democracy;democracy model;democracy rd direct;democracy model allows;democracy frd novel;democracy rd;democracy;direct versus representative;democracy dd voters;representative unlike liquid;democracy dd;set elected representatives;representative unlike;versus representative", "pdf_keywords": "flexible representative democracy;democratic decision binary;representative democracy theoretically;hybrid representative democracy;democracy decision representatives;electable representatives democratic;interactive democracy;model interactive democracy;representative democracy analyze;representative democracy propose;democracy called flexible;voters representative democracy;representative democracy decision;outcomes representative democracy;democracy proxy voting;scale representative democracy;electable representatives;interactive democracy smoothly;democracy majority voting;representative democracy;representative democracy overcome;elect electable representatives;based voter delegations;decision based voter;representative democracy direct;generalization representative democracy;systems representative democracy;voter majority decisions;democracy decision based;various democratic systems"}, "b8b813111c411ae61881ab9cd25707d9de6444ec": {"ta_keywords": "key value attention;standard attention heads;attention heads;value attention backbone;multi head key;attention heads learn;value attention blocks;value attention;attention mechanism uses;attention mechanism;attention backbone widely;attention blocks;multi head;head key value;multiple parallel key;parallel key;variants attention mechanism;importantly standard attention;parallel key value;attention backbone;attention;head key;variants attention;standard attention;model variants attention;key interactions retrieval;transformer;heads performing fundamental;transformer model;uses multiple parallel", "pdf_keywords": "query key attention;key attention retrievals;key value attention;attention searches;value attention multi;attention searches retrievals;searching retrieval objects;attention multi head;retrieval objects task;head attention searches;multi head attention;attention multi;key interactions retrieval;attention multiple;search features;value attention backbone;retrievals compositional attention;attention retrievals propose;able efficiently search;retrieval features;retrieval objects;retrieval objects difficult;attention mechanism multi;attention dynamically specializing;key attention;flexible search;defined search features;value retrievals allowing;attention retrievals;retrieval query key"}, "a469f2ec3ab15f20f06d95aea1839b1263d3385e": {"ta_keywords": "egalitarianism random assignment;random assignment mechanisms;optimal egalitarian;egalitarianism random;ordinality envy freeness;introduction egalitarianism random;optimal egalitarian value;approximate optimal egalitarian;assignment mechanisms;welfare aspects random;assignment mechanisms agents;assignment mechanisms common;assignment mechanisms approximate;egalitarian welfare;ordinality envy;random assignment;egalitarian welfare aspects;consider egalitarian welfare;different random assignment;like ordinality envy;egalitarian value;egalitarian value investigate;egalitarian;envy freeness;consider egalitarian;egalitarianism;mechanisms approximate optimal;envy freeness truthfulness;agents unrestricted cardinal;aspects random assignment", "pdf_keywords": "egalitarianism random assignment;random assignment mechanisms;fairness randomized mechanisms;uniform allocation envy;randomized mechanisms optimal;envy freeness allocations;easier randomized mechanisms;mechanisms optimal egalitarian;allocation envy;regarding fairness randomized;preferences uniform allocation;freeness allocations studied;fairness randomized;randomized mechanisms;assignment mechanisms;freeness allocations;achievablethe optimal egalitarian;deterministic mechanisms assignment;allocation envy free;optimal allocation agent;optimal allocations;assignment mechanisms approximate;allocation objects optimal;optimal allocation good;allocations compared optimal;assignment mechanisms agents;uniform allocation;randomized mechanisms deterministic;optimal allocations compared;compared optimal allocations"}, "18f4ec53a4221a97e1482f091f41a23f3d873cf2": {"ta_keywords": "evidence annotations strong;evidence annotations;combine evidence annotations;semi supervised evidence;supervised evidence extraction;practice evidence annotations;evidence extraction prediction;evidence annotations available;supervised evidence;evidence extraction;annotations strong;annotations strong semi;semi supervised;annotations;annotations available minority;strong semi supervision;predictions supporting evidence;supporting evidence;annotations available;supervision abundant document;backgroundweakly semi supervised;supporting evidence human;semi supervision;semi supervision abundant;combine evidence;extraction prediction tasks;methods combine evidence;supervised;practice evidence;evidence human", "pdf_keywords": "evidence annotations strong;evidence sequence labeling;evidence annotations;generating evidence label;evidence text classification;combine evidence annotations;evidence extraction predicted;linear evidence extraction;evidence label especially;labels evidence spans;evidence extraction;text classification evidence;evidence label;annotations methods;labels evidence;labels weak supervision;annotations strong;sequence labeling tasks;explanations evidence snippets;annotations methods allow;practice additional annotations;annotations;classification evidence sequence;evidence snippets challenge;evidence extraction approach;additional annotations;baselines annotations methods;extraction evidence text;evidence text;extraction predicted label"}, "c4536a5c7f47bfc48df202ba882002531248f955": {"ta_keywords": "electrolaryngeal el speech;electrolarynx type speaking;laryngectomees produce electrolaryngeal;introductionan electrolarynx;electrolarynx;introductionan electrolarynx type;generate excitation sounds;electrolarynx type;produce electrolaryngeal el;sounds help laryngectomees;mechanical excitation sounds;excitation sounds proposed;produce electrolaryngeal;excitation sounds make;electrolaryngeal el;excitation sounds;natural excitation sounds;excitation sounds help;help laryngectomees produce;electrolaryngeal;laryngectomees produce;laryngectomees;help laryngectomees;mechanically generate excitation;speaking aid device;generate excitation;mechanical excitation;generate natural excitation;el speech speech;excitation", "pdf_keywords": ""}, "f43ae70242aea3dbb80b7c3b5474356e9ee9079b": {"ta_keywords": "language processing meaningrepresentation;sentence semantic graph;meaningrepresentation amr;semantic graph;processing meaningrepresentation amr;meaningrepresentation amr popular;natural language represents;processing meaningrepresentation;meaning sentence semantic;semantic;meaningrepresentation;sentence semantic;semantic graph useful;language represents meaning;multilingual natural language;represents meaning sentence;natural language;semantic graph methods;formalism natural language;computational typology multilingual;natural language processing;language represents;typology multilingual natural;multilingual natural;language processing;represents meaning;typology multilingual;multilingual;amr popular formalism;describes", "pdf_keywords": ""}, "72c9663494827b2e87ad5a65a6ff7e769eb15a57": {"ta_keywords": "visual storytelling methodsin;visual storytelling;framework visual storytelling;storytelling methodsin;storytelling methodsin paper;relevance coherence expressiveness;topically coherent story;high quality story;storytelling;coherence expressiveness;relevance coherence;coherent story end;quality story human;coherence expressiveness observe;quality story;story human eye;coherent story;evaluation framework visual;natural topically coherent;criteria relevance coherence;expressiveness;visual;coherence;story end propose;expressiveness observe;topically coherent;story human;evaluation;expressiveness observe empirical;story end", "pdf_keywords": "visual storytelling task;model visual storytelling;evaluate visual storytelling;visual storytelling challenging;visual storytelling dataset;visual storytelling directly;visual storytelling;approach visual storytelling;visual storytelling problem;visual storytelling provide;storytelling task requires;naturalness generated stories;visual storytelling development;optimizes story generation;storytelling task task;stories propose reinforcement;storytelling challenging task;storytelling directly optimizes;generated stories;storytelling task;framework visual storytelling;generated stories context;backgroundthe visual storytelling;storytelling provide quantitative;interactive narrative;story generation quality;interactive narrative narrative;example generated stories;development visual storytelling;storytelling dataset"}, "15bb07d0996ece844de8cae24d3dc15972e6841a": {"ta_keywords": "summarization datasets;know summarization datasets;introduction know summarization;know summarization;summarization;datasets;introduction know;introduction;know", "pdf_keywords": "summarization datasets;analysed summarization datasets;popular summarization datasets;summarization systems trained;summarization datasets distinct;better summarization datasetsthe;summarization complexity datasets;popular summarization datasetswe;summarization datasets online;summarization datasetsthe;summarization datasetswe perform;summarization models reveals;summarization datasets gigaword;properties summarization datasets;summarization datasetswe;state art summarization;summarization models;summarization systems evaluate;summarizing data large;summarization systems;summarization complexity;different summarization systems;27 summarization models;summarization datasetsthe association;evaluation popular summarization;summarization common;summarization documents;summarization use outputs;creating better summarization;summarizing data"}, "674833d48a77ef009f751a66988590592dd5d996": {"ta_keywords": "history hymenopausal disorder;patient history hymenopausal;hymenopausal disorder;history hymenopausal;hymenopausal;characteristics patient;characteristics patient patient;patient history patient;patient patient;patient history;article characteristics patient;patient patient history;history patient;patient patient patient;patient;history patient history;disorder;article characteristics;characteristics;article;history;aim article characteristics;aim article;aim", "pdf_keywords": ""}, "49ee2270f3265ee27b36e05e130be79e05d5ba29": {"ta_keywords": "learning textbook knowledge;textbook knowledge;logical theory learning;explanation based learning;textbook knowledge methodsto;textbook knowledge grand;problem textbook knowledge;introduction learning textbook;learning textbook;introduction learning;natural language understanding;textbook;problem learning textbooks;analogical abductive;learning textbooks;theory learning problem;knowledge methodsto;theory learning;textbooks;learning problem textbook;abductive explanation based;technique analogical abductive;language understanding;based learning apa;translated logical theory;analogical abductive explanation;logical theory;learning textbooks aimto;learning apa;natural language", "pdf_keywords": ""}, "68ea2572584068befd441dccf461f3444ff14f4a": {"ta_keywords": "educational robot;generating educational robot;learning agent;educational robot embedding;learning agent physical;embedding learning agent;agent models student;simulates human learning;building intelligent agent;intelligent agent simulates;intelligent agent;agent simulates human;agent simulates;robot embedding learning;intelligent agent models;generating educational;human learning;models student learning;robot embedding;robot;art intelligent agent;learn building intelligent;learning process;human learning process;student learning;agent physical;educational;achievement learning;agent models;agent physical world", "pdf_keywords": ""}, "4cfc7d3c6a61f6db48b1f3c75235592c1609a54f": {"ta_keywords": "automatic transcription quality;automatic transcription;computer assisted transcription;quality speech transcription;speech transcription;assisted transcription;automatic transcription results;improve human transcription;case automatic transcription;human transcription quality;parts automatic transcription;transcription quality;transcription quality appropriate;assisted transcription promises;transcription quality insufficient;transcription;speech transcription reduced;human transcription;transcription reduced costs;transcription promises;transcription promises high;transcription results;transcription results conduct;high quality speech;transcription reduced;human effort transcribing;introduction computer assisted;effort transcribing;quality speech;focus iterative interfaces", "pdf_keywords": ""}, "e2ac96254d7e9ec0dde882e3a09797d00f26220f": {"ta_keywords": "automated modeling methods;automated automated modeling;automated modeling;method development automated;development automated automated;development automated;automated automated;modeling methods;automated;new methods development;methods development new;new method development;methods development;method development;modeling;development new methods;development new technologies;new technologies article;technologies article present;technologies article;new methods;new technologies;systematic review literature;methods;technologies;new method;development new;use new method;development;systematic", "pdf_keywords": ""}, "757acf616a38422c7186952e1075a28fed1a07c0": {"ta_keywords": "xyl veterinary anesthetic;anesthetic mechanism xyl;xylazole xyl veterinary;xyl using fetal;xylazine effects xyl;xylazole xyl;cells treated xyl;effects xyl vitro;xylazine effects;xyl vitro;fetal rat nerve;functionally similar xylazine;xyl vitro remain;similar xylazine effects;xylazine;veterinary anesthetic structurally;xylazole;background xylazole xyl;veterinary anesthetic;treated xyl methods;mechanism xyl using;anesthetic structurally functionally;similar xylazine;using fetal rat;xyl veterinary;mechanism xyl;treated xyl;xyl using;investigate anesthetic mechanism;rat nerve cells", "pdf_keywords": ""}, "fc912e9af47bf10428396b687b2bfb1e5832fcb1": {"ta_keywords": "cc based speech;connectionist temporal classification;losss regularization cc;temporal classification cc;speech recognition;automatic speech recognition;regularization cc based;cc loss regularizes;cc encoder network;layer cc encoder;regularization cc;intermediate cc loss;based speech recognition;cc encoder;intermediate losss regularization;speech recognition ar;automatic speech;based connectionist temporal;speech recognition methods;function automatic speech;mediate cc loss;classification cc;regularizes crc training;losss regularization;cc loss;efficient auxiliary loss;loss regularizes crc;intermediate layer cc;classification cc objective;connectionist temporal", "pdf_keywords": "loss speech recognition;intermediate loss speech;nonautoregressive speech recognition;speech recognition nonautoregressive;connectionist temporal classification;speech recognition recurrent;speech recognition;speech recognition ar;application speech recognition;recognition nonautoregressive speech;tool speech recognition;automatic speech recognition;speech recognition increasing;efficient auxiliary loss;encoder network intermediate;approaches speech recognition;auxiliary loss improving;cc encoder network;speech recognition model;automatic speech;recognition recurrent neural;function automatic speech;training multi layer;loss stochastic depth;recognition application speech;encoder network;temporal classification cc;loss speech;auxiliary loss stochastic;layer cc encoder"}, "f3132572bb3870dbe99b2d1c01ce17fa38783a2f": {"ta_keywords": "speech processing uncertainty;speech processing processing;speech processing;approach speech processing;speech processing fundamental;process speech processing;processing uncertainty propagation;process speech;speech processing overlooked;overlooked speech processing;uncertainty propagation;processing uncertainty;processing overlooked speech;fundamental process speech;approach speech;new approach speech;speech;processing processing;uncertainty;processing fundamental;processing processing new;processing new approach;processing;propagation;processing fundamental process;processing new;process;processing overlooked;overlooked speech;fundamental process", "pdf_keywords": ""}, "213e471bacff5c0852943988fcb955797f1e591f": {"ta_keywords": "metrics machine translation;quality automatic metrics;machine translation increasingly;machine translation;automatic metrics machine;automatic metrics;automated evaluation reporting;value automated evaluation;automated evaluation;metrics machine;systems metrics;translation increasingly;references compare;translation increasingly called;evaluation reporting;metrics;collect references compare;human evaluation;human evaluation variety;quality automatic;compare value automated;metric;references compare value;translation;value automated;metric important;correlation human evaluation;references critical;evaluation;evaluation reporting correlation", "pdf_keywords": "evaluation reference translations;evaluation machine translation;translationthe evaluation machine;metrics machine translation;human translations metric;translations complemented fluency;human translations generating;human translations generated;translationthe evaluation;concentrating translationthe evaluation;accuracy reference translations;translation task measure;quality translation;machine translation;professional translation service;reference translations different;machine translation increasingly;standard reference translation;reference translations;human translations;generated human translations;machine translation role;additional human translations;new reference translations;translations generating diverse;translation service;translations generated;translations generating;translation task;translation affect automatic"}, "77568c594470f9aa029f92774e2c12ab0451d9bb": {"ta_keywords": "language models;language models generally;training text;knowledge test distribution;likelihood training;introduction language models;maximum likelihood training;likelihood training remedy;paper training text;knowledge test;topics news reviews;training text outside;models generally trained;text outside test;methods paper training;generally trained data;news reviews;news reviews fiction;restaurant reviews methods;range topics news;distribution restaurant reviews;topics news;reviews methods;trained data;reviews methods paper;language;test distribution degrade;test distribution;distribution degrade test;reviews fiction applied", "pdf_keywords": "language models robust;robust language models;language modeling improves;language modeling robustness;language models;language models neural;language modeling;demonstrate language models;adaptation language modeling;robustness language modeling;likelihood language models;neural language models;language models degrade;language modeling demonstrate;language models new;language modeling fundamental;topic model robust;language modeling performance;backgroundlinguistic models generally;models neural language;language modeling apply;approach language modeling;robust language;field language modeling;backgroundlinguistic models;modeling robustness language;language models ptxrain;method language modeling;training text;resulting topic model"}, "89c64fd60ca58f4753a818cd0923f5041b51a807": {"ta_keywords": "network cost objective;routing multiple available;routing multiple;alternate routing multiple;connected multihop network;propose network cost;multihop network propose;multi connectivity;network cost;multihop network;achieving connected multihop;disadvantages multi connectivity;alternate routing;additive deployed relays;possible alternate routing;connected multihop;routing;multiple available paths;multi connectivity validate;network propose network;deployed relays;network propose;disadvantages multi;propose network;relays;advantages disadvantages multi;cost objective additive;cost objective;objective achieving connected;optimal policy", "pdf_keywords": ""}, "612d577534dbbf546405d4036d912666523a8164": {"ta_keywords": "learnability order representations;polynomial learnability;polynomial learnability order;results polynomial learnability;learnability order;learnability;learning concepts expressed;instead learnability;prolog highly restricted;prolog highly;learnable;prolog;learning concepts;shown learnable;subsets prolog highly;subsets shown learnable;study instead learnability;subsets prolog;focused subsets prolog;learnable paper;expressed order logic;concepts expressed order;shown learnable paper;pac model focused;logic relatively formal;learning;pac model;concepts expressed;order logic;analyses pac", "pdf_keywords": ""}, "0d516b476559485e04290e859ca59101c0a91ae1": {"ta_keywords": "obstacles millimeter wave;millimeter wave d2d;wave d2d communication;dynamic obstacles millimeter;d2d communication;d2d communication prominent;relay link susceptible;selected relay;selected relay link;newly selected relay;wave d2d;obstacles millimeter;millimeter wave;fluctuations 3d channel;select new relay;blocked dynamic obstacles;relay link;relay;dynamic obstacles known;channel quality causing;3d channel;dynamic obstacles;dynamic dynamic obstacles;3d channel quality;channel quality;introductionlocalrelay selection presence;new relay;introductionlocalrelay selection;relay current link;severe penetration losses", "pdf_keywords": "relay selection exploration;relay exploration switching;relay exploration;relay chosen exploring;relay selection;relay link decision;problem relay selection;exploring new relay;exploration problem relay;selecting relay;relay chosen;best relaying;selected relay;newly selected relay;selecting selecting relay;best relaying zone;new relay chosen;frequent relay switching;zone best relaying;relay link quality;frequency relay exploration;introduction new relay;selecting relay respectively;new relaying;frequent relay;relay switching;relaying;relay;selected relay link;new relaying ui"}, "99fe5475ab28fa7ad4bce51d7b294b3f40caad4d": {"ta_keywords": "concept learning;concept learning improved;concept learning essential;amplification;amplification amplification;using amplification;using amplification amplification;amplification amplification amplification;analyze cellular;required concept learning;improved using amplification;analyze cellular processes;detect analyze cellular;learning;cellular processes;learning improved;learning essential development;learning improved using;cellular;cellular processes required;learning essential;ability detect analyze;ability detect;concept;development recently ability;detect analyze;detect;development;processes;essential development", "pdf_keywords": ""}, "31392ad8722d9c66181b621936e2013199e02edc": {"ta_keywords": "words pretraining data;billions words pretraining;words pretraining;pretrained billions words;transformer ls learn;pretraining data anl;pretraining learn data;knowledge skills transformer;pretraining data;pretraining learn;evaluation classifier probing;skills transformer ls;language models;classifier probing information;language models like;classifier probing;scale pretraining learn;large scale pretraining;learn data;pretrained billions;words exact knowledge;like roberta pretrained;ls learn;skills transformer;learn large scale;pretraining;dominated language models;roberta pretrained billions;ls learn large;exact knowledge skills", "pdf_keywords": "rodent robertabase linguistic;linguistic features learnable;pretrained language models;semantic tasks significantly;nonludimental language learning;language learning improve;linguistic features acquire;language learning;acquire representations linguistic;pretraining billions words;pretrained language;higher semantic tasks;learning ability linguistic;probe learn task;language learning requires;syntactic semantic tasks;linguistic knowledge minibertas;generalpurpose pretrained language;reliably extract linguistic;semantic tasks minimum;language learning ability;learnable 100m words;semantic tasks;robot;learn task significantly;linguistic knowledge;language models acquire;data ability linguistic;ability learn task;ability linguistic"}, "6c82727731955a2332a0cc38ec56b35a971061eb": {"ta_keywords": "neural machine translation;machine translation toolkit;translation toolkit;translation toolkit methods;machine translation;translation toolkit aim;xnmetry extensible neural;extensible neural machine;aim xnmetry extensible;toolkit aim xnmetry;xnmetry extensible;describes xnmetry extensible;toolkit methods xnmetry;xnmetry available;neural machine;xnmetry available open;aim xnmetry;describes xnmetry;paper describes xnmetry;xnmetry;extensible neural;translation;methods xnmetry available;methods xnmetry;neural;toolkit;toolkit methods;toolkit aim;machine;extensible", "pdf_keywords": "translation toolkit methodsxnmt;machine translation toolkit;neural machine translation;translation toolkit;translation toolkit extensibility;machine translation;machine translation nm;toolkits neural machine;toolkits neural;processing text translation;machine translation paper;xnm neural machine;introduce xnm neural;text translation;text translation freely;similar alternatives xnm;nonmt toolkits;toolkit methodsxnmt;source nonmt toolkits;xnm neural;alternatives xnm;neural networks language;xnm novel useful;alternatives xnm novel;xnm novel approach;neural machine;introduce xnm;tool training inference;xnmt extensible neural;translation nm"}, "7e406537f52528527d10872d1807ad974599b13a": {"ta_keywords": "neural generation translation;neural machine translation;workshop neural generation;neural generation;generation translation;machine translation;machine translation nm;generation translation held;workshop neural;fourth workshop neural;efficient neural machine;tasks efficient neural;efficient neural;translation nm participants;neural machine;translation nm;neural;language ac 2020;tasked creating nonmm;computational language;computational language language;translation;translation held;language language ac;language ac;creating nonmm;language language;association computational language;creating nonmm systems;generation", "pdf_keywords": ""}, "5babe5334c6867db13fa7e6943f64059c7cba6ce": {"ta_keywords": "new new type;new type new;new type;type new new;features new new;type new;type new type;features new;article features new;new new new;features;new new;type;new;article features;purpose article features;purpose article;article;purpose", "pdf_keywords": ""}, "c0cce8955bf10b21753161ffaa1978a7c8b78a16": {"ta_keywords": "audible murmur enhancement;murmur enhancement based;murmur enhancement;methodsnon audible murmur;backgroundnon audible murmur;audible murmur;resultsno audible murmur;microphones noisy environments;murmur;microphones;microphones noisy;body conductive microphones;conductive microphones noisy;environments methodsnon audible;audible mur;environments resultsno audible;enhancement based statistical;conductive microphones;methodsnon audible;environments conclusionsno audible;backgroundnon audible;noisy environments;noisy;audible;based statistical conversion;noisy environments methodsnon;conversion using air;conclusionsno audible mur;statistical conversion;noisy environments resultsno", "pdf_keywords": ""}, "6027ef3b4e5585b45db0b9d333956425d3972351": {"ta_keywords": "commonsense reasoning opencr;commonsense reasoning research;commonsense reasoning;making commonsense reasoning;current commonsense reasoning;commonsense knowledge answer;open ended commonsense;ended commonsense reasoning;commonsense knowledge;use commonsense knowledge;models use commonsense;use commonsense;knowledge answer multiple;commonsense;making commonsense;reasoning opencr task;opencr task answering;reasoning opencr;choice questions systems;introduction current commonsense;current commonsense;answers choose;designed answer multiple;answer multiple choice;reasoning research;answer multiple;candidate answers choose;ended commonsense;multiple choice questions;list candidate answers", "pdf_keywords": "commonsense reasoning tasks;commonsense reasoning opencr;commonsense reasoning research;reasoning method commonsense;commonsense reasoning;make commonsense reasoning;method commonsense reasoning;backgroundcurrent commonsense reasoning;endened commonsense reasoning;opencr question answering;use commonsense knowledge;commonsense knowledge;ended commonsense reasoning;commonsense knowledge answer;open ended reasoning;open ended commonsense;diverse question answering;opencr task answering;reasoning opencr task;retrieval surface commonsense;open endened commonsense;commonsense questions opencr;commonsense facts formalized;generic sentences commonsense;choice question answering;commonsense questions demonstrate;question answering implement;question answering;method commonsense;knowledge answer multiple"}, "a38e0f993e4805ba8a9beae4c275c91ffcec01df": {"ta_keywords": "programsynthesis large language;introduction programsynthesis large;large language models;programsynthesis large;introduction programsynthesis;language models important;programsynthesis;language models;programming languages evaluate;large language;programming languages;parameters new benchmarks;purpose programming languages;new benchmarks;languages evaluate;new benchmark;benchmarks;benchmarks number parameters;languages;programming;benchmark;general purpose programming;new benchmarks number;new benchmarks set;benchmarks number;languages evaluate collection;benchmarks set new;benchmarks set;language;set new benchmark", "pdf_keywords": "programming languages synthesis;languages synthesis performance;automated program synthesis;models synthesizing programs;synthesis short programs;models program synthesis;language models synthesize;program synthesis model;program synthesis;program synthesis general;languages synthesis;program synthesis restricted;development large language;synthesizing programs;language models code;program synthesis major;development language models;language models program;large language models;program synthesis experimental;code synthesizing;language models development;language development automated;code synthesizing performance;language models performs;transformer language models;purpose language models;language models results;new language models;programming languages"}, "4f4da6fdb9496b0295764b2db11381dd390de02d": {"ta_keywords": "cost models transcription;correction speech transcripts;recognized speech transcripts;speech transcripts methods;cost sensitive correction;speech transcripts;sensitive correction speech;automatically recognized speech;transcripts methods framework;transcripts methods;manual correction automatically;correction speech;speech transcripts aims;transcripts;sensitive manual correction;models transcription;transcription;modeling cost sensitive;models transcription process;cost sensitive manual;transcripts aims propose;correction automatically recognized;manual correction;sensitive correction;transcriber;transcriber enrollment;user modeling cost;require transcriber;transcripts aims;require transcriber enrollment", "pdf_keywords": ""}, "c581686edbd7227e9eb4a0841cce16728ca27369": {"ta_keywords": "multimodal comprehension cooking;comprehension cooking recipes;comprehension cooking;reasoning cooking recipes;understanding reasoning cooking;reasoning cooking;instructional recipes;instructional recipes multiple;multimodal comprehension;20k instructional recipes;cooking recipes comprises;cooking recipes;comprehension reasoning tasks;dataset multimodal comprehension;recipes;recipes multiple;recipes comprises;cooking recipes fruitful;recipeqa dataset multimodal;introduce recipeqa dataset;introduce recipeqa;cooking;recipes fruitful;recipes multiple modalities;recipeqa dataset;recipeqa;recipes fruitful research;comprehension reasoning;reasoning tasks;comprehension", "pdf_keywords": "multimodal comprehension cooking;comprehension cooking recipes;comprehension cooking;multimodal comprehension tasks;multimodal machine comprehension;multimodal comprehension systems;multimodal comprehension;reasoning cooking recipes;questions answers recipes;recipes dataset multimodal;evaluating multimodal comprehension;answers recipes;dataset multimodal comprehension;questions answers recipe;recipeqa dataset multimodal;reasoning cooking;understanding reasoning cooking;answers recipe;instructional recipes multiple;cooking recipes fruitful;instructional recipes;20k instructional recipes;cooking recipes;recipes multiple;cooking recipes comprises;cooking recipes dataset;recipes;machine comprehension development;generating questions;answers recipe according"}, "ad48174ccbff6259a7d3cb0d0985e5aefa314b84": {"ta_keywords": "level machine translation;modeling machine translation;machine translation methods;translation competitions;machine translation;translation competitions results;western translation competitions;machine translation present;translation methods;character level modeling;translation methods despite;character level systems;character level machine;subword systems;subword systems virtually;use character level;comparable subword systems;literature character level;people use character;translation present literature;character level;use character;systems comparable subword;translation;art character level;level modeling machine;translation present;setups western translation;subword;literature character", "pdf_keywords": "machine translation models;translation models;machine translation model;modeling machine translation;modeling translation;subword models character;machine translation methodsdespite;machine translation association;modeling translation translational;translation model;translation models form;translation quality models;machine translation article;machine translation mt;translation models inwlv;language processing character;machine translation new;machine translation discuss;machine translation;translation association computational;task translation quality;character processing aetiology;w2e02ev0the translation models;models compare translation;language translation process;challenging task translation;character level models;word alignment morphological;translation model comparable;translation methodsdespite evidence"}, "1e3e2b03e28f48bb4d48154992cd6b62969c643e": {"ta_keywords": "annotated sequence entries;000 annotated sequence;entries 500 species;provides annotated sequence;annotated sequence;500 species;500 species contains;500 species used;000 annotated;sequence entries 500;species;sequence entries;80 000 annotated;provides annotated;database provides annotated;species used estimated;annotated;species contains;species used;species contains 80;web;sequence;web integrated database;000 users worldwide;database provides;swiss prot near;web integrated;swiss prot;entries;database", "pdf_keywords": ""}, "7b51209e7d9cbedc18b6ab202e6fcdabaebbb088": {"ta_keywords": "backgrounddiscriminative language modeling;backgrounddiscriminative language;language modeling;language modeling structured;learned word features;embeddings learned word;word features;modeling structured classification;word features extracted;neural network embeddings;structured classification;embeddings learned;structured classification problem;feature representation;product feature representation;network embeddings learned;feature representation used;function parameterized neural;embeddings;language;parameterized neural;classification problem log;neural network;classification;log linear models;parameterized neural network;network embeddings;automatically use convolutional;neural;features extracted", "pdf_keywords": ""}, "795aca47df94300fa6bfd464e6873aef56c7f3ae": {"ta_keywords": "extraction synsets semantic;synsets semantic relations;synsets semantic;lexical semantic resources;large lexical semantic;words babelnet resource;experiments natural language;semantic resources wordnet;samples large lexical;lexical semantic;natural language processing;semantic relations words;omegawiki evaluation training;resources wordnet wiktionary;wordnet;resources wordnet;tasks extraction synsets;language processing;semantic relations;wiktionary omegawiki evaluation;large lexical;extraction synsets;natural language;wordnet wiktionary;relations words babelnet;words babelnet;recurrent tasks extraction;lexical;language processing involve;semantic", "pdf_keywords": ""}, "4cd66273298128dfb5be290e891870085ecfc455": {"ta_keywords": "speech recognition ar;automatic speech;automatic speech recognition;speech recognition;end automatic speech;dn hmm systems;hmm systems;conventional dn hmm;ar attention based;hmm systems avoids;architectures ar attention;linguistic resources pronunciation;pronunciation dictionary tokenization;attention based methods;ar attention;use attention mechanism;attention based;end architectures ar;recognition ar;pronunciation;recognition ar popular;resources pronunciation;speech;resources pronunciation dictionary;pronunciation dictionary;use attention;attention;attention mechanism;end end automatic;end architectures", "pdf_keywords": ""}, "7099d5a4b2d4ed47905071fc23aff08580401e42": {"ta_keywords": "networks rnn;neural networks rnn;rnn layers models;networks rnn layers;speech recognition models;recurrent neural networks;rnn layers;randomized models trained;speech recognition;models inspired echo;rnn;automatic speech;automatic speech recognition;echo state network;rn conformer models;recurrent neural;subset recurrent neural;propose automatic speech;models trained efficiently;neural networks;models trained;inspired echo state;decoder fully randomized;randomized models;clinical messagewe;key clinical messagewe;layers models randomly;fully randomized models;recognition models inspired;rn conformer", "pdf_keywords": "neural automatic speech;new speech recognition;network recurrent input;speech recognition ar;neural network recurrent;recurrent neural network;recurrent neural networks;rnn layers models;neural networks rnn;language model acoustic;speech recognition;networks rnn;recognition speech;network recurrent;speech recognition speech;automatic speech;state network recurrent;networks rnn layers;evolution speech recognition;speech recognition tasks;recognition speech recognition;model acoustic inputs;automatic speech recognition;network recurrent matrix;recurrent input;echo state networks;speech recognition development;rnn layers;novel language model;untrained applied speech"}, "af460a6b3ecaddd4015b34255564c366ecfef802": {"ta_keywords": "computer ethics science;computer ethics;ethics science fiction;importance computer ethics;ethics science;moral imagination;fiction science fiction;good moral imagination;science fiction science;science fiction;fiction science;ethics;moral;students good moral;science fiction particular;fiction;discuss importance computer;good moral;computer;importance computer;fiction particular;science;imagination;fiction particular offers;students;students good;offers students;offers students good;discuss;particular offers students", "pdf_keywords": ""}, "5861dbfcb253ca02067dd182d42b7d567433c834": {"ta_keywords": "applies confounders observed;confounders observed;formula applies confounders;confounders observed frontdoor;applies confounders;effects functionals observational;confounders;transmits causal effect;estimators example backdoor;causal effect paper;causal effect;mediator transmits causal;causal;observed frontdoor formula;transmits causal;frontdoor formula applies;functionals observational joint;applies observed mediator;estimated empirically calculus;treatment effects functionals;backdoor formula applies;frontdoor formula;formula applies observed;observational joint distribution;corresponding estimators;empirically calculus identifies;applies observed;observational joint;empirically calculus;effects functionals", "pdf_keywords": "confounders observed mediators;observed confounders mediators;available observed confounders;observed confounders;confounders mediators observed;collecting confounders mediators;confounders observed;confounders mediators simultaneously;confounders mediators prove;confounders mediators;mix confounders mediators;mediators confounders cases;mixed confounders mediators;confounders mediators effective;confounders mediators studied;scenario confounders mediators;situation confounders observed;mediators confounders;causal inference combined;confounders mediators mediators;datasets causal inference;identified scenario confounders;confounders cases;frontdoor estimators combined;leverage observed confounders;causal graphical models;confounders cases better;confounders;frontdoor estimators;mix confounders"}, "0f655f0e1937ad19b038952e2df69e30d447aac8": {"ta_keywords": "rigid rigid rigid;series rigid rigid;rigid rigid;time series rigid;rigid;series rigid;backgroundmodeling missing data;missing data clinical;backgroundmodeling missing;clinical time series;backgroundmodeling;data clinical time;data clinical;missing data;clinical time;clinical;missing;time series;data;series;time", "pdf_keywords": ""}, "4857e0e3d720b87b4523a6435cc166bcb7ae328a": {"ta_keywords": "neural circuitry brain;processes neural circuitry;neural circuitry;neurobiological processes neural;circuitry brain fundamental;circuitry brain;neurobiological processes;neurobiological processes recently;development neurobiological processes;neurobiological;development neurobiological;process development neurobiological;reported neural circuitry;step development neurobiological;brain fundamental process;neural;processes neural;brain fundamental step;brain fundamental;circuitry;brain;reported neural;fundamental process;fundamental process development;processes;recently reported neural;process;process development;fundamental step;fundamental step development", "pdf_keywords": ""}, "8ff620f704a4151fd7abba1db792463fbd32bfe5": {"ta_keywords": "summarization task compressing;summarization datasets expensive;abstractive summarization task;abstractive summarization methods;summarization datasets;abstractive summarization;datasets collecting summarization;collecting summarization datasets;modern abstractive summarization;collecting summarization;summarization methods;summarization task;introduction abstractive summarization;summarization;short document;summarization methods based;long document;compressing long document;coherent short document;document retaining salient;document coherent short;short document retaining;long document coherent;deep neural;task compressing long;compressing long;methods based deep;deep neural networks;task compressing;retaining salient information", "pdf_keywords": "abstractive summarizer pretrained;summarization task compressing;summarizer pretrained;summarization datasets expensive;summarizer pretrained related;salient sentences train;datasets collecting summarization;collecting summarization datasets;summarization datasets;background abstractive summarization;identifying salient sentences;sentences training;collecting summarization;abstractive summarizer;abstractive summarization task;summarizing technical documents;theart abstractive summarizer;summarizing information;summarizer;salient sentences;sentences training set;abstractive summarization methods;abstractive summarization;summarization task;document retaining salient;extractive summary;novel approach summarizing;sentences train salience;sentences train;coherent fluent summaries"}, "baf34ac4080a365a7cec30b6877fa1a018eb31cf": {"ta_keywords": "multi answer retrieval;single answer retrieval;answer retrieval;answer retrieval explored;answer retrieval limited;multiple distinct answers;distinct answers;answer results prior;retrieval explored problem;retrieval explored;distinct answers given;answer results;retrieval;multi answer;study multi answer;focusing single answer;retrieved passages models;retrieval limited;retrieval limited reason;answers given question;modeling retrieved passages;repeatedly retrieve passages;containing answer;containing answer cost;answer cost missing;valid answer results;passages containing answer;repeatedly retrieve;modeling retrieved;models repeatedly retrieve", "pdf_keywords": "multi answer retrieval;answer retrieval;introductionmulti answer retrieval;retrieval answers;answer retrieval problem;passage retrieval model;answer retrieval underexplored;retrieval sequence passages;answer retrieval demonstrate;benefiting retrieval answers;evaluation passage retrieval;passage retrieval;question answering;downstream question answering;joint passage retrieval;answer generation model;combined answer generation;retrieval different answers;retrieval answers expect;multi answer datasets;retrieve answers improved;passage retrieval reporting;larger answer generation;passage retrieval jp;retrieval demonstrate reranking;retrieval sequence;answer datasets significantly;answer generation;retrieval model;question answering intrinsic"}, "f4465442a9b850a2c5b71a63fff0d24396b15f2c": {"ta_keywords": "model development new;development new model;new model development;model development;new model;development new;model;development;new", "pdf_keywords": ""}, "7fb1262d4484732c8f7295fa5fb5e6ed6eabb6a0": {"ta_keywords": "energy market models;energy market;locational marginal pricing;based energy market;market simulation provided;formulation market simulation;market simulation;market models;leads energy market;modeling formulation market;marginal pricing lmp;market models widely;pricing lmp based;marginal pricing;influence prices nodal;pricing lmp;influence prices;lmp based energy;locational marginal;pricing;mainly influence prices;formulation market;market;prices nodal;modeling formulation;leads locational marginal;prices nodal lamp;prices;based energy;models widely", "pdf_keywords": ""}, "9f208842f70503e8b71fd4c34ba682dcd0ea4788": {"ta_keywords": "adaptively design incentives;design incentives principal;incentives principal agent;principal agent problems;adaptive inentive design;design incentives;agents decision making;agent problems principal;incentives principal;agents decision;adaptively design;agents particular principal;principal objective depends;principal agent;particular principal objective;decision makers agents;principal objective;multiple agents particular;agent problems;control theoretic optimization;multiple agents;makers agents decision;adverse selection interaction;strategic decision makers;adaptive inentive;agents particular;adverse selection;inentive design;incentives;interaction multiple agents", "pdf_keywords": "adaptive incentive design;method adaptive incentive;adaptive incentive;learning incentive design;learning incentive;iteratively estimating preferences;utility learning incentive;preferences designing incentives;estimating preferences designing;optimization incentive algorithm;optimization incentive;optimize incentive;estimating preferences;incentive algorithm;incentive algorithm based;learning step incentive;algorithm optimization incentive;simultaneously designing incentives;incentive design algorithm;designing incentives change;designing incentives derive;utility learning game;control online learning;adaptive control online;incentives derive algorithm;preferences learned asymptotically;designing incentives;optimize incentive mapping;optimize utility learning;based utility learning"}, "f136a0fdc2065485c83396ae41d431395de51af4": {"ta_keywords": "introductionconference peer review;peer review;qualified reviewers;introductionconference peer;review constitutes human;qualified reviewers growing;ai conferences challenged;pool qualified reviewers;peer review constitutes;ai conferences;reviewers;leading ai conferences;reviewers growing;review process;human computation process;review process increasing;constitutes human computation;identifies best submissions;review;peer;human computation;review constitutes;ai;submissions;submissions acceptance ultimately;conferences challenged;best submissions acceptance;submissions acceptance;number submissions;impacts future research", "pdf_keywords": "reviewers large conferences;conference peer review;reviewer recruiting participants;experiment recruiting reviewers;reviewer pool recruited;reviewer selection process;recruiting reviewer pool;recruiting reviewers;recruited reviewers;reviewer pool;pool qualified reviewers;main reviewer pool;recruiting reviewers population;reviewer pool experiment;recruiting methods reviewers;reviewer selection;curated reviewers experimental;conference peer reviewed;icm reviewer pool;reviewer pool compare;reviewer recruiting;reviewer pool icm;qualified reviewers;number recruited reviewers;reviewer pool special;reviewers review process;ways reviewer recruiting;review process selection;lenient curated reviewers;significantly reviewers"}, "b24e2c3983c3207b1c7124c48d691cf459a3197b": {"ta_keywords": "bayesian speech language;bayesian speech;background bayesian speech;speech language processing;bayesian machine learning;speech recognition;speech recognition speaker;automatic speech;latent topic models;including automatic speech;mixture models gram;automatic speech recognition;gram models latent;topic models applications;hidden markov models;topic models;gram models;mixture models;learn apply bayesian;models gram models;bayesian machine;bayesian inferences;speech language;approximate bayesian inferences;models latent topic;language processing methods;markov models gassian;language processing;apply bayesian machine;markov models", "pdf_keywords": ""}, "0c0d9ecde0efead75e15353ac6c179c4fc22bdda": {"ta_keywords": "local nash equilibria;equitablelibria continuous games;local nash equitablelibria;nash equitablelibria continuous;equilibria continuous games;characterization local nash;nash equilibria continuous;convex strategy spaces;characterizing local nash;constitute local nash;non convex strategy;nash equilibria;continuous games;continuous games infinite;local nash;games infinite dimensional;nash equitablelibria;equitablelibria continuous;strategy spaces;nash equilibria term;strategies constitute local;strategy spaces methods;continuous games aims;convex strategy;equilibria continuous;equilibria;conditions ensuring strategies;equilibria term points;games infinite;equilibria term", "pdf_keywords": "local nash equilibria;equilibria continuous games;nash equilibria continuous;differential nash equilibria;local nash equilibrium;local differential nash;nash equilibria isolated;differential game modeled;characterizing local nash;differential nash equilibriums;nash equilibrium finitely;nash equilibrium degenerate;nash equilibria structurally;defining differential nash;game modeled dynamics;differential nash equilibrium;generic local nash;degenerate differential nash;characterization local nash;modeled dynamics game;theorem differential game;define nash equilibrium;loop differential game;continuous games;nash equilibria;continuous games finite;differential nash;nash equilibria generic;guaranteeing differential nash;continuous games infinite"}, "e8b026b36d8be73ed428f7e4e55c26b27c34a544": {"ta_keywords": "simultaneous determination electroactive;bifunctional electrochemical sensor;electroactive analytes device;determination electroactive;electroactive analytes;non electroactive analytes;electrochemical sensor simultaneous;electrochemical sensor;determination electroactive non;bifunctional electrochemical;case bifunctional electrochemical;electrochemical;electroactive non electroactive;electroactive;analytes device;electroactive non;analytes device used;non electroactive;sensor simultaneous determination;analytes;used simultaneous determination;simultaneous determination;sensor simultaneous;sensor;bifunctional;case bifunctional;device used simultaneous;report case bifunctional;determination;used simultaneous", "pdf_keywords": ""}, "f20d7185c47ce55cdcd9b839ef6fce595baba029": {"ta_keywords": "occurrence plethora plethora;occurrence plethora;speech relation occurrence;speech;speech speech;plethora plethora plethor;plethora plethora plethora;literature speech speech;plethora plethor;plethora plethora;speech speech relation;literature speech;speech relation;review literature speech;plethora;relation occurrence plethora;plethor;occurrence;literature;systematic;article present;relation occurrence;present results systematic;systematic review literature;article present results;systematic review;results systematic review;results systematic;purpose article present;review literature", "pdf_keywords": ""}, "3ebed41fa35e5902b692a3e380c7c9a035c04426": {"ta_keywords": "implications modern ai;ethical questions surround;modern ai research;ethical questions;engage ethical questions;ethical questions just;dealing ethical questions;ai research;ai research far;modern ai;students engage ethical;workforce ethical questions;ethical;dealing ethical;mechanized workforce ethical;ai;engage ethical;workforce ethical;surround dealing ethical;machines modern military;machines modern;cultural political implications;world advanced control;machines;modern military;political implications modern;implications modern;advanced visualizations image;visualizations image;research far concern", "pdf_keywords": ""}, "6b02fe6e0f6b2120a08e098513511e15a05f9073": {"ta_keywords": "detecting dataset shift;warnings machine learning;machine learning systems;dataset shift identifying;methods detecting dataset;detecting dataset;machine learning;building machine learning;dataset shift;learning systems fail;detecting;shift identifying;investigating methods detecting;software systems warnings;systems warnings machine;methods detecting;learning systems depending;learning systems;shift identifying exemplars;warnings machine;dataset;systems warnings;shift;inputs assumption tend;fail silently methodsthis;software systems;learning;inputs assumption;inputs designed software;typify shift", "pdf_keywords": "detecting dataset shift;dataset shift detection;domain shift detection;detecting shifts data;dataset shift identifying;detecting shifts;useful detecting shifts;detecting shift data;detect shifts;testing shift detection;detect large shifts;shift detection surprisingly;method detecting shift;dataset shifts;shift detection data;specific dataset shifts;domain classifiers demonstrate;generated shift detection;shifting shift detection;detecting shift;large dataset shift;domain classifier confidently;domain discriminating classifier;domain classifiers;shift detection best;dataset shifts present;shift detection;detecting shift dynamics;used detect shifts;shift detection useful"}, "3483d04a89dd69afd7b1393eadd8e8e4c5376d59": {"ta_keywords": "unsupervised conceptual clustering;conceptual clustering;conceptual clustering algorithm;clustering enhance classification;cobweb unsupervised conceptual;hierarchical clustering;classification presented based;introductionusing hierarchical clustering;based cobweb unsupervised;unsupervised conceptual;hierarchical clustering enhance;clustering;classification presented;disambiguation;classification;cobweb unsupervised;clustering algorithm modifications;disambiguation built;proposed improved classification;clustering enhance;clustering algorithm;enhance classification;enhance classification accuracy;new approach classification;improved classification;approach classification presented;corpus;classification accuracy new;improved classification accuracy;classification accuracy", "pdf_keywords": ""}, "b1a8c6de4fbfe485c8f1c7723404467b72788ff2": {"ta_keywords": "deep learning healthcare;breakthroughs deep learning;modeling multivariate clinical;deep learning;standard machine learning;learning healthcare;multivariate clinical time;clinical time series;recent breakthroughs deep;learning healthcare group;multivariate clinical;breakthroughs deep;pioneering regression;pioneering regression modeling;machine learning setup;machine learning;work pioneering regression;healthcare group;healthcare group work;healthcare;deep;clinical time;regression modeling multivariate;data focus recent;learning;modeling multivariate;clinical;regression;recent breakthroughs;discuss recent breakthroughs", "pdf_keywords": ""}, "b6b76f529d273a35180d0dc65912db1538539067": {"ta_keywords": "metadata document categorization;categorization text metadata;document categorization;document categorization aims;document classification concerned;document classification;supervised document classification;supervised categorization text;text metadata;categorization text;topic label document;text metadata document;supervised document;conventional supervised document;metadata document;metadata;topic label;supervised categorization;minimally supervised categorization;categorization;metadata domains text;classification;label document;classification concerned;categorization aims;context minimally supervised;assign topic label;tags;categorization aims assign;minimally supervised", "pdf_keywords": "categorization text metadata;categorize text metadata;contextdocument categorization;supervised document classification;metadata minimal supervision;supervised categorization text;text metadata minimal;text metadata;categorize text;framework categorize text;document classification;text classifications;text metadata fundamental;document classification concerned;text metadata implement;relationships text metadata;supervised document;text metadata information;metacat framework categorize;documents various metadata;text classification heterogeneous;conventional supervised document;text metadata method;framework metacat embedding;text classification;based text classifications;metacat embedding;text metadata demonstrate;encode text metadata;data text metadata"}, "ac713aebdcc06f15f8ea61e1140bb360341fdf27": {"ta_keywords": "language processing adversary;model extraction natural;language model bert;pretrained language model;problem model extraction;model extraction;extraction natural language;adversary victim model;model bert;victim model attempts;language model;access victim model;processing adversary query;victim model fine;processing adversary;clinical messagewe study;model attempts;adversary query;clinical messagewe;large pretrained language;natural language;key clinical messagewe;natural language processing;adversary victim;victim model;model attempts reconstruct;pretrained language;adversary does need;language processing;model assumptions adversary", "pdf_keywords": "adversary extract models;language processing adversary;extract models tasks;powerful language models;adversary able extract;attacker extract model;extracting nonsensical language;adversary extract;extracting good models;able extract models;nonsensical language models;user extract models;improving model extraction;task adversary able;simple task adversary;attack adversary extract;extract models;extract models diverse;language models;model extraction ability;model extraction natural;extract knowledge large;language models new;extract models simple;task adversary;extraction good models;heuristics extract models;language model bert;language models matching;processing adversary query"}, "3671dabbfd2e854060e1e382bad96b6bb00fcb46": {"ta_keywords": "robust auditory auditory;robust auditory;noise robust auditory;exemplars signals;auditory auditory auditory;auditory auditory;combination exemplars signals;auditory;exemplars signals proved;noise robust;exemplar based approaches;signals sparse;performance noise robust;signals sparse linear;linear combination exemplars;exemplar based;signals;model signals sparse;approaches model signals;performance noise;signals proved;noise;model signals;combination exemplars;exemplars;art performance noise;exemplar;sparse linear combination;backgroundthe exemplar based;backgroundthe exemplar", "pdf_keywords": ""}, "6bea71fa6deb19c67e9586428f8f240e789fb3df": {"ta_keywords": "linear stochastic bandit;stochastic bandit;bandit problem linear;bandit problem;bandit problem particular;armed bandit problem;multi armed bandit;bandit;armed bandit;stochastic multi armed;probability constant regret;algorithms stochastic;algorithms stochastic multi;auer ub algorithm;algorithm linear stochastic;performance algorithms stochastic;linear stochastic multi;ub algorithm anuer;stochastic multi;linear stochastic;stochastic;ub algorithm;constant regret;auer ub;problem linear stochastic;achieves high probability;empirical performance algorithms;constant regret importantly;regret;modification auer ub", "pdf_keywords": "linear stochastic bandit;stochastic bandit problems;stochastic bandit;various stochastic bandit;stochastic bandit problem;linear stochastic banditwe;bandit problem regret;stochastic banditwe;bandit problem method;bandit problem linear;bandit method;bandit problems;bandit problems particular;bandit problem studied;bandit method effective;stochastic banditwe develop;armed bandit problem;bandit problem;arm bandit method;bandit problem particular;method armed bandit;arm bandits learning;bandits learning;bandit problem multi;bandits learning established;regret algorithm;multi arm bandits;multi armed bandit;regret algorithm good;arm bandit problem"}, "2f0221142db900e75bd9c54fa153fb770a72f672": {"ta_keywords": "russian using crowdsourcing;crowdsourcing;synset assembly;describes synset assembly;synset assembly interface;using crowdsourcing;thesaurus russian using;yar russnet project;crowdsourcing paper describes;crowdsourcing paper;open thesaurus russian;thesaurus russian;using crowdsourcing paper;russnet project;russnet project started;describes synset;synset;paper describes synset;thesaurus;open thesaurus;large open thesaurus;yar russnet;assembly interface developed;assembly;assembly interface;russnet;interface developed project;russian using;interface developed;developed project", "pdf_keywords": ""}, "7dce2877758b0103d1f7a454c184dc641e123359": {"ta_keywords": "indexing querying annotation;contextstructured retrieval using;querying annotation graphs;document retrieval task;contextstructured retrieval;document retrieval;querying annotation;retrieval using sor;retrieval using;evaluated document retrieval;factoid question answering;retrieval task purpose;retrieval;retrieval task;question answering;task purpose factoid;text collection;purpose factoid;annotation graphs;aquaint text collection;question answering qa;annotation;text collection trec;annotation graphs generated;purpose factoid question;indexing querying;suite indexing querying;factoid;querying;software suite indexing", "pdf_keywords": ""}, "309b2c75dcdafea19a053876e56cef9747d428fb": {"ta_keywords": "extended recurrent neural;recurrent neural networks;recurrent neural;working extended recurrent;language processing tasks;attention handle lattic;extended recurrent;language processing;speech recognition;paradigm self attention;speech recognition hypotheses;attention;natural language processing;self attention;self attention handle;multiple speech recognition;introductionlattices efficient;model lattice inputs;systems natural language;attention handle;neural networks;encode ambiguity upstream;natural language;multiple linguistic;represent multiple linguistic;neural;recurrent;linguistic;multiple speech;linguistic analyses", "pdf_keywords": "attentional models lattice;attentional modeling lattice;attentional lattice model;attentional lattice;latticelstm self attentional;self attentional lattice;attention handle lattice;lattice biased attentional;self attentional decoder;attentional models;attentional modeling;self attentional models;attentional model efficiently;self attentional modeling;attentional decoder;attentional model based;attentional model;approach attentional decoder;self attentional encoder;decoder attention;attentional decoder design;attentional encoder;attentional model model;biased attentional decoder;attentional encoder based;attentional decoder attention;attentional modeling approach;attentional decoder use;sequential self attentional;self attentional model"}, "975551547fef77605fb85a551bbd7523b77746b7": {"ta_keywords": "automatic repository classification;repository classification;github repositories labels;repository classification problem;github repositories;majority github repositories;github;repositories labels;topic based search;introduction github important;github important platform;introduction github;majority github;automatic repository;search topic based;search topic label;repositories;github important;introduced majority github;massive number repositories;repositories labels impeding;based search topic;targets automatic repository;keyword driven hierarchical;topic label functionality;repositories available;topic label;topic based analysis;repositories available pressing;repository", "pdf_keywords": "repositories keyword enrichment;embedding keyword enrichment;hierarchical classification github;automatic classification github;automatically classification github;classification github repositories;keyword enrichment;potential mining github;classification github;employ keyword enrichment;keyword enrichment enrich;keyworddriven hierarchical classification;introduce keyword enrichment;implement keyword enrichment;github repositories keyword;propose keyworddriven hierarchical;hierarchy keywords train;mining github;keyword enrichment module;use keyword enrichment;keyword enrichment pseudo;network embedding keyword;github diagnosis classification;label hierarchy keywords;keywords linked;keywords train classification;keyworddriven hierarchical;repositories keyword;information keywords linked;learn node embeddings"}, "46ed42e4318e1363a0ec3dde195422cdfecf2017": {"ta_keywords": "phrasal compositionality model;analysis phrasal compositionality;phrasal compositionality;phrase embeddings;phrase embeddings methods;powerful phrase embeddings;phrase embeddings approach;compositionality model;compositionality;embeddings methods propose;methods propose contrastive;produce powerful phrase;propose contrastive fine;propose contrastive;embeddings;compositionality model paper;contrastive;embeddings approach;contrastive fine tuning;embeddings methods;contrastive fine;analysis phrasal;model analysis phrasal;paper propose contrastive;embeddings approach ph;powerful phrase;phrasal;phrase;fine tuning objective;generate model analysis", "pdf_keywords": "phrase representation learning;decode phrases context;phrase embeddings;encode phrase relatedness;context identify phrases;capturing phrase semantics;phrase embeddings employ;phrase level semantic;phrase representation;phrases context identify;phrase based neural;algorithm phrase embeddings;identify phrases context;concept phrase representation;contextual information bert;phrase level semantics;efficiently words phrases;phrase based corpus;phrases natural language;phrases context propose;phrase relatedness provide;phrases context demonstrate;phrase relatedness;decode phrases;word embeddings;word embeddings processing;word embedding;phrases heavily rely;embeddings processing text;phrases context employ"}, "d4af2654f97c09741aba9f0da9ace7bc84b9a63f": {"ta_keywords": "time bayesian network;time bayesian networks;bayesian network representation;bayesian network;modeling progression poverty;continuous time bayesian;bayesian networks useful;time bayesian;bayesian networks;causal dynamics evolution;causal dynamics;event driven continuous;event driven;novel event driven;network representation model;progression poverty integrated;bayesian;progression poverty;potential causal dynamics;context event driven;networks useful tool;model situations state;network representation;model parameters graphical;networks;influenced occurrences events;networks useful;modeling progression;poverty integrated social;driven continuous time", "pdf_keywords": ""}, "73a6e4574de038878be1bbb5985400998e420a5b": {"ta_keywords": "price strategyproofing peerer;strategyproofing peerer assessment;peerer assessment price;assessment price strategyproofing;peerer assessment;strategyproofing peerer;price strategyproofing;introductionthe price strategyproofing;assessment price;peerer;strategyproofing;assessment;introductionthe price;price;introductionthe", "pdf_keywords": ""}, "78438b61afc2c9123c28ca4d6b58e598462ae9be": {"ta_keywords": "source domain adaptation;domain adaptation;domain specific adversarial;domain adaptation utilizing;domain adaptation md;target domain diverse;specific adversarial training;adversarial training recently;adversarial training;domain diverse multi;domain diverse;specific adversarial;adaptation utilizing task;multi source domains;single source domain;multi source domain;source single target;adversarial;source domains;diverse multi source;target domain;source domain;adaptation;adaptation utilizing;single target setting;source domains using;multi source;domain specific;adaptation md;single target", "pdf_keywords": ""}, "1f0446dddd192e94f3930a3a449bd89796f4200f": {"ta_keywords": "transforms acoustic features;acoustic features adapted;fmr transforms acoustic;regression fmr transforms;transforms acoustic;acoustic features;linear regression fmr;regression fmr;fmr transforms;model space adaptations;features adapted;features adapted ones;space adaptations cmlr;adaptation performed preprecessing;adaptations cmlr;adaptations cmlr improve;realizes efficient adaptation;preprecessing independent decoding;efficient adaptation;acoustic;efficient adaptation performed;adaptation;space maximum likelihood;space adaptations;transforms;independent decoding;likelihood linear regression;fmr;model space;adaptations", "pdf_keywords": ""}, "2226560f94c1e90d6900d4674b649cc5522b78cc": {"ta_keywords": "translation models multilingual;attentional translation models;self attentional translation;models multilingual neural;sharing single translation;multilingual self attentional;single translation model;multilingual parameter sharing;multilingual neural machine;translation models;multilingual neural;neural machine translation;multilingual parameter;translation models able;attentional translation;translation model;models multilingual;bilingually trained models;accuracy translation models;single translation;translation model multiple;multilingual self;methods multilingual self;machine translation;uniform multilingual parameter;sharing methods multilingual;multilingual;improvements uniform multilingual;machine translation shown;translation shown sharing", "pdf_keywords": "multilingual translation neural;multilingual translation experiments;multilingual machine translation;multilingual translation model;task multilingual machine;model multilingual translation;learning ml translation;task multilingual;translation models;translation neural machine;translation neural;translation task target;performance multilingual translation;machine translation bilingually;multilingual machine;translation models aim;translation fully shared;multilingual translation;multilingual translation fully;machine translation important;method multilingual translation;translation application neural;perform multilingual translation;translation systems;translation task;translation task using;machine translation;multilingual bilingual model;neural machine translation;employ multilingual translation"}, "04b91791225a4f86b0715b41c6f56c00c197d810": {"ta_keywords": "accessed conversion bandwidth;conversion bandwidth;conversion bandwidth important;code conversion;resource efficient conversions;resource optimize conversions;limits conversion bandwidth;code conversion places;approach code conversion;convertible codes focused;resources convertible codes;efficient conversions existing;optimize conversions;nodes accessed conversion;accessed conversion;convertible codes;efficient conversions;convertible codes recently;optimize conversions paper;conversions existing;work convertible codes;bandwidth important resource;conversion;bandwidth;conversions existing work;codes enabling resource;conversions;conversion places;codes focused minimizing;proposed class codes", "pdf_keywords": "distributed storage codes;codes distributed storage;regenerating codes distributed;storage codes distributed;codes efficient conversion;erasure codes distributed;coding distributed storage;storage codes;regenerating codes;convertible codes bandwidth;bandwidth convertible codes;codes bandwidth optimal;codes erasure codes;generating vector codes;base code reconstructed;encoding reencoding;codes merge regime;erasure codes designed;code reconstructed;codes conversion;storage codes important;vector codes merge;codes propose conversion;encodes message symbols;convertible codes erasure;codes conversion bandwidth;vector codes constructed;optimal convertible codes;code conversion distributed;codes distributed"}, "43fe2d8781473360eeaae7a3284169a303200846": {"ta_keywords": "detection fake news;fake news challenge;news stance classification;fake news stance;news challenge;development fake news;stance classification task;news challenge models;news stance;stance classification;fake news paper;2017 fake news;news paper present;fake news;detection fake;news paper;news;challenge models detection;models detection fake;2017 fake;entry 2017 fake;stance;classification task;challenge models;development fake;leader board;leader board methodswe;classification;place leader board;11th place leader", "pdf_keywords": ""}, "d9b89de5c2a39479768c6e32f13ac3e816635cc1": {"ta_keywords": "training translation models;translation models;learning lexical translation;computer aided translation;lexical translation parameters;text training translation;translation models language;word lattices transcription;aided translation;aided translation existing;training translation;translation parameters directly;translation parameters;model learning lexical;lexical translation;models language pairs;paired written translation;directly word lattices;parallel text training;speech recognition speech;automatic speech recognition;automatic speech;speech recognition;translation existing approaches;lattices transcription sought;lattices transcription;recognition speech;learning lexical;language pairs data;word lattices", "pdf_keywords": ""}, "44e24aabd05bef8cb45646486f1a24b7caecee45": {"ta_keywords": "sequence speech recognition;sequenceence sequence speech;sequence speech;speech recognition architecture;dann hmm systems;speech recognition;low resource speech;backgroundmultilingual sequenceence sequence;10 boachi languages;resource speech research;backgroundmultilingual sequenceence;boachi languages;hmm systems;speech research;resource speech;conventional dann hmm;speech research relatively;speech research approach;boachi languages build;hmm systems methodsin;recognition architecture low;direction speech research;data 10 boachi;training using lexicon;speech;sequenceence sequence;sequenceence;10 boachi;model training;boachi", "pdf_keywords": "decoding attention cc;attention cc models;resource automatic speech;multilingual speech recognition;attention decoding;decoding attention;extract attention decoding;decoding multilingual neural;sequence seq2seq training;integrating attention connectionist;sequence neural;multilingual model trained;sequence neural network;training decoding;training decoding use;attention decoding attention;attention connectionist temporal;language model boosting;models extract attention;model trained multilingual;automatic speech;multilingual retraining seq2seq;sequenceto sequence neural;decoder aid attention;speech recognition toolkit;trained multilingual approach;multilingual neural network;speech recognition;multilingual speech;attention connectionist"}, "8b468872cf915c98ff46a2bea4d2a34112b7b0b0": {"ta_keywords": "relational classification retrieval;learning relational features;relational classification;relational features backward;learning relational;relational features;long relational rules;relational rules long;path ranking algorithm;rules long relational;walks path ranking;relational rules;path ranking;introduction learning relational;address relational classification;algorithms searching features;relational rules class;ranking algorithm pra;long relational;searching features;ranking algorithm;searching features resul;space relational rules;classification retrieval;relational;classification retrieval tasks;faster algorithms searching;larger space relational;algorithms searching;ranking", "pdf_keywords": ""}, "d8704a63517868475b3af7ec25eaa2fb2a44362b": {"ta_keywords": "cnn loss minimization;cnn loss;segmentation gradient descent;shallow segmentation gradient;weakly supervised cnn;dominate cnn loss;backgroundvariants gradient descent;models shallow segmentation;segmentation gradient;regularized losses;regularized losses inspired;approach regularized losses;context weakly supervised;descent dominate cnn;supervised cnn segmentation;shallow segmentation;loss minimization;cnn segmentation;cnn segmentation present;gradient descent context;weakly supervised;gradient descent;vision powerful loss;supervised cnn;backgroundvariants gradient;powerful loss functions;loss functions practically;optimization gradient descent;cnn;gradient descent fails", "pdf_keywords": "segmentation regularized loss;segmentation loss minimization;loss minimization deep;shallow segmentation loss;loss segmentation good;regularized loss segmentation;loss segmentation;segmentation loss;minimization deep;neural segmentation regularized;regularized losses network;regularized loss better;method regularized loss;losses network training;regularized loss optimal;loss minimization;regularized loss;minimizing regularized losses;method training deeper;minimization deep neural;segmentation regularized;shallow segmentation;optimizing regularized losses;training losses grid;network regularized losses;losses weakly supervised;deep neural segmentation;gradient descent alternative;training deeper;method shallow segmentation"}, "6dd6d4dfc3cf9ff41aad7e903cf1294de2ac5629": {"ta_keywords": "driving systems safety;validation automated driving;automated driving systems;safety assessment;automated driving;driving systems;automated vehicles;safety assessment efficient;systems safety;test systems safety;introduction automated vehicles;systems safety deployment;vehicles market safety;automated vehicles market;systems safety biggest;verification validation automated;traffic situations identified;validation automated;safety;safety deployment;safety biggest challenges;safety deployment real;traffic situations;market safety assessment;limited traffic situations;introduction verification validation;verification validation;validation;driving;parameter ranges crucial", "pdf_keywords": ""}, "1288d6570085a28518a9f3495e77dbb75899421c": {"ta_keywords": "active annotation;applied active annotation;active annotation named;active annotation main;introductionactive annotation paper;introductionactive annotation;annotation named entity;introduces semi supervised;annotation;initially annotate;annotator applied active;annotate;semi supervised learning;annotation paper introduces;material active annotation;human annotator;annotation paper;active learning;entity recognition biomedical;human annotator applied;annotation named;semi supervised;active learning framework;annotate imperfectly data;annotation main;annotate imperfectly;initially annotate imperfectly;annotator;corrected human annotator;named entity recognition", "pdf_keywords": ""}, "163a67b5b0371035fa6e0f88b36ba97a32e735bc": {"ta_keywords": "code data encoded;introduce convertible codes;code conversion;convertible codes;data encoded nf;code conversion process;converting data encoded;ki code data;encoded nf;notion code conversion;encoded nf kf;convertible codes new;desired decodability properties;data encoded ni;data encoded;code data;nf kf code;maintaining desired decodability;desired decodability;kf code;formalize notion code;decodability properties maximum;decodability properties;kf code maintaining;ki code;encoded ni ki;encoded;decodability;encoded ni;ni ki code", "pdf_keywords": "distributed storage codes;codes storage systems;codes distributed storage;coding distributed storage;coded storage systems;coded storage;erasure coded storage;coding codes storage;codes storage;storage codes;regenerating codes distributed;erasure code powerful;systems erasure code;erasure codes reliable;storage codes increasing;codes provide durability;erasure codes provide;erasure codes;codes efficiently;codes merge regime;convertible codes stable;stable convertible codes;storage systems erasure;regenerating codes;code resource efficient;reliable data storage;use erasure codes;codes distributed;codes reliable data;storage nodes redundancy"}, "027c5e44164a2ee3543ecdff73cd4d7888a42a90": {"ta_keywords": "preferences ai;preferences ai expect;reasoning preferences;reasoning preferences important;sciences preferences ai;preferences decisions adhere;preferences necessary understand;learning reasoning preferences;preferences decisions;consistent preferences decisions;introductionpreference central decision;decision making machines;consistent preferences;recommendations consistent preferences;preferences important;preferences important area;ai expect;introductionpreference;make decisions recommendations;social sciences preferences;preferences necessary;introductionpreference central;ai;decisions recommendations consistent;preferences;working preferences;guidelines working preferences;decisions recommendations;ai expect make;working preferences necessary", "pdf_keywords": ""}, "c17ccb7f0372ec98b7e070b0f70518f28516ecd5": {"ta_keywords": "course stochastic processes;stochastic processes;course stochastic;stochastic;stochastic processes format;notes course stochastic;processes;control applied mathematics;moscow institute ofphysics;control applied;mathematics informatics moscow;informatics moscow institute;applied mathematics informatics;informatics moscow;mathematical foundations control;institute ofphysics;ofphysics;mathematics informatics;informatics;control;department mathematical foundations;processes format;department mathematical;students department control;ofphysics technology;professors department mathematical;stoc;applied mathematics;mathematical;institute ofphysics technology", "pdf_keywords": "tetrahydrozyme etiology patient;tetrahydrozyme etiology;new tetrahydrozyme etiology;management patient tetrahydrosis;tetrahydrozyme aim article;patient tetrahydrosis aim;tetrahydrosis;tetrahydrosis aim article;patient tetrahydrosis;tetrahydrosis aim;tetrahydrozyme;new tetrahydrozyme aim;tetrahydrozyme aim;new tetrahydrozyme;tetrahydrozyme development new;knowledge tetrahydrozyme tetrahydrozyme;tetrahydrozyme role;current knowledge tetrahydrozyme;tetrahydrozyme development;role tetrahydrozyme;tetrahydrozyme tetrahydrozyme;knowledge tetrahydrozyme;development new tetrahydrozyme;tetrahydrozyme tetrahydrozyme role;role tetrahydrozyme development;tetrahydrozyme role development;approach diagnosis etiology;symptomatic asymptomatic history;patient diagnosed symptomatic;knowledge role tetrahydrozyme"}, "23e03cd57b5d75993545127f3fecf99d25021583": {"ta_keywords": "cancer biology precision;oncology present deep;biology precision oncology;genomic impact transformer;genomic impact;referred genomic impact;encoder;cancer biology;model encoder;precision oncology;network model encoder;deep neural;precision oncology present;task cancer biology;genomic alterations sgs;model encoder decoder;somatic genomic alterations;encoder decoder;somatic genomic;deep neural network;genomic alterations;encoder decoder architecture;genomic;architecture referred genomic;infer functional impact;oncology;referred genomic;fundamental task cancer;caused somatic genomic;cellular signaling", "pdf_keywords": "tumor embeddings predict;tumor embedding representations;tumor embeddings predictive;generate tumor embeddings;generating tumor embeddings;genomics impact cancer;tumor embeddings generate;cancer genomics impact;cancer genomics;personalized tumor embedding;predicting degs cancer;degs cancer genomics;embeddings cancer;deep learning genomic;gene embeddings tumors;tumorgene embeddings;tumorgene embeddings fundamental;gene embeddings tumor;embeddings tumor embeddings;embeddings cancer patients;produce tumor embeddings;embeddings tumors generate;cancer genes;tumor embeddings cancer;gene embedding;genes important cancer;mutations tumorgene embeddings;functions tumor embeddings;gene embeddings;gene embedding tumor"}, "1d3539a8d94bd3ab78993d7cc584efc06ed0e460": {"ta_keywords": "explaining model predictions;model predictions increasingly;model explainability;model predictions;machine learning models;research model explainability;feature attribution methods;model explainability given;feature attribution;tools explaining model;explaining model;learning models;models;learning models grow;predictions increasingly important;predictions increasingly;machine learning;explainability;models grow complex;attribution methods leime;explainability given rise;attribution methods;introduction machine learning;rise feature attribution;models grow;empirical evaluation metrics;attribution;model;evaluation metrics data;evaluations ideally", "pdf_keywords": "synthetic datasets explainability;real dataset explainability;datasets explainability;datasets allowing explainability;datasets explainability machine;explainability machine learning;dataset explainability;synthetic datasets models;metric synthetic datasets;dataset evaluating explainers;synthetic datasets;provide synthetic datasets;dataset explainability benchmarking;synthetic datasets allowing;benchmarking feature attribution;realistic synthetic datasets;explainability evaluation metrics;explainability methods benchmarked;machine learning provide;methods synthetic datasets;interpretability benchmarks literature;metrics explainability methods;datasets ground truth;interpretability benchmarks;benchmarking explainability;benchmarking explainability simulated;datasets accuracy explainers;synthetic datasets showcase;neural network interpretability;implementation synthetic datasets"}, "8a14b3a9e642f4ca7fad4df997fc1941bdcfb935": {"ta_keywords": "patient history history;patient history;case patient history;history history;history history history;history;present case patient;case patient;article present case;purpose article present;article present;purpose article;article;patient;present case;present;case;purpose", "pdf_keywords": ""}, "ffecb8b8b415149f5351b64a2dbb1a1fa64219f0": {"ta_keywords": "introductionmultilingual speech translation;speech translation;end speech translation;speech translation st;speech translation problem;machine translation mt;machine translation;ar machine translation;descriptionwhile multilingual models;automatic speech;utterances source languages;useful automatic speech;multilingual models;introductionmultilingual speech;automatic speech recognition;languages directly translated;speech recognition ar;multilingual models shown;descriptionwhile multilingual;speech utterances source;speech recognition;languages universal sequence;case descriptionwhile multilingual;speech utterances;translation mt;desired target languages;end end speech;multilingual;problem speech utterances;directly translated desired", "pdf_keywords": ""}, "9c78481004b7dbb601b83cc081ec23c02e6f5270": {"ta_keywords": "equilibria learning dynamics;equilibria learning;stable equilibria learning;learning dynamics necessarily;learning dynamics;equilibrium differential nash;differential nash equilibrium;nash equilibrium;player continuous games;continuous games;continuous games methods;nash equilibrium set;local equilibrium;differential nash;notion local equilibrium;seeking equilibrium;gradients player continuous;learning processes games;equilibrium;equilibrium set locally;games methods;grapple seeking equilibrium;individual gradients player;games methods games;stable equilibria;local equilibrium differential;equilibrium study natural;methods games;games explain players;seeking equilibrium study", "pdf_keywords": "stability differential nash;differential nash equilibria;equilibria stable nash;local stability nash;stability non nash;nash equilibrium stability;equilibria continuous games;equilibrium differential nash;stability nash optimality;learning player continuous;stability nash;differential nash equilibrium;learning dynamics;stable nash;stability game dynamics;local equilibrium game;stable nash robust;equilibria player scalar;non nash equilibria;differential nash;nash equilibrium;scalar games equilibria;scalar continuous games;equilibrium stability game;game dynamics fundamental;game dynamics;equilibrium game;games equilibria stable;player continuous games;stability game"}, "8b231737e0048a400527d89aa56c712e8b9bc690": {"ta_keywords": "end speech translation;speech translation;speech translation st;sequence architecture multilingual;multilingual end end;multilingual end;utterances source languages;translation st speech;automatic speech;useful automatic speech;multilingual models;end end speech;automatic speech recognition;framework multilingual end;languages directly translated;architecture multilingual models;end speech;speech utterances source;multilingual models shown;desired target languages;speech recognition;multilingual;st speech utterances;speech utterances;architecture multilingual;utterances source;languages universal sequence;target languages;source languages directly;target languages universal", "pdf_keywords": "multilingual speech translation;speech translation multilingual;translation models multilingual;speech translation task;multilingual translation feasible;multilingual translation task;model translation multilingual;processing speech translation;multilingual models trained;multilingual training decoding;translations multilingual models;end speech translation;speech translation research;memory speech translation;translation multilingual languages;descriptionwhile multilingual models;machine translation models;multilingual speech;multilingual translation multilingual;translation models;translation multilingual translation;bilingual multilingual models;multilingual pretraining;resulting speech translation;developments multilingual speech;speech translation corpus;multilingual models performing;multilingual models multilingual;translation multilingual;multilingual end"}, "da20ab7724335eb48bcd0e9be30f0ac4b6a464c6": {"ta_keywords": "novelty detection network;novelty detection;introduction novelty detection;detection network saliency;network saliency visual;network saliency;saliency visual;detection network;saliency visual based;learning driven safety;saliency;safety critical autonomous;detect situations trained;novelty;detection;visual based deep;able detect situations;cars able detect;deep learning;introduction novelty;driven safety critical;trained model;trustworthy prediction viewed;trustworthy prediction;detect situations;self driving cars;machine learning driven;driven safety;detect;situations trained model", "pdf_keywords": "model novelty detection;novelty detection;novelty detection propose;trained model novelty;detection novel scenarios;novelty detection method;salient features trained;detect situations trained;analyzing salient features;extract salient features;learning driven safety;features trained;salient features input;salient features;detection novel;trained making predictions;novel scenarios vision;trained prediction;features trained model;detection propose;leverages salient features;detect situations;visual saliency;detection;trustworthy prediction;able detect situations;model novelty;image based trained;make trustworthy prediction;determine new image"}, "e54e0d9eaa922cefb1c69e105979399fd34497b1": {"ta_keywords": "fairness aware learning;learning fair classifiers;fair classifiers;fair classifiers partially;annotated demographic group;demographic group labels;fairness aware;fully annotated demographic;recently fairness aware;annotated demographic;annotated group labels;learning fair;group label annotations;group labels increasingly;classifiers partially annotated;label annotations expensive;partially annotated group;annotated group;label annotations;applications recently fairness;labels emphasize assumption;group labels;classifiers partially;labels increasingly;fairness;classifiers;annotations expensive;group label;introduction learning fair;group labels emphasize", "pdf_keywords": "fairness group classifier;fairnessaware learning;fairness partially annotated;applicable fairnessaware learning;fairness methods group;fairnessaware learning method;algorithmic group fairness;group classifier fair;fair machine learning;fairness methods explicitly;group fairness partially;methods group fairness;fair classification;group fairness;fair training methods;fair training method;fairness group;group labels fair;fair classification present;fairness methods;readily applicable fairnessaware;fair classifiers;group label fair;fairness improved benchmarks;method fair training;group fairness considered;accuracies fairness improved;fair classifiers use;fairness development neural;training method fair"}, "dcb28c8ba94434eb8a06e81eb55bfdbc343d2340": {"ta_keywords": "sentence relation extraction;relation extraction generally;relation extraction;information extraction;information extraction methods;cross sentence relation;context information extraction;involve entity mentions;relations involve entity;sentence relation;binary relations;binary relations expressed;extraction methods focus;relations expressed single;entity mentions far;focus binary relations;precision oncology relations;relations involve;entity mentions;oncology relations involve;apart document;far apart document;relations;text spans;small text spans;relations expressed;sentences high value;single sentences high;involve entity;extraction methods", "pdf_keywords": "sentence relation extraction;relation extraction;extraction relations sentences;relation extraction generally;relation extraction approaches;relation extraction extract;extract relations entities;relation extraction model;extraction extract relations;extraction relations;relation extraction employ;relation extraction approach;extract relations document;ary relation extraction;extract relations;cross paragraph extractions;relation extraction method;involve entity mentions;information extraction;used extract relations;relation useful tool;paragraph extractions;relations document single;document single sentences;prediction entity tuple;entity centric formulation;entity mentions;context information extraction;information extraction methods;document level extraction"}, "900b785dbbea7ccd5846eafb14c6715f76fe5e00": {"ta_keywords": "infer devices compromised;devices compromised;devices compromised initial;byod guest wireless;device byod guest;guest wireless networks;guest wireless;channel analysis ping;devices owned enterprise;dynamic anomaly detection;analysis ping;administrators devices owned;owner device byod;anomaly detection;infer devices;bring owner device;analysis ping responses;devices owned;ping responses infer;administrators devices;device byod;challenge administrators devices;anomaly detection method;wireless networks use;difficult security;responses infer devices;demonstrated dynamic anomaly;networks use mobile;devices industry government;ping responses", "pdf_keywords": ""}, "2a81081c987da2bb8184b8e9a884cf6a73712ee8": {"ta_keywords": "diagnosis pulmonary infection;pulmonary infection;patients diagnosis pulmonary;diagnosis pulmonary;pulmonary;patients diagnosis;infection;management patients diagnosis;diagnosis;patients;management patients;approach management patients;approach management;new approach management;article discuss importance;importance new approach;management;new approach;article discuss;article;approach;aim article discuss;discuss importance new;aim article;discuss importance;new;importance new;importance;discuss;aim", "pdf_keywords": ""}, "126be977c03d732fbef2381565a41b957d41a2cc": {"ta_keywords": "narrative modeling tasks;narrative modeling;contextual representations;context question answering;architectures natural language;question answering fictional;book narrative modeling;question answering;deep learning architectures;answering fictional relationship;relationship understanding comic;fictional relationship understanding;natural language processing;level linguistic context;natural language;high level linguistic;learning architectures natural;deep learning;answering fictional;contextual;comic book narrative;form contextual representations;learning architectures;design deep learning;design deep;linguistic context;book narrative;level linguistic;neural network modules;similar neural network", "pdf_keywords": ""}, "aae8d332c3ff9d081ae36967d3d7b5f394b51bcc": {"ta_keywords": "differentiable self training;semi supervised;semi supervised weakly;various semi supervised;weakly supervised learning;supervised;supervised weakly;self training;supervised learning;supervised weakly supervised;weakly supervised;supervised learning tasks;pseudo labels student;self training achieves;learning tasks;labels student;training instability;learning;pseudo labels;training instability address;generates pseudo labels;student makes predictions;learning tasks method;rule leads training;training;leads training instability;teacher generates pseudo;labels;labels student makes;predictions models", "pdf_keywords": "supervised learning self;semi supervised learning;semi supervised;semi supervised method;method semi supervised;supervised;classification self training;self training classackelberg;supervised learning;use semi supervised;supervised method automatically;self training framework;supervised learning improve;self training method;weakly supervised learning;methods semi supervised;self training adaptive;employ semi supervised;weakly supervised;supervised method;teacher follower formulation;learning self training;supervised learning propose;method self training;differentiable self training;weakly trained;training method;train learning;method weakly supervised;model self training"}, "2270b8628fd8ca67ae39d277f45bc3c38ac63d5f": {"ta_keywords": "distributed tensor computations;distributed tensor;dimension mesh tensorflow;meshh tensorflow;meshh tensorflow language;mesh tensorflow;introduce meshh tensorflow;tensorflow language;tensors operations batch;class distributed tensor;tensorflow;tensor computations;tensor computations data;dimensional mesh processors;mesh tensorflow user;splitting tensors operations;user specify tensor;tensorflow language specifying;computations data parallelism;tensor dimensions split;splitting tensors;tensors operations;viewed splitting tensors;tensorflow user;tensorflow user specify;specify tensor dimensions;tensors;data parallelism;batch dimension mesh;specify tensor", "pdf_keywords": "processors meshh tensorflowwe;distributed tensor computations;tensorflow language distributed;dimension meshh tensorflow;dimensional mesh processors;meshh tensorflow;tensorflow language;introduce meshh tensorflow;tensorflow;tensorflow mesh;meshh tensorflow language;mesh processors;layers mesh processors;com tensorflow mesh;tensors operations batch;mesh processors meshh;computations data parallelism;com tensorflow;10sorflow code neural;language distributed tensor;meshh tensorflowwe;distributed tensor;compute parallelization;tensor computations data;processors meshh;tensor computations;data parallelism train;parallelization;achieve model parallelism;batch dimension meshh"}, "e2bd274c8dd2a3b2a0a6f5d8a29baee07df34eb9": {"ta_keywords": "t2s translation tree;translation tree;translation tree string;efficient accurate translation;machine translation;string t2s translation;t2s translation theoretically;inferior machine translation;machine translation mri;t2s translation;tree string t2s;performance tree string;translation mri methods;translation previous reports;phrase based hierarchical;accurate translation previous;translation theoretically holds;translation theoretically;performance tree;translation mri;hierarchical phrase based;evaluate performance tree;tree string;string t2s;based hierarchical phrase;hierarchical phrase;accurate translation;translation previous;translation;reports t2s", "pdf_keywords": ""}, "98b7d5611c0a128f45db100cc796b981573adcc5": {"ta_keywords": "management malignant diseases;etiology malignant disease;patients malignant disease;malignant diseases;malignant diseases patients;malignant disease;diseases patients malignant;mechanism etiology malignant;management malignant;etiology malignant;patients malignant;malignant disease poorly;malignant;strategy management malignant;diseases patients;disease;diseases;disease poorly understood;mechanism etiology;patients;etiology;disease poorly;strategy management;strategy;new strategy management;management;development new strategy;new strategy;mechanism;poorly understood", "pdf_keywords": ""}, "bcd4c46e4d75ddedb6138cfd77600c6d964a9aa8": {"ta_keywords": "language code models;natural language code;models code pretrained;incorporate program semantics;code pretrained;code models explicitly;program semantics execution;code models;semantics execution;backgroundgenerative models code;program semantics;code pretrained large;semantics execution results;large corpora programs;pretrained large corpora;language code;program generated;explicitly incorporate program;corpora programs;program;program generated set;single correct program;natural language;backgroundgenerative models;correct program generated;programs;translating natural language;training able generate;code;introduce execution", "pdf_keywords": "pretrained language code;language code pretrained;algorithm pretrained language;language code models;code pretrained large;pretrained large language;natural language code;models code pretrained;code models datasets;pretrained large corpora;programs decoding;large corpora programs;set programs decoding;language models learn;code models explicitly;large language model;code pretrained;executed code training;code models;pretrained language;semantics execution results;code training;incorporate program semantics;algorithm pretrained;generated programs;code samples;semantics execution;generated programs especially;dataset translate natural;program semantics execution"}, "8688169ad5701e726968e293ff7dc53d76dd8007": {"ta_keywords": "diagnosis malignant neoplasm;malignant neoplasm clinical;malignant neoplasm;neoplasm clinical;neoplasm clinical features;diagnosis malignant;patient diagnosis malignant;neoplasm;malignant;diagnosis;patient diagnosis;clinical features patient;features patient diagnosis;clinical features;article clinical features;features patient discuss;article clinical;management disease;disease;features patient;patient discuss possible;clinical;patient discuss;implications management disease;patient;aim article clinical;management;article;discuss possible;possible implications management", "pdf_keywords": ""}, "4f78624defde3b60551cfeb37e3943b267ea704a": {"ta_keywords": "distributed learning method;distributed learning;new distributed learning;compression gradient;able learn gradients;compression gradient differences;learn gradients;compression updates methods;regularizers slows convergence;issues compression gradient;based compression updates;learn gradients render;learning method;optimum batch mode;compression updates;learning method dianna;true optimum batch;smooth regularizers;optimum batch;gradient differences perform;regularizers;based compression;gradient differences;batch mode;propose new distributed;method based compression;gradients;non smooth regularizers;gradient;batch", "pdf_keywords": "distributed learning method;distributed learning;efficient distributed optimization;optimization quantized gradients;new distributed learning;quantized gradients sparser;distributed optimization;convex regularizer iteration;strongly convex minimization;distributed optimization large;distributed optimization important;method optimization quantized;optimization quantized;learning model regularized;convex regularizer;gradients sparser training;results quantized gradients;strongly convex objective;sparser training;closed convex regularizer;quantized gradients;regularized empirical;sparser training progresses;convex minimization;model regularized empirical;regularizer iteration complexity;complexity quantization;method stochastic gradient;gradients sparser;stochastic gradients method"}, "dbdefb498b619912a726fec7c85533594a1c6a1b": {"ta_keywords": "introductionminimax optimization smooth;minimax optimization smooth;smooth algorithmic adsversaries;introductionminimax optimization;algorithmic adsversaries;minimax optimization minx;minimax optimization;algorithmic adsversaries methodsthis;adversarial;adversarial networks;optimization minx maxy;training generative adversarial;considers minimax optimization;algorithmic adsversaries aimto;optimization smooth algorithmic;optimization smooth;adsversaries methodsthis paper;nonconcave optimization;optimization minx;adsversaries methodsthis;training generative;evaluate minimax optimization;generative adversarial networks;nonconvex nonconcave optimization;generative adversarial;minimax;nonconcave optimization problems;considers minimax;minx maxy challenging;paper considers minimax", "pdf_keywords": "nonconcave minimax optimization;concave minimax optimization;adversarially robust;gans adversarially robust;gans adversarially;minimax optimization popular;networks gans adversarially;non concave minimax;nonconcave minimax;nonconvex nonconcave minimax;strongly concave minimax;nonconvex concave minimax;minimax optimization;minimax optimization minx;adversarially robust models;nonconcave optimization;adversarial training procedure;concave minimax;minimax optimization problems;concave minimax problems;nonconcave optimization problems;nonconvex nonconcave optimization;adversarial training;networks gans;concave minimax problem;adversarial networks gans;adversarially;minimax optimization addition;adversarial;minimax optimization problem"}, "a3ce3004a0eade48a3ae652dbf5c04a60c2416aa": {"ta_keywords": "traits dialogue generation;dialogue particular personality;personality traits dialogue;persona labeled dialogue;personalized dialogues results;personalized dialogues;dialogue generation;deliver personalized dialogues;dialogue generation deliver;traits dialogue;embodying personality language;personality language expression;personality language;endowing dialogue particular;dialogue data;dialogue data research;labeled dialogue data;dialogue particular;human like conversations;incorporating explicit personality;explicit personality traits;labeled dialogue;dialogues results;dialogue;endowing dialogue;challenge embodying personality;particular personality;dialogues;personality traits;conversations challenge", "pdf_keywords": "personalized dialogue generation;traits conversation generation;generate personalized dialogue;traits dialogue generation;personalized dialogue model;personalized conversational model;personalized dialogue models;personalized dialogue responses;traits based dialogues;personalized dialogue systems;personalized dialogues;personalized dialogue;personalized conversational;method personalized dialogue;personality traits conversation;deliver personalized dialogues;dialogue particular personality;design personalized dialogue;human dialogue generation;study personalized dialogue;personality traits dialogue;propose personalized dialogue;use personalized dialogue;conversation generation;personalized dialogues firstly;dialogue generation;dialogue generation approach;conversation generation construct;conversational models;model human dialogue"}, "54a13bcc9613dcaa76fb25fbe96572f376cfcca9": {"ta_keywords": "stochastic optimization methods;stochastic optimization;neural network weight;proposed stochastic optimization;second moment estimators;adam adadelta parameter;estimators requires memory;optimization methods mrprop;network weight matrices;parameter second moment;gradients maintaining parameter;moment estimators requires;methods mrprop adam;weight matrices propose;moment estimators;weight matrices;mrprop adam adadelta;gradients maintaining;squared past gradients;past gradients maintaining;past gradients;optimization methods;adadelta parameter updates;mrprop adam;recently proposed stochastic;adam adadelta;adadelta parameter;gradients;neural network;optimization", "pdf_keywords": "model translation optimization;translation optimization;scale machine translation;translation optimization algorithm;popular machine translation;training model translation;machine translation;german machine translation;gradient accumulator training;models memory constrained;machine translation task;model translation;translation task approach;memory cost parameterized;nonnegative matrix factorization;larger models memory;translation task using;rates sublinear memory;models memory;memory constrained;sublinear memory;matrix factorization cost;translation task;sublinear memory cost;adafactor adaptive learning;adaptive learning rates;translation task called;memory cost;gradient accumulator;learning rates sublinear"}, "957e3ec3c722f5cb382fe8ac54fc846ee772a95f": {"ta_keywords": "algorithms adversarial bandits;adversarial bandits methods;adversarial bandits;combinatorial semi bandit;regret bounds improving;semi bandit problem;bandit problem generally;bandit problem;bandits methods;bandit problem instantiated;dependent regret bounds;adaptive algorithms adversarial;bandits methods develop;armed bandit problem;regret bounds;multi armed bandit;semi bandit;bandit;bandits;algorithms adversarial;adversarial multi armed;algorithm adversarial;generic algorithm adversarial;data dependent regret;armed bandit;algorithm adversarial multi;adversarial;adversarial multi;algorithm apply optimism;adaptive algorithms", "pdf_keywords": "semi bandit feedback;generic bandit algorithm;bandit algorithm;applications linear bandit;method bandit feedback;general bandit algorithm;prediction regret bounds;bandit feedback;bandit feedback based;bandit problem semi;based bandit feedback;consider combinatorial bandit;semi bandit problem;linear bandit problem;adaptive behavior bandit;bandit algorithm general;bandit feedback bandit;linear bandit;bandit algorithm using;problem semi bandit;regret bounds improving;bandit problem best;stochastic adversarial bandits;bandit feedback learner;combinatorial bandit;method based bandit;set semi bandit;combinatorial bandit problem;method bandit;bandit problem"}, "c4ce6aca9aed41d57d588674484932e0c2cd3547": {"ta_keywords": "natural language scientific;sciences extract information;introductionthecovid 19 pandemic;pandemic spawned diverse;knowledge base;pandemic;scientific literature;19 pandemic spawned;unified schema strikes;broad unified schema;scientific papers;pandemic spawned;language scientific papers;concept sciences extract;unified schema;scientific literature challenging;19 pandemic;scientific papers developing;information natural language;sciences extract;language scientific;extract information natural;natural language;knowledge case presentationwe;useful knowledge case;construction knowledge base;stimulating automated tools;knowledge case;schema strikes;schema", "pdf_keywords": "mechanism relations corpus;mechanism relations text;extract mechanism relations;scientific texts schema;mechanism relations scientific;mechanism relations literature;mechanism relations papers;mechanisms schema;schema mechanisms generalizes;unified schema mechanisms;annotate dataset mechanisms;mechanisms schema train;schema mechanism relations;natural language scientific;schema mechanisms;dataset mechanisms schema;analyze mechanism relations;mechanism relations provide;knowledge base mechanisms;identify mechanism relations;mechanism relations;mechanism relations generalized;mechanism relations train;grained mechanism relations;unified schema mechanism;concept mechanism relations;mechanism relations proposed;corpus scientific documents;classification mechanism relations;documents corpus"}, "73b22457a2f52a834d73d73a76b4124c1cb326be": {"ta_keywords": "decision dependent games;player performative prediction;dependent games learning;game theoretic;games learning problems;new game theoretic;performative prediction;game theoretic framework;learning decision;dependent games;learning decision dependent;games learning;formulates new game;introductionmultiplayer performative prediction;performative prediction learning;multi player performative;prediction learning decision;performatively stable equilibria;reacts competing decision;performative prediction methodswe;decision dependent;competing decision;called multi player;multi player;competing decision makers;stable equilibria;prediction;feedback mechanism population;games;concepts performatively stable", "pdf_keywords": "stochastic games;games stochastic gradient;stochastic game;stochastic games model;class stochastic games;games stochastic;stochastic game present;decision dependent games;monotone games stochastic;equilibrium stochastic game;performative prediction games;learning games probabilities;nash equilibrium stochastic;performingative prediction games;prediction games algorithm;prediction games;games performative prediction;performance prediction games;prediction games performative;dependent games nash;game theoretic model;games probabilities probabilities;dependent games method;formulate game theoretic;games probabilities;stochastic programming;stable equilibria nash;dependent games;games nash equilibria;performative prediction players"}, "6eb974721719056ba8dc74a898c64ae1d081e0ae": {"ta_keywords": "approximately validating invalidating;validating;validate various qualitative;objectivein safety;objectivein safety critical;validating invalidating;framework approximately validating;approximately validating;validate;discrimination protected class;outcomes discrimination protected;machine learning necessary;features checking undesirable;features checking;safety critical applications;applications machine learning;validate various;machine learning;discrimination protected;protected class methodsto;safety critical;accuracy order validate;metrics test accuracy;invalidating;protected class;outcomes discrimination;monotonicity respect feature;checking undesirable;discrimination;safety", "pdf_keywords": "automated dependence plots;interesting dependence plots;use dependence plots;dependence plots detect;dependence plots using;dependence plots;dependence plots identify;dependence plots extend;dependence plots implement;partial dependence plots;dependence plots demonstrate;dependence plots model;plots demonstrate usefulness;dependence plots multiple;dependence plots established;directional dependence plots;directing dependence plots;utility plot based;utility plot respect;dependence plots directional;utility plot;automated dependence;plots directional dependence;plots widely;plots widely used;ofthe utility plot;demonstrate use dependence;utility plot measured;interesting dependence;measure utility plot"}, "4e2c41466c8246af0a563ea36fbe80c896bbab2c": {"ta_keywords": "neural machine translation;translation systems;translation systems theoretically;machine translation systems;translation systems difficult;machine translation;homographs words different;translation based context;homographs words;introduction homographs words;difficulty machine translation;translation based;select correct translation;correct translation based;homographs;global sentential context;sentential context hypothesize;context advent neural;introduction homographs;neural machine;translation;sentential context;existing neural machine;neural;existing neural;words different;evidence existing neural;global sentential;words different meanings;context hypothesize", "pdf_keywords": "neural machine translation;sense disambiguation neural;machine translation nm;disambiguation neural;translation nm systems;disambiguation neural model;model translation words;machine translation nmr;disambiguation proposed neural;model improve translation;sense disambiguation free;translation systems difficult;translation accuracy nonmagnetic;translation systems;machine translation;machine translation systems;translations model better;neural models word;homographs neural machine;translation accuracy;demonstrate translation accuracy;translation based context;model word sense;word sense disambiguation;machine translation present;sense disambiguation proposed;improve translation words;context based neural;sense disambiguation;models word sense"}, "e9c52a3fac934919eca036909cc18d909db0d467": {"ta_keywords": "peridynamicdynamics patient history;model peridynamicdynamics patient;peridynamicdynamics patient;peridynamicdynamics;model peridynamicdynamics;present model peridynamicdynamics;patient history history;patient history;history history;history history history;history;present model;patient;model;present", "pdf_keywords": "models molecular dynamics;continuous models molecular;molecular dynamics displacements;surrogate molecular dynamics;models molecular;modeling paradigm formolecular;molecular model peridynamics;molecular dynamics data;model surrogate molecular;molecular model;molecular dynamics tool;molecular statistics simulations;molecular dynamics;extract molecular dynamics;peridynamic solid models;models displacement nonlocal;solid model surrogate;paradigm formolecular dynamics;importance molecular dynamics;formolecular dynamics md;formolecular dynamics;peridynamic solid model;rigorous modeling;solid models;rigorous modeling paradigm;models displacement;model multiscale physics;peridynamic model coarse;calculations principles molecular;elasticity peridynamic nonlocal"}, "52ec4713343083e69b87e36a7a12c7b5898e2780": {"ta_keywords": "multichannel automatic speech;speakers adaptation multichannel;speakers adaptation;adaptation multichannel end;features speech enhanced;adaptation multichannel;end speech recognition;introduction speakers adaptation;adaptation scheme multichannel;automatic speech recognition;speech recognition ar;speech recognition;speech recognition aim;automatic speech;speech features;speech enhanced pathway;noisy speech features;speech enhanced;end multichannel automatic;multichannel automatic;unprocessed noisy speech;speech features speech;multi path adaptation;end multichannel;features speech;end end multichannel;multichannel end;multichannel;end multichannel ar;multichannel end end", "pdf_keywords": ""}, "4d1a14352ffb526a1fa0e1cd90e2484e188cddc0": {"ta_keywords": "dialogue user simulator;training dialogue systems;creating dialogue data;dialogue data interaction;dialogue data;creating dialogue;possibility creating dialogue;dialogue scenarios self;data interaction dialogue;new dialogue scenarios;dialogue systems;interaction dialogue;interaction dialogue user;dialogue scenarios;training dialogue;dialogue user;dialogue systems lack;incorporate new dialogue;dialogue;difficulties training dialogue;user simulator;user simulator methodswe;user simulator resultsour;new dialogue;simulator;simulator methodswe explore;simulator resultsour goal;data interaction;simulator resultsour;scenarios self play", "pdf_keywords": "training dialogue systems;dialogue modelling development;dialogue user simulator;dialogue modelling;creating dialogue data;developing dialogue systems;multi domain dialogues;dialogue data interaction;dialogue data;dialogues reinforcement learning;dialogue systems development;developing dialogue;user simulator dialogue;simulator dialogue;creating dialogue;domain dialogues;dialogue systems emerging;dialogue systems;learning framework dialogues;possibility creating dialogue;dialogues reinforcement;oriented dialogue modelling;multidomain dialogues development;dialogue tasks development;neural dialogue systems;multi domain dialogue;dialogue systems area;framework based dialogue;dialogue tasks;development dialogue systems"}, "6d00b1024298e5b64ee873028385f7bb4396b05d": {"ta_keywords": "recombination compositional generalization;compositional generalization;limited compositional generalization;compositional generalization ability;structured expressions recursive;compositional generalization requires;recombining structured expressions;compositional generalization result;expressions recursive manner;parsing tasks compositional;structured expressions;semantic parsing tasks;semantic parsing;algebraic recombination compositional;generalization ability semantic;parsing;tasks compositional generalization;expressions recursive;recombination compositional;learn algebraic recombination;ability semantic parsing;limited compositional;parsing tasks;compositional;sequence models;recursive manner;recombining structured;generalization;neural model learn;generalization ability", "pdf_keywords": "compositional generalization semantic;neural architecture compositional;compositional generalization propose;syntactic algebral learning;neural model compositional;compositional generalization;algorithm compositional generalization;architecture compositional generalization;modeling syntactic algebral;exhibits compositional generalization;generalization semantic parsing;compositional generalization algebraic;model compositional generalization;novel algorithm compositional;free grammar underlying;contextfree grammar model;recombination comppositional generalization;algorithm compositional;learning latent syntax;recombination based semantic;expressions modeling syntactic;context free grammar;algebral learning operation;compositional generalization ffgo;modeling syntactic;free grammar;free underlying syntax;syntax expressions modeling;syntactic algebral;contextfree grammar"}, "6ccac8a95bc77549b98d045db6d5e0de3d356ba4": {"ta_keywords": "english text retrieval;neural ranking;neural ranking benefits;lexical translation model;document embeddings resultsthis;text retrieval;model english text;translation model imbi;retrieval particular neural;query document embeddings;imbi model english;text retrieval particular;translation model;neural model1 aggregator;embeddings resultsthis;embeddings resultsthis new;utility lexical translation;model english;document embeddings;retrieval;design neural ranking;use neural model1;lexical translation;use neural;english text;neural model1;ranking;contextualized query document;free contextualized query;retrieval particular", "pdf_keywords": "text retrieval neural;embeddings predict relevance;relevance query embedding;neural ranking;models text retrieval;neural ranker available;information retrieval neural;query document embeddings;retrieval neural model;lexical translation models;query embedding;neural ranking benefits;neural ranker;query embedding network;english text retrieval;nonparametric lexical translation;lexical translation model;text retrieval;translation models;free contextualized embedding;translation models text;contextualized embeddings;performance nonparametric lexical;embeddings bert based;contextualized embedding network;improve retrieval;contextualized embedding;free contextualized query;document embeddings;contextualized embeddings use"}, "69c515a62403fcc19125d3a6dd8e878aa5cde604": {"ta_keywords": "dialogue systems recent;incomplete utterance restoration;turn dialogue systems;utterance restoration;dialogue systems;challenge frequent coreference;generation sequence labeling;turn dialogue;sequence labeling text;multi turn dialogue;frequent coreference;dialogue;frequent coreference information;sequence labeling;utterance restoration brought;conversation data;coreference information omission;coreference information;coreference;large conversation data;conversation data development;introductiondilogue systems;incomplete utterance;introductiondilogue systems open;investigate incomplete utterance;autoregression generation sequence;conversation;text editing propose;labeling text editing;utterance", "pdf_keywords": "utterance restoration automatic;utterance restoration boosting;utterance restoration task;corpus development deep;model utterance restoration;incomplete utterance restoration;single turn corpus;generating added phrases;utterance restoration;turn corpus;saarg utterance restoration;turn dialogue systems;utterance restoration proposed;turn incomplete utterance;turn corpus development;phrase vocabulary utilize;dialogue systems recent;generator saarg utterance;conference natural language;predefined phrase vocabulary;adding phrases predefined;turn dialogue development;instead adding phrases;adding phrases;utterance cascade;model utterance;utterance utterance cascade;sequence labeling flexibility;appropriate model utterance;sequence labeling"}, "d2f327736c9b68f68ad64d0b1cefed9b4dd83313": {"ta_keywords": "language generation nlg;generation nlg task;nlg task generating;generating natural language;methodsnatural language generation;generation nlg;learning language generation;language generation;manually constructed linguistic;nlg task;backgroundimitation learning language;constructed linguistic;language generation unaligned;linguistic resources machine;backgroundimitation learning;constructed linguistic resources;natural language;structured prediction task;data methodsnatural language;nlg;training data phrase;structured prediction;natural language meaning;methodsnatural language;data phrase templates;linguistic;task generating natural;task generating;learning language;phrase templates", "pdf_keywords": ""}, "e37fb85e8869d464bae8eeebf4cd9321ec8c70ad": {"ta_keywords": "interpretability machine learning;interpretability;interpretability machine;interpretability instead propose;interpretability instead;statistical cost interpretability;cost interpretability machine;definition interpretability;cost interpretability;definition interpretability instead;agreed definition interpretability;machine learning discourse;machine learning;offs informal misconceptions;potential trade offs;trade offs seemingly;trade offs;study trade offs;trade offs informal;informal misconceptions abound;misconceptions abound;misconceptions abound work;informal misconceptions;learning;offs seemingly insurmountable;misconceptions;formal study statistical;statistical cost;offs informal;agreed definition", "pdf_keywords": "interpretability accuracy tradeoff;learning interpretability constrained;interpretability machine learning;interpretable classifiers;interpretability constraint learning;learning interpretability;learn interpretability constraint;classifier determination interpretability;constraint learning interpretability;interpretability accuracy;interpretable classifiers generalization;learn interpretability;interpretability constrained empirical;interpretability accuracy understood;accuracy interpretability;interpretable classifiers problem;interpretable classifiers impact;interpretability difficult propose;interpretable classifiers bounded;learning effects interpretability;interpretability constraint risk;interpretability difficult;interpretability constrained;tradeoffs associated interpretability;interpretable models;formalizing interpretability difficult;interpretability propose;error interpretable classifiers"}, "8b0b2b69657076fc1ce7cce75a6d69e3e5ba2d63": {"ta_keywords": "sex specific adolescent;specific adolescent sex;adolescent sex specific;adolescent sex;sex specific adol;patient sex specific;specific adolescent;case patient sex;sex specific;patient sex;adolescent;report case patient;sex;case patient;report case;specific adol;adol;patient;report;specific;case", "pdf_keywords": ""}, "6a173e22819480b891306eac65fd44be010dfca8": {"ta_keywords": "pandemic influenza virus;influenza virus influenza;influenza virus;influenza virus isolate;infection pandemic influenza;influenza virus infection;virus influenza;2009 pandemic influenza;seasonal influenza virus;pandemic influenza;relative seasonal influenza;seasonal influenza;influenza;virus influenza california;lungs cynomolgus macaques;influenza california;influenza california 04;enhanced pathogenicity lungs;pathogenicity lungs cynomolgus;virus infection pandemic;virus isolate kawasaki;functional pathway enrichment;infection pandemic;pathogenicity lungs;cynomolgus macaques;cynomolgus macaques relative;combining functional pathway;pathway enrichment;2009 enhanced pathogenicity;2009 pandemic", "pdf_keywords": ""}, "1d2a2b14ef14eeaf89169f738f7634cdc685c785": {"ta_keywords": "databases statistical ir;relational databases;vectors query language;relational databases statistical;built similarity predicate;similarity predicate term;systems query language;ir systems query;similarity predicate;query language given;query language;properties relational databases;databases statistical;systems query;predicate term vectors;databases;term vectors query;soft semantics;soft semantics roughly;given soft semantics;relational;built similarity;contains built similarity;combines properties relational;statistical ir;statistical ir systems;properties relational;predicate term;vectors query;semantics roughly", "pdf_keywords": ""}, "c968e8dc442102b38b134b1afadc7cc78fc5b5fb": {"ta_keywords": "named entity recognition;entity recognition ner;entity recognition;models natural language;evaluation named entity;natural language processing;named entity;natural language;language processing tasks;interpretable evaluation named;recognition ner task;language processing;interpretable evaluation;recognition ner;holistic metrics accuracy;entity;methodology interpretable evaluation;ner task;models natural;evaluation named;holistic metrics;metrics accuracy;metrics accuracy blueu;models relative merits;differences holistic metrics;models;accuracy blueu;methodology interpretable;accuracy;ner", "pdf_keywords": "named entity recognition;entity recognition task;entity recognition terms;entity recognition;models natural language;entity recognition application;natural language processing;evaluation named entity;language models use;language models;language processing tasks;named entity;language processing;models named entity;natural language;entities long;evaluation character language;character language models;entities;entities long entities;interpretable evaluation named;interpretable evaluation task;long entities;sequence labeling;long entities dataset;diagnose long entities;long entities long;studies named entity;long entities article;different long entities"}, "7a56aba1a4d4020c4933319588b9ed2b34d51125": {"ta_keywords": "party computation mpc;mc protocol malicious;multi party computation;cryptography enables computation;protocol malicious security;implementation mc protocol;theoretical mc protocols;mc protocols;mpc area cryptography;protocol malicious;mc protocol;sequentialz framework provides;efficiency sequentialz framework;computation mpc;cryptography enables;sequentialz framework;cryptography;computation sensitive data;malicious security;enables computation;computation sensitive;enables computation sensitive;protocols scale efficiently;efficiency sequentialz;mc protocols scale;sequentialz;protocol;party computation;privacy guarantees;protocols", "pdf_keywords": "accuracy privacy preserving;privacy preserving linear;accuracy privacy;numbers accuracy privacy;privacy preserving;new method cryptology;method cryptology;method cryptology cryptology;algorithms data decomposition;cryptology;privacy;cryptology cryptology;cryptology mpc;cryptology cryptology mpc;regression learning;lower plaintext computation;mpc machine learning;popular machine learning;plaintext computation new;regression learning using;machine learning algorithms;linear regression learning;plaintext computation;cryptology mpc distributed;data decomposition;learning algorithms;decomposition use iterative;sgs iterative approximation;data decomposition using;computation new"}, "e02f79b710cdcaa9135b835fad964f6f2c78b1a7": {"ta_keywords": "pipelinelined neural models;introduction pipelinelined neural;pipelinelined neural;recent tokenization approaches;tokenization approaches;explicit tokenization;tokenization;tokenizers;tokenizers techniques equally;tokenization approaches based;end neural modeling;tokenizers techniques;neural modeling nearly;models largely superding;manually engineered tokenizers;engineered tokenizers;neural models largely;explicit tokenization step;engineered tokenizers techniques;neural modeling;require explicit tokenization;neural models;recent tokenization;tokenization step;derived subword lexicons;introduction pipelinelined;subword lexicons;end neural;tokenization step recent;end end neural", "pdf_keywords": "tokenization free deep;tokenizationfree vocabulary;subword tokenization ability;trained tokenization;tokenizationfree vocabulary free;trained tokenization free;pre trained tokenization;subword tokenization;uses tokenizationfree vocabulary;standard subword tokenization;recent tokenization approaches;novel language models;free deep encoder;deep encoder;language modeling character;deep encoder described;modeling recent tokenization;tokenization approaches;deep encoder efficient;tokenizers;tokenizers techniques;pre trained encoder;trained deep encoder;tokenizers techniques equally;trained encoder;tokenization free;tokenization;encoder uses tokenizationfree;language models vocabulary;encoder efficient model"}, "4572ded23106285cbd8ebbe6c3b354973ac06ff7": {"ta_keywords": "emotional disturbances context;emotional disturbances;disturbances context conversational;etiology emotional disturbances;conversational speech;conversational speech poorly;context conversational speech;speech poorly;conversational;speech;context conversational;etiology emotional;emotional;speech poorly understood;concept context conversational;disturbances context;disturbances;context;poorly understood development;understood development;concept context;new concept context;development;etiology;understood development new;poorly;concept;new concept;development new concept;poorly understood", "pdf_keywords": ""}, "e214d2a6399925ce60fa5ce90c0374127a32b47e": {"ta_keywords": "impromptu deployment wireless;sensor networks motivated;applications impromptu;need applications impromptu;applications impromptu deployment;deployment wireless sensor;impromptu deployment;sensor networks;wireless sensor networks;deployment wireless;motivated need applications;wireless sensor;impromptu;applications;sensor;wireless;need applications;networks motivated need;deployment;networks motivated;motivated need;networks;motivated;need", "pdf_keywords": "algorithm impromptu wireless;impromptu deployment wireless;stochastic algorithm relay;relay impromptu deployment;relay deployment optimal;algorithm relay deployment;impromptu wireless network;deployment optimal policy;learning algorithm radio;deployment wireless sensor;sensor networks habitat;deployment wireless relay;methods deployment wireless;deployment algorithm feasible;sensor networks;wireless network deployment;algorithm relay;wireless sensor networks;method deploying wireless;relay arise optimal;deployment relay proposed;impromptu deployment based;policy impromptu deployment;impromptu wireless;algorithm radio propagation;deployment relay nodes;sensor networks increasing;sensor networks motivated;wireless relay networks;optimal policy impromptu"}, "7ee55c115470e1b86e552c5594e2e4258b4ccefb": {"ta_keywords": "elongated eluted eluted;elongated eluted;elongated elongated eluted;elongation elongated;elongation elongated elongated;elongation;eluted eluted eluted;eluted eluted;elongated elongated elongated;elongated elongated;eluted eluted elucidated;eluted elucidated;elongated;eluted eluted elucidation;eluted;eluted elucidation;eluted elucidation elicite;combination eluted eluted;elucidation elicite;elucidation;elucidated;eluted elucidated combination;elucidated combination eluted;combination eluted;elicite;elucidated combination;combination", "pdf_keywords": ""}, "a7766d4c41df235764dfaa9971ce861f6120ac27": {"ta_keywords": "latency meeting recognition;meeting recognition;meeting recognition understanding;meeting analyzer;group meetings methods;group meetings;real time meeting;meetings methods;analyzer group meetings;meetings methods using;time meeting analyzer;meeting analyzer group;low latency meeting;meetings;distant microphones challenging;latency meeting;distant microphones;meeting;information captured microphone;using distant microphones;captured microphone array;microphones challenging;microphone;captured microphone;microphone array;time meeting;microphone array omni;automatically recognizes speaks;microphones;recognizes speaks results", "pdf_keywords": ""}, "252ef125a8874fe8face4540f87f2e000275cc96": {"ta_keywords": "communities terrorist groups;networks communities terrorist;terrorism research;communities terrorist;terrorism research light;clustering;terrorist groups active;terrorist groups;clustering algorithm;task terrorism research;clustering algorithm designed;innovative clustering algorithm;innovative clustering;partite networks communities;grouping;test innovative clustering;networks communities;terrorism;groups active worldwide;multi partite networks;key task terrorism;communities;task terrorism;partite networks;finding hidden patterns;groups;networks;terrorist;different types grouping;groups active", "pdf_keywords": ""}, "803c7fdd6e01e1ff8cd43297f4e052078409456d": {"ta_keywords": "linguistic representations flexible;linguistic;requires linguistic representations;introductionlinguistics powerful;linguistic representations;introductionlinguistics;introductionlinguistics powerful solutions;flexible old words;requires linguistic;words acquire new;hierarchical adaptation inference;continual hierarchical adaptation;language;environment requires linguistic;adaptation inference;words say correspond;language use variable;intentions heads language;hierarchical adaptation;old words acquire;language use;inference;adaptation;heads language;introduce continual hierarchical;words acquire;continual hierarchical;heads language use;shared expectations words;representations flexible old", "pdf_keywords": "conventions emergence;theory coordination;conventions emergence social;hoc conventions emerge;coordination convention formation;bayesian theory coordination;communicative interactions;conventions shaped communicative;game emergence conventions;compositional language communication;unified cognitive;models coordination;stable social conventions;social conventions emerge;emergence conventions fundamental;coordination;coordination convention;emergence conventions;conventions emerge;reference game emergence;theory coordination convention;communicative interactions interaction;conventions emerge populations;language fundamentalthe emergence;language cognition article;convention formation argue;convention formation;hoc linguistic conventions;communication complex;unified cognitive model"}, "c6488f0c62ee4a4d48d0fbf8e8185655226294c1": {"ta_keywords": "controller manipulation attack;manipulation attack cme;attack cme reconstructable;attack called controller;manipulation attack;assisted communication transmitter;attack detection given;attack detection;attack detection probability;different attack detection;surface assisted communication;testing based attack;based attack detection;attack detection models;attack cme;communication transmitter receiver;controller manipulation;communication transmitter;detection given fading;detection probability receiver;called controller manipulation;block known channel;based attack;introduce new attack;transmitter receiver;constraint attack detection;attack called;attack;consider different attack;assisted communication", "pdf_keywords": "attack reconfigurable intelligent;attack detection aetiology;attack detection probability;probability attack detection;attack design minimize;attack detection design;attack detection;attack design;effective detecting attack;physical layer attack;attacker minimize;attack reconfigurable;transmitter receiver attacker;different attack detection;detecting attack;attack detection models;manipulation attack reconfigurable;aetiology attack detection;layer attack;receiver attacker potential;attacker minimize data;receiver attacker;problem attack design;probability attack;alarm probability attack;controller manipulation attack;design attack strategy;consider aetiology attack;manipulation attack cme;attacker potential manipulate"}, "2e4cdd36d77b9d814638fc2cd6c703535cb1d2f7": {"ta_keywords": "prompt tuning natural;prompt tuning paradigm;generation nlg tasks;tuning natural language;develop prompt tuning;language generation nlg;prompt tuning;natural language generation;tuning continuous prompts;prompt tuning takes;generation nlg;language generation;introductionthe prompt tuning;plm prompt tuning;pretrained language model;nlg tasks;prompts frozen pretrained;frozen pretrained language;prompts;attention tuning continuous;attention tuning;pretrained language;prompt;continuous prompts;develop prompt;plm prompt;model plm prompt;language model plm;nlg tasks methodsin;prompts frozen", "pdf_keywords": "translation prompt tuning;tuning natural language;language generation tasks;language models tune;machine translation prompt;prompt tuning task;pretrained language model;prompt tuning natural;frozen pretrained language;better prompt tuning;language generation;models prompt tuning;prompt tuning useful;machine translation development;prompt tuning;prompt tuning ability;natural language generation;language model prompt;model prompt tuning;propose prompt tuning;translation prompt;prompt tuning fine;pretrained language;machine translation;prompt tuning effective;generation tasks;prompt tuning limited;tool prompt tuning;prompt tuning input;help prompt tuning"}, "4c2d9136c579a0393d4f50bbbbc6f8dab43c38e9": {"ta_keywords": "speaker scenario wpe;backgroundweighted prediction;prediction error wpe;wpe usually assumes;backgroundweighted prediction error;wpe known;scenario wpe usually;wpe known dereverberation;wpe usually;performance automatic speech;speech recognition distant;dereverberation signal processing;speech recognition;wpe;scenario wpe;assumption wpe works;automatic speech;based assumption wpe;recognition distant speaker;assumption wpe;error wpe known;distant speaker scenario;wpe works;wpe works practice;automatic speech recognition;distant speaker;source priors;specific source priors;error wpe;speaker scenario", "pdf_keywords": ""}, "6f49026ff623c64ce6de81fd04cf6e1ffe7dd6d9": {"ta_keywords": "adjuvantarial networks gans;gans learn generate;networks gans learn;learn generate music;backgroundconvolutional generation adjuvantarial;networks gans;generate music;convolutional gan model;study convolutional gan;convolutional gan;gans learn;generation adjuvantarial networks;gan model;adjuvantarial networks;gan model directly;backgroundconvolutional generation;generate music form;binary valued piano;gans;represent music binary;piano rolls;valued piano rolls;rolls represent music;music form piano;form piano rolls;music binary valued;piano rolls represent;music binary;piano;piano rolls using", "pdf_keywords": "deep learning music;training generative adversarial;gans learn generate;generate binaryvalued piano;music generating;networks gans;networks gans learn;able generate piano;generating music;training generative;generative adversarial;algorithm generating music;convolutional generative adversarial;effective generating music;binarizing generated piano;learn generate music;piano roll dataset;generating music generating;gans learn;learning music generation;represent music binary;generate music;music generating track;binaryvalued piano;music generation transcription;model polyphonic music;generative adversarial networks;deep convolutional generative;generative adversarial network;adversarial"}, "57e7be6b404abfd7a56a73c0ff9bccc5b27ad7ae": {"ta_keywords": "infease translation models;translation models;translation models relies;translation models control;domain adaptation;unsupervised domain adaptation;neural machine translation;machine translation models;data domain adaptation;domain adaptation strategies;domain adaptation required;domain copied monolingual;machine translation;adaptation required domain;makes infease translation;monolingual translated data;translated data;translated data methods;infease translation;copied monolingual translated;training model domain;domain shift;model domain copied;copied monolingual;text regardless domain;unsupervised domain;adaptation;domain shift makes;methodsprevious unsupervised domain;translation", "pdf_keywords": "translation models;translation models introduce;translation neural machine;strong translation models;machine translation models;translation models relies;neural machine translation;infeasible translation models;translation models control;machine translation neural;translation neural;domain translation results;translation accuracy domain;translation results neural;friendly domain adaptation;data source translation;domain translation;machine translation experimental;monolingual translated data;output domain translation;translation enriches training;translate translation accuracy;machine translation;translation accuracy;comparison domain adaptation;data domain adaptation;domain monolingual data;domain adaptation;translation models article;effective domain adaptation"}, "2b3ab7e9c66bffc7af9e4413036e7bba686a7734": {"ta_keywords": "reviewers conferences;burden reviewers conferences;reviewers conferences started;competent reviewers growing;submission history papers;competent reviewers;submission history;previous submission history;quality peer review;reviewers growing;peer review;reviewers;authors declare previous;met skepticism authors;skepticism authors raise;number competent reviewers;science conferences experiencing;reviewers growing slower;skepticism authors;modern machine learning;computer science conferences;submissions challenges quality;conferences experiencing;reduce burden reviewers;declare previous submission;science conferences;previous submission;encouraging requiring authors;conferences started encouraging;challenges quality peer", "pdf_keywords": "reviewers bias resubmissions;reviewers resubmission bias;biases peer review;peer review reviewers;bias peer review;resubmission bias reviewers;review process peer;effectiveness peer review;process peer review;review peer review;peer review process;peer review;describes reviewers bias;peer review debate;review peer;review process bias;conference peer review;peer review impact;peer review discuss;reviewers bias;peer review design;bias reviewers decisions;peer review complex;reviewers resubmission;peer review peer;review reviewers bias;reviewers judgements demonstrate;reviewers decisions;peer review role;reviewers decisions measure"}, "536ce077f08886b5834b639da25068d877c98b2c": {"ta_keywords": "malignant neoplasm;diagnosed malignant neoplasm;diagnosed malignant;patient diagnosed malignant;neoplasm;malignant;dental dentistry;major dental practice;major dental;dentistry major dental;dental;dentistry;dental practice;dental practice difficult;dental dentistry major;dentistry major;report case patient;diagnose treat;case patient diagnosed;patient diagnosed;diagnose treat especially;difficult diagnose treat;diagnosed;diagnose;treat especially cases;practice difficult diagnose;difficult diagnose;case patient;treat;report case", "pdf_keywords": ""}, "b54efe01969adaa1c623331d5791897a4dd9f886": {"ta_keywords": "fruit fly genomics;fly genomics;genomics incorporation flybasee;fly genomics incorporation;macromolecules fruit fly;interactive information extraction;flybasee interactive information;information extraction designed;genomics;information extraction;genome used curation;macromolecules fruit;papers fruit fly;fruit fly;incorporation flybasee interactive;curation papers fruit;genomics incorporation;flybasee interactive;genome;genome used;papers fruit;fruit fly major;curation process interactive;flybasee;interactive information;incorporation flybasee;component genome used;curation papers;fruit;component genome", "pdf_keywords": ""}, "0e8da301b098f96fed39c5aa8e2194f678690a16": {"ta_keywords": "secret share dissemination;secret sharing;algorithm secret share;disseminating shares secret;share dissemination communication;shares secret dealer;share dissemination;shares secret;secret share;secret sharing important;secret dealer participants;protocols assume dealer;communicate respective shares;problem disseminating shares;provide algorithm secret;communication efficient distributed;shares participants;dissemination communication efficient;respective shares participants;dissemination communication;algorithm secret;efficient distributed deterministic;efficient distributed;disseminating shares;dealer direct communication;secret dealer;dealer participants;dealer directly communicate;cryptographic protocols;distributed deterministic", "pdf_keywords": ""}, "73569460b023f9ac1fe5a1876c3401460d2fc15d": {"ta_keywords": "exploit semantic preserving;source code understanding;models learn semantic;trained models code;introduce curriculum learning;trained models downstream;semantic preserving transformation;curriculum learning;learn semantic;source code;tasks source code;exploit semantic;code related tasks;transformations introduce curriculum;backgroundbridging pre trained;learn semantic features;curriculum learning organize;semantic preserving;trained models learn;pre trained models;downstream tasks source;methodswe exploit semantic;models downstream tasks;enrich downstream data;code understanding;introduce curriculum;models learn;downstream data diversity;transformation enrich downstream;help pre trained", "pdf_keywords": "code pre trained;learn code semantics;code pre trainedwe;code clone detection;learn code;code search tasks;augmentation learn code;code clone;trained models code;code corpus;code related tasks;code search network;classification code clone;trained models pretrain;trained models codebert;exploit semantic preserving;tasks source code;code search;augmentation techniques codebase;pre trained models;candidates code corpus;source code understanding;code semantics semantic;existing code pre;language pre trained;source code;surpass existing code;tasks exploit semantic;pretrained models;code related downstream"}, "23f1d4b46bc7c8f357a5a89144d5d32af7be13a5": {"ta_keywords": "tuned large summarization;news summarization;large summarization datasets;focusing news summarization;large summarization;summarization datasets;summarization datasets little;trained language models;models focusing news;language models;summarization;content selection generation;introductionpre trained language;language models brart;training models content;generation strategies learnt;models content selection;introductionpre trained;generation models focusing;pre training models;generation models;training models;learnt iterations;trained language;training dynamics generation;learnt;generation strategies;models content;selection generation;focusing news", "pdf_keywords": "generated summaries improves;models focusing summarization;targetwe summarization models;abstractiveness generated summaries;language models summarization;summarization skills learnt;summarization models particular;summarization models;generated summaries development;cnnm mediasum summaries;summaries improves token;generated summaries;abstractive summaries datasets;summarization skills;optimize targetwe summarization;summarization models fail;models summarization;different summarization skills;summaries improves;trained language models;consistency generated summaries;models summarization methodsin;summaries datasets model;summarization different datasets;learnt behavior summarization;summaries development;study different summarization;summarization methodsin;summarization different;focusing summarization"}, "9389af659f14239319186dff1cef49e8ece742c8": {"ta_keywords": "graph models massive;expressive graph models;machine learning graphs;scaling expressive graph;expressive graph;learning graphs methodswe;learning graphs;massive datasets resultswe;graph models;models massive datasets;massive datasets;datasets resultswe expressive;expressive models significantly;web large scale;graphs;graphs methodswe;graphs methodswe present;benchmark ogb stanford;resultswe expressive models;expressive models;scaling expressive;large scale challenge;challenge machine learning;experiments scaling expressive;models massive;public benchmark ogb;graph;benchmark ogb;data public benchmark;datasets resultswe", "pdf_keywords": "larger graph datasets;graph learning tasks;graph learning data;graph models large;expressive graph models;large scale graphs;graph datasets;prediction tasks graphs;scale graph learning;models large graphs;large scale graph;graph models massive;open graph benchmark;graph datasets small;graph level prediction;graph benchmark og;core graph tasks;graph learning;large graphs;massive datasets expressive;graph benchmark;core graph learning;graph provide large;expressive graph;graphs advanced expressive;graph models useful;large graphs advanced;graph representation learning;prediction graph regression;scaling expressive graph"}, "c68349ba142be731d6f3339d894764921c69b774": {"ta_keywords": "automatically twitter questions;diabetes mellitus t2m;like quiz data;quiz data;twitter questions designed;game like quiz;like quiz;twitter questions;automatically twitter;diabetes mellitus;social media driven;semi automatically twitter;train public health;twitter;diabetes;type diabetes;type diabetes mellitus;public health model;quiz data questions;preventable type diabetes;health model;training data;detection individuals risk;early detection individuals;build social media;data train public;public health;social media;health;health model applied", "pdf_keywords": "introduce random forest;automatically twitter questions;classifier public health;twitter questions designed;model natural language;random forest;food social media;like quiz;social media driven;data social media;like quiz data;quiz data;automatically twitter;social media results;game like quiz;used random forest;semi automatically twitter;nodes random forest;developed random forest;twitter questions;implement quiz;validate models obesity;models obesity epidemic;random forest classifier;decision trees introduce;natural language;quiz classify individuals;natural language questions;predict overweight population;data social"}, "6665e03447f989c9bdb3432d93e89b516b9d18a7": {"ta_keywords": "aetiology aetiology;aetiology aetiology understood;aetiology understood;aetiology development aetiology;aetiology;aetiology development;study shown aetiology;shown aetiology aetiology;development aetiology;shown aetiology;role aetiology development;role aetiology;consider role aetiology;aetiology understood addition;understood addition important;study;study shown;new study;development;understood addition;new study shown;addition important consider;addition important;understood;important consider role;new;important consider;consider role;addition;important", "pdf_keywords": ""}, "e92de0c4ef62a84201fac284eb66c37330b5fe1c": {"ta_keywords": "automated bug repair;bug fixing;bug repair;automated program repair;bug fixing generally;introduction bug fixing;automated bug;program repair;program repair aims;future bugs methods;bug repair paper;fixing generally manually;work automated bug;bugs methods;current future bugs;future bugs;bugs methods specificly;fixes propose fixes;repair subset bugs;fixing generally;past fixes propose;ways code mutation;leverage past fixes;code mutation;fixes propose;bugs;bugs different ways;propose fixes;fixing;past fixes", "pdf_keywords": ""}, "90848c88f56fcd421ac3cfd2c87d3e61211103ea": {"ta_keywords": "patient history history;patient history;case patient history;history history;history history history;history;present case patient;case patient;article present case;purpose article present;article present;purpose article;article;patient;present case;present;case;purpose", "pdf_keywords": ""}, "051a85bd1384767ea5882dcefa98aee5664aa2cf": {"ta_keywords": "unsupervised inference tasks;problem domain knowledge;domain knowledge;discriminative training;unsupervised inference;discriminative training relatively;inference straightforward discriminative;domain knowledge built;straightforward discriminative training;inference tasks adaptation;inference tasks;knowledge built constraints;tasks adaptation clustering;straightforward discriminative;discriminative;contrast unsupervised inference;addition unsupervised inference;adaptation clustering;knowledge;model based methods;adaptation clustering handled;model addition unsupervised;built constraints model;clustering handled natural;unsupervised;inference;constraints model;tasks adaptation;model based;knowledge built", "pdf_keywords": ""}, "6fde1c63b1a353cf539d319341ae9396000660ed": {"ta_keywords": "computer evaluation systems;computer evaluation;evaluation systems;human evaluation;evaluation systems robust;files human evaluation;shown computer evaluation;assisted learning systems;computer assisted learning;audio files human;learning systems good;quality audio file;quality audio files;quality audio;evaluation;audio file;audio;audio files;affect quality audio;learning systems;evaluation methods;recording software;robust human teachers;assisted learning;noise generated learners;computer assisted;poor quality audio;software;good human teachers;audio file including", "pdf_keywords": ""}, "e03d9684a19c8f8e29ee97b347d4f1e280a88e44": {"ta_keywords": "contraststive learning grounding;language gesture methodscrossmodal;gesture methodscrossmodal grounding;learning grounding spoken;gesture methodscrossmodal;backgroundcrossmodal clustered contraststive;grounding spoken language;gestures spoken language;spoken language gesture;learning grounding;gesture accompany semantically;clustered contraststive learning;language gesture accompany;gestures spoken;language gesture;contraststive learning;phrases makes crossmodal;crossmodal grounding;gestures;crossmodal grounding especially;timed gestures spoken;methodscrossmodal grounding;makes crossmodal grounding;gesture accompany;grounding spoken;gesture;backgroundcrossmodal;crossmodal;relevant timed gestures;backgroundcrossmodal clustered", "pdf_keywords": ""}, "d31b5b60e1b3af84cd977da8db0ed4faeb79e7f7": {"ta_keywords": "text style transfer;style transfer;pretrained text text;strong pretrained text;text style;extract style;context text style;pretrained text;style adjacent sentences;requiring style labeled;style labeled training;style transfer compared;style labeled;style vector text;extract style vector;style transfer problem;text text model;approaches requiring style;text model;requiring style;model extract style;labeled data inference;unlabeled text relying;text model extract;problem text style;text;sentences uses labeled;style vector;labeled training data;context text", "pdf_keywords": "style transfer sentences;inducing text style;textual style training;text style transfer;integrating textual style;text style representations;implement style transfer;style aware language;style transfer generalpurpose;learning textual style;style transfer;processing text style;style transfer strong;style methods learning;shot text style;style transfer model;style representations;transfer specified style;style transfer fundamental;style transfer use;style attribute transfer;source style vectors;textual style methods;style adjacent sentences;controlling text style;style representations reframe;style vectors;style training task;style transfer unlike;strong pretrained text"}, "5812e30eb4756aeaf0b013a65b98f8f8aa0f8315": {"ta_keywords": "english tasks inwt;systems english german;systems english;based phrasebased pre;nontassananist systems english;hierarchical phrase based;german english tasks;phrase based phrasebased;phrasebased pre ordering;domain adaptation;based phrasebased;english tasks;bayes risk combination;phrasebased pre;selection domain adaptation;risk combination sm;phrase based;generalized minimum bayes;2013 methodsthe systems;string hierarchical phrase;minimum bayes;sm systems forest;minimum bayes risk;bayes risk;hierarchical phrase;domain adaptation rescoring;bayes;systems interwt 2013;systems forest;german german english", "pdf_keywords": ""}, "cf2a953dc82115d34de51737fef46bf3ff4cd5a6": {"ta_keywords": "speech separation recognition;disease form speech;recognition disease form;recognition disease;speech separation;form speech separation;separation recognition challenge;separation recognition;disease understood report;disease understood;disease;speech;disease form;recognition;form speech;recognition challenge;strategy recognition disease;etiology disease understood;separation;etiology disease;understood report development;etiology;new strategy recognition;challenge;understood report;strategy recognition;report development new;form;report development;report", "pdf_keywords": ""}, "c44addf352f25f28f69ca9f9422c0e463783206f": {"ta_keywords": "walk based similarity;similarity measures parsed;parsed text corpus;graph walk based;measures parsed text;dependency parsed text;information graph walk;based similarity measures;text corpus;similarity measures;text corpus instance;syntactic relations graph;graph walks;word similar;nodes represent words;backgroundadaptive graph walk;specific word similar;words weighted directed;graph walk;relations graph walks;based similarity;represent words weighted;graph walk process;corpus;graph walks combined;parsed text;corpus instance labeled;labeled directed graph;similarity;words weighted", "pdf_keywords": ""}, "9c16dcbcdfe6991f5d448543e6f4cbdf37149883": {"ta_keywords": "interactions crucial multimodal;multimodal model learn;multimodal model;does multimodal model;crucial multimodal tasks;multimodal tasks;multimodal;does multimodal;crucial multimodal;multimodal tasks visual;empirical multimodally;empirical multimodally additive;multimodally additive function;cross modal interactions;multimodally additive;learn cross modal;multimodally;introduction does multimodal;expressive cross modal;tool empirical multimodally;modal interactions crucial;cross modal;modal interactions;modal interactions harder;visual question answering;exploiting unimodal;modeling expressive cross;exploiting unimodal signals;function projection emap;modal", "pdf_keywords": "multimodal classification tasks;multimodal models increasingly;model multimodal classification;multimodal models;multimodal classification;language multimodallyadditive models;multimodal model multimodal;multiple multimodal models;empirical multimodally additive;development multimodal classification;multimodallyadditive models used;interactions empirical multimodally;multimodallyadditive models;empirical multimodally;approach multimodal classification;multimodally additive;model multimodal;development multimodal models;multimodal models model;multimodal sentiment analysis;multimodal model;multimodal interactions leveraged;interactive model multimodal;multimodal content;multimodal interactions;performing multimodal model;multimodally additive function;multimodally additive projection;multimodal;multiple multimodal"}, "335bf6f23ccdae43e45a7c12f33bc4f3488e3762": {"ta_keywords": "multi source translation;machine translation;source translation approach;increase translation accuracy;translation accuracy;machine translation rife;translation accuracy methods;translation rife ambiguities;introduction machine translation;source translation;translation approach;translation approach attempts;increase translation;sentences different languages;translation;ambiguities word ordering;corpora mistakes frequent;corpora mistakes;corpora;large corpora mistakes;languages increase translation;translation rife;statistics large corpora;multiple inputs sentences;large corpora;ambiguities exploiting multiple;multi source;word ordering;ambiguities word;frequent multi source", "pdf_keywords": ""}, "82459c972cc1e439c759010acf7ddce1a89b66e0": {"ta_keywords": "fuzzy graph clustering;meta algorithm fuzzy;graph clustering algorithm;algorithm fuzzy graph;watset meta algorithm;graph clustering;fuzzy graph;hard clustering discover;clustering discover;clustering algorithm;hard clustering;clustering discover clusters;algorithm fuzzy;discover clusters disambiguated;clusters disambiguated;clusters disambiguated intermediate;clustering algorithm creates;clustering;meta algorithm;uses hard clustering;discover clusters;watset meta;analysis watset meta;computational analysis watset;clusters;watset shows competitive;disambiguated intermediate graph;complexity demonstrate watset;analysis watset;representation input graph", "pdf_keywords": "graph clustering word;clustering word sense;fuzzy graph clustering;graph clustering watset;semantic structure graph;graph clustering unsupervised;graph clustering useful;graph clustering widely;global graph clustering;word sense induction;language data clustering;sense induction lexical;method clustering words;clustering words;graph clustering effective;graph clustering algorithms;graph clustering;clustering data words;clusters frame semantic;graph semantically related;induction lexical semantic;clustering word;graph clustering resourcerich;graph clustering algorithm;clustering occurrence graphs;semantic aware grouping;graph clustering high;graph clustering method;clustering words synonyms;traditional verb clustering"}, "684821e2459c7fc3ef8a2ec8102678af3613a962": {"ta_keywords": "representation learning speech;self supervised learning;speech understanding performance;downstream speech applications;learning speech processing;downstream speech;learning speech;utilizing self supervised;self supervised;speech processing community;language representation learning;quality learned representations;speech processing;speech understanding;introduce speech understanding;range downstream speech;learned representations;natural language representation;speech applications;speech applications bridge;learned representations wide;representation learning;language representation;supervised;learning;supervised learning;learning methods pre;pre train network;methods pre train;natural language", "pdf_keywords": ""}, "2467b2daea0398709d7ea57d084cc1f00f9d168f": {"ta_keywords": "autoregressive multi speaker;estimated conditional speaker;conditional speaker chain;ct conditional speaker;speaker automatic speech;conditional speaker;speech recognition ar;input mixture speech;automatic speech recognition;automatic speech;speaker chain methodsin;multi speaker ar;speaker inferred using;backgroundmulti speaker automatic;mixture speech;output speaker inferred;multi speaker;speaker automatic;speaker ar specifically;speaker chain;mixture speech previously;speech recognition;speaker inferred;speech previously estimated;speaker ar;backgroundmulti speaker;specifically output speaker;output speaker;speaker;combinationing non autoregressive", "pdf_keywords": "multi speaker neural;encoders conditional speaker;speech recognition multi;speaker neural network;nonautoregressive automatic speech;multi speech recognition;nonautoregressive speech recognition;speech recognition models;recognition multi speech;speech recognition model;models multi speaker;speaker speech recognition;conditional speaker chain;auditory nar multi;speech recognition ar;speaker neural;nonautoregressive multi speaker;speaker speech datasets;speaker automatic speech;speaker speech processing;speaker ra models;multi speaker speech;speech processing model;speech corpus model;recognition multi speaker;multi speaker auditory;multi speech;automatic speech;nar multi speaker;speech datasets"}, "a1588ac6d582d30742f998464500bb5ead125dc6": {"ta_keywords": "auctions attention regulatorretnet;maximizing auctions;auctions attention;revenue maximizing auctions;maximizing auctions combines;optimal er auctions;er auctions attention;benefit bidding;auctions;participants benefit bidding;auctions combines expressivity;bidding;benefit bidding truthfully;er auctions;deep learning regret;regretnet;auctions combines;incentive;bidding truthfully methods;bidding truthfully;incentive compatibility constraint;incentive compatibility;modifications regretnet;quantify incentive;independent modifications regretnet;learning regret based;quantify incentive compatibility;revenue maximizing;attention regulatorretnet;design revenue maximizing", "pdf_keywords": "trained regretnet;trained regretnet regretformer;regretnet new neural;optimization auction;deep learning regret;procedure trained regretnet;lossthe optimal auction;outperforms regretnet;experiments optimal auction;optimization auction design;regretnet;procedures trained regretnet;objective regretnet;optimal auction;method optimization auction;maximizing auctions;outperforms regretnet equitablevariantnet;auction design regulatory;regretformer outperforms regretnet;original objective regretnet;optimal auction design;regulatory auction;auction;regulatory auction design;regretnet regretformer;solution optimal auction;deep learning solution;auction mechanisms;problem optimal auction;deep learning"}, "1c682dca13e47e6e1ee3c8db54af631a8e5e5792": {"ta_keywords": "modeled markov decision;markov decision processes;markov decision;solving mads mds;decision processes mds;systems modeled markov;computing optimal policy;optimal policy large;modeled markov;solving mads;optimal policy;proposed computing optimal;mds large state;large state space;decision processes;computing optimal;properties solving mads;mads mds;policy large state;complexity strategies proposed;state space large;complexity strategies;markov;optimal;systems modeled;processes mds;state space;state space long;mads mds large;processes mds particular", "pdf_keywords": ""}, "48e32ba9a891f36183a26f35316e8906d14d83c0": {"ta_keywords": "crowdsourcing computational quality;crowdsourcing means quality;control crux crowdsourcing;crowdsourcing computational;crowdsourcing process;crowdsourcing process workers;purpose crowdsourcing computational;parameterizing crowdsourcing process;crowdsourcing;parameterizing crowdsourcing;crux crowdsourcing;crowdsourcing means;allow parameterizing crowdsourcing;purpose crowdsourcing;general purpose crowdsourcing;crux crowdsourcing means;quality control toolkit;computational quality control;quality control;demonstrate crowdd;demonstrate crowdd kit;crowdd kit;quality control techniques;quality control crux;quality control organizational;computational quality;means quality control;paper demonstrate crowdd;crowdd;crowdd kit general", "pdf_keywords": "crowdsourcing computational quality;aggregating annotating crowddsourced;aggregation algorithms crowddsourced;crowdsourcing computational;data crowdsourcing;analyzing data crowdsourcing;parameterizing crowdsourcing;purpose crowdsourcing computational;crowdsourcing means quality;data crowdsourcing marketplace;annotating crowddsourced annotations;annotating crowddsourced;crowdsourcing;parameterizing crowdsourcing process;crowddsourced annotations;crux crowdsourcing;crowdsourcing marketplace;control crux crowdsourcing;allow parameterizing crowdsourcing;crowdsourcing means;general purpose crowdsourcing;crowdsourcing process workers;purpose crowdsourcing;algorithms crowddsourced;crowdsourcing process;algorithms crowddsourced environment;crux crowdsourcing means;data set crowddsourced;workshop aggregating annotating;crowdsourcing marketplace implement"}, "d4756d1a7b81f53e71f939ab387cad5f0a4a13b7": {"ta_keywords": "duration sound event;lstm polyphonic sound;memory lstm polyphonic;lstm polyphonic;sound event detection;polyphonic sound event;duration sound;sound event insertion;sound event;term memory lstm;sound event precisely;model duration sound;lstm;reduce sound event;memory lstm;short term memory;event detection sd;duration controlled long;polyphonic sound;sequence detection;sequence sequence detection;duration controlled;polyphonic;event detection;called duration controlled;called duration;controlled long short;term memory;long short term;duration", "pdf_keywords": ""}, "19418493b1f9c82809fe4584af427b8807b8ae2d": {"ta_keywords": "commonsense locatednear relation;sentence level classifier;vision natural language;commonsense knowledge computer;entity pairs;commonsense locatednear;background commonsense locatednear;extract relationship sentence;automatically extract relationship;scores entity pairs;useful commonsense knowledge;language understanding machine;natural language understanding;commonsense knowledge;useful commonsense;natural language;extract relationship;sentences methods;relationship sentence level;sentences methods enable;sentence level;background commonsense;entity pairs detected;commonsense;type useful commonsense;machine comprehension propose;knowledge computer vision;sentences;located objects;language understanding", "pdf_keywords": "normalized sentence representation;relation classication deep;sentence level relation;relation textual corpora;sentence representation method;sentence representation;classication deep neural;textual corpora;textual corpora using;large corpus;extract relationship sentence;relation human language;binary classication task;large corpus methodswe;textual corpora problem;sentence level;normalized sentence;relation textual;propose normalized sentence;corpus;enriching textual;approach enriching textual;relationship sentence level;automatically extract relationship;pairs large corpus;word features;aloonary relation textual;use normalized sentence;classication task feature;novel tasks extracting"}, "298ddceada580c46e40e2a0323c0e3b16ed5f3c9": {"ta_keywords": "signals brain;plethora signals brain;signals brain article;evolution plethora signals;plethora signals;importance plethora signals;signals brain discuss;signals brain addition;signals;addition plethora signals;brain addition plethora;brain article evolution;evolution plethora;brain article;brain;brain discuss importance;brain addition;brain discuss;article evolution plethora;plethora;importance plethora;discuss importance plethora;evolution;addition plethora;article evolution;importance;discuss importance;addition;discuss;article", "pdf_keywords": ""}, "562fbb5d706d46f3e250429ac48e6acd2bf18cb1": {"ta_keywords": "speech enhancement separation;speech recognition integration;implementation speech enhancement;speech enhancement;speech recognition;important speech recognition;automatic speech recognition;rich automatic speech;automatic speech;speech recognition related;denoising source separation;source separation;source separation provide;separation capable processing;enhancement separation capable;processing single channel;dereverberation denoising source;enhancement separation;implementation speech;including dereverberation denoising;recognition integration;dereverberation denoising;denoising source;functionalities important speech;multi channel data;recognition integration present;separation capable;single channel;end implementation speech;channel data", "pdf_keywords": "separation speech enhancement;enhancement speech separation;beamformer speech separation;speech enhancement separation;speech separation multichannel;multichannel speech separation;separation multichannel speech;introduction speech enhancement;program speech enhancement;speech separation systems;end speech enhancement;singlechannel speech separation;approach speech enhancement;speech enhancement speech;automatic speechthe e2e;speech enhancement;framework speech enhancement;signals speech enhancement;speech separation dataset;neural beamformer speech;enhancement speech;speech separation task;performance speech separation;beamformer speech;speech separation;speech separation speech;development speech enhancement;speech recognition module;art speech enhancement;effective neural beamformer"}, "2dbe78aa516cc911a71ff333a35a5ce0b1a49640": {"ta_keywords": "speech recognition ar;introductionautomatic speech recognition;speech recognition;speech information;surrounding speech information;latency based;speech information presented;adjust latency based;introductionautomatic speech;dynamically adjust latency;latency based different;latency;adjust latency;recognition ar models;higher latency;errors surrounding speech;speech;latency exists;higher latency exists;recognition ar;latency exists inevitable;multi mode ar;surrounding speech;ar models;multi mode multi;refer multi mode;multi mode;models;single model dynamically;mode multi", "pdf_keywords": "multimode audio;multimode audio audio;method multimode audio;mode automatic speech;dynamically latency training;multimode adaptive adaptive;automated speech;speech recognition novel;automatic speech;speech recognition rapidly;streaming baselines trained;multimode adaptive;automated speech recognition;propose multimode adaptive;end speech recognition;automatic speech recognition;trained multi mode;method automatic speech;speech recognition;development automated speech;approach audio encoder;multi mode adaptive;approach audio;adaptive adaptivewe multi;audio;audio auditory;streaming context;competitive streaming baselines;audio encoder;audio auditory ar"}, "0dc379de3a613110c5fdc9c0361372c1114ee18d": {"ta_keywords": "alamos neutron sc;hydrogen sup beams;alamos neutron;los alamos neutron;lansce proton storage;neutron sc;sup beams enable;kev hydrogen sup;proton storage ring;sup injector;sup injector los;hydrogen sup;sup beams;proton storage;neutron sc scattering;lansce proton;center lansce proton;injector los alamos;storage ring psr;spl mu operation;neutron;ring psr improved;200 spl mu;laboratory lanl upgraded;alamos national laboratory;spl mu;80 kev hydrogen;ring psr;beams enable 200;psr improved", "pdf_keywords": ""}, "c6c6b4d328381a530e933c208bb43db2a7fa93c8": {"ta_keywords": "symbiotic mutation aetiology;mutation aetiology symbiotic;aetiology symbiotic mutation;occurrence symbiotic mutation;symbiotic mutation;mutation aetiology;aetiology symbiotic;occurrence symbiotic;mutation;symbiotic;aetiology;occurrence", "pdf_keywords": ""}, "7225c2a42990f850f692f8d82e7f3bfaf312145c": {"ta_keywords": "curriculum learning;neural networks slow;curriculum learning framework;propose curriculum learning;large neural networks;learning rate schedules;reduces training time;curriculum;extensive hyperparameter tuning;use large neural;specialized learning rate;models reduces training;neural network models;learning rate;training time reduces;hyperparameter tuning;networks slow train;neural network systems;large neural;extensive hyperparameter;reduces training;hyperparameter tuning methods;requires extensive hyperparameter;tricks specialized learning;specialized learning;propose curriculum;training time;neural network;framework neural network;learning framework neural", "pdf_keywords": "neural machine translation;machine translation models;novel translation models;translation models ability;translation models;learning novel translation;translation development neural;machine translation reduces;machine translation;machine translation development;machine translation systems;translation reduces training;translation systems;conference machine translation;translation models approach;translation models argue;translation models time;machine translation important;machine translation wmt17;mutated translation models;models curriculum learning;curriculum learning natural;translation development;formulation curriculum learning;curriculum learning;continuous curriculum learning;duration curriculum learning;propose curriculum learning;curriculum learning method;models trained curriculum"}, "4a76869cda286efb20eb78cc6adb13daab37a0d1": {"ta_keywords": "fokker planck solutions;fecal simulations;backgroundfemale fecal simulations;fecal simulations frequently;evolution fokker planck;simulating time evolution;planck solutions;fokker planck;time evolution fokker;computational approach simulating;stochastic systems level;stochastic systems;approach simulating time;planck solutions terms;behaviour stochastic systems;stochastic;simulating time;approach simulating;behaviour stochastic;numerical solutions methodswe;density functions broadly;evolution fokker;simulations;probability density functions;resort numerical solutions;characterise behaviour stochastic;simulating;planck;density functions;fokker", "pdf_keywords": "foamkker planck equations;foamkker planck solutions;stochastic particle simulations;backgroundfowlkker planck equations;evolution foamkker planck;planck equations extensively;planck equations approach;foamkker planck;densities stochastic particle;planck equations;underlying densities stochastic;stochastic simulations approximating;stochastic simulations demonstrate;force particle diffusion;stochastic simulations;dynamics density pt;direct stochastic simulations;particle diffusion;simulation molecular dynamics;densities stochastic;based dynamics density;particle diffusion proportional;stochastic simulations resulting;planck solutions;dynamics density;stochastic particle;stochastic simulations linear;particle simulations;dynamics empirical;types foamkker planck"}, "674f892caa52fa400109defa1773a10088918124": {"ta_keywords": "model predictive control;predictive control mpc;model prediction control;predictive control;predictive control recently;integral projected gradient;prediction control;primal dual methods;prediction control underlying;primal dual method;method model predictive;augmented lagrangian iteration;methods model predictive;projected gradient method;minimizing augmented lagrangian;model predictive;control mpc;gradient method model;augmented lagrangian;lagrangian iteration;lagrangian iteration propose;dual method termed;dual methods;dual methods model;mpc require minimizing;increasing primal dual;method model prediction;projected gradient;primal dual;proportional integral projected", "pdf_keywords": ""}, "cc549a11d277d86f6228443cb16c231c9bda6c96": {"ta_keywords": "clustering emphasis contextual;utterances modeling emphasis;clustering emphasis;state clustering emphasis;modeling emphasis active;modeling emphasis;focus utterances modeling;cluster adaptive training;emphasis contextual;indicating word emphasized;focus utterances;modeled emphasis using;speech conveys focus;cluster adaptive;emphasis contextual factor;emphasis using state;utterances modeling;word emphasized;emphasized addition cluster;conveys focus utterances;emphasis using;modeled emphasis;background emphasis;emphasis;emphasis active;addition cluster adaptive;utterances;state clustering;using state clustering;speech conveys", "pdf_keywords": ""}, "c1546da843be7ea3e0adfb85b69a0b08d41c7159": {"ta_keywords": "mechanisms disease caused;disease caused disease;possible mechanisms disease;disease caused;disease;mechanisms disease;caused disease;disease discuss;disease discuss possible;topic disease discuss;literature topic disease;topic disease;systematic review literature;results systematic review;systematic review;discuss possible mechanisms;results systematic;present results systematic;possible mechanisms;systematic;caused;review literature;article present;mechanisms;article present results;review literature topic;article;literature topic;aim article present;aim article", "pdf_keywords": ""}, "5b3ca06a7673e2bf372d5f89afb15ae1eb714075": {"ta_keywords": "time bayesian network;causal dynamics evolution;bayesian network lectbin;novel event driven;event driven;continuous time bayesian;bayesian network;causal dynamics;time bayesian;influenced occurrences events;event driven continuous;event occurrences interventions;influence event occurrences;potential causal dynamics;evolution influence event;model situations state;influenced occurrences;event occurrences;capture potential causal;occurrences events various;occurrences events;driven continuous time;causal;events various types;dynamics evolution influence;events;network lectbin representation;dynamics evolution;model situations;variables influenced occurrences", "pdf_keywords": ""}, "594827fdb2047bc7be4ea2f0d2364f46d187247e": {"ta_keywords": "japanese speech corpus;corpus youtube;corpus youtube videos;construction corpus youtube;subtitles speech recognition;speech corpus;corpus called jtubespeech;videos subtitles speech;new japanese speech;japanese speech;largesize speech corpora;subtitles speech;youtube videos subtitles;speech corpora open;speech corpora;videos subtitles;corpus;construction corpus;speech corpus called;speech recognition speaker;construct new japanese;recognition speaker verification;corpora languages;subtitles;corpora languages english;sourced corpora languages;recognition speaker;jtubespeech;speaker verification;speaker verification method", "pdf_keywords": "japanese speech corpus;data japanese speech;speech recognition japan;japanese speech recognition;corpus youtube;corpus youtube videos;propose speech corpus;speech corpus;construct speech corpus;subtitles speech recognition;videos subtitles speech;construction corpus youtube;speech corpus construction;speech data;japan used corpus;speech corpus called;language speech recognition;japanese speech;datasets used speaker;new japanese speech;speech recognition auditory;corpus called jtubespeech;subtitles speech;speech recognition speaker;analysis audio data;videos subtitles;audio transcriptions;corpus useful building;corpus;speech data 110"}, "539631a828bf0badd20d2241784b4e06c223250e": {"ta_keywords": "model speaker clustering;mixture model speaker;speaker clustering;scale mixture models;mixture models;gaussian mixture model;gm speaker modeling;speaker modeling speech;speaker clustering noisy;modeling speech gams;speaker modeling;scale mixture model;mixture model;modeling speech;gaussian mixture;gibbs sampling based;mixture model methodsthe;mixture models assume;mixture model gm;represented gaussian mixture;multi scale mixture;mixture represented gaussian;speech gams;introductionblocked gibbs sampling;gibbs sampling;clustering noisy;model gm speaker;mixture represented;speech gams represents;component mixture represented", "pdf_keywords": ""}, "d5181375d242ed181bcde0d682a3c7ec4c4c6102": {"ta_keywords": "autism spectrum;measure autistic traits;autism spectrum conditions;skills position autism;position autism spectrum;autism spectrum members;autistic traits;measure autistic;objective measure autistic;autistic traits automatically;reasons autism spectrum;autism;position autism;social skills communication;social communication skills;social skills;verbal cognitive skills;social communication difficulties;non verbal cognitive;cognitive skills;communication skills;skills communication;communication skills use;variety reasons autism;autistic;trouble social skills;communication difficulties improve;reasons autism;skills communication greater;cognitive skills position", "pdf_keywords": ""}, "562fe9b2f5e7ede128dd9a93edc3971c5e0a2394": {"ta_keywords": "influence interlocutors polylogue;dialog acts word;word use polylogue;dialog acts;usage words influence;effect dialog acts;dialog act;interlocutors polylogue methods;interlocutors polylogue;context influence interlocutors;use polylogue;use polylogue work;polylogue work examine;words influence model;dialog act information;words influence;polylogue work;polylogue methods;model dialog act;effect dialog;influence interlocutors;influence model dialog;dialog;polylogue methods basic;influence interlocutors poly;polylogue;word use context;cache model influence;usage words;examine effect dialog", "pdf_keywords": ""}, "05710169c48ac1ffe6af514cc10e72d025023343": {"ta_keywords": "mechanism disease understood;disease understood development;new models development;models development new;mechanism disease;models development;development new models;underlying mechanism disease;new models;disease;disease understood;models;development new approaches;new approaches development;approaches development new;new approaches;development new;understood development new;approaches development;mechanism;underlying mechanism;development;understood development;approaches;new;understood;underlying", "pdf_keywords": ""}, "14a6452d6d026a3f384e425add6ab68f8e65037f": {"ta_keywords": "data labelling crowdsourcing;labelling crowdsourcing;labelling crowdsourcing shared;labelling public crowdsourcing;efficient data labelling;public crowdsourcing;data labelling public;crowdsourcing;public crowdsourcing marketplaces;crowdsourcing marketplaces present;crowdsourcing shared;data labelling;introduction data labelling;crowdsourcing marketplaces;efficient label collection;label collection project;crowdsourcing shared leading;label collection;efficient label;labelling;real label collection;labelling public;label collection tasks;labelling process;label;components efficient label;choose real label;launch label collection;label collection aimto;launch label", "pdf_keywords": ""}, "ea1ba6f5e5852e38ace7bbae4e4f60ffeeabe5b1": {"ta_keywords": "microfracturing repaired fixed;microfracturing repaired;patient treated microfracturing;treated microfracturing repaired;treated microfracturing;formation hydraulic fractures;hydraulic fractures;developed sustained microfracturing;sustained microfracturing produced;sustained microfracturing;hydraulic fractures patient;microfracturing produced;microfracturing;microfracturing produced formation;fractures;fractures patient treated;fractures patient;formation hydraulic;produced formation hydraulic;repaired fixed fixed;repaired fixed;repaired;hydraulic;patient developed sustained;patient treated;report case patient;case patient developed;treated;developed sustained;report case", "pdf_keywords": ""}, "70a2a554829f2cebb9fa89829994444fa1ec5a7b": {"ta_keywords": "preferences reach collective;collective decision multi;groups multi agent;decision multi agent;reach collective decision;collective decision methods;aggregate preferences;collective decision;notion distance cytogenetics;aggregate preferences reach;know aggregate preferences;multi agent;multi agent context;distance cytogenetics;individuals groups multi;single individuals groups;cytogenetics;agent context important;decision multi;agent context;decision making single;individuals groups;groups multi;collective;making single individuals;reach collective;single individuals;agent;groups;individuals", "pdf_keywords": ""}, "39c5740304b5f4072f92e4e012a4b57e7bc2e817": {"ta_keywords": "speech enhancement;speech enhancement techniques;speech separation studies;channel speech separation;speech separation;metrics signal quality;signal quality;signal quality overwhelming;signal todistortion ratio;single channel speech;enhancement;enhancement techniques typically;enhancement techniques;intrinsic metrics signal;sr measure fidelity;measure fidelity ground;channel speech;evaluation metrics;ground truth waveform;todistortion ratio sr;focus intrinsic metrics;diversity evaluation metrics;truth waveform;todistortion ratio;measure fidelity;metrics signal;intrinsic metrics;metrics usually;variants signal todistortion;separation studies", "pdf_keywords": ""}, "470fd4faf9b8499e8bf21c5d143145305d07fe83": {"ta_keywords": "geocoded tweets methods;geocoded tweets;methods geocoded tweets;tweets methods geocoded;geocoded tweets overly;applies geocoded tweets;entity linking tweets;tweets close space;sparse tweets;sparse tweets use;tweets methods;linking tweets;proximity utilizes geocoded;overly sparse tweets;linking tweets close;spatio temporal proximity;tweets overly sparse;collective entity linking;tweets close;temporal proximity;tweets use;tweets;entities mentioned spatio;fact events geographical;tweets overly;events geographical;temporal proximity approach;events geographical points;version spatial proximity;spatial proximity", "pdf_keywords": ""}, "5667f934c5bc008d0464878729eed34cbf7ec1df": {"ta_keywords": "supervised relation extraction;semi supervised relation;constraints semi supervised;supervised relation;modeling semi supervised;relation extraction;semi supervised learning;relation extraction methodswe;semi supervised;supervised learning constraints;learning constraints unlabeled;constraints unlabeled data;natural semisupervised learning;walks graph classifiers;semisupervised learning;graphs classifiers impose;graph classifiers;graphs classifiers;tasks natural semisupervised;natural semisupervised;classifiers impose constraints;learning constraints;semisupervised learning heuristics;constraints unlabeled;introductionusing graphs classifiers;supervised learning;supervised classification;supervised classification tasks;unlabeled data;traditional supervised classification", "pdf_keywords": ""}, "2f1743d1a1be46452ab90691ead8bf916ffd912b": {"ta_keywords": "probabilistic enhancement electroencephalogram;enhancement electroencephalogram;enhancement electroencephalogram fundamental;electroencephalogram;electroencephalogram fundamental;electroencephalogram fundamental process;neuropsychological disorders;neuropsychological disorders fundamental;neuropsychological;neuropsychological disorders report;development neuropsychological disorders;process development neuropsychological;development neuropsychological;probabilistic enhancement;probabilistic;enhancement;disorders fundamental process;disorders fundamental;disorders report results;disorders;disorders report;case patient developed;patient developed;results case patient;process;case patient;process development;case;fundamental process;fundamental process development", "pdf_keywords": ""}, "402ab2adcf9da95e6aad9884b1ec53271f39cd32": {"ta_keywords": "adversary kalman filter;estimating adversary kalman;adversary kalman;inversekalman filter;inverse filtering;formulations inversekalman filter;extended kalman filtering;kalman filtering;inversekalman filter ikf;counter adversarial systems;adversarial systems;adversarial systems example;counter adversarial;predicting adversary;estimating adversary;adversarial;kalman filter tracked;kalman filter;context inverse filtering;backgroundinverse extended kalman;kalman filtering important;predicting adversary future;example estimating adversary;extended kalman;inversekalman;formulations inversekalman;inverse filtering address;adversary future steps;filter tracked estimate;recent formulations inversekalman", "pdf_keywords": "adversarial denoting inverse;estimates inverse filter;adversary kalman filter;filtering nonlinear inverse;inverse filtering;theory inverse filtering;nonlinear inverse filtering;inverse filtering nonlinear;inverse filter prediction;inverse filtering systems;systems inverse filter;research inverse filtering;inverse filters;inverse filtering bayesian;inverse filtering augmented;inverse filtering problem;forward inverse filters;inverse filters optimal;based inverse filter;inverse filter;stability inverse cognition;formulation inverse filtering;achieves inverse filter;stability inverse ekf;denoting inverse filtering;estimating adversary kalman;inputs inverse filter;inverse filter efficiently;inverse filters presence;matrix inverse filter"}, "f82ae0a87cae2f3a43d4c0289d0cdf7ca57461d0": {"ta_keywords": "diagnosis gyrus induced;patient diagnosis gyrus;gyrus induced aetiology;diagnosis gyrus;gyrus induced;gyrus;gyrus based;gyrus based approach;literature role gyrus;role gyrus based;role gyrus;diagnosis;induced aetiology;patient diagnosis;management patient diagnosis;aetiology;induced;management patient;approach management patient;patient;approach management;article;management;article present;results review;results review literature;results;article present results;present results;present results review", "pdf_keywords": ""}, "217074971e3dfdbfab3a8c3819cd7953ae666da4": {"ta_keywords": "biomedical text mining;automatic biomedical text;biomedical text;text mining;text mining training;reliability biomedical text;pubmed selecting articles;scientific literature biological;discovered automatic biomedical;literature biological biomedical;behaviors querying pubmed;text mining answers;querying pubmed;automatic biomedical;querying pubmed selecting;pubmed selecting;biomedical science research;knowledge discovered automatic;literature biological;biological biomedical;biological biomedical science;mining answers;scientific literature;mining answers chal;biomedical;biomedical science;human behaviors querying;increasing scientific literature;pubmed;discovered automatic", "pdf_keywords": ""}, "1b759204e7c13f0e4af9fe00b052af4456ac3669": {"ta_keywords": "games reinforcement;games reinforcement learning;sequential decision;console games reinforcement;sequential decision making;reinforcement learning;reinforcement learning algorithms;practice sequential decision;reinforcement;decision making agents;atari console games;sensing steps clear;agents designed sense;benefit atari;benefit atari console;atari;sensing steps;indicate benefit atari;atari console;practice sequential;steps ignoring state;designed sense state;sequential;state information sensing;sense state;agents;learning algorithms deliver;policies run;sense state regular;better policies run", "pdf_keywords": "actions reinforcement learning;games reinforcement learning;actionrepetition sequential decision;state abstraction reinforcement;games reinforcement;temporal difference learning;actions reinforcement;reinforcement learning prediction;reinforcement learning;sequential decision making;console games reinforcement;macro actions reinforcement;sequential decision;learning action;optimal behavior action;skipping actionrepetition sequential;deep reinforcement learning;abstraction reinforcement learning;reinforcement learning algorithms;reinforcement;action selection optimal;reinforcement learning role;deep reinforcement;reinforcement learning present;policy based induction;selection optimal policy;actionrepetition sequential;optimal policy;abstraction reinforcement;learning action repetition"}, "c1125fa33a239ef4fd3378ccd46b2a0a0cf79a15": {"ta_keywords": "storage overheads codes;errorasure codes reed;errorasure codes;reed solomon codes;solomon codes extensively;data centers;codes reed solomon;data replication methods;reliability data replication;data replication;bandwidth disk reconstruction;solomon codes;higher reliability data;deployed data centers;codes reed;data centers offer;overheads codes;overheads codes mandate;methods lower storage;background errorasure codes;codes extensively;disk reconstruction data;codes mandate;lower storage overheads;bandwidth disk;network bandwidth disk;replication methods;errorasure;storage overheads;codes extensively deployed", "pdf_keywords": ""}, "ac8d33e4c0a45e227a47353f3f26fbb231482dc1": {"ta_keywords": "facts model memorize;language models;pretraining corpus;model memorize;pretraining corpus contain;setting pretraining corpus;model memorize methods;language models ls;corpus contain facts;corpus;introduce diagnostic dataset;plays language models;memorize methods introduce;memorize methods;memorize;models ls trained;diagnostic dataset;contain facts model;diagnostic dataset aimed;trained snapshots;trained snapshots data;facts model;corpus contain;ls trained snapshots;data collected;facts come expiration;dataset;dataset aimed;book setting pretraining;language", "pdf_keywords": "data training temporal;training temporal context;training temporal;acquisition temporal knowledge;knowledge understanding temporal;temporal context improves;temporal knowledge;facts model memorize;time aware language;temporally scoped pretraining;time aware pretraining;factual knowledge pretrained;aware language model;pretraining empirical evidence;temporally scoped facts;temporally scoped knowledge;new dataset temporally;model memorize;temporal shift knowledge;future training temporal;improves memorization facts;pretraining corpus;temporal context text;language model probing;temporal context data;training period temporal;dataset temporally;aware pretraining;setting pretraining corpus;pretraining corpus contain"}, "9b1057e1f6eb17abf3962d6cd2f49468d27b94c6": {"ta_keywords": "translation emphasis pause;emphasis pause prediction;machine translation emphasis;emphasis pause;speech translation systems;pause prediction speech;speech translation;speech speech translation;translation emphasis active;translation emphasis;improve translation emphasis;emphasis cross lingual;introductionimproving translation emphasis;translation systems methodsprosodic;translation systems;translation systems objectiveto;communicating machine translation;pause prediction;machine translation;prediction speech speech;translation word level;prediction speech;word level emphasis;introductionimproving translation;objectiveto improve translation;translation word;work translation word;emphasis;emphasis cross;lingual transfer", "pdf_keywords": ""}, "c985600f0aa223ddc76a2ea628f1fa23504dcbcd": {"ta_keywords": "introductiontarget speech extraction;speech extraction;speech extraction specific;speech recognition ar;automatic speech;source separation auxiliary;automatic speech recognition;optimize source separation;speech recognition;source separation;source separation systems;separation systems transcription;transcription based objective;end automatic speech;introductiontarget speech;saved anchor speech;speech examples target;anchor speech;separation auxiliary information;examples target speaker;anchor speech examples;transcription;case source separation;transcription based;separation auxiliary;extraction specific;target speaker;extraction;optimize source;systems transcription based", "pdf_keywords": ""}, "0e141942fa265142f41a2a26eb17b6005d3af29e": {"ta_keywords": "multilingualism linguistic diversity;language technologies;language technologies contribute;languages world represented;evolving language technologies;languages world;linguistic diversity world;languages;linguistic diversity;different languages;multilingualism linguistic;promoting multilingualism linguistic;7000 languages world;introduction language technologies;language technologies applications;languages resources representation;rapidly evolving language;7000 languages;language;trajectory different languages;different languages followed;languages resources;languages followed;multilingualism;linguistic;promoting multilingualism;evolving language;languages followed time;contribute promoting multilingualism;types languages resources", "pdf_keywords": "resource rich languages;language diversity inclusion;language diversity;languages world broadly;languages category rich;resources majority languages;languages resources representation;diversity inclusion nonlingual;unlabeled corpora languages;resource class languages;languages languages world;category languages;languages category;languages based unlabeled;languages world;languages labeled;language resource availability;language classes;language resource;languages non multilingual;disparity languages especially;rich languages;languages languages;corpora languages labeled;vast majority languages;disparity languages;languages;nonlingual world discussed;multilingual languages powerful;various languages"}, "683e201783bf76ab99791a02e3763fd3ab8dad96": {"ta_keywords": "deep active learning;named entity recognition;deep learning active;active learning named;learning named entity;entity recognition important;entity recognition;active learning outperform;entity recognition typical;learning active learning;active learning;named entity;deep active;learning named;manually labeling data;manually labeling;datasets result deep;learning active;labeling data;labeling data available;deep learning;result deep learning;labeling;deep learning employed;tool named entity;entity;combining deep learning;combining deep;emerge large datasets;result deep", "pdf_keywords": "entity recognition ner;encoder convolutional word;cnn lstm;named entity recognition;sequence tagging;cnn lstm modeldeep;word level encoder;neural approaches ner;architecture sequence tagging;sequence tagging present;cnn cnn lstm;sequence tagging conference;entity recognition;short term memory;deep neural;convolutional word level;datasets use deep;deep learning;propose lightweight neural;encoder long short;lightweight neural;lstm;word level representations;lstm modeldeep learning;deep active learning;encoder long;approach deep neural;reduced deep learning;character level encoder;encoder"}, "e318e554098224c9475dfc80765cbbb82fa4a409": {"ta_keywords": "ai collaboration pipeline;pipeline peer review;collaboration pipeline peer;human ai collaboration;ai collaboration;peer review;collaboration pipeline;peerreview pipeline nowadays;peerreview pipeline;automation parts peerreview;peer review mitigate;pipeline peer;parts peerreview pipeline;collaboration;peerreview;human ai;design human ai;peer;identified need automation;human referees required;ai;automation;parts peerreview;need automation;ensure science progresses;referees required interact;review mitigate issues;human referees;publication;science progresses fair", "pdf_keywords": ""}, "8c9033f976e7787dde9af5ba952d7f9ac9c34496": {"ta_keywords": "feature evolving streams;ensemble outlier detector;based ensemble outlier;ensemble outlier;evolving streams;evolving streams setting;outlier detection;outlier detector;outlier detector called;xstream extreme streaming;xstream extreme;feature evolving;emerging features time;outlier detection problem;streams;called xstream extreme;outlier;detector called xstream;density based ensemble;addresses outlier detection;extreme streaming;streams setting data;streams setting;xstream;extreme streaming setting;evolve feature;emerging features;newly emerging features;feature space evolve;problem feature evolving", "pdf_keywords": ""}, "36cbc3c24429ba3b69def38e6e64b41485b0a023": {"ta_keywords": "information resources search;search new information;resources search new;web important resource;new tool search;resources search;new information resources;tool search new;tool search;search new;information resources;search;web important;web;resource development new;development new technologies;new technologies;new technologies technologies;resource;technologies developed new;important resource development;new information;resource development;resources;developed new tool;technologies technologies developed;new tool;technologies developed;technologies technologies;important resource", "pdf_keywords": ""}, "119c33321fc0e1db837ce293f1b65cc26c1cc34e": {"ta_keywords": "fact checking articles;evidence detection reranking;detecting previously fact;fact checked spread;falsese claims previously;falsese claims;fact checking;introduction falsese claims;candidate fact checking;evidence detection;fact checked claims;detection reranking;previously fact checked;checking articles fc;providing evidence detection;claims previously fact;detection reranking candidate;introduction falsese;checking articles;reranking candidate fact;falsese;spread detecting previously;continual spread detecting;checked spread social;social media mitigate;fc articles retrieved;fc articles;articles fc articles;spread social media;articles retrieved m25", "pdf_keywords": "neural model summarization;summarization translation tasks;detection fact checking;embeddings claims sentences;factchecked claim detection;model summarization translation;claim detection fact;novel reranker memory;relevance fact checking;matching detecting fact;reranking data twitter;relevance segments summarization;claims sentences occurrence;summarization translation;fact checking articles;detecting fact checked;reranker molecular reranker;novel reranker molecular;reranker memory;propose novel reranker;claim experiments twitter;reranker memory enhanced;model summarization;molecular reranker combinewe;claim detection;fact checking;reranking data;molecular reranker;integrating textual information;novel reranker"}, "708dcd8456426cd609c89a86344e0007c04c80bf": {"ta_keywords": "factual knowledge retrieval;introductionlanguage models ls;assess factual knowledge;capturing factual knowledge;puntacana located knowledge;written queried languages;languages studies ls;introductionlanguage models;queried languages;factual knowledge completing;knowledge written queried;factual knowledge;knowledge retrieval;ls factual representation;located knowledge written;ls factual;studies ls factual;queried languages studies;factual representation ability;english methodsto assess;blank questions puntacana;questions puntacana located;questions puntacana;methodsto assess factual;languages studies;successful capturing factual;introductionlanguage;english methodsto;located knowledge;knowledge written", "pdf_keywords": "multilingual benchmark probing;new multilingual benchmark;multilingual benchmark probe;multilinguality factual knowledge;multilingual benchmark;factual knowledge retrieval;23 languages benchmark;multilinguality factual;token factr benchmark;languages benchmark;factr benchmark multi;languages benchmark unbiased;multi token factr;knowledge factr benchmark;retrieval factual knowledge;multilingual lims retrieval;multilingual multi token;factr benchmark performance;benchmark probing factual;facts different languages;retrieval factual;factr benchmark;token factr;queried languages;state art multilingual;languages accuracy;languages accuracy model;entities 23 languages;intersection multilinguality factual;factr benchmark relatively"}, "db0c587111cfed85dcea413e385b17881e6e0cbb": {"ta_keywords": "matrix adaptation evolution;recurrent neural network;term memory lstm;neural network language;memory lstm recurrent;lstm recurrent neural;improve speech recognition;matrix adaptation;short term memory;language models known;recurrent neural;lstm recurrent;language models;covariance matrix adaptation;speech recognition;parameter tuning neural;language model based;language model;memory lstm;network structures training;based language models;network language model;tuning neural network;lstm;speech recognition performance;structure discovery parameter;evolutionary algorithms;tuning neural;using evolutionary algorithms;structure discovery", "pdf_keywords": ""}, "3b30422b372040ad19a713b35006c21808287720": {"ta_keywords": "erasure codes;errorasure codes increasingly;erasure codes called;regeneration msr codes;storage regeneration msr;tolerance distributed storage;minimum storage regeneration;storage systems rs;errorasure codes;distributed storage systems;replication fault tolerance;storage systems;distributed storage;msr codes emerged;called minimum storage;class erasure codes;systems rs codes;storage regeneration;msr codes;data replication fault;minimum storage;fault tolerance distributed;data replication;alternative data replication;rs codes;codes called minimum;savings storage;storage;rs codes provide;savings storage space", "pdf_keywords": ""}, "6fa7de6f3ce3a599de6fab273a0d43939e176e9d": {"ta_keywords": "assistance help agents;ai agents encounter;deployed ai agents;ai agents;help agents;help agents overcome;leveraging human assistance;agents encounter problems;useful information assistant;deployed ai;enables agent determine;encounter problems autonomous;introduction deployed ai;assistant;agent determine request;agents encounter;human assistance;agent determine;agent;framework enables agent;human assistance help;autonomous problemsolving capabilities;enables agent;agents;ai;agents overcome;information assistant;contextually useful information;request contextually useful;general interactive", "pdf_keywords": "agent learns request;assistant reinforcement;task agent learns;agent trained respond;information assistant reinforcement;agent request description;agent performs tasks;learning agent;ai agents;assistant reinforcement learning;agent learns;agent requests;perform tasks autonomous;task agent requests;simulatedthe intention based;agent requests incorporates;equips learning agent;performs tasks autonomous;tasks autonomous;interaction policy agent;intentional requests human;tasks autonomous capabilities;learns request;task agent;information provided agent;agent request;implementing agent;agent learns decide;trained respond information;enables agent determine"}, "48e4ba2d04bd98843d5aab6e227b29584d63f7b6": {"ta_keywords": "crowdsourcing taxonomies cooperation;crowddsourcing cooperation linguistic;crowdsourcing taxonomies;crowdsourcing genre taxonomies;crowdsourcing genre;crowdsourcing gamification;current crowdsourcing genre;current crowdsourcing;approaches crowdsourcing gamification;survey crowdsourcing taxonomies;involved current crowdsourcing;annotators survey crowdsourcing;crowdsourcing;survey crowdsourcing;crowdsourcing gamification motivated;approaches crowdsourcing;use approaches crowdsourcing;cooperation linguistic resources;introduction crowddsourcing;introduction crowddsourcing cooperation;crowddsourcing;crowddsourcing cooperation;taxonomies cooperation linguistic;linguistic resources populated;cooperation linguistic;linguistic resources methods;resources methods linguistic;taxonomies cooperation;genre taxonomies;gamification", "pdf_keywords": "crowdsourcing projects mechanized;crowdsourcing projects;crowdsourcing;disadvantages crowdsourcing projects;various types crowdsourcing;present crowdsourcing;crowdsourcing contribution;types crowdsourcing;disadvantages crowdsourcing;paper present crowdsourcing;crowdsourcing contribution quality;advantages disadvantages crowdsourcing;background crowdsourcing;problem human annotators;types crowdsourcing contribution;crowdsourcing major public;annotators motivation availability;present crowdsourcing major;crowdsourcing mainstream suited;annotated examples community;corpus annotation;crowdsourcing mainstream;annotation information;human annotators motivation;human annotators;creation corpus annotation;corpus annotation information;crowdsourcing major;annotation information extraction;annotation"}, "43a87867fe6bf4eb920f97fc753be4b727308923": {"ta_keywords": "pretrained language models;large pretrained language;efficient transfer learning;language models downstream;transfer learning;pretrained language;nonlodo language learning;transfer learning methods;parameters pretrained model;parameters pretrained;finetune parameters pretrained;pretrained model;language learning;parameter efficient transfer;language models;models downstream tasks;language learning conventional;tuning large pretrained;pretrained model prohibitive;tasks facto learning;learning paradigm nonlodo;pretrained;models downstream;learning methods fine;efficient transfer;learning;nonlodo language;large pretrained;downstream tasks facto;downstream tasks", "pdf_keywords": ""}, "3e24375a1810375183d47ceadc7418e94533ba5f": {"ta_keywords": "envy freeness online;online allocation perishable;mechanisms online allocation;online allocation;concept envy freeness;envy freeness;allocation perishable resources;fairness efficiency;vehicle charging agents;fairness efficiency doing;concept envy;charging agents;allocation perishable;including fairness efficiency;extend concept envy;envy;freeness online settings;freeness online;electric vehicle charging;consider mechanisms online;perishable resources energy;vehicle charging;efficiency;charging agents arrive;allocation;consider mechanisms money;electric vehicle;perishable resources;explore trade offs;objectives including fairness", "pdf_keywords": ""}, "b63b698d177ba2861fe97d23763d66324bb1236a": {"ta_keywords": "viruses defective mi2;influenza virus undergo;mi2 ion channel;introduction influenza virus;mutant containing mi;influenza virus;defective mi2 ion;replication withoutm2ion channel;virus cell culture;introduction influenza;mi2 ion;defective mi2;loss mi2 ion;channel activity vitro;withoutm2ion channel activity;withoutm2ion channel;mi2;virus cell;virus undergo;influenza;replication withoutm2ion;generate viruses defective;virus undergo multiple;generate viruses;loss mi2;generated chimeric mutant;cycles replication withoutm2ion;type virus cell;ion channel activity;apparent loss mi2", "pdf_keywords": ""}, "dcd39e2eb27d17c369f3bf7a5a7a2a30bb9201c8": {"ta_keywords": "training language models;pre training data;training data semantics;introductionpre training language;trained downstream tasks;semantics make pre;training language;constructed datasets pre;datasets pre;make pre trained;language models;pre trained;directly trained downstream;trained downstream;language models ls;unlabeled text data;pre training;traits pre training;training data;large scale unlabeled;introductionpre training;data semantics make;directly trained;text data makes;data semantics;counterparts directly trained;semantics make;pre trained superior;downstream tasks;text data", "pdf_keywords": "semantic pre training;training language models;natural language downstream;language downstream tasks;language pre trained;language model trained;languages pre training;language downstream;trained human languages;sequence pre trained;language model robust;training data semantics;trained human language;semantic downstream task;training language;pre trained models;pre training models;language model data;models pre trained;introductionpre training language;training different semantic;datasets pre training;language models;pre training data;datasets makes linguistic;semantic downstream;human language yield;human languages pre;learn downstream task;language yield better"}, "c6af1ad95917badd7bc65b303a40f54950360279": {"ta_keywords": "statistical dialog managers;rule based dialog;based dialog managers;practice statistical dialog;statistical dialog;dialog managers;dialog managers difficult;dialog managers potentially;dialog managers important;based dialog;dialog;statistical rule based;rule based systems;rule based counterparts;rule based;decisions rule based;statistical rule;crafted rule based;hand crafted rule;robust decisions rule;practice statistical;make robust decisions;statistical;decisions rule;integration statistical rule;crafted rule;cost level integration;robust decisions;level integration statistical;managers important cost", "pdf_keywords": ""}, "595306f993993e44e2c2f674367103f44df03d9b": {"ta_keywords": "resource machine translation;large monolingual data;target monolingual data;monolingual data;challenges machine translation;machine translation;monolingual data regarded;machine translation terms;utilizing large monolingual;low resource language;machine translation using;fluency data augmentation;data augmentation low;resource language pairs;augmentation low resource;data augmentation;translation using target;using target monolingual;target monolingual;translation terms;translation using;language pairs paucity;data augmentation utilizing;language pairs;large monolingual;translation terms adequacy;framework data augmentation;monolingual;paucity parallel data;parallel data", "pdf_keywords": "machine translation increased;translation effective tool;low resource translation;resource machine translation;machine translation effective;unsupervised machine translation;machine translation terms;augmenting translated;translation translation models;translate translation data;methods lowresource translation;unsupervised translation humanll;translation data;improves quality translation;challenges machine translation;machine translation;augmenting translated humanll;translation data single;lowresource translation advantage;monolingual data;translation models;lowresource translation;translation data large;machine translation applications;target monolingual data;machine translation development;machine translation uses;unsupervised translation;translating related languages;machine translation important"}, "5d6f87e31d806a77d22e344106d0310be3342259": {"ta_keywords": "manipulation peer review;peer review randomized;randomized reviewer;randomized reviewer assignments;peer review reviewers;conference peer review;review randomized reviewer;peer review;review randomized;review reviewers;reviewer assignments;reviewing reviewers deliberately;reviewers deliberately;reviewers maliciously attempting;review reviewers maliciously;reviewers maliciously;reviewing reviewers;reviewers deliberately attempt;reviewers;torpedo reviewing reviewers;introductionimtigating manipulation peer;authors torpedo reviewing;reviewing;reviewer assignments methodswe;reviewer;manipulation peer;conference peer;challenges conference peer;dislike order reject;torpedo reviewing", "pdf_keywords": "reviewer anonymization peer;randomized reviewer paper;anonymization peer review;randomized reviewer;use randomized reviewer;assigning papers reviewers;optimally assign reviewers;reviewerthe paper assignment;reviewer paper assignments;reviewer anonymization;assign reviewers papers;study randomized reviewer;randomized assignment algorithms;assigning particular reviewer;efficient randomized assignment;reviewers deterministic assignment;randomized paper assignments;assign reviewers;reviewers assigned paper;reviewers reviewer anonymization;reviewer bias peer;iii reviewer anonymization;randomized assignment efficientlywe;optimal randomized assignment;peer review task;randomized assignment randomized;individual reviewer paper;limiting probability reviewer;randomized assignment;probabilities individual reviewer"}, "68d0b245e9754de9f36cba305e4ce50ff868cb6a": {"ta_keywords": "combinatory categorial grammar;unsupervised parsing chemical;grammar induction algorithm;robust grammar induction;grammar induction combinationatory;combinationatory categorial grammars;parsing chemical chemical;categorial grammars;categorial grammar achieves;based grammar induction;parsing chemical;categorial grammar;categorial grammars described;grammar induction;robust grammar;simple based grammar;simple robust grammar;unsupervised parsing;algorithm combinatory categorial;grammars described;based grammar;grammars;induction combinationatory categorial;combinatory categorial;parsing;grammar achieves;grammar;chemical chemical chemical;grammar achieves state;induction algorithm combinatory", "pdf_keywords": ""}, "7cc74ffa1215321712d4a830bb9dee19d9f0fb47": {"ta_keywords": "answering models;answering models struggle;question answering models;compositional generalization language;question answering;improve compositional generalization;background question answering;improve generalization learning;compositional generalization;generalization language;generalization learning;generalization learning permutation;compositions training patterns;approaches improve generalization;improve generalization;end models learn;input embedding;models learn;compositions training;generalize novel compositions;syntax context prior;novel compositions training;answering;learning permutation;models struggle generalize;generalization;training patterns longer;input syntax context;improve compositional;learn flat input", "pdf_keywords": "improve compositional generalization;compositional generalization language;learn compositionally generalizable;aware syntactic composition;syntactic composition attention;semantic compositions proposed;semantic compositions;generating compositionally generalizable;syntactic semantic compositions;semantics knowledge compositional;graph syntactic compositions;composition attention;generalization semantic syntactic;compositional generalization;generalization language representations;language representations grounding;structured predictions attention;generalization semantic;humans reason compositionally;representations grounding structured;compositionally generalizable;outputs syntactic compositions;truly reason compositionally;generating compositionally;encoder aware syntactic;syntactic compositions model;syntactic compositions;aware syntactic;generalization language;syntactic composition"}, "ee3ce47b79917974d30b6eeaaeeba99f1b1a5c59": {"ta_keywords": "transducer based end;transducer based;introduction study transducer;transducer;study transducer;end anrrar arrar;study transducer based;anrrar arrar arrar;anrrar arrar;arrar arrar arrar;arrar arrar;arrar arrar ar;end end anrrar;arrar;arrar ar;end anrrar;anrrar;end end;based end end;based end;ar;end;introduction;introduction study;study;based", "pdf_keywords": "escalating speech recognition;auxiliary tasks decoding;deep automatic speech;decoder speech recognition;decoder speech;task learning auxiliary;learning auxiliary tasks;speech recognition;automatic speech;speech recognition ar;model speech;speech recognition rapidly;speech recognition toolkit;model speech evaluated;automatic speech recognition;based model speech;task escalating speech;constrained beam search;beam search strategies;trained auxiliary task;multi task learning;based beam search;beam search;decoding strategy transducer;encoder decoder speech;beam search algorithm;beam beam search;task learning;learning auxiliary;various beam search"}, "5f96e3e00b36c5eeebff09a1bf4c804bd4ce4620": {"ta_keywords": "malicious sensors;data injection attack;potentially malicious sensors;observations remote estimator;attack remote state;remote estimator;remote estimator challenge;remote state estimation;injection attack remote;false data injection;malicious sensors start;attack remote;detection false data;data injection;observations remote;noisy linear observations;injection attack;report observations remote;estimator challenge presence;strategically manipulating observations;linear observations;manipulating observations random;quickest detection false;state estimation considered;state estimation;methodsa set sensors;observations random;detection false;observations random time;remote state", "pdf_keywords": "optimal bayesian attack;attack remote estimation;optimal attack detection;bayesian attack detector;bayesian attack;attack detection policy;detection attack;sensors linear attack;prediction probability attack;attack detection multisensor;attack detector;attack detector linear;probability attack launched;belief probability attack;attack detection;attack anomaly sensors;malicious sensors;quickest attack detectionwe;detection policy;estimator detection probability;detecting linear attack;linear attack multisensor;detection policy propose;remote estimator detection;detection attack anomaly;probability attack;detectionwe present optimal;prediction attack;remote estimation bayesian;detector linear attack"}, "64a106b707586345a055aa22c3356c4dc3d01877": {"ta_keywords": "dynamics network networks;networks efficiently measured;dynamics network;network networks efficiently;networks efficiently;network networks;dynamically regulated events;network networks fundamental;networks fundamental;networks;networks fundamental fundamental;shown dynamics network;dynamically dynamically regulated;dynamically regulated;events shown dynamics;network;regulated events shown;measured combination dynamically;efficiently measured;dynamics;regulated events;efficiently measured combination;events;recently shown dynamics;efficiently;dynamically;events shown;shown dynamics;systems;dynamically dynamically", "pdf_keywords": ""}, "d5a95567e079685322cd485033d334284c4b0a62": {"ta_keywords": "develop reversible polymerization;reversible polymerization approaches;reversible polymerization;controlled polymerization;polymerization approaches;polymerization;controlled polymerization growing;field controlled polymerization;polymerization growing;vinyl polymers;degradability particularly vinyl;facilitating polymer;polymerization growing evolving;vinyl polymers general;polymers general concern;property polymer;polymer;particularly vinyl polymers;property polymer materials;rates facilitating polymer;polymer materials variety;polymer materials;polymers;polymer scientists engineer;develop reversible;polymer scientists;structure property polymer;facilitating polymer scientists;polymers general;reversible", "pdf_keywords": ""}, "f16cf130ae75d1ea1ad3b926f605adef41af4af1": {"ta_keywords": "uncertainty propagates dynamic;uncertainty propagation dynamic;uncertainty propagates;study uncertainty propagates;uncertainty propagation;methods uncertainty propagation;existing methods uncertainty;methods uncertainty;uncertainty;method uncertainty propag;uncertainty propag;rigorous study uncertainty;based method uncertainty;method uncertainty;study uncertainty;propagation dynamic systems;propagates dynamic systems;propagation dynamic;propagates dynamic;dynamic systems;dynamic systems present;dynamic;propagation;conservation based;conservation based method;mathematically rigorous;structure universe;propagates;rigorous;conservation", "pdf_keywords": ""}, "b13e9d23983273c0c67b91ae70c55d4c3f745b8b": {"ta_keywords": "simultaneous translation agent;translationing real time;translation agent learns;neural machine translation;translation agent;machine translation nmt;simultaneous translation;conventional machine translation;machine translation;simultaneous translation outputs;translationing;introduction translationing real;machine translation methods;translationing real;translation words input;translation methods;time simultaneous translation;framework simultaneous translation;translation outputs translation;outputs translation words;translation nmt;translation nmt framework;introduction translationing;translation outputs;translation methods methods;outputs translation;translate interaction pre;pre trained nonmamoxemus;translate interaction;translation words", "pdf_keywords": "neural machine translation;simultaneous machine translation;learning algorithm translation;search translation segments;translation agent learns;translation jointly learning;machine translation;machine translation jointly;simultaneous translation agent;simultaneous translation challenging;maintaining good translation;translation non translational;machine translation trade;translation segments;simultaneous translation easier;translation algorithm effective;strategy simultaneous translation;greedy decoding translation;translation real time;simultaneous translation model;translation easier faster;machine translation present;improving quality translation;machine translation discuss;conventional machine translation;machine translation methods;simultaneous translation algorithm;model simultaneous translation;generate translations manner;translation words input"}, "ed1c17451a23471afde91c109ecadc6aab8b2ba6": {"ta_keywords": "multimodal disinformation detection;disinformation detection;multimodal disinformation;disinformation detection covering;art multimodal disinformation;disinformation online;news propaganda misinformation;propaganda misinformation disinformation;misinformation disinformation online;disinformation online initially;fake news propaganda;news propaganda;disinformation;misinformation disinformation;proliferation fake news;propaganda misinformation;multimodal;state art multimodal;art multimodal;textual content;propaganda;images videos gained;attention spread;images videos;attention spread simple;fake news;witnessed proliferation fake;online initially textual;content;proliferation fake", "pdf_keywords": "media deception detection;social media deception;disinformation detection;disinformation detection based;disinformation detection taking;multimodal disinformation detection;disinformation harmfulness detection;research multimodal disinformation;detection harmful content;disinformation message attractive;tackle disinformation detection;disinformation detection covering;media deception;news detection use;misleading harmful content;make disinformation message;fake news detection;deception detection use;factuality harmfulness disinformation;contents social media;modalities factuality harmfulness;misinformation detection article;disinformation harmfulness;misinformation detection;detection social media;harmful content social;multimodal disinformation;news detection social;disinformation message;media use textual"}, "8c838c7631a7408d5ea5801c9360213782665c9c": {"ta_keywords": "srrna mediated asrrna;asrrna mediated asrrna;mediated asrrna mediated;srrna mediated;asrrna mediated;case srrna mediated;mediated asrrna;asrrna mediated asr;srrna;case srrna;report case srrna;asrrna;mediated asr;asr;mediated;report case;report;case", "pdf_keywords": ""}, "db1dafd0c356491cbbf53338b9984de324e7239c": {"ta_keywords": "bilingual lexicon induction;validity bilingual lexicon;bilingual lexicon;lexicon induction semi;supervision non isometric;introduction bilingual lexicon;non isometric embedding;induction semi supervision;embedding spaces empirically;isometric embedding spaces;validity bilingual;isometric embedding;assumption isometry embedding;embedding spaces methods;evaluate validity bilingual;introduction bilingual;isometry embedding spaces;isometry embedding;semi supervision non;lexicon induction;bilingual;embedding spaces aim;embedding spaces;semi supervision;spaces empirically assumption;spaces empirically;embedding;spaces methods propose;non isometric;estimate assumption isometry", "pdf_keywords": "bilingual lexicons distribution;aligned bilingual lexicons;bilingual lexicon induction;bilingual dictionary acquisition;lexicons distribution matching;bilingual lexicon;bilingual lexicons;improve bilingual dictionary;bilingual dictionaries;development bilingual dictionaries;bilateral word embedding;bilingual dictionary;aligned bilingual;depended aligned bilingual;work bilingual lexicon;lexicon induction blendi;bilingual dictionaries major;lexicons distribution;translation space based;language embedding;language pairs distance;language embedding spaces;unsupervised distribution matching;embedding spaces empirically;machine translation bottleneck;lexicon induction;distant language pairs;check language embedding;improve bilingual;translation bottleneck distance"}, "9bb9b23823b45ba7521d872bb3e970ede4aafb8a": {"ta_keywords": "speech recognition powerful;recognize speech important;recognition speech;recognition speech recognition;speech recognition speech;speech recognition;speech important recognize;recognize speech;recognize recognize speech;ability recognize speech;recognize understand speech;speech recognition addition;speech important;tool speech recognition;understand speech;recognition powerful tool;recognition;recognition powerful;ability recognize recognize;speech;important recognize understand;tool speech;powerful tool speech;speech addition ability;ability recognize;understand speech addition;recognize recognize;recognition addition;recognition addition ability;important recognize", "pdf_keywords": ""}, "41675d91ad815f64b0df382c0944247811a62cc9": {"ta_keywords": "list bilingual phrases;bilingual phrases;phrases level granularity;learn phrase table;phrase table parallel;phrase table;machine translation;phrases granularities;bilingual phrases lies;introductionthe phrase table;phrases granularities included;table parallel corpus;machine translation systems;based machine translation;modeled phrases level;phrases level;phrase based machine;parallel corpus sentences;generally modeled phrases;list bilingual;method phrases granularities;translation systems;phrase table scored;modeled phrases;corpus sentences;parallel corpus;learn phrase;bilingual;directly learn phrase;translation systems methodswe", "pdf_keywords": ""}, "1fc6290fa3e6784501ca67cbef33a6a8edcbdb9e": {"ta_keywords": "convergence general measures;measures general compact;general measures;measures general;general measures general;asymptotic finite sample;general compact metric;measures exhibit;measures;measures exhibit radically;compact metric;convergence grows;spaces finite sample;general compact;sharp asymptotic finite;compact metric spaces;metric spaces finite;rates convergence grows;asymptotic finite;convergence general;rate convergence general;finite sample;sharp asymptotic;rate convergence;convergence;finite sample results;scale behavior measures;compact;results rate convergence;rates convergence", "pdf_keywords": "introductionthe wasserstein distance;wasserstein distance probability;wasserstein distance;distance probability measures;waterstein distance asymptotically;approaches wasserstein distance;convergence waterstein distances;wasserstein distance order;convergence waterstein distance;convergent waterstein distance;dimension measure rate;aymptotic dimension;probability measures metric;aymptotic dimension small;scales aymptotic dimension;intrinsic dimension measure;dimension measure significantly;measures waterstein distance;asymptotic dimension;asymptotic dimension small;inherent dimension measure;distance asymptotically optimal;sharper asymptotic dimension;bounds waterstein distance;quickly empirical measure;waterstein distances;smaller dimension metric;samples approaches wasserstein;waterstein distance fundamental;waterstein distances significantly"}, "f5b6819e05e087d2bbae3ddb6d58d2ab4e1d7ca2": {"ta_keywords": "inverse reinforcement;inverse reinforcement learning;uses inverse reinforcement;reward environment learn;demonstrations reinforcement learning;demonstrations reinforcement;reinforcement learning learn;learn maximize environmental;reward environment;maximize environmental rewards;maximize reward environment;constraints demonstrations reinforcement;reinforcement;reinforcement learning;environment learn;learning learn maximize;agents maximize reward;environmental rewards;ethical ethical values;environment learn follow;ethical ethical;ethical values;maximize reward;learn maximize;values important ethical;ethical;ethical values important;introduction ethical ethical;introduction ethical;maximize environmental", "pdf_keywords": ""}, "bfb13c6889626e833bf449fdb361d186467919af": {"ta_keywords": "feedback auxiliary annotations;feature feedback;feature feedback delivered;highlight salient evidence;exploiting feature feedback;annotations provided training;appeal feature feedback;salient evidence examples;instances highlight salient;salient evidence;salient spans text;annotations;feature feedback auxiliary;auxiliary annotations;auxiliary annotations provided;objects salient spans;objects salient;highlight salient;feature;salient spans;annotations provided;feedback delivered significant;feedback;boxes objects salient;iid holdout sets;holdout sets;salient;feedback delivered;holdout sets methods;intuitive appeal feature", "pdf_keywords": "incorporating feature feedback;feature feedback;feedback auxiliary annotations;exploiting feature feedback;feature feedback generalize;trained feature feedback;feature feedback does;feature feedback need;solicit feature feedback;feature feedback nonlack;rely feature feedback;tuneing feature feedback;benefit supplemental annotations;improve sentiment;analysis improve sentiment;feedback generalize better;feature feedback leads;supplemental annotations;characterize sentiment;sentiment analysis improve;feedback does improve;feedback generalize;supplemental annotations lessening;extraction sentiment;feature feedback auxiliary;improve sentiment analysis;data extraction sentiment;annotations provided training;domain evaluations inspired;extract characterize sentiment"}, "df9949abc06cb4f0f4c0ac1eb7ce0bc62ed5ec02": {"ta_keywords": "japanese text text;japanese text;pointwise approach japanese;japanese japanese text;sentence phoneme estimation;phoneme sequence estimation;word segmentation;word segmentation input;approach phoneme sequence;approach japanese japanese;approach japanese;phoneme estimation word;tasks word segmentation;sequence estimation sentences;input sentence phoneme;phoneme sequence;phoneme estimation;sentence phoneme;segmentation input sentence;estimation word tasks;text text;sequence estimation;pointwise classifiers referring;methods approach phoneme;text;pointwise classifiers;approach phoneme;japanese japanese;segmentation;solved pointwise classifiers", "pdf_keywords": ""}, "5331a846c854c3ecedf9ecf3ea516cb6dcaba4c8": {"ta_keywords": "saliency methods currently;saliency methods;saliency methods popular;introduction saliency methods;saliency;introduction saliency;adoption saliency methods;development adoption saliency;adoption saliency;synthetic evaluation framework;identifying important pixels;evaluation framework smerf;model predictive reasoning;feature attribution tools;input image development;feature attribution;synthetic evaluation;predictive reasoning;capture model predictive;predictive reasoning identifying;image development;feature;important pixels;attribution tools;model predictive;attribution tools aim;evaluation framework;image development adoption;input image;smerf allows", "pdf_keywords": "model reasoning saliency;reasoning saliency methods;reasoning saliency;based evaluation saliency;evaluation leading saliency;saliency methods able;evaluation saliency methods;saliency methods;saliency analysis;leading saliency methods;evaluation saliency;saliency methods effective;features saliency methods;saliency methods ability;features saliency;saliency;saliency methods controlling;effectiveness saliency methods;saliency methods popular;test saliency methods;saliency methods perform;leading saliency;effectiveness saliency;method saliency analysis;test saliency;method saliency;game evaluation saliency;saliency methods varying;irrelevant features saliency;saliency analysis masked"}, "807600ef43073cd9c59d4208ee710e90cf14efa8": {"ta_keywords": "neural information retrieval;benchmark information retrieval;retrieval models;retrieval models studied;information retrieval models;information retrieval;retrieval methods;information retrieval methods;retrieval methods leverage;retrieval;heterogeneous evaluation benchmark;evaluation benchmark information;benchmark information;existing neural information;evaluation benchmark;neural information;introduce benchmarking ber;benchmarking ber;insights distribution generalization;models introduce benchmarking;neural;benchmark;heterogeneous evaluation;benchmarking;introduce benchmarking;robust heterogeneous evaluation;existing neural;benchmarking ber robust;generalization capabilities;context existing neural", "pdf_keywords": "information retrieval benchmark;retrieval benchmark diverse;benchmark diverse retrieval;benchmark information retrieval;retrieval benchmark;neural information retrieval;retrieval data corpus;new retrieval datasets;18 retrieval datasets;diverse retrieval tasks;heterogeneous retrieval tasks;dense retrieval dataset;retrieval datasets comparison;performance heterogeneous retrieval;information retrieval recently;retrieval datasets;approach dense retrieval;models new retrieval;diverse retrieval;heterogeneous retrieval;retrieval dataset;retrieval datasets evaluating;text retrieval;broad range retrieval;different retrieval models;retrieval models;dense retrieval data;retrieval models evaluation;new information retrieval;information retrieval rt"}, "3a40cdd82f0706cda6c247e586d5054abeab4e1f": {"ta_keywords": "list question answering;answer list entities;improve question answering;question answering;answer list;question answering important;extended list;expected answer list;extended list including;expansion list;question answering expected;list including additional;produce extended list;list including;answering important tool;introductionautoset expansion list;list;list entities belonging;expansion list question;list entities;list question;se algorithms textual;seeds se algorithms;objectiveto improve question;algorithms textual resources;algorithms textual;se algorithms;answering;improve question;class objectiveto improve", "pdf_keywords": ""}, "2c3d02ce8780cc6648caf4ee996d9628c6388751": {"ta_keywords": "data labeling crowdsourcing;crowdsourcing data labeling;crowdsourced labels;noisy crowdsourced labels;crowdsourced labels methodsunder;labeling crowdsourcing;labeling crowdsourcing large;labels provided crowdsourcing;truth noisy crowdsourced;crowdsourcing data;increasing crowdsourcing data;noisy crowdsourced;minimax conditional entropy;increasing crowdsourcing;crowdsourcing;crowdsourcing large number;crowdsourcing large;provided crowdsourcing;crowdsourced;data labeling;rapidly increasing crowdsourcing;labeling;crowdsourcing workers;provided crowdsourcing workers;conditional entropy principle;conditional entropy;propose minimax conditional;labels methodsunder principle;low cost labels;crowdsourcing workers usually", "pdf_keywords": "data labeling crowdsourcing;crowdsourcing data labeling;crowdsourced labels;labeling crowdsourcing large;labels crowdsourcing;labeling crowdsourcing;crowdsourced labels principle;noisy labels crowdsourcing;noisy crowdsourced labels;labels provided crowdsourcing;labels crowdsourcing workers;truth noisy crowdsourced;minimax conditional entropy;increasing crowdsourcing data;crowdsourcing data;entropy observed labels;real crowdsourcing tasks;predicting probabilistic labels;predict crowdsourcing;probabilistic labels;crowdsourcing tasks;probabilistic labeling;data real crowdsourcing;probabilistic labels propose;method crowdsourcing;generating probabilistic labels;noisy crowdsourced;predict crowdsourcing crowdsourcing;real crowdsourcing;probabilistic labeling model"}, "ce5ff42d629e67a84731b3c62b57b47fc7f2b20d": {"ta_keywords": "transformer self attention;e2e automatic speech;transformer encoder introducing;self attention network;transformer encoder;attention network;recurrent neural networks;alternative recurrent neural;speech recognition;encoder introducing;method transformer encoder;automatic speech recognition;performance alternative recurrent;automatic speech;encoder introducing context;speech recognition ar;compute self attention;transformer self;encoder;recurrent neural;neural networks end;self attention methodswe;introductionthe transformer self;alternative recurrent;attention methodswe proposed;attention methodswe;attention network recently;neural networks;transformer;self attention", "pdf_keywords": "attention hybrid decoding;sequence decoder layer;decoding data attention;speech recognition neural;e2e automatic speech;beam search decoder;applications speech processing;performance alternative recurrent;networks automatic speech;algorithm streaming audio;speech recognition ar;attention network;alternative recurrent neural;feedforward network decoder;attention based attentionwe;speech recognition;recurrent neural networks;speech processing;self attention network;batch decoders;audio processing decoder;data attention based;batch decoders developed;speech recognition applications;sequence decoder;speech recognition english;streaming decoding;approach speech recognition;streaming decoder;automatic speech"}, "945d4addf8e94487f6199af71dc15a298791c1b4": {"ta_keywords": "opportunistic sampling energy;sampling energy harvesting;harvesting source opportunistically;minimization opportunistic sampling;opportunistic sampling;sampling energy;source opportunistically samples;energy harvesting;instant energy harvesting;opportunistically samples;energy harvesting source;methods energy harvesting;opportunistically samples multiple;information minimization opportunistic;harvesting source;minimization opportunistic;harvesting source considered;source opportunistically;time varying wireless;harvesting;sampling;information minimization;time instant energy;opportunistically;opportunistic;varying wireless;instant energy;samples multiple processes;wireless;age information minimization", "pdf_keywords": "markov decision process;observable markov decision;markov decision;information energy harvesting;processes markovian time;markovian fading;node optimal policy;threshold policy optimal;markovian time;state markovian fading;propose observable markov;stage markov decision;processes markovian;observable markov;harvesting source opportunistically;harvesting sensors efficiently;transmission state markovian;iteration stage markov;state derived optimal;optimal cost energy;multiple processes markovian;state markovian;action energy harvesting;simple threshold policies;energy harvesting process;markovian time varying;sensor derive optimal;management energy harvesting;asynchronous stochastic approximation;energy harvesting"}, "27df24c537b2d3c2a769d917adf92a6a059c5917": {"ta_keywords": "multiscale modeling effective;multiscale modeling;multiphysics systems largely;investigating multiphysics systems;multiphysics systems;multiscale;investigating multiphysics;model describes microscopic;approach investigating multiphysics;resolutions heterogeneous;models different resolutions;multiphysics;high fidelity model;different resolutions heterogeneous;resolutions heterogeneous descriptions;describes microscopic features;microscopic features refined;solver lower fidelity;size features models;homogeneous features expensive;microscopic features;discretization making overall;features refined discretization;coarse responsible simulating;modeling;refined discretization making;lower fidelity coarse;disparate size features;model describes;homogeneous features", "pdf_keywords": "multiscale modeling multiscale;integrating multiscale modeling;challenges multiscale modeling;multiscale modeling machine;multiscale modeling;multiscale modeling effective;multiscale modeling important;modeling multiscale;multiscale modeling approaches;modeling multiscale modeling;modeling multiscale simulations;integrate multiscale modeling;multiscale modeling mechanics;multiscale modeling simulation;multiscale simulations;multiscale modeling developing;multiscale coupling methods;framework multiscale modeling;applications multiscale modeling;multiscale coupling framework;coupling deeponet numerical;coupling deeponet computational;deeponet numerical model;coupling framework multiscale;idea multiscale modeling;multiscale simulations demonstrate;introduce multiscale coupling;deeponet computational model;development multiscale modeling;multiscale coupling"}, "46ef61536a01578e79b6d4e35e803a914afeb629": {"ta_keywords": "septic shock;history septic shock;septic shock aforementioned;atypical atypical patient;atypical patient;patient history septic;shock aforementioned atypical;septic;history septic;atypical atypical atypical;atypical patient history;new atypical atypical;atypical atypical;case new atypical;new atypical;shock;shock aforementioned;atypical;aforementioned atypical;patient;patient history;report case new;report case;case new;report;case;aforementioned;history;new", "pdf_keywords": ""}, "65ee083ce61576955d76b36819bf3ac271335597": {"ta_keywords": "reliability distributed storage;regenerating codes possessing;regenerating codes regenerating;regenerating codes;codes regenerating codes;exact regenerating codes;distributed storage systems;codes regenerating;distributed storage;storage systems minimizing;systems minimizing storage;storage systems;failed node codes;range distributed storage;minimizing storage;increase reliability distributed;storage overhead minimization;stored failed node;reliability distributed;node codes;storage systems used;minimizing storage overhead;node failure methods;storage;bandwidth required repair;regenerating;node codes require;increase reliability;introduce exact regenerating;exact regenerating", "pdf_keywords": "regenerating codes mrna;exact regenerating codes;regenerating codes possessing;regenerating codes mammalian;regenerating codes provide;codes mammalian recovery;regenerating codes fully;regenerating codes;codes regenerating codes;deterministic regenerating codes;regenerating codes regenerating;codes regenerating;regenerating codes introduced;regeneration regenerating code;method regenerating codes;regenerating codes mabr;dimension regenerating code;regenerating codes molecular;codes mrna;regenerating code;construction regenerating codes;code regenerating;codes mrna domain;regenerating code unique;point regenerating code;codes provide subspace;regenerating codes mr;codes mammalian;regenerating code molecular;unique completely regenerating"}, "092ee3a32b6cd951da971124a24872c7cccf3a9f": {"ta_keywords": "transductive transfer learning;transfer learning information;transfer learning labeled;transfer learning;unsupervised transductive transfer;problem transfer learning;transductive transfer;unsupervised transductive;inductive transductive approaches;learning labeled;learning labeled data;case unsupervised transductive;inductive transductive;transductive approaches adapt;learning information gained;transductive approaches;transductive;learning task;art inductive transductive;learning information;gained learning task;introductionthe problem transfer;learning task used;information gained learning;supervised;labeled data target;studied supervised version;transfer;domain available training;learning", "pdf_keywords": ""}, "fce10a1a9727cbda33d44b62409e303f1009417a": {"ta_keywords": "neural network grammars;modeling parsing;language modeling parsing;modeling parsing performance;language modeling;network grammars nng;network grammars;probablistic generative modeling;grammars nng;proposed probablistic generative;modeling composition crucial;generative modeling;grammars;probablistic generative;grammars nng recently;generative modeling family;art language modeling;modeling composition;generative;augmenting model attention;introductionrecurrent neural;model attention;model attention mechanism;explicit modeling composition;natural language state;introductionrecurrent neural network;learn linguistic;attention mechanism ga;rnng methodswe explicit;parsing", "pdf_keywords": "formalthe neural representation;neural network grammars;network grammars rnngs;modeling parsing;generative model nnn;formal formalthe neural;syntax recurrent neural;modeling parsing performance;language modeling parsing;language modeling;nnn neural representation;grammars rnngs;network grammars learn;processing phrases neural;network grammars useful;formalthe neural;neural representation;grammars rnngs recently;supervised formal formal;network grammars;modeling composition crucialthe;learning nnn neural;neural translation neural;nnn model using;translation neural;grammars learn syntax;neural representation learning;representation learning nnn;generative model uses;explicit modeling composition"}, "49af035c598901fbf766da2cfb040cca7336a8ac": {"ta_keywords": "transition based parsing;backgroundemantic parsers;parsers;parsing;based parsing;backgroundemantic parsers map;parsers map natural;similar dependency parse;dependency parse;dependency parse utilizes;parse;parse utilizes;map natural language;based parsing 7f1;example semantic formalism;parsers map;semantic formalism;natural language;parse utilizes graph;example semantic;semantic formalism similar;natural language statements;abstract syntactic;semantic;eliminate ambiguous interpretations;abstract syntactic phenomena;recent example semantic;syntactic;syntactic phenomena resolve;meaning representations abstract", "pdf_keywords": ""}, "d47ad0a606bedf41dcea614bfa7b7494879c7ba0": {"ta_keywords": "dataset tracking entities;domain procedural text;dataset tracking state;dataset tracking;present dataset tracking;tracking state changes;tracking entities;procedural text presented;changes procedural text;procedural text;tracking entities open;tracking state;open domain procedural;open vocabulary methods;procedural text arbitrary;background dataset tracking;domain procedural;open vocabulary example;text arbitrary domains;tracking;state changes procedural;unrestricted open vocabulary;changes procedural;vocabulary methods present;open vocabulary;vocabulary methods;text arbitrary;text;dataset;procedural", "pdf_keywords": "annotated dataset;annotated dataset oopenapi;predict state changes;tracking state changes;open vocabulary state;available annotated dataset;predicted open vocabulary;dataset open vocabulary;data open vocabulary;vocabulary state changes;vocabulary using crowdsourcing;dataset tracking state;dataset track entities;annotators;annotated using web;prediction state changes;available annotated;vocabulary state;friendly language model;trained language model;annotated;training dataset task;publicly available annotated;text understanding task;dataset user friendly;people annotated;predict predict state;help people annotated;task based dataset;predict state"}, "188928df74f9ce00bd1b58686db93ac8cdd07275": {"ta_keywords": "attention based encoder;encoder decoder network;decoder network;encoder decoder;right beam search;beam search;beam search algorithm;decoder;current beam search;encoder;based encoder decoder;decoder network uses;based encoder;background attention based;search algorithm inference;attention based;background attention;beam search expands;search expands hypotheses;attention;search algorithm;left right beam;parallelism technique;leads speed recognition;algorithm inference step;algorithm inference;hypotheses traverses;recognition;expands hypotheses traverses;right beam", "pdf_keywords": ""}, "2f369845ae7191196d65310210db2485feb3aa86": {"ta_keywords": "regularization weights grapheme;grapheme phoneme conversion;weights grapheme phoneme;conversion speech recognition;adaptive regularization weights;phoneme conversion speech;g2p conversion training;phoneme conversion;regularization weights narow;regularization weights;weights grapheme;adaptive regularization;introductionnarrow adaptive regularization;narrow adaptive regularization;grapheme phoneme;speech recognition;conversion training method;robust g2p conversion;regularization;speech recognition field;g2p conversion;new g2p conversion;conversion speech;weights narow online;g2p conversion increasing;conversion training;grapheme;online learning algorithm;weights narow;narow online learning", "pdf_keywords": ""}, "bd2f3822801a7e2f933d06c261b8783764d8ce18": {"ta_keywords": "adversarial samples text;distribution samples adversarial;samples adversarial advancing;samples adversarial samples;samples adversarial;adversarial samples;adversarial advancing samples;ood samples adversarial;samples text classification;text classification methods;hurting text classification;adversarial advancing;text classification;adversarial;text classification model;classification;classification methods;classification methods paper;classification model conduct;types anomalies;types anomalies oo;classification model;anomalies;anomalies oo;samples hurting text;samples text;compare types anomalies;distribution samples;statistically distribution ood;distribution ood samples", "pdf_keywords": "samples adversarial advancing;detection adversarial samples;adversarial advancing samples;distribution samples adversarial;adversarial samples;detect dversarial sample;samples adversarial;ood samples adversarial;adversarial samples using;dversarial sample;samples adversarial attacks;dversarial sample fundamental;distinguish detection adversarial;adversarial attacks datasets;datasets using adversarial;detect dversarial;detection adversarial;adversarial advancing;specific adversarial;adversarial;dversarial;adversarial samples modelwe;separate detection adversarial;specific adversarial attack;adversarial attack;algorithm detect dversarial;adversarial attacks;adversarial attacks important;robust text classification;using adversarial"}, "576860f910ea8fde366deb03c910ab30cd776966": {"ta_keywords": "patient history history;patient history;case patient history;history history;history history history;history;present case patient;case patient;article present case;purpose article present;article present;purpose article;article;patient;present case;present;case;purpose", "pdf_keywords": ""}, "95a35473fd1936927dd4a53fe0a5d2d6762d99b3": {"ta_keywords": "neural audio synthesis;audio synthesizers;audio synthesizers provide;hierarchical model musical;conventionalal audio synthesizers;audio synthesis;synthesizers provide detailed;synthesizers;neural audio;synthesizers provide;model musical instruments;audio synthesis concatenative;box neural audio;model musical;produce realistic audio;midii dp hierarchical;component musical;musical instruments;component musical practice;detailed expressive controls;realistic audio;realistic audio mechanisms;introductionmusic important component;introductionmusic;expressive controls;introduce midii dp;important component musical;introductionmusic important;midii dp;synthesis concatenative samplers", "pdf_keywords": "generative model musical;generate realistic audio;music synthesis neural;neural audio synthesis;audio predicting synthesis;autothe synthesis audio;audio synthesis;audio synthesis development;music synthesis;synthesis processing audio;neural music synthesis;generating audio;synthesis audio;microbial process musical;hierarchical model musical;developed generating audio;automatically generate audio;synthesis neural music;tool generating audio;audio synthesis detailed;generate audio note;audio synthesis using;differentiable audio synthesis;generation audio;generation audio note;realistic neural audio;generating audio present;hierarchical music modeling;generate audio;music modeling hierarchical"}, "55a3b36fd21dbbe9384ab3ba1bcf901235d95f47": {"ta_keywords": "introductionunsupervised question decomposition;question answering decomposing;decomposition question answering;question decomposition;improve question answering;questions cumbersome unsupervised;question answering;question decomposition question;capable answering labeling;labeling questions cumbersome;unsupervised sequenceence transduction;answering labeling questions;answering labeling;produce sub questions;unsupervised sequenceence;answering decomposing;answering decomposing hard;decomposing hard questions;algorithm unsupervised sequenceence;questions cumbersome;labeling questions;cumbersome unsupervised approach;simpler sub questions;questions simpler sub;systems capable answering;introductionunsupervised;sub questions;sub questions existing;sequenceence transduction;capable answering", "pdf_keywords": "question corpus decompositions;questions decompositions improve;question answering decomposing;simple question corpus;questions decompositions;question corpus;questions decomposition;questions decomposition effective;decompose questions generate;unsupervised sequenceence transduction;decompose questions datasets;capable answering labeling;improve question answering;question unsupervised decompositions;unsupervised sequence detection;question answering;web questions decomposition;reasoning unsupervised decompositions;decompose questions fully;friendly answer decomposition;questions cumbersome unsupervised;unsupervised sequenceence;answering decomposing hard;labeling questions cumbersome;decomposing hard questions;decompose questions;questions model decomposed;answering labeling;useful question inference;question corpuswe introduce"}, "e153713b0423b4bae325340b2211e704effd5252": {"ta_keywords": "inductive logic programming;fault prediction comparativative;study inductive logic;logic programming methods;hypotheses fault prediction;predicting fault density;known inductive logic;fault prediction;predicting fault;inductive logic;logic programming;algorithms known inductive;fault density classes;task predicting fault;abstract logical representation;directly abstract logical;learning algorithms known;programming methods;logical representation;study inductive;fault density;learning algorithms;prediction comparativative study;comparativative study inductive;prediction comparativative;programming methods methods;known inductive;class learning algorithms;abstract logical;inductive", "pdf_keywords": ""}, "e718ffe247a61a77b45953a7e8a5b86a45ed579f": {"ta_keywords": "introductiontraining dependency parsers;dependency parser trained;dependency parsers;dependency parser;partially annotated corpora;dependency parsers partially;parsers partially annotated;annotated corpora;annotated corpora introduce;mt dependency parser;annotated corpora allowing;parser trained;trained partially annotated;parser trained partially;partially annotated;dependency tree;parsers;introductiontraining dependency;linguistic resources reduction;parsers partially;parser;domain adaptation;available linguistic;corpora allowing effective;corpora introduce;corpora;use available linguistic;evaluate domain adaptation;linguistic resources;available linguistic resources", "pdf_keywords": ""}, "84e50df18c284b985d287b462c63c20186cc5da1": {"ta_keywords": "cognitive tutors programming;developing cognitive tutors;backgroundbuilding cognitive tutors;authoring cognitive tutors;cognitive tutors methodsthe;tutors programming demonstrationns;cognitive tutors;tool cognitive tutors;cognitive tutors objectivethe;cognitive tutors educators;tutors educators ai;tutors programming;programming demonstrationns important;programming demonstrationns;educators ai programmers;programming demonstration pad;tutors methodsthe pri;demonstration pad authoring;tutors methodsthe;educators ai;facilitate authoring cognitive;tutors;authoring tool cognitive;technique programming demonstration;incorporate technique programming;tutors objectivethe aim;ai programmers;authoring cognitive;tutors objectivethe;backgroundbuilding cognitive", "pdf_keywords": ""}, "efada589efdb0adf3aa9dc2b6cb6979a50658276": {"ta_keywords": "journalism project rumour;journalists estimation veracity;project rumour debunking;rumour debunking;rumour debunking methodsthe;debunking;rumoured claims;labelled journalists estimation;estimation veracity;300 rumoured claims;veracity true false;journalists estimation;journalism project;veracity;news articles collected;collected labelled journalists;veracity true;estimation veracity true;labelled journalists;project rumour;news articles;digital journalism project;digital journalism;journalism;associated news articles;contains 300 rumoured;journalists;derived digital journalism;rumour;debunking methodsthe data", "pdf_keywords": ""}, "3c6407554fb4ee599f42501cf5cba8fcefa88783": {"ta_keywords": "end speech recognition;automatic speech recognition;data speech;paired data speech;automatic speech;speech recognition models;end automatic speech;build automatic speech;speech recognition;speech utterances transcriptions;utterances transcriptions;data speech utterances;utterances transcriptions cycle;speech recognition systems;speech recognition aimto;consistency training end;introductioncycle consistency training;knowledge pronunciation dictionaryaries;consistency training;end speech;end end speech;speech utterances;expert knowledge pronunciation;knowledge pronunciation;training end end;training end;pronunciation dictionaryaries build;pronunciation dictionaryaries;transcriptions;recognition models", "pdf_keywords": "building automatic speech;text speech synthesis;automatic speech;speech recognition models;speech synthesis;years automatic speech;automatic speech recognition;approach text speech;text speech;speech recognition ar;end automatic speech;speech recognition;processing speech convolutional;speech synthesis employ;speech synthesis present;speech recognition systems;approach speech recognition;translation neural speech;speech convolutional;processing speech;speech recognition rapidly;speech convolutional neural;speech recognition understanding;approach processing speech;speech recognition development;neural speech recognition;text encoder model;model text encoder;new neural speech;neural speech"}, "b568c562fcfad8d7a943a9ea63aca36c487b6d7d": {"ta_keywords": "counterfactually augmented data;ence counterfactually augmented;counterfactually augmented;ence counterfactually;spurious associations;purpose ence counterfactually;spurious associations confounding;counterfactually;causality offers clarity;spurious patterns term;causality;indirect causal effects;confounding common cause;causal effects;spurious patterns;causal;called spurious patterns;systems called spurious;indirect causal;causality offers;spurious;confounding;language causality;language causality offers;frameworks language causality;cause direct indirect;called spurious;confounding common;direct indirect causal;associations confounding", "pdf_keywords": ""}, "afa9364ec48e38d19099cfc22ac9cb679c4baa39": {"ta_keywords": "biases interact multimodal;multimodal language models;intermodality associations biases;investigate multimodal language;text based bias;multimodal language;analyzed biases vision;biases vision pre;attention paid biases;biases vision;investigate multimodal;biases learned models;trained language models;multimodal;analyzed biases;methods investigate multimodal;biases interact;biases learned;studies analyzed biases;intermodality associations;pre trained language;bias analysis;bias;biases;intraand intermodality associations;bias analysis methods;language models analyzes;associations biases learned;language models;analyzes intraand intermodality", "pdf_keywords": "gender biases multimodal;biases multimodal language;bias visual linguistic;gender visual linguistic;biases multimodal;biases interact multimodal;predict gender visual;gender bias contextualized;multimodal language models;gender bias image;intermodality associations biases;visual linguistic models;gender bias entities;vision language multimodal;visual linguistic pretraining;investigate multimodal language;bias entities gender;individuals vision linguistic;language vision;visual linguistic pre;language multimodal;bias visual;biases vision pre;entities gender visual;behavior visual linguistic;language multimodal tasks;vision linguistic training;gender information language;analyzed biases vision;visual linguistic"}, "8b5071a38718194063cf17ca446ba8d9f4907a18": {"ta_keywords": "models natural language;sentiment analysis factoid;question answering tasks;question answering;natural language processing;learning compositionality inputs;learning compositionality;outperforms models sentiment;simple deep neural;deep neural;factoid question answering;models sentiment;models sentiment analysis;existing deep learning;focus learning compositionality;deep learning;deep learning models;natural language;language processing tasks;simple deep;learning models natural;answering tasks;deep neural network;sentiment analysis;introduction existing deep;answering tasks taking;model syntactically ignorant;neural;compositionality inputs;language processing", "pdf_keywords": ""}, "4240d8e1e5c2ef82d62ba9d7bb323c357c718c1c": {"ta_keywords": "persistent homology;persistent homology arbitrary;general persistent homology;based persistence landscapes;topological layer;persistence landscapes based;persistence landscapes;novel topological layer;landscapes based persistence;persistence landscapes work;topological layer general;models based persistence;inputs general persistent;general persistent;persistent;based persistence;persistence;play novel topological;novel topological;layer general deep;homology arbitrary filtration;landscapes work differentiability;homology arbitrary;topological;layer general;layer;homology;layer inputs;layer inputs general;landscapes based", "pdf_keywords": ""}, "1678eccf0f3895dbea6dfac44fc9d4f86de15ff6": {"ta_keywords": "emotion human interaction;human interaction emotion;conversation express emotion;interaction emotion;affective communication;emotion affected conversational;social affective communication;affective communication concerned;interaction emotion plays;express emotion affected;emotion human;express emotion;human conversation express;emotional triggers;human conversation;role emotion human;role human conversation;emotional triggers studying;emotion affected;increasing emotional triggers;affected conversational partner;human interaction;human computer interaction;emotion;role emotion;emotion plays way;affected conversational;conversational partner;emotion plays;study social affective", "pdf_keywords": ""}, "751816df0027c0ae6c337ba392a5447bef86ca77": {"ta_keywords": "training graph neural;pre training graph;conjugated oligomers polymers;oligomers methods training;transfer learning;training graph;oligomers polymers;graph neural networks;use transfer learning;graph neural;short oligomers methods;oligomers polymers remains;chemistry modeling optoelectronic;data short oligomers;learning techniques chemistry;long conjugated oligomers;oligomers methods;conjugated oligomers;transfer learning address;polymers;chemistry modeling;neural networks;polymers remains challenging;networks;networks using data;learning;training data able;training data;optoelectronic properties long;polymers remains", "pdf_keywords": ""}, "a4577911d247e472772e2101d21aeaf8f46053cc": {"ta_keywords": "challenges semantic parsing;semantic parsing;parsing;parsing ambiguous;parsing ambiguous input;semantic parsing sp;parsing given natural;problem parsing;parsing given;multi synchronous grammars;problem parsing given;parsing sp;focus parsing ambiguous;synchronous grammars;input nl applications;focus parsing;parsing sp cases;given natural language;input nl;natural language;challenges semantic;natural language sentence;grammars;nl applications underspecified;ambiguous input using;major challenges semantic;paper focus parsing;ambiguous input;nl applications;semantic", "pdf_keywords": ""}, "fdb3969b654ab01be1807bbf84707a80e6283a52": {"ta_keywords": "learning syntheses inorganic;learning syntheses;computational synthesis;supervised learning syntheses;computational synthesis planning;syntheses inorganic materials;syntheses inorganic;synthesis procedures;synthesis procedures readily;synthesis planning;synthesis;chemistry tabulated synthesis;synthesis planning approaches;articles aimthe syntheses;tabulated synthesis procedures;organic chemistry;syntheses;tabulated synthesis;introductionthe computational synthesis;aimthe syntheses inorganic;organic chemistry tabulated;aimthe syntheses;success organic chemistry;inorganic;inorganic materials;natural language narratives;chemistry;computational;natural language;inorganic materials primarily", "pdf_keywords": "synthesis text;synthesis text action;structured representations synthesis;action graphs synthesis;articles synthesis information;synthesis information extracted;synthesis information;extracting structured representations;synthesis procedures texts;action graph extraction;synthesis text accurate;representations synthesis procedures;extracting action graphs;approaches chemical synthesis;articles synthesis;chemical synthesis planning;structured synthesis pathways;syntheses inorganic materials;models extracting action;natural language narratives;representations synthesis;journal articles synthesis;structured synthesis;chemical synthesis;automatically extracting structured;syntheses inorganic;entity extraction;highly structured synthesis;syntheses inorganic compounds;backgroundthe syntheses inorganic"}, "19f727b7a42a21bc3f99536e8368029f4b9b8e14": {"ta_keywords": "mapping noun images;noun images;noun images annotated;nouns natural languages;language represents things;language represents;senses foreign language;foreign language lexical;languages meanings;natural languages meanings;languages meanings methods;language lexical;mapping noun;connect nouns natural;natural languages;word senses foreign;connect nouns;nouns;study mapping noun;language lexical resource;art photography;usage bilingual;lexical resource;nouns natural;images annotated;languages;foreign language;lexical resource usage;art photography drawing;collection word senses", "pdf_keywords": ""}, "97ef5081aa4e2984c16ea78b862266e4852c7faf": {"ta_keywords": "email consider activity;task email tagging;email tagging folders;task identify email;email important task;important task email;search email important;activity centred search;task email consider;email tagging;search email;evaluate task email;centred search email;tasks predicting;activity centered tasks;tasks email including;novel tasks predicting;tasks email;centered tasks email;task email;activity represented folder;tasks predicting future;folder novel task;email messages related;identify email messages;novel task identify;context activity centred;email messages;email including novel;consider activity centered", "pdf_keywords": ""}, "af034b0e893a0a24e41cdb54afb35d4250407f50": {"ta_keywords": "speech communication field;speech communication;state speech communication;communication field speech;field speech communication;current state speech;speech;field speech;state speech;communication field;communication;article provide overview;overview;overview current state;provide overview;current state;purpose article provide;article provide;state;article;provide overview current;purpose article;field;overview current;provide;purpose;current", "pdf_keywords": ""}, "7657b56d2ac9269b32e8bcbe2a20f99ea17afe09": {"ta_keywords": "perceived age singing;age singing voice;age singing;statistical voice timbre;controlling voice timbre;voice timbre control;voice characteristics singing;singing voice intuitively;characteristics singing voice;measures voice characteristics;statistical voice;controlling voice;control perceived age;voice timbre singers;proposed statistical voice;voice timbre;perceived age methodsthis;varieties voice timbre;voice characteristics;voice timbre extent;characteristics singing;singing voice;expressively controlling voice;based perceived age;voice intuitively;sing expressively controlling;singing voice singers;voice singers sing;measures voice;perceived age", "pdf_keywords": ""}, "869d53277b0ec5e47a30b874aeb157df88649ea0": {"ta_keywords": "diagnosis pulmonary infection;pulmonary infection;patients diagnosis pulmonary;diagnosis pulmonary;pulmonary;patients diagnosis;infection;management patients diagnosis;diagnosis;patients;management patients;approach management patients;approach management;new approach management;article discuss importance;importance new approach;management;new approach;article discuss;article;approach;aim article discuss;discuss importance new;aim article;discuss importance;new;importance new;importance;discuss;aim", "pdf_keywords": ""}, "bff4d630cbea6a90b149b28caff5489c1a4ccaad": {"ta_keywords": "structurally tagged corpus;corpus structurally tagged;tagged corpus structurally;structurally branded corpus;corpus structurally;tagged corpus;corpus;language model estimates;structurally tagged;branded corpus;branded corpus useful;language models;results structurally tagged;corpus useful analysis;analysis language models;language models report;corpus useful;language model;model estimates structurally;estimates structurally branded;structurally;estimates structurally;results structurally;language;analysis language;structurally branded;tagged;useful analysis language;report results structurally;models", "pdf_keywords": ""}, "7e0342b304ca8ce564a664eb17e85358b07488fe": {"ta_keywords": "speaker adaptation noise;joint speaker adaptation;speaker adaptation technique;adaptation noise mixture;speaker adaptation;result speaker adaptation;unsupervised joint speaker;noise suppression unsupervised;based noise suppression;noise mixture model;noise suppression including;noise suppression;suppression result speaker;noise suppression deal;variation speaker characteristics;approach variation speaker;noise mixture;model based noise;adaptation noise;noise suppression result;variation speaker;noise model crucial;accurate noise model;speaker characteristics crucial;joint speaker;estimation accurate noise;noise model;mixture model estimation;based noise;speaker characteristics", "pdf_keywords": ""}, "fcdac45272543b4f8b8eaa59d66044d1b7018494": {"ta_keywords": "neural machine translation;translation generating pseudo;machine translation generating;final translation models;translation models;translation quality pseudo;translation generating;translation models lower;machine translation;better translation quality;translation quality;background translation effective;translation effective strategy;pseudo parallel data;generating pseudo parallel;generate pseudo parallel;translation effective;parallel data pre;background translation;improve performance neural;performance neural machine;pseudo parallel;parallel data;quality pseudo parallel;performance neural;parallel data does;neural machine;parallel data recent;generating pseudo;generate pseudo", "pdf_keywords": "translation optimization forward;final translation models;neural machine translation;translation models;machine translation generating;translation models demonstrate;metaback translation outperforms;translation optimization;translation model generate;translation generating pseudo;translation models lower;translation generating;translation model;backgroundback translation effective;machine translation;machine translation results;backgroundback translation;multilingual translation models;step translation optimization;translation outperforms;experiments metaback translation;translation outperforms strong;forward model training;metaback translation;machine translation important;machine translation article;probabilistic framework translation;translation quality pseudo;forward model learn;learns adjust translation"}, "e8f42dd98d7f546036fa4a1109c3fe3dd98f9647": {"ta_keywords": "argument reasoning comprehension;natural language argumentation;presupposed argument comprehension;argument comprehension;argument comprehension requires;argument reasoning;analyze reasoning arguments;reasoning arguments highly;reasoning arguments;argumentation order comprehend;comprehend argument;language argumentation;comprehend argument reconstruct;arguments highly contextualized;reasoning comprehension;argumentation;reasoning related content;contextualized reasoning;contextualized reasoning related;highly contextualized reasoning;order comprehend argument;task argument reasoning;analyze reasoning;argument reconstruct;introduction reasoning;introduction reasoning crucial;language understanding logic;arguments;reasoning related;comprehension requires language", "pdf_keywords": ""}, "4358335263622fe189cf95c613f4d6fdcb67fbea": {"ta_keywords": "detailed description literature;literature topic;description literature topic;description literature;article discuss importance;literature;aim article discuss;discuss importance detailed;article discuss;importance detailed description;importance detailed;aim article;discuss importance;article;detailed description;importance;topic;description;detailed;discuss;aim", "pdf_keywords": ""}, "03fff40cff6ac531e340f6ffb376e34609770846": {"ta_keywords": "coordinated twitter accounts;networks coordinated twitter;twitter accounts examining;coordinated twitter;coordinated networks social;twitter accounts;accounts likely coordinated;networks social media;detect networks coordinated;networks social;networks coordinated;twitter;coordinated networks;traces shared accounts;influence campaigns;influence campaigns diverse;coordination networks;identities images hashtag;identify groups accounts;coordination networks based;detect networks;social media;likely coordinated methodsthe;images hashtag;groups accounts likely;cases detect networks;construct coordination networks;likely coordinated;networks based;networks", "pdf_keywords": "detect coordinated campaigns;coordination social media;coordinated spam campaigns;coordination twitter use;coordination twitter;campaigns twitter coordinated;hashtags disinformation campaigns;coordinated campaigns social;background coordinated campaigns;capture suspicious behaviors;detect coordination social;suspicious groups twitter;twitter coordinated spam;suspicious coordination tweeted;manipulate social media;coordination detection approach;coordinated campaigns used;identify coordinated accounts;coordination detection;twitter coordinated;coordinated campaigns;approach detecting coordinated;suspicious coordination;coordinated spam;detecting coordinated;accounts likely coordinated;accounts social media;suspicious clusters accounts;actions social media;hashtags synchronization"}, "eb07ff030df4c3dc20e85d89c2e0d0cc730918a0": {"ta_keywords": "separation using speaker;speech separation using;speech separation;known speech separation;speech separation received;facilitate speech separation;extracting target speech;separating participating speakers;target speaker snippet;speaker inventory susi;speaker information facilitate;using speaker inventory;additional speaker information;speaker information;leveraging additional speaker;speaker snippet jointly;speaker inventory;conversations speech signal;speaker snippet;target speech using;multi talker conversations;using target speaker;speech using target;separation using;talker conversations speech;target speech;target speaker;speech signal;speaker signals;speech signal contains", "pdf_keywords": "recording speech separation;speech separation task;continuous speech separation;separation using speaker;speech separation using;known speech separation;speech separation;speaker clustering;using speaker clustering;speech separation received;facilitate speech separation;speech separation susi;speaker clustering methods;separating participating speakers;recording speech;segment mixture recording;source separation segment;multi talker recordings;constructs speaker inventory;extracting target speech;introduce continuous speech;speaker inventory mixed;using speaker inventory;long recording speech;speaker inventory;automatic speech processing;speech speech processing;conversations speech signal;automatic speech;approach processing speech"}, "4c66979a31fa4be5b5814fdb5cb8411572d61da8": {"ta_keywords": "normalization historical texts;based normalization historical;normalization historical;normalization language data;normalization language;introductionrule based normalization;deals normalization language;based normalization;modern wordforms rules;paper deals normalization;historical wordforms;normalization;historical wordforms modern;maps historical wordforms;deals normalization;modern language methods;historical texts;modern wordforms;modern language;historical texts important;wordforms rules;wordforms modern wordforms;german methodswe unsupervised;unsupervised rulebased approach;language data;wordforms modern;high german methodswe;wordforms rules specified;language methods paper;versions luther bible", "pdf_keywords": ""}, "729260566c7fdf689bb04eaaecef59d40da93ef7": {"ta_keywords": "adversarial images important;adversarial images;detection adversarial images;dnn malicious deception;adversarial;detection adversarial;backgroundefefficient detection adversarial;malicious deception attacks;deception attacks;deception attacks attacks;network dnn malicious;vulnerability deep neural;dnn malicious;malicious deception;attacks pixel;attacks attacks pixel;attacks pixel values;dnn based classifier;neural network dnn;significant dnn based;vulnerability deep;dnn based;external attacker;deep neural;network dnn;deep neural network;modified external attacker;attacker;external attacker change;shown vulnerability deep", "pdf_keywords": "mitigation adversarial attack;adversarial attack images;adversarial attack;adversarial attack detection;algorithm adversarial attack;detection adversarial;mitigation adversarial;adversarial image;adversarial;attack deep neural;adversarial images;propose adversarial image;called adversarial;adversarial images demonstrate;detection adversarial images;effective detection adversarial;algorithm called adversarial;propose adversarial;classifies adversarial;adversarial images using;injected adversarial;adversarial image classification;algorithm adversarial;test adversarial image;novel algorithm adversarial;adversarial non;detection deception attack;adversarial perturbation;schemes detection adversarial;algorithm detection adversarial"}, "ede8ba65c4db10d357d9c3bf8e75b092f536fc84": {"ta_keywords": "clinical message navigation;natural language instructions;guided natural language;message navigation guided;natural language;learning reasoning;perceptual context machine;learning reasoning process;enable learning reasoning;reasoning problem instruction;followers natural language;instruction followers;key clinical message;message navigation;instruction followers natural;context machine;context machine learning;language instructions presents;instructions presents challenging;clinical message;navigation guided natural;annotated data;instruction;navigation guided;language instructions typically;problem instruction followers;perceptual context;language instructions;problem instruction;based perceptual context", "pdf_keywords": "vision language navigation;visual language navigation;challenge vision language;vision language;approach vision language;natural language instruction;submission vision language;visual language;model vision language;language motor navigation;natural language instructions;language navigation incorporating;instruction follower;navigation behaviors pragmatic;human generated instructions;inference crucial navigation;language navigation task;language navigation dataset;augmentation implement pragmatic;combines instruction follower;method visual language;instructions trained human;incorporating visually grounded;speaker annotated routes;instructions trained model;navigation incorporating visually;language navigation challenge;inference helps instruction;r2r vision language;language instruction"}, "d5634a21b3727258822b78f5c5ababf7261a5c79": {"ta_keywords": "speech enhancement separation;processing speech enhancement;methods speech enhancement;noise speech separation;speech enhancement;speech enhancement suppresses;background speech enhancement;speech separation extracts;speech separation;enhancement separation downstream;enhancement separation methods;robust speech processing;based enhancement separation;enhancement separation;experimental results voicebank;enhancement separation fundamental;speech interfering;voicebank;target speech interfering;robust speech;speech interfering speakers;speech processing;speech processing speech;learning based enhancement;tasks robust speech;processing speech;separation downstream;results voicebank;extracts target speech;background noise speech", "pdf_keywords": "separation speech enhancement;speech enhancement separation;enhancement separation speech;processing speech enhancement;approach speech separation;tool speech enhancement;noise speech separation;models speech enhancement;methods speech separation;speech separation extracts;speech separation separation;speech separation;background speech enhancement;speech enhancement;methods speech enhancement;performance speech enhancement;model extract speech;speech enhancement suppresses;speech separation important;ssl models speech;audio speech processing;automated speech processing;robust speech processing;extract speech;automated speech;improved speech performance;toolkit audio speech;extract speech representations;speech processing;speech separation article"}, "3813627f7fec57aa4c15b791e36912f470273bb1": {"ta_keywords": "twitter clustering hashtags;clusters twitter hashtag;topics twitter clustering;twitter clustering;discussion topics twitter;clustering hashtags;clusters twitter;topics twitter;clustering hashtags order;twitter hashtag;quality clusters twitter;analysis social media;global epidemic pandemics;social media data;analyzing discussion topics;epidemic pandemics;hashtags;twitter;backgroundthe global epidemic;epidemic pandemics produced;discussion topics topics;sites analysis social;global epidemic;activity social media;discussion topics;epidemic;hashtag;social media;hashtags order;insights discussion topics", "pdf_keywords": ""}, "3e8420d1bebb3f93d285da2de801d2e43b290880": {"ta_keywords": "backgroundneuro sequence labeling;sequence labeling important;sequence labeling;backgroundneuro sequence;large scale labeled;trained language models;human annotation;cost human annotation;pre trained language;labeled data large;task specific labeled;language processing np;labeled datasets difficult;trained language;backgroundneuro;labeled datasets;labeling;natural language processing;labeled data;specific labeled;labeling important;human annotation privacy;labeling important technique;language processing;specific labeled data;tasks large scale;labeled;np tasks large;annotation;employed natural language", "pdf_keywords": "neural sequence labeling;sequence labeling tasks;neural sequence taggers;sequence labeling datasets;pseudo annotate task;supervised sequence labeling;sequence labeling benchmark;sequence labeling models;annotate task;sequence labeling;sequence labeling methods;training neural sequence;sequence taggers;labeling tasks;labeling benchmark datasets;perform sequence labeling;annotate task specific;pseudo annotate;semi supervised sequence;self training framework;training labels;labeling benchmark;labeling tasks required;training meta learning;user training labels;training labels improved;challenge neural sequence;meta learning sequences;tagging using trained;labeling models specifically"}, "e51bca890c004c43b25c5a5e7aa968fe70ec2668": {"ta_keywords": "stacked graphical learning;meta learning scheme;relational datasets hyperlinked;stacked graphical model;graphical learning;meta learning;relational datasets;introductiontraditional machine learning;relation template stacked;datasets hyperlinked web;graphical models;machine learning methods;relational data thesis;learning methods;learning scheme;learning given relation;machine learning;relational data;learning scheme called;datasets hyperlinked;study meta learning;graphical learning given;stacked graphical;graphical models demonstrated;reality relational datasets;citations social networks;relation template;work graphical models;improvement relational data;graphical model augments", "pdf_keywords": ""}, "622e05f5d3dd430644288d5048f6050f37947de7": {"ta_keywords": "transfer learning named;domain adaptation;learning named entity;transfer learning information;supervised transfer learning;hierarchy transfer learning;transfer learning;named entity recognition;domain adaptation model;problem transfer learning;subproblem domain adaptation;feature hierarchy transfer;supervised transfer;entity recognition;structure supervised transfer;entity recognition methodswe;trained source domain;learning named;named entity;entity recognition motivated;hierarchy transfer;adaptation model trained;entity;learning information;natural language data;source domain;task natural language;prior structure supervised;learning information gained;transfer", "pdf_keywords": ""}, "146b84bdd9b9078f40a2df9b7ded26416771f740": {"ta_keywords": "risk sensitive reinforcement;sensitive reinforcement learning;reinforcement learning algorithm;agent risk sensitive;introductioninverse risk sensitive;reinforcement learning;agent risk;risk;reinforcement learning important;markov decision processes;risk sensitive;risk metrics;making agent risk;particular risk sensitive;introductioninverse risk;risk metrics models;risk sensitive particular;coherent risk metrics;models human decision;sensitive particular risk;markov decision;sensitive reinforcement;decision making agent;use coherent risk;human decision making;particular risk;coherent risk;reinforcement;problem markov decision;learning algorithm convergence", "pdf_keywords": ""}, "77899bac8f463b7a77c0c282748e989d419386e7": {"ta_keywords": "semi supervised learning;introduction semi supervised;semi supervised;learning agreement constraints;regularization constraints learners;supervised learning declaratively;strategies semi supervised;constraints entropic regularization;ensembles semi supervised;supervised learning agreement;entropic regularization constraints;agreement constraints entropic;learning declaratively specified;endtropy constraints challenging;regularization constraints;specified endtropy constraints;supervised learning sl;endtropy constraints;constraints learners;supervised learning;entropic regularization;constraints learners used;declaratively specified endtropy;learning declaratively;learning agreement;specify ensembles semi;supervised;regularization;agreement constraints;ensembles semi", "pdf_keywords": "semi supervised learning;semi supervised;supervised learning document;strategies semi supervised;learning document classifiers;specifying semisupervised learners;semisupervised learners based;semisupervised learners;ensembles semisupervised learners;document classifiers;algorithm text categorization;semisupervised learners agreement;regularization constraints learners;data classification supervised;classification supervised;text categorization;classification supervised logistic;information retrieval;supervised learning sl;supervised learning;ensembles semisupervised;classification hyperlinked text;regularization training;rules data classification;formal language classifier;document classifiers method;specify ensembles semisupervised;language classifier;rules network classifiers;regularizations training heuristics"}, "1a53e7446274016f737236bdd48e3ff05d966384": {"ta_keywords": "code summarization data;code natural language;learning aligned code;grained alignments stackedoverflow;natural language code;code summarization;alignments stackedoverflow promising;language code retrieval;code summarization code;summarization code summarization;retrieval code summarization;code synthesis natural;like code synthesis;code synthesis;data natural language;code retrieval;synthesis natural language;fine grained alignments;summarization code;language pairs stacked;grained alignments;alignments stackedoverflow;code retrieval code;stackedoverflow promising source;summarization data driven;language code;aligned code natural;natural language pairs;stackedoverflow promising;aligned code", "pdf_keywords": "mining nl code;code natural language;natural language code;extraction nl code;language processing tools;mining aligned nl;language query experiments;data natural language;nl code pairs;accurate nl code;natural language pairs;aligned nl code;language naturalness software;exhaustive extraction nl;method mining nl;data nl code;natural language processing;extracting aligned nl;language processing;natural language queries;natural language query;application natural language;nl code snippet;language code snippets;natural language models;mining code natural;mining nl;labeled nl code;parallel data nl;snippets correspond semantically"}, "d0e9c5cb669dec908a38eab4315cbf101bc4b0a0": {"ta_keywords": "statistical machine translation;domain adaptation statistical;machine translation;domain adaptation;machine translation idea;approach domain adaptation;language models trained;language models data;ngram language models;domain text select;adaptation statistical machine;general domain corpora;neural language models;language models;select similar sentences;use language models;adaptation statistical;domain corpora;small domain text;introductiondata selection effective;language models explore;domain text;introductiondata selection;models data selection;trained small domain;translation idea;text select similar;translation idea use;ngram language;similar sentences large", "pdf_keywords": ""}, "308eb6751a3a1da0f64f291366c8ee27f84b3f16": {"ta_keywords": "new model learning;model learning;model learning problem;models;learn order based;models useful tool;learn second dimensional;model new model;second dimensional models;dimensional models;models fundamental;models useful;based models;dimensional models fundamental;development new model;order based models;model;model new;models fundamental step;based models useful;learning problem;ability learn order;learn order;learning;new class software;class software;new model;learn second;ability learn second;new model new", "pdf_keywords": ""}, "2550fafc0cbd8bbf7aadd864ac569596d33db038": {"ta_keywords": "grounding facilitate interaction;grounding facilitate;non textual language;formally defines grounding;facilitate interaction language;textual language;defines grounding;interaction language technologies;language technologies;recent grounding facilitate;interaction language;textual language community;non textual;non textual modality;textual;language technologies world;data non textual;textual modality;communication interlocutors;cognitive science formally;defines grounding process;grounding;communication interlocutors definition;language;successful communication interlocutors;communication;facilitate interaction;language community;linking text data;language community seen", "pdf_keywords": "grounding natural language;grounding conversational language;grounding various contexts;dynamic grounding conversational;grounding conversational;facilitate interaction language;grounded language;grounding concepts relation;definitions grounding cognitive;interaction language;language understand interacts;generating grounded language;grounding concepts;human language fundamental;grounding context;discuss use linguistic;grounding cognitive;interaction language technologies;status grounded language;approach grounding concepts;non textual language;textual language;language research;grounded language learning;conversational language;discuss aspects grounding;grounding context complex;human language;concept knowledge sources;concept knowledge"}, "7ce80c7df1774e4483b32a813d54a8ff35dd0163": {"ta_keywords": "dynamics stakeberg games;learning dynamics stakeberg;stakeberg games;stakeberg games investigated;dynamics stakeberg;nash staackelberg equilibrium;learning dynamics;hierarchical game;convergence learning dynamics;consider hierarchical game;hierarchical game played;games consider hierarchical;gradient descent staackelberg;staackelberg equilibrium concepts;descent staackelberg equil;staackelberg equilibrium;investigated class games;simultaneous gradient descent;connections nash staackelberg;stakeberg;attracting critical points;equilibrium concepts;equilibrium concepts characterize;consider class games;class games;follower continuous action;descent staackelberg;games consider class;connections nash;convergence learning", "pdf_keywords": "games learning dynamics;sum games learning;learning dynamics;simultaneous gradient play;learning dynamics static;learning dynamics compared;multi agent learning;sum games dynamics;learning dynamics effectively;gradient play dynamics;gradient play simultaneous;play simultaneous gradient;stable non nash;games dynamics;convergence learning dynamics;games nash equilibrium;agent learning progress;stable differential nash;learning dynamics easily;games differential equilibria;learning dynamics model;differential nash equilibria;zero sum games;games dynamics dynamics;learning hierarchical games;learning model game;games learning;hierarchical game equilibria;agent learning;demonstrate learning dynamics"}, "203636315f7c9526189d88c541bedf623d63ea7c": {"ta_keywords": "factoid question answering;question answering;question answering progress;factoid question;factoid;progress factoid;strong progress factoid;answering progress easily;progress factoid question;answer questions require;answering;answer questions;answering progress;questions require depth;goal answer questions;reliable evaluation;reliable evaluation metrics;questions;evaluation metrics;evaluation metrics resulted;evaluation;quality data;questions require;form goal answer;explanations methodsthe hurdles;task long;explanations methodsthe;task long form;goal answer;quality data absence", "pdf_keywords": "factoid question answering;annotating question answering;question answering;approach question answering;answering method annotating;answers ambiguous factoid;question answering progress;question answering development;question answering method;developments question answering;question answering question;question answering form;question answering qq;answering question answering;human generated answers;answer ambiguous factoid;automated metrics answers;question answering demonstrate;ambiguous factoid questions;evaluating quality answered;method annotating questions;factoid questions challenging;quality answered answers;question answering open;form question answering;annotating questions;generated answers;retrieval aspects automated;ambiguous factoid;factoid questions important"}, "018bf5da2ba1f1901e98f72c7eedbf6b91967192": {"ta_keywords": "differential privacy bert;privacy bert;privacy bert fine;embeddings leak private;privacy variant;differential privacy;applying privacy variant;privacy preservation;effective privacy preservation;privacy preservation approaches;privacy variant local;privacy utility;privacy preservation remains;text embeddings leak;privacy utility implications;privacy;effective privacy;leak private information;embeddings leak;local differential privacy;background privacy preservation;private information;applying privacy;research effective privacy;investigate privacy utility;implications applying privacy;private information concern;leak private;investigate privacy;background privacy", "pdf_keywords": "privacy preserving nonludimentary;differential privacy bert;representation privacy preservation;privacy preserving nonlivory;privacy preserving;specific privacy preserving;differential privacy;text privatization privacy;privacy constrained training;effective privacy preservation;privacy preservation;text representation privacy;present privacy preserving;pretraining algorithms privacy;algorithms privacy adaptive;privacy adaptation privacy;privacy preserving mechanism;propose privacy adaptive;adaptation privacy;privacy adaptation;privacy bert;privacy adapted;agnostic specific privacy;adaptation privacy adaptive;privacy bert finetunthe;representation privacy;privatization privacy;privacy variant;privacy utility;algorithm privacy"}, "a6a7374c5ddac1446ceab9d7cbe5a3305238d0ee": {"ta_keywords": "conversation dialog corpora;conversation scripts television;dialog corpora television;conversation scripts;conversation corpora;human human conversation;dialog corpora;human conversation;utilize conversation scripts;conversation corpora created;human conversation work;conversation dialog;portray actual conversations;conversation processing;work conversation corpora;tools conversation processing;utilize conversation;conversation processing previous;conversations;actual conversations;television movie scripts;conversation work tedious;introduction conversation dialog;corpora television;transcribe real human;conversation work;conversation;scripts television movies;conversations people way;conversations people", "pdf_keywords": ""}, "963c4c34f1292f64a6e9fc04428fc7a0893b8ef3": {"ta_keywords": "image super resolution;single image super;super resolution sir;cnn based sir;image super;super resolution;resolution sir long;resolution sir significant;resolution sir;achieved existing cnn;existing cnn based;convolutional neural;single image;existing cnn;low quality images;cnn based;task single image;resolution;quality images neglecting;low level image;convolutional neural networks;quality images;level image processing;image processing task;cnn;image processing;convolutional;years convolutional neural;recent years convolutional;used single image", "pdf_keywords": ""}, "53feb3b34425ea95c259e8d0693edd490d6b470f": {"ta_keywords": "processing mistaken words;auditory stimuli ja;mismatch feelings semantically;semantically mistaken words;auditory analysis mismatch;auditory stimuli version;words visual auditory;sentences measuring auditory;stimuli auditory;mistaken words visual;stimuli auditory auditory;auditory stimuli auditory;mistaken words ja;auditory stimuli;feelings semantically mistaken;auditory auditory stimuli;visual auditory stimuli;results auditory;results auditory auditory;used results auditory;mistaken words;semantic processing mistaken;stimuli ja;stimuli ja methods;auditory auditory auditory;auditory auditory;auditory;visual auditory;analysis mismatch feelings;auditory analysis", "pdf_keywords": ""}, "a792d5a1e9a6a53edd8cbc00e387bc07c54e423c": {"ta_keywords": "bayesian truth serum;simple truth serums;truthful responses agents;eliciting truthful responses;truth serums;truth serums massively;method bayesian truth;truth serum;bayesian truth;problem eliciting truthful;truth serum quite;eliciting truthful;peer prediction method;peer prediction;crowddsourced evaluation tasks;responses agents;pioneered peer prediction;massively crowddsourced evaluation;known answers methodsthis;truthful responses;crowddsourced evaluation;prediction method bayesian;method bayesian;answers methodsthis;bayesian;serums massively crowddsourced;prediction method;responses agents absence;truth;introductiona simple truth", "pdf_keywords": ""}, "10efdde1ae3a9d359ac1aae0bd5ef7bfd68810dd": {"ta_keywords": "multilingual language models;introductionpre trained multilingual;language modeling masked;masked language modeling;trained multilingual language;trained multilingual;language learning objective;modeling masked language;language models mbert;language modeling objective;rely masked language;key language learning;language models;language learning;cross lingual setting;lingual setting pre;language modeling;masked language;multilingual language;lingual setting;gains natural language;cross lingual;multilingual;pre trained models;language processing non;shot cross lingual;non nl tasks;models rely masked;lingual;introductionpre trained", "pdf_keywords": "crosslingual sentence retrieval;multilingual language models;trained multilingual model;improving cross lingual;messagepre trained multilingual;pre trained multilingual;trained multilingual;trained multilingual language;cross lingual alignment;matched sentences monolingual;multilingual language processing;lingual alignment directly;sentence retrieval ttoeb;better cross lingual;improves cross linguality;sentences monolingual corpora;monolingual corpora;lingual representation learning;generating multilingual;cross lingual supervision;multilingual model mbert;entailment sequence labeling;multilingual model;entailment xnli crosslingual;cross lingual resource;lingual alignment;textual entailment sequence;clinical messagepre trained;cross lingual;translation multilingual language"}, "ef1d93b03c20b2f488b66e8e2c24fceb2105d58f": {"ta_keywords": "lingual model transfer;cross lingual model;detectingion cross lingual;lingual model;model using english;language using model;cross lingual;discriminate syntactic construction;lingual;using english resources;reliably discriminate syntactic;language using;english resources obtain;using lexical semantic;lexical semantic;syntactic construction;discriminate syntactic;using lexical;metaphorically using lexical;model transfer methodswe;previous work language;model transfer;model transfer approach;using model transfer;syntactic construction meant;semantic;work language using;syntactic;work language;language", "pdf_keywords": ""}, "3993788eb252f5eb7fc19e9f98357a72f9f0476d": {"ta_keywords": "membership network models;mixed membership graphs;mixed membership stochastic;membership block models;membership stochastic block;membership graphs;classical network models;network models;backgroundmixed membership network;membership graphs small;membership stochastic;interactions mixed membership;membership network;network models actual;stochastic block model;mixed membership block;graph different latent;network models permit;latent roles;backgroundmixed membership;block models;mixed membership;nodes usually;different latent roles;nodes;degree mixed membership;nodes usually taking;stochastic block;block model;slightly mixed membership", "pdf_keywords": ""}, "71c7104eaed93497824cf197949c77e7d6cb36d3": {"ta_keywords": "domain question answering;question answering iterative;question answering;answers drawn corpus;answering iterative retrieval;question answering answers;corpus knowledge base;corpus knowledge;retrieval knowledge bases;question answering consider;knowledge bases text;iterative retrieval knowledge;retrieval knowledge;answering answers;answering iterative;key clinical messagepulnet;corpus;drawn corpus knowledge;clinical messagepulnet open;answering answers drawn;corpus supplemented;knowledge base;answering;corpus supplemented large;knowledge bases;knowledge base kb;iterative retrieval;clinical messagepulnet;retrieval;setting corpus supplemented", "pdf_keywords": "corpus reasoning heterogeneous;corpus knowledge base;corpus reasoning;query guided retrieval;domain queston answering;knowledge base text;friendly approach retrieval;corpus knowledge;kb corpus reasoning;retrieved facts corpus;knowledge base;answers drawn corpus;answer entities significantly;query use knowledge;approach retrieval;using knowledge base;use knowledge base;answer entities;knowledge base algorithm;query guided;iterative query guided;queston answering;question entities answer;question entities;reasoning using knowledge;facts corpus;answer entities use;reasoning using text;drawn corpus knowledge;answer entities shortest"}, "911536dc3dfbbbf2bb8d71181b31e0aa7920b9f6": {"ta_keywords": "decision theoretic online;theoretic online learning;experts distributed prediction;experts algorithms;experts algorithms capable;adaptive algorithm expert;online learning;expert solutions;online learning discussed;algorithm expert solutions;distributed prediction;expert solutions aggregated;combination experts distributed;experts distributed;problem decision theoretic;decision theoretic;predictions suffering losses;methods experts algorithms;methods adaptive;methods adaptive algorithm;adaptive algorithm;prediction;adaptive;algorithm expert;prediction interval proposed;expert;distributed prediction interval;prediction interval;theoretic online;solutions methods adaptive", "pdf_keywords": ""}, "798e45ea830884be36c3f526d3b169eaba95f989": {"ta_keywords": "local nash equilibria;nash equilibriumlibria isolated;backgroundlocal nash equilibriumlibria;nash equilibria zero;equilibria continuous zerosum;continuous zerosum games;strictt local nash;nash equilibriumlibria;nash equilibria continuous;nash equilibria generic;differential nash equilibria;generic local nash;equilibria generic local;zerosum games;local nash;nash equilibria;equilibria zero sum;sum continuous games;zerosum games resultsthe;equilibria zero;equilibriumlibria;equilibriumlibria isolated strictt;differential nash;equilibria generic;equilibria continuous;equilibriumlibria isolated;continuous zerosum;continuous games;prove differential nash;zero sum setting", "pdf_keywords": "local nash equilibriumlibria;generic local nash;local nash equilibrium;local nash equilibria;refinement local nash;nash equilibria generically;genericity differential nash;local convexity game;characterizes local nash;local nash terms;backgroundlocal nash equilibriumlibria;degenerate local nash;differential nash equilibria;differential nash strictly;nash equilibria generic;isolated local nash;generality differential nash;nash equilibriumlibria;nondegenerate differential nash;strictt local nash;effiortsthe local nash;nash equilibria generality;differential nash structurally;nash equilibriumlibria isolated;local nash;differential nash equilibrium;games differential nash;nash equilibriumlibria zero;nash equilibria zerosum;map local nash"}, "8a0a8568acf2b95c9cb471e28ee6b25c5e4fe186": {"ta_keywords": "techniques sensticnet makes;techniques sensticnet;sensticnet common;sensticnet;sensticnet makes use;context sensticnet common;context sensticnet;sensticnet common common;driven senstiment analysis;senstiment analysis;sensticnet makes;sense knowledge;sense knowledge base;common sense knowledge;senstiment analysis methods;reduction techniques sensticnet;knowledge base cognitive;cognitive domains using;cognitive function driven;cognitive cognitive;cognitive cognitive function;cognitive;cognitive function;base cognitive cognitive;cognitive domains;driven senstiment;common common sense;base cognitive domains;common sense;function driven senstiment", "pdf_keywords": ""}, "e75c388b60cf447be7148be25feeee3e10d12cf4": {"ta_keywords": "extrapolation fundamental;interpolation extrapolation fundamental;extrapolation notion;extrapolation notion interpolation;extrapolation fundamental various;interpolation extrapolation;notion interpolation extrapolation;extrapolation occurs;convex hull extrapolation;hull extrapolation occurs;extrapolation;dimension amounts extrapolation;hull extrapolation;interpolation occurs;approximation interpolation occurs;interpolation;learning high dimension;extrapolation occurs falls;approximation interpolation;interpolation occurs sample;learning function approximation;function approximation interpolation;notion interpolation;amounts extrapolation notion;learning function;deep learning function;learning;deep learning;amounts extrapolation;learning high", "pdf_keywords": "dataset interpolation surely;data extrapolation regime;dimensional spaces extrapolation;interpolation observe dimensionality;interpolation extrapolation fundamental;interpolation surely happens;100 dataset interpolation;interpolation regime dataset;dataset interpolation;loose interpolation extrapolation;interpolation extrapolation information;interpolation surely occurs;extrapolation regime;interpolation surely;extrapolation fundamental;latent space interpolation;extrapolation fundamental various;interpolation extrapolation;extrapolation commonly;extrapolation regime real;extrapolation extrapolation commonly;samples interpolation regimes;interpolation regime;dimensions probability interpolate;real datasets embedding;new samples interpolation;samples interpolation regime;extrapolation information;data extrapolation;analyze data extrapolation"}, "42c3c50b8e368ee2e1b52d010b6c53b3d732770c": {"ta_keywords": "texts speech sentiment;speech sentiment analysis;learn sentiment information;transcripts based sentiment;learn sentiment;speech sentiment;sentiment information written;sentiment information;models learn sentiment;sentiment analysis;trained language models;sentiment analysis methods;automatic speech;sentiment analysis separately;pre trained language;employing automatic speech;trained language model;semi supervised training;language models learn;based semi supervised;texts speech;based sentiment analysis;semi supervised;language models;automatic speech recognition;trained language;written texts speech;sentiment;based sentiment;speech recognition ar", "pdf_keywords": "speech sentiment analysis;analysis speech sentiment;sentiment analysis speech;accuracy speech sentiment;pre speech sentiment;texts speech sentiment;speech sentiment;using speech sentiment;transcripts based sentiment;automated sentiment;2e speech sentiment;automated automated sentiment;automated sentiment analysis;learn sentiment information;method speech sentiment;pseudo labeling sentiment;train sentiment classifier;sentiment classifier;learn sentiment;sentiment classification;train false sentiment;train sentiment;automatic speech;labeling sentiment classification;classifier accuracy speech;sentiment classifier used;performance sentiment classifier;sentiment classification observed;false sentiment classifier;labeling sentiment"}, "5dcbdb9bf80575953b5d21f378d8139f0a44168b": {"ta_keywords": "human spoken dialogue;natural spoken dialogue;spoken dialogue essential;important situated dialogue;situated dialogue;spoken dialogue;triggers human spoken;dialogue recognition analysis;spoken dialogue recognition;dialogue recognition;dialogue essential;dialogue;emotion triggered speakers;dialogue essential consider;build natural spoken;identifying user emotion;user emotion investigating;emotion occurred methodsthe;human spoken;natural spoken;analysis objectivehuman communication;user emotion;objectivehuman communication naturally;objectivehuman communication;emotion investigating;emotional aspects identifying;emotion investigating reason;investigating reason emotion;essential consider emotional;emotion triggered", "pdf_keywords": ""}, "bc632f81dab322ac610a8d11463cc1bba6130eda": {"ta_keywords": "historical text normalization;backgroundhistorical text normalization;text normalization datasets;text normalization relies;text normalization;normalization datasets languages;multi task learning;task learning architectures;normalization datasets;task learning;normalization;normalization relies;languages using autoencoding;task learning lead;backgroundhistorical text;autoencoding;task learning configurations;using autoencoding;normalization relies small;63 multi task;sequence based historical;training datasets recent;multi task;datasets recent work;backgroundhistorical;different multi task;historical text;based historical text;datasets systematic;training datasets", "pdf_keywords": "contexthistorical text normalization;historical text normalization;text normalization datasets;text normaliza task;text normalization projects;text normalization relies;text normalization;normaliza task learning;text normalization important;standard attentional encoder;autoencoding auxiliary task;attentional encoder decoder;multitask learning;attentional encoder;multitask learning architectures;task historical text;text normalization use;text normalization results;multitask learning helps;mainly multitask learning;historical text normaliza;normalization datasets languages;different multitask learning;use standard attentional;multi task learning;neural machine translation;task learning;text normaliza;model historical text;better autoencoding generally"}, "13b674bb3078623608045a18570b47f6e49a8358": {"ta_keywords": "paintings text descriptions;paintings descriptions annotate;descriptions annotate paintings;descriptions simple embedding;identifying paintings text;identifying paintings descriptions;paintings text;descriptions annotate;annotate paintings;paintings descriptions;descriptions visual;layout identifying paintings;understanding descriptions visual;ontology associate image;annotate paintings contour;image regions text;identifying paintings;descriptions visual themes;text descriptions;text image;background identifying paintings;text spans descriptions;text image core;descriptions important task;simple embedding based;annotate;simple embedding;embedding based;paintings contour data;text descriptions important", "pdf_keywords": ""}, "4b9795493a937b9034be9c26afab23f6dc751f62": {"ta_keywords": "reftomtons retrieval automaton;retrieval automaton;present reftomtons retrieval;reftomtons retrieval;language models;based language models;language models model;language model examples;datastore search;retrieval;language model;costly datastore search;natural language text;examples retrieved;introductionretrieval based language;examples retrieved external;standard language model;model examples retrieved;search performed frequently;probability natural language;datastore search performed;natural language;search;language text;computationally costly datastore;automaton;language text combining;costly datastore;datastore;search performed", "pdf_keywords": "retrieval automaton;reftomtons retrieval automaton;automatedon augmented retrieval;retrieval kn linguistic;automaton augmented retrieval;symbolic language modeling;language modeling automaton;symbolic language model;symbolicic language modeling;language modeling automatic;language models;approach retrieval kn;language models lims;language neural machine;retrieval automaton report;automatedwe knn search;predicting knn searches;new approach retrieval;based language models;constructing search free;augmented retrieval methods;automaton efficiently;novel language model;automaton efficiently efficiently;retrieval methods;novel algorithm retrieval;language neural;present reftomtons retrieval;tokens constructing search;reftomtons retrieval"}, "a7b6802f20c399615dbac161678cd6a6d2df5a97": {"ta_keywords": "machine learning crowdsourcing;learning crowdsourcing;crowdsourcing;using human loop;learning crowdsourcing share;human loop pipelines;human loop important;systems human loop;human loop challenging;human loop;crowdsourcing share;crowdsourcing share experience;search engines yandex;combine machine learning;engines yandex;loop important;machine learning;loop important tool;yandex;human computer systems;tool human computer;computer systems human;loop pipelines;loop;search engines;loop challenging;systems human;using human;loop challenging challenging;important tool human", "pdf_keywords": ""}, "7df6aa19f50c8ec5f12d58e0685ed5c6e9a08bb2": {"ta_keywords": "like product reviews;using reviews;product reviews;reviews provide valuable;reviews proved useful;methods reviews provide;reviews provide;product reviews proved;paper using reviews;recommendation focus learning;approaches recommendation;methods reviews;approaches recommendation focus;traditional approaches recommendation;recommendation focus;reviews;natural language approaches;information like product;purchase natural language;recommendation;historical feedback;feedback;historical feedback estimate;reviews proved;natural language;performance methods reviews;estimate latent user;user click product;feedback estimate simple;latent user preferences", "pdf_keywords": ""}, "a3e3a9d878999c7038c275e75f5cd8a232aa4999": {"ta_keywords": "introductiontranslation learning;introductiontranslation learning proven;speech model pre;pretrained models;evaluate pretrained models;model pre trained;trained self supervised;self supervised learning;learning transfers;pretrained models various;benchmark evaluate pretrained;speech model;supervised learning transfers;evaluate pretrained;learning transfers remarkably;introductiontranslation;self supervised;learning;years speech model;pretrained;pre trained self;pre trained;speech natural language;advancing state speech;speech natural;state speech natural;trained self;learning proven crucial;supervised;language processing", "pdf_keywords": "tasks automatic speech;model speech tasks;self trained speech;domain automatic speech;automatic speech;model speech generation;speech model pre;benchmark 10 speech;speech generation tasks;intelligent speech interfaces;tasks speech translation;model speech;trained speech representations;trained speech;trained speech recognition;task based speech;speech tasks;new model speech;speech model;domain speech classification;10 speech tasks;speech generation;speech interfaces;speech translation domain;step speech processing;speech tasks speech;include speech translation;complete domain speech;downstream model speech;automatic speech recognition"}, "b5002aa334f8d0c0e1a4dedad79580e10a928c30": {"ta_keywords": "self supervised learning;ssl representations;spectral feature extractors;ssl representations depends;learning based speech;quality ssl representations;supervised learning sl;deep learning;based speech tasks;ssl training domain;various deep learning;ssl training;introduction self supervised;relatedness ssl training;deep learning based;self supervised;feature extractors;learning sl models;speech tasks particu;feature extractors log;speech tasks;spectral feature;filterbanks hand crafted;supervised learning;learning sl;filterbanks;log melt filterbanks;melt filterbanks;highly relatedness ssl;quality ssl", "pdf_keywords": "attention based fusion;features learnable fusions;self supervised speech;features attention based;fusion approach speech;combining self supervised;features attention;features ssl representations;supervised speech;learnable fusions;dimensional features attention;cross attention blocks;combine spectral features;supervised speech recognition;spectral features ssl;learning features learnable;spectral feature extractors;self supervised learning;parallel cross attention;combined features complementary;highly automated speech;multilingual speech recognition;learning features;task speech recognition;supervised learning features;feature extractors;based speech tasks;learnable fusions obtain;based feature extractor;features learnable"}, "8809d0732f6147d4ad9218c8f9b20227c837a746": {"ta_keywords": "speech processing toolkit;developments espnet toolkit;applications automatic speech;espnet toolkit boosted;espnet toolkit;speech recognition ar;automatic speech;automatic speech recognition;speech processing applications;recognition ar speech;end speech processing;speech recognition;speech processing;toolkit boosted conformer;recent developments espnet;ar speech translations;developments espnet end;developments espnet;speech translations;boosted conformer;espnet end;conformer convolution augmented;espnet end end;espnet;called conformer convolution;conformer convolution;boosted conformer study;convolution augmented transformer;ar speech;toolkit boosted", "pdf_keywords": "end speech processing;speech processing conformer;attention computation;attention computation structure;processing text speech;speech processing toolkit;speech processing;language corpora conformer;algorithm attention computation;speech translations;conformerbased models speech;automatic speech;model speech processing;speech recognition;corpus smr;amplification attention fundamental;conformer model speech;attention fundamental;translate text speech;models speech recognition;applications automatic speech;available corpora conformer;speech processing applications;models speech;model various speech;corpus smr tt;algorithm attention;model speech;speech separation text;speech translations st"}, "d8252e24b6036ca895800b547698ab44d09ae350": {"ta_keywords": "personal information management;management personal information;improve personal information;information email messages;personal information email;information management tools;email messages calendar;information management;machine learning techniques;email messages;messages calendar entries;messages calendar;introductionmanagement personal information;machine learning;personal information;information email;entries items workstation;calendar entries items;computer technology management;technology management personal;calendar entries;learning techniques effectively;items workstation documents;calendar;management tools;evidence machine learning;learning techniques;workstation documents highly;management personal;computer technology present", "pdf_keywords": ""}, "f91c24b0dc56a6b377e99e046d7540e5bb7aa46e": {"ta_keywords": "deepening china major;problem deepening china;deepening china;purposethe deepening china;health problem deepening;china major problem;china major public;china major;deepening;major problem medical;purposethe deepening;problem deepening;medical education reform;major public health;medical education;public health problem;problem medical medical;medical medical education;problem medical;informatization college student;health problem;education reform informatization;public health;reform informatization college;medical medical;health;medical;china;education reform;major problem", "pdf_keywords": ""}, "a6a7724763d8adba466519489b0b9d209e7f2d15": {"ta_keywords": "text generation;text text generation;generated texts;involve text generation;generated texts actually;text generation problem;evaluate generated texts;generated text;generated text text;text generation major;evaluation generated text;pre trained sequence;sequence sequence models;trained sequence sequence;sequence models;clinical messagea;text;clinical messagea wide;sequence models operationalize;key clinical messagea;trained sequence;text text;texts;texts actually fluent;texts actually;applications involve text;pre trained model;involve text;decoder based pre;sequence sequence", "pdf_keywords": "evaluation text generation;text generation task;conditional text generation;text generation;text text generation;generate text generation;evaluation generated text;generation task based;text generation work;generation task empirically;text generation form;generated text;generation task;implement generation task;generated text text;generate text;text generation problem;quality generated text;conditioned generation task;trained sequence sequence;effectively evaluate texts;evaluation neural models;evaluation neural;trained raw text;generation form extractive;pre trained sequence;automated evaluation growing;sentences abstractive summarization;evaluate texts;used generate text"}, "e3f1a9c3d87e9828cdeb08ba90a260c69e974a75": {"ta_keywords": "dance generation audio;generating dance piece;dance piece music;basic dance generation;dance step generation;generating dance;deep recurrent method;tools generating dance;networks basic dance;dance generation;deep recurrent neural;basic dance;dance piece;dance step;supervised deep recurrent;basic dance step;recurrent neural networks;time basic dance;generation audio;backgroundweakly supervised deep;deep recurrent;dance;recurrent neural;recurrent method;piece music;lsm process audio;supervised deep;short term memory;backgroundweakly supervised;piece music objectiveto", "pdf_keywords": "neural network music;dance sequences using;output trainingthe music;long dance sequences;sequences mapping music;dancing music work;deep recurrent neural;dance sequences;dancing music;generating music;generate long dance;decoder generate music;dance piece music;dance generate;generate music;trainingthe music beat;generating music music;motion generation music;ability generate music;generating dance piece;generating dance movements;method generating music;sequences using audio;music input dn;approach deep recurrent;dance generate motion;music beat retrieval;trainingthe music;track music beat;deep recurrent"}, "78d57a1ecd724c5f8b1534372969d5b35daa6d4b": {"ta_keywords": "models constituency parsing;backgroundimproving neural parsing;neural parsing;neural parsing disentangling;constituency parsing achieve;constituency parsing;base parsers decoding;base parsers;outputs base parsers;neural models constituency;parsing disentangling model;parsers decoding;parsers;parsers decoding straightforward;parsing disentangling;parsing;proposed generative neural;generative neural;parsing achieve state;generative neural models;parsing achieve;direct search generative;proposed generative;generative models difficult;search generative;backgroundimproving neural;generative;decoding straightforward;generative models;search generative models", "pdf_keywords": "generative neural parser;effective generating parses;generating parses highly;neural parser;generating parses;neural parser perform;generating generating parses;practical parser reranking;parser reranking model;parser reranking;generating parses depth;generating parses set;parses;parsing increasingly;search generative;parsers;constituency parsing increasingly;ability perform parser;search generative models;models improves generative;parser perform constituency;traversal generative model;practical parser;right traversal generative;traversal generative;parses highly;improves generative models;generative neural;neural biology generative;generative reranking systems"}, "b6c6e06b4bc68349845b30e01e01d7603f468547": {"ta_keywords": "optimal capacity relay;optimal placement relay;placement relay nodes;relay node placement;duplex radios relays;capacity relay node;multi relay channel;radios relays;placement relay;relay nodes;relay channel;relay channel study;capacity relay;relay nodes straight;placement multi hop;multi relay;formulas multi relay;relay node;multi hop wireless;node placement multi;duplex radios;use duplex radios;wireless network line;hop wireless network;introduction optimal capacity;wireless network;node placement;relay;problem optimal placement;optimal placement", "pdf_keywords": "optimal source relay;relay channel optimal;channel relay optimal;relay optimal source;relay nodes optimal;optimal relay location;optimal relay;relay node optimal;relay optimal;optimal multi relay;relays optimally line;relay location optimal;optimally placing relay;optimal distance relay;relays optimally;formulas optimal relay;relay optimal multi;allocation source relay;optimum relay locations;optimum location relay;optimal placement relay;relay channel achievable;optimal function relay;scheme relay optimal;channel capacity relay;optimum relay;relay nodes consider;number relays optimally;relay channel;relay power law"}, "7729fbebff327bebb9292dc1c51c51dd55390954": {"ta_keywords": "compositional distributional semantic;distributional semantic methods;distributional semantic;methods use tensors;use tensors;performance tensors;tensors;match performance tensors;tensors make use;tensors model;compositional distributional;tensors make;tensors low rank;use tensors model;tensors model multi;tensors low;size tensors;unfortunately size tensors;performance tensors low;semantic methods;way interactions vectors;size tensors make;semantic;semantic methods use;number compositional distributional;distributional;compositional;interactions vectors unfortunately;low rank approximations;multi way interactions", "pdf_keywords": ""}, "f3271e61dc0507183ee399393129d7888c2f82b9": {"ta_keywords": "reference based evaluation;manual evaluation;evaluation measures blaeu;evaluation measures;evaluation approaches;quality output language;based evaluation measures;evaluation approaches differ;evaluation;output language processing;based evaluation;translation speech recognition;machine translation speech;machine translation;focus manual evaluation;systems machine translation;practical requirements evaluation;requirements evaluation;requirements evaluation approaches;language processing;output language;language processing systems;blaeu orwer appealing;rapid quantitative comparison;quality output;reference based;speech recognition;quantitative comparison;measures blaeu orwer;translation speech", "pdf_keywords": ""}, "68aa7c7b65365c3303d5024b1273408fb435d178": {"ta_keywords": "entrainment affects dialogue;entrainment dialogue;effect entrainment dialogue;dialogue acts entrain;dialogue affects human;dialogue acts lexical;interaction entrainment lexical;entrainment dialogue acts;entrainment lexical level;affects dialogue abstract;dialogue affects;affects dialogue;dialogue abstract structural;entrainment lexical;dialogue abstract;dialogue;interaction entrainment;machine interaction entrainment;dialogue acts;introductionentrainment factor dialogue;choice given dialogue;given dialogue;factor dialogue affects;given dialogue acts;lexical level;entrainment;effect entrainment;human machine interaction;entrainment affects;acts lexical choice", "pdf_keywords": ""}, "b0ddd849c5ae0004678fa483908c06d87894f3ab": {"ta_keywords": "model hmm widely;model speech recognition;hmm based variational;constructing hmms practical;hidden markov model;hmms practical bayesian;constructing hmms;model hmm;criterion hmm based;hidden markov;acoustic model speech;method constructing hmms;markov model hmm;model speech;speech recognition;state hidden markov;selection criterion hmm;criterion hmm;speech recognition paper;hmm based;based variational baye;markov model;bayesian model selection;acoustic model;markov;variational baye;used acoustic model;hmm widely used;bayesian framework;bayesian framework methods", "pdf_keywords": ""}, "b0d644277933988c00b22d8ae012512fe498ad62": {"ta_keywords": "disambiguation word senses;word sense disambigu;word senses context;approaches word sense;sense disambigu;word senses;senses context easy;word sense;senses context;linguistic knowledge;disambiguation word;linguistic knowledge representations;disambiguation;introduction disambiguation word;quality linguistic knowledge;linguistic;senses;introduction disambiguation;unsupervised knowledge;supervised knowledge;knowledge representations;unsupervised knowledge free;approaches supervised knowledge;completely unsupervised knowledge;word quality linguistic;supervised knowledge based;instances given word;approaches word;knowledge free;context easy humans", "pdf_keywords": ""}, "ebc64974e9e0021984a0158b3c04b60327730a88": {"ta_keywords": "base question answering;neural semantic parsing;question answering;question answering kq;retrack neural semantic;knowledge base;semantic parsing framework;semantic parsing;question answering present;scale knowledge base;knowledge base question;neural semantic;parsing framework;answering present retriever;framework knowledge base;parsing framework large;parsing;checker retrack neural;large scale knowledge;answering;retrieve relevant;answering kq;semantic;scale knowledge;retrack neural;retriever transducer checker;relevant kb items;neural;retriever transducer;introductionretrack flexible", "pdf_keywords": ""}, "f0cd4de3cdf547dcdcc6995dca9ab3f65955b324": {"ta_keywords": "layer recurrent models;recurrent models limited;recurrent models;multi layer recurrent;lattice recurrent units;layer recurrent;lattice recurrent;recurrent units;called lattice recurrent;introductionrecurrent neural networks;learning deep;models called lattice;modeling sequences;recurrent;recurrent units lu;deep multi layer;sequences low resource;learning deep multi;modeling sequences low;introductionrecurrent neural;deep multi;neural;neural networks;challenge learning deep;models limited resources;resources lu models;models;neural networks shown;address challenge learning;models introduce", "pdf_keywords": "layer recurrent models;gradient recurrent networks;multi layer recurrent;gradient recurrent;recurrent networks;recurrent models limited;models lattice recurrent;layer recurrent;recurrent models;vanishing gradient recurrent;lattice recurrent units;lattice recurrent unit;recurrent networks grs;gated recurrent unit;based gated recurrent;weights recurrent;lattice recurrent;new lattice recurrent;time lattice recurrent;weights recurrent unit;gu lattice recurrent;decoupling weights recurrent;gated recurrent;recurrent unit;recurrent units;called lattice recurrent;recurrent units designed;highway networks lstms;comparing lattice recurrent;recurrent unit gu"}, "485b3f77b9913e151e7ca897d99497e70e7f30d1": {"ta_keywords": "neural machine translation;machine translation nm;subword units;translate using subword;subword units allow;translation nm standard;using subword units;words byte pair;words byte;granularity subword units;machine translation;nm standard translate;infrequent words byte;generating sub words;translation nm;sub words;standard translate using;sub words unsupervised;standard translate;subword;using subword;introductionin neural machine;words unsupervised resource;byte pair encoding;encoding bpe;pair encoding bpe;byte;neural machine;encoding bpe variants;introductionin neural", "pdf_keywords": "neural machine translation;translation neural machine;machine translation neural;optimize accuracy translation;translation neural;improves translation accuracy;machine translation nm;machine translation;encoding operations training;method improves translation;improves translation;translation accuracy significantly;translation accuracy language;languages accuracy translation;translation accuracy;words bytepair encoding;generating sub words;accuracy translation;translate using subword;accuracy translation accuracy;set accuracy translation;languages incremental amplification;infrequent words bytepair;translation accuracy ofthe;translation nm standard;words bytepair;languages incremental;embeddings demonstrate incremental;process translation;merging sub words"}, "1e4e2aceed87febcc643f1473507c9535ba5c19a": {"ta_keywords": "simultaneous speech translation;speech translation using;sl simultaneous speech;automatic speech;sl speech translation;speech translation;speech processing;design automatic speech;speech translation st;speech processing tasks;simultaneous speech;blockwise streaming transformer;streaming transformer based;transformer based contextual;success speech processing;streaming transformer;step streaming sl;understanding sl speech;streaming sl simultaneous;streaming sl;sl speech;step streaming;tasks like spoken;spoken language understanding;using blockwise streaming;blockwise streaming;contextual block processing;spoken language;translation using blockwise;language understanding sl", "pdf_keywords": "simultaneous speech translation;transformer streaming speech;streaming simultaneous speech;streaming speech language;recognition speech translation;streaming speech;attention simultaneous translation;speech text translation;translation speech text;translations acoustic input;translation tt systems;speech translation ctc;speech translation tt;simultaneous speech language;speech translation st;translation translations acoustic;lsl simultaneous speech;simultaneous translation translations;attention joint translation;propose crosslingual encoding;simultaneous translation;simultaneous speech;cross lingual encoding;approach simultaneous speech;simultaneous simultaneous translations;simultaneous translation using;crosslingual encoding;simultaneous translation new;speech language tasks;speech translation"}, "6e24bcfcdb31afbb313a13c1c84cb779ceb17500": {"ta_keywords": "decision problem reference;given decision problem;person given decision;decision methods;decision problem repeatedly;chooses action maximize;action maximize value;decision methods paper;person chooses action;decision problem;influences decision;loss influences decision;influences decision methods;repeatedly person chooses;person determines outcomes;given decision;person chooses;action maximize;outcomes perceived gain;chooses action;reference point person;point person determines;perceived gain;maximize value;maximize value function;value function reference;perceived gain loss;gain loss influences;determines outcomes;determines outcomes perceived", "pdf_keywords": ""}, "0d20360c5d533760d97d7ce19b78d4791a5173cb": {"ta_keywords": "dynamics terrorist attacks;analysis terrorist events;analyzing terrorist organizations;terrorist organizations dynamics;methods analyzing terrorist;analyzing terrorist;heterogeneous dynamics terrorist;terrorism context network;network analysis;terrorist attacks creation;network science analysis;terrorist attacks;network analysis popular;dynamics terrorist;terrorist events;terrorist events work;study terrorism;science analysis terrorist;study terrorism context;network science;analysis terrorist;terrorism;terrorist organizations;terrorism context;applied network science;applies study terrorism;attacks creation;attacks;terrorist;context network analysis", "pdf_keywords": ""}, "5d9fe38b750f59b4ef0a2b58fde0f60d4317c7ad": {"ta_keywords": "domain important metadata;domains attribute;best domains attribute;data domains apparent;data domains;datasets multiple metadata;multiple metadata attributes;metadata attribute context;single metadata attribute;domains apparent single;metadata attributes;data called domains;attribute context multi;context domain;multi attribute data;context multi attribute;metadata attribute used;context domain important;metadata attribute;multiple metadata;domains apparent;multi attribute;single metadata;important metadata attribute;domains;metadata attributes divide;divide data domains;domain important;attribute context;metadata", "pdf_keywords": ""}, "242c35b91fe1d7aedab9d1da7652aad2219d4784": {"ta_keywords": "speech translation e2e;end speech translation;speech translation;translation e2e methods;rn based models;translation e2e;end end speech;end speech;models transformer;transformer models;based models transformer;transformer architectures tasks;transformer models significantly;models transformer confirm;rn based;speech;confirm transformer models;transformer architectures;plethora plethora phgs;phgs;transformer;plethora phgs;compare rn based;plethora phgs available;transformer confirm;phgs available;confirm transformer;models;translation;based transformer architectures", "pdf_keywords": ""}, "924e43c4de98743d2e7c14c241b03b2109325b90": {"ta_keywords": "parallelization blocked gibbs;blocked gibbs sampling;gibbs sampling proposed;gibbs sampling;collapsed gibbs sampling;gibbs sampling multiple;distributing collapsed gibbs;parallel run metropolish;sampling multiple processors;blocks distributing sampling;distributing sampling block;blocked gibbs;gibbs;parallelization;distributing sampling;sampling block;sampling block multiple;uses blocked sampling;collapsed gibbs;sampling proposed;blocked sampling;simple correct parallelization;correct parallelization blocked;sampling;parallelization blocked;sampling proposed present;sampling multiple;blocked sampling dividing;run metropolish;correct parallelization", "pdf_keywords": ""}, "030fade3049e0847702393dde3100ecc41a5e86a": {"ta_keywords": "hill climbing search;climbing search;nding global optimum;climbing search settings;global optimum;nd locally optimal;learning tasks;learning systems;optimal;locally optimal;practical learning systems;global optimum intractable;intractable practical learning;hill climbing;hill climbing nd;optimum;practical learning;forms hill climbing;learning tasks involve;learning;optimal element;performing hill climbing;locally optimal element;climbing nd locally;learning systems use;climbing nd;introduction learning tasks;elements seeking;optimal element methods;climbing", "pdf_keywords": ""}, "bea54062d105b9fe3250ce3569cf817e54772894": {"ta_keywords": "phrase recognition historical;historical treebanks training;historical treebanks;modern historical treebanks;treebanks training data;automatic recognition phrases;treebanks training;recognition phrases historical;introductionautomatic phrase recognition;phrases historical german;phrase recognition;treebanks;historical syntax based;recognition historical german;neural sequence labeling;recognition phrases;historical syntax;probabilistic parser;sequence labeling;probabilistic parser created;phrases historical;tool probabilistic parser;sequence labeling tool;german lack annotated;historical german methodsusing;historical german;theories historical syntax;labeling tool probabilistic;parser;german methodsusing modern", "pdf_keywords": "phrase recognition german;phrase recognition historical;automatic recognition phrases;introductionautomatic phrase recognition;phrase recognition;recognition phrases complex;recognition phrases historical;extracted phrases training;phrases aforementioned corpus;trained extracted phrases;linguistic annotation;phrases inthe recognition;automatic syntactic analysis;recognition phrases selected;recognition phrases inthe;phrases historical german;recognition phrases;automatic syntactic;models phrase recognition;recognition phrases aforementioned;recognition german results;phrases training sets;constituent parsing german;extracted phrases;aforementioned corpus;historical treebanks evaluation;quality linguistic annotation;inthe recognition phrases;identify common phrases;phrases training"}, "38705aa9e8ce6412d89c5b2beb9379b1013b33c2": {"ta_keywords": "estimation deep learning;convergence deep feedforward;semiparametric inference;neural networks estimation;semiparametric inference resultswe;effects semiparametric esttimands;neural nets rates;feedforward neural nets;causal effects semiparametric;semiparametric esttimands methodswe;use semiparametric inference;networks use semiparametric;estimation deep;rates convergence deep;deep feedforward neural;neural nets;deep neural networks;semiparametric esttimands;deep feedforward;deep learning;effects semiparametric;purposedeep neural networks;neural networks use;deep neural;convergence deep;estimation inference;deep learning result;neural networks;inference step estimation;feedforward neural", "pdf_keywords": "estimation deep learning;deep neural nets;semiparametric inference neural;deep learning deep;networks deep nets;deep nets results;learning deep;rate deep networks;deep networks faster;estimation deep;neural networks deep;deep networks increasing;deep neural networks;functions rate deep;deep nets;deep learning delivers;models deep learning;deep deep networks;deep neural;rate deep;learning deep learning;deep net architectures;tool deep learning;deep learning economic;firststep estimation deep;deep learning important;deep networks;networks deep;utility deep learning;deep learning"}, "4dd85ae17a5fd0bce09ffef0455b6e827d7e1e2b": {"ta_keywords": "attack networked vehicular;networked vehicular cyber;vehicular cyber physical;deception attack networked;vehicular cyber;optimal deception attack;attack distributed cyber;distributed cyber physical;networked vehicular;attack distributed;deception attack;cyber physical systems;introduction optimal deception;optimal deception;attack networked;distributed cyber;injection attack distributed;network agent node;sensors agent nodes;data injection attack;cyber physical;vehicular;network agent;cyber physical considered;false data injection;multiple agent nodes;multiple sensors agent;agent nodes;hop network agent;agent nodes equipped", "pdf_keywords": "linear attack sensor;linear attack distributed;propose linear attack;linear attack feasible;linear attack scheme;attack feasible distributed;optimal linear attack;keeping attack detection;attack sensor;learning parameters attack;linear attack;attack distributed cyber;popular linear attack;attack detection;attack algorithm;cyber attacks;attack detection probability;attack distributed;attack algorithm simultaneous;cyber attacks increasing;attacks cyber physical;distributed estimation;vulnerable cyber attacks;attack strategy distributed;parameters attack algorithm;various cyber attacks;attacks cyber;attack sensor associated;perturbation stochastic approximation;attack feasible"}, "5143ebd23322fe805bed2667fcfb70920c105f7f": {"ta_keywords": "model cognitive tutor;demonstrationnstration cognitive modelinging;cognitive modelinging demonstration;cognitive tutor computerized;cognitive tutor;cognitive modelinging;demonstration simulated student;computerized tutor;tutor computerized;computerized tutor teaches;demonstrationnstration cognitive;better demonstrationnstration cognitive;model cognitive;tutor teaches human;simulated student;tutor computerized tutor;human students cognitive;cognitive model cognitive;students cognitive skills;simulated student machine;cognitive skills observing;cognitive model;students cognitive;cognitive skills methods;modelinging demonstration simulated;learning agent;modelinging demonstration;cognitive skills;tutor teaches;learning agent learns", "pdf_keywords": ""}, "cfbe9183f2fe2847f7a3c811f6309a2cab3f85cf": {"ta_keywords": "pretraining extractive summarization;extractive summarization scientific;summarization scientific documents;extractive summarization tasks;bert based extractive;summarization scientific;summarization datasets report;existing summarization datasets;summarization datasets;extractive summarization;pretraining bert based;summarization tasks;based extractive summarization;summarization tasks work;pretraining bert;influence pretraining bert;existing summarization;leverages existing summarization;pretraining extractive;success extractive summarization;summarization;effect pretraining extractive;documents large pretrained;bert based;scientific documents methodswe;bert;large pretrained models;pretrained models;scientific documents;scientific documents large", "pdf_keywords": ""}, "20086d6a9fab6081f300e08d3f952cb9b16e6de8": {"ta_keywords": "approximate search algorithms;analysis approximate search;approximate search;indexing deletion neighborhoods;search algorithms;search algorithms involve;involve indexing deletion;algorithms involve indexing;indexing deletion;indexing;require huge indices;involve indexing;shortest retrieval times;deletion neighborhoods methods;shortest retrieval;linear indices great;huge indices;search;provide shortest retrieval;indices great;huge indices sizes;algorithms;deletion neighborhoods;retrieval;algorithms involve;retrieval times;indices great provide;indices sizes;analysis approximate;neighborhoods methods", "pdf_keywords": ""}, "693f5d55e0561099944f5e00e301bf26db0b972d": {"ta_keywords": "statistical semantic analysis;utterance transform semantic;spoken language understanding;statistical semantic;human computer dialogue;computer dialogue systems;dialogue systems task;task spoken language;acoustic utterance;input acoustic utterance;methods statistical semantic;dialogue systems;semantic analysis;acoustic utterance transform;language understanding slus;computer dialogue;utterance transform;spoken language;systems task spoken;semantic analysis results;semantic representation;semantic analysis represent;utterance;language understanding;dialogue;semantic representation goal;semantic;transform semantic representation;transform semantic;goal semantic analysis", "pdf_keywords": ""}, "939a149f156425b83e48ea72e9e09a55ea33b8d7": {"ta_keywords": "crossing representation wavelet;wavelet transform;wavelet transform domain;representation wavelet transform;representation wavelet;wavelet;signal reconstruction perfectly;signal reconstruction;domain signal reconstruction;provides reconstruction image;stabilized zero crossing;zero crossing representation;image stabilized zero;reconstruction image;reconstruction perfectly reconstruct;reconstruct original image;algorithm provides reconstruction;zero crossing;original image stabilized;reconstruction image subjectively;clinical messagewe;reconstruction perfectly;transform domain signal;clinical messagewe present;image stabilized;key clinical messagewe;perfectly reconstruct original;crossing representation;new stabilized zero;transform", "pdf_keywords": ""}, "0fbb90b8fe1d02a4f0f616df9a09ec42eace53bd": {"ta_keywords": "voice activity detection;unsupervised method voice;voice activity;method voice activity;bayes framework voice;online unsupervised classification;unsupervised classification model;framework voice activity;unsupervised classification;variational bayes framework;online unsupervised method;introduction online unsupervised;variational bayes;based variational bay;activity detection vad;comparison variational bayes;method voice;online unsupervised;activity detection;new online unsupervised;unsupervised method;method based variational;voice;based variational;model comparison variational;unsupervised;classification;framework voice;classification model;variational bay", "pdf_keywords": ""}, "e21633b0b5e55dce56bc07e919c6d12ecf8cef0c": {"ta_keywords": "clauses pac learnable;learning nondeterminate clauses;determinate clauses pac;inductive logic programming;generalizations language pac;clauses pac;learnable obvious syntactic;pac learnable obvious;pac learning nondeterminate;pac learnable methods;language pac learnable;logic programs;restriction logic programs;introduction pac learning;logic programming;logic programming systems;obvious syntactic generalizations;syntactic generalizations;syntactic generalizations language;pac learnable;learning nondeterminate;pac learning;depth determinate clauses;nondeterminate clauses common;logic programs called;nondeterminate clauses;inductive logic;programs called locality;generalizations language;problem inductive logic", "pdf_keywords": ""}, "c305e3314c0853b14911f704c68b04cfc9ea7aa1": {"ta_keywords": "hindi dependenceency treebank;universal dependenceencies hindi;dependenceencies hindi dependenceency;dependenceencies hindi;dependenceency treebank universal;treebank universal dependenceencies;crosslingual dependency parsing;dependenceency treebank;dependenceency treebank conversion;hindi dependenceency;dependency parsing;dependency parsing paper;techniques crosslingual dependency;treebank universal;crosslingual dependency;dependenceencies;karakas universal dependenceencies;universal dependenceencies;treebank conversion paninian;treebank;treebank conversion;universal dependenceencies ud;dependency;dependenceency;dependenceencies ud;dependenceencies ud gaining;evaluation cross lingual;cross lingual techniques;lingual techniques crosslingual;parsing", "pdf_keywords": "hindi dependenceency treebank;dependenceency treebank;crosslingual dependency parsing;dependenceency treebank computational;inthe dependenceency treebank;dependency treebank implemented;dependency treebank;abundant hindi treebank;dependency treebank project;dependenceencies ud hindi;hindi treebank addressed;hindi treebank;layered treebank hindi;treebank hindi;stanford dependency treebank;dependency parsing;dependenceency treebank hdb;treebank project hindi;conversion hindi treebank;issue hindi treebank;syntactic structure hindithe;ud hindi dependenceency;hindi treebank uac;treebank uac hindi;dependency parsing paper;syntactic relations lexical;hindi urdu treebank;treebank implemented second;treebank implemented;treebank hindi urdu"}, "652579315d767331d8e05ea46489e6bd081ef48a": {"ta_keywords": "feedback evaluation students;evaluation feedback peer;feedback peer evaluation;evaluation students;peer evaluation;moos highly successful;introduction moos;evaluation students severe;moos;performing evaluation feedback;introduction moos highly;moos highly;evaluation feedback;feedback peer;performing evaluation;feedback evaluation;evaluation;classrooms critical;means performing evaluation;conventional classrooms critical;students;lectures download study;classrooms critical aspect;download study;lectures download;lectures;videos lectures;students severe;feedback;study", "pdf_keywords": ""}, "b293e4659e20815bcf0b6d31ce46b8bd9437c1fa": {"ta_keywords": "privacy protection machine;privacy preservation;protection machine learning;data privacy;problem privacy preservation;data privacy protection;privacy protection;privacy emerged;privacy preservation context;traditional data privacy;privacy;problem privacy;surveillance systems privacy;systems privacy emerged;privacy emerged big;concern machine learning;systems privacy;machine learning methods;machine learning act;emerged machine learning;machine learning based;machine learning quite;machine learning;note problem privacy;protection machine;learning methods strong;financial technology surveillance;learning based artificial;surveillance;smart healthcare", "pdf_keywords": "privacy machine learning;privacy learning;applications privacy ml;learning algorithm privacy;privacy learning fundamental;privacy ml;privacy neural networks;privacy deep learning;learning privacy learning;privacy learning understood;learning privacy;private machine learning;privacy ml investigating;neural networks privacy;computer learning privacy;privacy neural;networks privacy learning;privacy preservation machine;learning privacy individual;privacy ml divide;research privacy machine;privacy machine;data mining privacy;privacy private prediction;survey privacy ml;data privacy present;field learning privacy;train data privacy;data privacy;mining privacy"}, "8d17543c20f23b6a40bec9334d50e9c15a08c1c4": {"ta_keywords": "domain question answering;question answering;answers text knowledge;text knowledge base;extracting answers;question answering evolving;extracting answers text;end deep neural;large text corpus;entity linked text;text corpus building;answers text;developed extracting answers;knowledge base;text knowledge;deep neural;text corpus;end end deep;linked text;corpus building recent;specialized neural models;corpus building;knowledge base paper;corpus;networks specialized neural;end deep;linked text appropriate;answering;answering evolving;answering evolving complex", "pdf_keywords": "facts text networks;text networks;networks question answering;question answering knowledge;answering knowledge base;text networks specifically;text knowledge bases;text memory networks;question answering;base question answering;answers text knowledge;question answering task;natural language knowledge;domain question answering;networks integrate text;knowledge base processing;text based quality;natural language data;answering knowledge;large text corpus;text knowledge;simple question answering;knowledge bases;end deep neural;knowledge base;text corpus building;quality control text;learning understanding knowledge;learn node representations;question answering evolving"}, "1808b64aec21863489f0fe66f250890a3ac2b843": {"ta_keywords": "privacy preserving statistical;random noise secure;noise secure malicious;databases privacy;databases privacy obtained;privacy preserving;noise secure;implementation privacy preserving;privacy obtained perturbing;distributed implementation privacy;shares random noise;noise generation create;noise generation;preserving statistical databases;implementation privacy;noise generation method;random noise;secure malicious participants;purpose noise generation;privacy obtained;privacy;secure malicious;statistical databases described;preserving statistical;statistical databases;distributed protocols generating;noise;papers databases privacy;generating shares random;provide efficient distributed", "pdf_keywords": "verifiable secret sharing;malleable verifiable secret;databases privacy;malleably verifiably share;privacy preserving data;noise databases privacy;databases privacy obtained;verifiable secret;privacy preserving;secret sharings needed;database able share;secret sharing scheme;secret sharings;non malleable verifiable;malleable verifiable;method privacy preserving;secret sharings propose;able share data;secret sharing;privacy obtained perturbing;verifiably share;verifiably share value;number secret sharings;data publicly;applications cryptology;data publicly available;cryptography;cryptology applications;shares secret sharings;share data"}, "c3a662b864673d8cc7469051419ab8819926d4b0": {"ta_keywords": "multilingual bert;multilingual bert mbert;shown multilingual bert;multilingual representations enables;multilingual representations;quality multilingual representations;multilingual;high quality multilingual;multilinguality;multilinguality somewhat;multilinguality somewhat obscure;use crosslingual;quality multilingual;does use crosslingual;mbert linguistic properties;phenomenon reasons multilinguality;bert mbert;crosslingual signal training;mbert linguistic;reasons multilinguality somewhat;properties mbert linguistic;reasons multilinguality;crosslingual;shown multilingual;bert;introduction shown multilingual;bert mbert yields;languages;use crosslingual signal;crosslingual signal", "pdf_keywords": "trained languages multilinguality;fully trained multilingual;trained multilingual;necessary bert multilingual;multilinguality significantly improved;bert multilingual;language models multilinguality;multilinguality language significantly;models multilinguality suited;multilingual representations enables;multilingual bilingual;training stage multilinguality;multilinguality suited multilingually;integrating multilingual representations;multilingually trained languages;model multilingual language;stronger multilinguality;multilinguality languages high;multilingual bilingual bilingual;obtain stronger multilinguality;quality multilingual representations;multilinguality languages;multilingual representations;languages multilinguality fundamental;models multilinguality;multilinguality evaluate performance;improve multilinguality;trained multilingual present;languages multilinguality;multilinguality significantly"}, "b1d24e8e08435b7c52335485a0d635abf9bc604c": {"ta_keywords": "verification textual sources;sentences extracted fromwikipedia;fact extraction;fact extraction virification;dataset fact extraction;dataset verification textual;verification textual;fromwikipedia subsequently verified;extracted fromwikipedia;extracted fromwikipedia subsequently;textual sources;verified knowledge sentence;textual sources consists;fromwikipedia subsequently;fromwikipedia;sentence derived claims;dataset fact;noten enoughinfo annotators;altering sentences extracted;textual;enoughinfo annotators achieving;sources consists;enoughinfo annotators;claims generated;sentences extracted;annotators;annotators achieving;available dataset verification;subsequently verified knowledge;generated altering sentences", "pdf_keywords": "verification textual sources;extraction verification textual;fact extraction verification;verification textual;information extracted fromwikipedia;fact extraction;extracted fromwikipedia;extracted fromwikipedia using;fromwikipedia using;textual sources fever;extraction verification claims;dataset verification textual;articles generate claims;textual information;fromwikipedia using dataset;textual sources;textual information available;context journalism paper;fromwikipedia;annotation claims;textual sources discussed;dataset fact extraction;article generate claims;context journalism;attention context journalism;generate claims article;annotated claims;annotation claims dataset;humanannotated sentences able;annotation claims provide"}, "4a4bc9f6c5ec76b0d501b641d3092aceb2e083bd": {"ta_keywords": "food preferences restauranteurs;preferences restauranteurs;packages available restauranteurs;restauranteurs increasing software;restaurant customizable diner;preferences restauranteurs increasing;restaurant customizable;customizable diner;integration restaurant customizable;available restauranteurs;customizable diner profiles;restauranteurs;diner profiles;use food preferences;available restauranteurs today;diner profiles composed;restauranteurs increasing;food preferences;restauranteurs today including;restauranteurs today;network integration restaurant;restaurant;integration restaurant;venga opentable;diner;including venga opentable;noshlist fivestars business;venga opentable buzztable;offers variety services;use food", "pdf_keywords": ""}, "d5eeaac5c5e524ad05d9b1f3f3f41aece082955a": {"ta_keywords": "asbpc speech recognition;posteriors sparse training;bayesian posteriors sparse;bayesian predictive classification;speech recognition results;speech recognition;posteriors sparse;data speech recognition;asbpc speech;sparse training;speech recognition case;distribution bayesian predictive;training data speech;sparse training data;predictive distribution bayesian;data speech;bayesian predictive distribution;bayesian predictive;referred asbpc speech;results bayesian predictive;robust data sparseness;classification using variational;data sparseness;variational bayesian posteriors;predictive classification;recognition results bayesian;data sparseness experimentally;using bayesian predictive;variational bayesian;predictive classification referred", "pdf_keywords": ""}, "04d96a75b4383240cb15fb729b29f5775219d724": {"ta_keywords": "deep learning library;neural automatic speech;speech recognition ar;toolkit based deep;automatic speech;automatic speech recognition;speech recognition;deep learning;learning library pytorch;end neural automatic;machine translation toolkitfairseq;neural machine translation;recognition ar toolkit;pytorch popular neural;library pytorch;library pytorch popular;espresso open source;neural automatic;learning library py;popular neural machine;end neural;neural machine;end end neural;based deep learning;open source modular;library py;learning library;espresso;translation toolkitfairseq;machine translation", "pdf_keywords": "recognition speech translation;neural speech encoder;neural networks speech;speech encoder;implement novel speech;speech recognition ar;end neural machine;translation systems speech;speech encoder implemented;automatic speech;program speech recognition;end automatic speech;speech recognition novel;novel speech recognition;recognition text synthesis;improving speech recognition;neural machine translation;speech recognition text;speech recognition ts;neural speech recognition;neural systems speech;text synthesis;speech recognition speech;ts speech recognition;speech recognition;novel neural speech;speech translation;free speech recognition;neural speech;recognition speech"}, "312b12dd6aa558b92df3ddd9b1057aa80a0ad718": {"ta_keywords": "relation classification textual;shot relation classification;relation classification;relation classification exploiting;task relation classification;classification textual entailment;textual entailment datasets;exploiting relation descriptions;zero shot relation;relation classification pose;relation descriptions;textual entailment methods;textual entailment;entailment datasets;available textual entailment;328 relation descriptions;textual entailment formulation;entailment datasets enhance;shot relation;relation descriptions iii;relation descriptions 328;task textual entailment;entailment methods;classification textual;classification exploiting relation;relation;entailment formulation leads;descriptions 328 relation;entailment formulation;entailment methods consider", "pdf_keywords": ""}, "5885625fac055f4f8f47b0d6b5c026c8806896f0": {"ta_keywords": "retrospective analysis literature;retrospective analysis;results retrospective analysis;present results retrospective;results retrospective;method analyze data;analyze data;analysis literature use;analysis literature;retrospective;analysis;method analyze;new method analyze;article present results;present results;analyze;data;literature use;results;literature use new;purpose article present;article;purpose article;article present;literature;use;method;new method;new;use new", "pdf_keywords": ""}, "098076a2c90e42c81b843bf339446427c2ff02ed": {"ta_keywords": "deep learning influence;learning influence functions;influence functions;influences influence functions;influence functions understood;influence functions fragile;learning influence;influence functions defined;introduction influence functions;group influences influence;estimating group influences;influences influence;fragile deep learning;influence;deep learning;group influences;convex loss functions;deep learning non;convexity underlying loss;influences;learning non convex;convex loss;underlying loss function;non convex loss;context deep learning;loss functions;loss functions methods;loss function generally;functions fragile deep;loss function", "pdf_keywords": "deep networks influence;influence functions deep;influence functions imagenet;deeper influence estimates;networks influence estimates;network deeper influence;influence estimations network;influence estimations influence;influence estimates sensitive;influence estimations;analyze deep networks;influence estimates influence;influences influence functions;influence functions generally;influence functions;proper influence estimations;influence estimates;influence estimation quite;influence estimation;influence functions fairly;influence functions defined;functions deep learning;influence estimates certain;estimation influence estimates;interpreting influence functions;influence estimates observe;loss functions influence;influence estimates significantly;evaluating influence estimates;networks influence"}, "446f1eaec22a90574670491073cd5b03bfa1e273": {"ta_keywords": "statistical properties countries;claims statistical properties;simple claims statistical;learn supervised models;supervised models;identifies simple statistical;claims statistical;supervised models develop;properties countries freebase;statistical properties;annotating data;supervised baseline approach;supervised baseline;annotating data property;16 statistical properties;knowledge base raw;countries freebase approach;knowledge base;countries freebase;information text;numerical information text;distantly supervised baseline;statistical;supervised;text methodsin experiments;simple statistical;using knowledge base;statistical properties problem;instead annotating data;information text following", "pdf_keywords": ""}, "76468db928e18f97dadbd25c04c80ebd491fec9b": {"ta_keywords": "zircon compounds nd0;compounds nd0 transition;nd0 transition metal;structure pyrochlore zircon;pyrochlore zircon compounds;compounds nd0;transition metal cations;pyrochlore zircon;structure structure pyrochlore;structure pyrochlore;zircon compounds;metal cations important;nd0 transition;cations important structure;transition metal;metal cations;cations important;nd0;pyrochlore;cations;important structure structure;zircon;compounds;important structure;structure structure;metal;transition;structure;important", "pdf_keywords": ""}, "aaf7e94e1a2f8891e5c5f4d11d4f135a1687bb0a": {"ta_keywords": "traffic flow macroscopic;assess traffic flow;assessment traffic flow;traffic flow;assess traffic;tool assess traffic;macroscopic models traffic;traffic flow important;assessment traffic;models traffic;panel assessment traffic;traffic;flow macroscopic models;electronic medicine application;use panel assessment;flow macroscopic;electronic medicine;field electronic medicine;panel assessment;medicine application;tool assess;flow;flow important;new tool assess;macroscopic models;flow important issue;medicine application new;models;use panel;assess", "pdf_keywords": ""}, "c589c4ec7247980f38a6bd22f215fea8028a0f66": {"ta_keywords": "annotations interpretable nonlodosis;human annotations interpretable;annotations interpretable;examining human annotations;interpretable nonlodos;rationales datasets human;methods interpretable nonlodos;interpretable nonlodos explain;human annotations;human annotation;interpretable nonlodosis;extracting evidence rationale;truth rationales datasets;interpretable nonlodosis explanation;datasets human annotation;explanation methods interpretable;evidence rationale input;rationales datasets;annotations;methods interpretable;rationale input texts;annotation;nonlodosis explanation methods;datasets rationales;interpretable;datasets rationales released;evidence rationale;decision extracting evidence;truth rationales;extracting evidence", "pdf_keywords": "turkers provide annotations;analyze human annotations;details crowdsourced annotation;crowdsourced annotation;crowdsourced annotation process;human annotation results;annotators datasets surprisingly;produce reasonable annotations;agreed annotators datasets;reasonable annotations;annotation process largely;annotation results annotators;annotation results conduct;annotators giving;human annotations;annotations compared;human annotations use;annotators annotators;annotations agreed annotators;make annotations agreed;annotators datasets;reasonable annotations aim;agreed annotators;human annotations obtained;datasets human annotations;quality annotations;human annotation;annotating;provide annotations;annotating text developed"}, "ecc520794da34d2b141235002c70b06c999bda73": {"ta_keywords": "text representation sense;sense specific embeddings;existing sense representations;context text representations;sense representations;text representations;text representations critical;representation sense specific;inspecting language sense;word embeddings;text representation;language sense;type existing sense;sense representations uniformly;word sense;reflect word sense;prototype word embeddings;embeddings reflect word;representation sense;natural language processing;existing sense;word sense sentence;sense sentence;sense sentence better;modern natural language;natural language;word embeddings tied;sense specific;language processing;embeddings", "pdf_keywords": ""}, "c12e46d7d0bb9fa062bce0549f2a6a9de00758d5": {"ta_keywords": "supervised sound event;sound event detection;sound event detectingion;introductionweakly supervised sound;supervised sound;sound event;transformer weakly supervised;self attention modules;detectingion self attention;novel sound event;event detectingion self;attention modules;attention mechanism transformer;weakly supervised learning;weakly supervised;attention modules allowing;event detection;multiple self attention;incorporates self attention;event detection method;event detectingion;self attention mechanism;introductionweakly supervised;self attention;attention;utilizes transformer encoder;transformer encoder;transformer weakly;input feature;attention mechanism", "pdf_keywords": ""}, "f2fb4a931580c4f1c5bdb47ebc80c801b422cd1a": {"ta_keywords": "grounding language;semantics based agent;logic grounding language;grounding language research;task evolving neural;semantics based;experimenter lexicon grammar;logic grounding;control logic grounding;agent perform tasks;neural network understand;evolving neural;semantics;english commands understand;experimenter lexicon;control logic;instructed experimenter lexicon;neural;evolving neural network;language;agent perceptions actions;english commands;perform tasks interact;tasks interact;task evolving;language research;based agent perceptions;lexicon grammar kept;based agent;simple english commands", "pdf_keywords": ""}, "57b972ebe314cfe8e57fd6b9f9239123eb70e979": {"ta_keywords": "introductionconnectionist temporal classification;temporal classification cc;networks rnns;recurrent neural networks;temporal classification;neural networks rnns;rns encoder;speech recognition;rnns;instead rns encoder;networks rnns explore;automatic speech;rnns explore deep;automatic speech recognition;speech recognition typically;recurrent neural;based recurrent neural;introductionconnectionist temporal;sequence prediction;approach automatic speech;models based recurrent;popular sequence prediction;deep convolutional neural;rns encoder methodswe;encoder;convolutional neural networks;based recurrent;rnns explore;deep convolutional;sequence prediction approach", "pdf_keywords": "networks rnns;contextconnectionist temporal classification;recurrent neural networks;networks rnns explore;neural networks rnns;recurrent neural network;layer recurrent neural;rnns explore deep;sequence frame features;neural speech recognition;free speech recognition;cnns encoders ccc;bidirectional recurrent neural;neural encoder;network use recurrent;temporal classification ccc;single layer recurrent;rns encoder;unit bidirectional recurrent;recurrent neural;use neural encoder;use recurrent neural;encoder use neural;neural speech;bidirectional recurrent;rnns;speech recognition;cnns encoders;developments neural speech;neural encoder use"}, "6f79cbe893ae46a6f97617d14656ab57c26e6faf": {"ta_keywords": "directional automatic speech;multi speaker data;speaker data end;speech recognition ar;speaker data;field multi speaker;automatic speech recognition;speech recognition;microphone array;automatic speech;multi speaker;respect microphone array;microphone array defined;sources respect microphone;microphone;e2e neural network;respect microphone;neural network manner;far field multi;directional automatic;neural network;speaker;recognition ar;recognition ar methods;handling far field;end e2e neural;called directional automatic;far field;e2e neural;azimuth angle sources", "pdf_keywords": "separation speech recognition;localization separation speech;directional automatic speech;separation speech sources;multispeaker data end;multispeaker data;far field multispeaker;speech recognition ar;obtain speech separation;processing speech acoustics;speech extraction network;speech sources method;speech recognition source;multispeaker;models source speaker;speech separation speech;field multispeaker;field multispeaker data;speech separation;speech recognition location;speaker locations methodsind;guided speech extraction;multichannel overlapped speech;source speaker;backgrounddirectional automatic speech;field speech recognition;speech recognition multi;overlapped speech recognition;source speaker locations;speech recognition"}, "5cb74e269c57263d475734a66d34e4d2d2f9e1ac": {"ta_keywords": "truss core panels;additive manufacturing lam;vibration characteristics core;laser additive manufacturing;constructed using lam;particular truss core;truss core;structures particular truss;finite element method;resultsthe vibration characteristics;panels base laser;vibration characteristics;manufacturing lam;core panels base;manufacturing lam subject;element method fim;lam compared modal;particular truss;test resultsthe vibration;core geometries tetrahedral;base laser additive;additive manufacturing;core panels;study finite element;core geometries;laser additive;vibration;resultsthe vibration;using lam compared;truss", "pdf_keywords": ""}, "cefd3993db4d065b95ab8f105452fb728c02b60e": {"ta_keywords": "automate scientific reviewing;scientific reviewing discusses;scientific reviewing;reviewed scientific publications;peer reviewed scientific;review paper;automate scientific;reviewed scientific;reviewing;review paper laborious;reviewing discusses;question automate scientific;reviewing discusses possibility;peer reviewed;publications time review;scientific publications;time review paper;growth peer reviewed;quality reviews growing;review;reviews growing;scientific publications time;time review;automate;quality reviews;high quality reviews;reviewed;reviews;question automate;reviews growing number", "pdf_keywords": "automate scientific reviewing;automatic review generation;automated reviews generated;automated reviews;generating reviews scientific;automatic review;generate reviews scientific;evaluation generated reviews;review generation automatic;scientific peer review;scientific reviewing;peer reviewing;human reviewers reviewers;helping reviewers quickly;generated human reviews;human reviews generated;reviewing review generation;scientific paper reviewer;assistant automatic review;peer reviewing review;scientific review;approach generating reviews;human reviewers systems;review generation using;generating reviews;review generation;scientific review provide;human reviewers;review generation systems;generate quality review"}, "f491a5f09ee01436d772a6cff25f22d700d8c9c0": {"ta_keywords": "power distribution networks;nas active management;distribution networks dens;networks dens planned;transport sector electricity;operated traditionally nas;provision network support;demand management automation;electricity renewable energy;demand passively renewable;passively renewable generation;provision network;nas active;sector electricity renewable;automation provision network;electricity renewable;nas place support;sector electricity;network support services;distribution networks;power distribution;renewable generation storage;generation storage demand;renewable energy;way power distribution;networks dens;network support;storage demand management;demand management;dens planned", "pdf_keywords": ""}, "72213e24713264da816f43a42d606f115998fe7b": {"ta_keywords": "stacked graphical learning;graphical learning stacked;learning stacked graphical;graphical learning base;collective classification relational;learning stacked;graphical learning;slacked graphical learning;graphical learning efficient;collective classification;stacked graphical;classification relational datasets;classification relational;method collective classification;relational datasets;relational datasets thesis;learning scheme called;learning base;learning scheme;classification;base learner;expanded feature sets;base learner applied;learning base learner;studied learning scheme;feature sets;stacked;learning efficient method;learning efficient;datasets thesis studied", "pdf_keywords": ""}, "c04c865c8b33ce0251c9f37d0cccf2b3b1e4fd34": {"ta_keywords": "learning causal states;causal state representations;approximate causal states;causal states optimally;causal states partially;learning causal;partially observable markov;extracts causal state;mechanisms approximate causal;causal states;introduction learning causal;algorithm extracts causal;approximate causal;causal state;states partially observable;observable markov decision;partially observable environments;observations partially observable;actions observations partially;partially observable;state representations rns;observable markov;markov decision processes;extracts causal;state representations;markov decision;causal;history actions observations;observations partially;joint history actions", "pdf_keywords": "planning context;planning context planning;partiallyobservable markov decision;planning context planningwe;planning;partiallyobservable markov;learning latent plans;learning causal state;context planning;learning causal states;observations partiallyobservable markov;context planning context;markov decision processeswe;model planning context;reinforcement learning tasks;learn causal states;causal states reconstruction;markov decision processes;model planning;causal state representations;state abstractions;approximate causal states;task agnostic state;tasks partially observable;histories observable environments;gridworld navigation tasks;causal states environments;context planningwe;observable environments causal;environments causal states"}, "8504a5eb4638aeb2f61f8b7f93440b9e495b443b": {"ta_keywords": "sensor activation centralized;centralized tracking time;dynamic sensor activation;tracking process known;tracking time varying;tracking process;centralized tracking;dynamic sensor;activation centralized tracking;cyberphysical systems;cyberphysical systems article;problem cyberphysical systems;tracking;approach tracking process;time varying process;tracking time;process known distribution;sensor activation;problem dynamic sensor;active sensors;approach tracking;cyberphysical;present approach tracking;sensor;problem cyberphysical;sensors;common problem cyberphysical;activation centralized;varying process;number active sensors", "pdf_keywords": ""}, "8e7d063c681557c94382ff3da6415d3720fe11a7": {"ta_keywords": "staance detection bidirectional;encoding tweet target;bidirectional conditional encoding;detection bidirectional conditional;representation tweet dependent;encoding tweet;outperforms encoding tweet;context staance detection;tweet dependent target;detection bidirectional;bidirectional encoding;bidirectional encoding results;conditional encoding;conditional lsm encoding;staance detection;representation tweet;conditional encoding methods;tweet target;augmented bidirectional encoding;tweet target independently;tweet dependent;bidirectional conditional;builds representation tweet;lsm encoding;tweet;encoding;conditional lsm;demonstrate outperforms encoding;detection;encoding results", "pdf_keywords": "weakly supervised stance;detecting stance tweets;conditional stance tweets;representations stance detection;targetdependent representations tweets;target stance prediction;supervised stance detection;target stance detection;predict stance target;lstm predict stance;detectingion twitter dataset;staance detectingion twitter;stance tweets targets;tweets crucially representations;learn stance tweets;stance prediction;representations tweets crucially;predict stance;tweet lstm predict;stance target tweet;model stance detection;detectingion twitter;stance tweets;stance detection improve;tweets targets;representations tweets;tweets targets given;successfully weakly supervised;tweets target;stance detection useful"}, "8568f6eda2e4cb7921fe175ab44b2f5ecbb2b870": {"ta_keywords": "disrupt natural reading;natural reading methodsprevious;objectiveintuitively human readers;natural reading objectiveintuitively;human readers cope;human readers;readers cope easily;reading objectiveintuitively human;increased reading times;increased reading;natural reading;letter transpositions;letter transpositions result;reading methodsprevious work;reading methodsprevious;reading;reading objectiveintuitively;reading times;backgroundintuitively human readers;text typos;text typos misspelling;misspelling word substitutions;readers;indicates letter transpositions;typos misspelling word;result increased reading;misspelling;typos misspelling;misspelling word;readers cope", "pdf_keywords": ""}, "ae77189921ffade5ee4c4d4a0e93e879d7280b80": {"ta_keywords": "dialogue pose forecasting;forecast avatar pose;multimodal forecasting body;forecasting body pose;avatar pose gestures;accurately forecast avatar;pose forecasting;forecast avatar;model multimodal forecasting;multimodal forecasting;narrative dialogue pose;gestures virtual avatars;dialogue pose;avatar actions dialogue;avatar pose;pose forecasting resultsin;body pose gestures;forecasting body;avatar actions;body pose;pose gestures virtual;pose gestures;virtual avatars methoda;pose;avatars methoda;view avatar actions;virtual avatars;model multimodal;end model multimodal;avatars methoda novel", "pdf_keywords": ""}, "93e012cbf8e29aacb9654313250a81d53bbcbdf2": {"ta_keywords": "email information leaks;email leak potentially;email leak;email raised privacy;single email leak;prevent email information;prevent email;widespread use email;issue prevent email;email information;information leaks message;information leaks;use email;recipients;email;email raised;single email;use email raised;non desired recipients;message accidentally addressed;recipients increasingly;privacy concerns critical;raised privacy concerns;privacy concerns;desired recipients;privacy;leaks message;leaks message accidentally;recipients increasingly common;raised privacy", "pdf_keywords": ""}, "df99459a75328393a9a989498db46ec445335724": {"ta_keywords": "privacy preserving release;review data privacy;peer review data;data privacy preserving;release peer review;privacy preserving;privacy preserving manner;protecting identities reviewers;review unavailability peer;peer review unavailability;unavailability peer review;improving peer review;review data release;peer review;data privacy;sensitivity peer review;techniques release peer;data terms protecting;privacy;framework privacy preserving;review data;release data;release peer;propose framework privacy;identities reviewers;protecting identities;data release data;review unavailability;preserving release certain;identities reviewers authors", "pdf_keywords": "privacy peer reviews;peer review data;review data privacy;compromising privacy review;data peer review;privacy review process;preserving data reviewers;privacy review;privacy preserving data;algorithm privacy peer;improving privacy utility;release peer review;improve privacy utility;evaluation algorithm privacy;privacy peer;data privacy preserving;improving peer review;protecting identities reviewers;privacy preserving manner;histograms peer review;privacy preserving;peer review use;improves privacy utility;algorithm improve privacy;peer review;peer reviews;output privacy preserving;structures peer review;improving privacy;privacy utility"}, "c4bc2f7e04e02107aa6eaa0c811c3c046efbbc14": {"ta_keywords": "inductive logic programming;finite automata dfs;learning structured examples;finite automata;automata dfs;automata;inductive logic;including inductive logic;deterministic finite automata;hard cryptographic;alphabet examples deterministic;logic programming methodswe;automata dfs string;logic programming;learning structured;learning problem hard;df learning;problem concepts strings;structured examples;simple learning;fixed alphabet examples;cryptographic;learning problem concepts;problem hard cryptographic;introduction learning structured;structured examples necessary;learning problem;introduction learning;alphabet examples;simple learning problem", "pdf_keywords": ""}, "69b184f62c97513b03deed96a1443f79b34af0d7": {"ta_keywords": "bid manipulation attacks;bidding assignment robust;robust bid manipulation;reviewing robust bid;bid manipulation;robustness dishonest reviewers;assignment robust attacks;paper bidding assignment;robust bid;bidding assignment;paper bidding;manipulation attacks;approach paper bidding;dishonest reviewers collude;provides robustness dishonest;robustness dishonest;assignment robust;dishonest reviewers;bidding;manipulation attacks aim;attacks methods empirically;reviewers collude;robust attacks;paper reviewing robust;reviewers collude knowledge;bid;robust attacks methods;attacks methods;manipulation;reviewing robust", "pdf_keywords": "adversarial reviewer target;adversarial reviewer;allocate adversarial reviewer;adversarial reviewers;adversarial reviewers review;allow adversarial reviewers;reviewers adversarially;reviewers adversarially influence;reviewer driven bidding;dishonest reviewers adversarially;manipulation bids reviewer;adversarially influence paper;bidding reviewer driven;bidding assign reviewers;bidding behavior reviewer;detecting malicious reviewers;bidding reviewer;reviewers bids colluding;fake reviewer manipulate;bidding reviewer demonstrate;bids reviewer;reviewers bids;bids reviewers;demonstrate fake reviewer;bids reviewer design;reviewer manipulate assignment;malicious reviewers using;malicious reviewers;model bidding reviewer;adversarially influence"}, "b21fa4f31c4813444e50259dfbe2c56660161174": {"ta_keywords": "amplification amplification amplification;amplification amplification;amplification;translate resulting amplification;resulting amplification amplification;resulting amplification;ability translate resulting;ability translate;ability;translate resulting;translate;resulting", "pdf_keywords": ""}, "89fd287f7eacc7a40d0216ba3b919812da658b94": {"ta_keywords": "optimizing model hyperparameters;model hyperparameters optimized;hyperparameter tuning framework;hyperparameters optimized optimizing;model hyperparameters;hyperparameters optimized;hyperparameter tuning;appropriate hyperparameter tuning;model hyperparameters population;autolfd;use autolfd;demonstrate model hyperparameters;hyperparameters;framework combines regularization;hyperparameters population based;hyperparameter;combines regularization strategy;ensure appropriate hyperparameter;regularization strategy;tuning framework;regularization strategy coordinated;appropriate hyperparameter;autolfd ensure appropriate;regularization;hyperparameters population;combines regularization;autolfd ensure;framework optimizing model;tuning framework combines;optimized optimizing model", "pdf_keywords": "latent dynamics neural;neuronal time series;learning latent dynamics;resolution neuronal time;relevant neural dynamics;throughput neural recordings;super resolution neuronal;temporal super resolution;neural recordings;latent dynamics learning;neural dynamics models;neural population recordings;tracing neural;neural dynamics;neurons embedded latent;backpropagation time;latent dynamics recover;latent dynamics data;selective backpropagation time;resolution neuronal;train deep generative;millions neurons increasingly;inference latent dynamics;trade offs neural;neural devices;dynamics neural;dynamics neural population;learn nonlinear latent;neural information;increasingly exploited neural"}, "c5950fa3ee124cf2dcb8783db6f582f49170fb45": {"ta_keywords": "generalization gap training;generalization machine learning;typically bound generalization;data produce generalization;generalization bounds augmenting;assess generalization machine;bound generalization gap;generalization machine;assess generalization;generalization gap;bound generalization;produce generalization;empirically holdout data;produce generalization bounds;generalization bounds;generalization;empirically holdout;validate empirically holdout;training plug empirical;labeled training;labeled training set;augmenting labeled training;empirical risk;bounds augmenting labeled;machine learning;machine learning scientists;purposeto assess generalization;risk obtain bound;bound true risk;randomly labeled data", "pdf_keywords": "generalization gap training;tight generalization guarantees;bound generalization gap;provide tight generalization;tight generalization;learning demonstrate bounds;nonvacuous deep learning;generalization guarantees;obtaining generalization guarantees;typically bound generalization;generalization machine learning;bound generalization;generalization bounds directly;empirically accuracy classifier;generalization gap;trained gradient descent;generalization neural networks;generalization neural;closely tracked generalization;generalization bounds;classification empirical error;learning models demonstrate;bound nonvacuous deep;neural networks learn;generalization machine;linear classifier trained;deep learning models;nonvacuous linear classifier;generalization guarantees clean;obtaining generalization bounds"}, "a7822238f5db7d62731eaeabf9725a65f4edf893": {"ta_keywords": "molecular basis language;understanding molecular;understanding molecular basis;molecular basis;step understanding molecular;molecular;identify domains domains;identify domains;interact aforementioned domains;basis language;domains domains domains;domains domains;domains;language interact;language interact aforementioned;used identify domains;language;basis language recently;aforementioned domains;ability language interact;domains key step;ability language;language used identify;aforementioned domains key;domains key;shown language;shown language used;language recently;language used;basis", "pdf_keywords": ""}, "9cdf512f273083efa1ea01f7b31daa97a7bbe884": {"ta_keywords": "distributed computation infrastructures;latency computation distributed;computation distributed infrastructures;distributed computation;coded computation emerged;overhead coded computation;modern distributed computation;computation infrastructures;computation distributed;coded computation;computation infrastructures plagued;replicating computation entails;computation emerged resource;replicating computation;latency computation;resource overhead coded;computation emerged;overhead coded;tail latency computation;solution replicating computation;computation entails;distributed infrastructures;distributed infrastructures simple;modern distributed;slow servers unavailabilities;create parity;distributed;computation;parity;encoded create parity", "pdf_keywords": "computational locality code;locality codes design;coded computation propose;coded computation computational;coded computation computation;locality code equivalent;novel coded computation;locality codes introduce;coded computation;perform coded computation;locality code;coded computation schemes;concept locality codes;locality codes define;considered coded computation;coded computation best;coded computation functions;locality codes;coded computation scheme;development coded computation;computation computational locality;computation locality;coded computation new;coded computation using;coded computation discuss;studied locality codes;needed coded computation;coded computation admits;coded computation function;coded computation aids"}, "6aeb477e5f0882f8363a3a8e5e6f83962d91edc6": {"ta_keywords": "accelerated future learning;future learning;future learning learning;rapidly prior learning;future learning use;prior learning;future learning paper;prior learning considered;future learning little;learning mechanisms yield;robust learning growing;learning learning proceeds;demonstrates accelerated future;learning proceeds effectively;learning learning;learning mechanisms;learning;learning considered;accelerated future;standing learning;standing learning mechanisms;effectively rapidly prior;robust learning;focused standing learning;measures robust learning;learning growing;learning proceeds;learning considered interesting;demonstrates accelerated;model demonstrates accelerated", "pdf_keywords": ""}, "39ab4b9cdeb4a71b3e25fe5339962654c7c9ff8c": {"ta_keywords": "false information spreading;false information spread;information spreading methods;identify nodes spread;contain false information;endorse false information;information spreading;information spread refutation;falsese information;false information proactively;information spread;nodes spread;falsese information true;false information;exist social networks;fact checking exist;background falsese information;social networks competing;spread refutation information;likely endorse false;nodes spread path;checking exist social;networks competing influence;proactively identify nodes;information fact checking;spreading methods;information true information;social networks;contain false;fact checking", "pdf_keywords": "false information spreading;credibility features node;analyzing credibility features;spread false information;false information spreaders;false information spread;credibility features social;graph neural networks;based graph neural;credibility trust features;false information spreader;analyzing credibility;aggregate credibility features;credibility trust information;graph neural network;credibility features aggregated;credibility features proportionally;prediction network spreaders;credibility features;advances graph neural;spreader false information;graph neural;integrates credibility trust;fake news detection;credibility trust;method analyzing credibility;node neighborhood credibility;proportionally credibility trust;using graph neural;spreader prediction network"}, "00799dceb9e7209bb9d71b38fa5b49483e886978": {"ta_keywords": "deep learning programs;existing deep learning;dataflow graph construction;dynamic network architectures;repeating dataflow graph;deep learning;dataflow graph;recent deep learning;neural network architectures;deep learning models;dynamic neural network;handling dynamic network;dynamic neural;dynamic network;sample existing deep;moving dynamic neural;incorporate graph optimization;network architectures substantial;dataflow;repeating dataflow;graph construction processing;graph optimization;network architectures;inefficient handling dynamic;network architectures structure;existing deep;static graph;graph optimization techniques;used static graph;learning models moving", "pdf_keywords": ""}, "ab57c70c14b82d07c40c75fecaac98b0e2dc0510": {"ta_keywords": "matching referent records;referent records known;referent records;matching referent;record linkage;names record linkage;large record linkage;linkage large record;semi supervised;semi supervised methods;labeled data;record linkage problems;record linkage large;records known names;unsupervised semi supervised;preferable supervised methods;labeled data available;supervised;available unlabeled data;supervised methods preferable;preferable supervised;supervised methods;methods preferable supervised;data available unlabeled;unlabeled data;task matching referent;records known;known names record;names record;little labeled data", "pdf_keywords": "record linkage classification;linkage classification task;linkage classification;probabilistic record linkage;record linkage hierarchical;record linkage unsupervised;matching record linkage;semi supervised;linkage hierarchical;linkage hierarchical mixture;supervised method record;labeled data;semi supervised methods;record linkage;perform record linkage;semi supervised method;present semi supervised;unsupervised semi supervised;matching referent records;linkage unsupervised;method record linkage;linkage large data;linkage large record;matching referent;referent records known;large record linkage;names record linkage;labeled data available;supervised;record linkage major"}, "6413e6a4f68be0ea6aed0082b205147d9f893699": {"ta_keywords": "inflection generation character;inflection generation task;inflection generation;backgroundmorphological inflection generation;generating inflected;problem inflection generation;generating inflected form;generation character sequence;particular linguistic transformation;linguistic transformation methodswe;linguistic transformation;morphologically rich languages;sequence sequence learning;backgroundmorphological inflection;sequence learning;variant neural encoder;inflection;neural encoder;neural encoder decoder;datasets morphologically;task generating inflected;seven datasets morphologically;datasets morphologically rich;character sequence sequence;character sequence;sequence learning problem;corresponding particular linguistic;inflected form;inflected;linguistic", "pdf_keywords": "inflection generation learning;translation inflection generation;inflection generation based;inflection generation task;machine translation inflection;generate inflectional morphology;code inflection generation;inflection generation character;inflection generation;encoder generate inflectional;model inflection generation;generate inflectional;segmentation inflection generation;inflection generation complementary;inflection generation model;learning model inflection;novel inflection generation;method inflection generation;inflection generation transform;generating inflected;framework inflection generation;generating inflected form;linguistic transformation methodswe;learn language model;language models;tool inflection generation;learn transformation semantics;character language model;morphology generation;approach morphology generation"}, "8dd85c3a3700d0d282ddbd4dff5e238f24c00676": {"ta_keywords": "speech spectral envelope;modelling speech spectral;mixture modelling histogram;gassians mixture modelling;modelling speech;method modelling speech;speech spectral;mixture modelling;framework gassians mixture;envelope using mixture;spectral envelope using;bayesian framework gassians;variational bayesian framework;spectral envelope;novel variational bayesian;variational bayesian;using mixture gassians;mixture gassians;modelling histogram;mixture gassians mog;gassians mixture;methoda novel variational;modelling histogram enables;using mixture;histogram representation;parameter distributions model;variational;gassians mog methoda;novel variational;resultsa histogram representation", "pdf_keywords": ""}, "74dccd379776bbb50b352c19b8caf2a7896d58ee": {"ta_keywords": "sensitivity based coherency;coherency analysis method;based coherency analysis;coherency analysis;power optimisation problems;coherency analysis large;power optimisation;methodsa sensitivity based;coherent constrained variables;identifying coherent constrained;large power optimisation;based coherency;constrained variables;constrained variables included;coherent constrained;described methodsa sensitivity;methodsa sensitivity;optimisation problems described;optimisation problems;optimisation;coherency;method identifying coherent;sensitivity based;introductionapplication sensitivity based;formulations large power;introductionapplication sensitivity;constrained;analysis method identifying;analysis large power;problems described methodsa", "pdf_keywords": ""}, "6a3cc30d5d6342d912851deb4362b8c47fa5ede3": {"ta_keywords": "backgroundnul models exploit;backgroundnul models;self debiasing framework;debiasing framework;debiasing framework prevents;models exploit biases;utilizing biases knowing;debiasing methods;utilizing biases;introducing self debiasing;self debiasing;debiasing;proposed debiasing methods;backgroundnul;mainly utilizing biases;biases knowing;debiasing methods shown;biases knowing advance;exploit biases;recently proposed debiasing;proposed debiasing;biases achieve;biases;biases achieve high;high dataset;performance properly learning;achieve high dataset;high dataset specific;exploit biases achieve;dataset specific performance", "pdf_keywords": "biases new dataset;datasets self debiased;dataset anti biases;dataset biases;debiased models outperform;debiasing model lexical;model existing debiasing;bias self debiasing;unknown dataset biases;debiased models effective;dataset biases omitting;models debiased;self debiased models;datasetwe bias based;models debiased using;debiased models;models exploit biases;bias models performance;performance datasetwe bias;datasetwe bias;pre trained models;existing debiasing methods;demonstrate debiased models;debiased models used;biased examples trainingwe;existing debiasing;identification debiased models;bias models;debiased models able;unknown biases models"}, "51f6654b9d5925002ccaa5cd339b4377b96719ce": {"ta_keywords": "workstation users clustering;ongoing activities workstation;users clustering;activities workstation users;user activities;user activities automatically;users clustering email;activities workstation;activities workstation user;clustering email;collection user activities;introductioninferring ongoing activities;workstation user committees;activities automatically;clustering email methodswe;activities automatically inferred;workstation users;ongoing activities;automatically discovering;interested automatically discovering;clustering;key ongoing activities;workstation user;users work;activities;workstation;workstation resultsthe thesis;based contents workstation;contents workstation;user committees", "pdf_keywords": ""}, "f4906089f0720c83e57e4a46ae75283df4d67e5a": {"ta_keywords": "funding peer evaluation;peer review selection;peer evaluation;peer evaluation increasingly;peer review;effectiveness peer review;review selection foundational;grading moos;funding peer;review selection;scale grading moos;proposals funding peer;effectiveness peer;procedure candidates rated;grading moos settings;grading;evaluation increasingly popular;scale grading;evaluation increasingly;peer;necessary scale grading;candidates rated according;selection foundational;scientific funding;subset proposals funding;process scientific funding;scientific funding bodies;candidates rated;rated according opinions;funding bodies selected", "pdf_keywords": ""}, "05f8dd59d4184d38e240bdea4d58e424b8cd055c": {"ta_keywords": "persuasion deliberation online;persuasion deliberation;change persuasion deliberation;opinion change persuasion;deliberation online;change persuasion;deliberation online remain;shaping opinions drive;research examines persuasive;examines persuasive power;persuasion;persuasive power;examines persuasive;shaping opinions;opinions drive;introductiondeliberation individuals online;opinions drive votes;persuasive;deliberation;persuasive power textitethos;opinion change;votes purchases critical;determinants opinion change;role shaping opinions;textitethos individual reputation;online remain largely;drive votes purchases;votes purchases;behavior determinants opinion;critical offline behavior", "pdf_keywords": "persuasion deliberation online;persuasion analyse debates;reputation persuasion significantly;impact reputation persuasion;persuasiveness deliberation online;persuasion deliberation;reputation persuasion moderated;debate reputation useful;change persuasion deliberation;debate reputation;reputation impacts persuasiveness;effects reputation debate;reputation persuasion;reputation persuasion documented;effect reputation persuasion;debates persuasive power;opinion change persuasion;persuasion effect reputation;reputation persuasion effect;reputation debate success;persuasion use reputation;debates persuasive;inference effects reputation;reputation challenger debate;online argumentation;opinion variation approximating;effect reputation debate;impacts persuasiveness deliberation;successful debates persuasive;persuasion challenger influence"}, "59f41c5024a238ae8843f3dd67692961ecc63e75": {"ta_keywords": "neural networks dns;deep neural networks;deep neural;dn systems skilled;dn systems;expertise dns;performance systems deep;networks dns;networks dns constructed;development dn systems;understanding expertise dns;neural networks;deep understanding expertise;expertise dns necessary;systems deep;systems deep understanding;deep understanding;development dn;learning rate layer;performance speech processing;dns constructed;dns constructed considering;background deep neural;speech processing;dns;learning rate;hidden states learning;neural;dns necessary limits;skilled experts methods", "pdf_keywords": ""}, "f9e32b30fd9ad50cce12ffb753c7be88100b6dc2": {"ta_keywords": "crowd labeled data;denoising crowd labeled;model crowd labeled;aggregating denoising crowd;crowd labeled;denoising crowd;crowdsourcing;model crowd;labeled data;labeled data gained;based model crowd;crowdsourcing platforms;datasets propose permute;crowdsourcing platforms massive;crowd;massive datasets;advent crowdsourcing;massive datasets propose;labeled;aggregating denoising;advent crowdsourcing platforms;task aggregating denoising;labeled data significant;significance advent crowdsourcing;data significant generalization;datasets propose;datasets;minimax rates permute;platforms massive datasets;data", "pdf_keywords": "permutationbased model crowd;model crowdsourced labeling;crowd labeled data;model crowdsourced;crowdsourced labeling considerably;estimation permutation based;estimation permutation;crowdsourced labeling;method crowdsourcing majority;model crowd labeled;crowdsourcing majority;new method crowdsourcing;crowdsourced labels;crowdsourcing crowding;based model crowdsourced;method crowdsourcing;crowding crowdsourcing;crowdsourcing majority voting;crowdsourcing;denoising crowd labeled;case crowdsourcing crowding;crowdsourced;predictors crowd sourcing;predictors crowd;aggregating denoising crowd;use crowdsourcing crowding;estimator provided permutation;crowdsourcing crowding use;crowdsourcing useful tool;crowdsourcing important tool"}, "15ac2d8629ca9241ea558eb2b816272d82447ac7": {"ta_keywords": "crowdsourcing learning form;challenges crowdsourcing learning;crowdsourcing learning;crowdsourcing;data science crowddsourcing;science crowddsourcing data;crowddsourcing data;crowddsourcing data collected;challenges crowdsourcing;critical challenges crowdsourcing;science crowddsourcing;crowddsourcing;learning people;collected non experts;learning form people;people developing algorithms;background learning people;learning estimation;data collected;learning estimation operate;designing incentive;incentive mechanisms;incentive mechanisms elicit;people;learning;incentive;form people;designing incentive mechanisms;non experts online;data collected non", "pdf_keywords": ""}, "18ddcd250bbbe716a0616412ea329a8343f60542": {"ta_keywords": "incentive compatible payment;worker incentive compatible;incentive compatible;worker incentive;exists incentive compatible;assume exists incentive;case worker incentive;exists incentive;incentive;payment function prove;compatible payment function;payment function assume;payment function;compatible payment;payment;worker;consider special case;present case worker;case worker;article present case;present article;function prove contradiction;article present;prove contradiction let;special case;present article present;present case;contradiction let consider;contradiction let;prove contradiction assume", "pdf_keywords": ""}, "c18700ed4ef07dd85ba8bceab3b9584c6e6af49c": {"ta_keywords": "search new technologies;new tool search;web important resource;search new information;tool search;tool search new;new technologies technologies;new information resources;new technologies;technologies developed new;technologies technologies developed;development new technologies;technologies technologies;search new;web;web important;technologies developed;technologies;search;resource development new;information resources;important resource development;resource;resource development;developed new tool;new tool;resources;development new;new information;important resource", "pdf_keywords": ""}, "af4e11436268cf68505f1caee5d6f7ff0df9c99a": {"ta_keywords": "causal inference language;learn causal relationships;learn causal;social sciences causality;research learn causal;causal relationships;causal inference;sciences causality;causality;causal;causal relationships despite;causality importance natural;sciences causality importance;causality importance;convergence causal inference;natural language processing;emphasis predictive tasks;importance natural language;research convergence causal;inference language processing;language processing nl;convergence causal;natural language;predictive tasks;predictive tasks distinction;inference language;language processing;emphasis predictive;placed emphasis predictive;language processing methods", "pdf_keywords": "text data causal;causality non linguistics;language causality provide;language causality;linear language causality;causality provide language;causal inference language;language result causal;nlls identify causal;causal inference text;causality improve nonlack;research causality nonl;literature causal inference;causal conclusions text;causal explanations text;improve nonlack causal;nonlack causal inference;causality emphasize challenges;current literature causal;causal interpretations data;causality discuss challenges;literature causal;causality nonl;causality nonlack;causality improve;natural language tasks;processing research causality;causal inference traditional;causal inference poorly;nonlack causal"}, "f8b32c2edcd7ef098ce40b7fd2e68448ac818191": {"ta_keywords": "reading using eye;text using eye;unknown word detection;eye tracking features;using eye tracking;word detection;gaze duration word;native language reading;eye tracking;detect unknown words;using eye gaze;word detection non;eye gaze;language reading using;native language text;utilizes gaze duration;gaze duration;reading non native;language reading;gaze;words natural reading;utilizes gaze;approach utilizes gaze;eye gaze aim;reading using;native language;gaze aim;detection non native;natural reading;using eye", "pdf_keywords": ""}, "11e4346e60ac76ad018231a851fbbdb2112044d2": {"ta_keywords": "phrase level veracity;interpretable fact verification;veracity phrases;level veracity phrases;likewikipedia existing neural;verification claim phrase;verify veracity;veracity phrases serves;fact verification methods;statement verify veracity;natural language statement;claim phrase level;verify veracity large;approach interpretable fact;veracity large scale;neural models make;textual knowledge;textual knowledge source;oren approach interpretable;knowledge source likewikipedia;natural language;fact verification;claim phrase;decompose verification claim;large scale textual;interpretable fact;given natural language;source likewikipedia existing;likewikipedia;scale textual knowledge", "pdf_keywords": "phrase veracity predictions;phrase veracity boost;claim phrases accuracy;regularized reasoninging interpretable;verify textual claim;accurate phrase veracity;verification phrasal veracity;predictions claim phrases;accurate phrasal veracity;natural language inference;predict veracity claim;phrasal veracity predictions;phrase veracity improved;new prediction veracity;verification claim phrase;claims evidence sentences;predict veracity;construct phrasal veracity;prediction veracity;claim phrases;veracity predictions explanations;interpretable fact verification;regularized reasoninging;likewikipedia existing neural;prediction veracity use;textual claim;claims trustworthy knowledge;claim phrase level;reasoninging interpretable fact;fact verification decompose"}, "bcbac71ac64cd6a6aaae41e37ebe960f508ab741": {"ta_keywords": "neural knowledge model;subsymbolic neural knowledge;question answering;intensive question answering;question answering tasks;neural language model;training manipulating symbolic;factual information subsymbolic;neural knowledge;information subsymbolic neural;messagewe develop neural;answering tasks;neural language;knowledge intensive;knowledge intensive question;information subsymbolic;knowledge model dramatically;key clinical messagewe;answering tasks interestingly;clinical messagewe develop;manipulating symbolic representations;subsymbolic neural;knowledge model;develop neural language;clinical messagewe;language model;interpretable factual information;symbolic representations;symbolically interpretable factual;answering", "pdf_keywords": "question answering knowledge;knowledge neural reasoning;answering knowledge base;intensive question answering;question answering;question answering integrating;answering knowledge;knowledge neural;knowledge base semantic;reasoning natural language;question answering tasks;neural knowledge model;symbolic knowledge graph;symbolic knowledge base;subsymbolic neural knowledge;novel fact memory;question answering task;fact memory;factual information subsymbolic;answer entities;model symbolic knowledge;information neural language;neural language model;predicts answer entities;question model capable;interfacing neural language;fact memory present;fact memory context;aware contextual embeddings;knowledge intensive"}, "1ca247158522991ad54cccaac6c6938576a8bd26": {"ta_keywords": "ir rivals neural;msmarco document ranking;document ranking leaderboard;marco document ranking;document ranking;ranking leaderboard 2020;ir achieved mr;ranking;ranking leaderboard;neural models msmarco;leaderboard short document;bert based models;ir achieved;leaderboard 2020;leaderboard 2020 12;inferior bert based;traditional ir achieved;ranking leaderboard short;contexttraditional ir rivals;leaderboard;rivals neural models;ir;traditional ir;describes traditional ir;06 inferior bert;rivals neural;contexttraditional ir;models msmarco document;inferior bert;bert based", "pdf_keywords": "rerank documents using;rerank documents;rerank rerank documents;document ranking leaderboard;document ranking;ranking searching information;ranking searching;marco document ranking;new approach rerank;approach rerank rerank;rerank rerank;rerank;approach rerank;ranking leaderboard;ranking;method ranking searching;information retrieval;ranking leaderboard 2020;information retrieval article;ranking leaderboard leonid;bert based models;leaderboard;lexical translation models;text matching;text matching important;searching information web;information web track;retrieval article;translation models;leaderboard 2020"}, "2eef9173946078c402596b9b080b6878db00b8ac": {"ta_keywords": "food twitter community;twitter community data;language food twitter;food twitter;detecting obesity risk;detecting obesity;twitter community;community data;diabetes melitus t2m;methods detecting obesity;community data peculiarities;twitter;obesity risk factor;obesity;t2m language food;diabetes melitus;type diabetes melitus;type diabetes;obesity risk;key clinical messagewe;diabetes;factor type diabetes;clinical messagewe;clinical messagewe discuss;language food;community;task language food;melitus t2m language;data;messagewe discuss methods", "pdf_keywords": ""}, "38bd034e6a0589bf1132d3e8c79818b271377290": {"ta_keywords": "loss differencing mammalian;error weighted discriminative;margin based weightedclassification;weighted discriminative training;discriminative training;weightedclassification error modeled;generalized error weighted;loss differencing;hinge loss modeled;based weightedclassification error;malformation mpe loss;discriminative training methodsusing;term margin based;loss modeled;based hinge loss;weighted discriminative;gin term margin;error weighted;loss modeled using;term margin;differencing mammalian functionals;based weightedclassification;weightedclassification error;margin space integration;weightedclassification;functionals generalized error;margin based;mpe loss differencing;hinge loss;mammalian functionals generalized", "pdf_keywords": ""}, "3429a6b440fb6f71990bbeda9d097d709634a913": {"ta_keywords": "improve parser accuracy;parser output training;parser accuracy parsing;improve parser;parser accuracy;training uses parser;accuracy parsing;accuracy parsing greatly;known accuracy parsing;automatically generated parse;method improve parser;accuracy parsing errors;translation accuracy self;generated parse trees;generated parse;parser;translation accuracy;machine translation;parse;parsing;uses parser;machine translation known;affects translation accuracy;based machine translation;parse trees;parsing errors;translation known accuracy;parse trees contribute;parser output;parsing greatly affects", "pdf_keywords": ""}, "9ce09b03f056253252f3e8c0c65d86a27117a0ac": {"ta_keywords": "test time adaptation;unlabeled test data;adaptation supervised model;confronts unlabeled test;adaptation supervised;unlabeled test;labeled training data;time adaptation supervised;labeled training;adaptation model confidence;supervised model confronts;minimization approach adaptation;test data;data testing model;test time;training data;test data different;data testing;supervised;fully test time;training data propose;model confronts unlabeled;supervised model;different data testing;testing model;testing model adapt;help labeled training;adaptation model;objective measured entropy;time adaptation", "pdf_keywords": ""}, "2db020e3398c06e3a22f12d8caffe76b0d9d1dda": {"ta_keywords": "shot question answering;question answering;answering commonsense tasks;language models training;question answering commonsense;commonsense tasks guided;commonsense tasks;pre training models;answering commonsense;language models;pre existing knowledge;answering;neuro symbolic framework;models training;existing knowledge resources;clinical messagewe propose;tasks guided;set language models;knowledge resources form;clinical messagewe;training models;key clinical messagewe;existing knowledge;neuro symbolic;tasks extending prior;knowledge;training models vary;novel neuro symbolic;tasks;effective pre training", "pdf_keywords": "question generation techniques;question answering benchmarks;knowledge neural models;question answering;question generation;knowledge neural;knowledge commonsense learning;sources question generation;development question answering;language models learned;generating questions answers;gs question generation;pretraining language models;development commonsense reasoning;question answering commonsensebased;generating questions;knowledge context language;commonsense knowledge graphs;reasoning commonsense reasoning;question answering ability;general semantic reasoning;commonsensebased pre training;various knowledge neural;semantic reasoning contrast;knowledge bases commonsense;commonsense reasoning commonsense;shot transfer commonsense;shot qa commonsense;language models commonsense;qa commonsense knowledge"}, "68dca6ee694f22e2af66b56e60fdfa74041242e6": {"ta_keywords": "srrna mediated srrna;srrna mediated srr;mediated srrna mediated;mediated srrna proposed;srrna mediated;based srrna mediated;mediated srrna;model srrna mediated;srrna proposed model;srrna proposed;model based srrna;model srrna;srrna;based srrna;new model srrna;mediated srr;srr;mediated;model based;proposed model based;proposed model;new model;model;based;new;proposed", "pdf_keywords": ""}, "a0f8733dd84608b3cad97904624f8bfdc2d2fcbf": {"ta_keywords": "medical equipment equipment;characteristics medical equipment;equipment equipment medical;medical equipment;importance medical equipment;equipment medical care;equipment medical;equipment equipment;equipment equipment discuss;equipment;equipment discuss importance;equipment discuss;characteristics medical;medical;importance medical;article characteristics medical;medical care;discuss importance medical;characteristics;article characteristics;importance;aim article characteristics;discuss importance;care;article;aim article;aim;discuss", "pdf_keywords": ""}, "890317710697a9e41d0d9961d99986c4d865393f": {"ta_keywords": "app usage learning;app usage representations;aware app usage;semantic aware app;app usage;app usage smooth;usage learning semantic;personalized mobile apps;aware app;mobile apps;apps;usage learning;proliferation personalized mobile;model app usage;app;characteristics app usage;apps non trivial;time locations apps;locations apps;personalized mobile;semantic aware;usage representations capture;usage representations;locations apps non;apps non;learning semantic aware;mobile;model app;mobile apps poses;apps poses", "pdf_keywords": ""}, "b6c4a96e09b9f11e7c70e7f1fbe3f3971b92762d": {"ta_keywords": "controlled text generation;text generation future;text generation;generating text;generation future discriminators;generating text distribution;discriminators generation;model generating text;discriminators generation fudige;future discriminators generation;text generation resultsgiven;propose future discriminators;controlled text;future discriminators;generating;method controlled text;generation fudige;text distribution fuge;future discriminators methodswe;text distribution;generation resultsgiven;existing model generating;model generating;generation resultsgiven pre;discriminators methodswe propose;generation;discriminators;generation future;backgroundfuge controlled text;text", "pdf_keywords": "text generation models;controlled text generation;controlling text generation;text generation tasks;text generation weighted;text generation;text generation future;generation text;approach generating text;generation text conditional;controlled generation text;generating text;generation weighted decoding;text generation using;text generation method;generating text domain;text generation set;pretrained language model;method generating text;generating text presented;language model learn;generation task train;conditional language model;language model controllable;manipulate language model;model controllable generation;approach controlled text;generation task;effective controlling text;effective language models"}, "97e8430fe01394ea9a49fd841d9aecdfc294a796": {"ta_keywords": "optimization fuzzy;fuzzy reasoning genetic;method optimization fuzzy;optimization fuzzy reasoning;classification myocardial heart;fuzzy rules using;parameters fuzzy rules;fuzzy reasoning;optimizing parameters fuzzy;classification myocardial;fuzzy rules;parameters fuzzy;genetic algorithms application;genetic algorithms;gas classification myocardial;application discrimination myocardial;reasoning genetic algorithms;discrimination myocardial heart;using genetic algorithms;heart disease ultrasonic;genetic algorithms gas;algorithms gas classification;fuzzy;discrimination myocardial;heart disease methodsa;disease ultrasonic images;myocardial heart;myocardial heart disease;rules using genetic;ultrasound images", "pdf_keywords": ""}, "33206a493e3519d27df968e98eb7fe6af14ef985": {"ta_keywords": "ocam incremental learning;creating memory causal;memory causal relationships;memory causal;events constructs causal;constructs causal;causal relationships describes;describes ocam program;relationships describes ocam;causal relationships;constructs causal model;methods ocam;describes ocam;methods ocam incremental;ocam;ocam incremental;ocam program;program methods ocam;causal;methods used ocam;ocam program methods;causal relationships mich;causally connected events;incremental learning;causal model processes;sequences causally;causally;examples sequences causally;causal model;learning methods used", "pdf_keywords": ""}, "89440a4cb27d17ced5d54bbe0f81f3477bf16404": {"ta_keywords": "speech recognition recognize;automated speech recognition;speech recognition;automatic speech recognition;discrimination automatic speech;speech recognition article;ability automatic speech;automated speech;automatic speech;present kernelel machine;kernelel machine derived;dis discrimination automatic;development automated speech;kernelel;recognition recognize;kernelel machine;present kernelel;recognition recognize fundamental;discrimination automatic;recognition article present;article present kernelel;recognition article;recognition;minimum relative detropy;detropy dis discrimination;relative detropy dis;speech;machine derived minimum;relative detropy;dis discrimination", "pdf_keywords": ""}, "27dafb0b6076e050c31ede8d3e0184ef3592b364": {"ta_keywords": "metal organic framework;framework superior capacitive;organic framework superior;existing metal organic;organic framework;capacitive capacity aetiology;metal organic;superior capacitive capacity;superior capacitive;capacitive capacity;capacitive;framework superior;framework;organic;existing metal;case existing metal;metal;capacity aetiology aetiology;capacity aetiology;aetiology discussed;aetiology aetiology discussed;aetiology;aetiology aetiology;superior;case existing;capacity;existing;case;discussed;report case existing", "pdf_keywords": ""}, "591ebe6dccb388d041623840db29a5e58824b4b0": {"ta_keywords": "preference reasoning literature;preferences represented nets;preference reasoning;choice preference reasoning;preferences represented;social choice preference;nets experiments empirical;choice preference;preference;generation preferences represented;algorithms generating nets;nets experiments;social choice;preferences;used social choice;generating nets;generation preferences;represented nets experiments;introductionthe generation preferences;generating nets nodes;large statistical bias;experiments empirical;algorithms generating;statistical bias;statistical cultures;experiments empirical testing;result statistical cultures;statistical cultures commonly;bias;nets nodes", "pdf_keywords": ""}, "7f1a6c67d03de88b898271d52dd2e51907d5b615": {"ta_keywords": "language analysis span;span relationrepresentations methodsnatural;labeling spans relations;span relationrepresentations;analysis span relationrepresentations;relationrepresentations methodsnatural language;spans relations spans;relations spans;spans relations;tasks predicting syntax;methodsnatural language processing;predicting syntax semantics;labeling spans;language processing;natural language analysis;spans;natural language;predicting syntax;consisting labeling spans;language processing covers;relationrepresentations methodsnatural;backgroundgeneralizing natural language;syntax semantics information;span;language analysis;semantics information content;relationrepresentations;analysis span;syntax semantics;methodsnatural language", "pdf_keywords": "named entity recognition;entity recognition;natural language processing;entity recognition ner;tasks predicting syntax;language analysis tasks;relation extraction coreference;model natural language;extraction coreference resolution;language analysis span;tasks introduce span;models natural language;spans single task;backgroundnatural language processing;dependency parsing;language processing;ner relation extraction;speech tagging;tool natural language;language processing new;extraction coreference;relation extraction;tasks unified format;natural language learning;predicting spans relations;labeling spans relations;challenge natural language;predicting syntax semantics;labeling relation classification;language processing covers"}, "a9c0ffc760f65ccaa99a08fc66b31653fd4a5bd7": {"ta_keywords": "organ donation;organ donation donation;donation people organ;organ donors;transplantation organ;organ donation people;people organ donors;organ transplantation;great organ donation;transplantation organ transplantation;organ donors nurses;need transplantation organ;donation biological tissue;donation biological;donation donation biological;organ;organ transplantation rapid;people organ;organ human body;tissue organ;organ human;tissue organ human;biological tissue organ;transplantation;recipient need transplantation;great organ;donors nurses;donors;donation donation;modality great organ", "pdf_keywords": ""}, "a03675379685d88c727bc985a323cc71d06f2514": {"ta_keywords": "backgroundunsupervised learning syntactic;learns discrete syntactic;word representations unsupervised;structured generative;learning syntactic structure;structured generative prior;learning syntactic;generative models;network structured generative;continuous word representations;generative model;generative models discrete;novel generative model;generative prior;backgroundunsupervised learning;generative;discrete syntactic structure;using generative models;word representations;generative model jointly;syntactic structure continuous;novel generative;propose novel generative;jointly learns discrete;using generative;word representations work;discrete syntactic;performed using generative;neural network structured;cascading invertible neural", "pdf_keywords": ""}, "7f817600b612aab6039dfba576ae8e8e7460d8f1": {"ta_keywords": "learn pronunciation dictionary;automatic speech recognition;learn pronunciation;speech recognition;speech recognition systems;performance automatic speech;pronunciation dictionary containing;automatic speech;pronunciation dictionary;area learn pronunciation;learn directly speech;directly speech adapt;speech adapt;speech adapt new;pronunciation;training expert tuning;ability learn directly;recognition systems;learn directly;recognition systems recently;expert tuning;supervised training expert;extensive supervised training;ability learn;directly speech;supervised training;training expert;recognition;sufficient ability learn;speech", "pdf_keywords": ""}, "60a9438c24847a949419e0350a61fc2a330e4a09": {"ta_keywords": "biases kernel clustering;kernel clustering methods;kernel clustering criteria;popular kernel clustering;kernel clustering;kernel clustering directly;density biases kernel;biases kernel;clustering criteria density;density biases;sets density biases;density biases theoretically;analysis popular kernel;criteria density biases;clustering methods;density biases different;empirically observed;solution density biases;clustering criteria;clustering directly;clustering methods average;clustering;biases theoretically;popular kernel;biases theoretically explaining;biases;significant artifacts empirically;artifacts empirically observed;kernel;artifacts empirically", "pdf_keywords": "bandwidth kernel clustering;kernel clustering;kernel clustering clustering;known kernel clustering;clustering kernel means;clustering kernel;kernel clustering criteria;gini clustering kernel;power kernel clustering;apply kernel clustering;kernel clustering new;graph clustering;clustering criteria kernel;graph clustering apply;bias clustering spatially;bias clustering;clustering criteria density;clustering apply kernel;points graph clustering;clustering clustering objectives;clustering generality;continuous gini clustering;applicable clustering;clustering generality discriminating;gini clustering;bias kernel means;kernel strategy clustering;dense clusters;useful clustering;breiman bias clustering"}, "11db042ed2264f3ea1b8f20151adf725ec3461e8": {"ta_keywords": "critical points saddle;converge point weight;weights converge point;points true minima;local minima;critical points;merely critical point;critical point;neural networks typically;neural networks;converged models;critical point possible;suggest weights converge;suggest converged models;local minima recently;weights converge;minima;accurate neural networks;neural;stuck local minima;saddle points;true minima descriptions;true minima;saddle points true;points saddle points;weight space local;majority critical points;point weight space;learning;minima recently researchers", "pdf_keywords": "neural networks highly;neural networks appears;surfaces neural networks;neural network topologies;networks highly nonlinear;error surfaces neural;neural networks;neural network;nonlinear local minima;local minima;mnist database handwritten;neural;highly nonlinear local;local minima recently;surfaces neural;common neural network;weights converge point;handwritten digits;handwritten digits available;points true minima;converged models;networks highly;suggest converged models;stuck local minima;suggest weights converge;networks;minima;converge point weight;critical points saddle;highly non convex"}, "201b79be15b6b01e62a82b29ac4d30d3e6a11799": {"ta_keywords": "speech recognition single;decoder speech recognition;speaker speech recognition;encoder decoder speech;multi speaker speech;speech recognition model;speech recognition;decoder speech;speech recognition effective;multi speaker;rn based encoder;method multi speaker;end multi speaker;recognition model transformer;recognition single channel;speaker speech;single channel multi;channel multi;encoder decoder;speaker;multi channel;model transformer architecture;encoder;use transformer models;model transformer;transformer models;transformer models tasks;channel multi channel;based encoder;based encoder decoder", "pdf_keywords": "end speech recognition;speech recognition end;talker speech recognition;encoder decoder speech;model speech recognition;speech recognition module;decoder network speech;decoder speech recognition;recognition multi talker;speaker speech recognition;speakers speech recognition;speech recognition single;speech recognition multi;multispeaker speech recognition;speech recognition model;end multispeaker speech;neural networks speech;decoder speech;speech recognition;speech model;recognition module transformers;task speech recognition;speech recognition multisource;speech recognition challenging;model speech;recognition multi speaker;multispeaker speech;recurrent neural networks;speech model speech;speech recognition connectionist"}, "62606fbb3aa3ffb17c5427b3652c18a81425cd65": {"ta_keywords": "names weakly labeled;training named entity;named entity recognizer;weakly labeled text;protein names weakly;annotated training data;data annotated training;learning extract gene;entity recognizer;entity recognizer ner;gene protein names;significant annotated training;extract gene protein;weakly labeled;sources data annotated;learning extract;annotated training;generate significant annotated;data annotated;protein names;names weakly;named entity;labeled text;extract gene;labeled text methods;training data automatically;text methods training;training data results;training data;annotated", "pdf_keywords": ""}, "99848c6424556bce427d621e89b6d05dac131910": {"ta_keywords": "crowdsourcing annotation yandex;russian national corpus;nouns extracted russian;annotation yandex;300 frequent nouns;frequent nouns extracted;frequent nouns;crowdsourcing annotation;national corpus method;passed crowdsourcing annotation;national corpus;corpus;nouns;nouns extracted;extracted russian;crowdsourcing;extracted russian national;yandex;pairs passed crowdsourcing;passed crowdsourcing;hypernyms;corpus method;hypernyms possible;russian national;corpus method resource;hypernyms possible case;russian;produced hypernyms possible;annotation;resource including ruthes", "pdf_keywords": ""}, "7a1bcf3c84607f7aeb0601658845ca2083059f43": {"ta_keywords": "semi supervised;semi supervised learning;introduction semi supervised;methods semi supervised;learning vision language;supervised learning vision;supervised;vision language task;supervised learning;visual language tasks;leveraging large unlabelled;learning vision;vision language;labeled data leveraging;labeled data;unlabelled samples;large unlabelled samples;visual language;limited labeled data;labeled;large unlabelled;task using mixmatch;unlabelled samples limited;supervised learning algorithms;mixmatch methods semi;learning;language tasks;limited labeled;unlabelled;mixmatch", "pdf_keywords": ""}, "ef09dd6f5615e2b937d3f9dd555c2daafb4c4f4b": {"ta_keywords": "parallel virtual machines;parallel virtual machine;ofjavabased parallel virtual;performance ofjavabased parallel;parallel virtual;virtual machines;communication performance ofjavabased;virtual machine pvm;libraries parallel virtual;virtual machines crucial;virtual machine;ofjavabased parallel;parallel programs;implement parallel programs;pai implement parallel;pvm message passing;parallel programs multiple;implement parallel;machine pvm message;performance ofjavabased;pvm message;passing libraries parallel;libraries parallel;programming interface pai;independent computers;programs multiple computers;interface mpi provide;interface mpi;message passing interface;pvm", "pdf_keywords": ""}, "aa0d4f7cfa13758a02d248fd607547f045306519": {"ta_keywords": "person recognizers email;personal names email;extracting personal names;names email applying;recognizers email;entity recognition informal;clinical message extracting;namedd entity recognition;names email;named entity recognition;recognition informal documents;message extracting personal;recognizers email email;recognition informal text;personal names;entity recognition;extracting personal;email specific structural;person recognizers;recognition informal;informal documents;message extracting;informal text;informal documents like;named entity;documents like email;informal text little;namedd entity;email present methods;recognizers", "pdf_keywords": ""}, "36db0616e59c8ac5e9ba8ded820ef6c969f068c1": {"ta_keywords": "time southern africa;art time southern;literature art time;art time;southern africa;time southern;literature art;present literature art;africa;literature;art;present literature;article present literature;time;southern;purpose article present;purpose article;article present;article;purpose;present", "pdf_keywords": ""}, "78f1eef6d79a129f59b977a5037f5fc9cc7fda90": {"ta_keywords": "improve peer review;improved peer review;peer review processes;peer review;peer review particularly;venues peer review;toolkit improved peer;peer review faces;ai venues peer;need improve peer;improved peer;improve peer;review processes;submissions especially ai;peer;algorithmic toolkit;algorithmic toolkit improved;review particularly;venues peer;developing algorithmic toolkit;review;issues developing algorithmic;developing algorithmic;submissions especially;algorithmic;especially ai venues;review faces;number submissions especially;ai venues;toolkit improved", "pdf_keywords": ""}, "afdae523d420278670c30f45c015cc5860a0de22": {"ta_keywords": "gradient methods converge;backgroundadaptive gradient methods;converge minimizer optical;converge faster parameterization;backgroundadaptive gradient;gradient methods;minimizer optical rate;minimizer optical;faster parameterization line;faster parameterization;amsgrad converge minimizer;methods converge faster;convergence interpolation setting;study convergence interpolation;convergence interpolation;rate smooth convex;optical rate smooth;converge minimizer;gradient;momentum variants adam;interpolation setting methodsunder;rate smooth;adam amsgrad converge;minimizer;smooth convex;methods converge;parameterization line search;smooth convex functions;constant step size;zero momentum variants", "pdf_keywords": "stochastic gradient descent;adaptive gradient methods;algorithm adaptive gradients;adaptive gradients employ;adaptive gradients effective;algorithm adaptive gradient;adaptive gradients;slower convergence minimizer;converge faster parameterization;gradient descent;method adaptive gradient;gradients employ adaptive;employ adaptive gradient;construct adaptive gradient;size adaptive gradients;results adaptive gradient;adaptive gradient;propose adaptive gradient;adaptive gradient lemmas;problem adaptive gradients;stochastic accelerated gradients;size adaptive gradient;adaptive gradients algorithm;based adaptive gradient;gradient descent squares;accelerated stochastic gradient;gradient methods slow;faster parameterization adapted;adaptive gradient method;asymmetric adaptive methods"}, "656aedc681975c3c97b1764466832de537358150": {"ta_keywords": "auditory systems auxiliary;auditory auditory systems;auditory systems;end auditory auditory;auxiliary featurebased adaptation;auditory auditory auditory;featurebased adaptation end;end auditory;auditory auditory;end end auditory;featurebased adaptation;introductionauxiliary featurebased adaptation;auditory;systems auxiliary featurebased;auxiliary featurebased;adaptation end end;adaptation end;adaptation;systems auxiliary;auxiliary;introductionauxiliary featurebased;featurebased;systems;end end;end;introductionauxiliary", "pdf_keywords": ""}, "2ccfa631708b78130b1ea1b8ae3c2b688caf3938": {"ta_keywords": "estimate duration surgery;surgery case durations;scheduling surgeries;duration surgery surgery;scheduling surgeries challenging;duration surgery;estimate parameters surgery;background scheduling surgeries;surgery surgery specific;surgery specific notion;estimate duration;simultaneously estimate duration;surgery surgery;uncertainty duration;surgeries;neural regression algorithms;notion uncertainty duration;surgery specific;surgery case;neural regression;uncertainty duration res;surgeries challenging;parameters surgery case;surgeries challenging task;uncertainty clinical;heteroscedasticity methods seek;regression algorithms estimate;fundamental uncertainty clinical;uncertainty clinical environment;heteroscedasticity methods", "pdf_keywords": "predicting durations surgery;surgery duration data;neural regression probabilistic;estimate duration surgery;heteroscedasticity surgery duration;neural models survival;estimate parameters surgery;neural regression algorithms;models survival data;data heteroscedasticity surgery;durations surgery patients;modeling durations survival;surgery duration;surgery model useful;survival data;surgery model;durations surgery using;duration surgery surgery;neural regression;neural heteroscedastic regression;predicting durations;surgery case durations;method predicting durations;neural models;durations surgery;duration surgery;medical data heteroscedasticity;regression probabilistic;surgery using heteroscedastic;neural heteroscedastic"}, "58777f0af009a225e315b7240db20ba545207702": {"ta_keywords": "uncertainty social choice;computational reasoning uncertain;uncertainty artificial intelligence;decision making uncertainty;uncertainty social;making uncertainty social;reasoning uncertain information;autonomous agents uncertainty;complexity computational reasoning;agents uncertainty artificial;agents uncertainty;aggregation social choice;making uncertainty;uncertainty artificial;complexity theory autonomous;computational reasoning;reasoning uncertain;uncertainty;social choice manipulation;uncertain information methodsmy;choice manipulation;preference aggregation social;uncertain information;uncertain information focus;seeks insight complexity;complexity computational;insight complexity computational;artificial intelligence conclusionsinsights;social choice;preference aggregation", "pdf_keywords": ""}, "309fb4d4d0946ac746f352c13cd3be4e2cd86dae": {"ta_keywords": "bayesian approaches acoustic;modeling speech recognition;backgroundacoustic modeling important;acoustic modeling speech;backgroundacoustic modeling;approximating bayesian inferences;approaches acoustic modeling;modeling speech;ways approximating bayesian;approximating bayesian;posteriori approximation model;acoustic modeling;posteriori approximation;maximum posteriori approximation;speech recognition generally;applications bayesian approaches;inferences maximum posteriori;backgroundacoustic;speech processing;bayesian inferences;practical speech recognition;speech recognition;bayesian approaches;approximation model complexity;bayesian inferences maximum;speech processing applications;speech recognition problems;related speech processing;recognition related speech;applications bayesian", "pdf_keywords": ""}, "ce17dab00ddd2c86da508fc0502247f9b18a570f": {"ta_keywords": "multi winner approval;winner approval voting;winner proportional approval;proportional approval voting;approval voting methods;computing winner proportional;winner approval;approval voting satisfaction;prominent voting rules;voting satisfaction approval;approval voting;reweighted approval voting;winners rules proportional;voting methods;approval voting rule;satisfaction approval voting;winner proportional;multiple winners rules;approval voting hard;voting reweighted approval;approval voting reweighted;approval ballots;voting satisfaction;voting rule designed;compute representative winning;rules proportional approval;proportional approval;use approval ballots;computing winner;approval ballots select", "pdf_keywords": "computing winner approval;approval voting variants;multi winner approval;behavior multi winner;winner proportional approval;preferences multi winner;complexity determining winners;winner approval voting;voting rule strategyproof;approval based voting;multi winner rules;voting variants;voting rule competitive;elect multiple winners;proportional approval voting;rule winner determination;winner approval based;winners methodswe computing;winners use complexity;rule approval voting;computing winner proportional;voting variants rule;winner rules demonstrated;voting variants difficult;winners voting rule;prominent voting rules;voting hard dichotomous;voting satisfaction approval;hardness multi winner;given preferences agents"}, "1d1bbed89882ac1001b915ee73199a919aa0d13c": {"ta_keywords": "language model inductively;neural language model;language model;language modelling;vocabulary language modelling;training languages approximated;neural language;sample training languages;language modelling methodswe;training languages;learning human language;construct neural language;open vocabulary language;languages approximated laplace;prior held languages;vocabulary language;human language;languages approximated;languages;prior posterior network;constructing informative prior;posterior network weights;held languages task;level open vocabulary;informative prior;human language motivated;language;languages task;learning human;held languages", "pdf_keywords": "prior neural language;new linguistic prior;baselines uninformative priors;learning language models;neural language models;higher language models;training languages laplace;linguistic prior learning;universal prior neural;recurrent models universal;model language learning;priors zero shotwe;linguistic prior;language models new;priors zero shot;prior neural weights;prior outwe neural;training languages inferred;language modeling infer;models universal prior;probabilities language models;language models;uninformative priors zero;likelihood prior neural;languages inferred laplace;language model universal;language models cross;outperforms baselines uninformative;transfer language models;prior neural"}, "7b7f8fb08262fce3da64c09788fd4b595408e4e6": {"ta_keywords": "etiology speech disorders;speech disorders;symptoms speech disorders;speech disorders poorly;symptoms speech;presented symptoms speech;etiology speech;disorders;speech;disorders poorly understood;disorders poorly;patient presented symptoms;presented symptoms;symptoms;case patient presented;patient presented;understood case patient;case patient;etiology;patient;poorly understood case;case;understood case;poorly;poorly understood;presented;understood", "pdf_keywords": ""}, "d940e0192a6cc1ddd6288239b77b06e50f042114": {"ta_keywords": "supervised pretraining speech;pretraining speech data;self supervised pretraining;pretraining speech;speech signal learned;speech data achieved;supervised pretraining;speech data;fidelity representation speech;representation speech signal;self supervised;speech signal;pretraining;background self supervised;learned lot untranscribed;representation speech;signal learned;evaluating quality self;quality self;high fidelity representation;supervised;signal learned lot;progress high fidelity;fidelity representation;untranscribed data;untranscribed;speech;self;lot untranscribed data;learned", "pdf_keywords": ""}, "d5924c8cdef6270a955ba82c2b07a8282d869744": {"ta_keywords": "language speech gestures;speech gestures;speech gestures context;gestures study relationships;gesture predictions subword;gesture predictions;language freeform gestures;gestures study;text gestures inherently;freeform gestures study;second gesture predictions;gestures inherently;distributions text gestures;gestures;text gestures;introduction gestures;gestures context;introduction gestures left;freeform gestures;gestures left learning;gesture;gestures left;second gesture;tail second gesture;gestures inherently skewed;spoken language freeform;spoken language;gestures context key;language speech;spoken language speech", "pdf_keywords": ""}, "b82e9b84cb639f6bb061c8a43b97986ecfec00ea": {"ta_keywords": "fast similarity queries;similarity queries semi;similarity queries;dimensional representation entities;large partite graphs;similarity queries experiments;enabling fast similarity;fast similarity;structured data web;semi structured data;queries semi structured;partite graphs;partite graphs using;introduction fast similarity;embeddings represent large;polymeric embeddings represent;embeddings;embeddings represent;representation entities;proposed polymeric embeddings;polymeric embeddings;structured data;entities different datasets;datasets web;entities;similarity;low dimensional representation;representation entities different;represent large partite;datasets web methods", "pdf_keywords": ""}, "ee33d61522fd70fa4e6470decbdac6c17f8b4fdb": {"ta_keywords": "based speaker diarization;end speaker diarization;clustering based speaker;speaker diarization;speaker diarization unknown;speaker diarization outperformed;speaker diarization drawback;end speaker;speech embedding;end end speaker;speech embedding sequence;methodsthe speech embedding;introductionend end speaker;decoder based attractor;diarization outperformed conventional;based speaker;diarization outperformed;number speakers addressed;cea methodsthe speech;unknown number speakers;speaker;terms number speakers;diarization;number speakers;diarization unknown;encoder decoder based;diarization unknown number;encoder decoder;number speakers paper;diarization drawback", "pdf_keywords": "end speaker diarization;clusteringbased speaker diarization;speaker diarization;end neural diarization;approach speaker diarization;diarization speech mixtures;conventional clusteringbased speaker;clusteringbased speaker;speaker diarization unknown;speaker diarization speech;diarization speech;generating attractors speakers;toend speaker diarization;speaker diarization outperformed;end diarization framework;neural diarization framework;end end speaker;attractors speakers using;speaker diarization drawback;end speaker;diarization speech noise;speakers using clusteringbased;end toend speaker;speaker diarization discuss;method diarization speech;attractors speakers;speech mixtures flexible;neural diarization;neural network speech;end diarization"}, "d6d2003d112e3d9d93edd4920436fe2fe879eb87": {"ta_keywords": "scale text datasets;effective clustering;effective clustering methods;similarities data;elegant effective clustering;clustering;clustering methods;text datasets;wise similarities data;backgroundlarge scale text;similarities data points;text datasets long;similarity matrix;similarity;backgroundlarge;clustering methods exploits;scale text;backgroundlarge scale;pair wise similarities;operating similarity;similarity matrix state;operating similarity matrix;wise operating similarity;datasets long eluded;datasets long;similarities;datasets;text;wise similarities;fast simple method", "pdf_keywords": ""}, "3b8b6f27a5df5dc2c231d0fa1e471887e4583466": {"ta_keywords": "curatated citation networks;metadata citation networks;citation networks curated;knowing genes author;citation networks;citation networks improve;citations help predict;identifying genes academic;coauthors citations;related metadata citation;coauthors citations help;citations;genes author previously;metadata citation;genes author;previous coauthors citations;genes academic;citations help;genes academic biomedical;extraction link prediction;utilizing curatated citation;academic biomedical publications;link prediction utilizing;link prediction;identifying genes;task identifying genes;biomedical publications specifically;gene detection paper;biomedical publications;introductioninformation extraction link", "pdf_keywords": ""}, "4bfc185dcc67b3eddfa059fc5446f4df844a0728": {"ta_keywords": "symptomatic asymptomatic asymptomatic;symptomatic asymptomatic;asymptomatic asymptomatic asymptomatic;asymptomatic asymptomatic;history symptomatic asymptomatic;asymptomatic;symptomatic;patients history symptomatic;history symptomatic;patients;approach management patients;patients history;management patients history;management patients;effectiveness new approach;evaluating effectiveness new;effectiveness new;evaluating effectiveness;approach management;new approach management;approach;new approach;effectiveness;importance evaluating effectiveness;article;purpose article;new;evaluating;management;article discuss importance", "pdf_keywords": ""}, "9635e3c008f7bfa80638ade7134a8fb0ef1b37e1": {"ta_keywords": "canonical tokenisation train;tokenisation train;models typically tokenise;single canonical tokenisation;canonical tokenisation;tokenisations hurt model;tokenisation train test;introductionneural language models;language models;language model performance;best tokenisation;using best tokenisation;language model;alternative tokenisations;alternative tokenisations hurt;evaluation language model;tokenisations;best tokenisation ignores;tokenisation;tokenise input text;language models typically;tokenise input;typically tokenise input;tokenisations hurt;word units achieve;sub word units;tokenise;achieve open vocabulary;vocabulary standard approach;open vocabulary standard", "pdf_keywords": "language model tokeniser;tokenisations word model;tokenisation language modelling;models best tokenisation;tokenisation word accuracy;word accuracy tokenisation;best tokenisation likelihood;language models trained;language models increasingly;language model performance;tokenisation likelihood introduce;tokenisation language;tokenisations evaluation metric;tokenisations word;language models;tokenisations evaluation;language models best;tokenisation likelihood;likelihood tokenisations evaluation;canonical tokenisation suggest;exact tokenisation word;tokenisation word;language models evaluated;tokenisers accurately estimate;best tokenisation;language model posterior;language modeling neural;canonical tokenisation;tokenisation word marginal;tokenisers accurately"}, "3f2821dd40c12da560c89b9dad7f95cd4ad9354f": {"ta_keywords": "overwhelms storage infrastructure;storage clusters substantial;storage clusters;scale storage clusters;traces millions disks;storage infrastructure;large scale storage;overwhelms storage;data redundancy provides;tuning redundancy schemes;data redundancy;clusters load transitions;redundancy schemes observed;redundancy schemes;millions disks;disk failure rates;redundancy provides resilience;storage infrastructure termed;clusters load;schemes overwhelms storage;realized tuning redundancy;tuning redundancy;redundancy provides;scale storage;observed disk failure;storage;redundancy;clusters substantial space;world clusters load;clusters substantial", "pdf_keywords": "overhead diskadaptive redundancy;diskadaptive redundancy proposals;diskadaptive redundancy storage;diskadaptive redundancy orchestrator;diskadaptive redundancy;low overhead diskadaptive;previous diskadaptive redundancy;disk adaptive redundancy;redundancy orchestrator storage;redundancy storage;overhead diskadaptive;redundancy storage storage;orchestrator storage clusters;method diskadaptive redundancy;storage clusters;storage clusters new;disks established management;existing distributed storage;storage clusters supports;disks distributed storage;efficiently storage manage;large storage cluster;large scale storage;approach transitioning disks;diskthe use efficiently;storage new approach;diskadaptive;storage manage optimize;storage cluster;distributed storage increasing"}, "ff6ddbd7ba59e0fd4a74748942083391d6e9a666": {"ta_keywords": "operations oriented probabilistic;oriented probabilistic extraction;probabilistic extraction reasoning;probabilistic extraction;operations oriented probabilityistic;extraction reasoning analysis;probabilityistic extraction reasoning;oriented probabilityistic extraction;extraction reasoning;probabilityistic extraction;oriented probabilistic;core operations oriented;reasoning analysis;operations oriented;reasoning analysis methods;probabilistic;describes core operations;reasoning analysis results;core operations;oriented probabilityistic;operations;reasoning analysis conclusion;extraction;probabilityistic;describes core;reasoning;methods paper describes;introduction paper describes;paper describes core;core", "pdf_keywords": ""}, "9fe09ca520cb7ce106e65b39c455777d18ec6efe": {"ta_keywords": "expand textual taxonomies;expanding taxonomies implicit;expanding taxonomies;background expanding taxonomies;taxonomies implicit edge;textual taxonomies predicting;rapidly evolving taxonomy;taxonomies predicting;taxonomies implicit;arborist approach automatically;new taxonomy nodes;taxonomies predicting parents;textual taxonomies;evolving taxonomy;propose arborist approach;taxonomy nodes;parents new taxonomy;evolving taxonomy infeasible;arborist approach;automatically expand textual;propose arborist;taxonomies;approach automatically expand;implicit edge semantics;expand textual;taxonomy nodes compared;new taxonomy;arborist;work propose arborist;edge semantics challenging", "pdf_keywords": ""}, "ef4166a7fb2c40ca87b0ebb253e8ba1e80c09fd7": {"ta_keywords": "chime speech separationion;speech separationion recognition;reverberated noisy speech;speech separationion;noisy speech automatic;stationary interference reverberation;speech recognition;speech home noises;speech automatic;automatic speech recognition;automatic speech;interference reverberation;reverberation remains challenging;speech recognition presence;reverberation non stationary;reverberation;speech automatic speech;background speech home;separationion recognition challenge;interference reverberation remains;reverberation non;separationion recognition;time varying reverberation;speech home;natural background speech;reverberated noisy;feature transformation reverberated;2nd chime speech;chime speech;noisy speech", "pdf_keywords": ""}, "5db0fa82c322bb7d9f60109294d088ff139eebf3": {"ta_keywords": "backgroundgenerative adversarial networks;backgroundgenerative adversarial;networks gans;manifold learned thegan;networks gans transform;gans transform low;adversarial networks gans;gans transform;gans;denoise corrupted images;learned thegan contain;adversarial networks;clean images methodsin;learned thegan;gait manifold recovering;recovering latent vectors;propose denoise corrupted;adversarial;clean images;manifold recovering latent;clean images ostensibly;thegan contain clean;images real dataset;visually plausible images;thegan contain;contain clean images;contains clean images;manifold learned;latent vectors visually;images ostensibly manifold", "pdf_keywords": "quality image denoising;denoising images;denoised image quality;image denoising;image denoising deblurring;image denoising denoised;gan manifold recovering;gans transform lowdimensional;networks gans transform;denoised image;method image denoising;denoise corrupted images;manifold learned gan;learned gan;image denoising simple;method denoising images;gans transform;network gan;denoising denoised image;denoising images using;used image denoising;gan used image;image denoising using;networks gans;digital gamut gan;denoising deblurring unseen;learned gan contain;denoising deblurring;backgroundgenerative adversarial networks;smoother images trained"}, "1d05e91b6d94f06439b2b41291a8dcc3d8064149": {"ta_keywords": "color space clustering;approach color clustering;color clustering;color clustering using;energy color space;clustering using kernel;model fitting segmentation;fitting segmentation methods;kernel means energy;color space;fitting segmentation;log likelihood energy;means energy color;segmentation;segmentation methods;segmentation methods presented;energy color;likelihood energy;backgroundthe log likelihood;space clustering;probabilistic kg means;clustering;likelihood energy term;using kernel means;generalized probabilistic kg;approach color;kg means energy;alternative approach color;space clustering interpretation;clustering interpretation reveals", "pdf_keywords": ""}, "da1f22dd6d834e031eb733d2b70320f34ef9458f": {"ta_keywords": "parametric ordinal models;ordinal models pairwise;pairwise comparison data;pairwise comparisons arises;competitions peer grading;pairwise comparisons;ordinal models;models pairwise comparison;consider parametric ordinal;pairwise comparison;parametric ordinal;peer grading;comparison data;peer grading consider;ordinal;grading consider parametric;form pairwise comparisons;comparisons;preference elicitation;comparison data involving;including preference elicitation;qualities items compared;comparisons arises domains;compared consider parametric;competitions peer;grading;preference elicitation sporting;grading consider;sporting competitions peer;comparisons arises", "pdf_keywords": "models pairwise comparisons;models pairwise comparison;pairwise comparisons important;pairwise comparisons;pairwise comparisons arises;analysis pairwise comparisons;pairwise comparisons use;paired comparisons;empirical evaluation minimax;pairwise comparison data;ranking quality;method pairwise comparisons;competitions peer grading;popular models pairwise;ranking quality set;use pairwise comparisons;ordinal models pairwise;pairwise comparisons relatively;method ranking quality;pairwise comparison;evaluation minimax;pairwise comparison used;paired comparisons discuss;quality score vectors;ranking;paired comparison;selection comparisons;parametric ordinal models;analysis paired comparison;method paired comparisons"}, "b07b124852f897823490db0a04ea6e411bb77f00": {"ta_keywords": "classifiers maximizemizef1 measure;thresholding classifiers maximizemizef1;classifiers maximizemizef1;optimal thresholding classifiers;maximizing fi1 measures;thresholding classifiers;recall thef1 measure;precision recall thef1;mean precision recall;maximizemizef1 measure aim;maximizemizef1 measure;maximizing fi1;thef1 measure widely;introduction optimal thresholding;optimal thresholding;fi1 measures;classifier;classifiers;thef1 measure;multilabel classification;classification;classification methods harmonic;thresholding;fi1 measures context;multilabel classification methods;binary classification;binary classifier;precision recall;insight maximizing fi1;recall thef1", "pdf_keywords": ""}, "b6502b61bf8f0332c6caa30198cff3619a9790aa": {"ta_keywords": "sending redundant requests;redundant requests;requests reduce latency;redundant requests reduce;latency serving requests;introductionwhen redundant requests;redundant requests request;systems latency serving;latency serving;reduced sending redundant;requests potentially reduced;serving requests;serving requests potentially;sending redundant;requests reduce;reduce latency systems;latency systems possess;reduce latency;systems latency;latency systems;serve requests;latency;serve requests way;potentially reduced sending;flexibility serve requests;redundant;requests way systems;way systems latency;requests potentially;reduced sending", "pdf_keywords": "latency serving requests;performance redundant requests;redundant requests algorithm;requests reduce latency;redundant requests environment;redundant requests increasingly;redundant requests mechanism;redundant requests requests;redundant requests minimized;sending redundant requests;redundantrequesting reduce latency;queueing requests;redundant requests;centralized queueing requests;analysis redundant requests;latency minimized request;redundant requests actually;loads redundant requests;redundant requests reduce;latency requests;workloads latency requests;latency requests reduced;latency latency requests;serving requests;latency requests significantly;adding redundant requests;serving requests potentially;redundant requests help;latency serving;followed redundant requests"}, "b72c5236dacf2b958ebcf427d17a100bc54af504": {"ta_keywords": "transformer self attention;self attention network;e2e automatic speech;transformer encoder introducing;attention network;transformer encoder;encoder introducing context;encoder introducing;recurrent neural networks;alternative recurrent neural;method transformer encoder;compute self attention;encoder;performance alternative recurrent;speech recognition;automatic speech;transformer self;recurrent neural;automatic speech recognition;neural networks end;speech recognition systems;recognition systems transformer;alternative recurrent;introductionthe transformer self;attention network recently;self attention paper;neural networks;transformer;self attention;attention", "pdf_keywords": "e2e automatic speech;automatic speech;attention network;online speech recognition;neural networks speech;decoding speech;performance alternative recurrent;recurrent neural networks;encoder introducing context;alternative recurrent neural;networks speech recognition;self attention network;speech recognition;speech recognition discuss;novel contextual block;processing speech;speech recognition ar;encoder introducing;method decoding speech;automatic speech recognition;understanding speech recognition;tools understanding speech;speech recognize;speech recognition important;contextual embedding;networks speech;models speech recognition;contextual embedding vector;additional contextual embedding;application speech recognition"}, "6dfc2ff03534a4325d06c6f88c3144831996629b": {"ta_keywords": "cognition commonsense reasoning;task visual commons;cognition commonsense;visual commons;commonsense reasoning world;commonsense reasoning;visual understanding;object recognition glance;order cognition commonsense;visual understanding goes;image effortlessly imagine;context visual understanding;recognition glance;recognition glance image;object recognition;effortlessly imagine world;today vision systems;context visual;task visual;formalize task visual;vision systems requiring;vision;commonsense;image effortlessly;imagine world pixels;today vision;glance image effortlessly;vision systems;recognition;visual", "pdf_keywords": "commonsense visual reasoning;visual commonsense reasoning;visual question answering;task visual commonsense;model visual commonsense;generating commonsense visual;commonsense reasoning vr;visual commonsense;visual reasoning;commonsense visual;virtual commonsense reasoning;introduce visual commonsense;collect commonsense visual;demonstrate visual commonsense;reasoning engine recognition;visual reasoning problems;commonsense reasoning effective;problem visual commonsense;commonsense reasoning demonstrate;adjversarial matching provide;commonsense reasoning vc;humans visual commonsense;adjversarial matching;adjversarial matching new;built using adjversarial;commonsense reasoning;commonsense reasoning easy;creation adjversarial matching;vic adjversarial matching;using adjversarial matching"}, "df43f6ff7c66d39240235af3052be55222bef80d": {"ta_keywords": "nested gibbs sampling;mixture model methods;gibbs sampling develop;gibbs sampling;components mixture model;model components mixture;mixture model;mixture ofmixture model;nested gibbs;uses nested gibbs;sampling develop mixture;novel model estimation;components mixture;multilevel data comprising;model estimation;frame wise observations;analyzing multilevel data;multilevel data;mixture ofmixture;observations videos acoustic;model estimation method;analyzing multilevel;distribution model components;data comprising frame;mixture;signals composed frame;develop mixture ofmixture;suitable analyzing multilevel;acoustic signals composed;gibbs", "pdf_keywords": ""}, "964293c1cb1b619eb9b474381d2ba60cf44fcc2d": {"ta_keywords": "mapping distribution facility;maps managing distribution;develop mapping distribution;facilities develop mapping;mapping distribution line;maps managing;distribution facility management;distribution line maps;managing distribution facilities;mapping distribution;population maps managing;maps design;distribution facilities essential;develop mapping;japanese population maps;jobs using mapping;distribution facility;distribution facilities;maps design jobs;facility management japanese;using mapping;using mapping database;mapping database;maps;purposedevelopment mapping distribution;maps tekc making;managing distribution;maps tekc;population maps;line maps tekc", "pdf_keywords": ""}, "d389d8c2e15f9e9269c17fe6f960f70559eee840": {"ta_keywords": "word level analysis;meaningful subword structures;semantically meaningful subword;word embedding;word embedding techniques;regard word embedding;meaningful semantic granularity;individual word tokens;word tokens;subword structures;subword structures word;word tokens finest;semantic granularity;subword structures regard;meaningful subword;specialized corpora words;semantic granularity languages;word level;structures word level;text mining tasks;present subword structures;text mining;subword;corpora words;finest meaningful semantic;meaningful semantic;corpora words composed;concatenating semantically meaningful;semantic;treat individual word", "pdf_keywords": "unsupervised morpheme segmentation;morpheme segmentation entropy;segmentation morphemes formalize;integrating morpheme segmentation;morpheme segmentation;morpheme segmentation morphemes;morphemes level segmentation;segmentation morphemes;morpheme segmentation task;framing morpheme segmentation;morpheme segmentation tool;method morpheme segmentation;morpheme segmentation word;segmented recomputing morpheme;predict morpheme boundaries;meaningful morphemes extracted;semantically meaningful morphemes;incorporating meaningful morphemes;annotated morphemes;morpheme segmentation apply;unsupervised morpheme;annotated morphemes languages;human annotated morphemes;longer shared morphemes;morphemes extracted;morphemes state art;integrating morphemes morphemes;morpheme boundaries;shared morphemes;shared morphemes level"}, "3d1318bc66d534eefac7c665fd7cc891fba27b87": {"ta_keywords": "patients history disease;patients history;management patients history;history disease;management patients;patients;approach management patients;disease;new approach management;approach management;new approach;history;importance new approach;management;approach;purpose article discuss;article discuss importance;purpose article;article discuss;article;discuss importance new;new;discuss importance;importance new;importance;purpose;discuss", "pdf_keywords": ""}, "33972d9e9a102f9388e5850d8aed3d1aefc9d2e5": {"ta_keywords": "dealing fallacious argumentation;research argumentation quality;argumentation quality investigated;argumentation quality;fallacious argumentation calls;critical thinking argumentation;argumentation scholars;argumentation scholars nonlack;fallacious argumentation;argumentation ability;argumentative discourse deceptive;importance argumentation scholars;research argumentation;nonlack research argumentation;thinking argumentation ability;thinking argumentation;fallacious arguments omnipresent;argumentation calls;argumentation ability spot;fallacious arguments;argumentation;fallacies fallacious arguments;argumentative discourse;arguments omnipresent argumentative;importance argumentation;omnipresent argumentative discourse;argumentative;despite importance argumentation;investigated fallacies;quality investigated fallacies", "pdf_keywords": "game argumentation mining;game argumentation;argotario argumentative discourse;dealing fallacious argumentation;argumentation scholars;argumentation quality assessment;argumentation ability;gamification fallacy recognition;argumentation quality;game analyze fallacies;literature argumentation quality;fallacious argumentation calls;analyze fallacies argumentative;fallacies argumentative discourse;mining argumentation quality;fallacious argumentation;argotario argumentative;argumentation mining argumentation;argotario gamification fallacy;mining argumentation;literature argumentation;argumentation mining;argumentation mining argue;argumentation scholars nonliranian;usefulness argotario argumentative;critical thinking argumentation;argumentation calls;argumentation ability spot;argumentation mining approach;fallacies argumentative"}, "04138da3bac26f83a9d57152118d4cd5cc8c717d": {"ta_keywords": "similarity measures entity;walk based similarity;entity relation graphs;searching entities;searching searching entities;based similarity measures;similarity measures;graph walk based;searching entities objectivethe;relation graphs;relation graphs useful;based similarity;measures entity relation;backgroundadaptive graph walk;queries instance persons;similarity;persons related email;graph walk;arbitrary queries;searching searching;queries;related email message;entity relation;different arbitrary queries;useful tools searching;related email;tools searching searching;measures entity;arbitrary queries instance;walk based", "pdf_keywords": ""}, "395aae6e7a79e5760457ca38e868acc970016230": {"ta_keywords": "relational tables web;tables ubiquitous web;structure web tables;web tables;tables web;20 relational tables;tables ubiquitous;introduction tables ubiquitous;tables web 20;relational tables;tables;web tables results;large tables;introduction tables;large tables present;tables results mate;rows large tables;model structure web;20 relational;web 20 rows;tables present;structure web;tables present challenge;tables results;information 20 relational;transformer models;transformer models typically;uses sparse attention;transformer architecture;relational", "pdf_keywords": "tables ubiquitous web;table reasoning models;structure web tables;web tables ubiquitous;stronger table reasoning;relational tables web;web tables propose;attention encode hierarchy;attention transformer architecture;tables ubiquitous;table reasoning large;efficiently compute attention;tabular textual data;tables web;integrating attention large;text corpus tables;attention matrix useful;sparse attention transformer;web tables;large tables propose;tabular textual;reasoning tabular textual;tables propose novel;sparse attention matrix;attention transformer;task involving tables;propose sparse attention;attention encode;relational tables;attention flow transformers"}, "e07c2e66dab7b61091bb8a4ad132bf279c233027": {"ta_keywords": "citations topic modeling;latent topic modeling;topic modeling;modeling text citations;topic modeling framework;topic modeling allows;text citations topic;citations topic;text citations;citations;latent topic;link structure model;joint modeling text;ideas latent topic;modeling text;modeling arbitrary link;link structure;link la model;combines ideas latent;topic;arbitrary link structure;problem joint modeling;la link plsanitary;link plsanitary;pairwise link la;joint modeling;called pairwise link;pairwise link;link la link;arbitrary link", "pdf_keywords": ""}, "aeb4478619461ca25592e6d692f3591ec8c4091b": {"ta_keywords": "clinical messageadversarial semantic;messageadversarial semantic collisions;key clinical messageadversarial;clinical messageadversarial;messageadversarial semantic;generating semantic collisions;semantically unrelated text;summarization vulnerable semantic;including paraphrase identification;semantic collisions;generating semantic;vulnerable semantic collisions;approaches generating semantic;paraphrase identification;semantic collisions demonstrate;texts including paraphrase;text characteristics semantically;semantic collisions studied;messageadversarial;including paraphrase;analyzing meaning similarity;paraphrase identification document;summarization vulnerable;extractive summarization vulnerable;similarity texts including;unrelated text;semantic;suggestion extractive summarization;text develop gradient;similarity texts", "pdf_keywords": "adversarial texts generate;adversarial texts;nonsensical adversarial texts;adversarial;adversarial examples provide;adversarial examples;adversarial examples non;nonsensical adversarial;example inverse adversarial;output inverse adversarial;adversarial examples use;inverse adversarial examples;inverse adversarial;class nonsensical adversarial;algorithm inverse adversarial;deceptive output adversary;generating semantic collisions;defenses propose gradient;algorithms adversary produces;output adversary;adversary produces collision;output adversary produces;adversary produces;algorithms adversary;deceptive output;deceptive output inverse;semantic collisions demonstrate;adversary;semantic collisions;study semantic collisions"}, "3a3fb140890dbba93290e358af700f9a5c8bcc7a": {"ta_keywords": "question answering;question answering qa;knowledge intensive ai;domain question answering;end deep models;deep neural;symbolic meaning representations;intensive ai tasks;ai tasks open;question complexity responding;deep models need;ai tasks;intensive ai;end end deep;answering qa existing;answering qa;deep models;introduction deep neural;knowledge intensive;end deep;natural language processing;deep neural networks;limited knowledge intensive;meaning representations indexed;meaning representations;observing question complexity;natural language;ai;neural;neural networks great", "pdf_keywords": ""}, "2f780a18d44f4e3c5c4c74d4060b8dfd542a778d": {"ta_keywords": "sports commentator bias;racial bias fooball;investigating sports commentator;commentator bias;commentator bias large;examining racial bias;transcripts american football;racial bias;sports commentator;bias large corpus;broadcast transcripts american;bias fooball;american football broadcasts;background investigating sports;bias fooball perform;racial metadata results;investigating sports;broadcast transcripts;football broadcasts;bias;racial metadata;football broadcasts methods;commentator;researchers examining racial;corpus american football;bias large;linked racial metadata;transcripts american;mentions linked racial;examining racial", "pdf_keywords": "sports broadcasting bias;bias sports commentary;bias sports broadcasting;bias sports broadcasts;commentator bias sports;sports broadcasting racial;racial bias sports;bias studies sports;football broadcasts racial;bias sports;broadcasting bias;background sports broadcasters;commentator bias;studies sports broadcasting;football broadcasts significantly;sports broadcasts;sports broadcasting;sports broadcasters;public commentary sports;broadcasting racial;sports commentary;broadcasts racial;commentary sports;result bias sports;sports commentary collect;sports broadcasters inject;shown sports broadcasting;reflect racial bias;broadcasting racial distribution;sports commentary aim"}, "34fc6da7a88433478fd976fd0b9de3cf7134e652": {"ta_keywords": "diagnosis septic shock;patient septic shock;septic shock understood;septic shock;patient diagnosis septic;septic shock septic;shock septic shock;shock septic;diagnosis septic;case patient septic;patient septic;septic;aetiology patient diagnosis;shown aetiology patient;aetiology patient;shock understood report;patient diagnosis;diagnosis;aetiology;study shown aetiology;shock understood;shock;shown aetiology;report case patient;case patient;patient;new study shown;report case;new study;study shown", "pdf_keywords": ""}, "df1d89f4ca9c20e2c6703cdbf26a62f2b50ac71c": {"ta_keywords": "equilibriumlibria staackelberg games;dynamics staackelberg games;nash staackelberg equilibrium;staackelberg games continuous;learning dynamics staackelberg;messagecharacterizing equilibriumlibria staackelberg;descent staackelberg equilibria;staackelberg equilibrium concepts;equilibriumlibria staackelberg;staackelberg equilibria;staackelberg games;staackelberg equilibrium;staackelberg games important;dynamics staackelberg;hierarchical order play;games continuous action;staackelberg equilibria zero;learning dynamics;gradient descent staackelberg;connections nash staackelberg;messagecharacterizing equilibriumlibria;convergence learning dynamics;descent staackelberg;equilibrium concepts;equilibrium concepts characterize;equilibriumlibria;nash staackelberg;play agents establish;equilibria;games distinguished hierarchical", "pdf_keywords": ""}, "02980e5ba847282b683a85e7a8862c6c1b6e0d94": {"ta_keywords": "opportunistic opportunistic opportunistic;opportunistic opportunistic;orgogenic opportunistic opportunistic;opportunistic;orgogenic opportunistic;aqueous orgogenic opportunistic;aqueous;orgogenic;aqueous orgogenic", "pdf_keywords": ""}, "73aa33fd469b171d50c452c5e3fe0e9e03520520": {"ta_keywords": "english auditory auditory;auditory auditory auditory;auditory auditory;english auditory;naist english auditory;auditory;andkit naist english;kit andkit naist;kit andkit;andkit naist;2012 kit andkit;naist english;english;introductionthe 2012 kit;andkit;kit;naist;2012 kit;introductionthe 2012;introductionthe;2012", "pdf_keywords": ""}, "c330ec2047d019d98233abb59d13b3256c662cc7": {"ta_keywords": "counterfactual identification observational;bounding counterfactual queries;counterfactual distributions arbitrary;target counterfactual probabilities;counterfactual queries arbitrary;bounds target counterfactual;counterfactual identification;counterfactual probabilities provably;categorical counterfactual distributions;counterfactual distributions;bounding counterfactual;counterfactual queries;counterfactual probabilities;problem bounding counterfactual;arbitrary structural causal;target counterfactual;backgroundpartial counterfactual identification;methodswe counterfactual distributions;arbitrary causal;distributions arbitrary causal;categorical counterfactual;discrete structural causal;causal models;structural causal models;arbitrary causal diagram;represent categorical counterfactual;causal diagram methodswe;structural causal;causal models represent;counterfactual", "pdf_keywords": "counterfactual probabilities optimization;counterfactual distributions causal;counterfactual distributions generated;maintaining counterfactual distributions;representing counterfactual distributions;inferring counterfactual distributions;bounding counterfactual queries;counterfactual distributions arbitrary;counterfactual distributions structures;counterfactual queries arbitrary;counterfactual identification observational;bounding counterfactual probabilities;represent counterfactual distributions;analysis counterfactual queries;counterfactual probabilities achieve;counterfactual probability observational;counterfactual probabilities arbitrary;established counterfactual distributions;counterfactual distributions;counterfactual identification;counterfactual queries;counterfactual probabilities described;counterfactual distribution systematic;bounding counterfactual probabil;unknown counterfactual probabilities"}, "0a3caecce668731efe7abf37720793eed1fb951a": {"ta_keywords": "emotion microblogs political;information emotion;microblogs political contexts;politics sharing emotion;sharing emotion microblogs;emotion microblogs;microblogs political;motivated reasoners information;political information;political information important;receiving political information;context politics sharing;information evaluated emotional;interaction information emotion;emotional reaction influences;reasoners information;social media;politics sharing;sharing emotion;affect emotional reaction;steps social media;emotional affect;emotional affect emotional;affect emotional;emotional reaction;political contexts;motivated reasoners;social media prevalent;context politics;reasoners information evaluated", "pdf_keywords": ""}, "5bca90a331417402f5018f552e1a62656dd7fc5b": {"ta_keywords": "recovering communities features;communities features labeled;labeled graph observation;features labeled graph;problem recovering communities;corresponding community feature;graph observation;observed graph;recovering communities;communities features;edges observed graph;observed graph generated;labeled graph;graph observation methodswe;community feature;staochastic block model;symmetric staochastic block;communities;corresponding community;assume edges observed;community feature resultswe;staochastic block;graph generated symmetric;edges observed;graph;graph generated;features labeled;partially observed version;version corresponding community;information theoretic", "pdf_keywords": ""}, "5166c0e04d77ac0f7969c49c0f8f18129a114198": {"ta_keywords": "assess gait gait;measure gait gait;assess gait;measure gait;gait analysis;gait gait analysis;measurements assess gait;gait analysis demonstrate;used measure gait;gait gait;gait;motion measurements assess;motion motion measurements;motion measurements used;motion measurements;use automated motion;automated motion motion;automated motion;demonstrate automated motion;motion motion;motion;analysis demonstrate automated;measurements assess;demonstrate automated;use automated;automated;measurements used measure;assess;analysis;measure", "pdf_keywords": ""}, "205ff5dae21ca44c15d3b7d7a9febb7d84b47bc4": {"ta_keywords": "multilingual seed models;neural machine translation;machine translation systems;translation systems new;massively multilingual seed;translation systems;machine translation;multilingual seed;starting massively multilingual;low resourced languages;massively multilingual;adapting neural machine;new languages;languages problem adapting;languages lls effectively;resourced languages lls;multilingual;adaptation neural machine;seed models trained;adapting neural;resourced languages;rapid adaptation neural;languages lls;systems new languages;new languages problem;adaptation neural;seed models;problem adapting neural;introduction rapid adaptation;languages", "pdf_keywords": "multilingual translation model;neural machine translation;adapt multilingual translation;language regularization improve;translation new languages;corpus multilingual training;massively multilingual training;machine translation challenges;machine translation new;translation challenges neural;improve performance multilingual;language regularizing;multilingual translation;multilingual seed models;similar language regularization;language regularizing similar;regularizing similar languages;language regularization;machine translation;performance multilingual training;multilingual training effective;multilingual training;language regularization aim;starting massively multilingual;translation model;resource language regularizing;translation model new;massly multilingual model;multilingual model useful;machine translation ms"}, "488b1849dd81e63aae2cd327564077ae123c0369": {"ta_keywords": "expensive communication compression;communication compression;distributed methods;communication compression powerful;distributed methods applied;methods biased compression;biased compression;compression powerful approach;stochastic gradients iterates;vectors stochastic gradients;compression;nicating vectors stochastic;networks millions;expensive communication;stochastic gradients;compression powerful;commu nicating vectors;distributed;neural networks millions;networks millions billions;networks;biased compression error;compression error compensation;prohibitively expensive communication;gradients iterates prohibitively;training neural networks;gradients iterates;particular methods biased;vectors stochastic;training neural", "pdf_keywords": "stochastic gradient descent;large communication compression;staochastic gradient descent;gradient descent sg;absolute compression operators;compression operators;method absolute compression;communication compression powerful;communication compression;stochastic gradients iterates;stochastic estimator complexity;stochastic algorithms convex;compression operators case;method stochastic gradient;compression powerful approach;gradient descent;vectors stochastic gradients;stochastic gradients;gradient descent neural;absolute compression;absolute compressors theorem;biased compression;algorithms convex optimization;sg absolute compressors;descent neural information;estimator complexity bound;compressors theorem;convergence rates methods;particular biased compression;compression"}, "4cd92a56dca741190e453b4229eb9851abf6944c": {"ta_keywords": "convex stochastic optimization;new accelerated stochastic;accelerated stochastic;accelerated stochastic order;stochastic optimization heavy;stochastic optimization;theory stochastic optimization;smooth convex stochastic;noise stochastic gradients;convex stochastic;stochastic order method;stochastic gradients;distributed noise stochastic;stochastic gradients derive;noise stochastic;tailed noise methodswe;optimization heavy tailed;theory stochastic;tailed distributed noise;stochastic;stochastic order;noise methodswe propose;sm smooth convex;distributed noise;noise methodswe;gap theory stochastic;propose new accelerated;clipped sm smooth;smooth convex;optimization heavy", "pdf_keywords": "clipped stochastic gradients;stochastic gradients clipped;stochastic optimization heavy;stochastic gradient clipping;stochastic gradients heavy;convex stochastic optimization;stochastic convex optimization;distributed stochastic gradients;stochastic method clipped;method stochastic gradients;stochastic gradients significantly;based clipped stochastic;stochastic gradients;accelerated stochastic method;stochastic gradients stochastic;method stochastic optimization;distribution stochastic gradients;noise stochastic gradients;stochastic gradients algorithm;stochastic gradient;clipped stochastic;prediction stochastic gradients;new accelerated stochastic;stochastic gradients present;stochastic optimization;optimization stochastic computations;analysis stochastic gradients;using stochastic gradient;stochastic gradients stable;stochastic optimization stochastic"}, "40848b41ed8c9c255ecd8a920006877691b52d03": {"ta_keywords": "distribution shifts datasets;retrofitting distribution shifts;datasets mrna;datasets mrna community;used datasets mrna;distribution shifts generally;shifts datasets typically;shifts datasets;evaluating distribution shifts;distribution shifts;distribution shifts cause;retrofitting distribution;mrna community today;mrna community;mrna;shifts generally relied;shifts encountered wild;relied artificial shifts;shifts generally;artificial shifts need;artificial shifts;shifts;range machine learning;shifts encountered;work retrofitting distribution;kinds shifts;kinds shifts encountered;shifts cause;shifts need represent;datasets typically training", "pdf_keywords": "distribution shifts mrnas;mrna data training;distribution shifts mrna;mrna data benchmarks;mrna sequencing;shifts mrna models;mrna tool predict;datasets distribution shifts;analysis mrna sequencing;existing mrna data;shifts mrnas used;distribution shifts datasets;shifts mrnas;mrna sequencing method;shifts mrnas consider;framework mrna data;mrna data;shifts mrna;shifts mrnas mrnas;mrna tool;benchmark dataset distribution;data analysis mrna;dataset distribution shift;shifts datasets useful;shift wilds benchmark;allocation shifts datasets;mrna processing framework;shifts datasets used;shifts datasets;use mrna tool"}, "2e492af839e971d05592df1c76d4878908e1d4c0": {"ta_keywords": "factor graph grammars;grammars factor graphs;graph grammars factor;introductionfactorfactor graph grammars;graph grammars fggs;factor graphs general;factor graphs;graphs factor graph;graph grammars;factor graphs factor;replacement graph grammars;introductionfactorfactor graph;factor graph;graph grammars useful;graphs factor;factor diagrams;case factor diagrams;sets factor graphs;factor diagrams sum;grammars fggs;grammars fggs short;grammars factor;graphs general;hyperedge replacement graph;replacement graph;graphical models case;graphs;models case factor;graphical models;notation dynamic graphical", "pdf_keywords": "replacement graph grammars;factor graph grammars;grammars factor graphs;context free graphs;graph grammars factor;graph grammars;graph grammars provide;stochastic computation graphs;graph grammars new;networks factor graphs;graphs factor graphs;graph grammars resulting;model factor graphs;graphs using context;graph grammars fggs;markov networks factor;finite graph language;graphs factor graph;graph language;represent bayesian networks;markov networks hyperedge;computation graphs sgs;factor graphs general;factor graphs;hyperedge replacement graph;graphs used stochastic;replacement graph;factor graphs used;graph language makes;graph languages"}, "c66b59394f99d639b277a54ad357d20de30285bd": {"ta_keywords": "text images biomedical;images biomedical literature;biomedical literature slif;literaturee image finder;structured literaturee image;text mining image;literature slif provides;traverse literature slif;literature slif uses;literature slif;biomedical literature uses;context structured literaturee;biomedical literature;figures biomedical literature;information text images;images biomedical;information figures biomedical;topic modeling provide;structured literaturee;latent topic modeling;text images;literaturee image;image finder extracting;mining image;topic modeling;figures biomedical;image finder;text mining;image processing extract;extract information figures", "pdf_keywords": ""}, "8fc728b71f9e92f91455f957f10c7e496cbe4772": {"ta_keywords": "entity typing language;entity typing;propose entity typing;typing language model;types entity mention;semantic types entity;entity mention;introduction enterity typing;enterity typing;semantic types;language model enhancement;entity mention specific;classify semantic types;context sentences labels;typing language;classify semantic;semantic;sentences labels automatically;sentences labels;utilizes language model;language model;enterity typing aims;types entity;using distant supervision;aims classify semantic;context dependent labels;entity;compatibility context sentences;typing;typing aims classify", "pdf_keywords": ""}, "208e5c187e81f63024ece8e2003dbaef094703cb": {"ta_keywords": "malignant neoplasm gastrointestinal;neoplasm gastrointestinal;neoplasm gastrointestinal tract;gastrointestinal tract malignant;tract malignant neoplasm;malignant neoplasm;diagnosed malignant neoplasm;tract malignant;diagnosed malignant;patient diagnosed malignant;malignant;neoplasm;gastrointestinal tract patient;gastrointestinal;case patient diagnosed;gastrointestinal tract;patient diagnosed;diagnosed;present case patient;tract patient;tract patient treated;patient treated combination;case patient;patient treated;tract;article present case;patient;combination;treated combination;combination combination", "pdf_keywords": ""}, "7dadf1e4f6f7a6966d5f691c3707fe221038528b": {"ta_keywords": "computing allocations fair;fair allocations indivisible;maximizing fair allocations;allocations fair maximize;fair allocations;allocations fair;allocations indivisible goods;welfare maximizing fair;fair maximize utilitarian;computing welfare maximizing;fairness concepts envy;maximizing fair;allocations indivisible;tractable fairness concepts;computing welfare;tractable fairness;computing allocations;complexity computing allocations;allocations;indivisible goods;focus tractable fairness;indivisible goods methods;welfare maximizing;welfare sum agents;maximize utilitarian;maximize utilitarian social;fairness concepts;envy freeness item;fair maximize;utilitarian social welfare", "pdf_keywords": "allocation utilitarian maximal;allocation maximizing utilitarian;allocations highest utilitarian;fair maximize utilitarian;allocation utilitarian;computing allocations fair;fair allocations computationally;allocations fair maximize;computational problems utilitarianmaximalthe;utilitarian welfare maximization;fair item allocation;fair allocation indivisible;maximizing utilitarian welfare;efficient equitable allocation;welfare utilitarian maximal;utilitarianmaximalthe utility;pareto efficient equitable;utilitarianmaximalthe utility agent;fair allocations finding;problems utilitarianmaximalthe utility;maximizing utilitarian;um fair allocations;fairness tractable welfare;exists allocation utilitarian;fair allocations;allocations fair;maximize utilitarian;fair allocation;allocation indivisible goods;maximizing utility agents"}, "346081161bdc8f18e2a4c4af7f51d35452b5cb01": {"ta_keywords": "strategyqa question answering;question answering benchmark;creative questions crowdsourcing;questions crowdsourcing;benchmark required reasoning;questions crowdsourcing workers;question answering;answering benchmark;multi hop reasoning;steps answering question;answering benchmark required;crowdsourcing;crowdsourcing workers;elicit creative questions;challenge setup elicit;hop reasoning;reasoning steps implicit;steps answering;reasoning steps;required reasoning steps;crowdsourcing workers covering;hop reasoning required;answering question;required steps answering;reasoning required steps;datasets multi hop;introduce strategyqa question;strategyqa question;fundamental challenge setup;answering", "pdf_keywords": "crowdsourcing generate questions;crowdsourcing pipeline designed;crowdsourcing pipeline;questions strategyqa annotated;multistep crowdsourcing pipeline;strategy questions annotations;questions use crowdsourcing;annotate question decompositions;benchmark evaluating reasoning;collecting strategy questions;annotate strategies evaluating;build multistep crowdsourcing;crowdsourcing generate;human performance answering;strategy answer questions;reasoning process strategies;questions complexity strategy;reasoning strategy questions;annotate strategies;multistep crowdsourcing;crowdsourcing;implement multistep crowdsourcing;annotation pipeline eliciting;reasoning strategy;use crowdsourcing generate;questions strategyqa;evaluating reasoning process;use crowdsourcing;strategies annotate;questions annotations"}, "76df9c90d5359585f5501a4da1af1078c32be6d7": {"ta_keywords": "transcribe images text;unsupervised transcription;transcribe images;unsupervised transcription historical;accurately transcribe images;background unsupervised transcription;rendering glyphs unsupervised;transcribing images documents;transcription historical documents;glyphs unsupervised;inspired historical printing;accurately transcribe;images text methods;able decipher font;transcribe;structure accurately transcribe;historical printing processes;generative probabilistic;modeling text document;images text;modeling text;printing processes transcribing;generative probabilistic model;decipher font structure;rendering glyphs;decipher font;historical printing;process rendering glyphs;glyphs unsupervised able;transcribing images", "pdf_keywords": "modeling text document;manually transcribed text;modeling text;transcribed text text;transcribe images text;transcribed text;text model generate;processing text;manually transcribed;transcribe images;text model;text text model;decoding text historical;generate decoding text;learns font structure;generates images documents;generative modeling;learning glyphs;processing text language;transcription historical documents;transcribing images documents;decoding images text;generative probabilistic model;decoding text;mapping generative modeling;use manually transcribed;model processing text;algorithm decoding text;text language model;accurately transcribe images"}, "88c3f221a6fc8aff014268b0efb5ff119ab40906": {"ta_keywords": "irony detection methods;irony detection datasets;existing irony detection;irony detection;ironic tweets emoji;structures irony detection;irony detection important;introduction irony detection;tweets emoji classifiers;methods existing irony;emojis social media;emoji classifiers;emoji classifiers trained;datasets 10 ironic;ironic tweets;cues emojis social;tweets emoji;10 ironic tweets;verbal cues emojis;cues emojis;structures irony;emojis social;introduction irony;existing irony;emojis;role structures irony;irony;emoji;10 ironic;tweets", "pdf_keywords": ""}, "ce2d6de9cec4a6d135c32bb8d2d02bba09928b33": {"ta_keywords": "text categorization;text categorization discussed;text categorization problems;large text categorization;methods text categorization;categorization discussed;categorization discussed recently;categorization;classifiers allow context;categorization problems algorithms;categorization problems;context sensitive learning;classification ripper;classifiers;contribute classification ripper;classification;sleeping experts phrases;classifiers allow;learning algorithms ripper;ripper sleeping experts;sensitive learning methods;learning methods text;context sensitive;algorithms construct classifiers;implemented machine learning;machine learning algorithms;sensitive learning;machine learning;experts phrases evaluated;experts phrases", "pdf_keywords": ""}, "393c5c96e73dd3a82c175f9ab1f6c083830d3b82": {"ta_keywords": "forecasting company fundamentals;quantitative investing forecasting;evaluating financial health;investing forecasting company;report fundamentals financial;financial health;company methodsa systematic;fundamentals financial data;based quantitative investing;company resultsa systematic;financial health company;investing forecasting;important evaluating financial;fundamentals financial;evaluating financial;financial data;quantitative investing;insight financial health;forecasting company;stock portfolio simulator;systematic analysis;company fundamentals;systematic systematic analysis;systematic analysis performed;health company methodsa;portfolio simulator backtester;company fundamentals important;stock portfolio;methodsa systematic systematic;investing", "pdf_keywords": "stock market prediction;stock stock prediction;stock prediction;lstms stock prediction;stock prediction article;memory networks stock;stock performance model;stock performance;fundamentals financial data;stock previous months;stock market;stock stock markets;networks lstms stock;market prediction;stock performance demonstrate;stocks;networks rns effective;stock markets;neural networks rns;deep neural;stock stock;approach automated stock;current trends stock;sample stock performance;deep neural networks;neural networks;stock previous;future sample stock;consider stocks;trends stock stock"}, "e929c9b53c66d52ae5ea56f0dc2764aef4cc67f6": {"ta_keywords": "channel speech separation;speech separation;speech separation studied;single channel speech;channel speech;datasets mixer chime;mixer chime corpora;datasets mixer;mixer chime;construct datasets mixer;separation;separation studied great;deep learning;separation studied;deep learning based;mixer;speech work;single channel;speech work investigates;based single channel;learning based single;background deep learning;studio interviews dinner;speech;field read speech;read speech;read speech work;featuring studio interviews;chime corpora featuring;robustness representative techniques", "pdf_keywords": ""}, "be28821d510a99ffce40cdcf6860302def8533ef": {"ta_keywords": "users closest content;matching users contents;closest content users;principle matching users;users contents modeled;content recommended users;matching users closest;introductionrecommendation systems;users contents content;content users contents;closest content;matching users;introductionrecommendation systems extremely;contents content providers;content users;users closest;users contents;tools matching users;content providers strategic;content;contents modeled;offered content recommended;fact content providers;content providers;contents content;optimize offered content;content recommended;principle matching;basic principle matching;matching", "pdf_keywords": "user optimal social;neutral mediators game;mediators induce game;mediators game;mediator socially optimal;mediators game established;nash equilibrium social;recommendation systems strategic;principle matching users;mediator neutral players;strategic content providers;users closest content;user welfare mediator;strategic content;market mediator effectively;consider mediator design;proposed mediator arbitrarily;optimal social;content providers strategic;user optimal;designing mediators;proposed mediator;content recommended users;mediator arguably satisfy;social welfare equilibrium;equilibrium social;mediator design;optimal locations users;closest content users;mediator general social"}, "3d2dece28f566792b6dd3a190aa345fc30fee1ff": {"ta_keywords": "urban air mobility;air mobility concept;air mobility;modes transport urban;transport urban areas;transport urban;ground travel demands;introduction urban air;aircraft aggravate congestion;vertiports travelers embark;capacity vertiports travelers;ground travel;congestion ground vehicles;aerial modes transport;urban air;location capacity vertiports;flight;aircraft affect flight;extra ground travel;vertiports travelers;affect flight;mobility concept;travel demands methods;congestion ground;mobility;aircraft affect;ground vehicles;delays aircraft;selecting vertiports location;flight delays", "pdf_keywords": "airground transportation network;transportation network design;transportation network model;ground transportation network;transportation network objective;static traffic equilibria;capacity minimizing traffic;transportation network flow;hybrid airground transportation;vertiport air traffic;air ground network;traffic equilibria satisfy;traffic congestion hybrid;transportation network;capacity vertiports travelers;minimizing traffic congestion;transportation network terms;traffic equilibria particular;congestion network;urban air mobility;aircraft aggravate congestion;traffic equilibria;traffic traffic equilibria;reduce congestion network;airground transportation;air traffic traffic;transportation network nesterov;air traffic;network model equivalent;node capacity travel"}, "eadd73c3e1c20d16e32ee8656c4f954603b37450": {"ta_keywords": "note onset detection;onset detection;onset detection case;approach note onset;note onset;auditory events visual;auditory scenes;auditory scenes stereo;complex auditory scenes;introduction auditory events;auditory events;identifying musicians playing;introduction auditory;scenes stereo mixdown;auditory;musical ensembles methods;scenes stereo;identifying musicians;complex auditory;understanding complex auditory;large musical ensembles;musical ensembles;stereo mixdown;onset;musical;audio;musicians;visual information;musicians playing;knowledge visual", "pdf_keywords": "visual onset detection;auditory scenes;note onset detection;onset detection visually;complex auditory scenes;novel audiovisual dataset;audiovisual dataset;onset detection;auditory scenes paper;spatially oriented auditory;novel audiovisual;audiovisual;annotationthe visual onset;onset detection case;challenging clarinetist videos;audiovisual dataset hours;detecting noise spatially;onset detection problem;deep learning music;clarinetist videos;oriented auditory;introduce novel audiovisual;release audiovisual dataset;new 3d convolutional;hours clarinetist videos;3d convolutional neural;pooling release audiovisual;oriented auditory domain;note onset;auditory"}, "a4e937f0b6e0688f7f3c4fcaebbabefa4a36da85": {"ta_keywords": "text speech e2e;toolkit text speech;speech e2e ts;text speech;speech e2e;training neural vocoders;neural vocoders;e2e ts toolkit;ttsch models extensions;ttsch models;state art ttsch;ts toolkit;neural vocoders state;new toolkit text;text;e2e ts;art ttsch models;vocoders;toolkit text;ttsch;training neural;vocoders state art;speech;e2e ts developed;developed e2e ts;ts developed e2e;flexible pre processing;version e2e ts;features including thefly;joint training neural", "pdf_keywords": "text speech e2e;modeling text speech;text speech;speech integrating text;new speech processing;processing e2e speech;speech speech e2e;speech processing e2e;text speech systems;e2e speech speech;improved quality speech;quality speech improved;speech learning integrate;speech e2e speech;model speech based;generate natural speech;speech e2e;text speech synthesis;speech learning;processing speech;speech e2e ts;e2e speech;speech processing tasks;speech speech processing;natural speech improve;models speech;processing speech integrating;model speech;quality speech;speech processing"}, "2135c44087e06a6d95d04ad0afa400e926d37944": {"ta_keywords": "adortive speech recognition;speech recognition;speech recognition possible;improves speech recognition;speech recognition performance;map based bayesian;map estimation;noralization adortive speech;emplying map estimation;posteriori map based;model space adaptation;method improves speech;map estimation purpose;normalization feature spaces;applied normalization feature;based bayesian estimation;improves speech;bayesian estimation;normalization feature;posteriori map;bayesian estimation approach;adaptation data methods;adaptation data;maximum posteriori map;applied normalization;space adaptation method;consistently applied normalization;adortive speech;normalization;feature model space", "pdf_keywords": ""}, "99c80d608ba2aa638333f27bbe3f09cdc580a051": {"ta_keywords": "gpu compatible automatically;designed gpu;designed gpu compatible;gpu compatible;machine learning;machine learning applications;gpu;deployment machine learning;compatible automatically differentiable;blocks designed gpu;automatically differentiable production;automatically differentiable;compatible automatically;learning applications;differentiable production ready;ready building blocks;purposeto accelerate development;building blocks designed;differentiable production;ready building;automatically;accelerate development;production ready building;building blocks;learning;blocks designed;learning applications researchers;ready resultsthe building;development;purposeto accelerate", "pdf_keywords": "speech recognition toolkit;speech toolkit;generalpurpose speech toolkit;speech toolkit based;toolkit generalpurpose speech;priniles toolkit;speech recognition;toolkit integrates neural;priniles toolkit designed;audio speech functionalities;priniles priniles toolkit;analysis audio;audio speech;kaldi speech recognition;basic audio speech;development neural audio;recognition toolkit;toolkit functionalities available;implementation torchaudio toolkit;recognition toolkit generalpurpose;neural audio;audio;spectrogram computation unified;developed toolkit integrates;toolkit functionalities;basic audio;functionalities like audio;approach neural audio;toolkit integrates;tools field audio"}, "29bc6654abd34b2405f7a01341f790aed2aab9a4": {"ta_keywords": "distributed storage codes;storage codes methodswe;distributed storage;storage codes;storage codes efficient;designing distributed storage;efficient distributed storage;download efficient distributed;storage;piggybacking design framework;designing distributed;codes efficient data;node repair;efficient data read;data read downloaded;piggybacking design;read download efficient;efficient distributed;smallest data read;codes methodswe present;codes methodswe;data read;distributed;read downloaded repair;downloaded node repair;read downloaded node;codes efficient;framework designing distributed;new piggybacking design;read download", "pdf_keywords": ""}, "b35ad59ce9a3ea01a0980c90bc750273d1f99e7a": {"ta_keywords": "information retrieval;similarity databases contain;statistical information retrieval;similarity databases;information retrieval results;textual similarity databases;queries based textual;providing database like;retrieval results implemented;database like;names correspond entities;web using queries;queries based;retrieval results;names measured using;using queries based;database;names correspond;based textual similarity;databases;introduction providing database;database like access;names measured;data integration based;databases contain;using queries;retrieval;textual similarity;providing database;queries", "pdf_keywords": ""}, "04e0fb8b3bb06e1200288e6d2a17d55773e97504": {"ta_keywords": "downlink users cellular;users cellular network;location aware optportunistic;location dependent opportunistic;mobile users stocochastic;cellular networks methods;cellular network;cellular networks;users cellular;learning cellular networks;opportunistic bandwidth sharing;mobile moving downlink;dependent opportunistic bandwidth;static mobile users;cellular network cell;opportunistic bandwidth;optportunistic bandwidth sharing;static users mobile;sharing static mobile;users stocochastic learning;bandwidth sharing static;static mobile moving;cell network;cell network fixed;mobile moving;static mobile;stocochastic learning cellular;bandwidth sharing;aware optportunistic bandwidth;dependent opportunistic", "pdf_keywords": "cellular network policy;bandwidth sharing cellular;bandwidth allocation learning;downlink users cellular;users cellular network;allocation bandwidth mobile;sharing cellular networks;cellular networks user;bandwidth allocation mobile;cellular network users;cellular networks randomizing;cellular network;cellular networks share;performance cellular networks;opportunistic bandwidth allocation;wireless cellular networks;cellular networks propose;allocate bandwidth mobile;opportunistic allocation bandwidth;cellular networks;consider cellular network;bandwidth mobile users;bandwidth mobile;wireless cellular;dependent opportunistic bandwidth;sharing cellular;bandwidth sharing optimal;convergence cellular networks;optimize performance cellular;mobile consider cellular"}, "016d83091a60a6de67ba2395c063967686043380": {"ta_keywords": "estimation clustering speech;estimation clustering acoustic;clustering acoustic model;acoustic model adaptation;clustering speech recognition;clustering speech;clustering acoustic;bayesian estimation clustering;variational bayesian estimation;clustering triphone;clustering triphone states;structure clustering triphone;speech recognition vbeck;introductionapplication variational bayesian;variational bayesian approach;variational bayesian;using variational bayesian;estimation clustering;apply variational bayesian;speech recognition;recognition vbeck acoustic;adaptation apply variational;model adaptation;acoustic model;model structure clustering;model adaptation methodsthe;bayesian estimation;estimate parameter posteriors;parameter posteriors model;model adaptation apply", "pdf_keywords": ""}, "06708348b64e2e7b11a953389556c701bf3298da": {"ta_keywords": "sex specific sex;specific sex specific;sex specific;specific sex;role sex specific;literature role sex;role sex;sex;purpose article;literature role;specific;purpose article present;article present literature;present literature role;literature;article;present literature;purpose;role;article present;present", "pdf_keywords": ""}, "0098123efc851b67137c1028f7bac8d8bffbc8fd": {"ta_keywords": "semantic parsing methodsawakening;models semantic parsing;semantic parsing;conclusionsawakening latent grounding;grounding pretrained language;semantic parsing conclusionsawakening;methodsawakening latent grounding;semantic parsing resultsawakening;latent grounding pretrained;latent grounding;parsing methodsawakening latent;resultsawakening latent grounding;parsing conclusionsawakening latent;backgroundawakening latent grounding;pretrained language models;parsing;language models semantic;parsing resultsawakening latent;parsing methodsawakening;parsing conclusionsawakening;semantic;grounding pretrained;models semantic;pretrained language;grounding;parsing resultsawakening;language models;backgroundawakening latent;conclusionsawakening latent;methodsawakening latent", "pdf_keywords": "latent grounding semantic;latent grounding annotated;grounding semantic parsing;backgroundpretrained language models;grounding semantic;grounding annotated databases;pretrained language models;auxiliary concept prediction;guiding prediction words;semantic parsing;awaken latent grounding;language models erasing;grounding pretrained language;latent grounding understandable;grounding annotated;grounding awaken latent;parsers significantly improve;annotated databases;awakening latent grounding;prediction words;natural language processing;semantic parsing pls;latent grounding pretrained;backgroundpretrained language;latent grounding understood;latent grounding;supervised approach awaken;language models understandable;linking useful learning;latent grounding new"}, "45f59bd3ef8e1d76474199c08c140675c04a728c": {"ta_keywords": "agent learning agents;learning agents;learning agents anticipate;agent learning;multi agent learning;conjectures learning processes;implicit conjecture learning;forming conjectures learning;conjectures learning;conjecture learning;conjecture learning schemes;conjectures conjecture learning;processes devise learning;learning processes devise;conjecture learning leads;multi agent;learning schemes;simultaneous gradient play;devise learning;learning schemes lead;agents anticipate;gradient play empirically;learning rules;devise learning rules;learning;learning processes;framework multi agent;agents anticipate reactions;variations equilibrium;gradient play", "pdf_keywords": ""}, "a13c580250af3644fe368b08a540f4ea65dac919": {"ta_keywords": "phasecode noiseless;phasecode noiseless case;recovering complex signal;architecture phasecode noiseless;recovers sample complexity;complexity theta l_m;log computational complexity;complexity theta log;phasecode;linear scheme recovers;computational complexity theta;sample complexity theta;architecture phasecode;complex signal mathbb;complexity theta;computational complexity;sample complexity;signal mathbb cn;mathbb cn intensity;complex signal;recovering complex;signal mathbb;theta log computational;use architecture phasecode;a_i mathrmhx leq;complexity;intensity measurements form;noiseless case robustify;problem recovering complex;scheme recovers sample", "pdf_keywords": ""}, "0b2e9e978898b9fb1116ea964c8c470086ceed87": {"ta_keywords": "salient object detection;rgb salient object;level feature fusion;feature fusion;rgb salient;multi modal cues;feature fusion fundamental;object detection devise;salient object;object detection;multi level features;nature rgb salient;backgroundmulti level feature;multi modal learning;modal multi level;modal cues;modal learning;segment classify objects;aggregation multi modal;salient;features meet multi;modal cues optimal;level features;vision exploited detect;level features meet;detect segment classify;modal multi;level nature rgb;multi modal;multi modal multi", "pdf_keywords": "salient object detection;detection salient objects;saliency detection;effective predicting saliency;salient objects images;predicting saliency;detecting salient objects;salient objects image;rgbd salient object;rgb salient object;predicting saliency maps;method saliency detection;detection salient;scenarios rgb salient;salient objects spatial;cues depth modality;saliency detection segmentation;context salient objects;method predicting saliency;saliency;saliency maps using;rgb salient;approaches analysis saliency;approaches salient object;detect salient objects;analysis saliency;approach detecting salient;approach detection salient;saliency maps;rgbd salient"}, "0b8dbc4a899c836fe2b1a213b9dc064cdf62fd63": {"ta_keywords": "perturbations procedural text;procedural text goal;explanations paragraphs modeling;procedural text present;modeling explanation task;procedural text;constructs explanations paragraphs;explanations paragraphs;paragraphs modeling explanation;explanation task multitask;paragraphs modeling;multitask learning;task multitask learning;quantitatively constructs explanations;explanation task;constructs explanations;text goal explain;text;multitask learning problem;text present constructs;procedural;text present;perturbations procedural;modeling explanation;effects perturbations procedural;text goal;paragraphs;multitask;task multitask;explanations", "pdf_keywords": "natural language explanations;explanation task trained;language explanations qualitative;commonsense reasoning visual;visual reasoning;reasoning visual;procedural text understanding;reasoning visual reasoning;language explanations;explanation task identifying;natural language inference;visual reasoning recently;text understanding;learns explain;text good explanation;text understanding explain;prediction explanation structure;knowledge commonsense reasoning;knowledge natural language;natural language;natural language models;explanations qualitative structure;explanations qualitative;explanations frombackgroundquantitative reasoning;learns explain improve;goal procedural text;explanation task;enriching natural language;constructs explanations;procedural text methodswe"}, "f07c5c540233b22f0ca154c80c713e2aed3c9606": {"ta_keywords": "musical structure models;minute long compositions;music generation;approach music generation;largescale musical structure;long compositions;largescale musical;models using transformers;music generation goal;exhibit largescale musical;synthesizing minute long;long compositions exhibit;sequence autoregressive manner;musical structure;musical;sequence autoregressive;autoregressive manner unfortunately;music;synthesizing;autoregressive manner;synthesizing minute;observed sequence autoregressive;approach music;samples models;compositions exhibit largescale;models;dominant approach music;samples models tends;compositions;compositions exhibit", "pdf_keywords": ""}, "d15a7d00897f58a94def2a58c0cb0311851f2968": {"ta_keywords": "distant speech recognition;kaldi speech recognition;speech recognition toolkit;speech enhancement baseline;setup speech enhancement;recognition speech processing;speech recognition;speech recognition speech;automatic speech recognition;speech enhancement;speech processing;recognition using chime;repository kaldi speech;recognition speech;chime challenge setup;speech recognition using;using chime challenge;using chime;noisy automatic speech;speech processing communities;automatic speech;setup speech;chime challenge;challenge setup speech;distant speech;recognition toolkit;recognition toolkit methodsthe;art distant speech;chime;backgroundbuilding", "pdf_keywords": "enhancement speech recognition;speech recognition new;improvement speech enhancement;neural network speech;neural speech enhancement;network speech enhancement;speech enhancement speech;speech recognition speech;speech recognition combine;speech enhancement;processing noise speech;speech recognition;speech enhancement measures;enhancement speech;recipe speech enhancement;new neural speech;speech enhancement using;recognizing speech challenging;recognition speech;recognizing speech;speech challenging noisy;baseline automatic speech;different speech enhancement;speech recognition ar;context speech recognition;automatic speech;speech processingin;speech recognition report;neural speech;scenario recognizing speech"}, "04b44c518b145be625ff270af56cfd2e37900137": {"ta_keywords": "aware speaker diarization;speaker diarization single;overlap aware speaker;distributed micro headphones;speaker diarization;end neural diarization;neural diarization eend;diarization eend;headphones promising method;micro headphones promising;micro headphones;headphones promising;neural diarization distributed;headphones;neural diarization;diarization single neural;neural diarization en;diarization distributed micro;diarization eend recent;aware speaker;diarization distributed;channel end end;diarization;multi channel;using multi channel;diarization en;introductionmulti channel end;end end neural;diarization single;channel end", "pdf_keywords": "multi channel speech;single channel speech;channel speech processing;channel speech activity;speaker diarization single;speech activity single;aware speaker diarization;diarization speech speakers;channel speech;reverberant speech recognition;encoder used speech;speaker conversational dataset;speaker diarization;overlap aware speaker;speech processing;speech recognition;speech activity;multi channel encoders;approach speaker diarization;utterances speakers;diarization speech;speech speakers;distributed microphones methods;distributed microphones;domain adaptation speaker;cellular reverberant speech;based distributed microphones;microphone;outputs reverberant speech;encoders process multichannel"}, "a56dba9cabfc110df231051d7c9d6e439f6757dd": {"ta_keywords": "adaptable speech tagging;speech tagging;reranking adaptable speech;speech pos tagging;speech tagging aimto;sequence based reranking;tagging highly;pos tagging highly;based reranking adaptable;tagging;adaptable speech;pos tagging;reranking adaptable;trainable partially annotated;tagging highly domain;introductionpointwise prediction sequence;partially annotated;method speech pos;introductionpointwise prediction;accurate method speech;based reranking;prediction sequence based;reranking;speech pos;method speech;tagging aimto develop;annotated;tagging aimto;prediction sequence;speech", "pdf_keywords": ""}, "2ab9fd2be2bf82e0bbd558cc64c1c46728fc4f8a": {"ta_keywords": "adaptation automatic speech;change speech characteristics;model adaptation automatic;automatic speech;multiscale adaptation;automatic speech recognition;model adaptation;adaptation automatic;adaptations multiscale adaptation;speech recognition;multiscale adaptation potential;time incremental adaptations;incremental adaptations multiscale;change speech;conversation temporal changes;objective change speech;multiple time scalee;introduction model adaptation;incremental adaptations;speech recognition based;adaptations multiscale;speech characteristics;adaptation;speech characteristics originated;conversation temporal;time scalee evolution;include adaptation;world conversation temporal;robustness include adaptation;single time incremental", "pdf_keywords": ""}, "f17e182fcb7fbbff2257824174ed6f7df512a42b": {"ta_keywords": "end speech recognition;speech recognition attention;introductionmulti encoder multi;cc attention model;connectionist tempral classification;speech recognition ar;speech recognition;encoder multi;training joint decoding;joint cc attention;automatic speech recognition;recognition attention based;attention model achieved;attention model;tempral classification cc;end automatic speech;end end speech;recognition attention;automatic speech;end speech;classification cc network;multi task training;encoder multi resolution;introductionmulti encoder;tempral classification;joint decoding;connectionist tempral;cc attention;encoder;joint decoding work", "pdf_keywords": "end speech recognition;attention network;hierarchical attention network;propose multi encoder;multi endcoder;attention network model;multi encoder;encoders encodersin speech;propose multi endcoder;multi encoder multi;multi endcoder multi;single encoder models;novel multi encoder;training joint decoding;endcoder multi;encodersin speech;encoder multi;cc attention model;cc speech recognition;encoder models;endcoder multi resolution;rnn based encoders;model encoder;encoder multi resolution;encodersin speech acoustic;joint cc attention;attention model;network model encoder;resolution architecture encoders;hierarchical attention"}, "72b4ff7387223cf0398c298c3cc62ee07d9c0043": {"ta_keywords": "language models probability;completion challenging semantic;language models;simple language models;sentence estimated probability;probability sentence estimated;semantic modeling;semantic modeling task;language models applied;challenging semantic modeling;senstence completion;incorporate syntactic information;models probability sentence;sentence estimated;syntactic information;introduction senstence completion;incorporate syntactic;syntactic information methods;senstence completion challenging;semantic;approaches incorporate syntactic;syntactic;variety language models;completion;challenging semantic;complete sentence variety;models probability;task models;modeling task models;probability sentence", "pdf_keywords": ""}, "57fec656119e82b5e70b1a654f6d87d8c1137ef4": {"ta_keywords": "contents biological articles;biological articles typical;biological articles;scientific contents biological;scholarly articles scientific;results scientific contents;articles scientific;articles typical figure;captioned text;contents biological;scientific contents;captioned text text;global captioned text;source information crucial;semantic entities protein;crucial informative scholarly;text caption;entities protein names;major source information;scholarly articles;figures directly provide;caption contains important;global captioned;informative scholarly articles;caption;informative scholarly;information crucial informative;articles scientific journals;biological;entities protein", "pdf_keywords": ""}, "2797a36cd15b8c046683247995261546993c289d": {"ta_keywords": "modelbased speech enhancement;temporal speech noise;speech noise modeling;speech enhancement pre;speech enhancement;stationary noise based;spectral temporal speech;speech recognition;speech noise;introduction speech recognition;time varying noise;speech recognition presence;modelbased speech;noise modeling combined;recognizing speech;approach modelbased speech;recognizing speech presence;dynamic variance adaptation;temporal speech;varying noise sources;noise modeling;noise based spatial;noise based;non stationary noise;stationary noise;introduce recognizing speech;varying noise;pre processor adaptation;variance adaptation methods;speech presence", "pdf_keywords": ""}, "99bb811beb5d061d2b8fac5a1973b49cace93e2f": {"ta_keywords": "topic model tracking;tracking timevarying consumer;analyzing consumer purchase;interests item trends;consumer interests item;behavior consumer interests;topic model;new topic model;timevarying consumer purchase;consumer purchase behavior;purchase behavior consumer;interests trends based;track changes interests;purchase behavior methodswe;estimated interests trends;consumer interests;model analyzing consumer;item trends change;current purchase logs;item trends;tracking model analyzing;changes interests trends;analyzing consumer;purchase behavior;interests trends;timevarying consumer;model adaptively track;interests trends resultsthe;consumer purchase;model tracking timevarying", "pdf_keywords": ""}, "291b651654565cd88e4e56de5250219a71882a50": {"ta_keywords": "peer selection problem;subset winners peer;selection problem peer;peer selection;problem peer selection;winners peer;grants prizes condorcet;agents select subset;winners peer reviewed;prizes condorcet;prizes condorcet view;selection noisy assessments;group agents select;subset winners;peer reviewed grants;selection problem group;ordering agents;assessments problem peer;truth ordering agents;problem group agents;select subset winners;aggregation;agents select;selection;selection problem;aggregation problem ground;best set agents;peer;ordering agents wish;reviewed grants prizes", "pdf_keywords": "peer nomination weightedpeernosmination;peer selection algorithm;method peer selection;strategyproof peer selection;winners peer reviewed;agents improving peer;subset winners peer;winners peer;improving peer nomination;peer selection;algorithm weightedpeer nomination;peer nomination effectiveness;noisy assessments peers;improve peer nomination;nomination weighs reviewers;peer selection uses;peer selection problem;peer nomination simple;peer nomination;assessments peers relevance;nomination weightedpeernosmination;peers relevance;algorithm selection agents;reviewers act adversarially;disagreement reference ranking;selection algorithm weightedpeer;peer grading;improving peer;weightedpeer nomination;reweigh scores weightedpeernosmination"}, "9cf4609178e2739ed35f8d3e3d6efb7d5e2e1a41": {"ta_keywords": "unstable behavior traffic;detection unstable behavior;traffic dynamics methodswe;behavior traffic dynamics;approach instability detection;mitigation signalized traffic;instability detection;instability detection mitigation;operator approach instability;signalized traffic;traffic dynamics;detection unstable;automated detection unstable;detection mitigation signalized;unstable eigenvalues learned;introductionkoopman operator approach;signalized traffic objectivethe;koopman operator approach;behavior traffic;dynamics anomaly feature;application koopman operator;introductionkoopman operator;mitigation signalized;sequences unstable eigenvalues;eigenvalues learned dynamics;koopman operator;learned dynamics anomaly;operator approach automated;detection mitigation;unstable eigenvalues", "pdf_keywords": ""}, "74495e9735b601ce9060ac40ac27d196fdbf7462": {"ta_keywords": "codes distributed storage;storage distributed;distributed storage;storage distributed storage;distributed storage distributed;distributed storage setting;regenerating codes distributed;codes distributed;storage;stored nodes network;storage setting introduced;stored nodes;storage setting;data stored nodes;distributed;introduced dimakisemiakis flexible;dimakisemiakis flexible;dimakisemiakis units data;setting introduced dimakisemiakis;introduced dimakisemiakis units;dimakisemiakis flexible class;regenerating codes;dimakisemiakis units;introduced dimakisemiakis;stored;nodes additionally repair;class regenerating codes;dimakisemiakis;recovered connecting nodes;data stored", "pdf_keywords": ""}, "f75ba81828fd9d8c7fcba89dd98a0ee73d32dce6": {"ta_keywords": "microbial malignant neoplasm;neoplasm microbial malignant;malignant neoplasm microbial;neoplasm microbial patient;neoplasm microbial;microbial malignant;microbial patient;malignant neoplasm;diagnosed malignant neoplasm;microbial patient managed;diagnosed malignant;patient diagnosed malignant;malignant;neoplasm;microbial;case patient diagnosed;patient diagnosed;report case patient;diagnosed;report case;case patient;patient managed;patient managed combination;patient;report;managed combination;combination;managed combination combination;managed;combination combination", "pdf_keywords": ""}, "58fd3001c88e9784b0794eef06cb7c0eab0d8747": {"ta_keywords": "picture synthesis systems;text picture synthesis;text picture synthsynthesis;picture synthesis;picture synthsynthesis;picture synthsynthesis systems;ontology based;introductionan ontology based;synthesis systems;ontology;ontology based approach;introductionan ontology;synthesis systems paper;approach text picture;present ontology based;text picture;synthesis;present ontology;paper present ontology;synthsynthesis systems paper;synthsynthesis systems;based approach text;synthsynthesis;text;approach text;systems;based approach;based;systems paper;picture", "pdf_keywords": ""}, "c9472731afe5fca98f362f49e26d17a9d5d0cc8e": {"ta_keywords": "say machine learning;machine learning algorithms;machine learning suffers;learning algorithms opaque;usefulness machine learning;algorithms opaque human;machine learning;problem machine learning;learning algorithms led;learning algorithms;usefulness machine;introductionthe usefulness machine;algorithms led widespread;concept object;algorithms opaque;opaque human users;concept object purpose;concept;learning;human users;algorithms;black box;black box problem;situation say machine;end concept object;machine;object purpose;algorithms led;problem machine;usefulness", "pdf_keywords": "interpretability fundamental;define interpretability like;define interpretability;concepts interpretability explicability;algorithms interpretability;definition interpretability;interpretability explicability;concept interpretability enjoyed;interpretability;concept interpretability fundamental;concept interpretability;definition interpretability approach;interpretability fundamental issue;concepts interpretability;technical definition interpretability;algorithms interpretability framework;intelligibility explicability interpretability;concept interpretability needs;interpretability needs;problems notion interpretability;argued concepts interpretability;interpretability purpose;interpretability enjoyed;interpretability like;interpretability algorithm;interpretability conceptual;interpretability framework;explicability interpretability;word interpretability;interpretability explanation"}, "b103bb1dc05a48796a3ff0804c11909bf68db11b": {"ta_keywords": "detecting speculative language;task detecting speculative;detecting speculative;speculative language biomedical;speculative language using;speculative language;introduction detecting speculative;detection sentences;language biomedical text;detection sentences containing;task1 token classification;syntactic dependenceencies logisticregression;token classification task;language biomedical;language using syntactic;token classification;sentence label;determines sentence label;biomedical text results;syntactic dependenceencies;using syntactic dependenceencies;treated detection sentences;biomedical text;syntactic;information task1 token;determines sentence;using syntactic;task detecting;speculative;classification task", "pdf_keywords": ""}, "83ac0851a8f6fa02f5db251b260f635907d7a01e": {"ta_keywords": "vision language navigation;language navigation challenge;language navigation;backtracking fest navigategator;backtracking vision language;action decoding achieves;action decoding;navigation;navigation challenge;self correction backtracking;correction backtracking vision;r2r vision language;navigategator;language navigation methodswe;navigategator general;backtracking vision;aware search backtracking;navigation challenge result;rewind self correction;fest navigategator;vision language;framework action decoding;search backtracking fest;purposetactical rewind;self correction;purposetactical rewind self;fest navigategator general;navigategator general framework;search backtracking;room r2r vision", "pdf_keywords": "vision language navigation;novel navigation task;action decoding achieves;action decoding;navigation indoor scenes;approach visual navigation;visual navigation;asynchronous search neural;view agents navigating;search neural decoding;room vision language;visual navigation indoor;navigation task;scenes using deep;visual navigation ability;navigation task present;gains visual navigation;approach navigation;navigation challenge anderson;novel navigation;aware search backtracking;navigate;vision language;search neural;backtracking fest navigategator;framework action decoding;new approach navigation;language navigation challenge;neural decoding extensible;neural decoding"}, "b02acb3d159b06b2319a164378e1e61c3983676f": {"ta_keywords": "complex query answering;query answering knowledge;answering knowledge graphs;generalizability complex query;query answering cq;query answering;knowledge graphs;knowledge graphs current;answering knowledge;introduction complex query;complex query;task knowledge graphs;answering cq;answering cq important;qq learning;qq learning models;current qq learning;reasoning task knowledge;combinatorial generalizability complex;benchmark combinatorial generalizability;cq important reasoning;graphs current qq;combinatorial generalizability methods;query;able generalize atomic;formulas regarded combinatorial;qq;combinatorial generalizability;cq;regarded combinatorial generalizability", "pdf_keywords": "generalizability complex query;answering models cq;query answering models;cq knowledge graphs;complex query answering;queries useful construction;queries combinationatorial generalizability;queries abstract level;dataset knowledge graphs;queries abstract;query answering cq;query cq knowledge;knowledge graphs extend;answering models;query types improved;knowledge graphs including;formulas query types;answering qq models;unseen query types;query answering;hand crafted query;query answering qq;queries fully supported;abstract complex query;queries types;knowledge graphs;query types long;represent queries;models complex query;query expressive family"}, "eaa224ae5c969180503dda4972ab86d3a71c888c": {"ta_keywords": "polyphonic music recognition;polyphonic datasets;optical music recognition;polyphonic datasets suitable;end approaches polyphonic;music recognition;recognition sheet music;approaches polyphonic music;polyphonic optical music;scale polyphonic datasets;music recognition building;polyphonic music;end polyphonic optical;large scale polyphonic;end end polyphonic;end polyphonic;approaches polyphonic;music recognition methodswe;polyphonic optical;polyphonic;end end recognition;end recognition;end recognition sheet;optical music;scale polyphonic;sheet music;sheet music publicly;recognition building encoder;recognition sheet;datasets suitable end", "pdf_keywords": ""}, "baad427b45ac691763fe3de4ea3ac1bffd3c74e3": {"ta_keywords": "partytracker new visualization;partytracker visualizing;partytracker visualizing changes;present partytracker visualizing;visualizing changes party;information visualization increasingly;information visualization;partytracker;visualizing changes;partytracker new;visualization tool;visualization increasingly;time information visualization;party affiliation survey;new visualization tool;visualization tool allows;visualization increasingly important;present partytracker new;present partytracker;new visualization;visualization;temporal geographic party;geographic party affiliation;party affiliation time;party affiliation;visualizing;geographic party;paper present partytracker;context information visualization;changes party affiliation", "pdf_keywords": ""}, "64280761641d8f1eb285165160bd96efac0bb5f5": {"ta_keywords": "recognizing speech environmental;speech environmental sounds;auditory environment recognizing;treat environmental sounds;environmental sounds construct;speech environmental;environmental sounds;recognizing speech;studies environmental sounds;environmental sounds noise;backgroundin speech interfaces;speech recognition;speech recognition ar;sounds construct classifiers;automatic speech recognition;environmental sounds standard;environment recognizing said;automatic speech;classifiers environmental;environment recognizing;auditory environment;surrounding utterance automatic;backgroundin speech;overall auditory environment;difficult recognizing speech;speech interfaces;construct classifiers environmental;utterance automatic speech;sounds noise remove;utterance automatic", "pdf_keywords": ""}, "2391e7446d47f681ad705c8e75d9d2ce1b92ad5f": {"ta_keywords": "german corpus compilation;corpus compilation annotation;german corpus;revised corpus predecessors;corpus predecessors;introductionrem reference corpus;high german corpus;revised corpus;corpus compilation;reference corpus middle;available corpus;reference corpus;revised revised corpus;corpus;published available corpus;corpus predecessors methodsall;available corpus search;corpus middle;annotation standards;developing common annotation;common annotation;corpus search tool;common annotation standards;corpus search;annotation;compilation annotation;annotation standards allow;annotation access;compilation annotation access;corpus middle high", "pdf_keywords": ""}, "ff7e60b8d336aef5ed974609a63610641085177e": {"ta_keywords": "softmax;introduce softmax;introduced introduce softmax;reward function structured;structured prediction tasks;task specific reward;specific reward;optimize reward;directly optimize reward;prediction tasks;optimize reward function;prediction tasks incorporates;reward performing maximum;structured prediction;specific reward performing;reward function;reward performing;reward;function structured prediction;maximum likelihood updates;higher probabilities candidates;augmented maximum likelihood;likelihood updates candidate;performing maximum likelihood;probabilities candidates;maximum likelihood;gives higher probabilities;learning framework;learning;candidate outputs", "pdf_keywords": "reward model ml;reward model;specific reward model;softmax quantum distribution;propose softmax quantum;softmax quantum;theoretical interpretation reward;reinforcement learning;approximation softmax quantum;free reinforcement learning;model free reinforcement;softmax distribution training;reinforcement learning ll;decision theory softmax;interpretation reward;theory softmax distribution;specific reward;reward augmented maximum;softmax distribution regarded;distribution approximation softmax;reward function;softmax distribution estimation;raml bayesian decision;task specific reward;bayes decision boundary;reward payoff distribution;softmax distribution;interpretation reward augmented;introduce softmax distribution;prediction function learned"}, "4d991a83d6044b1aaed2c117b3d097ecd23cf6f4": {"ta_keywords": "speaker diarization clustering;diarization clustering speaker;speaker embeddings clustering;clustering speaker embeddings;speaker diarization;outputs speaker diarization;approach speaker diarization;speaker diarization results;clustering speaker;speaker embeddings;end neural diarization;diarization clustering;diarization eend neural;diarization eend;speaker recording methodsto;neural diarization;speaker recording;multi speaker recording;neural diarization eend;given multi speaker;multi speaker;embeddings clustering;outputs speaker;embeddings clustering based;diarization;eend neural network;approach speaker;speaker;diarization results;directly outputs speaker", "pdf_keywords": "outputs speaker diarization;speaker diarization;approach speaker diarization;speech diarization challenge;speaker diarization results;speech recognition interdisciplinary;speaker embeddings cluster;large dataset speaker;speech diarization;recognition speech separation;dataset speaker;speaker diarization problem;approach speech diarization;multi speaker conversations;automatic speech;captures global speaker;speech recognition large;speaker diarization context;outputs joint speech;speaker dialogue recordings;generate speaker embeddings;recognition speech;multi speaker recording;speaker embeddings;speech vector clustering;speech detection audio;end neural diarization;dataset speaker dialogue;speech recognition neural;speech recognition"}, "9f2cf7b35224aad3a8d261e4456fe2d65a5f5d3e": {"ta_keywords": "dual encoders retrieval;encoders retrieval tasks;encoders retrieval;size dual encoder;usefulness dual encoders;dual encoder;dual encoder model;dual encoders;decoders generalizable retrievers;introductionlarge dual decoders;dual decoders generalizable;dual decoders;decoders generalizable;encoder;encoder model;encoders;decoders;encoder model keeping;retrieval;generalizable retrievers;retrieval tasks;retrieval tasks methodsin;retrieval tasks especially;introductionlarge dual;bottleneck embedding size;bottleneck embedding;generalizable retrievers objectiveto;keeping bottleneck embedding;training surprisingly scaling;variety retrieval tasks", "pdf_keywords": "encoders dense retrieval;achieve generalizable retrieval;generalizable retrieval;scaling improve retrieval;retrieval models training;generalizable retrieval models;retrieval models surprisingly;retrieval model sparse;retrieval benchmark;retrieval use heterogeneous;retrieval performance dual;dense retrieval;neural retrievals open;dense retrieval use;retrieval models;improve retrieval performance;model neural retrieval;sparse dense retrievers;retrieval models multiple;neural retrieval majority;large dual encoder;shot retrieval benchmark;improve retrieval;bottleneck embedding dimension;improvement retrieval performance;retrieval models use;retrieval performance;retrievals;dual encoder training;sparse lexical retrieval"}, "d18d8d364bb18d66924919feebb2e892ebe6761c": {"ta_keywords": "neural machine translation;translation sm neural;statistical machine translation;machine translation sm;machine translation nm;machine translation phrase;machine translation;phrase based decoding;translation phrase based;phrase based sm;decoding cost nm;phrase based forced;translation sm;model compute phrase;forced decoding compared;sm neural machine;translation nm;based decoding cost;decoding cost;sm neural;forced decoding;decoding compared;based forced decoding;translation phrase;improving neural machine;decoding compared traditional;compute phrase based;existing phrase based;compute phrase;based decoding", "pdf_keywords": "neural machine translation;translation neural;statistical machine translation;translation neural network;translation sm neural;machine translation model;unrelated translation neural;naturally improve translation;translation significantly improved;translation application neural;machine translation;french translation tasks;machine translation nm;nonlingual translation nm;translation rules forced;machine translation sm;machine translation important;improve unrelated translation;translation tasks;phrasebased decoding cost;nonlingual translation;translation tasks obtaining;machine translation bmt;translation model;improve translation translation;improvements strong nontranslation;translation using forced;reranking translation rules;phrase based translation;translation rules"}, "1b96b89d5b3ba444126cebefdfc665d3866f14f0": {"ta_keywords": "algorithmic hiring;algorithmic hiring aimwe;explainability algorithmic hiring;interview algorithms models;site interview algorithms;interview algorithms;modern hiring pipeline;hiring increasingly technologies;modern hiring;make modern hiring;hiring;hiring pipeline;hiring increasingly;role hiring;role hiring increasingly;gets site interview;site interview;hiring pipeline resumes;fairness explainability algorithmic;resumes selected;resumes;hiring aimwe;introductionwe need fairness;interview;plays role hiring;automation;hiring aimwe make;algorithms models potentially;algorithmic;algorithms models", "pdf_keywords": ""}, "c888022dec626171d243d2a056709b9b053a0ed9": {"ta_keywords": "end speech recognition;end neural networks;speech recognition;recurrent encoder;recurrent encoder decoder;end end neural;speech recognition fundamental;end end speech;end neural;speech recognition midst;mechanism recurrent encoder;speech recognition field;end speech;problem speech recognition;recognition field speech;attention mechanism recurrent;hidden markov models;field speech recognition;hidden markov;encoder decoder architecture;end end training;markov models core;introductionmultichannel end end;encoder decoder;dynamic time alignment;end end;encoder;neural networks challenging;introductionmultichannel end;decoder architecture", "pdf_keywords": "end speech recognition;attention based encoder;decoder speech recognition;input attention based;encoder decoder speech;speech recognition task;speech recognition;speech recognition speech;speech recognition proposed;speech recognition framework;neural networks speech;speech recognition implement;recognition speech;speech recognition midst;decoder speech;speech recognition subsequent;multichannel speech recognition;location based attention;recurrent encoder decoder;speech recognition ot;neural beamformer speech;speech recognition developed;recurrent encoder;attention mechanism speech;acoustic language modeling;end neural networks;speech recognition development;speech recognition based;toend speech recognition;attention weight vector"}, "2526c510610c7220ecc56e6b08d09c4cbaf58c3c": {"ta_keywords": "regular expressions honorifics;expressions honorifics japan;expressions honorifics;translate regular expressions;appropriate honorific japanese;expressions appropriate honorific;honorific japanese;honorifics japan;propose anaphora resolution;anaphora resolution rulebased;regular expressions;translate regular;transforming regular expressions;honorifics japan incorporate;japan incorporate anaphora;rulebased machine translation;incorporate anaphora resolution;honorific japanese examine;honorifics;translation translate regular;anaphora resolution transforming;regular expressions appropriate;anaphora resolution;incorporate anaphora;honorific;automatically predicted anaphora;appropriate honorific;predicted anaphora resolution;annotated human automatically;machine translation", "pdf_keywords": ""}, "2a462e2b748d7e78f3af2621071265c1ad2683ea": {"ta_keywords": "optimal power flow;dispatchable wind energy;wind energy resource;wind parks operation;power flow problem;backgroundconsideration wind parks;integration dispatchable wind;parks operation optimal;wind parks independent;wind energy;power flow;linear programming approach;operation optimal power;successive linear programming;linear programming;power producers ipp;dispatchable wind;wind parks;parks independent power;optimal power;modification optimal power;energy resource power;backgroundconsideration wind;presence wind parks;independent power producers;operation optimal;parks operation;flow problem methodsin;energy resource;power producers", "pdf_keywords": ""}, "b61c9799f10e4de9cd222dfd8e423bbd950a7c44": {"ta_keywords": "extractive task unanswerable;natural language understanding;questions performance trained;extractive task;unanswerable questions performance;understanding text;essential natural language;dataset extractive task;language understanding;natural language;language understanding recent;sqd dataset extractive;enriching sqd dataset;text snippet does;task unanswerable questions;questions performance;text snippet;understanding text snippet;text;enriching sqd;performance trained sqd;snippet does provide;dataset extractive;unanswerable questions;extractive;trained sqd;task unanswerable;introduction understanding text;snippet does;information essential natural", "pdf_keywords": "questions identificationk extractive;language comprehension extractive;comprehension extractive designed;extractive task unanswerable;comprehension extractive;extractive task;extractive qa;essential natural language;natural language understanding;extract qa systems;ability answer text;extractive qa target;approach extractive qa;lower extractive task;natural language;extract qa;derived ace answering;avoid er corpus;extractive major challenge;answer text;essential language comprehension;questions identificationk;model extractive qa;extractive qa sufficient;methods extractive qa;corpus task discrimination;er corpus;new approach extractive;language understanding performance;answer text snippet"}, "4ebe5792fe2890590e7a5bf8ae0a29e0fb147ef9": {"ta_keywords": "introductionincomplete utterance rewriting;utterance rewriting semantic;utterance rewriting;incomplete utterance rewriting;utterance rewriting raised;rewriting semantic segmentation;introductionincomplete utterance;incomplete utterance;machine translation task;task incomplete utterance;machine translation;rewriting semantic;semantic segmentation task;utterance;formulates semantic segmentation;translation task;semantic segmentation;semantic segmentation recent;segmentation task results;shape machine translation;sequence based architecture;translation task employ;segmentation task;introductionincomplete;rewriting;segmentation recent;segmentation;sequence based;semantic;large attention previous", "pdf_keywords": "generation semantic similarity;utterance rewriting semantic;rewritten utterance generation;machine translation task;generate rewritten utterance;text generation semantic;semantic similarity measurement;rewritten utterances boost;utterance rewriting;rewritten utterances challenging;machine translation;model rewritten utterances;rewritten utterances;processing text generation;incomplete utterance rewriting;semantic similarity;utterance generation;utterance rewriting raised;model rewritten utterance;rewritten utterances dialogues;utterance generation human;fluency rewritten utterances;rewritten utterance algorithm;rewriting semantic segmentation;rewriting semantic;generating word level;approach rewritten utterances;text generation;rewritten utterance;word level edit"}, "e6fe601c44835d3654131d0312d65227d3523373": {"ta_keywords": "structured text networks;walk structured text;text networks;text corpus labeled;corpus labeled directed;text networks methodswe;representing text corpus;text corpus;walks derive similarity;learning walk structured;labeled directed graph;graph based similarity;corpus labeled;derived dependency parsing;words use supervised;corpus;structured text;nodes represent words;dependency parsing;words weighted edges;introduction learning walk;parsing given graph;learning walk;similarity measure words;based random walks;directed graph nodes;dependency parsing given;based similarity measure;labeled directed;similarity measure based", "pdf_keywords": ""}, "5a4d4c0824b5e113c39105c71d42b93d3900d87e": {"ta_keywords": "microtask crowdsourcing;crowdsourcing applications food;microtask crowdsourcing implies;crowdsourcing applications;background microtask crowdsourcing;successful crowdsourcing applications;amazon mechanical turk;crowdsourcing;examples successful crowdsourcing;successful crowdsourcing;mechanical turk;mechanical turk used;crowdsourcing implies;crowdsourcing implies decomposing;platform like crowddflower;tasks human workers;turk used submit;tasks human;micropayments altruism solve;crowddflower;motivated micropayments;crowddflower amazon;submit tasks human;motivated micropayments altruism;human workers motivated;crowddflower amazon mechanical;workers motivated micropayments;micropayments altruism;like crowddflower amazon;human workers", "pdf_keywords": ""}, "e2b6193a24cd6c9f736139aa66618d1b8bf2a60b": {"ta_keywords": "speech transcription;speech transcription tool;transcription accuracy cost;transcription tool;present speech transcription;guides transcription;guides transcription process;transcription;transcription tool targeted;transcription accuracy;models transcription accuracy;uses models transcription;actively guides transcription;choose segments transcribed;segments transcribed achieve;segments transcribed;transcribed;models transcription;automatically created transcript;transcription process;transcribed achieve highest;transcription process taking;transcribed achieve;errors transcriber;transcript starting point;transcript;transcriber;transcript starting;contain errors transcriber;errors transcriber specifies", "pdf_keywords": ""}, "fc848789b557a7581c51c79fd01897dc5aa7e8a8": {"ta_keywords": "question multimodal transformers;multimodal transformers;relevance retrieved knowledge;retrieved knowledge;multimodal transformers leverage;question multimodal;retrieved knowledge used;multimodal;different question multimodal;knowledge retrieval;explicit knowledge reasoning;knowledge retrieval followed;paradigm knowledge retrieval;explicit knowledge;knowledge reasoning;knowledge used reasoning;transformers optimizing information;knowledge reasoning methodsexisting;knowledge;leverage explicit knowledge;retrieval;relevance retrieved;knowledge used;answer prediction;unimodal methods explored;reasoning methodsexisting primarily;largescale transformers;largescale transformers optimizing;paradigm knowledge;retrieval followed answer", "pdf_keywords": "explicit knowledge image;visual question answering;knowledge retrieval image;knowledge based visual;knowledge image;explicit knowledge retriever;knowledge augmented transformer;knowledge image wiki;retrieve explicit knowledge;finegrained knowledge entries;explicit knowledge reasoning;new knowledge reasoning;knowledge reasoning module;image wiki knowledge;explicit knowledge based;jointly reasoning knowledge;retrieved knowledge;model knowledge reasoning;knowledge augmented;retrieve finegrained knowledge;knowledge retriever;finegrained knowledge;model retrieved knowledge;leverage explicit knowledge;extracted knowledge implicit;implicit knowledge retrieval;reasoning knowledge;retrieve knowledge;improving retrieval reasoning;models explicit knowledge"}, "db729f2f55a92465cf88682ba7917621fd4c000b": {"ta_keywords": "student tutor explanations;explanations tutoring;explanations tutoring activities;provide explanations tutoring;tutor learning using;facilitates tutor learning;tutor explanations;facilitates tutor;tutor learning;tutor explanations methodswe;tutoring;students provide explanations;effect tutor learning;tutoring activities facilitates;asks student tutor;tutor;student tutor;tutoring activities;teachable agent asks;using teachable agent;activities facilitates tutor;teachable agent;agent asks student;effect tutor;peer learner allows;introductionstudying effect tutor;learner allows students;explanations methodswe;peer learner;allows students learn", "pdf_keywords": ""}, "b2da0f022a48ebd10a23572b5310b7d7341b6448": {"ta_keywords": "symptomatic asymptomatic asymptomatic;symptomatic asymptomatic;asymptomatic asymptomatic asymptomatic;asymptomatic asymptomatic;history symptomatic asymptomatic;asymptomatic;symptomatic;patients history symptomatic;history symptomatic;patients;approach management patients;patients history;management patients history;management patients;effectiveness new approach;evaluating effectiveness new;effectiveness new;evaluating effectiveness;approach management;new approach management;approach;new approach;effectiveness;importance evaluating effectiveness;article;purpose article;new;evaluating;management;article discuss importance", "pdf_keywords": ""}, "004ddf5a39a735d0f8ec7547629c2bee65eb1f93": {"ta_keywords": "reviewers induces biases;biases scholarly research;biases scholarly;issue biases scholarly;identities reviewers induces;author identities reviewers;reviewers induces;identities reviewers;biases;biases certain groups;induces biases;induces biases certain;biases certain;peer review;bias;scholarly research;research specifically peer;specifically peer review;exposing author identities;scholarly research specifically;issue biases;scholarly;reviewers;debate exposing author;consider issue biases;peer review long;investigate existence biases;author identities;existence biases;research specifically", "pdf_keywords": "testing biases peer;biases peer review;test biases peer;bias peer review;biases peer reviewed;testing biases papers;reviewer bias peer;biases scholarly research;bias reviewers bias;reviewers bias hypothesis;reviewers induces biases;bias reviewers;testing biases;biases scholarly;bias testing;nontrivial bias testing;analysis bias testing;testing biases using;bias behaviour reviewers;tests detect biases;biases publication academic;biases peer;test biases;biases publication;testing biases corresponding;bias reviewers positive;publishing reviewer bias;think bias testing;reviewers bias;relative bias reviewers"}, "1bd7d16340642948142d7608ef8f085d934d94a3": {"ta_keywords": "derivative free optimization;optimization method momentum;free optimization method;free optimization;unconstrained minimization smooth;unconstrained minimization;minimization smooth objective;smooth objective function;stocochastic derivative free;minimization smooth;objective function mathbbrd;problem unconstrained minimization;momentum version stochastic;method momentum;smooth objective;optimization method;introduction stocochastic derivative;ball momentum particular;propose mp momentum;optimization;objective function;mp momentum version;stocochastic derivative;derivative free;minimization;method momentum aims;momentum particular propose;mp momentum;momentum particular;heavy ball momentum", "pdf_keywords": "method importance sampling;momentum importance sampling;stochastic point method;importance sampling method;stochastic points method;bounds randomized methods;randomized methods;importance sampling mp_is;importance sampling;momentum method importance;method stochastic;method sample complexity;minimization smooth objective;randomized methods understood;focusing importance sampling;unconstrained minimization smooth;2d importance sampling;free optimization mp;sampling mp_is momentum;mportance sampling method;unconstrained smooth minimization;stochastic gradient based;method analysis stochastic;unconstrained minimization;improves sample complexity;order methods complexity;importance sampling version;method strongly convex;zero order methods;order momentum method"}, "db8376698c06d6a688a39bff0300780ef0383821": {"ta_keywords": "decentralized control method;decentralized control;decentralized;corrugated board;corrugated board make;manufacturing corrugated cardboard;faced corrugated board;corrugated cardboard;line manufacturing corrugated;manufacturing corrugated;corrugated;corrugated board pastes;liner corrugated;corrugated cardboard pastes;pastes liner corrugated;cardboard pastes liner;single faced corrugated;faced corrugated;cardboard pastes;cardboard;method line manufacturing;liner corrugated medium;corrugated medium make;control;local control;board pastes liner;board make;board pastes;pastes liner;corrugated medium", "pdf_keywords": ""}, "4b1555368fd2c5f1234eaac5e41296003481754a": {"ta_keywords": "lithium oxygen battery;oxygen battery lob;deep eutectic electrolyte;lithium oxygen;lithium metal anode;eutectic electrolyte;oxygen battery;oxidizing environment battery;battery lob promising;stable electrolyte;high reactivity lithium;battery operation evaporation;battery lob;electrolyte simultaneously withstand;stable electrolyte simultaneously;energy storage;environment battery;lack stable electrolyte;electrolyte;energy storage systems;reactivity lithium metal;lithium metal;reactivity lithium;electrolyte simultaneously;environment battery operation;lithium;tool energy storage;anode developed deep;battery;metal anode developed", "pdf_keywords": ""}, "005879e6587eb6e05f56c20d345f784ee84a44c4": {"ta_keywords": "neural network grammars;generative dependency models;grammars generate sentences;grammars generate;network grammars generate;parsing language modeling;generate sentences;generative models dependency;models dependency syntax;generative dependency;syntax methodsboth models;dependency syntax methodsboth;network grammars;generate sentences using;recurrent neural nets;explore generative dependency;dependency models similarly;dependency syntax;dependency models;parsing language;phrase structure syntax;syntax perform parsing;perform parsing language;phrase structure;grammars;perform parsing;models use recurrent;parsing;use recurrent neural;new generative models", "pdf_keywords": ""}, "f61862b286c9e4894302faf716eedb0eb60a2f5f": {"ta_keywords": "newsgroup style conversations;thread structure newsgroup;structure discussion forums;thread structure discussion;discussion forums explicit;isolate discussion related;implicit thread structure;discussion related specific;forums explicit meta;style conversations;recovering thread structure;discussion forums;implicit thread;conversations;isolate discussion;discussion related;thread structure;thread structure reconstructed;related particular conversational;structure newsgroup;recovering thread;forums explicit;structure newsgroup style;style conversations valuable;approach recovering thread;conversational goals methodsin;work thread structure;particular conversational;conversational;discussion", "pdf_keywords": ""}, "20819855b9517c927a1262850146e525c8083fb4": {"ta_keywords": "recurrent neural networks;short term memory;dependent lstm;rolele dependent lstm;lstm;dependent lstm layers;memory lsm recurrent;lsm recurrent neural;lstm layers;recurrent neural;spoken language understanding;networks rnns;neural networks rnns;term memory lsm;lstm layers methodsnural;term memory;spoken language;lsm recurrent;sequence dialog;memory lsm;language understanding;rnns;language understanding slus;spotken language understanding;intentions accurately dialog;understand speaker intentions;language understanding using;speaker intentions accurately;slus understand speaker;sequence dialog turns", "pdf_keywords": ""}, "f74d4fa99ccedfea7a2662fe6944d99f34533912": {"ta_keywords": "fairness machine learning;classification thresholds demographic;fairness constraints group;fairness machine;introductionthe fairness machine;fairness constraints;optimize multiple fairness;adaptive classification thresholds;mitigate discriminated model;classification thresholds;multiple fairness constraints;mitigate discriminated;group aware threshold;discriminated model;thresholds demographic group;thresholds demographic;multiple fairness;aware threshold adaptation;threshold adaptation;discriminated;aware threshold;discriminated model behaviors;threshold adaptation methodswe;diversify mitigate discriminated;introductionthe fairness;fairness;demographic group optimizing;optimizing confusion matrix;machine learning getting;classification", "pdf_keywords": "accuracy fairness classification;fairness classification optimal;fairness classification;discrimination aware classification;fairness fair classification;unfairness classify groupaware;employ classification threshold;optimize classification threshold;fairness accuracy generalization;unfairness constrained classification;thresholds optimize fairness;fair classification;fairness machine learning;fair classification fundamental;setting classification thresholds;classification threshold;optimize fairness accuracy;classification thresholds;classification threshold sensitive;generalization unfairness classify;fairness metrics accuracy;groups improve fairness;fairness datasets;improve fairness accuracy;classification threshold demographic;accuracy fairness optimal;methods fairness datasets;unfairness classify;adaptive classification thresholds;group aware threshold"}, "25fec1e150a273b3bec3655ace0ff6b97c338a96": {"ta_keywords": "regenerating codes resilient;errors erasures codes;errors erasures nodes;erasures codes optimal;regenerating codes;node repair operations;erasures codes;resilient errors erasures;codes resilient errors;explicit regenerating codes;erasures nodes;nodes distributed storage;reconstruction node repair;repair failed nodes;node repair;data efficient repair;codes resilient;erasures nodes links;handling errors erasures;distributed storage systems;errors erasures;distributed storage;storage systems;storage systems paper;data reconstruction node;failed nodes;resilient errors;failed nodes distributed;introductionregenerating codes;introductionregenerating codes class", "pdf_keywords": "erasures regenerating codes;regenerating codes distributed;codes distributed storage;regenerating codes resilient;errors erasures codes;erasures codes optimal;resilient errors erasures;approach regenerating codes;regenerating code distributed;erasures distributed storage;erasures codes;code distributed storage;resilient regenerating code;codes resilient errors;regenerating codes provide;errors erasures nodes;regenerating codes;using regenerating codes;errors erasures distributed;code regenerating codes;explicit regenerating codes;regenerating code resilient;regenerating codes particular;codes resilient;errors erasures regenerating;error erasure capacity;handling errors erasures;matrix code regenerating;data efficient repair;regenerating code"}, "2875d6cac905c3dfec4c8a8e1d89a2a7e4d71d40": {"ta_keywords": "functional broadcast repair;wireless distributed storage;failures wireless distributed;broadcast repair;distributed storage systems;broadcast repair multiple;consider distributed storage;distributed storage;distributed storage inline;partial failures wireless;wireless distributed;storage systems;storage systems discussed;failures wireless;functional broadcast;partial failures;storage inline;nodes user recover;distributed;multiple partial failures;storage;latex tex math;broadcast;tex math notation;storage inline formula;formula tex math;discussed consider distributed;repair;consider distributed;tex math", "pdf_keywords": "storage repair algorithm;storage repair schemes;broadcast repair code;bandwidth storage repair;broadcast repair multiple;storage node repair;nodes repair bandwidth;repair code distributed;broadcast repair;regenerating codes distributed;codes distributed storage;broadcast repair surviving;distributed storage nodes;consider broadcast repair;feasible repair bandwidth;repair bandwidth storage;storage repair bandwidth;constructed broadcast repair;node repair construction;optimal storage repair;channel nodes repaired;nodes coding regenerating;explicit storage repair;node repair bandwidth;achieve storage repair;nodes repair;improved node repair;storage multinode repair;distributed storage multinode;trade broadcast repair"}, "190865e2c3d4908ff20bf9a31f5a2773d6fec5cb": {"ta_keywords": "question answering genera;domain question answering;question answering;answering genera;answering genera ally;generated question answer;question answer pairs;corpus knowledge base;open book approach;answer pairs methods;open book;text corpus knowledge;corpus knowledge;knowledge base;used open book;new qa;new qa aug;answering;large text corpus;produce answer recent;genera ally;genera ally used;text corpus;answer pairs;methods new qa;generated question;question answer;previously generated question;approach information retrieved;qa", "pdf_keywords": "task question retrieval;question answering;question retrieval;question answering augments;question retrieval pre;answer extraction;train answer extraction;domain question answering;step question retrieval;extraction question generation;question generation models;answer extraction question;text retrieval tasks;better interpretation retrieval;retrieval applied questionanswer;retrieval tasks;questionanswer dataset;question generation generate;question generation;base question generation;questionanswer dataset generated;question retrieval applied;retrieval tasks demonstrate;retrievals assist prediction;interpretation retrieval;interpretation retrieval performance;retrieval user friendly;answering augments text;retrieval retrieval;retrieval pre training"}, "4dc8ddef938699d0d8a0b685ad9f56d0b735a25d": {"ta_keywords": "learns incrementally;explanation based learning;learns incrementally method;knowledge intensive inductive;concept learner;inductive learning algorithm;intensive inductive learning;inductive learning;eb learns incrementally;alternative learning algorithm;concept learner disadvantage;learning algorithm called;based learning eb;background knowledge improve;learns;based learning;learning algorithm calledia;learning algorithm;abductive explanation based;eb learns;alternative learning;performance concept learner;learning eb;learning;aimto alternative learning;algorithm called abductive;knowledge improve;knowledge intensive;uses background knowledge;described knowledge intensive", "pdf_keywords": ""}, "5677d2b565c8265fef1693a9be861739cb01bf2f": {"ta_keywords": "vocabulary speech recognition;large vocabulary speech;speech recognition;speech recognition systems;tuning meta parameters;experts tuning meta;tuning meta;meta parameters automate;markov model deep;tune meta para;numerous meta parameters;meta parameters;vocabulary speech;model deep neural;tune meta;meta parameters specifying;deep neural network;meta para;performance numerous meta;human experts tuning;recognition performance numerous;model deep;experts tuning;deep neural;parameters automate;highest recognition performance;speech;recognition systems consist;recognition performance;large vocabulary", "pdf_keywords": ""}, "3d2da57c2de69b02fa0fee5c12ace618718a3926": {"ta_keywords": "introductionassociating sub images;matching sub figures;images sub captions;subcellular location image;sub captions;sub images;sub images sub;combination text images;images sub;sub captions important;text images journal;sub figures sentences;sub figures;text images;image finder;image finder extracts;captions important tool;images journal articles;subcellular;mining data text;information text image;captions;location image finder;text image;images journal;subcellular location;biology combination text;figures sentences text;location image;matching sub", "pdf_keywords": ""}, "d21a0e01514732f241b9c138eceb76ecaef17a27": {"ta_keywords": "phrase based active;effective machine translation;better translation models;machine translation;introductionactive learning;translation models effort;introductionactive learning framework;translation models;active learning;machine translation making;particularly annotators translate;based active learning;effective phrase based;active learning fails;translation making possible;annotators translate;train better translation;annotators translate short;translation making;translate short phrases;introductionactive;phrases instead sentences;short phrases instead;effective phrase;effort particularly annotators;phrases instead;method effective phrase;phrase based;selecting informative examples;instead sentences", "pdf_keywords": ""}, "b4bc1a98eb79545f8da4385a6dfb643b0c62a07e": {"ta_keywords": "siultaneous translation prediction;syntaxbased translation methods;syntaxbased translation;communication machine translation;translation prediction unseen;translation prediction;sultaneous translation method;machine translation mt;machine translation;methods sultaneous translation;syntax based siultaneous;based siultaneous translation;problems syntaxbased translation;translation methods;translation methods difficult;siultaneous translation;prediction unseen syntactic;performing translation short;accurate parse trees;translation short segments;translation method;unseen syntactic;performing translation;segments performing translation;generate accurate parse;accurate parse;parse trees;sultaneous translation;unseen syntactic constituents;parse trees sub", "pdf_keywords": ""}, "425a4a9c0598e4101ca2f2b930f5c6986ce40a99": {"ta_keywords": "privacy regularization;privacy regularization joint;differential privacy;privacy utility optimization;models privacy;models privacy guarantees;regularization joint privacy;differential privacy da;training samples privacy;correspondence differential privacy;train models privacy;background privacy regularization;privacy utility;joint privacy utility;samples privacy;privacy;privacy da popular;privacy da;privacy guarantees;samples privacy im;joint privacy;privacy guarantees comes;privacy im plications;utility optimization languagemodels;languagemodels methods nural;optimization languagemodels methods;privacy im;optimization languagemodels;nural language models;background privacy", "pdf_keywords": "privacypreserving regularization methods;propose privacypreserving regularization;privacypreserving regularization;privacy regularization demonstrate;privacy regularization;regularization demonstrate privacy;purpose privacy regularization;privacy language model;privacy use discriminator;differential privacy language;private language models;data propose privacypreserving;jointly optimize privacy;privacy language;optimize privacy;optimization utility privacy;model differential privacy;optimize privacy utility;results differential privacy;adversarial triplet based;improved mitigations privacy;slower mitigations adversarial;adversarial training program;mitigations adversarial triplet;propose privacypreserving;mitigations differential privacy;demonstrate privacy model;adversarial training;mitigations privacy model;privacypreserving"}, "4383e714f4535777ffb7b4f618d4ccede4b08bd3": {"ta_keywords": "allophones phonemes;allophones phonemes 14;phonological units allophones;phonological context phonemic;218 allophones phonemes;mappings 218 allophones;phonemic representations language;language specific phonetic;methodsphonemes contrastive phonological;specific phonetic representations;allophones various;specific phonetic;phonetic representations;phonemes 14 languages;phonemic representations;phonetic representations stated;terms allo phones;languages methodsphonemes contrastive;units allophones various;allovara provides mappings;allophones;14 languages methodsphonemes;languages methodsphonemes;phonetic;units allophones;context phonemic;allo phones;contrastive phonological;context phonemic representations;phonemes", "pdf_keywords": "mappings allophones languages;generating mappings allophones;mappings allophones alloviral;mappings allophones alloviralthe;allophones alloviral languages;mappings allophones;mappings 218 allophones;allophones languages;allophones alloviralthe database;allophones phonemes;specific phonetic representations;allophones languages english;allophones phonemes 14;phonetic representations;phonemic representations language;phonological units allophones;language specific phonetic;218 allophones phonemes;phonology syllabic phonemes;allophones various;allophones alloviral;allophones alloviralthe;allovira org phonological;specific phonetic;phonological context phonemic;phonetic representations article;phonemic representations;syllabic phonemes;alloviral languages;phonemes 14 languages"}, "c549b3f2d262efc1f68dfdd842174634f37519ed": {"ta_keywords": "adaptation techniques speech;adaptation using time;incremental adaptation;introductionpredictor corrector adaptation;novel incremental adaptation;incremental adaptation framework;methodsinremental adaptation techniques;corrector adaptation using;changes speaker speaking;methodsinremental adaptation;corrector adaptation;scale methodsinremental adaptation;changes speaker;adaptation using;adaptation framework;time variant acoustic;adaptation techniques;adjusting acoustic models;acoustic models time;adaptation framework models;speech recognition;time scale methodsinremental;factors changes speaker;adaptation;speech recognition aimed;techniques speech recognition;acoustic models;models time variant;adjusting acoustic;using time evolution", "pdf_keywords": ""}, "40f4d7fe800810288a80f84cdb357a8f4c28e880": {"ta_keywords": "cnn transformer;cnn transformer based;networks cnn transformer;vision transformer;vision transformer vit;architecture existing convolutional;introduction vision transformer;design principles cnn;transformer based architecture;transformer based;neural networks cnn;spatial dimension conversion;transformer;networks cnn;transformers language processing;convolutional neural networks;convolutional neural;transformer vit;existing convolutional neural;conversion effectiveness transformer;transformers language;cnn;dimension conversion effectiveness;range transformers language;transformers;existing convolutional;innovative computer vision;principles cnn;computer vision modeling;computer vision tasks", "pdf_keywords": "cnn transformer;cnn transformer based;networks cnn transformer;poolingbased vision transformer;architectures vision transformers;integrated vision transformer;vision transformer;pooling convolution strides;convolutional architecture;architecture existing convolutional;vision transformer vit;vision transformers;poolingbased vision;abstract vision transformer;layers convolutional architecture;based vision transformer;networks cnn;vision transformer pit;convolutional network improved;design principles cnn;pooling based vision;neural networks cnn;cnn;called poolingbased vision;convolution strides;performance convolutional network;pooling convolution;layers performance convolutional;imagenet training;vision transformers leverage"}, "8a73eed98873d91086201f41c6e1f613fcdefe18": {"ta_keywords": "self supervised anr;supervised anr ts;self supervised;supervised anr;context self supervised;language model reward;language model;hypotheses forwarding tt;ts models;ar hypotheses forwarding;direction equipped language;anr ts models;equipped language model;ts eat model;features arts direction;supervised;ts models suffer;hypotheses forwarding;arts direction equipped;model reward;anr ts;models suffer domain;models;features arts;arts direction;penalize ar hypotheses;models suffer;eat model;ar ts eat;ttar direction", "pdf_keywords": "training text;training text data;training data ar;supervised training text;recognition field text;self trained speech;augmentation training data;speech recognition eat;data augmentation training;self supervised training;supervised anrras ar;text data model;recognition eat model;self supervised anrras;trained speech recognition;model training;self supervised;language model reward;recognition eat;model trained;model trained specaugment;equipped language model;text data augmentation;using self supervised;ts model training;trained speech;new approach recognition;recognition;language model;language model implemented"}, "42be8ed9973b3326a6e3d838c4742bc1d7704704": {"ta_keywords": "controllable speech modification;backgroundarticulatory controllable speech;speech modification based;novel speech modification;speech modification;modify phonemic sounds;speech modification methodsthe;controllable speech;manipulating articulatory;manipulating articulatory parameters;estimated input speech;input speech signal;phonemic sounds input;sounds input speech;backgroundarticulatory controllable;input speech;modification based statistical;speech signal propose;intuitively manipulating articulatory;compensate unmodified articulatory;modify phonemic;articulatory parameters estimated;possible modify phonemic;speech signal;articulatory parameters;backgroundarticulatory;articulatory movements;articulatory;gassian mixture models;modification based", "pdf_keywords": ""}, "7c0ada3511b05897fb4d75c5f657b5fbd953caa8": {"ta_keywords": "use vote aggregates;vote aggregates;social influence votes;aggregate user feedback;sum votes commonplace;influence votes;votes commonplace;influence votes far;vote aggregates odds;platforms use aggregate;biases reputation;different biases reputation;standard use vote;sum votes;poster social influence;reputation poster social;biases reputation poster;use vote;say sum votes;votes commonplace gold;voters susceptible different;social influence;votes;suggests voters susceptible;voters susceptible;reputation poster;reputation;voters;aggregate user;feedback say sum", "pdf_keywords": "biases online platforms;voter biases online;quantify voter biases;quantifying voter biases;quantifying voter bias;bias online platforms;voter biases sites;study voter biases;influence bias online;votes estimation causal;bias social media;insights online voting;research voter biases;platforms determine biases;voter biases used;voter biases;reputation bias causal;models quantifying voter;estimates quantifying voter;suggest voter bias;social influence votes;voter biases detecting;biases estimating causal;voter bias;influence bias data;vote causal effects;affected voter biases;influence bias significantly;bias causal estimates;susceptibility voter biases"}, "e79be3f9ce409f1a9b7084ef880298665e5212d0": {"ta_keywords": "token aware contrastive;cascade contrastive learning;aware cascade contrastive;contraststive learning widely;contraststive learning;introduction contraststive learning;improves contrastive learning;contrastive learning;aware contrastive;contrastive learning tco;cascade contrastive;contrastive learning using;aware contrastive loss;modal representation learning;vision language models;video text alignment;language models video;improves contrastive;models video text;tco improves contrastive;contrastive loss;representation learning;representation learning paper;contraststive;vision language;video text;contrastive loss computed;contrastive;introduction contraststive;based vision language", "pdf_keywords": "casscade contrastive learning;aware casscade contrastive;contrastive learning pipelines;token aware contrastive;contrastive learning pipeline;background contraststive learning;aware contrastive loss;improves contrastive learning;contraststive learning widely;current contrastive learning;improve contrastive learning;effective contrastive learning;casscade contrastive;contraststive learning;contrastive learning tco;modal representation learning;contrastive loss optimize;learning video text;simple contrastive learning;aware contrastive;contrastive loss finegrained;contrastive learning;finegrained alignment pretraining;vision language models;language models video;computing contrastive loss;video language tasks;token level contrastive;propose contrastive learning;token aware casscade"}, "e40a5c25d39d0f9add6a26c82613cf29edbcccf5": {"ta_keywords": "biometrics analyze brain;electrog biometrics analyze;using electrog biometrics;electrog biometrics;accuracy user identification;user identification performance;user identification authentication;user identification using;comparing user identification;user identification;biometrics;analyze brain waves;biometrics analyze;identification using electrog;identification authentication;brain waves;identification performance;identification authentication methodsfirst;identification performance various;identification using;brain waves acquired;electronic device investigate;electronic devices;authentication methodsfirst statistical;electronic electronic devices;accuracy user;dimensionality reduction;combinations dimensionality reduction;electronic device;electronic electronic device", "pdf_keywords": ""}, "92bd9e8a83e82dbbcafd8cde4f5a42d7bb4a5859": {"ta_keywords": "individuality transforming text;transforming individuality using;method transforming individuality;individuality writer speaker;transforming individuality;analysis individuality transforming;individuality transforming;transforming text speech;statistical machine translation;language model adaptation;express individuality writer;individuality writer;machine translation;adaptation analysis individuality;transforming text;machine translation smt;individuality using;individuality using technique;express individuality;features express individuality;paraphrasing characteristic;paraphrasing characteristic words;transforming;gram clustering;model adaptation analysis;analysis individuality;using gram clustering;method paraphrasing characteristic;adaptation analysis;individuality", "pdf_keywords": ""}, "d1d23675d2e65cd734f2955c10ec1028b1139b5b": {"ta_keywords": "translation accuracy training;neural machine translation;final translation accuracy;translation accuracy;machine translation nm;machine translation;translation nm systems;evaluation metrics bmc;improve final translation;bmc based evaluation;translation nm;accuracy training bmc;metrics bmc based;evaluation metrics bleeur;translation;improve evaluation metrics;metrics bmc;training bmc based;training bmc;evaluation metrics;based evaluation metrics;bmc based;final translation;bmc;bleeur significantly improve;nm systems trained;directly improve evaluation;neural machine;improve evaluation;metrics bleeur significantly", "pdf_keywords": "optimizing machine translation;training translation models;neural machine translation;metric semantic similarity;similarity model translation;translation quality evaluation;metric learning translations;translation models significantly;bligua semantic similarity;translation task training;model translation quality;translation quality;machine translation;machine translation association;translation models;machine translation development;training translations;metrics bligua semantic;machine translation systems;training translation;blizzelle semantic similarity;learning translations increasingly;textual similarity metrics;learning translations;improve semantics translated;machine translation nm;semantic similarity;metric semantic textual;semantic similarity model;en translation models"}, "a538a05864a23e2f80f9b003d5ecbdfb8025b954": {"ta_keywords": "backgroundin stance detection;classify stance tweet;stance detection goal;stance detection;classify stance;favor stance detection;stance tweet target;backgroundin stance;stance tweet;goal classify stance;stance;target favor stance;favor stance;tweet target favor;backgroundin;tweet target;detection;detection goal;detection goal classify;classify;target favor;tweet;goal classify;target;favor;goal", "pdf_keywords": ""}, "3376118362db3751cfbd88acd0c090b8a3897733": {"ta_keywords": "improving bert interpretations;bert interpretations complex;words derivational morphology;pretrained language models;bert interpretations;semantic representations derivationally;segmentation pretrained language;bert example plm;words derivational;improving bert;complex words derivational;derivationally complex words;taking bert example;language models;bert example;derivational morphology;derivational morphology aimto;im improving bert;pretrained language;language models plms;interpretations complex words;semantic representations;plm focusing semantic;bert;focusing semantic representations;question taking bert;taking bert;focusing semantic;semantic;segmentation pretrained", "pdf_keywords": "interpretations complex words;novel lexical semantic;complex words semantic;morphologically informed vocabulary;lexical semantic probing;lexical semantic;complex words interpreted;segmentation complex words;processing word language;complex words stored;vocabulary input tokens;segmentation derivational words;meanings complex words;words complex;word language derivational;pretrained language models;complex words significantly;complex english words;predict semantic;words processing;complex words humans;language derivational segmentation;meaning complex words;complex words present;words semantic classes;language models;concept complex words;example complex words;generalization new words;wordsthe concept complex"}, "1392df13a80a962057e979a294a850a50b7deb7e": {"ta_keywords": "correcting automatic annotations;automatic annotations natural;automatic annotations;automatically segmenting corpus;annotations natural language;segmenting corpus chunks;segmenting corpus;annotations natural;annotations;automatically segmenting;natural language efficient;corpus chunks;method automatically segmenting;corpus chunks uncertain;corpus;natural language;correcting automatic;labels grouped chunk;chunk human supervision;segmenting;chunks uncertain labels;manually correcting automatic;language efficient;language efficient manner;uncertain labels grouped;manually correcting;uncertain labels;grouped chunk human;grouped chunk;altogether segments", "pdf_keywords": ""}, "8df3f3f72eb239eb212bd3fc929bd754ce2e03d6": {"ta_keywords": "la topic modeling;topic modeling;articles yeast protein;pubmed articles yeast;jointly modeling pubmed;topic modeling approach;modeling pubmed articles;protein interaction networks;articles yeast;biology domain jointly;modeling yeast biology;yeast biology literaturee;joint modeling yeast;fusion entity annotated;documents graphs entity;entity links;la yeast biology;topic coherence emergent;yeast biology domain;domain jointly modeling;biology literaturee protein;text documents graphs;entity entity links;modeling yeast;yeast biology;modeling pubmed;documents graphs;yeast protein;yeast protein protein;entity annotated text", "pdf_keywords": ""}, "aae3d5e24d02ae538030ef3995a86118c5323ae1": {"ta_keywords": "cluster storage systems;cluster storage;approach cluster storage;storage systems;storage systems based;disk reliably heterogeneous;cluster;reliably heterogeneous systems;storage;heterogeneous systems;use disk reliably;disk reliably;new approach cluster;reliably heterogeneous;approach cluster;based use disk;disk;use disk;heterogeneous;systems;systems based;systems based use;reliably;based use;new approach;present new approach;use;approach;new;present new", "pdf_keywords": ""}, "148bca569a0d2833f05df1297788f64bc6686fa8": {"ta_keywords": "challenges temporal misalignment;text data time;end task performance;challenges temporal;temporal misalignment;temporal misalignment methods;waits analysis challenges;analysis challenges temporal;data resulting temporal;time waits;resulting temporal misalignment;trained text data;trained text;model trained text;diverse tasks;temporal misalignment degrade;temporal;tasks different;suite diverse tasks;background time waits;diverse tasks different;task performance;time waits analysis;task performance work;waits analysis;data time;tasks;end task;degrade end task;background time", "pdf_keywords": "temporal misalignment training;task performance temporal;benchmarkwe temporal;evidence benchmarkwe temporal;language modeling tasks;modification task performance;timestamp models trained;tasks temporal;mitigate temporal misalignment;effective temporal adaptation;tasks domain adaptation;benchmarkwe temporal misalignment;adaptation effective temporal;finetuning effective temporal;tasks demonstrate temporal;temporal domain adaptation;vocabularies affected temporal;algorithms mitigate temporal;tasks report temporal;temporal adaptation additional;modeling tasks;mitigate temporal;task data analysis;performance demonstrate temporal;evaluation timestamp models;domain temporal adaptation;temporal adaptation;performance temporal;tasks temporal domain;affects task performance"}, "bb80f7d2269308c3e91da8c47b290645e9d3d7d5": {"ta_keywords": "learning rotation invariant;learning rotation;rotation invariant latent;basis discovery;basis discovery data;poses characterize manifold;projected poses obtained;projected poses;bases poses;poses obtained images;problem learning rotation;poses obtained;bases poses characterize;poses characterize;factor model training;set bases poses;technique basis discovery;set projected poses;poses;motions movemes training;latent factor model;invariant latent factor;dimensional projections;human motions;rotation invariant;camera angles methodsthe;dimensional projections original;projections original feature;primitive human motions;lower dimensional projections", "pdf_keywords": "learned observations poses;dimensional bases poses;modeling pose space;representation bases poses;poses rotation;projected poses;poses easily visualized;pose space model;poses rotation invariant;poses characterize manifold;modeling pose;learning rotation;poses characterize;view bases poses;learning latent factor;learning rotation invariant;projected poses obtained;poses easily;rotation invariant latent;rotation invariant learned;poses training;observations poses;bases poses rotation;poses;bases poses;factor model learn;poses approach;bases poses training;bases poses easily;factor model training"}, "ed1da1abf0f50ca758e422fbd945f891b6cda690": {"ta_keywords": "based dialog retrieval;dialog retrieval;dialog retrieval using;paraphrase identification methods;neural network paraphrase;network paraphrase identification;dialog systems utilizing;chat oriented dialog;example based dialog;dialog systems;paraphrase identification;oriented dialog systems;human human conversation;human conversation;based dialog;dialog;human conversation shown;example based chat;oriented dialog;simple retrieval techniques;relatively simple retrieval;vocabulary hov database;retrieval using recursive;retrieval techniques resulting;simple retrieval;retrieval techniques;network paraphrase;based chat oriented;conversation shown;conversation", "pdf_keywords": ""}, "7596030e0ce7a8df2f43e6ba4e9de5fa4a19240f": {"ta_keywords": "sochastic extragradient methods;extragradient methods;speed extragradient methods;extragradient methods staple;convergence speed extragradient;extra gradient methods;extragradient methods variable;gradient methods overcome;sochastic extragradient;gradient methods;extragradient;speed extragradient;scaling methodsoowing stability;conservatively sochastic extragradient;scaling methodsoowing;step extra gradient;algorithms use extrapolation;methodsoowing stability convergence;scale saddle point;stepsize scaling methodsoowing;methodsoowing stability;saddle point problems;extrapolation step performing;extra gradient;saddle point;extrapolation step;large scale saddle;gradient;scale saddle;aggressively update conservatively", "pdf_keywords": "convergence stochastic extragradient;stochastic extragradient methods;stochastic extragradient algorithms;extragradient methods stochastic;method stochastic extragradient;extragradient method stochastic;gradient descent stochastic;stochastic extragradient;vanilla stochastic extragradient;anchored gradient stochastic;convergence speed extragradient;convergence stochastic methods;extragradient algorithms;stochastic anchored gradient;extragradient algorithms used;extragradient methods large;descent stochastic anchored;stochastic gradients;extragradient methods;gradient stochastic;extragradient methods overcome;analysis stochastic gradients;descent stochastic;extragradient methods method;gradient descent ascent;speed extragradient methods;step extragradient methods;extragradient method;stochastic gradients problem;extragradient method simple"}, "0d1cd3baad7d0734c9bbb008a33e2d10846968cd": {"ta_keywords": "specific coding sequence;coding sequence described;analysis amplification amplification;coding sequence;analysis amplification;amplification amplification amplification;amplification amplification;expression specific coding;applied analysis amplification;amplification;sequence described method;specific coding;method analyzing expression;analyzing expression specific;analyzing expression;coding;sequence described;sequence;expression specific;described method applied;expression;described method;new method analyzing;method analyzing;analyzing;method applied;analysis;method applied analysis;method;new method", "pdf_keywords": ""}, "739c2181f7894050a06b53b41ac5debe8ffc4829": {"ta_keywords": "stochastic gradient descent;gradient descent sgd;sgd stochastic differential;sgd stochastic;stochastic gradient;properties stochastic gradient;trajectories sgd stochastic;convex deep learning;sgd non convex;gradient descent;descent sgd;sgd rigorous;descent sgd non;trajectories sgd;gradient noise;modeling trajectories sgd;sgd rigorous treatment;stochastic differential;tailed gradient noise;deep learning problems;generalization properties stochastic;stochastic differential equations;sgd;deep learning;peculiar characteristics sgd;characteristics sgd rigorous;gradient noise recently;characteristics sgd;heavy tailed gradient;stochastic", "pdf_keywords": "generalization bounds stochastic;bounds stochastic gradient;stochastic gradient descent;trajectories stochastic learning;complexity trajectories stochastic;stochastic gradient;stochastic optimization;stochastic learning;stochastic optimization sg;analysis stochastic gradient;generalization error stochastic;bounds stochastic;stochastic differential;method stochastic optimization;stochastic gradient noise;recent stochastic differential;stochastic differential equations;markov processes learning;described stochastic differential;error stochastic gradient;stochastic learning algorithm;stochastic;continuous time stochastic;method stochastic;trajectories stochastic;training process generalization;stochastic process;analysis stochastic;gradient descent deep;dimension trajectothe optimization"}, "94f8ef5944ecbdd0350d406bf3a16a7f2dff7349": {"ta_keywords": "neural beamformer multi;backgroundmimo speech end;output speech recognition;speaker speech recognition;architecture withmimo speech;multi output speech;backgroundmimo speech;source neural beamformer;channel multi speaker;multi speaker speech;speech recognition model;neural beamformer;speech recognition;multi source neural;multi speaker;speech recognition methodsa;sequence architecture withmimo;withmimo speech proposed;output speech;beamformer multi output;beamformer multi;withmimo speech;multi channel multi;channel multi;sequence sequence architecture;speech end end;multi channel;speaker speech;speech end;end multi channel", "pdf_keywords": "speech recognition sequence;channel input speech;text sequences speaker;channel multispeaker speech;speech recognition multistage;speech separation recognition;multichannel multispeaker speech;single speaker neural;model speech recognition;sequences speaker;multimicrophone speech processing;speaker neural;speaker speech recognition;sequences speaker end;mimo speech specifically;mimo speech techniques;neural networks speech;speech recognition stage;end speech recognition;input speech signals;multimicrophone speech recognition;multimicrophone speech;based multimicrophone speech;sequence model trained;speech recognition;speech recognition model;multispeaker speech recognition;mimo speech;speech processing;model speech"}, "ab193c05bc447f368565c1ff37064b1c517a750f": {"ta_keywords": "dialogue policy learning;learning agents dialogue;policy learning bbq;agents dialogue systems;policy learning;introductionefficient dialogue policy;exploration deep learning;explore thompson sampling;learning bbq networks;agents dialogue;learning agents;exploration deep;dialogue policy;introductionefficient dialogue;deep learning agents;thompson sampling;dialogue systems;efficiency exploration deep;learning bbq;algorithm learns;agents explore thompson;dialogue systems resultsour;algorithm learns faster;dialogue;agents explore;network algorithm learns;thompson sampling drawing;dialogue systems methodswe;learns;learns faster", "pdf_keywords": "learning agents dialogue;deep reinforcement learn;promising deep reinforcement;deep reinforcement learning;reinforcement learning speech;dialogue optimal policy;exploration deep reinforcement;model dialogue optimal;learn policies dialogue;deep reinforcement;reinforcement learning discuss;agents dialogue systems;exploration deep learning;dialogue optimal;policies dialogue systems;explore dialogues;dialogue systems agents;greedy exploration boltzmann;exploration dialogues;explore dialogue;approach exploration deep;reinforcement learn;explore dialogue fundamental;ability explore dialogues;reinforcement learning useful;exploration deep;deep learning agents;natural language dialogue;dialogue systems;learning agents"}, "2b5d553cb2f298f36aff1a1519f7f2f6be4db5da": {"ta_keywords": "supervised taxonomy expansion;existing taxonomy expansion;taxonomy expansion;taxonomy expansion model;study taxonomy expansion;expand existing taxonomies;supervision existing taxonomy;taxonomies new concept;taxonomy expansion problem;self supervised taxonomy;existing taxonomies;new concept terms;taxonomies new;existing taxonomies new;concept terms propose;existing taxonomy;supervised taxonomy;taxonomies;concept terms;taxonomy;ontologies underpin numerous;knowledge ontologies;taxonomies used;ontologies;knowledge ontologies underpin;propose self supervised;important knowledge ontologies;ontologies underpin;daily basis taxonomies;taxonomies used practice", "pdf_keywords": "supervised taxonomy expansion;self supervised taxonomy;self taxonomy expansion;taxonomy expansion propose;effective taxonomy expansion;existing taxonomy expansion;taxonomy expansion natural;supervision existing taxonomy;taxonomy induction expansion;taxonomy expansion world;taxonomy expansion;approach taxonomy expansion;self taxonomy;taxonomy expansion model;taxonomy expansion fuse;expansion taxonomy construction;expansion taxonomy;taxonomy expansion task;natural self taxonomy;taxonomies new concept;taxonomy expansion employing;knowledge existing taxonomy;taxonomy labeling efforts;supervised taxonomy;performs taxonomy expansion;expansion taxonomy text;existing taxonomy labeling;taxonomy expansion method;expand taxonomy propose;expand existing taxonomies"}, "433b24d63146605d25c0c271062e129608462f03": {"ta_keywords": "reducing dimensionality kernel;nonlinear classification;achieve nonlinear classification;recognition framework nonlinearly;hidden markov models;speech recognition;speech recognition framework;dimensionality kernel;dimensionality kernel spaces;models automatic speech;automatic speech recognition;convexity training;nonlinear classification original;convexity training log;maintain convexity training;training log linear;hidden markov;high dimensional features;kernel spaces;recognition framework;automatic speech;reducing dimensionality;linear models mixtures;classification;markov models automatic;kernel spaces results;dimensional features;functions hidden markov;recognition;markov models", "pdf_keywords": ""}, "a0e0ce316ce0fdca2db61a52fdc7100e24906075": {"ta_keywords": "ensemble outlier detector;based ensemble outlier;ensemble outlier;outlier detector;outlier detector called;measures outlierness;outlierness multiple;algorithm measures outlierness;measures outlierness multiple;outlier;outlierness;outlierness multiple scales;density based ensemble;xstream extreme streaming;called xstream extreme;xstream extreme;extreme streaming;detector called xstream;extreme streaming setting;xstream;called xstream;streaming;density based;streaming setting;based ensemble;propose density based;streaming setting following;high dimensionality distance;ensemble;high dimensionality", "pdf_keywords": ""}, "1a0d8dbd0252193abe9d64f72fc56cc1f05ed3eb": {"ta_keywords": "situational reasoning;situational reasoning elicit;reasoning elicit consequences;graph relevant consequences;consequences explicitly structured;elicit consequences;consequences new situation;effects unexpected situations;elicit consequences new;goal situational reasoning;relevant consequences;relevant consequences explicitly;reasoning elicit;situations cloudy skies;consequences explicitly;situations;situations cloudy;consequences;unexpected situations cloudy;situational;unexpected situations;reasoning;build graph relevant;consequences new;predict effects;graph relevant;situation st arises;skies help hinder;models;predict effects unexpected", "pdf_keywords": "situational reasoning graphs;situational reasoning datasets;constructing situational reasoning;situational reasoning framework;reasoning graphs pretrained;situational reasoning effective;effective situational reasoning;situational reasoning elicit;situational reasoning method;situational reasoning;st reasoning graphs;reasoning datasets st;reasoning graphs;proposed situational reasoning;reasoning graphs validated;reasoning datasets;reasoning framework;reasoning framework proposed;constructing situational;datasets st reasoning;generated situational graphs;generating st reasoning;st reasoning improve;situational graphs especially;situational graphs;improve st reasoning;st reasoning generalize;downstream reasoning task;present situational reasoning;reasoning improve performance"}, "8529af634b443427d87d62d64467d2f1adfc230f": {"ta_keywords": "characterization malware samples;identification characterization malware;malware samples;malware samples primarily;characterization malware;samples malware;unsupervised identification characterization;methods unsupervised identification;samples malware remains;unsupervised learning algorithm;malware;unsupervised learning;multi modal clustering;unsupervised identification;actors samples malware;malware remains;standard unsupervised learning;modal clustering;clustering;malware remains active;modal clustering problem;methods unsupervised;identifying possible threat;unsupervised;identification characterization;ramifications cybersecruity;standard unsupervised;important ramifications cybersecruity;threat actors samples;clustering problem", "pdf_keywords": ""}, "8424dd233577e3bd3fbd7ecdfd8b4d442531a20e": {"ta_keywords": "regression hidden markov;linear regression hmms;regression hmms;regression hmms considering;model hmm parameters;hmm parameters widely;model hmm;hidden markov model;speech processing regression;hmms considering regression;hmm parameters;gaussians hmms;hidden markov;markov model hmm;gaussians hmms gassians;introductionlinear regression hidden;sets gaussians hmms;introductionlinear regression;regression tree;regression tree structure;speech processing;hmms gassians clusters;training time series;markov model;considering regression tree;regression hidden;time series pattern;processing regression parameters;especially speech processing;realizes fully bayesian", "pdf_keywords": ""}, "7b72f79015a0a5e06cc019bae78f268b16a8e659": {"ta_keywords": "speech recognition number;variability noisy speech;speech recognition;noisy speech;dnn model low;automatic speech recognition;size dnn model;neural networks dnns;dnn model;singular value decomposition;focused clean speech;deep neural networks;reduce size dnn;networks dnns;successful automatic speech;automatic speech;clean speech;speech additional variability;networks dnns proven;clean speech additional;deep neural;size dnn;weight matrices computed;weight matrices;approximations weight matrices;value decomposition vd;low rank approximations;neural networks;recognition number parameters;computed using singular", "pdf_keywords": ""}, "0ea90d783a76b7c119fb5471fc71b6bc2defa06d": {"ta_keywords": "codes model distributed;distributed storage methodsunder;distributed storage;model distributed storage;regenerating codes model;efficiently recoverable nodes;regenerating codes;minimize download;node efficiently recoverable;storage methodsunder;setting regenerating codes;minimum bandwidth;purposeto minimize download;storage methodsunder model;storage;bandwidth setting regenerating;stored node efficiently;distributed;codes model;recoverable nodes;minimum bandwidth setting;bandwidth;node efficiently;efficiently recoverable;nodes data stored;recoverable nodes lower;model distributed;stored node;bandwidth setting;minimize download focus", "pdf_keywords": "codes distributed storage;distributed storage resultthe;storage node recovery;node distributed storage;distributed storage data;distributed storage;codes model distributed;download storage node;codes distributed;node efficiently recovered;efficient recovery data;storage node;analysis regenerating codes;constructing codes distributed;read download storage;node recovery consider;download storage;bounds regenerating codes;distributed storage rest;node recover data;storage rest nodes;regenerating codes powerful;regenerating codes model;model distributed storage;node recovery;recovering contents node;regenerating codes;storage data;recovery data;storage resultthe"}, "2030551b2590fa70eb5132131e6627c93128f0a1": {"ta_keywords": "parametric utility learning;utility learning non;utility learning;learning non cooperative;cooperative agents mixture;cooperative continuous games;modeling non cooperative;multiple utility functions;combining multiple utility;agents mixture utilities;utility functions;non cooperative agents;cooperative agents;mixture regression models;games using probabilistic;adaptation mixture regression;mixture utilities;mixture utilities non;mixture regression;non cooperative continuous;multiple utility;non cooperative;continuous games;parametric utility;cooperative continuous;utility;cooperative;creating mixture utilities;adaptation mixture;utility functions creating", "pdf_keywords": ""}, "b34f254083b012bafd9b5ebd6c27450b4213c984": {"ta_keywords": "captions biomedical publications;understanding captions biomedical;captions biomedical;captions dense information;awareness captions biomedical;understanding captions;accompanying captions;captions;accompanying captions captions;captions captions;introductionincreased awareness captions;captions captions dense;awareness captions;captions dense;figures accompanying captions;biomedical publications extracting;scheme understanding captions;publications extracting classifying;standard information extraction;publications figures accompanying;image pointers references;publications extracting;scientific publications figures;classifying image pointers;information extraction methods;information extraction;biomedical publications important;biomedical publications;publications figures;image pointers", "pdf_keywords": ""}, "4a19211e077bc4ada685854245ba9fab381cbb06": {"ta_keywords": "relation extraction open;relation extraction;including relation extraction;extracting structured relation;relation tuples unstructured;introductioninformation extraction;text based relation;tuples unstructured texts;structured relation tuples;introductioninformation extraction refers;relation specifications free;automatically extracting structured;refers automatically extracting;extracting structured;informal relation specifications;based relation tuples;relation tuples;unstructured texts common;structured relation;extraction refers automatically;relation specifications;unstructured texts;relation types informal;extraction open systems;limited relation types;specifications free text;types informal relation;including relation;relation tuples order;relation types", "pdf_keywords": "relation extraction;question answering machine;relation extraction open;approaches question answering;question answering;question answering based;q4ie question answering;novel question answering;common relation extraction;extracting structured relation;text based relation;machine reading comprehension;answering machine reading;parsing q4ie;performance question answering;results question answering;problem question answering;answering based framework;question answering setting;tuples unstructured texts;structured relation tuples;relation tuples unstructured;parsing q4ie question;friendly knowledge base;information extraction;neural network qa;sentences annotators;existing knowledge base;knowledge base;extraction based semantic"}, "409280796e924bfe71421fe5bf4986bd3591ea72": {"ta_keywords": "similarity joins word;joins word based;similarity joins;web based databases;using similarity joins;integrating data sources;distributed heterogeneous databases;heterogeneous databases;data integration;heterogeneous databases available;joins word;based databases satisfy;data integration using;databases satisfy;databases contain informal;based databases;problem proposed databases;proposed databases contain;proposed databases;word based information;databases satisfy requirement;databases;common object identifiers;databases available world;joins;information representation language;integration using similarity;names objects web;object identifiers solution;databases contain", "pdf_keywords": ""}, "4e3016617e5e254bafebcbd7e96c509f670bdd37": {"ta_keywords": "music performance synthesis;synthesize musical score;text speech synthesis;speech synthesis;speech synthesis present;speech music;novel score audio;notes speech music;synthesize musical;music contains polyphony;music contains polyphon;score audio music;music performance;speech music contains;score audio;compared speech music;musical score natural;musical score;aims synthesize musical;polyphony long notes;performance synthesis aims;performance synthesis;background music performance;polyphon;musical;audio music performance;deep performer novel;text speech;performer novel score;advances text speech", "pdf_keywords": "audio music synthesis;music synthesis;synthesize musical score;music performance synthesis;backgroundmusic performance synthesis;synthesis musical;computational alignment music;synthesis musical instrument;synthesize musical;mixer synthesis musical;music synthesis employ;speech synthesis;aims synthesize musical;text speech synthesis;polyphonic mixer synthesis;encoder decoder polyphonic;speech synthesis application;neural speech synthesis;deep performer synthesize;novel score audio;alignment synthesis models;generating musical music;alignment music;generating musical;alignment music fundamental;decoder polyphonic inputs;decoder polyphonic;speech synthesis present;music fine alignments;method generating musical"}, "2a39a4f2d18e376ef8a6e2f45416e7b87957481e": {"ta_keywords": "historical text normalization;multi task learning;text normalization;nonllinguistic tasks multi;text normalization suffers;task learning;task learning 10;task learning mainly;task learning used;nonllinguistic tasks;tasks multi task;observed nonllinguistic tasks;multi task;tasks multi;normalization suffers;normalization;context historical text;benefits multi task;normalization suffers small;learning 10 different;shown multi task;tasks;text;task;representing different languages;historical text;learning;learning 10;datasets exhibit high;datasets exhibit", "pdf_keywords": ""}, "bd17620c6cb5ca97ef773499223d1509d123745f": {"ta_keywords": "purposedive deep learning;make deep learning;deep learning;deep learning open;deep learning approachable;purposedive deep;interactive examples;interactive examples self;math interactive examples;learning;learning open source;math interactive;injupyter notebooks;learning open;deep;learning approachable;resource freely;seamlessly integrating exposition;attempt make deep;exposition figures;make deep;code;exposition;self contained code;injupyter notebooks seamlessly;resource;code entire book;resource freely available;examples self contained;exposition figures math", "pdf_keywords": "deep learning method;linear regression learn;learning based linear;method deep learning;learning regression;learning method;methods deep learning;deep learning approach;emphysema new method;linear model neural;deep learning process;deep learning model;molecular basis emphysema;learning deep model;deep learning methods;machine learning method;implement deep learning;method deep neural;linear models deep;deep learning developed;methods deep neural;method deep network;linear regression method;algorithms deep learning;networks develop neural;approach deep learning;emphysema essential development;implement linear regression;regression learn model;techniques deep learning"}, "a30f912f8c5e2a2bfb06351d4578e1ba3fa37896": {"ta_keywords": "decoder pre training;models natural languages;pre trained models;encoder decoder pre;transfer programminging languages;natural languages;decoder pre;programminging languages;natural languages nih;introduction pre trained;programminging languages pl;transfer programminging;methods rely encoder;training suboptimal generation;encoder;trained models natural;programminging;rely encoder;pre trained;encoder decoder;rely encoder decoder;pre training;pre training suboptimal;languages nih;languages pl;languages;trained models;like biert andgpt;decoder;languages nih like", "pdf_keywords": "identifier aware pretraining;identifieraware pre training;programming language natural;code specific tokenizer;language models code;new code models;novel identifier aware;code understanding generation;code models human;generation code snippets;pre trained language;identifiers code propose;identifier aware pre;code models challenging;novel identifieraware pre;natural language implement;learning code;code models;decoder pre training;propose novel identifieraware;trained language models;identifiers code;snippet based gram;pre trained encoderdecoder;identifier aware;like bert andgpt;based gram code;encoder decoder pre;novel identifieraware;models natural language"}, "70bc4dc0bc72816773006c71b56fa5885c729caa": {"ta_keywords": "neural music generation;music generation model;convohutional gan based;demonsting convohutional gan;convohutional gan;music generation;generative adversarial;gan based model;applying generative adversarial;new neural music;gan based;neural music;generative;piano roslls generating;gan;generating realistic aesthetic;generating realistic;applying generative;model generating;musegan;model generating mimulting;musegan methodswe exploit;musegan methodswe;realistic aesthetic pieces;potential applying generative;adversarial;based model generating;generating;proposed called musegan;called musegan", "pdf_keywords": ""}, "78c9181abe18575925fbbb6e6d8c72d7bf90d06d": {"ta_keywords": "adaptive macs proposed;based adaptive macs;adaptive macs;macs proposed;channel seek mac;wireless channel;time slotted wireless;wireless channel seek;macs proposed methodswe;slotted wireless channel;mac provides;seek mac;macs;seek mac provides;nodes sharing time;hybrid adaptive adaptive;mac;hybrid hybrid adaptive;slotted wireless;hybrid adaptive;adaptive adaptive based;adaptive based adaptive;control central scheduler;based adaptive based;wireless;adaptive based;mac provides low;channel seek;central scheduler;collocated nodes sharing", "pdf_keywords": "protocol based qzmaca;based qzmaca protocol;cyclic services implementation;based zmac protocol;policy qzmaca protocol;scheduler implemented decentralized;qzmaca protocol;services implementation qzmac;qzmaca protocol effective;consider centralized scheduler;propose centralized scheduler;centralized scheduler thein;scheduler protocol;zmac protocol;centralized scheduler;decentralized scheduling;propose distributed scheduler;propose scheduling protocol;decentralized scheduling wide;zmac protocol useful;scheduling protocol;method decentralized scheduling;scheduler centralized;distributed scheduler;qzmaca protocol protocol;knowledge scheduler protocol;distributed scheduler implemented;scheduler centralized knowledge;centralized scheduler following;implementation qzmac policy"}, "9bbdcc03d872987eef9165f4a63c3878a5b05189": {"ta_keywords": "text representation encoders;transformer language models;trained transformer language;representations efficient text;encode text sequences;deep ls encode;retrieval dense encoders;ls text representation;text representation;representation encoders;transformer language;ls encode text;introductionpre trained transformer;encode text;models ls text;language models ls;trained transformer;representation encoders prior;ls encode;encoders;dense encoders;dense encoders require;text sequences;language models;encode;text sequences sentences;comparison retrieval dense;dense vector representations;vector representations efficient;encoders prior research", "pdf_keywords": "retrieval dense encoders;encode text sequences;dense retrieval;lexical retrieval dense;dense retrieval systems;representations efficient text;large scale retrieval;lexical retrievers efficient;text representation encoders;retrieval dense retrievers;application dense retrieval;retrieval dense;processing text;encoder pre training;efficient text;text large collections;large dataset language;trained bi encoders;dense encoders require;sentence level tasks;retrievers efficient task;text sequence task;encode text;tuning general lexical;search tasks condenser;comparison retrieval dense;processing text text;bi encoder;transformer language models;bert encoder"}, "2dd81061e0b11c828446f6a1843741ae51facbd2": {"ta_keywords": "models cortical networks;cortical networks;cortical networks layer;hierarchical models cortical;delayed processing stimuli;models cortical;neurons;response lag inherent;finite neurons;latent equilibrium;response time physical;introduce latent equilibrium;response lag;elements finite neurons;learning networks;latent equilibrium new;response time;networks;time physical computational;lag inherent property;lag inherent;learning introduce latent;introduces response lag;finite neurons exception;neurons exception hierarchical;stimuli causes timing;cortical;physical computational;inference learning networks;processing stimuli", "pdf_keywords": "neural synaptic dynamics;learningneuronal dynamics synaptic;neuron dynamics;neural synapse dynamics;relaxation dynamics neuronal;deep learningneuronal dynamics;learningneuronal dynamics;neuron dynamics present;neuronal dynamics synaptic;neuronal dynamics;principle neuron dynamics;model dynamics neurons;neuron dynamics gradient;dynamics neurons;generating neuron dynamics;synaptic dynamics;dynamics neuronal;neuronal dynamics gradient;neuronal dynamics present;dynamics neuronal neurons;synaptic plasticity dynamics;neural circuits efficiently;neurons demonstrate dynamics;synaptic dynamics based;approach dynamics neurons;dynamics neural;synapse dynamics;neuron network;dynamics synaptic;demonstrate dynamics neuronal"}, "87f42406de084e60d2365adac8a159ed3e455856": {"ta_keywords": "streaming heterogeneous graphs;stream heterogeneous graphs;persistent threat detection;detecting streaming heterogeneous;advanced persistent threat;annomaly detecting streaming;efficient annomaly detecting;spot anomalous ones;edges spot anomalous;spot anomalous;detecting streaming;persistent threat;streaming heterogeneous;advanced persistent;backgroundfast memory efficient;heterogeneous graphs important;heterogeneous graphs containing;memory efficient annomaly;persistent;heterogeneous graphs;stream heterogeneous;detection given stream;anomalous ones real;backgroundfast memory;graphs containing;graphs important;nodes;annomaly detecting;consuming bounded memory;anomalous ones", "pdf_keywords": "detection anomalous graphs;detect anomalous graphs;anomaly flow graphs;detect anomaly flow;cluster detect anomaly;anomaly detection flow;clustering detection anomalous;detection anomalous;detect anomaly;anomaly detection present;clustering anomaly detection;anomalous graphs;clustering based anomaly;detect anomalous;detecting anomalies efficient;anomaly detection;approach detect anomalous;mining outliers graphs;algorithm detection anomalous;clustering detects anomalies;anomalous graphs given;outliers graphs powerful;based anomaly detection;detection anomalous heterogenous;anomaly flow;approach clustering anomaly;graphs originating stream;detecting anomalies;clustering anomaly;propose streamspot clustering"}, "4cc70dd760c2c8cfc0107921bade45fb5efe860e": {"ta_keywords": "content based recommendation;recommendation methods;external knowledge graphs;recommendations use external;based recommendation methods;recommender systems using;collaborative filtering techniques;based recommendations using;based collaborative filtering;recommender systems;recommendation methods paper;recommendations using;collaborative filtering;based recommendations;based recommendation;improving performance recommender;knowledge graphs kgs;recommendations use;knowledge graphs important;content based collaborative;using knowledge graphs;knowledge graphs;recommender;use external knowledge;performance recommender systems;focused recommendations use;external knowledge;kg based recommendations;performance recommender;recommendation", "pdf_keywords": ""}, "fa2125641578934de12d7f792b094ffcfdf82ee2": {"ta_keywords": "text picture synthesis;text picture russian;picture russian language;picture designed russian;picture synthesis methodology;russian language processing;picture synthesis;designed russian language;text picture designed;picture russian;russian language described;russian language;purpose text picture;designed russian;text picture;introduction text picture;described text picture;language processing;language analysis subsystem;language processing operates;russian;natural language;processing subsystem rendering;subsystem processing stage;subsystem stage processing;operates natural language;text;language described;rendering subsystem processing;stage processing subsystem", "pdf_keywords": ""}, "c818f9be503a1ed94f991a2949c29e3ee477e8b8": {"ta_keywords": "speech enhancement;speech enhancement important;background speech enhancement;suppression speech enhancement;improve automatic speech;noise suppression speech;automatic speech recognition;speech recognition ar;speech enhancement causes;speech recognition;expectation automatic speech;noise suppression;automatic speech;speech recognition decoding;recognition ar noisy;uncertainty enhanced features;additional distortions speech;distortions speech signals;speech signals degrades;wrong noise suppression;suppression speech;uncertainty enhanced;enhanced features;distortions speech;consider uncertainty enhanced;improve automatic;enhanced features achieved;speech signals;enhancement important;enhancement", "pdf_keywords": ""}, "da1d5dc331c2839dfc3e6a79ee17f3bdf2231a8b": {"ta_keywords": "entity list completion;relation list extraction;entity examples seeds;entity list;perform entity list;target entity examples;relation list;list extraction;list extraction techniques;query entity target;entity examples;list completion;given query entity;objects complete set;list completion task;query entity;partial set seed;entity target entity;retrieval process methods;seed objects complete;target entity;retrieval;entity;retrieval process;entity target;billion triple dataset;complete set;triple dataset ranked;set expansion refers;focus relation list", "pdf_keywords": ""}, "eb28e82ca0bbc5d83e1cc07807da16874105d2fa": {"ta_keywords": "semantic frame induction;unsupervised semantic frame;semantic frame;unsupervised semantic;backgroundunsupervised semantic frame;perform unsupervised semantic;web scale corpus;frame induction;backgroundunsupervised semantic;framework word embeddings;frame induction using;frame induction problem;word embeddings;corpus perform unsupervised;triframesthe concept leadershipship;corpus;scale corpus;semantic framework word;semantic;using triclustering supplementary;semantic framework;frame induction cast;triclustering supplementary;new approach semantic;cast frame induction;triclustering problem generalization;context text;frame text;clustering triadic;induction using triclustering", "pdf_keywords": ""}, "572535aff31c400578fdd75313c896c0650b2d4c": {"ta_keywords": "beamforming automatic speech;automatic speech;automatic speech recognition;dereverberation beamforming automatic;speech recognition;paradigm automatic speech;speech recognition ar;dereverberation beamforming;integration dereverberation beamforming;sequence s2s modeling;focus transcribe end;transcribe end;beamforming automatic;dry focus transcribe;focus transcribe;transcribe end end;transcribe;speech;sequence s2s;sequence sequence s2s;beamforming;recognition ar sequence;end integration dereverberation;s2s modeling;sequence sequence sequence;sequence sequence;dereverberation;ar sequence sequence;s2s modeling popular;end end e2e", "pdf_keywords": ""}, "8495c1722e1f5107733c842839c2d298b9116921": {"ta_keywords": "answer embedding conversational;history answer embedding;conversational question answering;embedding conversational question;answer embedding;model conversation history;conversation history answer;embedding conversational;question answering methodsberer;question answering resultsberer;conversation history;question answering;model conversation;prepend history;used model conversation;prepend history turns;methods prepend history;answer current question;answering methodsberer;answering;conversational question;answering resultsberer;answering methodsberer bbert;conversational;conversation;history turns current;methodsberer bbert history;bbert history answer;answering resultsberer behr;history answer current", "pdf_keywords": "predicting conversation history;conversational search model;turn conversational search;conversational search;conversation history easily;model conversation history;conversation history conversational;conversation history based;conversation history dialog;conversation history effective;handling conversation history;conversational question answeringwe;search model conversation;results conversation history;conversation history conversation;conversation history;turns conversation history;conversation history giving;predicting conversation;conversational search apply;framework conversational search;handle conversation history;demonstrate conversation history;conversation history propose;important predicting conversation;process conversation history;conversation history answer;history conversation history;conversation history convqa;history dialog"}, "14f925098d57b0fa491a100fa73e52dbc764efa6": {"ta_keywords": "cognitive neuroscience used;neuroscience used sociology;cognitive neuroscience;theory cognitive neuroscience;economics marketing psychology;sociology economics marketing;neuroscience used;neuroscience;marketing psychology;economics marketing;used sociology economics;marketing psychology disciplines;sociology economics;theory cognitive;psychology;cognitive;used sociology;economics;psychology disciplines recent;tool theory cognitive;psychology disciplines;sociology;marketing;disciplines;theory;aimthe tool theory;purposethe tool theory;disciplines recent;disciplines recent years;tool theory", "pdf_keywords": ""}, "32a2a8baf217d29f628d22d793cace95634f51d5": {"ta_keywords": "email corpora tests;leak detection recipient;solutions real email;email users methods;mozilla thunderbird;email increasingly ubiquitous;email corpora;email increasingly;static email corpora;information leak detection;real email users;thunderbird;email users;extension mozilla thunderbird;static email;detection recipient recommendation;real email;background email increasingly;information leak;email;recipient recommendation;recipient recommendation study;problems information leak;leak detection;detection recipient;promise static email;recipient;corpora tests;background email;mozilla", "pdf_keywords": ""}, "1092da11519fe8427c8113b16a012f34f4a3fb6b": {"ta_keywords": "ethical federated learning;accuracy fairness privacy;ethical federated;fairness privacy;presents ethical federated;federated learning;fairness metrics federated;predictions data privacy;learning differential privacy;metrics federated learning;differential privacy work;differential privacy;federated learning model;federated learning differential;privacy researchers tackle;data privacy researchers;privacy researchers;privacy;ethical;data privacy;fairness metrics;introducing fairness metrics;privacy work;accuracy fairness;issues introducing fairness;federated;interplay accuracy fairness;introducing fairness;ethical concerns;privacy work presents", "pdf_keywords": "fairness differential privacy;discriminate privacy fairness;incorporate fairness privacy;fairness privacy model;privacy fairness;privacy agent training;fairness privacy;fairness privacy allowed;privacy decoupling training;fairness privacy evident;learning privacy;privacy fairness human;fairness privacy guarantees;asymmetrical privacy ff;privacy discrimination aggregator;discriminate privacy;ability discriminate privacy;federated learning privacy;privacy guarantees artificial;model privacy agent;phases fairness privacy;discrimination privacy;model privacy;privacy discrimination;privacy evident experimental;new discrimination privacy;trade fairness privacy;privacy ff framework;privacy fairness criminal;role privacy fairness"}, "ed913bed529d6bb7beac3b6086a853698abf627d": {"ta_keywords": "field speech recognition;home assistants spoken;digital home assistants;far field speech;speech recognition commands;speech processing digital;speech recognition;recognition commands spoken;speech processing;home assistants;assistants spoken language;field speech;home assistants important;spoken language interface;assistants spoken;commands spoken distance;spoken distance;spoken distance sound;recognition commands;learning called far;distance sound capturing;processing machine learning;far field;sound capturing;assistants important topic;language interface ubiquitous;sound capturing device;signal processing machine;digital home;spoken language", "pdf_keywords": ""}, "914626e2e13bd42a5f06c28ff02ba7c428e81ff1": {"ta_keywords": "coupled ionosphere thermosphere;global coupled ionosphere;coupled ionosphere;ionosphere thermosphere complex;ionosphere thermosphere global;thermosphere global coupled;ionosphere thermosphere;thermosphere complex;thermosphere global;thermosphere complex complex;ionosphere;disturbances global coupled;thermosphere;global coupled;complex global coupled;elucidation disturbances global;disturbances global;model elucidation disturbances;elucidation disturbances;coupled;disturbances;complex global;model elucidation;present model elucidation;complex complex global;global;model;present model;elucidation;complex", "pdf_keywords": ""}, "e829ee7fe48f4b1e451378b6a21470b2f86c0aa6": {"ta_keywords": "retrieval pir systems;retrieval pir;information retrieval pir;replication based systems;replication based;systems based erasure;based erasure codes;pir considers replication;retrieval;erasure codes;record public database;erasure codes gaining;data systems;considers replication based;data systems based;public database;replication;public database revealing;based systems storage;purposeprivate information retrieval;information retrieval;entire data systems;based erasure;pir systems allow;pir systems;revealing server record;database revealing;database revealing server;systems storage;server record retrieved", "pdf_keywords": ""}, "31603b3339f4da5bdc6b7de4231bd1ddfb32a50a": {"ta_keywords": "messageneurocontrolled differential equations;clinical messageneurocontrolled differential;emphcontrolled differential equations;messageneurocontrolled differential;mathematics emphcontrolled differential;emphcontrolled differential;key clinical messageneurocontrolled;clinical messageneurocontrolled;temporal dynamics;temporal dynamics fundamental;modelling temporal dynamics;messageneurocontrolled;modelling temporal;understood mathematics emphcontrolled;mathematics emphcontrolled;dynamics fundamental;dynamics;differential equations attractive;differential equations;initial condition mechanism;temporal;dynamics fundamental issue;emphcontrolled;trajectory based subsequent;option modelling temporal;differential equations resulting;ordinary differential equation;trajectory based;differential equation;differential equation determined", "pdf_keywords": "modelling temporal dynamics;neural controlled differential;model neural controlled;generating neural controlled;approach neural controlled;modelling temporal;equation model neural;controlled differential equations;mathematics controlled differential;neural time series;differential equation models;time series models;series models neural;temporal dynamics;temporal dynamics fundamental;method generating neural;neural controlled;equations extend neural;differential equations attractive;training neural time;modelling;controlled differential;generating neural;rn neural;input neural;demonstrate controlled differential;trajectory based;model capable processing;controlled differential equation;generating neural value"}, "5f563da2843e005c4b236f7889e7a22631b53210": {"ta_keywords": "conference peer review;peer review revisiting;peer review determine;reviewer quality scores;peer review;reviewer quality;inconsistency conference peer;review revisiting;examined inconsistency conference;review revisiting 2014;quality scores impact;quality scores subjective;review determine;experiment accepted papers;quality scores;reviewer;conference peer;inconsistency conference;variation reviewer quality;papers correlation quality;correlation quality scores;2014 neurips experiment;experiment examined inconsistency;examined inconsistency;paper revisit 2014;neurips experiment examined;backgroundinconsistency conference peer;revisit 2014 neurips;revisiting 2014 neurips;scores subjective", "pdf_keywords": "conference peer review;consistency peer review;inconsistency peer review;consensus conference review;conference review process;conference review;peer review revisiting;peer review randomly;reviews conference;reviews conference submitted;conference suggest reviewers;reviewers evaluation scientific;conference having reviewed;peer review;reviewer assessments;examined inconsistency conference;peer review process;reviewers evaluation;consistency reviewers evaluation;scoring reviewers;inconsistency conference peer;reviewer normally distributed;reviewer quality scores;papers undergo review;subjective scoring reviewers;subjectivity reviewer assessments;edition conference review;improve consistency reviewers;consistency reviewers;random committee review"}, "dd5e54b08b2c1520d179e88cd524e9bd4fe1f6ab": {"ta_keywords": "divergences sds regularized;emphsinkhorn divergences sds;divergences sds;transport maximum mean;backgroundoptimal transport maximum;backgroundoptimal transport;machine learning compare;emphsinkhorn divergences;learning compare probability;compare probability measures;divergences;regularized variant distances;paper emphsinkhorn divergences;sds regularized;learning compare;sds regularized variant;regularization strength varepsilon;regularization strength;depending regularization strength;transport maximum;transport;probability measures methodswe;depending regularization;probability measures;variant distances;variant distances interpolate;regularization;discrepancies mmd routinely;interpolate depending regularization;used machine learning", "pdf_keywords": "regularized optimal transport;regularization optimal transport;variant optimal transport;standard optimal transport;optimal transport;transport introduce regularized;complexity measured transport;optimal transport probability;optimal transport maximum;transport probability measure;optimal transport introduce;divergences sds regularized;optimal transport consists;problem optimal transport;optimal transport problem;field optimal transport;transport problem optimal;transport maximum mean;introduce regularized optimal;divergence continuous regularized;transport maximum;entropic regularization optimal;learning sinkhorn divergence;regularized optimal;divergences prove optimal;measured transport model;measured transport;transport problem fundamental;transport probability;regularization optimal"}, "8b192503f119a0b0cc30ef5179a00f231c20fb93": {"ta_keywords": "event datasets sequences;modeling events;datasets sequences events;parametric deep neural;modeling events using;deep neural;non parametric deep;event datasets;work modeling events;context event datasets;primarily tasks prediction;parametric deep;deep neural network;capture historical dependencies;tasks prediction;sequences events;events using conditional;events;events various types;tasks prediction methods;sequences events various;events various;non parametric models;historical dependencies;neural;conditional intensities rely;parametric models focus;events using;datasets sequences;event", "pdf_keywords": "deep event model;deep event;novel deep event;modeling event sequences;parametric deep learning;parametric deep neural;modeling events;non parametric deep;event data sequences;short term memory;deep neural;temporal attention multivariate;events sequential model;modeling event;continuous time history;event sequences model;deep learning;temporal attention;attention event channels;event sequences;modeling events using;data sequences events;parametric deep;time nonparametric;continuous time nonparametric;model temporal;events sequential;work modeling events;history dependent conditional;new model temporal"}, "0ca2575a1ef73930dc2abe205b44e079eadc426c": {"ta_keywords": "decision theoretic lane;lane changing model;model traffic flow;model traffic;lane changing behavior;theoretic lane changing;macroscopic model traffic;multiple lanes;lanes multiple populations;backgroundlane pricing decision;traffic flow;nonlinear pneumococcal drug;multiple lanes multiple;lanes multiple;lane changing;drivers local decision;backgroundlane pricing;pricing decision theoretic;incorporates multiple lanes;traffic flow incorporates;traffic;lanes;theoretic lane;nonlinear pneumococcal;pneumococcal drug drug;resulting nonlinear pneumococcal;multiple populations drivers;lane;pneumococcal drug;drug drug pdes", "pdf_keywords": ""}, "b5991b1018bb89b053a2c8229248f97956391bb5": {"ta_keywords": "estimationion grapheme phoneme;grapheme phoneme conversion;native speech recognition;native pronunciations design;non native pronunciations;crafted pronunciation lexicons;native pronunciations;hand crafted pronunciation;phoneme conversion;phoneme conversion systems;grapheme phoneme;methodsnon native speech;speech recognition;native speech differs;significantly native speech;native speech resulting;non native speech;pronunciations design new;pronunciations design;crafted pronunciation;pronunciation lexicons;automatic speech recognition;speech recognition ar;estimationion grapheme;speech recognition methodsnon;pronunciation lexicons used;native speech;performance automatic speech;joint estimationion grapheme;pronunciations", "pdf_keywords": ""}, "b19cba7bfe318c69d5e62f8322cb5d75228452f4": {"ta_keywords": "pretrained language models;language models downstream;scale pretrained language;non nl benchmarks;nl benchmarks fine;nl benchmarks;pretrained language;large scale pretrained;benchmarks fine tuning;language models;tuning weights models;tasks fine tuning;weights models millions;performance non nl;models downstream tasks;fine tuning weights;adapting large scale;weights models;adapting large;tuning weights;models downstream;fine tuning;scale pretrained;benchmarks fine;non nl;pretrained;downstream tasks fine;benchmarks;downstream tasks;background adapting large", "pdf_keywords": "pretrained language models;language models efficiently;scale language models;large scale language;language models better;scale language model;pretrained model weights;pretrained model weightswe;layers pretrained model;language models excellent;pretrained language;weight matrices pretrained;matrices pretrained model;language models;language models exploit;models efficiently adapt;model new tasks;allows pretrained language;parameters task performance;parameters pretrained model;large number tasks;language model;model weightswe propose;matrices pretrained;trainable parameters task;language models withwe;scale language;language models comparatively;adapt new tasks;layers improve performance"}, "3ba013b8b56646d66aac8472fb90a5c029ef55a0": {"ta_keywords": "equations parameterized neural;trajectories derivatives efficient;introductiondifferential equations parameterized;derivatives solution trajectories;solution trajectories derivatives;automatic differentiation optimizing;solve numerically training;mode automatic differentiation;automatic differentiation;trajectories derivatives;learned dynamics;differentiation optimizing;parameterized neural networks;differentiable surrogate time;derivatives efficient compute;introduce differentiable surrogate;derivatives efficient;parameterized neural;learned dynamics easier;differentiable surrogate;numerically training progresses;numerically training;neural networks expensive;numerical solvers;introductiondifferential equations;higher order derivatives;dynamics easier solve;standard numerical solvers;derivatives;numerical solvers using", "pdf_keywords": "automatic differentiation optimizing;slower adaptive solvers;provides automatic differentiation;trajectories derivatives efficient;automatic differentiation asymptotically;solve numerically training;numerically training progresses;solvers regularizing;automatic differentiation;dynamics neural algorithms;generalizewe regularizing dynamics;taylormode automatic differentiation;neural algorithms regularizing;equations parameterized neural;regularizing dynamics;numerically training;adaptive solvers;equations regularizing dynamics;learned dynamics easier;standard solvers regularizing;solvers regularizing orders;differentiation optimizing;differentiation optimizing additional;derivatives efficient compute;learned dynamics;differentiation asymptotically efficient;neural differential equations;equations regularization trajectories;differential equations regularization;differential equations regularizing"}, "26299d5fdc5137291dc6a091573b3d18aba1d1c2": {"ta_keywords": "pretrained multilingual models;pretrained multilingual;languages unseen pretraining;multilingual models;art pretrained multilingual;multilingual models multilingualbert;models multilingualbert xlim;cross lingual transfer;lingual transfer limited;models multilingualbert;lingual transfer;languages zero shot;low resource languages;multilingual;bootstrapping nonlinguistic applications;cross lingual;multilingualbert xlim;enabling bootstrapping nonlinguistic;nonlinguistic applications low;languages unseen;resource languages;bootstrapping nonlinguistic;shot cross lingual;languages languages unseen;lingual;multilingualbert xlim enabling;resource languages languages;unseen pretraining methodswe;languages zero;resource languages zero", "pdf_keywords": "pretrained multilingual models;multilingual pretrained models;adapting pretrained multilingual;adapt pretrained multilingual;pretrained multilingual model;existing pretrained multilingual;pretrained multilingual;multilingual model training;multilingual pretrained;multilingual model target;multilingual target language;multilingual multilingual models;multilingual models multilingual;multilingual models better;multilingual models;multilingual model train;languages unseen pretraining;adapts multilingual model;adapters multilingual pretrained;diverse languages tasks;models multilingual;multilingual target;transferring multilingual multilingual;high resource multilingual;model train multilingual;train multilingual systems;multilinguality adapts;multilingual models used;train multilingual;modelagnostic use multilingual"}, "cce8cf3d7f45113a4cba984b878802a5b16d5967": {"ta_keywords": "management malignant disease;malignant disease world;management malignant;etiology malignant disease;malignant disease;etiology malignant;malignant disease poorly;strategy management malignant;malignant;disease world;disease;disease poorly understood;disease poorly;etiology;development new strategy;strategy;new strategy;management;new strategy management;strategy management;world;understood development new;understood development;development new;development;poorly understood development;poorly understood;understood;new;poorly", "pdf_keywords": ""}, "77ce1b8d425b7538c21ce0be976ee24a58e797c1": {"ta_keywords": "channel source separation;matrix factorization michf;source separation accurately;source signals mixtures;negative matrix factorization;matrix factorization;source separation;recover source signals;single channel source;signals mixtures;factorization michf;signals mixtures non;factorization michf popular;source signals;factorization;discriminative training nf;mixture desired source;channel source;training nf basis;separation accurately recover;single channel;separation accurately;objective single channel;discriminative training;coefficients obtained mixture;paper introduces discriminative;mixtures non negative;non negative matrix;source optimally;nf basis", "pdf_keywords": ""}, "5e8c52ddbd3581320f7e536b7cd10d7263b81eb2": {"ta_keywords": "learned grammar siulated;developing intelligent agents;learning agents;features learned grammar;simulate human learning;intelligent agents simulate;learning agents able;learned grammar;intelligent agent developers;grammar siulated student;create features learned;generating features learned;intelligent agent;intelligent agents;developing intelligent;using developing intelligent;agents simulate human;grammar siulated;science learning agents;human learning;agents simulate;artificial intelligence cognitive;produce intelligent behavior;intelligent behavior human;features learned;human learning long;past intelligent agent;intelligent behavior;simulate human;artificial intelligence", "pdf_keywords": ""}, "4481244de2cc0c55d91cebbb152eec79a76386f3": {"ta_keywords": "conductive film 3d;density reliable packaging;reliable packaging technology;packaging technology;packaging technology non;flip chip assembly;reliable packaging;non conductive film;innovative flip chip;density reliable 3d;conductive film;packaging;film 3d;film 3d trajectories;chip assembly;reliable 3d;chip assembly process;conductive film contributes;technology non conductive;flip chip;3d;high density reliable;3d trajectories integrations;3d trajectories developed;reliable 3d trajectories;3d trajectories;package tier structure;methods innovative flip;3d trajectories high;package tier", "pdf_keywords": ""}, "d26254cf3ec537f37708afaaf7f5a76a7922d4a2": {"ta_keywords": "statistical machine translation;word reorderings hierarchical;reorderings hierarchical phrase;word reordering models;local word reorderings;reorderings source words;machine translation;machine translation methodsa;reorderings word pairs;existing word reordering;word reorderings;word reordering;learn reorderings word;reorderings word;hierarchical phrase based;data hierarchical phrase;machine translation conducted;hierarchical phrase;reorderings hierarchical;reordering models learn;translation methodsa systematic;learn reorderings source;models learn reorderings;translation methodsa;reordering models;learn reorderings;phrase based statistical;reorderings source;reorderings;learning local word", "pdf_keywords": ""}, "10e221c7d4636703c5c97b54f53b1cb57c25f3a6": {"ta_keywords": "stochastic gradient descent;reinforcement learning recently;networks cross entropy;optimized stochastic gradient;reinforcement learning;unregularized linear networks;gradient descent;stochastic gradient;backgroundinference learning;backgroundinference learning important;loss optimized stochastic;linear networks;evaluation reinforcement learning;entropy loss optimized;gradient descent converge;optimized stochastic;linear networks cross;separable data unregularized;policy evaluation reinforcement;cross entropy loss;networks;learning important problems;causal inference;learning;cross entropy;loss optimized;data unregularized linear;field causal inference;proved separable data;causal", "pdf_keywords": ""}, "6dfecb5915e8b10841abe224c5361bbda7100637": {"ta_keywords": "morphological parsers figrinya;parser combinators figrinya;morphological parsers;parsers figrinya andoromo;based morphological parsers;parsers figrinya;based parser combinator;parser combinators;languages based parser;parser combinator;parsers;theoretical efficiency parsers;parsers produce;parser combinator finite;rule based morphological;efficiency parsers produce;parser;efficiency parsers;based morphological;based parser;figrinya andoromo languages;background parser combinators;morphological;andoromo languages based;morphology;figrinya andoromo morphology;morphology methods;morphology methods present;andoromo morphology methods;andoromo languages", "pdf_keywords": ""}, "73e401dead436aabd0cd9c941f7b13bfdeda9861": {"ta_keywords": "distributed training;distributed distributed training;efefficient distributed trained;distributed training better;communicationefficient training 3p;communication efefficient distributed;distributed trained;distributed trained training;communicationefficient training;gradient communication;point compressors communication;compressors communication efefficient;gradient communication mechanisms;class gradient communication;mechanisms communicationefficient training;efefficient distributed distributed;3p efefficient distributed;compressors communication;theory lazy aggregation;efefficient distributed;lazy aggregation;lazy aggregation methods;communication mechanisms communicationefficient;communicationefficient;communication efefficient;3p point compressors;distributed distributed;point compressors;mechanisms communicationefficient;distributed", "pdf_keywords": "gradient communication;designing gradient communication;gradient communication mechanisms;communicationefficient training point;class gradient communication;distributed nonconvex optimization;distributed training;communicationefficient training;compressed distributed optimization;distributed optimization;lazily aggregation gradients;stochastic gradient methods;useful distributed training;gradient methods nonconvex;efficient distributed nonconvex;distributed training large;algorithm distributed training;gradients variance reduction;lazy aggregation communication;efficient distributed learning;adaptive nonconvex optimizing;nonconvex federated optimization;communication efficient distributed;distributed optimization perm;gradient process aggregation;mechanisms communicationefficient training;distributed learning;distributed training f21;applications stochastic gradient;stochastic gradients"}, "1c2c9e5d0588599516a78adda1fe3935dc5ae5d7": {"ta_keywords": "efficiency estimate;stochastic gradient play;efficiency estimate actually;known efficiency guarantee;method stochastic gradient;known efficiency;method unconstrained optimization;reduces known efficiency;strongly monotone game;unconstrained optimization argument;unconstrained optimization;method stochastic;stochastic gradient;efficiency guarantee method;efficiency;efficiency guarantee;monotone game;optimization argument;gradient play;optimization argument present;interpreting method stochastic;stochastic;method unconstrained;optimization;perturbed strongly monotone;strongly monotone;guarantee method unconstrained;estimate actually reduces;unconstrained;gradient play slightly", "pdf_keywords": ""}, "311381feeb6346bfcb2ba622bd8f713261a4075d": {"ta_keywords": "backgroundmanagement collaborative documents;collaborative documents;summarizing edits detecting;categorizing summarizing edits;user track document;collaborative documents difficult;track document flux;backgroundmanagement collaborative;summarizing edits;track document;profusion edits comments;visually rearranging comments;helping user track;relationship edits comments;edits comments crucial;edits comments;document evolution;edits detecting completed;edits detecting;user track;document evolution reliably;tasks categorizing summarizing;categorizing summarizing;modeling relationship edits;collaborative;edits comments multiple;comments multiple authors;authors make document;authoring tasks categorizing;rearranging comments", "pdf_keywords": ""}, "efcdb62b59e4dfb3f51b53850a81d6149ec3dfc8": {"ta_keywords": "interpretable machine learning;argue mri community;instead viewing mri;interpretable machine;mri community embrace;viewing mri methods;mri community;field interpretable machine;viewing mri;interpretable;argue mri;mri;mri methods panacea;field interpretable;mri methods;embrace diagnostic vision;machine learning im;machine learning;diagnostic vision field;gap argue mri;learning;diagnostic vision;progress field interpretable;learning im;diagnostic;embrace diagnostic;vision;community embrace diagnostic;viewing;machine", "pdf_keywords": "interpretable machine learning;use interpretability tools;interpretability tools human;interpretability tools machine;interpretability tools;use interpretable machine;approach interpretable machine;machine learning increasingly;use interpretability;systems use interpretable;machine learning context;discuss use interpretability;performance use interpretability;use interpretable;interpretability;interpretable machine;machine learning mm;use machine learning;useful insights use;use diagnostics;field interpretable machine;cases use interpretable;diagnostics use;use ai explainability;diagnostics use diagnostics;use diagnostics fundamental;machine learning increasing;ai explainability techniques;approach interpretable;athe use diagnostics"}, "null": {"ta_keywords": "probabilistic hierarchies graphs;learning probabilistic hierarchies;probabilistic hierarchies;probabilistic model hierarchies;hierarchies graphs;model hierarchies graphs;hierarchical clustering;hierarchies graphs important;hierarchical clustering efficient;tree based hieries;continuous relaxation tree;hierarchies graphs obtained;perform hierarchical clustering;relaxation tree;connections markov chain;relaxation tree based;novel probabilistic model;novel probabilistic;markov chain theory;end learning probabilistic;learning probabilistic;propose novel probabilistic;connections markov;markov chain;clustering efficient end;hierarchies;hierarchical;probabilistic;probabilistic model;model hierarchies", "pdf_keywords": ""}, "42fc352a0db1e742b0248a02b812db4aaf7b2cd3": {"ta_keywords": "neural machine translation;autoregressive neural machine;autoregressive neural;machine translation;machine translation nm;studied autoregressive neural;mle task measures;estimation mle task;autoregressive nm computational;mle task;mle training;training algorithms mle;algorithms mle training;autoregressive nm;score studied autoregressive;translation nm resulted;task measures blueu;alternative training algorithms;training objective task;neural machine;translation nm;mle training remains;estimation mle;autoregressive;approach autoregressive nm;translation;facto approach autoregressive;algorithms mle;mle;objective task measure", "pdf_keywords": "machine translation models;translation models large;translation models;machine translation ability;regressive translation tasks;machine translation increasing;neural machine translation;machine translation nmt;machine translation;translation models important;resource machine translation;ranking translation tasks;machine translation natural;translation use energy;machine translation important;approximation metric decoding;machine translation development;metric decoding;decoding metric use;translation tasks;decoding metric;translation tasks using;machine translation use;metric decoding metric;translation development neural;ranking translation;performance autoregressive neural;translation tasks present;translation models article;invasive translation tasks"}, "740afdd1619d797145b056877865f941891e6a65": {"ta_keywords": "risk minimization geometrically;expected loss minimization;risk minimization;dependent risk minimization;loss minimization;loss minimization given;distribution dependent decision;minimization geometrically deaying;decision dependent risk;problem expected loss;minimization geometrically;minimization;minimization given data;geometric decay process;expected loss;geometrically deaying dynamicmic;decision dependent;dependent decision;geometric decay;dependent decision maker;minimization given;according geometric decay;deaying dynamicmic;gradient oracle;deaying dynamicmic environments;decision maker;der gradient oracle;decision maker action;novel algorithms information;setting decision maker", "pdf_keywords": "risk minimization geometrically;dynamic optimization;expected loss minimization;action optimal;optimal point performatively;loss minimization;decision dependent learning;performative optimal;dynamic optimization approach;stocochastic dynamic optimization;performatively optimal points;performatively optimal point;gradient decision maker;target performative optimal;algorithms dynamic;action optimal arg;risk minimization;minimization geometrically deaying;performatively optimal;gradient decision;algorithms dynamic setting;stochastic algorithm optimize;distribution dependent decision;optimal points optimal;learning problem geometrically;target performatively optimal;dynamic pricing;feasible performatively optimal;decision maker bandit;loss oracle smoothed"}, "b9c026ab6e161a0f8c4b4db82ee8ad10792084cc": {"ta_keywords": "electrolaryngeal speech enhancement;electrolaryngeal speech minimizing;approach electrolaryngeal speech;electrolaryngeal speech;naturalness electrolaryngeal speech;speech enhancement;speech enhancement based;sounds enable laryngectomees;improving naturalness electrolaryngeal;electrolarynx device artificially;enable laryngectomees produce;electrolarynx device;enable laryngectomees;voice conversion;conclusion electrolarynx device;hybrid approach electrolaryngeal;statistical voice conversion;speech minimizing degradation;electrolarynx;naturalness electrolaryngeal;approach electrolaryngeal;voice conversion case;electrolaryngeal;intelligibility conclusion electrolarynx;laryngectomees;subtraction statistical voice;laryngectomees produce;speech minimizing;statistical voice;generates excitation sounds", "pdf_keywords": ""}, "066f2023b2b5ba5df61dc193c205785fa5e73fed": {"ta_keywords": "kernel clustering regularization;partitioning kernel clustering;kernel clustering;regularization based segmentation;clustering regularization;data partitioning kernel;clustering regularization based;partitioning kernel;segmentation;regularization kernel;methodologies data partitioning;data partitioning;based segmentation;clustering;explain regularization kernel;optimization spectral relaxation;partitioning;regularization based;formulation optimization spectral;segmentation addressing closely;optimization spectral;regularization;spectral relaxation;maxflow explain regularization;explain regularization;spectral relaxation vs;segmentation addressing;based segmentation addressing;relaxation vs maxflow;formulation optimization", "pdf_keywords": ""}, "9578679e028777dd709881f938114aa59fbbf481": {"ta_keywords": "matching entity names;java toolkit matching;comparison string distance;entity names investigate;compare string distance;string distance metrics;matching entity;distance metrics matching;toolkit matching methods;entity names;experimentally compare string;metrics matching;token based distance;heuristic string comparators;toolkit matching;string distance;metrics matching tasks;names investigate;introduction comparison string;compare string;matching methods;metrics task matching;communities edit distance;edit distance metrics;string comparators;task matching entity;metrics fast heuristic;fast heuristic string;based distance metrics;comparison string", "pdf_keywords": ""}, "ab627ba77dced941f9f45eeaee17bc6644308d89": {"ta_keywords": "classification email speech;classification email messages;email act classification;collective classification email;consider classification email;classification email;email speech acts;email speech;text classification;certain email acts;text classification algorithm;new text classification;email acts;act classification;act classification specifically;correlation email messages;context collective classification;email acts consider;acts consider classification;sequential correlation email;speech acts;improve email act;email act;email messages thread;email messages;collective classification;classification;speech acts consider;email messages contain;based dependency network", "pdf_keywords": ""}, "3429d0529d3e77f9e4606f13b2d252d5d964abad": {"ta_keywords": "prevention malignant disease;strategies prevention malignant;prevention malignant;etiology malignant disease;patients malignant disease;malignant disease;malignant disease patients;etiology malignant;disease patients malignant;patients malignant;malignant disease poorly;malignant;new strategies prevention;strategies prevention;prevention;disease patients;disease;disease poorly understood;etiology;patients;disease poorly;development new strategies;new strategies;strategies;report development new;report development;understood report development;development new;development;report", "pdf_keywords": ""}, "66b83f0801d0c2d4194ff60c5ef9c754b51ce521": {"ta_keywords": "segmentation pancreas ct;method segmentation pancreas;segmentation pancreas;image segmentation models;segmentation models;pancreas ct scans;interpreting image segmentation;segmentation models learning;image segmentation;segmentation;pancreas ct;models learning regions;ct scans qualitatively;learning regions images;regions images noise;method interpreting image;ct scans;learning regions;pancreas;method segmentation;regions images;interpretability model quantitatively;interpreting image;grad cam occlusion;interpretability model;methods interpretability model;cam occlusion sensitivity;occlusion sensitivity;scans qualitatively;apply method segmentation", "pdf_keywords": "segmentation pancreatic segmentation;segmentation models utility;pancreas segmentation using;pancreas segmentation;pancreatic segmentation;approach pancreas segmentation;pancreatic segmentation task;segmentation models learning;segmentation pancreatic;image segmentation models;pancreas segmentation pancreatic;segmentation models;segmentation using utility;interpreting segmentation models;backgrounddeep neural networks;image segmentation;models learning regions;segmentation models using;interpreting image segmentation;learning regions images;segmentation;user trained utility;segmentation task publicly;backgrounddeep neural;learning regions;approach interpreting segmentation;tasks pancreas identification;segmentation using publicly;interpreting segmentation;trained utility"}, "568efa8d71d8f2a086c8debcdf547e7053269021": {"ta_keywords": "technologies enhance learning;development new technologies;enhance learning;new technologies technologies;new technologies;technologies technologies enhance;ability enhance learning;technologies enhance;enhance learning context;enhance learning key;technologies technologies;learning process;technologies;learning process article;learning;learning context development;development new;discuss development new;learning context;ability enhance;development;learning key;learning key component;component learning process;discuss development;context development new;article discuss development;enhance;context development;key component learning", "pdf_keywords": ""}, "4f0d485cbcde840533f23f0c8b0f3fa1ca2d74df": {"ta_keywords": "transductive linear bandit;linear bandit problem;linear bandit;bandit problem;bandit problem given;drug discovery recommender;problem drug discovery;transductive setting;bandit;discovery recommender;drug discovery;transductive;transductive setting fundamental;transductive linear;mathbbrd fixed confidence;systems introduce transductive;discovery recommender systems;introduce transductive linear;introduce transductive;discovery;mathcalxsubset mathbbrd;mathbbrd set items;recommender systems introduce;recommender systems;mathcalxsubset mathbbrd set;mathcalxsubset;recommender;vectors mathcalxsubset mathbbrd;measurement vectors mathcalxsubset;mathcalzsubset mathbbrd", "pdf_keywords": "transductive linear bandit;transductive bandit problem;complexity linear bandits;exploration transductive bandit;transductive bandit probability;general transductive bandit;transductive linear bandits;generalization linear bandit;method transductive bandit;transductive bandit;linear bandit algorithm;algorithm linear bandit;sampling linear bandits;linear bandit strategy;bandit strategy optimal;linear bandit problem;bandit problem adaptive;withthe transductive bandit;bandit algorithm;generalizes linear bandits;bandit problem dimensional;bandit algorithm linear;design linear bandit;bandit probability possible;bandit strategy propose;linear bandit;linear bandits;bandit problem simple;bandit strategy;bandit problem"}, "7b19e6540c786b80a3615a8ae2ef706242a1fa5b": {"ta_keywords": "compressive phase retrieval;compressive phase;tackle compressive phase;compressive;phase retrieval problem;recovers sample complexity;recovers sample complexitysup;phase retrieval;tackle compressive;sublinear scheme recovers;sample complexitysup;sample complexity logsup;complexity log sublinear;linear scheme recovers;sample complexitysup sup;scheme recovers sample;sample complexity;log computational complexity;log sublinear scheme;recovers sample;complexity logsup;computational complexity log;complexitysup sup log;complexity log;complexity logsup sup;complexitysup;probability linear scheme;scheme recovers;computational complexity;complexitysup sup", "pdf_keywords": "compressive phase retrieval;sparse phase retrieval;noisy compressive phase;dimension compressive phase;compressive phase;method compressive phase;phase retrieval schemes;phasecode algorithm decoding;phase retrieval linear;graph proved phasecode;recovers sample complexity;complexity log sublinear;phase retrieval single;phase retrieval problem;signal dimension compressive;phase retrieval;phasecode algorithm;proved phasecode recover;application sparse phase;sublinear computational complexity;computational complexity sublinear;present phasecode algorithm;schemes noisy compressive;complexity signal;proved phasecode;phasecode recover fraction;sparse phase;computational complexity signal;novel method compressive;algorithm decoding"}, "a0cbaf59f563580f68523ab6839a436e38b6db18": {"ta_keywords": "multilingual corpus training;neural machine translation;nm multilingual corpus;translation nm multilingual;multilingual corpus;machine translation;low resource neural;machine translation nm;high resource language;nm multilingual;multilingual;low resource nm;corpus training;resource neural machine;data auxiliary languages;translation nm;resource neural;resource language generally;corpus training related;improve low resource;resource language;low resource;corpus;translation;language generally effective;smart data selection;data selection strategy;high resource;resource nm data;auxiliary languages", "pdf_keywords": "sampling helpful multilingual;learning multilingual translation;multilingual data sampling;multilingual translation models;sampling multilingual;optimal multilingual training;sampling multilingual data;data sampling multilingual;multilingual data training;multilingual corpora training;machine translation low;parameters optimal multilingual;learning multilingual;helpful multilingual data;translation low resource;optimal multilingual;multilingual translation;method multilingual training;nmt multilingual corpora;translation models;translation nmt multilingual;multilingual data constructing;multilingual corpora;multilingual data;multilingual training;translate low resource;multilingual parallel data;neural machine translation;resource language translation;multilingual training designed"}, "105146a7872835a52c8c5c55a3aae62c5d8852a1": {"ta_keywords": "morphological analysis transliteration;analysis transliteration tokenization;transliteration tokenization;analysis transliteration;transliteration tokenization required;machine translation traditionally;transliteration;machine translation;lexical processing steps;additional lexical processing;tokenization;lexical processing;translation traditionally formulated;morphological analysis;transduction strings words;words according white;processing steps morphological;introduction machine translation;simply dividing words;translation traditionally;steps morphological analysis;dividing words;structure words;dividing words according;morphological;tokenization required;tokenization required process;result additional lexical;transduction strings;target language result", "pdf_keywords": ""}, "9d10bbd21f475d500c3a7e24052e02596e052e2e": {"ta_keywords": "speech recognizer calledjlaser;speech recognizer;speech recognition;article speech recognizer;efficient speech recognition;speech recognition state;gram language models;networks efficient speech;recognizer calledjlaser based;recognition vocabulary;efficient speech;language models;recognizer calledjlaser;calledjlaser based neural;huge recognition vocabulary;neural networks efficient;recognition vocabulary article;language models applied;gram language;neural networks;recognizer;discovered neural networks;effect gram language;corpora applications;approach based gassian;recognition;recognition state art;calledjlaser based;based neural networks;vocabulary article speech", "pdf_keywords": ""}, "36d193c7a9523f55f9fe5ffd0730f248c241f5c7": {"ta_keywords": "bias peer review;peer review;improve peer review;peer review methods;peer review results;peer review sociootechnical;fairness bias peer;systems peer review;bias peer;peer review backbone;background peer review;bias unfairness urgent;bias unfairness;pertaining bias unfairness;fairness bias;challenges pertaining bias;improve peer;tutorial fairness bias;scholarly research;need improve peer;scholarly;scholarly research faces;review results associationi;review backbone scholarly;review sociootechnical intelligent;review results;tutorial fairness;peer;review sociootechnical;2020 tutorial fairness", "pdf_keywords": ""}, "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad": {"ta_keywords": "commonsense natural language;natural language inference;adjversarial filtertering data;new challenge dataset;adjversarial filtertering;achieve adjversarial filtertering;language inference;achieve adjversarial;adjversarial;challenge dataset;48 achieve adjversarial;language inference proves;commonsense natural;commonsense;challenge dataset questions;hellaswag new challenge;natural language;inference proves difficult;challenge;dataset questions trivial;new challenge;inference;discriminators iteratively;paradigm series discriminators;dataset;language;models struggle;models presenting;discriminators;models presenting hellaswag", "pdf_keywords": "robust commonsense reasonining;robust commonsense;demonstrate robust commonsense;commonsense inference challenging;present dataset commonsense;dataset commonsense;commonsense inference;nonli dataset commonsense;commonsense reasoning constructing;commonsense reasoning ability;dataset commonsense nonli;commonsense nonli dataset;models achieve adjversarial;adjversarial;commonsense reasonining ability;achieve adjversarial;dataset adversarial robust;commonsense reasoning;commonsense reasonining;commonsense inference proves;approach commonsense reasoning;problem commonsense inference;deep pretrained models;constructing dataset adversarial;commonsense;produced dataset adversarial;adjversarial filtertering;generative pre training;dataset adversarial;training generative pre"}, "de7d0c87794c3de6f8ab2c753ecc398c18c26631": {"ta_keywords": "generating descriptive grammar;descriptive grammar language;grammatical specification raw;specification raw text;pass grammatical specification;descriptive grammar;automated framework extracting;raw text concise;grammar language;language documentation preservation;extracting pass grammatical;generating descriptive;language documentation;grammatical specification;step language documentation;introduction generating descriptive;grammar language indispensable;specification raw;framework extracting;grammar;raw text;documentation preservation;readable format;language indispensable step;extracting;indispensable step language;text concise;automating;text concise human;readable format methods", "pdf_keywords": "grammatical specification extracted;extract grammar rules;automatically obtained syntactic;extract grammar;language rules efficiently;specification extracted rules;extract agreement rules;standard syntactically analysed;language propose automated;grammatical specification raw;syntactically analysed data;experiments extract grammar;grammar rules raw;automatic annotating raw;agreement rules language;syntactic analysis rules;agreement rules languages;pass grammatical specification;extracting ruleswe propose;used extract grammar;rules efficiently extracted;rules languages extensive;evaluation rules language;syntactic analysis developed;syntactically analysed;extract linguistically;specification extracted;treebanks tool evaluating;descriptive grammar language;learn syntactic rules"}, "b709da495f15a4a9c1173192ecd755d1697dedf0": {"ta_keywords": "representation publication databases;publication databases rich;publication databases;users reading citation;reading recommendation requires;reading recommendation;develop reading recommendation;monitor published literature;reading citation history;graph representation publication;reading citation;citation history methodswe;research biology increasing;citation history;recommendation requires input;published literature effectively;research biology;users reading;rich metadata representation;databases rich metadata;random walk pcrw;recommendation requires;metadata representation;literature effectively;published literature;rich metadata;constrained random walk;representation publication;metadata representation path;reading", "pdf_keywords": ""}, "bef4548a43fca8a7410734e4200157d50e257a29": {"ta_keywords": "automatic speech;automatic speech recognition;speech recognition ar;speech recognition;speech processing;concern automatic speech;processing speech;processing speech processing;language processing speech;speech processing key;transformers selfattention;transformers selfattention attends;recognition ar input;input sequence layer;components transformers selfattention;language processing;input sequence length;input sequence long;ar input sequence;sequence layer computational;selfattention attends input;recognition ar;speech;memory cost selfattention;input sequence;sequence layer;tasks natural language;automatic;selfattention square input;natural language processing", "pdf_keywords": ""}, "dad121213a17c6cc977d2298c7a9927639ca58e6": {"ta_keywords": "hearing impairment patients;case hearing impairment;patients hearing impairment;hearing impairment patient;hearing impairment present;hearing impairment associated;associated hearing impairment;impairment associated hearing;hearing impairment;clinic hearing impairment;hearing impairment impairment;problem hearing impairment;hearing impairment common;impairment patients hearing;common problem hearing;problem hearing;hearing clinic hearing;patients hearing;associated hearing;admitted hearing clinic;case hearing;patient admitted hearing;clinic hearing;hearing;hearing clinic;present case hearing;impairment present case;admitted hearing;impairment patients;impairment patient admitted", "pdf_keywords": ""}, "9f832bdcbc9d9566f7ab07b7455364bee62086fb": {"ta_keywords": "code natural language;pseudo code describes;source code written;pseudo code;pseudo code tedious;code written unfamiliar;source code;written unfamiliar programming;unfamiliar programming language;usefulness source code;corresponding pseudo code;programming language difficult;programming language;behavior source code;describes workings code;code describes;unfamiliar programming;understanding difficult code;code describes workings;code written;source code does;workings code natural;code does corresponding;code natural;difficult code;code tedious;code;code add corresponding;difficult code add;programming", "pdf_keywords": ""}, "f79361dda56ee755fc56ab83cf0d9f12d42b2d5e": {"ta_keywords": "pairwise comparisons items;algorithm ranks items;counting algorithm ranks;number pairwise comparisons;ranking items analyze;recovering ranking items;ranking items;pairwise comparisons;comparisons items;copeland counting algorithm;ranks items order;comparisons items goal;algorithm ranks;pairwise comparisons won;alternatively recovering ranking;recovering ranking;analyze copeland counting;ranks items;ranking;counting algorithm;copeland counting;items analyze copeland;order number pairwise;form pairwise comparisons;counting;precisely identifying items;items order number;pairwise;items order;comparisons", "pdf_keywords": "recovering ranking items;recovering ranking subset;alternatively recovering ranking;recovering ranking;ranking items;ranking subset items;ranking items analyze;exact recovery ranking;ranking items simple;recovery rankings counting;algorithm ranks items;rankings counting;entire ranking items;counting algorithm ranks;approximate recovery rankings;recovery ranking;recover entire ranking;items ranked scores;ranked scores items;scores items ranked;recovery ranking n1;ranked items;recovering highly ranked;ranked items based;items ranked;ranked scores analyze;highly ranked items;entire ranking;pairwise comparisons items;recovery rankings"}, "092687dc06b0b264a524c6d4ea151780ba85a02a": {"ta_keywords": "verbal communiation autism;communiation autism plus;communiation autism;audio modalities nonverbal;non verbal communication;non verbal communiation;nonverbal;non verbal social;autism;autism plus uses;utterances visual audio;autism plus;verbal communication incorporating;uses utterances visual;modalities nonverbal;nocoa non verbal;utterances visual;verbal communiation;verbal communication;communication incorporating visual;verbal social signals;non verbal;verbal social;visual audio modalities;background non verbal;visual audio contextual;incorporating visual audio;audio contextual;uses utterances;visual audio", "pdf_keywords": ""}, "2d8d51d483a50c6fbf16a0cc120465539f4055da": {"ta_keywords": "tweet multilingual information;tweet multilingual;multilingual information theoretic;ssaid tweet multilingual;microblog text twitter;multilingual information;text twitter;microblog text;post microblog text;text twitter 26;single post microblog;twitter;paper describes multilingual;information theoretic;introduction ssaid tweet;multilingual;post microblog;ssaid tweet;information theoretic approach;multilingual study information;information theoretic perspective;microblog;tweet;twitter 26 different;describes multilingual;multilingual study;fashion information theoretic;information contained single;describes multilingual study;twitter 26", "pdf_keywords": ""}, "27ad78b72c3fb77a117b15855008b65e838314e8": {"ta_keywords": "patients diagnosis malignant;diagnosis malignant disease;diagnosis malignant;malignant disease;patients diagnosis;management patients diagnosis;malignant;diagnosis;patients;approach management patients;management patients;evaluating effectiveness new;evaluating effectiveness;effectiveness new approach;importance evaluating effectiveness;effectiveness new;effectiveness;disease;evaluating;approach management;discuss importance evaluating;new approach management;importance evaluating;approach;purpose article discuss;management;article discuss importance;purpose article;new approach;article discuss", "pdf_keywords": ""}, "51b2dd5cbec02f016c6fa716705ede9b3846a410": {"ta_keywords": "case srr rr;srr rr rr;srr rr;report case srr;case srr;rr rr rr;rr rr;srr;rr;report case;case;report", "pdf_keywords": ""}, "3e86ecbd41ab55b90d3b45601aeb15d2e5c1c8f8": {"ta_keywords": "balanced knockout tournaments;knockout tournaments;tournament methods prove;knockout tournaments common;tournament methods;player tournament methods;particular player tournament;draw player wins;player tournament;tournament;tournaments;optimal draw;tournaments common;finding optimal draw;competitions;optimal draw particular;formats sports competitions;sports competitions;tournaments common formats;balanced knockout;player wins value;competitions used elections;draw particular player;draw player;wins value;sports competitions used;knockout;competitions used;exists draw player;draw", "pdf_keywords": ""}, "7580df14bf01438e7174bbff260508a39a44df84": {"ta_keywords": "distributed storage codes;codes distributed storage;erasure codes distributed;explicit erasure codes;storage codes fast;storage codes;erasure codes;systematic distributed storage;codes distributed;distributed storage;distributed storage following;codes fast encoding;storage following desirable;explicit code construction;storage following;constructing explicit erasure;code construction theoretically;storage;code construction;fast encoding;codes fast;explicit erasure;encoding;explicit code;presents explicit code;fast encoding aims;codes;code;guarantees desired properties;existence constructions", "pdf_keywords": "distributed storage codes;codes distributed storage;codes optimal storage;erasure codes distributed;storage codes increasingly;storage codes;product matrix codes;explicit erasure codes;constructing sparse codes;matrix codes optimal;sparse codes;sparse codes established;codes sparse sparse;storage codes called;regenerating codes distributed;erasure codes;matrix codes;matrix codes presented;sparse codes theorem;distributed storage;distributed storage fundamental;codes sparse;sparse code;code sparse encoding;matrix codes use;sparsity sparse codes;distributed storage systems;matrix codes review;codes distributed;regenerating codes equivalent"}, "92fc770721e95249f8db01c5019d1cc4cf79ff00": {"ta_keywords": "process hubble spacetelescope;hubble spacetelescope assisted;hubble spacetelescope;selection process hubble;spacetelescope assisted;introductionthe proposal selection;spacetelescope;proposal selection;spacetelescope assisted robust;query language capabilities;process hubble;proposal selection process;hubble;query language;query program taccos;command query language;language capabilities macro;query program;use query program;macro procedure definition;easy use query;macro procedure;macro;standard command query;proposal;use query;capabilities macro procedure;capabilities macro;flexibility standard command;language capabilities", "pdf_keywords": ""}, "6c5144872c259611dceb32fe4e4486a6865e6c42": {"ta_keywords": "stationary noise estimation;decomposition robust speech;robust speech recognition;stationary noise decomposed;noise decomposed stationary;noise estimation;robust speech;noise estimation method;stationary noise sequences;noise suppression;non stationary noise;speech recognition methods;speech recognition;noise decomposed;estimation non stationary;noise suppression problem;stationary noise;addresses noise suppression;residual component decomposition;component decomposition robust;bias residual component;noise sequences;suppression problem estimation;decomposition robust;residual signal bias;decomposed stationary non;decomposed stationary;based bias residual;noise sequences problem;recognition methods paper", "pdf_keywords": ""}, "cf6352c789ab51320fa7ca9b1440c685b57fd769": {"ta_keywords": "speaker activity;dihard diarization evaluation;diarization hard experiences;recordings variety difficult;dihard diarization;speaker activity currently;provided microphone recordings;consider speaker activity;inaugural dihard diarization;microphone recordings;microphone recordings variety;microphone;fully consider speaker;consider speaker;recordings;recordings variety;speaker;task provided microphone;provided microphone;diarization hard;introduction diarization hard;diarization evaluation;diarization evaluation conclusion;introduction diarization;inaugural dihard challenge;diarization;dihard challenge;dihard challenge case;inaugural dihard;learned thejh team", "pdf_keywords": ""}, "2e1a1588955a8a64ec618b3cc04be961ed0cb59c": {"ta_keywords": "training graph neural;pre training graph;conjugated oligomers polymers;oligomers methods training;transfer learning;training graph;oligomers polymers;graph neural networks;use transfer learning;graph neural;short oligomers methods;oligomers polymers remains;chemistry modeling optoelectronic;data short oligomers;learning techniques chemistry;long conjugated oligomers;oligomers methods;conjugated oligomers;transfer learning address;polymers;chemistry modeling;neural networks;polymers remains challenging;networks;networks using data;learning;training data able;training data;optoelectronic properties long;polymers remains", "pdf_keywords": ""}, "9237d6efc603465765e80eb5ca1268c2bd7b5c24": {"ta_keywords": "language generation tasks;systems language generation;machine translation;tasks machine translation;language generation;machine translation main;generation tasks;generation tasks machine;translation main goal;tool holistic analysis;results systems language;holistic analysis comparison;systems language;mt tool holistic;tool holistic;salient differences systems;comparison results systems;translation;compare mt tool;translation main;view salient differences;holistic analysis;generation;differences systems;differences systems used;salient differences;language;analysis comparison;main goal tool;comparison results", "pdf_keywords": "comparing machine translation;machine translation application;machine translation implements;machine translation systems;techniques machine translation;machine translation language;translation systems using;analyze translation accuracy;machine translation;results machine translation;translation accuracy systems;machine translation important;translation systems;translation important tool;tasks machine translation;machine translation article;translation language generation;automatic error analysis;differences translation accuracy;literature machine translation;language generation systems;analyze translation;translation application;translation accuracy;translation accuracy fundamental;comparison results systems;language generation tasks;translation systems article;systems language generation;compare mt tool"}, "a6336fa1bcdeb7c84d2c4189728f0c1b2b7d0883": {"ta_keywords": "neural networks recurrent;networks recurrent;recurrent neural networks;networks recurrent networks;recurrent networks;networks sequence learning;recurrent neural;recurrent networks retain;neural networks sequence;sequence learning;sequence learning presented;review recurrent neural;methods recurrent neural;results recurrent neural;neural networks connectionist;methods recurrent;sequences cycles network;networks sequence;presented methods recurrent;recurrent;neural networks traditionally;dynamics sequences;dynamics sequences cycles;results recurrent;sequences;neural networks;review recurrent;sequences cycles;neural;capture dynamics sequences", "pdf_keywords": "neural networks sequence;dependencies recurrent neural;recurrent neural networks;learning sequences;recurrent neural network;range dependencies recurrent;sequence agnostic models;rns sequence learning;neural networks rich;networks learning sequences;sequence learning;dependencies recurrent;short term memory;research recurrent neural;recurrent neural;neural networks increasingly;networks sequence learning;sequence prediction;use recurrent neural;sequence learning provide;learning sequences emphasize;neural networks rns;simple recurrent neural;new neural networks;sequence learning evolution;novel neural networks;sequence learning present;neural networks feasible;networks rns;network research recurrent"}, "7b7416c90e8d3fc9ad5c9fb3923a638f69294ed7": {"ta_keywords": "domain question answering;question answering;retrieving assimilating factual;assimilating factual information;language understanding tasks;text corpus transformer;large text corpus;question answering require;corpus transformer model;source factual knowledge;corpus transformer;assimilating factual;factual knowledge methodsthe;text corpus;entity mention input;representation large text;factual information multiple;entity mention;corpus;language understanding;memory layers entity;factual knowledge;introductionnatural language understanding;model source factual;semi parametric representation;layers entity mention;information multiple sources;model tome transformer;factual information;answering require retrieving", "pdf_keywords": "mention memory corpus;corpus virtual knowledge;generate mention memory;large text corpus;integrating information corpus;memory corpus;entity mention corpuswe;extractive question answering;information corpus;sources corpus;domain question answering;information corpus new;memory corpus use;question answering dataset;knowledge mention memory;scalable efficient retrieval;efficient retrieval;learning useful information;retrieving assimilating factual;employ text corpus;memory corpus model;question answering;information corpus language;assimilating factual information;multiple sources corpus;using text corpus;efficient retrieval method;batch passages relatedwikipedia;optimizing retrieval useful;text corpus"}, "8be39979cb2eb1aeaba15b57e1e4bc712eb962cb": {"ta_keywords": "paragraph embeddings remember;classification paragraph embedding;embeddings remember senstence;embeddings remember;paragraph embedding models;paragraph embeddings;paragraph embedding;improves classification paragraph;introductionencouraging paragraph embeddings;paragraph embedding method;classification paragraph;classification tasks learn;embedding models remarkably;art paragraph embedding;embeddings;embedding models;classification tasks;embedding;embedding method proposed;tasks learn encode;identity improves classification;downstream classification tasks;paragraph;state art paragraph;improves classification;embedding method;introductionencouraging paragraph;remember senstence identity;alzheimer;classification", "pdf_keywords": "paragraph embedding models;training sentence content;paragraph embedding model;sentence content task;paragraph embedding;sentence content objective;paragraph embedding method;sentence content;introduction paragraph embedding;objective sentence content;word content task;novel sentence content;classification tasks learn;semi supervised;formulate sentence content;processing word representations;paragraph level analogue;content task paragraph;downstream classification tasks;classification tasks;learning processing text;based sentence content;semi supervised learning;task paragraph level;art paragraph embedding;paragraph level;state art paragraph;word representations;content objective sentence;word content"}, "f9ee690d223beac6d893aedae13c09dbf0fb694e": {"ta_keywords": "semantic verb clustering;verb clustering thoroughly;models verb clustering;verb clustering;verb clustering methodsin;backgroundunsupervised constrained dirichlet;mixture models verb;dirichlet process mixture;constrained dirichlet process;processing nl lexical;lexical semantic verb;natural language processing;process mixture models;language processing nl;nl lexical semantic;mixture models;constrained dirichlet;backgroundunsupervised constrained;semantic verb;dirichlet process;dmms particular clustering;mixture models dmms;lexical semantic;apply dirichlet process;language processing;clustering methodsin;process mixture;nl lexical;clustering thoroughly;task natural language", "pdf_keywords": ""}, "7354b87a1b4c99ccd9cf25b7314927ced8b156f7": {"ta_keywords": "interactive writing assistant;writing assistant generates;writing assistants;controllable writing assistants;improved writing assistance;writing assistant;interactive writing;writing assistants explore;writing assistance;build interactive writing;assistant generates rephrases;significantly improved writing;writing assistance functionalities;improved writing;pretrained language models;assistance functionalities autocomplete;grained author specifications;advances language modeling;language modeling;language modeling build;intent guyd assistant;controllable writing;language models;generates rephrases text;assistant generates;language models significantly;pretrained language;writing;input intent guyd;autocomplete", "pdf_keywords": "guided authoring assistant;interactive writing assistant;writing assistant generates;improved writing assistance;authoring assistant;human author assistance;ai assisted writing;intent guided authoring;controllable writing assistants;writing assistance;assisted writing;assisted writing implement;authoring assistant ig;writing assistants;text generation models;writing assistant;approach writing assistance;assisted writing prefer;sentences ai assisted;interactive writing;guided authoring;writing intent guided;build interactive writing;writing assistants explored;writing assistance functionalities;human ai authoring;participants write help;grained author specifications;users write help;human user assistance"}, "0f395654e69cd2e063a6ef221fb66fb46e68cefd": {"ta_keywords": "introductionclassification strategicallyheld data;strategicallyheld data useful;introductionclassification strategicallyheld;strategicallyheld data;classifier create incentive;classifier;chosen classifier;classifier specific;classified favorableably;strategically hold features;chosen classifier specific;depends chosen classifier;admission classified favorableably;specific classifier;scores missing data;classifier specific classifier;classified favorableably contexts;incentive halt certain;specific classifier create;classifier create;strategicallyheld;incentive halt;certain feature;admission classified;feature;features;test scores missing;create incentive halt;introductionclassification;college admission classified", "pdf_keywords": "classifiers robust strategic;strategic machine learning;features agents strategy;classifiers eliminate incentive;classifiers directed;learn classifiers;optimal classifiers directed;datathe problem strategic;classifier significantly;strategically hold features;classifier create incentive;ability learn classifiers;classifiers;optimal classifiers;classification best;good classifier;optimal classifier;designing classifiers;hold features prediction;problem strategic machine;classifier important;holding features agents;classifier significantly efficient;classifiers robust;features agents;best features classifiers;significantly improved classifier;ability learn classification;strategic machine;machine learning"}, "60dd53fca1f538fabe18e4d6a9326b2f40e358dd": {"ta_keywords": "statistical topic models;topic models;topic models latent;models latent dirichlet;latent dirichlet;latent dirichlet al;summarize large document;large document collec;statistical topic;document collec;model visualize summarize;background statistical topic;summarize large;visualize summarize large;parallel implementations variational;document collec tions;electronic document collec;large document;models fast scalable;implementations variational;visualize summarize;important models fast;dirichlet;variational;models latent;summarize;models fast;dirichlet al location;topic;dirichlet al", "pdf_keywords": ""}, "084ddb77fce5a7f0b6418ef4e38dbb1bedf4ae78": {"ta_keywords": "produce electrolaryngeal speech;electrolaryngeal speech;speech enhancement;speech enhancement methods;electrolaryngeal speech proficient;proposed speech enhancement;statistical voice conversion;voice conversion;speech proficient laryngectomees;sounds produce electrolaryngeal;electrolarynx device artificially;using statistical voice;laryngectomees produce intelligible;electrolarynx device;introduction electrolarynx device;statistical voice;proficient laryngectomees produce;electrolarynx;introduction electrolarynx;proficient laryngectomees;voice conversion showed;produce intelligible speech;speech using device;laryngectomees;laryngectomees produce;produce electrolaryngeal;intelligible speech using;generates excitation sounds;intelligible speech;voice", "pdf_keywords": ""}, "5b2c5eeea9ac8f26908b9dfc8fd0a2d0e7aa5bb1": {"ta_keywords": "multichannel robust speech;adaptive beamforming networks;memory adaptive beamforming;robust speech recognition;robust speech;adaptive beamforming;beamforming networks multichannel;beamforming networks;speech recognition;speech recognition discussiondeep;speech recognition case;networks multichannel robust;beamforming;multichannel robust;short term memory;presentationa deep long;term memory adaptive;memory adaptive;presentationa deep;networks multichannel;multichannel;speech;recognition discussiondeep long;long short term;recognition case presentationa;term memory;adaptive;case presentationa deep;robust;deep long short", "pdf_keywords": "lstm adaptive;lstm adaptive beamformer;deep lms acoustic;adaptive speech;adaptive speech recognition;speech recognition reverberant;lstm;network lstm adaptive;field speech recognition;trained deep acoustic;maximizing adaptive speech;deep acoustic model;short term memory;multichannel speech recognition;adaptive beamformer deep;speech recognition making;speech signals beamformer;acoustic speech recognition;achieve robust speech;acquiring speech signal;network lstm;training lms adaptivewe;deep lms;speech recognition noisy;beamformer deep acoustic;robust speech;recurrent neural network;multichannel speech;lms acoustic;speech recognition"}, "d38b686b8b68d0b91b294fd8a55ac7dea191706f": {"ta_keywords": "abstractive summarization models;introductionnural abstractive summarization;abstractive summarization;guided summarization framework;extensible guided summarization;summarization models;summarization models flexible;summarization framework gum;guided summarization;summarization framework;produce coherent summaries;summarization;coherent summaries;summaries;coherent summaries unfaithful;summaries unfaithful;summaries unfaithful difficult;introductionnural abstractive;abstractive;different types guidance;guided;general extensible guided;types guidance;introductionnural;strategies compare contrast;paper propose general;contrast methodsin;methodsin paper propose;guidance;contrast methodsin paper", "pdf_keywords": "guided neural summarization;neural summarization models;neural summarization;summarization using neural;neural abstractive summarization;meeting neural summarization;extensible guided summarization;summarization models generate;guided summarization;summarization benchmarks best;guided summarization framework;neural summarization annual;summarization benchmarks;popular summarization benchmarks;abstractive summarization models;summarization models;model abstractive summarization;sentence guided models;abstractive summarization model;popular summarization datasets;summarization datasets;summarization model bertabs;model generate summaries;summarization model;sentence guided model;generate summaries sentence;summarization models use;ability generate summaries;summarization framework gum;abstractive summarization results"}, "e114618157e025ed17b7e45684d67becd34a14f3": {"ta_keywords": "dimensional gassian distributions;distribution learning analytically;distribution learning;gassian distributions studied;complexity distribution learning;gassian distributions;lower bounds mixtures;bounds mixtures methodsexploiting;total variation distance;statistics learning theory;distributions studied extensively;variation distance characteristic;variation distance;tight functional approximations;bounds mixtures;mixtures methodsexploiting;function mixture provide;extensively statistics learning;high dimensional gassian;distributions;function mixture;statistics learning;distributions studied;characteristic function mixture;theory total variation;variation distance appears;mixtures methodsexploiting connection;functional approximations;total variation;sample complexity distribution", "pdf_keywords": "distance dimensional mixtures;dimensional gassian distributions;distance onedimensional mixtures;dimension mixtures distance;dimension mixtures significantly;dimensional gassian mixtures;lower bounds mixtures;dimensional mixtures;onedimensional mixtures fundamental;variation distance projecting;variation distance gassian;statistics learning theory;mixtures lower bound;onedimensional mixtures;mixtures distance gassian;dimension mixtures;distance gassian mixtures;distribution learning;total variation distance;complexity distribution learning;distribution learning analytically;dimension mixtures present;bounds mixtures;distance mixtures;gassian mixtures distance;high dimension mixtures;dimensional mixtures total;distance mixtures lower;mixtures distance;projection depends mixtures"}, "3ed91aae1038b8b0130fb3974060a50b10de1345": {"ta_keywords": "speech recognition ar;field automatic speech;machine recognition speech;automatic speech recognition;recognition speech spoken;recognition speech;speech recognition;recognition ar;automatic speech;recognition ar received;speech spoken distance;spoken language interface;home assistants spoken;spoken distance microphones;far field automatic;digital home assistants;assistants spoken language;text speech;spoken distance;text text speech;distance microphones known;introductionthe machine recognition;assistants spoken;machine recognition;microphones known far;spoken language;distance microphones;recognition accuracy;speech spoken;recognition", "pdf_keywords": "farfield speech recognition;field speech recognition;speech recognition ar;field speech processing;speech recognition acoustic;challenges farfield speech;speech recognition increasingly;channel speech recognition;speech recognition challenges;field automatic speech;speech recognizers;speech recognition accuracy;speech recognition;speech recognition speech;far field speech;recognition speech;speech recognition everyday;recognition speech recognition;speech recognizers increasingly;speech recognition fundamental;recognition speech spoken;reconstruct speech recognition;approach speech recognition;speech recognition application;perform speech recognition;speech audio;beamforming speech recognition;machine recognition speech;speech recognition designed;automated speech recognition"}, "addd2d86d19c1e7c8854e827fb2656a50c250440": {"ta_keywords": "generating focused summaries;text summaries;backgroundaspect based summarization;analysis text summaries;focused summaries based;text summaries aid;based summarization;summaries based specific;summarization task generating;summaries based;summaries aid efficient;summarization;based summarization task;focused summaries;summarization task;summaries aid;summaries;specific points summaries;points summaries aid;domains sentiment product;sentiment product features;points summaries;sentiment product;analysis text large;efficient analysis text;analysis text;domains sentiment;different domains sentiment;aspects different;aspects different domains", "pdf_keywords": "summarization task generating;aspect summarization dataset;generating focused summaries;summarization dataset derived;summarization various aspects;summarization model;summarization dataset;score summarization model;aspect discovery summarization;generated summaries;extractive summaries software;results generated summaries;summarize words summarize;summarization various;summarization model used;summarization task effective;summaries extractive summaries;generated summaries present;score summarization;summarize words extractive;subject based summarization;summaries aid efficient;summarization task model;multi aspect summarization;summaries based specific;summarize words;aspect summarization;words extractive summaries;focused summaries based;challenges regarding summarization"}, "0c3c4c88c7b07596221ac640c7b7102686e3eae3": {"ta_keywords": "biomedical question answering;pubmedq answer research;pubmedqa answer research;pubmed abstracts task;abstracts task pubmedq;pubmed abstracts;task pubmedq answer;pubmedq answer;abstracts task pubmedqa;question answering dataset;collected pubmed abstracts;pubmedq;task pubmedq;pubmedqa novel biomedical;pubmedqa answer;dataset collected pubmed;question answering;answer research;answer research questions;task pubmedqa answer;introduce pubmedqa;pubmedqa;introduction introduce pubmedqa;pubmed;answering dataset collected;answering dataset;task pubmedqa;novel biomedical question;biomedical question;collected pubmed", "pdf_keywords": "biomedical question answering;question answering pubmed;pubmedq biomedical qa;pubmed abstracts task;pubmedqa answer research;answering pubmed;answering pubmed database;pubmedq biomedical;biomedical qa dataset;question answering;question answering qrd;pubmedq novel dataset;research question answering;present pubmedq biomedical;qa dataset answering;dataset answering research;abstracts methods pubmedq;question answering identify;answering research;literature question answering;biomedical qa;question answering using;pubmedq 1k expert;pubmed abstracts;biomedical named entities;abstracts task pubmedqa;answering identify questions;methods pubmedq 1k;questions masking biomedical;collected pubmed abstracts"}, "b7731a9b9142a6deb132e99bc55ddbe458a537a6": {"ta_keywords": "online estimateion causal;estimateion causal;estimateion causal effects;probabilistic model causal;model causal;causal;data fusion;causal effects dedeciding;causal effects;data fusion problems;causal effect;model causal effect;online estimateion;backgroundefefficient online estimateion;observe methods researchers;multiple data sources;probabilistic model;data sources;estimate functional probabilistic;probabilistic;formulations typically data;data sources available;functional probabilistic model;researchers face data;face data fusion;data acquisition ongoing;dedeciding observe methods;data acquisition;observe methods;multiple data", "pdf_keywords": "causal inference data;inference data fusion;probabilistic model causal;models data learning;causal graphical models;methods causal inference;causal inference using;causal inference;method causal inference;data adaptive policies;data fusion econometrics;gm model learning;data fusion;causal graphs regret;inference data;data learning;learning optimal policy;model causal;settings causal inference;causal effect efficiently;causal graphs;synthetic dataset estimate;model learning;graphical models data;adaptively collected data;model parameters leveraging;data fusion important;new approach causal;data adaptive;model learning optimal"}, "3ea5468e6d3007a94d4318932d7778693526145c": {"ta_keywords": "geographically distributed computing;distributed computing resources;utilizing geographically distributed;distributed computing;grid computing;sites geographically distributed;geographically distributed;area grid computing;geographically distributed transfer;integrates geographically distributed;cluster computer systems;parallel computing;computing resources communication;grid computing integrates;cluster computer;parallel computing using;resources communication networks;computing resources shared;effectively utilizing geographically;computing integrates geographically;grid computing resolve;years grid computing;sites geographically;utilizing geographically;distributed;spotlight parallel computing;conventional cluster computer;wide area grid;computing resources;distributed transfer", "pdf_keywords": ""}, "97906df07855b029b7aae7c2a1c6c5e8df1d531c": {"ta_keywords": "non linguistic pipeline;linguistic pipeline pre;linguistic pipeline;linguistic pipeline interpretable;trained text encoders;bovine text encoders;linguistic information captured;non linguistic;non linguistic tasks;pre trained text;trained text;model bovine text;linguistic information;linguistic;text encoders;text encoders rapidly;linguistic tasks;pipeline pre trained;quantify linguistic information;text encoders aim;classicalal non linguistic;bovine text;quantify linguistic;pipeline interpretable localizable;pipeline interpretable;linguistic tasks focus;aim quantify linguistic;encoders aim quantify;traditional non linguistic;art non linguistic", "pdf_keywords": "deep language model;organized deep language;deep language;deep domain;deep domain domain;layers model linguistic;trained text encoders;natural language models;natural language tasks;information organized deep;use deep domain;linguistic pipeline interpretable;linguistic pipeline;linguistic information captured;text encoders rapidly;english corpus explore;language models;corpus explore;tagging use deep;model linguistic;traditional linguistic pipeline;word english corpus;corpus explore information;text encoders;language tasks;trained text;language models important;english corpus;linguistic domains;language model"}, "e31a3f52890dcb68f596020e45f8c9718b700466": {"ta_keywords": "logical algorithm method;algorithm logical algorithm;logical algorithm logical;logical algorithm;based logical algorithm;algorithm logical;logical algorithm new;method based logical;algorithm method based;based logical;algorithm method;algorithm;algorithm new approach;data method based;algorithm new;logical;data method;analysis data method;method based;new approach analysis;approach analysis data;approach analysis;analysis data;method;data;new approach;approach;analysis;based;new", "pdf_keywords": ""}, "77c98b45c95121fc2a3d2ab4906fc00364cf381c": {"ta_keywords": "speech separation recognition;speech separation;separation recognition skills;recognize speech important;recognize speech;recognition skills speech;recognize recognize speech;speech important development;speech recognition;speech group fundamental;development speech recognition;skills speech group;speech group;speech recognition recognition;speech important;separation recognition;fundamental development speech;recognition skills;skills speech;speech addition ability;important development speech;development speech addition;ability recognize recognize;speech;development speech;ability recognize;speech addition;recognition;separation;recognition addition ability", "pdf_keywords": ""}, "11a28f9e6fb6581d0a01428dd27a3fb649454395": {"ta_keywords": "differences chymotrypsin trypsin;chymotrypsin trypsin hydrooxylamine;trypsin dep chymotrypsin;following differences chymotrypsin;chymotrypsin trypsin;differences chymotrypsin;chymotrypsin ethylamine cyclopropylamine;chymotrypsin ethylamine;trypsin hydrooxylamine completely;dep chymotrypsin ethylamine;trypsin hydrooxylamine;dep chymotrypsin;chymotrypsin;effect chymotryptic hydrolysis;chymotryptic hydrolysis;acetylethanolamine effect chymotryptic;tryptic hydrolysis acetylethanolamine;dep trypsin;trypsin dep;cyclopropylamine accelerate tryptic;trypsin;reactivate dep trypsin;effect chymotryptic;chymotryptic;dep trypsin dep;hydrolysis acetylethanolamine effect;ethylamine cyclopropylamine;cyclopropylamine;hydrolysis acetylethanolamine;tryptic hydrolysis", "pdf_keywords": ""}, "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4": {"ta_keywords": "predicting caption;task predicting caption;predicting caption goes;learn image representations;learn image;million image text;way learn image;text images;text images promising;visual concept learning;learning directly raw;image text;raw text images;image text pairs;training task predicting;image representations;learning directly;caption;concept learning directly;labeled data;text;clinical messageadditional labeled;raw text;learning;specify visual concept;labeled data needed;representations scratch dataset;training task;concept learning;image representations scratch", "pdf_keywords": "predicting caption;knowledge shot learning;trained automatic captions;zero shot imagenet;shot imagenet classification;task predicting caption;shot imagenet;predicting caption goes;shot transfer learning;supervised imagenet;shot transfer imagenet;trained natural language;supervised imagenet models;shot learning;accuracy supervised imagenet;training natural language;predicting image representations;learn visual models;zero shot recognition;imagenet models significantly;supervision zero shot;learning transferable visual;imagenet classification previously;shot learning report;zero shot classifiers;shot transfer learn;trained models multimodal;captions 100 million;reference learned visual;learn visual representations"}, "de0e1f9980afa7949df64d53b8ae7a2f59c55579": {"ta_keywords": "style transfer task;style transfer using;style transfer;available style transfer;style transfer india;large stylelabelled corpora;stylelabelled corpora;sentence target style;extracting target style;shot style transfer;target style methods;background style transfer;stylelabelled corpora recent;access large stylelabelled;stylelabelled;sentences inference extracting;large stylelabelled;datasets available style;style methods;transfer india languages;inference extracting;inference extracting target;sentence target;shot style;sentences inference;corpora;input sentence target;target style;style methods work;corpora recent work", "pdf_keywords": ""}, "1d79d055cf9711944f14e1388a9d054cbe81ddd0": {"ta_keywords": "language modeling turkish;discriminative language modeling;semi supervised discriminative;supervised discriminative language;modeling turkish;discriminative language;introduction semi supervised;modeling turkish anrra;semi supervised;turkish anrra ra;language modeling;supervised discriminative;turkish anrra;turkish;discriminative;supervised;anrra ra ra;language;introduction semi;anrra ra;ra ra;ra ra ra;ra;semi;anrra;modeling;introduction", "pdf_keywords": ""}, "8cfd299be05bf3df91e0bf656a7e2fb973056350": {"ta_keywords": "cross lingual algorithms;cross lingual transfer;languages cross lingual;lingual transfer;cross lingual;current cross lingual;lingual algorithms;languages scaling speech;speech systems support;resource languages cross;scaling speech systems;lingual algorithms shown;low resource languages;speech processing systems;lingual transfer offers;speech related tasks;speech systems;scaling speech;speech processing;based tasks speech;lingual;languages cross;tasks speech related;resource languages scaling;languages lack data;resource languages;tasks speech;languages scaling;introduction speech processing;text based tasks", "pdf_keywords": "transfer language similarity;language similarity speech;similarity speech data;cross lingual algorithms;language similarity measures;measure language similarity;transfer languages speech;lingual transfer languages;language similarity useful;speech based similarity;acoustic cross lingual;lingual transfer language;language similarity approaches;demonstrate language similarity;lingual algorithms;languages scaling speech;language similarity;multilingual speech recognition;languages cross lingual;similarity speech;speech text data;language similarity demonstrate;lingual algorithms shown;different language similarity;speech data;data 700 languages;tools universal speech;cross lingual text;lingual transfer;transfer languages"}, "3ba26e897d0085ecd8cb695e1728a083f9227447": {"ta_keywords": "bayesian approach speech;recognition development bayesian;speech recognition;approach speech recognition;speech recognition development;speech recognition important;bayesian approach;bayesian;tool speech recognition;development bayesian approach;development bayesian;approach speech;tool speech;recognition development;speech;recognition;important tool speech;recognition important tool;recognition important;approach;tool;important tool;development;important", "pdf_keywords": ""}, "8f6763b339363216794f48895b9381d1a7caa88c": {"ta_keywords": "learning dynamics;agent learning quadratic;disturbance decoupling gradient;multi agent learning;learning dynamics respect;overall learning dynamics;agent learning;learning dynamics subset;based learning dynamics;players disturbance decoup;decoupling gradient based;decoupling gradient;learning quadratic costs;learning quadratic;robustness gradient;present robustness gradient;dynamics subset players;gradient based learning;robustness gradient based;disturbance decoupling;gradient based multi;disturbance decoup;based multi agent;gradient based;multi agent;players disturbance;learning;subset players disturbance;based learning;affect overall learning", "pdf_keywords": "learning dynamics quadratic;learning bimatrix games;learning multi agent;bimatrix games dynamics;dynamics quadratic game;matrices learning dynamics;players payoff matrices;learning dynamics;agent policy gradient;multi agent learning;subspace individual players;quadratic game game;games dynamics;learning dynamics equivalent;agent learning;quadratic game;agent learning settings;subspaces players;constraints players;analyzing learning dynamics;analysis continuous games;bimatrix games;game graph fundamental;equivalent dynamics game;game equivalent dynamics;policy gradient algorithms;dynamics game equivalent;games dynamics game;necessary constraints players;constraints players payoff"}, "51e5e7093e0183feab61b00ca6c3df61cd8c46de": {"ta_keywords": "discriminative language modeling;lists discriminative language;best lists discriminative;methods discriminative language;discriminative language;gram language models;language modeling best;language models;language modeling;language models using;training gram language;lists discriminative;semi supervised methods;semi supervised;supervised methods discriminative;language modeling aim;investigates semi supervised;hallucinated best lists;modeling best lists;best lists hallucinated;methods discriminative;english ct comparing;baseline english ct;gram language;english ct;discriminative;lists hallucinated;training gram;models using perceptron;baseline english", "pdf_keywords": ""}, "9d03a125a9568af8af3fae5091752017d6abe59e": {"ta_keywords": "learning techniques succeed;learning;explore learning techniques;learning techniques;explore learning;answers existing problems;addressing problems previously;addressing problems;existing problems;relaxing restrictive assumptions;begin addressing problems;assumptions allowed fail;existing problems able;auxiliary data known;auxiliary data;assumptions bring;restrictive assumptions bring;assumptions bring resources;techniques succeed situations;situations common assumptions;techniques succeed;assumptions;methods;restrictive assumptions;common assumptions;data known;problems previously unanswerable;unlabeled auxiliary data;better answers existing;methods dropping", "pdf_keywords": ""}, "7cbb56da008163df09d254f85b7165f11389f298": {"ta_keywords": "natural language argument;argument reasoning comprehension;argument reasoning;makes comprehension arguments;comprehension arguments easy;arguments easy humans;comprehension arguments;reasoning comprehension methods;arguments easy;task argument reasoning;reasoning comprehension;language argument composed;language argument;arguments;explaining reasoning;argument composed;argument composed claim;explaining reasoning usually;comprehension methods;introduction natural language;argument;natural language;comprehension methods given;reasoning;task argument;given premises claim;premise claim topic;reasons given premises;comprehension;context common sense", "pdf_keywords": ""}, "3f16887a8e5c81aea554c7a266b08ea70dd2aa9a": {"ta_keywords": "preferences cooperative competitive;induce cooperative behaviour;cooperative behaviour assign;cooperative behaviour;regarding preferences cooperative;preferences cooperative;induce cooperative;cooperative competitive environments;cooperative competitive;approach induce cooperative;efficiently alleviated cooperative;cooperative setting;alleviated cooperative setting;cooperative;alleviated cooperative;rewards based agents;multi agent credit;agent credit assignment;cooperative setting state;rewards based;assign additional rewards;additional rewards based;multi agent;agent credit;rewards;additional rewards;credit assignment;issue multi agent;competitive environments;rational regarding preferences", "pdf_keywords": "learning selfish agents;selfish reward;selfish reward standard;define selfish reward;selfish social rewards;selfish incentives cooperativecompetitive;learning selfish;incentives cooperativecompetitive environments;selfish reward standardthe;quantitative learning selfish;incentives cooperativecompetitive;selfish social incentives;agents mixture selfish;social selfish incentives;selfish agents able;cooperativecompetitive environments;preferences cooperative competitive;balance selfish combined;cooperativecompetitive;cooperativecompetitive environments demonstrate;learning agents cooperative;selfish incentives;selfish agents;social preferences cooperative;combining social selfish;preferences cooperative;balance selfish;reward agent;regarding preferences cooperative;social rewards"}, "a6aed0c4e0f39a55edb407f492e41f178a62907f": {"ta_keywords": "paperrobot automatic research;paperrobot;paperrobot automatic;human written papers;automatic research assistant;attention incrementally writing;attention contextual text;automatic research;background knowledge graphs;performs automatic research;contextual text attention;new paper based;written papers target;written papers;paper based memory;paper based;attention contextual;papers;memory attention networks;memory attention;text attention;papers target;attention incrementally;text attention incrementally;background knowledge;graph attention;paper;graph attention contextual;attention;based memory attention", "pdf_keywords": "attention network;memory attention network;attention network architecture;background knowledge graphs;paperrobot predict related;predict related entities;text related entities;predicted related entities;generate background knowledge;knowledge graph;develop paperrobot predict;memory attention;entities present paperrobot;knowledge graph significantly;attention;enriched knowledge graph;knowledge graph enriched;text related entity;extract relationships abstracts;graph enriched knowledge;paper new ideas;generating abstracts;paperrobot predict;text related;generating abstracts human;relationships abstracts;novel memory attention;paperrobot novel writing;effective generating abstracts;knowledge graphs"}, "5695847f8ffbb3da078842c3683ef74175eb59e5": {"ta_keywords": "japanese speech synthsynthesis;speaker individuality synthetic;lingual speech synthesis;speech synthesis;speech synthsynthesis preserving;based speech synthesis;synthsynthesis preserving speaker;synthetic speech;individuality synthetic speech;speech synthesis generating;speech synthsynthesis;voice conversion andhmm;based voice conversion;speech synthesis tends;preserving speaker individuality;voice conversion;read japanese speech;degradation speaker individuality;japanese speech;ja speakers based;naturally sounding english;speaker individuality based;correction prosody phonetic;preserving speaker;sounding english speech;speakers based voice;speaker individuality;uttered ja speakers;prosody phonetic;prosody phonetic sounds", "pdf_keywords": ""}, "37a0f28f6aa41028e64d0440001ff525d67c1305": {"ta_keywords": "fair efefficient allocation;constrained round robin;agent resource allocation;algorithm fair efefficient;assignment papers reviewers;efefficient allocation case;efefficient allocation;round robin algorithm;robin algorithm fair;achieving allocation;algorithm fair;allocation case;allocation setting models;allocation;issue allocation;allocation setting;allocation problems;recurring issue allocation;achieving allocation embed;resource allocation;issue allocation problems;papers reviewers;assignment papers;multi agent resource;efficiency fairness given;resource allocation setting;allocation case reportwe;round robin;consider multi agent;allocation embed", "pdf_keywords": "fair allocation indivisible;finding fair allocations;fair allocations;allocation fair allocation;allocation fair;fair allocation;fair allocations new;optimal allocation indivisible;allocation indivisible items;preferences agents fundamental;arbitrary allocations;fair allocation wethe;allocation optimal agents;arbitrary allocations fundamental;allocation way provably;preferences agents;allocation indivisible;article fair allocation;allocation indivisible goods;efficient finding fair;assign indivisible goods;ordinal preferences approach;analyze preferences agents;efficient allocation;allocation agent hard;optimal agents assigned;allocation agent;indivisible items optimal;optimal allocation;efficient allocation way"}, "7bdb04ba2da682e4c0b19b5d61e999d648826edd": {"ta_keywords": "recovering sparse covariance;sparse covariance matrix;sparse covariance;covariance matrix rnn;rnn quadratic measurements;matrix rnn quadratic;recovering sparse;covariance matrix;matrix rnn;problem recovering sparse;rnn quadratic;quadratic measurements;consider simplified noise;additive noise;quadratic measurements yi;simplified noise;additive noise assume;sparse;ln measurement vector;covariance;wi additive noise;measurement vector wi;measurement vector;matrix;ai ln measurement;vector wi additive;noise assume non;zero diagonal entries;rnn;noise assume", "pdf_keywords": ""}, "d8c1eb86cc4546e4355ed368d8400d7640926cee": {"ta_keywords": "clustering method tvclust;model based clustering;clustering;based clustering method;based clustering;clustering method;consensus information;method tvclust robustly;tvclust robustly incorporates;tvclust robustly;incorporates noisy information;noisy information soft;consensus information observed;noisy information;seek consensus information;method tvclust;information soft constraints;tvclust;robustly incorporates noisy;information observed data;observed data;information soft;observed data methodswe;data methodswe propose;seek consensus;soft constraints;robustly;propose model based;information observed;consensus", "pdf_keywords": "views underlying clustering;bayesian methods clustering;underlying clustering;latent clustering structure;clustering information stochastic;clustering information;latent clustering;information clustering;underlying clustering structure;information cluster;clustering information clustering;clustering structure;independent information cluster;information clustering information;method clustering information;clustering model;probabilities clustering information;clustering information algorithms;clustering using relational;effective clustering;clustering;given latent clustering;clustering model method;nonparametric bayesian modelwe;effective clustering tasks;clustering information use;clustering tasks;based nonparametric bayesian;nonparametric bayesian;clustering loss information"}, "fe0ec764813fbcb6b6fd77d82188e81826088103": {"ta_keywords": "discriminative objective functions;discriminative objective;discriminative training applied;discriminative training;suitable discriminative training;functions suitable discriminative;speech recognition;view discriminative objective;sequential pattern recognition;discriminative;suitable discriminative;automatic speech recognition;unified view discriminative;strings analysis generalizes;view discriminative;automatic speech;pattern recognition;recognition problems automatic;speech recognition focusing;strings analysis;objective functions weighting;recognition;measure strings methodsthis;recognition problems;pattern recognition problems;observations strings analysis;problems automatic speech;probabilities observations strings;objective functions sum;objective functions based", "pdf_keywords": ""}, "659b476a10b0e676a031b1b17ebfe405c1904227": {"ta_keywords": "speech processing toolkit;end speech recognition;end speech processing;text speech;text speech tacs;speech recognition;speech processing applications;speech processing;speech recognition experiments;range speech processing;includes text speech;speech tacs;wide range speech;speech tacs tacs;end end speech;end speech;sequence sequence modeling;sequence modeling project;sequence modeling;range speech;speech;processing toolkit;espnet includes text;processing applications espnet;processing toolkit project;toolkit;based sequence sequence;text;toolkit project;sequence sequence", "pdf_keywords": "speech processing toolkit;speech recognition source;tool speech processing;speech processing new;speech processing applications;speech processing;end speech recognition;speech processing tasks;speech recognition field;speech recognition;speech recognition synthesis;speech recognition decade;approaches speech recognition;toend speech processing;range speech processing;downstream speech processing;approach speech recognition;tool speech recognition;speech recognition article;recognition field speech;speech applications incorporating;speech systems unified;speech systems;field speech recognition;separation speech processing;speech translation tasks;speech conversion;speech applications;synthesis text speech;diseases espnet web"}, "f02948f2976991bb76419775f303c27fc8afb7b5": {"ta_keywords": "text classifiers;learning text classifiers;rules classify mailed;classify mailed;text classifiers compared;classify mailed discussed;learning rules classify;mail messages traxiitionm;classifiers;learning text;learning rules;personm mail messages;word spotting rules;rules classify;mail messages;methods learning text;classifiers compared;classification;classify;classifiers compared classification;personm mail;classification problems;compared classification;messages traxiitionm;classification problems arise;compared classification problems;key word spotting;word spotting;introduction learning rules;filing personm mail", "pdf_keywords": ""}, "62a5b47def8d21825d06f7407a505ff0b64ecb1a": {"ta_keywords": "semantic parsing ambiguous;semantic parsing;parsing ambiguous ungrammatical;existing semantic parsing;semantic parsing framework;parsing ambiguous;parsing;parsing framework;method semantic parsing;ungrammatical input search;input search queries;context free grammars;grammar takes ambiguous;parsing framework uses;grammars cfr jointly;free grammars cfr;ambiguous ungrammatical input;grammars cfr;free grammars;takes ambiguous input;input sentence output;building existing semantic;existing semantic;model input sentence;input search;semantic;search queries;ambiguous input;ambiguous input string;grammar", "pdf_keywords": ""}, "05b0c768ecd4a82e486923e83250ddd53bacbf67": {"ta_keywords": "metric nn search;non metric search;pruning algnostic algorithms;metric search tree;metric search;introduction pruning algnostic;pruning algnostic;algorithms low dimensional;short indexing;nn search case;algnostic algorithms low;having short indexing;nn search;short indexing time;algnostic algorithms;pruning;introduction pruning;non metric nn;search tree based;pruning rule avoid;search tree;efficient accurate retrieval;pruning rule;search case;require pruning;partitioning require pruning;metric nn;dimensional non metric;require pruning rule;retrieval having short", "pdf_keywords": "metric search tree;non metric search;metric nn search;metric search;search non metric;nonmetric similarity search;use nearest neighbor;search metric spaces;nearest neighbor;search metric;metric trees;metric pruning;efficient accurate retrieval;useful tool searching;similarity search;approximate search dissimilarity;metric trees general;single partition search;efficient metric variant;neighbor nn search;approximation metric pruning;searching query optimization;similarity search complex;approach search metric;search tree based;applied metric trees;nearest neighbor nn;search dissimilarity spaces;searching non symmetric;exact approximate search"}, "1d938731dfad31c09b2f58c365f630c640f2ca1a": {"ta_keywords": "active learning al;active learning;resorted active learning;unlabeled data better;enhance label efficiency;require excessive labeled;pre trained language;label efficiency researchers;label efficiency;unlabeled data;labeled data fine;labeled data;excessive labeled data;labeled;excessive labeled;unlabeled;language models natural;potential unlabeled data;models natural language;unlabeled data ignored;trained language models;natural language processing;enhance label;performance enhance label;natural language;trained language;learning al;language models;label;pre trained", "pdf_keywords": "selftraining weakly supervised;trained language models;trained language model;self supervised learning;demanding labeled data;selftraining active learning;self training framework;pre trained language;performance natural language;self supervised;tuning natural language;trained language;model self training;models nlp;self training model;labeling;demanding labeled;learning model self;learning minimize labeling;supervised learning tasks;active self training;labeled data;natural language models;improve label efficiency;selftraining weakly;fine tuning formal;self training method;method self training;oriented self supervised;weakly supervised learning"}, "ae82f831bda5681edfe40ec15de4e9d2096ea92f": {"ta_keywords": "recognizing lexical entailment;recognizing lexical;lexical entailment;involve recognizing lexical;lexical entailment different;term entailment;narrower term entailment;natural language processing;lexical;tasks natural language;entailment different approaches;term entailment subset;entailment different;entailment;entailment subset broader;language processing;natural language;entailment subset;term second supervised;language processing involve;similarity measure;asymmetric similarity measure;supervised approach classifier;similarity measure designed;similarity;methods asymmetric similarity;asymmetric similarity;recognizing;supervised approach;contexts narrower term", "pdf_keywords": "clustering word senses;determining word similarity;word similarity;clustering word compare;recognizing lexical entailment;clustering word;method clustering word;recognizing lexical;recognize lexical entailment;lexical entailment techniques;use clustering word;senses compare clustering;involve recognizing lexical;recognize lexical;lexical entailment results;approaches lexical entailment;lexical entailment;word senses compare;lexical entailment different;state art lexical;entailment compare convecs;lexical entailment paper;word similarity paper;ability recognize lexical;lexical entailment important;predict sense word;approaches lexical;similarity measure;lexical;showed clustering word"}, "8c9069641876d025c66ab6800939c278b07f60a3": {"ta_keywords": "hierarchies documents documents;topic taxonomies;hierarchies documents;context topic taxonomies;topic documents;topic documents methods;topic taxonomies present;documents methods hierarchies;collection general topics;taxonomy specific topics;topic taxonomies allow;topics live taxonomy;live topic taxonomies;methods hierarchies documents;hierarchies;document collection;topics;specific topics;taxonomies;hierarchy tree;drill topic documents;hierarchy tree inferred;document collection general;taxonomies present;view document collection;topics live;topics live topic;nodes hierarchy tree;documents methods;level view document", "pdf_keywords": ""}, "423044220d9642a2d5839cfb19e32171e8a16a83": {"ta_keywords": "algorithms powering recommender;bandit setup modeling;modeling satiation;satiation enjoyment;satiation enjoyment declining;modeling satiation dynamics;recommender systems seldom;goods subject satiation;powering recommender systems;satiation dynamics;satiation dynamics time;enjoyment declining repeated;powering recommender;subject satiation enjoyment;enjoyment goods subject;shows enjoyment goods;enjoyment goods;recommender systems;satiation;bandit setup;recommender;bandit;multi armed bandit;dynamical model;enjoyment declining;subject satiation;research shows enjoyment;dynamical;declining repeated;enjoyment", "pdf_keywords": "rebounding bandits stochastic;learning rebounding bandits;stochastic bandits satiation;bandits stochastic dynamics;model rebounding bandits;rebounding bandits based;policy rebounding bandits;systems rebounding bandits;method rebounding bandits;bandits stochastic;stochastic bandits;linear stochastic bandits;bandits satiation;rebounding bandits propose;rebounding bandits;bandit setup satiation;rebounding bandits multi;approach rebounding bandits;rebounding bandits consider;bandits satiation agent;bandits associated processes;behavior bandits;bandits rebounding bandits;literature behavior bandits;rebounding bandits problem;behavior bandits associated;bandits event optimal;satiation reward dynamics;bandits rebounding;expected reward satiation"}, "be0ad0710bfb09f6c875dd6cd834ac643713c93d": {"ta_keywords": "diagnosis hepatic neoplasm;hepatic neoplasm;patients diagnosis hepatic;diagnosis hepatic;hepatic;neoplasm;patients diagnosis;diagnosis;management patients diagnosis;management patients;approach management patients;patients;approach management;new approach management;management;importance new approach;approach;article discuss;new;article discuss importance;new approach;discuss importance;importance;discuss;article;discuss importance new;aim article discuss;importance new;aim article;aim", "pdf_keywords": ""}, "4a0f96bb17836b4c4d6e19627f176fba8fe05127": {"ta_keywords": "fault current limiting;active fault current;dc circuit breakers;current limiting circuit;circuit breaker active;breaker active frault;circuit breakers;dc circuit breaker;circuit breaker;frault current limiting;current limiting;existing limiting circuits;breaker active;limiting circuits solid;fault current;limiting circuit methods;designed active fault;limiting circuit;limiting circuit designed;limiting circuits;active frault current;circuit breakers problems;active fault;limiting method active;method active fault;breakers problems;breaker;breakers;circuits solid state;circuit designed active", "pdf_keywords": ""}, "1d5d170670889bd82364fbcc594dadcb5481e9e4": {"ta_keywords": "neural machine translation;translation nm recall;machine translation nm;recall appropriate translation;machine translation;translation examples incorporating;translation examples;engine retrieve sentence;introductionguiding neural;introductionguiding neural machine;retrieve sentence pairs;translation nm;retrieve sentence;translation low frequency;sentence pairs source;seen translation examples;nm recall appropriate;incorporating nm decoding;sentence use search;nm decoding;nm recall;translation low;previously seen translation;recalling;appropriate translation low;decoding;method recalling;frequency words phrases;recalling previously;neural machine", "pdf_keywords": "neural machine translation;machine translation model;machine translation;machine translation systems;machine translation nm;machine translation narrow;model translation tasks;novel translation model;translation nm recall;processing translation pieces;processing translation;translation systems;retrieved source sentences;translation model;translation examples incorporating;translation tasks;retrieve translation;retrieves translation;retrieved sentence pairs;model processing translation;translate retrieve translation;retrieve translation pieces;retrieved target sentences;guided non translation;model translation;retrieves translation pieces;using collected translation;recall appropriate translation;retrieve sentence pairs;novel translation method"}, "f32c67daa6a93281bd8645fc2fa423dca67aea00": {"ta_keywords": "assignment papers reviewers;reviewers conference peer;automated assignment papers;conference peer review;papers reviewers;papers reviewers conference;fairness statistical accuracy;accuracy results fairness;review focus fairness;peer review;reviewers conference;automated assignment;maximize review quality;review quality disadvantaged;results fairness objective;fairness statistical;results fairness;focus fairness statistical;total quality papers;quality papers;peer review focus;quality disadvantaged paper;problem automated assignment;reviewers;fairness objective maximize;conference peer;review quality;automated;assignment papers;fairness objective", "pdf_keywords": "reviewer assignment algorithm;reviewers papers algorithm;algorithm assigning reviewers;fairness accuracy reviewer;assigning reviewers papers;reviewer assignment process;reviewer assignment procedures;reviewer paper assignment;peer reviewer assignment;algorithm reviewer assignment;assignment papers reviewers;conference reviewer assignment;assign paper reviewers;paper assigned reviewers;reviewers assigned paper;accuracy reviewer paper;fairness propose reviewer;accuracy resultsour fairness;reviewer assignment;assign reviewer paper;algorithm allows reviewers;assigning reviewers;performance reviewer assignment;reviewers conference peer;automated assignment papers;develop reviewer assignment;propose reviewer assignment;assignment discriminate papers;assignment reviewer;reviewer assignment context"}, "827e0def212f6834d615e4f3f25b55fe27f6460d": {"ta_keywords": "annotated crowdsourcing relations;crowdsourcing relations capture;annotations using crowdsourcing;manually annotated crowdsourcing;annotated crowdsourcing;crowdsourcing relations;grained semantic information;grained semantic;relations manually annotated;fine grained semantic;spatiotemporal verb relations;verb relations manually;manually annotated;semantic verb relation;annotations;propose novel semantic;annotation;annotation approach;multi step annotation;annotations using;novel semantic;annotation approach scaling;semantic information verb;annotated;temporal entailment relations;verb relation scheme;entailment relations propose;novel semantic verb;step annotation approach;verb centric propositions", "pdf_keywords": ""}, "d8ad713ffde54d0a837e6a9cab4e70739d649d41": {"ta_keywords": "based dialog retrieval;dialog retrieval;dialog retrieval using;chat oriented dialog;example based dialog;dialog systems;dialog systems utilize;example based chat;oriented dialog systems;distributed word representations;human human conversation;human conversation;based dialog;dialog;retrieval using distributed;simple retrieval;human conversation promising;simple retrieval techniques;based chat oriented;retrieval techniques resulting;chat oriented;vocabulary hov database;retrieval techniques;word representations recusive;distributed word;previous simple retrieval;using distributed word;word representations;retrieval using;representations recusive autoencoders", "pdf_keywords": ""}, "f45c777b29e0a00f7b1e1f33daa751853015724a": {"ta_keywords": "computer systems acm;acm presented;systems acm;systems acm presented;acm;acm presented members;american society computer;society computer;society computer systems;computer systems;annual activity report;activity report presented;activity report;activity report american;worldwide activity report;computer;group academia industry;agencies worldwide activity;systems;group academia;members group academia;agencies worldwide members;industry government;worldwide members group;government agencies worldwide;industry government agencies;agencies worldwide;annual activity;activity;industry", "pdf_keywords": ""}, "1f7fb2c16e51f207eb1816f4f3fc3e1649c364f0": {"ta_keywords": "robust speech recognition;speech recognition environment;robust speech;factors robust speech;speech recognition;data simulation mismatches;simulation mismatches important;recognition environment data;recognition environment;speech recognition addition;simulation mismatches;robust;mismatches important factors;mismatches important;important factors robust;data simulation;environment data simulation;recognition addition environment;mismatches;recognition;factors robust;speech;simulation;recognition addition;environment data;data;environment;addition environment data;addition environment;important", "pdf_keywords": ""}, "83f648f01d858d02b20f9327bebb1d5e91d0b6a9": {"ta_keywords": "discriminative optimization decoding;minimizing speech recognition;optimization decoding networks;decoding networks;optimization decoding;optimize decoding networks;optimize decoding;wft based decoding;decoding networks important;decoding networks extending;speech recognition;minimizing speech;discriminative optimization;introductionthe discriminative optimization;model decoding;based decoding;reported optimize decoding;speech recognition error;decoding;based decoding processes;important minimizing speech;model decoding processes;decoding processes linear;paper model decoding;conditional random fields;mutual information mmi;decoding processes;mutual information;maximum mutual information;linear classification process", "pdf_keywords": ""}, "099346e2837c53ded931d98135edbb261039764a": {"ta_keywords": "political polarization;political polarization growing;background political polarization;polarization contentious;causes polarization contentious;numerous sociologists political;sociologists political;sociologists political scientists;polarization contentious numerous;contentious numerous sociologists;political scientists;political;world cases polarization;political scientists weighing;elected decision makers;background political;causing democratic;causes polarization;democratic process grind;causing democratic process;polarization;numerous sociologists;sociologists;turn causing democratic;polarization growing phenomenon;polarization growing;cases polarization;polarization leading;decision makers creating;contentious", "pdf_keywords": ""}, "cbdccaa4a5bceaae190f78b1ac0a0cf47391968d": {"ta_keywords": "curation microcontent process;curation microcontent;microcontent process;microcontent process created;microcontent microcontent process;microcontent process process;process created microcontent;process completely curationable;created microcontent microcontent;created microcontent;microcontent microcontent;curationable product process;microcontent;curation;curationable;completely curationable;curationable product;completely curationable product;process completely;process process completely;process created;process;process process;product process;created;product;completely", "pdf_keywords": ""}, "09fcc7ed1f867bcf9133ab12065ee7366cfaa652": {"ta_keywords": "recommendation models dlrs;recommendation models;checkpointing;training progress checkpointing;learning based recommendation;distributed systems mitigated;used fault tolerance;checkpointing primary approach;deep learning;progress checkpointing;based recommendation models;fault tolerance;large distributed systems;model memory;checkpointing primary;hundreds servers;servers;fault tolerance systems;deep learning based;server failures common;users dlrs large;progress checkpointing primary;distributing model memory;servers server failures;large embedding tables;failures common large;personalized content users;trained distributing model;distributed systems;server failures", "pdf_keywords": "recommendation model training;handling failures neural;training checkpointing;erasure coded training;training progress checkpointing;recommendation applications deep;approach fault tolerant;coding fault tolerance;failures neural network;dlr training checkpointing;training checkpointing training;checkpointing training;fault tolerant;efficient fault tolerance;recommendation models dlrs;checkpointing training periodically;fault tolerant dlm;training checkpoint redoing;erasure coded learning;adapting simple erasure;erasure codes storage;training checkpoint;learning based recommendation;erasure coding overcome;approach erasure coded;writen stable storage;erasure codes ideas;codes ideas storage;used fault tolerance;checkpointing elucidating"}, "8d939637b3a5ecf681130619cd35f295dbb9db03": {"ta_keywords": "le581thro susceptibility venous;susceptibility venous thrombosis;backgroundkong1 le581thro susceptibility;risk venous thrombosis;rs710446 risk venous;venous thrombosis;venous thrombosis aimto;le581thro susceptibility;venous thrombosis vt;thrombosis aimto investigate;associated polymorphisms rs2731672;polymorphisms rs2731672 rs9898;thrombosis vt methodsa;susceptibility venous;apt associated polymorphisms;risk venous;polymorphisms rs2731672;thrombosis aimto;thrombosis;thrombosis vt;backgroundkong1 le581thro;rs710446 risk;healthy patients 1542;associated polymorphisms;rs9898 rs710446 risk;venous;le581thro;backgroundkong1;patients 1542 patients;patients 1542", "pdf_keywords": ""}, "0bcd8210e9b90b33ab8467b94fd9b9511aad0f86": {"ta_keywords": "evaluation optimize ump;optimize ump;network transmission speed;ump focusing network;optimize ump focusing;speed methods performance;performance evaluation optimize;network transmission;transmission speed results;speed results performance;transmission speed methods;ump;transmission speed conclusion;transmission speed;performance evaluation;speed conclusion performance;speed results;methods performance evaluation;speed methods;evaluation optimize;focusing network transmission;speed conclusion;ump focusing;results performance evaluation;performance;speed;methods performance;optimize;results performance;network", "pdf_keywords": ""}, "cece2d2f7cc38a512325122401f8aa658121b80e": {"ta_keywords": "automatic deception detection;deception detection;automatic deception;detect deception;attempt detect deception;deceptive conversation partner;potentially deceptive conversation;deceptive conversational partner;deception detection focuses;detect deception perform;deceptive conversation;deception asking questions;deceptive conversational;deception asking;deception perform actions;signs deception asking;work automatic deception;deception;unveil deceptive conversational;telltale signs deception;deception perform;signs deception;constructing dialog asks;potentially deceptive;catch potentially deceptive;conversation partner method;deceptive;constructing dialog;attempt unveil deceptive;dialog", "pdf_keywords": ""}, "4bc9d6596069c9277b57a7ee1e1127d231f28663": {"ta_keywords": "outside recursive autoencoder;recursive autoencoder daora;recursive autoencoder;learns induce syntactic;autoencoder daora self;induce syntactic tree;autoencoder daora;sentence soft dynamic;autoencoder;syntactic tree;syntactic tree structures;structures input sentences;self supervised neural;trees sentence soft;neural model learns;input sentences access;sentences access labeled;tree structures input;daora self supervised;self supervised;sentences access;induce syntactic;input sentences;model learns;binary trees sentence;supervised neural;daora exhaustively encodes;outside recursive;inside outside recursive;learns", "pdf_keywords": ""}, "dbb4035111c12f4bce971bd4c8086e9d62c9eb97": {"ta_keywords": "graph learning models;graph machine learning;graph learning;ways graph learning;graph machine;nature social networks;networks developing countries;large scale graph;poverty estimation urban;global poverty;social networks people;social networks;study global poverty;global poverty recent;networks people related;poverty estimation;response poverty estimation;scale graph machine;networks people;humanitarian response poverty;poverty recent applications;poverty;myriad ways graph;poverty recent;urban planning epidemic;networks;response poverty;graph;networks developing;ways graph", "pdf_keywords": "graph learning models;graph machine learning;graph learning;ways graph learning;graph convolutional networks;graph convolutional network;graph convolutional;view graphs;applied graph convolutional;multi view graphs;graph machine;views graph;large scale graph;semi supervised;graph multiple views;semi supervised learning;views graph reconstructed;views graph used;graph structured data;view networks;learning multi view;performance graph convolutional;multi view networks;graph structured;graph based semi;multiple views graph;approach semi supervised;graph reconstructed;based semi supervised;world social networks"}, "8376b18a4dd228ea4c33d606b32b081cee9bf80a": {"ta_keywords": "minimizing smooth convex;stochastic optimization;convex function available;stochastic optimization problems;similar stochastic optimization;function available noisy;optimization problems stochastic;unconstrained problem minimizing;minimizing smooth;smooth convex function;smooth convex;problem minimizing smooth;convex function;additive noise;second additive noise;available noisy observations;optimization;convex;minimizing;noisy observations;problems stochastic;optimization problems;problem minimizing;problems stochastic nature;noisy observations values;values noise consisting;stochastic;additive noise unknown;consider unconstrained problem;unconstrained problem", "pdf_keywords": "optimization stochastic computations;convex optimization stochastic;stochastic convex optimization;stochastic optimization numerical;similar stochastic optimization;free stochastic optimization;stochastic optimization;smooth convex optimization;method stochastic convex;algorithms stochastic smooth;stochastic gradients;stochastic programming nonconvex;optimization stochastic;nonconvex stochastic programming;stochastic optimization problems;minimizing smooth convex;function noisy stochastic;stochastic computations;stochastic convex;free optimization point;approach stochastic convex;convex optimization point;bandit convex optimization;optimization point feedback;noisy stochastic oracle;optimization problems stochastic;unconstrained optimization;convex optimization;free convex optimization;problem stochastic optimization"}, "1e5b826ddf0754f6e93234ba1260bd939c255e7f": {"ta_keywords": "autoregressive machine translation;machine translation nat;data pretrained autoregressive;pretrained autoregressive model;machine translation;pretrained autoregressive;translation nat systems;autoregressive machine;autoregressive model better;autoregressive models existing;translation nat;speed compared autoregressive;autoregressive models;compared autoregressive models;backgroundnon autoregressive machine;autoregressive model;knowledge distillation;data pretrained;training data pretrained;improvements generation speed;nat models usually;compared autoregressive;nat systems predict;autoregressive;nat models;knowledge distillation creates;technique knowledge distillation;existing nat models;backgroundnon autoregressive;systems predict sequence", "pdf_keywords": "autoregressive machine translation;machine translation increasing;neural machine translation;sequence generation knowledge;machine translation nat;machine translation important;autoregressive translation nat;machine translation;translation nat models;knowledge distillation empirically;machine translation results;machine translation present;knowledge distillation empirical;generation knowledge distillation;non autoregressive translation;autoregressive translation;machine translation applications;nat models trained;sequence generation model;machine translation propose;data pretrained autoregressive;pretrained autoregressive model;autoregressive nat models;translations distilled data;understanding knowledge distillation;pretrained autoregressive;knowledge distillation;machine translation apply;small sequence generation;performance knowledge distillation"}, "41d4763792db8ea420efcfbd112a55deec971fee": {"ta_keywords": "endcyclopedic commonsense knowledge;commonsense knowledge;commonsense knowledge methods;endcyclopedic commonsense;community based knowledge;results envision wordnet;wordnet;marriage endcyclopedic commonsense;knowledge results envision;envision wordnet;knowledge methods community;based knowledge useful;knowledge results;based knowledge;commonsense;wordnet mediator systems;knowledge useful;envision wordnet mediator;knowledge useful different;knowledge;wordnet mediator;knowledge methods;types knowledge results;methods community based;community based;types knowledge;endcyclopedic;marriage types knowledge;marriage endcyclopedic;methods community", "pdf_keywords": ""}, "4cc97c3858b558b4fa80ad73a894fcc7df841114": {"ta_keywords": "predictive performance fairness;fairness practice exploring;notions fairness practice;designing fair model;fair model model;fairness different notions;fairness practice;different notions fairness;fair model;notions fairness;performance fairness;designing fair;performance fairness different;fairness different;fairness;offs designing fair;predictive performance;model predictive performance;exploring trade offs;inherent trade offs;model predictive;trade offs designing;model resultsour;model resultsour work;model model predictive;trade offs training;predictive;explore trade offs;human computational resources;fair", "pdf_keywords": "group fairness diagnostic;classifying group fairness;incompatibility groups fairness;diagnostic based fairness;group fairness notions;different groups fairness;groups fairness notions;fairness diagnostic based;fairness diagnostic group;diagnostic group fairness;group fairness general;assessment multiple fairness;fairness diagnostic;fairness class fairness;offs fairness diagnostic;fairness notions group;tensor fairness fairness;groups fairness;group fairness reformulated;group fairness;dataset fairness fairness;fairness confusion tensor;fairness general model;performance group fairness;fairness notions fairness;group fairness strong;fairness notions empirically;class fairness notions;notions group fairness;fairness provide model"}, "72d89aa7cd77c3f22a667f2b0707758eb8d52a7a": {"ta_keywords": "context aware translation;contexts human translators;aware translation models;aware machine translation;translation models;human translators use;human translators;translation models designed;machine translation models;aware translation;inaccurately disambiguate pronouns;machine translation;translators use;disambiguate pronouns;disambiguate pronouns polysemous;translators;translators use resolve;ambiguous words models;context resolution;contexts human;pronouns polysemous words;words require context;resolve ambiguous words;inaccurately disambiguate;translation models pay;contextual;contextual information;disambiguate;contexts;introduction context aware", "pdf_keywords": "translators use disambiguation;translation quality attention;context aware translation;disambiguate hard translation;baselines improving translation;human translation models;aware translation models;aware translation accuracy;approach disambiguate pronouns;context translation processes;translation models;improving translation;improving translation quality;translation quality context;contextual translational;context human translators;disambiguate pronouns;disambiguate pronouns polysemous;machine translation;context useful disambiguate;increasing accuracy translation;improve translation quality;machine translation association;machine translation research;translation models use;ambiguity translations models;machine translation implications;machine translation impact;translation quality improves;translations models"}, "d170bd486e4c0fe82601e322b0e9e0dde63ab299": {"ta_keywords": "adaptive input representations;adaptive inputrepresentations neural;neural language modeling;input representations neural;adaptive softmax;extend adaptive softmax;representations neural language;adaptive inputrepresentations;inputrepresentations neural language;introduction adaptive inputrepresentations;input representations;softmax;representations neural;adaptive softmax grave;inputrepresentations neural;input representations variable;representations variable capacity;language modeling extend;model words characters;adaptive input;introduce adaptive input;neural language;language modeling;model words;layers model words;2017 input representations;language modeling introduced;representations;inputrepresentations;representations variable", "pdf_keywords": "extend adaptive softmax;inputs adaptive softmax;adaptive softmax;adaptive softmax novel;neural language models;neural language modeling;softmax;adaptive softmax grave;model wordsthe gnas;softmax novel approach;representations neural language;adaptive input embeddings;attentional networks introduce;representations natural language;word embeddings improved;adaptive input representations;attentional networks;language models;word representations natural;softmax novel;word representations;language modeling extend;gnas adaptive input;language models reducing;adaptive word;input representations neural;self attentional networks;gnas adaptive;word embeddings;language models important"}, "3d3f01feee0dd3eea22e390c80deaadc6f11eb9a": {"ta_keywords": "cnn based multichannel;end speech recognition;chime challenge methods;chime challenge;environments target chime;speech recognition;target chime challenge;convolutional neural;speech recognition systems;speech recognition study;automatic speech;convolutional neural network;employing convolutional neural;network cnn;chime;automatic speech recognition;cnn based;conversations;neural network cnn;performances automatic speech;network cnn based;convolutional;target chime;casual conversations;recognition systems challenging;multichannel;multichannel end;end speech;cnn;employing convolutional", "pdf_keywords": "attention model multichannel;deep cnn;parallel deep cnn;attention model extensions;deep cnn encoder;parallel cnn encoder;network cnn;parallel cnn;deep learning speech;attention model;ct attention model;cnn based multichannel;cnn encoder;attention model described;cnn encoder residual;neural network cnn;convolutional neural;attention model based;learning batch renormalization;neural networks speech;deep learning;end speech recognition;networks speech recognition;batch renormalization network;residual learning batch;convolutional neural network;language modeling batch;attention model evaluated;joint ct attention;cnn"}, "b2fd7297f7681f9e3ea860cecf1ec97b2cc8ccc3": {"ta_keywords": "delivering device patient;device patient method;deliver device patient;method delivering device;use device deliver;device patient use;delivering device;patient use device;device deliver device;deliver device;device deliver;device patient;patient method;method delivering;patient method effective;new method delivering;use device;patient use;device;reducing risk malignancy;delivering;deliver;method;effective reducing risk;patient;risk malignancy;reducing risk;new method;malignancy;method effective", "pdf_keywords": ""}, "ff783c4709a095cc581534fec58ef9515613ebc9": {"ta_keywords": "continual learning chloro;continual learning learn;continual learning;goal continual learning;catastrophic forgetting;forgetting methods goal;methods goal continual;phenomenon catastrophic forgetting;continual;catastrophic forgetting results;forgetting methods;repulce goal continual;goal continual;catastrophic forgetting methods;learning chloro learn;forgetting;chloro learn;results goal continual;learning chloro;forgetting results;chloro learn sequence;forgetting results goal;learning learn;learning;explanations repulce goal;explanations repulce;learn;sequence tasks suffering;repulce goal;learn sequence tasks", "pdf_keywords": "explanations continual learning;catastrophic forgetting previous;catastrophic forgetting;model explanations memory;explanations memory;learn memory;learning memory;models incorporate remember;leveraging memory;continual learning learn;recall previous tasks;networks learn memory;remembering;called remembering;words model remember;explanations memory samples;forgetting;forgetting previous;continual learning;tasks hypothesize forgetting;shown leveraging memory;phenomenon catastrophic forgetting;remember evidence previously;perform continual learning;reconstruct prior reasoning;learn memory ability;memory form replay;better recall previous;learn sequence tasks;learning learn sequence"}, "f2e7598464a0b9376771ffc4ba243233ee12c677": {"ta_keywords": "lexical sememe prediction;chinese characters words;prediction conclusionsincorporating chinese;conclusionsincorporating chinese characters;introductionincorporating chinese characters;characters words lexical;words lexical sememe;approaches lexical sememe;presentationincorporating chinese characters;characters words forlexicical;lexical sememe;forlexicical sememe prediction;words forlexicical sememe;case presentationincorporating chinese;sememe prediction conclusionsincorporating;chinese characters;conclusionsincorporating chinese;sememe prediction;characters words;sememe prediction case;sememe prediction useful;words lexical;novel approaches lexical;presentationincorporating chinese;approaches lexical;introductionincorporating chinese;lexical;words forlexicical;forlexicical sememe;prediction case presentationincorporating", "pdf_keywords": "lexical sememe prediction;words sememe prediction;word wesememe prediction;prediction character sememe;prediction internal character;information lexical sememe;sememe prediction character;external information lexical;lexical sememe;neural sememe prediction;trained word embeddings;methods lexical sememe;propose lexical sememe;words sememe kinesiometry;prediction character;character sememe embeddings;word embeddings;human annotated sememe;external context words;existing methods lexical;words ensemble internal;definition lexical sememe;hownet sememe prediction;word sememe set;words sememes consider;words sememes;information lexical;relationships words sememes;enhanced sememe prediction;annotated sememe kb"}, "48b18bf5c9cad0e4c36b2d885f380c5c637e1a09": {"ta_keywords": "learning disentangled representations;disentangled representations;learning disentangled;variational autoencoders;disentangled representations focuses;focuses variational autoencoders;variational autoencoders vaes;representations focuses variational;literature learning disentangled;disentangled;autoencoders;developments demonstrate disentanglement;autoencoders vaes recent;variational;autoencoders vaes;factorized prior distribution;disentanglement;prior distribution latent;focuses variational;demonstrate disentanglement;factorized prior;disentanglement obtained fully;disentanglement obtained;employing factorized prior;demonstrate disentanglement obtained;distribution latent variables;unsupervised setting inductive;representations;inductive biases models;setting inductive biases", "pdf_keywords": "learning disentangled representations;disentangled representation learning;disentanglement learning vae;disentangled representations latent;learn disentangled representations;disentangled representations using;disentangled representations;method disentangled representations;disentangled representations method;disentanglement learning;disentangled learning;method disentanglement learning;disentangledrepresentations;disentangledrepresentations form conditioning;disentangled models data;disentangled representations according;disentangled representation;learning disentangled;disentangled models;method learning disentangled;vae disentangledrepresentations;disentangled disentangled representations;disentangled learning suggest;vae disentangled representations;disentangled representations major;approach learning disentangled;disentangledrepresentations form;disentangled representations experimental"}, "ee5dc631a682696a4704b742ea087e8abb5df897": {"ta_keywords": "speech recognition tasks;unsupervised speech recognition;self supervised training;speech recognition;language model reward;self supervised;unsupervised speech;language model;supervised training;benefits unsupervised speech;included language model;recognition tasks;hypotheses forwarding tt;background self supervised;supervised;recognition tasks paper;supervised training drawn;speech;features arts pipeline;model reward;features arts;recognition;hypotheses forwarding;enhanced anr ts;ar hypotheses forwarding;anr ts;training;training drawn;anr ts eat;cycle consistency algorithm", "pdf_keywords": ""}, "162c3cf78af48ddf826ec76a1a3767a88a730170": {"ta_keywords": "diagnosis pulmonary infection;pulmonary infection;patients diagnosis pulmonary;diagnosis pulmonary;pulmonary;patients diagnosis;infection;management patients diagnosis;diagnosis;patients;management patients;approach management patients;approach management;new approach management;article discuss importance;importance new approach;management;new approach;article discuss;article;approach;aim article discuss;discuss importance new;aim article;discuss importance;new;importance new;importance;discuss;aim", "pdf_keywords": ""}, "c8a5d05cb741b3448ec4106d2006ae24a7a401b4": {"ta_keywords": "streaming automatic speech;transducer models streaming;training transducer models;regularization directly sequence;automatic speech;rn transducer;automatic speech recognition;speech recognition networks;optimization transducer models;speech recognition;probability training transducer;training transducer;latency regularization directly;latency regularization;including rn transducer;rn transducer transformer;sequence probability training;emission regularization;applies latency regularization;models streaming automatic;optimization transducer;fastemit suitable sequence;streaming automatic;transducer models;regularization directly;level optimization transducer;models streaming;regularization;level emission regularization;sequence level optimization", "pdf_keywords": "prediction sequence transducer;sequence transducer models;streaming automatic speech;sequence level regularization;sequence transducer;sequence based transducer;transducer regularization;effective transducer regularization;term speech recognition;transducer models streaming;streaming device speech;transducer regularization method;automatic speech;improving speech recognition;fastemit sequencewe;emission regularization;speech recognition;speech recognition networks;probabilities fastemit sequencewe;fastemit sequence level;regularization method fastemit;sequence level emission;rn transducer;emitting vocabulary tokens;automatic speech recognition;fastemit improves recognition;fastemit sequence;sequence level optimization;regularize emission latency;fastemit sequencewe present"}, "9918ea4b68e90e1257953b6f2665b2ce29f2bc8b": {"ta_keywords": "speech enhancement 3d;speech enhancement challenge;channel speech enhancement;speech enhancement;enhancement challenge l3;l3 das22 challenge;3d ambisonic microphones;consists speech enhancement;challenge l3 das22;multi channel speech;ambisonic microphones;ambisonic microphones methods;enhancement challenge;das22 challenge;l3 das22;das22 challenge task;submission l3 das22;das22 challenge paper;microphones methods core;low distortion multi;enhancement 3d ambisonic;enhancement 3d;microphones;channel speech;distortion multi channel;distortion multi;das22;challenge l3;combines deep neural;microphones methods", "pdf_keywords": "speech enhancement challenge;speech enhancement 3d;beamforming speech recognition;acoustic beamforming speech;enhanced speech multi;channel speech enhancement;neural beamforming enhancement;beamforming speech;speech enhancement;approach acoustic beamforming;speech temporal convolution;estimation acoustic beamforming;acoustic beamforming;placed speech enhancement;acoustic beamforming method;generate enhanced speech;iterative neural beamforming;multiframe neural beamforming;enhanced speech;beamforming enhancement architecture;consists speech enhancement;speech enhancement important;beamforming enhancement;neural beamforming;multi channel speech;new approach acoustic;neural beamforming mfmwf;speech multi channel;speech signal farfield;recover dry speech"}, "30109a213aa10765486c676ecfa511db227ab543": {"ta_keywords": "sentences mini batch;neural machine translation;machine translation nm;shorter sentences mini;translation nm models;machine translation;mini batched training;pad shorter sentences;sentences mini;sentence length making;mini batches;mini batch;uses mini batches;mini batched;sentence length;mini batches reduces;longest sentence efficient;shorter sentences;making mini batches;length longest sentence;mini batch equal;purposes mini batched;mini batches efficiency;based sentence length;translation nm;longest sentence;batched training;batch equal length;batched training process;batches", "pdf_keywords": "sentences mini batch;mrna learning size;neural mrna learning;target words minibatch;mrna learning;words minibatch significantly;machine learning minibatched;learning minibatched;learning size minibatch;mini batches;learning minibatched training;mini batch;words minibatch;mini batch size;use mini batches;batch size training;shorter sentences mini;suggest mini batch;neural machine translation;mini batch creation;mini batches efficiency;analyzed mini batch;making mini batches;size mini batch;pad shorter sentences;minibatched training process;sentences mini;mini batches reduces;longest sentence efficient;follows mini batch"}, "5e6acc5c73f22c2dbbb4910f656a03cf40a2fe15": {"ta_keywords": "aetiology understood aetiology;aetiology aetiology understood;aetiology understood;understood aetiology aetiology;understood aetiology;aetiology aetiology;shown aetiology aetiology;aetiology;study shown aetiology;shown aetiology;study;study shown;understood;new study;new study shown;new;shown", "pdf_keywords": ""}, "03e62d5f0265608c6ebdebba0870131b056b79a6": {"ta_keywords": "mitigating harm content;harmful content online;harm content moderation;harm content;harmful content moderated;harmful content;harm online;experiences harm online;implemented harmful content;harmful content implemented;content implemented harmful;harm approaches mitigating;harm approaches;harm online development;content moderation;policy harmful content;understandings harm approaches;practices harm;practices harm mitigation;mitigating harm;harm mitigation;approaches mitigating harm;harm mitigation understandings;proliferation harmful content;harm researched policy;content online social;mitigation understandings harm;content moderation implicitly;harm researched;content moderated", "pdf_keywords": "online harm content;online harm empirical;online harms research;computing online harm;harm online content;online harms methodswe;harmful content online;severity harm online;online harm;severity online harm;severity online harms;examining harm content;online harms;harm experienced online;online harm challenge;experiences harm online;mitigating harm content;harm online;harmmful content online;harm content moderation;harm associated content;harm online development;harmful content interactions;harm content;harmful content moderated;assess harms framework;online harms major;understandings harm approaches;severity social media;harmful content"}, "33ce3cd897a3473973f338c154f3fe5c1175643c": {"ta_keywords": "predicate query language;open predicate query;query language oprql;query language;language oprql;language oql;query language oql;open predicate;predicate query;language oprql method;language oql method;predicate;present open predicate;trained entirely text;oprql method constructing;oprql;results demonstrate oql;oql;oql method constructing;oprql method;kb vkb trained;entirely text results;oql method;demonstrate oql;entirely text methods;query;text results;vkb trained;constructing virtual kb;text methods", "pdf_keywords": "virtual knowledge bases;knowledge base queries;question answering;neural knowledge language;virtual knowledge base;neural knowledge base;large knowledge bases;building virtual knowledge;question answering recommendation;knowledge base text;knowledge language model;knowledge bases;knowledge base;question answering using;domain question answering;reasoning virtual knowledgewe;scalable document comprehension;opql memory retrieval;knowledge language;reasoning virtual knowledge;entities natural language;model virtual knowledge;virtual knowledge;applications question answering;structured queries effectively;construct virtual knowledge;knowledge base based;aware language modeling;based knowledge base;semi structured queries"}, "b00bc4dcce60e7c631a23d60894e51001de1c630": {"ta_keywords": "ebola virus glycoprotein;virus glycoprotein;virus glycoprotein amino;vesicular stomatitis virus;glycoprotein cleavability viral;properties ebola virus;stomatitis virus;stomatitis virus vv;ebola virus;cleavability viral infectivity;viral infectivity;virus vv pseudotype;cleavability viral;infectivity ofvvv pseudotyped;functional properties ebola;viral infectivity viruses;vesicular stomatitis;infectivity ofvvv;glycoprotein cleavability;properties ebola;reduce glycoprotein cleavability;change infectivity ofvvv;glycoprotein amino acid;glycoprotein amino;infectivity viruses;glycoprotein;virus vv;backgroundthe vesicular stomatitis;infectivity viruses did;ebola", "pdf_keywords": ""}, "2583e7e279e2969493c3290c8f300ab32da40bf9": {"ta_keywords": "lingual entity linking;referents target language;backgroundcross lingual entity;entity linking;finding referents target;entities target language;entity linking xe;knowledge base mentions;referents target;lingual entity;finding referents;entity candidate generation;target language knowledge;base mentions extracted;base mention methods;language knowledge base;mentions extracted source;candidate entities target;knowledge base mention;target language;task finding referents;entities target;backgroundcross lingual;candidate generation methods;plausible candidate entities;candidate generation retrieves;mentions extracted;source language texts;linking;entity candidate", "pdf_keywords": "lingual entity linking;entity linking ability;entity linking;entity linking development;entity linking model;entity linking xe;referents target language;based entity linking;entities target language;disambiguation model present;disambiguation model;cross lingual entity;lingual entity;downstream disambiguation model;mention entity mismatch;mention entity pairs;entities mentions;candidate entities mentions;language knowledge base;target language knowledge;introduce mention entity;entity candidate generation;mention entity;entity pairs training;mismatch mention entity;entities mentions extracted;generation candidate entities;candidate entities manually;candidate entities target;generate candidate entities"}, "0bb30ed3340d2c34fe9f37c5002929bc5f458c23": {"ta_keywords": "biomedical text mining;biomedical relation statement;biomedical text;backgrounda biomedical relation;existing biomedical text;biomedical relation;relation extraction;relation extraction task;extract information biomedical;information biomedical literature;biomedical literature;ary relation extraction;sentences use graph;text mining;biomedical literature existing;literature existing biomedical;use graph neural;entities multiple sentences;information biomedical;text mining approaches;backgrounda biomedical;graph neural;biomedical;relation statement commonly;sentence ary relation;detects relations;detects relations entities;graph neural network;existing biomedical;concepts including gene", "pdf_keywords": "relation attention networks;attention mechanism graph;text directed graph;biomedical natural language;graph transformer encode;attention networks brr;relation attention;sentence relation extraction;language processing biomedical;endcoderrepresentations transformers graph;relation extraction bovine;natural language processing;biaffine relation attention;relation extraction;attention networks;graph transformer abstract;machine translation;biomedical relation;graph transformer architecture;biomedical relation usuallyconsists;propose graph transformer;attention layer implement;abstract meaning representation;attention layer;ary relation extraction;using graph transformer;graph transformer;natural language;relation extraction using;sentence relation"}, "9bbc8ca94810e8a21e4a6a55a5913c5b0b6c787f": {"ta_keywords": "speech transcription respeaking;speech transcription;introductionefefficient speech transcription;line speech transcription;transcription respeaking;transcription respeaking methods;transcription respeaking propose;automatic transcript respeaking;respeaking methods speech;transcript respeaking performed;efficient line speech;transcript respeaking;transcription;methods speech segmented;speech segmented;segmented smaller utterances;automatic transcript;initial automatic transcript;speech segmented smaller;smaller utterances using;introductionefefficient speech;smaller utterances;speaking vs typing;experiments comparing speaking;methods speech;line speech;utterances using;utterances using initial;utterances;transcript", "pdf_keywords": ""}, "7e0570f498a5de4f2a861546d4e67ba208f71d12": {"ta_keywords": "speaker recognition benchmark;spoken interactions benchmark;available chime corpus;speaker recognition;introduce speaker recognition;introduction speaker recognition;chime corpus;chime corpus goal;speaker recordings naturally;multi speaker recordings;speaker recordings;using chime corpus;chime corpus paper;benchmark using chime;field multi speaker;spoken interactions;occurring spoken interactions;corpus;speaker multi;using chime;publicly available chime;multi speaker;recordings naturally;recognition benchmark;corpus goal foster;single speaker multi;corpus goal;recognition benchmark derived;available chime;conditions single speaker", "pdf_keywords": ""}, "1410f7d9470a24fb4055c6685c2dda758b9d995f": {"ta_keywords": "evolutionary game theory;evolutionary game;games play evolve;play evolve strategically;agents static games;dynamic agents;game theory;dynamic agents interact;evolve strategically time;evolve strategically;game theoretic;archetypal game theoretic;paradigm evolutionary game;online learning games;game theory generally;play evolve;dynamic agents static;population dynamic agents;learning games;static games introduce;agents games;static games;games introduce analyze;agents games play;learning games based;divide dynamic agents;archetypal game;evolutionary;arguably archetypal game;games introduce", "pdf_keywords": "time evolving games;games evolution;learning game theoretic;evolving games time;evolving games;games evolution zero;evolving games constant;evolutionary game theory;evolution learning game;evolutionary game;games thethe evolution;evolution game fundamental;evolving games possess;strategy recurrent dynamics;agents time evolving;game time evolving;thethe evolution game;game theoretic games;learning agents time;game strategy recurrent;evolution game;games play evolve;evolutionary learning dynamic;process evolutionary game;recurrent time evolving;time evolving generalized;game theoretic;learning dynamic obeys;field evolutionary game;evolve strategically time"}, "e8e62a80c7355bcf5dbc9fabafff4025e00cf540": {"ta_keywords": "combinatory categorial grammars;inducing combinatory categorial;categorial grammars pos;categorial grammars;induction combinatory categorial;categorial grammars described;model inducing combinatory;grammars pos tagged;combinatory categorial;inducing combinatory;induces linguistically plausible;model induction combinatory;induces linguistically;linguistically plausible lexicons;pos tagged text;grammars pos;languages induces linguistically;bayesian model induction;grammars described;linguistically plausible;novel nonparametric bayesian;grammars;model inducing;induction combinatory;languages induces;nonparametric bayesian;nonparametric bayesian model;plausible lexicons;linguistically;tagged text", "pdf_keywords": ""}, "4ef46d5daf6a7a9536e2ebe3c7aa2296bffcf43e": {"ta_keywords": "unsupervised speech tagging;unsupervised pos tagging;speech tagging;speech tagging using;fully unsupervised speech;hybrid hm unsupervised;pos tagging;pos tagging promising;unsupervised speech;unsupervised pos;infinite human mammalian;hm unsupervised pos;tagging;tagging promising tool;infinite human;tagging using;tagging promising;tagging using non;mammalian hybrid hm;fully unsupervised;human mammalian hybrid;hm single task;introductionthe infinite human;human mammalian;hm unsupervised;mammalian hybrid;unsupervised;hybrid hm;speech;called infinite hm", "pdf_keywords": ""}, "467b14cc8337dd7efe1d374f9a7feb90ae9d2c12": {"ta_keywords": "speech modulation speech;use speech modulation;speech modulation;speech modulation used;modulation speech;modulation speech fundamental;modulate speech speech;modulate speech;used modulate speech;shown speech modulation;modulation used;modulation;use speech;modulation used modulate;speech fundamental;speech speech;speech fundamental step;development speech;speech;development speech recently;shown speech;modulate;used modulate;speech recently shown;step development speech;speech recently;recently shown speech;use;development;used", "pdf_keywords": ""}, "63a604942f1238e9678aebd697a2379981e9a20a": {"ta_keywords": "inhibit tutor learning;tutor learning effect;tutor learning studied;studying tutor learning;facilitate inhibit tutor;tutor learning;tutor learning understood;studying tutor;inhibit tutor;effect tutor learning;area studying tutor;tutor;suggest students learn;teaching computer;students learn;effect tutor;learn teaching computer;students learn teach;students learn teaching;learning environment;learning studied various;students;teach cognitive;learning studied;learn teach cognitive;learning effect cost;teach cognitive social;suggest students;learning environment students;learn teach", "pdf_keywords": ""}, "db392858262b17aa9c8ff8659738f68fbf832ebe": {"ta_keywords": "code completion;code completion generating;approach code completion;code completion leverages;problem code completion;abstract syntax tree;code snippet tree;syntax tree decomposing;syntax tree;syntax programming languages;languages model code;tree structural language;programming languages model;syntax programming;strict syntax programming;snippet tree structural;program abstract syntax;snippet tree;source code;completion generating;programming languages;language modeling sl;piece source code;abstract syntax;code snippet;structural language modeling;completion generating missing;language modeling;syntax;structural language", "pdf_keywords": ""}, "040a1abdbef2a0e087a586d719259c32c95bfc78": {"ta_keywords": "dialog management task;solving dialog management;dialog management;dialog;solving dialog;model solving dialog;action belief propagation;clinical messagewe design;task planning learning;key clinical messagewe;belief propagation inference;clinical messagewe;log linear probabilistic;belief propagation;optimal action belief;planning learning;context free grammars;task planning;planning learning optimize;probabilistic model solving;messagewe design;estimation optimal action;linear probabilistic;linear probabilistic model;messagewe design log;free grammars;policy optimization;learning optimize;propagation inference;action belief", "pdf_keywords": ""}, "e1a20480e4168d58deec743035b7ff02720672d7": {"ta_keywords": "end speech recognition;character based lstm;language models end;speech recognition;based lstm receptorn;language modeling decoding;speech recognition ar;attention connectionist temporal;automatic speech recognition;based lstm;speech recognition propose;decoding open vocabulary;hybrid attention connectionist;language models;connectionist temporal classification;language modeling;end automatic speech;lstm;end end speech;level language modeling;automatic speech;vocabulary end end;based language models;open vocabulary end;modeling decoding open;end speech;lstm receptorn;attention connectionist;classification architecture character;models end end", "pdf_keywords": ""}, "a13d9c8e5a2fc028ad597e2bd46a9c60aca0ede4": {"ta_keywords": "based speech synthesis;synthesize speech specific;based speech synthsynthesis;speech synthesis;synthesize speech;speech synthesis resultsthe;speech synthsynthesis prosody;expressive speech synthesis;modification based speech;speech synthsynthesis;prosody modification based;synthsynthesis prosody modification;speech synthesis order;interface synthesize speech;user speech inputs;based speech input;speech inputs hmm;prosody modification method;propose prosody modification;prosody modification;hmm based speech;synthsynthesis prosody;speech input;speech inputs;purposehmm based speech;using user speech;user speech;inputs hmm based;speech specific target;speech input aimto", "pdf_keywords": ""}, "d26a7a86013b3be57acc0f5df73393cab7c302d9": {"ta_keywords": "constraining structural degeneration;particle flows constraining;flows constraining structural;structural degeneration sde;flows constraining sdes;deterministic particle flows;introductiondeterministic particle flows;degeneration sde;constraining sdes described;structural degeneration;flows constraining;degeneration sde described;particle flows;constraining sdes;sdes described deterministic;sde described deterministic;introductiondeterministic particle;constraining structural;described deterministic particle;deterministic particle;sde described;sdes described;flows;degeneration;described deterministic;introductiondeterministic;particle;sde;deterministic;constraining", "pdf_keywords": "deterministic particle flow;flows employing deterministic;forward probability flows;optimization optimal drift;optimal drift;flows path costs;optimal drift terms;flow propagation optimal;probability flows employing;costs flow dynamics;solving flow dynamics;deterministic particle methods;deterministic particle dynamics;path costs flow;particle flow control;exponentials2 flow dynamics;probability flows;dynamics flows path;exponentials2 flow;logarithmic gradients forward;dynamics flows;stochastic control;flow dynamics flows;costs flow;flow dynamics;operator exponentials2 flow;drift terms logarithmic;particle flow;gradients forward probability;flow control"}, "73c401e29cb83157bc6dfb33d5ce4364a7d2731b": {"ta_keywords": "training generative models;training generative;transfer diversity information;pretraining transfer diversity;introduction training generative;source domain pretraining;generative models limited;transfer diversity;generative models;diversity information source;diversity information;generative;domain pretraining transfer;domain pretraining;preserve relative similarities;loss reduce overfitting;reduce overfitting present;easily result overfitting;reduce overfitting;source target methods;diversity;source target;overfitting;distance consistency loss;large source domain;overfitting present;pretraining transfer;domain distance consistency;overfitting work;instances source novel", "pdf_keywords": "shot image generation;diverse realistic images;generating diverse realistic;generate diverse realistic;image generation;images efficiently adapted;images similar target;generating images;method generating images;generating images limited;correspondences images generated;resulting images similar;networks gans;resulting images efficiently;generative models visual;realistic images propose;photorealistic domains;shot image;extensive results photorealistic;domains resulting images;networks gans method;target domain preserving;algorithm shot image;efficient generative models;loss relaxed realism;generative adversarial networks;images generated source;framework shot image;adaptation source;image generation demonstrate"}, "4702bfd200ceb6de126a60afb4db9da5c413476e": {"ta_keywords": "backgrounduncertainty propagation deep;backgrounduncertainty;backgrounduncertainty propagation;improve automatic automatic;improve automatic;automatic automatic;automatic automatic automatic;automatic;propagation deep neural;deep neural networks;deep neural;propagation deep;neural networks;neural networks challenge;order improve automatic;neural;networks challenge general;networks;networks challenge;deep;improve;challenge general;propagation;order improve;challenge;general;challenge general population;population order improve;order;general population", "pdf_keywords": ""}, "b946ce2c3405969bf615bedc623845b0d3d9b010": {"ta_keywords": "transformer self attention;e2e automatic speech;transformer encoder introducing;self attention network;transformer encoder;attention network;recurrent neural networks;alternative recurrent neural;speech recognition;encoder introducing;method transformer encoder;automatic speech recognition;encoder introducing context;automatic speech;performance alternative recurrent;speech recognition ar;compute self attention;transformer self;encoder;recurrent neural;neural networks end;self attention methodswe;introductionthe transformer self;alternative recurrent;attention methodswe proposed;attention methodswe;attention network recently;neural networks;transformer;self attention", "pdf_keywords": "attention algorithm mocha;processing attentions automatic;e2e automatic speech;attentions automatic automatic;automatic automatic attention;mocha transformer decoder;automatic speech;automatic attention algorithm;algorithm processing attention;encoder;computing attentions;encoder introducing;mocha inference transformer;automatic attention;efficiently computing attentions;attentions automatic;soft attention computed;transformer self attention;algorithm mocha automatic;computing attentions fixed;inference transformer decoding;attention algorithm;speech recognition easy;transformer encoder introducing;attention algorithm able;transformer encoder;implement neural speech;input mocha;speech recognition ar;processing attention used"}, "3ad287cf3b17cb109bf991731d2c0dcf8b7db2b1": {"ta_keywords": "lingual morphological tagging;morphological tagging methodsmean;cross lingual morphological;morphological tagging improves;morphological tagging;models cross lingual;lingual morphological;work morphological tagging;cross lingual;resource languages lambs;language hrl;language hrl family;high resource language;cross lingual training;lingual;low resource languages;tagging improves performance;lingual training;tagging methodsmean age;lambs cross lingual;resource languages;resource language hrl;tagging improves;languages lambs cross;tagging methodsmean;languages lambs;resource language;languages;lingual training high;factoror graph models", "pdf_keywords": "neural morphological tagging;morphological tagging language;morphological tagging association;approach morphological tagging;morphological tagging improves;morphological tagging;morphological tagging low;morphological tagging task;tagging language task;tagging language;parameters morphological tagging;methods morphological tagging;morphological tagging important;method morphological tagging;work morphological tagging;morphological tags difficult;tagging combines neural;morphological tags correlated;morphological tags;morphological tagging article;tags correlated syntactic;syntactic properties morphological;sequence tagging combines;predicting syntactic;predicting syntactic traits;framework sequence tagging;sequence tagging;tagging association computational;knowledge neural morphological;tag sets syntactic"}, "27e1dbe9f7c71cd6cc1b0357f49aef497e572d09": {"ta_keywords": "create pseudo code;pseudo code generated;production pseudo code;code pseudo;code pseudo code;code corresponding pseudo;pseudo code;pseudo code pseudo;code generated automatically;corresponding pseudo code;pseudo code human;source code corresponding;code unfamiliar programming;pseudo code redundant;code written natural;source code;create pseudo;given source code;code generated;code human effort;source code unfamiliar;comprehension source code;backgroundpsuudo code written;backgroundpsuudo code;code corresponding;majority source code;unfamiliar programming;laborious create pseudo;unfamiliar programming languages;code unfamiliar", "pdf_keywords": ""}, "1abd1efae8c3849e28de926e52d166b7800965a1": {"ta_keywords": "graph embeddings aggregating;embeddings aggregating;especially graph embeddings;embeddings aggregating collection;rank aggregation known;specifically rank aggregation;unsupervised deep learning;graph embeddings;rank aggregation;aggregation specifically rank;unsupervised deep;deep neural representation;backgroundpreference aggregation;backgroundpreference aggregation specifically;aggregation known;incomplete rank lists;deep neural;recommendation systems inspired;recommendation systems;leverage unsupervised deep;rank lists;aggregation specifically;rank lists accordingly;specifically rank;aggregation known problem;deep learning;neural representation learning;embeddings;aggregation;collection incomplete rank", "pdf_keywords": ""}, "d513a3583bd168ee341ce3b26d54a4e4096da471": {"ta_keywords": "codes redundant requests;latency performance maximumdistance;centers analyze latency;md queue model;latency performance;redundant requests md;requests md queue;requests data centers;md queue;redundant requests data;based average latency;data centers;latency strictly;redundant requests;latency;latency strictly reduces;replication;analyze latency performance;data centers analyze;jobs replication;jobs replication based;service times negligible;average latency strictly;average latency;latency performance number;queue model;codes redundant;exponential service times;queue;service times", "pdf_keywords": ""}, "1d634d645bfe0b289fd6a2a0d9210b2a04c9237b": {"ta_keywords": "differentially private learning;differentially private learners;learners differentially private;private learners differentially;private staochastic gradient;private learning;differentially private;differentially private staochastic;strong differentially private;private learning seen;private learners;applying differentially private;sgd non language;private staochastic;private;learners differentially;staochastic gradient decent;gradient decent sgd;language models strong;learning seen limited;models strong differentially;staochastic gradient;large language models;large deep learning;decent sgd;learning models text;sgd;decent sgd non;differentially;learning seen", "pdf_keywords": "improved private learning;private learning tasks;simplifying private learning;introductiondifferentially private learning;private learning useful;private learning;private learning algorithms;method private learning;learning useful privacy;private learning problem;experimentally improved private;private learning seen;non private learning;private staochastic gradientdescent;private learning individual;efficient non private;method improved private;common tasks privacy;improved private;models modest privacy;differentially private staochastic;introductiondifferentially private;accuracy privacy;effective private;formal notions privacy;private fine tuning;privacy budgets enables;differentially private;differential privacy;method effective private"}, "5ee580fba44c6efb2a9b06f4c62de6b053db7784": {"ta_keywords": "reviewers reject highly;peer review;empirical risk minimization;peer review paper;reviewers major source;reject highly novel;inspired empirical risk;recommendations different reviewers;reviewers reject;inconsistency peer review;handful reviewers reject;empirical risk;reviewers;common handful reviewers;risk minimization;different reviewers major;reviewers major;handful reviewers;review paper;final recommendations different;risk minimization er;scores final recommendations;different reviewers;highly novel paper;final recommendations;criteria scores final;reject highly;inspired empirical;review paper present;mapping criteria scores", "pdf_keywords": "reviewer scores;aggregate reviewer scores;peer review;reviewer scores paper;recommendations capturing opinion;recommendations paper reviewer;reviewer overall recommendations;reviewer reviews;loss reviewer reviews;scores recommendations capturing;analyzing quality reviewer;recommendations reviewer;dataset reviews;scores associated review;reviewer reviews paper;quality reviewer relative;peer review important;overall recommendations reviewer;employ dataset reviews;minimizer loss reviewer;opinions consensus mapping;peer review paper;peer review combining;peer review approach;reviewer norm;loss reviewer;behavior peer review;reviews paper;evaluate overall recommendations;recommendations provide empirical"}, "45dcccef42ed09cfd2babb630c117e95136b35d1": {"ta_keywords": "purposebuilding universal dialogue;universal dialogue systems;language descriptions schema;dialogue systems;descriptions schema elements;dialogue systems seamlessly;convey schema semantics;universal dialogue;descriptions schema;natural language descriptions;seq2seq modeling uses;prompt format seq2seq;dialogue;schema semantics;seq2seq modeling;format seq2seq modeling;indirectly convey schema;systems descriptions indirectly;enable systems descriptions;schema semantics work;leveraged natural language;seq2seq;systems descriptions;convey schema;language descriptions;purposebuilding universal;descriptions indirectly convey;natural language;format seq2seq;semantics work propose", "pdf_keywords": "dialogue semantics schema;example dialogue semantics;dialogue semantics;service simple dialogue;automated automated dialogues;semantics single dialogue;automated dialogues;dialogue model;automated dialogues demonstrate;efficient dialogue model;simple efficient dialogue;labeled example dialogue;make dialogue;dialogue model model;able make dialogue;language descriptions schema;example dialogue;simple dialogue;approach decode dialogue;conveying service semantics;dialogue agents;natural language descriptions;simple dialogue effective;efficient dialogue;democratization dialogue agents;user make dialogue;dialogue slots demonstrate;descriptions conveying service;dialogue;payment service simple"}, "6b9c3f82a0c0fd62f8ae527126b118890cfd452d": {"ta_keywords": "learning learning;disease understood development;learning;etiology disease understood;disease understood;disease;new method learning;method learning learning;etiology disease;method learning;etiology;understood development;development new method;development;understood development new;new method;method;development new;new;understood", "pdf_keywords": ""}, "1b06fe6ca5f4404e68b066cdea1a74a36e3e0e13": {"ta_keywords": "backgroundimproving robot success;camera detect success;backgroundimproving robot;rgb camera detect;depth camera perception;robot success detection;camera detect;rgb camera;camera perception methodswe;depth camera;camera perception;noisy depth camera;colored objects;using rgb camera;robotics tasks;certain robotics tasks;colored objects difficult;robotics;robotics tasks clearing;robot success;robotics tasks actions;detection using static;robot;camera;backgroundimproving;static object data;object data important;objects;object data;perception methodswe", "pdf_keywords": "objects robot use;og object robot;object robot trained;object robot;objects robot;robot success detection;processing objects robot;detecting actions robot;object robot stacking;egocentric object data;human robot interactions;improving robot success;trained robot observations;objects improve detection;detection success robot;robot observations outcomes;robot action outcomes;robot observations;camera robot;vision language data;robot interactions;camera robot stacking;robot action;robot use egocentric;detection stacking objects;robot;robot equipped egocentric;human annotations object;robot success;actions robot"}, "4cb3275ec95f4ad407f153aa9dc2d527bc2744e5": {"ta_keywords": "based speech synthesis;speech synthesis technologies;application speech synthesis;prosody synthesized speech;speech synthesis;synthesized speech speech;synthesized speech;speech synthesis makes;prosody synthetic speech;control prosody synthesized;humanmm based speech;synthetic speech;synthetic speech target;prosody synthesized;guide prosody synthetic;speech speech input;control prosody;speech input;speaker using speech;prosody synthetic;using speech preserving;speech input aimto;speech preserving original;proposed application speech;synthesis technologies proposed;based speech;speech preserving;users guide prosody;possible control prosody;using speech", "pdf_keywords": ""}, "34c9e3152c9a14af711994230d8a3909daeaa7cf": {"ta_keywords": "reviewers reject highly;peer review methodsin;peer review;empirical risk minimization;recommendations different reviewers;reject highly novel;reviewers major source;reviewers reject;handful reviewers reject;inconsistency peer review;inspired empirical risk;reviewers;common handful reviewers;empirical risk;different reviewers major;risk minimization;reviewers major;review methodsin paper;handful reviewers;final recommendations different;scores final recommendations;different reviewers;risk minimization er;final recommendations;review methodsin;highly novel paper;criteria scores final;reject highly;mapping criteria scores;criteria scores", "pdf_keywords": ""}, "46f88a062df05673ae0731aa17f9f9cc9d3e87bf": {"ta_keywords": "strategies management patients;management patients;patients;detailed description structural;description structural;structures;aforementioned structures;structural;structures implications;structural features aforementioned;structures implications development;structural features;description structural features;aforementioned structures implications;features aforementioned structures;strategies management;detailed description;new strategies management;importance detailed description;development new strategies;management;new strategies;article discuss importance;purpose article discuss;strategies;description;importance detailed;discuss importance detailed;article discuss;features", "pdf_keywords": ""}, "c4e83bfddb38642debb31097501aec8768f9020e": {"ta_keywords": "artificial intelligence research;intelligence technologies artificial;journal artificial intelligence;artificial intelligence technologies;artificial intelligence presented;technologies artificial intelligence;artificial intelligence;technologies artificial;intelligence technologies;application artificial intelligence;article journal artificial;intelligence presented article;intelligence research;journal artificial;describes application artificial;application artificial;intelligence presented;article describes application;article journal;article describes;technologies;intelligence;research;artificial;article;recent article journal;presented article describes;journal;presented article;describes application", "pdf_keywords": ""}, "f5813bb0b398007cae10ffdddeab221d4b9b0dc7": {"ta_keywords": "fault characteristics pumping;pumping storage power;characteristics pumping storage;storage power station;pumping storage;characteristics pumping;simulation technology fault;power station;dynamic simulation technology;fault characteristics;technology fault characteristics;pumping;dynamic simulation;report dynamic simulation;storage power;simulation technology;simulation;fault;station;technology fault;dynamic;report dynamic;storage;power;characteristics;technology;report", "pdf_keywords": ""}, "27de5fb45af9799ed0020c978fe3a3080c60401e": {"ta_keywords": "morphokinetics embryonic stem;based morphokinetics embryonic;morphokinetics embryonic;identify morphokinetics embryonic;model embryonic development;embryonic stem cells;new model embryonic;model embryonic;embryonic development presented;embryonic development;embryonic stem;model based morphokinetics;based morphokinetics;embryonic;morphokinetics;stem cells;identify morphokinetics;stem cells used;used identify morphokinetics;stem;cells;development presented model;cells used;cells used identify;model based;model;development;new model;development presented;presented model", "pdf_keywords": ""}, "bab35e88a510938d22cb28f2ecc6f6e189c3d8ea": {"ta_keywords": "speech recognition arab;arab speech recognition;recognition arab speech;speech recognition endtoensing;recognition endtoensing modular;speech recognition;recognition arab;approach speech recognition;arab speech;recognition endtoensing;endtoensing modular;arab;novel approach speech;endtoensing modular systems;recognition;modular systems human;endtoensing;approach speech;speech;modular;modular systems;systems human;systems;human;novel approach;novel novel approach;approach;article present;novel;novel novel", "pdf_keywords": "transformer auditory auditory;automated auditory auditory;e2e transformer auditory;3e2e transformer auditory;automated auditory;human speech recognition;results human speech;transformer auditory;speech machine;automated speech recognition;end speech recognition;automated automated auditory;speech recognition;speech machine ar;automatic speech recognition;speech recognition modular;speech recognition ar;speech recognition systems;speech recognition hmm;speech recognition discuss;process speech recognition;speech recognition hr;assess phonetic;speech recognition article;automated speech;speech data;approach speech recognition;development automated speech;ar human speech;automatic speech"}, "0132cb4384c3a6402353d8f349f8dd450d8ea4a2": {"ta_keywords": "normalize spelling historical;deep bi lstm;normalize spelling;approach normalize spelling;bi lstm;bi lstm network;deep neural network;language processing historical;lstm;deep neural;lstm network;spelling historical words;lstm network applied;processing historical documents;normalization;normalize;spelling historical;normalization algorithms;neural network architecture;language processing;previously established normalization;neural network;approach normalize;common approach normalize;historical documents complicated;introductionnatural language processing;neural;processing historical;normalization algorithms evaluated;historical documents", "pdf_keywords": "deep bi lstm;term memory networks;normalizing historical texts;historical language processing;bi lstm;character sequence labeling;lstm;short term memory;bi lstm network;prediction word level;deep neural;processing historical texts;sequence labeling historical;normalization using deep;softmax;historical words normalizations;normalization historical texts;normalize spelling historical;language processing historical;words normalizations character;deep neural network;lstm network;memory networks;align historical words;words normalizations;labeling historical texts;word level accuracy;sequence labeling;texts dataset diverse;historical spelling normalization"}, "39e734da43eb8c72e9549b42e96760545036f8e5": {"ta_keywords": "seeking qa dialogs;qa dialogs;information seeking qa;qa dialogs 100k;question answering context;dataset question answering;qa;seeking qa;question answering;text qua;present quac dataset;key clinical messagewe;quac dataset;clinical messagewe;quac;quac dataset question;clinical messagewe present;dialogs;messagewe present quac;answers questions providing;dialogs 100k questions;dialogs involve;answers questions;answering context;hiddenwikipedia text teacher;qua;freeform questions learn;dialogs involve crowd;text teacher answers;questions learn", "pdf_keywords": "learning dialog;learning dialog used;collected dialog questions;seeking dialog students;learning learning dialog;dialog students;learn answer dialog;seeking dialog methods;dialog students repeatedly;information seeking dialogs;seeking dialogs;dialog approach;user friendly dialog;friendly dialog approach;information seeking dialog;seeking dialog;dialog questions;dialog approach uses;dialogs;dialog used learn;question answering context;dialog progresses;collected dialog;dialog methods;modeling conversations challenging;student list dialog;dialog progresses ability;question answering;answer dialog;dialog methods present"}, "68b3905c2f82814294631f2ce29d5be4165e6b1f": {"ta_keywords": "deployment wireless relay;impromptu multihop wireless;wireless relay nodes;sequential deployment wireless;hop wireless relay;wireless relay network;wireless relay;multi hop wireless;relay nodes person;relay nodes;deployment multi hop;relay network;impromptu deployment multi;hop wireless;relay network studied;deployment wireless;multihop wireless network;relay;wireless network;based impromptu deployment;optimal sequential deployment;impromptu deployment;multihop wireless;measurementment based impromptu;multi hop;walks line random;nodes person walks;sequential deployment;wireless network connecting;random length known", "pdf_keywords": "relay optimal;optimal relay;relay selection strategies;optimal relay placement;placed relay optimal;wireless relay networks;optimal position relay;relay sink optimal;optimal place relay;nature optimal relay;wireless relay nodes;deployment wireless relay;relay selection;sequential relay placement;wireless relay;relay networks;function relay optimal;optimal state relay;relay optimal policy;implementing relay placement;implementing relay deployment;relay nodes;relay deployment;sequential relay;implementation sequential relay;relay placement;selection strategies relay;cost placing relay;relay rate deployment;relay placement policies"}, "dc8ebb6d9908542ae474dc2b21bfb6a14216f678": {"ta_keywords": "performance large multilingual;large multilingual translation;multilingual translation model;baselines pmi corpus;multilingual translation;large multilingual;pmi corpus;translation model;pmi corpus discice;translation model using;multilingual;benchmark;gpu 100 hours;gpu;benchmark standard;multiindicmetry sharedd task;corpus;translation;introduction far gpu;methods benchmark;far gpu;far gpu 100;single gpu;hours coatal multiindicmetry;gpu 100;leaderboard methods benchmark;performance;evaluate performance large;single gpu maximum;gpu maximum", "pdf_keywords": ""}, "010df54445ab5f47582eb668dc3488a3e46b55d3": {"ta_keywords": "unsupervised hidden markov;neural hidden markov;neuralizing unsupervised hidden;hidden markov models;neuralization unsupervised hidden;hidden markov model;neuralizing unsupervised;hidden markov;neuralization unsupervised;evaluate neuralization unsupervised;results neuralizing unsupervised;purposeunsupervised neural hidden;existing generative models;unsupervised hidden;generative models;neuralizing;neural hidden;markov models;markov model methodsin;markov models used;outperforms existing generative;purposeunsupervised neural;unsupervised;markov;present results neuralizing;neuralization;markov model;results neuralizing;existing generative;generative", "pdf_keywords": "generating lexical information;neuralizing unsupervised;segmental information language;unsupervised hidden markov;natural language processing;unsupervised models increasingly;unsupervised learning;unsupervised learning used;language models;word embeddings;framework unsupervised learning;generating lexical;neural language models;word embeddings present;unsupervised models;results neuralizing unsupervised;generate segmental information;source word embeddings;neuralizing unsupervised hidden;language processing;language processing 2015;model human sentence;report unsupervised neural;morphological context based;tag induction;unsupervised neural;method generating lexical;language models important;approach tag induction;goal natural language"}, "90b9d19af75c86f42865052c21305c70f884b5fe": {"ta_keywords": "multicommodity selfish routing;selfish routing game;selfish routing;routing game uncertain;equilibrium behavior multicommodity;introductionuncertainty multicommodity routing;multicommodity routing;multicommodity routing networks;routing game;congestion costs multiplicative;multicommodity selfish;behavior multicommodity selfish;routing;routing networks;routing networks challenge;equilibrium behavior;networks challenge policy;policy study equilibrium;user underestimates congestion;congestion costs;equilibria;underestimates congestion costs;equilibrium;game uncertain users;impacts equilibria;distinct impacts equilibria;behavior multicommodity;costs multiplicative;underestimates congestion;study equilibrium behavior", "pdf_keywords": ""}, "124385efee78010a4408329dffea4798f5a1ad47": {"ta_keywords": "translation systems wait;delay translation;delay translation process;large delay translation;speech translation systems;speech translation language;conventionalal speech translation;speech translation;utterance pause boundaries;utterance pause;translation process;translation systems;translation language pairs;translation process methods;useful speech translation;translation language;input utterance pause;starting translation causing;starting translation;sentence starting translation;translation causing large;translation causing;pause boundaries;pause;similar word order;word order;pause boundaries methods;language pairs;introduction conventionalal speech;delay", "pdf_keywords": ""}, "0eac6cbd150b1a7e9d757ccc871eea2bf0d89e42": {"ta_keywords": "interclass ordinal relationships;context ordinal regression;crucial classify;interclass ordinal;ordinal regression;classify;adequate interclass ordinal;ordinal regression attempts;truth label representations;ordinal relationships;label representations;learning adequate interclass;crucial classify class;order crucial classify;ordinal relationships methods;label representations encoding;context ordinal;classification problems categories;intraclass interclass relationships;ordinal;classify class;classification;deep neural;interclass relationships;ground truth label;interclass relationships explicit;classify class correctly;relationships categories;truth label;categories", "pdf_keywords": "ordinal regression classification;ordinal classification;regression ordinal;embedding ordinal information;classification problems ordinal;crucial classify;approach ordinal classification;ordinal regression attempts;frames ordinal regression;ordinal regression;ordinal regression ordinal;regression traditional classification;regression ordinal regression;way embedding ordinal;classify;demonstrate deeplabv3 useful;regression classification;ordinal regression traditional;deeplabv3 able efficiently;deeplabv3 useful;crucial classify class;embedding ordinal;classification tasks;use ordinal regression;predict classes natural;truth vectors encoding;approach ordinal regression;ordinal regression used;demonstrate deeplabv3 able;classification networks easy"}, "59f3e3cad309eb4965d67773d68bc2f91b2e376f": {"ta_keywords": "automatic speech recognition;audio automatic speech;speech recognition ar;automatic speech;automate processing audio;recognition ar auch;processing audio automatic;speech recognition;learning tools automate;documentation endangered languages;machine learning tools;audio automatic;endangered languages es;ar auch auchen;endangered languages;auchen challenging;auchen;develop machine learning;recognition ar;auch auchen challenging;automate;automate processing;auch auchen;tools automate;auchen challenging aspect;tools automate processing;learning tools;ar auch;documentation endangered;introduction documentation endangered", "pdf_keywords": ""}, "2ac98a28fdae4c01a89f09393c736e72445a4c4e": {"ta_keywords": "adaptive adaptive adaptive;speech loss understood;adaptive adaptive;speech loss;etiology speech loss;adaptive;development adaptive adaptive;development adaptive;speech;etiology speech;report development adaptive;loss understood;loss;loss understood report;development;understood report development;understood report;report development;understood;etiology;report", "pdf_keywords": ""}, "da660ca9e6fedefe815e305efd0dcd3bf9b4bb60": {"ta_keywords": "relationship extraction limited;relationship extraction;relation extraction;relation extraction process;approaches relation extraction;drivenn relationship extraction;relation extraction following;distant supervision recent;learn entity filters;seed relation extraction;knowledge bases extract;distant supervision paradigm;backgroundapplication drivenn relationship;limited distant supervision;supervision relations;extract substantial supervision;entity recognizers;distant supervision;following distant supervision;supervision relations real;supervision recent approaches;named entity recognizers;knowledge bases;large knowledge bases;entity filters;substantial supervision relations;relations real world;entity filters jointly;entity recognizers necessary;relations", "pdf_keywords": ""}, "2078d466766b6876d73ac1981392fa8bd2b9520d": {"ta_keywords": "bidding peer review;optimizing paper bidding;paper bidding peer;paper bidding;bidding peer;algorithm optimizing paper;peer review;bidding;optimizing paper;article algorithm optimizing;article algorithm;algorithm optimizing;aim article algorithm;review;optimizing;paper;peer;algorithm;article;aim article;aim", "pdf_keywords": ""}, "dcac1abd2ae5af180e51994a9c8334a6de915765": {"ta_keywords": "distributed gd;distributed gd arbitrary;gd arbitrary compressions;variants distributed gd;errorror compensated gd;stochastic methods;stochastic methods paper;compensated gd;arbitrary compressions delayed;gd arbitrary;compressions delayed updates;converging errorror compensated;compensated gd common;variants distributed;field stochastic methods;gd;arbitrary compressions;stochastic;analysis variants distributed;compressions delayed;distributed;gd common problem;gd common;field stochastic;problem field stochastic;compressions;delayed updates;errorror compensated;delayed updates methodswe;backgroundlinearly converging", "pdf_keywords": "method distributed stochastic;distributed stochastic method;distributed stochastic gradient;distributed stochastic;distributed distributed stochastic;methods distributed training;consider distributed stochastic;distributed stochastic model;stochastic gradient descent;distributed training;lemma complexity learning;biased communication compression;distributed training biased;unbiased compressions quantizations;method stochastic quasi;stochastic reformulation;sg arbitrary compressions;method efficiently distributed;communication compression;method asynchronous stochastic;method stochastic gradient;generalization stochastic gradients;distributed efficiently;methods stochastic;processing stochastic gradients;gradient methods stochastic;complexity learning;unbiased compressions;method distributed distributed;asynchronous stochastic gradient"}, "6a9394e5d49c1251c0fb6d7fb0c0813d26c6a907": {"ta_keywords": "semantic parsers;canonical utterances programs;introduction semantic parsers;semantic parsers map;meaning representations programs;utterances programs grammar;programs grammar paraphrasing;parsers;grammar paraphrasing utterances;canonical utterances;training examples canonical;paraphrasing utterances;map natural language;natural language utterances;utterances programs;utterances meaning representations;examples canonical utterances;introduction semantic;natural language;zero shot learning;parsers map natural;grammar paraphrasing;semantic;utterances;shot learning synthesizing;programs grammar;annotation efforts;meaning representations;shot learning;annotation efforts recent", "pdf_keywords": "zero shot parsers;parsers emerging domains;new semantic parser;semantic parsers;parsers emerging;semantic parser trained;semantic parsers emerging;semantic parsing important;semantic parsing;parser trained;parsers;automated paraphrasing;utterance domain knowledge;semantic parser;shot semantic parser;automated paraphrasing semantic;shot parsers;domains expressive grammars;generalization linguistic;expressive grammars;automated automated automated;parsing;paraphrasing semantic parsers;general generalization linguistic;zero shot semantic;automated automated;catered domain linguistic;expressive grammars used;compact grammar;semantic parser closes"}, "5446a8bbadc2ba2c575353b257f26abae27b3b2a": {"ta_keywords": "model user preference;preferences based comparisons;recommendation systems;model user preferences;point user prefers;user preferences based;recommendation systems learn;preference user represented;comparisons relative attractiveness;attractiveness different items;item user ideal;preferences based;items model closer;user ideal point;ideal point user;relative attractiveness;problem recommendation systems;preference;user prefers;user preference user;relative attractiveness different;user preference;model closer item;user represented point;preference user;user ideal;closer item user;point model user;items model;user preferences", "pdf_keywords": ""}, "f49ccfb32aad8e6893e8cbb037c1282572fe6e21": {"ta_keywords": "vulnerable adversarial samples;vulnerable adversarial;deep testing methods;deep testing;known vulnerable adversarial;detect various adversarial;adversarial samples generated;adversarial samples;kinds adversarial attacks;adversarial attacks;various adversarial samples;model mutationation testing;adversarial samples detection;adversarial;various adversarial;vulnerability dn;number deep testing;vulnerability dn systems;proposed vulnerability dn;kinds adversarial;mutationation testing;mutationation testing mmm;different kinds adversarial;networks known vulnerable;dnn models recently;dnn models;model mutationation;application dnn models;systems model mutationation;deep neural", "pdf_keywords": ""}, "62dc7bdae6700c4409e6d9773d6ecb5c0fab75a4": {"ta_keywords": "approximate dictionary searching;dictionary searching comparativative;dictionary searching methodsto;methods approximate dictionary;approximate dictionary;dictionary searching;indexing methods approximate;introductionindexing methods approximate;searching comparativative;searching comparativative analysis;indexing methods;infrequently updated dictionaryaries;indexing;searching methodsto improve;state art indexing;dictionaryaries;dictionaryaries used primarily;dictionaryaries used;introductionindexing methods;searching methodsto;updated dictionaryaries used;updated dictionaryaries;sequence based filtering;introductionindexing;retrieval;searching;primarily retrieval;filtering methods;dictionary;based filtering methods", "pdf_keywords": ""}, "02aebef93baeef3396f3cb4468a7054067f190c6": {"ta_keywords": "entities bayesian nonparametrics;named entities bayesian;entities bayesian;nonparametric bayesian;bayesian nonparametrics;nonparametric bayesian approach;extract structured database;structured database entities;bayesian nonparametrics aims;extract structured;present nonparametric bayesian;bayesian approach extract;canonical entities;named entities;approach extract structured;structured databases;entities text;database entities text;context structured databases;databases named entities;structured database;entities;structured databases named;characterize entity provided;characterize entity;entity;database entities;set canonical entities;entities fields;nonparametrics", "pdf_keywords": ""}, "f262ef2f50dfcaf07dc6598f22fb9b2470b37cf1": {"ta_keywords": "confident entity spans;entity spans;information extraction;entity spans linking;coreferences dynamic span;information extraction tasks;constructed span graphs;span representations using;dynamically constructed span;framework information extraction;dynamic span graph;spans linking nodes;span graph allow;span graphs;spans linking;dynamic span;share span representations;refine span representations;span graph;span graphs graphs;span representations;refine span;iteratively refine span;relation types coreferences;linking nodes confidence;coreferences dynamic;graph allow coreference;constructed span;span representations aimto;spans", "pdf_keywords": "entity relation detection;relation detection tasks;relation extraction tasks;overlapping entity extraction;overlapping entity recognition;relation detection;relation extraction;entity spans linking;entity recognition relation;entity recognition;entity spans;neural relation extraction;recognition relation extraction;relation extraction neural;confident entity spans;entity extraction;information extraction context;information extraction tasks;information extraction framework;coreferences dynamic span;general information extraction;frameworks information extraction;information extraction;contexts soft coreference;results entity recognition;entities overlapped spans;integrating information extraction;entity recognition using;annotated overlapped spans;coreference relation links"}, "36906613dcef29263afe711f128da1fc916cbbee": {"ta_keywords": "kernelelized self attention;speech recognition aims;attention long sequence;speech recognition ar;speech recognition;attention sa based;automatic speech recognition;iself attention sa;introduction iself attention;ct based speech;automatic speech;based speech recognition;iself attention;end automatic speech;attention long;self attention long;attention sa;attention;context modeling capability;flexible context modeling;context modeling;self attention;recognition ar;applying gassian kernelelized;long sequence data;gassian kernelelized;based speech;gassian kernelelized self;recognition ar systems;recognition", "pdf_keywords": "speech recognition fundamental;speech recognition;speech recognition direct;automatic speech;automatic speech recognition;speech recognition form;models speech recognition;approach speech recognition;frame indexing attention;approach automatic speech;speech recognition ar;attention algorithm based;speech recognition article;attention algorithm;representation phonetic features;applied speech recognition;sequence data recognition;self attention algorithm;phonetic features long;speech recognition ability;development speech recognition;attention algorithm shared;frame indexing input;decoder models speech;acoustic models speech;end automatic speech;indexing attention statistically;features long sequence;models speech;phonetic features"}, "9a334566b79bc6c6906e2b5285d5ea50b9b99479": {"ta_keywords": "adversarial minimax game;adversarial minimax;learning representations invariant;formulated adversarial minimax;learning representations;learning meaningful representations;problem learning representations;representation learning;adversarial;minimax game analyze;formulated adversarial;minimax game;representations maintain content;representations maintain;representations invariant specific;representations;process formulated adversarial;meaningful representations maintain;representation learning process;representations invariant;meaningful representations;game analyze optimal;problem learning;methods representation learning;optimal equilibrium game;minimax;representation;game amounts maximizing;machine learning;optimal", "pdf_keywords": "adversarial minimax game;representation learning minimax;formulated adversarial minimax;adversarial minimax;learning representations invariant;invariant representation learned;adversarial game encoder;game encoder discriminator;learning minimax;learning representations;learning optimal discriminator;learn unbiased representation;representation learned;adversarial game;learning minimax game;discriminator optimal predictor;problem adversarial game;encoder discriminator predictor;learning problem adversarial;game optimal discriminator;learn representations;learn invariant representation;learn representations invariant;optimality minimax game;problem learning representations;formulated adversarial;representation learning;data representation learning;adversarial;problem adversarial"}, "939dfa4dec88ca167e6572904c4ad2fcbf726f48": {"ta_keywords": "edgeweights converges novel;euclidean gradient flow;edgeweights converges;function edgeweights converges;described gradient flow;curve space graphons;gradient flow;gradient flow technically;function edgeweights;gradient flow suitable;graphons;suitable function edgeweights;flow technically curve;graphons appropriately described;edgeweights;graphons appropriately;space graphons;euclidean gradient;space graphons appropriately;described gradient;appropriately described gradient;gradient;technically curve maximal;curve maximal;curve maximal slope;flow;novel continuum limit;curve space;flow suitable function;flow suitable", "pdf_keywords": "gradient flow graphon;gradient flows graphons;gradientient flow graphons;gradientient flows graphons;gradient flows graph;matrices gradient flow;algorithm gradient flow;flow graphons;analysis gradient flows;euclidean gradient flow;convergence gradient flows;analysis gradientient flows;edgeweights converges novel;flow concept gradient;iteration gradient flow;referred gradient flow;flow graphons simple;flows graphons;gradient flow metric;gradient flow;gradient flow concept;gradient flows;described gradient flow;gradientient flows;gradientient flow;networks described;flow gradient;gradient flow gradient;defining gradient flows;function edge weights"}, "c377bf3ae52dee4075c1e807de9c5579d553de22": {"ta_keywords": "nl dialogue systems;processing automatic dialogue;processing text speech;text speech semantic;speech recognition corpora;text speech;text speech processing;intertwining nl dialogue;speech spoken language;automatic dialogue systems;applications text speech;dialogue systems;speech processing automatic;automatic dialogue;dialogue systems topics;conference corpora texts;speech semantic processing;parsing text speech;text speech integrating;speech semantic;speech analysis;speech processing;conference corpora;texts transcription speech;recognition corpora language;nl dialogue;included speech recognition;spoken language generation;language resources speech;speech recognition", "pdf_keywords": ""}, "ea71f5f59727b63b8912c6db097ba811da41bf5b": {"ta_keywords": "stochastic nonlinear systems;model stochastic nonlinear;nonlinear model stochastic;stochastic nonlinear;flows measured nonlinear;particle flows;particle flows measured;corresponding particle flows;nonlinear systems;measured nonlinear model;nonlinear systems corresponding;nonlinear model;systems corresponding particle;particle flows fundamental;model stochastic;stochastic;case nonlinear model;nonlinear model results;measured nonlinear;nonlinear;case nonlinear;flows measured;particle;results corresponding particle;corresponding particle;flows;report case nonlinear;flows fundamental;flows fundamental step;systems corresponding", "pdf_keywords": "constraining stochastic systems;constraining stochastic;stochastic controlled dynamics;introducing constraints stochastic;approach control stochastic;controlling stochastic systems;constraints stochastic systems;stochastic systems predefined;method controlling stochastic;method control stochastic;constraints stochastic;stochastic controlled;stochastic path sampling;interventions constraining stochastic;stochastic control;stochastic systems challenging;approach stochastic controlled;stochastic control method;stochastic systems manipulating;control stochastic processes;controlling stochastic;control stochastic;avoids stochastic path;stochastic path;deterministic particle methods;iterative stochastic path;optimal control stochastic;solution avoids stochastic;novel stochastic control;stochastic systems"}, "5a26eeda7c2ca58c2d56f1d580fbbae9eb1a19cd": {"ta_keywords": "bounds approximating complexes;approximating complexes neural;approximating complexes;complexes neural networks;elliptic polydes dirichlet;linear elliptic polydes;neural networks parameters;elliptic polydes;complexity bounds approximating;polydes dirichlet;polydes dirichlet boundary;small neural networks;elliptic polyde;approximating solutions;developed approximating solutions;elliptic polyde representable;introductionparametric complexity bounds;parameters required approximate;approximating solutions linear;complexity bounds;introductionparametric complexity;prove elliptic polyde;solution scale polynomially;approximate solution scale;bounds approximating;neural networks methodsa;dirichlet boundary conditions;polyde representable small;polynomially input dimension;networks parameters", "pdf_keywords": "approximated neural networks;pdes neural networks;deep networks approximate;neural network approximations;coefficients approximated neural;neural networks approximatingwe;approximated small neural;approximated neural;networks approximate solutions;small neural networks;networks approximatingwe coefficients;dimensional elliptic pdes;parameters neural network;networks approximate;small neural network;parameters neural;network approximations;neural networks finite;equations pdes neural;networks finite parameter;power neural networks;complexity bounds neural;network approximations solving;gradient descent fundamental;neural networks introduce;neural networks;neural network depends;bounds neural;neural networkswe derive;dimensional elliptic"}, "7647a06965d868a4f6451bef0818994100a142e8": {"ta_keywords": "alguistic sequence labeling;sequence labeling;sequence labeling general;annotations train;annotations train models;named entity recognition;speech tagging;entity recognition;labeling general modeling;sufficient annotations train;speech tagging named;entity recognition recent;annotations;labeling;problems speech tagging;alguistic sequence;tagging named entity;labeling general;tagging named;obtain sufficient annotations;sufficient annotations;tagging;introduction alguistic sequence;recent advances neural;advances neural networks;neural networks;neural networks ns;neural;models;advances neural", "pdf_keywords": "sequence labeling tasks;term memory networks;linguistic sequence labeling;backgroundlinguistic sequence labeling;word level lstm;labeling framework lstm;language model neural;short term memory;sequence labeling framework;sequence labeling novel;model sequence labeling;sequence labeling task;labeling tasks language;named entity recognition;enhancing sequence labeling;entity recognition;entity recognition recent;sequence labeling;neural language model;recurrent neural networks;annotations train;model neural language;labeling novel language;sequence labeling effective;propose sequence labeling;effective sequence labeling;language model sequence;approach sequence labeling;tasks language model;speech tagging"}, "e65676b43338e914ad77afd0fd6ce4bef87943a1": {"ta_keywords": "singing voice conversion;convert singing voice;voice conversion technique;voice conversion methods;voice conversion;converted singing voice;quality converted singing;convert singing;voice preserving conversion;statistical singing voice;possible convert singing;speech quality converted;conversion accuracy singer;conventional statistical singing;converted singing;singing voice preserving;introduction statistical singing;statistical singing;singer speech quality;singing voice significantly;singing voice characteristics;singing voice;natural singing voice;singing voice various;voice preserving;compared natural singing;preserving conversion accuracy;voice characteristics arbitrary;conversion methods sv;voice characteristics", "pdf_keywords": ""}, "d29d33f3b92b447d6011606be41b64439a1da088": {"ta_keywords": "constraints online ai;behavioral constraints online;online agent learns;online ai;introductionincorporating behavioral constraints;online ai systems;behavioral constraints observation;behavioral constraints;constraints online;set behavioral constraints;agent learns;learned constraints;learned constraints guide;uses learned constraints;online rewards guiding;agent learns set;novel online agent;online agent;ai systems increasingly;ai;constraints observation uses;constraints;learns set behavioral;ai systems;constraints priorities;additional constraints;online rewards;constraints guide;constraints observation;rewards guiding", "pdf_keywords": "formulation contextual bandit;constrained contextual bandits;contextual bandit problem;bandit problem constraints;described contextual bandit;contextual bandit;contextual bandits;contextual bandit situation;whenthe contextual bandit;regret contextual bandit;bandit situation formalism;artificial intelligence ethics;bandit problem described;machine ethics;machine ethics research;bandit problem whenthe;contextual bandits bcb;constraints online agent;bandit problem;policy learned constraint;online agent learns;agent knows constraints;statistics bandits;bandits recommender;known statistics bandits;algorithm agent;bandit setting ideal;learning constraints;behavior constrained contextual;bandits recommender systems"}, "721d7c82b80f14246d353251837e1711824a9e60": {"ta_keywords": "ffield speech recognition;channel speech recognition;speech recognition unified;unified dereverberation beamforming;reverberation methods;far ffield speech;multi channel speech;ffield speech;dereverberation beamforming;dereverberation beamforming challenging;speech corrupted reverberation;reverberation methods paper;reverberation;speech recognition despite;end far ffield;speech recognition;speech recognition performance;corrupted reverberation methods;recognition unified dereverberation;channel speech;corrupted reverberation;far ffield;dereverberation module end;beamforming;dereverberation module;beamforming challenging;beamforming challenging task;integrate dereverberation module;degrades severely speech;multi channel", "pdf_keywords": "recognition multichannel speech;multichannel speech recognition;channel speech recognition;reverberant speech use;multichannel speech;multi channel speech;speech recognition single;development multichannel speech;use multichannel speech;speech recognition unified;speech use multichannel;novel multichannel speech;reverberant speech;speaker speech recognition;multi speaker reverberant;strategies multichannel speech;anraviral single speaker;source speech recognition;recognition single speaker;reverberation methods;speaker reverberant;reverberant conditions speech;speech recognition architecture;speech recognition speech;speech recognition;algorithms single speaker;recognition speech recognition;speech recognition framework;speech recognition explore;speech corrupted reverberation"}, "a45c3120c077994409093771077a2d16f77674c5": {"ta_keywords": "feficent transfer learning;transfer learning methods;efficient transfer learning;transfer learning;parameters pretrained model;parameter efficient transfer;pretrained model;parameters pretrained;finetune parameters pretrained;learning methods fine;learning methods fundamental;parometer feficent transfer;efficient transfer;step learning;fundamental step learning;learning methods;learning;feficent transfer;pretrained model prohibitive;transfer;step learning paradigm;learning paradigm;approaches finetune parameters;pretrained;learning paradigm non;non coding language;finetune parameters;methods fine tune;non coding;parameter efficient", "pdf_keywords": ""}, "62d17b6f6ad77fd71ef9954c7784700d5e316f1f": {"ta_keywords": "data adversary methodsa;analyze data adversary;adversary methodsa;language reflects private;data adversary;methodsa adversary;adversary methodsa adversary;adversary;privacy;language models;methodsa adversary exploit;making privacy;introductionnatural language;identities making privacy;adversary exploit;real life language;making privacy concerns;privacy concerns;phrases present training;language;introductionnatural language reflects;language models lack;phrases present;privacy concerns broad;context data collected;life language;extract training;life language models;adversary exploit tendency;tendency extract training", "pdf_keywords": "privacy preserving language;privacy context language;privacy use language;confidential information language;downstream nonlinguisticthe privacy;learning privacy;nonlinguisticthe privacy;guarantees privacy context;methods learning privacy;preserving notions privacy;privacy preventing context;preserving language models;preserve privacy meaningful;recognizing delineating confidential;notions privacy preventing;challenge privacy enhancing;nonlinguisticthe privacy risks;named privacy preserving;building privacy preserving;equivalent privacy existing;preserve privacy situations;existing called privacy;training language models;privacy preserving;designing privacy preserving;language models learn;context following privacy;privacy formal;privacy preserving ls;privacy defenses"}, "211a6838b9550d227ce81d0bec542ec5b70e290b": {"ta_keywords": "predict voting patterns;predict voting;predict future votes;votes prediction;votes prediction voting;prediction voting behavior;voting patterns politicians;roll votes prediction;patterns politicians voting;forecasting key political;prediction voting;politicians voting record;votes cast predict;politicians voting records;voting behavior politicians;behavior politicians voting;voting patterns;official voting records;voting records;voting record exists;fails predict voting;votes fails predict;patterns politicians;politicians voting;voting behavior;past votes cast;voting record;future votes;voting records united;votes cast", "pdf_keywords": ""}, "cf9fa9ebbefab1877aa7a501c888a8a618c31abb": {"ta_keywords": "content political blogs;political blogs;content political blog;political blog;political blog posts;political blogs identify;political blogs form;status political blogs;community discussed blog;blogs identify topics;blog posts aim;blogs form social;discussed blog;blogs identify;polarity content political;blogs;overview content political;blog;comment polarity content;discussed blog post;content political;political discourse aim;blog post;blog posts;predicting comment polarity;political discourse;blogs form;form political discourse;comment polarity;predicting comment", "pdf_keywords": ""}, "42605dca59a3aafe2e5b33741a98dad9ba117395": {"ta_keywords": "similarity reranking;based similarity reranking;similarity reranking case;personal information graph;measure entity similarity;walk based similarity;entity similarity;graph walk based;entity similarity viewed;graph walks;improving graph walk;graph walks induce;random graph walks;information graph;search graph;graph walk;reranking;information graph includes;personal information management;reranking case studies;similarity viewed tool;based similarity;performing search graph;represent personal information;similarity;reranking case;search graph result;messages terms persons;finite random graph;similarity viewed", "pdf_keywords": ""}, "2a94fa0de804b5efaae1a66f50c3ea96539c46b8": {"ta_keywords": "database drama conversations;dialog based;build conversational agent;constructing dialog;dialog example database;dialog based examples;drama conversations retrieving;conversational agent interact;goal dialog based;build conversational;constructing dialog example;filtering constructing dialog;conversational agent;dialog;non goal dialog;drama conversations;dialog example;example database drama;aim build conversational;database drama;drama television aim;conversations retrieving;conversations retrieving proper;examples drama television;goal dialog;conversational;conversations;based examples drama;drama television;examples drama", "pdf_keywords": ""}, "d0895dccd61c567034d197eecfa5d7d59332061f": {"ta_keywords": "distributed storage codes;regenerating codes distributed;stored storage rrna;storage rrna;storage rrna rrna;codes distributed stored;storage codes;regenerating codes;codes distributed;exact regenerating codes;nodes regenerating code;distributed storage;methods regenerating codes;storage codes allow;distributed stored storage;class distributed storage;rrna rrna;codes class distributed;rrna rrna points;rrna;regenerating codes class;distributed stored;rrna points;nodes regenerating;regenerating code;stored storage;storage;failed nodes regenerating;rrna points described;codes allow efficient", "pdf_keywords": "distributed storage codes;storage codes distributed;distributed storage code;storage codes optimally;storage code distributed;regenerating codes achieve;minimizing repair bandwidth;code distributed storage;regenerating code nodes;regenerating codes;regenerating code highly;data storage codes;regenerating codes codes;satisfying regenerating code;codes distributed data;regenerating codes mass;employing regenerating code;regenerating codes discuss;regenerating data storage;regenerating codes use;method regenerating codes;storage code;storage codes;linear regenerating codes;regenerating code data;associated regenerating codes;node repair bandwidth;order regenerating codes;code large files;codes distributed"}, "ffe6d7573bb2c4fbfac0cc474804b5b1734a1179": {"ta_keywords": "using contextual bandits;utilizing contextual bandits;contextual bandits behavioral;contextual bandits;bandits behavioral constraints;online movie recommendation;movie recommendation methods;constrained online movie;movie recommendation results;movie recommendation;movie recommendation conclusions;bandits behavioral;recommendation methods;constraints constrained online;constrained online;behavioral constraints constrained;bandits;online movie;recommendation methods using;utilizing contextual;behavioral constraints;results using contextual;recommendation results;background utilizing contextual;using contextual;contextual;methods using contextual;recommendation results using;constraints constrained;recommendation conclusions", "pdf_keywords": ""}, "40fc6e46f2921be346eacff86ce765ff5b28fbdd": {"ta_keywords": "perpetual swapp contracts;funding rates bitcoin;perpetual swap contracts;funding correlation bitcoin;rates bitcoin inverse;rates bitcoin;swapp contracts bitcoin;bitcoin exchange rate;bitcoin inverse perpetual;perpetual swapp;contracts bitcoin;perpetual swap;inverse perpetual swapp;bitmex funding correlation;swapp contracts;contracts bitcoin derivative;bitmex funding;inverse perpetual swap;bitcoin derivative akin;bitcoin derivative;futures margin funding;rates levied bitmex;exchange rate;swap contracts;correlation bitcoin;bitcoin;funding rates levied;futures;correlation bitcoin exchange;margin funding rates", "pdf_keywords": "granger causality test;based granger causality;contracts based granger;granger causality;funding rate granger;bitcoin derivative markets;granger causality results;funding rates bitcoin;forecast trends bitcoin;granger cause price;perpetual swaps bitcoin;rate granger cause;trends bitcoin derivative;followed granger causality;currency fluctuations general;predicting price bitcoin;funding rates causality;bitcoin useful predicting;test conditional heteroskedasticity;bitmex funding rates;trends bitcoin;currency fluctuations;funding rate exchange;fluctuations currency;prove bitmex funding;rate granger;markets graph bitcoin;margin lending bitcoin;model fluctuations currency;based granger"}, "f7c9521dcd80127d6d4a72fb407e81a9c518ae8d": {"ta_keywords": "use knowledge inductive;knowledge inductive;knowledge inductive learning;methodsrelevant knowledge come;learn effective concept;concept definitions research;methodsrelevant knowledge;concept definitions generated;introductionthe use knowledge;inductive learning;design methodsrelevant knowledge;use knowledge;knowledge come forms;inductive learning critical;effective concept definitions;concept definitions reducing;good concept definitions;concept definitions;definitions research;effective concept;definitions research design;knowledge;examples descriptions advice;research design methodsrelevant;inductive;knowledge come;examples descriptions;definitions generated;learn effective;improving quality concept", "pdf_keywords": ""}, "737aff546a9112127d7a13a5b835e27a6e1e935e": {"ta_keywords": "speech recognition ar;networks automatic speech;automatic speech;speech recognition;automatic speech recognition;deep transformers outperform;introduction deep transformers;masked language model;autoregressive transformer;non autoregressive transformer;deep transformers;autoregressive transformer structures;short term memory;audio conditional masked;ar audio conditional;conditional masked language;audio conditional;term memory networks;memory networks;ar audio;autoregressive models;memory networks automatic;autoregressive models computational;transformers outperform conventional;structures ar audio;masked language;audio;transformers outperform;language model;transformers", "pdf_keywords": ""}, "39fdea1c34832f9bb1644bff81f53fb8ce6b2679": {"ta_keywords": "automated modeling methods;automated automated modeling;automated modeling;methods management disease;modeling methods management;automated automated;modeling methods;management disease;automated;development automated automated;development automated;modeling;disease demonstrate method;methods management;new methods management;report development automated;management disease demonstrate;disease;development new methods;method useful;method useful development;new methods;disease demonstrate;methods;method;report development;management;demonstrate method useful;useful development;useful development new", "pdf_keywords": ""}, "b58e80ad8c6e6844c41535080ccbdef06bce3b6e": {"ta_keywords": "chalet 3d house;3d house simulator;house simulator;chalet 3d;3d house;house simulator support;present chalet 3d;room layouts chalet;layouts chalet;layouts chalet supports;simulator support navigation;house room layouts;simulator;methodswe present chalet;chalet;3d;simulator support;room layouts;house configuration allows;house room;introductionwe present chalet;rooms 10 house;chalet supports range;house configuration;new house room;chalet supports;rooms;10 house configuration;navigation manipulation methodswe;present chalet", "pdf_keywords": "chalet 3d house;interact objects house;thechalet novel software;objects house software;objects house ischalet;chalet 3d;present chalet 3d;house simulator;objects household software;3d house simulator;house software tool;house software;simulator allows interactive;interactive interactions objects;interactive;room layouts chalet;natural language simulator;interact objects household;allows interactive interactive;interact objects;agents interact objects;simulator;house ischalet model;tool allows agents;3d house;house simulator support;interactive interactions;introductionwe present chalet;manipulate objects house;allows agents interact"}, "b0efb62aa2a435704a3412d592e73faf6be5ecea": {"ta_keywords": "evolving relationshipships literary;relationshipships literary characters;modeling relationships characters;character relationship;relationships characters methodology;inter character relationship;unsupervised modeling relationships;relationships characters;relationshipships literary;character relationship types;literary characters;learning evolving relationshipships;understanding character intentions;character intentions;fundamental understanding character;literary characters fundamental;inference inter character;characters fundamental understanding;understanding character;unsupervised modeling;characters methodology model;evolving relationshipships;modeling relationships;unsupervised learning evolving;character intentions goals;relationships dynamic phenomenon;relationships dynamic;characters methodology;unsupervised learning;model relationships dynamic", "pdf_keywords": ""}, "8b48c55808636a52699b38869df3eba9c8b999d9": {"ta_keywords": "statistical voice conversion;voice conversion;voice conversion promising;speaking methods laryngectomees;laryngectomees successfully implemented;statistical voice;methods laryngectomees successfully;alaryngeal speech produced;methods laryngectomees;speech produced alternative;speech communication alaryngeal;communication alaryngeal speech;time statistical voice;laryngectomees successfully;speech produced;alaryngeal speech;laryngectomees;speech communication;produced alternative speaking;silent speech communication;alternative speaking methods;voice;communication alaryngeal;speech silent speech;real time statistical;conversion promising approach;body conducted speech;speaking methods;alternative speaking;silent speech", "pdf_keywords": ""}, "a0b47c7162d1a3b04b27e27c9fadd2eabc4dab0e": {"ta_keywords": "translation etic task;machine translation etic;nationalist machine translation;translation etic;machine translation;etic task comprehensive;tuning word alignment;sed task exploring;etic talk tasks;techniques sed task;etic task;techniques sed;11 languagepairs;existing techniques sed;word alignment;word alignment combination;translation;sed task;nationalist machine;etic talk;languagepairs;total 11 languagepairs;etic;sed;introductionthe nationalist machine;decoding mergi;participated etic talk;nationalist;alignment combination morphology;risk decoding mergi", "pdf_keywords": ""}, "c3f9c1f702d0c3b35b99502674757b3d8e7dd352": {"ta_keywords": "naturalness preserving speaker;preserving speaker individuality;speaker individuality synthetic;speech preserving speaker;individuality synthetic speech;speaker individuality based;speech preserving;text speech preserving;speaker individuality methodsthe;native text speech;speaker individuality;preserving speaker;synthetic speech proposed;synthetic speech;correction prosodic phonetic;text speech;native ja speakers;prosodic phonetic;speech proposed methods;prosodic phonetic characteristics;speech uttered native;correction prosodic;partial correction prosodic;individuality synthetic;phonetic characteristics aimto;improve naturalness preserving;phonetic characteristics;native text;improving naturalness preserving;speech", "pdf_keywords": ""}, "89b2a1dc68a7232bc3c68eb4b3e597f99755f7fe": {"ta_keywords": "modeling textual compositionality;context text classification;question answering;factoid question answering;modeling textual;textual compositionality;input modeling textual;question answering typically;tasks like factoid;textual compositionality apply;text classification;textual;introduce recursive neural;context text;recursive neural network;recursive neural;question text;question text contains;words representations methods;neural network rnn;words representations;like factoid;text classification methods;rnn model;reason input modeling;bag words representations;contains individual words;answering typically use;like factoid question;text", "pdf_keywords": ""}, "0025b963134b1c0b64c1389af19610d038ab7072": {"ta_keywords": "order classify instances;desirable order classify;learning order instances;learning order;order classify;preference judgments;problem learning order;preference judgments statements;binary preference function;order instances;classify instances;instance ranked ahead;binary preference;order instances given;preference function indicating;rank instance;ranked ahead methodswe;preference function;instance ranked;effect instance ranked;desirable order;instances given feedback;advisable rank instance;means binary preference;classify instances consider;classify;applications desirable order;indicating advisable rank;order;ranked ahead", "pdf_keywords": "converselyweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighedweighed"}, "448406c38e739695b926d112b2b7aebd4e840322": {"ta_keywords": "introductiononline meeting recognizer;meeting recognizer multichannel;meeting recognizer;multichannel speaker diarization;estimated automatic speech;speaker diarization methodswe;speaker diarization;voices estimated automatic;recognizer multichannel speaker;speech recognition ar;automatic speech;automatic speech recognition;speech recognition;conversation analyzer;analyzer group meetings;conversation analyzer group;voices estimated;automatically speaks obtained;group meetings;time conversation analyzer;estimate automatically speaks;multichannel speaker;speaks obtained estimating;different voices estimated;automatically speaks;real time conversation;recognizer multichannel;meetings;group meetings goal;introductiononline meeting", "pdf_keywords": ""}, "06431546c21d7c2528aaa170c2e1078e0a82d12e": {"ta_keywords": "trained multilingual models;pre trained multilingual;cross lingual transfer;lingual transfer emerging;trained multilingual;lingual transfer;lingual transfer aims;transfer language;multilingual models;multilingual models completely;transfer language exhibit;shot cross lingual;cross lingual;target languages;english zero shot;collect target languages;tuned transfer language;multilingual;target languages methods;lingual;pre trained models;english zero;fine tuned transfer;primacy english zero;languages methods zero;languages;transfer aims;transfer;transfer aims despite;languages methods", "pdf_keywords": "multilingual bilingual transfer;lingual transfer multilingual;transfer multilingual training;transfer multilingual;trained multilingual models;lingual transfer languages;lingual transfer english;bilingual bilingual transfer;trained multilingual;language transfer;transfer languages;bilingual transfer;transfer languages english;source language transfer;lingual transfers;transfer language;fine lingual transfers;pre trained multilingual;cross lingual transfer;multilingual language models;transfer program multilingual;performance multilingual training;multilingual training sets;multilingual training;results multilingual bilingual;lingual transfer emerging;bilingual transfer program;language transfer reinforcedwe;multilingual bilingual;multilingual models"}, "7570afa31c68e24fce1342b7d67c591787219bc1": {"ta_keywords": "summarization source documents;document summarization source;document summarization;extractive summarization coarsely;extractive summarization;multi document summarization;introductiongeneratingwikipedia summarizing long;summarization coarsely;summarizing long;summarization source;summarization coarsely identify;use extractive summarization;introductiongeneratingwikipedia summarizing;summarization;summarizing long sequences;summarizing;introductiongeneratingwikipedia;generating english;task generating english;generating english english;generate article;english articles approached;model generate article;english articles;english english articles;neural abstractive model;documents use extractive;generate article case;information neural abstractive;neural abstractive", "pdf_keywords": "extractive summarization coarsely;summarization source documents;summarizing information news;extractive summarization;document summarization;summarization coarsely;multidocument summarization source;summarization coarsely identify;headline generation task;larger headline generation;headline generation;multi document summarization;structure summaries significantly;summarizing information;use extractive summarization;summarization source;multidocument summarization;summaries summarization based;summaries summarization;summarization based tensorflow;summarization problem large;approached multidocument summarization;summarization based;structure summaries;approach summarizing information;document summarization problem;summarization;summarizing;approach summaries summarization;summaries significantly"}, "4f9a4afc0ba500d839f7ee245513af9b87add8be": {"ta_keywords": "learn audio visual;audio visual representations;audio visual instance;discrimination video audio;representations video audio;audio visual;learn audio;learning cross modal;approach learn audio;contrastive learning cross;visual representations video;modal discrimination video;video audio methods;video audio;introduction audio visual;cross modal discrimination;video audio vice;video audio method;self supervised learning;present self supervised;contrastive learning;representations video;discrimination video;audio methods;self supervised;visual representations;audio methods present;audio method;audio;discrimination cross modal", "pdf_keywords": "audio visual learning;learning audio visual;modality learning audio;visual audio representations;learning video audio;audio visual representations;video audio memories;learning audio;learn audio visual;learn visual audio;audio representations;audio memories training;representations video audio;learning audio audio;self supervised visual;optimized audiovisual correspondence;cross modality learning;audio visual correspondence;audio visual instance;audiovisual correspondence;audio contrastive learning;discrimination video audio;audiovisual correspondence task;audio memories;representations multiple audios;visual audio;optimized audiovisual;sufficient learning audio;learn audio;self supervised representation"}, "2fb54dfcb1a62deac6565e82f2a87919d33074da": {"ta_keywords": "convex functions patient;analyzing convex functions;method analyzing convex;analyzing convex;convex functions;patient ph naive;convex;functions patient ph;naive ph naive;naive ph;ph naive ph;ph naive;patient ph;functions;functions patient;ph;naive;method analyzing;analyzing;method;patient", "pdf_keywords": ""}, "5aa3c6ab6cc55c24bab224505e8ad5a4d9863706": {"ta_keywords": "learning attention historical;historical text normalization;normalization learning pronounce;text normalization learning;text normalization;normalization modern word;attention historical text;learning pronounce automated;learning attention;normalization learning;task learning;task learning mtl;introduction learning attention;learning pronounce;processing historical texts;multi task learning;training encoder decoder;pronounce automated processing;normalization modern;normalization;encoder decoder architectures;automated processing historical;pre normalization;pre normalization modern;decoder;learning mtl;training encoder;learning mtl architecture;relies pre normalization;novel encoder decoder", "pdf_keywords": ""}, "86db47e228167439f15ee320a8a81d386f529a0c": {"ta_keywords": "language based environment;introductionlanguage based environment;environment manipulation tasks;based environment manipulation;environment manipulation requires;environment manipulation;agents manipulate environment;language instructions challenging;natural language instructions;introductionlanguage based;manipulate environment;introductionlanguage;language based;following natural language;environments address challenge;natural language;manipulate environment following;language instructions;manipulation requires agents;requires agents manipulate;manipulation tasks;agents manipulate;lemon general framework;language;manipulation tasks resultswe;environments;based environment;environment;framework language based;general framework language", "pdf_keywords": "language based environment;natural language tasks;environment manipulation tasks;backgroundlanguage based environment;environment human language;strategy natural language;natural language promising;performance natural language;language tasks;lemon language based;backgroundlanguage based;based environment manipulation;language tasks predict;language tasks execution;propose lemon language;backgroundlanguage;natural language instructions;environment manipulation execution;manipulation tasks models;language gm systems;language instructions challenging;language promising;natural language;manipulation execution guided;environment manipulation requires;environment manipulation;lemon language;language promising approach;tasks execution guided;manipulation tasks"}, "2842c21e879ee581aa50704817454f21b539fc69": {"ta_keywords": "language expression emotion;twitter methods linguistic;english speakers twitter;linguistic research multilingual;senstiment hindi english;ofopinion senstiment hindi;expression emotion sentiment;senstiment hindi;hindi english speakers;language preference expression;linguistic research;hindi english;language preference;methods linguistic research;understanding language preference;hindi;research multilingual societies;speakers twitter methods;emotion sentiment;multilingual societies;research multilingual;emotion sentiment paucity;expression emotion;multilingual;linguistic;language expression;twitter methods;preferred language;multilingual societies indicated;language", "pdf_keywords": ""}, "a010b3aa83d7d80e52c84d5f239f940eb33df904": {"ta_keywords": "architecture automatic speech;speech recognition ar;modal data augmentation;encoders acoustic input;automatic speech recognition;speech recognition;data augmentation network;acoustic input architecture;automatic speech;encoders acoustic;data augmentation;augmentation network;trained using emphsymbolic;separate encoders acoustic;acoustic input symbolic;acoustic input;input architecture;recognition ar trained;input architecture utilizes;encoders;recognition ar;input sharing attention;attention decoder parameters;emphsymbolic input addition;utilizes separate encoders;using emphsymbolic input;augmentation;traditional acoustic input;emphsymbolic input;architecture multi modal", "pdf_keywords": "multilingual training decoder;decoder training using;training decoder;training decoder training;decoder training;train attention decoder;text train attention;data transcribed speech;attention decoder;decoder trained;training attention based;trained audio data;machine translation speech;attention based encoder;translation speech recognition;pretraining augmenting data;transcribed speech languages;decoder trained audio;trained audio;decoder networks;decoder trained developed;decoder decoder trained;training attention;network trained grapheme;speech processing;multilingual training;transcribed speech;transcribed speech depend;language models;speech speech processing"}, "f784ab218692364b9c8a1f8064809e4524116f3a": {"ta_keywords": "protocol secure byzantine;secure byzantine;tolerant decentralized training;secure byzantine tolerant;decentralized training;byzantine synbil attacks;byzantine synbil;byzantine tolerant decentralized;decentralized training emphasizes;bounds resistance byzantine;byzantine attackers;synbil attacks;byzantine tolerant;resistance byzantine synbil;novel protocol secure;novel protocol;presence byzantine attackers;protocol secure;byzantine;modeling presence byzantine;rigorously analyze protocol;protocol;key clinical messagewe;clinical messagewe;clinical messagewe propose;presence byzantine;synbil attacks marginal;attacks marginal communication;propose novel protocol;resistance byzantine", "pdf_keywords": "distributed training protocol;byzantine users training;algorithm distributed training;distributed training described;byzantinetolerant decentralized training;distributed training;distributed training publicly;use distributed deep;distributed deep learning;reduce distributed training;distributed training techniques;method distributed training;distributed deep;distributed training use;protocol secure byzantinetolerant;secure byzantinetolerant;modern distributed training;approach blocking byzantine;byzantine tolerant learning;method blocking byzantine;distributed training strategy;byzantine tolerant optimization;byzantine synbil attackers;byzantine users presented;deep learning workloads;novel distributed training;vulnerability distributed learning;data parallel training;distributed learning;secure byzantinetolerant decentralized"}, "2dd1504d54f8d7e01e1323a9f876f35bb86356da": {"ta_keywords": "policy nudging commuters;intelligent transportation;solutions intelligent transportation;agent based simulation;planners need simulation;demand mobility services;intelligent transportation technologies;nudging commuters;mobility services;road network;agent based;nudging commuters alternate;agent;demand mobility;road network city;rise deep learning;transportation;incentive policy nudging;personal demand mobility;policy nudging;network city mitigate;travel methodsin;planners;problem city planners;existing road network;city planners;deep learning models;planners need;deep learning;incentive policy", "pdf_keywords": ""}, "c9d65eee1b5df8ccda87c024b88e1b620099b316": {"ta_keywords": "language communication robots;natural language commands;humans instructions robots;communication robots challenging;grounded natural language;communication robots;instructions robots;instructions robots using;unrestricted natural language;robots;natural language;robots challenging;humans robots;language commands;introductionnatural language communication;robots instantiate framework;propose neural architectures;language communication;language commands collect;humans robots instantiate;robots instantiate;empirically testable algorithms;neural architectures interpreting;contextually grounded natural;gap humans robots;propose neural;architectures interpreting contextually;setting humans instructions;testable algorithms bridging;robots using", "pdf_keywords": ""}, "f6db40e1f0477d27a34240b2e11d6893b9e85b7b": {"ta_keywords": "afford air purifiers;affordable air purifier;air purifier alerts;air purifiers;air purifier;air purifiers protect;purifier alerts;purifier alerts users;healthy air affordable;hazardous air quality;create affordable air;change wildfires pollution;affordable air;air affordable;air quality;people afford air;air quality conditions;activate filters;wildfires pollution;purifiers protect unaware;activate filters universally;purifiers protect;wildfires pollution affected;hazardous air;afford air;severely hazardous air;time activate filters;healthy air;conclusionsusing healthy air;purifier", "pdf_keywords": ""}, "9a36d6b76b3b223aa877b4243e5fdfe5c998689e": {"ta_keywords": "use electromyography pump;electromyography pump;electromyography pump pump;use electromyography;electromyography;pump pump;pump pump pump;pump;use", "pdf_keywords": ""}, "3f59122d4cac12f27ad6ae379deefd9f3fa81f29": {"ta_keywords": "goals execution robot;execution robot;natural language command;robot;steps order robots;implied human instructions;level human instructions;instructions correspond behaviors;robots;execution robot key;robots useful;robots useful real;human instructions correspond;human instructions work;order robots useful;motions intermediate goals;goals implied human;human instructions;robot key feature;order robots;intermediate goals execution;sequence intermediate goals;command sequence intermediate;natural language;robot key;level human;language command sequence;goals execution;learning representations;language command", "pdf_keywords": "predict actions robots;visualizing robot actions;actions robots;actions robots challenging;robot actions;robot actions execution;learning robots;actions robots using;plans natural language;robots using deep;robot able predict;robotics learn;language communication robotics;visualizing robot;communication robotics learn;robot used predict;learning learning robots;learning robots using;robots important goal;predict actions;natural language commands;goals execution robot;approach visualizing robot;robot;task demonstrate robot;execution robot propose;execution robot;input development robots;robots;used predict actions"}, "0b2ff02ab23e5c9910b98fb87c4d58045dbe89ce": {"ta_keywords": "reverberant speech recognition;vaginal reverberant speech;reverberant speech;includes reverberant speech;speech recognition;speech recognition task;aproach vaginal reverberant;vaginal reverberant;introduced reverberant speech;task includes reverberant;discriminative training;discriminative training various;techniques discriminative training;reverberant;mixture model deep;anrrar techniques discriminative;recently introduced reverberant;deep neural networks;model deep neural;includes reverberant;gassian mixture model;deep neural;recognition task includes;recognition task methodsthe;introduced reverberant;techniques discriminative;model deep;recognition task;recognition;neural networks", "pdf_keywords": ""}, "d72a1579074a1a2bc500f257474144b1957d5166": {"ta_keywords": "coded computation framework;coded computation approaches;based coded computation;performing coded computation;coded computation;existing coded computation;coded computation impart;learning based coded;potential coded computation;performing coded;distributed computing;coded;challenges performing coded;based coded;computation impart resilience;systems existing coded;distributed computing systems;existing coded;computations support;computation framework overcome;linear computations support;computations support limited;computation framework;computing;non linear computations;computing systems;occur distributed computing;computing systems existing;computations requiring high;linear computations requiring", "pdf_keywords": ""}, "c5141ed9ed785a6a1df61b36883e6dfa19a59ff7": {"ta_keywords": "channel speech separation;speech separation;speech separation multiple;learning based separation;speech separation conducted;synthetic overlap datasets;based separation frameworks;separation frameworks;based separation;overlap datasets;single channel speech;overlap datasets necessary;separation multiple domains;separation multiple;separation;backgroundbuilding corpora single;synthetic overlap;quality synthetic overlap;backgroundbuilding corpora;channel speech;corpora single channel;separation conducted using;separation conducted;datasets necessary deep;separation frameworks resultswe;field read speech;backgroundbuilding;deep learning;deep learning based;read speech representative", "pdf_keywords": "speech separation evaluation;speech separation;generate mixtures utterances;channel speech separation;speech separation conducted;generating mixtures utterances;standard speech separation;mixtures utterances corpus;speech separation technologies;utterances speaker mixtures;speaker mixtures efficiently;datasets analysis speech;mixtures utterances speaker;mixture corpus;mixture corpus generate;mixtures utterances;corpus generate mixtures;overlap mixeder corpora;speakers large corpus;mixture data speakers;synthetic overlap datasets;learning based separation;speaker mixtures;set utterances corpus;wj0 overlap corpora;speech data;analysis speech data;utterances corpus;use mixture corpus;overlap corpora"}, "77000dba4b0638bb8f4222efcd731e040938c846": {"ta_keywords": "prediction driver interaction;prediction driver intention;prediction driver action;driver cognitive load;driving history prediction;reduce driver cognitive;driver cognitive;driver speech;intention driver speech;driver action intention;driver interaction;driver interaction car;use prediction driver;prediction driver;driver intention driver;intention driver;driver intention;presented prediction driver;driver speech example;driver action;based driving history;care based driving;driving history;interaction car health;history prediction driver;features vehicle history;meaningful features vehicle;driving;use prediction;interaction car", "pdf_keywords": ""}, "ab8be9e585e599db99d8451e63a2311d88ff9293": {"ta_keywords": "cache workloads collecting;caching extensively;memory caching systems;memory caching extensively;cache workloads;effectiveness memory caching;caching systems;industrial cache use;cache use cases;caching systems coverage;memory caching;industrial cache;use memory caching;cache use;world cache workloads;real world cache;caching extensively increase;cache;caching;spectrum industrial cache;world cache;services use memory;collecting production traces;production traces;workloads collecting production;modern web services;latency workload analyses;workloads collecting;workloads;workload analyses production", "pdf_keywords": ""}, "c612905cffc5a9aa9f0d8ac7ce1fd17f90413dab": {"ta_keywords": "modeling argumentative dialogue;argumentative dialogue;modeling argumentative;argumentative dialogue explicitly;predicting argument;dialogue explicitly models;view argumentation;argumentation methods;argument goal predicting;architecture modeling argumentative;argumentation methods present;predicting argument successfully;changes view argumentation;view argumentation methods;argumentation;dialogue;dialogue explicitly;argumentative;goal predicting argument;argument goal;context attentive interaction;models interplay opinion;attentive interaction model;challenger argument goal;attentive interaction;interplay opinion holders;present neural architecture;context attentive;argument;explicitly models interplay", "pdf_keywords": "modeling argumentative dialogue;modeling argumentative;argumentative interactions aimed;argumentative interactions;argumentation social applications;model argumentative interactions;discussion challengers;architecture modeling argumentative;ipate discussion challengers;predicting argument;model discussions change;comment discussion;argumentative dialogue;computational argumentation social;discussions change view;process productive argumentation;computational argumentation;discussion challengers try;views talking;discussions change;interaction comment model;argumentation social;model discussions;discussions;dialogue explicitly models;evaluate model discussions;productive argumentation;argumentative dialogue explicitly;discussion use;productive argumentation benefit"}, "5547eff5376c56358be56f8bcc3a4b6ce4600bb5": {"ta_keywords": "speech recognition ar;toolkits speech enhancement;robust automatic speech;model toolkits speech;toolkits speech;automatic speech recognition;deep learning toolkits;toolkits deep learning;ar toolkits language;speech recognition;automatic speech;speech enhancement microphone;speech enhancement;ar toolkits;general ar toolkits;recognition ar;toolkits available robust;available robust ar;robust ar;recognition ar techniques;language model toolkits;learning toolkits;microphone;recent robust automatic;toolkits deep;enhancement microphone;enhancement microphone array;major toolkits available;major toolkits;toolkits language model", "pdf_keywords": ""}, "f430c43018f17cabccd3a2e9258aff3da508afe1": {"ta_keywords": "gaze features patient;features eye gaze;eye gaze features;gaze features;features eye;visual features eye;eye gaze;features patient history;gaze;patient history;patient history history;features patient;present visual;article present visual;present visual features;history history;eye;history history history;history;visual features;visual;patient;features;article present;aim article present;present;aim article;article;aim", "pdf_keywords": ""}, "4b34a4cc5bc9defb0f530d61f9b0f843071e227c": {"ta_keywords": "menstrual hygiene;menstrual hygiene important;effects menstrual hygiene;menstrual hygiene excessive;introduction menstrual hygiene;excessive vaginal bleeding;hygiene excessive vaginal;vaginal bleeding;vaginal bleeding delivery;vaginal bleeding high;bleeding delivery;bleeding delivery methods;effects menstrual;hygiene excessive;excessive vaginal;introduction menstrual;bleeding;menstrual;hygiene;bleeding high;hygiene important;hygiene important prognosticator;vaginal;prognosticator excessive vaginal;address effects menstrual;india study;family health survey;india study included;states india study;national family health", "pdf_keywords": ""}, "39025112f6a40d8aae38f2e966bb27cbc35ea25d": {"ta_keywords": "systematic review literature;systematic review;results systematic review;systematic;review literature;present results systematic;results systematic;review literature topic;literature topic;purpose article;purpose article present;article;article present results;literature;article present;results;present results;topic;review;purpose;present", "pdf_keywords": ""}, "0db557c4315b1e08ef65ff15b96eb7630014bf72": {"ta_keywords": "digressions discussion detecting;discussion detecting;discussion detecting unnecessary;detecting unnecessary utterances;unnecessary utterance detection;utterance detection avoiding;utterance detection;utterances having dialogue;unnecessary utterances;having dialogue;utterances;dialogue intervene results;background unnecessary utterance;having dialogue intervene;unnecessary utterance;dialogue intervene;avoiding digressions discussion;dialogue;utterance;unnecessary utterances having;topic shifts performance;utterances having;topic shifts;word frequency topic;discussion possible methods;discussion possible;frequency topic shifts;discussion;detecting unnecessary;detection avoiding digressions", "pdf_keywords": ""}, "09a169c853e24b3a5196eefeab4c94eaac744cda": {"ta_keywords": "crowdsource political annotations;political annotations phrase;political annotations;introductiongovernmental ideologies identified;introductiongovernmental ideologies;task identifying political;crowdsource political;identify ideology text;elements crowdsource political;identifying political;ideology text focus;ideology text;ideologies identified;techniques identify ideology;ideologies;identifying political position;ideologies identified individual;introductiongovernmental;annotations phrase;subsentential elements crowdsource;identify ideology;annotations phrase sentence;annotations;elements crowdsource;crowdsource;sentence level methodswe;phrase sentence level;ideology;wordlists;words wordlists", "pdf_keywords": ""}, "3a72f1346f3cd41e14b45c7fba5259bc77357ed4": {"ta_keywords": "pac learnability methodsin;model pac learnability;clauses efficiently learnable;pac learnability;ary recursive clauses;efficiently learnable;ary recursive;learnability;cryptographically hard learn;recursive clauses efficiently;determinate ary recursive;learnability methodsin particular;learnability methodsin;learn programs unbounded;learnable;classes cryptographically hard;program classes cryptographically;hard learn programs;generalization class hard;recursive clauses;natural generalization class;recursive;program classes;classes cryptographically;efficiently learnable paper;natural generalization;cryptographically hard;hard learn valiant;learnable paper;showing natural generalization", "pdf_keywords": "learnability recursive logic;recursive logic programs;recursive clauses programs;learnability recursive;inductive logic programming;learning recursive programs;recursive logic program;recursive programs valiant;recursive programs hard;recursive programs provide;oneinductive logic programming;linear recursive logic;logic programs described;oracle learning recursive;limitations learning recursive;learning recursive clauses;recursive logic;recursive programs;expressed logic programming;clauses pac learnable;clause pac learnable;logic programs;learnable program consisting;logic programs present;learning expressed logic;determinate recursive programs;analyze learnability clauses;clause analyze learnability;logic programs presented;learnability clauses constant"}, "b62d63580b81a2cbb20c3c1593dd62d118e4cb07": {"ta_keywords": "models code generation;code generation;trained models code;reliability pre trained;code generation proposed;code generation framework;clinical messagewe propose;clinical messagewe;key clinical messagewe;pre trained models;improving reliability pre;models code;reliability pre;generation framework;code;improving reliability;substantially improving reliability;generation framework substantially;messagewe propose framework;trained models;clinical;pre trained;reliability;generation proposed;messagewe propose;key clinical;models;messagewe;generation;trained", "pdf_keywords": "code generation constraints;language code models;free code generation;models code generation;code generation;automatically generated grammar;programs constrained semantic;performance natural language;code generation framework;generated grammar automatic;code generation propose;programming language constraints;code models;natural language models;language constraints language;language processing constrained;natural language descriptions;generate programs difficult;development natural language;trained models code;program similarity;user natural language;optimization language code;based constrained semantic;api language grammar;processing constrained semantic;based similarity programs;generate correct code;generate programs;similarity programs improving"}, "af85c67a1f30f8359be1091234118492b511a088": {"ta_keywords": "pulmonary artery disease;diagnosis pulmonary artery;patients diagnosis pulmonary;pulmonary artery;diagnosis pulmonary;artery disease;pulmonary;patients diagnosis;artery;management patients diagnosis;diagnosis;management patients;patients;approach management patients;disease;article;article discuss importance;new approach management;management;new approach;importance new approach;approach management;article discuss;discuss importance new;new;approach;aim article discuss;discuss importance;aim article;importance new", "pdf_keywords": ""}, "cd9e1eac4c93a314254cf8a8682ed5f01b6a808f": {"ta_keywords": "neural approaches reasoning;deeper language understanding;generalizations knowledge base;reasoning rely embedded;embedded generalizations knowledge;distinguish plausible inferences;natural language processing;generalizing facts kb;knowledge base;approaches natural language;generalizations knowledge;plausible inferences;deeper language;rely embedded generalizations;kb generalizing facts;generalizing facts;natural language;language understanding;neural approaches;knowledge base kb;inferences;reasoning rely;language understanding particular;approaches reasoning;embedded generalizations;approaches reasoning rely;particular neural approaches;language processing;fail logical reasoning;neural", "pdf_keywords": "embededding query language;answers query embedding;query embedding;query language natural;emql embededding query;knowledge base queries;embededding query;query embedding qe;compositional logical queries;knowledge graph embeddings;query representation;develop query language;queries query language;important query language;query language;natural language query;queries represented;emql embededding;compute query representation;query useful tool;answer queries;query model feasible;query representation encode;logical queries;base queries neural;tothe query language;exactly logical queries;queries neural;reasoning application query;base queries"}, "9712ebfbc4f86c68403f64918463edad3e553ac6": {"ta_keywords": "sensor activation centralized;centralized tracking time;dynamic sensor activation;tracking process known;tracking time varying;tracking process;centralized tracking;dynamic sensor;activation centralized tracking;cyberphysical systems;cyberphysical systems article;problem cyberphysical systems;tracking;approach tracking process;time varying process;tracking time;process known distribution;sensor activation;problem dynamic sensor;active sensors;approach tracking;cyberphysical;present approach tracking;sensor;problem cyberphysical;sensors;common problem cyberphysical;activation centralized;varying process;number active sensors", "pdf_keywords": "algorithm centralized tracking;centralized tracking process;centralized tracking time;sensor activation centralized;activation centralized tracking;centralized tracking;sensor activation constrained;algorithms dynamic sensor;method stochastic sensor;algorithms tracking;algorithms tracking process;stochastic sensor;sensor networks;method sensor network;observations sensors algorithm;tracking process;sensor constrained problem;stochastic sensor selection;sensor constrained;sensor network;sensors algorithm;problem sensor constrained;sensing greedy algorithm;process sensor nodes;active sensing greedy;sensors optimization;precursor algorithms tracking;tracking time varying;dynamic sensor activation;sensors optimal"}, "873dff010c00f0601d6939324929eeabb1ddbd6e": {"ta_keywords": "secret sharing dealer;sharing dealer;secret sharing;problem secret sharing;sharing dealer does;algorithm disseminating shares;dealer participants;participants instead dealer;dealer participant;disseminating shares network;instead dealer participants;shares network;coordination communication network;dealer participant requiring;distributed algorithm;distributed algorithm disseminating;dealer participants form;secure message transmissions;network present distributed;transmissions dealer participant;communication network present;message transmissions dealer;consider problem secret;coordination communication;communication network;sharing;disseminating shares;distributed;based separate secure;separate secure message", "pdf_keywords": "distributed secret sharing;secret sharing network;secret sharing protocol;algorithm secret sharing;secret sharing scheme;secret share dissemination;secret sharing;secret sharing problem;secret sharing graphs;methods secret sharing;method secret sharing;higher secret sharing;sharing protocol;secret sharing node;problem secret sharing;distributed secret;secret sharing dealer;complexity secret sharing;algorithm secret share;secret sharing setting;secret sharing general;sharing scheme;secret sharing complexity;reshold secret sharing;sharing protocol called;shares secret participants;shares efficiently transmitted;sharing scheme present;sharing protocol takes;sharing dealer"}, "35cb2b9febada179689724c78dfe31d9fa3f74c4": {"ta_keywords": "azure storage;windows azure storage;azure storage cloud;storage cloud storage;cloud storage;storage cloud;use erasure coding;erasure coding;cloud storage provides;erasure coding paper;data cost storage;storage low use;storage;storage provides customers;storage provides;azure;low use erasure;cost storage;windows azure;use erasure;cost storage low;storage low;store provide durability;erasure;store seemingly limitless;durability data cost;store seemingly;ability store;context windows azure;data cost", "pdf_keywords": "storage codes scalable;existing erasure coding;modern storage codes;erasure coding fast;erasure coding;read erasure coding;storage codes;storage codes significantly;coding erasure coding;erasure coding design;3d storage codes;uses erasure coding;erasure coding implementation;erasure coding erasure;storage overhead reconstruction;reconstruction codes way;erathe erasure coding;erasure coding designs;erasure coding process;coding erasure;azure storage;erasure coding algorithm;continuouslythe erasure coding;contextwindows azure storage;failures erasure coding;data storage;erasure coding paper;reconstruction codes provide;reconstructed data fragments;structuredd storage"}, "35c6bdab35e8fd4e982302b5270da3c8098c58b1": {"ta_keywords": "language instructions sequences;natural language instructions;instructions sequences diverse;instructions sequences;instructions specific subgoal;following natural language;architectures used instruction;carry natural language;compositions subgoals navigating;natural language;compositions subgoals;novel compositions subgoals;subgoals navigating landmarks;specific subgoal;sequences diverse subgoals;language instructions specific;language instructions;instruction following;approach subgoal;architecture following natural;instruction;instruction following struggle;approach subgoal modules;diverse subgoals;subgoals navigating;subgoal modules;compositions;subgoal type;propose modular architecture;modular architecture", "pdf_keywords": ""}, "32feca141fce06c6588b4014d27953a3fc25f19b": {"ta_keywords": "introduction igfffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff;igfffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff;introduction", "pdf_keywords": "interaction grounding language;model natural language;interaction natural language;action linguistically based;modeling language;dynamics model language;models natural language;learns physical commonsense;knowledge ground language;learning interaction natural;demonstrate natural language;ground language proposed;model linguistic;grounding language;learning physical commonsense;model language;language annotated natural;grounded world language;physical commonsense knowledge;physical commonsense interaction;ground language;language annotated model;modeling language form;model linguistic form;language annotated human;step human language;model learns grounded;language model dynamics;natural language;unified model linguistic"}, "d9c2242e3aa17db649c92d7d4db46509f3d203db": {"ta_keywords": "backgroundconstrained markov decision;markov decision processes;confidence reinforcement learning;reinforcement learning settings;markov decision;stochastic decision problems;backgroundconstrained markov;stochastic decision;reward function constraints;reinforcement learning;confidence reinforcement;learning settings reward;upper confidence reinforcement;class stochastic decision;decision processes;auxiliary cost constraints;priori transition kernel;cost constraints;reinforcement;decision problems;constraints described cost;select policy;cost constraints paper;problems decision maker;decision problems decision;decision maker;decision processes class;settings reward function;reward function;decision maker select", "pdf_keywords": "bandit problem constraints;confidence bounds reward;known reward constraint;learning reward optimal;reward optimal stochastic;reward constraint costs;learning optimal policy;bounds reward cost;confidence reinforcement learning;bandit problem;learn optimal policy;reinforcement learning clr;bandit problem single;learning probability reward;reward function constraints;reward constraint;linear program reward;reward optimal;algorithm learning reward;bandit problem demonstrate;robust linear programming;optimal stochastic policy;armed bandit problem;learn optimal;program reward regret;reward cost solve;robust linear program;optimal policy randomized;learning constraints;reinforcement learning optimal"}, "b990517fbbf4499861d7aa00407b0422874ab990": {"ta_keywords": "computer assisted interpreting;interpreters accurate translation;untranslated interpreter;assisted interpreting cai;translation speech language;assisted interpreting;interpretation translation speech;spoken word detect;tools analyze spoken;likely untranslated interpreter;analyze spoken word;siultaneous interpretation translation;interpreters accurate;interpreter reduce translation;translation speech;language real time;untranslated interpreter reduce;translation difficult terminology;challenges faced interpreters;interpreters;word detect terms;faced interpreters accurate;analyze spoken;interpreter;word detect;accurate translation difficult;terms likely untranslated;faced interpreters;reduce translation;speech language", "pdf_keywords": "untranslated terminology prediction;predicting untranslated terminology;automatically predicting terminology;predict untranslated terminology;terminology prediction supervised;predicting terminology;task predicting terminology;annotations untranslated terminology;terminology prediction;predicting terminology likely;predicting terminology simultaneous;terminology human translations;interpreters accurate translation;terminology simultaneous interpreters;annotations untranslated;intelligent computerassisted interpreting;untranslated interpreter;manual annotations untranslated;predicting untranslated;tools analyze spoken;capture words likely;capture words;interpreters analyze translation;spoken word detect;untranslated terminology;method predicting untranslated;human translations;word detect terms;predict untranslated;information source speech"}, "fb38451ff87254ac1ff15e79154ef958b4efb6a6": {"ta_keywords": "neural networks tutorial;neural networks fundamental;tutorial neural networks;neural networks fundamentally;neural networks provide;fundamental neural networks;practical tutorial neural;networks fundamental neural;tutorial neural;neural networks;neural networks neural;approach neural networks;networks neural networks;networks tutorial;networks neural;application neural networks;networks tutorial present;present tutorial neural;computational approach neural;networks fundamental;neural networks focus;approach neural;application neural;tutorial;fundamentally fundamental neural;networks fundamentally fundamental;fundamental neural;networks provide practical;networks;practical tutorial", "pdf_keywords": ""}, "e2d770b9ab691753a7ec1eb439185303118c8455": {"ta_keywords": "multilingual ai agent;maia multilingual ai;multilingual ai;build multilingual ai;multilingual artificial intelligenceligence;ai agent assistant;ai agent assistants;multilingual artificial;intelligenceligence agent assistant;ai agent;presents multilingual artificial;purposeproject maia multilingual;agent assistant maia;maia multilingual;agent assistant;agent assistant methodsthis;multilingual;artificial intelligenceligence agent;build multilingual;assistants eliminating language;technologies build multilingual;agent assistants;intelligenceligence agent;assistant maia;ai;paper presents multilingual;presents multilingual;assistant maia project;assistant;agent", "pdf_keywords": ""}, "c8f78575bfb642b2dab6ed542a683ade9527c17d": {"ta_keywords": "organ matching;methodsdeceased organ matching;organ matching unusual;kidney transplants;kidney transplants performed;decisions methodsdeceased organ;transplants performed australia;donations deceased patients;85 kidney transplants;transplants performed;algorithmic decisions;methodsdeceased organ;algorithmic decisions methodsdeceased;transplants;introductionmatching donations deceased;matching;deceased patients patients;deceased patients;kidney;organ;australia algorithms;algorithms given responsibility;patients waiting list;account 85 kidney;donations deceased;algorithmic;patients waiting;performed australia algorithms;decisions methodsdeceased;matching unusual", "pdf_keywords": ""}, "4cc07c367e4a1f932e159678ef711e1802edf49f": {"ta_keywords": "spotken intent prediction;tasks spotken intent;intent prediction;intent prediction example;purposedecomposable tasks;tasks spotken;spotken intent;purposedecomposable tasks complex;sub tasks spotken;task result models;differences sub tasks;intent;tasks allow insightful;speech recognition;understanding existing benchmarks;speech recognition natural;similar performance benchmarks;existing benchmarks;combines automatic speech;performance benchmarks unobserved;models similar performance;tasks;automatic speech;sub tasks;benchmarks unobserved performance;recognition natural language;performance benchmarks;existing benchmarks typically;benchmarks;sub tasks allow", "pdf_keywords": "dataset fluent speech;speech commands dataset;spoken intent prediction;fluent speech commands;model fluent speech;tasks using spoken;speech commands;splits fluent speech;training test sets;model based spoken;fluent speech;speech language processing;unseen utterance splits;gram overlap training;speech commands nipsin;snips datasets provide;dataset decomposable task;novel methods speech;sequence tasks directly;snips datasets;spoken language understanding;librispeech corpus based;utterance splits;based librispeech corpus;smartlights snips datasets;test set diverse;utterance splits desirable;language understanding models;librispeech corpus;sequence tasks"}, "8fa6b06cb96e5ae98dfff1c50f6940ef43af223f": {"ta_keywords": "storage overheads codes;errorasure codes reed;errorasure codes;reed solomon codes;solomon codes extensively;data centers;codes reed solomon;data replication methods;reliability data replication;data replication;bandwidth disk reconstruction;solomon codes;higher reliability data;deployed data centers;codes reed;data centers offer;overheads codes;overheads codes mandate;methods lower storage;background errorasure codes;codes extensively;disk reconstruction data;codes mandate;lower storage overheads;bandwidth disk;network bandwidth disk;replication methods;errorasure;storage overheads;codes extensively deployed", "pdf_keywords": ""}, "1941f5b053ccc80fa44980d38ac074145591b4ec": {"ta_keywords": "sentence embedding models;semantic sentence embedding;sentences bilingual data;language sentences vectors;semantics sentences bilingual;sentence embedding;sentences vectors closeness;sentences vectors;sentences bilingual;learning embeddings;learning embeddings properties;encode natural language;bilingual data;deep latent variable;embedding models encode;embedding models;shared sentences translation;closeness semantics sentences;language sentences;vectors closeness embedding;semantic divergent properties;sentences translation pair;shared sentences;semantic divergent;embeddings;semantics sentences;closeness embedding;embeddings properties;propose deep latent;deep latent", "pdf_keywords": "semantic sentence embeddings;common semantic embedding;incorporating sentence embedding;semantic embedding explaining;sentence embedding decoder;embeddings useful semantic;sentence embeddings useful;latent semantic vector;semantic embedding;sentences models semantics;natural language inference;sentence embeddings;language inference data;logit sentence embedding;common latent semantic;semantics sentences models;learn underlying semantics;embedding explaining;sentence embeddings important;sentence embedding;latent semantic;parallel sentences isolating;models natural language;separation common semantic;models semantics sentences;baselines unsupervised semantic;crosslingual language models;semantic similarity improve;isolating common semantic;separation parallel sentences"}, "553028f7f7c850371379c621e40d7d00e75303a6": {"ta_keywords": "modern multilingual models;multilingual models trained;multilingual models;high resource languages;low resource languages;multilingual;resource languages phenomenon;text multiple languages;multiple languages;languages phenomenon;modern multilingual;resource languages;introduction modern multilingual;languages phenomenon known;multiple languages hopes;trained concatenated text;languages;languages hopes conferring;trained concatenated;models trained concatenated;resource languages recent;languages hopes;languages recent work;negative interference;study negative interference;languages recent;known negative interference;high resource;negative interference methods;performance high resource", "pdf_keywords": "multilingual models alleviate;multilingual models demonstrate;interference multilingual models;languages multilingual models;languages monolingual models;multilingual models effective;negative interference multilingual;improve monolingual performance;multilingual models suggesting;interference improve multilingual;multilingual models;multilingual models present;models languages multilingual;monolingual performance;improve multilingual representations;monolingual models languages;modeling crosslingual;monolingual models;languages mitigate negative;bilingual models monolingual;multilingual models stimulate;capacity multilingual models;models effective bilingual;monolingual performance language;high resource languages;effective bilingual models;specific multilingual models;resource languages monolingual;learning model languageuniversal;multilingual neural"}, "1606dc1e966ad59dd96dc8e74722dca06b1f1a58": {"ta_keywords": "educational neuroscience useful;educational neuroscience;evolutionary causal;based educational neuroscience;evolutionary causal matrices;educational interventions;developing educational interventions;neuroscience useful;neuroscience;neuroscience useful tools;evolutionary;causal matrices markov;causal;developing educational;chain based educational;causal matrices;educational;tools developing educational;markov chain based;interventions;markov chain;based educational;markov;matrices markov chain;matrices markov;developing;chain based;chain;useful tools;useful", "pdf_keywords": "predicting educational interventions;interventions model;intervention models;interventions using modeling;educational interventions educational;influences educational interventions;educational interventions using;intervention models diverse;educational interventions;interventions educational;educational interventions outcomes;apply intervention models;educational interventions adolescents;interventions classroom contexts;predicting outcomes interventions;interventions classroom;interventions educational settings;interventions outcomes educational;effectiveness educational interventions;educational interventions etiology;impact educational interventions;educational interventions effective;learning interventions;interventions adolescents development;outcomes educational interventions;intervention experiments dynamics;developed interventions classroom;longitudinal outcomes interventions;effect interventions"}, "e3480d9395e692833b722b2e957d51139985f310": {"ta_keywords": "generative question answering;question answering making;question answering;pretrained language models;answering making;language models high;language models;versatile generative;generative;macaw built unifiedqa;versatile generative question;answering making available;results macaw built;successes pretrained language;present versatile generative;macaw built;generative question;pretrained language;results macaw;answering;macaw;community results macaw;built unifiedqa;unifiedqa built;built unifiedqa built;unifiedqa;models high quality;models high;unifiedqa built 45;models", "pdf_keywords": "question answering making;generative question answering;question answering systems;question answering;answering systems freely;answering systems;generate answers answers;generate answers;purpose question answering;answering making;pretrained language models;non based answering;user friendly automated;user friendly tool;answering making available;tool based language;based answering;based answering surprisingly;used generate answers;user friendly text;question types produces;versatile generative;provide user friendly;based answering released;simple useful tool;new language model;language models able;friendly automated;pretrained language;friendly automated automated"}, "6d19d73909ffaa6c94cae6a2535ed52d138cb63b": {"ta_keywords": "chat oriented dialog;build conversational agent;conversational agent interact;developing chat oriented;build conversational;conversational agent;twitter conversations methods;scripts twitter conversations;method build conversational;human human conversation;human conversation examples;human conversation;developing chat;dialog utilizing;method developing chat;conversations methods;chat oriented;oriented dialog utilizing;conversation examples movie;twitter conversations;dialog utilizing real;conversations methods aim;dialog;conversation examples;movie scripts twitter;agent interact users;interact users natural;scripts twitter;interact users;oriented dialog", "pdf_keywords": ""}, "d1678032a9eee94ec0a9c54fb008e1addc7213d4": {"ta_keywords": "parametric utility learning;robust utility learning;utility learning;utility learning framework;utility learning method;utility learning using;cfgls utility learning;game robust utility;energy efficiency social;feasible generalized squares;learning using heteroskedasticity;feasible generalized;efficiency social game;smart building energy;social game robust;building energy efficiency;constrained feasible generalized;cooperative continuous game;robust utility;heteroskedasticity inference adapt;building energy;framework parametric utility;heteroskedasticity inference;using heteroskedasticity inference;parametric utility;learning method estimator;energy efficiency;utility;efficiency social;game framework parametric", "pdf_keywords": ""}, "33cd5965745dc2e8bb8d0400d0b3c18d4e6369d4": {"ta_keywords": "cache workloads collecting;memory caching systems;caching extensively;memory caching extensively;cache workloads;caching systems;effectiveness memory caching;industrial cache use;cache use cases;caching systems coverage;industrial cache;memory caching;cache use;real world cache;use memory caching;world cache workloads;caching extensively increase;cache;caching;spectrum industrial cache;world cache;collecting production traces;services use memory;production traces;workloads collecting production;modern web services;workloads collecting;latency workload analyses;workloads;workload analyses production", "pdf_keywords": ""}, "0b79cb7fe16aa8b99d521989f39e49034394f701": {"ta_keywords": "web human computation;human computation;human computation applications;human computation new;human computation systems;types human computation;online gamers generate;web human;gamers generate useful;online gamers;gamers generate;games purpose;games purpose specifically;target online gamers;computation applications exist;intelligence solve computational;human intelligence;computational;growth web human;human intelligence solve;web perform complex;computation applications;games;web;harnessing human intelligence;algorithms growth web;computation systems;gamers;people web;computation new evolving", "pdf_keywords": ""}, "61cd4ffdaf2c0daa3d432ff9fecdd064d6e72886": {"ta_keywords": "capable reasoning nonl;benchmark logicnli;proposed benchmark logicnli;benchmark logicnli method;logicnli method;reasoning nonl;logic fo reasoning;language models ls;reasoning nonl work;logicnli;language models;logic fo;order logic fo;capable reasoning;logic;truly capable reasoning;order logic;method order logic;fo reasoning;fo reasoning new;recently language models;models ls;reasoning;reasoning new;models ls achieved;reasoning new proposed;nonli tasks;diagnostic method order;nonl;models", "pdf_keywords": "reasoning natural language;inference natural language;natural language inference;natural language tasks;natural language processing;language inference natural;logic commonsense reasonable;based logic commonsense;dataset commonsense reasoning;performance natural language;logic commonsense;methods natural language;commonsense reasoning approach;process natural language;ls commonsense inference;commonsense reasoning ls;issue natural language;natural language ls;natural language;effective reasoning natural;commonsense inference results;commonsense reasonable logic;capable reasoning natural;field natural language;language inference;commonsense reasoning;language inference ability;language inference important;ls reasoning ability;achieving effective reasoning"}, "a829d65de0cc19da49ad6b4a294dd31545aed2bb": {"ta_keywords": "use data technology;data technology tools;data technology;technology tools increase;productivity facilitate;productivity facilitate new;increase productivity facilitate;authors use data;tools increase productivity;productivity;technology tools;use data;increase productivity;new forms scholarship;technology;forms scholarship;scholarship;data;tools;facilitate new forms;facilitate;facilitate new;authors use;tools increase;new forms;use;forms;authors;increase;new", "pdf_keywords": ""}, "bc5e4b9fb3a40057df4994354a403202218d53a6": {"ta_keywords": "probabilistic parser;probabilistic parser marginalize;instance probabilistic parser;sequence semantic;language involve combinatorial;searched sequence semantic;models human language;parser marginalize exponentially;parser marginalize;semantic;marginalize exponentially trees;parser;human language;exponentially trees;human language involve;computational models human;probabilistic;exponentially trees make;problems instance probabilistic;dynamic programming;instance probabilistic;optimal asymptotic runtime;computational;trees make predictions;asymptotic runtime;algorithms;employ dynamic programming;computational models;trees;predictions algorithms problems", "pdf_keywords": "semantically equivalent programs;program transformations;program optimization graph;equivalent programs graph;program transformations simple;transformed program heuristic;dynamic program ability;set program transformations;fast dynamic program;dynamic program based;semantics preserving transformations;improving dynamic programming;dynamic programming algorithms;programs graph classical;dynamic program program;semantics preserving;program heuristic search;programs graph;sequence semantics preserving;analysis dynamic program;dynamic program;program optimization;dynamic programming;typical dynamic programming;practice program optimization;search semantically equivalent;algorithm search;ability optimize program;novel algorithm search;programming algorithms"}, "ab17c315f7ee4fe69fde2f3d8ae0e30e4e2f3a2b": {"ta_keywords": "question answering systems;question answering;structured documents answer;documents answer complex;hop question answering;answering systems step;answering systems;dochopper uses query;structured documents;answer complex questions;heirarchically structured documents;documents answer;document combines retrieved;information document combines;systems step dochopper;document combines;multi hop qa;answering;uses query;step dochopper uses;step dochopper;information produce query;hop qa;dochopper uses;document;complex questions methodssimilar;information document;combines retrieved information;dochopper;qa", "pdf_keywords": "question hierarchical attention;query answering tasks;documents hierarchical attention;question answering systems;question answering;hierarchical attention useful;use hierarchical attention;tasks query answering;hierarchical attention optimize;query answering;answering tasks hierarchical;tasks hierarchical attention;hop question answering;hierarchical attention adopted;models hierarchical attention;hierarchical attention algorithm;hierarchical attention;attention useful tool;language models hierarchical;tool query answering;hierarchical attention successfully;task answering;queries long structured;task answering complex;pretrained language models;reading comprehension models;answering systems;approach task answering;structured documents hierarchical;novel hierarchical attention"}, "e54ffc76d805c48660bb0fd20019ca82ac94ba0d": {"ta_keywords": "pretrained language models;gradient descent algorithms;pretrained language;gradient descent;vanilla gradient descent;introduction pretrained language;strong regularization tune;descent algorithms strong;algorithms strong regularization;language models fine;language models;regularization tune model;hundreds millions parameters;regularization tune;millions parameters datasets;strong regularization;regularization;wide range language;descent algorithms;parameters datasets hundreds;tune model hundreds;models fine tuned;language understanding tasks;thousands labeled examples;introduction pretrained;datasets hundreds thousands;model hundreds millions;datasets hundreds;millions parameters;language understanding", "pdf_keywords": "pretrained language models;language models intrinsic;dimension pre trained;learning highly compressed;dimension generalization improves;language representations highly;language models effective;pretrained language;pre trained models;dimension common language;pre trained representations;common language models;language models;backgroundalthough pretrained language;compressed task representations;intrinsic dimension generalization;generalization bounds large;models intrinsic dimension;generating language representations;intrinsic dimension task;millions parameters datasets;language representations powerful;provide generalization bounds;training minimizes intrinsic;language models fine;compression based generalization;pre training minimizes;pre trained model;model intrinsic dimension;intrinsic dimension pre"}, "2a82a16bdb793dc388391be57d6424f0d5090513": {"ta_keywords": "papers scores used;scores papers;scores papers scores;ranking papers reviewed;papers scores;provide scores papers;peer review reviewers;peer review;scores used;scores usually elicited;provide scores;asked provide scores;ranking papers;introductionin peer review;scores usually;process scores usually;review reviewers usually;provide ranking papers;scores;review reviewers;reviewers usually asked;reviewers usually;ranking;provide ranking;process scores;additionally provide ranking;scores used area;ask reviewers additionally;ask reviewers;reviewers additionally provide", "pdf_keywords": "rankings peer review;rankings peer;ranking data peer;peer review data;datasets peer reviews;collecting rankings peer;review incorporates rankings;algorithm peer grading;algorithm peer review;ranking information scores;rankings scores;peer reviews;provided rankings scores;collecting rankings;ranking information;dataset peer review;ranking ranking data;reviewer provided rankings;data peer review;peer review;score pertaining review;effectiveness collecting rankings;rankings;integrate ranking information;ranking ranking;ranking;peer review based;incorporates rankings approach;ranking data;score reviewer"}, "188fd1373aefdbf564e90a76fed43e1b8b7052dc": {"ta_keywords": "management pheromones pheromones;pheromones pheromones;management pheromones;pheromones;occurrence plethora pheromones;plethora pheromones pheromones;pheromones pheromones major;plethora pheromones;pheromones major;pheromones major cause;strategy management pheromones;mortality;plethora;mortality world;morbidity;morbidity mortality world;occurrence plethora;mortality world development;morbidity mortality;major cause;cause morbidity;major cause morbidity;new strategy;cause morbidity mortality;strategy;management;world;major;strategy management;new strategy management", "pdf_keywords": ""}, "69e2d1f5374918111432fae23212c2759b1357c2": {"ta_keywords": "ranking pairwise comparisons;backgroundactive ranking pairwise;comparisons items ranked;sequential active ranking;ranking pairwise;item ranking;item ranking refers;active ranking;backgroundactive ranking;ranking set items;chosen item ranking;ranking;ranking refers;items ranked according;active ranking set;items ranked;ranking set;ranking includes special;scores notion ranking;ranked according probability;ranked according;ranking includes;pairwise comparisons items;noisy pairwise comparisons;comparisons items;according scores notion;ranking refers partitioning;ranked;notion ranking;pairwise comparisons", "pdf_keywords": "ranking sample complexity;pairwise comparisons ranking;sample complexity ranking;comparisons ranking;ranking items pairwise;noisy pairwise comparisons;comparisons items ranked;ranking algorithm estimating;ranking items sample;recovering ranking sample;ranking sample;sequential active ranking;complexity ranking log;models active ranking;ranking items;ranking problem pairwise;statistically sound rankings;complexity ranking;simple active ranking;active ranking algorithm;item ranking;comparisons ranking multi;active ranking;items ranked algorithm;ranking algorithm;pairwise comparisons items;ranking probability length;items pairwise comparisons;pairwise comparison models;ranking probability"}, "926d827aef568ed97431a7845c9a8138930c80fd": {"ta_keywords": "responses political information;affective responses political;reactions social sharing;political information hypothesize;social affective responses;affective reactions social;political information;reactions social;responses political;affective responses;information group sources;political information paper;social sharing behavior;messages group sources;sources differently information;social sharing;exploring social affective;information hypothesize people;social affective;background social affective;information outgroup sources;people treat messages;link affective reactions;link affective;sharing behavior subjects;sharing behavior;treat information group;sources differently messages;information outgroup;differently information outgroup", "pdf_keywords": ""}, "9b534639bcadc9ad232b338e760c523a4d74c8de": {"ta_keywords": "grammar descriptions consumption;descriptions language;grammar descriptions;descriptions language terms;creating descriptions language;descriptions consumption linguists;summarized grammar descriptions;introduction language complex;introduction language;language complex;understanding language;principles summarized grammar;language terms;linguistics;language learners manual;creating descriptions;linguists language;summarized grammar;language complex systems;systems word phrase;consumption linguists language;manual creation descriptions;grammar;sentence construction guiding;sentence construction;learners manual creation;language terms bias;creation descriptions fraught;phrase sentence construction;creation descriptions", "pdf_keywords": "automated linguistic;automated linguistic information;descriptions language terms;linguistic exploration description;creating descriptions language;descriptions linguistic;comprehensible descriptions linguistic;parser automated linguistic;automated syntax;linguistic questions classification;automated automated syntax;processing natural language;descriptions language;language question extract;grammar descriptions;descriptions linguistic phenomena;tool linguistic exploration;automatic parser;automated syntax analysis;extract comprehensible descriptions;predict answer linguistic;summarized grammar descriptions;readable assistance linguistic;tool linguistic;problem language classification;automatic parser automated;converting linguistic questions;powerful tool linguistic;question extract features;natural language"}, "987c5ad75d5092bed03e9f523aec00dc43bc17e4": {"ta_keywords": "urban air quality;traffic congestion smog;air quality jinan;traffic urban air;air quality quantitatively;road traffic characteristic;road network density;remote sensing traffic;impacts road traffic;parameters urban air;air quality;traffic characteristic;traffic characteristic parameters;analyzes impacts road;road traffic urban;characteristic parameters urban;traffic urban;congestion smog;traffic congestion;road area occupancy;road traffic;sensing traffic congestion;aerosol optical thickness;parameters urban;geographical weighted regression;quantitatively based aerosol;impacts road;density road area;sensing traffic;area occupancy intersection", "pdf_keywords": ""}, "89c2b3bfcc309ce16c85d2ab0c8cac5295400715": {"ta_keywords": "stacked sequential learning;sequential learning meta;sequential learning;tacked sequential learning;sequential learning scheme;meta learning algorithm;sequential learning methods;stacked sequential;learning meta learning;meta learning;new sequential learning;called stacked sequential;sequential stacking;learning meta;learning scheme called;learning algorithm arbitrary;sequential partitioning;learning scheme;learning algorithm;method sequential partitioning;problems sequential stacking;learning methods tacked;methods tacked sequential;sequential partitioning problems;sequential;learning methods;arbitrary base learner;base learner;tacked sequential;learning", "pdf_keywords": ""}, "9b263129548dc09369e8bc34560fe5bb6047fcee": {"ta_keywords": "greek electricity market;electricity market greek;electricity market;simulator wholesale market;natural gas transportation;transportation methods simulator;market greek regulatory;gas transportation;gas transportation methods;market operations dynamics;liberalization greek electricity;implementation simulator wholesale;market operations;natural gas constraints;gas constraints;scheduling based bids;simulator wholesale;scheduling;market greek;scheduling based;greek electricity;energy undertook design;participants natural gas;implementation simulator;design implementation simulator;day ahead scheduling;greek regulatory;ahead scheduling;greek regulatory authority;ahead scheduling based", "pdf_keywords": ""}, "9333d372ad3887e02029d2eab0dbc0c0478582c7": {"ta_keywords": "unsupervised speech tagging;unsupervised learning methods;unsupervised learning natural;speech tagging;unsupervised learning;natural language processing;backgroundevaluating unsupervised learning;resultsusing unsupervised speech;learning natural language;unsupervised speech;methods natural language;tagging;development unsupervised learning;annotated data learn;natural language;learning methods natural;unsupervised;language processing;backgroundevaluating unsupervised;standard resultsusing unsupervised;language processing tasks;resultsusing unsupervised;learning natural;manually labeled gold;learning methods;annotated data;data learn;manually labeled;development unsupervised;data learn model", "pdf_keywords": ""}, "35b376ad9e03e5e0b930c53a48817bfb5703108d": {"ta_keywords": "text style transfer;machine translation models;style transfer task;neural machine translation;leverage semantic similarity;semantic similarity metrics;machine translation;translation models;style transfer;semantic similarity;supervised models text;models text style;parallel corpora;translation models explicitly;text style;parallel corpora makes;training algorithms;explore training algorithms;corpora;lack parallel corpora;models text;training algorithms instead;tuning neural machine;neural machine;corpora makes;similarity metrics;fine tuning neural;train supervised models;tuning neural;style transmitted", "pdf_keywords": "semantic similarity outputs;semantic similarity metrics;leverage semantic similarity;semantic similarity metric;assess semantic similarity;measure semantic similarity;texts improved automatic;leveraging semantic similarity;semantic similarity source;style transfer leveraging;semantic similarity;similarity source sentences;corpus benchmarks metrics;similarity metric content;metric content reward;text style transfer;style transfer task;trained language models;style transfer models;automatic evaluation metrics;styles text generation;text significantly improved;corpus benchmarks;improved automatic evaluation;existing automatic evaluation;automatic evaluation;supervised models text;text generation;transfer leveraging semantic;automatic human evaluation"}, "c96970cfb1c13ae6dccc30de482ce6b0d4414f2b": {"ta_keywords": "predicate invention;version predicate invention;predicate invention instead;introductionpredicate invention;invented predicate;creating new predicates;predicates instead explicitly;soft version predicate;new predicates implicitly;predicates instead;new predicates;new predicates instead;predicates;predicates implicitly;introductionpredicate;predicate;introductionpredicate invention pi;chosen invented predicate;version predicate;invented predicate lead;predicates implicitly group;predicate lead;rules using structured;predicate lead error;closely related rules;structured sparsity regularize;rules using;related rules using;using structured sparsity;using structured", "pdf_keywords": ""}, "4fee3d5d476568deb971768f8a5191eb627309d0": {"ta_keywords": "stability differential nash;nash equilibria robustness;game dynamics linearized;differential nash equilibria;continuous games methods;game dynamics;player continuous games;continuous games;equilibria robustness variation;optimization landscape games;nash equilibria;differential nash;equilibria analyze stability;equilibria robustness;spectrum game dynamics;linearized local equilibria;games methods;gradient based dynamics;fixed points player;points player continuous;player continuous;local equilibria;bound spectrum game;local equilibria analyze;robustness variation agent;dynamics near fixed;characterizing optimization landscape;dynamics linearized;games methods introduce;optimization landscape", "pdf_keywords": "game jacobian symmetric;blockdiagonals game jacobian;stability potential games;game stable gradient;game game jacobian;optimization continuous games;game jacobian similar;decomposing game jacobian;game jacobian;gradientbased learning player;continuous games methods;nash equilibria robust;learning player continuous;stability learning present;diagonal blockdiagonals game;stability learning;game dynamics;spectrum game dynamics;learning dynamics;equilibrium game stable;optimization landscape games;sum games robust;differential nash equilibria;game dynamics prove;study stability learning;continuous games;learning dynamics general;learning player;differential nash;player continuous games"}, "5df0b8b80aecda1efdebac5d1ab7bcf94a88c68f": {"ta_keywords": "purposecontextualized word embeddings;trained contextualized embeddings;domain trained contextualized;word embeddings derived;word embeddings;contextualized embeddings;contextualized embeddings methodswe;trained contextualized;pre training domain;trained language models;pre trained language;purposecontextualized word;embeddings derived pre;domain trained;training domain;contextualized;domain specific corpora;intrinsically domain trained;training domain specific;corpora biomedical articles;purposecontextualized;language models;embeddings;tasks pre training;trained language;derived pre trained;language models ls;biomedical articles improves;specific corpora biomedical;embeddings derived", "pdf_keywords": "deep bidirectional language;trained deep bidirectional;trained contextualized embeddings;backgroundcontextualized word embeddings;contextualized embeddings probe;trained language models;domain trained contextualized;deep bidirectional;trained contextualized;word embeddings derived;contextualized embeddings;pre trained deep;word embeddings;trained biomedical corpus;use contextualized embeddings;non language modeling;pre trained language;domain trained;intrinsically domain trained;masked language modeling;trained deep;domain corpus;bidirectional language model;trained language;pre training domain;training transformer networks;contextualized embeddings methodthe;general domain corpus;domain corpus 3b;embeddings derived pre"}, "5930efbf01efa8944258b1c0f7349111702f779e": {"ta_keywords": "natural language processing;implementation natural language;corpora annotations data;corpora annotations;reading corpora annotations;language processing;natural language;annotations data structures;text representation;text representation extracting;constructs reading corpora;language processing requires;annotations data;machine learning components;corpora;augment text representation;annotations;reading corpora;representation extracting;representation extracting features;structures represent language;represent language constructs;represent language;learning components;creating data structures;features training machine;extracting features training;data structures;effort creating data;training machine learning", "pdf_keywords": ""}, "44268b5a208e8f48a5883bb12e3e80a13101e752": {"ta_keywords": "congestive heart failure;undergoing elective cardiomyopathy;kidney injury cki;heart failure hh;heart failure;elective cardiomyopathy;kidney injury;elective cardiomyopathy complete;acute kidney injury;cardiomyopathy complete pre;cardiomyopathy;cardiomyopathy complete;induced acute kidney;acute kidney;surgery creatinine data;congestive heart;severity hh cki;injury cki relationship;post surgery creatinine;creatinine data;injury cki;contrast induced acute;creatinine;surgery creatinine;objectives congestive heart;kidney;classification severity hh;factor contrast induced;risk factor contrast;factor contrast", "pdf_keywords": ""}, "96ed7a7da69d654668b35b50344debd44e87c1a1": {"ta_keywords": "topic identification spoken;topic identification;topic identification topic;language topic classification;introduction topic identification;topic classification;identification topic id;topic classification instead;method topic identification;topic shifts;topic id;identification spoken segments;variable topic shifts;topic id real;spoken segments;world unstructured audio;topic shifts broken;identification topic;unstructured audio;unstructured audio audio;language topic;acoustic modeling translation;variable topic;spoken segments low;instance variable topic;audio audio instance;audio instance variable;topic;audio instance;english language topic", "pdf_keywords": "topic identification spoken;topic identification speech;language topic classification;topic identification;improve topic identification;topic identification topic;topic identification use;application topic identification;context segments spoken;identify topics;processing spoken segment;learn topic classifiers;identification topic identification;identification spoken segments;introduction topic identification;context acoustic modeling;evaluation topic identification;topic classification;topic identification real;topic classifiers;identification speech wild;spoken segment representations;spoken segment;modeling context acoustic;learning speech segments;topic classifiers based;transcribed topic labeled;identify topics themes;information spoken segments;spoken segments"}, "b9c3e87bc09c4c6167a03a835c30b1c23bef7a40": {"ta_keywords": "question answering knowledge;answering knowledge bases;knowledge bases bqs;question answering;questions test distribution;answering knowledge;assumption training distribution;training distribution questions;test distribution achievable;studies question answering;knowledge bases;training distribution;sampling training examples;questions test;sampling training;randomly sampling training;distribution achievable desirable;test distribution;assumption training;distribution achievable;standard assumption training;true user distribution;distribution questions test;training examples enormous;user distribution;user distribution hard;questions;large scale bs;sampling;training examples", "pdf_keywords": "questions knowledge bases;knowledge bases bmcs;use knowledge bases;knowledge bases;knowledge base answering;access knowledge bases;knowledge bases bs;data knowledge base;knowledge bases use;user knowledge base;web knowledge base;knowledge base;downloadable knowledge base;knowledge base constructed;processing web knowledge;knowledge base classification;generalization dataset serve;analysis knowledge bases;information knowledge base;generating questions knowledge;entities knowledge base;field knowledge base;knowledge bases fundamental;generalization analysis knowledge;like sparql;queries like sparql;analyze data knowledge;like sparql consensus;answering field knowledge;data knowledge"}, "05b6be9aec266072669f6f287a846637eedf19b5": {"ta_keywords": "soil nematode community;soil necmatode community;biotic communities soil;soil microbial community;soil nematode;soil microbial;communities soil studies;function soil nematode;invasion affect biotic;soil necmatode;plant invasion affects;communities soil;goldenrod invasion diversity;plant invasion affect;invasion affects diversity;nematode community results;diversity function soil;warming alien plant;function soil necmatode;soil studies;soil studies focused;focused soil microbial;alien plant invasion;studies focused soil;plant invasion;biotic communities;affect biotic communities;nematode community;warming canada goldenrod;invasion diversity", "pdf_keywords": ""}, "449310e3538b08b43227d660227dfd2875c3c3c1": {"ta_keywords": "black box differential;deep neural;deep neural network;derivative hidden state;derivative hidden;neural network models;neural network;neural network output;solver continuous depth;layers parameterize derivative;parameterize derivative hidden;differential equation solver;differential;depth models constant;using neural network;sequence hidden layers;hidden layers;models constant memory;family deep neural;box differential;neural;state using neural;using neural;depth models;continuous depth models;hidden layers parameterize;continuous depth residual;box differential equation;differential equation;depth residual", "pdf_keywords": "depth neural networks;continuous depth neural;deep neural;depth neural;depth residual networks;learning differential equations;learning differential;deep neural network;residual networks;solver latent neural;neural networks important;neural network models;solver continuous depth;useful learning differential;neural network;neural networks;sequence hidden layers;continuous time neural;current knowledge neural;blackbox differential;continuous normalizing flows;time neural networks;neural algorithms;neural networks demonstrate;train neural;neural network output;model continuous depth;neural neural algorithms;neural networks machine;using blackbox differential"}, "ab1e5a3c5521b6204dc7c6f1fa72b88000bc30ee": {"ta_keywords": "messagewhen question answering;question answering systems;answering systems;question answering;precede answering;pipeline precede answering;answering systems deployed;identifying correct answers;answers passages;assistants typing questions;answers passages assuming;correct answers passages;clinical messagewhen question;key clinical messagewhen;answering;clinical messagewhen;formed question components;question components pipeline;engine translating questions;questions search engine;question components;typing questions;translating questions languages;assistants typing;qa;translating questions;questions languages;typing questions search;questions search;voice assistants typing", "pdf_keywords": "answering systems increasingly;answering systems;question answering systems;assistants typing questions;answers ability manually;question answering;downstream question answering;answering systems deployed;question answering research;manually translating questions;use question answering;question answering biomedical;approach question answering;modern qa systems;robustness automated automated;question answering conduct;assistants typing;questions accuracy model;backgroundwhen question answering;voice assistants typing;engine translating questions;typing questions;robustness automated;automated automated;automated automated automated;status question answering;typing questions search;questions search engine;tool understanding language;english questions accuracy"}, "5f609f252d8815c5fb660d83c0dc71af21ecf65d": {"ta_keywords": "keywords event monitoring;known keywords monitoring;keywords monitoring;known keywords streaming;event monitoring twitter;keywords streaming text;monitoring twitter finding;keywords monitoring methods;keywords streaming;keywords event;automatically finding noun;monitoring twitter;finding noun phrases;counting known keywords;twitter finding;ns keywords event;known keywords;phrases ns keywords;select known keywords;noun phrases;twitter finding ns;keywords;noun phrases ns;event monitoring;conversational nature twitter;ns keywords;detect sudden spikes;automatically finding;finding noun;event monitoring systems", "pdf_keywords": ""}, "33c691ca050e1806d44c08e55e63fcd7e555899a": {"ta_keywords": "positive unlabeled learning;classify unlabeled sample;positive negative classifier;classification case positive;negative classifier;positive unlabeled;classify unlabeled;unlabeled learning;unlabeled mixture;negative classifier generally;unlabeled mixture aims;considered unlabeled mixture;objectives classify unlabeled;background positive unlabeled;positive sample clean;unlabeled learning analog;clean negative sample;unlabeled sample;train unbiased positive;unlabeled sample train;sample clean negative;mixing proportions positives;instances positive class;supervised binary classification;positive class considered;classification case;classify;unlabeled;class considered unlabeled;positive class", "pdf_keywords": ""}, "a8ea980b63deaf1404cd9f539a575b4e7135466e": {"ta_keywords": "models prediction serving;prediction serving systems;prediction serving;parity models new;parity models;introduce parity models;machine learning models;machines cluster;parity;models prediction;inference models prediction;results introduce parity;deploy models prediction;serving systems queries;predictions performing inference;queries return predictions;return predictions;introduce parity;learning models primary;return predictions performing;prone slowdowns failures;learning models;run machines cluster;machine learning;prediction;cluster;machines cluster settings;tail latency;predictions performing;latency results introduce", "pdf_keywords": ""}, "fb7caddac20dca012f48c90b2e1e2383f7185051": {"ta_keywords": "use interpretability tools;existing interpretability tools;interpretability tools machine;interpretability tools interpretl;interpretability tools;interpretability tools uncover;use interpretability;use existing interpretability;existing interpretability;interpretability;observe use interpretability;tools interpretl;contextininterpretinginterpretinginterpretingability understanding data;scientistss use interpretability;tools interpretl implementation;contextininterpretinginterpretinginterpretingability understanding;interpretl implementation;interpretl;contextual inquiry 11;contextininterpretinginterpretinginterpretingability;interpretl implementation gaams;contextual inquiry;understanding data scientistss;contextual;tools machine learning;conduct contextual inquiry;data scientists use;study data scientists;data scientistss use;data scientists", "pdf_keywords": "existing interpretability tools;interpretability tools improve;interpretability tools widely;interpretability tools;interpretability tools aware;interpretability tools designed;interpretability tool;used interpretability tools;interpretability tools data;interpretability tools useful;interpretability tools interpretl;understanding interpretability tools;use interpretability tools;interpretability tools intended;interpretability tools used;interpretability tools implement;models interpretability tools;interpretability tools participants;interpretability tools turn;interpretability tools exploratory;interpretability tools identify;interpretability tool participants;interpretability tools field;interpretability tools depend;knowledge interpretability tools;interpretability tools rationalize;interpretability tools explain;interpretability tools conducted;familiarthe interpretability tools;suggest interpretability tools"}, "b1d8c868e1d6d4980ee2f8c50e6fc5e4e7027ca2": {"ta_keywords": "model character dialogue;dialogue modeling character;dialogue character relationship;character dialogue;character dialogue plus;dialogue plus character;character dialogue character;contexttelling stories multi;dialogue character;user dialogue modeling;dialogue modeling;multi user dialogue;contexttelling stories;modeling character relations;character relationship information;based story continuation;stories multi;story continuation;character relations;user dialogue;task model character;dialogue;character relationship;dialogue plus;story continuation resultsto;trains character dialogue;character relations aimto;stories multi user;modeling character;multi task model", "pdf_keywords": "model characterdriven storytelling;story continuation modeling;characterdriven storytelling;characterdriven storytelling using;tasks character prediction;character driven storytelling;character prediction story;storytelling automatically augmenting;story continuation model;character dialogue plus;automated storytelling;dialogue plus character;modeling character relations;character driven story;character dialogue;automated storytelling techniques;development automated storytelling;roleplaying various characters;predict character;driven storytelling automatically;storytelling focusing interactions;prediction story continuation;story playing role;predict character act;character prediction helps;driven story continuation;information character prediction;collaboratively telling story;modeling character;based story continuation"}, "e785441f5ccd6e4e29b3123e61121df5c65b88f7": {"ta_keywords": "introductionvariational autoencoders popular;introductionvariational autoencoders;deep latent variable;variational learning;autoencoders;accompanying variational learning;latent variables vaes;autoencoders popular;autoencoders popular combination;variational learning technique;posterior latent variables;model accompanying variational;latent variable model;model posterior latent;neural inference;deep latent;combination deep latent;vaes efficiently parameterize;latent variables;variables vaes efficiently;using neural inference;accompanying variational;neural inference network;inference network;posterior latent;variational;latent variable;variables vaes;practice vae training;vae training", "pdf_keywords": "variational inference training;variational autoencoders;variational autoencoder;variational autoencoders experience;variational autoencoder vae;introductionthe variational autoencoder;methodwe variational autoencoders;autoencoders experience posterior;deep generative models;development variational autoencoders;variational inference;variational autoencoders text;component variational inference;model accompanying variational;generative models;variational learning;train deep generative;deep generative;accompanying variational learning;training procedure automaticencoders;autoencoders;automaticencoders addresses posterior;models trained;inference training;generative models beneficial;automaticencoders;neural inference;autoencoders experience;optimization inference network;manually trained model"}, "f7a2f2ae829545ee992a2214b3600cf914544e22": {"ta_keywords": "sistuddent learning cognitive;cognitive skills observing;skills observing methodsthe;skills observing;performance sistuddent learning;students performance sistuddent;observe human students;sistuddent learning;idea sistuddent observe;human students solving;learning cognitive;learning cognitive skills;students performance model;human students performance;sistuddent observe;evaluate students performance;skills observing objectiveto;students solving problems;cognitive skills;students performance;replicate students performance;introductionpredicting students performance;observing methodsthe;observing methodsthe basic;students solving;sistuddent observe human;students performance novel;human students;performance sistuddent;basic idea sistuddent", "pdf_keywords": ""}, "9112be1801598125d463febb96a525227c32acc1": {"ta_keywords": "automatic differentiation weighted;automatic differentiation;framework automatic differentiation;structured loss functions;finite state transducers;structured loss;pruning transition models;new structured loss;prior knowledge learning;state transducers;pruning transition;learning algorithms;dynamically training;state transducers wfts;differentiation weighted finite;differentiation weighted;knowledge learning;used dynamically training;learning algorithms framework;loss functions turn;loss functions;weighted finite state;operations graphs;graphs operations;encoding prior knowledge;operations graphs framework;knowledge learning algorithms;graphs operations graphs;learning;combine pruning transition", "pdf_keywords": "automatic differentiation deep;differentiation deep learning;deep learning;automatic differentiation graph;automatic differentiation weighted;deep learning differentiable;deep neural;useful automatic differentiation;automatic differentiation;development weighted automata;weighted automata;deep learning present;deep neural network;work automatic differentiation;automatic differentiation framework;learning differentiable wft;differentiation deep;deep learning use;sequence level loss;library automatic differentiation;automatic differentiation using;automata wfts;finite state transducers;state automata wfts;finite state transducer;weighted automata important;structured loss functions;propose convolutionalwft layer;dynamically training;mode automatic differentiation"}, "4cf633d0893a1d3af97723ce1f2fae33c2a30043": {"ta_keywords": "similarity relations knowledge;relations fact distribution;quantify similarity relations;similarity relations fact;similarity relations;relations knowledge bases;fact distribution methodswe;quantify similarity;backgroundquantifying similarity relations;distributions entity pairs;knowledge bases;fact distribution;knowledge bases resultsspecifically;method quantify similarity;similarity tractable provide;similarity tractable;similarity;relations knowledge;probability distributions entity;relations fact;exact similarity tractable;exact similarity;entity pairs;computing exact similarity;backgroundquantifying similarity;entity pairs paper;distributions entity;conditional probability distributions;relations;pairs paper distributions", "pdf_keywords": "similarity relations knowledge;quantify similarity relations;similarity relations based;quantify relation similarity;similarity relation discovery;relations similarity;relational similarity;relations knowledge bases;similarity relations defined;relations similarity relations;relation similarity;quantification similarity relations;similarity relations;similarity relations considered;relational similarity text;evaluate similarity relations;knowledge incorporated similarity;relation similarity provide;relational similarity relation;relation discovery knowledge;relation extraction models;relation extraction;publications relational similarity;relation discovery;knowledge bases;distributions entity pairs;similarity relations function;representations relations similarity;distance similarity relations;similarity score entity"}, "9f1059006e4ba303f8945114eddadd50d58a9f3e": {"ta_keywords": "knowledge bases;fortensorflow large knowledge;knowledge base query;soft symbolic database;query language fortensorflow;large knowledge bases;knowledge base;knowledge bases bs;query language knowledge;language knowledge base;base query language;bs useful ai;database using differentiable;introductionneural query language;query language;symbolic database;soft symbolic;language fortensorflow;write neural models;accessing soft symbolic;symbolic database using;useful ai;useful ai tasks;database;gradient based learning;neural models;large knowledge;ai tasks difficult;accessing soft;base query", "pdf_keywords": "knowledge bases;introductionlarge knowledge bases;queries kb neural;knowledge bases ks;soft symbolic database;learning rules fundamental;learning rules;learning rules ability;kb neural;nql expression vector;learning rules unknown;approaches learning rules;relations nql;relations nql expression;entities relations nql;write neural models;soft symbolic;kb neural model;useful tool deep;knowledge form;matrix nql;nql expression widely;symbolic database;learn tothe nql;vector matrix nql;relations useful tool;methods learning rules;rules ability learn;accessing soft symbolic;matrix nql novel"}, "02b932416751674dc25353620a1df4b53c3a5f6f": {"ta_keywords": "predict transcriptions linguistic;model automatic speech;transcriptions linguistic annotations;transcribing annotating audio;automatic speech;annotating audio linguistic;automatic speech recognition;predict transcriptions;end automatic speech;speech recognition ar;audio linguistic information;jointly predict transcriptions;phonemic transcripts speech;transcripts speech pos;audio linguistic;speech recognition;transcriptions linguistic;transcripts speech;simultaneously transcribing annotating;speech pos tags;transcribing annotating;annotating audio;phonemic transcripts;information phonemic transcripts;transcriptions;sequence model automatic;linguistic information phonemic;linguistic annotations;transcripts;speech pos", "pdf_keywords": ""}, "811531c959b0543a8e7abe1e827770e36b96f817": {"ta_keywords": "speech s2s translation;speech speech s2s;speech s2s;s2s translation systems;speech languages mosts2s;emphasis estimation using;emphasis estimation;translate speech languages;level emphasis estimation;translation systems;models hmm emphasis;s2s translation;hmm emphasis translation;paralinguistic information emphasis;translate speech;translation systems combine;systems ignore paralinguistic;word level emphasis;speech languages;languages mosts2s;emphasis translation;paralinguistic information;languages mosts2s systems;translates word level;ignore paralinguistic information;emphasis translation translates;emphasis target;level emphasis target;speech speech;ignore paralinguistic", "pdf_keywords": ""}, "6c34b7b0441bff66cce2418d36acfd9776ad7bd2": {"ta_keywords": "existing rule learning;rule learning algorithm;proposed rule learning;rule learning systems;fast ective rule;rule learning;ective rule induction;rule induction existing;rule induction;ective rule;induction existing rule;learning algorithm;learning algorithm irep;existing rule;large noisy datasets;noisy datasets;machine learning proceedings;higher andc rules;machine learning;fast ective;learning systems computationally;noisy datasets paper;andc rules;benchmark problems;benchmark problems irep;benchmark;algorithm irep large;learning systems;induction;rules propose", "pdf_keywords": ""}, "d723630c585aa0e4084fdd6e71bc6586cfa30e9d": {"ta_keywords": "syntactic information prosodic;syntactic structure spoken;prosodic information;information prosodic information;prosody speech closely;introduction prosody speech;prosodic information pauses;prosody speech;information prosodic;spoken sentence analysis;syntactic information;annotation syntactic information;sentence analysis models;speech closely;annotation syntactic;manual annotation syntactic;introduction prosody;structure spoken sentence;prosodic;speech;speech closely related;related syntactic structure;syntactic structure;structure spoken;syntactic;prosody;closely related syntactic;related syntactic;spoken sentence;sentence analysis", "pdf_keywords": ""}, "81bc64ce5553798c058f25fe5bd537d4bed67aed": {"ta_keywords": "quantum dots luminescence;quantum dots grown;dots quantum dots;quantum dots;quantum dots quantum;pyramids quantum dots;70as quantum dots;dots quantum;dots luminescence;dots luminescence narrow;vapour deposition inxga1;deposition inxga1 xas;dots grown inverted;dots grown;deposition inxga1;dots;chemical vapour deposition;vapour deposition;inxga1 xas al0;luminescence narrow;pyramids quantum;luminescence narrow lines;inxga1 xas;emission energies;inverted pyramids quantum;tuning emission energies;xas al0;luminescence;state excited state;organometallic chemical vapour", "pdf_keywords": ""}, "a8c62c42509c45a708ba477b603ee3fb81c77056": {"ta_keywords": "dissemination false news;false news social;propagation false news;wide dissemination false;news social media;social media verifiably;real time false;news social;false news news;published social media;dissemination false;media verifiably false;automated real;social media;background social media;consume share news;social media set;wide dissemination;false news;posts published social;news news posts;enabled wide dissemination;news posts;share news;prevent propagation false;share news enabled;automated real time;media set competition;dissemination;verifiably false causing", "pdf_keywords": "detection false news;false news detection;news weibo social;false news weibo;detect false news;news posts weibo;news weibo;dissemination false news;tag false news;weibo social platform;false news implement;news detection use;news social media;false news posts;false news popularity;news detection provide;false news social;propagation false news;false news encourage;news detection;content social media;news detection approaches;artificial media social;posts weibo;news social;weibo accuracy classification;false news constructed;modalities social media;published social media;detection provide topic"}, "44775500a5380be3776e876aedc43921d42d8de9": {"ta_keywords": "urban mobility data;hidden markov model;urbanization process systematically;urban mobility;markov model sshimm;urbanization process;scale urban mobility;sharing hidden markov;hidden markov;urbanization;introductionthe increasing urbanization;increasing urbanization process;population scale urban;markov model;mobility data;people activities urban;increasing urbanization;activities urban;activities urban space;mobility data sheds;time series modeling;modeling people activities;scale urban;urban;markov;urban space recognized;state sharing hidden;urban space;mobility;state sharing", "pdf_keywords": ""}, "470bfbde1dc0ed6ca989957dcd551213720657c0": {"ta_keywords": "neural machine translation;translation systems require;machine translation;machine translation systems;translation systems;syntax model needs;syntax model;introduction machine translation;translation systems assume;model needs translation;knowledge requiring parses;question syntax model;requiring parses;grammatical understanding neural;attention mechanism decoder;syntax alleviates burden;translation methods;knowledge grammatical understanding;parses;explicit syntax alleviates;syntax alleviates;needs translation methods;incorporating explicit syntax;semantic knowledge grammatical;translation methods address;syntax;explicit syntax;systems require semantic;require semantic knowledge;requiring parses expensive", "pdf_keywords": "attention model translation;syntactic attention model;neural machine translation;syntactic attention;self attention translation;attention useful translating;attention translation;machine translation;machine translation nm;propose syntactic attention;translation models;treesneural machine translation;attention encoder;generate syntactic information;machine translation nmr;attention translation english;attention model;attention encoder neural;improve performance translation;conference machine translation;machine translation article;translation models model;generate syntactic;performance translation induction;translates inducing dependency;translation nm model;self attention encoder;syntactic information model;model translation;machine translation wmt17"}, "bc494b9c6d9602a69b76ab9ea0e95d348a2fce19": {"ta_keywords": "evaluation generated summaries;summarization research;reference evaluation summaries;evaluation summaries highres;generated summaries;progress summarization research;evaluation summaries;summarization research enabled;summarization;generated summaries inconsistent;summaries inconsistent;substantial progress summarization;summaries inconsistent difficulty;summaries highres;manual evaluation highlight;highlight based reference;progress summarization;summaries;evaluation highlight based;based reference evaluation;evaluation highlight;manual evaluation generated;reference evaluation;recent advances neural;highlight based;manual evaluation;approaches manual evaluation;advances neural network;expert readers methodsto;highlight", "pdf_keywords": "reference summaries evaluation;summarization systems evaluation;reference evaluation summarization;summarization dataset assess;summarization systems evaluate;evaluation assess summaries;evaluation generated summaries;summaries evaluation framework;evaluation summarization;human evaluation summaries;summaries evaluation;reference evaluation summaries;evaluation summarization hihres;summarization quality captured;dataset assess summaries;evaluation summaries challenging;generate reference summaries;recent summarization systems;summarization quality;summaries documents highlights;assess summaries;summarization systems;summarization natural language;evaluation summaries;text summarization results;summaries assess accuracy;automatic summarisation;quality summaries assess;evaluation summaries source;summarization dataset"}, "e2ffd0ea7aa9cebaafba4afaee3cbe78070c8aa2": {"ta_keywords": "speech based bayesian;classification variational bayesian;clustering speech recognition;based variational bayesian;bayesian approach speech;variational bayesian estimation;estimation clustering speech;variational bayesian approach;variational bayesian;approach speech recognition;clustering speech;speech recognition;speech recognition vbe;prediction classification variational;ms based variational;recognizes speech based;backgroundapplication variational bayesian;speech recognition paper;classification variational;state triphone human;based variational;approach recognizes speech;recognizes speech;bayesian estimation clustering;triphone human;triphone human ms;bayesian prediction classification;shared state triphone;bayesian approach recognizes;based bayesian prediction", "pdf_keywords": ""}, "3321c947a4a399803592f26879927e58f587fd74": {"ta_keywords": "predictions analysis crowddsourcing;algorithmic risk assessments;algorithmic risk;impact algorithmic risk;analysis crowddsourcing studies;analysis crowddsourcing;crowddsourcing studies methodsa;crowddsourcing studies;crowddsourcing studies aimthe;assessments human predictions;human predictions analysis;risk assessments human;predicting future arrests;crowddsourcing;arrests conducted resultsour;human predictions;laypersons tasked predicting;participants predict;future arrests conducted;future arrests;risk assessments;predictions analysis;follows participants predict;impact algorithmic;introductionthe impact algorithmic;risk;arrests conducted;resultsour key findings;conducted resultsour;predicting", "pdf_keywords": "predictions criminal recidivism;human predictions criminal;prediction arrests participants;predicting future criminal;predictions criminal;participants predicted offender;predictions arrest participants;predicted arrest participants;prediction arrests;predictions depend offender;survey predict arrest;participants predicted arrest;arrests offender algorithmic;arrests participants underperformed;se predictions participants;human predictions participants;human prediction participants;judgments crowdsourcing studies;incarceration likelihood predicted;impact crowdworkers predicting;predicts likelihood offender;offender algorithmic;crowdworkers predicting;prediction participants;algorithmic risk assessments;human predictions evaluated;participants judgments crowdsourcing;incarceration predictions arrest;judgments crowdsourcing;ai likelihood incarceration"}, "0554246ebb6c53e88d7ac2aabf0c96a91ad500a0": {"ta_keywords": "domain speech enhancement;channel speech enhancement;speech enhancement model;speech enhancement;speech enhancement experiments;multi channel speech;time domain speech;channel speech;domain speech;multi channel conv;channel conv;models conv tasnet;deep learning;multi channel;applying multi channel;single channel;conv tasnet;enhancement model;channel multi;single channel multi;time domain models;channel multi channel;enhancement;deep learning based;potential single channel;backgroundthe deep learning;domain models conv;enhancement model simulated;tasnet;enhancement experiments time", "pdf_keywords": "multichannel speech enhancement;channel speech enhancement;timedomain speech enhancement;speech enhancement ccc;domain speech enhancement;speech enhancement large;addition speech enhancement;speech enhancement;approach speech enhancement;speech enhancement fundamental;new multichannel speech;robustness multichannel speech;speech enhancement auditory;speech enhancement model;integrated multichannel speech;speech enhancement experiments;multichannel speech integrated;enhanced speech spectrum;noise noisespeech enhancement;speech integrated multichannel;integrating enhanced speech;multichannel speech;multi channel speech;multichannel speech integrating;channel speech recognition;speech integrating enhanced;noisespeech enhancement;speech spectrum noise;enhanced speech;time domain speech"}, "c2bd176f8f9c84f9ba52ffb8f8bd4e9299c0f0cf": {"ta_keywords": "quantile regression prediction;quantile regression;quantile regression approach;nonparametric quantile regression;multiple quantile regression;nonparametric quantile;prediction parking data;quantile;regression prediction parking;multiple quantile;prediction parking;paper multiple quantile;tool prediction parking;parking data;regression approach wind;parking data important;wind price tracks;nonparametric;parking;regression prediction;price tracks geefcom2014;regression;parking data paper;prediction;approach wind price;regression approach;price tracks;tracks geefcom2014;tracks geefcom2014 implemented;wind price", "pdf_keywords": ""}, "a9a7058b39768ece13608e31341cfb16c4faf2c3": {"ta_keywords": "fair machine learning;formulations fairness;formulations fairness conclude;different formulations fairness;fairness conclude;fairness conclude critical;fairness;fair machine;literature fair machine;machine learning ideal;recent literature fair;literature fair;ideal approach political;political philosophy;approach political;political philosophy recently;approach political philosophy;machine learning;political;learning ideal approach;algorithms reflect broader;shortcomings proposed algorithms;algorithms;algorithms reflect;learning ideal;proposed algorithms;proposed algorithms reflect;uncovered shortcomings proposed;philosophy recently uncovered;learning", "pdf_keywords": "addressing algorithmic injustices;approaches algorithmic fairness;algorithmic fairness;algorithmic injustices;algorithmic fairness consider;algorithmic fairness non;algorithmic fairness use;fairness metrics;fairness metrics aim;issue algorithmic fairness;algorithmic injustices development;algorithmic injustices article;fairness metrics limited;conceptualization fairness;algorithmic fairness types;works algorithmic fairness;fair machine learning;conceptualization fairness implications;fairness non ideal;fairness use statistical;fairness implications;justice development fairness;development fairness metrics;fairness consider challenges;fairness implications development;fairness types;review conceptualization fairness;approach fair machine;justice metrics;matters justice metrics"}, "156323f4d87af6cf105c97bf29d324c9e3bc8f92": {"ta_keywords": "speech translation systems;mm speech synthesis;speech translation;translation quality independently;translation mm speech;optimized improve translation;machine translation mm;speech synthesis;ar machine translation;translation quality;improve translation quality;speech synthesis general;translation systems;automatic speech;introduction speech translation;machine translation;translation systems consist;speech recognition ar;automatic speech recognition;components automatic speech;speech recognition;improve translation;minimize word error;word error rate;translation mm;mm speech;speech;synthesis general ar;optimization mm;proposed optimization mm", "pdf_keywords": ""}, "3cd4797725ca9cf954946ed5309e15ebab80b92a": {"ta_keywords": "conditional image synthesis;multimodal conditional image;multimodal conditional;introductionmultimodal conditional image;conditionsal image synthesis;image synthesis existing;image synthesis;process multimodal conditional;image synthesis fundamental;image synthesis frameworks;multimodal;conditional image;multimodal user inputs;leverage multimodal;generate images;generate images based;process multimodal;existing conditionsal image;multimodal user;leverage multimodal user;conditionsal image;introductionmultimodal conditional;segmentation sketch style;synthesis existing conditionsal;unable leverage multimodal;fundamental process multimodal;sketch style;frameworks generate images;synthesis existing;synthesis", "pdf_keywords": "experts generative adversarial;experts generative;experts generationerative adsversarial;generative adversarial networks;gan approach multimodal;fusion generative adversarial;generative adversarial;product experts generative;trained single modality;deep fusion generative;single modality trained;generative;modality trained;networks paoe gan;fusion generative;experts generator;experts generationerative;experts generator effectively;modality trained single;generate images conditioned;generationerative adsversarial networks;multimodal conditional image;synthesize images conditioned;generationerative adsversarial;gan framework generate;adsversarial networks paoe;modelthe multimodal conditional;gan framework synthesize;multimodal user inputs;adsversarial networks"}, "365e049ecb7299cc6925483127f2f2123a97c35f": {"ta_keywords": "change point detection;kernelel sample test;kernel selection sample;changes time series;detecting emergence abrupt;kernelel sample;kernel selection;practice kernel selection;approaches selecting kernels;problem kernelel sample;abrupt property changes;samples change point;selecting kernels;detecting emergence;challenging problem kernelel;kernelel;selecting kernels non;time series;selection sample test;time series challenging;samples change;emergence abrupt property;practice kernel;kernels;abrupt property;problem kernelel;success developed kernel;trivial practice kernel;developed kernel;change point", "pdf_keywords": "kernel sample test;existing kernel learning;kernelel sample test;kernel selection sample;detection change points;kernel sample;kernel kernel sample;driven kernel selection;polytest optimizing kernels;time series kernel;practice kernel selection;new kernel learning;kernel learning approaches;change point detection;kernel selection demonstrate;propose kernel learning;kernel method sample;data driven kernel;kernel learning framework;kernelel sample;automatic kernel learning;series kernel deep;data propose kernel;kernel learning;kernel selection;approaches selecting kernels;deep kernel dynamic;detection change;present deep kernel;kernel learning algorithm"}, "464a75c05a5ce709fc515a2577b43acc8e3d45ce": {"ta_keywords": "inference explanation regeneration;explanations questions assembling;explanation regeneration task;manual explanatory relevancy;2020 shared task;explanatory relevancy;2021 shared task;multi hop explanations;hop explanations questions;explanations questions;hop explanations;descriptionthe 2021 shared;task multi hop;explanatory relevancy ratings;regeneration asks participants;explanation regeneration asks;multi hop inference;relevancy;task data summary;asks participants;shared task data;supporting knowledge base;relevancy ratings augment;hop inference;shared task uses;task multi;explanations;introductionthe sharedd task;knowledge base;shared task", "pdf_keywords": ""}, "a6e61164e7b385cec0e12093bc270eafd3ef1dbc": {"ta_keywords": "activity recognition method;proposes activity recognition;activity recognition;user activities using;activities using labeled;user activities;activities using;activities using labele;users sensor data;users sensor;activity;acceleration sensor data;end user activities;gender users sensor;activities;acceleration sensor;sensor data;unlabeled acceleration sensor;user physical characteristics;sensor data obtained;labeled unlabeled acceleration;recognition method models;user physical;end user physical;user model;unlabeled acceleration;sensor;paper proposes activity;information end user;acceleration", "pdf_keywords": ""}, "d7c1bdafb51fe1a757604f9daeaea812f124320f": {"ta_keywords": "technology forecasting;makes technology forecasting;technology forecasting widely;technologies technology forecasting;nowadays technology forecasting;technology forecasting english;systems technology forecasting;technology forecasting multidisciplinary;technology forecasting inrussia;forecasting widely used;forecast trends future;forecasting widely;forecasting;forecast trends;sources order forecast;forecasting english;forecasting english russian;forecasting multidisciplinary field;forecast;forecasting multidisciplinary;different technologies technology;russian patent databases;technologies technology;order forecast trends;trends future;makes technology;technologies;state different technologies;patent databases citation;different technologies", "pdf_keywords": ""}, "fba7c0a51a6301ca4086a5ce59b1f13af9acad7f": {"ta_keywords": "ja morphological analysis;morphological analysis;morphological analysis ma;approach ja morphological;structured predictors using;structured predictors;ja morphological;morphological;similar structured predictors;structured approach ja;learning tagging despite;structured;similar structured;learning tagging;structure information learning;structured approach;clinical messagewe;clinical messagewe present;accuracy similar structured;key clinical messagewe;information learning tagging;predictors using feature;state art structured;tagging;structure information;ignores structure information;tagging despite lack;structure able outperform;tagging despite;ignores structure", "pdf_keywords": ""}, "5d07db93e6fbd9e10713a2f372131c777077062d": {"ta_keywords": "effort deep quantization;deep quantization;quantization bits;deep quantization lead;quantization bits significantly;introductiondeep quantization bits;deep reinforcement learning;quantization;introductiondeep quantization;discovering bitwidths;deep reinforcement;discovering bitwidths end;end deep reinforcement;bitwidth network encodings;decreasing bitwidth network;digital network computation;bitwidth network;network computation;bits significantly reduce;reinforcement learning;network computation storage;quantization lead;process discovering bitwidths;network encodings;bitwidths;decreasing bitwidth;network encodings arduous;bitwidths end;bits;reduce digital network", "pdf_keywords": ""}, "e63e1db25f33162cc6e498f983dc3f6e10c9867e": {"ta_keywords": "speech separation;introduction speech separation;speaker conditional chain;speech separation extensively;speaker conditional;speakers observation based;complex speech recordings;named speaker conditional;speakers observation;cocktail party problem;process complex speech;numbers speakers observation;speech recordings;variable numbers speakers;strategy named speaker;cocktail party;speech recordings methods;complex speech;speaker;tackle cocktail party;separation extensively;numbers speakers;named speaker;separation;speech;cocktail;separation extensively explored;speakers;party problem studies;recordings methods proposed", "pdf_keywords": "speaker conditional chain;model speech separation;based speaker inference;inference speech extraction;speaker inference;speech separation model;speaker inference speech;structure speech separation;model speech;approach speech separation;proposed model speech;speech separation processing;separation speech recognition;task speech separation;speech separation extensively;introduction speech separation;speech separation;continuous speech separation;analysis speech separation;speakers accuracy inference;speaker conditional;speech extraction modules;standard speech separation;speakerconditional chain model;model speech recognition;speech separation methods;speech separation speech;introduce speaker conditional;speech separation fundamental;new model speech"}, "c3d2c60e70cad17ea37cb116ab30e1239405dbdd": {"ta_keywords": "etiology patient;patient diagnosis etiology;diagnosis etiology etiology;diagnosis etiology;etiology patient unclear;etiology etiology;etiology;patient diagnosis;model patient diagnosis;diagnosis;model patient;new model patient;patient unclear report;patient unclear;patient;model;unclear report;development new model;new model;new;unclear;report;unclear report development;development;report development;report development new;development new", "pdf_keywords": ""}, "6a730aff0b3e23423a00cb3407eb04e7f6e83878": {"ta_keywords": "ibm alignment models;improving ibm alignment;models word alignment;word alignment statistical;ibm alignment;alignment models using;backgroundimproving ibm alignment;alignment models;word alignment;variational bayes ibm;bayes ibm models;alignment statistical;bayes ibm;ibm models word;improving ibm;variationational bayes aimim;ibm models;variationational bayes methodsa;using variationational bayes;alignment;variationational bayes;ibm;bayesian technique variational;variational bayes;models using variationational;technique variational bayes;aimim improving ibm;apply bayesian technique;bayesian technique;backgroundimproving ibm", "pdf_keywords": ""}, "c06410ce8f9b1c941115d6d96780794e66b27eac": {"ta_keywords": "adaptation techniques speech;inremental adaptation techniques;changes speakers speaking;sources inremental adaptation;inremental adaptation;adaptation detection environmental;adjusting acoustic models;including changes speakers;adaptation detection;speech recognition;acoustic models quickly;auditory characteristics changed;changes speakers;adaptation techniques;techniques speech recognition;line adaptation detection;acoustic models;line adaptation;speech recognition aimed;adaptation;time variant acoustic;variant acoustic characteristics;simultaneous line adaptation;bayesian model selection;introduction auditory;variant acoustic;speaking styles noise;adjusting acoustic;incorporating bayesian model;introduction auditory characteristics", "pdf_keywords": ""}, "68167af17980a14ed5fa2514e61d76d5a6a9bed7": {"ta_keywords": "vaccine discussion twitter;content vaccine discussion;anti vaccine conspiracies;vaccine conspiracies;vaccine conspiracies hindered;external content vaccine;vaccine discussion;content vaccine;anti vaccine;articles social media;coronavirus anti vaccine;vaccine;external articles;theories fake news;content external articles;fake news competed;news competed legitimate;responses pandemic;external content articles;articles shared twitter;social media posts;content articles social;spread coronavirus;legitimate sources information;objectivethe spread coronavirus;competed legitimate sources;conspiracy theories fake;conspiracies hindered public;examined content external;discussion twitter", "pdf_keywords": "discussion twitter outbreak;vaccine discussion twitter;discussion twitter examining;twitter outbreak article;public discussion twitter;vaccine conspiracies online;outbreak article content;twitter outbreak;topics conversation coronavirus;discussion twitter;outbreak content;twitter examining;content shared outbreak;social media posts;articles social media;conspiracies online;outbreak article;outbreak content linked;conspiracies online hindered;shared outbreak content;social media activity;articles shared twitter;public conversations vaccines;vaccine discussion;public public discussions;public public discussion;discussions public discussion;discussion public discussion;public discussion;public discussions"}, "4a96b2b33786301d59670fa647f99e3dd807abb8": {"ta_keywords": "agents converge learning;stable nash equilibrium;agents learn deterministic;equilibrium agents converge;uniform learning rates;neighborhood stable nash;non uniform learning;nash equilibrium;converge learning;gradient stochastic settings;stable nash;gradient stochastic;uniform learning;nash equilibrium particular;individual gradient stochastic;games agents learn;agents learn;learn deterministic;learning rates;stochastic settings unbiased;continuous games agents;agents converge;consider continuous games;continuous games;equilibrium agents;provide convergence guarantees;stochastic settings;convergence guarantees;guarantees neighborhood stable;alter equilibrium agents", "pdf_keywords": ""}, "d9b458d39e0912524032887aaaf922f0e950f0c1": {"ta_keywords": "neurons aforementioned regions;large number neurons;neurons;number neurons aforementioned;neurons aforementioned;number neurons;aforementioned networks;networks;aforementioned regions unique;sequencing;regions unique;feature aforementioned networks;regions unique feature;sequencing large;regions aforementioned regions;regions;aforementioned regions;sequencing large number;regions aforementioned;report sequencing;report sequencing large;aforementioned regions aforementioned;unique feature;unique feature aforementioned;unique;feature;aforementioned;number;large number;large", "pdf_keywords": ""}, "76f9f4bf8d97de5e95d2fd9dd8b50041524fb1cc": {"ta_keywords": "iterative entity alignment;entity alignment joint;entity alignment;joint knowledge embeddings;entities aswikipedia links;alignment joint knowledge;approach entity alignment;link entities counterparts;entity alignment aims;link entities;knowledge embeddings method;entities aswikipedia;knowledge embeddings;information entities aswikipedia;multiple knowledge graphs;entities counterparts multiple;knowledge graphs;iterative entity;entities relations;knowledge graphs kgs;introduction iterative entity;entities relations various;entities counterparts;encodes entities relations;aswikipedia links;aims link entities;counterparts multiple knowledge;joint knowledge;aswikipedia links require;entities", "pdf_keywords": ""}, "476ff888fe3917f92b221c522ffb7bfaa4e1861b": {"ta_keywords": "retrieval conversational search;open retrieval conversational;retrieval conversational;approaches conversational search;conversational question answering;conversational search;retrieval conversational question;introduction conversational search;conversational search methods;response ranking conversational;conversational search simplified;ranking conversational;ranking conversational question;conversational search ultimate;role retrieval conversational;question answering;question answering orcon;approaches conversational;question answering answer;research approaches conversational;information retrieval;introduction conversational;introduce open retrieval;response ranking;conversational question;open retrieval;conversational;goals information retrieval;information retrieval recent;retrieval recent research", "pdf_keywords": "retrieval conversational orconv;open retrieval conversational;retrieval conversational search;retrieval conversational;retrieval conversational setting;retrieval conversational deals;retrieval conversational question;friendly retrieval retrieval;retrieval conversational introduce;conversational question answering;question answering conversational;conversational search;user friendly retrieval;question answering orconvqa;approach conversational search;friendly retrieval;response ranking conversational;retrieval retrieval;approaches conversational search;propose open retrieval;provide open retrieval;conversational search build;model conversational search;conversational search fundamental;retrieval convqa dataset;conversational search simplified;role retrieval conversational;introduce open retrieval;ranking conversational;open retrieval convqa"}, "b169c4b6c23efe8cbd4dc29eb97939cbcfba0f28": {"ta_keywords": "persuasive dialogue systems;corpus persuasive dialogues;persuasive dialogues;effective persuasive dialogue;persuasive dialogues real;persuasive dialogue;introductionpersuasive dialogue systems;dialogue systems;dialogue systems systems;actions dialogue participants;dialogue participants;dialogues real humans;construction corpus persuasive;corpus persuasive;dialogue systems important;introductionpersuasive dialogue;thoughts actions dialogue;dialogue participants gained;dialogues;dialogue;actions dialogue;dialogue literature;construct effective persuasive;effective persuasive;recent dialogue literature;persuasive;dialogues real;dialogue literature order;persuasiveness;gained recent dialogue", "pdf_keywords": ""}, "65d3575b1c380b1bcc14ec69ccf6989c04be9493": {"ta_keywords": "semantic web graph;mapreduce good solution;use mapreduce;methodssome use mapreduce;mapreduce;use mapreduce good;semantic web;mapreduce good;introductionthe semantic web;web graph growing;web graph;knowledge interlinking analyzing;unconnected data sets;knowledge interlinking;new knowledge interlinking;unconnected data;previously unconnected data;semantic;interlinking analyzing previously;interlinking analyzing;algorithms graph missing;graph missing methodssome;algorithms graph;various algorithms graph;graph growing incredible;data available programming;data sets confronts;analyzing previously unconnected;graph growing;interlinking", "pdf_keywords": ""}, "cf7e8f47ad1c57738dc586109dcf28a22ab67b72": {"ta_keywords": "bids review papers;papers reviewer bids;applications involve sequential;bidding process conference;reviewer bids;review papers order;bids primacy effects;paper bidding process;bidding process;bids review;sequentially reviewer;reviewer bids review;sequentially reviewer needs;bids primacy;conference peer review;focus paper bidding;reviewers enter sequentially;paper bidding;impact bids primacy;papers order;enter sequentially reviewer;process conference peer;order papers;user ordering;bids;papers order papers;ordering;order papers shown;bidding;peer review", "pdf_keywords": "peer review bidding;peer review algorithm;peer review;peer review systems;peer review process;peer review fundamental;inefficient peer review;efficiency peer review;review peer;review peer review;super peer review;review bidding algorithms;peer reviewers;review systems peer;peer reviews;reviewer bidding;peer review world;similarity reviewers bid;peer review quality;problem peer review;process peer review;reviewer bidding function;systems peer review;reviewer bidding behavior;approach peer review;advantages peer review;bidding process peer;review use peer;peer review present;peer review complex"}, "adc273bd25ab1e2a66543f23c7a801af0dd80e5b": {"ta_keywords": "speech recognition speaker;speaker diarization single;speaker diarization monaural;recognition speaker diarization;speaker diarization;speaker automatic speech;simultaneous speech recognition;dialogue recordings target;speech recognition ar;speech recognition;automatic speech recognition;channel dialogue recordings;monaural dialogue recordings;recordings target speaker;automatic speech;dialogue recordings;recognition speaker;ar simultaneous speech;simultaneous speech;diarization monaural dialogue;dialogue recordings resultsthe;single channel dialogue;speaker acoustic models;introductionmultaneous speech recognition;target speaker automatic;channel dialogue;use target speaker;target speaker acoustic;diarization error rate;target speaker", "pdf_keywords": "estimation speaker embeddings;speech recognition speaker;recognition speaker diarization;estimation target speaker;estimated speaker embeddings;method estimation speaker;speaker automatic speech;speech recognition speakers;target speaker embeddings;speech target speaker;recognition speaker;speaker speech target;total speech recognition;speaker diarization single;single speaker dialogue;estimation speaker;speaker diarization method;speaker dialogue recording;target speaker neural;utterance speaker extracted;based speaker diarization;based estimated speaker;knowing speaker information;speaker diarization;speaker embeddings recordings;sample utterance speaker;automatic speech;speaker embeddings;clustering based speaker;recognizing speech"}, "b20cadef0c59e80f7dfdf825b07442619d920fd5": {"ta_keywords": "search cc attention;vectorized beam search;attention based speech;cc attention based;beam search cc;attentionbased encoder decoder;attentionbased encoder;ar attentionbased encoder;efficient beam search;recognition ar attentionbased;hypotheses beam search;beam search;speech recognition;speech recognition ar;beam search techniques;vectorizing multiple hypotheses;cc attention;automatic speech recognition;automatic speech;end automatic speech;vectorized beam;beam search replace;speech recognition methods;architecture accelerate decoding;ar attentionbased;based speech recognition;decoding process vectorizing;attention based;search cc;decoder architecture accelerate", "pdf_keywords": ""}, "16326359081a42c0b254ee6be39824fd2db07e48": {"ta_keywords": "pivot target translation;target translation models;translation models;translation language pairs;translation process;introductionpot translation allows;translation models source;used translation process;translates combining source;translation language;translation accuracy conventional;target translation;high translation accuracy;translation accuracy;introductionpot translation;method translates;triangulation method translates;method translates combining;translation allows translation;translates combining;known high translation;allows translation language;high translation;information pivot phrases;allows translation;language pairs;translates;translation allows;combining source pivot;translation", "pdf_keywords": ""}, "336ee50043b916c9e932338c02fd1abc87a6e849": {"ta_keywords": "compositional generalization learning;compositional generalization;expressions compositional generalization;generalization learning analyticalal;contextcompositional generalization learning;analyticalal expressions compositional;contextcompositional generalization;learning analyticalal expressions;learning analyticalal;generalization learning;expressions compositional;generalization;compositional;analyticalal expressions;contextcompositional;analyticalal;learning;expressions", "pdf_keywords": "compositional generalization automatically;compositional generalization neural;achieve compositional generalization;learning analytical expressions;approach compositional generalization;problem compositional generalization;compositional generalization;compositional generalization aim;learn analytical expressions;compositionality neural networks;generalization neural;proposed compositional task;expressionsthe compositionality language;models compositional tasks;generalization automatically learning;compositional tasks;semantics neural networks;learning syntax;generalization automatically;compositional task recently;compositional task;syntax semantics neural;compositionality language;generalization neural networks;learning syntax semantics;systematicity compositional tasks;automatically learning analytical;compositional task compositionality;semantics neural;analytical expressionsthe compositionality"}, "89ba434b30a3f1b61bcbcf917842899fe3d2eea4": {"ta_keywords": "individuality writer speaker;statistical machine translation;machine translation;express individuality writer;dialogue systems;individuality writer;machine translation smt;creation dialogue systems;dialogue systems consider;transforming individuality using;text speech various;text speech;method transforming individuality;express individuality;individuality using;introductionin text speech;individuality using technique;transforming individuality;dialogue;speech various;speech various features;features express individuality;speech;writer speaker;statistical machine;individuality proposing method;systems consider individuality;individuality;creation dialogue;translation smt", "pdf_keywords": ""}, "63a35d8822a042f6d6cd919fd5d3c9e94df6ee18": {"ta_keywords": "detecting change points;change point detection;detecting change;change point instances;detect kinds changes;changes want detect;change point;change points;novel change point;require detecting change;true change point;point instances supervision;detection unsupervised consequence;change points complex;learning ground metric;sequential data existing;sequential data;point detection framework;complex sequential data;instances supervision learning;point detection unsupervised;point instances;detection unsupervised;detection framework;methods change point;point detection;points complex sequential;instances supervision;supervision learning ground;supervision learning", "pdf_keywords": "supervised change detection;metric change detection;change point detection;change points learning;detecting change points;change detection;poor change detection;metric detect change;detecting changes;detect change points;change detection performance;detecting changes data;supervised change;change detection using;detecting change;change detection present;reliably detect change;detect change;detecting abrupt changes;change points learn;features predict changes;points training sequences;method detecting changes;learned metric detect;detect kinds changes;method detecting change;predict changes;change points training;learning ground metric;ground metric learning"}, "00717c695e4a33318fe5655e2b69e1ba8b61f981": {"ta_keywords": "genre broadcast challenge;multi genre broadcast;ieee automatic speech;genre broadcast;broadcast challenge official;automatic speech;genre broadcast broadcast;broadcast challenge;broadcast broadcast challenge;speech text english;broadcast challenge combination;speech text;presented speech text;challenge combination deep;automatic speech recognition;speech recognition;nationalist anr 2015;speech recognition understanding;2015 multi genre;resultsthe presented speech;challenge ieee automatic;contribution premiere challenge;official challenge ieee;multi genre;speech;challenge official challenge;deep learning;premiere challenge;deep learning systems;broadcast", "pdf_keywords": ""}, "ba201da15899e78629ee5471e8d336b6b2eb7279": {"ta_keywords": "riding public transit;communities ride sharing;transportation networks communities;networks communities ride;ride sharing platforms;public transit demand;ride sharing;public transit;uber lyft commonplace;uber lyft;communities ride;convenient riding public;transit demand;transportation networks;platforms uber lyft;congestion major cities;uber;sharing platforms uber;transit demand nature;transportation;considerate multi modal;constrained transportation networks;constrained transportation;platforms uber;multi modal;deemed convenient riding;socially considerate multi;riding public;transit;urbanization added", "pdf_keywords": "distributing mobility demand;modal routes best;multi modal routes;mobility demand;suggesting route choices;mobility demand dimensions;routes best possible;modal routes;mobility decision support;routing games propose;multi objective routing;route choices;transportation networks propose;route planning;transportation demand management;routes best;route suggestions;routing decisions congestion;routing choices;routing decisions users;considerate route suggestions;paths public transportation;mobility decision;objective routing algorithms;transportation networks difficult;constrained transportation networks;route choices socially;mobility demand potentially;routing game;routing choices socially"}, "0af2ff552ab0555914dee90ccfae18297b2792c9": {"ta_keywords": "discriminative speaker embeddings;discriminative speaker;speaker embeddings;dis discriminative speaker;speaker embeddings methodswe;speakers local global;number speakers local;variable number speakers;handling speaker overlap;end diarization models;performs meeting diarization;handling speaker;speaker overlap;number speakers;meeting diarization;end deep network;diarization models;speakers local;speaker;meeting diarization single;audio recordings resultsend;discriminative training;speaker overlap enabling;deep network model;diarization single channel;channel audio recordings;clustering based diarization;diarization models advantage;networks dis discriminative;end diarization", "pdf_keywords": "diarization network speaker;diarization speaker neural;neural speaker diarization;speaker diarization;diarization speaker wise;speaker detection module;speaker diarization use;diarization speaker;speaker module jointly;diarization speaker speaker;network speaker module;method speaker diarization;speech meeting model;speaker diarization speaker;binary speaker activity;speaker diarisation using;speaker module train;speaker detection;speaker module;speaker activity permutation;speaker activity;speaker neural;meeting style audio;loss speaker detection;diarization network;automatic speech;probabilities binary speaker;method speaker diarisation;speaker diarisation;network speaker"}, "12f3bc02d649645fa8734977e28b0ac839e56371": {"ta_keywords": "parking network queues;parking scarcity congestion;curbside parking network;parking scarcity;scarcity congestion block;parking network;queueing network;network queues;scarcity congestion;relationship parking scarcity;congestion block;kind queueing network;curbside parking;canonical queueing network;queueing;queues;queueing network results;network queues methodswe;network queues nature;queueing network subject;network canonical queueing;faces curbside parking;queues nature network;queues nature;congestion;parking;new kind queueing;canonical queueing;relationship parking;congestion block faces", "pdf_keywords": "parking network queues;parking scarcity congestion;parking convex program;maximizing occupancy queues;network model parking;parking convex;parking impact queuing;parking network;allowable congestion queues;queueing network model;approach traffic congestion;problem economics parking;curbside parking network;queue network customers;queueing network;congestion queues;queue network;queueing network demonstrated;searching parking convex;develop queueing network;congestion constraints convex;network customers queues;parking scarcity;traffic congestion;demonstrate congestion constraints;service maximizing occupancy;analysis network queues;parking policies;model parking areas;future parking policies"}, "b7637d1148da569d2211b5dd9851bca82c6aac43": {"ta_keywords": "based dependency parsing;dependency parsing;graph based dependency;dependency parsing objectivewe;online sentence methodwe;parsing objectivewe propose;imitation learning techniques;imitation learning;features added sequentially;solve imitation learning;dynamic feature selection;sentence methodwe;parsing;sequential decision making;sentence methodwe model;parsing objectivewe;decisions online sentence;selection features;sequential decision;feature selection;based dependency;sequentially needed;objectivewe propose faster;selection features added;feature selection features;sequentially;problem solve imitation;sequential;online sentence;model sequential decision", "pdf_keywords": "dependency parser fast;based dependency parsing;generating dependency parser;based dependency parser;dependency parsing choosing;dependency parsing;parsing algorithm fast;dependency parser;parse tree efficiently;parser fast fast;parser fast;parsing algorithm best;dependency parsing application;improves parsing;improves parsing accuracy;faster projective parsing;sentences parse tree;dependency parsing major;parsing major challenge;dependency parser uses;languages dynamic parser;projective parsing algorithm;tree parsing algorithm;dynamic parser;superior parsers using;tree parsing;parsing algorithm;comparable superior parsers;spanning tree parsing;method dependency parsing"}, "5f8d2da91a6c4b9dd079ccb2706c31bda14ef320": {"ta_keywords": "recognition audio captioning;audio captioning tasks;audio captioning;audio captioning generate;modeling joint speech;joint speech recognition;audio datasets speech;introduction joint speech;datasets speech;captioning tasks;joint speech;captioning generate natural;audio captioning aimthe;captioning generate;speech recognition audio;labeled audio datasets;captioning;aim joint speech;descriptions contents audio;audio datasets;captioning tasks demonstrate;recognition audio;labeled audio;speech recognition;lack labeled audio;natural language descriptions;contents audio samples;contents audio;audio samples methodswe;captioning aimthe aim", "pdf_keywords": "models audio captioning;trained models audio;automated audio captioning;transcripts audio captions;audio captioning;captioning audio;speech transcripts audio;audio captions multi;audio captioning audio;audio captions;captioning audio audio;clean speech samples;modeling annotated speech;combining clean speech;audio representations separately;speech samples captioned;effective audio representations;automatic speech;dataset noisy speech;audio representations;annotated speech transcripts;approaches automatic speech;speech samples recorded;transcripts audio;captions multi task;speech transcripts;audio dataset;speech samples;transcript caption labels;approach automatic speech"}, "298b72096b8a770b0cdb263dd53cf2463b8a1a1d": {"ta_keywords": "causal inference text;inference text documents;causal inference important;causal inference;documents address causal;inference text;introduction causal inference;address causal inference;author affect popularity;effects observational;effects observational data;causal;affect popularity post;introduction causal;post estimate effects;theorem paper affect;post author affect;inference important;popularity post estimate;affect popularity;inference;estimate effects observational;inference important issue;author affect;address causal;quality text suffices;estimate effects;popularity post;observational data confounded;text documents", "pdf_keywords": "document embeddings causal;text causal inference;inference text causal;text suffices causal;causal topic models;embeddings produced causal;embedding methods causal;text causal;embeddings causal;language modeling causally;causally sufficient embeddings;causal topic model;topic model causal;causal inference text;causally sufficient text;causal effects text;causal effect text;embeddings text documents;context causal inference;effects observational text;sufficient document embeddings;embeddings causal biingering;mediating causal inference;causal inference improved;sufficient text representations;causally sufficient document;causal inference challenging;causal bert;causal bertwe language;inference estimate causal"}, "ffa07e4d7c8fade2ded5ffeea7265d22d8a0c0ab": {"ta_keywords": "3d construction;neural network gan;3d construction methods;cnn generative;cnn generative neural;introduction 3d construction;generative neural network;generative neural;network cnn generative;network gan;networks designed image;furniture using light;gan;construction methods;combining convolutional neural;based expensive furniture;convolutional neural;convolutional neural network;furniture using;3d;generative;furniture;expensive furniture using;neural network cnn;construction;designed image processing;expensive furniture;network cnn;introduction 3d;designed image", "pdf_keywords": ""}, "41e49fd3af628f1c8201942a659769f7cc21d812": {"ta_keywords": "unlabeled nodes propagating;initially unlabeled nodes;unlabeled nodes;nodes propagating label;classify initially unlabeled;propagating label information;label information structure;propagating label;seed nodes;nodes propagating;nodes;initially unlabeled;unlabeled;nodes number algorithms;label information;graph starting seed;starting seed nodes;information structure graph;seed nodes number;methods classify initially;nodes number;classify initially;methods classify;structure graph starting;structure graph;classify;applications methods classify;label;algorithms;graph starting", "pdf_keywords": "semi supervised learning;graphbased methods learning;based semi supervised;semi supervised;efficiently store labels;graph based semi;labeled nodes;graph based algorithm;labels use graph;based algorithm graph;algorithm graph based;novel graph based;supervised learning algorithm;graphbased;label labeled nodes;supervised learning;labeled nodes present;learning algorithm compactly;similarity data words;graphs method fast;graph based;supervised;use graph based;algorithm graph;graphbased methods;store labels;method supervised;label labeled;compactly stores label;stores label distribution"}, "3bfa808ce20b2736708c3fc0b9443635e3f133a7": {"ta_keywords": "graph neural network;graph neural;introduction graph neural;relationships edges gn;nodes relationships;nodes relationships edges;neighbor nodes;propagating information neighbors;graph absorbs information;node graph;elements nodes relationships;neighbor nodes paper;neural network gn;nodes;flowing neighbor nodes;information neighbors creates;information flowing neighbor;relationships edges;edges gn;neural network;node graph absorbs;information neighbors;containing elements nodes;edges gn variants;elements nodes;gns mechanism propagating;differ node graph;network gn;introduction graph;propagating information", "pdf_keywords": "training graph neural;graph neural;graph neural networks;model graph neural;graph neural network;limitation training graph;proposal graph neural;training graph;harmful graph neural;neural networks squashing;neural networks bottleneck;nodes relationships;general networks;information distant nodes;neural networks gns;distant nodes graph;networks;graph proposethe bottleneck;development graph neural;layer graphs improves;neural network gn;gnn able predict;target node graph;distant nodes;layer graphs;general networks increased;networks gns;gns receptive;elements nodes relationships;nodes"}, "9ab3622b3a801b90907f3ee399f881764db05d06": {"ta_keywords": "parameters linear attack;linear attacks methodsthe;linear attack learnt;linear attack;linear attacks;attacks methodsthe parameters;attacks methodsthe;class linear attacks;attack learnt;lagrange multiplier optimization;attack learnt line;develop constrained optimization;optimization technique involving;constrained optimization;optimization technique;attacks;stochastic gradient descent;multiplier optimization technique;constrained optimization problem;online stochastic gradient;methodsthe parameters linear;optimal parameter;optimization;multiplier optimization;lagrange multiplier;gradient descent;problem optimal parameter;update lagrange multiplier;attack;optimization problem", "pdf_keywords": "attacks distributed estimation;attack distributed process;distributed estimation algorithm;attack algorithm distributed;distributed estimation;algorithm distributed estimation;method distributed estimation;distributed estimation using;distributed estimation distributed;estimation algorithm distributed;distributed process estimation;nodes kalman consensus;distributed estimation based;estimation distributed estimation;distributed estimation consider;linear attack distributed;attacks distributed;scheme distributed estimation;attack distributed;attacks use stochastic;kalman consensus;attack scheme distributed;online linear attack;attack distributed cyber;estimation distributed;linear attack algorithm;kalman consensus filterwe;attack algorithm karush;nodes kalman;attack algorithm"}, "f2e544c5333125ee30c1c34b08936b6ef87c97dd": {"ta_keywords": "implementations spoken language;spoken language processing;neural network models;neural network based;neural network;research article neural;language processing systems;article neural network;development neural network;implementations spoken;based implementations spoken;spoken language;automate tuning meta;language processing;neural network important;concepts neural network;types neural network;automate tuning;introduction development neural;article neural;neural;basic concepts neural;tuning meta parameters;development neural;effort automate tuning;concepts neural;parameters using evolutionary;tuning meta;using evolutionary;meta parameters", "pdf_keywords": ""}, "ecfac0d377db229d58bc88698ad3bfd4b384ef37": {"ta_keywords": "propose argument mining;argument mining based;argument mining;introductionpeer reviewing;reviewing;review workload;review workload especially;high review workload;introductionpeer reviewing central;propose argument;work propose argument;high review;review;reviewing central process;reviewing central;argument;results high review;conferences methodin work;major conferences methodin;conferences methodin;discussed major conferences;especially senior researchers;process modern research;methodin work propose;researchers;research essential;introductionpeer;discussed;research;modern research", "pdf_keywords": "mining arguments peer;argument mining present;argument mining;mining argumentation;argument mining field;mining argumentation schemes;argument mining based;argument mining approach;argument mining applications;propose argument mining;tool argument mining;new argument mining;argument detection peer;argument mining argumentation;mining arguments;previous argument mining;argument mining forthe;argumentation association computational;studies argument mining;argument detection;argument search engine;detection argument detection;driven argumentation association;argumentation analyzed;basis argument mining;informationargument mining;driven argumentation analyzed;argument search;informationargument mining task;argumentation association"}, "6ec6fa4e34200e13d80ee79b95d1cc6ec0f6b424": {"ta_keywords": "human interactive dialogues;interactive dialogues;natural language interaction;interactive dialogues complete;communicates natural language;human interactive;language interaction;dialogues;language interaction understanding;human human interactive;dialogues complete household;engage natural language;instructions using conversation;natural language;using conversation;interactive;robots operating human;robots;task communicates natural;dialogues complete;operating human spaces;conversation;robots operating;human spaces;using conversation resolve;task communicates;conversation resolve;tasks simulation;household tasks simulation;human spaces able", "pdf_keywords": "simulation human dialogue;robots natural language;human dialogue sessions;human dialogs;human human dialogs;tasks ai2;human dialogue;platform embodied ai;dialogues human;embodied agents chat;human dialogs simulating;human interaction dialogues;embodied ai;embodied ai models;user interacting robot;interaction dialogues;human conversational dialogues;tasks ai2thor simulation;action sequences dialogue;annotators simulation human;tasks ai2 thor;interactions human annotator;dialogues human commander;household tasks ai2;environments hypothesize dialogue;tasks ai2thor;human robots;natural language dialogue;human human conversational;object centric embodied"}, "51b0609155e3a63afd1dd7dcc3034a5950f90ee0": {"ta_keywords": "predictions feature influence;influence model predictions;disentangling influencece using;disentangling influencece;feature influence;influencece using disentangled;feature influence direct;audit model predictions;feature influence expressed;features influence model;background disentangling influencece;features influence;features feature influence;influence model outcomes;data features influence;direct influence model;model outcomes influenced;influence model;direct influence;influenced proxy features;model predictions feature;direct direct influence;influence direct;predictions methods motivated;influence;audit model;influencece using;disentangled reptures audit;influence expressed;indirect model outcomes", "pdf_keywords": "disentangled influence audits;disentangling influence audits;influence audit disentangling;audit disentangling influence;wedisentangled influence audits;audit model predictions;auditing model predictions;indirect influence audits;features audit model;audit demonstrate disentangled;audit indirect influence;audit direct features;influence audits;audit disentangling;influence audit;influence audits theory;indirect influence audit;influence audits audit;influence audit useful;audit results influence;influence audits useful;influence audits provide;influence audits correctly;influence audit complex;features audit direct;features audit;influence audit demonstrate;influence audits procedure;auditing models;data features audit"}, "1a27c23453d3f718d854ac4b57dcf3e81ac51aa8": {"ta_keywords": "backgroundactive learning;backgroundactive learning widely;training active learner;active learner iteratively;active learner;high cost annotation;development labeled datasets;annotation rapid pace;training active;annotation typically based;annotation;labeled datasets;annotation budget;cost annotation rapid;annotation typically;fixed annotation budget;annotation budget rounds;annotation rapid;selects examples annotation;examples annotation;cost annotation;training strategy;backgroundactive;learning widely used;examples annotation typically;used training strategy;learner iteratively;labeled datasets remain;fixed annotation;datasets", "pdf_keywords": ""}, "9c403ca58853fbb223f6e9fce446bb638f291692": {"ta_keywords": "corpus entity mention;annotations 595 material;material science synthesis;entity mention annotations;mention annotations;new corpus entity;corpus entity;backgroundmaterial science synthesis;scientific nonpolymerization;materials operations entities;domain scientific nonpolymerization;annotations;scientific nonpolymerization fundamental;mention annotations 595;science synthesis procedures;entity mention;synthesis procedural texts;labels materials operations;models material science;science synthesis;building information extraction;material science;information extraction models;present new corpus;procedural texts;annotations 595;labels materials;new corpus;information extraction;extraction models material", "pdf_keywords": ""}, "33e5e4b079535957d1275497f8870ea57762a03d": {"ta_keywords": "sentence attackability online;attackable sentences argument;demonstrate sentence attackability;analysis sentence attackability;sentence attackability associated;attackability online arguments;sentence attackability;finding attackable sentences;attackable sentences;successful refutation argumentation;attacks argumental;refutation argumentation present;attacks argumental identify;refutation argumentation;reasons attacks argumental;attackability online;argumentation present;sentences argument;argumental identify;online arguments;argumentation;argumental identify relevant;argumental;online arguments methods;argumentation present large;sentences argument step;attackability;arguments methods analyze;attackability associated;attackability associated characteristics", "pdf_keywords": "sentence attackability online;attackable sentences arguments;demonstrate sentence attackability;sentence attackability demonstrate;sentences impact attackability;detecting attackable sentences;sentence attackability associated;attackable sentences argument;analysis sentence attackability;attacks argumentation identify;attackable sentences quantify;attacks argumentation;detect attackable sentences;attackable sentences;sentence attackability;sentence attackability regard;attackability sentences used;attackability sentences;attackability online arguments;introductionfinding attackable sentences;attackable sentences comparably;characteristics attackable sentences;objectivefinding attackable sentences;features sentence attackability;refutation argumentation objectivefinding;argumentation objectivefinding attackable;reasons attacks argumentation;successful refutation argumentation;attacked sentences ability"}, "81dfa45c568d7c1d9771ba2a1f07dad96558cff6": {"ta_keywords": "hidden markov kernelel;markov kernelel machine;markov kernelel;classifierbased hidden markov;hidden markov models;phoneme classification methodsthis;hidden markov;kernel methods proposed;classification derived kernel;phoneme classification;novel classifier sequential;classifier sequential data;application phoneme classification;markov models hmms;classifier sequential;classification performance mixture;models hmms emission;nonlinear classification derived;sequential pattern classifierbased;models hmms;functions hidden markov;nonlinear classification performance;nonlinear classification;based nonlinear classification;classification methodsthis paper;markov models;classification;enhanced kernel methods;classifier;kernel methods", "pdf_keywords": ""}, "be12a8d9ddb12c9ed292430c38d50093191dd442": {"ta_keywords": "fuzzy graph clustering;nodes clustering soft;introductionedistant nodes clustering;equilibriumdistant nodes clustering;nodes clustering;graph clustering called;graph clustering;clustering soft clustering;clustering called equilibriumdistant;clustering soft;soft clustering algorithm;nodes clustering enc;soft clustering;clustering called;synset induction;applied synset induction;synset induction paper;clustering;algorithm applied synset;clustering enc;clustering algorithm;nodes community reach;clustering algorithm applied;nodes community;clustering enc core;algorithm fuzzy graph;called equilibriumdistant nodes;introductionedistant nodes;nodes;fuzzy graph", "pdf_keywords": ""}, "90db4ddb08df23a4c587e6136e66cb388311473b": {"ta_keywords": "learns filter documents;methods learns filter;filters learned users;filters learned;learns filter;learned filters;transfer learned filters;learned users collaborative;collaborative learning;learning methods direct;symbolic learning methods;learned filters setting;methods learns;learning methods;filter documents suffer;learning methods use;different learning methods;introductionvarious methods learns;symbolic learning;transfer learned;case collaborative learning;direct transfer learned;filter documents;conclude symbolic learning;exploit filters learned;learned users;collaborative learning resultswe;learning;filters;learns", "pdf_keywords": ""}, "d409ff05d70f7b9787baf6431a84a178ad726e8d": {"ta_keywords": "inverse reinforcement learning;novel inverse reinforcement;inverse reinforcement;environments learning demonstrations;learning demonstrations;learning demonstrations aimto;ai agents;ai human teams;constrained environments learning;equip ai agents;learning implicit;ai human;agents model humans;reinforcement learning;effective ai human;ai agents model;environments learning;withstrained environments agents;method learning implicit;reinforcement learning method;reinforcement;create effective ai;environments agents;effective ai;ai;teams equip ai;environments agents able;mirror human behavior;agents able mirror;demonstrations", "pdf_keywords": "inverse reinforcement learning;novel inverse reinforcement;ai agents;inverse reinforcement;ai human teams;ai human;ai agents model;models human decision;equip ai agents;ai safety;autonomous agents;autonomous agents critical;agents model humans;field ai safety;ai;ai safety value;create effective ai;constrained environments agents;effective ai human;constraints field ai;autonomous autonomous agents;field ai;teams equip ai;learning constraints demonstrations;approach learning constraints;environments agents;create autonomous autonomous;effective ai;environments agents able;agent architecture"}, "f16c0699a873b0209a370e8e6301b0189785c614": {"ta_keywords": "learning constrained dirichlet;constrained dirichlet process;messageactive learning constrained;verb clustering incorporating;clustering incorporating supervision;constrained dirichlet;verb clustering;task verb clustering;dirichlet process mixture;messageactive learning;active learning;active learning approach;learning constrained;applied dirichlet process;clinical messageactive learning;process mixture models;mixture models task;mixture models;dirichlet process;clustering incorporating;mixture models discussed;introduce active learning;learning approach constraint;constraint selection;constraint selection employing;applied dirichlet;links constraints instances;dirichlet;key clinical messageactive;clustering", "pdf_keywords": ""}, "363eb288abf76f7ab52d7789b30399b4b909dd5a": {"ta_keywords": "introductionbribery voting nets;voting nets;voting rules kinds;voting rules;consider voting rules;traditional bribery problem;voting nets important;bribery problem;vote inter dependencies;generalize traditional bribery;introductionbribery voting;bribery;agents vote;traditional bribery;set variables voters;variables voters;voting domains;important problem voting;variables voters use;voting domains candidate;voting;agents vote inter;issues agents vote;resultswe consider voting;voters use nets;bribery problem account;problem voting;voters;consider voting;vote", "pdf_keywords": ""}, "0d6a4e45acde6f47d704ed0752f17f7ab52223af": {"ta_keywords": "hierarchical tasks;hierarchical tasks propose;decomposition hierarchical tasks;sparse remission reinforcement;instructions action trajectories;task learning;remission reinforcement learning;multi task learning;task learning requires;step human demonstrations;efficient multi task;human demonstrations form;instructions action;reinforcement learning;human demonstrations;language instructions action;multi task;automatic decomposition hierarchical;complex task problems;demonstrations form;reinforcement;action trajectories methods;demonstrations form natural;reinforcement learning setting;decomposition hierarchical;tasks;policies facilitate automatic;hierarchical;natural language instructions;complex task", "pdf_keywords": "hierarchical reinforcement learning;hierarchical reinforcement;imitation learning reinforcement;hierarchical tasks propose;use hierarchical reinforcement;reinforcement learning imitation;hierarchical tasks;generate action instructions;solve hierarchical tasks;language instructions hierarchical;learning imitation learning;demonstrations natural language;multi step tasks;instructions hierarchical;instructions model learns;sparse reward reinforcement;supervised reinforcement learning;supervised reinforcement;instructions hierarchical decision;tasks human demonstrations;studied hierarchical reinforcement;demonstrations useful learning;learning imitation;combined supervised reinforcement;reward reinforcement learning;sub task policy;learning reinforcement;leveraging human demonstrations;step tasks;learning reinforcement learning"}, "88b66f705a329da8292e7b8aa4bfe26de4759cfa": {"ta_keywords": "machine translation possible;machine translation words;machine translation;introduction machine translation;character based translation;accurate machine translation;based translation model;translation model;transduction grammar alignment;translation model using;words substring alignment;phrase based mt;translation words possible;inversion transduction grammar;grammar alignment techniques;phrasal inversion transduction;transduction grammar;substring alignment;translation words;transformation character strings;substring alignment paper;grammar alignment;translation possible;alignment techniques character;words substring;translation possible concept;phrasal inversion;possible words substring;based translation;applying phrasal inversion", "pdf_keywords": ""}, "a901185ee0710770420044cace33003109d478e3": {"ta_keywords": "rating systems better;rating systems;rating systems learn;rely rating systems;participants practice ratings;questions rating systems;ratings highly inflated;quality market participants;rating resultswe;practice ratings;practice ratings highly;consider questions rating;rating;ratings;rating resultswe analyze;levels rating resultswe;discriminate quality;discriminate quality altering;quality market;learn quality market;quality altering;questions rating;better discriminate quality;quality methodswe consider;ratings highly;rely rating;levels rating;quality methodswe;critically rely rating;importance levels rating", "pdf_keywords": "rating scales informative;rating systems aim;informative rating systems;rating systems motivated;rating scales naive;rating systems demonstrate;online rating systems;rating systems provide;rating systems;rating scale study;rating scale fact;effective online rating;rating systems learn;ratings scales;rating systems important;rating scales fundamental;produce informative ratings;rating design;ratings scales quality;scale ratings informative;ratings informative ratings;rating scales substantially;rating using;informative ratings obtained;ratings informative;rating systems online;optimize rating systems;rely rating systems;rating systems apply;inflated informative ratings"}, "ee9f40f1c1e77b0b39b6e4a158208614fb4995c0": {"ta_keywords": "predict urban anomalies;urban anomaly detection;urban anomalies automatically;urban anomalies;urban anomaly;driven urban anomaly;urban anomalies result;detect predict urban;urban big data;context urban anomalies;predicting anomalies happening;data driven urban;predict urban;anomaly detection prediction;predicting anomalies;automatically alerting anomalies;alerting anomalies;alerting anomalies early;urban big;anomaly detection;anomalies happening great;anomalies automatically;urban;utilize urban big;stage predicting anomalies;detection prediction frameworks;anomalies happening;detection prediction;big data;anomalies automatically methods", "pdf_keywords": "urban anomaly detection;urban anomaly analysis;predict urban anomalies;prediction urban anomalies;predict anomalies urban;urban data anomaly;forecast urban anomalies;prediction urban anomalous;urban anomalies automatically;new urban anomaly;driven urban anomaly;algorithms urban anomaly;urban anomaly;urban anomalies researchers;introduction urban anomalies;urban anomalies;anomaly anomalous urban;urban anomalies commonly;algorithms urban anomalies;urban anomalies various;urban anomalies etiology;anomalies urban;urban anomalous;data driven urban;problems urban anomaly;urban big data;urban anomalous events;patterns urban anomalies;anomaly detection prediction;urban anomalies unexpected"}, "5bcbc4554a68b38ff4a22b848fb0817b809608b2": {"ta_keywords": "patient history history;patient history;case patient history;history history;history history history;history;present case patient;case patient;article present case;purpose article present;article present;purpose article;article;patient;present case;present;case;purpose", "pdf_keywords": ""}, "c159725940750adbad262ac946ce161bb68e41b5": {"ta_keywords": "auditory auditory auditory;auditory auditory;based auditory auditory;attention based auditory;based auditory;auditory;attention based;background attention;background attention based;attention;based;background", "pdf_keywords": "new speech recognition;speech recognition ar;e2e automatic speech;speech recognition;dot product attention;automatic speech;automatic speech recognition;attention encoder;speech processing;applied automatic speech;attention convolutional layer;attention simple model;reverberant speech processing;speech recognition article;attention encoder convolutional;attention hybrid method;product attention method;self attention encoder;decoding speech;attention convolutional;attention method extended;recognition ar sequenceto;attention hybrid;approach decoding speech;attention method;speech processing research;dynamic convolution architectures;convolution architectures;training decoding convolutional;recognition ar"}, "ec6499842d3e51b7dda94f5d0620d6df5c1a1b6d": {"ta_keywords": "language speech technology;speech technology;introduction speech technology;speech technology unfortunately;speech text;text speech;speech text text;text text speech;trained speech text;text speech subsystems;speech used human;pre trained speech;speech technology plays;trained speech;language speech;speech subsystems methods;speech subsystems;speech used;unwritten language speech;speech;human computer interaction;text;text text;used human computer;human computer;computer interaction;introduction speech;life speech used;language;information retrieval", "pdf_keywords": ""}, "55faed1fbb1575ffa2609bdc4490586e30df441a": {"ta_keywords": "question answering systems;machine translation mt;machine translation;bases question answering;clarified machine translation;answer questions language;answering systems;question answering;systems answer questions;questions language based;translation mt tool;questions language;answering systems come;using knowledge bases;qa clarified machine;translation mt;cross lingual qa;knowledge bases question;knowledge bases;lingual qa;qa systems;qa systems answer;language based information;lingual qa clarified;clarified machine;build qa systems;topics knowledge bases;answer questions accurately;knowledge bases limited;systems answer", "pdf_keywords": ""}, "e52115834ac7a529b1f4a7769dd538f143cf3eea": {"ta_keywords": "erasure codes distributed;designing erasure codes;erasure codes;codes distributed storage;analyze piggybacking codes;piggybacking codes;codes analyze piggybacking;piggybacking codes low;reed solomon codes;codes distributed;solomon codes analyze;designing erasure;low repair bandwidth;codes desirable properties;solomon codes;distributed storage;characterization repair schemes;repair bandwidth complexity;distributed storage empirically;repair schemes;storage empirically proven;framework designing erasure;codes low substriping;erasure;piggybacking framework;codes analyze;used design codes;repair bandwidth;codes desirable;introductionthe piggybacking framework", "pdf_keywords": "storage piggybacking code;erasure codes distributed;repairing piggybacking codes;designing erasure codes;codes distributed storage;codes repair scheme;erasure codes;correcting codes distributed;piggybacking codes achieve;codes analyze piggybacking;piggybacking codes exist;analyze piggybacking codes;piggybacking code theorem;distributed storage piggybacking;proof piggybacking codes;piggybacking codes weaker;piggybacking codes powerful;piggybacking codes;piggybacking codes using;piggybacking codes scalar;piggybacking codes strictly;regenerating codes distributed;storage piggybacking;piggybacking code achieve;piggybacking codes difficult;repair bandwidth piggybacking;bandwidth piggybacking codes;piggybacking code;piggybacking codes low;equivalent repair scheme"}, "777d7b4141c9ce163de99b747e94c8d1db12e11e": {"ta_keywords": "receive machine learning;machine learning services;learning services cloud;contextin;prediction discovering subset;prediction discovering;learning services;accurate prediction discovering;services cloud;machine learning;accurate prediction;service provider actually;services cloud based;reveals information service;offer accurate prediction;service provider;service;services;discovering;cloud based service;models reveals information;based service;discovering subset;contextin order receive;image models reveals;service provider consumers;contextin order;models reveals;entire image models;cloud", "pdf_keywords": ""}, "89e53f116ef732d0abe81ee2218fa862ddc5ddce": {"ta_keywords": "speech translation toolkit;speech translation toolskit;ipsilateral speech translation;speech translation;translation toolkit;translation toolskit;translation toolkit present;translation toolskit present;ipsilateral speech;present ipsilateral speech;toolskit present ipsilateral;toolkit present ipsilateral;toolskit;speech;toolkit;translation;ipsilateral;present ipsilateral;toolskit present;introduction present ipsilateral;toolkit present;introduction present;introduction;present", "pdf_keywords": "speech processing toolkit;translation speech recognition;speech translation development;neural speech translation;speech translation technology;speech translation systems;translation models speech;developments speech translation;speech translation corpuss;translation text speech;development automated speech;augment speech data;processing text speech;automated speech;development speech translation;automatic speech;speech data leveraging;toolkit useful translation;implements automatic speech;methods speech translation;multilingual speech translation;speech speech translation;speech data;functions speech translation;speech translation novel;useful translation speech;text speech development;speech translation;ar machine translation;recognition machine translation"}, "6d654bab72d062d91f731331f16ea01d7cac0812": {"ta_keywords": "biases use tropes;use tropes narrative;tropes narrative elements;tropes narrative;gender bias;investigate gender bias;use tropes;tropes;gender bias large;tropes methods;societal biases;reinforces societal biases;trope;societal biases use;tropes methods enable;popular media reflects;archetypal characters plot;narrative elements archetypal;background popular media;popular media;media reflects reinforces;media reflects;investigate gender;collection tropes;narrative elements;frequently media;biases;large collection tropes;reinforces societal;reflects reinforces societal", "pdf_keywords": "topics femaleleaning tropes;gender bias mediathe;biases use tropes;female tropes heavily;media use genderedness;genderedness media;analyzing gender bias;bias mediathe genderedness;femaleleaning tropes;measure genderedness media;gender bias text;genderedness media use;associated gender bias;tropes male gender;favor female tropes;femaleleaning tropes male;male dominated tropes;dominated female tropes;gender bias;female tropes;male tropes;investigate gender bias;female leaning tropes;male leaning tropes;analysis gender bias;bias demonstrate genderedness;gender bias social;gender bias large;tropes male dominated;mediathe genderedness"}, "e79d1206292bc5e67ba19737d87d4b2ea4a37105": {"ta_keywords": "learns subword tokenization;bias learns subword;subword tokenization algorithms;based subword tokenization;subword tokenization;rigid subword tokenization;subword tokenization end;learns subword;subword tokenization methodsstate;gradient based subword;inductive bias learns;bias learns;character transformations gradient;models natural language;tokenization algorithms;natural language processing;tokenization;fast character transformations;tokenization algorithms limit;subword;separate rigid subword;tokenization methodsstate art;based subword;model inductive bias;language processing rely;rigid subword;inductive bias;language processing;character transformations;backgroundcharformer fast character", "pdf_keywords": "learns subword tokenization;subword representations;subword representations characters;subword tokenization novel;bias learns subword;subword tokenization module;latent subword representations;subword tokenization;based subword tokenization;modeling subword blocks;learns subword;subword tokenization operate;subword tokenization end;learns latent subword;character sub words;subword based models;introduce charformer deep;subwordbased models efficient;gradient based subword;subword character level;charformer deep;character level subword;subword blocks model;modeling subword;words directly characters;subword character;subwords model;characters data driven;charformer deep transformer;string subword blocks"}, "4cdd533963d8fb21fbf4bb3487bf6a6d60e14e93": {"ta_keywords": "cell image segmentation;segmentation method cell;image segmentation usingmf;image segmentation;segmentation usingmf based;segmentation usingmf;segmentation;images using markov;cell images using;segmentation method;cell images;cell image;preprocessing cell images;method cell images;paper segmentation;paper segmentation method;markov random field;introduction paper segmentation;china restaurant process;focus cell image;cell images focus;images focus cell;random field mf;restaurant process model;restaurant process;images;china restaurant;images using;markov;using markov", "pdf_keywords": ""}, "396e942542904dd32d0d70daa39613e5a27cc059": {"ta_keywords": "collective classification methods;collective classification;collective classification predicts;existing collective classification;background collective classification;graphs related instances;maintaining large graphs;large graphs;related instances predicting;predicts class labels;large graphs related;instances predicting class;graphical models learning;instances predicting;expensive iterative inference;predicting class instance;class labels simultaneously;group related instances;iterative inference graphical;labels simultaneously group;collective;class labels;classification;predicting class;separately existing collective;related instances;classification predicts;classification predicts class;dataset large cost;iterative inference", "pdf_keywords": ""}, "d7851e80f6072991bc99e2157f05515564f894f4": {"ta_keywords": "phoneme g2p conversion;grapheme phoneme g2p;g2p conversion vocabulary;introduction grapheme phoneme;grapheme phoneme;phoneme g2p;conversion structured learning;text speech systems;g2p conversion structured;conversion vocabulary words;text speech;systems text speech;introduction grapheme;recognition systems text;conversion vocabulary;grapheme;g2p conversion;approach g2p conversion;speech systems;conversion structured;structured learning;speech systems aim;phoneme;structured learning based;learning based margin;approach g2p;art approach g2p;g2p;algorithm mira online;recognition systems", "pdf_keywords": ""}, "254491f0d981fb5d796c374287d439d8d1967088": {"ta_keywords": "cardiometabolic risk children;improves cardiometabolic risk;adolescents established vitamin;supplementation improves cardiometabolic;supplementation markers cardiometabolic;markers cardiometabolic risk;cardiometabolic risk;treatment cardiometabolic risk;effect vitamin supplementation;effect vitamin;vitamin supplementation effective;vitamin supplementation improves;vitamin supplementation;report effect vitamin;vitamin;improves cardiometabolic;established vitamin supplementation;vitamin supplementation markers;risk children adolescents;markers cardiometabolic;effective treatment cardiometabolic;established vitamin;supplementation effective;supplementation improves;treatment cardiometabolic;cardiometabolic;adolescents report effect;risk children;supplementation;supplementation effective treatment", "pdf_keywords": ""}, "e68762a32ec91587d9761030fc75a8f5ee71c45b": {"ta_keywords": "topic mixture language;topic mixture modeling;unsupervised topic mixture;approaches topic mixture;topic mixture;allocation topic tracking;latent dirichlet allocation;topic tracking;observations unsupervised topic;dirichlet allocation topic;modeling latent dirichlet;mixture language model;unsupervised topic;language model adaptation;dirichlet allocation;mixture modeling latent;latent dirichlet;mixture language;mixture modeling;model adaptation methods;observations unsupervised;uncertain observations unsupervised;model adaptation;language model;allocation topic;mixture;adaptation methods propose;unsupervised;topic;adaptation methods", "pdf_keywords": ""}, "e66ade4e28d9f401277194ed8feea5c6e9f18253": {"ta_keywords": "similarity terrorist groups;clusters terrorist groups;detecting clusters terrorist;operational similarity terrorist;terrorist groups sharing;clusters terrorist;terrorism intelligence monitoring;similarity terrorist;terrorist groups;counter terrorism intelligence;terrorism intelligence;counter terrorism;insights counter terrorism;terrorist groups critical;groups sharing similar;terrorism;operational similarity;tactics attacked targets;deployed tactics attacked;detecting clusters;dynamics operational similarity;behaviors focusing groups;attacked targets;tactics attacked;terrorist;deployed tactics;attacked targets utilized;clusters;groups sharing;tactics", "pdf_keywords": "terrorist organizations clustering;clustering terrorist organizations;patterns terrorist organizations;terrorist organizations similar;operational similarity terrorist;terrorist organizations clustered;cluster terrorist organizations;behaviors terrorist organizations;clusters terrorist organizations;similarity terrorist groups;terrorist organizations significantly;terrorist organization database;operational patterns terrorist;terrorist organizations impact;complex terrorist organizations;terrorist organizations modularity;active terrorist organizations;technique clustering terrorist;method clustering terrorist;terrorist database analyze;impact terrorist organizations;terrorist organizations;terrorist organizations period;ascertain terrorist organizations;clustering terrorist;terrorist behaviors coupling;terrorism dynamics terrorists;terrorist organization;terrorist organizations study;higher terrorist organizations"}, "82c4be27b0803c08c56bba4352669c1230a3ea19": {"ta_keywords": "regenerating codes;regenerating codes framework;original regenerating codes;storage networks;storage storage networks;storage nodes;methods distributed storage;storage networks node;distributed storage;distributed storage storage;storage nodes network;introductionregenerating codes efficient;chosen storage nodes;introductionregenerating codes;data reconstruction repair;storage storage;codes efficient methods;reconstruction repair resilient;storage;arbitrarily chosen storage;repair resilient;codes framework;codes framework introduced;codes efficient;chosen storage;repair resilient presence;original regenerating;regenerating;reconstruction repair accessing;extensions original regenerating", "pdf_keywords": "regenerating codes distributed;regenerating codes storage;repair coding schemes;codes distributed storage;codes storage distributed;efficient regenerating code;regenerating codes;codes storage;data regenerating code;repaired code optimal;regenerating codes present;method regenerating codes;use regenerating codes;conventional regenerating codes;resilient distributed storage;regenerating code;repair coding;codes distributed;exact repair coding;coding scheme distributed;regenerating code code;repair scheme decoders;storage networks;regenerating code given;named regenerating codes;decoding repair;valuable coding scheme;storage uses coding;reducing repair bandwidth;storage networks consider"}, "4c42d6412c080fef23ad95b4469efe9cf321ae5d": {"ta_keywords": "sequence automatic speech;semi supervised training;automatic speech;automatic speech recognition;unsupervised semi supervised;speech recognition ar;semi supervised;speech recognition;improvements semi supervised;sequencero sequence automatic;unsupervised semi;sequence automatic;supervised training models;supervised training;sequencero;training models;supervised training using;sequencero sequence;supervised;introduction sequencero;training models work;training using cycle;introduction sequencero sequence;speech;unsupervised;training using;recognition ar models;improvements semi;recognition ar;surge unsupervised semi", "pdf_keywords": "integrating unpaired speech;predict encoded speech;train synthetic speech;unpaired speech text;sequence automatic speech;use unpaired speech;speech unpaired text;speech text data;synthetic speech sentences;automatic speech;synthetic speech;unpaired speech;speech unpaired;speech recognition ar;unpaired speech unpaired;information unpaired speech;speech encoded speech;encoded speech;neural networks speech;speech encoded;shows unpaired speech;speech text;end speech recognition;speech recognition;encoded speech propose;semi supervised training;training automatic regressive;unsupervised training automatic;automatic speech recognition;speech text ain"}, "a714ca5254fb3cd7b06ead36d026c4eb154a7134": {"ta_keywords": "study fairness algorithmic;fairness algorithmic decision;fairness algorithmic;study fairness;mitigating ms disparate;algorithms exhibit disparate;ms disparate;ms disparate impact;exhibit disparate treatment;intentionally discriminate;disparate treatment;fairness;policy notions prejudice;discriminate;disparate treatment formally;require disparate treatment;disparate treatment methods;shape study fairness;algorithmic decision making;disparate impact;algorithmic decision;does mitigating ms;mitigating ms;disparate;decision making algorithms;intentionally discriminate proxy;notions prejudice;exhibit disparate;notions prejudice come;disparate impact require", "pdf_keywords": "mitigating based discrimination;formal discrimination algorithm;discrimination algorithm available;discrimination rule;discrimination rule short;algorithms identify discriminatory;use discrimination;discrimination algorithm;disparate learning;discriminatory intent;criteria disparate learning;discrimination treatment;use discrimination treatment;discriminatory;formal discrimination;based discrimination;discrimination aware;fundamental issue discrimination;optimality treatment disparity;discrimination attracted policy;induce class discrimination;identify discriminatory;discriminatory patterns;discrimination;issue discrimination aware;disconnect disparate learning;issue discrimination;disparate learning processes;discriminatory intent demonstrate;traditional discrimination rule"}, "69320030be096e78380a097810554b648e7409c0": {"ta_keywords": "bayesian speaker clustering;speaker clustering based;introduction speaker clustering;speaker clustering methodswe;preliminary speaker clustering;speaker clustering experiments;speaker clustering;bayesian speaker;fully bayesian speaker;clustering based utterance;dirichlet process mixture;utterance orientedd dirichlet;process mixture model;mixture model;dirichlet process;clustering methodswe carried;process mixture;mixture model aimthe;clustering based;clustering methodswe;clustering;non parametric bayesian;based utterance orientedd;clustering experiments using;orientedd dirichlet process;speaker;clustering experiments;parametric bayesian manner;dirichlet;algorithm uo dmm", "pdf_keywords": ""}, "0639cbb07ec3e03de7c8c1d828a90049c92cf5df": {"ta_keywords": "orthorhombic perovskites asno3;perovskites asno3 ca;ion orthorhombic perovskites;perovskites asno3;orthorhombic perovskites;spectroscopic properties ion;properties ion orthorhombic;perovskites;asno3 ca sr;energy zero phonon;asno3 ca;properties ion;octahedral mn bond;spectroscopic properties;ion orthorhombic;deviation octahedral mn;transition deviation octahedral;phonon line 2e4a2;study spectroscopic properties;asno3;zero phonon line;zero phonon;2e4a2 transition;comparative study spectroscopic;2e4a2 transition deviation;study spectroscopic;phonon line;spectroscopic;ion;deviation octahedral", "pdf_keywords": ""}, "62924cef027a66a75b5465ebb7a926c06f95790f": {"ta_keywords": "domain adversarial approaches;domain adversarial;proposed domain adversarial;adversarial approaches;introductiondomain adaptation addresses;introductiondomain adaptation;adversarial approaches consist;drifts source training;source training distribution;adversarial;adaptation addresses;source target;adaptation;source target encodings;source training;adaptation addresses common;target distribution generating;training distribution;target distribution;aligning source target;test data drifts;drifts source;target encodings;data drifts source;target encodings motivating;problem target distribution;theoretical bound target;bound target;training distribution absent;target", "pdf_keywords": "corresponding adversarial domain;adversarial domain;domain adversarial algorithms;distances domain adversarial;domain adversarial;domain adversarial learning;adversarial domain classifier;standard domain adversarial;domain classifiers adversarial;problem domain adversarial;minimized adversarial training;adversarial learning objectives;corresponding adversarial;domain adsversarial;adversarial training propose;optimized adversarial training;domain adaptation challenging;adversarial training;distances corresponding adversarial;practically minimized adversarial;domain adsversarial networks;adversarial training introduce;new distance adversarial;distance adversarial;function optimized adversarial;distance adversarial learning;minimized adversarial;domain adaptation generalize;optimized adversarial;ratio domain adversarial"}, "9b5cf607f9cd3eb5ef47d3597bb9360ea6034264": {"ta_keywords": "academia source unfairness;research peer review;peer review;scientific research peer;peer review called;peer review backbone;research peer;background peer review;source unfairness;review called biased;richer academia source;scientific research;richer academia;conferences methods prevalence;unscientific scientific disciplines;scientific disciplines problem;scientific disciplines;science conferences methods;academia source;peer;biased broken unscientific;science conferences;computer science conferences;getting richer academia;scientific;matthew effect rich;broken unscientific scientific;academia;conferences methods;backbone scientific research", "pdf_keywords": ""}, "9a7a4f125d8016e0fad9f6f5e9e0bca4e38b0784": {"ta_keywords": "scalable probabilistic logic;probabilistic logic;stochastic logic programs;extends stochastic logic;probabilistic logic called;stochastic logic;order probabilistic logic;probabilistic logic order;learning inference graphs;inference graphs;learning inference;new scalable probabilistic;scalable probabilistic;generated parameter learning;inference graphs using;efficient learning inference;logic programs;parameter learning;logic called proppr;learning parameter learning;proppr extends stochastic;second order probabilistic;logic programs slps;probabilistic;parameter learning parameter;logic order theories;learning parameter;inference;proppr new scalable;theories automatically generated", "pdf_keywords": ""}, "684e712f59f11d2bdc98be4c210824ab9e6f11f4": {"ta_keywords": "deep transfer learning;deep transfer;transfer learning;transfer learning approaches;latent relational graphs;modern deep transfer;vectors task transferable;embeddings language pretrained;generic latent relational;language pretrained convolutional;structured graphical representations;relational graphs capture;transfer unary features;graphical representations;latent relational;pretrained convolutional features;embeddings;learning generic latent;graphs capture dependencies;generic feature vectors;graphical representations methodsthis;tasks word embeddings;embeddings language;pretrained convolutional;learning generic feature;feature vectors;feature vectors task;word embeddings;transferable tasks word;relational graphs", "pdf_keywords": ""}, "22655979df781d222eaf812b0d325fa9adf11594": {"ta_keywords": "answering qa datasets;question answering;question answering qa;existing question answering;documents answer questions;answering qa;explanations answers methods;qa datasets;supporting documents answer;reasoning strong supervision;answer questions;answers methods;answer questions diverse;level supporting facts;train qa systems;answers methods introduce;reasoning provide explanations;finding reasoning;reasoning provide;sentence level supporting;existing knowledge bases;knowledge bases;finding reasoning multiple;complex reasoning provide;explanations answers;answering;knowledge bases knowledge;knowledge schemas provide;fail train qa;required reasoning strong", "pdf_keywords": "question answering dataset;question answering provides;question answering;reasoning multiple documents;task question answering;questions answers database;answering dataset aimed;answers database;supporting facts answering;knowledge base;existing knowledge base;natural language systems;knowledge base knowledgein;answering dataset;documents answer questions;dataset requires reasoning;answer questions hotpoq;answers database wikis;facts answering;diverse natural language;scale question answering;knowledge base knowledge;questions entities;facts answering question;finding reasoning multiple;reasoning diverse natural;questions test systems;reasoning diverse;supporting documents answer;questions answers"}, "0f726fcd676baff957574b223b99fd84163ebe6e": {"ta_keywords": "stacked graphical learning;meta learning scheme;relational datasets hyperlinked;stacked graphical model;learning given relational;graphical learning;meta learning;introductiontraditional machine learning;relational datasets;datasets hyperlinked web;relational template stacked;graphical models;machine learning methods;machine learning;datasets hyperlinked;citations social networks;learning methods;relational data thesis;learning scheme;graphical models demonstrated;stacked graphical;relational data;relational template;graphical learning given;study meta learning;learning scheme called;reality relational datasets;work graphical models;called stacked graphical;graphical model augments", "pdf_keywords": ""}, "4dfa9de9b3b2b222ddbdda934975bf608b8e1fda": {"ta_keywords": "dialogue systems research;collaborative conversations solving;collaborative conversations;engagement chatbots research;containing collaborative conversations;group conversations;collaborative dialogues;systems collaborative dialogues;user engagement chatbots;collaborative dialogues explored;engagement chatbots;dialogue systems;conversations solving;chatbots research systems;task oriented dialogue;chatbots research;conversations;focused dialogues interlocutors;group conversations previous;introduction dialogue systems;oriented dialogue restaurant;dialogues interlocutors largely;dialogues interlocutors;traditionally focused dialogues;dialogue restaurant bookings;chatbots;dialogue restaurant;dialogues;conversations previous research;dialogues explored area", "pdf_keywords": "generating conversations;conversations generating;conversations participants collaborate;method generating conversations;generating conversations generating;collaborative conversations;utterances probe conversation;conversations generating conversations;generating conversations simple;analyze group dialogues;human conversations;conversations participants;collaborative conversations solving;corpus human conversations;group dialogues;containing collaborative conversations;conversations solving cognitive;analyze conversations;group dialogues using;utterances multiparty discussions;analysis human conversations;conversations conversations;human conversations evaluate;conversations solving;containing conversations participants;conversations group;conversations comparable;linguistic features conversations;conversations context conversation;multiparty conversations conversations"}, "bdf6ad58338279634d647447751442db8a6e2f77": {"ta_keywords": "critical points saddle;converge point weight;weights converge point;points true minima;local minima;critical points;merely critical point;critical point;neural networks typically;neural networks;converged models;critical point possible;suggest weights converge;suggest converged models;local minima recently;weights converge;minima;accurate neural networks;neural;stuck local minima;saddle points;true minima descriptions;true minima;saddle points true;points saddle points;weight space local;majority critical points;point weight space;learning;minima recently researchers", "pdf_keywords": ""}, "88051a6dce3b67541d8096647da2f6d31daa9e9a": {"ta_keywords": "relation language models;latent relation language;knowledge graph relations;language models;probability entity spans;words document entities;language modeling;knowledge graph;language models lms;entity spans;improves language modeling;document entities occur;relation language;document entities;entities occur knowledge;entity spans given;occur knowledge graph;language models parameterizes;posterior probability entity;joint distribution words;graph relations methods;class language models;language modeling performance;probability entity;latent relation;distribution words document;relations methods model;annotate posterior probability;propose latent relation;graph relations", "pdf_keywords": "relationship language models;relation language models;models natural language;model natural language;conditional language models;language models natural;neural language models;conditional language modeling;language models;model onwikipedia articles;models text generation;language models conditioned;subgraph topic entity;neural knowledge language;generating models text;models text sequence;model onwikipedia;knowledge language model;neural language model;language modeling tasks;neural models text;topic entity;aware language model;vocabulary language modeling;latent relation language;conditioned knowledge graphs;language modeling open;knowledge graphs generative;language modeling;language models use"}, "bc33c151a375d30d85a99d4e269185bad360b7bf": {"ta_keywords": "efficiency electronic devices;enhancing efficiency electronic;applied device efficiency;device efficiency;device efficiency evaluated;efficiency electronic;method enhancing efficiency;enhancing efficiency;efficiency evaluated;efficiency;devices described method;electronic devices;electronic devices described;devices described;method applied device;devices;applied device;device;electronic;novel method enhancing;method enhancing;described method applied;described method;novel method;method applied;method;enhancing;evaluated;applied;novel", "pdf_keywords": ""}, "72ae4bba9aaa30dfba45f6e7e076952a76e2d751": {"ta_keywords": "conversational model;conversational model incorporates;dialog corpus model;role party conversations;methods conversational model;dialog corpus;present conversational model;conversational model function;lstm language model;conversations different architectures;ubuntu dialog corpus;memory lstm language;lstm language;conversations;party conversations;conversational;term memory lstm;model methods conversational;short term memory;party conversations different;present conversational;memory lstm;conversations different;paper present conversational;lstm;methods conversational;language generation model;dialog;language model;model language generation", "pdf_keywords": "conversation model incorporating;conversation model;build conversation model;conversational model;conversation role speaker;dialog corpus model;conversational model function;language model conversational;model conversational model;conversation use recurrent;specific conversation role;conversation model analyze;driven conversation model;conversation role;based conversation model;lm based conversation;conversation model use;dialog corpus;role party conversations;conversational model incorporates;layer conversations;output layer conversations;user driven conversation;dialogue corpus;model conversational;conversation model introduce;present conversational model;analyze conversations;build conversation;model analyze conversations"}, "9b52f250376e07c2caddb5f43b8db8b2f300bb51": {"ta_keywords": "health organization iranian;gender development world;gender development;sex gender development;organization iranian;world health organization;role sex gender;gender;sex gender;development world health;world health;health organization;iranian;topic role sex;role sex;development world;organization;health;systematic review literature;purpose article;purpose article present;article;review literature topic;review literature;systematic review;article present;results systematic review;development;article present results;literature topic", "pdf_keywords": ""}, "fd8b33299ce6ca81ce54e7d2de555a1a96ca96f1": {"ta_keywords": "hmm acoustic models;speech recognition ar;automatic speech recognition;automatic speech;speech recognition;model hmm acoustic;introduction automatic speech;model hmm;markov model hmm;sequences acoustic;observation sequences acoustic;sequences acoustic waveform;classify structured sequence;hidden markov model;hidden markov;gram language models;acoustic waveform sequential;hmm acoustic;structured sequence data;sequences sentences inferred;systems classify structured;label sequences sentences;combining hidden markov;language models;sequence data label;classify structured;data label sequences;speech;recognition ar;generative classifiers", "pdf_keywords": ""}, "457e1c9476f08fa2c253982e3effcb364487073e": {"ta_keywords": "mutation adnexal cells;adnexal cells adnexal;adnexal cells;cells adnexal cells;model adnexal cells;mutation adnexal;cells adnexal;adnexal cells rare;specific mutation adnexal;model adnexal;adnexal;new model adnexal;sex specific mutation;cells rare;cells rare event;mutation;cells;specific mutation;occurrence sex specific;occurrence sex;sex specific;model;rare event development;new model;rare event;development new model;sex;development;development new;rare", "pdf_keywords": ""}, "b80ce55fbb4aa427439009985c0ce28a34324dc6": {"ta_keywords": "nutritional supplements pregnancy;supplements pregnancy important;supplements pregnancy;malnutrition supplement;malnutrition malnutrition supplement;malnutrition supplement form;nutritional supplements;use nutritional supplements;woman diagnosed malnutrition;supplement form nutritional;diagnosed malnutrition;form nutritional supplements;diagnosed malnutrition malnutrition;use nutritional;malnutrition;nutritional;malnutrition malnutrition;supplements;form nutritional;supplement;important pregnancy;pregnant woman diagnosed;pregnancy important pregnancy;pregnancy important;important pregnancy care;pregnancy care present;pregnancy care;pregnancy;pregnant woman;case pregnant woman", "pdf_keywords": ""}, "e23c5dafc718f9e55ccf7729ce2d2834b650540a": {"ta_keywords": "bayesian speaker clustering;speaker clustering based;speaker clustering method;speaker clustering;novel speaker clustering;fully bayesian speaker;bayesian speaker;utterance oriented dirichlet;dirichlet process mixture;speaker variability;intra speaker variability;speaker variability successfully;mixture model methods;hierarchically structured utterance;process mixture model;clustering based hierarchically;dirichlet process;nonparametric bayesian manner;structured utterance oriented;mixture model;oriented dirichlet process;mixture model proposed;intra speaker;structured utterance;nonparametric bayesian;process mixture;bayesian manner intra;speaker;using nonparametric bayesian;proposed novel speaker", "pdf_keywords": ""}, "773e752ab6dc04b43aaf984bcbdd4895c9ab8c2f": {"ta_keywords": "continuous speech recognition;speech recognition usingwft;vocabulary continuous speech;speech recognition;backgroundlarge vocabulary continuous;linear classifier structured;continuous speech;backgroundlarge vocabulary;based linear classifier;recognition usingwft based;classifier structured;classifier structured data;gram models;markov models gram;linear classifier;vocabulary continuous;recognition usingwft;wftbased decoding;models gram models;usingwft based linear;models gram;reviews wftbased decoding;process linear classifier;gram models reviews;wftbased decoding process;scores decoding graph;decoding graph composed;decoding graph;scores decoding;hidden markov models", "pdf_keywords": ""}, "510aef8370d82c4c4ec50de0f645f34f11e549a7": {"ta_keywords": "protein entity recognition;recall protein entity;semicfs dictionary highmms;dictionary summary protein;entity recognition;entity recognition using;introductionhigh recall protein;semicfs dictionary;recall protein;summary protein extraction;task semicfs dictionary;mining biological literature;protein extraction;protein entity;protein extraction important;dictionary highmms resultsthe;recognition using dictionary;dictionary highmms;using dictionary;cfs datasets improvement;using dictionary summary;introductionhigh recall;cfs datasets;normal cfs datasets;summary protein;mining biological;methods task semicfs;semicfs;step mining biological;task semicfs", "pdf_keywords": ""}, "7ddddea393c2cd70fe716e2dfc5d77daf58449c0": {"ta_keywords": "content influence twitter;influencece social networks;influence twitter;person content influence;influence twitter methodsusing;ego people retweets;people retweets;people retweets alters;twitter methodsusing ego;person influencece social;person influencece;person person influencece;introductioninducing influencecers;content influence;influencecers evaluating person;granger causality examine;influencece social;introductioninducing influencecers evaluating;granger causality introduce;granger causality;networks using granger;influencecers evaluating;retweets alters;influence;retweets;framework granger causality;using granger causality;influencecers;twitter methodsusing;influencece", "pdf_keywords": "person influence twitter;influence twitter methodsusing;people retweets alters;content influence twitter;identify topics tweets;disinformation spread online;trump people retweets;data trump twitter;influence twitter;twitter methodsusing ego;topics tweets used;online disinformation various;influence twitter using;people retweets;trump twitter;online disinformation;tweets topics;retweets alters;topics tweets;clustering tweets likely;mis disinformation spread;medoids clustering tweets;clusters topics tweets;spreading mis disinformationwe;clustering tweets;twitter methodsusing;tweets likely relatedthe;tweets;twitter using ego;trump twitter archive"}, "1890775da6ba2627a5d6c17a639e2dca7cdc388d": {"ta_keywords": "patient presented speech;speech recognition everyday;speech;presented speech recognition;speech recognition overlooked;presented speech;expression speech;speech recognition;recognition everyday practice;expression speech key;element speech recognition;element speech;speech key;key element speech;recognition overlooked routine;recognition everyday;speech key element;patient presented;recognition overlooked;practice case patient;routine practice practice;recognition;practice practice case;case patient presented;routine practice;overlooked routine practice;everyday practice;practice case;patient;case patient", "pdf_keywords": ""}, "ccad27088b9098de4eaca8dc449b18766db4b3ab": {"ta_keywords": "style transfer inherently;unsupervised style transfer;style transfer;style transfer modifying;style transfer systems;transfer modifying style;transfer systems paraphrases;reformulate unsupervised style;designed style transfer;outputs style transfer;unsupervised style;transfer changes semantic;systems paraphrases inputs;paraphrases inputs existing;paraphrases inputs;modifying style;systems paraphrases;modifying style given;attribute transfer;changes semantic properties;paraphrases;changes semantic;task style transfer;properties sentiment methodsin;style given sentence;changing semantics;semantic properties sentiment;attribute transfer changes;sentiment methodsin;appreciably changing semantics", "pdf_keywords": "style transfer paraphrase;transfer paraphrase generation;diverse paraphrasing pretrained;paraphrases use style;normalization introducing paraphrase;paraphrase model style;style transfer evaluation;style transfer terms;style normalization introducing;paraphrasing pretrained language;paraphrasing pretrained;syntactically diverse paraphrasing;simple style transfer;transfer paraphrase;prior style transfer;paraphrase generation;unsupervised style transfer;transfer systems paraphrases;diverse paraphrasing;paraphrase generation use;style normalization;use style transfer;doing style transfer;style transfer propose;paraphrase model achieves;style transfer;approach style normalization;current style transfer;output paraphrases optimizing;paraphrase model efficient"}, "703a8252585948a96f5815025f7f03d68033b8bf": {"ta_keywords": "training agent bots;dialog systems trained;human dialogs;clinical messagetask oriented;human human dialogs;clinical messagetask;key clinical messagetask;messagetask oriented dialog;human dialogs collected;dialog systems;dialogs;messagetask oriented;bots self play;agent bots;approaches training agent;dialog;oriented dialog systems;agent bots user;dialogs collected;user bots self;messagetask;systems trained human;bots self;training agent;api;trained human human;trained human;dialogs collected wizard;bots user bots;user bots", "pdf_keywords": "conversational game systems;dialog agents;dialog agents self;dialog agent;dialog systems trained;training agent bots;collaborative dialogs;oriented dialog agents;conversational game;collaborative dialogs implement;role conversational game;human dialogs;development conversational agent;oriented dialog agent;human human dialogs;conversational agent;tool collaborative dialogs;conversational game theory;human dialogs collected;dialogue goal oriented;dialog agent working;development conversational game;dialog systems;task oriented dialog;bots self play;bots learned using;agent bots;self play task;agents self play;play collaborative task"}, "fa6c76d466fef633df51745bad85e991c371622c": {"ta_keywords": "speech recognition fundamental;speech recognition important;speech recognition everyday;complexity speech recognition;process speech recognition;speech recognition;understand complexity speech;complexity speech;process speech;fundamental process speech;recognition important understand;recognition fundamental process;speech;important understand complexity;recognition fundamental;recognition everyday;understand complexity;recognition important;recognition everyday everyday;recognition;complexity;everyday everyday environments;everyday environments;process;fundamental process;important understand;fundamental;environments;everyday everyday;understand", "pdf_keywords": ""}, "41a47363d261459c594525ef330e5fccaa8518a0": {"ta_keywords": "features authorship attribution;authorship attribution models;authorship attribution accuracy;authorship attribution;authorship attribution methodsmethodsmethodsa;applied authorship attribution;affect authorship attribution;features authorship;features affect authorship;analysis applied authorship;useful features authorship;authorship;approach authorship;affect authorship;existing approach authorship;applied authorship;attribution models based;attribution models;attribution accuracy;attribution methodsmethodsmethodsa systematic;attribution accuracy varying;attribution;attribution methodsmethodsmethodsa;introductiontopic style;introductiontopic style exploring;methodsmethodsmethodsa systematic analysis;systematic analysis widely;widely used datasets;systematic analysis;analysis widely used", "pdf_keywords": ""}, "98e6197e21ae530cd33eeff144ee556c5cf91dc8": {"ta_keywords": "cognitive tutors;environment cognitive tutors;cognitive tutors author;manually write cognitive;intelligent authoring;model writing cognitive;writing cognitive model;build intelligent authoring;cognitive model writing;write cognitive model;intelligent authoring environment;automatically generates cognitive;authoring environment cognitive;generates cognitive model;writing cognitive;learning agent;write cognitive;student automatically generates;cognitive model sample;siulated student automatically;study build intelligent;trained cognitive scientist;learning agent called;student automatically;tutors author need;trained cognitive;tutors author;tutors;cognitive model;generates cognitive", "pdf_keywords": ""}, "af679d69fcc1d0fcf0f039aba937853bcb50a8de": {"ta_keywords": "nested linear attention;linear attention functions;softmax attention;softmax attention nested;linear attention;transformer attention;attention nested linear;transformer attention mechanism;unified nested attention;approximates softmax attention;complexities transformer attention;nested attention;nested attention mechanism;attention functions;attention functions yielding;attention mechanism approximates;attention nested;attention mechanism;attention mechanism limited;softmax;traditional attention mechanism;mechanism approximates softmax;attention;approximates softmax;attention mechanism luna;memory complexities transformer;modeling long sequences;transformer;traditional attention;methodsa traditional attention", "pdf_keywords": "nested linear attention;softmax attention transformer;softmax attention;unified nested attention;linear attention;softmax attention nested;attention operation linearly;regular softmax attention;linear attention functions;nested attention functions;softmax attention introduce;attention architecture;attention nested linear;effective linear attention;attention transformer;approximates softmax attention;nested attention;attention mechanism analysis;linear attention mechanism;uses nested attention;attention operation;attention nested;attention functions approximate;attention functions yielding;attention functions;nested attention mechanism;softmax attention arxiv;attention mechanism approximates;attention transformer compared;attention architecture luna"}, "682e69be87f181edcf71800b54083595874d4ec6": {"ta_keywords": "learning hierarchical models;learning hierarchical;improve learning hierarchical;hierarchical models methods;hierarchical models;methods hierarchical models;models methods hierarchical;hierarchical models utilized;task hierarchies predictions;subtasks predictions networks;trained subtasks predictions;methods hierarchical;trained subtasks;networks trained subtasks;characterized task hierarchies;hierarchical;task hierarchies;predictions smaller subtasks;subtasks useful trying;subtasks predictions;hierarchies predictions;task typically neural;predictions networks;subtasks useful;predict final task;predictions networks subsequently;subtasks;typically neural networks;neural networks;hierarchies", "pdf_keywords": "subtasks predictions networks;networks trained subtasks;trained subtasks predictions;learning hierarchical;predicting speaker;predictions networks;model predicting persuasion;predicting speaker persuasiveness;improving learning hierarchical;sparsely trained intermediate;learning hierarchical modelsthe;predicting persuasion;level speaker traits;trained intermediate network;method predicting persuasion;predicting persuasion online;trained subtasks;predictions networks subsequently;predictions smaller subtasks;networks trained;intermediate trait networks;typically neural networks;persuasion semi supervised;performance predicting speaker;trait networks;neural networks;task hierarchies predictions;ability predict persuasion;novel neural architecture;subtasks predictions"}, "7c8314e6138ce968f3b9f3bc55d5461ffbbec4aa": {"ta_keywords": "optimization tourism route;route using genetic;optimization tourism;tourism route using;tourism route;using genetic algorise;using genetic;genetic algorise;genetic algorise algorise;tourism;genetic;algorise algorise algorise;route;optimization;algorise algorise;algorise;route using;using", "pdf_keywords": ""}, "97883f37c62b4b0e52cdc31dea1a375597db3804": {"ta_keywords": "fixed deep neural;fixed neural network;task masks learned;learn binary masks;use fixed neural;single fixed deep;fixed neural;new task masks;binary masks;network quantization;neural networks work;task masks;concepts network quantization;masks learned;network quantization sparsification;deep neural network;neural network important;binary masks piggyback;learned tasks;performance learned tasks;deep neural;task neural networks;quantization sparsification learn;neural network;task neural;important task neural;tasks single fixed;sparsification learn binary;fixed deep;neural networks", "pdf_keywords": "trained task imagenet;training deep;pre trained network;architecture pre trained;training deep neural;task imagenet classification;work training deep;task imagenet;trained network;convolutional network starting;pre trained task;convolutional network;fully convolutional network;deep neural;weights network;existing weights learn;model imagenet;imagenet classification;weights learn binary;trained task;imagenet;fixed weights network;datasets network architectures;approach model imagenet;trained network diverse;fixed deep neural;imagenet object classification;dataset network architecture;pre trained model;layer architecture pre"}, "e4c8447e56fc9cc3867087748acc4b259b9efe19": {"ta_keywords": "training recurrent neural;recurrent neural networks;training recurrent;recurrent neural;introduction training recurrent;external linguistic knowledge;external linguistic;neural;subgraphs introduce model;linguistic knowledge explicit;sequence typed edges;use external linguistic;model memories;recurrent;directed acyclic subgraphs;neural networks;inform model memories;external knowledge;linguistic knowledge;neural networks model;knowledge explicit signal;specifically external knowledge;subgraphs introduce;acyclic subgraphs introduce;external knowledge used;augment sequence typed;introduce model encodes;long term dependencies;knowledge explicit;model memories utilize", "pdf_keywords": "text recurrent neural;model text comprehension;recurrent network;information recurrent network;recurrent network important;memory recurrent neural;recurrent neural network;recurrent neural networks;network recurrent neural;neural network recurrent;network recurrent;text comprehension tasks;training recurrent neural;explicit memory recurrent;comprehension use coreference;representations recurrent neural;information recurrent;base reading comprehension;memory recurrent;introduction training recurrent;coreference annotators;training recurrent;neural network memory;model recurrent neural;text comprehension;output representations recurrent;model coreference relations;recurrent neural;text recurrent;use coreference annotators"}, "be8d6a8d3dfe87a4d9171f25bf9a18d502498756": {"ta_keywords": "fuzzy graph clustering;meta algorithm fuzzy;algorithm fuzzy graph;watset meta algorithm;graph clustering algorithm;graph clustering;fuzzy graph;hard clustering discover;clustering discover;clustering algorithm;hard clustering;algorithm fuzzy;clustering discover clusters;discover clusters disambiguated;meta algorithm;clusters disambiguated second;clusters disambiguated;clustering algorithm creates;uses hard clustering;clustering;watset meta;analysis watset meta;discover clusters;computational analysis watset;clusters;analysis watset;watset shows competitive;complexity demonstrate watset;input graph;representation input graph", "pdf_keywords": ""}, "bd1cf4279d834699db871e1451d289c49ff2b6de": {"ta_keywords": "dance convolution;dance dance convolution;dance convolution popular;background dance;background dance dance;dance dance;perform steps dance;steps dance;dance;learning choreograph;rhythm based video;steps dance platform;dance platform synchronization;learning choreograph results;learning choreograph methodswe;rhythm based;convolution popular rhythm;choreograph;dance platform;rhythm;choreograph methodswe introduce;choreograph methodswe;task learning choreograph;popular rhythm based;synchronization music directed;step charts;audio track;players perform steps;choreograph results;step chart task", "pdf_keywords": "dance convolution algorithms;dance convolution useful;benchmarking dynamic dance;dance convolution;neural networks music;dance dance convolution;automatic rhythmic choreography;choreography using neural;rhythm based video;perform steps song;approach automatic rhythmic;music develop convolutional;rhythmic choreography;automatic rhythmic;choreography num songs;steps dance;rhythmic choreography num;perform steps dance;steps song significantly;audio track;dynamic dance;music information retrieval;audio track goal;rhythm;steps song;dance;rhythmic patterns music;rhythm based;step placement neural;steps dance platform"}, "cf46ecac1cb1bdae153be2b909ff3e313034ac9e": {"ta_keywords": "social skills training;social skills communication;social communication skills;social skills;autism spectrum;existing social skills;autism;autism spectrum disorders;characteristics autism;trouble social skills;characteristics autism spectrum;traits characteristics autism;skills communication;social communication difficulties;communication skills;difficulties improve social;communication skills methodsthis;skills communication greater;communication difficulties improve;improve social communication;skills training aid;social communication;improve social;skills training;communication difficulties;enable people social;people social communication;context existing social;skills methodsthis;skills", "pdf_keywords": ""}, "ef59f05a30972742a714b8903848e4b5dfc5cdaf": {"ta_keywords": "interpretable machine learning;interpretable machine;interpretable;evaluation actionable taxonomy;field interpretable machine;evaluation actionable;actionable taxonomy methods;methods evaluation actionable;actionable taxonomy;machine learning im;researchers consumers illustrated;machine learning;consumers illustrated;actionable;field interpretable;conceptualize;im methods evaluation;tool conceptualize;researchers consumers;goals consumers use;im methods;persists technical objectives;consumers use;serves tool conceptualize;technical objectives targeted;consumers use cases;learning;goals consumers;consumers;objectives targeted researchers", "pdf_keywords": ""}, "9b9ee9a25fc4d9f8ad22c2923c49b8d5d0b83356": {"ta_keywords": "aware hypernymy extraction;extracting disambiguated hypernymy;sense aware hypernymy;hypernymy extraction methodswe;improve hypernymy extraction;hypernymy extraction;hypernymy extraction common;hypernyms sets synonyms;disambiguated hypernymy relationships;unsupervised sense representations;disambiguated hypernymy;hypernyms sets;propagates hypernyms sets;introductionunsupervised sense aware;hypernymy relationships propagates;extracting disambiguated;synonyms synsets;introductionunsupervised sense;synonyms synsets constructs;relationships propagates hypernyms;sense representations;unsupervised sense;propagates hypernyms;sets synonyms synsets;aware hypernymy;hypernyms;method extracting disambiguated;sense representations used;synsets constructs embeddings;hypernymy relationships", "pdf_keywords": "extracting disambiguated hypernymy;hypernyhypernymy semantic relation;hypernyms sets synonyms;informative hypernyms synset;disambiguated hypernymy relationships;recognizes hypernyhypernymy semantic;hypernyhypernymy semantic;analyzing hypernymy relations;lexical databases wordnet;wordnet disambiguate words;standard hypernymy extraction;identifying informative hypernyms;hypernymy extraction;sense aware hypernymy;german hypernymy extraction;hypernymy extraction using;hypernymy extraction task;propagate hypernymy relationships;synsets important hypernyms;generate hypernymy relationships;approach analyzing hypernymy;synsets propagate hypernymy;disambiguated hypernymy;relationships propagate hypernyms;approach generalizes hypernyms;hypernymy extraction method;hypernyms given synsets;hypernyms synset;relationships relevant wordshypernymy;method hypernymy extraction"}, "923ddc71f8a453c7995e97b0681a674224a5fc09": {"ta_keywords": "translation errorror analysis;accuracy machine translation;machine translation errorror;methodss machine translation;machine translation systems;machine translation;translation systems;manual error analysis;errorror selection methodss;human translators methods;errorror analysis methods;translators methods proposed;translation errorror;translation systems varying;translators methods;errorror analysis;errorror selection;translations references translated;translations references;error analysis;error analysis used;analyzing errors proposed;independently human translators;human translators;analyzing errors;differences translations references;error analysis results;errors proposed methods;errors proposed;references translated independently", "pdf_keywords": ""}, "407eacc5ade80b54126c300b57b81f4b4f411487": {"ta_keywords": "quality machine translation;machine translation increased;professional human translation;human translation;machine translation;translation increased remarkably;human translation number;news translation showing;translation showing;translation increased;news translation;investigation china english;translation showing finding;translation number;translation number empirical;translation;human machine parity;english news translation;china english news;china english;evaluation;machine parity;quality machine;introductionthe quality machine;human machine;finding human machine;english news;quality;evaluation design;english", "pdf_keywords": "machine translation evaluating;evaluation machine translation;human translation significantly;human machine translation;translation evaluating documents;human translations experts;translation quality assessment;machine translation increased;automatic translation quality;human translation outputs;human automatic translation;general translation quality;translation evaluating;machine translation human;translation quality;humans quality translations;quality machine translation;machine translation test;machine translations;accurate machine translation;professional human translation;translation human translation;outputs human translations;quality underlying translations;quality translations;expert translators;assessing parity translations;translator machine translation;machine translation availability;development human translations"}, "4bf1ea102e1eb1246929bb77c11ebbd6b6d27500": {"ta_keywords": "introductionprototype driven text;driven text generation;sentence prototypes;sentence prototypes modify;text generation;library sentence prototypes;text generation uses;introductionprototype driven;prototypes reference training;prototype generate;introductionprototype;driven text;training corpus existing;text effective methods;corpus existing methods;training corpus;identify prototypes reference;prototypes modify;generate output text;entire training corpus;prototypes reference;prototype generate output;modify prototype generate;corpus existing;heuristics identify prototypes;identify prototypes;prototypes modify prototype;corpus;prototypes;output text effective", "pdf_keywords": ""}, "93a55f3341aa70bb42c0f76b112e2e8da27b3df2": {"ta_keywords": "entrainment affects dialogue;ebdm dialogue based;dialogue based entrainment;selection ebdm dialogue;ebdm dialogue;entrainment factor dialogue;dialogue affects human;dialogue human machine;response selection ebdm;human dialogue human;human human dialogue;human dialogue;dialogue human;dialogue affects;affects dialogue;affects dialogue abstract;dialogue abstract structural;dialogue;dialogue based;dialogue abstract;interaction entrainment lexical;introduction response selection;entrainment lexical level;machine interaction entrainment;interaction entrainment;selection ebdm;response selection;factor dialogue affects;entrainment lexical;entrainment analysis", "pdf_keywords": ""}, "bed0452305633791340f80cb0be02f46e4a34b0d": {"ta_keywords": "voice conversion challenge;voice conversion;approach voice conversion;baseline voice conversion;key voice conversion;voice conversion vc;transcribe input speech;seq2seq baseline voice;speech processing toolkit;end speech processing;input speech automatic;speech recognition ar;conversion challenge vc;speech automatic speech;speech automatic;automatic speech;conversion challenge 2020;input speech;baseline voice;conversion challenge;speech processing;automatic speech recognition;speech recognition;conversion vc transcribe;toolkit key voice;voice;sequence seq2seq baseline;using transcriptions;transcribe input;conversion", "pdf_keywords": "voice conversion challenge;voice conversion;new voice conversion;voice conversion development;approach voice conversion;introduction voice conversion;voice conversion techniques;voice conversion variety;voice conversion technique;voice conversion vc;objectivethe voice conversion;bilingual tt models;tasks converting speaker;different voice conversion;neural vocoder converting;neural text speech;source speech waveform;bilingual tt model;based bilingual tt;seq2seq based auditory;speech waveform;transcribe input speech;vector based bilingual;automatic speechthe vc2020;tt models neural;speaker converting;neural vocoder;text speech systems;method bilingual tt;vocoder converting speaker"}, "ce97452d031a1a156212f038bab6f47a51575236": {"ta_keywords": "recognition stance taking;automatic recognition stance;text stance taking;stances negotiations;stance taking activity;stance taking spontaneous;stance taking;people stances negotiations;recognition stance;taking spontaneous speech;stances negotiations decision;exclusively text stance;subjectivity sentiment analysis;spontaneous speech;speech;people stances;text stance;subjectivity sentiment;spontaneous speech methods;speech methods presents;sentiment analysis;stances;speech methods;stance;subjectivity;areas subjectivity sentiment;sentiment analysis received;activity carried verbally;sentiment;budget people stances", "pdf_keywords": ""}, "995f4e670c0cdcd5afdef08719c2528a682bff05": {"ta_keywords": "speech translation model;speech recognition ar;intermediate automatic speech;end speech translation;aimthe multi decoder;speech translation;automatic speech;anr machine translation;multi decoder md;ar decoder;automatic speech recognition;multi decoder;high translation quality;recognition ar decoder;translation model;translation quality searching;speech recognition;ar decoder states;translation model demonstrated;decoder md end;decoder md;machine translation;machine translation sub;decoder;translation quality;decoding model;translation sub tasks;pass decoding model;intermediates pass decoding;tasks decoding", "pdf_keywords": "speech translation decoding;speed improve decoding;improved decoding speed;improve decoding speed;model improve decoding;improved decoding;translation decoding;ar decoder ct;intermediate ar decoder;speech translation task;improve decoding;speech translation model;decoding speed improve;performs nonar decoding;nonar decoding;new model decoding;decoder ct output;decoder ct;speech recognition ar;transformer ar decoder;new approach decoding;increasing decoding speed;increase decoding speed;intermediate automatic speech;multi decoder;increase decoding;ar decoding;increasing decoding;multi decoder md;decoding decoding na"}, "e2198b039ee5bfa233cf06e65f26a9f3233ada9f": {"ta_keywords": "dialogue detrainment factor;dialogue detrainment;dialogue participants converge;dialogue participants;dialogue act word;dialogue speech;speech different;comparing speech;speech different speakers;comparing speech different;dialogue speech speaker;act word selection;switchboard corpus comparing;dialogue act;selection observable dialogue;speech speaker different;observable dialogue detrainment;corpus comparing speech;dialogue;studies dialogue participants;word selection;switchboard corpus;pattern dialogue act;using switchboard corpus;parts dialogue speech;speech;speech speaker;pattern dialogue;observable dialogue;studies dialogue", "pdf_keywords": ""}, "29da62b3f8aed3fe98b3f02bbfd436dd8e65a532": {"ta_keywords": "utility wireless networks;maximize throughput;wireless networks;maximize throughput based;throughput based utility;throughput;approach maximize throughput;networks message passing;terminals symmetric channels;wireless networks message;evaluation real networking;throughput based;real networking scenarios;guarantees performance protocols;symmetric channels;performance protocols;real networking;networking scenarios preliminary;networking scenarios;utility wireless;performance protocols evaluation;evaluation optimal cm;networking;symmetric channels drive;message passing synchronization;based utility wireless;networks message;optimal cm;protocols evaluation real;protocols evaluation", "pdf_keywords": ""}, "4264599665522594d9ecb521dd2e1d002e85a961": {"ta_keywords": "matching reviewers papers;automatically matching reviewers;matching reviewers;paper matching algorithms;common paper matching;reviewers assigned paper;automatically matching;paper matching;reviewers papers;reviewers papers crucial;matchings suffering critical;matching algorithms;peer review process;matching algorithms construct;novel local fairness;local fairness formulation;algorithms construct matchings;matchings;reviewers assigned;peer review;fairness formulation paper;matching;group reviewers assigned;group reviewers;formulation paper matching;fairness formulation;reviewer workloads highly;reviewers;local fairness;expertise reviewer workloads", "pdf_keywords": "fairness paper matching;fair matching algorithms;algorithms fair matching;fair matching algorithm;reviewer paper matching;matching algorithms fair;matching papers reviewers;constructing fair matchings;efficient fair matching;match papers reviewers;paper matching algorithms;paper matching algorithm;based paper matching;fair matchings faster;matching algorithms competitive;fair matchings;common paper matching;ofthe paper matching;best matching algorithms;algorithm paper reviewer;propose fair matching;matching algorithms propose;paper reviewer assignment;algorithms propose fair;matching algorithms;fair matching;algorithms construct matchings;matching algorithms best;paper matching problem;algorithms fair"}, "1578fba4a2b2ba819986e32c7da6ebbaf9aacf41": {"ta_keywords": "crosslinguality context morphology;context morphology aimto;morphological analysis lemmatization;context morphology;crosslinguality context;morphology aimto;morphological analysis;morphology;morphological;morphology aimto submission;2019 task morphological;lemma morpho syntactic;morpho syntactic description;lemmatization context methodsthis;lemmatization context;task morphological analysis;analysis lemmatization context;morpho syntactic;task crosslinguality context;task morphological;crosslinguality;syntactic description;lemmatization;task crosslinguality;syntactic;syntactic description token;analysis lemmatization;introductioncmu 01 sigorhon;sharedd task crosslinguality;produce lemma morpho", "pdf_keywords": "model contextual morphological;contextual morphological;contextual morphological analysis;2019 task morphological;lingual transfer learning;task morphological analysis;grained morphological features;language transfer learning;treebanks;multilingual model morphosyntactic;language transfer model;gender language;107 treebanks approach;treebanks approach task;grained morphological;morphological analysis verbs;morphological features incorporating;morphological analysis shared;terms gender language;multi lingual transfer;treebanks approach;task morphological;107 treebanks;learning present multilingual;gender language specific;sequence 107 treebanks;languages learning;multi language transfer;multilingual model;morphological analysis lemmatization"}, "6e2e7df21a5b5457ea4167133a40bc729028250d": {"ta_keywords": "neural ranking;neural ranking stack;relevance labels observed;modern neural ranking;relevance labels;queries passage ranking;neural ranker;ranking stack judged;judged relevant items;returned neural ranker;passage ranking development;passage ranking;current information retrieval;judged relevant item;ranking;set relevance labels;sparse set relevance;information retrieval;preference judgments item;retrieval requires sparse;ranking stack;neural ranker appear;stack judged relevant;items employed crowdsourced;retrieval;preference judgments;information retrieval requires;judgments item;items returned neural;set relevance", "pdf_keywords": "neural ranking stack;neural ranking;retrieval results recent;modern neural ranking;information retrieval results;information retrieval tasks;retrieval results;core information retrieval;neural ranker best;neural ranker compare;retrieval information retrieval;information retrieval evaluation;retrieval evaluation;neural rankers;neural ranker;neural ranker nearly;retrieval tasks observed;deep learning search;retrieval tasks;neural ranker qrels;information retrieval;document ranking efforts;learning search;modern neural ranker;crowdsource comparisons results;modern neural rankers;retrieval information;retrieval systems;known answer ranking;ranking ongoing identification"}, "80257b7d02ad4d6a762ebc0d7f1560e0ef182354": {"ta_keywords": "polite sentences preserving;automatically labeled politeness;politeness transfer;politeness encourage benchmark;politeness transfer involves;sentences polite;labeled politeness;converting non polite;polite sentences;task politeness transfer;polite sentences polite;non polite sentences;sentences polite sentences;labeled politeness encourage;new task politeness;politeness;task politeness;politeness encourage;sentences preserving;sentences preserving meaning;sentence target style;generates sentence target;polite;generates sentence;non polite;sentence target;identifies stylistic attributes;target style preserving;subsequently generates sentence;style preserving", "pdf_keywords": "politeness transfer generate;style transfer tasks;formality style transfer;style transfer task;politeness transfer automatic;polite sentences preserving;politeness transfer task;politeness transfer fundamentally;politeness transfer;style transfer techniques;style transfer nonparallel;style transfer;use style transfer;transfer generate polite;task politeness transfer;style tagger trained;source style transfer;preserving style transfer;form politeness transfer;generate polite;linguistic strategies politeness;corpus email conversations;text polite sentences;politeness transfer provide;types style transfer;style tagger;tasks like sentiment;method style transfer;converting non polite;sentences preserving"}, "09093e29b1f705bb7a68ea2e9240b3f122efe92b": {"ta_keywords": "speech recognition speech;recognition speech recognition;speech recognition report;technique speech recognition;speech recognition;recognition speech;speech recognition important;tool speech recognition;new technique speech;recognition important tool;tool speech;technique speech;important tool speech;recognition report;speech;recognition report use;recognition;recognition important;new technique;important tool;use new technique;use new;tool;use;technique;new;report use new;important;report use;report", "pdf_keywords": ""}, "3edfccbe6adf18f5263cd2adf3d977bbc5811e0b": {"ta_keywords": "augmentation method attention;data augmentation end;speech recognition e2e;novel data augmentation;end automatic speech;attention based end;data augmentation;automatic speech;data augmentation method;augmentation end end;automatic speech recognition;augmentation end;speech recognition;style data augmentation;attention based;method attention based;backgroundback translation;augmentation;backgroundback translation style;augmentation method;attention;translation style data;recognition e2e;method attention;recognition e2e ar;end end automatic;promising method attention;end automatic;e2e ar promising;translation style", "pdf_keywords": "text speech synthesis;translation models speech;speech text models;neural text encoder;sequence characters training;attentional paired speech;augmentation method attentionbased;text speech;speech synthesis;text paired speech;neural machine translation;decoder paired speech;automatic speech;paired speech text;augmentation method attention;speech recognition e2e;attentionbased attentional paired;translation build neural;speech text speech;text speech text;speech text data;end automatic speech;attention based end;neural textto encoder;machine translation models;speech text;novel data augmentation;machine translation build;neural networks speech;training textto encoder"}, "e6aaac94df717786a467d057cb2157b9d49f0974": {"ta_keywords": "game theoretic learning;regret algorithms;regret algorithms created;algorithm measured regret;regret regret algorithms;measured regret methods;regret methods;learning agents simultaneously;game theoretic;learning agents;regret methods context;measured regret;theoretic learning agents;performance learning algorithm;introduction game theoretic;learning algorithm measured;measured regret regret;performance learning;theoretic learning;terms game theor;regret;regret regret;equal terms game;learning algorithm;learning;stationary player perspective;game theor;introduction game;context performance learning;terms game", "pdf_keywords": "game theoretic learning;learning continuous games;learning strategies player;games proposed learning;learning games proposed;methods learning games;learning games;learning player learning;player learning;learning rate players;learning rate game;game learning;learning strategy variationally;games continuous action;continuous games;learning player;learn game;learning player fundamental;regret players bounded;learning strategy;finite games;regrets finite games;finite games based;players behavior optimal;learning agents;learning player simple;learning agents simultaneously;player algorithms;player learning player;step learning player"}, "efaf07d40b9c5837639bed129794efc00f02e4c3": {"ta_keywords": "continuous representations authorship;representations authorship attribution;repositories authorship attribution;gram repositories authorship;authorship attribution paper;authorship attribution;representations authorship;learns continuous representations;authorship attribution important;authorship attribution contrast;field authorship attribution;representations gram features;repositories authorship;authorship;continuous representations gram;feature representations model;representations model learns;gram features neural;feature representations;network jointly classification;attribution paper presents;attribution paper;gram features;field authorship;discrete feature representations;continuous representations;using continuous representations;features neural network;neural network jointly;learns continuous", "pdf_keywords": ""}, "9d9159026023f21e633f84fd61f3efad2e410214": {"ta_keywords": "formulas probabilistic relational;logic formulas probabilistic;probabilistic relational models;probabilistic relational;probabilistic order logic;logic formulas;relation extraction knowledge;relational models;knowledge base completion;formulas probabilistic;reasoning tasks artificial;relational models methods;relation extraction;including relation extraction;represent order logic;order logic formulas;extraction knowledge base;complex reasoning tasks;relational;logical facts predicates;generalize symbolic representations;symbolic representations;knowledge base;reasoning tasks;formulated inference;formulated inference problems;facts predicates;facts predicates challenging;complex reasoning;extraction knowledge", "pdf_keywords": ""}, "46f66dd37e6366ce102cfd97e718947151d5b1eb": {"ta_keywords": "backgroundfew news detection;differentiate fake news;news environment fake;backgroundfew news;information external news;news detection;external news;environment fake news;external news environment;disseminated news environment;news detection crucial;news environment;fake news post;news real;news real ones;misinformation social media;patterns news post;fake news real;news environment represents;language patterns news;news post created;preventing dissemination misinformation;created disseminated news;media differentiate fake;fake news;patterns news;dissemination misinformation social;dissemination misinformation;disseminated news;social media differentiate", "pdf_keywords": "news environment perception;detect fake news;detection fake news;fake news detection;news environment fake;perceived news environments;detection construct news;fake news detector;news detection better;news present novelty;news environments;new news environment;news detection construct;news detection;news detection effective;fake news detectors;perceived news;news detector;news environment;news detectors;differentiate fake news;information perceived news;news environments using;news detection systems;news environment represents;news detection social;news detectors given;different news environments;news detector prediction;propose news environment"}, "8122eaeb63098e94416108df918c9669e9105e65": {"ta_keywords": "memory object caching;object caching;caching;clusters object stores;erasure coding;data intensive clusters;using erasure coding;erasure coding methods;object caching meet;object stores increasingly;caching meet performance;intensive clusters object;increasingly relying memory;servers degraded performance;relying memory;using erasure;intensive clusters;memory object;relying memory object;imbalance servers;caching meet;memory;approach using erasure;erasure;imbalance servers degraded;object stores;degraded performance;load imbalance servers;clusters;severe load imbalance", "pdf_keywords": ""}, "8d64be0d3bb2650ff99a4c1ae8049eb5fece27a1": {"ta_keywords": "emotional speech recognition;features emotional speech;bottleneck features emotional;speech signal emotional;emotional speech emotion;signal emotional speech;bottleneck features deep;emotional speech;speech emotion;emotional speech degrades;bottleneck features;speech recognition;speech recognition automatic;speech recognition ar;introduction bottleneck features;automatic speech recognition;recognition automatic speech;features deep neural;features emotional;speech emotion infulence;recognition methods bottleneck;methods bottleneck features;used emotional speech;speech recognition methods;emotion infulence speech;automatic speech;speech signal;bottleneck;deep neural network;features deep", "pdf_keywords": ""}, "f8f17f32e651840531276423c7196856d27bcdd0": {"ta_keywords": "mutation adolescent adult;specific mutation adolescent;mutation adolescent;sex specific mutation;diagnosed adolescence patient;adolescence diagnosed;diagnosed adolescence diagnosed;adolescence patient;history adolescence diagnosed;diagnosed adolescence;adolescence diagnosed adolescence;adolescence patient history;specific mutation;mutation;adolescent adult male;patient history adolescence;diagnosed;adolescence;adolescent;adolescent adult;occurrence sex specific;male reported case;history adolescence;occurrence sex;sex specific;adult male reported;history history adolescence;adult male;male;adult", "pdf_keywords": ""}, "ee7af49291c030a3e29ad7a9cb5c1975d1b644f4": {"ta_keywords": "nutritional supplements pregnant;supplements pregnant women;supplements pregnant;intake nutritional supplements;concern intake nutritional;attending antenatal clinic;antenatal clinic;nutritional supplements;antenatal clinic major;intake nutritional;women attending antenatal;supplements;nutritional;major concern intake;pregnant women attending;attending antenatal;pregnant women;antenatal;concern intake;pregnant;clinic major concern;clinic major issue;women attending;intake;clinic;major concern;clinic major;major issue;women;issue", "pdf_keywords": ""}, "1cbb43b4d7f79d986a4a78ad3b53368c49e496ee": {"ta_keywords": "automatic speech;uses deep recurrent;recurrent neural network;using deep recurrent;automatic speech recognition;deep recurrent neural;speech recognition;challenge using deep;calls automatic speech;deep recurrent;rn based feature;speech recognition systems;tum verb challenge;recurrent neural;reverb challenge;submission reverb challenge;reverb challenge calls;network rn based;neural network feature;neural network rn;verb challenge;verb challenge using;speech;using deep;recurrent;network rn;challenge calls automatic;feature enhancement;submission reverb;feature enhancement case", "pdf_keywords": ""}, "d0fbae81d870bbfb34430654f70fd6a21e8bd1cc": {"ta_keywords": "mentions utilizing coreference;utilizing coreference problems;utilizing coreference;reasoning multiple mentions;coreference;multiple mentions utilizing;multiple mentions entity;information multiple mentions;coreference problems;mentions entity;coreference problems nonlack;multiple mentions;recurrent neural network;mentions utilizing;existing recurrent neural;existing existing recurrent;recurrent neural;mentions entity far;recurrent layer;coreferent dependencies;network rnn;existing recurrent;coreferent dependencies layer;present recurrent layer;biased coreferent dependencies;rnn layers;recurrent layer instead;neural network rnn;network rnn layers;introductionneural models reasoning", "pdf_keywords": "modeling task coreference;reading comprehension coreference;task coreference resolution;comprehension coreference;coreference based;comprehension coreference important;coreference based reasoning;task coreference;coreference important memory;coreference resolution;coreference annotation;text coreference annotation;coreference annotations;coreference resolution important;coreference information;text coreference;coreference information long;neural language models;given coreference information;word text coreference;coreference annotations extracted;coreference;layer uses coreference;uses coreference annotations;coreference annotation use;uses coreference;reading comprehension bidirectional;language modeling task;mentions entity;processing reading comprehension"}, "071216d944bcd2f05deafdb94e657167cce148d9": {"ta_keywords": "recognizing personal names;recall precision;recall precision tradeoff;personal names email;entity extraction;entity extraction methods;names email;change recall precision;task recognizing personal;personal names;classifier change recall;recognizing personal;learned sequential classifier;recall;change recall;trade entity extraction;sequential classifier;existing learned sequential;learned sequential;precision trade entity;introduction ner;introduction ner systems;recognizing;sequential classifier change;ner systems;names;task recognizing;classifier;entity;tweaking existing learned", "pdf_keywords": ""}, "1ebf54c0a8b38e8c26ed857cb9d4e565a8f17f17": {"ta_keywords": "graph walkbased similarity;walkbased similarity measures;measure entity similarity;walkbased similarity;personal information graph;entity similarity;adaptive graph walkbased;entity similarity viewed;similarity measures;graph walkbased;similarity measures application;information graph;graph walks;graph walks induce;personal information management;random graph walks;introduction adaptive graph;information graph includes;similarity viewed tool;represent personal information;similarity;messages terms persons;search graph;adaptive graph;similarity viewed;search graph results;graph includes messages;personal information;relations like sent;finite random graph", "pdf_keywords": ""}, "72a5c01afe276d06ca9179e24b1c925e206454f3": {"ta_keywords": "introductionexplainsable recommendation important;introductionexplainsable recommendation;explanations content reviews;generate explanations content;items knowledge graph;knowledge graph;recommendation important task;knowledge graphs;personalized page rank;knowledge graphs methodswe;leveraging external knowledge;content reviews;generate explanations;ranks items knowledge;generating explanations;explanations content;knowledge graph entities;reviews written items;proposed generate explanations;generating explanations hard;external knowledge form;recommendation important;external knowledge;content reviews written;explanations generated;review text;items review text;items knowledge;form knowledge graphs;items review", "pdf_keywords": "learning recommend using;learning recommend use;learning recommend;recommendations explanations proposed;recommendations explanations;recommending movies context;recommendation important task;introductionexplainsable recommendation important;introductionexplainsable recommendation;results recommendations explanations;mobile devices recommending;algorithm learning recommend;generate explanations content;devices recommending movies;recommending movies;recommending;explanations content reviews;leveraging external knowledge;recommendation important;items knowledge graph;knowledge graph;knowledge graphs resultsour;consolidates results recommendations;method user friendly;knowledge graphs;generating explanations;knowledge graph entities;generate explanations;propr learning recommend;external knowledge form"}, "f800f60db4427a51e564f1b875ae01d2c642fdce": {"ta_keywords": "network regenerating codes;codes distributed storage;regenerating codes possess;regenerating codes;regenerating code;explicit regenerating code;regenerating code point;codes distributed;node network regenerating;code point storage;repair bandwidth;network regenerating;purposeregenerating codes;developed codes distributed;purposeregenerating codes class;recovery subset nodes;possible repair bandwidth;distributed storage;explicit regenerating;data recovery;data recovery subset;present explicit regenerating;regenerating;storage bandwidth tradeoff;permit data recovery;repair failed node;storage bandwidth;recently developed codes;distributed storage permit;codes class", "pdf_keywords": "storage repair bandwidth;codes repair distributed;regenerating codes distributed;repair storage bandwidth;nodes regenerating codes;repair distributed storage;repair bandwidth distributed;nodes repair arbitrary;codes distributed storage;repair arbitrary nodes;distributed storage repaired;stored nodes repair;regenerating codes possess;regenerating code distributed;min storage repair;storage exact repair;bound exact repair;regenerating code storage;repair bandwidth tradeoff;exact repair storage;repair node information;codes repair;repair arbitrary node;existence exact repair;storage repair;exact repair codes;repair bandwidth;repair nodes derived;code distributed storage;regenerating codes"}, "b2f46145f2a50b609482a69d0581b218a6767cef": {"ta_keywords": "web based information;information sources pre;information sources common;information sources converted;knowledge integration systems;different information sources;search engines;information sources;processed web based;sources common database;knowledge integration;based information systems;information systems varies;common database representation;pre processed web;processing knowledge integration;database representation;search engines like;web based;methodsin information sources;information systems;degree information sources;processed web;based information;greatly search engines;common database;information;database representation paper;database;sources pre processed", "pdf_keywords": ""}, "80fdacd50ba9ad2e594dd2ddb0b1fa0e591f37ea": {"ta_keywords": "event extraction abstracts;introductionbiomedical event extraction;extraction abstracts;extraction abstracts papers;focused abstracts bionl;event extraction;decomposes event extraction;abstracts bionl 2011;event extraction set;abstracts bionl;systems abstracts;focused abstracts;abstracts papers using;work focused abstracts;systems abstracts papers;bionl 2011 shared;abstracts;introductionbiomedical event;abstracts papers methodsin;abstracts papers;submitted systems abstracts;based structured prediction;structured prediction;structured prediction useful;bionl 2011;classification tasks;molecular biology recent;search based structured;researchers molecular biology;extraction set classification", "pdf_keywords": ""}, "d46ecbacf42748ac9ce1fecd9f1b4ed0b9e34980": {"ta_keywords": "classification email messages;email conversational analysis;clinical messagein email;messagein email conversational;contextual information messages;email act classification;email conversational;key clinical messagein;intents message exchange;certain intents email;message preprocessing;clinical messagein;email messages;email messages contain;careful message preprocessing;intents email acts;messagein email;messages contain certain;messages contain;consider classification email;conversational analysis;intents email;gram sequence features;classification email;information messages;messages;intents message;conversational analysis useful;email acts;improve email act", "pdf_keywords": ""}, "b350be3836c3d183464642815b26b061f24e8314": {"ta_keywords": "integer embeddings mathematical;trained embeddings integers;integer embeddings;embeddings mathematical;embeddings mathematical knowledge;embeddings integers;probe integer embeddings;embeddings integers capture;embedding words;trained embeddings;embeddings;embedding words high;similarly trained embeddings;learning representations mathematical;embedding;introduction embedding words;introduction embedding;integers capture concepts;learning representations;representations mathematical;numerical reasoning tasks;words high dimensional;representations mathematical sequence;dimensional vector spaces;high dimensional vector;reasoning tasks learning;tasks learning representations;mathematical knowledge;representations;mathematical sequence data", "pdf_keywords": "integer embeddings trained;trained embeddings integers;integer embeddings mathematical;introduce integer embeddings;integer embeddings;integer embeddeddings textual;embeddings promising mathematical;embeddings mathematical knowledge;embeddings integers;embeddings promising;trained word embedding;embeddings mathematical;embeddeddings textual data;probe integer embeddings;embedding words high;embeddings trained;word embeddings;embedding words;embeddings integers capture;word embedding;integer representations vocabulary;trained embeddings;embeddeddings textual;multidimensional integer embeddeddings;word embedding model;oeis embeddings promising;integer embeddeddings contexts;integer embeddeddings;text learning vectors;embeddings trained online"}, "e602bde46bca5f424a3d53675c1275386544eb1e": {"ta_keywords": "sex specific mutation;mutation aetiology sex;aetiology sex specific;sex specific disease;aetiology sex;mutation aetiology;specific mutation aetiology;model sex specific;occurrence sex specific;sex specific;specific mutation;occurrence sex;mutation;model sex;new model sex;specific disease characterized;specific disease;aetiology;disease reported development;specific disease reported;disease;disease characterized;disease reported;presence sex specific;sex;characterized presence sex;presence sex;disease characterized presence;specific;model", "pdf_keywords": ""}, "7fcc2cc70498e409168a6c3dfd7c59652b1160c2": {"ta_keywords": "transforms acoustic features;regression fmr transforms;fmr transforms acoustic;acoustic features adapted;transforms acoustic;transformation matrices regression;linear regression fmr;regression fmr;fmr transforms;acoustic features;features adapted;precessing independent decoding;features adapted ones;constrained llr uses;matrices regression;uses multiple transformation;matrices regression tree;single transformation matrix;transforms;single transformation;multiple transformation matrices;multiple transformation;constrained llr;transformation matrices;efficient adaptation performed;transformation matrix;independent decoding;acoustic;efficient adaptation;realizes efficient adaptation", "pdf_keywords": ""}, "dda3f2a2803c80e5b3332868bf86901d6239befc": {"ta_keywords": "stochastic distributed methods;distributed methods unbiased;distributed methods;new distributed methods;methods unbiased compression;analysis stochastic distributed;distributed methods error;stochastic distributed;converging local sgd;methods error compensation;new optimization methods;local updates methodsusing;unbiased compression;methods unbiased;methods linearly converging;optimization methods linearly;optimization methods;compensated linearly converging;sgd arbitrarily heterogeneous;converging errorror compensated;error compensation local;stochastic;local sgd arbitrarily;distributed;converging local;analysis stochastic;local sgd;compensation local updates;frameworks analysis stochastic;new optimization", "pdf_keywords": "method stocochastic gradient;convex loss logistic;stocochastic gradient;methods stochastic optimization;stochastic gradient descent;staochastic optimization methods;stochastic optimization methods;robust method logistic;method stochastic optimization;method logistic regression;optimization stochastic optimization;loss logistic regression;stochastic gradients workers;stochastic optimization method;convex optimization;stocochastic gradient gg;stochastic optimization;approach stochastic optimization;convex optimization fast;optimization stochastic method;loss logistic;stochastic optimization problems;gradient descent;gradients workers method;stochastic gradient methods;optimization stochastic;distributed optimization methods;gradient descent development;optimization machine learning;convex optimizing"}, "9dc4a5284ecfd37ab8bc8990eddf1b39113e004b": {"ta_keywords": "context machine translation;machine translation postulate;machine translation understood;translation postulate causes;local context;machine translation;problem machine translation;effect local context;translation postulate;source target domain;target domain mismatch;source target;translation;world different places;different parts world;local context machine;translation understood live;different cultures;pertain specific place;context;domain mismatch problem;parts world work;introductionthe source target;effect local;domain mismatch;cultures;strikingly different cultures;target domain;context machine;translation understood", "pdf_keywords": "resource machine translation;predicting translationese translations;training machine translation;translation low resource;translation data originating;predicting translationese;machine translation tasks;method predicting translationese;machine translation data;mismatch machine translation;target monolingual data;machine translation low;learning machine translation;source target language;translation systems;translation tasks;machine translation systems;translation systems accuracy;monolingual dataset target;machine translation;datasets translationese affected;translation tasks particularly;data source monolingual;machine translation important;translation self training;source monolingual dataset;translation systems apply;improved quality translation;translation data;effective translating data"}, "128610c7df12bff1610949c551b6236cb350dcd9": {"ta_keywords": "speech recognition ar;automatic speech recognition;automatic speech;speech recognition;speech recognition systems;end automatic speech;recognition ar autoregressive;deployment automatic speech;speeding decoding;bottleneck speeding decoding;autoregressive;autoregressive nar;autoregressive structure bottleneck;ar autoregressive;speeding decoding process;recognition ar;non autoregressive nar;decoding;ar autoregressive structure;non autoregressive;autoregressive structure;fast inference;achieving fast inference;propose non autoregressive;speech;transformers;recognition systems;recognition systems desired;transformers achieved promising;fast inference objectives", "pdf_keywords": "nonar adaptive recognition;speech text representations;token embedding linguistic;learning speech representations;acoustic language models;speech representations;speech representations method;representation acoustic linguistic;linguistic representation acoustic;acoustic linguistic representations;novel speech recognition;speech recognition;adaptive recognition;speech text modalities;recognition methods speech;acoustic language data;supervised learning speech;modality gap speech;integrating acoustic language;speech recognition various;nonar anar adaptive;pre trained wav2vec2;trained acoustic language;gap speech text;acoustic linguistic knowledge;representations linguistic recognition;learning speech;quality speech recognition;token embedding;speech text"}, "4e1d27c68a60bfd8393462107677469bf286f0f8": {"ta_keywords": "modeling program synthesis;program synthesis;construct infer programs;programsynthesis techniques construct;program synthesis task;introduction programsynthesis;introduction programsynthesis techniques;programsynthesis techniques;programsynthesis;insights recursive reasoning;infer programs;recursive reasoning models;recursive reasoning;synthesis task rational;output examples specifications;modeling program;drawing insights recursive;users leave synthesis;programs simultaneously satisfy;insights recursive;program;synthesis task;specification research;derived modeling program;specification research design;infer programs user;reasoning models;examples specifications;programs;examples specifications especially", "pdf_keywords": "program synthesis;pragmatic program synthesizer;modeling program synthesis;program synthesis techniques;program synthesis task;frustrating program synthesis;pragmatic program;implementation pragmatic communication;introductionprogramming;interacts pragmatic program;program synthesizer;construct infer programs;reasoning models pragmatics;synthesis program;probabilistic recursive reasoning;synthesis techniques construct;priors synthesis program;operationalize pragmatics using;pragmatics develop efficient;efficient implementation pragmatic;operationalize pragmatics;recursive reasoning models;program synthesizer versus;pragmatic models capture;implementation pragmatic;syntactic priors synthesis;frustrating operationalize pragmatics;synthesis program insufficient;user interacts pragmatic;pragmatics develop"}, "09e4e0eee756da5658c6d572871130d53a89c72b": {"ta_keywords": "strategic feature;decision subjects strategically;making strategic feature;strategic feature modifications;situations underlying predictive;automated decision;automated decision making;subjected automated decision;predictive model deliberately;underlying predictive;strategically;rely incomplete information;subjects strategically modify;decision subjects rely;strategically modify;underlying predictive model;opacity forces decision;decision making;information making strategic;predictive;outcome situations underlying;forces decision subjects;making decision subjects;decision making decision;strategic;features ways believe;desirable outcome situations;settings game bay;incomplete information making;making strategic", "pdf_keywords": "policies bayesian incentive;signaling policies bayesian;game bayesian persuasion;optimal signaling policy;signaling policy decision;bayesian incentive compatible;bound decision maker;decision maker optimal;decision maker signaling;desirable actions decision;bayesian incentive;actions signaling policy;optimized signaling policy;signaling policies;problem game bayesian;game bayesian perthe;signaling policy matches;signaling policy theorem;known signaling policy;signaling policy expected;policy decision maker;signaling policy based;game bayesian;settings game bayesian;decision maker compute;policies bayesian;decision maker utility;policy based optimal;subjected automated decision;actions decision maker"}, "933b03a81110676f4c61c449f1926ebd58bc47f7": {"ta_keywords": "interacting dynamic touchscreens;touchscreens everyday lives;inaccessible dynamic touchscreens;independently touchscreens;dynamic touchscreens everyday;dynamic touchscreens difficult;dynamic touchscreens;independently touchscreens way;touchscreens;touchscreens way control;use independently touchscreens;touchscreens way;touchscreens difficult non;touchscreens everyday;touchscreens difficult;introduction blind people;blind people frequently;introduction blind;user interfaces;screens easy accidentally;visual user interfaces;screens easy;blind people;blind;screens;visually visual user;different screens easy;entertainment systems interacting;visual user;user interfaces change", "pdf_keywords": "touchscreens accessible statelens;interactive devices;people interact touchscreens;videos touchscreen interfaces;screencast videos statelens;conversational agent ios;touchscreens wild demonstrate;challenges public touchscreen;interactive devices applications;accessible public touchscreen;screencast;public touchscreen devices;user interfaces technologies;interactive interactive devices;screencast videos discuss;statelens ios;user interfaces;public touchscreen;interact touchscreens;sceenshots screencast;touchscreen interfaces camera;touchscreen interfaces study;visually impaired users;interactive devices important;touchscreens accessible using;used sceenshots screencast;access touchscreen interfaces;focused sceenshots screencast;sceenshots screencast videos;accessibility blind users"}, "d462eae8dd5c1415e03651b9fc1c2ca80a69521f": {"ta_keywords": "perceptual learning external;introductionintegrating perceptual learning;learning external;perceptual learning;learning external world;external world knowledge;knowledge siulated student;prior perceptual skills;world knowledge siulated;world knowledge;background knowledge includes;learning;knowledge;knowledge siulated;perceptual skills;knowledge includes;introductionintegrating perceptual;background knowledge;perceptual skills large;amounts background knowledge;text extensive understanding;article selection english;prior perceptual;siulated student methods;knowledge includes ability;selection english;ability parse text;complex prior perceptual;extensive understanding semantics;needed background knowledge", "pdf_keywords": ""}, "730e5e83586dd5784051f933e7bb82571cec4c94": {"ta_keywords": "separation speaker counting;diarization speech separation;speaker diarization encoder;neural speaker diarization;speaker diarization;performs speaker diarization;speaker counting methodswe;speaker counting;end speaker diarization;speech separation speaker;speaker counting proposed;speaker diarization speech;speech separation methods;separation speaker;speech separation;end neural speaker;neural speaker;diarization speech;end speaker;jointly performs speaker;end end speaker;speaker;diarization encoder decoder;diarization encoder;performs speaker;decoder based attra;separation methods;separation methods end;diarization;speech", "pdf_keywords": "speech separation audio;separation speaker diarization;separation speech recognition;audio separationion network;diarization speech separation;speech separation eend;speech separation methods;speech separation model;domain audio separationion;joint speech separation;domain audio separation;speech separation speaker;separation audio;audio separationion;separation speaker;method speech separation;audio separation;separated speech signals;separation speaker counting;approach speech separation;audio separation method;use speech separation;speaker diarization encoder;end speaker diarization;neural speaker diarization;speech separation;separation audio signal;end neural speaker;audio separatein;timedomain audio separatein"}, "1548142a6be92f41e45dcbde9ff8afd71134ac1d": {"ta_keywords": "cancer risk pm10;polycyclic aromatic hydrocarbons;lung cancer risk;aromatic hydrocarbons pahs;aromatic hydrocarbons;polycyclic aromatic;china results pm10;hydrocarbons pahs summer;risk pm10;pollution level;sources lung cancer;hydrocarbons pahs;risk pm10 bound;cancer risk;pm10 bound polycyclic;pollution level sources;bound polycyclic aromatic;lung cancer;background pollution level;concentrated pollution level;pm10 samples;results pm10 samples;pollution;background pollution;aromatic;study concentrated pollution;concentrated pollution;pahs summer nanjing;results pm10;pm10 bound", "pdf_keywords": ""}, "f48792e8a24e369c80e39a2a2b7451d108f02941": {"ta_keywords": "question answering xq;explainable question answering;question answering;question answering 9001;web similar ai;answering xq alleviate;details learning reasoning;tackle question answering;answering xq;information pollution web;information web similar;pollution web requires;similar ai applications;learning reasoning;similar ai;ai;learning reasoning steps;details learning;pollution web;access information web;information pollution provides;ai applications black;augmenting answer aim;information web;ai applications;answering;information pollution;augmenting answer;user friendly interfaces;information", "pdf_keywords": ""}, "642c85d35b4a3cc9648b269e32fe9d0a18907c98": {"ta_keywords": "speech separation cms;continuous speech separation;speech separation;level speech separation;separate speech;separation cms;task separate speech;separation cms task;background continuous speech;segment long recording;separate speech sources;continuous speech;overlapped recording involves;partially overlapped recording;overlapped recording;utterance level speech;separation;cms task segment;long recording;level speech;speech sources long;separate;utterance level;conventional utterance level;speech;extension conventional utterance;window separately simple;recording involves;window separately;utterance", "pdf_keywords": "speech separation deep;continuous speech separation;recording speech separation;level speech separation;speech separation task;separation continuous speech;speech separation continuous;speech separation ss;speech separation;speech separation conventional;interactive speech separation;speech separation decade;talker speech separation;modeling continuous speech;speech separation ca;long recording speech;speech separation real;separate speech;task separate speech;separate speech sources;recording speech;modeling long recording;backgroundthe continuous speech;segment long recording;speech recognition;continuous speech;speech sources long;long recording;evaluation continuous speech;better separation performance"}, "acf0ccc8b67cc441c51d4281c305359073b9c7cc": {"ta_keywords": "speech translation evaluation;approach speech translation;speech translation interwomen;speech translation;speech translation novel;interwomen speech translation;speech translation paper;translation interwomen speech;transcription text translation;speech transcription text;kyotou speech translation;submissions speech translation;translation evaluation campaign;translation evaluation;speech transcription;text translation components;text translation;systems speech transcription;translation components;translation novel approach;network systems speech;translation interwomen;interwomen speech;systems speech;transcription text;novel approach speech;kyotou speech;translation paper describes;approach speech;translation paper", "pdf_keywords": ""}, "9cfc4e94e76d8025cd86d6652a641b1440681d28": {"ta_keywords": "combinationatory categorial grammar;combinationatory categorial grammars;categorial grammars methodsinduction;linguistic structure combinationatory;grammars methodsinduction linguistic;structure combinationatory categorial;categorial grammars resultsour;methodsinduction linguistic structure;combinationatory categorial;categorial grammar;categorial grammars;specific combinationatory categorial;methodsinduction linguistic;backgroundinduction linguistic structure;language specific combinationatory;grammars methodsinduction;linguistic structure;categorial grammar cg;backgroundinduction linguistic;structure combinationatory;grammars resultsour consists;linguistic;grammar cg lexicon;grammars resultsour;specific combinationatory;lexicon based;cg lexicon based;combinationatory;grammars;induces language specific", "pdf_keywords": ""}, "d0a6b70c9dc1942169f48211d47843732c57a3a9": {"ta_keywords": "grounded navigation photo;natural language grounded;language grounded navigation;learning generalized navigation;grounded navigation;navigation photo realistic;generalized navigation model;navigation photo;generalized navigation;navigation model novel;following natural language;navigation model;seen environments;environments aim learning;photo realistic environments;language grounded;seen unseen environments;natural language;unseen environments;realistic environments following;unseen environments aim;study natural language;novel perspectives methods;learning generalized;natural language instructions;navigation;previously unseen environments;realistic environments;aim learning generalized;environments following natural", "pdf_keywords": "multitask navigation trained;agnostic learning multitask;language grounded multitask;agnostic multitask learning;natural language grounded;grounded multitask;learning multitask;learning effective multitask;language grounded navigation;agnostic multitask navigation;grounded multitask environment;tasks learns generalized;multitask learning;tasks multitask learning;learning demonstrate multitask;learns generalized environment;learning multitask learning;multitask learning demonstrate;effective multitask learning;multitask learning effective;multitask navigation model;grounded navigation photo;learning generalized navigation;demonstrate multitask learning;environment agnostic training;approach navigation multitask;environment agnostic multitask;multitask reinforcement learning;navigation trained;multitask learning train"}, "cdf17da4a7638985cb62a5dbf1161239b315eb85": {"ta_keywords": "link modeling jointly;jointly modeling links;topic models;models topic models;entity link modeling;link modeling;topic models improve;modeling links text;modeling links;latent groups entities;text entities linked;links text entities;entities linked;entity link;protein interactions social;pairs entities frequently;mixed membership stochastic;jointly modeling;modeling jointly modeling;block models topic;identifying latent groups;model datasets protein;social networks methods;entity entity link;stochastic block models;groups entities observed;modeling jointly;latent groups;membership stochastic block;pairs entities", "pdf_keywords": ""}, "c0484ac1677b942e8b06ea0ac3cad5b01e52ced4": {"ta_keywords": "questions answered questions;question answered questions;questions answered respondents;answered questions answered;answered questions;questions answered;answered respondents;respondents;questions;answered;question answered;question", "pdf_keywords": ""}, "092b80cc6250f74a2c1e0ba7820c31a8f0153c0a": {"ta_keywords": "literary theory italo;literary analyses;large collections literary;methods single literary;literary analyses remains;literary works;literature;collections literary;literary theory;digital humanities methods;methods aid literary;collections literary works;distant reading algorithmically;humanities methods;single literary theory;distant reading;challenge digital humanities;literary;theory italo;background literary;background literary critics;single literary;literature careful reading;aid literary analyses;literary critics;digital humanities;literary critics attempt;reading analysis;humanities methods previous;literature careful", "pdf_keywords": "computational literary;computational literary studies;literary analyses;case computational literary;analyze literary coherence;characterize text novel;analyze literary;literature literary;literary analyses remains;literature;literary history world;literary coherence;model literary coherence;literary analyses obtain;support literary analyses;text representation methods;literary studies;overview literature literary;analyzing novel narratives;literary;literary critics;text representation;background literary critics;model literary;novel text;used analyze literary;literary studies present;methods single literary;literary critics attempt;texts algorithm"}, "63cd8df0041638b0aa74834a81f99ff136951ff1": {"ta_keywords": "binarygan novel generative;adversarial networks binary;training generationerative adversarial;generationerative adversarial;generationerative adversarial networks;methods propose binarygan;binarygan;propose binarygan;propose binarygan novel;generative adversarial network;novel generative adversarial;binarygan novel;generative adversarial;gradients binary neurons;networks binary neurons;binary neurons train;adversarial network;estimate gradients binary;binary neurons end;adversarial;adversarial network uses;end backpropagation;binary neurons output;adversarial networks;end end backpropagation;end backpropagation methods;training generationerative;generative;binary neurons;gradients binary", "pdf_keywords": "binarygan novel generative;propose binarygan;binary neurons trained;network gan based;binarygan;network gan;gradients binary neurons;neurons trained objective;binary neurons train;sequence generative adversarial;adversarial network learn;backgroundwe propose binarygan;generative adversarial network;adversarial network gan;propose binarygan novel;network gan target;estimate gradients binary;generative adversarial;binary neurons gait;propose generative adversarial;novel generative adversarial;trained objective implement;gan based model;binarygan novel;adversarial network uses;trained objective;gradients binary;adversarial network;neurons trained;backpropagation employ sigmoid"}, "655b842ae905756b2949758bd7e52e5fd32c3642": {"ta_keywords": "beam search speech;search speech recognition;vitrerbi beam search;search speech;beam search;resultsmost speech recognizers;beam search based;speech recognition lvr;employ beam search;speech recognition;continuous speech recognition;lvr resultsmost speech;beam search speed;minimization vitrerbi beam;search speed decoding;minimization large vocabulary;optimize vitrerbi beam;speech recognizers;minimization vitrerbi;method optimize vitrerbi;speech recognition methodsthis;speech recognizers employ;optimize vitrerbi;search error risk;hypotheses pruned decoding;risk minimization vitrerbi;resultsmost speech;search speed;recognition lvr resultsmost;vitrerbi beam", "pdf_keywords": ""}, "28421c7f28adfb9ab8aeb56c196ac3ba326efdbb": {"ta_keywords": "trypsin inhibitors inhibit;inhibitors inhibit trypsin;inhibitors trypsin inhibitors;effect trypsin inhibitors;inhibit trypsin inhibitors;effective inhibitors trypsin;trypsin inhibitors effective;trypsin inhibitors mechanism;trypsin inhibitors reducing;inhibitory effect trypsin;trypsin inhibitors;inhibitors trypsin;unclear trypsin inhibitors;shown trypsin inhibitors;inhibit trypsin;inhibitors reducing inhibitory;inhibitors effective inhibitors;inhibitors inhibit;inhibitors mechanism;inhibitors effective;effect trypsin;effective inhibitors;inhibitors mechanism remains;inhibitors;inhibitors reducing;reducing inhibitory effect;study shown trypsin;inhibitory effect;reducing inhibitory;trypsin", "pdf_keywords": ""}, "7c72e63aa112193590861887c5d03b640ce90911": {"ta_keywords": "automated modeling model;automated automated modeling;automated modeling;modeling model;modeling;automated automated;technologies automated automated;new technologies automated;automated;model;technologies automated;development new technologies;new technologies;technologies;development;development new;report development;report development new;new;report", "pdf_keywords": ""}, "3a6334953cd2775fab7a8e7b72ed63468c71dee7": {"ta_keywords": "social skills training;skills training audiovisual;social skill training;skills social interaction;effectiveness social skill;social skills;introductionautomated social skills;appropriate skills social;social skill;training audiovisual information;skills social;training audiovisual;audiovisual information people;based social skills;training using audio;computer based social;social communication;audiovisual information;audiovisual information established;people social communication;social communication difficulties;social interaction;audiovisual;effectiveness social;using audio;skills training systems;skills using computers;skill training using;skills using;social interaction methodsdata", "pdf_keywords": ""}, "4e9328b2801e158647dff69606ed47d47045eca8": {"ta_keywords": "web api;api online documentation;web api online;api;data important tool;platform web api;api online;tool machine learning;machine learning active;machine learning;online documentation;data;online documentation hopefully;web platform;web platform web;platform web;data important;web;documentation;documentation hopefully;released web platform;documentation hopefully meet;development supported;learning active;learning;active development supported;tool machine;learning active development;released web;needs researchers", "pdf_keywords": "data analysis tool;analyzing data dataset;useful tool data;gender bias dataset;tools facilitate data;data oriented;datasets developed tool;based tool data;tool data data;tool data analysis;data analysis diagnostics;data oriented platform;datasets diagnostic;datasets diagnostic ones;plain datasets diagnostic;tool data;platform data analysis;propose datalab;propose datalab unified;typology data operations;data operations platform;based platform data;interactive data analysis;information tools;analyze data data;data diagnostics;datasets datalab web;facilitate data analysis;new tool data;tool natural language"}, "1e9771a264334c45020421b1c847f6bcd88adc60": {"ta_keywords": "approaches delineating 3d;delineating 3d;precisely delineating 3d;delineating 3d structure;accurate annotations train;interpret visually 3d;3d structure depend;annotations train networks;delineating 3d large;3d structure;visually 3d;accurate annotations;3d;annotations train;visually 3d interfaces;annotation;annotations;annotation inaccuracies;3d large scale;approaches delineating;depend accurate annotations;3d interfaces;3d interfaces awkward;purposedeep learning;precisely delineating;networks;delineating;structure depend accurate;3d large;purposedeep learning based", "pdf_keywords": "3d neural annotation;annotations train networks;neural annotation data;neural annotation;3d neural;automatic 3d neural;learning based deformable;annotations curvilinear 3d;neural annotation approach;accurately annotate structures;neural annotation using;accurate annotations train;annotate structures challenge;learn network weights;annotations train;deep delineation linear;network weights learning;deep delineation;deep network training;3d neural network;annotate structures;precisely delineating 3d;applied neural annotation;delineating 3d structure;accurately annotate;deep nets;approaches delineating 3d;delineating 3d;weights learning;approach deep"}, "80111013916dae3306316c34e13fe856cb08b87b": {"ta_keywords": "inheritance hierarchies;inheritance network;inheritance network research;inheritance hierarchies recent;topology inheritance network;work inheritance hierarchies;hierarchies;inheritance;depend topology inheritance;topology inheritance;common sense reasoning;sense reasoning produced;algorithms depend topology;sense reasoning;purpose algorithms depend;reasoning produced;algorithms depend;work inheritance;important work inheritance;purpose algorithms;human common sense;general purpose algorithms;algorithms;hierarchies recent;hierarchies recent years;common sense;reasoning;reasoning produced proliferation;explicit exceptions important;explicit exceptions", "pdf_keywords": ""}, "69e8c4327193af4549c06809c821c99deb4022cd": {"ta_keywords": "distributed storage coding;codes distributed storage;storage coding;codes distributed;distributed storage described;distributed storage;optimal codes distributed;storage coding problem;paper distributed storage;optimal codes;network capable storing;stored nodes network;stored nodes;data stored nodes;downloading data nodes;capable storing symbols;distributed;explicit optimal codes;storage;data nodes;storage described;data reconstructed downloading;coding problem consider;capable storing;reconstructed downloading data;reconstructed downloading;storing symbols;storing;nodes downloading;storage described paper", "pdf_keywords": ""}, "a77643bff6f50ccc4f80ec081e4d078a2e788ae7": {"ta_keywords": "existing subword regularization;subword regularization methods;subword regularization;subword segmentation;subword segmentation algorithms;rely subword segmentation;segmentation especially languages;introductionmultilingual pretrained representations;shared multilingual vocabulary;multilingual vocabulary;sub optimal segmentation;multilingual vocabulary standard;pretrained representations;pretrained representations generally;generally rely subword;subword;shared multilingual;existing subword;regularization methods kudo;introductionmultilingual pretrained;create shared multilingual;segmentation especially;multilingual;optimal segmentation especially;vocabulary standard heuristic;regularization;regularization methods;optimal segmentation;applying existing subword;sub optimal", "pdf_keywords": "multilingual bert;multilingual pretrained models;rely subword segmentation;tasks multilingual bert;multilingual pretrained representations;multilingual bert xlim;existing subword regularization;subword segmentation;subword regularization methods;uses subword segmentation;method multilingual pretrained;effective multilingual models;subword segmentation algorithms;subword regularization;shared multilingual vocabulary;subword segmentation use;languages different segmentation;multilingual processing;useful multilingual processing;multilingual models;multilingual pretrained;multilingual translation effective;multilingual representations effective;models demonstrate multilingual;segmentation especially languages;multilingual models propose;languages tasks multilingual;performance multilingual pretrained;multilingual vocabulary;crosslingual language learning"}, "bf50833a46839d3932663b472d6145418f9d0bd6": {"ta_keywords": "speech recognition ar;multiple microphone arrays;microphone arrays;speech recognition;automatic speech recognition;attention based multi;anrar stream attention;microphone arrays achieved;stream attention based;automatic speech;classification cc attention;connectionist tempral classification;multiple microphone;tempral classification;using multiple microphone;tempral classification cc;recognition ar;stream attention;microphone;recognition ar using;anrar stream;attention based;introduction automatic speech;e2e anrar stream;advances joint connectionist;attention mechanism end;multi array;joint connectionist tempral;connectionist tempral;multi array framework", "pdf_keywords": "multi array auditory;attention based multi;multiple microphone arrays;end speech recognition;array auditory;array auditory auditory;microphone arrays;microphone array modeled;stream attention based;microphone array;attention based electronic;stream attention achieved;multimodal speech recognition;microphone array situation;microphone arrays achieved;attention mechanism end;output microphone array;classification cc attention;attention model;stream attention;multimodal speech;arraye2e architecture joint;distributed microphone array;multi arraye2e;hierarchical attention mechanism;introductionautomated speech recognition;attention model hierarchical;use multimodal speech;speech recognition variety;multiple microphone"}, "d6e21619df572d04b2b2d97b4c5d1fd604f185fb": {"ta_keywords": "machine translation autoregressively;neural machine translation;translation autoregressively generate;autoregressive decoding;fully parallelize decoder;parallelize decoder computations;autoregressive decoding produce;parallelize decoder;semi autoregressive decoding;decoders neural machine;translation autoregressively;standard decoders neural;decoder computations training;decoders neural;timestep slows inference;machine translation;decoder computations;standard decoders;training time inference;decoder;decoders;decoding;decoding produce multiple;slows inference;slows inference especially;token timestep slows;tokens timestep independently;introduction standard decoders;decoding produce;multiple tokens timestep", "pdf_keywords": "machine translation autoregressively;nonautoregressive translation development;neural machine translation;translation token decoder;nonautoregressive translation language;autoregressive translation token;syntax nonautoregressive translation;translation autoregressively generate;automatic decoding syntax;translation language generation;machine translation;translation based automatic;translation demonstrate performance;nonautoregressive translation demonstrate;translation compare performance;non autoregressive translation;autoregressive translation;sentence optimize decoding;automatic automatic decoding;nonautoregressive translation;translation autoregressively;parse decoder bottleneck;translation token;framework nonautoregressive translation;decoding syntax chunks;decoder computations training;abstraction nonautoregressive translation;sequence parse decoder;automatic decoding;translation language"}, "a5f42552b2368a587aea0a81175b4a79aa614601": {"ta_keywords": "web based extraction;extracting information web;extracting web data;extracting web;specifically collaborative filtering;extracting information;collaborative filtering;based extraction systems;based extraction;web data certain;web data;extracted information;filtering;certain machine learning;extraction systems;problem extracting web;machine learning systems;extracted information used;machine learning;extracting;extraction systems usually;noise free data;information web generally;information web;extraction;data clear semantics;web based;data certain;free data clear;work extracting information", "pdf_keywords": ""}, "0e61536550b7263d67b2928473355171dc37c0ae": {"ta_keywords": "data literature provide;analyzing data literature;data literature;literature provide overview;provide overview literature;overview literature;analyzing data;discuss importance analyzing;literature provide;article discuss importance;analyzing;importance analyzing data;importance analyzing;purpose article discuss;article discuss;purpose article;data;article;literature;overview;provide overview;discuss importance;importance;purpose;discuss;provide", "pdf_keywords": ""}, "7f0dbd30dc839fd95ea953a9229c879396ca11c0": {"ta_keywords": "representing symbolic knowledge;symbolic knowledge base;symbolic knowledge;neural modules fully;sparse matrix reified;semantics kb expressive;enables neural modules;hop inferences scalable;neural modules;reified kb representation;inferences scalable;matrix reified kb;representing symbolic;kb expressive model;knowledge base;inferences scalable use;kbs sparse matrix;multi hop inferences;way representing symbolic;semantics kb;matrix reified;original semantics kb;representation enables neural;kb representation enables;kb expressive;faithful original semantics;knowledge base called;kbs sparse;large kbs sparse;hop inferences", "pdf_keywords": "symbolic knowledge base;optimized reasoning symbolic;relations represented sparse;representing symbolic knowledge;relations sparse;learn relation sets;generating logical reasoning;relations sparse matrices;learning knowledge base;incorporating reasoning large;reifying knowledge base;natural language reasoning;optimized reasoning;knowledge graphs;knowledge base;knowledge base bb;incorporating reasoning;labeling knowledge base;modules optimized reasoning;model relations sparse;base knowledge base;knowledge base based;symbolic knowledge;logical reasoning single;entities 20x relations;semantics kk expressive;knowledge base kk;knowledge graphs present;knowledge base knowledge;scheme incorporating reasoning"}, "c2c6c9947dc9d28bb4fc6f965310be517f4d8c57": {"ta_keywords": "gans achieve task;gans achieve;networks gans achieve;networks gans;generative adjversarial networks;gans;generative adjversarial;adjversarial networks gans;use generative adjversarial;generative;voxel based shape;voxel based;use generative;novel method voxel;method voxel;shape synthesis;voxel;adjversarial networks;based shape synthesis;method voxel based;shape synthesis build;adjversarial;text color shapes;method use generative;color shapes;shapes state art;shapes;based shape;color shapes state;shapes state", "pdf_keywords": ""}, "81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85": {"ta_keywords": "__ profession prompts;profession prompts;profession prompts usually;prompts obama __;language models having;obama __ profession;prompts obama;prompt oama;language models;prompts;optimal prompt oama;predicting correct profession;prompts usually;prompt oama worked;contained language models;prompt;optimal prompt;__ profession;blanks prompts obama;oama worked __;sub optimal prompt;knowledge contained language;correct profession given;correct profession;profession;prompts usually manually;results examining knowledge;models having;having blanks prompts;obama __", "pdf_keywords": "mining based paraphrasing;query linguistic knowledge;knowledge extraction;knowledge extraction existing;ensembling paraphrases;factual knowledge retrieval;knowledge language models;knowledge retrieval;natural language processing;ensembling diverse prompts;knowledge retrieval accuracy;field knowledge extraction;discovering better prompts;linguistic knowledge language;based paraphrasing based;paraphrasing based;question field knowledge;query linguistic;ensembling paraphrases significant;retrieving factual knowledge;paraphrasing based methods;automatically discovering better;diverse prompts query;methods natural language;prompt candidatesthe lexical;prompt paraphrasing;language models;ability query linguistic;knowledge neural language;large corpus diversify"}, "2c5a410b781f90c145efac05fea235c5c3e44861": {"ta_keywords": "source voice conversion;voice conversion framework;voice conversion;speech representation s3r;self supervised speech;supervised speech representation;supervised speech representations;open source voice;speech representation;source voice;supervised speech;speech representations;speech representations methodsin;synthesis self supervised;voice;recognition synthesis self;s3prl open source;conversion framework self;conversion framework;representation s3r;expensive supervised representation;paper introduces s3prl;introduces s3prl;task s3r analysis;representation s3r valuable;supervised representation adopted;self supervised;recognition synthesis;conversion;s3r analysis", "pdf_keywords": "voice conversion challenge;synthesis voice conversion;voice conversion;approach voice conversion;speech voice conversion;voice conversion approach;synthesizing converted speech;transform voice conversion;based voice conversion;speech synthesizing converted;voice conversion ability;recognition synthesis voice;source speech synthesizing;single voice conversion;conversion single voice;converted speech extracted;voice conversion single;latest voice conversion;speech synthesizing;voice conversion vc;transform voice;tool integrating speech;speaker dataset implement;encode speech;speech synthesis;converted speech;capable integrating speech;speaker dataset;integrating speech vocoder;encode speech information"}, "3db9649f2ae986cac13f3e748375f8802f9b07fc": {"ta_keywords": "parameters deep neural;popular compression;compression;popular compression techniques;compression techniques centered;compression techniques;compression techniques taken;computerestricted environments compression;deep neural networks;high resource datasets;environments compression;parameters deep;deep neural;incurred popular compression;environments compression techniques;stateof art networks;neural networks increasingly;number parameters deep;resource datasets;networks;neural networks;resource datasets work;datasets;datasets work;art networks;networks increasingly challenging;neural;networks increasingly;deep;high resource", "pdf_keywords": "generalization lowresource languages;generalization low resource;generalization lowresource;low resource languages;language models trained;compression low resource;textbased language models;machine translation models;lowresource languages small;pruning parameters languages;generalization performance text;languages small performance;machine translation wild;generalization reveals sparse;lowresource languages;sparse models improve;language models;sparsity generalization performance;machine translation important;distribution generalization lowresource;generalization performance;pruning non textbased;translation models;machine translation;machine translation extremely;reduction quality translations;neural machine translation;optimizing pruning;models trained translate;compression data limited"}, "12442420adf1c36887fafd108f4b7f4fc822ae60": {"ta_keywords": "semi supervised;semi supervised methods;simplest semi supervised;original labeled dataset;labeled dataset unlabeled;labeled dataset;augment original labeled;unlabeled data paired;supervised;dataset unlabeled data;dataset unlabeled;unlabeled data;labeled;prediction pseudo parallel;supervised methods;self training;prediction pseudo;model prediction pseudo;unlabeled;original labeled;self training earliest;background self training;pseudo parallel data;supervised methods key;paired model prediction;dataset;pseudo;data paired;training;pseudo parallel", "pdf_keywords": "unsupervised translation pseudotraining;translation pseudotraining;machine translation text;like machine translation;machine translation;improvement translation performance;sequence generation tasks;data self training;neural sequence generation;translation text summarization;summarization dataset 8m;translation pseudotraining step;tasks machine translation;training sentences;translation performance;performance neural sequence;text summarization contributes;8m training sentences;results unsupervised translation;translation performance continuing;summarization dataset;self training unlabelled;training sentences noisy;summarization contributes;sequence generation large;sequence generation;data pseudo training;training perform sequence;machine translation unclear;method self training"}, "299ab255f3d940a20891128dfa9e0736d74a936c": {"ta_keywords": "earlyfuson vision models;introducing earlyfuson vision;vision architectures;vision architectures require;vision pipeline;modern vision architectures;computer vision pipeline;earlyfuson vision;perceptual systems robotics;vision models;building perceptual systems;vision pipeline modern;building perceptual;introduction building perceptual;computer vision;perceptual systems;vision models condition;pipeline modern vision;traditional computer vision;modern vision;vision;robotics;paradigm introducing earlyfuson;robotics perform;systems robotics perform;representation entire scene;agent build summary;scene input;architectures require agent;introducing earlyfuson", "pdf_keywords": "early visual pipeline;scenes tasks challenging;task master visual;visual pipeline;scenes tasks;complex scenes tasks;visual attention application;modeling visual attention;backgroundthe goal robots;approach integrating attention;visual attention;visual attention systems;robotic object retrieval;modeling visual;integrating attention;basis visual attention;task specific representation;goal robots;information early visual;objects complex scenes;image processing stack;master visual data;attention systems fully;representation image;bottleneck imposed perception;visual;object retrieval task;visual data;master visual;integrating attention attention"}, "5c6ff5836639e87e8afeaad47e64d0e2234566e8": {"ta_keywords": "fake news detection;fake news misinformation;aware fake news;automatically fact check;attentive network fact;evidence aware fake;widespread fake news;news misinformation various;trend fake news;news detection;news detection utilize;fact check information;automatically fact;news misinformation;news detection methods;fake news;evidence level attention;need automatically fact;fact check;evidence external sources;attention evidence level;head attentive network;network fact;attentive network;attention evidence;sources existing evidence;level attention evidence;recent trend fake;existing evidence aware;evidence aware", "pdf_keywords": "detection fake news;fake news detection;detection false news;word attention layer;evidence attention word;news crucial detection;fact check textual;word attention evidence;evidence level attention;attention evidence attention;attention layers word;document attention layer;layers word attention;evidence attention resultsoverall;attention layer;evidence attention;attentive network fact;attention layers;hierarchically stacking attention;attention evidence level;fact checking mac;evidence attention methodswe;attention important evidence;important evidence attention;attention evidence;analyze false news;word level attention;viewing true news;attention layer second;automatically fact check"}, "963c62b7c4b44ff1fe6aa1f45fa8a7d62b3d5051": {"ta_keywords": "answering science exam;knowledge textual entailment;questions utilizing query;introduction answering science;rewriting background knowledge;textual entailment trained;exam questions utilizing;utilizing query rewriting;knowledge textual;background knowledge textual;answering science;textual entailment;query rewriting background;generic textual entailment;query rewriting;questions utilizing;combining query rewriting;entailment able outperform;introduction answering;utilizing query;textual entailment able;entailment trained scitail;entailment trained;science exam;question using;entailment;methods combining query;exam;generic textual;question using rewriting", "pdf_keywords": "question answering systems;question answering;question answering pipeline;accuracy question answering;construct question answering;question answering ability;domain question answering;approach question answering;knowledge conceptnet;question answering algorithms;knowledge textual entailment;natural language inference;knowledge textual;questions natural language;textual entailment trained;answering systems paraphrase;component question answering;knowledge conceptnet embeddings;answering systems;knowledge conceptnet tandem;new knowledge base;natural language questions;answering systems accuracy;question answeringwe;current knowledge base;requirements answering questions;conceptnet;field natural language;knowledge base;conceptnet embeddings aid"}, "e28b9bc26f5f7eb3b0532d823713400202372da2": {"ta_keywords": "actor critic algorithms;actor critic interaction;model actor critic;interaction actor critic;critic based reinforcement;hierarchical interaction actor;critic interaction player;critic algorithms;known staackelberg game;staackelberg game;staackelberg actor critic;critic interaction;actor critic based;framework staackelberg actor;game given abstraction;staackelberg game given;critic actor critic;based reinforcement;reinforcement learning algorithms;reinforcement learning;interaction actor;critic based;reinforcement;critic actor;game theoretic interpretation;actor critic;general sum game;sum game leader;game theoretic;critic", "pdf_keywords": "actor critic algorithms;actor critic algorithm;actor critic algorithmsin;critic algorithms staackelberg;critic algorithms reinforcement;policy gradient algorithms;critic algorithms game;actor critic method;critic algorithms;critic algorithm policy;framework actor critic;functions actor critic;deterministic policy gradient;policy gradient;critic algorithm;critic algorithms meta;model actor critic;critic based reinforcement;critic algorithms introduce;critic algorithmsin;illustrative reinforcement learning;modeling reinforcement learning;actor critic interaction;actor leader algorithms;critic algorithms experiments;modeling reinforcement;agent reinforcement learning;interaction actor critic;algorithms reinforcement learning;reinforcement learning algorithms"}, "b63bd17d4bb28ba90cc6ff66b51ba5b0377467bf": {"ta_keywords": "models rnnls speech;rnnls speech recognition;language models rnnls;recurrent neural network;training recurrent neural;short term memory;models speech recognition;word error training;speech recognition results;language models speech;training recurrent;neural network language;speech recognition;models rnnls;models speech;speech recognition methods;memory recurrent neural;term memory recurrent;mwe training recurrent;language models;rnnls speech;recurrent neural;recognition results rnls;minimum word error;entropy estimated word;memory recurrent;network language models;word sequence;term memory;error training", "pdf_keywords": ""}, "93d3e45395117e21214d404c8753b578c29266d1": {"ta_keywords": "open question answering;introductionopen question answering;question answering obbc;text question answering;question answering result;question answering;question answering challenging;documents contain answers;open tabular textual;answers question open;dataset open table;open question;open table text;question open systems;answering obbc;answering challenging;tabular textual data;open table;answering challenging task;retrieving information unstructured;unstructured text;unstructured text consider;textual data present;textual data;dataset open;tabular textual;challenging task open;open tabular;question open;contain answers question", "pdf_keywords": "answering text retrieval;open question answering;question answering dataset;question answering tables;question answering text;question answering tabular;question answering open;question answering oog;text question answering;answering tabular textual;messageopen question answering;dataset question answering;task corpus fused;question answering systems;question answering;approach retrieval text;answering tables text;answering tabular;answering text;retrieving information unstructured;answering systems;closureze task corpus;retrieval text;retrieval user friendly;task corpus;answering tables;answering dataset;domain question answering;opendomain question answering;corpus retriever"}, "a0035379f93e0e95bdadd77a1d8eb27ba89dcf60": {"ta_keywords": "story generation models;evaluate story generation;story generation;systems story generation;story generation asked;stories given input;diverse stories originate;evaluate story;number diverse stories;stories originate;diverse stories;plausible enjoyable stories;build evaluate story;enjoyable stories;context systems story;stories originate single;enjoyable stories given;generation models existing;generation models;existing evaluations crowdsourced;generation;stories;meaningfully guide models;evaluations crowdsourced;crowdsourced;long form creative;crowdsourced automatic;evaluations crowdsourced automatic;stories given;produce plausible enjoyable", "pdf_keywords": "storytelling storium database;computational storytelling storium;story generation storium;storytelling community storium;storium user story;storytelling storium;generate stories;story generation user;online collaborative storytelling;collaborative narrative game;story generation uses;story generation;collaborative storytelling community;story generation built;model story generation;storytelling community;generate stories using;story game collaborative;story story game;game collaborative narrative;tool analyzing narrative;storytelling community resultswe;storythe association computational;community storium dataset;collaborative storytelling;platform storytelling using;tool computational storytelling;computational storytelling;story game;experts generate stories"}, "3e4d80e43346b9538504c0a7ee5562f3c6a09178": {"ta_keywords": "incentives recommendations users;incentives recommendations;bandits evolving users;design incentives recommendations;platform providers continually;platform providers;emerge design incentives;design incentives;incentives;common platform providers;users common platform;bandits evolving;common platform;bandits;recommendations users common;platform;evolving users;recommendations users;armed bandits evolving;multi armed bandits;providers continually emerge;armed bandits;evolving users unknown;introductionincentives;type common platform;providers;users common;continually emerge design;providers continually;emerge design", "pdf_keywords": ""}, "740ecaa7fc1f4fc02116181b1757f03c815c7ea9": {"ta_keywords": "time series lstm;clinical time series;lstm;lstm recurrent;series lstm recurrent;lstm recurrent neural;time series clinical;novel application lstm;series lstm;application lstm recurrent;recurrent neural networks;application lstm;introductionphenotyping clinical time;multilabel classification diagnoses;neural networks multilabel;recurrent neural;classification diagnoses;multilabel classification;clinical time;networks multilabel classification;length time series;classification diagnoses given;series clinical measurements;clinical measurements methodsour;recurrent;multilabel;introductionphenotyping clinical;classification;series clinical;clinical measurements", "pdf_keywords": "networks predict health;predict health;short term memory;neural networks prediction;networks sequence learning;neural networks;neural networks article;new method learning;neural networks predict;recurrent neural networks;neural networks sequence;use neural;development neural networks;use neural networks;new methods learning;predict health outcomes;classification diagnoses;linearthe development neural;neural networks important;neural networks multilabel;sequence learning;development novel linear;method learning;method learning key;novel linear linear;discovery data;methods learning;classification;knowledge discovery data;classification diagnoses given"}, "191ef1408406569f0e9a69344add1ae350365431": {"ta_keywords": "characterizing predictive fairness;different predictive fairness;predictive fairness;predictive fairness properties;fairness properties methods;fairness properties;fairness properties set;algorithmic risk assessments;algorithmic risk;predictions individual cases;fairness;characterizing predictive;predictive models deliver;background algorithmic risk;framework characterizing predictive;different predictive;predictive models;predictive;multiple predictive;predictions individual;differ markedly predictions;groups different predictive;multiple predictive models;markedly predictions individual;risk assessments;decisions wide variety;markedly predictions;settings multiple predictive;risk;risk assessments used", "pdf_keywords": "predictive fairness;different predictive fairness;characterizing predictive fairness;predictive fairness properties;predictive disparities fairs;algorithmic framework fairness;classification cost fairness;fairness binary classification;fairs investigate predictive;fairness properties methods;fairlearn model discrimination;algorithmsthe cost fairness;disparities fairs propose;predicting discrimination general;fairness properties;predicting discrimination;predicted discrimination;method predicting discrimination;accuracy predicted discrimination;framework fairness rashomon;approaches discrimination probabilities;predicted discrimination significantly;fairness properties set;disparities fairs;develop framework fairness;predictive disparities set;discrimination probabilities discrimination;domain adaptation;cost fairness;framework fairness"}, "9eeaeadc1e0e300337b47d867a314caeae5c10a9": {"ta_keywords": "brain age population;brain age uniform;learning brain age;brain age;report brain age;healthy population deep;age population healthy;deep learning brain;population deep learning;age population;population deep;population healthy;healthy population;population healthy population;learning brain;age population major;brain;uniform healthy population;problem report brain;age uniform healthy;report brain;population major health;deep learning;population;age uniform;age;health;healthy;deep;health problem", "pdf_keywords": ""}, "9813446d9545b600de9a4972c1382c5e3b22a351": {"ta_keywords": "sistuddent learns cognitive;intelligent tutoring;skills observing methodsthe;intelligent tutoring induces;performance sistuddent learns;cognitive skills observing;problems intelligent tutoring;observe human students;skills observing;tutoring induces cognitive;sistuddent learns;tutoring;students performance sistuddent;human students solving;tutoring induces;learns cognitive;learns cognitive skills;idea sistuddent observe;human students performance;backgroundpredicting students performance;skills observing objectiveto;predicts human students;students solving problems;sistuddent observe;human students;backgroundpredicting students;students solving;evaluate students performance;cognitive skills;students performance", "pdf_keywords": ""}, "78dadbfb6710ac65f178b5e12bd975184aae62fe": {"ta_keywords": "patient history history;patient history;case patient history;history history;history history history;history;present case patient;case patient;article present case;purpose article present;article present;purpose article;article;patient;present case;present;case;purpose", "pdf_keywords": ""}, "241e890c70f6d013de7fe5e174e061ff824dc5e9": {"ta_keywords": "learning agent;learning agent called;agent interactively learns;teaching live machine;learning teaching sistuddent;computer agent interactively;machine learning agent;interactively learns;agent interactively;smstudent computer agent;teaching sistuddent;cognitive skills tutored;interactively learns cognitive;computer agent;teaching sistuddent technical;learns;skills tutored;tutored problem solving;learns cognitive;teaching live;learning teaching;learns cognitive skills;agent called sistudent;sistudent methods smstudent;create students learn;skills tutored problem;tutored;learn teaching live;methods smstudent computer;students learn", "pdf_keywords": ""}, "ca00ead4e5ddd14cbbbce03d89a57d14b430e320": {"ta_keywords": "consumer valuation privacy;valuation privacy design;guarantees privacy screening;privacy design;valuation privacy;guarantees privacy;varying guarantees privacy;privacy customer;privacy customer segmentation;setting privacy;privacy design screening;privacy;privacy screening;setting privacy concerns;privacy concerns natural;privacy screening process;privacy concerns;context privacy customer;context privacy;smart grid;segmentation smart grid;smart grid electricity;deployed setting privacy;electricity grid networked;electricity grid;grid electricity grid;grid electricity;energy consumer;grid networked sensors;offered energy consumer", "pdf_keywords": "metering privacy;smart metering privacy;metering privacy major;formulation privacy implications;designing privacy contracts;metric privacy present;increasing cost privacy;probability successful privacy;metric privacy;formulation privacy;designing privacy;challenging problem privacy;attack model privacy;new metric privacy;model privacy;active incidence privacy;privacy contracts;overview formulation privacy;cost privacy;privacy contracts consider;cost privacy setting;framework designing privacy;privacy setting utility;infer private information;incidence privacy;privacy implications smart;privacy insurance rate;privacy customer segmentation;privacy breach utility;consumer offered privacy"}, "83c7335904002d2b7c7cb403f3538703c9a69025": {"ta_keywords": "nonaudible murmur;murmur allows silent;introductionnaudible murmur nam;nonaudible murmur allows;introductionnaudible murmur;conduction nonaudible murmur;murmur nam soft;murmur;soft whispered voice;murmur nam;murmur allows;nonaudible voice;nam soft whispered;message nonaudible voice;silent speech communication;allows silent speech;nam microphone;whispered voice;nonaudible voice intelligibility;whispered voice recorded;soft whispered;silent speech;speech owing acoustic;nam microphone body;speech communication;recorded nam microphone;voice recorded nam;voice intelligibility;whispered;speech communication makes", "pdf_keywords": ""}, "9f1a1d2cb6b278b7ee24e67d4c2ac38c1161fa1d": {"ta_keywords": "introduction multilingualism indonesia;multilingualism indonesia;multilingualism indonesia gradually;speech translation ethnic;languages english indonesian;translation ethnic languages;introduction multilingualism;english indonesian;multilingualism;ethnic languages;speech speech translation;english indonesian collection;speech translation;ethnic languages english;indonesian;translation ethnic;indonesia methods;indonesia methods paper;indonesian collection;cultural preservation available;communication elders;cultural preservation;indonesia gradually;languages;development speech speech;indonesia;communication elders younger;community rare indonesia;speech speech;rare indonesia methods", "pdf_keywords": ""}, "5bf7f468b763f181c31a5e1edc57bce9a6dbd00c": {"ta_keywords": "extracorporeal membranous oxygenation;ards death extracorporeal;death extracorporeal membranous;respiratory complications coronavirus;membranous oxygenation em;severe acute respiratory;death extracorporeal;acute respiratory;acute respiratory distress;extracorporeal membranous;acute respiratory respiratory;oxygenation ventilation therapy;membranous oxygenation;respiratory distress syndrome;sustaining oxygenation ventilation;respiratory distress;oxygenation ventilation;complications coronavirus;respiratory complications;pneumonia acute respiratory;respiratory syndrome sars;introduction respiratory complications;escalate severe acute;coronavirus disease;coronavirus;complications coronavirus disease;oxygenation em life;coronavirus disease covid;covid 19 escalate;oxygenation em", "pdf_keywords": "viral pneumonia critically;pneumonia critically ill;pneumonia critically;pneumonia acute respiratory;viral pneumonia acute;severe acute respiratory;acute respiratory infections;patients viral pneumonia;acute respiratory syndrome;pneumonia eligible emphysema;pneumonia acute;viral pneumonia;acquired pneumonia covid;acute respiratory distress;pneumonia developed cohort;pneumonia eligible;pneumonia eligible ectomesenchymal;assess pneumonia risk;acute respiratory;pneumonia covid 19;pneumonia risk scores;critically ill patients;pneumonia aim study;introductionrepiratory complications coronavirus;assess pneumonia;pneumonia risk score;pneumonia covid;hospital pneumonia pneumonia;pneumonia patient pulmonary;prevalence viral pneumonia"}, "ea2b138583e587850153f2825fe9e4339aa5f5f9": {"ta_keywords": "speech separation diarization;speech separation recognition;distinguish speech separation;speech separation di;speech separation;separation diarization important;tools speech separation;separation diarization;separation recognition;separation recognition addition;recognize distinguish speech;separation diarization addition;distinguish speech;diarization important;diarization important tools;separation di;important recognize distinguish;separation;recognize distinguish;diarization;recognize important recognize;ability recognize recognize;important tools speech;recognize recognize important;tools speech;diarization addition important;recognize recognize;recognition;speech;ability recognize", "pdf_keywords": ""}, "ca1645abedae3b4caa3345aa8720c8b90f7c37db": {"ta_keywords": "clinical messagevoting rank;voting rules rank;messagevoting rank dependent;messagevoting rank;rank dependent scoring;key clinical messagevoting;rules rank dependent;clinical messagevoting;rules rank;dependent scoring rules;voting rules;scoring rules rdrs;operators include scoring;dependent scoring;scoring rules new;include scoring rules;voting variety statistical;scoring rules;rank dependent;messagevoting;voting variety;based ordered weighted;ordered weighted average;family voting rules;rules empirically;rank;rules empirically certain;rules rdrs based;rules rdrs;manipulable borda voting", "pdf_keywords": ""}, "b2c3d660aaefb80085fe72c80ce81c5fa71980e9": {"ta_keywords": "pivot translation methods;pivot translation useful;based pivot translation;pivot target translation;pivot translation;introductionthe pivot translation;translation useful method;translation methods;useful method translating;translation methods merge;method translating languages;target translation models;method translating;translation models;tree based pivot;source pivot;translation models source;translating languages;target translation;source pivot pivot;merge source pivot;based pivot;phrase based tree;translating languages contain;target phrase pairs;pivot pivot target;pivot target;pivot;translation useful;pivot pivot", "pdf_keywords": ""}, "b7d6829d9eccdbd3d3a5d6f5321a87158588033b": {"ta_keywords": "traits worker bias;continuous traits worker;traits worker;labeling crowd sourced;personality traits methodswe;label patterns continuous;worker bias particularly;calibration workers;label patterns;humans continuous traits;ratingss application personality;worker bias;calibration workers task;personality traits;labeling crowd;pattern labeling;task label patterns;typically labeling crowd;personality;labeling pairwise ratingss;application personality traits;continuous traits;workers task label;problems pattern labeling;application personality;bias particularly;labeling;labels situation;traits;problem calibration workers", "pdf_keywords": ""}, "47adb249ce8f7f5f1e92112ba0f3757f8fbfbfc3": {"ta_keywords": "question answering;estimate semantic similarity;semantic similarity;performance question answering;monolingual alignment models;semantic similarity question;question answering general;lexical semantic models;alpha lexical semantic;answer pairs neural;estimate semantic;language models learn;semantic models;semantic models provide;term alignment probabilities;learn term embeddings;question answer candidates;answer candidates methods;language models;lexical semantic;monolingual alignment;example monolingual alignment;alignment models;alignment models acquire;semantic;introduction alpha lexical;neural network language;term embeddings unstructured;question answer pairs;network language models", "pdf_keywords": ""}, "21ac57d41843ac5367e11b8b784aa57f2ef7a1fc": {"ta_keywords": "stochastic convex optimization;smooth convex stochastic;stochastic convex;convex stochastic convex;convex stochastic;high probability convergence;probability convergence;convex optimization;tailed noise methodswe;convex optimization problems;probability convergence results;logarithmic dependence confidence;non smooth convex;derive high probability;non sub gaussian;gaussian heavy tailed;smooth convex;sub gaussian heavy;stochastic;noise methodswe;tailed noise;sub gaussian;convergence results logarithmic;heavy tailed noise;convex;gaussian heavy;confidence level non;noise methodswe resolve;convergence;dependence confidence level", "pdf_keywords": "convex stochastic optimization;stochastic optimization strongly;smooth stochastic optimization;clipping stochastic gradients;stochastic methods gradient;convex stochastic problems;method stochastic optimization;stochastic optimization;norms stochastic optimization;smooth convex stochastic;convex stochastic;stochastic optimization problems;stochastic gradients;review stochastic optimization;results convex stochastic;stochastic stochastic gradient;stochastic gradient;stochastic problems heavy;clipping stochastic;complexity strongly convex;stochastic methods;stochastic gradient expectation;convergence stochastic stochastic;probability convergence stochastic;stochastic gradients itwe;stochastic optimization light;smooth convex optimization;convergence stochastic;high probability complexity;rules stochastic methods"}, "6cdff2505560390b28db5a96c2ae3070712077cf": {"ta_keywords": "gradient based bandits;reinforcement learning gradient;gradient reinforcement learning;policy gradient reinforcement;competitive gradient based;agents employing gradient;competitive gradient;agents employ gradient;gradient reinforcement;policy gradient;online convex optimization;framework competitive gradient;including policy gradient;learning gradient;algorithms potential games;learning gradient based;algorithms including policy;reinforcement learning;gradient based learning;behavior competitive agents;limiting behavior competitive;learning algorithms including;based bandits;online convex;bandits certain online;competitive agents;convex optimization algorithms;general sum games;based bandits certain;sum games agents", "pdf_keywords": "games gradient learning;gradient algorithms games;continuous games gradient;learning competitive gradient;algorithms gradient play;agent policy gradients;potential games gradient;games gradient based;gradient multi agent;game gradient based;policy gradient algorithms;nash equilibrium gradient;game gradient;behavior competitive gradient;games gradient;competitive gradient based;equilibria game gradient;equilibrium gradient play;competitive gradient;policy gradients;continuous games limiting;agent reinforcement learning;learning gradient flows;consider competitive gradient;multi agent learning;reinforcement learning algorithms;gradient learning evolution;agent employs gradient;simultaneous gradient play;generalization potential games"}, "7301c7aba3c0824b91f69747e7e50f4db56d7fc1": {"ta_keywords": "translation paralinguistic information;translation sensitive paralinguistic;paralinguistic information duration;paralinguistic information;paralinguistic information methodsin;sensitive paralinguistic information;translation paralinguistic;space translation paralinguistic;model speech speech;model speech;paralinguistic;proposed model speech;speech speech translation;speech translation sensitive;sensitive paralinguistic;speech translation;spoken words model;continuous space translation;duration power spoken;speech speech;target acoustic features;source acoustic features;acoustic features target;features target acoustic;speech;acoustic features directly;words model;power spoken words;map source acoustic;space translation", "pdf_keywords": ""}, "8837530b23a2d51054d8752ae2f0ffef8998da8e": {"ta_keywords": "cross domain recommendation;domain location recommendation;recommendation target domain;location recommendation task;domain recommendation;domain recommendation typical;field location recommendation;cross domain location;recommendation task methods;domains location based;recommendation specifically data;location recommendation;data auxiliary domain;location recommendation specifically;cross domain;recommendation task;recommendation target;interaction domains location;perform cross domain;improve recommendation target;location based;target domain typical;improve recommendation;target domain;location based check;background cross domain;domain location;interaction domains;domains location;data perform cross", "pdf_keywords": ""}, "cb15c1c51e8a7da42d5b2ebac955bf1cd9dd4022": {"ta_keywords": "text generation knowledge;generating texts;knowledge graph graphicalal;generation knowledge graphs;knowledge graph;knowledge graphs;particular knowledge graph;generating texts express;knowledge graphs graph;information extraction;text generation;extraction particular knowledge;objective generating texts;multi sentence texts;output information extraction;sentence texts output;information extraction particular;context text generation;text objective generating;texts output information;sentence texts;structured representation content;texts output;texts express complex;sentences requires structured;spanning multiple sentences;graph graphicalal;scientific text objective;problem scientific text;scientific text", "pdf_keywords": "graph text generation;knowledge graphs decoding;knowledge graph text;graph attention network;generating knowledge graphs;attention model graph;graph attention;automatic information extraction;knowledge graph title;generate graph contextualized;sciencedomain information extraction;graphs decoding;generating scientific abstracts;information extraction;graph text pairs;scii annotations;attention network;graph text;knowledge graph;information extraction towe;model graph encoding;graphs decoding decoding;knowledge graphs;data knowledge graph;use graph text;graph encoding;recent graph attention;text generation model;information extraction systems;particular knowledge graph"}, "a54019645dd8e9cfd8d71ab60155449307de3d83": {"ta_keywords": "labeled data crowdfunding;data crowdfunding;crowdsourcing propose simple;data crowdfunding cheap;challenge crowdsourcing;fundamental challenge crowdsourcing;crowdsourcing;challenge crowdsourcing propose;crowdsourcing propose;crowdfunding cheap;crowdfunding;crowdfunding cheap fast;background crowdfunding;background crowdfunding gained;crowdfunding gained;popularity machine learning;machine learning;crowdfunding gained immense;workers answer questions;labeled data;incentivize workers answer;payment mechanism incentivize;simple payment mechanism;payment mechanism;amounts labeled data;popularity machine;simple payment;machine learning applications;large amounts labeled;workers answer", "pdf_keywords": "mechanism crowdsourcing;reward mechanism crowdsourcing;mechanism crowdsourcing ensure;crowdsourcing setup incentive;crowdsourcing workers incentivized;crowdsourcing ensure;results experiments crowdsourcing;crowdsourcing propose simple;experiments crowdsourcing workers;experiments crowdsourcing;crowdsourcing labels;crowdsourcing workers;challenge crowdsourcing;challenge crowdsourcing propose;crowdsourcing data;crowdsourcing;crowdsourcing propose;potential crowdsourcing data;fundamental challenge crowdsourcing;crowdsourcing labels method;potential crowdsourcing;crowdsourcing human;present crowdsourcing;recent developments crowdsourcing;crowdsourcing setup;crowdsourcing ensure collection;limitations crowdsourcing;provide mechanism incentive;challenges limitations crowdsourcing;present crowdsourcing setup"}, "2cc7db7b17ee7349800334b3a154f708850c6410": {"ta_keywords": "separable md codes;powerful erasure codes;erasure codes provide;erasure codes;reed solomon codes;solomon codes achieve;replication powerful erasure;reliability maximum storage;data storage methods;md codes reed;data replication powerful;codes reed solomon;solomon codes;md codes;data replication;data storage;data centers;storage methods;data centers increasingly;simple data replication;reliability replication significantly;reliability replication;replication powerful;level reliability replication;codes reed;maximum storage efficiency;storage methods simple;codes achieve;replication;evolving data storage", "pdf_keywords": ""}, "0ff5b1e61bbebd2f077a4ef24c3afdb344e5b3d4": {"ta_keywords": "patients malignant neoplasms;malignant neoplasms;imaging mri diagnosis;mri diagnosis treatment;mri diagnosis;neoplasms;imaging mri;resonance imaging mri;magnetic resonance imaging;patients malignant;mri;resonance imaging;treatment patients malignant;malignant;magnetic resonance;imaging;diagnosis;role magnetic resonance;diagnosis treatment;diagnosis treatment patients;magnetic;role magnetic;treatment patients;resonance;patients;knowledge role magnetic;treatment;article present;purpose article;purpose article present", "pdf_keywords": ""}, "8e992116bbc8afb075577a30672de7a90fbeba78": {"ta_keywords": "transformer self attention;self attention network;synchronous beam search;e2e automatic speech;attention network;recurrent neural networks;alternative recurrent neural;performance alternative recurrent;compute self attention;beam search;self attention source;beam search algorithm;speech recognition ar;speech recognition;target attention methodsin;automatic speech recognition;attention source target;automatic speech;neural networks end;recurrent neural;attention network shown;attention methodsin;blockwise synchronous beam;blockwise processing encode;source target attention;alternative recurrent;attention source;transformer self;synchronous beam;neural networks", "pdf_keywords": ""}, "3163392f56cdffaa009fbc59f299989a1b8baec1": {"ta_keywords": "classification positive unlabeled;classification data labeled;classification leveraging unlabeled;dealing binary classification;binary classification;class classification;labeled class data;classification;learns labeled positive;learns labeled;class occ classification;classification positive;data labeled class;unlabeled pu learning;leveraging unlabeled data;labeled class;occ classification positive;learning learns labeled;leveraging unlabeled;binary classification data;occ classification;classification data;class classification leveraging;utilizes unlabeled data;unlabeled data improve;labeled positive data;unlabeled data;data utilizes unlabeled;class data scientists;data labeled", "pdf_keywords": "learns labeled positive;unlabeled pu learning;reliable unlabeled data;unlabeled data pulmodynamics;classification positive unlabeled;learning learns labeled;positive unlabeled data;learns labeled;predicting decision function;unlabeled data propose;effective unlabeled data;unlabeled data robust;dealing binary classification;reliable unlabeled;predicting decision;labeled positive data;classification model unlabeled;risk predicting decision;unreliable unlabeled data;classification positive;model unlabeled positive;classification data labeled;pox learning algorithm;leverage unlabeled data;think unlabeled data;classification;unlabeled data improve;apply unreliable unlabeled;binary classification;algorithms leverage unlabeled"}, "73472692b6090a72e36e03127bb99fc2e6bc8de0": {"ta_keywords": "communities latent networks;identifying communities latent;derive networks communities;mapping identifying communities;socio cultural data;unsupervised machine learning;communities latent;identifying communities;networks communities socio;communities socio cultural;cultural cognitive mapping;communities socio;networks communities;communities;community assignments socioo;cultural data;latent networks;methodology derive networks;unsupervised machine;cultural data resultswe;cognitive mapping identifying;cultural data methodology;socioo cultural cognitive;assignments socioo cultural;derive networks;socioo cultural;current unsupervised machine;machine learning techniques;cultural cognitive;socio cultural", "pdf_keywords": ""}, "2f6843f9345ca56af3fd9df5512daa1e7f80bedf": {"ta_keywords": "word level quality;quality estimation;level quality estimation;word level;level quality;submission word level;quality;sheffield submission word;word;estimation;introductionwe university sheffield;submission word;university sheffield submission;introductionwe;sheffield submission;introductionwe university;level;sheffield;university sheffield;submission;university", "pdf_keywords": ""}, "faa4468f2ad1c7cedaf04bf56ebb20ae4b349952": {"ta_keywords": "language models lstm;language models rnn;models lstm lims;models lstm;models rnn lims;lstm lims;lstm lims superior;recurrent neural network;models rnn;memory recurrent neural;term memory recurrent;information lstm lims;short term memory;lstm lims complex;lstm;neural network language;language models;history information lstm;language models various;models various speech;information lstm;modeling word history;memory recurrent;recurrent neural;network language models;rnn lims;based language models;term memory;rnn lims recently;speech recognition", "pdf_keywords": ""}, "872c2d9d8b27ff49367854a7cf67b5dff2010406": {"ta_keywords": "bionl 2009 event;2009 event detection;bionl 2009;event detection task;event detection;2009 event;bionl;event classes;detection task designed;theme event classes;event;detection task;maintaining reasonable recall;recall overall scores;theme event;single theme event;reasonable recall overall;recall overall;event classes range;reasonable recall;recall;detection;domain independent unsupervised;task designed domain;unsupervised;2009;unsupervised possible;precisions achieved;task designed;classes", "pdf_keywords": ""}, "c6713071291729386955586c6309778b1637b852": {"ta_keywords": "laq dynamic games;strategy selfish agents;hvac resource allocation;pricing laq dynamic;control strategy selfish;resource allocation buildings;agents context hvac;selfish agents;dynamic games;selfish agents context;planner influences game;game dynamically;allocation buildings;pricing mechanisms;use pricing mechanisms;pricing laq;dynamic games paper;management pricing laq;quadratic game dynamically;strategy selfish;hvac resource;agents uncoupled social;game dynamically coupled;planner social;purposeenergy management pricing;allocation buildings methodswe;pricing mechanisms means;occupants agents agents;resource allocation;linear quadratic game", "pdf_keywords": ""}, "d5084f48212bed80e8c11e1e69669deea3ba2f83": {"ta_keywords": "hierarchical script knowledge;script knowledge procedural;procedural language;knowledge procedural language;script knowledge;introductionthe hierarchical script;event making pasta;script corpus;parallel script corpus;hierarchical script;example boiling pasta;procedural language requires;procedural;pasta sub event;kidscook parallel script;knowledge procedural;making pasta;boiling pasta sub;requires reasoning hierarchical;reasoning hierarchical temporal;boiling pasta;reasoning hierarchical;introduce kidscook parallel;pasta dish typically;hierarchical temporal;making pasta dish;happens pasta requires;pasta dish;events example boiling;pasta", "pdf_keywords": ""}, "8451d8e20bb9a94c6a576e52ca1a63470f8d2390": {"ta_keywords": "convex composite optimization;method decentralized distributed;smooth optimization methodsin;non smooth optimization;composite optimization;free method decentralized;distributed non smooth;decentralized distributed distributed;smooth optimization;derivative free method;composite optimization problem;decentralized distributed;optimization methodsin paper;distributed distributed non;convex composite;method decentralized;distributed distributed;optimization methodsin;distributed non;2019 convex composite;distributed;new derivative free;sliding algorithm lan;derivative free;sliding algorithm;method based sliding;optimization;free method;convex;optimization problem includes", "pdf_keywords": ""}, "a6a9c06d138537002aaca79dba359cc320b951df": {"ta_keywords": "etiology malignant disease;etiology malignant;disease developed malignant;developed malignant disease;malignant disease developed;malignant disease;patient developed malignant;developed malignant;malignant disease poorly;malignant;disease poorly understood;disease;disease developed;etiology;case patient developed;patient developed;disease poorly;case patient;understood case patient;patient;poorly understood case;poorly understood;understood case;developed;case;poorly;understood", "pdf_keywords": ""}, "03bbbaa03cb57413c2581cc8dc5cbfa532bbea15": {"ta_keywords": "biometrics analyze brain;electrog biometrics analyze;using electrog biometrics;electrog biometrics;accuracy user identification;user identification performance;user identification authentication;user identification using;comparing user identification;user identification;biometrics;analyze brain waves;biometrics analyze;identification using electrog;identification authentication;brain waves;identification performance;identification authentication methodsfirst;identification performance various;identification using;brain waves acquired;electronic device investigate;electronic devices;authentication methodsfirst statistical;electronic electronic devices;accuracy user;dimensionality reduction;combinations dimensionality reduction;electronic device;electronic electronic device", "pdf_keywords": ""}, "73e1dcf5f0f3cf4e645b0bba62d9b1e2ef47b706": {"ta_keywords": "computer methods lexical;human computer communication;lexical class;lexical;human communication;methods lexical;way human communication;communication medium human;methods lexical class;ai speech;human communication need;intelligenceligence ai speech;ai speech natural;introduction human computer;lexical class single;computer communication;semantic;speech;human computer methods;topics artificial intelligenceligence;using communication medium;semantic information;set lexical;communication need using;using communication;specific semantic;specific semantic information;communication medium;human computer;need using communication", "pdf_keywords": ""}, "122b75042daae44f93153dedda15b0fb11b3f279": {"ta_keywords": "question answering datasets;bert based models;datasets evaluating bert;qa datasets answering;evaluating bert based;learn question answering;question answering;popular question answering;answering qa datasets;answering datasets models;question answering methods;evaluating bert;question answering qa;datasets answering;answering datasets qqd;task question answering;bert based;answering datasets;datasets answering datasets;models learn question;answering methods;answering qa;bert;answering methods paper;models learn;datasets qqd outperform;models really learning;qa datasets;answering;qqd outperform humans", "pdf_keywords": "comprehension model efficiently;bert based models;question answering;predict answer models;answer models;task question answering;reading comprehension model;answer models correctly;datasets evaluating bert;question answering methods;comprehension qa datasets;popular question answering;question answering qa;answering qa datasets;evaluating bert based;comprehension user friendly;models learning reading;learning reading comprehension;perform reading comprehension;evaluating bert;approach reading comprehension;comprehension model;reading comprehension qa;reading comprehension user;questions models;language models correctly;questions models need;good reading comprehension;bert based;reading comprehension"}, "e6602786132e040e02df93f729f737f65a116677": {"ta_keywords": "home assistants spoken;digital home assistants;field speech recognition;far field speech;processing deep learning;signal processing deep;speech processing digital;speech recognition;speech processing;home assistants combining;speech recognition commands;deep learning;home assistants;recognition commands spoken;assistants spoken language;processing deep;deep learning techniques;assistants spoken;field speech;assistants combining mixing;learning called far;spoken language interface;assistants combining;processing machine learning;spoken distance sound;mixing signal processing;spoken distance;commands spoken distance;recognition commands;signal processing machine", "pdf_keywords": ""}, "d8682a269523a868f2bc9714b00f0519aa0e931f": {"ta_keywords": "information retrieval resultswirid;integration information sources;deductive databases;text database;databases ranked retrieval;deductive databases ranked;style deductive databases;text database like;introductionthewirl approach integration;information retrieval;retrieval resultswirid;database like queries;retrieval methods;fragments text database;like queries structured;queries structured;ranked retrieval methods;retrieval methods information;databases;like queries;methods information retrieval;database like;integration information;retrieval;ranked retrieval;database;new integration information;approach integration overview;databases ranked;queries", "pdf_keywords": ""}, "8cb74fe4f598699c9c24d88acd4906e2489267af": {"ta_keywords": "detection mri brain;detection mri;imaging mri brain;method detection mri;mri brain;detection navigation brain;magnetic resonance imaging;imaging mri;mri brain important;diagnosis navigation brain;resonance imaging mri;mri;navigation brain;resonance imaging;magnetic resonance;navigation brain developed;brain important tool;detection navigation;imaging;diagnosis navigation;brain important;brain;navigation;detection;tool diagnosis navigation;brain developed new;brain developed;method detection navigation;magnetic;method detection", "pdf_keywords": ""}, "48aced0919e29722d6eed9544353d5507c541cfc": {"ta_keywords": "identify genes text;genes text provide;genes text;gene names text;text ability identify;identification gene names;identify genes;text provide information;names text;gene names;names text ability;information text text;ability identify genes;text text;information text;identification gene;text text processing;provide information text;text provide;clinical messagethe key;text processing;text;clinical messagethe;key clinical messagethe;gene;identification identification gene;text ability;genes;key clinical;clinical", "pdf_keywords": ""}, "b3d9a0308ba6c4ca583a2b4e5be2b3eed466ccbc": {"ta_keywords": "quality d2d channel;d2d channel;d2d channel needs;millimeter wave device;device d2d communication;d2d communication highly;millimeter wave;d2d communication;introduction millimeter wave;channel needs propagated;bbc uv equipments;uv equipments;station bbc uv;uv equipments uvs;device d2d;line sight communication;delay quality d2d;quality d2d;device device d2d;equipments uvs;equipments uvs incur;path quality d2d;uv;sight communication path;bbc uv;introduction millimeter;sight communication;channel;communication path quality;propagated base station", "pdf_keywords": "algorithm relay selection;network based mmwave;mmwave network based;based mmwave network;relay link optimal;propose algorithm relay;network mmwave network;network network mmwave;locally probability relaying;selection relays based;mmwave network;relay selection;communication mmwave communication;network mmwave;mmwave communication;mmwave communication highly;relay selection presence;problem relay selection;algorithm relay;communication mmwave;obstacles millimeter wave;best relay zone;probability relaying;relays based aforementioned;server based mmwave;mmwave channel needs;mmwave network used;relays based;relaying;selection relays"}, "fc8e226c20800c8ccc095bb6a3c0f8dcb637b683": {"ta_keywords": "rectal cancer associated;liver rectal cancer;proportion rectal cancer;rectal cancer;metastasis colorectal cancer;rectal cancer cases;colon cancer china;cancer china proportion;lung metastases compared;metastasis colorectal;colorectal cancer crc;incidence lung metastases;colorectal cancer;lung metastases;site metastasis colorectal;metastases compared colon;crc liver rectal;cancer china;cancer crc liver;compared colon cancer;china proportion rectal;cancer crc;liver rectal;metastases compared;colon cancer;metastasis;cancer associated;cancer associated higher;common site metastasis;cancer cases", "pdf_keywords": ""}, "6c0a3029afd65c83982b3fb96f623da382344286": {"ta_keywords": "tensor matrix factorization;matrix tensor factorization;tensor factorization;tensor factorization theory;natural language processing;matrix factorization methods;matrix 10sor factororization;matrix factorization;factorization methods;semantics dependency parsing;factorization methods attracted;factorization theory;factorization;language processing;factororization methods natural;factororization;factorization theory optimization;lexical semantics dependency;dependency parsing;10sor factororization;factororization methods;10sor factororization methods;basics matrix tensor;methods natural language;matrix tensor;methods tensor matrix;tensor matrix;language processing described;article methods tensor;introduction matrix 10sor", "pdf_keywords": ""}, "928f942baf03dd56aae662fa94d85d22b5600f83": {"ta_keywords": "translation memories paraphrases;memories paraphrases methodstrans;paraphrases methodstrans memories;paraphrases retrieve sentences;use paraphrases searching;paraphrases retrieve;paraphrases searching;paraphrases searching ts;using paraphrases retrieve;memories paraphrases;paraphrase pairs used;paraphrases methodstrans;paraphrase pairs;using paraphrases;use paraphrases;exactly paraphrase pairs;paraphrases;introductionsearching translation memories;resultsby using paraphrases;propose use paraphrases;match exactly paraphrase;retrieve sentences meaning;translation memories;paraphrase;retrieve sentences;translating texts;tools translating texts;exactly paraphrase;introductionsearching translation;translating texts narrow", "pdf_keywords": ""}, "e30b22e692b3c7d2653832bf2901abd8a9375b6e": {"ta_keywords": "distributed content ach;line distributed content;content ach strategy;ach strategy cellular;cellular networks methods;distributed content;strategy cellular networks;cellular networks;distributed;strategy cellular;content ach;cellular;gibbsian line distributed;cells mobile users;cells mobile;line distributed;popularity distribution;collection base stations;according popularity distribution;mobile;popularity distribution known;stations scattered;stations base;ach strategy;base stations;downloads finite;base stations scattered;stations base station;stations;mobile users", "pdf_keywords": "caches cellular networks;caches cellular;placement caches cellular;cell network cache;sampling based caching;caches cellular bs;distributed cache;caching popular contents;based caching finite;cache content;network cache;caching heterogenous;propose cache;network demonstrate caching;content update cellular;content placement caches;iterative distributed cache;caching popular;randomized cache;distributed cache update;cache propose;caches efficiently;cache update algorithm;demonstrate caching;cache update algorithms;caching delivery content;based caching;caching;caching finite;placement consider cache"}, "2406cf39805c70264c4226b7325a09b506c70921": {"ta_keywords": "learning neural sql;neural sql executor;table pre training;neural sql;modeling natural language;pre trained language;trained language models;semi structured tables;structured tables existing;structured tables;executor synthetic corpus;sentences semi structured;language models hit;corpus obtained automatically;table pre;language models;synthetic corpus;existing table pre;sql executor synthetic;language sentences semi;trained language;corpus;synthetic corpus obtained;synthesizing executable queries;sql;natural language;sentences semi;tables;tables existing;natural language sentences", "pdf_keywords": "training structured tabular;table pre training;table pretraining achieved;table corpus pre;training approach tabular;table pretraining;pre training table;pre training database;table corpus;pre training structured;efficient table pre;nl table corpus;perform table pre;tool table pretraining;training table;sentences pretraining better;table pre;novel table pre;table related tasks;neural sql executor;training table related;learning neural sql;neural database executor;pre training corpus;pre training data;large corpus tables;sentences pretraining;structured tabular data;training structured;table related task"}, "3ee38da21d8cf9cb7d4077b729e57f68e9c8d671": {"ta_keywords": "background text generation;text generation learning;text generation largely;text generation;approaches text generation;generation learning demonstrations;generation learning;background text;learning demonstrations;approaches text;text;learning demonstrations current;samples mismatched learning;generation;history distributions;mismatched history distributions;current approaches text;quality samples;learning;samples;generated;learning objective evaluation;learning objective;mismatched learning;background;model generated;generation largely;generation largely rely;rely autoregressive models;demonstrations current approaches", "pdf_keywords": "training text generation;text generation likelihood;text generation tasks;good text generation;model text generation;text generation largely;accuracy text generation;text generation task;text generation comparable;generating text generation;text generation;approaches text generation;generation likelihood learning;text generation propose;novel text generation;generation tasks accuracy;generating text;performance text generation;generation tasks improved;training objective rna;rna sequences human;generation tasks;objective rna sequences;conditional text generation;training text;text generation argue;generation tasks using;generation task;generate text challenging;generation task using"}, "fdaad09b1a897c0a04b9a9579081d542e2b4546c": {"ta_keywords": "omicron infection sars;cov omicron infection;sars cov omicron;infection sars cov;diagnosis sars cov;omicron infection;infection sars;patient diagnosis sars;sars cov;diagnosis sars;cov omicron;omicron;infection;sars;patient diagnosis;report case patient;diagnosis;case patient diagnosis;cov;case patient;patient;report case;report;case", "pdf_keywords": ""}, "74276a37bfa50f90dfae37f767b2b67784bd402a": {"ta_keywords": "textt textt transfer;language nonlinguistic tasks;textt transfer;mt5 multilingual;introduce mt5 multilingual;textt transfer transformer;mt5 multilingual variant;multilingual variant text5;unified text text;textt;multilingual;leveraged unified text;nonlinguistic tasks;unified text;textt textt;language nonlinguistic;text5 pre trained;english language nonlinguistic;multilingual variant;nonlinguistic tasks methodsin;recent textt;recent textt textt;nonlinguistic;covering 101 languages;text text;variety english language;text5;text;text5 pre;wide variety english", "pdf_keywords": "massively multilingual models;multilingual language models;generate massively multilingual;multilingual model massively;language models trained;model massively multilingual;multilingual models;multilingual pre trained;useful tool multilingual;massively multilingual pre;models tasks multilingual;multilingual models useful;trained language models;massively multilingual;introduce mt5 multilingual;massively multilingual language;tasks multilingual benchmark;mt5 multilingual;tool multilingual model;multilingual pre training;multilingual language model;mass multilingual model;multilingual benchmark;approach mass multilingual;translation language processing;mass multilingual;multilingual model;mt5 multilingual variant;multilingual multilingual model;massively multilingual variants"}, "00b2afaf5935b4dea41f134fe11a21a1ed56fa0e": {"ta_keywords": "patients history disease;management patients history;patients history;history disease;management patients;patients;disease;approach management patients;new approach management;approach management;history;new approach;management;role new approach;approach;article discuss role;article discuss;article;new;discuss role;discuss role new;aim article discuss;role;discuss;role new;aim article;aim", "pdf_keywords": ""}, "1bf36cb3453b51550ebadd904a840c75d59f171b": {"ta_keywords": "speech recognition robust;robust speech recognition;errora robust speech;robust speech;robust automatic speech;speech recognition ar;robustness issues deep;new errora robust;errora robust;automatic speech recognition;speech recognition;recognition robust automatic;recognition robust;robustness;deep learning;deep neural network;robustness issues;robust;robust automatic;automatic speech;issues deep neural;deep learning aim;deep neural;recognition ar;based anr methods;network based anr;emergence deep learning;anr methods;recognition ar technologies;anr", "pdf_keywords": ""}, "8b20173b98914f36302389e4c761c334fe867dcd": {"ta_keywords": "text generation systems;dependency parse morphosyntactic;morphosyntactic formedness text;parse morphosyntactic rules;text generation;parse morphosyntactic;context text generation;morphosyntactic rules language;evaluate morphosyntactic formedness;language processing;dependency parse;morphosyntactic rules;natural language processing;evaluate morphosyntactic;metric evaluate morphosyntactic;using dependency parse;morphosyntactic formedness;morphosyntactic;language processing applications;text using dependency;ubiquitous natural language;natural language;parse;formedness text using;automatically extract various;especially multilingual;challenge especially multilingual;formedness text;multilingual;automatically extract", "pdf_keywords": "dependency treebanks propose;dependency parsers robust;robust dependency parsers;directly dependency treebanks;dependency parsers accuracy;dependency treebanks;dependency parse morphosyntactic;generated natural language;robust dependency parser;dependency parsers languages;text verifying morphosyntactic;dependency parsers;parsers accuracy morpho;capturing morpho syntactic;trained parsers;treebanks propose;evaluate morphosyntactic wellformedness;natural language processing;robust parsers;useful analyzing grammatical;treebanks propose measure;german robust parsers;morphosyntactic wellformedness text;parse morphosyntactic rules;evaluating nonlinguisticlinguistics tool;performance dependency parsers;grammar based metrics;assessing nonlinguisticlinguistics metric;dependency parser train;metric evaluating nonlinguisticlinguistics"}, "3dcba175248d0e8d2da44e3731e4adbfb9f00e97": {"ta_keywords": "open information extraction;introduction extracting entities;extracting entities relations;sentence extract relation;information extraction;information sentence extract;extracting entities;information extraction systems;corpora open information;entities relations text;extract relation;sentence extract;relations text;extraction systems relation;introduction extracting;sentences relation tuples;relations text important;relation tuples entity;string relation sentences;massive text corpora;entities relations;open information;sentences relation;relation sentences;schema relations interests;context information sentence;predicate string relation;extracting;text corpora;relation sentences relation", "pdf_keywords": "corpus extraction entities;entity phrase extraction;extraction tuples corpus;relation phrase extraction;phrase extraction;open information extraction;corpus extraction;domain speci corpus;relation tuples extraction;framework ective extraction;information sentence extract;clinical message extracting;used corpus extraction;supervised entity phrase;phrase extraction performed;er corpus introduce;phrase extraction task;sentence extract relation;extraction entities relationships;recognize entities sentences;extraction entities;corpus open ionization;extracting entities relations;extraction relation tuples;corpus er corpus;sentence extract;er corpus;information extraction;tuples corpus tertiary;entity phrase pairs"}, "96dbffb71e4d62a985f826197845623b1415c267": {"ta_keywords": "learners better metacognition;metacognitive learning;yielding metacognitive learning;metacognitive learning acquiring;build better metacognitive;better metacognition;metacognition acquire knowledge;better metacognitive educational;yielding metacognitive;better metacognitive;algorithm yielding metacognitive;metacognitive educational methods;better metacognition acquire;metacognitive educational;metacognition;metacognitive;free grammar induction;metacognition acquire;grammar induction;learners better;grammar induction algorithm;assist future learning;future learning;learning;learners;learning acquiring deep;acquire knowledge faster;better models learning;context free grammar;learning acquiring", "pdf_keywords": ""}, "ce9919ffb9dab701babd67a945b1590917345789": {"ta_keywords": "multiple explanations training;explanation based learning;multiple inconsistent explanation;theories multiple inconsistent;explanations training;explanations training instance;inconsistent explanation problem;produces multiple explanations;inconsistent explanation;imperfect theories multiple;applying explanation based;imperfect theories;eb imperfect theories;explanation problem occurs;learning eb imperfect;multiple inconsistent;theories suffer multiple;multiple explanations;theories multiple;explanations;explanation based;correct domain theories;problem multiple inconsistent;domain theories suffer;explanation problem multiple;applying explanation;suffer multiple inconsistent;theory produces multiple;inconsistent;domain theories", "pdf_keywords": ""}, "53e161d4434576355fc5f63fe56afd8e135174b2": {"ta_keywords": "understand narratives providing;understand narratives;neural language models;narratives providing expectations;trained neural language;narratives providing;help understand narratives;neural language;narratives;high quality scripts;language models;language models finet;pre trained neural;event sequences describing;quality scripts;trained neural;scripts;event sequences;standardized event sequences;scripts standardized event;text;sequences describing;neural;language;sequences describing typical;time pre trained;text work demonstrate;scripts varying levels;everyday activities shown;describing typical everyday", "pdf_keywords": "prediction script generation;crowdsourced scripts;crowdsourced scripts introduce;script generation models;script generation tasks;script edge prediction;data crowdsourced scripts;prediction entire script;prediction script;script generation;novel natural language;neural language models;generation structured prediction;graph task predict;automatedproscriptedge pred neural;language models successfully;proscriptgen neural language;existing data crowdsourced;language model generate;entire script generation;trained neural language;structured prediction;computational natural language;high quality scripts;natural text generation;generation task graphproscriptgen;language models;data crowdsourced;text generation structured;neural language model"}, "6c59a6ad00d82ca9f76fef92232ff3e2f3c1acc8": {"ta_keywords": "speaker diarization;speech speaker diarization;speaker diarization speech;handling overlapping speech;overlapping speech speaker;overlapping speech;diarization speech;combining outputs diarization;diarization speech natural;overlapping segments diarization;diarization systems majority;ensemble techniques propose;diarization outputs;speech speaker;algorithm combining outputs;outputs diarization systems;ensemble techniques;diarization systems;diarization;outputs diarization;algorithm combining;segments diarization outputs;propose algorithm combining;voting methods;majority voting methods;diarization outputs modify;ensemble;speaker;segments diarization;combining outputs", "pdf_keywords": "overlap aware diarization;handling overlapping speech;overlapping speech;speaker diarization;speech speaker diarization;diarization speech;speaker diarization speech;combine diarization systems;combining diarization outputs;combining diarization data;aware diarization systems;overlapping speech speaker;speech used overlap;aware diarization;combining outputs diarization;combining diarization;diarization speech natural;difficult combine diarization;pair diarization widely;combine diarization;combining diarization hypothesis;method combining diarization;diarization widely;auditory diarization multiparty;diarization systems majority;diarization data difficult;overlapping segments diarization;diarization systems;diarization systems used;diarization multiparty meeting"}, "a5881560968963d0c845c468a273261fde0b7248": {"ta_keywords": "inputs fragile interpretations;fragile interpretations deep;interpretability methods;interpretability;fragile interpretations;interpretability methods like;robust trustworthy nonlinguistic;interpretations need robust;trustworthy nonlinguistic;word perturbations input;trustworthy nonlinguistic applications;natural language model;deep natural language;methods interpretability;explaining natural language;processing methods interpretability;methods interpretability methods;predictions relative word;language model predictions;natural language processing;natural language;introductionperturbing inputs fragile;interpretations deep;word importance scores;interpretations deep natural;relative word importance;making simple word;perturbations input text;simple word perturbations;word perturbations", "pdf_keywords": "small adversarial perturbations;adversarial perturbations input;adversarial perturbations;make small adversarial;perturbed text examples;adversarial perturbations aim;small adversarial;fragile explanations;fragile explanations propose;method perturbed sentences;adversarially;predict interpretations sentences;adversarially robust;increasingly perturbed text;adversarially trained results;gradients predict interpretations;adversarial;models adversarially;adversarially trained;perturbed sentences;inputs explanations significantly;fragile interpretations optimize;models adversarially robust;perturbed sentences demonstrate;models models adversarially;possible perturbed text;models adversarially trained;perturbed text inputs;text inputs explanations;swaps adversarial perturbations"}, "3004a3e4d8969dc3c36c9274b0f76ecc874f2e6a": {"ta_keywords": "separation speech embedded;speech embedded;separation speech;introduction separation speech;speech embedded non;dynamics deep recurrent;deep recurrent networks;noisy spectrum;applied noisy spectrum;deep network;noisy spectrum viable;stationary interference challenging;improvements using deep;recurrent networks;estimating masking function;modeling dynamics deep;recurrent networks shown;non stationary interference;using deep network;interference challenging;stationary interference;deep recurrent;deep network based;estimating masking;function applied noisy;interference challenging problem;speech;dynamics deep;interference;using deep", "pdf_keywords": ""}, "e29e43d9c0772d44cff53044484970599db30d5f": {"ta_keywords": "domain adaptation;domain adaptation strategies;use domain adaptation;source target domains;domains drawback statistics;labeled data domain;statistics different domains;networks known data;domains tasks inherently;target domains;different domains tasks;hungry domain sensitive;domains tasks;data hungry domain;introductionnural networks;introductionnural networks known;target domains drawback;networks;domain sensitive nearly;domain sensitive;networks known;domains;data domain;domains drawback;different domains;data domain interested;adaptation;hungry domain;adaptation strategies;statistics source target", "pdf_keywords": "domain adaptation deep;domain adaptation neural;domains deep adaptation;domain adaptation;unsupervised domain adaptation;adaptation dd deep;domain adaptation strategies;deep adaptation dd;approach domain adaptation;use domain adaptation;domain differential adaptation;method domain adaptation;adaptation neural language;target domains deep;adaptation deep network;adaptation deep;domain adaptation active;machine translation domain;trained domain domain;trained domain;domains deep;domain monolingual data;framework domain adaptation;deep adaptation;predicting domain words;domain domain monolingual;article deep adaptation;domain monolingual;deep adaptation method;shiallow deep adaptation"}, "f6eafb82d2450f28f668443b689c91e896a0d63e": {"ta_keywords": "stochastic bandit problem;stochastic bandits linear;solve stochastic bandits;stochastic bandits;stochastic bandit;applied stochastic bandit;linear bandit problem;bandits linear bandit;bandit problem;bandit problem using;linear bandit;bandits linear;optimism face uncertainty;problem using optimism;bandits;using optimism;bandit;uncertainty principle;optimism;using optimism face;uncertainty;approach solve stochastic;uncertainty principle method;solve stochastic;stochastic;optimism face;method applied stochastic;face uncertainty principle;applied stochastic;face uncertainty", "pdf_keywords": ""}, "8225b047e0fe90c2d5f9bb77fd94396a9d0fd21e": {"ta_keywords": "gene identifiers documents;associating gene identifiers;article identifier gene;gene identifiers;organism database curation;identifiers documents;framework associating gene;identifiers documents methodsa;organism database;model organism database;identifiers documents resultsone;identifier gene;documents methodsa graph;identifier gene discussed;graph search framework;backgrounda graph search;methodsa graph search;graph search;gene discussed article;associating gene;article identifier;search framework associating;automated systems article;database curation;database curation process;gene;search;process article identifier;suitable semi automated;search framework", "pdf_keywords": ""}, "3873e60de2d20aa33829e2d3d79221e716785546": {"ta_keywords": "language modeling perceptron;features unlabeled speech;speech discriminative language;discriminative language modeling;unlabeled speech discriminative;speech discriminative;estimating discriminative language;discriminative language models;unlabeled speech;conversation based features;language models;discriminative language;language modeling;gram features;modeling perceptron algorithm;language models correct;deriving conversation based;perceptron algorithm;modeling perceptron;errors output auditory;perceptron algorithm used;output auditory auditory;deriving conversation;auditory simplest version;speech;background deriving conversation;output auditory;gram features appear;auditory auditory auditory;auditory auditory", "pdf_keywords": ""}, "229c0c13e5c2d8e189efccf77b8179ec16500212": {"ta_keywords": "clinical messagewe decoder;string machine translation;forest string machine;tree transducers provides;machine translation engine;based tree transducers;messagewe decoder forest;tree transducers;decoder forest string;forest string pipeline;machine translation;translation engine based;translation engine;clinical messagewe;key clinical messagewe;forest string;string machine;string pipeline;decoder forest;entire forest string;decoding evaluation;decoder;hypergraphmert training;string pipeline rules;extraction tuning decoding;hypergraphmert training sparse;pipeline rules extraction;messagewe decoder;tuning decoding evaluation;rules extraction tuning", "pdf_keywords": ""}, "14fce3cfa503894f244fc6ea8a7a00fa0ddfd94e": {"ta_keywords": "design computer ethics;computer ethics;ethics;approach design computer;design;design computer;approach design;new approach design;purpose article present;computer;purpose article;approach;new approach;present new approach;article present;purpose;article present new;article;present new;new;present", "pdf_keywords": ""}, "a54a3a7b02cacd92b3bc633be7ea54e4f365fa65": {"ta_keywords": "detecting malware communities;malware communities;malware communities using;detecting malware;malware features explored;computer malware features;malware features;context detecting malware;computer malware;berlin characterized malware;characterized malware;malware;characterized malware binaries;malware binaries;malware binaries benign;sm computer malware;cultural cognitive mapping;benign malicious based;malicious based;binaries benign malicious;cognitive mapping sm;cognitive mapping;cognitive mapping methods;malicious based 1024;benign malicious;features derived deep;malicious;socio cultural cognitive;cultural cognitive;program features derived", "pdf_keywords": ""}, "935c275868bec7301f4bd254159978d8ded138b9": {"ta_keywords": "xylazole exerts anesthesia;analgesia fetal rat;xylazole;anesthesia analgesia;anesthesia analgesia fetal;xylazole exerts;exerts anesthesia analgesia;analgesia fetal;analgesia;nerve cells cgp;fetal rat nerve;rat nerve;rat nerve cells;anesthesia;cells cgp signaling;cgp signaling pathway;nerve cells;exerts anesthesia;nerve;cgp signaling;fetal rat;cells cgp;signaling pathway;signaling;cgp;rat;fetal;pathway;cells;exerts", "pdf_keywords": ""}, "b9f0c7e99bcc94c2cd75fd8e1cef45188f51270e": {"ta_keywords": "connectionist temporal classification;temporal classification gc;temporal classification loss;graph semi supervised;based temporal classification;temporal classification;graph based temporal;speech recognition;graph based supervision;automatic speech recognition;automatic speech;connectionist temporal;label sequences graph;semi supervised;speech recognition ar;improve automatic speech;semi supervised learning;sequences graph semi;pseudo label sequences;form connectionist temporal;graph semi;supervised learning paper;label sequences;sequences graph;supervised;supervised learning;graph based;temporal;classification loss;classification gc", "pdf_keywords": "transitions speech processing;connectionist temporal classification;end speech recognition;continuous speech recognition;temporal classification gc;speech recognition;corresponding transitions speech;recognition continuous speech;temporal classification loss;speakers speech recognition;automatic speech;label transitions neural;speech processing;speech recognition highly;recognition automatic speech;speech recognition continuous;decoding speech recognition;transitions speech;automatic speech recognition;transitions neural network;speech recognition acoustic;speech recognition automatic;speech recognition model;speech recognition ar;temporal classification;based temporal classification;classification multi speaker;approach automatic speech;improve automatic speech;backgroundgraph based temporal"}, "dfa34a10e2ba861545549c3188ef245b1e69bcdf": {"ta_keywords": "bio event extraction;text event extraction;word embedding methods;word embedding;event extraction;event extraction important;extracting biological networks;advances word embedding;event extraction using;latest word embedding;word distribution;introduction bio event;words text event;word embedding make;word distribution ef;bio event;extracting biological;goal extracting biological;computation word distribution;biological networks scientific;networks scientific literature;using bag ofwords;biological networks;extracting;words text;embedding methods;bag ofwords;extraction using;embedding methods methods;distributional characteristics words", "pdf_keywords": ""}, "2ea5b0f5e476ddc00ae4450f2888a51fa25dd1d3": {"ta_keywords": "training task augmentation;better shot learning;shot learning;pre trained language;trained language models;shot learning methods;self training task;task augmentation better;trained language;task augmentation;augmentation better shot;non language tasks;language tasks;self training;strategy self training;training task;training examples;language tasks large;pre trained;language models perform;training examples available;learning;models perform shot;perform shot;augmentation better;scale pre trained;language models;augmentation;trained;better shot", "pdf_keywords": "benchmarks task augmentation;training task augmentation;performance task augmentation;task augmentation selftraining;approaches task augmentation;task augmentation domains;examples task augmentation;task augmentation effective;task augmentation approach;algorithms task augmentation;combining task augmentation;approach task augmentation;task augmentation novel;propose task augmentation;task augmentation;task augmentation significantly;augmentation selftraining improving;selftraining improving sample;implementation task augmentation;tasks task augmentation;task augmentation self;training task improve;demonstrate task augmentation;task augmentation uses;novel data augmentation;uses task augmentation;augmentation selftraining;performance shot tasks;improves model training;augmentation self training"}, "92f93c0014ba4da59180c4cd141ad0dcaad5803f": {"ta_keywords": "multilingual transfer learning;transfer learning multilingual;multilingual deep retrieval;multilingual transfer;learning multilingual deep;transfer learning instance;learning multilingual;type multilingual transfer;multilingual deep;transfer learning;deep retrieval case;based transfer learning;transfer learning improve;transfer learning data;improve performance multilingual;deep retrieval;instance based transfer;multilingual;performance multilingual;retrieval case presentationperhaps;languages pooled;languages pooled single;multilingual setting instance;auxiliary languages pooled;performance multilingual setting;learning instance;learning instance based;type multilingual;multilingual setting;retrieval case", "pdf_keywords": "multilingual deep retrieval;transfer learning multilingual;learning multilingual deep;multilingual deep;multilingual deep identified;learning multilingual information;cross lingual dependencies;multilingual information transfer;deep retrieval tasks;formulated multilingual deep;cross language instance;contexts corpus train;deep retrieval;deep retrieval search;shared embedding vocabulary;learning multilingual;lingual dependencies;tasks sentence prediction;cross lingual;transfer learning surprisingly;examine deep retrieval;transfer learning significantly;lingual dependencies tail;transfer learning;target languages tasks;transfer learning effective;learning target language;sentence prediction inverse;tiny cross lingual;embedding vocabulary"}, "4a9c80e263fd0a88ad8220aa076ede4a3e77fcc1": {"ta_keywords": "vulnerable adversarial samples;vulnerable adversarial;deep testing methods;deep testing;known vulnerable adversarial;detect various adversarial;adversarial samples generated;adversarial samples;kinds adversarial attacks;adversarial attacks;various adversarial samples;model mutationation testing;adversarial samples detection;adversarial;various adversarial;vulnerability dn;number deep testing;vulnerability dn systems;proposed vulnerability dn;kinds adversarial;mutationation testing;mutationation testing mmm;different kinds adversarial;networks known vulnerable;dnn models recently;dnn models;model mutationation;application dnn models;systems model mutationation;deep neural", "pdf_keywords": "testing method adversarial;detect adversarial;detect adversarial samples;detecting adversarial;detecting adversarial samples;testing mmt adversarial;method detecting adversarial;detect bugs adversarial;approach detecting adversarial;testing gt adversarial;vulnerable adversarial samples;deep testing methods;models adversarial detection;detect various adversarial;vulnerable adversarial;easy detect adversarial;adversarial detection neural;mmt adversarial detection;adversarial detection performance;method adversarial detection;bugs adversarial samples;algorithm adversarial detection;detection adversarial samples;adversarial attack methods;adversarial samples developed;models adversarial;determine input adversarial;deep testing;bugs adversarial;distinguish adversarial"}, "885fe11ed7ab81c8609ccddb3e10f62577c04ab9": {"ta_keywords": "learning agents dialogue;exploration deep learning;explore thompson sampling;deep learning agents;agents dialogue systems;exploration deep;learning agents;dialogue systems agents;thompson sampling;algorithm learns;efficiency exploration deep;agents dialogue;agents explore thompson;algorithm learns faster;dialogue systems;greedy boltzmann bootstrapping;thompson sampling drawing;learns;bootstrapping intrinsic reward;dialogue;deep learning;boltzmann bootstrapping intrinsic;network algorithm learns;bayes backprop neural;agents explore;learns faster;intrinsic reward based;explore thompson;boltzmann bootstrapping;backprop neural", "pdf_keywords": "learning agents dialogue;deep reinforcement learn;promising deep reinforcement;deep reinforcement learning;reinforcement learning speech;dialogue optimal policy;exploration deep reinforcement;model dialogue optimal;learn policies dialogue;deep reinforcement;reinforcement learning discuss;agents dialogue systems;exploration deep learning;dialogue optimal;policies dialogue systems;explore dialogues;dialogue systems agents;greedy exploration boltzmann;exploration dialogues;explore dialogue;approach exploration deep;reinforcement learn;explore dialogue fundamental;ability explore dialogues;reinforcement learning useful;exploration deep;deep learning agents;natural language dialogue;dialogue systems;learning agents"}, "a1340029d8a5c57bee8a5995ac3beafd3d0ba96c": {"ta_keywords": "sensing tracking markov;collection wireless sensors;sensing tracking;tracking markov chain;purposeactive sensing tracking;tracking markov;averaged sensors presence;wireless sensors;wireless sensors form;sensors presence computing;sensors presence;estimates averaged sensors;active sensors;active sensors mean;averaged sensors;tracking;number active sensors;network collaboratively estimates;sensors mean;purposeactive sensing;sensing;hop network collaboratively;markov chain considered;sensors;estimates process node;sensors form;markov chain;node measurements locally;multi hop network;methodsa collection wireless", "pdf_keywords": ""}, "c6854064cb5053e67d23394eee6d1646108f6d56": {"ta_keywords": "textual entailment task;textual entailment;novel textual entailment;setting textual entailment;language inference multiple;entailment task;textual entailment evaluate;multiple premise sentences;language inference;inference multiple premises;introductionnatural language inference;entailment task requires;trivial lexical inferences;premise sentences;lexical inferences emphasizes;inference multiple premise;lexical inferences;entailment;premise sentences present;inferences emphasizes knowledge;inference multiple;inferences emphasizes;requires inference multiple;multiple premise task;entailment evaluate;task requires inference;textual;textual en;premise task;multiple premises", "pdf_keywords": "neural entailment textual;neural entailment baselines;texts neural entailment;entailment task trained;neural entailment;textual entailment task;neural models entailment;strong neural entailment;entailment baselines dataset;entailment task;premise sentences task;standard entailment task;novel textual entailment;textual entailment;entailment task involves;entailment textual;entailment baselines;premise entailment labels;entailment textual entailment;premise sentences event;textual entailment propose;single premise entailment;multiple premise entailment;sentences event premises;entailment task requires;premise entailment;multiple premises attention;entailment literature data;setting textual entailment;multiple premise sentences"}, "74b05adf1ec74849a4f7963fe3f17fd61b92af4b": {"ta_keywords": "learn query languages;like query languages;query languages;learn query;queries contextual;language interfaces database;interfaces database nldb;queries contextual information;natural language interfaces;search databases using;having learn query;query languages multi;multiple queries contextual;database nldb;search databases;databases using natural;users search databases;database nldb attracted;language instead sql;attention nldb allow;query languages saving;query intents;involves multiple queries;query intents methods;queries;attention nldb;considerable attention nldb;databases using;interfaces database;nldb allow", "pdf_keywords": "semantics follow queries;query analysis semantics;termed follow query;follow queries interpreted;natural language queries;follow queries;follow query analysis;follow queries processed;interpret follow queries;learn query languages;follow queries present;queries contextual;like query languages;queries contextual information;preceding follow queries;learn query;followup query weakly;queries user friendly;language queries;query languages;followup dataset;ability follow queries;query followup query;queries interpreted precedent;query followup;followup data dataset;context independent queries;followup query;databases nlb;multiple queries contextual"}, "a5f214e23b8cd35a370a182c155ef333d77c5bb2": {"ta_keywords": "phonetic correlates stance;stance natural speech;phonetic properties stance;acoustic indicators stance;correlates stance taking;correlates stance;acoustic phonetic;annotated stance strength;understand acoustic phonetic;annotated stance;hand annotated stance;stance natural;acoustic phonetic properties;context phonetic correlates;stance taking;phonetic correlates;natural speech;development natural speech;natural speech important;stance taking important;natural speech using;stance;context phonetic;phonetic;stance taking objective;speech using;phonetic properties;corpus collaborative conversational;stance strength;important understand acoustic", "pdf_keywords": ""}, "b33caf27fe5584b9b773c75fc35ee0e8b1421864": {"ta_keywords": "population potential games;potential games population;continuous population potential;type population potential;subset strategies potential;continuous type population;population potential;strategies potential game;games population mass;potential games considered;games population;potential game;strategies subset;potential games;continuous population;subset strategies;potential game difference;strategies subset results;dimensional continuous type;strategies potential;corresponds subset strategies;preference strategies subset;extension continuous population;type population;introductionmulti dimensional continuous;continuous type;games considered consider;distribution multi dimensional;population member type;games considered", "pdf_keywords": ""}, "5ee96dd7e3395d8a53d6d3ceb62593477a4e0fe1": {"ta_keywords": "automatic colorization;language conditioned colorization;automatic colorization process;images automatic colorization;colorizations language;plausible colorizations language;colorization;colorizations language agnostic;learning color language;colorized image;conditioned colorization;color language;colorized;manipulate colorized image;colorizations;colorization process;learning color;accurate plausible colorizations;users manipulate colorized;colorized image feeding;manipulate colorized;colorization process adding;plausible colorizations;captions methods;colorization produce accurate;conditioned colorization produce;colorization produce;feeding different captions;captions methods present;introduction learning color", "pdf_keywords": "colorization convolutional;convolutional layers colorization;convolutional layer colorization;layers colorization convolutional;colorization convolutional layer;learning colorization;learning colorization large;colorization large scale;backgroundautomatic colorization;language conditioned colorization;learn colorization;approach colorization humans;colorization;new approach colorization;colorization ability;colorization humans use;plausible colorizations language;colorizations language;colorization humans;approach colorization;method learning colorization;ability learn colorization;layer colorization ability;accurate plausible colorizations;colorization large;colorizations;conditioned colorization;backgroundautomatic colorization process;manipulate colorized image;colorizations language agnostic"}, "53f1fb4dc887540ef134a8d08c152789c313aa5c": {"ta_keywords": "phoneme transcripts based;trained phoneme transcripts;speech recognition systems;end speech recognition;triphone based dann;speech recognition;using triphone based;extraction using triphone;phoneme transcripts;kaldi toolkit corpus;using triphone;triphone based;hmm trained phoneme;corpus spontaneous ja;trained phoneme;toolkit corpus spontaneous;hmm character based;toolkit corpus;triphone;dann hmm character;transcripts based large;feature extraction;composite embedding systems;embedding systems;corpus;based dann hmm;performance feature extraction;novel composite embedding;dn hmm trained;feature extraction using", "pdf_keywords": ""}, "7262bc3674c4c063526eaf4d2dcf54eecea7bf77": {"ta_keywords": "paraphrase pairs generated;sentential paraphrase pairs;paraphrastic senstence embeddings;paraphrase pairs;neural machine translation;english sentential paraphrase;sentential paraphrase;millions machine translations;paraphrastic senstence;paraphrastic;large parallel corpus;machine translation;machine translations;paraphrase;paranm;parallel corpus;machine translation translate;parallel corpus following;translations methodswe paranm;paranm 50m dataset;million english english;limits paraphrastic senstence;paranm 50m;machine translations methodswe;embeddings millions;embeddings millions machine;translations;million english;50 million english;limits paraphrastic", "pdf_keywords": "generating paraphrases;sentential paraphrases generated;generating finding paraphrases;paraphrases freelyavailable datasets;database sentential paraphrases;model paraphrase generation;paraphrase database;paraphrase generation;paraphrases generated;generating paraphrases purposes;paraphrase generation canonicalization;paraphrase pairs generated;supervised model paraphrase;ones incorporating paraphrase;paraphrase corpus;incorporating paraphrase knowledge;paraphrastic sentence embeddings;wietingthe paraphrase database;used generating paraphrases;finding paraphrases freelyavailable;introductionparaphrastic sentence embeddings;paraphrases generated simple;paraphrase corpus using;sentential paraphrases;trained sentence embeddings;development paraphrase corpus;sentential paraphrase pairs;obtain paraphrase lexicon;incorporating paraphrase;paraphrase lexicon paranmt"}, "6d2d86cf5e80b58a03360559095ea3603548248f": {"ta_keywords": "estimate matrix rows;constraint permutation columns;increasing unknown permutation;shape constraint permutation;squares estimator optimal;permutation columns methodswe;unknown permutation columns;estimate matrix;constraint permutation;permutation columns;permutation columns past;efficient algorithms;statistical seriation problem;algorithms computing squares;computing squares estimator;algorithms;estimator optimal logarithmic;unknown permutation;matrix rows;estimator optimal;statistical seriation;goal estimate matrix;squares estimator;factors efficient algorithms;efficient algorithms computing;matrix rows satisfy;squares estimator remain;rows satisfy shape;case rows monotonically;rows monotonically", "pdf_keywords": ""}, "2d9769ce319a8acbe97438b45b0d381db2a538d1": {"ta_keywords": "case srr rr;srr rr rr;srr rr;report case srr;case srr;rr rr rr;rr rr;srr;rr;report case;case;report", "pdf_keywords": ""}, "c143d2b09bdfc0dff784dce2668fd5657806dbf2": {"ta_keywords": "inference explanation regeneration;multi homop inference;homop inference;explanation regeneration tasks;homop inference explanation;task multi homop;fact explanations standardized;multi fact explanations;explanations standardized;tasks participants regenerating;explanations standardized science;2020 shared task;multi homop;regeneration tasks participants;inference;detailed multi fact;explanation regeneration;explanation regeneration methodsthe;detailed multi;fact explanations;homop;shared task multi;task multi;inference explanation;participants regenerating;methodsthe 2020 shared;regeneration tasks;tasks participants;multi fact;large detailed multi", "pdf_keywords": ""}, "2873053aa18059a61ead5880d449f5bccda2d213": {"ta_keywords": "interdependent scheduling games;players interdependent services;scheduling games;scheduling games player;interdependent scheduling;schedule independently player;schedule services;services schedule independently;model interdependent scheduling;interdependent services;services schedule;scheduling;player free schedule;set services schedule;player predecessor services;free schedule services;interdependent services motivated;services controlled player;schedule services time;reward player predecessor;schedule independently;model players interdependent;players interdependent;reward player;accrue reward player;services controlled;free schedule;predecessor services controlled;schedule;services time", "pdf_keywords": "interdependent scheduling game;interdependent scheduling games;scheduling game;scheduling games;scheduling game players;consider interdependent scheduling;interdependent scheduling;scheduling;scheduling model;scheduling game ig;scheduling games motivated;scheduling model suitable;optimizing schedule conflict;maximizing schedule;optimal schedule program;scheduling large;maximizing schedule necessarily;class interdependent scheduling;restoring critical infrastructure;schedule integer programming;scheduling large number;given schedule conflict;approach deterministic scheduling;welfare maximizing schedule;deterministic scheduling large;optimal schedule;schedule maximizes;welfare optimizing schedule;schedule maximizes welfare;setting interdependent scheduling"}, "fee62123e1d2ac56065675983475b079e1e9106f": {"ta_keywords": "compared greedy decoding;neural sequence models;cross entropy training;decoding algorithm neural;test time decoding;cross entropy trained;greedy decoding;greedy decoding methodsin;models beam decoding;algorithm neural sequence;entropy trained models;entropy training;entropy training procedures;entropy trained;neural sequence;final decoding;beam decoding yield;decoding algorithm;time decoding algorithm;decoding;beam decoding;time decoding;behaviour final decoding;final decoding method;sequence models;trained models beam;decoding method;decoding yield;decoding methodsin;introductionbeam search desirable", "pdf_keywords": "loss optimization;gradient optimization end;direct loss optimization;directly gradient optimization;approximation beam search;loss optimization dl;optimization end end;optimization end;gradient optimization;beam search decoding;objective using backpropagation;computed beam search;output beam search;beam search continuous;loss metric beam;directly optimizes continuous;continuous relaxation argmax;beam search algorithm;backpropagation;relaxation beam search;discontinuous difficult optimize;difficult optimize;directly optimizes;surrogate training objective;neural machine translation;search neural;composition final loss;search decoding;optimizes continuous;training neural sequence"}, "a96e05353032cc6f3d72eb5eca192295beac065e": {"ta_keywords": "stacked graphical learning;graphical learning efficient;graphical learning generally;graphical learning;stacked graphical;classification problems stacked;graphical models faster;learning efficient;accuracy graphical models;base learner methodsin;problems stacked graphical;base learner;graphical models;backgroundthe stacked graphical;efficient especially inference;learning efficient especially;faster inference resultsin;learner methodsin;faster inference;learner methodsin experiments;models faster inference;comparable accuracy graphical;classification;inference capable;especially inference capable;stacked;accuracy graphical;learning generally;kind base learner;learning generally achieved", "pdf_keywords": ""}, "c52ac453e154953abdb06fc041023e327ea609a4": {"ta_keywords": "attention acoustic modeling;attentional acoustic models;self attentional acoustic;self attention acoustic;attentional acoustic;attention acoustic;acoustic models;acoustic modeling;acoustic modeling computational;acoustic models useful;encoding sequences vectors;acoustic modeling proposing;apply acoustic modeling;modeling discrete sequences;sequences vectors relating;sequences vectors;encoding sequences;self attentional;self attention;tools encoding sequences;apply self attention;attentional;sequences;trivial apply acoustic;pairwise similarities models;acoustic;similarities models;attention;similarities models recently;discrete sequences", "pdf_keywords": "attention acoustic modeling;modeling acoustic sequences;acoustic sequences highly;acoustic sequences;self attention acoustic;algorithm acoustic sequences;acoustic sequences introduce;attention acoustic;acoustic modeling self;acoustic sequences use;states acoustic sequences;acoustic sequences atypical;attention applied acoustic;acoustic speech corpus;speech large corpus;acoustic speech;acoustic modeling computational;modeling self attentionwe;acoustic modeling;speech corpus;modeling acoustic;network speech;acoustic modeling acoustic;sequences use spherical;tool acoustic sequences;speech recognition;acoustic speech large;encoding sequences vectors;algorithm acoustic;scale acoustic speech"}, "7ac4227d0b4d38b16da27ed55bd53ce240a32404": {"ta_keywords": "autoregressive nar models;ar models;ar models methods;autoregressive baselines;non autoregressive nar;autoregressive nar;compared autoregressive baselines;gap ar models;non autoregressive;nonar models;autoregressive baselines showing;autoregressive;nonar models explored;compared autoregressive;nar models simultaneously;performance gap ar;background non autoregressive;drop compared autoregressive;nar models;models simultaneously generate;number nonar models;reduces inference speed;inference speed;ar;models simultaneously;inference speed cost;potential real time;real time applications;real time;nonar", "pdf_keywords": "autoregressive speech recognition;autoregressive language recognition;neural automatic automatic;temporal classification cc;neural automatic;nonautoregressive speech recognition;masked language modeling;connectionist temporal classification;autoregressive language;end neural automatic;non autoregressive language;length speech processing;output decoder trained;decoder trained;non autoregressive speech;temporal classification;speech processing;speech recognition;autoregressive speech;application speech recognition;speech recognition 2020;approach speech recognition;language modeling;decoder based;processed improve decoder;classification cc based;speech recognition form;autoregressive baselines;token decoder trained;development nonautoregressive speech"}, "2f153172b92ea32f242d9cb6b94d162e52ef5f0b": {"ta_keywords": "dual daf learning;daf learning problem;daf learning;dual dual daf;dual daf;alphabet examples dafs;examples dafs string;dafs string;daf paclearning problem;dual version daf;learning problem extended;version daf paclearning;dafs string represents;daf paclearning;examples dafs;problem hard learning;learning order representations;learning problem;hard learning log;hard learning;learning order;learning log depth;dafs;represents set dafs;paclearning problem concepts;set dafs;learning log;daf;hardness results programming;problem concepts strings", "pdf_keywords": ""}, "4a348e4725a2bc677e4aa40aa63c1421e8f335c9": {"ta_keywords": "mean precision recall;multilabel classification methodsfor;multilabel classification;used multilabel classification;classifier;classifier class rare;success binary classifier;precision recall;binary classifier;achievablef1 score decision;classification;classification methodsfor classifier;classification methodsfor;classifier class;best achievablef1 score;binary classifier class;scores used multilabel;recall 91 score;classifier produces real;precision recall 91;methodsfor classifier;achievablef1 score;mean precision;classifier produces;harmonic mean precision;score decision;decision making threshold;score decision making;multilabel;score widely", "pdf_keywords": "predicting rare labels;classification optimal thresholding;optimal threshold predictions;classifiers maximize probability;multilabel classification optimal;optimal multilabel classification;score thresholds optimal;optimal threshold macro;classifier thresholding present;features rare labels;classifiers optimal decision;classifier thresholding;mean precision recall;classification optimal;uninformative classifier thresholding;recall withf1 score;achievable score thresholds;threshold predictions based;classifiers optimal;multilabel classification classifier;threshold predictions;thresholded random guess;probabilistic classifiers optimal;optimal thresholds;result optimal threshold;multilabel classification;classifiers maximize;rare labels frequently;optimal thresholds sufficient;multilabel classification demonstrate"}, "c2a79e2a65b721d4de5f6d4806323174b9f8f393": {"ta_keywords": "zero label learning;pretrained language models;human annotated data;explore zero label;annotated data;label learning natural;zero label;unsupervised data generation;annotated data used;human annotated;pretrained language;learning natural language;training data creation;nq human annotated;named unsupervised data;training data;powerful pretrained language;label learning;trained purely synthetic;language models specifically;models trained purely;learning natural;present training data;leveraging powerful pretrained;data used training;language models;natural language processing;annotated;purely synthetic data;natural language", "pdf_keywords": "augmentation language models;pretrained language models;predicting label language;zero label learning;learning augmentation language;generating predicting label;learning language models;processing human annotated;strong supervised baselines;human annotated data;label learning natural;augmentation text classification;natural language models;learning natural language;language model generate;language models unsupervised;human annotated label;supervised baselines achieves;supervised baselines;human annotated;annotated data;shot learning;labeled data inference;language learning;results human annotated;language learning introduce;shot learning results;powerful pretrained language;augmentation language;learning results text"}, "3b00e642de51d0f8378c7c35eca89f2ecb6f3af8": {"ta_keywords": "21 microdeletion syndrome;microdeletion syndrome region;velocardiofacial microdeletion region;microdeletion syndrome;velocardiofacial microdeletion;digeorge velocardiofacial microdeletion;22q11 21 microdeletion;microdeletion region duplications;congenital anomalies deletions;distal 22q11 21;distal 22q11;22q11 region distal;region distal 22q11;21 microdeletion;repeats lrs located;syndrome region recently;retardation congenital anomalies;located distal 22q11;microdeletion region;anomalies deletions mediated;21 digeorge velocardiofacial;syndrome region;duplications predicted occur;region duplications predicted;backgrounddeletions 22q11 region;mental retardation congenital;microdeletion;copy repeats lrs;repeats lrs;backgrounddeletions 22q11", "pdf_keywords": ""}, "7a6c61b57bac074f7cd85963fd13da8f3321e087": {"ta_keywords": "latent dirichlet allocation;unsupervised topic discovery;specific influence blogs;topic discovery;topic discovery estimation;influential blog postings;influence blogs;blog postings topic;influence blogs methodswe;highly influential blog;influential blog;blogs methodswe propose;dirichlet allocation;topic specific influence;blogs;citation hypererlink development;unsupervised topic;latent dirichlet;called latent dirichlet;blog postings;citation importance citation;importance citation hypererlink;citation importance;blog;citation hypererlink;blogs methodswe;citation process;importance citation process;problems unsupervised topic;topic user resultswe", "pdf_keywords": ""}, "8c25e1c223fc70509172a32111c91fe4b9f86a56": {"ta_keywords": "traditional internet architecture;internet architecture;internet architecture needs;original internet design;internet design;emphasized decentralized network;decentralized network operations;internet design emphasized;architectural ossification;architectural ossification lack;perceived architectural ossification;decentralized network;network operations colocated;design emphasized decentralized;architecture needs evolved;consensus traditional internet;decisions original internet;architecture;architectural;network operations;traditional internet;architecture needs;perceived architectural;program network situation;original internet;program network;decentralized;network;emphasized decentralized;colocated data control", "pdf_keywords": "software defined networking;programmable wireless networking;defined networking virtualizable;networking virtualizable wireless;software defined wireless;software defined network;programmable wireless networks;defined wireless networks;networking development programmable;software defined networks;defined networking;network software defined;networking resulting programmable;defined networking sn;virtualizable wireless networks;applications wireless networking;networks software defined;networking virtualizable;systemprogrammable wireless networks;programmable wireless architecture;programmable networks increasingly;architecture active networking;network systemprogrammable wireless;architecture wireless networking;network architectures wireless;development programmable networks;network virtualization opensig;defined networks increasingly;virtualizable wireless;programmable networks fully"}, "4a36a00db217fd98f1bd943aa2f2d6303adbc456": {"ta_keywords": "fair principal component;defines fair principal;principal component analysis;based fair principal;pca optimization;pca optimization staiefel;analysis pca optimization;component analysis pca;formulation fairness good;formulation fairness;mathematical formulation fairness;efefficient multidimensional;analysis pca;pca;defines fair;principal component;fairness good statistical;paper defines fair;md based fair;component analysis;dimensionality reduced conditional;component analysis minimizing;backgroundfast efefficient multidimensional;efefficient multidimensional md;mean discrepancy dimensionality;fair principal;fairness;fairness good;discrepancy dimensionality reduced;dimensionality reduced", "pdf_keywords": "fairness supervised learning;fairness supervised;definitions fairness supervised;fairness context supervised;learning fair representations;fair representation optimal;fairness optimization formulation;fair principal component;fairness optimization;mathematical formulation fairness;formulation fairness;fairness present constrained;demonstrate fairness constraint;definition fairness optimization;formulation fairness good;fairness constraint;method fair principal;fairness constraint good;define fair principal;fairness considered projected;fairness general generalized;fair pca subject;fair representations variety;fair representations;canonical notion fairness;definition fairness general;result fair representation;definition fairness;definitions fairness;fairness good statistical"}, "101d619b5911e9c2fda6f02365c593ae61617cb6": {"ta_keywords": "cooperative persuasive dialogue;persuasive dialogue policies;persuasive dialogue humans;persuasive dialogue based;persuasive dialogues human;learning cooperative persuasive;tailored persuasive dialogue;persuasive dialogues;persuasive dialogue;common persuasive dialogue;corpus persuasive dialogues;dialogue policies using;dialogue policies;dialogue humans methodsin;dialogue humans;dialogues human interlocutors;cooperative persuasive;reinforcement learning automatically;based corpus persuasive;dialogue based;dialogues human;dialogue based corpus;dialogue;automatically learning cooperative;corpus persuasive;user simulators reward;policies using framing;dialogues;specifically tailored persuasive;evaluate learned policy", "pdf_keywords": ""}, "d4762619b55c65120307ceebe4a0646984f6045a": {"ta_keywords": "statistical machine translation;machine translation;readable transcripts approach;machine translation smt;readable transcripts;create readable transcripts;introductionautomated speech recognition;speech recognition ar;transcripts approach;translate anr results;transcript style text;speech recognition;transcript style;transcripts approach statistical;transcripts;anr results transcript;results transcript style;smt translate anr;transcript;introductionautomated speech;translate anr;results transcript;finite state trans;translation smt translate;translation smt;state trans;speech;statistical machine;weighted finite state;smt translate", "pdf_keywords": ""}, "50ec3d960ac458573a1e4a1556420c5e96d58609": {"ta_keywords": "answering evidence annotation;question answering evidence;evidence annotations training;evidence annotation open;answering evidence;evidence annotation;evidence annotations;intermediate evidence annotations;training intermediate annotations;distantly supervised evidence;evidence retrieval enables;annotations training intermediate;supervised evidence retrieval;domain question answering;question answering;evidence retrieval;annotations training;evidence retrieved;intermediate annotations expensive;context distantly supervised;question answering answers;supervised evidence;intermediate annotations;retrieval enables question;enables question answering;based evidence retrieved;evidence retrieved large;annotation open domain;annotations expensive methods;question based evidence", "pdf_keywords": ""}, "d59c7b1c85f8c459863762361f251575785347a8": {"ta_keywords": "porous membranes;regularly porous membranes;introduction membraneanes;membrane isoporous methods;cylindrical pores collision;membraneanes compelling devices;introduction membraneanes compelling;porous membranes consists;moving cylindrical pores;pores collision;membraneanes;membranes consists hard;membrane isoporous;cylindrical pores;ideally membrane isoporous;membranes;pore size distribution;model regularly porous;separation processes;intrinsic permeability;intrinsic permeability selectivity;membraneanes compelling;permeability selectivity;reduce pore size;distribution ideally membrane;industrial separation processes;membranes consists;membrane;separation processes subject;ideally membrane", "pdf_keywords": ""}, "35750f1908f405bb38b0708972f33fe07b378b64": {"ta_keywords": "provability logic modal;provability logic provability;provability logic;logic provability logic;provability logic arithmetically;logic provability;propositional logic provability;provability logic simple;evaluation provability logic;interpretation provability theory;provability theory;provability theory simple;provability;interpretation provability;method interpretation provability;evaluation provability;modal propositional logic;logic modal;logic modal propositional;method evaluation provability;logic arithmetically incomplete;propositional logic;logic arithmetically;modal propositional;logic simple useful;logic;propositional;logic simple;arithmetically incomplete;modal", "pdf_keywords": ""}, "90766546b29836eb96f54fe8fd70ec51a3e699ba": {"ta_keywords": "3d structures proteins;structures proteins complexes;structures proteins;predicted native structures;predicted structures;accuracy predicted structures;proteins complexes;predicted structures tends;proteins complexes despite;computational prediction dimensional;prediction dimensional 3d;proteins;dimensional 3d structures;computational prediction;evolutionary information;prediction dimensional;evolutionary information recently;progress computational prediction;tremendous progress computational;use evolutionary information;3d structures;progress computational;computational models depends;complexes despite progress;structures tends vary;computational models;structures;complexes;native structures;dimensional 3d", "pdf_keywords": "tessellations protein structures;3d tessellations protein;cnn protein structures;tessellations protein;predicting protein structures;predict protein structures;protein structure prediction;protein structures trained;predict protein structure;tessellation graph representation;graph protein model;3d structures proteins;network protein structures;model protein structures;3d tessellation graph;protein structures accuracy;graph protein;protein structure euclidean;protein structure euclideanthe;tessellation graph;protein structures using;protein structures;model protein;cnn protein;protein models;protein models suited;graph convolutional networks;bioinformatics protein structure;given graph protein;protein models developed"}, "7181a5139301c8a407da75a105dd457bf03d7057": {"ta_keywords": "stochastic network optimization;emphmarkovian network optimization;network optimization primal;generalize network optimization;network optimization;network optimization problems;network optimization accommodate;flows heterogeneous planning;stochastic extension wadrop;wadrop equilibrium principle;stochastic network;wadrop equilibrium;provide dynamic stochastic;principle generalize network;dynamic stochastic;emphmarkovian network;heterogeneous planning time;generalize network;heterogeneous planning;class stochastic network;termed emphmarkovian network;extension wadrop equilibrium;multi commodity flows;optimization primal dual;flow multi commodity;optimization primal;stochastic;flows heterogeneous;dynamic stochastic extension;optimization problems", "pdf_keywords": ""}, "af553d6121d338fc74dbd5faa43d5383a222198d": {"ta_keywords": "communication skills autism;skills autism spectrum;skills autism;skills communication difficulties;social skills communication;autism spectrum;social communication skills;verbal communication skills;communication skills use;communication skills;autism;skills communication;social skills;autism spectrum quotient;people social skills;communication difficulties;non verbal communication;improve social communication;communication difficulties greater;difficulties improve social;verbal communication;social communication;people difficulties improve;enable people difficulties;skills use;improve social;communication;difficulties enable people;skills;people difficulties", "pdf_keywords": ""}, "a8315b5d3ff1b834fb58420397b13b9d169efad1": {"ta_keywords": "names publication;multiple publication attributes;publication attributes metadata;distinguishing similar entities;distinguishing various entities;publication attributes;propose disambiguation mechanism;author names publication;names publication venues;paper propose disambiguation;disambiguation mechanism;versa attributes metadata;similar entities;metadata including author;attributes metadata;disambiguation mechanism uses;publication venues titles;attributes metadata including;metadata;propose disambiguation;disambiguation;including author names;uses multiple publication;metadata including;author names;similar entities contingent;various entities grouping;various entities;distinguishing similar;multiple publication", "pdf_keywords": ""}, "83ddc47f6dd0434c12eff9e4e42b727217a200a8": {"ta_keywords": "multi agent learning;agents learn deterministic;nash equilibrium methods;agent learning;agent learning algorithms;local nash equilibrium;stable local nash;agents learn;games agents learn;unbiased estimator gradient;continuous games agents;algorithms non cooperative;learning algorithms non;gradient ii stochastic;nash equilibrium;based multi agent;multi agent;learn deterministic;estimator gradient;local convergence guarantees;continuous games;local nash;guarantees neighborhood stable;learning algorithms;equilibrium methods;convergence guarantees neighborhood;stochastic settings unbiased;consider continuous games;convergence guarantees;estimator gradient result", "pdf_keywords": "agents learning rates;agents learning equilibrium;agents learn deterministic;algorithmic policy gradients;multi agent learning;learning rates agents;agents learning convergent;continuous game learning;learning convergent equilibria;agents learning;learning behavior autonomous;learning dynamics;agent learning framework;agent learning;non uniform learning;scaling agents learning;learning equilibrium;autonomous agents stochastic;dynamics agents learning;convergence policy gradients;autonomous agents learning;policy gradients;uniform learning rates;optimization multi agent;agent learning algorithms;agents stochastic setting;agents choose learn;rates autonomous agents;learning player continuous;uniform learning"}, "48ea80b65f42e9fbb96b856286d12347d1df52d2": {"ta_keywords": "tiered reasoning intui;language understanding reasoning;language understanding tasks;introduce tiered reasoning;tiered reasoning;language understanding;pre trained language;reasoning intui;trained language models;evaluating underlying reasoning;ability language understanding;underlying reasoning process;language models;reasoning process;trained language;breadth language understanding;understanding reasoning;language models ls;reasoning process addition;performance breadth language;underlying reasoning;scale pre trained;reasoning;true ability language;based end task;understanding tasks evaluations;ability language;reasoning paper highlight;understanding tasks;breadth language", "pdf_keywords": "commonsense reasoning benchmark;commonsense reasoning dataset;commonsense reasoning provides;commonsense reasoning based;reasoning dataset benchmark;commonsense reasoning;physical commonsense reasoning;reasoning benchmark;reasoning dataset;human interpretable reasoning;commonsense reasoning posing;language understanding reasoning;reasoning benchmark physical;language understanding tasks;reasoning end task;reasoning provides traces;reasoning process supported;learning reason;commonsense language understanding;benchmark language understanding;evaluating underlying reasoning;reasoning powered;reasoning based;natural language inference;context language;underlying reasoning process;reasoning story provide;proposed reasoning powered;reasoning provides;interpretable reasoning process"}, "fa2657c0d66f048dee6b080536abbd1f947e822f": {"ta_keywords": "brain age estimation;estimate brain age;model brain age;population deep learning;brain age based;brain age;age estimation availability;age estimation;train deep learning;deep learning structural;mris healthy population;estimate brain;brain mris healthy;learning structural mri;age distribution;age distribution adult;deep learning;healthy population deep;structural brain mris;population deep;brain mris;objectiveto estimate brain;uniform age distribution;age based healthy;deep learning model;large scale dataset;sources train deep;age based;mris healthy;enables uniform age", "pdf_keywords": "cnns age estimation;predict brain age;age using deep;brain age estimation;neural age age;neural age;neuroimaging deep convolutional;cnns age;2d cnns age;estimate brain age;model brain age;deep learning brain;divergence estimated age;neuroimaging examine aging;age estimation regression;predict age fundamental;age prediction functional;predict age;agethe divergence estimated;dataset estimated age;age estimation framework;deep convolutional neural;age age prediction;age prediction;mri data neural;age predicted normal;age predicted;implications age prediction;age estimation demonstrate;brain age"}, "eb1b89751cac821792df36d3a1a2fb01dc4db2d1": {"ta_keywords": "symptomatic asymptomatic asymptomatic;asymptomatic asymptomatic asymptomatic;symptomatic asymptomatic;asymptomatic asymptomatic;asymptomatic;history symptomatic asymptomatic;symptomatic;patients history symptomatic;history symptomatic;patients;patients history;management patients;management patients history;approach management patients;approach management;article;approach;new approach management;new approach;history;new;purpose article;management;importance new approach;article discuss importance;article discuss;importance new;purpose article discuss;purpose;discuss importance new", "pdf_keywords": ""}, "120839995e64f8ed734b5249ab681328c4955f5d": {"ta_keywords": "multidimensional congestion games;toll design problem;strategy formulate toll;tolls multidimensional congestion;consider toll design;toll design;congestion games methodswe;constraint satisfactionisfaction tolls;toll synthesis problem;congestion games;consider toll;multidimensional congestion;tolls multidimensional;congested stochastic network;toll;congestion;toll synthesis;methodswe consider toll;formulate toll;tolls;designer congested stochastic;constraint satisfactionisfaction;game designer congested;network decision makers;satisfactionisfaction tolls multidimensional;introductiononline constraint satisfactionisfaction;optimal strategy formulate;stochastic network decision;decision makers adaptively;satisfactionisfaction tolls", "pdf_keywords": ""}, "4d16a47fb6708704b155855045c9e5d2ea380bb0": {"ta_keywords": "sentiment analysis czech;senstiment analysis czech;czech social media;analysis czech social;sentiment analysis;methods sentiment analysis;learning methods sentiment;czech social;social media english;analysis czech;czech language;senstiment analysis;czech language systematical;introduction senstiment analysis;case czech language;methods sentiment;czech;case czech;machine learning methods;machine learning;machine learning aim;sentiment;senstiment;supervised machine learning;social media;research machine learning;social media using;introduction senstiment;available case czech;supervised machine", "pdf_keywords": ""}, "7212cca9be971997434c2b3a27411a163bbd89c3": {"ta_keywords": "cardiology ct inference;conditioned ct probabilistic;new conditioning methods;self conditioned ct;conditioning methods;conditioning methods based;ct probabilistic model;conditioned ct;tractable conditioning framework;pass conditioning methods;ct inference;improved ct inference;intermediate prediction latent;conditioning framework;conditioning framework propose;ct probabilistic;ct inference paper;conditioning methods paper;multi pass conditioning;intermediates improve cardiology;tractable conditioning;ct inference searched;provides tractable conditioning;cardiology ct;propose new conditioning;cardiology;conditioned;model intermediate prediction;improve cardiology ct;pass conditioning", "pdf_keywords": "connectionist temporal classification;nonautoregressive speech recognition;conditioning audio encoder;autoregressive automatic speech;improved connectionist temporal;audio encoders conditioning;encoder intermediate predictions;encoders conditioning audio;selfconditioned audio encoders;audio encoder intermediate;automatic speech recognition;conditioning audio;speech recognition increasingly;temporal classification cc;speech recognition;audio encoders ctc;automatic speech;method automatic speech;speech recognition models;selfconditioned audio;method selfconditioned audio;audio encoder;speech recognition context;encoders conditioning;temporal classification;development nonautoregressive speech;approach speech recognition;audio encoders;improved inference conditioning;speech recognition model"}, "6bb2b856d9a9b873259ba9dc48bc450c96eb3318": {"ta_keywords": "malignant neoplasm gastrointestinal;neoplasm gastrointestinal;neoplasm gastrointestinal tract;gastrointestinal tract malignant;tract malignant neoplasm;malignant neoplasm;diagnosed malignant neoplasm;tract malignant;diagnosed malignant;patient diagnosed malignant;malignant;neoplasm;gastrointestinal tract patient;gastrointestinal;case patient diagnosed;gastrointestinal tract;patient diagnosed;diagnosed;present case patient;tract patient;tract patient treated;patient treated combination;case patient;patient treated;tract;article present case;patient;combination;treated combination;combination combination", "pdf_keywords": "annotated speech transcript;annotate annotated transcripts;annotated transcripts method;automatic speech transcript;annotated transcripts;processing annotated speech;speech transcript cost;annotated speech;speech transcription;study speech transcription;manual transcription costly;cost sensitive annotation;correction speech transcripts;speech transcripts;transcript cost sensitive;automated automated transcription;speech transcript;automated transcription;automated transcription methods;supervised language annotation;transcription costly;transcript cost;automatic speech;cost sensitive annotator;annotator cost models;transcription speech;annotator challenging cost;method transcription speech;speech transcripts simulated;supervising method transcription"}, "79b8ef3905a42b771248719495a2117271906445": {"ta_keywords": "neural architecture search;estimation energy cost;costs estimation energy;search evolved transformer;evolved transformer;machine learning grown;neural architecture;energy cost;architecture search evolved;energy use carbon;estimates neural architecture;architecture search;energy cost helps;estimation energy;demand machine learning;energy use;carbon footprint recent;earlier estimates neural;carbon footprint;estimates neural;calculate energy use;neural;strategies calculate energy;energy;use carbon footprint;computation demand machine;computation demand;search evolved;machine learning;greener strategies calculate", "pdf_keywords": "energy optimized cloud;efficiency use cloud;training carbon footprint;carbon footprint training;free energy cloud;energy cloud;energy cloud companies;energy consumption carbon;energy consumption training;carbon footprint computationally;footprint training energy;optimized cloud;cloud companies efficient;reduce compute cost;costs training carbon;estimation energy cost;costs estimation energy;datacenters cost training;compute cost carbon;increasing use cloud;datacenters propose carbon;energy consumption;energy cost;cost energy;energy global datacenters;energy use carbon;carbon impact computing;cloud cloud;improve energy efficiency;improvements cost energy"}, "1c709eef701d933af1383c790c13209f06806b60": {"ta_keywords": "sequenceential predictions sequenceence;predictions sequenceence models;predictions sequenceence;sequenceential predictions;sequenceence models critical;introductionrationales sequenceential predictions;sequenceence models;predictions sequential rationales;model predictions sequential;model explanations rationales;consider model explanations;model explanations;predictions sequential;nonlack systems predictions;sequenceence;model predictions;sequenceential;sequential rationales;systems predictions difficult;models critical;introductionrationales sequenceential;explanations rationales subsets;individual model predictions;predictions difficult explain;systems predictions;model predictions consider;explanations rationales;sequential rationales solving;models;sequential", "pdf_keywords": "rationalizing sequence predictions;sequence model greedy;corpus greedy rationales;problem sequence models;sequence predictions;learning sequence model;predict language models;dataset sequence rationales;greedy rationalization best;learning sequence;machine translation rationales;model greedy rationalization;lambada corpus greedy;sequence models critical;sequence models;algorithm greedy rationalization;algorithmwe greedy rationalization;greedy rationalization efficient;greedy rationalization methods;greedy rationalization greedy;rationalization greedy;greedy rationalization;sequence models present;dataset annotated rationales;translation rationales greedy;method learning sequence;predict language;rationales greedy algorithm;language models machine;sequence predictions exact"}, "0de86afbf91d0cf3e595a23a5b7a4d19deefb891": {"ta_keywords": "introductionparameter inference bifurcation;inference bifurcation diagrams;inference bifurcation;bifurcation diagrams;bifurcation diagrams important;bifuration diagram methodsin;bifurcations;known bifurcations;known bifurcations lie;bifurcation;bifurcations lie;bifuration diagram;points known bifurcations;called bifuration diagram;bifuration;condition called bifuration;called bifuration;dynamical systems;quantitative time series;bifurcations lie function;dynamical systems theory;condition dynamical systems;models quantitative time;time series;time series data;controlled condition dynamical;introductionparameter inference;theory change points;condition dynamical;dynamical", "pdf_keywords": "models desired bifurcation;model bifurcations;bifurcation inference complex;bifurcation inference;bifurcation models;model bifurcations match;bifurcation diagrams cost;bifurcation models measure;desired bifurcation diagrams;dynamics bifurcation models;minimal model bifurcations;predicted bifurcations;bifurcation diagrams specified;diagrams specified bifurcation;bifurcation diagrams;method bifurcation inference;bifurcation diagram cost;bifurcation points propose;bifurcation model;targets bifurcation;locate desired bifurcation;bifurcation measure gradients;tools assessing bifurcation;bifurcating parameter regimes;assessing bifurcation dynamics;bifurcation analysis;specified bifurcation diagram;number predicted bifurcations;bifurcation dynamics bifurcation;bifurcations"}, "4f02d8775123624088a91fcfff20625463e5239a": {"ta_keywords": "learning analytics collaborative;introduction learning analytics;analytics collaborative filtering;learning analytics;collaborative filtering regression;filtering regression experts;analytics collaborative;collaborative filtering;regression experts;analytics;regression experts aahhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh;filtering regression;experts aahhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh;experts;regression;introduction learning;collaborative;learning;filtering;introduction;aahhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh", "pdf_keywords": ""}, "614dc4001ad68cac31484887f16542f04693eca4": {"ta_keywords": "lobbying probabilistic;lobbying probabilistic environment;model lobbying probabilistic;formal model lobbying;lobbybying uncertain world;complexity lobbybying uncertain;model lobbying;complexity lobbybying;lobbying;lobbybying uncertain;introductionthe complexity lobbybying;influence voter preferences;probability vote desired;voter preferences;voter preferences voting;influence voter;voter preferences represented;issues voter preferences;lobbybying;actor influence voter;voting multiple issues;preferences voting;probability vote;probabilistic;multiple issues voter;probabilistic environment;vote desired issue;preferences voting multiple;voting multiple;issues voter", "pdf_keywords": ""}, "3261728694c0a53a2e8f95326f94147a28e03a83": {"ta_keywords": "deep quantized training;sinareq deep quantized;learn layer quantization;deep quantized;quantized training learn;quantized training;layer quantization;layer quantization bitwidth;quantization bitwidth scale;sinusoidal regularization;quantization bitwidth;quantization;weights values quantized;novel sinusoidal regularization;regularization called sinareq;values quantized;quantized;quantized levels;sinareq deep;values quantized levels;sinusoidal regularization called;called sinareq deep;learning period sinusoidal;convexity profile sinusoidal;learn layer;regularization;sinusoidal functions automatically;training learn layer;sinareq;profile sinusoidal functions", "pdf_keywords": "deep quantized training;neural network quantized;learn layer quantization;deep quantized;quantizes neural network;sinusoidal regularizer quantization;quantized training learn;waveq deep quantized;quantized training;quantizing weights activations;automatically quantizes neural;layer quantization;layers quantizing weights;quantizing weights gradient;learning multiple quantization;network quantized;regularizer quantization;layers quantizing;optimization sinusoidal regularization;quantizes neural;waveq deep;quantizing weights;bits deep neural;desired quantization levels;network periodic regularizer;layer quantization bitwidth;quantization apply regularizer;sinusoidal regularization;desired quantization;quantum neural networks"}, "80ef8b8a1284790e0d8f7cbf9727c9e0b2a89332": {"ta_keywords": "shift label marginal;correcting label shiftft;label shiftft;shift label;label shift;shift correct classifiers;label shift label;distribution shift training;detect quantify shift;label marginal changes;detecting correcting label;label shiftft black;quantify shift;shift training test;quantify shift correct;focus label shift;shift training;correcting label;shiftft black box;black box predictors;test set labels;labels;shift;label marginal;label;diagnosis diseases targets;distribution shift;shiftft;set labels;faced distribution shift", "pdf_keywords": "estimate label shift;estimation shifted label;label shift supervised;label shift distributions;label shift assumption;shift supervised learning;shifted label distribution;shift label marginal;shift supervised;label shift detection;distribution shift training;shift correct classifiers;estimate label;label shift using;confusion matrix estimated;method label shift;label shift correction;shifted label;demonstrate label shift;shift label;label shift;processing label shift;correction label shifts;black box predictor;correct label shift;label shift label;shiftft estimation;label marginal changes;label shifts;box shiftft estimation"}, "843966d4b567033abff9775c5958f7be4db5c0ad": {"ta_keywords": "disease past decade;disease diagnosed;diagnosed disease;patient diagnosed disease;diagnosed disease past;disease diagnosed disease;patient diagnosed;case patient diagnosed;diagnosed disease diagnosed;occurrence disease world;occurrence disease;disease;disease past;diagnosed;disease world challenging;disease world;challenging case patient;case patient;world challenging case;patient;occurrence;decade;challenging case;world challenging;past decade;world;challenging;case;past", "pdf_keywords": ""}}