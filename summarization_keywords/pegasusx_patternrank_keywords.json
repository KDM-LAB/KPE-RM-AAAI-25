{"9c61f5f51a232049635e6f3441e6397af4d91298": {"ta_keywords": "harmonic trap;dimensional harmonic oscillator;harmonic oscillator;dynamics", "pdf_keywords": ""}, "a05c3e8bd6dacbd192ffa28543e60e2c93c66d76": {"ta_keywords": "tweets;dynamics;analysis;graph;tool;new tool;users", "pdf_keywords": ""}, "bfa10ea6a4c9fa585f21f39858da517c31a76343": {"ta_keywords": "dialogue management;dialogue systems;system;decisions;knowledge;user;process;method;introduction;list;interests;needs", "pdf_keywords": ""}, "0bf2a0a3216c79b62b3664c596f44d7a8add498a": {"ta_keywords": "introductory computer science courses;curriculum;real science fiction reviews;subject;research topics;student;current knowledge;sections;contents;section;description;series;form", "pdf_keywords": ""}, "a4cb2a401c78bfafee69e823306b0cc9e4d673db": {"ta_keywords": "papers;submodularity;fair order;fair phase;robin mechanism;minimum possible number;mechanism;fair assignment;relaxation;robin;extension", "pdf_keywords": "fair reviewer assignments;reviewer assignment problem;reviewer round robin;reviewer assignment;uniform random assignment algorithms;fair allocation;fair allocation problem;peer review;robin allocations;submodular optimization;paper assignment problem;optimal assignment;standard fair allocation;optimal submodular functions;submodular maximization;optimal algorithms;comparable fairness metrics;reviewers;reviewer;median paper score;fair reviewer;algorithms;paper scores;mean paper score;many scientific journals;uniform random assignment;algorithm;papers;new algorithm;roundrobin allocationwe"}, "242c73ea34833910ad2643ec3a1096bb18c6d04d": {"ta_keywords": "speech extraction;recurrent network;rnn;training framework;novel training scheme;noisy background;background show;relative character rate;relative performance gain;performance;time;framework;best case;experiments", "pdf_keywords": "target speech extractionin;target speech extraction;target speech encoder;speaker recognition;target speech starget;speech recognition;enhanced speech;end speech recognition problem;speaker identification entropy;speech enhancement uncertainty measures;speech enhancement uncertainty;noisy speech mixture;speech separation;noisy speech;speaker;speaker identity;recurrent neural network;speaker identity uncertainty;etarget encoder;encoder;convolutional neural network;clean speech;joint decoding;timedomain;neural network;rnn;decoder module;decoder;train framework;cnn"}, "a4df5ff749d823905ff9c1a23b522d3f426a1bb6": {"ta_keywords": "similarity measure;random fields;personalized page;evaluation;computation;connection;set", "pdf_keywords": ""}, "f32108602fb0dbda29030cac780165a4b89048a3": {"ta_keywords": "tables tables tables;tables;comparison relations;novel method;form;method", "pdf_keywords": ""}, "04745fe1306d10c915d27a454c157c837dacefce": {"ta_keywords": "mixture;phase difference;complex mask;layer;discrete representations;discrete representation;target;methods;novel approach;source;similar type;new type;problem", "pdf_keywords": ""}, "20140fcf0bdd932c1886ff1c7674c23649b1e3b8": {"ta_keywords": "speech synthesis;parameter generation;iterative parameter generation method;rich context models;speech;spectral component;f0 components;f0 component;method;new approach;hmm;approach;paper", "pdf_keywords": ""}, "e32177e38060637ac8a2ebc9990d43d1ab8bdb8a": {"ta_keywords": "recommendation systems;social networks;similarity;communities;similarities;start problem;users;method;information;performance", "pdf_keywords": ""}, "6680b1e863c394f00307cb3818f7c7d75c9919aa": {"ta_keywords": "interference alignment;network;storage;code;noise;nonmulticast setting;data;failures;new approach;concept;approach", "pdf_keywords": ""}, "c096ec97ecc4f8325f6db7f32398445d6a39f959": {"ta_keywords": "multiple fairness metrics;fairness concerns;recommender systems;recommendation domains;performance evaluation;utility;performance;design;framework;mechanism", "pdf_keywords": "recommendation fairness;optimal fairness measures;multiple fairness metrics;multiple fairness criteria;optimal fairness;fairness criteria;multiple fairness concerns;fairness measures;multiple fairness;fairness measure;fairness concerns;multiple fairness dimensions;fair allocation;recommendation system;personalized recommendation system;fairness;recommendation recommendation recommendation systems;recommendation accuracy;collaborative filtering;recommendation domains;recommender systems;recommendation delivery;novel collaborative recommendation algorithm;recommendation history;recommendation lists;personalized diversity measure;recommendation list;optimal ranking function;recommendation result;optimal items"}, "27636090a87fab750fccff4c6ede161ab62bcab4": {"ta_keywords": "traffic traffic networks;periodic safety messages;visibility;data;bes;new mechanism;mechanism;document;article", "pdf_keywords": "channel congestion;intersecting beacons;network visibility;neighbor table;blind communications;network;communication method;road junctions;beacon;intersection vehicle;beacons;type beacon;neighbors;communication range;mobile vehicles;other mobile vehicle;data dissemination;vehicular ad;channel status;protocol;visibility;networks;mobile vehicle;new protocol;vehicle;oncoming vehiclesin;distance;periodic safety messages;collision;collision percentage"}, "c3fc0b1041dcdd5b47ffaa0d584e40aa841628bf": {"ta_keywords": "binary relational concepts;set expansion;entities;character;set;wrappers;seal;seeds;level;system;context;construction;technique;means;practice;performance", "pdf_keywords": ""}, "7f79ac114d30c2c7dae91075210fbfda90c9d76f": {"ta_keywords": "anonymous games;human players;external regret minimization;diplomacy;agent;step lookahead search;new agent;supervised learning;human data;press variant;popular website;rank", "pdf_keywords": "arbitrary game;press diplomacy game;strategy game;regret minimization;anonymous games;reinforcement learning method;anonymously game;human players;simple game;reinforcement learning algorithm;supervised agent;optimal strategy;agents;reinforcement learning approach;games;imitation;agent;competitive game;conventional search bot;global strategy;sum board game;human player;strategy;diplomacy;search bot;popular diplomacy website;poker game;tvc game;press diplomacy;game"}, "70dc18bb6607e408ec1cd3f71c0fdac3534c288d": {"ta_keywords": "accurate speech enhancement;term memory;novel framework;framework", "pdf_keywords": ""}, "6e07fb796c75cac6432cdf0c314b933d0f9f45e5": {"ta_keywords": "gene name recognition;tagger;training data;task;high accuracy;novel method;system;set;paper", "pdf_keywords": ""}, "24fcdaf969089e6a411f7cebc9274bbc53c25e42": {"ta_keywords": "unsupervised learning;noncausal features;underlying causal model;empirical evidence;data;models;efficacy;domain performance;novel analysis;noise;domain;hypothesis", "pdf_keywords": "natural language inference;causal features;unsupervised learning;domain models;causal inference;neural natural language processing;causal models;natural language processing;natural language;structural causal model;novel generative models;domain generalization;sentence encoders;nlp;supervised learning;causal model;domain accuracy;noncausal features;linguistic structure;sentimentflipping methods;structured models;semantics;classifiers;training data;inherent bias;anticausal structured model;supervised learning pipeline;bias;feedback;models"}, "1d5c07e7415a7e9be078717197ddf9f3c70a2875": {"ta_keywords": "clinical models;clinical notes;clinical model;training process;patients;model;data;methods;sensitive information;novel experimental approach;technique", "pdf_keywords": "sensitive information;privacy;sensitive data;potential privacy implications;electronic health records;patient information;patient records;clinical data;clinical information processing;predictive clinical tasks;deidentified data;deidentified clinical notes;training data;medical record;documents;novel memorization model;learned model;natural language models;patient information andwe;patient names;nondeidentified data;data access;models;bert pretraining strategy;available data;such models;risks;machine learning;epidemiological models;simple memorization model"}, "ca9047c78d48b606c4e4f0c456b1dda550de28b2": {"ta_keywords": "recurrent convolutions;novel deep learning model;challenging speech;memory;range memory;layer;linear state;space layers;trainable subset;space layer;state;structure;power;art approaches;approach", "pdf_keywords": "recurrent neural networks;recurrent models;recurrent model;deep nonlinear sequence model;rnns;deep learning models;memoryless sequence model;continuous time memory;time memorization models;convolutional state space;convolutional models;neural network representation;memoryless sequences;time memory model;time memory;time memorization;convolutional model;recurrent;rnn;principled memory representations;linear state layer;popular nonlinear sequence models;neural networks;linear recurrence;term memory;simple sequence model;linear recurrences;linear recurrence equations;modern time series models;discrete memory"}, "b2a090506264bc9706dc9bcc5d61b4965ae919e7": {"ta_keywords": "knowledge graph;probabilistic soft logic;uncertain extractions;identification graph;entities;synthetic data;facts;graph;novel approach;task;world set;approach;power;method", "pdf_keywords": ""}, "8ca5a1e6cec68ef515ac1eb28d069a23dc9c14df": {"ta_keywords": "clustering probability;clustering exponents;clustering;clustering clustering framework;data sets;subsets;large collection;data;probability;properties;method;novel method;terms;use", "pdf_keywords": ""}, "a75c2d26ca6a06cbee62a8d1dad5993356d96793": {"ta_keywords": "various ranking algorithms;set expansion;many seeds;seeds;iseal;expansion;system;method;performance;version;choice;user;manner;mixture", "pdf_keywords": ""}, "a2ce385fc8d5068e8c87ebe4699c8f9b295cad5e": {"ta_keywords": "linguistic representations;new languages;languages;subword units;phonemes;graphemes;entity;text;structure;power;approaches;methods;way", "pdf_keywords": "training word embeddings;multilingual embeddings;dimensional word embeddings;word representation;forward linguistic embedding;linguistic representations;continuous word representations;linguistic adaptation;multilingual dictionaries;morphological representations;phonological representation;entity recognition;phonemic representations;related language;embeddings;morphemes;phonemes;word2 classifying;nernst training corpus;graphemes;high resource languages;parallel corpora;high resource language;neural machine translation;text recognition;low resource translation model;rich languages;low resource languages;like morphological analyzer;forward linguistic model"}, "a0e92f6e9564b8c38b6649ae71b892ddfb988faa": {"ta_keywords": "semantic parsing;generative seq2seq models;generative seq2seq;semantic data;query representation;unstructured data;generative model;query;available database;novel method;method;application", "pdf_keywords": "generative semantic parser;semantic parser;semantic queries;semantic parsing;generative seq2seq model;exemplar retrieval;unstructured text exemplars;semantic template;parsers;novel parser guiding technique;exemplar seq2seq model;seq2seq generator;parser;related exemplars;semantic schema;corresponding exemplars;semantic labels;semantics;exemplars;retrieval augmentation;generic generative model;exemplar;queries;output parse;problematic queries;specific queries;unstructured text;retrieval index;retrievalwe present;computational linguistics"}, "a30d7a3aa5e50d0b7abb448b6692e419b84018b8": {"ta_keywords": "accurate prefix boosting;accurate prefix;sequences;beam search;partial correct sequence;correct sequence;character error rate;training objective;new training scheme;papb;training procedure;minimization;better score;token;score;hypotheses;other hypotheses;set", "pdf_keywords": "speech recognition;end speech recognition problem;character error rate estimation;automatic speech recognition;word level recognition performance;natural language processing;softmax margin loss function;word level rnnlm;decoder;decoding;character level rnnlm;softmax margin objective need;probabilistic binary networks;better recognition performance;probabilistic threshold;deep neural network;training objective;training scheme;input symbols;seq2seq models;parameterized loss objective;seq2seq model;minimum bayes risk;token error rate;beam search;new training strategywe;loss objective;text;subsequent prediction;only model predictions"}, "a41c81e5c3f86e18217069b94b44ceaf281e451c": {"ta_keywords": "simple heuristic placement policies;optimal policies;wireless network;placement;optimal values;sensor;location;decision process;network;policies;sensitivity results;various formulations;set;problem;results", "pdf_keywords": "relay placement;relay deployment process;relay deployment;optimal sequential decision problems;simple heuristic placement policies;optimal placement policies;cost network deployment;relay locations;wireless relay network;wireless networks;optimal decision;relay position;wireless sensor network;wireless network model;next relay node;relay;lv relay deployment;cost network requirement;optimal policies;optimal average cost minimization problem;deployment process;lv relay location;optimal power consumption;minimum mean distance problem;relays;optimal policy structure;sum outage probability;pure relay network;total hop cost;optimal threshold policy"}, "f20654f481843ec9eb11bcd00e418aec2470dfa5": {"ta_keywords": "storage codes;data centers;new design framework;codes;rebuilding;design framework;system considerations;data;settings;variety;meeting requirements;amount", "pdf_keywords": "grid storage codes;storage codes;arbitrary storage configurations;efficient codes;piggybacked codes;grid storagewe;explicit piggybacking codes;storage systems;grid storage;optimal storage;storagewe;symmetric code;explicit codes;piggybacked code;simple piggybacked code;storage;systematic node repair algorithm;code designs;antisymmetric code structures;standard model code;mesh;arbitrary data structures;systematic code;hypergraph;systematic nodes recovery;code;dm code;smallest data;simultaneous download;data structure"}, "6e05d35d072cd73fa039fd60696a8fe110f1d6cd": {"ta_keywords": "publication databases;publication database;reference list;invariant random walk model;recommendation;discovery;data representation;path;novel approach;interconnected problems;restart;novel;connections;classic approach;approach;combination", "pdf_keywords": ""}, "3c4dfc252c214d559fadb5e3159bcc9c7db08fbc": {"ta_keywords": "fluorosis;dehydration dynamics;ray imaging;teeth;tooth;dsdis;study;surface layers;results;technique;influence;presence;aim", "pdf_keywords": ""}, "294f8307f26eb3ec7bbf19f15092f3c473ece821": {"ta_keywords": "entity classification;entity classifier;imitation learning;standard entities;relation;novel method;method;points;quality;problem", "pdf_keywords": ""}, "6a2c4a0f04c6ba2f6fbc171dcea8730423a298e5": {"ta_keywords": "learned pruner;metric spaces;probe locality;performance;effective approach;new approach;parameter;approach;state;art approaches;art approach", "pdf_keywords": ""}, "20d4105b276da6d6d38ed3c1bfc436f76198c240": {"ta_keywords": "large event datasets;events;effect association;human raters;conditional intensity rates;pairwise associations;synthetic datasets;world dataset;pairs;ground truth;related algorithms;performance;scores;assessments;general framework;suite;cause", "pdf_keywords": ""}, "695d4c04f6e4f7ba5f771ac7853fdbaa81713ae8": {"ta_keywords": "social knowledge graphs;network embeddings;knowledge base;latent topics;embeddings;online academic search system;large unlabeled data;concepts;data;word;scale structure;novel method;scale;modalities;method", "pdf_keywords": "social knowledge graphs;social knowledge graph;word embeddings;network embeddings;knowledge modeling;user embeddings;embeddings;knowledge base;continuous embeddings;topic models;latent dirichlet allocation;social networks;continuous semantics;social network;knowledge;unlabeled corpus;model inference;knowledge concepts;entities;entity;novel bayesian inference method;keyword extraction;online search;inference;bayesian inference;online academic search system;generative model;multivariate data mining;bayesian inference framework;topic space"}, "3b8494614903dc47579da30477b21b109b29f8cd": {"ta_keywords": "effect", "pdf_keywords": ""}, "74c881830a9cd7ea49795faa5c582b7ec56bd0bf": {"ta_keywords": "noisy speech recognition task;automatic speech recognition;asr;dereverberation module;new subnetwork;architecture;field;module;performance;systems;nodes;end;new sequence;ability;report", "pdf_keywords": "entire multichannel speech enhancement;automatic speech recognition;reverberant data;beamforming;reference microphone;advanced signal processing;inelastic scattering;joint signal processing;minimum variance distortionless response;advanced advanced dereverberation;bounding sound;noise ratio;neural network architecture;multichannel end;different noisy input signals;neural extension;dereverberation components;magnitude spectrum;prediction error;noisy noisy signal;asr;s2sin;end architecture;model iswe;dss;sound;dereverberation;conventional pipeline;joint training;talk"}, "3d3b1300c7cd6a820a6d08605248f875a3ad20b9": {"ta_keywords": "global interpretability;interpretability;interpretability scores;local interpretability;path recall;models;possible rules;rules;benchmark;metrics;evaluation;rule;approximate strategy;framework;experiments;paper", "pdf_keywords": "rule interpretability score;annotated knowledge graphs;multihop reasoning models;interpretability evaluation;reasonable interpretability scores;interpretability evaluation results;annotated rules;interpretability score;global interpretability;knowledge graph;interpretability scores;knowledge graphs;preliminary knowledge graph;rule mining methods;interpretability evaluation experiment;reasoning model;interpretability;low interpretability scores;abstract interpretationability;path recall;local interpretability;interpretation score;annotated datasets;mined rules;manual annotation;annotation;generative annotations;path collection strategy;automatic generation;link prediction"}, "e0c54e18cf2372414042bf67eed0272b0a432190": {"ta_keywords": "propagation;zeeman field;particle;background medium;field;medium;effect;topic;main interest;intense research", "pdf_keywords": ""}, "7da967be8f6367f6174bf99d0d019ff545ac5966": {"ta_keywords": "semantic annotation;annotated data;lingvosemantic project;sentences;user questions;method;methodology;analysis;creation;purpose;fact;novel method;process;recommendations;set;paper;advantage", "pdf_keywords": ""}, "49db57f300b270f16cbcb1891ca39e16981d42b5": {"ta_keywords": "pandemic activity;public health data;covidcast;time indicators;open access repository;central repository;united states", "pdf_keywords": ""}, "bad416f073a08086ee428e5a264eac3a7d3251e5": {"ta_keywords": "point source;image;position;depth;method;shape;new method;analysis;use;variety;combination", "pdf_keywords": ""}, "d9dbdd254b02ef1af2769af403cba373c1b1bcb1": {"ta_keywords": "novel speaker diarization method;speaker diarization problem;end speaker diarization method;speaker representations;speaker;diarization errors;neural networks;permutation;clustering;free objective function;separate modules;extraction;method;label;end;paper", "pdf_keywords": "neural speaker diarization method;neural speaker diarization;efficient speaker diarization;novel speaker diarization method;speaker diarization method;speaker diarization;speaker diarization problem;speaker labels;speaker representation;label separation;robust speech recognition;speech mixtures;single neural network;neural network architecture;neural network;noisy speech;speech activity detection;speaker;advanced neural network architecture;noisy noisy speech;diarization;diarization errors;diarization error rate;segmentation;label permutation problem;minimal diarization errors;deep learning;speech;permutationinvariant training criterion;utterance"}, "5931c8ac145baf17cec9effc25c051049b7dfd4c": {"ta_keywords": "dialogue task;neural dialogue model;dialogue agent;observable reference game;board;relative improvement;task;people;positions;dots;shades;number;art;udagawa;eq;previous state;sizes", "pdf_keywords": "collaborative reference dialogue games;neural dialogue model;dialogue game;dialogue task;dialogue systems;human dialogue;human dialogues;expression dialogue tasks;structured referent grounding module;dialogue;dialogues;structured referent prediction module;referent prediction modules;observable reference game;structured referents;pragmatic generation;pragmatic generation procedure;utterances;referent selection subtasks;available visual reference games;human utterance;accurate utterances;collaborative games;centric pragmatic generation procedure;dialogue amplitudes;structured reference resolution model;reference resolution;referents;discourse;informative discussions"}, "6c4258f6a6a4bee7b9d914379c44aea6073cdc37": {"ta_keywords": "energy disaggregation problem;energy disaggregation;adaptive filtering;adaptive filtering problem;level disaggregation problem;new energy;algorithm;optimality;problem", "pdf_keywords": "energy disaggregation;energy disaggregation problem;power consumption data;power consumption patterns;aggregate power consumption patterns;online energy data;energy data;disaggregation;power consumption;intrusive load monitoring;total energy consumption;dynamic filtering algorithms;ground disaggregation;scalable scalable scalable consumer energy filter bank;energy harvesting system;adaptive filtering;energy harvesting systems;adaptive filtering algorithm;energy;filters;energy reservoir;effective filter bank;scalable consumer filter bank;heterogeneous energy harvesting;supervised models;supervised learning model;supervised learning setting;filter bank;system identification problem;learning model"}, "3dcf9c900f5f28e082a2fcdea4763b6063a76f09": {"ta_keywords": "defeasible reasoning;mental model;scenario;defeasible way;problem scenario;relevant influences;approach;person;question;system;new approach;fact;curious;graph;state", "pdf_keywords": "defeasible reasoning datasets;inference graph;defeasible inference query;defeasible inference;reasoning mechanism;optimal inference graph;defeasible reasoning;inference;natural language models;optimal inferencewe;taskmachine learning;graph representations;question elements;natural language;questions;hypothesis prediction;experts;graph representation;computational linguistics;annotators;defeasible learning problem;graphs;graph generator;neural models;neural architectures;cognitive science literature;task;knowledge;graph encoding;conjecture"}, "dbdb7f25f1538c2a2885d3992e5320e2ee5c23a1": {"ta_keywords": "classroom;classes;learning models;class;novel class;environments;use", "pdf_keywords": ""}, "660119405bb48777cd71d85caa5ec2e90a336caf": {"ta_keywords": "historical text normalization;neural encoder;different languages;dataset;literature;data;best method;distance metrics;comparative study;several methods;training process;intrinsic structure;performance;context;advantage;art;state", "pdf_keywords": "historical text normalization systems;historical text normalization;statistical machine translation;neural machine translation models;historical spelling normalization;neural machine translation;natural language processing;normalization task;normalization techniques;normalization;standard language model;information retrieval;word accuracy;nlp;normalisation;historical text collections;word classifier;historical word form;historical text;term memory units;text;linguistic version;original text;historical texts;linguistic structure;word form;neural network;annotation;similar data;supervised learning algorithm"}, "98554bd8a15172e9a6ef3cc3db3bc52504110fc9": {"ta_keywords": "stochastic bandits;optimal strategy;optimality;original strategy;variant;task", "pdf_keywords": "optimal stochastic bandits;stochastic bandits;stochastic bandit;stochastic bandit models;optimalwe study stochastic bandits;optimal stochastic reward;optimal item selection process;optimal items;adversarial bandits;optimal item;stochastic reward;optimal rewards;regret minimization;stochastic algorithms;optimal optimal optimal stopping time;optimal strategy;optimal stopping times;optimal optimality;optimal optimal optimality;optimality;optimal payoff;optimal arm identification;optimal performances;optimal performance;optimal object;optimal exploration;optimality conditions;item selection process;stochastic;optimal vaccine design"}, "6b3fa9157a8120a6eb86ae06a93611a1fcd9e219": {"ta_keywords": "particular soft database;hard database;local optimum;hardening;linear time algorithm;optimization problem;new approach;problem;approach", "pdf_keywords": ""}, "ef9ddbc35676ce8ffc2a8067044473727839dbac": {"ta_keywords": "natural language;matrix factorization problem;expressiveness;new method;art perplexities;method;effective method;state", "pdf_keywords": "neural language models;softmaxes;softmax;softmax bottleneck;dimensional softmax bottleneck;parametric language models;rank language models;dimensional softmax;softmax bottleneck problem;language modeling;rank language model;softmax function;treebank;natural language;log probability matrix;benchmark languages;languages;novel matrix factorization framework;dependent context;treesbank test;rank stochastic recurrent;possible contexts;context dependency;low rank models;treesbank test data;context;learning model;neural architecture;expressiveness;learning rate"}, "d29f155060f96becef0247ee77dc038f96b2d983": {"ta_keywords": "machine translation system;translation processing time;machine translation module;translation speed;translation unit;base translation;phrase table;phrase;accuracy;method;paper", "pdf_keywords": ""}, "d415b724fbc35afcc8dd91738123edfa6a5db634": {"ta_keywords": "deep policy gradient;trust region policy optimization;proximal policy optimization;level optimizations;algorithmic progress;cumulative reward;algorithm augmentations;such optimizations;popular algorithms;core algorithm;code;implementations;gain;auxiliary details;consequences;case study;tt;roots", "pdf_keywords": "deep policy gradient;deep policy gradient methods;policy gradient algorithms;prominent policy gradient algorithms;policy gradient applications;policy gradient methods;trust region policy optimization;standard policy gradient algorithms;policy optimization layer;disjoint policy gradient methods;policy optimizations;proximal policy optimization;policy optimization;deep neural network learning;learning rate optimizations;reward allocation;dimensional reinforcement learning;level optimizations;benchmark tasks;reward;dimensional reinforcementwe;optimizations;several fundamental optimizations;such optimizations;algorithm augmentations;false reward;reward management;algorithmic progress;trust region;optimizationwe"}, "3315dee45b1edb8f8286816629de7b8c31d270d6": {"ta_keywords": "search behavior;social cues;simple social signals;information search strategies;voting task;social environments;search;information;participants;neutral information environment;influence;different patterns;items;subjects;many other people;control group;changes", "pdf_keywords": ""}, "387754dc8d4185fadd7c3c15e43956a4d085e8fe": {"ta_keywords": "nearest neighbor search;benchmark search methods;benchmark datasets;permutation methods;distance;permutation;permutations;data;original points;machine;methods;available software;good proxy;literature;assumption", "pdf_keywords": "approximate knearest neighbor search;nearest neighbor;nearest neighbors;efficient approximate similarity search;search methods;closest neighbor;permutation search;search algorithm;scalable algorithm;network search algorithm;graph search algorithm;near points;data points;search;large data set;algorithm;large databases;distances;data sets;clustering;art benchmarks;candidate points;distance distributions;multidimensional matching matrix;similarity;metric structure;pivots;image data sets;permutation prefix index;metric"}, "60a121c55b5144bfe3aef5b6ea8959a9f6dd12ae": {"ta_keywords": "ensemble learning framework;speech enhancement;simple averaging;voting techniques;enhancement performance;several algorithms;improvement;combination;possibility;problem", "pdf_keywords": ""}, "ba56bb1eb67b188a89060058ef8ad02ce3c660ac": {"ta_keywords": "relativistic quantum mechanics;spin;interaction;dominant component;dynamics;framework", "pdf_keywords": ""}, "9fc33c53d1f59aa9fd7f1b642c3859900865b0e3": {"ta_keywords": "closed systems;same closed set;sets;set;new representation;representation;concept", "pdf_keywords": ""}, "d7729f2ff21f97d56d10c54adc1f1f5ffbec9e5c": {"ta_keywords": "oral premalignant lesions;lasers;lesions;different surgical techniques;treatment;scalpels;use;present study;tools;study;effective results", "pdf_keywords": ""}, "649eb9fd9a18f9601270b7fcde8d6548bfc6ec75": {"ta_keywords": "speech separation benchmark demonstrate;speech mixture;automatic speech recognition model;mixed speech;speaker;corresponding label sequences;2mix dataset;model;labels;end;state;performance;supervision;scratch;art;experiments", "pdf_keywords": "end speech recognition;end speech recognition model;automatic speech recognition model;speech recognition;multiple label sequences;reference label encoder;short term memory recurrent;parallel attention module;speaker;speech;encoder;decoder model;decoder network;parallel attention;label permutation problem;independent attention modules;decoder;new neural network architecture;novel encoder;decoder framework;attention;previous end;invariant training;layer lstm;permutation;neural networks;auxiliary information;joint attention;layer cnn;permutation problem"}, "5884948777dfc003ba49e1513420830616281839": {"ta_keywords": "multilingual representations;multilingual bert;representations;joint training;direct comparisons;alignment;training performance;novel framework;exclusive approaches;art results;methods;framework", "pdf_keywords": "unified multilingual representations;multilingual representations;unsupervised language model pretraining;bilingual word embeddings;multilingual word representation;monolingual representations;diverse crosslingual tasks;crosslingual nernst;multilingual corpora;thebilingual word embeddings;unsupervised machine translation;word embeddings;supervised alignment methods;crosslingual mappings;lingual tasks;multilingual word;novel linguistic inference;supervised alignment;word representations;monolingual invariance;computational linguistics tasks;embeddings;joint training methods;novel alignment;unsupervised joint training;essential linguistic structure;joint training method;linguistic information;computational linguistics;languages"}, "56bc2a1eebedab3e452a7ca3969aa1e4dd5946c3": {"ta_keywords": "influence maximization;influential nodes;node diversity;diverse ranking;nonmonotonic submodular optimization problem;optimization;original optimization problem;network;approaches;real data;im;framework;experimental results;problem;show", "pdf_keywords": ""}, "af0adbaa0c1ea6abaed4b3d21f1dc4121c35fb30": {"ta_keywords": "shot language coordination;language navigation task;language theory;referential game;better communication performance;brain;interlocutors;task;ability;instructions;population;fundamental problem", "pdf_keywords": "shot language coordination game;shot language coordination;shot language coordination task;better language coordination;simple referential game;language games;multilingual referential games;referential game;novel language navigation model;language navigation task;referential games;language navigation;mind modeling;better communication performance;language;level language comprehension;neural listeners;conversational partners;predictive agent;listener;speaker;linguistic structure;natural language;reinforcement learning;listeners;agent;shot brain model;novel listeners;next action;agents"}, "3c57a1aa483d8bffe1339914b80d2913f2dc8376": {"ta_keywords": "generative adversarial networks;gans;feature matching;multiple benchmark datasets;bad generator;art results;preferred generator;objective;novel formulation;state;definition;new formulation", "pdf_keywords": "generative models;adversarial feature matching approach;generative model;generative learning;vanilla feature matching;discriminator objective;generative adversarialwe;supervised learning task;feature matching;generative adversarial networks;discriminator;generative adversarial network;generative adversarial network approach;machine learning;tofeature matching;feature;image classification;generalization;features;conditional entropy;feature space;generalized models;conditional entropy term;generator entropy;synthetic data;generalized model;benchmark datasets;complement generator;several benchmark datasets;good generator"}, "866f231970f93f4a201febc2fb46aff06f501e4b": {"ta_keywords": "automatic normalization;historical language data;historical forms;rule entries;norma;new tool;modules;tool;form;set;way", "pdf_keywords": ""}, "4d96ec46cda5d3b223fc7d33a920ab85864ea36d": {"ta_keywords": "novel functional motifs;underlying protein sequence;copper oxygen reductase;functional motifs;neural network architecture;amino acid sequences;proteins;deep learning approach;model;features;architecture;functional role;accuracy", "pdf_keywords": ""}, "6c170fe3fec5a477c938d07fa00935bb6f7b87cc": {"ta_keywords": "speech parameters;individual speech vectors;converted speech;novel rich context model;conversion;conventional random matrix model;novel statistical sample;mixture;novel multivariate;polynomial form;model;features;quality;method;paper;approach;significant improvements;experimental results", "pdf_keywords": ""}, "adac290d72c86c186837a884aae922bee4dee684": {"ta_keywords": "more reading difficulty;human readers;error words;misspellings;unimpaired comprehension;eye;words;error rates;correct words;error types;errors;surprisal;tracking study;character;transpositions;traditional word;computational model;spite", "pdf_keywords": "reading behavior;more reading difficulty;human reading;human readers;language model;unimpaired comprehension;neural language model;word forms;error words;misspellings;vocabulary items;words;word length;letters transpositions;reading;new language model;texts;letters;correct words;text;fixation dynamics;error word;word;standard word;animal movement;animals;universal behavior;human mathematical physics;fixation rate;processing difficulty"}, "23918ed366c60ae0ef85b0c80def63127f035e02": {"ta_keywords": "noisy data;noise distributions;cloud storage;cloud access;many machine learning;lossy information;remote servers;reliable estimation;data;loss;security applications;robust mechanism;performance;common choices;key requirement;mechanism", "pdf_keywords": ""}, "e7e1f5a713d20cdf31e732022731fdf0d8fb4fc5": {"ta_keywords": "natural language inference;explanations;sentences;description;order;novel method;approach;box;use", "pdf_keywords": "neural entailment model;sentence classification;supervised description;entailment task;tokenlevel explanations;attention thresholding approach;attention;sentence labels;instance learning;natural language processing;attention matrix;entailment relation;natural language;level explanations;attention mechanism;explanations;hypothesis sentences;sentences;accurate explanations;semantics;neural architecture;recall;supervised environment;premise sentences;informative cues;neural network;neural network model;task;underlying hypotheses;sentence"}, "37ef7941909527aaf123d7b8f90adbf4606f4917": {"ta_keywords": "parallel implementation;quadratic learning algorithm;parallel description;novel unsupervised learning model;data structure;data;ihmm;implementation scales;nodes increases;version;number", "pdf_keywords": ""}, "58e5ce12c23f815e9b394220044eaf99b28cfffe": {"ta_keywords": "einstein condensate;repulsive interaction;dimensional schrdinger equation;component bose;dependent hamiltonian;condensate;dynamics;interaction;time;presence", "pdf_keywords": ""}, "cf2fcb73e2effff29ceb5a5b89bbca34d2d27c1a": {"ta_keywords": "deceptive attention mechanisms;attention masks;attention mechanism;feedback;predictions;model;consumers;dataset;claims;choices;adequacy;identity;simple method", "pdf_keywords": "adversarial attention;deceptive attention masks;attention models;attention mechanisms;adversarial models;attention;human annotators;attention distribution;impermissible words;natural language processing;attention mass;attention scores;neural architectures;training models;interpretability;synthetic tasks;lower attention mass;representations;manipulation;decoder;impermissible tasks;sequence tasks;models;partial classification task;manipulation scheme;semantic structure;supervised learning;output tokens;encoder;machine learning model"}, "79655bfc45039b4d7cfe6cc86d52a4ced492f43a": {"ta_keywords": "rank methods;rank formula;generative learning;learning;performance;experiments;handtuned formula", "pdf_keywords": ""}, "5c3cc301a892094d5bfca3c41a78a3a8ebd755f8": {"ta_keywords": "dropouts;multipletress trees;regression;other models;regression model;classification tasks;mart;novel method;data;significant margin;method;large set;level", "pdf_keywords": "multiple additive regression trees;random forest;classification tasks;feature dropouts;complete trees;classification;trees;ensemble;employing dropouts;ranking;rapid regression;available datasets;adaptive learning algorithms;ranking task;method outperforms;regression;advanced regression;dataset;outperforms;linear regression framework;rank analysis;algorithms;linear regressionin;training;models;multiple node models;quadratic loss function;quadratic loss;logistic loss function;novel method"}, "9eecfdb7c8ad9af4f3863e9f6ed857211fb710e7": {"ta_keywords": "novel explanation system;optimal decision making;explanation system;financial services;liniger strategy;lieb;specific explanations;independent techniques;domains;portability;domain;system;real time;combination", "pdf_keywords": ""}, "dfd8fc9966ca8ec5c8bdc2dfc94099285f0e07a9": {"ta_keywords": "same empirical privacy guarantee;privacy;privacy guarantees;private mechanisms;real text classification datasets;utility;text;constrained optimization problem;novel class;class;experiments;state;art", "pdf_keywords": "empirical privacy metric;empirical privacy;empirical privacy guarantee;same empirical privacy guarantee;privacy measure;privacy guarantees;privacy;privacy parameter;noisy nearest neighbor selection;nearest neighbor selection;privacy type;text generation mechanisms;nearest neighbor;text generation;dp text generation mechanisms;nearest neighbors;second nearest neighbor;random selection;noisy vocabulary;noised embedding;novel algorithm;redacted text;noisy context;candidate words;natural language processing;novel utility mechanism;adversary;empirical reconstruction risk;original text;deterministic algorithm"}, "e8c4a4e81084e17b0c71a6a69bdf1e4e2b6f6af1": {"ta_keywords": "speech separation;contextual output sequences;mixture sequence;output sequences;multiple sources;input;models;model;use", "pdf_keywords": "contextual output sequences;multiple output sequences;many sequence transduction problem;conditional signal transduction problems;novel conditional chain model;long sequence modeling;output sequences;speech separation tasks;conditional chain model;conditional chain;speech data;end speech recognition;speech separationwe;sequence model;end speech separation problem;conditional conditional seq2meq model;sequence mapping approach;speech separation;end speech recognitionspeech;th speech classification;transduction task;multiple sequences;input sequence;novel speech separation method;probabilistic chain rule;reference sequence;seq2meq separation;serial mapping;extra sequence;conditional model"}, "59121b847fd7eb4cf92cbfccb54f1705733d8b65": {"ta_keywords": "reverberation;reverberation effect;acoustic features;novel dereverberation preprocessor;dereverberation;dynamic mismatches;novel method;model;presence;method;novel;paper;factor;chapter;use", "pdf_keywords": ""}, "cec37cd54a940bec818db7216cc1086672f3fec0": {"ta_keywords": "concept inventory induction method;lexical substitutions;crowdsourcing;synsets;duplicate synsets;individual word senses;concepts;concept;open source;mobile application;oor approach;oor;novel approach;approach", "pdf_keywords": ""}, "fa10752ab1768d1633001420b48be5e2518a4f80": {"ta_keywords": "proton cross section;ion collision;relativistic mean field theory;proton;rhic;calculation;new method;method;function;combination", "pdf_keywords": ""}, "9fcfbc662d4095d72eb9a4e1c4f5ae8f0ffc4222": {"ta_keywords": "dehydration;optical coherence tomography;secondary lesions;thermal imaging;teeth;average lesion size;future studies;study;prevalence;results;measurements;rate;relationship", "pdf_keywords": ""}, "5801974fcebc11b4a8085fb02e77f792454caf7c": {"ta_keywords": "social skills training system;social skills training;computer interaction;training;accuracy;system;effect;means", "pdf_keywords": ""}, "20f166f7809d1af9065cd1c71ec1e38d5d92993f": {"ta_keywords": "deep reinforcement learning;intrinsic fear;learned reward;catastrophic states;imminent catastrophe;periodic catastrophes;supervised learning;policies;average return;model;guards;same average return;probability;objective;strong assumptions;approach;second model;weaker assumptions", "pdf_keywords": ""}, "c43d9d868f5288738cd625d365f0b3a5c18d4a20": {"ta_keywords": "professional simultaneous interpreters;simultaneous interpretation system;translation corpus;corpus;interpretation results;experience;nuclear science;performance;different levels;differences;information;different devices;cnrs;japan;system;different amounts", "pdf_keywords": ""}, "3be5e7310b1bec9b4431ad0f1264f536b6a39f14": {"ta_keywords": "machine translation models;translation quality;noisy translation;pivot languages;single pivot;multiple pivots;level models;human judgments;model;character;context;paper;use;choice", "pdf_keywords": "robust translation models;level machine translation models;translation models;statistical machine translation;machine translation prediction;translation quality;translation model;optimal translation;translation parameters;pivot translation method;joint translation;bulgarian english;synthetic data translation;untranslated words;movie subtitles;phrase filtering;translation;european language standard;standard european standard language;corpora;character alignment;languages;phrase table;level pivot word;pivot word;limited training data;complex languages;sparse training data;single characters;noisy datasets"}, "6e7e095f46deb297713dcde05991faf635768d29": {"ta_keywords": "algorithmic fairness research;critical race theory;race;ethnicity;sociological work;conceptualizations", "pdf_keywords": "algorithmic fairness research;algorithmic fairness;algorithmic unfairness;algorithmic fairness frameworks;algorithmic fairness methodologies;algorithmic fairness literature;algorithmic fairness analysis;algorithmic fairness problem;racial categories;critical race theorywe;racial classifications;racial disparities;race;different racial taxonomies;different races;human race;way race;ethnic disparities;eugenicist thought;algorithmic model;social diversity;racewe;quantitative social science;scientific objective;disparities;categories;objective differences;classifying;empirical equality measures;methodological perspective"}, "4d10d7c02ce01d71f11c296b09b389c6f20b354b": {"ta_keywords": "crowdsourcing marketplaces;efficient data labeling;real data labeling process;efficient aggregation;aggregation;data;dynamic pricing;incremental relabeling;projects;world tasks;feedback;applicability;tutorial;theoretical results;strengths;weaknesses;practical advice", "pdf_keywords": ""}, "191169031c7646c02ecb1aaa9c8a6b6e05009730": {"ta_keywords": "graphene oxide;hollow graphene spheres;new electromagnetic shielding film;graphene sheets;average shielding effectiveness;synthesized rgo;ng film;dielectric;rgo;catalyzed crystallization;thickness;film;metal;situ synthesis;reflection;db", "pdf_keywords": ""}, "19b6e7158ee4f13caa004a0b6c6a6e0ef965ea8f": {"ta_keywords": "robust automatic speech recognition;chime challenges;robust recognition;robust systems;everyday environments;challenges;tasks;task;systems;chapter;techniques;overview", "pdf_keywords": ""}, "53f6c82035d43a19b9c8be0de651cae25bdd4bda": {"ta_keywords": "finite state transducers;spoken word;automatic transformation;text;words;simple noisy channel model;disfluencies;substitutions;wfsts;deletion;system;rule;absolute gain;paper;baseline", "pdf_keywords": ""}, "821532ecef5bc2252823b190c35f1e4c44ddc41c": {"ta_keywords": "multilingual word embeddings;parallel text;language pairs;different language pairs;language models;alignment quality;robust performance;model;art models;varieties;objectives;experiments;methods", "pdf_keywords": "multilingual word alignment;monolingual word alignment;high quality word alignments;multilingual word aligners;multilingual language models;multilingual corpora;multilingual word corpora;crosslingual machine translation;multilingual word performance;novel neural word alignment extraction method;parallel corpus;translation language modeling;word alignment;machine translation;empirical alignment extraction method softmax;diverse language pairs;generative neural machine translation model;monolingual models;neural machine translation model;novel statistical word aligner;translation language model;computational machine translation;word embeddings;other language pairs;language models;language pairs;target language pair;novel alignment extraction method;new alignment extraction method;optimal alignment settings"}, "2b4edb9515a26561ea3f9ee2a63a506721c8369e": {"ta_keywords": "aspect prediction;sentiments analysis;machine learning;scientific reviews;active learning framework;aspect;reviewers;training dataset;8k reviews;dataset;useful information;top conferences;international workshop;decision;paper;field;ilc;chair", "pdf_keywords": "sentiment analysis;collect peer review data;annotated datasets;review text;aspect sentiment features;sentiment patterns;annotation framework;corpus;sentiment vector;top machine learning conferences;active learning;aspect sentiment;sentiments patterns;active learning framework;peer review process;machine learning;scientific reviews;review scores;classifiers;sentiments;class classification task;reviewers;sentences;reviews;review;abstracts;international conference;large scale dataset;learning algorithm;novel aspect representation"}, "f6d6c4dd0115386c234a0b027dd38f7aa9d9df2f": {"ta_keywords": "nonlinear maps;nonlinear map;documents;certain tasks;form;challenges;field;tutorial;art;certain problem;case study;state;order", "pdf_keywords": ""}, "31c53acd2a43dcec4342d9c42d0ffbfbef36e855": {"ta_keywords": "noisy label;maximum likelihood estimation;classifier;synthetic data;miscalibration;calibration;estimation error;confusion matrix invertibility condition;matching approach;sample error;moment;saddle;consistency;unified view;experiments;findings;method;decomposition;terms;conditions;impacts", "pdf_keywords": "label shift estimation;unsupervised domain adaptation algorithm;target domain label distributions;label shift estimation problem;label estimation;label distribution;generalized distribution matching approach;minimum learning;distribution matching;label shifts;label shift;probabilistic classifier;classifiers;importance weights;machine learning;target label;machine learning methods;label;classifier;novel classifier;machine learning pipeline;maximum likelihood estimation;predictor weights;deep learning;mlls;arbitrary predictor fc;time machine learning;synthetic data;empirical distributions;binary classification example"}, "ccfaccf36b9cd7c0c05af2285ec90ecf5f51a34c": {"ta_keywords": "optimal relay location;optimal power allocation;relay channel;relay nodes;relay node;single relay case;individual power constraints;optimal placement;loss channel models;law path;source node;power;exponential;explicit formulas;line;source;problem", "pdf_keywords": ""}, "d8d12c922fc571d081bae27c67fcf50cdbb17d90": {"ta_keywords": "standard text summarisation dataset;historical text summarisation;summarisation model;corresponding modern language;parallel data;modern language;chinese news;documents;historical forms;hundreds;task;techniques;quality gold;years", "pdf_keywords": "large monolingual summarisation dataset;historical text summarisation;standard text summarisation dataset;high quality text summarisation;quality summarisation corpus;summarisation model;first summarisation corpus;machine translation systems;news texts;crosslingual language modelling paradigm;comprehensive literature search;elegant transfer learning method;historical news stories;historical documents;historical languages;chinese news;linguistic knowledge sets;historical news;succinct word representation model;individual sentences;historians;parallel supervision;modern sentences;literature;modern corpora;identical lexicon pairs;digital humanities researchers;topics;linguistic experts;unsupervised generation"}, "c4607387ee863d5c5e5dc9f8adfbe7930508e286": {"ta_keywords": "machine learning;international machine;icml;finland;helsinki;brief review;21st annual conference;conference;iml;review process;individual review members;papers;society", "pdf_keywords": ""}, "7f4fa7c6f16f2965a104fa45071ea0c92b4366fe": {"ta_keywords": "repulsive interaction;stiffness;dimensional polymer;polymer;simple model;monomers;number", "pdf_keywords": ""}, "7634b0cf93169d2a95d4d7193f47f97a61e3b4b2": {"ta_keywords": "dimensional games;game design framework;human behavior;surveys;different players;players;traits;different traits;data;approach;framework;combination;numerical methods", "pdf_keywords": "diagnostic game design;diagnostic games;model players planning;game effectiveness;prospect theory;behavioral observations;game design;games;designing games;game design framework;mutual information objective;game;large game space;mutual information maximization problem;player classification;distinguishable behavior;player traits;mutual information;game space;reinforcement learning;reward function;player space;other social games;maximization;optimization model;decision rules;optimization;prior knowledge;behavior;player classifier accuracy"}, "1ccf412212873ae1b020762b8b86291e1fb11f65": {"ta_keywords": "crowdsourcing;audio transcriptions;reliable data collection;principled data collection methods;russian language;resource language;data;scale dataset;general challenge;work;methodology;development;applicability", "pdf_keywords": "crowdsourced audio transcriptions;crowdsourced audio annotations;crowdsourced audio recordings;audio annotations;audio transcriptions;crowdsourcing;crowdsourcingthis work;ground truth speech recordings;best transcription;transcription;transcription aggregation algorithms;datasets;voice recordings;synthetic speech generator;dataset;available datasets;available dataset;speech recognition;annotated documents;perfect transcriptions;speech recognition systems;data collection pipeline;annotations;correct transcription;text translation tasks;real datasets;synthetic transcriptions;high quality data sets;data collection;scale datasets"}, "3ba529f732d3c4a31e9ce57f1c78ddf911846bf4": {"ta_keywords": "weak supervision sources;weak supervision;sequence tagging;extensive comparisons;sbc evaluation;popular sbc methods;benchmark platform;standardized evaluation;classification;implementations;world datasets;sbc;wrench;approaches;modular framework;range", "pdf_keywords": "present weak supervision benchmark platform;standardized weak supervision benchmark platform;weak supervision sources;multiple weak supervision sources;weak supervision model;weak supervision;weak supervision problem;strong supervision;supervision source;supervision sources;sequence tagging;major machine learning tasks;novel labeling function generator class;training label models;supervised learning;labeling function generators;intensive machine learning problemswe;training data;annotation;deep learning;diverse datasets;joint learning;label models;large datasets;weak version;powerful tool;labeling functions;machine learning;tagging schemes;relation classification"}, "64bc7fe1c46c4d4106afba4621ff1bd4376c077a": {"ta_keywords": "aaryngeal model;ofaryngeal distortion;patient saryngeal packet;mode mode;spectrum;peaks;mode;estimation;model;patient;method;amount;number", "pdf_keywords": ""}, "04db62a14f78f693d6bd14a4803b9b73325b36bb": {"ta_keywords": "fake news detection;fake news;external knowledge graph;news item;subgraph classification task;news;subgraph;neural network;knowledge;graph;gnn;model;work;problem;aim", "pdf_keywords": "fake news detection problem;fake news detection;popular fake news detection system;false news detection;knowledge graph;knowledge graphs;external knowledge graph;subgraph classification model;subgraph classification task;news content;subgraph;strong false news signal;news items;news record;subgraphs;news item;news articles;multiple news items;fake news;relation extraction method;embeddings;news source;original news item;false news;textual content;social networks;scale news record;news;social media;false information"}, "9bd6cdae71506eb307507e44df7abe0c285b3ca7": {"ta_keywords": "machine translation;neural model;neural action;transformation;diffusion network;linear machines;quality;model;activation;output;input;results;effect;final result;standard;study;set", "pdf_keywords": "neural machine translation;machine translation;translation quality;standard machine translation system;translation engine;neural language model;translation system;generic translation engine;neural mt reranking;translation output;neural model reranking;dimensional language models;gram language model;neural language;translation;discrete translation operations;dimensional attentional model;neural models;language system;sentence comparisons;ranking;target language;neural model;sentences;englishin;languages;sentence structure;reranking;baseline model;neural network"}, "e0a0b3438aef008fece5b8bbf76105b470f10f25": {"ta_keywords": "single quantum dot;quantum entanglement;quantum dot;quantum dots;entanglement;quantum information processing;coherent state;observation;new type;key resource;fundamental resource", "pdf_keywords": ""}, "1817c9f0fd8a17e31c65963dd8cee9783059495b": {"ta_keywords": "", "pdf_keywords": ""}, "167adafac25ee108ca99c688cceded8bca710bb1": {"ta_keywords": "age dependence;human brain;statistical analysis;age;statistical methods;mouse;control sample;stimuli;same age;size;analysis;relationship;single copy;results", "pdf_keywords": ""}, "538466f2a69271617bf4f5b0df4e5fd854c11c35": {"ta_keywords": "graph codes;defective items;computational complexity;algorithm;high probability;order;systematic methodology;10k", "pdf_keywords": "adaptive group testing schemes;new adaptive group testing scheme;noisy group testing framework;noisy group testing;group testing code;group testing;nonadaptive group testing;group testing problems;group testing problem;graph codes framework;graph codes;error reconstruction property;sparse bipartite graph;noisy test results;group testi ng;tests;error threshold;groups;test;robustified singleton detection algorithm;defective items;novel algorithm;trivial items;computational complexity;complexity algorithm;complexity;new algorithm;robust framework;arbitrary nodes;defective item"}, "7e358ffc2731a82420d84a7f0bedb155a487c39d": {"ta_keywords": "hop datasets;datasets;dataset;25k questions;new multihop question;improved performance;better performance;goal", "pdf_keywords": "connected reasoning;reasoning graphs;question decompositions;traditional question decompositions;knowledge generation tasks;challenge multihop dataset;connected multihop questions;universal dataset constructionwe;natural language pipeline;comprehension tasks;domain comprehension;challenging dataset;new multihop dataset;multihop datasets;challenging questions;natural answer questions;multihop annotation tables;knowledge domains;knowledge discovery;approach partitions multihop datasets;questions;annotated multihop table topographies;multihop questions;network representations;answerable questions;datasets;domain qcd;dataset;answerability;reasoning"}, "61cce75554a6d1bb802f26758c3b0ba97de6918d": {"ta_keywords": "attentional networks;graph context;positional embeddings;positional information;nodes;graph;node;enhanced gat architecture;content information;gats;model predictive;work;framework;experimental results", "pdf_keywords": "graph attention layers;graph attention networks;graph networks;attentional networks;attentional layer;node classification taskin;graph context;positional embeddings;node classification tasks;positional embedding;corresponding attention module;supervised loss;embeddings;convolutional networks;graphs;deep learning;graph;nodes;neural network;supervised learning;node;generalized neural networks;supervised learning problem;perturbative learning;vertices;gnns;learning;homophilic datasets;labeling;undirected graphs"}, "b81acc013c42796a5eea0fc20cfb04846da3a589": {"ta_keywords": "multilingual language discovery;language data;linguistic networks;powerful linguistic processing tools;linguistic features;documentation system;language theory;recent discovery;large scale;system;novel;description;goals;efficient manner", "pdf_keywords": "linguistic annotation backend;linguistic annotation;linguisto corpus;machine translation pipeline;language data;neural machine machine translation;annotation candidates;linguistic data;statistical machine translation models;natural language processing;linguistic documents;linguist;linguistic systems;linguists;native language;native native language;languages;transcription;transcription results;workflow;parallel data;ngt;ngts;new project;process;list;users;technology;use;system"}, "398a0625e8707a0b41ac58eaec51e8feb87dd7cb": {"ta_keywords": "learning pipeline;text;train agents;new agents;agents;novel simulator;abstract knowledge;rich visual environment;abstract policies;alfred benchmark;effective model;environments;creation;goals;novel method;approach;ability;method", "pdf_keywords": "abstract text world;alfworld framework;textworld;textual environment;arbitrary task descriptions;abstraction;text agent;abstract language;novel alfworld;abstract language modality bywe;natural language;abstract tasks;expressive representation;text language;alfworld;visual world;natural language representations;arbitrary language;autonomous tasks;text;abstract environment;training agents;linguistic tasks;arbitrary tasks;novel framework;natural language encoder;novel environments;learning processwe;natural language generation process;environments"}, "4f7bbcef3d40cafad17936fdf562a121667af1e8": {"ta_keywords": "vessel tree reconstruction;new geometric regularization principle;vector fields;geometric regularization constraint;arteries;blood flow;divergence;nonzero divergence;veins;bifurcations;prior knowledge;convergent;model;principle;quality", "pdf_keywords": "vessel tree reconstruction;unsupervised vessel tree extraction;vessel reconstruction problems;vessel reconstruction;vessel tree estimation;complete vessel trees;vessel divergence;vessel pathline tangents;vessel trees;vessel tree;vesselvessel tree estimation;curvature regularization;vessel enhancment filtering;new geometric regularization principle;curvature approximation;vessel directions;sparse vector field;divergence penalty;priori curvature regularity;vector fields;standard vessel filters;blood flow pattern;regularization;topology extraction;curvature;flow pattern;vector field;iterative minimization;divergence;nonzero divergence"}, "0431f60546381a9e91fb156236c3c7056f57081f": {"ta_keywords": "pitch augmentation;voice synthesis;different data augmentation methods;augmentation methods;augmentation;public singing databases;consistent training strategy;training strategy;training;performance;mix;subjective evaluations;several strategies;cycle;systems;extensive experiments;ss;work", "pdf_keywords": "singing voice synthesis;public singing datasets;synthesized singing;voice augmentation;acoustic features;pitch augmentation;local acoustic feature;acoustic model;simple data augmentation methods;data augmentation methods;deep learning;data augmentation;public japanese singing corpora;acoustic structure;lyrics information;underlying music score;synergetic acoustic model;augmentation method;music;cryptographic systems;music score;consistent prediction;vocoder;prediction;model;supervised classification tasks;improvements;consistent training strategy;original training process;new training strategy"}, "dbe87b171bfb789e1d22a047aeeee69105e6fd02": {"ta_keywords": "sentence embeddings;encoder;language processing tasks;decoder models;full encoder;sentences;decoder model;spontaneous emission;performance;sequence;art performance;characterization;novel approach;state;variety;approach", "pdf_keywords": "encode sentence representations;sentence encoders;sentence encoder;sentence transfer tasks;sentence embeddings;high quality text representations;text encoder;raw embeddings;text transformers;contrastive learning task;text transfer transformers model;encoder;encoder models;contrastive learning;sentencet5;text transformer;large linguistic datasets;text model;contrastive learning architecture;type embeddings;encoder encoder;encoder architecture;decoder;contrastive learning method;sentences;encoder modules;encoder encoder encoder;dual encoder module;decoder model;decoder transformer model"}, "95788ed8affd06c0c2c6159c26ff7c123c4f2e0a": {"ta_keywords": "end speaker diarization methods;speaker;speaker issue;novel speaker;wise conditional inference method;speakers;probabilistic chain rule;wise parametric model;stop sequence condition;variable number;method;number;state;art end;paper;experimental results", "pdf_keywords": "speaker diarization;speech separation task;speakerwise conditional signal;speech separation;speakerforcing;speech activity;speaker;speaker counting;distance speech;neural diarization method;speaker outputs;similar speaker;neural diarization;speaker issue;novel speaker;conditional conditional inference;wise conditional inference method;speakers;neural network;neural networkwe;deep learning setting;deep neural network embeddings;probabilistic chain rule;separation;standard eend method;eend;projection function;learning process;training loss;pairwe"}, "84908a28a03d0d7c467d9556ed36f0e416de7171": {"ta_keywords": "hybrid semantic analysis system;semantic analysis;utterances;free grammars;extended context;recognition;system;article;idea;large number;fraction;ones", "pdf_keywords": ""}, "b46be3ac246499655cc442e93c5878e7a9640ae3": {"ta_keywords": "timelines;timeline;saga;related events;like events;explicit representations;running sequence;unsupervised methods;methods;novel method;performance", "pdf_keywords": ""}, "8da992b611df508b1803f66ffa53bd1fb741a76c": {"ta_keywords": "conditional neural language model;hierarchies;hierarchy;input document;rich structure;experiments;quantitative quality metrics;quality;data;experiment;novel;proc", "pdf_keywords": "challenging text generation task;novel text generation task;question generation;question generation systems usedin;neural question generation experiment;text generation process;neural text generation pipeline;unstructured text;natural language processing;answer spans;automatic approaches;natural language;answerable questions;answer model;corpus;reading comprehension datasets;meaningful sentences;reading comprehension dataset;neural machine translation;unstructured hierarchies;language modelling;paragraph;meaningful hierarchy;answer length;annotated version;useful hierarchies;specific questions;specific paragraph;unstructured data;document"}, "2eea63f896deed47cc0c0000e1482ec5c860fd0b": {"ta_keywords": "level controversy detection problem;sentiment information;semantic information;structure;components;method;contributions;combination;reply;novel method", "pdf_keywords": ""}, "881ce19455a9923e4798e9d77d2d8623ca9d2e03": {"ta_keywords": "speech recognition;predictive distribution;variational bayes;data sparseness;novel statistical framework;clustering;bpc;analytical approximations;data version", "pdf_keywords": ""}, "6d9603be7e79ff33677327a0edd5bd3f7da6347b": {"ta_keywords": "turbulent medium;zeeman field;particle;propagation;stationary medium;vicinity;effect", "pdf_keywords": ""}, "81af4e14050c410e2afee226be583088a9791ddf": {"ta_keywords": "unsupervised semantic role induction;argument embeddings;bias argument embeddings;sim999 word similarity task;dependency relations;art embeddings;dependency roles;context;neural model;dataset;multiplicative factors;slc;model;state", "pdf_keywords": ""}, "ec99cf93ef22a0c0d669abe90c9509f642b2cf69": {"ta_keywords": "novel adaptive downsampling technique;semantic boundaries;target classes;accuracy;locations;uniform;computational efficiency;method;balance", "pdf_keywords": "good semantic segmentation;accurate semantic segmentation;adaptive downsampling;semantic segmentation;downsampling;adaptive adaptive downsampling technique;object classification;deep learning;high quality feature maps;art segmentation;deep learning nodes;downsampling block;segmentation;semantic boundaries;object classificationwe;person segmentation;road traffic scenes;high resolution;images;deep learning algorithms;deep convolutional neural network;convolutional network;pixel level;computer vision;better inference speed;resolution datasets;input images;deep learning kernel;pixel resolution;semantic structure"}, "48e8e8085907192d501eb2bcc582035e90431a2f": {"ta_keywords": "tags;several benchmark tasks;recurrent unit;conditional random field layers;sequence;task;specific sequence;words;ner;model;state;art results", "pdf_keywords": "deep hierarchical recurrent neural network;training corpus;encode morphology;entity recognition;compact word embeddingswe;recurrent units;conditional random field layer;conditional random fields;conditional random field model;neural representations;conditional random field;word levels;extended tag sequence;multilingual data;linguistic structure;rich encoding structure;tag tagging model;tagging;language;data representations;languages;multiple languages;wise point tagging;sequential information;latent conditional model;semantics ofwe;words;sequence;entity;encodes"}, "5431098723db5858c4553f0259921cbbdd6492d5": {"ta_keywords": "prey;infectious diseases;transition;key step;predator;spread;key", "pdf_keywords": "translation infrastructure;translation initiative;translation data;machine translation;machine translation systems;multilingual machine translation;optimal translation standards;translation standards;translation benchmark set;translation quality assessment;translation study;translations;translation directions;deep bidirectional machine translation;pandemic data;translation;coronavirus;pandemic;corona virus;translation cross section;covid19;new corona virus;imminent outbreak;english translation cross section;widespread outbreak;language;natural language processing;different languages;several other languages;crises"}, "a72e732f2d11075aa0103b72b4f9884ddcaaaa85": {"ta_keywords": "inductive logic programming;negative learnability;logic programs;polynomial learning;probabilities;theory;worst performance;combination;study;results;success;sum", "pdf_keywords": ""}, "015dc5b71894dd4d05e7668d015e545ab2e162ba": {"ta_keywords": "speech processing;espnet;new toolkit;end text;extension;models;new end;paper;state;art", "pdf_keywords": "source speech processing toolkit;speech synthesis;more accurate speechwe;novel speech synthesis system;deep learning;sourcenet;speech;utterance;art tts toolkits;new toolkit;deep mixture density networks;wavenet;big data;generative network;toolkit;generative models;vocoder;generative;neural network;end text;espnet;available dataset;generative model;novel software;knowledge distillation;input text;generative adversarial network;generative adversarial networks;new end;predictive models"}, "3122a2d7799ba585b993e432b3deb47659b3f3c1": {"ta_keywords": "sparse attention;contrastive retriever learning;task;lfqa;form question;heuristic;retriever;novel approach;mean;approach;mean value;steps", "pdf_keywords": "sparse attention model;sparse attention;retrieval;long document;correct retrievals;retrievals;neural text generators;natural language processing tasks;neural summarization systems;retriever language models;quantum;contrastive retriever learningwe;generative description;deep learning pipeline;knowledge representation;generative model;task formulation;natural language processing;contrastive learning;paragraph;neural representation;task;novel retriever expression;generative quality;coherent generation;relevance;parametric knowledge;lqa;simple retriever model;questions"}, "15d643f4c27d373aa46f26a760051e76fde81dc2": {"ta_keywords": "end entity resolution;differentiable knowledge graphs;public datasets;model;differentiable model;task;end;e2e;context;independent approach;recent method;question;kgs;problem;gaussian;time", "pdf_keywords": "knowledge base queries;external knowledge graphs;best answer entity;knowledge graphs;differentiable knowledge graphs;knowledge graph;differentiable knowledge graph;entity resolution;entities;entity;specific entity;entity resolutionwe;arbitrary entities;intermediate annotations;candidate entities;end learning;end models;knowledge;related tasks;answers;scalable representation;questions;question text;multihop inference;query;end model;span detection;models;task;discrete representations"}, "5f9d8fe21efb3c2b241427869a333472ab09a22d": {"ta_keywords": "photon laser pulse;laser pulse;single atom;laser;atom interaction;atom interaction region;atom;simple model;interaction;model;generation;idea", "pdf_keywords": ""}, "ea6547e877c1cc3d37229a6f488ac04e9a11de18": {"ta_keywords": "protein protein interfaces;colic protein;water positions;water contacts;docking models;protein portion;blind predictions;early discovery domain;recall fraction;predictions;accuracy;features;simulation;good descriptions;complex;first assessment;groups", "pdf_keywords": ""}, "fd1b59d22eb1fb32e0360d4fcbe58dc4ebb25af9": {"ta_keywords": "audio deepfakes;deepfakes;fakes;detection methods;detection;text;shortcomings;recent trends;domain;full survey;clear need;first survey;version;generation", "pdf_keywords": "audio deepfakes;audio deepfake research;audio deepfake generation framework;fake fake fakes;fakes;high quality speech;fake fake fake image;real speech;target speaker;deepfake news;speech prediction;speech synthesis;fake fake;deepfakefakes;deepfakes;deepfake;recording;target speakerwe;speaker;speech synthesis model;real news outlets;fake version;truth network;replay attacks;false information generation process;fake image;false signals;unique source;deep survey;noisy media"}, "86b91922923b03c66497accfa88c638299fc8d26": {"ta_keywords": "variational encoder;discrete encoder;encoder;decoder;decoder method;decoder problem;discrete latent variable;latent variable;gaussian variable;vector;novel approach", "pdf_keywords": ""}, "f35a01c1e5d5375453c39e6161526633492fb574": {"ta_keywords": "standard erasure code system;data erasure;code queue;latency;average latency;throughputs;codes;system;various systems;analytical bounds;results;significant improvement;environment;form expressions;effect", "pdf_keywords": "queue system;storage systems;data storage systems;queues;queue;batch scheduling;underlying queue model;storage nodes;storage system;memoryless service times;data storage;batch batch scheduling policy;batch processing jobs;storage;effective scheduling policy;tier queue;scheduling policies;scheduling policy;latency performance;multiplicative markov process;join queues;markov process;memory systemswe;processing;memory systems;throughput;latency;reservation framework;servers;sds"}, "9b71542ef5d5178041048b9a330309053bb0bcfc": {"ta_keywords": "synthesis model;high speech quality;target speech;discrete symbols;different languages;mixture;discrete;input;recognition;quality;novel method;method;negligible loss;paper", "pdf_keywords": "speech discretization vocoder;speech separation models;speech discretization;speech enhancement;novel speech separation;speech enhancement andseparation;speech separation tasks;speech separation;speech separation task;continuous speech signals;pseudo vocoder;vocoder;discretization;multiple speaker mixture signal;mixed speech;noise ratio representation;separation performancewe;speechwe;speech;enhancement model;encoder;decoder;synthesis models;classification;target speech;discrete symbols;enhancement paradigm;enhancement;human hearing;noise ratio"}, "79ab3a0d6dc5d6fd3b466ea2814fdbb93a3672d0": {"ta_keywords": "spin dynamics;spin;zeeman field;effect", "pdf_keywords": ""}, "da10c4bc1de7b9b7ddbb21d70ff5092a15cb866f": {"ta_keywords": "domain adaptation;unsupervised approach;unsupervised setting;transfer;maximum entropy model;novel approach;methods;promising future directions;art methods;comparison;performance;state;discussion;problem", "pdf_keywords": ""}, "9db77e925015ad02efc9beeab233bedbfe04e4b7": {"ta_keywords": "novel reinforcement learning method;several benchmark reinforcement learning tasks;smooth strategy;reinforcement learning tasks;new strategies;more robust error estimates;dula;other approaches;method;scalability;novel variant;competitiveness;development;efficacy", "pdf_keywords": "reinforcement learning strategy;several benchmark reinforcement learning problems;standard reinforcement learning;novel reinforcement learning;reinforcement learning;scalable reinforcement learning approach;reinforcement learning approach;reinforcement learning problems;reinforcement learning framework;learning strategy;reinforcement learning system;policy search strategy;reinforcement learning problem;generalized uncertainty strategy;policy search;exploration strategy;evolution strategy;gradient approximations;strategy;exploration;approximated gradient;optimal search direction;benchmark benchmark es;variable learning rate;optimization;new strategy;several benchmark problems;optimal response function;directional smoothing;noisy search stream"}, "af0aa62d243c761b56a83369bc9b1f75805003cf": {"ta_keywords": "coordinate term extraction;text mining;similarity measure;coordinate term;supervised learning;extraction;similarity;text;words;task;notion;essential features;accuracy;framework", "pdf_keywords": ""}, "f765b23f0b0d2a196bc0fe562ad24278d0c9cee4": {"ta_keywords": "global adaptive learning;deep learning model;adaptive learning;stochastic optimization method;convolutional neural network;learning rate;gradient method;new method;parameter;method", "pdf_keywords": ""}, "ccbc17d42f2b260079eee702fd97a75de705d8ac": {"ta_keywords": "sentences;vector;language;background;generation;method;form;set;presence", "pdf_keywords": ""}, "75b13e7131997ff6fd21325d68a2222d2c1b7157": {"ta_keywords": "spatiotemporal beamforming;beamforming formulations;spatial separation;spectral separation;novel convolutional architecture;neural network;multiframe context size;neural networks;noise ratio loss function;window size;signal;scale;block size;novel method;average improvement;effects;method;novel", "pdf_keywords": ""}, "00b874f8346cedadc2a6366c4b72e60140f99556": {"ta_keywords": "semantic similarity;text;neural networks;attention;words;data;model", "pdf_keywords": ""}, "43c844c30765f3fa25bfabd83490ef826b9ceca1": {"ta_keywords": "novel word recognition model;unseen words;keyboard mistakes;accuracy reduction;random adds;models;greater robustness;several new backoff strategies;drops;pipeline;swaps;spread;vanilla;form", "pdf_keywords": "adversarial spelling mistakes;accurate word recognition models;novel word recognition system;word recognition models;word recognition;adversarial results;adversarial perturbations;adversarial examples;downstream word recognition task;adversarial attacks;word models;input corpuses;classifiers;simple word model;small corpus;larger corpus;deep neural networks;character attacks;unseen words;misspellings;adversary;words;deep learning model;machine learning problem;individual words;dropwords;prediction accuracy;word;natural words;unk tokens"}, "25ae911c13da7ef9def56ee30170920ebd48a668": {"ta_keywords": "new computational argumentation task;web arguments;arguments;relation classification;topics;convincingness;relation;qualitative properties;large dataset;findings;total ordering;data;global constraints;same stance;properties;pair;problem;16k pairs", "pdf_keywords": ""}, "b48f0652605f981b5d407496aba3d9756725264f": {"ta_keywords": "preferences;decision maker;choice;decision;performance;users;comparative study;user;system;impact", "pdf_keywords": ""}, "a636768c2fc6cadccd8bb4d704f651dd54dad395": {"ta_keywords": "emotion recognition;colorful utterances;indonesian speech;colorful speech;sdb corpus;classifier;support vector machine;acoustic features;corpus;utterance;feature selection;recognition performance;television talk;parameter optimization;result;terms;first study", "pdf_keywords": ""}, "f664e6635d0514b0cb398a713f08bab90b4a3d81": {"ta_keywords": "word graph;arcsecond;novel algorithm;large collections;algorithm;documents;characterization;novel;sub;version;use", "pdf_keywords": ""}, "bd1bdb3c5f28001a4cee92c0e1669512d0f06a35": {"ta_keywords": "periodic potential;usual periodic potential;hamiltonian;particle density;particle;dynamics;numerical simulations;simple model;model;limit;generalization;study;good agreement", "pdf_keywords": "generalized zipf;hypergraph;free energy density;energy density;hypergraph andwe;gaussian;simple formal derivation;generalized version;gdf;temperature phase diagram;rigorous derivation;derivation;boytsov;day;function;paper;law;few simple assumptions;circle;succinct modification;evaluation;new method;proof;value;wordsin;equivalence;linear combination;method;note;result"}, "a9b9404962760731d6d2fc2ecbc6da7bc2f21be7": {"ta_keywords": "signal processing;gaussian distribution;probability distribution function;distribution;output signal;input signal;signal;snp;reduction;model;method;paper;size", "pdf_keywords": ""}, "32367e7587d5b2de0391cff9ad2d600ff8624e60": {"ta_keywords": "human social skills training system;social skills training;social skills;audiovisual information;visual features;presence;pitch;method;simulation;users;experimental evaluation;novel method;results;paper;yaw;ratio", "pdf_keywords": ""}, "22f4eb19be4031e63194bbd7c355914533004918": {"ta_keywords": "dedicated hybrid transmission;energy driving gear shift;transmission;powertrain;dht;low impact;road;impact mechanism;vehicles;unprecedented energy efficiency;impact;resolution simulation model;model;work;novel;building process", "pdf_keywords": ""}, "ea77b71385648f5c6ea533a0e3685f0e76302eba": {"ta_keywords": "quality entity recognizers;active learning;effective training data;sized languages;languages;model predictions;novel method;approach", "pdf_keywords": "entity recognition;large corpus;annotated tokens;annotators;annotation data;annotation;corpus;lingual transfer;natural language processing;entities;entity;human annotation experiments;new languages;linguistic representations;resource languages;lower languages;high quality entity gazetteers;active learning;effective training data;linguistic structure;novel active learning algorithm;languages;active learning strategies;resource nernetiation;different languages;ner systems;ner;classification;fewer tokens;model predictions"}, "0822f8d7e6a72a65e65f147d3a8d8fccd485da40": {"ta_keywords": "transformers;short subsequences;language;transformer;recurrence methods;sequences;perplexity;short sequence;words;longer ones;model;tokens;new methods;methods;maximal length;models condition;first method;second method", "pdf_keywords": "transcription sequence;long sequences;shorter subsequences;shorter inputs;short subsequences;gene transcription transcription process;subsequence length;subsequence lengths;sequences;nonoverlapping subsequences;pattern learning;transformer efficiency;sequence;subsequences;input length;small computational effort;memory;sparse transformers;training cycle;approximate transformer models;transformers;transformer models;transformer output;range sequence;recurrence methods;overall training time;efficiency improvements;longer ones;discrete representations;efficiency"}, "253ad629cd2d396201d71aa605bec233bff66dca": {"ta_keywords": "speech recognition;probabilistic mixture model;complicated acoustic model;appropriate acoustic model topology;variational estimation;clustering;decision tree;efficient model search algorithm;automatic determination;vbec;vbec framework;gmm;efficient method", "pdf_keywords": ""}, "fac95cc5f52f954fe89b3aa4b75895568ff6a6d4": {"ta_keywords": "normalization;language data;historical texts;frequency data;baseline;rule;exact matches;results;terms;better performance;approach;variety;method;type;state;art", "pdf_keywords": ""}, "8b73e226815d57bf66fc94905ebd063e4957b449": {"ta_keywords": "peer review;reviewer identity;privacy;efficient algorithms;utility;theoretical study;challenges;problem;tradeoff;frontier", "pdf_keywords": "differential privacy;privacy;randomized conference decisions;paper reviewers;reviewer assignment;peer review paper;reviewer functions;adversary assignment;peer review;reviewers;peer review process;open review;optimal calibration;posteriori computation;optimal calibration strategy;potential adversary;optimal algorithms;new optimal calibration strategy;review systems;optimal assignment;joint information sharing;review system;conference paper;constant adversary;peer;adversary;quantum adversary;global information processing;conference calibration;miscalibration functions"}, "f249e3a7d4f7f964e9a4ca6e633ac31410a91dd8": {"ta_keywords": "inflection decoders;monolingual data;step attention architecture;multiple languages;models;data;novel architecture;scale;accuracy;effects;generation;magnitude;order", "pdf_keywords": "novel language representations;novel attention model;morphological inflection;attention;step attention architecture;powerful lexical processing tool;interpretable attention matrices;language pairs;monotonic attention;resource languages;attention monotonicity;crosslingual labeling;inflection tasks;many related languages;languages;neural machine;attention mechanisms;new language;new candidate language;target language;language;language transfer languages;speech transcription;lemma character sequences;language transfer;candidate languages;inflection generation;language pair;encoder;multiple related languages"}, "a309cb82c27233948f9b09f440be171a8d24ffff": {"ta_keywords": "novel peer selection algorithm;competitive peer selection accuracy;peer;selection algorithms;algorithms;algorithm;selection problem;agents;explicit partitioning;variety;metrics;recent generation", "pdf_keywords": "unbiased peer selection algorithms;peer selection algorithm;new peer selection method;exact peer selection;peer selection;impartial peer selection;automatic ranking;peer selection decision;peer selection outcomes;ranking;reviewers;review pools;selection selection algorithm;random selection;review pool;best strategyproof selection algorithm;networked agents;selection process;decentralized evaluation;probabilistic completion;selection;algorithms;novel algorithm;rank;reviews;algorithm;selection pipeline;new algorithm;agents;ordinal peerwe"}, "04a94c15fec43e7563d58be697246a0dd6c57021": {"ta_keywords": "virtual content;social media platforms;spatial coherence;content;spatial coherence results;web;generation;emergence;generous regulatory environment;existence;post;ample supply;case", "pdf_keywords": "media creators;information content providers;information content provider;algorithmic promotion;creators;user content;content;tech platforms;intermediaries;news media;platforms;media products;internet;creation;technology;curation;extremism;creation activities;publishers today;technology industry;new definition;definition;algorithms;article;usual definitions;users;machine;companies;intense scrutiny;systems"}, "a99de68ee8d6729eee5ca5943b152aba7e4738ee": {"ta_keywords": "neural network;neural networks;neural network models;edits;new inputs;salient information;models;edit;structure;new class;form", "pdf_keywords": "neural editor models;neural editor;neural editor model;novel neural editor system;neural editing model;computational neural editor model;novel edit patterns;source code edit data;probabilistic editor;edits representations;code edits;edit representations;editing structure;edit representation;sequential edit encoder;graph edit encoder;similar edits;new graph edit encoder;edit pattern;like edit;edit encoder;novel annotation patterns;editing process;abstract structural editwe;sdb code edits;source code;natural language;meaningful edits;natural language data;edits"}, "7f20366098665cd508fe82255cc1a65e1e733a14": {"ta_keywords": "speech recognition;speech recognition system;acoustic features;maximum likelihood estimation;discriminative criterion;variance;available database;access database;adaptation;access;method;novel method;paper;results", "pdf_keywords": ""}, "22d2f8030221bd0c27bfb9416eeffe4e86633780": {"ta_keywords": "semantic similarity computation;accurate ranking;lexical scores;query processing efficiency;hybrid indexes;documents;interpolation;performance;improvements;sdss;mesh;novel method;scale experiments;techniques;method", "pdf_keywords": "dense retrieval models;dense document retrieval;dense retrieval model;sparse retrievalwe;document retrieval;fast query processing;faster query processing;information retrieval;semantic similarity computation;possible ranking;semantic scores;retrieval;ranking;dense retrieval depth;forward indexes;retrieval datasets;dense indexes;forward index;query processing latency;index optimization;document scores;dense index;sparse matching bywe;efficient sparse models;query processing techniques;sparse matching;forward search;search engine;sparse matching strategies;efficient matching"}, "4e0610ac4c5e055ac56b2ae0d91386a10ffbd325": {"ta_keywords": "specific learning agent;deep feature learner;student machine;specific prior knowledge;agent;student;integration;novel approach;domain;effectiveness;design;approach;problems", "pdf_keywords": ""}, "8d019c77989100a51385e4b4a5fa5250445d8f1d": {"ta_keywords": "complementary systems;speech recognition;system combination;deep neural networks;acoustic modeling;spontaneous activation;new method;new approach;method;methods;performance;challenging tasks;effectiveness;paper", "pdf_keywords": ""}, "1b97c38d0156dc8bf300b41c2ba5c0463c3a2c00": {"ta_keywords": "best data augmentation strategy;data augmentation;training data;training;workshop;results;new method;aim;paper;chapter;germany;hamburg;work", "pdf_keywords": ""}, "3df825e086b00dd4132c34ecbf638f9a6dc4320d": {"ta_keywords": "artificial intelligence agent;separate transfer;transfer;module;integration;student;design;illustrative examples;method;potential benefits", "pdf_keywords": ""}, "3e59b3e1e3ef65f9574a0fe30f18ba7a815ea0af": {"ta_keywords": "dialogue tasks;quantum networks;dialogue;networks;nodes;exploration;network;rich structure;repertoire;improvement;novel approach;efficiency;approach;quality;number", "pdf_keywords": ""}, "790eb7e93f1d3fce470c0222fd2be83bab55a428": {"ta_keywords": "automatic speech;language model;models;recognition;end;novel word;nns;nn;significant error reduction;common feature;combination", "pdf_keywords": "external rnn language model;end speech recognition;deep cnn;word representations;novel language modeling;end speech processing pipeline;neural network;language models;rnn;language model;convolutional neural network;deep convolutional neural network;automatic speech recognition;rnnlms;convolutional neural networks;end speech;word search;vocabulary end;attention;word level;cnns;large text corpus;encoder;encoder decoder;target language;decoding;beam search;linguistic model;decoder;word error rate"}, "83cf7b9611fabe9da2d08722445039023f1b19e9": {"ta_keywords": "gaussian noise;electron density;dimensional electron gas;strong magnetic field;dynamics;effect;presence;factor", "pdf_keywords": ""}, "9b09ff09b88bb793b161f284ca6e66031bc5a992": {"ta_keywords": "repulsive fluid;viscous fluid;viscosity;repulsive interaction;dynamics;fluid;interaction;velocity;density;2d;states", "pdf_keywords": ""}, "68f2f32e0e8fc868920971077a11042784be2616": {"ta_keywords": "data mining;data mining course;linear algebra;rating;comprehensive introduction;parameters;book;methodology;supplemental book;fundamentals;students;point;field;use;anyone", "pdf_keywords": ""}, "a3ca4893ae941bd1601322aface4840e47339761": {"ta_keywords": "crowdsourcing scores;agents;other strategyproof mechanisms;aggregate contributions;new strategyproof mechanism;decentralized fashion;reviews;mechanism;literature;idea;better performance;fact;necessary properties", "pdf_keywords": ""}, "a810d2f4a1fefd4175d8cdda9702ee1b829e5831": {"ta_keywords": "adipose area;obesity;metabolism;pseudogap;subcellular excitons;mechanism;new mechanism;pdcs;discovery;distribution;population;occurrence", "pdf_keywords": ""}, "66081634c17b089cb47fd1b0ad7ad842c7fb3f87": {"ta_keywords": "teachingable agent;mock student;student learning;simple learning strategy;student;simple tasks;teaching;learning;agent;tutee errors;project;simple examples;computation;tutee;parallel;problems;problem;paper;predictions;relationship", "pdf_keywords": ""}, "d706645fbbc6edfad5fb642b1dfc3019fcabbd99": {"ta_keywords": "text generation papers;story evaluation experiments;descriptions;reproducibility;text;references;tasks;model;english teachers;performance;strict qualification filters;bothunlike teachers;teachers;crucial details;survey;human;vast majority;aim;series", "pdf_keywords": "text generation tasks;text generation;text generation task;text generation papers;prompt generation;story generation process;story generation;text evaluation;designing text;neural language modeling;neural language model;prompt description;sophisticated machine translation setups;text;human texts;texts;generation;sentences;large corpus;natural language processing;sentence structure;generative model;nlp;ratings;generative model thatwe;reproducibility;second language task;arbitrary linguistic specificity;likert scale ratings;first generation"}, "ad7129af0644dbcafa9aa2f111cb76526ea444a1": {"ta_keywords": "neural fake news;controllable text generation;real text;vaccines;news;exposure bias;grover;strong generators;best current discriminators;autism;public release;model;similar strategies;effects;link;strategies;importance", "pdf_keywords": "neural fake news;fake news articles;disinformation attack;fake news examples;online disinformation;real news articles;large scale propaganda;natural language generation;news articles;propaganda;available large news domains;news websites;fake news;controllable text generation;large corpus;false news;deep bidirectional machine translation;news article;artificial intelligence;same news source;news articleswe;language modeling framework;new neural model;news;real news;content;machine learning model;machine learning;text messages;underlying language"}, "946e5e31b0779fc33550e8681994e7afd8d549a5": {"ta_keywords": "motion;measurement;central patient;clinical environment;patient;vicinity;method;novel method;target;time", "pdf_keywords": ""}, "81d4357afae9680e64a645cbb36aa090c3619b19": {"ta_keywords": "critical slowing;slowing;particle;energy;loss;complete description", "pdf_keywords": ""}, "a7d6b5e61024127bf4fe8f04c0182a16ff97bccf": {"ta_keywords": "bribery methods;voters;stochastic environment;parameterized complexity;probabilities;formal analysis;terms;forms;actor;dynamics;evaluation criteria;problems;multiple issues;preferences;model;issue weighting", "pdf_keywords": "stochastic lobbying problem;hard probabilistic lobbying problems;probabilistic lobbying;efficient lobbying strategy;lobbying;effective lobbying strategy;voter bribery;bribery problems;election process;greedy bribery algorithm;voter corruption problem;election criterion;bribery scheme;elections;special election criterion;bribery action;dodgson election system;voting power;voting;american elections;voter;particular election;vote influencing;election;referendums;referendum instance;voters;large political influence network;underlying social choice problem;parameterized complexity"}, "419e714f22c3fa2599abebd630cae5595c70bdef": {"ta_keywords": "automatic speech recognition;speech input;learning representation;asr;iris;model;self;best performance;end;literature", "pdf_keywords": "kakuded speech recognition;large scale speech datasets;channel speech enhancement model;large scale speech recognition;speech enhancement;automatic speech recognition;robust speech recognition;deep learning extension;speech input;standard deep learning;speech recognition problem;speech signal;advanced convolutional neural model;deep neural networks;learning representation;noisy speech;learning model;speech;feature;recognition task;new model;asr;model;end model;input signal;ability;hybrid;channel;dsl;automatic synthesis"}, "888c81cd3d1e953e2b7f8cc4ce68ca9f908c1e8d": {"ta_keywords": "differential privacy;text representation learning;privacy loss guarantees;implementation;several recent papers;general empirical check;dp mechanism;dp", "pdf_keywords": "differential privacy;partial differential privacy;differential privacy mechanism;partial differential privacy mechanism;deep datasets;text representation learning;deep networks;privacy loss guarantees;representation learning;networksampling;dataset;distribution;arbitrary complexity;discovery;probability distribution analysis;data;public data release experiment;network;examples;complexity;dynamic behavior;several recent papers;inverse continuous density function;dp;deparametrization trick;implementation;accidental access;model;internet;new paradigm"}, "596b46dbe4fa8eee72e517ea9fd5f8ef83c9c64e": {"ta_keywords": "next round;selection;brief overview", "pdf_keywords": "qb answer strings;qb answer stringswe;best answer probabilities;traditional machine learning tasks;qb game;qb questions;neural qb model;machine reading comprehension dataset;traditional information retrieval models;answer type;qb datasets;natural language systems;natural language processing;bestwe introduce qb;qb community;trivia game;qb;questions;qb models;distinct guessing;selection;hybrid task;challenges;simple game;natural language processing problem;human player;sequential decision;queries;bayesian classifier;qb question"}, "650f2afca6d72d6b6e2e08849e1224f1e8b7900c": {"ta_keywords": "binary rating estimation;graph side information;rating matrix;optimal sample complexity;efficient algorithm;algorithm;world graphs;novel statistical model;sharp threshold;quality;entries;number;relationship;problem;function;limit;experimental results", "pdf_keywords": ""}, "932404745d960291925b3f27b71734dff5b23633": {"ta_keywords": "blind classifier;impact disparity;learning processes;group;policies;policy;effective strategy;empirical results;world datasets;practical consequences;strategy;subgroup;members", "pdf_keywords": ""}, "7bbd132f40c7630aeebf6379b00e307c3fff738c": {"ta_keywords": "data storage;data sets;simulation;data;algorithms;algorithm;bandwidth;maximum likelihood estimator;method;performance;use;sequence;problem;form", "pdf_keywords": "data regeneration;systematic node regeneration code;systematic node regeneration model;systematic regeneration algorithm;data collector;nodes;systematic regeneration scheme;data storage;systematic nodes;other systematic nodes;systematic node;systematic regeneration;entire data;other systematic node;exact regeneration;regeneration condition;regeneration;storage;data collection;algorithm;partial regeneration;systematic reconstruction;node;data points;explicit linear construction;data;reconstruction;partial transpose;matrix;systematic nodein"}, "e8c3090e66fdb05a2c169a12c52dd94bb8786fb5": {"ta_keywords": "natural language explanations;subreddit algorithmic architecture;explanations;neural methods;explanation;strong predictive power;echoing;word;view;features;dynamics;process;set", "pdf_keywords": "natural language explanations;persuasive comment;natural language subreddit review;natural language explanation;natural language lemmas;everyday explanations;persuasion;word embeddings;level comments;explanations;explanationswe;content words;sentences;comments;arguments;neural methods;conversation;discourse;reasoning;new explanation;stopwords;word level;like sentence;comment;argument;predictingwe;text;public opinion;term memory model;explanatory context"}, "ed535e93d5b5a8b689e861e9c6083a806d1535c2": {"ta_keywords": "elementary models;universal transformers;systematic generalization;model configurations;transformers;embeddings;relative positional embedding;accuracy;tricks;devil;new method", "pdf_keywords": "systematic generalization tasks;several systematic generalization tasks;deep transformer;systematic generalization;transformer datasets;promising generalization strategy;neural networks;deep learning library;relative positional embeddings;embeddings;baseline transformers;relative positional embedding;generalization;neural network;compositional generalization;accurate models;training configurations;large scale tasks;baseline models;datasets;novel encoding strategy;models;relative positional embeddings canwe;dataset;machine translation;various datasets;transformers;accurate encoding;layers;accuracy"}, "9abb50813e05de849dbbd89535bc7d0206f5e36a": {"ta_keywords": "verb data;verb classes;empirical estimation;partial reconstruction;maximum likelihood estimation method;data;maximum likelihood estimator;sample;method;results;users;predictions;performance;problem;process", "pdf_keywords": ""}, "0bbfa6ab7451aea5cbb842cce97b54500bafdfc7": {"ta_keywords": "preference learning;optimal decision maker;human preferences;decision maker;decisions;sample complexity;suboptimal case;human;number;popular framework;case;problem", "pdf_keywords": "inverse decision theory;inverse decision problem;inverse reinforcement learning;suboptimal decision maker;binary decisions;suboptimal decision making;optimal decision maker;optimal decision makers;decisionsreinforcement learning;optimal decision rule;decision function;optimal decision rules;suboptimal decision rules;discrete decision problem theory;sequential decision making;preference learning problem;suboptimal decision ruleswe;global optimal decision maker;decision function thatwe;human preferences;inherent uncertainty;decision maker;decision rule;optimal decision rule forwe;optimality;decision process;decisions;decision problem;identical decision makers;decision rules"}, "a31ab366b0a349ee5f341f1179810bc9805d32a4": {"ta_keywords": "storage;data file;security;codes;matrix construction;data;l2;optimality;product;size;threshold;msr;point", "pdf_keywords": ""}, "04d18fc81cc232b3d3dece0994c0fa8aaabaf4b7": {"ta_keywords": "effective domain adaptation;cost domain adaptation strategy;word segmentation;tagging;domain;annotations;pointwise approach;prediction results;shape;part;method;paper;process;problem;ssp;minimum amount", "pdf_keywords": ""}, "d5dcbb144a2be999610b4838d94cc3fb228f837c": {"ta_keywords": "network slicing;thelatency backup configurations;reinforcement learning approach;deployment;deployment costs;quality;model;paper;accuracy;novel model;end;end end;time;potential;results;context;effect", "pdf_keywords": ""}, "df689bdc6c497949e9ab3b7ba19950d9fade7180": {"ta_keywords": "orbit interaction;spin;orbit;coupled system;dynamics;enhancement;effect;suppression", "pdf_keywords": ""}, "d7fe9b46f96ae9df7fa64e1c575c7114e5ef0aaa": {"ta_keywords": "new tensor method;tensor method;tensor methods;convex optimization problems;optimization problems;objective function;lipshitz;condition number;complexity analysis;method;additional assumption;several classes", "pdf_keywords": "accelerated tensor method;accelerated tensor methods;new tensor method;tensor methods;convex objective function;certain optimization problem;th order condition number;optimization;unconstrained nonlinear optimization;accelerated hybrid proximal extragradient method;new optimal solution;optimization problems;stochastic optimization;approximate stationary points;optimallyfresco;gradient method;optimal solutions;uniformly convex functions;tensor network;optimization problem;th order derivative;fast convergence;order method;optimal performance;second order method;optimalfresco data;objective functions;newton method;evaluation complexity;starting point complexity"}, "4c6f7fb5c2e1bd12899c3ec2788f9ce7eb2f8a5c": {"ta_keywords": "speech tagging;dependency parsing;paraphrase identification;syntactic knowledge;nonlinear programming model;downstream applications;nlu;more data;data size;models;performance;part;better performance;study;analysis;impact;trade", "pdf_keywords": "syntactic generalization performance;deep bidirectional linguistic models;syntactic structures;different language models;syntactic knowledge;syntactic structure;syntactic evaluation;different syntactic phenomena;syntactic tests;deep biaffine neural dependency parsers;deep biaffine neural dependency parser;linguistic structure;text prediction;pretraining;sentences;bidirectional context;natural language;training data;bidirectional model;linguistic properties;unstructured learning tasks;learning;models;simple models;more data;structural ambiguity;downstream applications;resources;raw text data;data source"}, "b73191adcc938cfcf20ce0327cf5cd1f539f7f81": {"ta_keywords": "neural tagging;unannotated articles;semeval task;scienceie;data;art results;method;state", "pdf_keywords": "neural tagging model;entity recognition;sequence tagging;supervised key classification;unannotated articles;keyphrases;annotated training data;optimal word representations;keywords;semisupervised learning model;optimal word representation;word embeddingwe;deep learning task;scientific articles;novel supervised learning model;supervised neural model;tags;unlabeled data;best classification;sentence level scores;semisupervision;sentence level score;unsupervisedwe;empirical discovery;training data;optimal word;documents;words;forward neural networks;labelwe"}, "06d77cc8970b59102a0caffb5e4c5b7a3242563a": {"ta_keywords": "quantum field;schrdinger equation;particle;wave function;particles;energy;asymptotic limit;calculation;model;large number;new model;exact solution", "pdf_keywords": ""}, "4715ee17ca4f52762fdf67c9a8ef8fb751c88484": {"ta_keywords": "rank minimization problem;electrical appliances;relaxed convex formulation;arxes models;power consumption;identification;arxes;only measurements;constant output;constant input;piecewise;method;new method;contribution", "pdf_keywords": "blind deconvolution method;blind deconvolution method andwe;convolution model;regressive exogenous input;sparse signal representations;corresponding arxiv model;output system;rank minimization problem;model power consumption;multiple output systems;quadratic estimators;certain arxiv model;blind identification problem;measured outputs;identification problem;finite impulse response;noisy channel;common noisy channels;only output measurements;only output observations;estimation;common noisy channel;convex optimization model;blind identification;impulse response;identification;unknown inputs;convex relaxation;estimatingwe;filter"}, "cac008e541af58f738407c7f2ee86d547053188f": {"ta_keywords": "heisenberg antiferromagnet;zeeman field;spin;orbit coupling strength;orbit coupling;field;effect", "pdf_keywords": ""}, "1e2ef0c9a494c7949f38940ee735a88c56355202": {"ta_keywords": "sensors;algorithms;high accuracy;characterization;novel method;methods;paper;possible thanks;use;help", "pdf_keywords": ""}, "00c3a86551f1bc812b676025210e295021853f66": {"ta_keywords": "big bang;pacs numbers;questions;numbers;historical figures;important facts;volume;collection;authors", "pdf_keywords": ""}, "69d5579955a5a8859d78a70b3d1afede0f91fa09": {"ta_keywords": "energy disaggregation;energy data;power consumption;individual appliances;disaggregation;individual device;power;output system;whole building;inputs;device;output;input;task;novel framework", "pdf_keywords": "energy disaggregation task;power consumption data;aggregate energy data;energy data;power consumption dynamics;power consumption;scale energy data collection experiment;disaggregation;disaggregation results;individual appliances;device usage;hybrid system estimation;power;observed powerwe;individual devices;powerwe;unsupervised methods;data;data processing;individual device;computing environment;timeseries measurements;devices;supervised approach;aggregated signal;system identification;single device;blind system identification;output system;nonintrusive setting"}, "834d68b9befcc6c68415b460b33435a1822799fb": {"ta_keywords": "argumentation mining;web discourse;new gold standard corpus;corpus;guidelines;context;source codes;community;article;user;free license;task;problem;novel method", "pdf_keywords": "argumentation mining;argumentation mining literature;argumentation mining field;argumentation content;novel argumentation models;argumentative web document;popular argumentation model;robust argumentation models;argumentative documents;argument component classification;argumentationwe;argument extraction process;argumentation model;argumentation structure;argumentation phenomena;argumentation scholars;argumentation mechanism;argument component identification;argumentative generation;argumentative text;expressive argumentation;argument patterns;argumentative discourse;argumentations;annotated arguments;dialectical argumentation level;argumentative argumentation;argumentative analysis;argumentation theories;argumentation"}, "972a74968d2522908b06c5bd1e26266194c5a9ee": {"ta_keywords": "decontextualization task;decontextualization;sentences;text;context;new context;sentence;framework;meaning;system;means;models;user", "pdf_keywords": "sentence decontextualization;wikipedia corpus;decontextualization results;decontextualization;retrieval corpus;decontextualization process;decontextualization tool;lexical tasks;decontextualization model;decontextualization setup;contextualization;passage retrieval tasks;sentence level segmentation;annotation procedure;sentences;costwe define decontextualization;annotation;annotated examples;wikipedia;annotators;linguistic domain domain;linguistic domain;phrases;paragraph answers;original sentences;unified text;certain sentences;understanding tasks;word level segmentation;annotator"}, "6b387d18bae978202af501c4795f37a0c73781a6": {"ta_keywords": "hybrid proximal extragradient method;numerical convex optimization problem;smooth convex optimization problems;recent tensor method;tensor method;hessian;gradient;approximate solution;auxiliary problem;method;convergence rate;iteration;methods;function;paper;class;step;author", "pdf_keywords": ""}, "2bb1e1a5b9a16f6828fe94736cea5dab264533a6": {"ta_keywords": "ungrounded language models;semantics;language models;assertions;meaning;ability;ways", "pdf_keywords": "semantic emulation;semantic transparency;ungrounded language models;assertion information;semantics;assertions;denotation;natural language;natural languagewe;linguistic properties;indirect grounding;semantic equivalence;textual contexts;semantic relations;same semantics;strong transparency;meaning representations;linguistic model;valid context;language;linguistic accuracy;languages;strong notion;language corpora;computable functions;different contexts;emulation;definition;oracle turing machines;expressions"}, "96b32b204a62777bef66eea595de2c47b4e9d6e9": {"ta_keywords": "dependent data sets;reverse gradient method;domain;data;orthogonal projection;original domain;model;representation;subspace;method;battery;performance;combination;small number", "pdf_keywords": "deep neural network;neural network;synthetic datasets;neural network architecture;synthetic data;domain representation;neural building block;accurate domain classification;textural representations;textural information;textural representation;semantic labels;multilayer perceptron;semantic structure;textural images;images;unseen domains;data;continuous learning problem;textural image;regular domain model;noisy data;differentiable structure;minimal learning time;single layer;multiple datasets;large data;feature function;image;objects"}, "636611068825cb4b7bdab6ad16ef415adf4fb96c": {"ta_keywords": "multidomain methods;multidomain;multidomain setting;domains;specific class biases;general overview;systematic analysis;approaches;performance;open questions;implications;questions;respect;paper;goal", "pdf_keywords": ""}, "6f902b8128b218563b276c1ebff46ef668dcb185": {"ta_keywords": "arbitrary collusions;incentive mechanism;other noncolluding mechanism;agents;mechanism;setting;priori", "pdf_keywords": "crowdsourcer;novel incentive mechanism;incentive mechanism;heterogeneous agents;mobile crowdsensing systems;incentive;agents;crowd;intrinsic bias;mobile crowd;mobile agents;agent;heterogeneous setting;fair game theory;bias;optimal numbertheorems;insensetive mechanism;cooperative equilibrium;optimal strategy;thresholds;tasks;threshold;task;optimal number;social game;budget constraint;best algorithm;bids;mechanism;world data revealwe"}, "7650d705b85dc399112a5b6a79e9c6f81c7c6146": {"ta_keywords": "discrete optimization problems;large domain specific corpora;diverse datasets;powerful neural network;cloze style questions;base documents;data;system;different domains;approach", "pdf_keywords": "extractive questions;cloze corpus;specific annotated corpora;natural language processing;supervised learning task;learning tasks;large scale data;supervised construction;answer sentences;supervised learning;large scale;supervised learning setting;theoretic datasets;supervised learning problem;source data;questions;scalable machine learning system;clozes;datasets;gene search;base documents;large domain;documents;learning problem;learning model;comprehension;wikipedia;answer triples;intrinsic memory;examples"}, "58834a447c749758e7f57498c6dd88a281af41a0": {"ta_keywords": "training constituency parsers;training parsers;constituency parsers;parsers;novel dynamic oracle;static oracle;dynamic alternative;languages;utility;novel approach;subset;approach", "pdf_keywords": "training constituency parsers;parser training method;novel constituent parser;constituent parser;policy gradient parser;constituency parsers;parsers;down parser;parser;low cost lstm parser;global chart decodera novel constituent parser;component parser;novel dynamic oracle;dynamic oracles;dynamic oracle;several dynamic oracle;computational linguistics;discriminative transition;large corpus;natural language;oracle;novel policy gradient method;candidate trees;english ptb;policy gradient approach;gold tree;policy gradient;likelihood training;softmax margin loss augmentation method;training methods"}, "1fa32503bce4f01ab2ccb65dedd374310c488fe8": {"ta_keywords": "partial compliance;compliance partners;fair employers;regulatory frameworks;employment market;incentive effects;local parity measures;statistics;extreme segregation;simple model;design;implications;number;level", "pdf_keywords": "fair machine learning;fair employers;algorithmic fairness;partial fairness;fair policies;compliant employers;fairness framework;fairness measures;discrimination;additional compliant employer;compliance;partial compliance;employment market;full compliance outcomes;fair machine;fair representation;bias;regulation;labor market;employers;fairness;affirmative action;employment;compliant institutions;job market;algorithmic decision makers;enforcement;global parity policy;global parity policies;empirical literature"}, "6ea353ada2b89763f58d8068a74b2e6def526948": {"ta_keywords": "information extraction techniques;biomedical text;biomedical texts;corpus;drug interactions;pharmacological substances;novel corpus;descripion challenge;spi;ddis;detection;npi pipeline;sem;ognition;evaluation;field;rec", "pdf_keywords": "annotated corpus;annotated corpora;natural language processing;use corpus;annotated text;corpus;gold standard corpus;pharmacovigilance corpus;annotation process;biomedical text;annotation;annotation task;annotation guidelines;interaction extraction;pharmacovigilance annotation;drugbank database;pharmacological substance recognition;nlp;corpora;annotation schema;protein interaction;annotation schemathe task;drug names;drug interactions;pharmacological substances;protein;bioinformatics;pharmacology experts;standoff annotation principle;extraction"}, "c507ad8b7bec5d29da7cf0ee92e2bf4361a5c92f": {"ta_keywords": "deep quantization;deep reinforcement learning framework;quantization;policy optimization methods;quantization levels;neural networks;training framework;releq;action spaces;various hyperparamters;network architecture;state;tuning;best design decisions;end;process;systematic approach;problem", "pdf_keywords": ""}, "2899eb53cddf050e3a34f07bbc0bc0ee7907d5d0": {"ta_keywords": "speech tagging problem;language resource additions;word segmentation problem;training corpus;japanese;part;experimental results;effect", "pdf_keywords": ""}, "626f8a50a7bd24d869f25bddb6fbaa59b090268c": {"ta_keywords": "pattern recognition device;signal;level systems;scene;system;method;presence;idea;collection", "pdf_keywords": ""}, "3681456f29398e42cc2baafb0b72d166070a3cf1": {"ta_keywords": "dynamic leader;linear game;linear games;sequential updating;strategy;policy gradient;standard policy gradient;algorithm;variant;generalization;novel method;method;form;use", "pdf_keywords": "quadratic dynamic games;quadratic control;quadratic games;optimal control;sequential game;quadratic programming;static linear policies;stackelberg leadership model;stochastic optimal control;follower algorithm;optimal control rate;stable stabilizability;follower type algorithms;natural gradient policy;sequential policy updates;policy gradient;stochastic control problems;quadratic quadratic quadratic gradient equations;control equations;natural gradient policy update;free sequential algorithms;multivariable linear quadratic regulator;stochastic control;natural gradient game;optimal strategies;dynamic games;optimal strategy;quadratic quadratic cost function;nonlinear stochastic control problem;quadratic programming problem"}, "a711e02f85fa52c15df0a830a8ba88df2c3928ec": {"ta_keywords": "particle wave function;spectral density;hamiltonian;calculation;method;new method;combination;use", "pdf_keywords": ""}, "4c67c129dab9805ab248407b77a6d542c2e40d41": {"ta_keywords": "minimization;original rank;matrix;random graph;new algorithm;algorithm;rank;value filtering;entries;subset;renyi;erds;set;perturbations;problem;necessary conditions", "pdf_keywords": "rankone matrix completion;matrix completion problem;matrix completion problems;matrix completionwe;adversarial corruptions;task assignment matrix;noisy matrix;crowdsourcing problem;new crowdsourcing method;novel crowdsourcing algorithm;task assignment algorithms;sparse recovery;crowdsourcing scenario;crowdsourcing model;original task assignment problem;crowdsourcing;optimal algorithms;optimal algorithm;rank;underlying matrix;same adversarial strategy;crowdsourcing experiments;large crowdsourcing experiment;original rank;matrix;task assignment problem;corrupted agents;adversarial model;minimization;resilient consensus problem"}, "840fabc2a7773e1bd771f152f76210b2ea5845b9": {"ta_keywords": "novel probabilistic conditional stochastic beam search;conditional stochastic beam search;conditional conditional stochastic sampling;noisy beam;stochastic process;best items;sst;many applications;pmsn;method", "pdf_keywords": "conditional stochastic beam search;stochastic beam search;stochastic beam search method;present conditional stochastic beam search;beam search strategy;beam search strategies;beam search;conditional random walk algorithm;beam search strategieswe;stochastic version;optimal sampling strategies;optimal sampling strategy;first search heuristic;neural machine translation;beam;neural machine translation system;target beam;stochasticity;conditional sampling design;novel algorithm;machine translation;efficient estimators;beams;nodes;stochastic process;stochastic model;algorithm;importance sampling;noisy subset;unbiasedness"}, "509b42fc150a057a64c4608f64e779ef04fdff47": {"ta_keywords": "entity recognition;english tweets;diverse training data;temporal information;temporal drift;entities;novel data;documents;better models;performance;task;better use;methods;effect;focus", "pdf_keywords": ""}, "11c6d0851152b6bec34726be40d90bea8d8a90f0": {"ta_keywords": "annotation noise;novel crowdsourcing model;annotator expertise;common noise adaptation solution;common noise;annotator basis;individual noise;instance difficulty;common confusions;perinstance;world benchmarks;confusion;experiments;effectiveness;source", "pdf_keywords": "noisy annotations;annotation noise;noisy annotation process;annotated annotators;annotated annotations;annotation quality;annotators;crowdsourced labels;annotations;annotator annotations;best annotations;annotation;annotator information;crowdsourced data;crowdsourcing;available annotations;annotator;single annotator;common confusion models;annotation basis;neural classifiers;neural network classifier;noise adaptation layers;accurate classifiers;simple noise models;individual noise;crowds;learner;neural networks;probabilistic classification model"}, "6c520d983923dbe1e437c01086424fdcdd8f430a": {"ta_keywords": "statistical parametric speech synthesis process;statistical parametric speech synthesis;speech parameter parameter;quality;trajectories;freedom;literature;phenomena;paper;degree;aforementioned mentioned", "pdf_keywords": ""}, "5f46d8e18138fe572b6fae897475a2ad645a3e1a": {"ta_keywords": "depthless representation;neurons;single neuron;depthless network;biological tissues;biological process;structure;representation;network;novel approach;density ratio;terms;approach;scenarios;variety;problem;case;large number", "pdf_keywords": "hybrid neural networks;deep learning models;deep learning;deep neural network model;speech recognition;deep learning process;automatic speech recognition;deep network representations;voice search;speech data;conversational speech recognition;neural network models;deep learning experiments;dimensional speech;neural networks;neural network;dimensional acoustic model;andtranscript generalizations;nonlinear neural networks;tune networks;acoustic model;language model;decoder;encoder;train networks;rnn;domain model;fusion;target domainthe development;shallow fusion"}, "2ce3428ba8777c723b9b12e9f8eaeb2c87a5a793": {"ta_keywords": "faithful sentence rationales;sentence level;standard bert blackbox;nondifferentiable models;supervision;target task;reinforcement learning approach;correct rationales;model;high quality models;pipeline counterpart;datasets;new framework;cases", "pdf_keywords": "sentence prediction task;sentence prediction;rationale supervision;sentence models;reasoning tasks;simple sentence model;trainable neural networks;gold rationales;predictive models;neural networks;standard bert blackbox;blackbox approach;underlying sentence structure;prediction;sentences;trustworthiness;correct target prediction;faithful rationales;correct predictions;movie review;movie reviews;transparent decision;rational models;best rationale;rationales;correct prediction;rationalewe;standard bertarian bertarian bertarian;bertarian;standard blackbox approach"}, "cd1d915604826e5fb0ba2bbcdf8479a9b90fb289": {"ta_keywords": "random lattices;random lattice;random lattice path;scalable lattice model;random number;identical particles;model;generation", "pdf_keywords": "relay placement;optimal optimal optimal deployment;optimal optimal deployment;random lattice path;relay nodes;relay propagation;relay placement sowe;optimal placement boundary;optimal deployment;optimal placement set;optimal placement sets;optimal placement;optimal placement policy;deployment ofa communication scenario;random path;relay;relays;lattice path;impromptu network;multihop wireless network;optimal stopping problem;periodic measurement packets;optimalwe;network;periodic measurement packet;optimal time;placement set;optimal control systems;nlos scenario;global optimality"}, "a425a11b9b249cb768d0f54d4a32f4f1d007e279": {"ta_keywords": "pe rform feature selection;online learning;several online learning tasks;svm;balanced winnow algorithm;novel algorithm;new algorithm;pass;margin;algorithm;problem;results", "pdf_keywords": ""}, "2d71fb62c71e49479c1b6ce832ee1bb88df20556": {"ta_keywords": "least common subsumer;subsumption;chain equalities;descriptions;tractability;pair;set;method;problem", "pdf_keywords": ""}, "6fa85c46ea68c754ef903edc70058ba525f1fc4d": {"ta_keywords": "algorithm;copies;computer;generation;new method;large number;large numbers;method;number;ability", "pdf_keywords": ""}, "0c89b1ec80de46222ed7efc6261c03e52a1e2c54": {"ta_keywords": "description decoders;description encoders;natural words;global contexts;examples;literature;large class;model;large set;novel method;combination;effectiveness", "pdf_keywords": ""}, "b2b9b0d7afd85c5d708b79a61d9a000c6c906d8c": {"ta_keywords": "syntactic relations;parsed text problem;graph walk process;graph walk method;nodes;specific similarity measure;meaningful edge sequences;graph;task;level knowledge;novel approach;new path;framework", "pdf_keywords": ""}, "3042bc348d6cc7959cd574756f720e5afad236de": {"ta_keywords": "indented paperboard;simple anisotropic tray;paperboard;tray;paper;shallow rectangle box;deep drawing;indented bottom;top corners;round corners;wavy bottom walls;anisotropic;mechanical properties;simple method;application;method;use;key aspect;ability", "pdf_keywords": ""}, "39c5cfc0ff6660a17364cb4af1eb071d6efa463d": {"ta_keywords": "mechanical turk;comparative ordinal measurements;empirical evidence;sample noise;experiments;models;tasks;humans;cardinal ones;luce;popular thurstone;choice;pairwise;variety;unknown quantity;amazon;measurements;liv", "pdf_keywords": "comparative ordinal measurements;mechanical turk;ordinal samples;ordinal sample;ordinal approaches;sampleestimation;ordinal data;crowdsourcing;estimation;minimax estimation;measurement scheme;expert human evaluators;empirical evidence;comparisons;cardinal data;cardinal samples;cardinal sample;selection;metric;natural parameter estimationwe;better estimate;comparison graph;online critical search task;statistical analysis;comparison;relative merits;critical search;scoring;evaluation mechanism;fundamental theoretical bounds"}, "67b29c3fe6f110125a8892e8ed128d20b23957ea": {"ta_keywords": "higher linking accuracy;disambiguation;entity candidates;languages;same resources;resources;systems;model;limited availability;generation;account", "pdf_keywords": "crosslingual entity disambiguation;multilingual entity disambiguation;entity disambiguation;reference disambiguation;flexible disambiguation model;disambiguation method;disambiguation;resource languages;reference reference entity;threeentity disambiguation;entity candidate generation;heavy resource;entity;resource constraints;inherent link structure;xel sources;resource;resource restrictions;resources;semantics;socialentity linking;disambiguation ofemergent;natural language;resource availability;extensive empirical evaluation;resource settings;requisite resources;most languages;different resource availability assumptions;xel"}, "2a64da1ed300e49f2d665312146c8bb2f66920b7": {"ta_keywords": "statistical machine translation;smt parameters;optimization;smt;systems;research;recent results;years;issues;interest", "pdf_keywords": ""}, "d530a007ae0493ef6a8167c25bd007104623c504": {"ta_keywords": "code renaming;source code;variable names;binary;binaries;lexical information;decompiler;corpus;github;structural information;novel probabilistic technique;models;training;method;technique", "pdf_keywords": "code renaming;variable name recovery;annotated source code;binary namesin;identifiers;variable names;dcompiled identifier engine;source code corpora;source code;identifier;possible intrinsic identifiers;dire reconstructs information;decompiler;training corpus;decompiler output;novel encoding architecture;lexical output;code;acompiled code;meaningful names;modern decompilers;encoding;decompilers;encoding process;corpus;unstructured data sets;descriptive names;encoded process;binary data;git data"}, "bc1bf0a21d7838ec167e77c76163afc1f5f76c3d": {"ta_keywords": "channel electroencephalogram;spectral transform;background noise;channel signal;covariance matrices;frequencies;frequency bins;stft;potentials;different spatial spread;trial event;eweg;time;experiments;effectiveness;new method;method", "pdf_keywords": ""}, "4c9f20ce99f9b93527fd76ec04a44fcef9082005": {"ta_keywords": "recommender accuracy;interactive recommenders;aware recommender;aware recommenders;recommenders;fairness;reinforcement learning;accuracy;performance;low cost;terms;comprehensive framework;new framework;high degree;framework;analysis;characterization", "pdf_keywords": "personalized fairness;fairness state;aware recommendation framework;interactive recommendation;user preference state;novel fairness;fairness status;current fairness status;fairness measure;interactive recommender systems;reinforcement learning framework;fairrec;recommendation process;standard fairness methods;unbalanced recommendation results;fairness;structured recommendation system;reinforcement learning;recommender systems;reward function;unfair allocation;fair rec;fairfair;aware state representations;critic framework;aware state representation;user preferences;recommendation;state representation;policy"}, "cb153d8469ac466606032ea457b934bc61ae86ae": {"ta_keywords": "fake news detectors;dual emotion representation;emotion features;dual emotion;emotions;fake news;emotion;feature;signal;world datasets;novel approach;relationship;art task;enhancement;extensive experiments;state;performance;outperforms", "pdf_keywords": ""}, "029fa34b291c3f60b8a00cdf386e6048d45c394d": {"ta_keywords": "mixed membership clustering;scalable approach;graph;node;particle;large scales;representation;weighted sum;data;variety;approach;fact;state;art approaches;terms", "pdf_keywords": ""}, "04b876e95ac3e4754c8f0c8a9355e7acc3dc70b9": {"ta_keywords": "language resource addition;training corpus;annotated sentences;corpora;dictionaries;new words;dictionary;contexts;several real occurrences;entries;word;boundary information;effect;following cases;paper", "pdf_keywords": ""}, "7393d2618c7478d937112865458862e8d5f10475": {"ta_keywords": "sequence models;domain domains;dimensional vector field;generation;model;undirected vector field;vector;set;case study;size", "pdf_keywords": "cross domain reasoning challenge;seq reasoning tasks;structured knowledge base;underlying knowledge base;sequence models;next generation machine learning;knowledge generation;natural language models;diverse domains;seq models;seds;crossdomain reasoning;sequence model;language models;high quality sentences;natural language;natural languageneural networks;language model;cross domain;sequence;conditional patterns;seq;crossdomain;commonsense food habits;templates;unique templates;decoder models;dataset;models;domain"}, "7137a842d496a1a5581db31ad946fa0c0827e663": {"ta_keywords": "nonlocality;model;new model", "pdf_keywords": ""}, "f735f5f55cbc5a9d372ea1cd9b4e81d35f043a00": {"ta_keywords": "stochastic transitivity;transitive model;pairwise comparisons;classical parametric models;transitive class;standard parametric models;outcomes;models;various examples;probabilities;same rate;other hand;logarithmic terms;poor fits;greater flexibility;matrix;natural form", "pdf_keywords": "latent quality vector;stochastic transitivity model;pairwise comparison probabilities;statistical models;stochastic transitivity;stochastic transitivity conditions;pairwise comparisons;pairwise comparison data;optimal estimators;threshold;pairwise comparison;stochastic transitivity property;singular value thresholding;comparison probabilities;optimal estimates;minimax rate;singular value thresholding operations;various estimators;noisy observation model;various parametric models;minimax risk;threshold level;minimax;estimators;time estimators;parametric models;pairwise probability matrices;complete rankings;pairwise probability matrix;optimal rates"}, "380278716f4d78ad9dcc3ece9e12b235ca1d1569": {"ta_keywords": "deep learning problem;neural network infrastructure;probabilistic logic;parallelization;logic;hundreds;thousands;problems;examples;parameters;novel approach;precise tuning;approach;integration", "pdf_keywords": "probabilistic deductive databases;stochastic deductive knowledge graphs;logic embeddings;logic tensor network;deductive knowledge graphs;deductive knowledge graph;stochastic logic programs;probabilistic logics;probabilistic logic;probabilistic reasoning;probabilistic knowledge sets;tensorlog;deductive programs;probabilistic logical reasoning;deep learners;database predicates;tensor networks;probabilistic inference;deep learning dataset;new deep learning framework;deep learning framework;deep learning;tensor network;tensorflow;deep learning task;logics;machine learning tools;inference;deep connection;belief propagation"}, "8a880680b28dee5642ac88431b3ae1085b911f96": {"ta_keywords": "translation quality;translation data;translation length;translation;regularizer;english;word frequencies;use;case study;method", "pdf_keywords": ""}, "4f7b108830de2e7964b6e1a89bf1c2da60140a34": {"ta_keywords": "posterior collapse data;posterior collapse;predictive model;posterior;surrogate objective;worse evidence;empirical evidence;novel method;simple heuristic;heuristic;method;state;art methods", "pdf_keywords": "language models;powerful language model;language modeling;language model;effective representation learning framework;language modeling setting;latent variable encoder;generative skip models;variational autoencoders;inference network;posterior collapse;posterior collapse2;generative model;autoencoder;intractable marginal data likelihood;autoencoder objective;low dimensional representations;encoder;decoder;latent codes;marginal likelihood;unstructured words;posterior;dimensional representations;probabilistic classification;supervised classification;text datasets;surrogate objective;probabilistic mixture model;encoding"}, "14119210e5f9e0d962e329c833557dfb5524c4bd": {"ta_keywords": "oxygen batteries;nanofibers;battery;cathode;ices;scaffold;silicate;polymer;nss;sslobs;matrix;3d;structure;novel", "pdf_keywords": ""}, "de8ded0d66f3227d99751a89fdd5f4b438d6e8ee": {"ta_keywords": "harmonic trap;orbit coupling;coupling;coupling strength;spin;amplitude;particle;frequency;motion;effect;study;results", "pdf_keywords": ""}, "e74d7523d7d96ab65f05f059284f9d0a994bb074": {"ta_keywords": "new treebank;talks;proceedings;international workshop", "pdf_keywords": ""}, "3efee0095cb578659dfaaf0d87a616f133ecf85c": {"ta_keywords": "acoustic modeling;comparative study", "pdf_keywords": ""}, "9896a68e999298410bf16ffd08e8e67a54ad6a91": {"ta_keywords": "various natural language processing tools;natural language processing tools;corpora;cloud computing;framework;performance;evaluation;paper", "pdf_keywords": ""}, "52c040c4b1786166325a0d930af94a529e2b5023": {"ta_keywords": "neural network;neural networks;speaker;vector extractor;single training network;input;message;sequence;network;main network;method;novel method;training;standard training;significant improvement", "pdf_keywords": ""}, "da564ff902a5490088f60c9fb100531fc9f97288": {"ta_keywords": "probabilistic logic;personalized pagerank;entity resolution;joint inference;large databases;facts;classification;tasks;new approach;approach;number", "pdf_keywords": "stochastic logic programs;stochastic logic program;stochastic logic network;probabilistic language;new probabilistic language;database predicate;inference tasks;entity resolution task;efficient inference;personalized optimization;supervisedwe;search graph;prototypical inference task;underlying nodes;stochastic gradient descent;search;rapid learning;weight learning;optimal optimal search;search process;algorithms;personalized graph;query goal;nodes;stochastic gradient descent problem;order language;weights;learning;learning process;scalable approach"}, "27724bd19946d6a824d06cdca3cdfe5d40f71003": {"ta_keywords": "code snippet;past edits;learned model;neural models;sequential models;thorough evaluation;edit;representation approaches;higher accuracy;multiple strong models;model;l3po;novel approach;relative gain;variety;experiments;art;state", "pdf_keywords": ""}, "0cee58946a13a5c2845647b4af8b9d2bf52a8b6b": {"ta_keywords": "entity recognition;language model;ner tasks;novel training algorithm;accurate predictions;accuracy;tasks;several benchmark datasets;ner problem;ner;predictions;distance;new framework;new approach;framework;approach;superiority", "pdf_keywords": "entity recognition;usingnamed entity recognition;supervised language model;distant entities;label entities;distant labels;distant label;natural entities;external knowledge bases;language models;distant supervision;ner networks;entity;training sentences;nergniest machine learning tasks;natural language framework;manual annotations;natural language representations;language model;several ner networks;several ner benchmark datasets;supervised learning;classification tasks;ner tasks;ner models;description framework;noisy distant labels;deep learning models;entitieswe;deep learning framework"}, "8274799029bfac4402685e1efd995a8aeb9e7426": {"ta_keywords": "unsupervised sentence compression;unsupervised text compression;novel unsupervised encoder;sequence encoder;decoder model;large corpus;decoder pair;sentences;sequence;benchmark data;data;essential information;qualitative features;model;promising results;set", "pdf_keywords": "new unsupervised sentence compression model;unsupervised abstractive sentence compression;new supervised abstractive sentence compression model;abstractive sentence compression task;machine translation encoder;sequence encoder;novel machine translation paradigm;latent word sequences;decoder autoencoder;compressing text;decoder;natural language;encoder;decoder pairs;novel encoder;deep learning task;encoders;sentences;paraphrasing;summaries;neural machine;sentence structure;decoder pair;deep learning;underlying encoder;compression ratios;conditional encoding;supervised supervised unsupervised supervision;standard encoder;structured data"}, "927efd299cffcfca3716efefcc904331b70c153e": {"ta_keywords": "interpretable reasoning graph;logical graph;mathematical expressions;natural language processing;model;question;answer;form;novel method;set;case", "pdf_keywords": "mathematical language;deep bidirectional neural proof generation;mathematical description;natural language;reasoning process;conversation;common mathematical structures;answer quality;interpretation graph;linear programming language;mathematical structure;linguistic structure;natural language problem;conversations;annotators examples;mathematical representation;deep neural solver;description;questions;linguistic aspects;complex questions;conversation collection problem;order evidences;characterization;satake;noahqa;order evidence;translation;mathematical model;best baseline"}, "6a116b897569fe4d6ea9ad4c3ba9a18825b96f49": {"ta_keywords": "knowledge base completion;neural controller system;primitive differentiable operations;differentiable operations;differentiable model;reasoning tasks;order rules;model;sets", "pdf_keywords": "neural logic programming;knowledge base reasoning tasks;knowledge base reasoning;knowledge base;recent probabilistic differentiable logic;knowledge vectors;new statistical relational learning method;statistical relation learning;logic programming outperforms;neural rules;neural lp;logical rules;structured queries;knowledge basewe;neural lp system;structure learning;learned system;memory attention vectors;natural language systems;neural network;natural language;neural controller;learning process;neural tensor networks;neural network model neurallp;neural controller system;convolutional neural networks;relations;embeddings;longer rules"}, "3c0e8f7337491ca4f714de14021eb23ca43d1d5e": {"ta_keywords": "multidimensional speech representation system;automatic speech recognition systems;automatic speech recognition;reverberant environments;acoustic models;noisy environments;robust system;robustness;aspire;challenge;high performance data;system;performance;set;work", "pdf_keywords": ""}, "6c78bac2dd71efb89951d9bab72c8129bbc07f67": {"ta_keywords": "topic models;regularization framework;regularization;language modeling;membership models;regularization technique;latent;star;prediction task;observed variables;preferences;better perplexity;pseudo;aggregate functions;characteristics;experiments;error rates;mechanism", "pdf_keywords": ""}, "ce45aa1c64da82bfd02db0e147efa268da6980e4": {"ta_keywords": "orthogonal multiple access;bit loading algorithms;classic bit loading algorithms;ofdm;maximum entropy method;greedy algorithm;algorithm;new method;performance;comparative study;system;use;framework;article", "pdf_keywords": ""}, "d32fb57467d64bb82dce60e904ddc5c18b3f0f91": {"ta_keywords": "parking spots;spatial distribution;stochastic mixture model;predictive approach;optimal point;city;view;data;combination;method;point;novel method", "pdf_keywords": "parking demand;generalized gaussian mixture model;generalized gaussian mixture;parking dynamics;parking;spatial demand characteristics;parking zones;parking area;curbside parking;spatial demand;spatial distribution;parking spot;spatial heterogeneity;parking array;spatiotemporal structure;occupancy patterns;standard spatial data;spatial weight matrix;spatial autocorrelation;spatial homogeneity;spatial constraints;current parking price;spatial variational approach;least local spatial homogeneity;spatial structure;areas;gaussian markov chain;temporal demand;random matrix;area"}, "ab5c6703fceb3dce6558be309cc65a4a8615c774": {"ta_keywords": "fast neighborhood graphs;neighborhood graph;neighborhood graphs;graph;search algorithm;algorithm;search;distance;construction", "pdf_keywords": "direct nearest neighbor search;navigable nearest neighbor graphs;nearest neighbor queries;similarity search;distance symmetrization;metric learning;symmetric distances;metric learning method;generic distances;retrieval algorithm;neighborhood graphs;approximate search;approximate neighborhood graph;neighborhood graph;supersymmetry search;metric data;distance learning;search algorithm;unmodified search algorithm;metricspace mapping;navigable distances;rare data sets;search;similarity;distance functionwe;index construction algorithm;symmetrization approaches;distance function;algorithms;search engines"}, "d6741241efb9ffd933df974b43d7109c72238371": {"ta_keywords": "generative generator;novel generative system;generative model;music;generation;track;note density;control;structure;system;note;ability;user", "pdf_keywords": "music processing;music representation;midi instruments;musical tracks;music generation;additional musical material;musical events;musical material;multiple tracks;track sequences;several tracks;music;tracks;multiple track types;different tracks;generative system;piano;specific musical excerpt;onmusic;instrument;quality music samples;generative transcription network;track;iterative generation;single track;quality music;novel computational system;track information;original track;nintendo entertainment system"}, "d41216f2f809e9fe26a684392f0ded4778f79e74": {"ta_keywords": "automatic speech recognition;independent neural network architecture;utterance;asr;language switches;language;tracking;dependent models;model;novel model;end;experiments", "pdf_keywords": ""}, "66cbda3e730285cb572c4792edcef209af32c564": {"ta_keywords": "generative model;structured data;supervised model;supervised data;retriever model;neural network;questions;data;model;collection;question;variety;method;combination", "pdf_keywords": "sequence reader;information retrieval module;reader model;structured semantic model;attention scores;retriever models;retriever classification problem;knowledge distillation;large reader;unsupervised learning system;input documents;attention activations;reader;queries;retriever model;retriever module;reader module corresponds;annotations;knowledge distillation process;knowledge source;retriever question;retrievers;retriever;generative model;iterative training procedure;scale retrieval;sequence architecture;knowledge;relevance;annotated pairs"}, "7b96f6165ce5f686e46868c53b111b8e43b93de3": {"ta_keywords": "natural language processing;older papers;old papers;citations;literature;age;results;rate;number;study;factor", "pdf_keywords": ""}, "863b2b38b33ffc4e5462adbc7aaf84aeb93adda8": {"ta_keywords": "simultaneous annotation;annotation;single source languages;multiple languages;multiple sources;algorithm;tasks;methods;method;novel method;problem", "pdf_keywords": "parallel corpus;parallel corpora;joint annotation projection;parallel corpora show;separate corpora;corpora;parsers;syntactic dependencies;parallel text;multiple source languages;random corpora;dependency graph;target sentences;annotation;simple sentence splitting;tokenization models;target languages;decoded tree;low resource languages;novel joint projection algorithm;sentences;source sentence;languages;multiple tasks;entity;ilp constraint set;translation;treewe;graph;binary decision variables"}, "d16d24dd5135f148556df1b2304b3747eee19e00": {"ta_keywords": "text email messages;reply lines;signature blocks;email message;lines;sequential representation;features;sequence;algorithms;sequential machine;line;nonsequential machine;different feature sets;set;several state;art;methods;new method;method", "pdf_keywords": ""}, "7f54429be66319dc19a42c0c9fceda3ac33fc92d": {"ta_keywords": "inductive logic programming;classroom;teaching;simulation;language;student;demonstration;rules;approach;novel approach;path;finding;preliminary evaluation;set;use", "pdf_keywords": ""}, "c97500763de8a0871f1b83b1f968fcf4a8b31aee": {"ta_keywords": "unscripted conversations;unscripted corpus;different speaking style;standing;unscripted activity;dyads;presence;involvement;tasks;pairs;unscripted settings;levels;tool;experimental study;high density;degree", "pdf_keywords": ""}, "2ded680be56e03c8c17a04065deaac8ea6d4fa12": {"ta_keywords": "optical spectra;optical depth;single molecules;single molecule;optical lines;sample;simple method;identification;method;number", "pdf_keywords": ""}, "753d10503a3cf340e41552109087ffd15ec96446": {"ta_keywords": "harmonic trap;electron gas;dimensional electron gas;electron;trap;dynamics;motion", "pdf_keywords": ""}, "998bd8862ab4193e672bb16fe1aae4d446f7536e": {"ta_keywords": "image synthesis;unpaired image;contrastive learning;input image;image translation setting;images;input;novel method;patches;output;efficient way;other patches;similar point;method;approach", "pdf_keywords": "conditional image synthesis;unpaired image translation;unpaired image translation problems;unsupervised visual representation learning;image translation;multilayer contrastive learning framework;unpaired image;generative adversarial networks;contrastive learning;simple single image translation problems;single image training;contrastive learning loss;adversarial loss;input images;contrastive learning objective;contrastive learning framework;generative models;conditional imagewe;unsupervised learning;first image translation algorithm;standard contrastive learning framework;input image;image unpairedwe;depth representation;deep network;dimensional image spaces;adversarial network;consistent adversarial network;generative model;embedding"}, "d1206ccabd1980848f14472d6548251c2fab7963": {"ta_keywords": "transfer learning;transferability;question answering;source task;text classification;target task;sequence labeling;compliant benchmark task;task data;transfers;problems;performance;part;extensive study;different types", "pdf_keywords": "task embeddings;quantum information processing;explicit task embeddings;quantum computation;quantum search algorithms;quantum computing system;quantum computing framework;task transferability;task similarity;quantum computing platform;question answering;large language models;arbitrary source tasks;transferable source tasks;quantum computing units;quantum networks;best source task;linguistic task;source tasks;scale quantum problems;source task;linguistic representations;novel task relation;novel target task;scale language models;promising source tasks;qubit;tasks;task performance;target task"}, "e9d26b9f5e6b619bbb759a67560cb949a9f034ba": {"ta_keywords": "unconstrained continuous action;ascent learning;nonconvex optimization problem;gradient descent;strict local min;sum games;timescale separation;stable points;dynamics;ojasiewicz;time;objective;player;polyak;sc;system", "pdf_keywords": ""}, "6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91": {"ta_keywords": "multimodal machine learning;multimodal machine;late fusion categorization;common taxonomy;new taxonomy;broader challenges;future research;recent advances;researchers;field;paper;directions;state", "pdf_keywords": "multimodal machine learning;supervised multimodal alignment model;multimodal representations;joint multimodal representation;multimodal representation;multimodal feature;multimodal translation research;multimodal alignment;multimodal translation;multimodal data;multimodal machinemultimodal representation;earliest multimodal machine learning models;multimodal fusion;multimodal machine;multimodal translation system;multimodal alignment problem;multimodal fusion approaches;multimodal model;multimodal modalities;multimodal context;multimodal deep belief networks;multimodal sequences;multimodal applications;multimodal image;multimodal space;multimodal version;multimodal signal;multimodal integration;multimodal source instance;late fusion categorization"}, "480d545ac4a4ffff5b1bc291c2de613192e35d91": {"ta_keywords": "more complex network models;network structure;neural network models;computational graph;dynamic declaration;toolkit;dddresearch;researchers", "pdf_keywords": "computation networks;neural network toolkit;deep neural network models;deep learning;neural network framework;computational architecture;deep learning representation;efficient computation graph construction framework;neural network optimizer;tensor network network library;tensor network library;neural network;fast computation;computation graphs;dynamic declaration toolkit;scalable computation;computation graph;programming model;neural networks;tensor networks;dynamic declaration paradigm;model libraries;network architecture;implementations;simple implementations;efficient graph construction framework;efficient computation;dynamical learning network;new toolkit;large scale networks"}, "aeccb1d53e08adcfe271d1e4b08c0a2cdc3c42b4": {"ta_keywords": "novel open information extraction;individual sentences;relation phrases;remine;world corpora;level extractions;entity;distant supervision;sentence;local context;global structural signal;local context signal;objective;joint optimization problem;unified framework;ie;experiments;system;quality", "pdf_keywords": ""}, "824cd8db8a68732db04f4d8b7139eb4475e59ff2": {"ta_keywords": "natural language generation models;natural language generation;new natural language benchmark;natural language benchmark;nlg;benchmark;evaluation;description;goal;challenges;data;intrinsic structure;framework;issues", "pdf_keywords": "natural language generation benchmark;natural language generation systems;natural language generation;nlp tasks;rich dataset generation landscape;machine translation community;new machine translation;novel machine translation evaluation method;generative models;machine translation evaluation method;conversational models;natural language processing;generative data;generative model;neural summarization data sets;response generation;natural language;automatic evaluation;natural language sentences;nlp;multilingual nlp;natural languages;unsupervised summarization;computational linguistics;live benchmark;novel linguistic structures;sentences;text sequences;benchmark;novel evaluation measure"}, "508e9bb13fcb1fa0c4dbac47288e8a3c2487bfc2": {"ta_keywords": "proof tree;generalization;algorithms;number;properties;desiderata;framework", "pdf_keywords": ""}, "7a733a8d8f8649cc07e3ea9091f454ae117573af": {"ta_keywords": "biomedical text;attentionmesh;attention mechanism;annotations;word level;textual evidence;deep learning;end model;novel end;interpretability;model;accuracy;final week;contest;speed;novel masking mechanism", "pdf_keywords": ""}, "abb9b27440719ca44db5947a537fde07f0547973": {"ta_keywords": "kuramoto code;nodes;storage;system parameters;repair;code;explicit code;implicit code;data;feasible values;problem;paper", "pdf_keywords": ""}, "808a9c9dece4c21be50f41e6caf50101f2b24b47": {"ta_keywords": "human preferences;artificial intelligence researchers;decision makers;preferences;experiments;experimental psychologists;testing;empirical properties;computational challenges;models;multidisciplinary research environment;comparative study;particular mathematical model", "pdf_keywords": ""}, "104f75283ae9027eb478e7984bd26b680277ce6f": {"ta_keywords": "text representations;navigation;long sequential action;stochastic;expert actions;agent;unseen instructions;actions;training;challenge;effective methods;scheme;own mistakes;first method;scale;considerable gap", "pdf_keywords": "natural language navigation agent;language navigation;stochastic action sampling;natural language instruction understanding;memory language model;language models;stochastic learningwe;memory;text representations;scalable reinforcement learning;training;unconstrained context;instruction;language model zt;study;long sequential actionwe;level navigation;novel action sequence agent;stochastic;reinforcement learning;training dataset;reconstructed document;expert actions;large class;inference time;unseen instructions;student;language encoder zt;context;trainable projection matrix"}, "f83ef3250ba1166d7c1c7585da7dd78e0641fae7": {"ta_keywords": "generative adversarial networks;rock music;gans;tracks;composer model;drums;piano;jamming model;bass;models;dataset;bars;rolls;network architectures;hybrid model;paper;framework;assumptions", "pdf_keywords": "polyphonic music;music;generative adversarial network;musical features;generative adversarial networks;novel generative model;random vector networks;tracks;deep convolutional gans;music accompaniments;convolutional gans;gans;stochastic generator;composer model;musical performance;random piano;ai cooperative generation;random generator;original track;convolutional neural networks;training data;deep learning framework;joint data generation;models;piano;single track;generator;invariant patterns;jamming model;temporal models"}, "199f383e9acd62649121ccde1e06631ce62c89e9": {"ta_keywords": "secret sharing;general network;communication complexity;network;direct communication links;general networks;dealer;efficient algorithm;information;node;lower bounds;participants;condition;theoretic", "pdf_keywords": ""}, "645bc7a5347a299a1e8aa965867bd097f6f4bddd": {"ta_keywords": "navigation agent;second agent answers;agent model;agent;agents;reinforcement;navigation step;reward signal;language;questions;answers;goal;turn models;environment;substantial progress", "pdf_keywords": "dialogue model;dialogue process;dialogue navigation;human dialogue paradigm;recursive mental model;human dialogues;generic language navigation agent;dialogue exchanges;human dialoguewe;second agent answers;dialogue context;novel navigation agent;agent task;guide agent;simple recurrent strategy;natural language;navigation tasks;agent;dialogue setting;human navigation;human navigation models;language navigation instructions;navigation behavior;agents;next navigation action;language guide;navigation;communication paradigm;next action;language"}, "d00a403028eb0786915dab7a76692e5eeadf60be": {"ta_keywords": "unsupervised transductive transfer learning;protein name extraction;iterative feature transformation;transfer;novel maximum entropy;target domain;training;data;models;process;ift;current state;approaches;technique;comparable performance;problem;art", "pdf_keywords": ""}, "7abcc79e10ff651ef59dea84d347fa64c51e11b2": {"ta_keywords": "organic heterostructures;assembly geometries;axial structures;assembly strategy;assembly;lattice;sequential self;branch;self;simulation;types;rates;approach;wide range;new approach", "pdf_keywords": ""}, "31b3e84f0a66e27c53c7fe403a0c6cd2319ed797": {"ta_keywords": "novel parallel learning framework;novel probabilistic logic;unknowns;problems;framework;thousands;use", "pdf_keywords": ""}, "805e49c7282b847faee048a63c1f43ceb08f5257": {"ta_keywords": "several voting systems;candidates;preferences;system;discrepancy;number", "pdf_keywords": ""}, "641af3bc3cc17993dc72098725d2eb9c0d98049d": {"ta_keywords": "trivial graphs;trivial graph;graph spacing;graph size;graph;density;properties;power law;study", "pdf_keywords": ""}, "84f2cfbc142ad3165ea3bcacd189a3d1110660e0": {"ta_keywords": "scalable segmentation;hierarchical segmentation;clustering;separate encoders;nodes;data;relevant nodes;novel framework;monotone;framework;joint", "pdf_keywords": "attention network;hierarchical attention network;speech recognition;scale speech recognition;attention model;automatic speech recognition;speech recognition system;speech processing;deep neural network;neural network;free speech recognition system;deep learning task;deep convolutional neural network;noisy speech encoders;online speech recognition problem;deep learning model;joint attention;stream attention;attention;advanced signal processing;encoder;convolution layers;convolutional layers;encoders;standard linear encoding model;convolution layer;supervised learning model;speech;relative word error rate;networks"}, "ceefd51b4b391668e313afe8edb3588197002e37": {"ta_keywords": "speech synthesis;conventional speech synthesis;speech parameter sequence;modulation spectrum;synthesis;trajectory;new method;hybrid;method;hmm;new feature;effect;paper", "pdf_keywords": ""}, "2ec99c834bd67ac64ec04b426e5f9fd04f639024": {"ta_keywords": "crowdsourced audio transcriptions;audio transcriptions;crowdsourcing;quality datasets;datasets;data collection;language;principled pipeline;novel domain;challenge;insights;applicability;best practices", "pdf_keywords": ""}, "4375cccdfaf2ce3d013e4129d39f7801ef8a468e": {"ta_keywords": "parallel translation tasks;parallel translation;translation;following tasks;tasks;beijing;6th workshop;results;china;may", "pdf_keywords": ""}, "50a1dd504037463578f6ba8ee40afe4143f3d6fa": {"ta_keywords": "planar sphere;planar spheres;norm;spheres;vector;large class;small class;problem", "pdf_keywords": "bosonic system;einstein condensate;classical phase transition;liquid phase transition;condensatethe behavior;repulsive interaction;particles;condensate;dispersive potential;condensate size;cold atoms;periodic potential;atoms;free electron gas;dynamics;wave superconductors;strong interactions;electron gas;bose;dimensional phase;transition;electron gas problem;symmetry breakingthe interaction;interaction;interactionthe problem;turbulent medium;potential energy;ground state;potential;theory"}, "fac2368c2ec81ef82fd168d49a0def2f8d1ec7d8": {"ta_keywords": "information extraction;entities;connections;context;particular context;framework;tasks;language;variety", "pdf_keywords": "event extraction;event extraction event detection;natural language processing;entity recognition;event extractionwe;relation extraction;relation extraction performance;entity recognition performance;natural language documents;scientific entity recognition;contextual language model;event prediction;natural language discovery framework;entity mentions;span representations;spans;sentences;bert embeddings;text;relations;entity;neural networks;event arguments;spatial context;eventwe;general entity;joint entity;relation;feedforward neural network;tasks"}, "25ee819bc444b02db43fcbeced982c975edee033": {"ta_keywords": "crowdsourced tasks;binary task labels;worker clustering;majority voting;inference algorithm;task types;recovery accuracy;worker;information;problem", "pdf_keywords": "weighted majority voting;worker clustering;binary task labels;task specialization model;binary tasks;crowdsourced data;cluster voting algorithm;worker skill estimation;tasks;task;type worker;recovery accuracy;workers;majority voting;labeling;label estimation;robust clustering algorithm;clustering;cluster;inference algorithm;worker;type specialization model;semidefinite programming;new clustering algorithm;clustering algorithm;standard clustering algorithm;oracle;novel algorithm;type;majority"}, "6b13c4ac18f621155a550238a037a670bdce8969": {"ta_keywords": "zeeman field strength;zeeman field;zeeman;ep cluster;splitting model;stability;field;framework;interplay;suitable choice;effect", "pdf_keywords": ""}, "06d0af396fb08caa6a665dd476380aa16b6199b2": {"ta_keywords": "quantum system;phase space;reversible measurement;qubit;local measurement;system;particles;method;new method;application", "pdf_keywords": ""}, "3e33c988969b4c9f1d9af8c1c0f7644a30d0311f": {"ta_keywords": "speed translation system;speed translation signal;shift sensor;shift sensors;nonlinear phase;sentences;system;novel approach;approach;natural way;design;combination;idea", "pdf_keywords": ""}, "6fb3d5a48be16fe1a4cff5e83093b77fbcd1013b": {"ta_keywords": "smart grid;new privacy metric;private parameter;adversary;smartmeter;data;simulation", "pdf_keywords": ""}, "032e660447156a045ad6cf50272bca46246f4645": {"ta_keywords": "output softmax;machine translation system;translation accuracy;target text;efficient adaptation;bias;speaker traits;adaptation;languages;improvements;particular user;better reflection;user;factored approximation;parameter;experiments", "pdf_keywords": "speaker adaptation process;machine translation system;output softmax;softmax bias;translation accuracy;linguistic accuracy;personal linguistic variations;linguistic features;target words;speaker traits;target sentence;efficient adaptation technique;neural machine;10domain adaptation;linguistic similarities;logarithmic markov chain;several languages;quality representations;translation;speaker;talks;bidirectional encoder;encoder;talk;target wordswe;representations;models;improvements;underlying representation;words"}, "2aad7765250f7d9e312c9382f929ea5239b0fd73": {"ta_keywords": "nonlinear dna molecule;nonlinear dynamics;dna molecule;biological systems;molecule;simulation;dna;like molecules;coupling;bath;method;environment;new method;use", "pdf_keywords": ""}, "a1d578646cf42f2f69ee996742af484d03cc9121": {"ta_keywords": "efficient smp5 language model;machine translation;crosslingual tasks;parallel data;model capacity;performance;larger model sizes;gains;impact;combination;straightforward way", "pdf_keywords": "multilingual neural machine translation;multilingual language models;multilingual encoderde models;machine translation;translation task;crosslingual transfer;mc4 corpus;parallel machine learning model;different languages;parallel data;nmt5;translation;training sequence;mt5;language;language family;tasks;high performance;performance;encoderde models;multi;best performance;model capacity;novel encoder;computational effort;decoder sequence;data;cross;encoder;strategies"}, "0ba3e29dac0857100935b6eb22bce9cee4afcf17": {"ta_keywords": "naive inference methods;global domains;local names;normalization;short queries;wide web;logic;faster way;whirl;data;world", "pdf_keywords": ""}, "cec6de30eea5b4a5a414cf99830fbdb5c56a481c": {"ta_keywords": "scalable architecture;content distribution;storage;network;large scale;vd;data;communication;device;user;goal", "pdf_keywords": ""}, "987658ba918710bbce5de8d92eb44bd127cf72c5": {"ta_keywords": "phoneme mappings;interpretable probabilistic phone;speech recognition systems;differentiable allophone graphs;universal phone;rich pronunciation variations;diverse languages;system;construction;framework;benefits;approach", "pdf_keywords": "phoneme prediction;phoneme mappings;phoneme disambiguations;phoneme annotations;multilingual phone;new phoneme mappings;universal phone transcriptions;specific phonemes;specific phonemic annotations;multiple phonemes;several phonemes;phoneme sets;phoneme rule;probabilistic phone;phonetic realizations;single phoneme;supervised phone supervision;transformational low resource speech recognition model;phoneme level;speech recognition systems;allophone discovery;phoneme;allophones;ambiguous phone;planar phoneme;allophone;uninterpretable transcriptions;differentiable allophone graphs;manifold phone;unique linguistic applications"}, "c0a32c68b992b44f1812492c95ac91fb62a6df37": {"ta_keywords": "competitive gradient;certain online convex optimization algorithms;policy gradient reinforcement learning;bandits;learning algorithms;gradient;smale games;strict saddle points;learning;unstable critical points;algorithms;wide breadth;guarantees;unstable limit cycles;general framework;free settings;full information", "pdf_keywords": ""}, "01138945dc9de691cd559d09a46597cca7659efb": {"ta_keywords": "bias;fairness;data science;data collection;social science;algorithmic advances;management;tutorial;basics;use;future;fields;eye", "pdf_keywords": ""}, "ca7cd3a90d2953b2f8e45686afa3e79eb3a39add": {"ta_keywords": "edit completion;code snippet;past edits;code;learned model;edits;models;higher accuracy;dataset;model;novel approach;available library;approach;state", "pdf_keywords": "code completion;edit completions;code completion program;code edits;program editing;editcompletion task;editcompletion;editcomplewe;additional edits;partial program;edit operations;completion;code snippet;abstract syntax tree;edits;automatic translation;prediction step;contextual code changes;programming language;program;entire program;code;modeling approaches;prediction;syntactic representation;programs;edit function;pointer networks;edit scripts;abstract syntax path"}, "d7bebb71635cb818d2f5e0ca0a70434283deb4b6": {"ta_keywords": "novel regret minimization algorithm;multiagent decision;humanlike behavior;human behavior;actions;person;modeling;accuracy;task;examples;problems", "pdf_keywords": "humanlike policies;reinforcement learning;correct human strategy;reinforcement learning paradigm;strong reinforcement;simple reinforcement learning algorithms;stochastic reinforcement learning model;game strategy;arbitrary games;sparse information games;human game data;determined games;optimal strategies;chess;optimal strategy;human players;strategies;strategy;reinforcement learning approach;underlying strategy;chess player;players actions;strategy changes;imitation;complex strategic planning;typical search strategy;large game;strategy distribution;game theory;novel strategy"}, "4d7a50f6cfd8f27ebd4d5201fad6c5ef42c33733": {"ta_keywords": "hospital mortality predictions;patient information;electronic health records;clinical notes;intensive care unit;ehealth;information;icu;model;time series;sources", "pdf_keywords": ""}, "1cf2e9e198feef3893da2800a7949f6880ddc084": {"ta_keywords": "explainable graphs;leaderboards;different tasks;plot plots;meaningful insights;learning;implementation;summaries;interesting insights;data;different systems;users;form", "pdf_keywords": "computational linguistic performance;natural language processing;future leaderboards;nlp;large corpus;new leaderboards;new leaderboard;standard leaderboards;linguistic text;linguistic structure;structure prediction tasks;nonlinear text;term memory;interpretable evaluation;system prediction;functionality;prediction;software;predictive model;text;systems;novel framework;explainaboard pipeline;useful insights;unified framework;analysis;neural network;novel research platform;ability;performance"}, "5665d864d0f1bce6672d6d2bf9f8d8646093cb37": {"ta_keywords": "semantic parsing challenge;sequences;sequence;numbers;context;system;claim;furious version", "pdf_keywords": ""}, "e050cd9cec5eed73bd56cb2c9726ea85e985384b": {"ta_keywords": "incremental sentence compression;long short term memory recurrent;autoencoder;compression rates;human references;neural networks;art tree transduction models;rsns;better accuracy;network;possibilities;paper;experimental results;state;method", "pdf_keywords": ""}, "63567f348231abed171c02f99d4c49c2892a2ade": {"ta_keywords": "private deep learning;privacy;accuracy;data;small imbalances;model;results;disparate impacts;impact", "pdf_keywords": "differential privacy;differential privacy guarantees;privacy;loose privacy guarantees;exhausted privacy budget;privacy level;privacy expenditure;deep learning models;deep neural networks;deep learning;sgd;deep learning model;scalable digital classifier;logistic regression;neural networks;model accuracy;logistic logistic regression;dataset;models;smile classification;accuracywe;ondeep learning;accuracy;machine learning;predictedwe;representation models;feature;data;model;small imbalances"}, "d558c6b953e0267781ed5da90a35c122ba360f10": {"ta_keywords": "complex text identification problem;parallel version;simple learning models;features;strong baselines;performance;comparative study;future development;art performance;state;area", "pdf_keywords": "complex word identification task;complex abstract abstract word identification;crosslingual dataset;monolingual approach;crosslingual model;annotation;binary classification task;same target language;other languages;simple learning models;common language;classifier;native language;languages;target word;supervised learning algorithm;features;english;rare word count statistics;training data;text;identification models;words;task;similar accuracy;complex words;strong baselines;same datasets;feats;models"}, "d33d6c16d7c34dd387841efca74b457b7e60933a": {"ta_keywords": "recursive logic programs;linear recursive logic programs;simulation;force2;new algorithm;examples;class;technique;set", "pdf_keywords": ""}, "68ca176c7566067ae4b3311957cc4a134bfbc819": {"ta_keywords": "neural cognitive architecture;human cognition;multiple distinct perception;supervision;self;multiple problems;system;problems;multiple noisy sources;thesis;action modalities;number;time;nca", "pdf_keywords": ""}, "163c6b06d948d0869eb8173b537c441c9a786977": {"ta_keywords": "congestion game;public transportation system;population constraints;payoffs;payoff payoff payoff;game;public information service;strategy;simulated version;method;use", "pdf_keywords": "monotone congestion game;stochastic population games;continuous population games;congestion game;mdl congestion games;game equilibrium;underlying multiplicative game;multiplicative game;congestion process;optimal payoff functions;selfish agents;population mass constraints;wardrop equilibrium;equilibrium dynamics;optimal population distribution;reward functions;noncooperative games;stochastic mass propogation equation;equilibrium;reward function;payoff function;simulated seattle ride;incentives;continuous time decision problem;population mass distribution;continuous population;sharing model;discrete stochastic process;markov decision process;multiple rewards"}, "2b63812db40152b12925ce4a848b929fa591b858": {"ta_keywords": "sequences;sequence;data;new method;translation;method;use;article", "pdf_keywords": "statistical machine translation;neural machine translation algorithms;neural machine translation;machine translation system;machine translation;recurrent neural network language model;attentional neural machine translation;dependency machine translation algorithm;machine translation applications;neural network language model;translation model;language modeling;language models;decoder translation model;language model;translation process;linear language models;gram language models;linear language model;predictive inneural machine translation;gram language model;language modeling exercise;sequence models;translation accuracy;recurrent neural networks;recurrent neural network architecture;recurrent neural network;sequence prediction;translation output;neural sequence"}, "8b608ad2ec6d0300b6a0bb8f616d4a2b01150693": {"ta_keywords": "topic tracking technique;meeting event;speech recognition performance comparison;language content;meeting;changes;content;paper;method", "pdf_keywords": ""}, "0a4b8b161931799d5c6bc3ecf07c53bae0e9e502": {"ta_keywords": "high quality text;high quality content;quality;quality fil011 ter;literary acclaim;text;filter;factuality;other sensible metrics;novel method;process", "pdf_keywords": "high quality news articles;low quality news sources;corpus;training corpora;corpora;language ideologies;language models;popular online articles;linguistic ideologies;newspaper articles;document quality;newswire articles;various texts;comprehensive language model;linguistic structure;generic lexical changes;articles;news sources;high quality text;linguistic properties;underrepresented authors;wikipedia;literature;language ideology;literary acclaim;popular internet content;sociolinguistic literature;united states school newspaper dataset;language language;news outlets"}, "7a737872a6693ba3f0c99651191b93dad0dadcee": {"ta_keywords": "ray data;laplacian;data;representation;comprehensive system", "pdf_keywords": "speaker diarization;empirical speech representation;speech models;convolutional neural network;voice activity detection;audio recording;best diarization error;measured speech;discrete speaker diarization;dereverberated audio model;deep learning approach;diarization;network representations;tensor network models;multiple diarization results;speaker;speech;training dereverberation;tdnn;supervised adaptation;supervised learning;snn;network;dereverberation error rates;empirical representation;language;training;linear prediction;speakers;outputs"}, "4d44f2c3f269ea6cbc840b99c3f8119a13829509": {"ta_keywords": "particle beam;polarized background;spin;dynamics;continuum;background;vicinity", "pdf_keywords": ""}, "94c0a8be74d69787a1f3f6e91dcb480a2fd0dd56": {"ta_keywords": "natural language benchmarks;language models;language model;program;theorems;model performance;model;data;architecture;performance;execution result;discovery;paradigm;variety;approach;power", "pdf_keywords": "program reasoning instantiations;advanced reasoning capabilities;various reasoning capabilities;reasoning capabilities;reasoning capability;several reasoning tasks;reasoning capacities;reasoning ability;quantitative reasoning dataset;sufficient reasoning capability;program executors;reasoning procedures thatwe;executor programs;executor programin;programming;program context;language modeling;program programs;program;language models;executor models;program execution optimizer;concrete instantexecutors;reasoning;linguistic representation;several natural language;textual data;mathematical reasoning;executors;programs"}, "de41f897ea6ca5447cfae81e9505f94ccf50e6a5": {"ta_keywords": "heisenberg antiferromagnet;zeeman field;spin;orbit coupling;orbit coupling strength;orbit;field;effect", "pdf_keywords": ""}, "0a4dd1e51616b422aa2d437610dbfbdd3733a114": {"ta_keywords": "novel dialog agent;dialog management techniques;dialog management;human conversation examples;semantic similarity retrieval;statistical machine translation;cosine similarity retrieval;twitter;agent;human;database design;tf;various data;movies;user;approaches;idf;collection;time requirement;paper", "pdf_keywords": ""}, "d5ec188a5a39e504788c1fe33457eeb816a99f31": {"ta_keywords": "unsupervised grammar induction model;structure grammars;smaller grammar size;word concreteness;constituency;structure;structural vision;dependency;heuristic;art models;extension;current state;paper;experiments", "pdf_keywords": "constituency grammar induction;unsupervised grammar induction model;grammar induction;syntactic parsing;structure grammars;syntactic structure;syntax models;natural language processing;word concreteness priors;dependency induction performance;sentence structures;grammars;constituency induction;word concreteness;natural language;parsed tree;phrase levels;neural representations;language modeling objective;dependency structure;sentences;concreteness;syntax;neural model;function words;pure text;neural linear programming;visual representationwe;unsupervised learning;structural vision"}, "d878828c2345b665ab9651f20fb0e60e1ffe9de5": {"ta_keywords": "heterostructures;heterostructure;heterostructure interface;magnetometer;lattice;strains;detection;sensitivity;frequency;sample;method;novel method;custom;use", "pdf_keywords": ""}, "84e566e326b64b105cabf0c47dff336c4f632a1c": {"ta_keywords": "dimensional gaussian particle;harmonic potential;boltzmann equation;particle;harmonics;phase diagram;scalar field;potential;simple model;model;mode;parameter space;region;large number", "pdf_keywords": ""}, "18268bdfc8a6e0a51f373bc4acf65c8b9a7bd6a0": {"ta_keywords": "neural network;particle;noisy environment;prediction;model;concept;state;behavior;presence", "pdf_keywords": ""}, "9e77f94e5a12cb33b8b464dc834fd81da1a609e2": {"ta_keywords": "delay;dynamical system;type functional;dynamics;dds;lkf;type;product;paper;novel approach;use", "pdf_keywords": ""}, "24219135d563b1cb24523bf522366c91a55d7604": {"ta_keywords": "multilabel classification;optimal thresholding;classifier;extreme thresholding behavior;f1;average skill score;properties;alternative", "pdf_keywords": ""}, "274b4ad4840b0a8a70c5bac3fe4b4861ce5fbb95": {"ta_keywords": "shot relation classification problem;relation classification;shot learning methods;shot learning problem;shot learning model;supervised learning approach;art methods;state", "pdf_keywords": "shot relation classification task;shot relation classification problem;relation classification;shot classification tasks;shot relation;shot learning models;shot learning methods;shot learning task;shot learning;new fewshot classification dataset fewrel;fewshot learning;fewshot classification;fewshot learning methods;shot learning model;relation extraction;shot tasks;distant supervision methods;relations;different supervised learning tasks;relation;linguistic tasks;relation structure;distant labeling;few training instances;shot;sentences;new dataset;nonlinear labeling kernel;novel labeling method;dataset"}, "78e838bcd2268260ddce6be6db4907df6f29f04f": {"ta_keywords": "expressive text;text balloons;emotional speech;emotional information;text;static text;anime films;acoustic features;comic books;novel approach;shape;means;system;experimental results;many possible ways", "pdf_keywords": ""}, "66f7d22d6373af5032074b25828331958b07e7f9": {"ta_keywords": "networks;dnns;network;nodes;model;data;new model;parameters;large number;set;behavior;use;good way", "pdf_keywords": "private convolutional neural networks;deep neural networks;convolutional neural networks;privacy;differentialin;interpretability;better privacy;noisy data;dnns;explainability;privacy gap;personal data;models;explanation quality;noise;diffusive character;model;explanations;accuracy;class activation map;importance;features;health;big data;medicine;linear model;cams;class;important features;dp"}, "26a238217321008cd1daaa649683d461e16e7574": {"ta_keywords": "policy gradient training;neural networks;future microwave signal processors;transcription;transcription transcription transcription sequences;transcription sequences;microwave signal;accurate normalizations;text;accuracy;novel method;performance;particular interest;method;design", "pdf_keywords": ""}, "eec490a41bdc716fccf98f4a7996c1d31334985a": {"ta_keywords": "open source library", "pdf_keywords": "symbolic music generation;symbolic music;music generation systems;music generation system;music database;music generation;music structure;music;datasets;various datasets;large datasets;dataset perplexities;data preprocessing;generative representation;muspy;larger datasets;lmd maestro nes music library;empirical data;lmd maestro nes music21 jsb music21jsb wikifonia;piano;novel representations;smaller datasets;open source python library;supervised fashion;data set;machine learning models;data;open source library;different data types;library"}, "dc26c3775d233a5fa9516d21fee12aa5b46f8a25": {"ta_keywords": "unannotated scientific papers;scientific term extractor;knowledge graph;unannotated papers;unsupervised relational signals;scientific terms;recommendation;novel approach;large collection;paper;minimal use;large amount;state;way;small amount;art performance", "pdf_keywords": "relation extraction;scientific knowledge graph;relation extraction model;scientific information extraction;relation extraction paths;knowledge bases;knowledge graph;knowledge base completion tasks;entity recognition;scientific term extractor;underlying knowledge base;knowledge base;natural language processing;automatic annotation;knowledge graph construction;structured knowledge base;neural sequence tagging;neural tagging model;unannotated papers;unannotated data;modeling relations;natural language;dependency relations;downstream scientific recommendation tasks;underlying knowledge structure;unstructured text;annotated data;deep links;unsupervised approaches;wikipedia"}, "ef2e2f3a847667000b591c8708b543eaf259113b": {"ta_keywords": "threshold speech recognition problem;distant recognition challenge;hybrid beamforming;language model;bidirectional;distance improvement;shallow state;previous state;art system;system;combination", "pdf_keywords": ""}, "b176a46ec214b9f75df751dcd2c894f0a7a72a9a": {"ta_keywords": "unlabeled discourse;debate portals;unlabeled data;unlabeled data evaluation scenarios;supervised learning;clustering;several baselines;domain;novel features;approach", "pdf_keywords": ""}, "83145b7a391b792e24d8d38f74ed6b6ae7a149dc": {"ta_keywords": "machine translation system;machine translation systems;aware word dropout;lexical cohesion;context;context usage;contrastive datasets;new metric;metric;aware models;amount;method;experiments;new method", "pdf_keywords": "aware machine translation models;neural translation model;neural machine translation;neural machine translation system;generative machine translation framework;wemachine translation models;level machine translation;translation system;translation process;target sentences;source sentences;target context;translation time;higher lexical cohesion;extra context;translation;lexical cohesion;context;context usage;translation problem;translation quality ofwe;current source sentence;sentences;target sentence;aware models;context usage bya;side context;contrastive networks;automatic metrics;higher context sizes arewe"}, "45eea76ac46b402f3a209de57e469275419fdc9e": {"ta_keywords": "random graphs;dimensional gaussian states;gaussian states;dimensional gaussian state;gaussian state;graph;single node;large collection;linear combination;method", "pdf_keywords": ""}, "94a11c9425bf5f4f9b8ed1b07ea1d15a81b96e9f": {"ta_keywords": "crowdsourcing applications;crowdsourcedin;online crowdsourcing;mechanical turk;open source community;software development projects;crowd participation;software;large datasets;software industry;community;recognition;image transcriptions;knowledge;services;contributions;development;scale tasks;research;audio files;intrinsic incentives;acquisition backend services;use;contexts;image;identical subtasks;modularity;paper;variety;iterative design", "pdf_keywords": ""}, "222ae836430ad0c922b47a9345c17212f9584097": {"ta_keywords": "oforaloraloraloral diseases;gingival smile;inoral diseases;oforal gradients;novel technique;lip;strongoral gradient;surgery;aoral region;microwave;technique;efficacy;theoral region;prevention;young female;gggg;case study;generation;establishment;lrs", "pdf_keywords": ""}, "723770d9ac418e923db5e087ae18c04702f5986e": {"ta_keywords": "new rule learner;building learner;benchmark problems;art rule;lower error rates;slipper;state", "pdf_keywords": ""}, "b9ede62d1d586e1a3b1ef7ec046f09e4e35639bf": {"ta_keywords": "effective retrieval;quadratic term approximation;data;new method", "pdf_keywords": "nearest neighbor search;effective retrieval pipelines;similarity search;featureless quadratic search engine;nn search;quadratic search engine;efficient nearest neighbor algorithms;benchmark search engine moses;query terms;search;search engine;high dimensional documents;large databases;index;queries;similarity measures;unstructured corpora;document documents;document storage;similarity;word embeddings;several document storage;candidate documents;dimensional lexical objects;similarity function;first story detection;text library;best document;quadratic similarity function;clustering"}, "6dafc41e9bbd3aa476a0a1c15ca2c459eaef6b98": {"ta_keywords": "inflection model;inflection;lemma overlaps;models;test split;usual lemma;sloan;train;systems;experiments;method;new method;task;generalized capacity;problem;efficacy", "pdf_keywords": "splitting languages;morphological inflection;inflection prediction;morphological datasets;linguistic inflection;language models;lemma characters;test splits;skew computation;new split;linguistic output;languages;splitting;split;random splits;splits;inflection structure;hard split;inflections;inflection;standard split;machine learning;learning model;lemmas;languageswe;pattern characters;original split;generalization capacity;models;lms"}, "21066ab388b386f3d3552a4a4c25322e0ca69632": {"ta_keywords": "hypernym extraction;word embeddings;hypernyms;projection learning;regularization;extraction;word;results;explicit negative examples;model;accuracy;performance;new approach;quality;art approach;state", "pdf_keywords": "hypernym extraction;hypernymy extraction;word embeddings;hypernymy relations;hypernymy prediction task;distributional word vectors;other relation extraction tasks;gaussian word embeddings;hypernyms;hypernym vector;projection learning;lexical coverage;language processing;hypernymy;semantic similarity;explicit negative training instances;large corpus;corpus;descriptional models;hyponym vector;semantic features;vocabulary;relations;regularization;hyponyms;relatedness;rich language;synonyms;neural network;generative model"}, "900ce63d71dce47059434cdf2d5e1d77bc716e8d": {"ta_keywords": "spin dynamics;electron gas;spin;zeeman field;matter physics;fascinating subject;effect", "pdf_keywords": ""}, "8ae392fc9acbada67a4288a6affc2a77f83befcd": {"ta_keywords": "variational autoencoder;neural machine translation;autoregressive node;text;input text;vq;shape;novel method;components;hong kong;paper;method;taiwan;taiwan university;experimental evaluation", "pdf_keywords": "variational autoencoder;neural vocoders;generative sequential wave models;encoder models;source e2e speech processing toolkit;novel autoregressive network;autoregressive transformer;encoder;neural machine translation;speech synthesis;output speech;vertex encoder;gan;generative adversarial networks;input speech;nmt models;term memory;resource speech challenge;vae model;speech corpus;nmt model;nonautoregressive vector;generative joint signal;vae;vq;speech;end text;models;loss;discrete symbol sequence"}, "317d95f99ef62237f6c7d7834d1d19027166b392": {"ta_keywords": "languages;e2e;new generation;performance;naturalness;systems;terms;first results", "pdf_keywords": ""}, "db190db2567c334b772fd653dca10f300074e421": {"ta_keywords": "speaker adaptation;automatic speech recognition;target speaker;quality training data;asr;training;accurate tasks;end;systems;method;novel method;recent advances;field", "pdf_keywords": ""}, "02cbb0db288af2c83b48a023f245812bd22a2408": {"ta_keywords": "semistructured data;text models;accurate tables;table;literature;text;books;novel method;parent;high precision;cambridge library;parents;cambridge;method;peer;ones", "pdf_keywords": "natural language prediction;automatic text generation;text models;unstructured natural language descriptions;natural language generation;automatic evaluation;unstructured text;sentence quality;fewer lexical items;machine translation;novel evaluation metric;information extraction;sentences;ideal target text;knowledge extraction;reference text;novel evaluation method;source text;entailed items;text;neural generator;evaluation;tables;divergent references;reference texts;manuscript generation;neural generator generator;human judgments;table;evaluation method"}, "20937a0f03bcb845afbedda901a6d4e93a2b5c34": {"ta_keywords": "probability distribution function;probability distribution;distribution;probability density;method;new method;analysis;account", "pdf_keywords": ""}, "b8e2e764ac82f81a5bc645c818d0d5ad7806e806": {"ta_keywords": "entanglement;quantum information processing;particles;discovery;interaction;classical world;background medium;new type;state;art;presence", "pdf_keywords": ""}, "7c976b0b54ace7d13b87e8feefe6f29c0599d78d": {"ta_keywords": "semantic word network;lexicical ontology datasets;distributional semantics methods;other relation extraction methods;available dictionary resources;hierarchical contexts;relations;russian language;wfn;datasets;recall;f1;method;paper;score", "pdf_keywords": ""}, "a83bbc7bf70b1beedbfe0140d24d556e2dc5acc8": {"ta_keywords": "gaussian noise;gaussian random phase;random phase;diffusion;disordered medium;particles;medium;effect", "pdf_keywords": ""}, "2aaf2ee779cd4ff0f26bb73958ea9fb0faa61907": {"ta_keywords": "fluorescence microscope image;binary classification;fluorescence;fluorescence distribution;fluorescent;classifier;graining;images;dataset;image;features;protein;novel method;combination;method;use", "pdf_keywords": ""}, "08f199ebfd27a5f9ada79edd07ac41e46c7278d5": {"ta_keywords": "social interaction;noise;verbal communication;early time;equation;fact", "pdf_keywords": ""}, "34d5d2f75934caff89311ef20d18a275da5abb47": {"ta_keywords": "orbit coupling;orbit coupling strength;spin;zeeman field;effect", "pdf_keywords": ""}, "fbf2a6a887ea92311cf207d522c535daf867a6ba": {"ta_keywords": "large corpus;corpus;text;discrete representations;models;translation;data;original data;literature;baseline;qualitative analysis;quantitative comparisons;performance;means;end;same type;set", "pdf_keywords": ""}, "da06caf4f340ebc81395f092f9dc3a3101827506": {"ta_keywords": "electrollary speech;electrollarylarylary control method;electrollarynx;statistical excitation prediction;face conversation;listenability;direct control method;multiple speakers;intelligibility;method;intel;naturalness;significant improvements;experimental results;paper", "pdf_keywords": ""}, "acc2ad56a9c68c799747e08d978f9803997c1527": {"ta_keywords": "perovskite compounds;perovskite materials;novel perovskite compound;inorganic materials;synthesizability screening;syntheses;only training data;precursors;model;nonlinear equations;novel method;data;behavior;set;literature;decade", "pdf_keywords": "materials synthesis;materials synthesis planning;materials discovery;novel materials;entity recognition model;language models;precursors;available precursors;synthetic synthesis routes;synthesis planning;binary oxide precursors;synthesis;likely precursors;new organic materials;synthesis routes;perovskites;synthesis actions;organic synthesis;precursor selection;word embeddings;materials systems;materials;natural language text;stable perovskite material;generative model;materials design;transition metal dichalcogenides;organic semiconductors;elements;chemical properties"}, "2aea6cc6c42101b2615753c2933a33e57dd665f2": {"ta_keywords": "large scale knowledge base;knowledge base;knowledge base graph;many inference tasks;inference;new beliefs;learning;web;new approach;novel approach;random walks;approaches;rank;precision;approach;nell", "pdf_keywords": ""}, "1134ec4cdfc1c2161d157b0f4e03dec85d8c4c8a": {"ta_keywords": "deep convolutional networks;irrigation canals;canals;road connectivity;connectivity;aerial images;roads;network;false positive roads;loss function;loss;unwarranted disconnections;background regions;standard road benchmarks;like structures;output;new data set;novel;experiments", "pdf_keywords": "road network reconstructions;deep network;deep convolutional networks;deep networks;deep learning;connectivity loss function;level convnet;road delineation;road maps;drainage canals;connectivity;connectivity loss;roads;network;proper connectivity;uniform network;canals;simple linear network architecture;aerial images;binary segmentation;global connectivity;road;artificial road;network architecture;loss function;images;recurrent architecture;sensor network;binary mask;segmentation"}, "b9b83860bc0d79b3b629b3035c4b7b7f9f71b5af": {"ta_keywords": "deployment algorithms;propagation;forest trail;algorithms;markov process;forest;performance;statistical model;measurement;raw measurement data;data;experimental experiences;strategies;measurements;ms;terms;one;amount", "pdf_keywords": "optimal deployment strategy;wireless relay nodes;simple deployment algorithm;conventional deployment algorithm;relay placement;deployment algorithms;sequential wireless relay;relay location;sensor network;wireless networks;wireless sensor networks;wireless sensor network;network search;wireless sensor;network scenarios;stochastic optimization problem;generic sensor network;generic sensor networkswe;optimal placement;deployment agent;network performance;small scale deployment;relays;deployment;easy deployment;network parameters;wireless link;stochastic;few relays;single relays"}, "386bfd0e411dee4f512a8737c55dd84846981182": {"ta_keywords": "joint learning;tabular data;simple tabular representation;prediction tasks;table;simple pretraining objective;data;tasks;model;suite;approaches;set;use;art performance;goal;state", "pdf_keywords": "tabular representations;unstructured table;table substructures;tables;training tables;table;tabular table;column type prediction;art tabular tabular tabulation solvers;specific tables;joint table identification;benchmark table;language modeling objective;structured data;prediction tasks;tabbie;associated text;rows;columns;novel probabilistic classifier;downstream table;same table;joint learning;cell relationshipswe;financial tables;cell;reasoning tasks;corruption cells;corrupted text;semantics"}, "466865aaeb8902f6f8ed93ceeb5fbf9fc8b593b1": {"ta_keywords": "online speech enhancement;deep learning;deep neural network;nonlinear filtering;nlnl;frame;nldw;input signal;node;system;novel combination", "pdf_keywords": "online speech enhancement;speech enhancement task;speech enhancement;deep learning;deep neural network;latency enhancement system;online beamforming;low algorithmic latency;additional algorithmic latency;deep nonlinear interaction;reverberant speech;dynamical neural network;online enhancement;algorithmic latency;new nonlinear network;deep virtual beams;neural networks;better enhancement;nlnlnl;nlnl;inelastic nonlinear dynamical network;frame prediction technique;full nonlinear beamformer;encoder;nlt architecture;frame;stft domain;enhancement;networks;nonlinear dynamical network"}, "4759aaacd71fbb2b5ca253aa13ccceac0bc7fe8a": {"ta_keywords": "computational argumentation;arguments;argument pair;argument;convincingness;natural language;explanations;26k explanations;certain controversial topic;qualitative properties;task;strengths;experiments;article;other one;flaws;pair;order", "pdf_keywords": ""}, "2cf21fc85af45512bf34d710f325872dca8a5331": {"ta_keywords": "traffic conditions;traffic;local density fluctuations;temporal distribution;city;prediction;local extremal conditions;spatio;area;model;method;assumption;new method;paper", "pdf_keywords": ""}, "fb0a68981dae15f31cbcf5442509a3b8279b264c": {"ta_keywords": "robust speech recognition;noise features;dimensional feature space;dimensional feature spaces;vector;multi;method;novel method;paper", "pdf_keywords": ""}, "4d41c2fa74dd018e39ddb3cbbfead1b42615612c": {"ta_keywords": "deep network;deep learning model;deep learning;benchmark network;diverse networks;benchmark benchmark network;network;parameters;model", "pdf_keywords": "deep learning pipeline;deep learning pipelinewe;deep network benchmark;deep nets;deep learning architectures;neural network parameters;deep learning network;neural parameter prediction models;deep network representation;deep learning models;deep learning;deep learning networks;deep learning paradigm;deep networks;graph hypernetworks method;deep feedforward;deep learning systems;neural architectures;rich hypernetwork;convolutional network;dense network representation;dense graph networks;imagenet;feedforward networks;graph networks;neural networks;neural models;parameter prediction task;feedforward neural networks;network parameters"}, "709f0a4229e40339b595072ae9fbd3a1ae1fd93e": {"ta_keywords": "dynamic neural network toolkits;dynamic neural networks;batching;batch;layer;computations;use;novel method;principle implementation;method;proxy;proof", "pdf_keywords": "batching algorithm;batches;batching;batch;computation graphs;single instance computations;parallel architectures;smaller batch volumes;structured networks;efficient algorithm;more nodes;efficient machine learning models;processing tasks;lazy evaluation;lightweight interpreter;scalablewe propose;convolutional architectures;minibatches;graph construction;neural network representation;computations;nodes;algorithms;computation;algorithm outperforms;parallel;operations;neural network;neural networks;algorithm"}, "d745ba895cf8dcba5670fb01feea931fc72f9c77": {"ta_keywords": "reinforcement learning;reinforcement;more complex tasks;complex tasks;agents;transfer;agent;experiments;performance;benefits;problems;set;source;structure;state", "pdf_keywords": ""}, "af6c6e66fe0a9ba19c304665e01db1c5a5fba1e4": {"ta_keywords": "robust reinforcement learning framework;stochastic decision processes;reinforcement learning;decision maker;underlying decision process;decision making;transition kernel;policies;constraints;algorithm;situations;framework;simple example;paper;assumption;powerful technique", "pdf_keywords": ""}, "3389b6b8ee5a1ef0395df9f383e771650087b828": {"ta_keywords": "domain experts;domain expert fashion;domain expert advice;domain domain;information;systems;users;queries;direct references;efficacy;rigorous manner;answer;paper", "pdf_keywords": "domain expert systems;domain expert response quality;classical information retrieval;information retrieval tasks;document retrieval systems;authoritativeness score;information retrieval approach;expert quality responses;document ranking;information retrieval framework;document retrieval;natural language processing;language models;natural language;natural language sources;unstructured web pages;language model;document learning;deep relevance matching;comprehension systems;retrieval;long document models;text corpus model;unstructured knowledge graphs;domain expert advice;corpus models;sophisticated document;task learning;high quality responses;knowledge"}, "f432d10677897cf72f9594ba7bd4c9199b270fb3": {"ta_keywords": "performance measures;classification applications;desirable properties;systematic analysis;properties;list;impossibility theorem;new family", "pdf_keywords": "classification performance measures;binary classification validation measures;binary classification measure;binary classification measures;classification evaluation;binary classification evaluation indices;validation measures;binary classification evaluation indices f1;multiclass averagings;classification results;classification;binary classification;high accuracy measures;random classifier;many performance measures;many evaluation measures;label accuracy;cij bi bi validation measure;binary classification system;cij fimbi validation measure;class imbalance;binary measures;classification problem;balance accuracy;image classification;true labels;labelings;several popular measures;binary labels;performance evaluation"}, "da9ec5053c8ad8854bdd2ddc3f9c3d82a4114d71": {"ta_keywords": "languages;recognition error rate;algorithms;data;scarce setting;machine;systematic analysis;training", "pdf_keywords": "language correction system;language documents;language text;textual data;endangeredthe preservation;language document;scanned books;scanned book;language texts;language transcription;endangered science;unannotated pages;ocr system;word recognition rates;text corruption;machine translation;scanned images;improved postcorrection method;language books;text;transcription correction system;documents;correction system;annotated version;recognition error rate;recognition;translation encoder;word error rate;languages;manuscript"}, "f66c82ca087b435463ef4fa0de49825c4eb55885": {"ta_keywords": "novel probabilistic parser;semantic parsing;free parser;atis corpus;general probabilistic context;data formalization;free probability estimation;context;system;article;method;novel method;practical applications;different domain", "pdf_keywords": ""}, "b37d073109cfcf913cf53aded3872e6158e828a0": {"ta_keywords": "expert models;multiple modalities;models;language;vision;different modalities;prediction time;lab;novel approach;set;performance;ability;access;procedure", "pdf_keywords": "language navigation tasks;visual experts;visual agent;navigational strategies;visual inputs;several modelsnatural language annotations;visual goal;vision;visual features;neural machine translation;deep reinforcement learning;object detections;expert models;visual instructions;novel training scheme;navigation instructions;path learning process;visual goalwe report;representations;training;experts approach;prediction task;grounding;specific language;multiple decoders;models;representation;agents;single decoder;agent"}, "e2ef0dc26a669ed764e2d70257b162298b8b608e": {"ta_keywords": "secrecy rate;multicast;front radar;physical layer design;physical layer design approach;dfrc;communication;rate;problem", "pdf_keywords": "optimal secrecy rate;radar channels;multiple radar receivers;radar network;multiple radar targets;simplified radar network;radar;conventional channel security technique;wireless physical layer security;secrecy rate;output radar;physical layer security techniques;radar links;simplified radar networkwe;radar radar reference;multiple orthogonal frequency;physical layer securitywe;nonlinear parametric radar;security;physical layer design approach;novel physical layer design approach;target detection device;input noise layer;intermediate channel;channels;target detection;channel;optimal solution;physical layer;multiple users"}, "243880fde63abfc287bd1356c2e1dbf68a1a0aac": {"ta_keywords": "languages;rapid development;technology;development;challenges;opportunities;country", "pdf_keywords": "language technologies;effective linguistic technology;machine translationwe;multiple possible spelling arrangements;dictionaries;computer language learning;multiple languages;actual dialectal texts;machine translation;computational linguistics;languages;resource languages;computational linguistics system;language;keyboard technology;novel computational linguistics system;dialects;corpora;spell;text;endangered languages;noun morphology;ocr;indigenous languages;novel learning tools;character recognition;multiple text pages;computational libraries;document;speech"}, "9364eff879a9dcb34fe3dfdd0843e69c14dd333b": {"ta_keywords": "interpretability model;interpretability;model;iterations;optimization loop;loop;features;several datasets;approach;humans;hybrid approach;number;new method", "pdf_keywords": "interpretable machine learning;interpretable models;interpretability;optimizing models;machine learning;interpretation;models;bayesian learning;artificial intelligence;natural models;neural networks;prediction;training objectives;synthetic datasets;natural language;predictions;model complexity;intelligent optimization approaches;model;supervised learning experiment;model class;description;performance decision trees;artificial data;decision trees;novel feature importance kernel;datasets;l1 penaltywe;proxy models;high accuracy"}, "64c575bb8b3e11097605028de5c289b0b2d839a4": {"ta_keywords": "synthetic speech;speaker individuality;target speaker;naturalness;individuality;method;experimental results;paper", "pdf_keywords": ""}, "5af9ab65d186e4e1e0b1cef1962ca15336f37931": {"ta_keywords": "dependent semantic parsing;semantic parser;influential semantic utterances;corpus;new corpus;semantic data;context;simplicity;fact;ease;use;novel method;ones", "pdf_keywords": ""}, "4f8e1a4247ce06a15760fc2692c6849601d41b6f": {"ta_keywords": "entailment models;underlying knowledge graph;contextual subgraphs;multiple textual datasets;breakingnli dataset;semantic information;personalized knowledge;external knowledge;subgraphs;convolutional networks;graph;prediction accuracy;model;use;noise;approach", "pdf_keywords": "natural language inference;multiple textual entailment datasets;underlying knowledge graph;entailment models;conventional entailment model;knowledge graph;natural language data;knowledge graphs;large relational networks;external knowledge sources;contextual subgraph;contextual subgraphs;joint entailment;contextual graph;natural language processing;nlp models;external knowledge;graph representations;underlying knowledge structure;natural language;semantic information;relational graph;large corpus;pagerank;subgraph filtering;general knowledge structure;standard knowledge sources;lexical content;semantics;nlp"}, "9cf75483deee77b3c0ee4f996d808437ab4a7435": {"ta_keywords": "thermal gelation process;thermal gelation;thermal gel;gelation process;gelation;thermal fluctuations;fluctuations;process;kinetic energy;competition;small number;number;consequence", "pdf_keywords": ""}, "0ce184bd55a4736ec64e5d82a85421298e0373ea": {"ta_keywords": "text;phase;sequence model;symbols;sequence;novel approach;problem", "pdf_keywords": "new attention encoder;decoder attention;encoder;decoder networks;automatic speech translation;end speech recognition problem;encoder architecture;intelligent encoders;attention;speech applications;neural network architecture;new encoder;conventional recurrent;conventional speech recognition paradigm;speech signal;neural network;rnn;sequence model;attenuater encoder;novel speech discovery problem;art encoder;source sequence;standard speech discovery problem;speech discovery problem;encoder output;novel sequence;unstructured filterbank features;target sequence;sequence;speech"}, "448e15e267b20bee1644034e18630da2e68cf36e": {"ta_keywords": "strong magnetic field;field;time;space", "pdf_keywords": ""}, "4c94dc1b2391d78c9cfdd69955d20b56d7a16982": {"ta_keywords": "convertible codes;code redundancy;optimal linear mds;linear mds;conversion;access cost;valid parameters;explicit construction;new class;access", "pdf_keywords": "optimal convertible codes;systematic linear mds code;convertible code;convertible codes problem;storage code;storage codes;convertible codes;linear mds;efficient storage systems;linear code;optimal conversion;optimal access;arbitrary code;systematic linear fdm code;matrix multiplication;code construction;conversion efficiency;minimum access cost;conversion cost;storage;access cost;single code;stripe code;minimum possible cost;pure code;original code;multiple copies;generalized split;sequence recovery code;conversion procedure"}, "c55d5805a6eb8b1482f21581fe893484eaf9ffb5": {"ta_keywords": "regression mixture model;singing voice;voice conversion;singer;age;model;version;novel method;problem;paper", "pdf_keywords": ""}, "4275d4c4bd10742b321467f175f16198ed7d17d7": {"ta_keywords": "noisy midi files;generative adversarial networks;coherent music;composer model;jamming model;human inputs;models;bars;network architecture;hybrid model;underlying model assumption;domain;extensions;scratch", "pdf_keywords": ""}, "802ddaf5bd731b91e64d8cee43f7fb614b42c1df": {"ta_keywords": "nonlinear optical oscillator;nonlinear optical field;quantum emitter;nonlinear dynamics;nonlinear interaction;quantum;electric charge;device;new method", "pdf_keywords": ""}, "75f90cbbf3c27a8b27567d6a9c8c4538743c8fff": {"ta_keywords": "text generation tasks;extensible toolkit;tensor;toolkit;texar;text;network models;diverse tasks;powerful tool;common patterns;variety;versatility;methodologies", "pdf_keywords": ""}, "7e386158f474a395618c5e065ac55844b507007c": {"ta_keywords": "speech processing stage;models;model;novel framework;performance;universal stage;phase;framework;task;art performance;top;results;novel;space", "pdf_keywords": "speech processing tasks;speech representation;available speech dataset;speech representation encoders;speech processing;generalizabilitywe introduce speech;lightweight prediction heads;speech system;deep representation;speech;deep representations;universal benchmarks;discriminative modeling;supervision tasks;different pretraining schemes;generative modeling;unsupervised learning;multitask;deep bidirectional transformer encoders;models;learning framework;training;tasks;representations;model;wav2;performance;synthetic data;representation;other popular representations"}, "9976ed0d88a4156ecdd3ebe39714c5fb4a5a0246": {"ta_keywords": "covariance matrix adaptation evolution strategy;spontaneous speech recognition;large vocabulary speech recognition systems;slacius;high accuracy;services;novel approach;design;approach;experiments;state;art", "pdf_keywords": ""}, "814421bb20ba1fba88928fc168db1b7175cca6ac": {"ta_keywords": "harmonic trap;collective motion;particles;particle;dynamics;motion;moves;random fashion;dependent density;time;states;critical value;investigation;basis", "pdf_keywords": ""}, "e8bd03ff376ab3c863f72f931c91e90eeb9b2be9": {"ta_keywords": "spin chain;spin;orbit interaction;magnetic field;effect", "pdf_keywords": ""}, "36c95e3ef362742a5c1844257e8b79d3251a781e": {"ta_keywords": "robotic language understanding;visual language;3d objects;novel reasoning task;objects;natural language description;vision;several rnare;models;language;new benchmark;model;recent advances", "pdf_keywords": "object representation;robotic language understanding;visual objects;new benchmark shapenet;novel 3d object model;visual language;captioning task;3d objects;tabletop referent objects;natural language description;novel reasoning task;object views;language models;referent objects;natural language representations;object evaluation task;language model;objects;predictive language model;future taskswe;object expression;mathematical representations;deep bidirectional automatic image;underlying 3d model;vision;object;robotics;visual grounding;representations;promising representation"}, "807e421679d4a9d629d2fad1f60f28787dca60e7": {"ta_keywords": "training question answering models;generative questions;generative model;novel domain adaptation algorithms;questions;reinforcement learning;novel training framework;data distribution;data;unlabeled text;human;distribution;model;problem;discrepancy", "pdf_keywords": "question answering models;adaptive generative;unsupervised domain adaptation;domain adaptation;paragraph prediction;novel domain adaptation algorithms;generative models;novel domain adaptation method;domain adaptation framework;recurrent network;generative model;question representation;generalized autonomous autonomous quantum system;paragraph representation;domain adaptation techniques;reinforcement learning;reinforcement learning framework;domain adaptation method;modified generative model;learning;deep learning;paragraphwe;adaptive learning;novel training framework;discriminative model;supervised learning models;gans;deep network;questions;text"}, "d06493373421c86ba33dbb8834ccb725105a665f": {"ta_keywords": "lexical distinctions;vocabulary items;distinctions;word;language;new language;meaning;means;method", "pdf_keywords": "lexical selection;lexical distinctions;word sense disambiguation procedure;language learning tasks;lexical choices;lexical selection model;lexical selection rules;lexical choice;language learning;word alignments;language learner;semantic subdivisions;vocabulary words;parallel corpus;focus words;corpus;different word classes;different lexical manifestations;vocabulary;language acquisition pipeline;lexical manifestations;linguistic specificity;python natural language processing toolkit;novel language learning approach;natural language;simple language;sentence selection;l1 language;spanish;concise descriptions"}, "ece62ada00cef99d9fc7a60e7d4b773f6d87c8f9": {"ta_keywords": "entanglement;single level system;level systems;level system;efficient generation;system;novel method;method;environment;set;use", "pdf_keywords": "aware machine translation system;multilingual speech data;machine translation system;parallel machine translation tasks;machine translation;best translation model;tasks machine translation;supplementaryneural machine translation;bilingual word2 model;scalable translation model;translation model;multilingual data;human language technology;new morphological analyzers;next translation model;language synthesis;corpus;target language;computational linguistics;recent bilingual extension;scale speech models;human language;incident languages;gram systems;rich linguistic representation;cnn monolingual corpora;supplementary language packs;language languages;natural language processing;neural machine translation"}, "73635c9dc0ffb61c2eac79234108c6eee1362c1b": {"ta_keywords": "rewards;stochastic model;time bounds;algorithms;new algorithms;methods;performance;formulation;new formulation;fashion;effect", "pdf_keywords": "dynamic bandit problem;stochastic bandit problems;bandit algorithms;stochastic bandit algorithms;stochastic bandit model;stochastic bandit algorithm;traditional bandit algorithms;bandit algorithm;bandit problems;traditional bandit algorithm;bandit problem;new bandit algorithm;restless bandit problems;stochastic bandits;bandit research;bandit chain;bandit environment;stochastic random rewards;stochastic reinforcement learning algorithms;state bandits;reward models;smoothed reward feedback;observed rewards;individual bandits;smoothed reward;large reward feedback;reward feedback strategies;stochastic reinforcement learning approach;trivial reward feedback;stationary reward"}, "18289b2b04fc8a7a86f474236e55a3b1070a98ad": {"ta_keywords": "electron gas dynamics;gaussian noise;dimensional electron gas;strong magnetic field;dynamics;field;effect;presence;terms", "pdf_keywords": ""}, "db500c4e746897e5d5adafbf222b959c512445ad": {"ta_keywords": "novel data poisoning attack;adversary;trigger phrase;model predictions;sentiment models;several popular data models;defenses;attack;language modeling;input;cost", "pdf_keywords": "adversarial attack;adversarial part;adversarial attacks;adversarial behavior;adversarial setting;data poisoning attacks;new data poisoning attack;adversarial settingwe;adversarial adversarial setting;dangerous new vulnerabilities;machine translation;simple poisoning attack;data poisoning;adversary inserts;poisoning methods;attacks;language modeling;novel vulnerability;adversary;gradient descent;poison examples;poisoning;poison attack;nlp;sentiment predictionwe;text generation;trigger phrase;target language;model predictions;sentiment prediction"}, "00936aa7c8f64fc919dd4dcee6192ccc83e0d26e": {"ta_keywords": "radiography;reflectance devices;goldstone transform;interproximal lesions;lesion depth;transillumination;goldstone;teeth;diagnostic performance;pseudo pseudo pseudo;wavelength;pseudo;devices;microct;study;ability", "pdf_keywords": ""}, "58b0800ef48da2678e15e5e8bc1d786e24190742": {"ta_keywords": "field speech enhancement;automatic speech recognition;microphones;deep learning;end signal enhancement;recognition;more speakers;recent advances;extraction;field;latest advances;separation;asr;distances;major breakthroughs;focus;back;special issue;end;article;topic;summary", "pdf_keywords": ""}, "901fbb51d6fb9078e572c83a446b408da4de9b2b": {"ta_keywords": "", "pdf_keywords": ""}, "d0a58b6da9f7788534aa9963a78c24a87038e4fc": {"ta_keywords": "social choice;best paper;aggregate mapping;community;reviews;paper;only choice;natural axiomatic properties;collection;conference;approach;task;problem;framework;ijc anomaly", "pdf_keywords": ""}, "490c31b460316b7f68e9b8f5ff0d26aef2f7f45f": {"ta_keywords": "game equilibria;optimal strategy;stochastic dynamics;stochastic version;sba conjecture;conjecture;sensitivity;sba;existence;impact;magnitude;use", "pdf_keywords": "stochastic congestion games;stochastic optimal games;congestion game problem;optimal congestion;optimal games;stochastic optimality;optimal strategy game;optimal state action;mixed game dynamics;optimal dynamics;actionwe study stochastic optimality;style game equilibria;congestion;stochasticity;stochastic user equilibrium assignment model;optimality;optimal optimal trade;stochastic version;pure games;theorems ofwe study stochasticity;robust optimal trade;stochastic trajectories;optimal lagrange;hop game;deterministic function;action probability distribution;stochasticwe;mdp;games;optimal cost"}, "2e0b1484740047d6d6fb6bd2c9d8816b54b33811": {"ta_keywords": "neural networks;training tools;learning process;biological sciences;unbiased review;evaluation;performance;data;set;tasks;development;topic;recent availability;large amounts;paper;interest;new perspective;goal", "pdf_keywords": "reviewers scores;reviewers;junior reviewers;more reviewers;volunteer reviewers;senior reviewers;student reviewers;other reviewers;review network;review process;rating scale;reviewerswe;fifth annual european conference;reviewer profile similarity;reviewers andwe;international conference;ordinal rankings;review;future conferences;recent ranking;reviews;submissions;review processwe;nips papers;area committee;student reviews;top 2k papers;conference;evaluation system;scientists"}, "2444be7584d1f5a7e2aa9f65078de09154f14ea1": {"ta_keywords": "distillation tasks;neural networks;networks;language modeling;small clusters;essential features;vision;tasks;experiments;ability;presence;framework", "pdf_keywords": "memory networks;model compression;deep network;stochastic neural network representation;deep neural networks;memory network;networks;neural networks;dense binary networks;different network representations;teacher nodes;neural;scalable networks;sneural networks;student nodes;convolutional neural networks;dense binary network;network models;learning rate;layers;continuous learning task;neurons;models;regularization;training samples;dark knowledge;teacher model;network architecture;machine learning;unseen data"}, "f17ee5b9d3120960eddd2bdb9af2f4f689cebb3a": {"ta_keywords": "relation extraction;relation mentions;test relation mentions;same relation types;embeddings;similar representations;text features;direct supervision;dimensional spaces;qa;types;f1 score;model;novel framework", "pdf_keywords": "relation extraction sentences;relation extraction;relation extraction task;relation extraction model;relation extraction pipeline;relation type embeddings;entity mentions;relation mentions;entity mention pairs;similar relation mentions;supervised text analysis;partial supervision;similar relation mention pairs;relation mention;text corpus;knowledge extraction;training corpus;richer semantic knowledge;relation mention pairs;test relation mentions;indirect supervision method;relation type representations;text corpora;qa entity mention pairs;relation mention pair;entity pairs;semantic similarity;indirect supervision;specific corpora;relation vector space"}, "90af87c1e4fba127d6db8f5e1f9e1ef3472507e8": {"ta_keywords": "relativistic quantum mechanics;orbit coupling;orbit coupling strength;spin;zeeman field;fermions;framework;effect;presence", "pdf_keywords": ""}, "39365d95992c8294ba32d85c69d337040ddb8e54": {"ta_keywords": "balanced binary tree structure;standard seq2seq models;arbitrary tree structure;various target tree structures;neural network;side syntax;model;translation;new representation;bleu points;target;target side;other methods;experiments", "pdf_keywords": "efficient nonlinear thirdparty parsers;neural machine translation;neural machine translation model;target parse tree;parse trees;recurrent neural network;syntactic information;best structural representation;term memory;tree structures;tree;tree structure;rnn;novel tree;final tree structure;decoder;translations;syntax;free binary trees;sentences;translation;trdec;complex languages;sequences;nodes;target side tree topology;side syntax;forward node;balanced binary trees;models"}, "162515d87256f13888d9d7ba95275ac4b6c35396": {"ta_keywords": "novel word recognition model;standard semicharacter model;unseen words;error reduction;pipeline;literature;factor;spread;approaches", "pdf_keywords": ""}, "615358de8e9a7cf318c172afafc2a303eab93d98": {"ta_keywords": "recommender system;fashion item;clothing coordinates;fashion style;fashion magazines;fashion magazine;style sharing region;top recommendations;photographs;real photographs;recommendations;tops;other items;services;bottom;method;task;effectiveness", "pdf_keywords": ""}, "58c04126a5196deb57ae31d6174cd4aae154f138": {"ta_keywords": "active learning;partial feedback;annotater;relative accuracy;accuracy;examples;alpf;data;method;lower cost", "pdf_keywords": "annotation label;active learning approaches;active learning;popular active learning methods;active learning model;annotation;disparate annotation costs;popular active learning method;partial labels;annotation budget;efficient classifier;label;available classifier;potential labelswe;active selection framework;classification problem;multiclass classifier;active selection;partial feedback;classes;optimal learning;wordnet;machine learning models;learning process;learners;learner;empirical discovery;new learner;activein;deep learning"}, "bdfb9f1c79ad726049a3563c741311391e18532a": {"ta_keywords": "coherent state generator;coherent states;coherent state;generation;combination;collection;method;new method", "pdf_keywords": ""}, "eadf5023c90a6af8a0f8e8605bd8050cc13c23a3": {"ta_keywords": "robust automatic speech recognition;speech recognition;separation challenge;chime;asr;machine learning;signal processing;speech;language processing;challenge;data;context;interface;effective way;research", "pdf_keywords": "acoustic utterances;distant multimicrophone conversational;binaural speech recognition;binaural speech recognition challenge;speech recognition;microphone capture;acoustic recordings;speech separation;utterances;voice;conversational system;speaker signals;speech corpora;reverberation;advanced speaker;cardiac arrest;noisy noisy noisy speech;speaker;other cardiac arrest events;chiral neural networks;advanced sound front;neural network;recognition pipeline;advanced binaural sound front;language modeling;home speech;multimodal extension;recognition;cambridge multimodality;noise"}, "11465566a1f5ec7d4176bb7ab8edd26a154a1b60": {"ta_keywords": "smart meter;smart meters;data;devices;analysis;efficiency;number;use;operation;additional information;approach;users;novel approach;behavior;paper", "pdf_keywords": "privacy contract;privacy management;privacy protection;privacy;novel privacy;privacy policy;privacy setting;utility contract;low privacy;utility contracts;privacy breach;privacy breaches;smart grid;smart meters;aware data collection;electric utility;smart sensors;utility company;contract design problem;service contract;security;side electricity service;information rent;predictive value;contracts;data collection;contract;consumers;own contract;consumer"}, "ba00cbd314dc52b299a8b0c34f1887bcd43cdc12": {"ta_keywords": "word sense induction;word embeddings;synsets;dictionaries;ambiguous input graph;new graph;graph;clustering algorithm;ambiguous words;empirical observation;approach outperforms state;approach", "pdf_keywords": "synonymy dictionaries;synonymy graph;synonyms;word similarity evaluation;clean synonyms;word sense induction;semantic network;word embeddings;corresponding words;disambiguation;universal dictionary;text corpus;lexical coverage;semantic information;word sense;semantics;synonymywe;fuzzy clustering;hard graph clustering;large corpus;meta clustering algorithm;fuzzy clusters;semantic properties;fuzzy graphwe;clustering;similarity;arbitrary text documents;language synsets;word;intrinsic similarity"}, "48220433a2fb07761b26b2d6aa59b615289a3d4c": {"ta_keywords": "node attack;arbitrary graph types;graph network;single nodes;graph types;nodes;graphs;specific node;graph;attacker;attack;other nodes;robust code;variety;ability", "pdf_keywords": "graph network;graph graph network architecture;graph datasets;node attack;graph nodes;edge attacks;parallel graph representation learning algorithm thatwe;large graphs;graph representation;attacker node;graph edges;graph topology;graphs;graph structure;global graph structure;edge attack;gnn architectures;node injection attacks;graph;attacks;graph model;social networks;adversarial perturbations;adversarial model;direct attacks;nodes;targeted attacks;attack;own edges;attacker"}, "25c50ef5a902586a06099ceb29e7f34e2172020a": {"ta_keywords": "entangled particles;single entangled particle;generation;network;pair;new method;computer;method", "pdf_keywords": ""}, "e79bd5d5ad084009233c8524b02ac887029c5fe2": {"ta_keywords": "surface plasmon;planar plasmon;flexible dielectric;plasma interface;tunable dispersion;laser;dispersion;planar;new design;design;concept", "pdf_keywords": ""}, "f1b52bf723d7f5c4b68c8551c4d168ed1224f016": {"ta_keywords": "complete mitochondrial genome;complete genome;natl;acad;generation;sci", "pdf_keywords": ""}, "c14fb834ac6ede13f94f71cfaf5649b55e70a2c2": {"ta_keywords": "generalized nash;data markets;aggregator case;equilibria;many equilibria;gn;sufficient conditions;none;class;contrast", "pdf_keywords": "data aggregators;data aggregator;different data aggregators;crowdsourcing;data market;multiple data aggregators;aggregators;unique aggregator equilibria;other data aggregatorwe;interacting data sources;aggregator;multiple aggregators;data sources;single aggregator;generalized nash equilibria;data buyers;equilibrium games;incentives;data source;multiple data sources;aggregator desires;networked data sources;nash equilibrium approach;decentralized data processing;single data source;inefficient equilibria;incentive;nash equilibrium problem;aggregation;data"}, "7c085d7f50a76cf1a09a114986206256e0ee1931": {"ta_keywords": "spin;magnetic field;photons;atom;coupling;order;significant increase;effect", "pdf_keywords": ""}, "293ed3367027c99a81ead6ff3f31be7de43bce9c": {"ta_keywords": "random walks;random walk;efficient generation;partitioning;new method;method;parts", "pdf_keywords": "peer review;novel peer selection mechanism;vanilla peer selection mechanism;reviewers;literature peer selection;other strategyproof mechanisms;review process;strategyproof mechanism;ratings;arbitrary selection;selection mechanism;simple voting mechanism;new strategyproof mechanism;agents;selection;unbiased members;reviews;decentralized allocation;minimal randomization;credible subset;agent;nice allocations;ranking sequence;allocations;allocation;decisions;effectiveness;insincere valuations;expertise;quota allocations"}, "adeed0816a2cab763e3bee769957ff1849985759": {"ta_keywords": "automatic normalization;normalization;interactive normalization tool;string distance measures;automatic data;text;historical texts;lemmatization;tool;results;several methods;paper;method;variety;different types;ways", "pdf_keywords": ""}, "bd8334c1246adbd47f80eea60249c30a74925d7a": {"ta_keywords": "average cost stochastic decision process;wireless relay networks;relay nodes;link outage probabilities;relay node moves;deployment;line placement decisions;network;deployment problem;mdp;line quality measurements;power consumption;line;agent;number;problem", "pdf_keywords": "optimal sequential decision problem;optimal deployment policy;wireless relay networks;relay networks;sensor network;wireless sensor networks;average cost markov decision process;optimal deployment;autonomous sensor networks;wireless sensor network;network deployment;wireless networks;generic sensor network;efficient deployment;optimal network placement;relay placement policy;long forest trails;forest trails;relay nodes;wireless relay;optimal connectivity;supposing relay;scalable deployment strategy;stochastic optimal control problems;parallel relay nodes;network cost;average cost optimality equation;stochastic optimal control;optimal optimal strategy;relay"}, "7e0eb21f4903c2fe860d1c4f213879e99d7cd23c": {"ta_keywords": "electrolarynx;voice conversion method;listenability;spectral parameters;hybrid method;naturalness;method;experimental results", "pdf_keywords": ""}, "1d56a0b8fb560a79ca28b44bfd6f1e645a36549a": {"ta_keywords": "new thermocouple thermal control system;thermocouple junction;thermocouple data;thermocouple;differential cross section;differential cross;single layer;junction;material;system;design;concept;idea", "pdf_keywords": ""}, "ead3182dd47bdd8da98476cca1cfe0373dfc2edc": {"ta_keywords": "speech recognition;probabilistic mixture model;complicated acoustic model;appropriate acoustic model topology;variational estimation;clustering;decision tree;efficient model search algorithm;automatic determination;vbec;vbec framework;gmm;efficient method", "pdf_keywords": ""}, "b593be8ff3c09c6994657678fcde0c5adf43328e": {"ta_keywords": "unsupervised parsing;corpus;unsupervised constituency;span constraints;distant supervision;constituency;text;entities;phrase;craft;performance;form;f1;method;experiments", "pdf_keywords": "supervised syntactic parsing;unsupervised parsing performance;unsupervised parsing;parsing;natural language processing;valid parse trees;craft corpus;recursive autoencoder;modern language processing tasks;deep subordination;supervised constituency;large corpus ofwe;distant supervision;linguistic data;span constraints;dimensional semantic structures;linguistic structure;sentence structure;biomedical text;dimensional semantic structure;supervised supervised tasks;unstructured data;entity structure;span constraint;supervised lps;supervised learning;lexicon;sentences;linguisticwe;span"}, "07a5536c0570804f816fdb5a0a5ae890630e61bd": {"ta_keywords": "speech enhancement;statistical voice conversion;noise reduction;el speech;el;significant improvements;intelligibility;naturalness;hybrid approach;method;degradation;conventional approaches;experimental results;paper", "pdf_keywords": ""}, "d14afc470cd90521147130e153c0d3e1324cd104": {"ta_keywords": "neural machine translation system;languages;semantic features;semantics;phonetic features;syntax;data;novel method;system;method", "pdf_keywords": "multilingual neural machine translation;multilingual neural network;neural machine translation system;multilingual machine translation system;neural machine translation systems;uriel language typology database;typology prediction;language vector;language encoder;computational linguistics;multiple typological databases;languages;new language;term memory machine;language;typology;prediction accuracy;predictingwe;neural networks;prediction;vector machine;texts;machine learning;semantic consistencies;unsupervised neural model;sentences;dataset;representations;new vector representation;vector representation"}, "6aac35ec3bfaf7e835ac633414419c9623838007": {"ta_keywords": "spectrogram features;auditory features;deep learning;convolutional neural network;cochleogram;cnn;single features;features;mel filter banks;accuracy;log;task;novel method;various possibilities;framework", "pdf_keywords": ""}, "9895531c6dc3854f082de1a1ec651a9e179bbd07": {"ta_keywords": "connectionist temporal classification loss function;tonal languages;tonal signals;neural network architecture;tonal;language;same language;consistent description;novel method;method;use;approach;power", "pdf_keywords": ""}, "8d1fd086a76d30343d2224b61cb7ddab2125d0b2": {"ta_keywords": "symbollevel;solution paths;solutionpath;programs;novel optimizations;performance;novel representation;framework;model;analysis;use;paper;mechanisms;series", "pdf_keywords": ""}, "3256198d819f23f82640490b9160e85139627d6c": {"ta_keywords": "wavelet transform domain;image processing;edge intensity;threshold operation;representation;application", "pdf_keywords": ""}, "f9a86c2df17f408105c2d3e9429410cdc376c6f0": {"ta_keywords": "distributional semantics;distributional semantic modeling;distributional methods;novel methods;new methods;workshop;discovery;ideas;problems;results;first part;field;participants;second part;exchange;forum;aim", "pdf_keywords": ""}, "66713fbcb8d5e48a9eb6425bd7fdbb53751e60b1": {"ta_keywords": "fastest schemes;modern processors;arrays;scheme;singleinstruction;integers;simd;multipledata;varintg8iu;pfor;superscalar nature;instructions", "pdf_keywords": "new integer compression scheme;integer compression;variable byte compression scheme;other vectorized bit;vectorized bit;vectorized bit packing;efficient encoding;encoding speed;optimal index compression performance;encode integers;efficient bit packing;vectorized integer;variable byte encoding technique;lambda compression;binary encoding;index compression;data encoding;differential coding;encoding;encoding process;simple bit packing;vectorization instructions;differential decoding;binary code;bestindex compression;vectorized version;encoding array;variablebyte encodings;competitive compression ratios;superior compression ratio"}, "3a2446c47000c3d0681b2cdf6d8b87a11ff630e2": {"ta_keywords": "wall insulation board;exterior wall insulation boards;simulation;software library;measured function;function;device;design;assembly;configuration;output;data", "pdf_keywords": ""}, "5ce3148ed36a1ea034da2c05b8cde9efbaf43e6a": {"ta_keywords": "field speech recognition network;beamforming network;spatial covariance features;spatial co;phase information;individual frequency bands;network;performance evaluation show;error rate;performance;information;use;weight;extensive experiments;paper", "pdf_keywords": ""}, "981dbdf6f87f13f3f3047a925c519fc39a35202b": {"ta_keywords": "next word;novel neural model;short input contexts;language;local concatenation layer;baseline model;term dependencies;original word;model;lower perplexity;level model;simplicity;state;flexibility;art;several aspects", "pdf_keywords": "neural probabilistic language model;word embeddings;level language modeling datasets;biological language models;attention layerwe;deep nonlinear generative models;language model;classic language model;graph compression;deep learning;short input contexts;short contexts;noisy graph;next word;neural architecture design;neural network;nodes;term dependencies;word;models;forward network;layer models;dimensional text;graph theory;network;model;entities;nplmwe;rare words;underlying structure"}, "d92e0443768ec3715205cb232ef1a1917372b0af": {"ta_keywords": "novel nlu models;novel open source models;open source software toolkit;slu;espnet;generation", "pdf_keywords": "source speech processing toolkit;source speech recognition;simple speech processing tasks;speech processing tasks;speech processing;various speech processing tasks;speech recognition;spoken language representation;spoken language understanding;open source toolkit;translation systems;source toolkit;spoken language;speech recognition problem;speech;toolkit;neural network architecture;language;deep clustering;datasets;unified pipeline;deep bidirectional transformers;intent classification task;languages;encoder;classifier;library;attention;novel nonlinear classifier;pretraining"}, "04e3a3ee41c1ee977e023052435bbb5f4c680f66": {"ta_keywords": "quantum stores;quantum information processing;store;qip;stores;applications;new class;design;operation;new method;boundaries", "pdf_keywords": ""}, "eba4c6b0b860a34461ffb8544111c89a3ef0d8b7": {"ta_keywords": "noisy pairwise comparisons;ranking problem;strong stochastic transitivity;top items;algorithms;linear time algorithm;comparison;competitive ratio;pair;general noise model;sst;noise;set;problem;model", "pdf_keywords": "optimal ranking;ranking problem;global ranking order;domination instances;pairwise comparisons;domination problem;largest collection;efficient algorithm;best algorithm;possible algorithm;algorithms;largest means;such algorithm;algorithm;new decision problem;domination;top;arbitrary threshold;case algorithm;simple algorithms;new algorithm;competitive ratio;natural algorithm;complexity;comparisons;adaptive sampling problemwe;simple algorithm;important graph;sample complexity;order"}, "9d628e420922cc23a8944de1511ca5d3309f5d58": {"ta_keywords": "clinical summaries;utterances;corpus;patient visit records;clinical data;full conversation;extraction;dataset;machine;quality;power;points;approach;set;heuristics;model", "pdf_keywords": ""}, "40ba59c9945e7c19d06dadfa8f496da5810ee30d": {"ta_keywords": "parallel beam search algorithm;speech recognition;multiple speech utterances;multiple utterances;original beam search algorithm;encoder decoder;line recognition use;multiple hypotheses;speedup;inference step;original beam;loop program;traverse;hypotheses;technique;regard", "pdf_keywords": "multiple speech utterances;libri speech corpus;multiple utterances;multiple input utterances;search hypotheses;csj corpus;beam search algorithm;new search algorithm;beam search;original beam search algorithm;global pruning;multiple input streams;utterance;search process;libri corpus;end speech recognition system;pruning techniques;algorithms;neural computation;search;supervised manner;multiple hypotheses;deep learning;hypotheses expansion;automatic detection;recognition time;algorithm;process parallelism;recognition;vectorization"}, "ccd33442fef058c7c0eafc57d2c6e6a4cde10a3b": {"ta_keywords": "3d protein models;protein molecules;graph structure;model assessment;prediction benchmarks;graph nodes;quality assessment;equivariant convolutions;rotation;structure;quality;critical assessment;standard message;novel method;method;approach", "pdf_keywords": "3d protein graph;protein graph;protein models;protein model;protein structure prediction;protein structure;molecular graphs;theprotein structure prediction;protein molecule;protein molecules;spherical graph;novel biomolecular models;biological networks;protein function;equivariant spherical filters;convolutional networks;convolutional network architecture;convolutional network;graph networks;spherical convolutions;protein;dimensional topological networks;protein scores;convolutional neural networks;graphs;equivariant convolution operations;protein residueswe;networks;classical gcn architecture;quantum networks"}, "5dd34d2781ca702d0e3cd1224517ff60d6c3e2ee": {"ta_keywords": "informal digital communication;informal decipherment;latin script languages;informal text;character mappings;languages;visual priors;inductive bias;channel model;performance;users;model;same underlying principles;hypothesis;form;range", "pdf_keywords": "informal romanization algorithm;informal romanization;informal romanization pattern;transliteration;transliteration process;latin character sets;unsupervised neural architecture;romanized language;phonetic similarity;language model;character similarity mappings;character mappings;source language;languages;decoding;visual character similarity;target language;useful character substitutions;large corpus;language;world language;bigram encoding;unsupervised learning;same language;common language styles;arabic;egyptian arabic;characters;native script;informal digital communication"}, "054ba27fe5cc6085d20ea2707de886db6865dbed": {"ta_keywords": "proximity measures;graph learning method;random walks;random walk model;graph structure;supervised learning;random walk;novel method;domain;rwr;weighted combination;method;rnr;restart;novel variant;variant", "pdf_keywords": ""}, "7621bfe36cc649a5876cea587366201e158a8b38": {"ta_keywords": "domain adaptation;biological domain;source representation;annotations;novel domain;neural network architecture;domain;biological processes;sequence;system;approaches;performance;version", "pdf_keywords": ""}, "c1a4c5380d90dc77064de6003cfb9611ad218600": {"ta_keywords": "neural dialogue response generation;dialogue generator;dialogue history;conditional adversarial learning;adversarial discriminator;sentiment label;sentiment labels;response;sentiment;reasonable responses;fluency;training;model;method;feasibility;experimental results;paradigm", "pdf_keywords": "neural dialogue generation;dialogue generation;dialogue response entropy;conditional generative adversarial network;dialogue system;dialogue responses;dialogue response;conditional adversarial learning;adversarial nets;dialogue history;gans;sentimentcontrolled responses;conditional generator;cgan;novel adversarial learning approach;sentiment classifier;policy gradient training;dimensional sentiment;encoder;utterances;encoderin;conditional discriminator generator;sentiment accuracy;gold response encoder;generation;generator;sentiment;random generator;sentiment labels;decoder structure"}, "9d06638df32f8feefb95ef5a4769adbb1ae6297d": {"ta_keywords": "new rule learner;benchmark problems;slipper", "pdf_keywords": ""}, "4c78943e11195fb72a3c878a03b248bc317180e0": {"ta_keywords": "polynomial learnability;learningability;enable tractable learning;order description logics;syntactic restrictions;order logic;examples;vocabulary;positive examples;restricted subset;subset;size", "pdf_keywords": ""}, "f0baf134f0a2ee6e99f6f2287791109cf93305e7": {"ta_keywords": "subcellular patterns;fluorescence microscope;microscopy images;biological network architecture;fluorescence;protein;image;analysis;novel approach;identification;approach;use", "pdf_keywords": ""}, "db253b17043b6a86e02173b6aa597664b0c7f256": {"ta_keywords": "embeddings;rare words;semantic content;text classification task;convolutional neural network;words;compositionality;visual space;characters;sparsity;novel method;method", "pdf_keywords": "standard character embeddings;similar embeddings;visual embeddings;text representation;embeddings;compositionality;natural language processing;representations;natural language;natural languages;recurrent neural networks;linguistic tasks;visual representation;model learnsin;similar characters;characters;meaning representation;text classification task;convolutional neural networks;representation;convolutional neural network;category classification;cnns;character;text;character symbol;rare characters;semantics;cnnwe;encoding"}, "f516c98c3d2dde5b31931715fbc48bbbc0580e27": {"ta_keywords": "workgroup structure;workgroup leadership;workgroups;workgroup information storage system;work groups;team members;email messages;leadership roles;text;document;data;dynamics;collection;good understanding;combination", "pdf_keywords": ""}, "553b74de8cb7ebca42a686e2a3a2d6aae170946e": {"ta_keywords": "speech enhancement;clean speech signals;probabilistic model;noisy signals;novel diffusion;generative models;diffusion;aforementioned diffusion;duuse model;model;duuse;wave architecture;dp;wave;processes;source framework;performance", "pdf_keywords": "noisy noisy speech networks;speech enhancement;noisy speech signal;noisy speech;diffusive filtering;clean speech signals;quality speech signals;enhanced speech;noisy noisy regime;speech separation;diffusion model;diffwave model;noisy data;simple diffusive model;diffusive filtering equation;diffusion process;diffusion;novel diffusion;noisy signals;diffuse model;noisy noisy channels;noisy signal;additional noisy signal;noisy input;noisy channels;diffuse method;generative models;phenomenological diffusion model;diffuse;noise"}, "616cc6826066184a8c77c3f2562e4e891ce42911": {"ta_keywords": "reinforcement learning;intrinsic fear;catastrophes;toy problems;dangerous states;novel approach;guards;approach;new method;problems;unacceptable performance", "pdf_keywords": "intrinsic fear model;reinforcement learning model;natural fear model;reinforcement learning;fear model;intrinsic fear;time reinforcement learning;catastrophic forgetting;learned reward;learning model;intrinsic reward;learning process;catastrophic states;risk state;optimal agent;learning;optimal policy;catastrophes;imminent catastrophe;catastrophe;periodic catastrophes;powerful reward signal;exploration;generic learning environment;stochastic dynamics;catastrophic transitions;natural model;agent;natural phenomena;agents"}, "7e82015c386726f4b8f6f686b6e6bb7d1e7564bb": {"ta_keywords": "topic;model", "pdf_keywords": "controversy detection;level controversy detection;controversial posts;social media posts;contentrelated posts;controversial articles;comment statistics;reply relationship modeling;topicexisting methods;dissimilar topics;public sentiment;controversy;discussions;social media;posts;intense discussion;topics;diverse topics;wikipediarecent advances;reply structure;convolutional networks;content;comments;novel graph;convolutional network;controversies;news articles;topic;particular topic;views"}, "b8cabd2f7fbf816d667701c5d756b5fcb982e6fe": {"ta_keywords": "finite timescale separation parameter;sum games;learning rate;gradient descent;ascent;player;role", "pdf_keywords": "gradient descent ascent;generalized differential stackelberg game;continuous time gradient descent;ascent converges;ascent limiting dynamics;finite learning rate ratios;finite learning rate ratio;concave optimization;differential stackelberg equilibria;learning dynamics;differential differential stackelberg equilibrium theorems;ascent games;stable differential stackelberg equilibrium;stochastic gradient descent;gradient descent;nonconvex optimization;ascent game;stackelberg equilibria;local minimax equilibria;differential stackelberg equilibrium;regular local minmax equilibria;learning rates;finite time convergence guarantee;gradient penalty;learning rate ratio;parallel game theory;differential equilibrium games;differential nash equilibria;optimality;differential nash"}, "c5a323f8744838093ee36bee3739dea599ce62f0": {"ta_keywords": "heisenberg antiferromagnet;spin dynamics;spin liquid;zeeman field repulsion;zeeman field;effect;field;system;results", "pdf_keywords": ""}, "e82eff0f3e3d150617f9a721f83046940a963c03": {"ta_keywords": "abstraction functions;learnability;abstraction mechanism;concept learners;learning system;generalization;abstract spaces;implementation;algorithms;pac;explanation;analysis;framework;thesis;model;function;several applications;problem", "pdf_keywords": ""}, "c8171eaa3a3aac78c3b37351412101bc06e5f359": {"ta_keywords": "monolingual annotators;translation accuracy;comparable training data;target languages;captions;languages;comparable data;data;quality;pivot;images;source;set;novel method;method", "pdf_keywords": "low caption diversity;multilingual image descriptions;monolingual annotators;caption complexity;large multilingual dataset;machine translation performance;translation tasks;translation quality evaluation;large corpus;machine translation systems;multilingual multilingual machine translation;captions;translation task;multilingual tasks;translations;linguistic content;neural machine translation systems;linguistic similarity;ted parallel corpus;structured text;unstructured text;acceptable translations;automatic evaluation;low resource languages;annotators;source language;comparable training data;quality training data;translation;target languages"}, "51c33a79e05425b6335c8676a166a0f4e178c0a2": {"ta_keywords": "test items;automatic coding;overall test performance;specific knowledge gaps;teachers;essential features;line;performance;information;set;novel approach;approach", "pdf_keywords": ""}, "2c1cb736df7bf526fc3facecd078980e007abceb": {"ta_keywords": "yeast calmodulin;vertebrate calmodulin;yeast;calmodulin;alpha;baker;energy;concentration;dependence;difference", "pdf_keywords": ""}, "62d1a3137b01a69443bebf4d92c1990ec512a6a1": {"ta_keywords": "large language models;language model;adversary;novel attack;training data;attack;text sequences;model;possible safeguards;hundreds;public internet;ability;factors;success", "pdf_keywords": "novel memorization attack;training data extraction attack;large language models;powerful memorization problem;private machine learning;memorization data;large language model;private datasets;simple baseline extraction attack;memorized examples;memorization;simple memorization model;memorized sequences;memorized data;memorized text generation;memorized data iswe;sensitive attack;adversarial setting;attacks;memorized text;sensitive information;language models;memorization problem;memorized patterns;memorized sequence;key memorization problem;memorized content;memorization example;training data;eidetic memorization"}, "bc4cb14af1023123b3122a5f0b6f3bb76334ffb4": {"ta_keywords": "rich erenkov barrier;neutron;rich isotopes;erenkov technique;rich nuclei;nuclear matter;barrier;rich phase;key ingredient;characterization;development", "pdf_keywords": ""}, "073798fde720d5f08dccfbb0c1917a064828c399": {"ta_keywords": "dimensional optical lattice;einstein condensate;component bose;bose;dynamics;analog;summary;study;results", "pdf_keywords": ""}, "81f5ef41dfa72679cb7cb38999a41a1c534c3871": {"ta_keywords": "stationary state;dependent stochastic process;stochastic process;simple method;method;application;time", "pdf_keywords": ""}, "8d69f466bdf56ce6663c2f809514577e79dd3bed": {"ta_keywords": "sixth sense technology;wearable device;device;interface;hardware restrictions;technologies;internet;architecture;responsiveness;novel architecture;environment;lab;user;paper", "pdf_keywords": "gesture extraction;gesture recognition;gesture segmentation;gesture;gesture signatures;hand gesture;smart tools;color marker detection;wearable device comprises;own gesture;sensor;image processing;feature extraction;camera;segmentation;projector;segmentation process;new technologies;interface;electromechanical lab;features;lab;technologies;hardware restrictions;image;lab controlsin;applications;information technology;objects;hand"}, "79c6713c41b4fedf9c7454b7e2bb48d0aeb1ae0f": {"ta_keywords": "sample designs;high quality space;surrogate modeling;filling designs;image reconstruction;several benchmark optimization functions;inertial confinement fusion;optimization framework;spectral domain;metric;dimensions;estimator;icf;new approach;approach;different applications;paper", "pdf_keywords": "better quality sampling designs;random sample designs;optimal sampling;sample designs;dimensional isotropic sample design;spectral sampling;sample distances;sample design;disk sampling;spectral designs;optimal spectral coverage;dimensional poisson disk sampling problem;spatial statistic;point samples;random coverage function;empirical design;spline sampling;high quality space;random graph representation;samples;sample;spectral design;novel spectral design;random graphs;discrepancy designs;designs;empirical data sets;random graph theory;empirical graph theory;quality metric"}, "2821db8962fce43265215a9c4b8d66af02e16ae7": {"ta_keywords": "optimal scheduling policy;optimal average job latency;actual optimal scheduling policy;redundant requests;such cancellation delays;cancellation overheads;scheduling;cancellation delay;data storage systems;cancellation;small cancellation;computing;model;results;efficacy", "pdf_keywords": ""}, "94245856c88e3e08777c876fc038ed1adf8f3285": {"ta_keywords": "closed quantum system;quantum numbers;many possible quantum numbers;quantum state;particles;calculation;system;new method;integration;method", "pdf_keywords": ""}, "9ef4f6a070c750b746fe98ef34083d6a08c9ba42": {"ta_keywords": "quadratic dynamic games;optimal control strategies;pricing;diffusion;cost;total cost;nominal cost;general network;other players;player;explicit dependence;leader;methods;method;problem;fundamental importance;design", "pdf_keywords": ""}, "ff86133b3b49974f06fc881548c6f3c7a8ceffee": {"ta_keywords": "voice timbre control method;voice timbre;singer;individuality;age;technique;adverse effect;development;experimental results;paper;recent work", "pdf_keywords": ""}, "43d82bc8203c09edc7eb6b2bedcf4ab500690852": {"ta_keywords": "large multilingual language models;small parallel corpus;lingual adjustment;natural language processing;shot transfer;nlp;unrelated words;np task;adjustment;tasks;task;machine;logical model;performance;separation;novel approach;impact;variety", "pdf_keywords": "large multilingual language models;large multilingual models;multilingual models;crosslingual translation;multilingual bertarian model;large parallel corpus;multilingual resources;multilingual text relate;multilingual resource evaluation;different natural language inference;target language;small parallel corpus;target languages;multilingual documents;crosslingual alignment;lingual tasks;translation accuracy;machine translation system;natural language inference;shot transfer tasks;unrelated word representations;parallel sentence retrievalwe;multilinguality problem;different linguistic resources;embeddings;linguistic representations;shot transfer;shot transferwe;languages;minimal corpus size"}, "b57da3ccf214e8dad49116c8db9590c2c89629f5": {"ta_keywords": "entity recognition;available dataset;ner;new approach;results;problem;art methods;power;settings;same settings", "pdf_keywords": "entity recognition;language annotators;linguistic data;online news corpora;linguistic structures;information retrieval;unconstrained natural language equation;linguistic similarity;training corpus;corpus;entity recog nition;linguistic agents;language adaptation;annotators;such languages;languages;language;linguistic transmission;language specification;different languages;entities;language families;language family;sentence structures;entity;informative domains;partial knowledge;inter_annotator agreement;target entity;neural networks"}, "05c8f15dbdd7c6661b9176638262bbc1e11de85f": {"ta_keywords": "interpretable sense embeddings;original sense;word;novel method;method;form", "pdf_keywords": "contextual word sense selection task;interpretable sense representations;sense embeddings;sense induction;interpretable representations;art sense embeddings models;word senses;semantic vectors;sense disambiguation;noncontextual word similarity;context embeddings;distinguishable senses;discrete senses;word similarity;text recognition;sense distribution;senses;semantic similarity assignments;embeddings;appropriate senses;word embeddings arewe;specific embeddings;semantic drift;polysemous words;artificial languages;attention;deep learning;duplicate senses;lexicographer;fewest duplicate senses"}, "58737fba500075136ee0f33f7801a5ac7f82ab68": {"ta_keywords": "rapid prototyping;document style;scoring features;novel methodology;heterogeneous data;reproducibility;methodology;paper;available databases;several variants;common practice;split;feasibility;form;effectiveness;field;novelty;version", "pdf_keywords": ""}, "14919b6a453a71f2a007d5fa57241887a982575f": {"ta_keywords": "independent learning;theorems;logics;representation", "pdf_keywords": ""}, "fb089347919e8dada9335b4bac01f16eea758c56": {"ta_keywords": "computer ethics;teaching tool;machine stops;topics;machine;story;issues;questions;suggestions", "pdf_keywords": ""}, "e2c05b3abf77900ec82ffa8a95aa774308d2780f": {"ta_keywords": "level language detection technique;languages;tweets;code;global patterns;novel unsupervised word;patterns;level labeling;words;competitive baselines;relative error reduction;word;region;quantitative analysis;scale;technique;system;respect", "pdf_keywords": ""}, "719916251f7e36d2e7a40e70f89f20ab97a8bc29": {"ta_keywords": "single channel speech enhancement;soft mask target;soft mask;soft masks;multichannel input;original mask;student network;chi;model;cut;paper;paradigm;cross", "pdf_keywords": "channel speech enhancement technique;channel speech enhancement model;single channel speech enhancement;multichannel speech enhancement;multichannel speech enhancement problem;speech enhancement method;noise mask;robust speech recognition;acoustic signal processing;mask estimation;single channel track data;original noisy speech;channel input;beamformer;noise ratio estimation;multichannel input;teacher network;noise ratio;soft mask;standard mask model;teacher learning;student teachers technique;teacher networks;mask;new training paradigm;teacher spectra;noise ratio mismatches;soft masks;student teacher;student network"}, "0909fee90833e20913adb553bf6667c9a3b854b0": {"ta_keywords": "wrapper learning;wrapper learning problems;flexible learning system;tables;learning system;bang news laboratory;multiple types;collection;active use;literature;new domains;system;problem", "pdf_keywords": ""}, "4a45ace1f8c6a30ba00201b30acd93844b9797eb": {"ta_keywords": "proton interaction;monte carlo simulation;proton;ab initio;ground state;simple model;calculations;study;results", "pdf_keywords": ""}, "dd3ba828dbbb17cf478f6840a37954f6ebc81770": {"ta_keywords": "long short term memory;dialog;attention;utterance;level context;aforementioned task;local word;better performance;lds;lds yield;work", "pdf_keywords": ""}, "81e57827bebb04305cc9e6c85e96c83951244ec2": {"ta_keywords": "knowledge graph embeddings;polypharmacy side;tensor decomposition approach;snp;new drug class;decagon model;clinical trials;effects;idea;art methods;new method;approach;state", "pdf_keywords": "knowledge graph;novel knowledge graph;knowledge graphs;underlying knowledge graph;new knowledge graph;structured knowledge graph;knowledge entities;relational machine learning;rank embeddings;link prediction task;knowledge;graph learning;approach models polypharmacy side;tensor factorisation;vertex model;flexible representation;entities;possible adverse effects;protein data;polypharmacy;polypharmacy side;effects data;dependant scoring function;relational setting;bioinformatics framework;possible associations;vector representation;models;severe complications;morbidity"}, "e98621050e52e9d8c60829d8d861e81ac86a8617": {"ta_keywords": "single point source;amplitude;point source;estimation;single point;map;method;new method;simple example", "pdf_keywords": ""}, "e2b097bce656db9215505659357263c43190194b": {"ta_keywords": "optimal assignment;reviewers;review process;assignment;reviews;paper;oracle;phases;theoretical bounds;suboptimality;conditions;set;problem;strategy", "pdf_keywords": "popular paper assignment method;paper assignment problem;optimal papers;standard paper assignment problem;phase paper assignment problem;paper review process;optimal assignment;optimal assignments;optimal assignment bounds;stage paper assignment;phase paper review process;peer review paper;total assignment similarity;paper assignments;assignment quality;random reviewer split;optimal matching;peer review system;assignment problem;additional reviewers;review process;consistent assignments;reviewers;many scientific conferences;paper split;papers;assignment;assignments;optimal similarity;similar reviewers"}, "c0c1950fb0a129b71a218ffa8b9fbc6d088cba2d": {"ta_keywords": "stochastic growth;stochastic process;discrete states;emergence;new class;problem;context", "pdf_keywords": ""}, "27f9b91bd7c70a99f578c8a5cb52d37e4123da47": {"ta_keywords": "convolutional neural networks;networks;topological structure;proton;regression weights;representation;parameters;activations;rank constraints;high accuracy;data;motion;novel approach;equation;ability;approach;problem;large number", "pdf_keywords": "deep learning experiments;deep learning;tensor regression network;tensor networks;tensor regression layers;deep network;novel tensor network architecture;deep neural networks;deep convolutional neural networks;tensor regression layer;tensor network models;tensor regression;trainable neural networks;tensor regression weights;tensortensor network models;arbitrary activation tensors;popular imagenet dataset;tensor factorisation learning;convolutional layers;tensor entanglement generation;convolutional neural network;tensor network model;new trainable neural network layer;deep learningwe;regression weight tensors;neuroscience dataset;activation tensor;neural networks;tensor systems;tensor"}, "b09d49c3eacd93782a32ad16ab52f98a21ecc206": {"ta_keywords": "nonlinear conductivity;trivial conductivity;conductivity;nonlinear response;conductor;nonlinear analogue;perturbation;method;calculation;new method;analysis", "pdf_keywords": ""}, "1bf49ef0b33bf8fcc3ebdd16326db419f3af65d8": {"ta_keywords": "class manifolds;monte carlo representation;representation space;hidden class;monte carlo;representation;class;distance;neighboring points;approximation;points;mc;method", "pdf_keywords": "soft nearest neighbor loss;soft nearest neighbor loss canwe;nearest neighbor loss;entangled representations;generative loss;entangled representation;deep learning;entangled models;class similarity structure;deep learning models;training manifold;adversarial examples;nearest neighbors;entanglement;adversarial data;deep learning model;local nearest neighbor constraints;independent similarity structures;nearest neighbor error estimators;similarity structure;soft nearestwe;new loss measure;representations;gradient descent;discriminative models;similarity;convolutional network;generative models;representation space;representation spaces"}, "3a4f39dbb5e06a5fc55622315797da7a97cc76f6": {"ta_keywords": "recurrent neural networks;local context information;global context information;global context;neural network architecture;encoded text;context;target word;feedforward;quantitative description;stack;paper;tg;part;method", "pdf_keywords": "recurrent neural networks;recurrent networks;new forward neural model;word embeddings;forward error prediction;softmax layer;better sequence prediction;novel deep neural architecture;neural network approach;target words;annotated text sources;rnn;source words;embedding;target word;gradient model;language pairs;sentence structure;local context information;global context information;dimensional convolution layers;dimensional convolution layer;languages;local context;encode thewe;forward;reference text;layer;words;feed"}, "b9a701c90f3d3df27366f5b29a97f798eb940ac7": {"ta_keywords": "sequential learning;multidimensional learning;underlying language;range language;language;discourse;dynamics;structure;novel approach;ground;art models;state;approach;end;traditional approach;power;problem;first time", "pdf_keywords": "range language models;standard language models;language model;range language model;level language model;long document summarization;chapter breaks;chapter break;discourse prediction;long fan fictions;computational linguistics;long input sequence;sequence lengths;narrative;characters;discourse boundaries;long documents;novel rv;long fan stories;long segment;break;models;chapter;structured inputs;challenge dataset;several simple encoder models;novel feature;actors;same narrative;several models"}, "2559417f8a3d6ab922cfa824b43f9f0c642a1dae": {"ta_keywords": "entity recognition;entities;data integration methods;entity;sequential classifying;ner systems;other useful features;ner;data;level similarity;new formalism;multiple domains;direct use;words;previous methods;performance;problem;experiments", "pdf_keywords": ""}, "3ad3ba8d7fc793a19dfe6a87e32453449195c074": {"ta_keywords": "end speech recognition;text autoencoders;automatic speech recognition;encoders;large speech;training datasets;decoders;speech;end ssn;asr;tts;ssn;model;performance;subset", "pdf_keywords": ""}, "765bdcf27ebc1eb03a14f1e47aefa4dda1e03073": {"ta_keywords": "robust training;noisy texts;model robustness;representations;convolutional neural network;model;character;noise;multiple kinds;comparative study;approaches", "pdf_keywords": "invariant word representations;invariant word representation;robust training;word representation;noisy texts;neural machine translation;recurrent neural networks;model robustness;stable linear noise;noisy textswe;adversarial examples;similar increased robustness;robustness;machine translation;convolutional neural network;representations;cnn;new language;neural network;synthetic noise;character information;machine translation scenario;charcnn;clean texts;random noise;word;models;text;randomization;sequence models"}, "1263e36598dd95cc4becf0e18398f832bb5cf337": {"ta_keywords": "low resource languages;encoder;encoders;new encoder;decoder approach;dynamic behavior;systems;receivers;behavior;exploration;concept;combination", "pdf_keywords": ""}, "978582ad754eab481856d62bdc7b0ee5bcf21811": {"ta_keywords": "cluster information;iterative federated algorithm;unlabeled datasets;clusters;individual datapoints;generative models;probabilistic approach;statistical heterogeneity;clients;novel method;information;method;environment;properties;problem", "pdf_keywords": ""}, "675098c4611b13920d163a9a9b972da7751460cb": {"ta_keywords": "latent topic models;spoken language understanding;entire dialog;scale corpora;novel recurrent;rnn;neural network;networks;sequences;word;term characteristics;different time scales;paper;architecture;order;novel approach;combination", "pdf_keywords": ""}, "dd64013273eb4398821bf2fc8f024735466e5a1d": {"ta_keywords": "procedural knowledge;human learning;prior knowledge engineering effort;perceptual representation;sim student;plausible simulation;student;novel method;experience;example solutions;algorithm;integration;problem;performance", "pdf_keywords": ""}, "1abbe9b6bf3f134ce86e618bba83bf5c94f60f03": {"ta_keywords": "crowdsourcing platform;agents;prediction;parametric model;training data;experts;expertise;parametric;mechanism;behavior;joint design;goal;subset;problem", "pdf_keywords": "optimal joint incentive mechanism;incentive mechanism;appropriate incentive mechanism;optimal prediction;informed prediction;new prediction mechanism;new incentive mechanism design;incentivize agents;parametric prediction process;parametric prediction;probabilistic agent;parametric prediction problem;prediction process;mobile crowdsourcing;parametric agent;agent model;optimal estimation;prediction;agent learning process;truthful estimations;prediction algorithm;optimality;multiple agents;game theory;informed estimate;prediction performance;private information;stochastic agent;multi agent system;stochastic information exchange"}, "a2ea2261bd56ae2505750d7571b501d9836175f0": {"ta_keywords": "complementary acoustic systems;automatic speech recognition;noisy speech recognition experiments;complementary system;parameter update algorithm;mutual information criterion;lattice;new approach;method;paper", "pdf_keywords": ""}, "39456ca31a530d85ec182b2676dc94266dada597": {"ta_keywords": "compositional distributional semantics;standard semantic similarity tasks;tensors;rank tensors;similarity;representations;machine learning models;rank approximations;words;models;full matrices;parameters;performance;new class;number", "pdf_keywords": ""}, "bc8e67d532693818eb33aa8e401260fe2b774a18": {"ta_keywords": "asymptotic behaviour", "pdf_keywords": ""}, "29dc4b10e60ed1e2b84598cc6f2622c786841fdf": {"ta_keywords": "mobile robots;control;framework;article", "pdf_keywords": "linear linear temporal logic systems;temporal logic specifications;quadratic control barrier function;control barrier functions;signal temporal logic specifications;linear temporal logic;control architecture;temporal logic;barrier function;mobile robotic systems;local signal temporal logic tasks;mobile robots;quadratic programming;control methodology;controller framework;quadratic program;robotics;constrained reachability sequences;control flow;quadratic programming model;quadratic programs;novel linear control method;robust control;control inputs;control flow trajectories;linear control solutions;abstraction;control system;robotic manipulation;control parameters"}, "3f0f6c19c6f5d4e4d5066984c5f3e922a2c2ff85": {"ta_keywords": "sparse reinforcement learning;language abstraction;reinforcement learning;language;language instructions;agents;exploration;training strategy;ella;complexity;novel approach;essential features;relevant classifier;ability;accuracy;absence;high degree;low degree", "pdf_keywords": "level reinforcement learning;reinforcement learning;reinforcement learning experiment;novel reinforcement learning framework;language abstraction;reinforcement learning paradigm;level instruction;language abstractions;sparse task;optimal subtask rewards;reinforcement learning approach;learning strategy;language instruction;level tasks;intrinsic rewards;level task;subtask reward;optimal policy;learning;level behaviors;intrinsic motivation methods;intrinsic rewards towe;level policy;level primitives;exploration;level instructions;reward;instruction;learning rate;intrinsic motivation"}, "81ea04f822a1d5317e5846783900ac424a8f7528": {"ta_keywords": "autism spectrum disorders;single utterances;automatic classifiers;full narratives;children;features;characteristic differences;differences;comparative study;types", "pdf_keywords": ""}, "98fdc7e1e167eb465cdb1c8ee0800db750101155": {"ta_keywords": "observed prosodic changes;prosodic changes;prosodic features;utterances;many speech samples;speech;single speaker;experimental evaluations;same sentence;results;method", "pdf_keywords": ""}, "ae25ca24eb6c2772ef88e2d0315fc428feb8553e": {"ta_keywords": "information extraction process;knowledge base;entities;meaningful sets;clusters;data;high accuracy;novel method;performance;method;experimental results;new method", "pdf_keywords": ""}, "689ab475e8a0f552bf6e39a2f774d9d20e50b9cb": {"ta_keywords": "entangled state;single photon;level atom;laser pulse;generation;new method;method;use", "pdf_keywords": ""}, "38a73e6f48d057cb58264f5148f8b05522d0d030": {"ta_keywords": "semantic parsing;machineable meaning representation;natural language;nl;mller;task;applications;number", "pdf_keywords": ""}, "045f90129a8d7148eec4a58770bc4166b51330ca": {"ta_keywords": "parking demand;spatiotemporal characteristics;spatial autocorrelation;parking spot;spatial auto;bayesian mixture;empirical data;spatial variable;bayesian approach;demand;mixture;time;method", "pdf_keywords": ""}, "fd0aa185be4e1f1fe3975779aec179348ec19ea8": {"ta_keywords": "gradient synchrotron model;review paper;reviewers self;research papers;affiliations;review process;papers;visibility;authors;quantitative study;arxiv sample;arxiv;majority;report;clarity;icml sample;degree", "pdf_keywords": "accurate peer review;online preprints;blind reviewing;online posting;publishing research;blind computer science conferences;peer review process;accurate reviewers assignment;submissions;publishing rate;publication;reviewers;researchers;research papers;scientific journal rankings;tier computer science conferences;internet conference;reviewer;reviewwe;research;online format;review process;new research;paper visibility;preprints;lower rank paper;higher rank paper;papers;online search;namelythe recent advances"}, "f837bf72e5b864e1c162e924fed59b778e946e23": {"ta_keywords": "aware embeddings;classic guessing game;mystery game;encoders;game;accuracy;category;guesswhat;auto;downshot;art competitors;context;scenario;novel approach;approach;state;problem", "pdf_keywords": "aware embeddings;conceptual representations;object representation;novel perceptual module;aware objects;embeddings;latent conceptual representation;deep learning embeddings;perceptual information;deep learning potential;semantic representation;imagination module;deep generative model;objects;deep learning;novel representation learning approach;context information;novel deep learning framework;imagination;categories;object;autoencoders;guesser accuracy;inference time;quantum object;game model;category;context;aware latent;encoders"}, "df4e3aa275b8f81e22a5332ab550805083094dae": {"ta_keywords": "parallel neurons;parallel images;neural network;rich unsupervised generation;translation;computational model;model;data;nms;set;essential features;large number", "pdf_keywords": "neural machine translation;minimum timeneural machine translation;efficient translation;neural machine;natural language processing;nonlinear evolutionary tracks;nonlinear evolutionary models;linguistic structure;translation process;new documents;neural networks;neural attention model;parallel datasets;document;comprehensive knowledge discovery;training data;linguistic function;documents;parallel computing tasks;generation;nle;linguistic noise;corpora;translation;new generation;high quality documents;third workshop;algorithmic advances;models;following tasks"}, "e5efd7e2087e58c5a8860398dfcf143aa9dc865e": {"ta_keywords": "event detection;supervised label inference;deep neural network;boundary detection;event;unlabeled data;supervised methodologies;scale weakly;score;hybrid bottom;absolute improvement;advantage;hybrid approach;large amounts;benefits;framework;approach", "pdf_keywords": "acoustic event classification;sound event detection;acoustic event detection;sound event classes;sound events;acoustic events;novel event detection system;complex acoustic scenes;acoustic scene;scene audio;sound categories;sound classes;acoustic signal;accurate event boundaries;audio;event boundaries;restricted labeling system;event classes;classification;boundary detection;classification system;event;acoustic properties;multiple simultaneous events;supervised learning;unsupervised segmentation;recognition method;supervised methods;convolutional neural networks;scenes"}, "0ae93646ad058853eb6424c1dc0ec1559414e5af": {"ta_keywords": "automatic language processing models;automatic verification;data uncertainty;model predictions;instance rejection;difficult instances;model;estimates;methods;method", "pdf_keywords": "deep conversation;rumour verification;deep learning;deep learning models;rumour;predictive uncertainty;deep learning process;epistemic uncertainty;conversation;high uncertainty;uncertain instances;data uncertainty;data uncertainty estimation;epistemic layer;veracity;noisy data landscape;higher uncertainty;uncertainty estimation;supervised learning;uncertainty;model decisions overrumour detection;uncertainty estimates;information;machine learning;validation;predictions;misinformation;accuracy;learning;confidence"}, "0c12e4c611b32997f8be5811021ead80395a7e5c": {"ta_keywords": "lorentz violation;effect", "pdf_keywords": ""}, "9af2264799bdc3490e4650e2f5d126762caf420f": {"ta_keywords": "end speech recognition;decoder baselines;encoder;attention;eev;chan framework;chan;end;novel method;improvement;method;methods;paper;experimental results", "pdf_keywords": "end speech recognition;attention encoder;connectionist temporal classification;deep encoder;recurrent neural network;attention model;novel attention model;encoder;usual attention model;neural network;speech processing;label sequences;encoder layer;long character sequences;noisy speech corpus;decoder;decoders;attention;joint attention;convolutional neural network;term memory;elementary utterance length;recognition;rnn;decoder framework;ctc loss function;output sequences;input frames;speech;acoustic reference frame"}, "9d555ed29496850c4ef8a3facd7dce734c86aae7": {"ta_keywords": "codecompletion tool;standard code editor;open source project;code;natural language model;language input;output;evaluation;input;model;results;total amount;use", "pdf_keywords": ""}, "7d9863258ef44ca8a6b87b68be738f7a83ac849a": {"ta_keywords": "automatic speech recognition;speech enhancement;neural beamformer;asr;single neural network;language models;strong background noise;unified architecture;end systems;end;novel framework;framework;framework simplifies;comprehensive experiments;presence;results", "pdf_keywords": ""}, "c6c18ad62f39060e2547a0b683525e83312d0700": {"ta_keywords": "novel bidding scheme;bidding scheme;paper reviews;incentives;reviewers;large academic conferences;bidding phase;bid distribution;demand;budgets;paper;prices;assignment;prototypical algorithm;analysis;approach", "pdf_keywords": "simple paper bidding mechanism;proportional bidding scheme;bidding assignments;bidding algorithm;bidding scheme;least costly papers;bidding behavior;bidding process;bidding;greedy bidding strategy;costly papers;bids;discrete allocation papers;price paper;incentives;bidders;multiple multiple bids;bidding phases;bidding sequence;paper deals;paper reviews;bid prices;bid;peer groups;ordinary assignment algorithm;priority papers;fair course allocation;assignment algorithm;low probability papers;allocation"}, "be4d47a61fee83d332ca2f3fe097f19f63863d6c": {"ta_keywords": "node clustering performances;stochastic block model;undirected graphs;spectral families;spectral family;cut method;art algorithms;empirical study;state;significant impact", "pdf_keywords": ""}, "554eade16fb6040bbd21a72bacf903245d7458f1": {"ta_keywords": "human reasoning;artificial intelligence system;artificial intelligence;vision;metrics;decisions;ability;research direction;set;reason;terms;paper", "pdf_keywords": "ai;artificial intelligence;artificial intelligence systemwe;human capabilities;machine learning;machine learning system;knowledge;complex decision;capabilities;consciousness;ability;abstraction;machine;causal reasoning;similar capabilities;systems;insights;unconscious decisions;specific skills;thinking;generalizability;generalization;rational thinking;adaptability;tools;system;vision;complex internal machinery ofwe;imprecise;complex situations"}, "b8f5f3c8816ab389c2f366fd8a45603550ea9667": {"ta_keywords": "text mining;biomedical text;genetic association database;researcher;mining;stm;resistance;behavior;behaviors;effectiveness;example;reliability;corresponding methods;system;method", "pdf_keywords": ""}, "5e327c2285ddf2a76d08e5c00d16c7358bc5412c": {"ta_keywords": "conference peer review;submission;evaluator;assignment;expertise;quality guarantees;strategyproofness;dataset;time algorithms;methods;problem;amount", "pdf_keywords": "peer assessment;peer grading;assignment algorithms;optimal assignment;wise optimal strategyproof assignment;decentralized assignment;arbitrary assignment;evaluation tasks;standard assignment process;conference peer review;peer;peer review;assignment algorithm;strategyproof assignmentwe;best possible peer;strategyproof assignment;assignment;empirical evaluation;evaluations;partition assignment;similarity assignment;several algorithms;experimental evaluations;evaluation results;fast algorithms;optimality mechanism;employee performance;new partitioning algorithm;submissions;paper scores"}, "f878a7c756b90c0ed612838492fbbc02ecaaab70": {"ta_keywords": "interleaved problem order;effective learning;next problem type;interleaved order;skill;student;problems;problem;effectiveness;experience;performance;order;type", "pdf_keywords": ""}, "1ee276db29ba9127e81d9a7d9cb08f5138339412": {"ta_keywords": "dimensional product codes;optimal compute time;optimal computation;scheme;order;costs;novel", "pdf_keywords": ""}, "34cb1f081c1d1d6b3dc16a9278940a9ee85fb2e0": {"ta_keywords": "machine translation interpreter;machine translation output;simultaneous performance estimation;simultaneous performance;interpreter;languages;quantitative estimation;quantitative model;quality;model;quantitative approach;validity;several experiments;approach", "pdf_keywords": "machine translation output;interpreter performance;interpreter features;simultaneous interpreter performance;machine translation;interpreter output;interpreter;translation quality;novel interpreter;translation systems;translation data;large corpus;language pairs;european parliament transcript;interpretation strategy;output;prediction accuracy;quality estimation;qe pipeline;evaluation measures;language settings;pipeline;meteor evaluation metric;performance;accuracy;sentence;feature;quantitative quality measure;results;accurate rendition"}, "839cbcf5c13d5875e952e40ec2da14b19eee2202": {"ta_keywords": "smooth convex optimization problems;unconditional optimization problems;accelerated descent method;conditional optimization problems;full gradient;convergence;random direction;method;estimates;rate;approach;structure;difficulties", "pdf_keywords": "condensate interaction;nonlinear potential;atomsthe dynamics;nonlinear dynamics;small repulsive interactionthe dynamics;harmonic potential;quasithe dynamics;nonlinear charge;nonlinearity;random phase transition;harmonic oscillator;quantum fieldthe transition;nonlinear wave;nonlinear one;periodic potential;nonlinear system;dynamics;nonlinear dispersion relation;small repulsive interaction;repulsive interaction;liquid phase transition;symmetric potential;large repulsive interaction;modelthe dynamics;nonlinear partial differential equations;condensate;chiral perturbation theory;dimensional phase;nonlinear partial differential equation;dynamical stability"}, "ebcb425ed1a51e8f1a6eca422882abd454fe04f2": {"ta_keywords": "second language learners;learners;classroom;library;students;class;useful information;use;system;performance;set", "pdf_keywords": "word sense disambiguation;rich lexical information;target vocabulary;vocabulary learning system;natural language processing;vocabulary acquisition;target words;vocabulary;natural language documents;natural language;word definition;target word;corpora;collocations;semantics;actual word sense;second language learners;word sense;grammar patterns;linggle booster;ambiguous mentions;own words;learners;unsupervised learning;appropriate word definition;adjectives;key words;language skills;common verbs;deep knowledge"}, "4bff8862ba7956fdc2288e8399fb187b9595982b": {"ta_keywords": "gene mention recognition;gene text;annotated corpus;gene expression;scientific text;false positives;performance measure;dfg workshop;performance;results;false negatives;probabilities;science;workshop;measure;ability;uk", "pdf_keywords": ""}, "d8aeb318f68f4635b34c72aa1a0369fadcd79450": {"ta_keywords": "mining user community behaviors;derive user topic distribution;user communities;probabilistic graphical model;twitter datasets;topics;emotional topics;community;sentiments;networks;content;representative behaviors;behaviors;art models;model;experiments;other state", "pdf_keywords": ""}, "8442f9fd620ea34e1de3128b9388bddd1263f29b": {"ta_keywords": "conic optimization;gradient method;infeasibility;exex pipg;order method;novel method;application", "pdf_keywords": "dual conic optimization problem;convex optimization method;constrained optimal control;constrained optimal control problems;convex optimization;convex programming;convex optimization problems;dual hybrid gradient method;conic optimization;nonlinear optimization;optimization framework;new optimal control problem;optimization;conic optimization problem;integral feedback control;optimization results;optimization research;optimization problem;dual method;constrained constrained problem;alternating gradient method;optimal solution;gradient method;gradient method whichwe;quadrotor path planning problem;operational constraints;common objective function;nonlinear variational problems;closed convex cone;constraint violation"}, "ee24fb876e6f1b345d492101c499bc5dd6b8196b": {"ta_keywords": "novel probabilistic signal enhancement method;probabilistic signal enhancement;electroencephalogram;multichannel wiener;eeg;spatial correlation;empirical potentials;signal;empirical evaluation;p300 shows;experimental evaluation;p300;artifacts;method;increase", "pdf_keywords": ""}, "ce0fce520c639af010c71cc6adf57cdeb2790322": {"ta_keywords": "probabilistic black box optimization techniques;automatic speech recognition;automatic speech recognition system;black box optimization;speech recognition performance;neural network;covariance mean adaptation strategy;optimization;word accuracy;conditional conditional distribution;conditional distribution;input;parameters;function;model;effectiveness;paper;use;experiments", "pdf_keywords": ""}, "516a0faeab9ec3a68bc6e7ec13a2df235a27ab52": {"ta_keywords": "unstructured clinical data;accurate diagnosis decision making;convolutional neural network;diagnosis;possible diagnosis problems;clinical notes;data;network;incomplete list;network performance;novel method;quantifies;method;ability;presence", "pdf_keywords": "cnn model;novel cnn model;cnn;disease coreference resolution;text classification model;neural network;clinical information;neural networks;convolutional networks;unstructured clinical notes;convolutional neural networks;coreference resolution;health record;clinical notes;novel machine learning framework;supervised learning approach;hospital;clinicians diagnosis decision making;medical diagnosis;bacterial disease;joint imaging dataset;traditional machine learning models;clinical noteswe;dataset;corpus;disease class;discharge diagnosis;prediction;bacterial diseases;text"}, "d3793ae5b3b31f72605978b749e41811e6dcacd4": {"ta_keywords": "autonomous reinforcement learning;constrained policy;agents;policies;reward;constraints;actions;behavior;environment;novel approach;best actions;society;approach;set;novel ways;domain;members", "pdf_keywords": "inverse reinforcement learning;reinforcement learning agent;reinforcement learning;simple reinforcement;reinforcement learningwe;reinforcement;traditional reinforcement learning;simple reinforcement learning algorithm;autonomous agents;contextual bandit;reinforcement learning approach;behavioral constraints;bandit policies;reward function;reward;agent model;bandit selection;standard bandit algorithm;agent;environmental rewards;learner;agents;actions;rewards;strategy;bandit selection process;activities;demonstrations;classical game theory;environment"}, "405c0d9b7cf5482d2e1197167f690e7b7801b9bd": {"ta_keywords": "training data;supervised learning;learning performance;hotpot qg;datasets;data;hybrid;framework;context", "pdf_keywords": "unsupervised learning;like questions;sentence generation model;paraphrasing questions;structured questions;natural language questions;text generation;ungrammatical questions;training data;reasoning graphs;quality training data;parallel machine translation;supervised supervised task;shot learning data;performance tp paraphrasing model;paraphrasing;questions;unstructured data;tp paraphrasing model;reasoning chains;unsupervised framework;heterogeneous data sources;qa;supervised model;shot learning approach;qg;supervised learning setting;entities;supervised supervised model;text"}, "7bf2620188c0a66e1d0e779083cf61960a2f3e2f": {"ta_keywords": "multilevel synthesis;timing constraints;synthesis system;combinational logic;standard cell libraries;delay;logic;combination;optimization;levels;technology;system;number;user", "pdf_keywords": ""}, "d04c91bbb043666ebd6dae51995ee5bbc4291ddf": {"ta_keywords": "orbit interaction;spin;orbit;coupled system;dynamics;effect;enhancement;suppression;ways;number", "pdf_keywords": ""}, "b209f2acbf0fbc62a3fd19ad6c13abbd46547736": {"ta_keywords": "new speaker diarization system;diarization accuracy;diarization error rate;diarization track;project;system;levels;der", "pdf_keywords": "speaker recognition;speaker diarization system;speaker extraction;joint speaker recognition;speaker verification;novel speaker recognition system;speaker filtering system;advanced speaker filtering;speech separation system;single speaker;speech separation;speaker;diarspeaker diarization;convolutional neural networks;monaural speakerwe;convolutional neural network;diarization track;segmentation;single utterance;speakers;neural networks;multiple spekaer diarization systems;utterance;extractors;extractor;youtube;vocla challenge;outputs;session;sv"}, "9e3e6ddf958c2005f7041cc9dd5fe050a0dbd02e": {"ta_keywords": "multiscale wavelet transform;similar signals;crossing points;scales;correspondence;plausible correspondence;cost function;dynamic programming;practical range;computational complexity;mwt;points;accuracy;method;paper;result", "pdf_keywords": ""}, "4ec1d3407a5136c525b53f703c803571200902a4": {"ta_keywords": "spin dynamics;electron gas;spin;perpendicular magnetic field;magnetic field;effect;field;presence", "pdf_keywords": ""}, "f2818da69bb72526fff9d601677db38f24a62ecc": {"ta_keywords": "dialogue modeling framework;user satisfaction prediction;multiple response candidates;best system response;user satisfaction;response pairs;system response;system responses;system response pairs;user feedback;user query;prediction;evaluation;query;example;examples;methods", "pdf_keywords": ""}, "febb305a854d02b138250a8a19af956ffa0ada4f": {"ta_keywords": "convergence guarantees;nash equilibrium;gradient algorithms;continuous action;policy;state space;sufficient conditions;counterexamples;existence", "pdf_keywords": ""}, "f21a9d70319ca99227300349d7bcab5dee5869cd": {"ta_keywords": "missing source translations;incomplete multilingual corpora;source neural machines;source translations;translation accuracy;translation;nmts;special symbol symbols;mixture;simple implementation;use;method", "pdf_keywords": "incomplete multilingual corpus;incomplete multilingual corpora;real incomplete multilingual corpora;multilingual corpus;full multilingual corpora;neural machine translation;incomplete corpora;source translations;multilingual language pair;multisource nmt;nmt;translation problem;translation;one nmt;nmtwe;input sentences;sentences;incomplete situations;consistent improvements;null;input sentence;bleu scorewe;talks;ted talks;network;machine learning method;training time;empirical structure;empirical data;mixture"}, "b168fc72fa39e9669567bd099bab179549a15e14": {"ta_keywords": "a2gp1 iga;iga;patients;bearing;incidence;clinical practice;study;results;data;literature;context", "pdf_keywords": ""}, "de43afd166a79c24b3a7dd16c5695059d9f0aa71": {"ta_keywords": "cognitive development;cognitive learning;child;emergence;age;transition;new concept;transitivity;concept;study;new approach;stage;timescale", "pdf_keywords": ""}, "ab94fae3d49cd7016a47020469dc257d8090f5bb": {"ta_keywords": "challenging speech separation;deep clustering;speaker;enhancement layer;deeper signal approximation;deeper architecture;signal estimates;end signal approximation;end training;model;new approach;performance;end;temporal context;approach;problem", "pdf_keywords": "independent speaker separation;speech separation;speaker training;deep clustering training;deep clustering;network speaker system;speech recognition;monaural source separation;speech recognition error rates;speaker system;speaker;best speaker;recurrent network units;separation;deep clustering framework;speakerwe;deep recurrent neural network;speech;better regularization;noisy mixture;speakers;deeper architecture;mixture;term memory;noisy noisy crowd;end signal approximation objective;neural networks;source partition;natural language processing;clustering"}, "6c477a65f0922d405c3665e31581eaa0f269116e": {"ta_keywords": "word representation;word representations;distributional semantics;parallelization;parallelization method;representations;word;relational objective;performance;method;situations;class;essential features;variety", "pdf_keywords": "word embeddings;word representations;wordnet;knowledge base completion;relational semantics;base embeddings;nnn model;dimensional semantic data;tensors;semantic data;neural network;relational objective;analogy tests;hypernymy;knowledge;extrinsic features;distributional objective;nnn;model;direction;raw text;training results;intrinsic structure;admm;complexity;joint objective problem;plausible data;essential features;direction method;new type"}, "10085f7fb0871329d34529cc54df0a8f75756fce": {"ta_keywords": "automatic transcription;style training texts;languages;language;reference texts;national congress;labels;minutes;style transformation;sst;documents;lsv;new framework;framework;characterization;approaches;estimation method", "pdf_keywords": ""}, "248824ec5d9b4ddf0c36cdc51b6b57af6e881328": {"ta_keywords": "typical natural language processing task;feature;features;experimenter;essential features;model;intuition;standard strategy;problem", "pdf_keywords": "language prediction task;rank languages;candidate transfer languages;transfer languages;best transfer languages;optimal transfer languages;potential transfer language;promising transfer languages;lang rank model;task language;machine translation task;tasklanguage;crosslingual tagging;languages;resource task language;crosslingual morphological tagging;machine translation;different linguistic features;language;promising languages;natural languages;neural machine translation;lang model;different linguistic distances;nonlinear linguistic processing;ranking problem;best translation strategy;natural language processing;linguistic structure;corpus statistics"}, "70170035ef870df1c064cc52804178a52f6a69ef": {"ta_keywords": "zeeman field;effect", "pdf_keywords": ""}, "75c4aefc55bf0b345587740cad0a4e994f29962a": {"ta_keywords": "polyphonic sound detection;sound activity detection network;bidirectional long shortterm memory recurrent;sound activity;frame detection method;neural network;rnn;hidden model;dataset;frame;segments;system;new hybrid approach;method;evaluation;paper;conventional methods;methodology;experiment", "pdf_keywords": ""}, "cc19de8d0782917098029ed20261cbe0b0c62bf5": {"ta_keywords": "peer reviews;peer review;biases;bias;subgroup membership indicators;hiding policy change;nonparametric framework;identification strategy;secondary data;observations;ground truth;identity;visibility;text;time periods;application;framework", "pdf_keywords": "peer review text;peer review;peer review ratings;peer reviews;national science foundation peer review system;review text;blind peer review;cambridge linear peer review network;biases;bias;quantifies biases;affiliation bias;subgroup membership indicators;significant bias;reviewers;unobserved confounders;empirical findings;implicit gender bias;linguistic feature;empirical discovery;counterfactual differences;review textwe;economic bias;summary text;review ratings iswe;discovery;scientific research;evaluation;differences causal inference methodology;identifying attribute"}, "8484fdb56e4690927dc0191ede11c2d24bc5e2ef": {"ta_keywords": "text generation;human text;text;divergence frontiers;novel measure;distribution;quantitative measure;measure;differences;difference", "pdf_keywords": "modern text generation models;text generation;neural autoregressive language models;ended text generation;autoregressive language models;neural language models;text generation system;web text generation;language modeling;generative models;machine text;standard language modeling;human text;model text;human text analysis;large synoptic text sets;large synoptic text;generation length;unstructured text;generation quality;degenerate text;natural language processing;text;embeddings;ancestral sampling;quality text;information divergences;human annotation;distributional evaluation metrics;several communication generation approaches"}, "aff5d7f43823e06bb68220db41de3bc82e2f3990": {"ta_keywords": "small cell networks;network model;micro base stations;network structure;mobile users;network;static users;heterogeneous structure;larger cells;convergence properties;users co;macro;average number;model;various assumptions;ss", "pdf_keywords": ""}, "45cdf5e239a1f0057c350f6654ccd348fb4e2332": {"ta_keywords": "uncertain preferences;indifference model;compact indifference model;lottery model;matching;uncertainty;stability probability;joint probability model;choice;models;unknown order;problem;significant function", "pdf_keywords": "stable matching exists;stable matching problem;stable matchings;stable matching;optimal matching;possible matching;uncertain linear preferences;uncertain preferences;matching matching matching;matchings;preference distribution;stable marriage problem;matching;indifference model;uncertain agents;lottery model;compact indifference model;different uncertainty models;stability probability;uncertainty model;linear preference;uncertainty;joint probability model;indifferences;compact indifference;stable lists;joint probabilitywe;preference profiles;lottery;stable behavior"}, "4731f89169604cd0d8b5352380baa1b4728bca0b": {"ta_keywords": "machine translation;discriminative language models;effective error analysis;mt;frequency;systems;standard frequency;analysis;effectiveness;method;use;experiments;new method", "pdf_keywords": ""}, "601408d6617bf72894c9f41ae54cf9c17905903a": {"ta_keywords": "translation;english pairs;t2s;tree;string;accuracy;par;systems;basic system;peripheral elements;bleu;number;paper;art methods", "pdf_keywords": ""}, "3dc20be709818630e2249ab28b35b0666b4b544d": {"ta_keywords": "paralinguistic information;paralinguistic process;conventional speech;focus;sentence;information;later processing;study", "pdf_keywords": ""}, "ead6323f137c2f99ef0ffcfa34fa6eb1c6eca3c6": {"ta_keywords": "unsegmented speech recognition;speech recognition;speaker diarization;recognition;speaker;challenge;problem;aspects;steps", "pdf_keywords": "distant conversational speech recognition;distant microphone speech recognition;automatic speech separation;speech recognition;speaker diarization;new speaker diarization system;empirical speaker diarization dataset;speech separation;automatic speech recognition;speech datasets;speech enhancement;voice;deep learning;speaker;chime6;speechwe;signal separation;scale speaker level;diarization;underlying speech;recognition;distant conversationalwe introduce;noisy noisy noisy array;speech;recognition researchers;chime;neural network;noisy noisy noisy systems;noisy array;machine learning setting"}, "af5c4b80fbf847f69a202ba5a780a3dd18c1a027": {"ta_keywords": "natural language inference;commonsense reasoning;inference;potential counterfactuals;inference problems;art language models;high accuracy;humans;data;various competitive models;novel approach;diverse set;comprehensive analysis;state", "pdf_keywords": "commonsense inference;commonsense reasoning;natural language inference;human reasoning;adversarial dataset;linguistic entailment;natural language;inference;adversarial filtering;adversarial filtering process;human biases;natural language documents;effective dataset construction;linguistic features;other datasets;annotation artifacts;lemma learning;datasets;new dataset;nonlinear learning tasks;dataset;large corpus;biases;deep learning;context;learning;annotation artifact;unintuitive examples;linguistic structure;simple context"}, "d15eb5744474cec2d0634651bb30000b3873a309": {"ta_keywords": "time expression normalization;twitter;tm benchmarks;art normalization methods;temporal value;rules;operations;sequence;model;novel method;method;state", "pdf_keywords": "normalizing time expressions;time expression normalization;time expressions;normalization rules;normalization;natural language;corpora;time fields;good normalization result;time expression;semantic structure;temporal fields;time field;unstructured representation;semantics;annotated tweets;time term;temporal behavior;artime;document creation time;temporal value;timewe;basic operations;new rules;art rule methods;time;expression;operations;dataset;empirical data"}, "3b563c16e9a918631d63a20027dad735b625625a": {"ta_keywords": "text generation;text generation tasks;versatile toolkit;toolkit;popular interoperability paradigm;arbitrary model architectures;easy reuse;components;modular structure;novel feature;different algorithmic paradigms;features;distribution;powerful feature;flexibility;broad set", "pdf_keywords": "extensible text generation toolkit;text generation tasks;unstructured text generation;text generation;neural machine translation toolkit;versatile toolkit;powerful toolkit;machine translation;natural language documents;extensible toolkit;toolkit;arbitrary text;text style transfer tasks;natural language;text;texar;automatic captioning;large scale machine learning;vector learning engine;generation;summarization;content manipulation;generative hierarchical neural network model;language model;dialog;tools;diverse tasks;deep learning;simple translation algorithm;unified machine learning models"}, "a8372f7cb2e482a455b06c3e47f65aec5c7a924b": {"ta_keywords": "circulation circuits;pump efficiency;bismuth eute;future large scale liquid;pump;bismuth;lbe;critical step;commercial simulation;potential;design;near future;context", "pdf_keywords": ""}, "0c07cc7ba1b862556f5cfee0d5d849866d21a693": {"ta_keywords": "oblivious updates;storage networks;mft codes;mft;codes;communication requirements;algorithm;communication;problem;important subclass;terms", "pdf_keywords": "oblivious update protocol;oblivious updates;oblivious update;such oblivious updates;arbitrary memory encodings;stale node needs;information storage;storage node;storage code;storage codes;data storage;storage;storage systems;stale node;storage network;arbitrary code;linear encoding;nodes;stale messages;memory storage system;stale message;lower bounds;memory;data download;infinite connectivity;message symbols;algorithms;database;communication;data symbols"}, "9650dbe79d34498113371770dcdb48f1bd7c9711": {"ta_keywords": "information retrieval;term extraction;term similarity;journal maps;natural language processing techniques;research papers;research topics;journal;research group;database;paper;field;system;new system;applicability;creation;methods;title;combination;calculation;extensible system", "pdf_keywords": "visualization system;bibliographic data;visualization;document similarity;several possible explorative visualization uses;text collections;graph maps;semantic retrieval system;large relational data sets;large text corpora;maps;geographic map metaphor;graph clustering;term extraction;map;heatmap;map representations;visual exploration;word content;clustering;heatmaps;natural language processing;topic space;word similarity;clusters;word extraction techniques;research tools;simple clustering structure;semantic preservation;information processing"}, "889c3b4394826639d483c039467cd9a05e68e73c": {"ta_keywords": "generative model;partial musical scores;composition;neural network;convolutional neural network;model;likelihood;personalized way;initial score;task;initial step;log;procedure;version", "pdf_keywords": "part chorales;melodic matching;standard model chorales;chorales;stochastic gradient descent;part chorale;polyphonic music;stochastic amplitude matching process;music generation;random matrix theory;orderless neural models;ancestral sampling;musical scores;music;generative models;generative stochastic network;direct ancestral sampling;stochastic transition operators;random matrix;neural autoregressive distribution estimator;bach;generative model;flexible generative model;stochastic;convolutional neural networks;lambda neural model;contemporary music;harmonic transitions;neural networks;stochastic model"}, "68af273e04906e0450a5d01d5606c8313da01453": {"ta_keywords": "stochastic subset selection;stochastic subset selection problem;sensor networks;stochastic approximation;sensors;statistics;data;mean number;novel approach;case;methods;strategy;terms;number", "pdf_keywords": "stochastic sensor selection;sensor subset selection;sensor networks;stochastic approximations;sensor data;stochastic approximation;multihop sensor network;sensors;sensor;active sensors;active sensing;sensorwe;efficient algorithms;expectation maximization;algorithms;optimizer;optimal error;gibbs;unconstrained optimization;optimal solutions;markov chain;data estimation error;iterative propagation;novel algorithm;estimation error;iterative algorithm;heterogeneous parameters;activation probability matrix;algorithm;gaussian noise"}, "04f8f739924a19c01d196a48783b914554ac0fe5": {"ta_keywords": "composite convex optimization;domain newton methods;order algorithms;lower approximation;convergence;iteration counter;contracting;method;affine;methods;objective;global rate;order;smooth component", "pdf_keywords": "composite convex optimization;stochastic convex optimization;domain newton methods;composite convex function;stochastic hessian;stochastic optimization;convex functions;general convex functions;empirical risk minimization problem;hessians;unconstrained quadratic optimization;convexity class;stochastic algorithms;convex combination;stochastic average gradient;domain convergence rate;quadratic regularisation approach;stochastic algorithm;theoretic method;lower approximation;tammann method;order algorithms;order method;algorithms;optimization problem;superlinear learning model;stochastic extension;present computational results;global convergence;linear algorithm"}, "86ae1161026f23f9df691a867fd7453cee56fd28": {"ta_keywords": "lexical change;lexical landscape;phylogenies;meaning;evolution;approaches;models;examples;model;collection;subject;recent work;article", "pdf_keywords": "semantic change;evolutionary linguistics;historical linguistics;semantic shift;language change;lexical cognate models;phylogenetic word embeddings;biological meaning classes phylogenies;lexical replacement;phylogenies;language structures;lexicon;linguistic systems;cognate evolution models;languages;language;vocabulary;phylogenetics;word embeddings;ancestral states;phylogenetic splits;phylogenetic trees;phylogenetic relationship;words;changes;phylogenetic analysis;language users;evolutionary approaches;word items;typology"}, "5b8eaaf660b9e2d6a19886991350fffa1320b372": {"ta_keywords": "entities;sentences;relations;inference;local classifiers;ilp;training;integer linear programming;study;different approaches;ibt;others;experiments", "pdf_keywords": ""}, "781e0e81834119c135091c8bdfcd1966c10b09ab": {"ta_keywords": "new integer compression scheme;stateoftheart compression;modern processors;common processors;32bit integer;simd instructions;cpu cycles;singleinstruction;simd;speed;multiple data;stateoftheart approach;instructions;scheme", "pdf_keywords": "fastest integer compression;integer compression;integer compression instructions;new integer compression algorithm;integer compression schemes;integer compression scheme;efficient data compression;integer prefix sum compression;compression instruction;new compression scheme;secure index compression algorithm;index compression;index compression framework;multiple integer encodings;efficient computation;simd algorithms;efficient algorithms;compression technique;compression;fast algorithm;parallelization;fast unstructured lists;fast intersection algorithm;differential coding;decompression speed;binary format;execution speed;short algorithm;higher computational cost;bit integers"}, "65f632cbac465633a13b1e3f8c8c410c2f3aec3d": {"ta_keywords": "theoretic reinforcement learning;novel game;algorithm", "pdf_keywords": ""}, "76862a851bd2c17dcf6bfc2cecbf4af186730123": {"ta_keywords": "grayscale document image;nontext objects;halftone images;graphs;paper;connected operators;tables;unconventional method;solution;method;use;shortcomings", "pdf_keywords": ""}, "1a20d6c6891f3a0462515ff9560bc37e66eb422a": {"ta_keywords": "spectral density;strong magnetic field;proton;schrdinger equation;consistent mean field approximation;approximate solution;continuum limit;exact solution;calculation;method;approach;presence;new approach;predictions;self;results", "pdf_keywords": ""}, "68258e0541132027ef86f872b92406de1c6edab3": {"ta_keywords": "circularly symmetric generator;gaussian noise;generator;stability;noise;pacs numbers;effect", "pdf_keywords": ""}, "b1d309073623d46548e55269fb73485a3b7f11a8": {"ta_keywords": "totipotent language model;language model;linguistic knowledge;developmental process;embryology;different learning speeds;speech;world knowledge;downstream tasks;different parts;tokens;parameters;set;results;proceeds", "pdf_keywords": "token reconstruction;language model;totipotent language model;linguistic tasks;natural language processing pipeline;input tokens;nonlinear linguistic interactions;early learning;linguistic identitieswe;tokens;pretraining;natural language;different linguistic identities;bert;speech;natural language processing;syntactic knowledge;nlp;machine learning model;additional pretrain steps;linguistic structure;token nodes;novel representation;mask prediction;early training;exotic knowledge bases;knowledge bases;models;early pretraining stage;machine learning system"}, "0110abf15bf0ee1bdf28061ad05f85b1c9f6e1c3": {"ta_keywords": "structured information sources;similarity measures;similarity;databases;documents;information;text;query;web;integration;observation;veneer;novel method;use;general technique;method", "pdf_keywords": ""}, "a3da7028a1b721e392c421c2f15096abb1a71afb": {"ta_keywords": "atherosclerotic plaque vulnerability;lipid variability;plaque vulnerability;elevated glycated hemoglobin a1c;lipid profiles;lipid metabolisms;elective coronary intervention;glucose;hba1c;multimodal multimodal regression analysis;hb;analysis;pci;influence;patients;visit;results;close association", "pdf_keywords": ""}, "3ed07f6643856b9ac4687b3bc667767f3ab4b563": {"ta_keywords": "voice quality control;voice quality;appropriate voice quality expression words;regression mixture model;acoustic features;perceptual scores;control parameters;independency;method;report;choice", "pdf_keywords": ""}, "ecde7c041e9ac48bccef7a8d078a3f80239b0479": {"ta_keywords": "object detection;frames;video;temporal context;convolutional neural network;improvement;predictions;consistency;new framework;domain;baseline baselines;subset;approach;average precision", "pdf_keywords": "object classification;object detection;video sequences;object classification problem;video data;video streams;recurrent neural network;convolutional neural network;video tubelets;video;deep convolutional neural networks;recurrent neural network model;neural network;recurrent model;objects;videowe;frames;target frame;pattern recognition;consecutive frames;class;accuracy;object object setting;background signal;classification problem;classification process;objectness confidence score;real time;prediction;neighboring frames"}, "085072963b33367b842369b9ce81394d32ac8843": {"ta_keywords": "channel speech separation system;separation error;separation;noisy environment;speech;noise;mixture;training strategy;complex environment;system;different sources", "pdf_keywords": "channel speech separation;end speech separation;noisy speech network;noisy speech signal;separation models;channel speech corpora;noisy ground truth source;speech quality;separation;noise content;noisy mixtures;domain separation;field speech;noise signals;background noise;pure noise;real speech;conversational environment;noisy noisy signal;noisy signal;speech;noise;noise ratio;synthetic mixtures;synthetic mixture;background signal;training data;noisy oracle source problem;mixture;signal waveforms"}, "76fe5f80dd25078eefa522e59a7763bc5d5da826": {"ta_keywords": "gaussian field;field;dynamics;simple model;component;generalization;model", "pdf_keywords": ""}, "9165d5e99b2106825dd00b9f5daf60e454434399": {"ta_keywords": "translation data;simultaneous interpretation systems;other simultaneous interpretation systems;simultaneous interpretation system;different languages;description;performance;collection;large number", "pdf_keywords": ""}, "23d299b35366c18e397faeb2c8687c20f8e17688": {"ta_keywords": "deception attack;attack detection algorithm;detection scheme;deep neural network;detection;principal component analysis;image classification;low false alarm rate;dnn;preprocessing algorithm;novel preprocessing technique;low computational complexity;physical systems;random perturbation;certain combination;paper", "pdf_keywords": ""}, "72302d8c5cdcf59b6df96290ffc874d3613fe6b1": {"ta_keywords": "particle cascade model;cascade;cascades;particles;onset;model;time;method;simple method;case;use", "pdf_keywords": ""}, "12e9d005c77f76e344361f79c4b008034ae547eb": {"ta_keywords": "textual sequences;gram count vector;dimensional embeddings;several similarity tasks;convolutional neural networks;dimensional character;art architectures;use;new approach;approach;state", "pdf_keywords": "sentence similarity;text similarity;sentence similaritywe;word similarity;charagram embeddings;semantic similarity datasets;linguistic similarity;similarity tasks;several similarity tasks;neural machine translation;gram count vector;embeddings;textual sequences;neural representation;word level modeling;sentence length;word types;level similarity tasks;dimensional embedding;word classification;sentence pairs;deep learning;natural language processing;dimensional representations;compositional semantics;characters;sentences;charagram;underlying text;convolutional neural networks"}, "f053137323a88eb932d590bcdfc959ee805e2520": {"ta_keywords": "dependency parsing;treebanks;dependency tree top;stack;whole sentence;point networks;leaf;depth;novel architecture;first search;root;top;child;first fashion;step;evaluation;model;pstr;status", "pdf_keywords": "dependency parsing;pointer networks;attention networks;simple parsing model;stackptr parsers;joint parsing;parsing;complete sentence parsing;simple parser;real world parsers;parsers;parser;semantic dependencies;neural network architecture;neural architecture;neural networks;dependency structure;neural network;semantic representation;encoders;novel encoder;encoder;syntactic;sentences;stack;multilingual nlp;biological language;corpora;thin films;architecture"}, "a4b1afd75bd2da0b21df58cd4ae1649fefabd8dd": {"ta_keywords": "mobile fitness game;utility learning;simulated game;equilibrium strategy;utility maximizers;multiplayer settings;privacy;individual agent;utility function parameters;game;players;agents;feasible set;data;chicken;theoretic framework;dare;world trial;method;problem", "pdf_keywords": ""}, "cbf9a2560eac548e7b3d5eb7074c40b7bb861909": {"ta_keywords": "end speaker diarization;model speaker diarization;speaker diarization;speech activity;novel conditional learning method;multitask eend system;overlap detection;conventional eend systems;eend;subtask;di;method;performance;paper;terms", "pdf_keywords": "neural speaker diarization;end speaker diarization method;speaker diarization;multivariate speaker diarization;independent speaker diarization;first speaker diarization method;speaker activity distribution;automatic speaker recognition;speech activity;blind blind speaker diarization;speaker signal;speaker;multitask evaluation;multitask mechanism;conditional learning approach;novel multitask;speaker setting;diarization error rate;speech;overlap detection;signal processing;subtasks;subtask;hard task search;novel subtask;subtask order;task evaluation;neural network;speakers;deep neural networks"}, "e9dfccd86b6116f7601d44590985de2df434a094": {"ta_keywords": "tutoring;student explanations;student responses;student learning;students;successful learning;adaptive help;study;feedback;social factors;hints;problems;relationship;accuracy;results;quality;crucial component;article", "pdf_keywords": ""}, "c933fed82e7b5cbf7230f0f970b69590b40f86a1": {"ta_keywords": "decentralized stochastic optimization;stochastic averaging;pairwise gossip updates;universal convergence rates;sdas;smooth problems;local updates;asymptotic results;iid;rates;data settings;approaches;data;many special cases;paramet;new framework", "pdf_keywords": "decentralized stochastic gradient descent;decentralized stochastic optimization methods;decentralized stochastic optimization problem;random gossip algorithm;gossip algorithm;stochastic gradient descent;decentralized gossip;stochastic gradient descent algorithm;pairwise gossip updates;novel stochastic convex optimization algorithm;decentralized stochastic differential equation;stochastic gradient;gossip averaging;stochastic subgradient method;stochastic gradient method;stochastic subgradient;stochastic optimization;stochastic graph theory;stochastic optimization methods;noisy noisy networks;stochastic noise;adaptive network topology;decentralized sde;convex noiseless;decentralized aggregation;strong convex optimization;optimal convergence rates;stochastic optimization problem;federated learning;convex optimization"}, "91d98b0a175237b48122e7560010e87a968fb6e0": {"ta_keywords": "robust speech recognition;nonnegative matrix factorization;deep computational architectures;deep neural networks;recognition;speech;noise;separation;level enhancement;performance;signal;environments;conventional statistical techniques;superior results;problem;use;variants", "pdf_keywords": ""}, "cf8f2ca0c2d618104bc8724a6effc509088f16c4": {"ta_keywords": "language learner;learner;machine learning system;machine learning;neural model;nell;new paradigm;properties", "pdf_keywords": ""}, "cc7858e74a79edceb5a42c30fc5c2dc5117f365b": {"ta_keywords": "generative adversarial tree search;generative tree search;generative networks;learned model;planning;model;algorithm;environment;gats;novel approach;performance", "pdf_keywords": "high dimensional linear reinforcement learning;deep reinforcement learning;generalized neural network;generative adversarial tree search;reinforcement learning model;deep generative models;gan model;generative model;reinforcement learning;policy learning model;optimal reinforcement learning;generative dynamic models;reinforcement learning framework;gan;nonlinear agent learning model;standard generative dynamical model;gaussian;gaussian matrix;exploration;gdm;deep quantum network;neural density model;exploration process;positive definite reward formulas;learning process;reinforcement;popular gdm framework;large markov decision processes;positive definite reward;global selection"}, "82cb0c428f5edb1db6e733dc4b1b20023a2ce15f": {"ta_keywords": "voting systems;voting rules;netflix prize dataset;election data;preferential preference;preference;continuous preference;statistical models;available data;testing;domain restriction;efficiency;theories;main classes", "pdf_keywords": ""}, "f0bbc7b84c166e2258b6ba4f9d9835ecac04e842": {"ta_keywords": "spontaneous speech recognition;speech recognition;coupled acoustic oscillators;clustering;coupled nonlinear oscillators;coupled nonlinear oscillator;practical probabilistic framework;discrete model;coupled system;variance;model;inference;framework;problem", "pdf_keywords": ""}, "19b6537012412bee0a36e3e271f84b95868fe859": {"ta_keywords": "explainable neural networks;ad hominem;arguments;explainable neural network architectures;semantics;argument;linguistic insights;hypotheses;typology;triggers;experiments;paper;ibm settings", "pdf_keywords": "novel argumentation strategies;ad hominem arguments;first level ad hominem arguments;noisy argumentation;argumentative dynamics;argumentations;argumentative remarks;argumentation;onwe study argumentation;argumentative arguments;argumentative properties;argumentative role;computational linguistics;arguments;ad hominem;incorrect argumentation;bidirectional neural networks;discourse participants;correct argumentation;rhetorical triggers;discourse;argument;discoursewe;neural models;single argument;lively debate;sentence structures;annotated posts;fallacies;neural model"}, "36a5e0e0a8ce67e4cd9077d86e3b4d50fdcff15f": {"ta_keywords": "energy splitting ofcatalysts;new energy efficientcatalysts;energy splitting;acatalyst;key factor;design", "pdf_keywords": ""}, "d3e13d2514edaf74b863bfbe45a739c32a7689e1": {"ta_keywords": "neural code generation model;code generation tasks;code examples;code;subtree retrieval;natural language;explicit reference;method;performance;approach", "pdf_keywords": "neural code generation;neural syntactic code;neural code generation model;code generation tasks;neural machine translation method;gram action subtree;subtree retrieval;code examples;natural language;automatic generation;action subtree retrieval method;subtree;purpose code;tree;code;intrinsic language;trees;neural model;neural structures;high level sketch;surface code;generation target;retrieval;novel features;text;lowlevel sketch;copy mechanism;obvious linear structure;generation;novel method"}, "ba3322280992d0425bc9e2b4c59de24857e5f4e7": {"ta_keywords": "performative risk minimization;risk minimization;classifier;performative alignment;weights;process;trajectories;various equilibria;loop behavior;attraction;convergence;geometric condition;data;sufficient conditions;notion;problem;region", "pdf_keywords": "strategic classification task;local performative risk minimizer;local performative risk minimizers;risk minimizer;risk minimization;risk minimizers;classifier;perturbative risk minimization;decision variable;global optimal point;local optimal point;local performative risk;performative risk;generalization;multiple local minimizers;successful predictions;risk;dependent distribution shifts;predictions;stochastic model;gradient;prm gradient flow;dependent distribution shift;information;state behavior;stable point;term behavior;class;dependent distributions;various equilibria"}, "3c6670ecdfccd4633755c4b19d774453bfb77de3": {"ta_keywords": "fairness;donors;organs;same patient;patient;problem;different types;number", "pdf_keywords": ""}, "d79b613a67cf79740e1c08037f7d054585a12284": {"ta_keywords": "end speech translation;encoder;encoder architecture;benchmark;e2e;incremental generation;conformer;orthros;performance;training methods;st;framework;experimental evaluations", "pdf_keywords": "end speech translation task;connectionist temporal classification;nar decoder;end speech translation;auxiliary shallow decoder;language model;improved encoder architecture;novel encoder architecture;decoder;encoder;encoder architecture;efficient unified nar framework;transcription model;new linguistic states;translation quality;empirical length prediction;length predictor;rnms;unified nar e2e;speech;text translation;novel auxiliary mm;nonlinear sequence model;nar;form speech;enhanced training methods;neural network;efficient end;models;model library"}, "fd9e38e240b4372c49b9205d6f909d070ff3804c": {"ta_keywords": "text classification;unstructured text;information retrieval community;version;new method;method;use", "pdf_keywords": ""}, "cd96cae0f8eabc7bb327c6f30151741bfdd62ee0": {"ta_keywords": "global pandemic dynamics;unstable modes;current state;current status;new generation;brief summary;field;art", "pdf_keywords": ""}, "dec6bb3c7bb671c86296a2a089e0e38aa3f69279": {"ta_keywords": "knowledge distillation;best translation quality;distilled data;models;complexity;theart model;performance;capacity;data sets;output data;strong correlation;variations;theart;paper;findings;effect", "pdf_keywords": ""}, "6bfeb25ea4bb41ab0840bb1be09f9b2de7eea8e4": {"ta_keywords": "guinea pig virus;gpcmv protein;receptor;viral growth;cellular signaling;cellular signaling pathways;gpcmv;pathogenesis;protein;pathways;roles;type", "pdf_keywords": ""}, "609010cb866a19dd996281d00818c3fc7363ec94": {"ta_keywords": "different languages;entity;feature augmentation;ner;adversarial learning;recognition;word;models;model;task;parameter sharing;good margin;level;effectiveness;art;experiments;state;approach", "pdf_keywords": "unsupervised crosslingual nerns knowledge sharing;crosslingual nernst;entity recognition;second language task;target language sentences;target language;entity recognition problem;second language;ner knowledge;bilingual dictionary;word embeddings;linguistic adaptation process;languages;target ner model;different languages;language;source language;adversarial training;deep learning;linguistic structure;linguistic structures;ner;common character embeddings;feature embeddings;translation mechanism;unsupervised way;entities;first language;adversarial learning;nernst"}, "a1c4ce9de92338646c6ee93c7c2e5ee366784b1a": {"ta_keywords": "semantic parsing datasets;semantic parsing;different meaning representations;meaning representations;missing logical forms;new unified benchmark;missing execution engines;unified benchmark;datasets;representations;performance;different performance;benchmark reveal;first comprehensive evaluation;approaches;experimental results", "pdf_keywords": ""}, "facefd2fc4b718c6a0d8096b4eb02866028a04c2": {"ta_keywords": "answer span;weak supervision;approximate matching;passage;answer;system;approach;method;idea;new method", "pdf_keywords": "retrieval conversations;retrieval conversation datasets;supervised answer;span answers;retrieval;retrieval questions;weak supervision approach;retrieval question;weak supervision approachwe;question encoder;weak answers;freeform answers;weak supervision;conversational environments;retriever passage encoder;intrinsic knowledge;weak answer;conversations;natural language;learning approach;training approach;supervised learning model;answers;good weak answer;gold passages;much attention;span;questions;retriever;large collection"}, "75d33c125eba966b50d4dccd359a2f6aa4e0e2e7": {"ta_keywords": "policy risk functionals;robust estimators;risk;plugin estimates;risks;importance;cvar;important features;variance;conditional value;framework;collection", "pdf_keywords": "contextual bandits;condition reward distribution estimator;bandit problem;reward distribution;bandits;condition reward distribution;armed bandits;risk estimators;risk estimates;corresponding risk functionals;risk functionals;underlying risk functional;empirical behavior policy estimator;risk functional;robust risk functionals;risk estimation;different risk functionals;distributional reinforcement learning;sensitive reinforcement learning;risk functionalswe;standard risk functionals;stochastic stochastic risk estimation;empirical estimates;risk measure;empirical behavior policywe;random selection bias;empirical behavior policy;random random selection bias;empirical estimate;risk"}, "cb0de2de79533d4faada3d745f43702eb89d1a60": {"ta_keywords": "reusable documentation templates;datasets;classes;case studies;efforts", "pdf_keywords": "documentation;standard documentation practices;natural language;documentation schemata;detailed documentation;natural language processing;different documentation;data card templates;machine learning;datasets;data sheet sheets;available dataset;templates;dataset;model reporting;synthetic datasets;main datasets;nlp;datasetsthe subreddit subreddit subreddit;data sets;data;data cards;usefulness;toolkit;fields;structured task;reproducibility program;useful framework;document theirwe;models"}, "13b6c8cce3b4557ad7a3188f2d54636e755e8145": {"ta_keywords": "multichannel source separation;multichannel mixture model;domain multichannel audio;deep computational network;generative model;novel computational network architecture;network;model;frequency;states;dynamics;applicability;effects;original", "pdf_keywords": ""}, "77c63e8f102465e3fc4a46e0b07c32fa8d2f8a54": {"ta_keywords": "grammar induction algorithms;parsers;gnus library;check errors;check error;dependencies;evaluation;accuracy;method;novel method;paper;cross", "pdf_keywords": ""}, "c065f9997794b13565dd49a6e475fc5e8c9d54ce": {"ta_keywords": "lamina wave;tensile stiffness;wave;kinetostatic model;joints;equivalent spring constant;finite element analysis;joint;material structure;model;paper;results;approach;novel approach", "pdf_keywords": ""}, "112eb8a8273ab725d47789efb87237edbc4f02db": {"ta_keywords": "learnability;tractable learning;simple description logic;order logics;primitive classes;monomials;expressive power;alphabets;concepts;roles;kadanoff;results;result;positive result", "pdf_keywords": ""}, "e6accbbb366387faf817126dc7b0260c450bd2e6": {"ta_keywords": "mathini;inline;formula;notation;tex", "pdf_keywords": ""}, "ca7a67aa29c67b006017f651601091145644f243": {"ta_keywords": "speaker localization;speech detection;statistical speech detection;localization errors;classifier;accuracy;domestic environments;development;method;conventional methods;baseline;integration;test;paper", "pdf_keywords": ""}, "a61aebbfe029c4b8eafae4042e6242cdca8f54b7": {"ta_keywords": "relation structures;target answer entity;artificial questions;relational database;extractive training;domain questions;database;eq;novel framework;accuracy;framework;pwd;measurements", "pdf_keywords": "relational wikidata;answer entity;latent relations;relation reasoning;natural language representation;generative knowledge;target answer entity;relations;questions datasets;knowledge;natural language processing;relation predictor;natural language;latent relation;extractive qa;questions;relation;mining task;retrieval;dymp depth;domain questions;service;quantitative learning model;wikipedia;wiki;human labeling;models;narrative exploration;answers;wikipedia hyperlinks"}, "2d1f442578feb7034aa2b68bbf95f608f2342256": {"ta_keywords": "group fairness;bandit;fairness;groups;resources;algorithm;novel algorithm;world datasets;intervention settings;cmab;regret;synthetic data;bounds;context;arbitrary number;novel formulation;notions", "pdf_keywords": "group fairness;fairness framework;individual fairness;biased feedback;agent decision;fair algorithm;fairness constraint;observed bias;optimal bandits;bias;relevant bandit setting;fairness definitions;biases;empirical bias;inherent fairness;bandit;measurement bias;bandits;fairness;fair operators;group fairval;agents;individual decision process;optimal selection;societal bias term;human decision making;sequential decision maker;societal bias;decision processes;exploitation"}, "147ba336fcba32fadca470e14a858ce069375475": {"ta_keywords": "contour generation;synthesis;model;generation;method;use", "pdf_keywords": ""}, "00c8d88abef116d8d3d673a28ff4098115cf8da3": {"ta_keywords": "cooperative persuasive dialogue system;natural language;natural language generation;learned system;reward;wizard;modules;policy;system;users;paper;experimental results", "pdf_keywords": ""}, "04a7d9f0388ded93c1ec16e36a6df3cd44cb95b0": {"ta_keywords": "crosslingual linking problem;multilingual entity;dual encoder;specific mentions;agnostic knowledge base;language;context;new model;model;performance;frequency;new formulation;quality;scale;analysis", "pdf_keywords": "multilingual entity;single entity retrieval model;crosslingual entity;entity retrieval task;multilingual dual encoder architecture;entity references;entity mention;dual encoder retrieval models;mining entities;crosslingual content;knowledge base;universal machine learning tasks;shot entity retrieval;entities;binary entity candidates;entity;single entity;new binary reference dataset;auxiliary entity;low resource languages;word embeddings;neural machine translation system;multilingual environments;featurized entity encoder;associated references;necessary machine translation parameters;natural language;crosslingual version;wikipedia links;languages"}, "9768d7ba9d09ac3bf3d52ec674bde1a6e615daad": {"ta_keywords": "pairwise comparisons;adaptive probabilities;pairwise comparison data;adaptive estimators;adaptivity index;future comparisons;minimax risk;estimation;complexity;upper bounds;probabilities;count;theoretic result;order;step;result;problem", "pdf_keywords": "pairwise comparison probabilities;strong stochastic transitivity;optimal adaptivity index;pairwise comparison data;stochastic optimal ranking problem;optimal ranking;largest indifference;adaptivity index;optimal adaptivity;indifference sets;efficient estimators;stochastic version;matrix estimator;isotonic matrices;future comparisons;efficient estimatorwe;bivariate isotonic matrices;ranking;indifference;optimal estimate;single observation matrix;observed matrix;oracle estimator;multivariate estimators;stochastic;optimality;minimal matrices;underlying matrix;optimal choice;multivariate estimator"}, "b7ffc8f44f7dafd7f51e4e7500842ec406b8e239": {"ta_keywords": "comprehension tasks;sentences;gating mechanism;interaction;words;characters;novel approach;approach;properties;problem;art results;state;number", "pdf_keywords": "natural language processing tasks;attention reader;natural language processing;comprehension tasks;various attention architectures;token representation;social media tag prediction task;ga word char feat;gating;meaningful representations;semantics;tokens;normal language;twitter tag prediction dataset;scalar gating;key task;token;comprehension;scalar weighting;sequential extraction;level tasks;words;representations;paragraph;gating model;gating mechanism;gating method;word properties;representation;document"}, "90357a6dc817e2f7cec477a51156675fbf545cf1": {"ta_keywords": "multimodal script knowledge;temporal commonsense;youtube videos;speech;box representations;video;free manner;corresponding words;images;mix;time;label;level;model;objectives;millions;state", "pdf_keywords": "multimodal world representations;multimodal representations;multimodal script knowledge representations;powerful multimodal world representation;multimodal event representation;strong multimodal;multimodal script knowledge;deep context;complex web videos;video learning;training corpus;temporal commonsense;underlying visual language;long videos;video actors;web videos;major challenge intranscriptity intranscript news articles;video corpora;temporal representations;video sequences;natural language representation;learning tasks;video content;captions;useful image representations;transcript tasks;visual representation;youtube videos;joint attention patterns;language pretraining"}, "ba3f39606cfd4150ea80fec1b2e1137933c6d143": {"ta_keywords": "stationary state;dependent probability density function;charges;system;simple method;time;method;application;collection", "pdf_keywords": ""}, "8ae4a584539a8f30d654e2678dde64a8334461b7": {"ta_keywords": "denotated recommendation systems;personalized recommendation system;recommendation system;generative model;underlying message;product;content;behavior;model;user;approach;new method;family", "pdf_keywords": "personalized reviews;random matrix representations;user ratings;recommendable reviews;generative concatenative network;random matrix representation;likely reviews;novel generative model;reviews;unsupervised generative language model;generative models;classify items;rating;review;rnns;coherent product reviews;generative model;random matrix;sentiment;recurrent neural network;simple classification tasks;recurrent neural networks;unseen reviews;star rating;short term memory cell;level rnn architecture;standard generative model;novel unsupervised learning model;deep recurrent neural networks;neural networks"}, "b01ecfd2322437fcc9c7ce6605d6f5a50f67ec50": {"ta_keywords": "active learning;training data;dataset;training;models;data;successor model;popular strategy;approaches;inconsistent gains;question;scale practice;benefits;al;lack;particular domains", "pdf_keywords": "active learning;deep bayesian active learning;nerns benchmark text classification task;text classification tasks;supervised learning tasks;text classification;active data acquisition;term memory;deep learning models;active acquisition strategy;training data;deep learning;cnn acquisition model;machine learning;supervised learning;cnn;class prediction;unstructured text representations;natural language processing;cnns;neural networks;nlp;text documents;random data acquisition model;models;sentences;corpus;retrospective evaluations;bi cnn strings;random sampling strategy"}, "2177bf060aaf2c0c2b551d3e805779cb35c19bb1": {"ta_keywords": "optical fiber;temperature;thermal bath;density;new method;quality;purity;generation;power;combination;method", "pdf_keywords": ""}, "2068825cabd94c951a0282ed731a8b8f2da1721c": {"ta_keywords": "semantic parsing;python code generation;strong supervised models;available unlabeled data;extra unlabeled data;parallel data;atis domain;model;novel model;limited amounts;experiments", "pdf_keywords": "learned parser;supervised semantic parsing;supervised parser;semantic parsing;abstract syntax trees;sequence parser;parser;semantic structures;abstract syntactic syntax;transition parser;probabilistic grammar;deep generative model;probabilistic language;structured latent variables;structvae learns;principled deep generative model;natural language;deep generative methods;different structured latent representations;abstract abstract syntax;available unlabeled utterances;code generation tasks;generative model;semanticwe;unlabeled utterances;semantic datasets;supervised counterpart;semantics;unsupervised learning tasks;utterances"}, "8ca58f3f6e59a6d243f3da6c196e9f730e6e9993": {"ta_keywords": "neural speaker diarization;speaker;offline eend method;speakers;speech;online end;buffer method;calldar;flexible numbers;variable numbers;comparable performance;end;method;paper;experiments", "pdf_keywords": "online speaker diarization method;novel speaker diarization framework;conventional speaker diarization framework;conventional speaker diarization framework bywe;speaker models;online diarization problem;end speaker;speaker;neural diarization;novel speaker;speakers;diarization;speakerin;offline method;neural online source separation problem;offline system;online system;offline offline method;offline model;online method;speech;buffer;second latency;online mode;stb;flexible number;chunk setting;flexible numbers;network;onlinein"}, "ca6d5c7829a76d10069fa3aa6776c35cc044b7ba": {"ta_keywords": "undergraduate courses;undergraduates;teaching;learning;lessons;large state university;american association;tools;data;collection", "pdf_keywords": ""}, "6e3f8187f8fef3e11578a73f32da07d33dbf8235": {"ta_keywords": "structured data release;domain tables;domain data;semantic editing;semantic triples;data;open domain;dta;novel framework;framework;construction;novel method", "pdf_keywords": "semantic parsing datasets;rich semantic relationships;semantic parsing;novel tree ontology annotation;domain corpus;semantic datasets;structured text;structured data;table annotations;sentence annotation;semantic frame;structured data record;semantic dependencies;tree ontologies;text models;knowledge base data;text generation;natural language;text corpora;table annotations thatthe baron;ontologies;annotations;text tasks;flat table schema;data extraction;natural language questions;sentences;meaning representation tasks;declarative sentences;ontology"}, "bdbf635476477eec5be5a292b494e20b8902cc35": {"ta_keywords": "translation systems;clean data;underlying text;translation;accuracy;machine;noise;typos;mt;systems;small number;use;method;form;variety;ability", "pdf_keywords": "noisy text;social media text;improved robustness;synthetic noise induction;machine translation;natural noise;computational linguistics;noisy input sequence;artificial noise;robustness;social network;noise;messageswe;social networks;underlying language;social media;neural machine;mtnt dataset;messages;milky way;text;clean data;profanity;electroweak equivalence;translation;adaptation;data;electroweak interactions;network;improvement"}, "b3979990dc2080138021cb3d767c7ec6d3e96194": {"ta_keywords": "global relationship descriptors;digital humanities;topic model baselines;annotations;novels;relationships;interesting correlations;descriptors;social sciences;novel;relationship;neural network;dataset;raw text;model;tasks;trajectory;set", "pdf_keywords": ""}, "e12c52fb542f76b3f0d29178842428d6a4edfe1e": {"ta_keywords": "arbitrary decision maker;decision maker;biased models;fairness;accuracy;process;system;framework;problem", "pdf_keywords": "adaptive rejection learning;rejection learning;predictive decision maker;adaptive decision;external decision;machine learning;decision makers;probabilistic learning model;machine learning model;decision uncertainty;potential biases;defer;many machine learning applications;decision making;decisions;fast learning;deferral rate;artificial intelligence;generalization;fair binary classification problem;deep learning algorithms;novel regularization;binary classification;neural networks;deferwe;artificial neural networks;fairness principle;judgment;models;regularization"}, "4ce47dd7a8674f8ffd53f1883bc57e62460a83f0": {"ta_keywords": "aware service contracts;privacy;security;power company;vulnerability tradeoff;purchase insurance;energy sector;interoperability;inherent efficiency;service;incentive;consumers;modern internet;inefficiencies;wallet;statistics tools;things;contribution;solution;needs", "pdf_keywords": ""}, "fa9b043ae8da3cc60c975762ae9066d2fb010f41": {"ta_keywords": "natural language processing;nlp;graph;algorithms;representation;method;tutorial;tasks;evaluation;use;variety", "pdf_keywords": ""}, "52540497682c4209b8e20125c8255358b22d0fa7": {"ta_keywords": "cognitive learning;future learning tasks;extended student;problem student;students;student;tasks;small prior knowledge;problem problem;behavior;novel representation;novel approach;results", "pdf_keywords": ""}, "2ed6f376e9e7eee6d833ad7b6aba63d7ad40c0f8": {"ta_keywords": "thermal fiber;thermalized plasma;thermal instability;fiber;thermal layer;ground state;new type", "pdf_keywords": ""}, "bd0db679d595399b91c5acca1db33a2803697d53": {"ta_keywords": "social cues;online political information search;candidate evaluation;negative cues;candidate;judgments;voters;informed counterparts;evaluation;effect;heuristic;such signals;subjects;absence", "pdf_keywords": ""}, "79f47ebf896b848e7c981c8aa6862ca1a7e5e7e5": {"ta_keywords": "kernel clustering;adaptive kernels;heuristic kernel;kernels;small clusters;density mode isolation;continuous generalization;fundamental bias;gini criterion;bias;general class;common class;special case;conditions", "pdf_keywords": ""}, "0222a48657d554b2a5a3d7ec3bb0b6833b8970a1": {"ta_keywords": "thrombocytopenia;rapid lateral flow assay;parasitized parasitized blood;parasitized blood;lateral flow;diagnosis;patients;large prospective cohort;pretest probability;4t9s;lfia;gsp;pilot study;system;analysis;use;results;number", "pdf_keywords": ""}, "8c5ba1c914eab16b705da03352fe69d5bcfc72ea": {"ta_keywords": "abstractive summaries;summarization;decoder models;sentences;explicit dependencies;level structure induction;source documents;content;source document;cnn;document;dm dataset;grams;latent;end;framework;coverage", "pdf_keywords": ""}, "7fc0097f6a51282dc1e9020d7c28e12cecaef519": {"ta_keywords": "relativistic heavy ion;orbit coupling;ray lattice;spin;rhic;dynamics;effect;presence;enhancement;case", "pdf_keywords": ""}, "3cfb319689f06bf04c2e28399361f414ca32c4b3": {"ta_keywords": "natural language processing;learning;transfer;models;model;new framework;data;novel approach;framework;mining;novel;large scale;problem;np;combination", "pdf_keywords": "natural language tasks;transfer learning;natural language models;text transfer tasks;text models;natural language processing;machine translation;text classification task;machine translation pipeline;machine translation model;text tasks;digital natural language processing;translationtransfer learning;machine translation systems;natural language representations;natural language;natural language representationswe;machine translationwe;objective outperforms language modeling;text transfer;classification tasks;natural english text;neural machine translation system;underlying text;nlp;translation task;decoder tasks;specific classification tasks;machine learning context;new machine learning framework"}, "d19e097f388ca12ff111989b2bac7d3cc3cf15ca": {"ta_keywords": "user coordination network graph;coordinated messaging;similar textual content;coordinated user clusters;social media;capitol riots;networks;user parleys;text;elections;different narratives;analysis;user;support;method;general methodology;january", "pdf_keywords": "social media activity;rumor spreadthe dynamics;social network parler;social media posts;social media content;capitol riots;tweets;political unrest;recent parler event;social media;social information;social media users;social network;social networks;social media websites;january 6th capitol riot;free speech;disinformation campaign;social media site;coordinated messaging;message content;dissemination;user parleys;spreading;parler;content;disinformation;community structures;noisy graphs;recent demonstrations"}, "2c9158e20f58df04a6c5cd54dd3ee7d8df656421": {"ta_keywords": "optimization;optimization problems;search problems;extensible library;framework;new framework;methodology;variety;use", "pdf_keywords": "similarity search methods;similar search;accurate text retrieval;sparse vectors;search library;retrieval pipeline;unstructured query;search;unstructured text;document vectors;indexing;pattern recognition system;dimensional nonmetric data;order search;distance;reranking system;n00010;neuneuart;training data;neuneur refinement;neural machine translation system;rerankers;order search results;vectors;classification;data sets;generic small world graphs;novel scoring system;data structure;scoring documents"}, "d864944df8e765d597484ace12dbc3ac99e950a9": {"ta_keywords": "proximal policy optimization;surrogate reward function;dimensional robust estimators;gradients;tailedness;policy;agent;detailed empirical study;pop;nature;issues", "pdf_keywords": "theproximal policy gradient;proximal policy gradients;policy gradient algorithms;standard policy gradient methods;policy gradients;proximal policy optimization;stochastic policy gradient optimization;policy gradient;stochastic policy gradient method;gradient aggregation;policy gradient method;robust mean aggregation technique;highdimensional robust aggregation method;actor gradients;generalized advantage estimation;policy distribution;optimal policies;critic networks;seeded policies;robust estimator;gradient distribution;actor gradients canwe;surrogate reward function;conservation policy optimization;empirical multiplicative advantage estimates;noisy policy;critic loss;stochastic gradient noise;deep reinforcement learning;actor gradient norm"}, "e6239cc789da289929d49ffed2c0a562213d4703": {"ta_keywords": "titanium alloy ring;fillet welding;welding position;welding;cylindrical shell;residual stress;deformation;structural stability;finite element software;constraint tooling;gravity;results;little effect;influence", "pdf_keywords": ""}, "60e339d25d43c026cf96395aa8accf34eae744a5": {"ta_keywords": "scale crowdsourcing dataset;pairwise comparisons;smoothed imagecaptions;pictures;gaussian filtering;scale dataset;tempboxa;well;goal", "pdf_keywords": "scalable crowdsourcing algorithm;pairwise comparisons;toloka crowdsourcing platform;pairwise data;reasonable ranking order;optimal index;nodes;available dataset;index;algorithm;graph;same index;individuals;scale dataset;performance;list;several baselines;centralities;methods;method;sanity check;novel method;methodology"}, "4ef77ef9b8ca8c8a96f1e62fa86a988feb582bd1": {"ta_keywords": "domain adaptation;machine learning;underlying distributions;model;concept;larger set;framework;large set;powerful concept;small set;terms;behavior", "pdf_keywords": "language model adaptation;domain adaptation;domain training strategies;generalization loss;specialized language model;language modeling;domain pretraining;neural language models;deep neural language models;limited training data;language adaptation;machine learning;better generalization;training data;model training weights;conditional lszlthis paper;training set;training data andwe;domain likelihood;neural networkswe;neural machine translation model;machine learning theory;lm adaptation;learning process;neural networks;models;domain set;regular lszl;neural machine translation machine;regular linear algebra"}, "09ec8d8e2251e079abb0e109979f33ee120211fa": {"ta_keywords": "proximal extragradient method;tensor method;numerical order;numerical accuracy;method;order;hybrid;author;plan", "pdf_keywords": ""}, "a8a863e85a95919773868204d672f1260e0058ce": {"ta_keywords": "neural machine translation;domain adaptation;monolingual data;shot translation;multiple languages;single universal model;nmt;language;models;iwsl;training;specific parameterization;simple modification;system;art performance", "pdf_keywords": "neural machine translation;scalable machine translation system;various machine translation tasks;machine translation;nt machine translation;language encoder;multilingual nmt systems;multilingual translation;decoder networks;learnable parameters;language embeddings;universal language embeddingwe;contextual parameter generator;parameter generator network;autoencoders;target language;parameter generator;encoder;source language;different languages;decoder;different parameter generators;language pair;decoder network;linear parameter generator network;language pairs;open source language language language language library;languages;language;translation"}, "d82592f3a110308366dfc7c42565d437b5bf59af": {"ta_keywords": "social skills training;human social skills;task testing;training;tasks;participants;new tasks;method;process;experiments;creation;paper;set", "pdf_keywords": ""}, "5dab371fecc43904c0b785a50136d20cee43a99a": {"ta_keywords": "machine translation system;source text alignment;parallel texts;text alignment source;source speech;text alignment;speech;translation;such models;model;new approach;steps;power;problem", "pdf_keywords": "direct speech translation models;speech prediction;standard speech recognition task;speech recognition;speech translation;successful speech recognition;auxiliary speech translation;speech translate speech;speech applications;advanced speech recognition;sequence models;auxiliary training data;audio encoder;stage attention model;spoken language;direct model;crosslingual transfer tasks;models;speech;decoder;cascade;corpus;additional auxiliary data;encoder;neural networks;model;novel attentionwe;crosslingual transfer;neural network;stage models"}, "461188735d46dc1062f5d1d382d940a24c355fad": {"ta_keywords": "similarity extraction;similarity;large corpus;sentences;physical objects;statistical method;context;novel method", "pdf_keywords": ""}, "f6be5d90199d1644b85e6b41a7a7f42fb29dbc9a": {"ta_keywords": "multiple objects;dependent assessment tool;young elementary students;children;ospt ability;evaluation;ability;space;training;ospt;tool;purpose;problems;novel", "pdf_keywords": ""}, "a278c07c8bd2921e59dd862cd91a0540dd340030": {"ta_keywords": "treatment effects;neural networks;external field;novel methods;analytical models;empirical data;effect;methods;field;accurate estimation;presence;combination;paper", "pdf_keywords": "treatment effect estimation;novel outcome prediction method;parametric treatment prediction objective;neural networks;prediction quality;neural network;targeted regularization method;regularization;neural network architecture;propensity score;regularization procedure;dragonnet;effect estimation performance;estimation quality;other training methods;regularization method;prediction;propensity score sufficiency;bias;treatment effect model;target estimator;estimators;downstream estimator;generative adversarial nets;estimation performance;estimation adjustment;treatment effect;training objective;nednet;minimum loss estimation"}, "9c5c794094fbf5da8c48df5c3242615dc0b1d245": {"ta_keywords": "unsupervised learning;disentanglement;inductive biases;representations;robustness;models;evaluation metrics;prominent methods;data;different data sets;experimental study;presence;different methods;scale", "pdf_keywords": "recent unsupervised disentanglement learning methods;disentanglement learning;disentanglement representations;unentangled representation learning;unsupervised learning;disentangledtheorem;unentangled learning;disentanglement measures;good disentanglement score;disentanglement methods;disentanglement metrics;disentanglement_lib2;arbitrary deep representations;learning representation;unentangled representations;high quality disentanglement;inductive biases;unsupervised way;deep generative representations;entanglement;generative entangling;separate learning algorithm;neural representation;efficient learning;representations;different supervised learning models;supervised learning;learning;challenging learning;generative models"}, "2a2d03a1534b365c5b048c824c0886e16ccf7dfa": {"ta_keywords": "semantic inference rules;large knowledge base;binary relations;parsed text;random walk model;relations;graph representation;freebase;entire text;model;high accuracy", "pdf_keywords": ""}, "535c58ec8020782d41ed3ca72cf94aff7fd65120": {"ta_keywords": "neural network;recognition;speech;individual nodes;nodes;single mode;model;network;concept;task;information;way", "pdf_keywords": ""}, "4c93bcc05d6b41cb3df451d703f34bab9c9e9201": {"ta_keywords": "private text mechanisms;privacy;privacy amplification step;utility;noise;framework;word;proposals;local region;paper;tradeoff;challenges", "pdf_keywords": "privacy;privacy framework;private data analysis;privacy approach;privacy problem;user trust;privacy ampli cation step;noisy data;data sensitivity;machine learning;di erent techniques;robust algorithm;mining tasks;plausible deniability statistics;utility metric;utility;user de;analysis process;denoise;analysis;users;noise;powerful measure;deniability statistics;representation;suitable processing;algorithm;curator;original algorithm;data mining problem"}, "96abcdded2985bd44b9514e28f5b8da4fa1e4371": {"ta_keywords": "slippery particle;slippery;interpretability;analysis;data;objects;behavior;concept;relevance;widespread tool;word;collection;use", "pdf_keywords": ""}, "18e8646001fc53465fdc8f8eb01523e24c134493": {"ta_keywords": "ordinal rankings;cardinal scores;ranking;ratings;rating;arbitrary miscalibrations;possible estimators;estimators;superiority;variety;assumptions;plug;applications", "pdf_keywords": ""}, "f78e5aaf34cc1e4874490e9155c640b73c630021": {"ta_keywords": "gradient conjectures;surrogate cost function;heterogeneous conjectures;opponents;games;equilibrium concept;dynamic behavior;conjectural variations equilibrium;conjectures;learning;players;agents;game;theoretic outcomes;equilbion;behavior;general framework;suitability;number", "pdf_keywords": ""}, "f132f6534ec326a1ba61870b701015cd3d1560a2": {"ta_keywords": "domain adaptation;target domain generalization;machine translation;language modeling;domain data;data selection;selection;fine tuning;performance;data", "pdf_keywords": "new machine translation classifier;machine translation;neural machine translation pipeline;language models;domain adaptation;target domain generalization;machine translationwe;neural machine translation;language modeling;translation accuracy;slowwe propose novel domain classifiers;language model;neural language modeling;translation task;multilingual domain;domain classifiers;discriminative domain classifier;intermediate data selection;data selection;selection;pretraining;targetwe evaluate data selection;contrastive data selection;language;different languages;selection step;linguistic properties;effective data selection problem;target domain validation data;smaller selection sizes"}, "835587dbe94b70adeb0b16384e10bb6e0e29de84": {"ta_keywords": "behaviour", "pdf_keywords": ""}, "97bcea32979ed602fd404448a4e4cedad4171d79": {"ta_keywords": "nonverbal communication;nonverbal behaviors;nonverbal communication process;nonverbal behavior;input language representation;prediction;models;sentences;participants;framework", "pdf_keywords": ""}, "89c148d3d4edcb7b13c35da36b97ffb881c38058": {"ta_keywords": "electrolarynx;statistical excitation prediction;more natural electrolaryl;speech;effective control;ngectomees;el;experimental results;effectiveness;method", "pdf_keywords": ""}, "b8dcc2ae3346e41a421232169c2ca07957c654c4": {"ta_keywords": "such coevolving network game;sum game;dynamics;games;agents;conservation laws;polynomial time algorithm;average behavior;game;information;time;regularities;theoretic setting;theoretic flavor;system;number;setting", "pdf_keywords": ""}, "135ace829b6ad2ec9db040d8e5fd137034e83665": {"ta_keywords": "conditional random fields;rich information synthesis;protein;rich information processing;rich information;features;semi;conditions;contexts;variety;novel class;new class;conventional approaches", "pdf_keywords": ""}, "635932ee917d71e01f07211c0359abf3e0e65e47": {"ta_keywords": "automatic speech recognition;output token sequence;insertion;token generation;new insertion;automatic manner;asr;joint training scheme;models;arbitrary generation order;model;public benchmarks;competitiveness;paper;problem;experiments", "pdf_keywords": "speech recognition;automatic speech recognition;connectionist temporal classification;speech recognition tasks;insertionion models;output token sequence generation probability;decoding;sequence generation probability;insertionion;insertion;models;ctc probability;predict problem;asr;ctc;right models;corpora;model;high accuracy;right model;new joint modeling;joint modeling;mask;end;position representation;vertex vertex type;generation method;smart smart devices;smart smart smart devices;vertex vertex"}, "b30195763eb103e2e5564228119f3810ab423b2e": {"ta_keywords": "text classification;sentiment classification;label names;categories;label name;unlabeled data;class;words;benchmark datasets;topic;training;documents;self;method;model;set", "pdf_keywords": "text classification model;document classification;text classification;benchmark text classification datasets;neural language model;neural machinelabel names;new supervised classification method;classificationwe;language model self;classification;classifier;large corpus;label names;labeling;corpus;supervised models;category prediction;categoryindicative words;supervised learning approach;top vocabulary;parameterized vocabulary;level category prediction task;new supervised nonlinear model;vocabulary words;neural models;labels;label name;categories;semantics;vocabulary"}, "002c256d30d6be4b23d365a8de8ae0e67e4c9641": {"ta_keywords": "language model;model;tasks;performance;recent model;parameters;method;essential features;context;problem;original model;large variety;small number", "pdf_keywords": "large language models;improved language models;efficient retrieval;improved language model;retrieval tokens;large corpus;retrieval models;several language models;language models;retrieval token;autoregressive language model;neural language model;input oflarge language models;simple language model;language model;language representation;unstructured text retrieval;lexical models;retrieval database;c4 eval language models;retrieval;retrieval datasets;standard language representation;retrieval feature;scale retrieval datasets;retrieval encoder output;large vocabulary;document generator;lexical stacks;efficient learning models"}, "933396f5b9111f6acdd76710ee6ab4d24e8673dd": {"ta_keywords": "automatic speech recognition;large unpaired speech;encoder network;feature extraction;text data;speech;text;end;advantages;dissimilarity;method;ability", "pdf_keywords": ""}, "4c49e9b57c8fc58b0df29a27ecca8cc2e376f02b": {"ta_keywords": "collaborative filtering;data collection;spider;spiders;autonomous spider;data;algorithm;real users;approaches;cf;plausible hand", "pdf_keywords": ""}, "7d5f83cb234a5640487bd258ace06d9dc967d222": {"ta_keywords": "statistical relational learning;latent context learning;joint learning;latent context invention;world datasets;inference;structure;art baselines;ie;tasks;novel approach;sl;multiple domains;approach;results;state", "pdf_keywords": ""}, "f1c7419b87cbf853e691e500643f71720b68fb86": {"ta_keywords": "learning;datasets;training time;minimal memory;pass;scheme;paper", "pdf_keywords": ""}, "9b3fd2525a2d1abc44145308e013f117d3d7bdee": {"ta_keywords": "speech enhancement technique;electrolarynx;electrolaryl;statistical excitation prediction;el speech;face conversation;el;direct f0 control;intelligibility;significant improvements;face;method;experimental results;naturalness", "pdf_keywords": ""}, "1afa3ab80abda57920b8d456a6513e6f01cc82e7": {"ta_keywords": "conversations datasets;conversations;survival regression;art survival regression methods;regression techniques;event;time;length;novel method;family;method;state;art methods", "pdf_keywords": ""}, "5cfdb162ffa4dce18f7c576d352bd459b6a11292": {"ta_keywords": "future purchases;credit card providers;scale transaction records;rush;category information;category;time;novel framework;large number;various baselines;framework;value;higher expected value;systematic evaluation", "pdf_keywords": ""}, "1ddcc9671dd6486e34cefadfe71bbbc1bc55035a": {"ta_keywords": "word embeddings;source word embeddings;standard word embeddings;rich subword information;word similarity task;embeddings;novel class;initialization", "pdf_keywords": ""}, "33d2ebe41477811296abc4077bf9ce09b927ef98": {"ta_keywords": "voice conversion;source speaker;speaker model;noisy channel model;joint density model;target;framework;novel framework;method;paper;effectiveness;experiments", "pdf_keywords": ""}, "c0e8846eb5ce574a6dca3f3a600e82b184339254": {"ta_keywords": "new interactive flight;language;rewards;natural language;simple reinforcement learning approach;optimal actions;unseen environments;context;model;ones", "pdf_keywords": "new interactive flight booking game;reward inference;natural language representations;natural language;reward functions;posterior reward inference;reward function;natural language navigation instructions;human utterances;user flight;reinforcement learning;listener models;utterances;flights;order reinforcement learning;language;reward;underlying reward function;reward estimates;robot dialogue;jetblue flight;rewards;pragmatic model;flight;natural context;true reward;human behavior;agent;expressive description;utterance"}, "a3cc75975a5998d5a7dd494e70a479ba0a550013": {"ta_keywords": "expert model;student;study;problem;author;process;paper;quality;underlying principles;time;strategy;results", "pdf_keywords": ""}, "04f4e55e14150b7c48b0287ba77c7443df76ed45": {"ta_keywords": "physical commonsense reasoning;physical commonsense;dataset physical interaction;natural language;knowledge;objects;models;behavior;future research;task;benchmark;question;analysis;process;significant opportunities;dimensions", "pdf_keywords": "physical commonsense reasoning;physical commonsense understanding;physical commonsense knowledge;physical commonsense;key commonsense concepts;physical knowledge;dataset physical interaction;instructables;natural language understanding models;physical world;everyday objects;physical interaction;basic physical properties;natural language understandingwe;natural language;descriptions;natural language models;objects;new physical benchmark;annotator testing tasks;language models usefulwe;tools;simple linguistic tricks;generative knowledge;questions;interactions;knowledge;encyclopedia entries;benchmark;language models"}, "5e0f2f82b4a28d59f1aa8b8ffe497790de1cdf9d": {"ta_keywords": "deep reinforcement learning;reinforcement learning;catastrophic states;catastrophic mistakes;intrinsic fear;catastrophes;toy problems;policies;dangerous states;guards;unacceptable performance;novel approach;approach;new method", "pdf_keywords": ""}, "7a9f4a8a99f9a38e9213da890f9eab6150ae928e": {"ta_keywords": "similarity measures;similarity;random walks;large graph;edge sequence;graph;degree;novel approach", "pdf_keywords": ""}, "ebaae38a09c5a4909049e16af759c71db9cc87dc": {"ta_keywords": "neural model;noisy examples;simple desireable model;noise;model;words;input;level;outputs;performance;variety;result", "pdf_keywords": ""}, "b17fa6625681d99370122145ba9911f701dd92cb": {"ta_keywords": "semantic parser;typical context modeling methods;context modeling methods;domain datasets;grammar;datasets;best model;significant improvements;state;top;art performances", "pdf_keywords": "effective context modeling;semantic parser;semantic parsing;typical context modeling methods;context modeling;different context modeling methods;context modeling methods;natural language processing;parser;frequent contextual phenomena;contextual clues;context;nonterminal parser;textual data;natural language front;complex contextual phenomena;unstructured database;schemas;schema;knowledge base;grammar;decoder;specific grammar rules;domain model;nonterminal decoder;entity;datasets;grammar rule;novel encoding;better inference"}, "bf4da952df7a6ef9c0b2be8b4b4b69ad63848b8f": {"ta_keywords": "transfer learning framework;spatiotemporal data;various effective spatiotemporal features;traffic speed;source areas;target areas;little historical data;empirical structure;approach", "pdf_keywords": ""}, "879cd78b0d4413aef614bc6b6cce075e8e6ad4be": {"ta_keywords": "streaming codes;size arrivals;code constructions;code construction;code;arrival;rate;simple construction;generalized model", "pdf_keywords": ""}, "bd318e959236b0d33a7567b6d3afc8d5e92b8ea3": {"ta_keywords": "domain;notion;form", "pdf_keywords": "artificial intelligence;artificial intelligence system;agents;gaussian preferences;agent;general machine learning constraints;decision support systems;expressive logics;decision support;games;tractable preference distance method;game;same agent;representation;fit agent;networked representations;comparative representation;preference structure;ethical properties;machine;nets;model preferences;rules;hybrid;hybrid approach;swarm;flexible version;right decision;important decision;bias rating"}, "4a93f7654f795871ed99dece2e1805e4950fd194": {"ta_keywords": "spatial utterances;semantic relations;tabletop scene;comprehension data;objects;spatial configurations;sentences;landmarks;symbolic representations;referents;referent;human performance;inference problem;sentence;human speakers;system;model;production;level;meaning", "pdf_keywords": ""}, "9e172f35b2b0ebcff090f01d40e61fa5aecefa68": {"ta_keywords": "different adversarial losses;adversarial losses;discriminative adversarial networks;generative adversarial networks;regularization approaches;new simple comparative framework;new comparative framework;different component functions;component functions;sdms;framework;dans;sufficient conditions;extensive set", "pdf_keywords": ""}, "ebe04f06580abab035408c4c2e65245b3950934e": {"ta_keywords": "nonlinear optical parametric oscillator;parametric oscillator;nonlinearity;new type;good agreement", "pdf_keywords": ""}, "dd36aca034312a266d6f10b37414d3342c3b9c79": {"ta_keywords": "quantum field;transverse momentum;particle;field;energy;calculation;method;new method;basis", "pdf_keywords": ""}, "17351cfeac949c266f4d1ff86c515250b931bdc2": {"ta_keywords": "logic networks;logic programming framework;logic programs;new probabilistic logic;efficient structure learning;structures;interpretive learning method;graphs;proppr;tractable fashion;several important domains;methods;foil;approach;experiments", "pdf_keywords": ""}, "394e17f5ee5e8a734b2714795b7da3cd704716da": {"ta_keywords": "online courses;cardinal scores;moocs;massive learning platform;ordinal approach;conventional cardinal approaches;precision;class;content;evaluators;expertise;approach;evidence;lack", "pdf_keywords": ""}, "57e4074c588c0e27e4c0bc89f12512ccdb900d79": {"ta_keywords": "unsupervised text style transfer;unsupervised style transfer tasks;sentiment transfer;deep generative model;unsupervised fashion;parallel corpus;formality transfer;related language translation;nonparallel data;wordment;domains;model models;sequences;domain;effectiveness;method;wide range", "pdf_keywords": "unsupervised machine translation model;deep generative model;unsupervised machine translation;latent parallel corpus;unsupervised text style transfer;other recent unsupervised style transfer;amortized variational inference;novel variational inference procedure;unsupervised style transfer;general unsupervised machine translation usingwe;variational inference;generative model;challenging parallel corpus;parallel corpus;generative framework;machine translation;parallel latent sequence;text style transfer tasks;inference networks;machine translation techniques;unstructured sentiment transfer task;latent variable learning;variational objective;style transfer tasks;sentiment transfer tasks;data augmentation objective;novel linguistic tasks;unsupervised objectives;unsupervised fashion;sentiment transfer"}, "8a99e1eb3285f127eed7169441679d47be7f1633": {"ta_keywords": "text generation;data generation tasks;neighbor text;generation;simple graph;data;powerful method;variety;theoretic framework;method", "pdf_keywords": "text generation;text generation models;informative text generation models;sequence generation;splicing approach;neighbor text;controllable generation;structured outputs;unstructured text;text;natural language;training corpus;novel generation;sequential generation;novel encoding;high quality text;neural machine translation;corpus;natural language processing;level text;collaborators;generation process;source tokens;generation;computational tasks;encoder;encoding;novel representation;free grammar;reference text"}, "3c40fc36217a56aafb0abc735ff7d132b17e83a0": {"ta_keywords": "melody harmonization;melody;chord pairs;different triad chords;template matching;deep learning;algorithm;canonical approaches;hidden model;dataset;evaluation;performance;different metrics;variant;comparative study;result;task", "pdf_keywords": "melody harmonization model;automatic melody harmonization;melody generation model;novel music harmonization model;music harmonization models;melody harmonization;melody generation;deep recurrent neural network;melody harmonicity metrics;melody;deep learning models;deep model;harmonization;chords;chordal accompaniments;neural networks;melody note;classical melodies;chord sequences;music discovery;harmony;music progressions;rnn;corresponding chord sequence;novel models;functional harmony;classical melodieswe;chord sequence;music;chord progression metrics"}, "77efa3102456c9f921b05b95eefe845d2ce6bc4b": {"ta_keywords": "minimum relative entropy discrimination;continuous phoneme recognition tasks;conventional discriminative training methods;discriminative training method;acoustic models;constraints;mred;methods;method;principle;rigorous interpretation", "pdf_keywords": ""}, "803a0d2677a7d6b20c3964533595775fa5c7c750": {"ta_keywords": "ranked preference data;pairwise comparison models;pairwise comparisons;sample test;samples;test;world datasets;results;distributions;modeling assumptions;lower bounds;range;assumptions;sets;parameter;extensive simulations", "pdf_keywords": "stochastic pairwise comparisons;partial ranking;pairwise comparisons;partial ranking data;partial rankings;partial ranking model;minimax optimal tests;partial rankingwe;total ranking;complete ranking;sample complexity;total rankings;comparison data;rankings;comparison models;optimal hypotheses;comparisons;testing algorithms;luce ranking problem;testing algorithm;rank breaking algorithm;new hypothesis testing algorithm;sample testing;comparison;random tests;test statistic;permutation testing method;quantum test statistic;testing procedure;pairwise"}, "c859416a8e5682bee3c35df29bc02e02a22de072": {"ta_keywords": "natural language processing;natural language resources;tao li;darwin brown;literature;article;distribution;field;results;players;major players;joint work", "pdf_keywords": ""}, "90705ece92a71efcf256cd047da53cbc1d4e5295": {"ta_keywords": "gear meshing frequency;gear vibration signal;transmission frequency;transient dynamics analysis;transmission transmission;machines;key component;link;results;paper;novel approach", "pdf_keywords": ""}, "30fa01df767339a6c8bd37c32160992fcb19ed18": {"ta_keywords": "armed bandit problem;scale games;novel algorithm;large set;algorithm;new algorithm;extensive experiments;accuracy;baselines;performance;improvement;equi;number", "pdf_keywords": ""}, "196be0bdec3b7bcb3ee35cd126fb2730a9d742d6": {"ta_keywords": "student projects;student problem;students;student;study;different projects;simple problems;problem;novel approach;approach;variety;set;use", "pdf_keywords": ""}, "46619f0547b1a9c2e7649d0e5c931e9aa857a938": {"ta_keywords": "spin;orbit coupling;light;new method;method;subsequent generation;generation;observation;idea", "pdf_keywords": ""}, "8963602d4b9c3b1054a5ed6fb2a2088dec774824": {"ta_keywords": "personal information management tools;personal information;machine learning;searches;performance;experimental evidence;circumstances", "pdf_keywords": ""}, "daedf33077099f7c808e9f4022469e15bf224ad7": {"ta_keywords": "immune system;healthy organisms;survival;immune cells;organisms;fitness;life;crucial factor;choice;number", "pdf_keywords": ""}, "59d487d6ef839c82ae128550e35fa44058b03d37": {"ta_keywords": "einstein condensate;magnetic field;condensate;bose;gaussian;evolution;model;interaction;combination;study", "pdf_keywords": ""}, "60ce57713261b41fe2e3d222f1d4530c4fc69241": {"ta_keywords": "probabilistic serial;computational complexity;polynomial time;sequential allocation;expected utility;lexicographic best response;discrete objects;time algorithm;agents;agent;better response;np;rule;best response;case;ps;connection;result", "pdf_keywords": "random allocations;stochastic dominance;expected utility;random games;random assignment;allocation;probabilistic serial;utility distribution;optimal assignment problem;sequential allocation;same allocation;random object;utility;polynomial time;behaviour;stingy algorithm;indifferences;computational complexity;other agent;assignment;other agents;computational theory;agents;agent;lexicographic best response;preferred available object;mechanism;canonical response;power law;algorithm"}, "74d8a998269bcdd087a21840b0e28d86c256c121": {"ta_keywords": "preference models;partial learning algorithm;learning bounds;exponential improvement;algorithm;novel class;performance;case study", "pdf_keywords": "more general preference models;preference models;utility model;preference model;optimal choice;intertemporal choice;learnability;learning rule;algebraic preference relations;utility functions;learning bounds;preference relations;active learning model;preference parameters;active learning;hypothesis preference relation;preference relation;active learning framework;indifference data;optimal time;models;payoffs;preference;polynomial time;choice;learning data;popular models;choices;economic parameters;selective sampling"}, "c994372b3c33bbc1ad6b504c5efb5afd515a5009": {"ta_keywords": "target speech extraction;available speech recordings;speaker identity loss;mixture consistency loss;speaker characteristics;weak supervision;recognition;mixture;available dataset;available library;parts;loss;words;novel approach;paper", "pdf_keywords": ""}, "09d88e0bb8863fd402030aeb625c52c0492c4fef": {"ta_keywords": "robust automatic speech recognition systems;encoder;deep recurrent;early fusion scheme;word error rate;challenge;kit;baseline;significant improvement;version;joint effort;paper;approach", "pdf_keywords": ""}, "320278b24a3c53a44f95e8ef5465bebe56f24225": {"ta_keywords": "joint dependency parsing;pause prediction;pauses;pause;syntactic information;text;data;main information;model;effective use;paper;absolute improvement;measure;methods;method", "pdf_keywords": ""}, "568462ab0a0a59a2575b70db2cd9022572526f3f": {"ta_keywords": "learning dynamics;stackelberg game;sum games;novel gradient;stochastic updates;deterministic updates;optimization landscape;dynamics;integrodifferential equations;theorem;only critical points;sum;implicit function;convergence analysis;natural structure", "pdf_keywords": ""}, "92622a58377a4671b2ba59e8e59b19b0ab5119bb": {"ta_keywords": "knowledge graphs;aware partitioning;ontology;graph identification;distributional information;extractions;model performance;millions;novel approach;approach;larger number;magnitude speedups;orders;flexible technique", "pdf_keywords": ""}, "2ab481028dda04197283c03115bb5f46f5998cc3": {"ta_keywords": "adjoint dirac equation;inverse beta functions;inverse beta function;inverse beta function method;adjoint;potentials;solutions;solution;self;problem;values;framework;class;variety", "pdf_keywords": ""}, "582089a00a6c9fb534f16d1dbbafc50cc4e3912a": {"ta_keywords": "specific domain;queryd;query;domain;specific information;framework;problem", "pdf_keywords": ""}, "1adadbfa95e43a70fcd17e6ce947a0652b86bfc3": {"ta_keywords": "", "pdf_keywords": "unstructured text corpora;large corpora;large corpus;internet archive;available internet dataset;common unstructured text;corpus;unstructured data;unstructured text;corpora;unstructured text patterns;large databases;machinegenerated text;unstructured unstructured text;unstructured unstructured data;provenance;unmodified documents;available natural language datasets;metadata;documents;early corpora;benchmark data contamination;computational linguistics;lexicons;similarity;search;standard search tasks;texts;language models;suited forneural networks"}, "90129b0733ac48ead26b7c86e8b4df917568e208": {"ta_keywords": "background field;probability density;particles;distribution;states;calculation;presence;case;method;new method;analysis;set;finite number;number;results", "pdf_keywords": ""}, "02d98ca8f4ecd1a2b885d6867f4c1407ae8d1007": {"ta_keywords": "semantic parsers;wiki database;weak supervision;extra supervision;human database;query;examples;cold start;performance;novel method;baseline;experiments;method;state;art", "pdf_keywords": "neural semantic parser;semantic parsing;semantic parser;semantic parsing model;semantic parsers;neural semantic parsing model;neural parser;freebasesemantic parsing;parsing;natural language processing;corpus training;weak supervision;strong supervised supervised learning problem;annotator;annotations;supervised learning;natural language interface;safe supervised search;decode sentences;parallel learning;active learning;extra supervision;supervised learning approach;wiki database;supervision;learning framework;sentences;learning;structured query;wikidbx database"}, "601398838250a4e69c69cc339d65f5c51e727ad1": {"ta_keywords": "semantic roles;semantic role;coverage ontology;meaningful questions;predicates;roles;passage;generation;model;large class;set;approach", "pdf_keywords": "semantic relations;fluent natural language questions;possible semantic roles;semantic roles;coverage ontology;semantic role;semantic counterpart;lexical content;natural language text;natural language;correct semantic role;whichwe leverage corpus;ontology;semantics;seq2seq questions;predicate mention;linguistic structure;contextindependent prototype;generative annotations;annotators;context;questions;predicate;syntactic structure;corpus;additional questions;predicate type andwe;predicate type;sentences;seq2seq model"}, "c783bc02f5f901e4604eb3b0d504a036369afd91": {"ta_keywords": "perovskite layer;layer;range behavior;shape;effect;comparative study", "pdf_keywords": ""}, "a3452276ada37727d0008dad8ca7c27bbbee6984": {"ta_keywords": "rumor detection;social media;rumor;conversation thread;user responses;target claim;gaussian;approach;representation;waveguide;novel method;steps", "pdf_keywords": "rumor detection;rumor propagation;novel rumor detection model;rumor detection results;rumor detection accuracy;rumor classification;novel rumor detection method;rumor representations;early rumor detection task;rumor detection datasets;early rumor detection;rumor level;rumor front;rumorous event;graph attention networks;hierarchical graph attention encoding;microblogs;social networks;graph attention layer;rumors;networks;rumor;aware attention graph;hierarchical graph attention mechanism;fake claims;inferencebased attention;social media;nodes;twitter datasets;neural network representation"}, "19be8dd52d949fed1a3e5aca7630669da2575d73": {"ta_keywords": "matching;indifference model;stability probability;lottery model;compact indifference model;joint probability model;uncertainty;colors;preferences;models;agents;unknown order;limited number;problem;significant function", "pdf_keywords": ""}, "35ee53492c7f32dbb3b4ed7ba4d1395218b13ee9": {"ta_keywords": "everyday algorithm auditing;everyday algorithm users;such auditing behaviors;problematic machine behaviors;auditing;everyday users;algorithms;users knowledge;tools;detection;future platforms;design;world cases;cases;power;lessons;forms;gaps", "pdf_keywords": "everyday algorithm auditing;effective everyday algorithm auditing;everyday algorithm audits;everyday algorithm audit;everyday algorithm auditing practices;other approaches everyday algorithm auditing;everyday algorithm users;past literature everyday algorithm auditing;harmful algorithmic behaviors;algorithm auditing;auditing behaviors;everyday auditing;problematic algorithmic behaviors;day interactionseveryday algorithm users;everyday audit;everyday algorithm;sucheveryday algorithm auditing;algorithmic behaviors;everyday algorithmic resistance;problematic machine behaviors;algorithmic behavior;yelp everyday algorithm;everyday users;audit;machine behaviors;algorithmic systems;more everyday users;popular algorithmics;algorithmic system;algorithmic decision making"}, "1f38ba33063f118f574cf57ff9f1a0e7de2857ff": {"ta_keywords": "semantic similarity evaluation;semantic similarity;similarity;task;representations;sssep;ssep;language;challenge;novel approach;framework;set", "pdf_keywords": "russian semantic similarity evaluation;semantic similarity measures;semantic similarity;semantic similarity extraction;semantic relation classification;semantic relatedness;semantic relatedness tables;semantic relations;different semantic representations;semantic vectors;semantic languages;synonym database;semantic models;similarity measures;similarity;word pairs;russian language;synonyms;semantic reference database;ongoingsemantic similarity;similar words;semantic information processing;semantic data;gram models;similarities;orthographic similarity;large scale evaluation;news corpus;novel evaluation datasets;russian"}, "857036a25401c19e484cc32d974c90cd9a46cd66": {"ta_keywords": "local nash equilibria;noncooperative continuous games;player strategies;equilibria;players;games;dimensional differentiable manifold;game;dimensional space;computational methods;conditions;sufficient conditions;number;equivalence", "pdf_keywords": ""}, "d473ff103565d8c76e0cbfa33bdd4b0db1cbb23f": {"ta_keywords": "natural evolution strategies;finite strategy;incremental strategy generation framework;equilibrium computation;games;shot games;strategies;equilibria;response strategies;noisy payoff samples;nes;approximation;box simulation access;comprehensive approach;function space;problem", "pdf_keywords": "symmetric games;symmetric symmetric games;deep game model;probabilistic games;symmetric game theory framework;orthogonal games;deep strategies;natural evolution strategies;approximate strategy solutions;multiple symmetric players;agent strategies;deep reinforcement learning;optimal strategies;incremental strategy generation;stochastic evolution;approximate neural network strategies;optimal strategy;succinct game representation;complex strategy space;symmetric symmetric payoffs;stochastic search algorithm;sum games;deep model optimization;unconditional games;discrete strategy;pure equilibrium computation;game uncertainty;neural networks;move symmetric;multiple strategies"}, "ecd2b355f250abfd4eb9d6c7c598c33c7cd6bcb0": {"ta_keywords": "novel probabilistic labeling model;real crowdsourcing datasets;labels;novel conditional entropy principle;noisy labels;multiclass ordinal labels;ground truth;item difficulty;difficulty;workers;principle;variety;method;ability", "pdf_keywords": ""}, "0fc01a915cc7bf7025f80d44f805bd54b6425a33": {"ta_keywords": "general nonintrusive load monitoring algorithm;aggregate power consumption signal;measurement noise;device usage;bounds;probability;theory;general framework;behavior;error;scenarios;case;function;results", "pdf_keywords": "general nonintrusive load monitoring;nonintrusive load monitoring;aggregate power consumption signal;power consumption signal;power consumption;household appliances;additive noise;noisy systems;bayesian algorithms;classical noise model;noisy system;estimation;hypothesis testing framework;noisy environment;device usage;sensor;random systems;noise;aggregate system;algorithms;blind source separation;modeling;sampling rate;successful disaggregation;powerwe;real data;measurement error;theoretic framework;noisywe present;available theoretical tools"}, "16457c13a40aa589fa06d8533a47b3f96aede474": {"ta_keywords": "metastability;agents;action;medium;phenomenon", "pdf_keywords": ""}, "639cc01afcc1c78063f7a6bbdae998cd147911c4": {"ta_keywords": "novel utility learning framework;utility learning framework;robust parametric inference;robust utility;noncooperative players;players;approximated correlations;noise covariance;user interaction;bagging;game;toy example;performance;methods;scheme;framework", "pdf_keywords": "robust parametric utility learning framework;robust utility learning;robust utility learning framework;robust utility learning approach;utility learning;utility learning methods;utility learning framework;utility learning method;utility learning problem;heteroskedastic inference adaptation;utility maximizers;heteroskedastic inference;generalized gradient descent;robust utility;feasible generalized least squares estimation;ensemble methods;simultaneous prediction;novel ensemble method;prediction;optimal strategy;utility functions;utility function;forecasting performance;regression methods;greater forecasting accuracy;convex optimization problem;continuous games;weak learners;stochastic optimization problem;simple convex optimization problem"}, "03d2af05e41aac58ebae4ab37f09155e53d4b35b": {"ta_keywords": "matrix matrix completion;matrix completion;bit observations;bit measurements;estimate;data;accurate estimate;suitable constraint;experiments;constraint;theory;extreme case;distribution;suite;implications", "pdf_keywords": "bit matrix completion;bit matrix completion algorithm;bit matrix completion problem;matrix completion;matrix completion rate;rank matrix estimation;observed matrix;dimensional matrix estimation;compressed sensing;bit matrix;noisy matrix;binary matrix;convex programming;random matrix;convex relaxation algorithm;rank matrices;sparse data;large matrices;sparse vector;rank matrix;convex programs;bit observations;original matrix;dimensional matrices;matrix;convex optimization program;incomplete observations;matrices;unknown matrix;noisy observations"}, "3bfa1fe0a8d031d59dfc0cfa4975c296951ee56c": {"ta_keywords": "complex signals;speech recognition system;recognition system;living systems;recognition;complex;system;temporal domains;unified approach;combination;variety", "pdf_keywords": ""}, "8873d1369590249113e1f0491ce49d1502395b9c": {"ta_keywords": "text compliance;trainable compliance network;text compliance problem;compliance;videos;text instruction;natural language instructions;human activity;dataset;vtc dataset;video;atomic activities;new video;baseline accuracy;vtc;collection;novel end;end", "pdf_keywords": ""}, "7488429131b8970425a66f3410920d98ff6e9c36": {"ta_keywords": "regularization;algorithm;experimental evaluations;evaluation;theoretical guarantees;method;performance;amount", "pdf_keywords": "student bias;student ratings;bias bias;student review;student rating database;bias problem;bias;optimal learning;bias term;stable bias;final bias;course grades;evaluations;bias distribution;semiparametric model;bias distributionwe;optimal assignment;author evaluations;evaluation;peer review;students;isotonic regression;known partial ordering constraint;fine grades;training sample;minimizers;grade;estimators;rank ordering;student"}, "579e01c3c864cc98e57c728f84fcf553c5b1bcba": {"ta_keywords": "fluid dynamics;visibility;american physical society division;talk;enhancement;framework;parametrized version;annual meeting;use;organizers;report;november;aps", "pdf_keywords": ""}, "4f1eef4acaf0164593b9e654dba4b8cd3e72421d": {"ta_keywords": "graphical learning;base learner;datasets;other related instances;instance;sample;features;predictions;times;important dependencies;experiments;average time;scheme;theoretical analysis", "pdf_keywords": ""}, "31884a623af77136413d997049b5787b394db461": {"ta_keywords": "quantum state estimation;quantum information processing;quantum state;wave function;estimation;amplitude;parameters;phase;analysis;evolution;method;new method;key quantity;problem", "pdf_keywords": ""}, "2818bd090206ef33f9d7e1be03bc4f742c6762d1": {"ta_keywords": "agglutinative language;word sequence;suffixes;suffix;language;word;structure;stem;number;study;paper", "pdf_keywords": ""}, "8f99f9409f254134aa32fbf072475100f688d613": {"ta_keywords": "storage networks;theoretic secrecy capacity;storage network;codes;code;type code;random variable;information;random fashion;different nodes;data;different configurations;class", "pdf_keywords": "theoretic secrecy capacity;secure codes;storage codes;storage nodes;storage network;storage systems;storage;secrecy requirements;storage system;eavesdropper;store data;security;storage overwe;codes;nodes;code;optimal message;network models;finite field fq;data;information;node;threat model;communications;access;repair;network;new type;networks;data domain"}, "b130b6387b105ecd9b4718b179b1e128157f9516": {"ta_keywords": "field model;mean;field;estimation", "pdf_keywords": ""}, "191543c7cb084d3af6a48ae771ca3dfd0588ab22": {"ta_keywords": "recommendation text;recommendation tasks;review text;deep learning;deep learning model;transnets;additional latent layer;layer;target user;target item pair;new model;model;previous state;training time;art", "pdf_keywords": "rating prediction;recommendation item;recommender system;deep neural net model;recommendation services;neural nets;neural net implementation;neural network;recommender;ratings;actual user reviews;nonlinear neural network;network measurement;rating;neural networks;new deep neural network model;accurate prediction;structured recommendation process;deep learning;item representations;deep neural networks;target network;transnets;predictions;deep conn;reviews;transformational neural networks;network architecture;factorization model;target review"}, "c72cdb5ce7e0911c7f442ab503652d6fdeef35e0": {"ta_keywords": "efficacy;different models;comparative study", "pdf_keywords": "weakly supervised parsers;supervised parsers;supervised parser;unsupervised parsers;natural language processing;downstream semantic analysis;parsers;minimal syntactic supervision;semantic evaluation;semantic performance;first semantic evaluation;syntactic dependencies;unsupervised syntax;intrinsic syntactic;semantic information;structured perceptron;semantics;novel entity slot;sentences;linguistic domains;entity;predicate alignments;ungrounded graphs;entities;ungrounded graph;grammars;downstream tasks;freebase graph;sentence;unstructured data sets"}, "61d2dda8d96a10a714636475c7589bd149bda053": {"ta_keywords": "source code captioning;image captioning;novel encoder;decoder framework;review network;conventional encoder;decoders;tasks;framework", "pdf_keywords": ""}, "12239e761e8c7cd05e12e18f43dba7b46dfd8ac1": {"ta_keywords": "neural machine translation;translation accuracy;single source language;translation;augmentation;parallel optimization;nmt system;nmt;data;accuracy;novel approach;target;system;approach", "pdf_keywords": "source neural machine translation;neural machine translation model;translation machine;minimum translation translation machine;machine translation system;multilingual translation;incomplete multilingual corpora;cost translation machine;translation approach;pseudotranslations;monolingual data;data augmentation approach;data augmentation;source language;iteratively augmentation;missing language;source nmt;single target language;multilingual situations;translation;original translations;such incomplete parts;target language;multiple sources;multiple languages;augmentation;incompletewe;nmt;subword segmentation;neural network"}, "d2a2be6ce932a0f1939f31cfff4d64ea3d76723d": {"ta_keywords": "humanlike uncertainty;deep neural networks;new benchmark dataset;explicit training;classification performance;dataset;training;model;improvement;current generation", "pdf_keywords": "accurate categorization;human categorization judgments;image classification;empirical classification data;art cnn classifiers;empirical classification;natural image classification;human labels;deep images;deep learning;generalized cnns;deep learning pipeline;other classification models;available empirical classification data;training datasets;soft labels;classification models;generalization accuracy;accurate machine learning architectures;label distributions;like label distributions;novel dataset;sample classification benefits;training data;labels;fsnds classification benchmark;kernel learning models;machine learning;human judgments;adversarial attacks"}, "1668b0b9cc631cdfc0dfaf77b71627f5524a866c": {"ta_keywords": "quantum field theory;classical field theory;effective mass;field theory;spectral function;particle;calculation;method;new method;one", "pdf_keywords": "smooth stochastic convex optimization problems;stochastic convex optimization;stochastic convex optimization problems;stochastic convex optimization problem;stochastic optimization;stochastic stochastic optimization;stochastic optimization problems;free smooth stochastic optimization;smooth stochastic convex;smooth stochastic convex functions;linear stochastic optimization problems;stochastic gradient methods;noisy stochastic approximation;stochastic gradient method;stochastic optimization problem;stochastic approximations;stochastic approximation;stochastic intermediate gradient method;stochastic calculus;stochastic composite optimization;noisy stochastic realizations;stochastic constraints;noisy gradient;randomized directional derivative evaluation;stochastic learning problem;free optimization methods;free optimization;stochastic;stochasticity;additive noise"}, "3e398bad2d8636491a1034cc938a5e024c7aa881": {"ta_keywords": "vision transformer;pyramids;pyramid;image classification;dense prediction model;convolution;big bang;vision problem;pvt;vit;novel", "pdf_keywords": "cnn pyramids;conventional cnn backbone;cnn backbones;cnn backbone;cnn;free object detection pipeline;pyramid vision;dense object detection;dense prediction tasks;pyramids vision transformers;many dense prediction tasks;dilated convolutional neural networks;reduction attention layer;object detection efficiency;new attention layer;deep network;convolutional neural network;dense prediction framework;deep networks;convolutional neural networks;pyramid;atrous separable convolution;deep convolutional neural networks;pyramids;object detection;dense predictions;convolutional;pyramid transformer;novel backbone network;atrous convolution"}, "115f1366318e7622f89f3a870e5863282670b1ad": {"ta_keywords": "acute kidney injury;statins;high cholesterol levels;independent protective factor;iqi;treatment;aki;patients;drug;difficulty;type;use;study", "pdf_keywords": ""}, "d516daff247f7157fccde6649ace91d969cd1973": {"ta_keywords": "machine learning algorithm;prediction;shape;model;machine;output;novel approach;problem", "pdf_keywords": "interpretable models;interpretability;interpretability research;assertions thatmachine learning models;interpretations;new interpretation approach;interpretation;simple models;decision maker;models;supervised learning algorithms;human decision;linear models;model properties;notions;machine learning;intuition;credit ratings;decisions;learning;inherent transparency;economics;information;different notions;several insights;decision tables;planning;intuitive interaction;absent further explanation;accuracy"}, "5ba57ff3c3e6e319586b86a990b6e082f4ecf972": {"ta_keywords": "speech recognition;probabilistic models;probabilistic model;information principle;maximum variance model;maximum likelihood approach;variational version;variational versions;models;accuracy;theoretical framework;new class;fwm;pm;paper;factor", "pdf_keywords": ""}, "e5d143ae82ede67726aa1a9aeac3de4bf53d8920": {"ta_keywords": "knowledge graph embeddings;language pretraining;image object tags;knowledge;learning;representations;several validation benchmarks;large data sets;text;vision;model;fine tune;substantial improvement;approach", "pdf_keywords": ""}, "7f52e3914a61994f68583635e43bc1bb9203e3b3": {"ta_keywords": "genetic modifications;human risk;lung cancer;risk;working environment;present study;south india;study;promising strategy;results;us;high rate;distinct countries", "pdf_keywords": ""}, "60a4ad8e8f4389f317d109550f5da2a571cbb515": {"ta_keywords": "natural language query;large corpus;text;scale datasets;answer;systems", "pdf_keywords": "text comprehension;answer extraction;machine reading comprehension;large corpus;quasar datasets;unstructured text;large corpora;quasar;comprehension;lexical completeness;text;new datasets;related questions;questions;answers;passages;dataset;opendomain research network;literature;candidate answers;search;search system;documents;tasks;context;research platform;knowledge;data;bioinformatics;machines"}, "15513c732d6af975f312307be3b5e2bd674ac0ef": {"ta_keywords": "word error rate metric;speech recognition errors;wwer metric;brain potential;standard wer;event;performance;new metric;impact;results", "pdf_keywords": ""}, "7261b088c48be7eca10263e765739f7347665481": {"ta_keywords": "single spin;spin;magnetic field;dynamics;model;presence", "pdf_keywords": "stochastic network equilibrium models;stochastic user equilibrium problems;stochastic game;novel network equilibrium model;routing games;optimal traffic assignment;stochastic network;network game;stochastic cost;discrete time stochastic models;markov process;classical wardrop equilibrium;stochastic process;shortest path problem;equilibrium;equilibrium problems;corresponding mean field game;optimal time scale;stochastic change;equilibrium problem;mdp;layer optimization model;deterministic outcome;network;optimal value;efficient transportationwe;convex optimization;variable demand;backward induction probability;optimal solutions"}, "4fb8009422903f7cb6f9a929409264b7fbca55e3": {"ta_keywords": "novel feature enhancement method;dimensional signal processing;noise feature vectors;clean features;piecewise linear compensation;splice;linear transformation;relative improvements;efficient operation;piecewise;low computational cost;algorithm;environments;method;paper;multistyle conditions", "pdf_keywords": ""}, "359dfdfea38f645d5fa49efc846a3b5ebce317fe": {"ta_keywords": "interpretable machine learning;conjectures;assumptions;world data;model;world;novel approach;approach;set", "pdf_keywords": "interpretable machine learning;interpretable models;interpretationsable machine;machine learning;supervised learning;policy experts;interpretations;definitions;models;stakeholders;proper definitions;algorithms;data;processes;analysis;scale decision;decision;ones;matrix elements;random matrix elements;literature;problem formulation;demand;terms;novel approach;consensus;wealth;basis;rise;unripe fruit"}, "2c94bc68388517aa4a2d2dfc7d35df95ce24b1a8": {"ta_keywords": "adversarial minimax game;invariant representations;trait;better generalization;data;factor;test performance;framework", "pdf_keywords": ""}, "5a0c9bbf0432dac8bd357a4aabf82b83a6c95524": {"ta_keywords": "document similarity measure;document similarity;document mining;pairwise document classification task;aspect;novel aspect;research papers;document;literature;specific aspect;datasets;context;terms;approaches;experiments;approach", "pdf_keywords": "traditional document similarity measures;document similarity;pairwise document classification task;aspect information;pairwise document classification problem;dissimilar documents;aspects;document pairs;aspect;similarity;wikipedia articles;novel aspect;large text corpora;disambiguation;document sequences;knowledge graph;distinction;citations;articles;text documents;document sequence;annotated trees;research papers;document;label citations;documents;term memory machines;word vectors;scientific paper;open research dataset"}, "4f74be7e5dd4b8e9113e86132cf792da2c32ca3d": {"ta_keywords": "turbulent medium;zeeman field;particle;propagation;stationary medium;vicinity;effect", "pdf_keywords": ""}, "90720bba46dd79bc340359617a7a07fcecc890c1": {"ta_keywords": "continuous strategy spaces;strategy spaces;local optimality emerge;equilibria;structural stability;games;perturbations;respect;sense", "pdf_keywords": ""}, "834fb0d09e764b88ef76ee77e0befb8faeaad7fe": {"ta_keywords": "speech parameters;synthetic speech;individual acoustic parameter segments;rich context models;parameter generation;statistical models;maximum likelihood criterion;posterior distribution;novel methods;methods;paper;use;experimental results", "pdf_keywords": ""}, "2507a6924007efbe0c3116048a85108398f23007": {"ta_keywords": "linguistic information;distinctive descriptive text;translation;automatic cross;encoded text;neural networks;accurate cross;accuracy;terms;different sources;model;art models;novel approach;robustness;power;state", "pdf_keywords": ""}, "5ad44a9d6b850405da42f989711af431427425b5": {"ta_keywords": "single language;different languages;mixed text;bilingual speakers;code;text;spellings;system;experiments;capabilities", "pdf_keywords": ""}, "d95b66901d72a13d0c96c7e9bfd4a999ed7fb19c": {"ta_keywords": "language;single text;global representation;structure;data sets;data set;single expression;standard approaches;novel approach;model;approach;use", "pdf_keywords": ""}, "ad21f0709a3e5d7db91606e2f67926f93e01838d": {"ta_keywords": "games;information;payoffs;players;values;novel class;new class", "pdf_keywords": "successive contextual games;contextual games;contextual game;contextual regret;adversarial linear contextual bandits;contextual regret factor;contextual equilibria;game equilibria;strategy game;contextual regrets;classical polynomial polynomial polynomial bandit game;optimal contextual welfare;finite size game;game outcomes;games;optimal policy;contextual information;stochastic context;agent learning game;global optimality;novel online algorithms;contextual coarse;contextuality;players payoffs;different contextual information;optimality;strategy;game;novel online algorithm;strategies"}, "f2d5861a24b7aa33036208ba81e11bb9b2090e7c": {"ta_keywords": "multilingual paraphrasing corpora;paraphrasing;shot baselines;softmax training scheme;autoencoding objective;languages;translation;human;novel training procedure;high quality;low cost;model;computational metrics;generation;novel approach;battery", "pdf_keywords": "large scale paraphrasing;parallel paraphrasing;paraphrasing sentences;shot paraphrasing;paraphrasing;paraphrasing words;lingual softmax baseline;sentence embeddings;efficient machine translation;multilingual corpus;neural machine translation system;high quality word embeddings;word embeddings;multilingual text;paraphrases;multilingual model;fluent monolingual rewriting;softmax;large parallel corpora;parallel corpora;autoencoding;translation;final softmax layer;text generation;computational linguistics;languages;autoencoding objective;text simplifications;parametric parametric parametric parametric parametric lemma breaking;effective parameter sharing"}, "0e02765103001a792b20242b4dee6dc81917b850": {"ta_keywords": "gene names;gene name;gene mentions;biomedical text;annotation scheme;novel method;new test;method;performance;extensions;approach;ability", "pdf_keywords": ""}, "5b94512a17483595c5dffc503a16ba0b46c347e5": {"ta_keywords": "synonyms;hypernymy relationships;synsets;aware relationships;russian languages;sets;gold standard datasets;sense;method", "pdf_keywords": ""}, "0e52ce6cfd1385e1e9304dcf71d66b53fdc2d4bd": {"ta_keywords": "information retrieval;recent trends;news;ir workshop;information;2nd international workshop;budapest;second workshop;european;large scale workshop;hungary;workshop;centre;conjunction", "pdf_keywords": ""}, "e705255814756178dba75638c29b602095c3cdf4": {"ta_keywords": "transfer;training;useful representation;layers;policy;sample complexity;representation;game;benefits;insight;requirements;majority", "pdf_keywords": "deep reinforcement learning model;quantum learning algorithm;reinforcement learning;complex game;quantum state;agent transfer policy;simple game;child agent;networks;training strategy;agents;variable reward;layers;double qutton network;representations;reward function;deep learning implementation;model complexity;transferability;child network;games berzerk;representation;sample complexity;useful representations;game;parent network;environment states;rainbow;policy;initial layers"}, "51203e9d5620abdcdf6c9be93b1e221e79cda67d": {"ta_keywords": "sequence transfer;dimensional languages;independent language models;independent fusion transfer;target language;transfer;language;hybrid system;sequence;de;novel approach;words;system;presence;performance", "pdf_keywords": "external language model;multilingual adaptation;novel language model;multilingual transfer;long short short term memory;bidirectional long short term memory;semantic linguistic model;high resource languages;unseen target language high2;independent deep learning;deep fusion;target languages;resource languages;higher dimensional language;speech recognition;conventional speech model;languages;end speech recognition;unseen target languagewe;attention;language;linguistic context;linguistic processing;sequence model;decoder network;unified sequence;additional text data;corpora;rapid adaptation;adaptation"}, "8319786ed7b9cb13e29130b5617bf0aef586cd6f": {"ta_keywords": "expert model;expert models;tutoring;cognitive student;study;accuracy;higher accuracy;better facilitates;evaluation studies;demonstration;author;key results;steps;completeness;strategy;same level;one;number;decreases", "pdf_keywords": ""}, "c37c40db51ccfd6f93004e788102ede72578e5d8": {"ta_keywords": "information networks;tweets;machine learning;information;earthquake warning;data;japan earthquake;users;framework;number;situations;october;amount", "pdf_keywords": ""}, "92259193a9d7377368790bf8517cd9798f30caae": {"ta_keywords": "information pollution;explainable questions;information;provenance;explainable answer;correctness;validity;approach;principles;examples;context;novel approach;answer;position paper;circulation", "pdf_keywords": ""}, "0e1a665334b1ec35d77ab1cd4f21bd0da9745548": {"ta_keywords": "text categorization;expert knowledge;algorithmic approach;recent algorithmic approach;accuracy;word;performance;terms;method;utility;efficiency;result", "pdf_keywords": ""}, "9ca95a09c8bf2d7d28234ff37ece182836dd8632": {"ta_keywords": "imitation learning;particle;synthetic data;random world;algorithm;novel transition;statistical model;behavior;task;action;performance;set;problem;fact", "pdf_keywords": ""}, "0aa0131253b832fdba27ac43f8fa78a322763191": {"ta_keywords": "paralinguistic information;many different possible paralinguistic features;output speech;input speech;target language;acoustic features;language;input;features;duration;continuous space;method;paper;power;end;independent fashion;first step", "pdf_keywords": ""}, "a94ec1cd89839aa5132118916849d46dff861914": {"ta_keywords": "mind modeling;interactions;dynamics;computational models;systems;mind;theory;particles;like entity;context;study;point;task;pair;form;case;set", "pdf_keywords": "situated collaborative tasks;game interactions;situated language communication;human collaborative behaviors;situated dialogue;collaborative dialogue task;interaction discourse;collaborative tasks;human dialogue;interaction;minecraft;dialogue task;collaborative agent evolve;popular minecraft game;interplay;cooperative game;cooperative behavior;dialogue utterances;dialogue exchanges;mental models;3d world;interactions;human mind;virtual blocks world;dialogue;communication;natural language tasks;dialogue utterance embeddings;disparate games;player beliefs"}, "8278e5c2a894793e2c93c6c9f0e7535109e7858f": {"ta_keywords": "2d electron gas;electron gas;repulsive interaction;2d;dynamics;attractive interaction;box;effect;study;results", "pdf_keywords": ""}, "c637636a2afd7968bdb893af8d2fd220fd39df8f": {"ta_keywords": "time meeting analyzer;meeting transcription;ongoing group meeting;meeting table;meetings;distant microphones;time meeting;conversations;latency monitoring;sensor interface;directional camera;aim;best possible performance;center;context;system;novellatency;techniques;paper;experimental results", "pdf_keywords": ""}, "ffac42087ee4ad50df9203762db715dedd209c0b": {"ta_keywords": "semantic tags;semantic analysis;free grammars;active tagsin;propagation instruction;context;propagation mechanism;such instructions;method;internet pages;new method;article;idea", "pdf_keywords": ""}, "919c929dfa665cb0595a835b4380f96da4cd0143": {"ta_keywords": "dimensional bandlimited sensors;dimensional bandlimited fields;field reconstruction;sampling strategies;simulation study;random paths;matrices;reconstruction;performance;paper;strategies;stability;results", "pdf_keywords": "sampling points;mobile sensor;location sensor;optimal sampling trajectories;sampling strategies;underlying sampling geometry;sampling;field samples;field reconstruction;noisy field;bandlimited field;location unawareness;dimensional bandlimited field;noisy data;random paths;random walk;field;simulations;location;trajectories;noise;countable number;measurement;reconstruction;position;regression;measurements;nonuniform collection;novel method;region"}, "59bdf61a81e46a6c9a96c0c5f96f2f77b82ab09f": {"ta_keywords": "subsurface detonations;free fusion;multiphase;dimensional electron gas;multipole;fusion;layers;msp;dynamic behavior;new generation;systems;context", "pdf_keywords": ""}, "c340b89e7b7fa84fac85cdcf38ba7007e2e71930": {"ta_keywords": "speaker diarization problem;speaker embeddings;real dialogue recordings;clustering;attention blocks;real telephone calls;mixtures;art clustering;self;method;end;new method;performance;state;study;experimental results", "pdf_keywords": "speaker diarization model;automatic speaker diarization;speaker diarization approach;speaker diarization method;speaker diarization;typical speaker diarization systems;nonlinear speaker diarization technique;nonlinear speaker representation;nonlinear speaker recognition;speaker label sequence;speaker embeddings;speech mixtures;spontaneous speaker separation;speaker;real speech datasets;audio mixture;real speech recordings;traditional speech recognition system;diarization error rate;diarization errors;clustering;neural network;audio recording;multiple speakers;conventional clustering;speakers;microphone;utterances;vector neural network;clustering loss"}, "0fcdf20477f907aa50578876226f5fabf5e074ea": {"ta_keywords": "link bias;world citation networks;citations;recommender systems;exposure probabilities;data;loss function;literature;experiments;methods;largest fraction;methodology;approach", "pdf_keywords": "empirical link prediction;empirical link prediction models;citation networks;citation network;world citation networks;citations;citation recommendation;link propensity;world citation datasets;link probabilities;empirical biases;link recommendation;social networks;empirical risk estimator;exposure bias;bias;intrinsic bias;scientific recommendation;optimal recommendability;lower bias;recommender;propensity scores;empirical observation;negative links;recommender systems;exposure probabilities;empirical data;complex networks;links;propensities"}, "da1296f071bb2b65ae9e0b016d914d24b4edb2d2": {"ta_keywords": "stability;weighted average;new quantitative measure;weighted;average;measure", "pdf_keywords": ""}, "916c8553beb3ba4e0d20ba6d7eb2bca365d820c8": {"ta_keywords": "protein;dynamics;process;study;presence", "pdf_keywords": ""}, "7f613ab03d776f996eb582f04d258a51868dca03": {"ta_keywords": "alkyl benzohydrazides;alkyl benzohydrazide;lipase;hydrazine;synthesis;classic organic compound;organic compound;method;yield;optimal conditions;insertion;equation;state", "pdf_keywords": ""}, "2aa85084315a4107e2b9b935506b4e9f11428601": {"ta_keywords": "caries mechanism;caries;model bovine surface;remineralization;remineral;incipient lesions;smooth surface;smooth model;smooth domain;model;results;study;combination", "pdf_keywords": ""}, "542ad79bb96fc10c46086778aaafc8f3509c5c18": {"ta_keywords": "nonconcave minimax problems;gradient descent ascent;nonconvex;novel algorithm;sublinear rate;agda;algorithm;faster rate;stochastic;sg;method;class", "pdf_keywords": "nonconcave minimax optimization;sequential gradient descent;gradient descent ascent;nonconvex minimax problem;stochastic nonconvex optimization methods;stochastic nonconvex optimization;convex optimization method;nonconcave objectives;gradient ascent;gradient descent;stochastic optimization;minimax problem;minimax problems;nonconvex optimization problems;stochastic optimization method;minimization problems;optimality;optimization method;stochastic gradient method;sided linear programming;global minimax points;global minimax point problem;global minimax point;semidefinite programming;generalized gradient methods;geometric optimization;optimal strategy;optimizationwe;sublinear convergence rate;iterative procedure"}, "69ee9b3a915951cc84b74599a3a2699a66d4004f": {"ta_keywords": "end networks;ma10 nipulation;unseen objects;stream archi9 tecture;segmentations;tasks;pathways;explicit representations;language;object poses;instance;manipulation;end;framework;worlds;history;variety", "pdf_keywords": "manipulation tasks;grasping;semantic manipulation;training tasks;unstructured tasks;grasping process;tasks;robot;world tasks;language grounding systems;pick action;broad semantic understanding;simple object representations;task models;robot gripper grasping;actions;unseen language benchmark;gripper robot;world robot;powerful tools;task protocol;semantic streams;new tasks;training agents;manipulation;task;action;task model;object representation framework cliport;objects"}, "fa3826770207f7bf8bd85a8e97c9ac437f46b061": {"ta_keywords": "false positives;scale experiments;multiplicity;significant reduction;threshold;effect;probability;value;comprehensive study;adjustment;number", "pdf_keywords": ""}, "ce89ee7aaeeea2c9d474707690f3ea9d948776a3": {"ta_keywords": "noisy machine translation systems;translations;noisy comments;japanese comments;benchmark dataset;english;dataset;adaptation;noise;performance;models;small training;reddit;domain data;methods;types;number;phenomena", "pdf_keywords": "neural machine translation system;machine translation domain adaptation;machine translation models;machine translation;noisy comments;best translation strategy;noisy text;translation task;translation strategy;japanese comments;translation system;translations;natural language processing;neural networks;systematic noise;translation;monolingual data;large scale machine learning;contrast corpora;underlying text;new benchmark dataset;simple domain adaptation method;comments;neural network;noise;machine learning;more noise;language pairs;pattern extraction process;gram model"}, "7f85b7ee0fc6cdb5b92417035a7049247729545a": {"ta_keywords": "class imbalance;optimal classifiers;classifiers;training data size;dataset;instances;bayesian framework;data;performance;results;method;number;idea;effects;problem;study", "pdf_keywords": ""}, "ce5a57c0ccc8993f4a8e3a07101140a757024d9f": {"ta_keywords": "spoken quotes;public talks;quotes;original quotes;memorableness;acoustic features;popularity;factors;form", "pdf_keywords": ""}, "61ad8a0778598022e71c0ee3ba9bc53ddd616517": {"ta_keywords": "interrelated questions;questions;conversational setting;language;implicit way;collection", "pdf_keywords": "sequential semantic parsing;sequential question answering;semantic parsing;semantic parsing system;semantic parsers;question answering;new semantic parser;semantic parser;first semantic parsing dataset;dependent parsing;parser;structured questions;complex intents;question sequences;neural enquirer;fewer complex questions;normal conversation;questions;simple related questions;intent;final answer prediction;soft attention modules;complicated questions;answer predictions;task;compositional questions;realistic task;wikipedia tables;context;soft attention mechanism"}, "9837207d3f4ee8c493375a97077c6f8b22cadac9": {"ta_keywords": "durational residency test;validity;tests;theorem;consequence;fact;result", "pdf_keywords": ""}, "de9d3a28f9e112a248d097d72ba6ad41a71c8a78": {"ta_keywords": "determinate clauses language;polynomials;polynomial;depth;logarithmic;particular type;method;novel method;fact;use", "pdf_keywords": ""}, "bcde1ba141078cf37a69a691fd329d8fd7e70b9b": {"ta_keywords": "active galaxies;single active galaxy;simultaneous generation;generation;new method;method;set;necessary step", "pdf_keywords": ""}, "83d6b4bfa8701578c291e55f5f1e5e6508aff313": {"ta_keywords": "medical ethics;technology ethics;patient autonomy;medical settings;technology design;technology;technological perspectives;different outcomes;future;practical suggestions;context;differences;use;similarities;areas", "pdf_keywords": ""}, "1bc87dba9838b3028b636f456084252f2beac108": {"ta_keywords": "incentive design;energy efficiency;social game;noncooperative game;more energy;game;occupants;utilities;building manager;interaction;individual particles;reversed stackelberg", "pdf_keywords": ""}, "f016ac107259d6d222c9f52b37208fca4fa1d6bc": {"ta_keywords": "novel use case;specific domain;domain;methodology;quality", "pdf_keywords": ""}, "36c770b79937db2e3416204b8cf177d0c9881f54": {"ta_keywords": "magnetized plasma;magnetization;magnetic field;magnetic properties;plasma;effect", "pdf_keywords": ""}, "0a485fd94b2cb554e281d0f8d7e9f71db4891ce0": {"ta_keywords": "novel token downsampling method;vision transformers;intermediate token representations;transformers;reconstruction error;images;objects;shape;accuracy;set;layer;computational cost;output;redundancies;cross;method;sense;problem;applicability", "pdf_keywords": "novel token downsamplingtransformers;novel token downsampling method;token downsampling;token downsampling principle;deep transformerswe;vision transformers;visual transformers;aware transformers;deep learning;softmax;aware downsampling;deep neural representations;deep learning task;intermediate token representations;transformers;large scale vision;downsampling;attention layers;dimensional transformers;deep neural networks;transformer;attention layer;convolution;attention field;neural networks;convolutional neural network;token selection;best features;tokens;redundant tokens"}, "d1f32060e921b6e06badd7fdb2b750638b2d131c": {"ta_keywords": "acoustic modeling networks;deep beamforming;channel speech recognition;np2012 meeting corpus;unified computational network;single network;joint training;power;advantages;method;effectiveness", "pdf_keywords": ""}, "98290fb02a844108df202e9a6dc3461e3f14ee32": {"ta_keywords": "optimization problems;stationary points;surrogate problem;th order taylor approximation;order;problem;approach", "pdf_keywords": "nonconcave minimax optimization;stronglyconcave minimax optimization;nonconcave maximizers;nonconcave maximizer;nonconvex minimization problems;optimal point;nonconcave problems;concave saddle point problems;global maximizer;optimality condition;optimization problems;proximal algorithm;optimization;sparse nonconvex optimization;weakly convex primal function;corresponding minimizer;hessian;hessian functions;convex functions;nonlinear constraints;optimal solution;maximization problem;local equilibrium;point approximation;quadratic optimization problem;stochastic optimization;convex;quadratic optimization algorithm;convexity;quadratic approximation"}, "ad26e5105b6019ff68404962e39ea3a1dfb5931d": {"ta_keywords": "incentive sequence;finite decision horizon;stochastic decision process;incentives;temporal logic formula;deterministic transitions;mdp;time algorithm;agent;principal;behavior;objective;design;cost;novel method", "pdf_keywords": "optimal incentive design;incentive design;incentive design method;novel incentive design;optimal agent policy;incentive sequence;optimal deterministic policy generation;minimum incentives;reward function;incentive offers;optimal policy;incentives;stochastic policy;optimal behavior;finite decision horizon;incentive design forwe;stochastic decision process;linear temporal logic;bandit model;temporal logic formula;optimal state;total reward;agent model;optimal control;agent system;polynomial time;decision horizon;optimal design;polynomialtime algorithm;agent behavior"}, "89a8edbc0fe2ea8b9ee703ca37e5d5d6d34c571a": {"ta_keywords": "end speech translation;translation performance;e2e;eq;model;end;st;novel approach;work", "pdf_keywords": "neural machine translation;neural network translation;end speech translation;machine translation corpus;automatic transcription task;standard automatic transcription task;deep bidirectional;high quality sentences;sequencelevel knowledge distillation;speech models;source transcriptions;large corpus;machine translation iswe;end translation;source text task;transcriptions;bilingual end;backward autoregressive e2e;semiautoregressive training;language model;corpus level utterance;sentence fragments;neural machine;source sequencewe;single decoder;encoder;common language model;deep knowledge;corresponding reference encoders;machine learning tasks"}, "daa7e6af585d03e9cb05487413a6495f23400398": {"ta_keywords": "wireless networks;assignment problems;assignment problem;permutation matrices;new unsupervised learning algorithm;binary variables;nonconvex projection task;assignment;nonconvex projection problem;multiple variables;set;letter;method;effectiveness;presence", "pdf_keywords": "convex assignment problems;network assignment problems;deep learning approximation;deep learning network;arbitrary assignment tasks;nonlinear neural network;binary assignment problems;deep learning;assignment problems;nonlinear matrix representation;network cost function;generic assignment tasks;deep learning approach;neural network;nonlinear programming language;original assignment problem;new deep learning approach;neural networks;learning algorithms;permutation matrices;wireless networks;permutation matrix spaces;novel neural network;unsupervised training algorithm;training task;traditional nonlinear learning method;cost function;joint utility maximization problem;utility maximization algorithm;training dataset"}, "f11ed27f4640dd8785ea7c4aff9705ddaad2b24d": {"ta_keywords": "representative detection methods;fake news;visual content;effective visual features;detection methods;detection;multimedia;readers;role;recent developments;concepts;field;comprehensive review;chapter;brief introduction", "pdf_keywords": ""}, "ad4b09832454a821e925e45e96e769f0c01bd3d6": {"ta_keywords": "word graph;arcsecond;novel algorithm;large collections;algorithm;documents;characterization;novel;sub;version;use", "pdf_keywords": ""}, "1f133158a8973fb33fea188f20517cd7e69bfe7f": {"ta_keywords": "accurate text classification;text classification tasks;benchmark datasets;models;simple models;de gennes;new class;counterparts;variety;larger sizes", "pdf_keywords": "fnet models;fnet encoder;attention models;attention sublayers;attention sublayer;fnet;fnet architecture;random encoder;novel attention mechanism;attention mechanism;convolutional neural networks;neural networks;convolution model;encoder;fast fourier transform;neural network;simple exponential time bottleneck model;transformer networks;simple random random random sublayer;random access networks;vertex training;memory;vertex learning;dft;simple random sublayer;large benchmarks;brownian motion benchmark;network models;bert;gaussian benchmark"}, "c9ce3889c03fee2990b2277423bbc0fb4366df53": {"ta_keywords": "discriminative language modeling problem;feature representation;embeddings;neural network;linear model;linear function;continuous space setting;log;data;paper;new approach;approach;form;results;use;experiments", "pdf_keywords": ""}, "2de8019fd7d04e3d1305d5efaeeb591f0d966550": {"ta_keywords": "speech recognition;deep transformers;easiest predictions;difficult ones;apsa;machine show;state;strategy;factor;method;results;art;case;problem", "pdf_keywords": ""}, "ca2144b895cf6812eec535261df9294896417425": {"ta_keywords": "relation classification task;relation extraction;semantic relation extraction;end relation extraction model;concept candidate embeddings;classification task;classification;information;information theory;ieee transactions;extraction;scientific tasks;results;end;first time", "pdf_keywords": "semantic relation extraction;relation extraction;relation classification task;relation classification;relation extraction model;semantic relations;semantic relation type;information extraction;semantic scholar corpus;relation;semantic information;specific relation types;large unannotated scholarly datasets;semantic ecosystem;linguistic structures;neural networks;scientific literature;neural network;neural models;complex linguistic structures;large scholarly dataset;deep learning;dependency layer;entities;scientific terms;scientific papers;classification;respective classification layer;subtask;artificial intelligence framework"}, "3638e5dfc79ba3fb757900f46ac0c7e7f6dadb05": {"ta_keywords": "aware cameraphone application;mobile media sharing;cameraphone;social uses;social network;prior photographic activity;photographic practice;communicative uses;photos;uses;new uses;use;prototype context;imaging;users;new functional uses;terms;empirical study", "pdf_keywords": ""}, "86eb740bbc54a6d734242be28fccf76fd4d2c1ba": {"ta_keywords": "stochastic game theory;parallelization;time learning;games;algorithms;learning;gradient;long time limit;computer science;convergence;several numerical examples;theory;time;chemistry;convergent;physics;class;problems;powerful technique;central issue", "pdf_keywords": ""}, "0fdc3efc11526995d192f18e19f07fba062a76f7": {"ta_keywords": "programmatic weak supervision;weak supervision paradigm;weak supervision;machine learning;gws framework;context;data scenarios;conjunction;gws;complementary approaches;recent advances;comprehensive survey;use;paradigm;sph", "pdf_keywords": "programmatic weak supervision;weak supervision models;weak supervision;weak supervision area;labeling tasks;labelings;labeling functions;supervised learning tasks;labeling;label models;labeling process;label closures;available labeling rules;labeling function;supervised learning methods;labeling modules;several label models;noisy partial labelers;specific labeling heuristics;label systems;training data;following label models;novel label model;supervision;labelers;labeling bias;labels;partial learning;label;learning framework"}, "66340a93813d8f816a8c82354a8f39fa985de27f": {"ta_keywords": "conceptnet;generic textual entailment system;large corpus;background knowledge;queries;text;several strong baselines;results;task;rewriter;end test;question;support;domain question;system;end;tandem", "pdf_keywords": ""}, "2ac6b8ade2a5e1ac89b99012ca6548eca4f8323f": {"ta_keywords": "robust deep learning;novel topological layer;topological features;persistent landscapes;robust generative function;layer;network architecture;various datasets;data structure;noise;outliers;classification experiments;effectiveness", "pdf_keywords": "novel topological layer;topological persistent homology approach;topological layer;topological data;global topological layer;weighted persistence landscapes;topological information;persistent homology;global persistence;topological model;persistence landscapes;novel topological approach;topological representation;weighted persistence landscape;general persistent homology;topological features;persistence landscape;topological structure;global persistence landscape;topological fashion;extreme topological distortions;underlying space;trivial topological structure;persistence diagrams;persistence;topological spaces;layer;persistence diagram;topology;dimensional topological insulator"}, "1fa02e5a5adffe82a41225f61f5f8ce86cf229d0": {"ta_keywords": "new segmentation model;random field;model", "pdf_keywords": ""}, "1cfd9b1db68fc320698da05fc6876dd0ea96fc9b": {"ta_keywords": "connectionist temporal classification;automatic speech recognition model;pruning method;mobile devices;pruning process;layer;size layer;model;end;method;factor", "pdf_keywords": "deep connectionist;connectionist temporal classification;end speech recognition;novel approach tolayer pruning;automatic speech recognition;deep learning;layer pruning;deep learning models;deep learning framework;layer pruning process;canonical correlation analysis;neural networks;vertex learning;pruning;computer vision;regularization;complete knowledge representation;regularization methods;training method;stochastic depth;layer similarity analysis;regularization power;singular vector;network;segmentation;sv cca;end speech;layers;layer;mobile"}, "acbb4495dd698b3190db6899d7d35b0817e0a85e": {"ta_keywords": "generative adversarial networks;standard gradient descent;generalization performance;proximal point method algorithms;minimax model;generalization properties;maximization subproblem;convex;optimization algorithm;iteration;results;key role", "pdf_keywords": "generalized minimax learning;convex minimax objective;new optimal minimax optimization method;minimax learning;minimax optimization;standard gradient descent ascent;stochastic minimax objective problems;stochastic gradient descent;convex optimization method;minimax learning problems;minimax learning task;minimax learners;minimax learning problem;convex optimization objective;stochastic optimization algorithms;stochastic generalization algorithms;popular optimization methods;convex optimization problem;task isminimax learning;generalized gradient approximation;minimax models;novel proximal algorithm;popular optimization algorithms;proximal point algorithm;optimization algorithms;minimax risk;minimax model;new minimax problem;optimization;learning objective"}, "0053f75b7053f43b9787a9955426281e672b147b": {"ta_keywords": "generative model;sentence;novel approach;shape;different trees;kklo algorithm;message;weights;approach;approaches;combination;methods;problem", "pdf_keywords": "unsupervised parsing;unsupervised tree chart parser;supervised parsing;latent tree chart parser;natural language inference;parse;natural language processing;phrase representations;accurate phrase representations;unsupervised unlabeled chunking;neural latent tree;unsupervised learning;free grammar induction;autoencoder language;highest scoring parse;phrase classification;tree;syntactic information;novel linguistic models;deep learning;trees;possible binary trees;external representation;deep learning perspective;syntactic structure;segment recall;novel deep learning framework;novel generative model;nodes;supervisedwe"}, "95e8edd26744ecc2bc23996cfaa68fe6252442a9": {"ta_keywords": "aware fake news detection;semantic sneture mining;claims;contextual semantic information;claim;empirical evidences;context;graph;information redundancy;sequential models;state;novel approach;problem;approach;effectiveness;art methods", "pdf_keywords": "evidence graphs;semantic sneture mining;fake news detection task;claim veracity;graph structure learning;fake news detection model;fake news detection;news claims;semantic structure mining framework;semantic structure;semantic structures;semantic data;veracity;evidence publisher information;complex semantic structure;semantic information;semantic relationship;belief propagation;semantic dependencies;semantic structure refinement;semanticin;evidence interaction modules;semanticwe;semantics;structured data;final claim verification prediction;semantic dependency;evidence;evidences;information propagation"}, "452059171226626718eb677358836328f884298e": {"ta_keywords": "natural language input sequences;dynamic memory network;natural language processing;input sequences;questions;sentiment analysis;large number;tasks;model;question;part;behavior;relevant answers;results;variety", "pdf_keywords": "recurrent network;natural language processing tasks;natural language processing task;episodic memory module;natural language representation;episodic memory modules;question representation;natural language processing;simple recurrent;natural language processing problems;iterative attention process;episodic memory problem;dynamic memory network;answer module;deep nonlinear memory;episodic memory;neural network;unstructured word embeddings;natural language;natural language systems;novel recurrent;many natural language question;deep network;natural language question;memory;sentences;deep learning;dynamical learning network;neural networkwe;sentiment analysis"}, "c39ac49e2d3feec992e84868256cb0a0ff028346": {"ta_keywords": "convex optimization;optimization framework;oracle call rate;oracle calls;novel method;new method;dimensional system;recent publication;method;paper;use;combination;sense", "pdf_keywords": "decentralized convex optimization;decentralized convex optimization problem;decentralized optimization;stochastic convex optimization problems;stochastic convex optimization;convex optimization;decentralized optimization setup;convex composite optimization problem;optimal decentralized algorithm;convex optimization problem;convex objectives;decentralized optimization method withwe;convex optimization algorithm;convex optimization method;linearly convex objective;new stochastic convex optimization algorithm;convex objective;convex objective function;stochastic composite optimization;optimal optimization;stochastic optimization;composite quadratic penalty;convex dual problem;decentralized algorithm;stochastic optimization problem;proximal gradient descent problem;strong convexity;optimal information;convexity;convex function"}, "affb8d759af00540458c19696532220dd1c1373a": {"ta_keywords": "deep neural network;text datasets;vocabulary words;languages;dnn;model;600k;hmm;system;competitive results;range", "pdf_keywords": ""}, "5c5bedaf66cadebbcd9116f38acd3df9ed43d816": {"ta_keywords": "paul wavelet transform;wavelet functions;continuous wavelet;spectral decomposition;frequency analysis;carbonate gas reservoir;entropy;absorption gradient;eopwt;reservoir;southwestern china;data;time;novel method;method;power;structure;use", "pdf_keywords": ""}, "bc247abf8180f583a42de392e4f7d2b2a41ad72d": {"ta_keywords": "independent interactive approach;novel parser;multichoice questions;domain databases;text;piia;limited interaction;users;experiments;power", "pdf_keywords": "parsers;semantic parsing process;parser;semantic parsing;arbitrary base parsers;identifysemantic parsing;structured queries;natural language pipeline;parserwe;natural language processing;independent interactive approach;complex queries;wikisql;complex databases;novel interactive approach;knowledge extraction process;natural language;sql;independent interaction framework;knowledge extraction;unstructured databases;uncertain tokens;queries;sql technique;complex text;uncertainty tokens;potential uncertain tokens;database;natural language utterances;sql database"}, "c5bcb690b0aa85ad0a5fd7e7aa4b8c468cd8c69a": {"ta_keywords": "discriminative training;novel discriminative training framework;margin estimation;energy minimum;error statistics;cost minimum;minimum;elementary graphical representation;dmmi;energy;framework;error;new framework;wide range;field;problems;efficient way;article", "pdf_keywords": ""}, "f300a62d0522d9a623b62f1305052928d8d7170c": {"ta_keywords": "emojis;interesting features;largest datasets;social media;dataset;balanced dataset;detection;data;subset;method;new method;area", "pdf_keywords": ""}, "50851e9e16b52e14c422b6e937cfd3ed063b6998": {"ta_keywords": "multilingual cross;multilingual representations;sequence tagging;amber;explicit alignment objectives;retrieval;parallel data;code;models;different tasks;average accuracy;large model;shot;average f1 score;different granularities;experiments;gains", "pdf_keywords": "multilingual bidirectional encoder;multilingual encoders;multilingual machine translation;multilingual representations;multilingual learning;language modeling;attentional neural machine translation;same monolingual data;parallel translation strategy;machine translation;parallel corpus;parallel translation;novel computational linguistics model;sequence tagging;word representations;word alignment accuracy;sentence classification show;alignment supervision;sentence retrieval;parallel corpora;universal bert pretraining strategy;source sentences;sentence pairs;bert pretraining strategy;corpora;explicit alignment objectives;target attention matrix;novel dark energy model;soft source data;sentences"}, "37e06f3622c17dc6194b547c944462b2a513b878": {"ta_keywords": "factual correction models;summary quality;summaries;factual consistency;corrections;span selection;knowledge;models;system;suite;question;experiments", "pdf_keywords": "accuracyabstractive sentence summarization;summarization quality;abstractive summarization systems;abstractive summarization system;abstractive sentence summarization;standard summarization quality metric;summarization system;abstractive summarization;summarization datasets;original abstractive summarization;abstractive summaries;summarization;multiple summarization datasets;summary quality;summaries;new sequential fact correction model toin;answer span;sentences;multiple factuality measures;fact correction;factual errors;natural language;factual consistency;source text;summary;sequence rnns;factual correctors;informativeness;factual correction;factual correctness"}, "e487f2508e5f62b2745a2e56ceb3c601c286d2e3": {"ta_keywords": "", "pdf_keywords": ""}, "705e6b53f88ec733e3c186c6232c41b268248c01": {"ta_keywords": "netflix prize dataset;sparse data;social choice;real movie ratings;behavioral modeling play;behavioral perspective;inference;analysis;notions;key roles", "pdf_keywords": ""}, "740182c3aa9a3045fcd9370269d446455ae9f623": {"ta_keywords": "neural finite state transducers;fast inference;inference algorithms;motion;conditional probability distributions;strings;parts;models;novel features;tasks;pairs;training;fnts;versions;ability;variety", "pdf_keywords": ""}, "ba159dbf205193d0cb7c9c18dd01f830d2f56eb8": {"ta_keywords": "natural language;semantic content;semantics;intrinsic structure;text;statistical comparison;information;method", "pdf_keywords": ""}, "b694472c13420acb599a5b1d25d5f2bd42eb8c1b": {"ta_keywords": "novel de novo assembly algorithm;algorithm;data;computation;information;minimum amount;space;theoretic perspective;respect", "pdf_keywords": ""}, "71d649dcb3dee2ca57d0775a9679cb68f82f22d5": {"ta_keywords": "snn training data;predictive neural network;feature extraction;snn;acoustic summary;input;main network;data;novel method;method;performance;approach;ability", "pdf_keywords": ""}, "0be998fffc5f44496042f7757fb2ffa8924e54cd": {"ta_keywords": "stochastic stochastic data entry model;stochastic data entry model;differentiable data selection;data scoring network;reinforcement learning approach;image classification tasks;ddd;ddd framework;translation;model;rich content;optimization;score;machine;quality;approach;power", "pdf_keywords": "shot learning;multilingual neural networks;data scoring feedback;training data;scoring network;training network;reinforcement learning;learning;domain adaptation;learning rate;scoring data;curriculum learning;iterative training;parameterized learning;neural networks training data;multilingual tasks;scorer network;machine learning pipeline;intuitive reward function;training data usage;machine translation;current training examples;machine learning;training;deep learning;multilingual nmt tasks;machine translation performance;conditional adaptation pipeline;classification;neural networks"}, "c5bb38b8e3ce21063670dfd81ac64dcb2ecf10b2": {"ta_keywords": "spectral notches;notches;finer contours;residual cepstrum method;frequencies;fast computation;linear prediction;images;pinna;pin;method;problem;comparison", "pdf_keywords": ""}, "8abd724b770348bd21b16b9aaf2ba0a77596b2ed": {"ta_keywords": "neural diarization;attractor calculation module;attractors;wise attractors;encoder;speaker;speakers;decoder;sequence method;sequence;eend;unknown number;basis;eda;end;number;method;paper", "pdf_keywords": ""}, "af787fda38ce6fa1d14ad2fb8568088faf973a21": {"ta_keywords": "spectral density;harmonic oscillator;frequency comb;mode;calculation;method;new method;use", "pdf_keywords": ""}, "be312e930f6739a709e60547aa0dfb9c3dc44497": {"ta_keywords": "multilingual machine translation;novel multilingual lexicon encoding framework;resource languages;encoding;languages;lexicon;nmt;latent space;meaning;standard dataset;word;framework;results;purpose;art;state", "pdf_keywords": "multilingual neural networks;multilingual neural machine translation;multilingual lexicons;multilingual translation;large monolingual data;deep neural machine translation;multilingual languages;multilingual nmt;small monolingual data sets;deep lexical representations;linguistic embeddings;translation encoder;novel lexical representation strategy;word vectors;novel word spelling model;related languages;high resource language;lexical similarities;lexical similarity;low resource language;nneural networks;nonlinear string learning model;languages;language;specific encoding;word pairs;word similarity;decoder;encoding;latent word"}, "e961c8de1df75f70254656e98ca82f9d9fbd640c": {"ta_keywords": "compressive phase retrieval;classical phase retrieval problem;phase retrieval problem;sparser signal;graph codes;phase;efficient algorithms;graph;measurement vectors;algorithms;extensive simulation results;novel family;framework;variants;practical power", "pdf_keywords": "friendly compressive phase retrieval;general compressive phase retrieval;compressive phase retrieval;novel compressive phase retrieval algorithm;phase retrieval problem;sparse signal;sparse signals;complexity phase retrieval algorithm;phase retrieval problem iswe;regular phasecode;phasecode algorithm;phase information;sparse graph code;sparse graph;sparse vector;novel sparse;algorithm phase code;sparse;arbitrary signal;matrix measurement matrix modulation technique;decoding;algorithm phase;noisy intensity measurement;efficient encoding;measurement matrix code;measurement matrix;exponential decoherence problem;signal processing;phase space;quadratic measurements"}, "e2a4e1a9f8e66baf12a49a3e5d8e33291f9347e7": {"ta_keywords": "neural semantic matching models;aggregated semantic matching;short text entity;public tweet datasets;disambiguation;semantic information;rank aggregation mechanism;novel neural network framework;matching signals;local context;representation;model;ess;evaluation;state;framework;art", "pdf_keywords": ""}, "f951aad88e244182b37e4918c3d570560108c68c": {"ta_keywords": "robust classifiers;gradients;randomized smoothing;novel method;property;alternative means", "pdf_keywords": "robust neural networks;adversarial examples;adversarial attacks;robust classifiers;smoothed neural network;gradient ascent;training classifiers;deep learning classifier;gradients;smoothed neural network assigns;gradient estimator;gradient;classifier;randomized smoothing;neural networks;random input image;randomized smoothing technique;smoothed network;robustness;convex relaxation barrier;high accuracy;stable predictions;trainings;stable stable predictions;smoothing;input image;convolutional entropy;smoothed version;convolutional neural network;threshold accuracy"}, "5eaa425af39339e0ae30202b348cc6e253813993": {"ta_keywords": "scientific information retrieval system;citation indexes;russian language;expertise process;patents;results;popular baseline;system;comparison;government contracts", "pdf_keywords": ""}, "0761a69310f7b8f4ab01495f31a30c6fe53d83b8": {"ta_keywords": "robust speech recognition performance;multiscale adaptation scheme;adaptive adaptation;acoustic model;dependent stochastic evolution;adaptation;macroscopic time scales;language model;posterior distributions;wave;function approach;novel method;time;methods;use", "pdf_keywords": ""}, "225767ce707781d0114815068c355622869ee642": {"ta_keywords": "cognitive signal;cognitive process;specific cognitive task;stochastic process;emergence;data;hypothesis;system;model;collection;sequence;presence;set", "pdf_keywords": ""}, "91184a2d40be8a0171b5c926b336666ed717ec6e": {"ta_keywords": "scientific digital libraries;biases;bias;structure;pedagogical introduction;popular paper;tutorial;selection;paper;parallel;focus;problem", "pdf_keywords": ""}, "7d148b46f45e935765e56887d720492b2b716e55": {"ta_keywords": "collective dynamics;independent reconstruction theory;reconstruction theory;networks;larger networks;intrinsic dynamics;networked systems;reconstruction;model;biology;social science;important role", "pdf_keywords": "neuron network;intrinsic collective dynamics generate;stochastic neuron network;synaptic connectivity;intrinsic collective dynamics;collective dynamics;neural dynamics problem;synaptic connections;neuronal behavior;neuron;neuronal activity;synaptic inputs;neurons;neuronal circuits;synaptic activity;physical connectivity;synaptic response;random network;dynamics;spike sequences;dynamical system;networks;few neurons;neural networks;unit dynamics;dynamical units;time evolution;interaction patterns;network;brain"}, "d3dd80269f2542cc173afb3a1df24b582a1e4af2": {"ta_keywords": "transformer;transformers;accuracy;simple normalization;different models;same input;input;position;cross;choice;right;section", "pdf_keywords": "bit strings;transformer encoder;encoder;deep bidirectional transformers;position encodings;shorter training strings;strings;output strings;encoding process;longer strings;stable transformer representations;string;alphabets;input strings;alphabet;certain languages;transformers;languages;training transformers;real machine translation task;simple programming languages;transformer;subnetworks;transformer definition;different languages;language;testing strings;machine translation;convolutional neural network;input input string input"}, "920257774e2caee8a8c74968c64c10bcb79a136c": {"ta_keywords": "hybrid genetic algorithm;transmission lines;propagation function;vector;approximation;conventional method;characterization;number;approach;viable alternative", "pdf_keywords": ""}, "147b954ba0881d643706c918e017f7d66a15b827": {"ta_keywords": "student representation learning framework;student representation learning;intelligent tutoring systems;cognitive skills;such cognitive models;cognitive model;cognitive models;students;representation learning models;student behavior;cognitive science;student;powerful learning environment;detailed representation;skills;cognitive context;model;instruction;algebra;robust learning mechanism;manual construction;stoichiometry;domain expertise;important component;construction;approach;novel approach;essential features;combination;generality", "pdf_keywords": ""}, "3426f000673aae995a55ade9273c842bb484ad18": {"ta_keywords": "monoand multilingual phoneme recognizers;crosslingual phoneme recognizers;phoneme boundaries;automatic detection;bantu language;audio recordings;phoneme units;unknown language;unwritten language;monolingual gold standard;segments;boundaries;technique;performance;different configurations", "pdf_keywords": ""}, "89c2cbdf1a5049a4068ca9215aa8859a1a97b1a3": {"ta_keywords": "contextual bandit learning;reward;supervised cost;sensitive classification problems;learner;optimal performance;algorithm;several strong baselines;concept experiment;new algorithm;excellent computational performance;action;rounds;oracle;access;approach;proof", "pdf_keywords": "contextual bandit learning problem;contextual bandit problem;contextual bandit setting;randomized learning;optimal regret guarantee;maximum reward;optimal strategy;reward;rewards;stochastic stochastic stochastic learning;global coordinate descent algorithm;learning;coordinate descent algorithm;empirical policies;unconstrained random walk matrix;learning algorithm;coordinate descent;coordinate descent procedure;scale search algorithms;regret analysis;rewardwe;policy class;optimality;sensitive learner;quadratic learning model;cumulative regret;regret guarantee;optimal quadratic optimization;exploration;observed context"}, "89f7db77a755d44d3aabdbcc7549b743d7debcc5": {"ta_keywords": "preference networks;preference relation;individual preference relations;preferences;dependence structure;dependence links;features;choice;objects;uncertainty;particular object;structure;set;new formalism;terms", "pdf_keywords": ""}, "18a82459d495fa3ad22a60bd7c9527df8bd55e1e": {"ta_keywords": "network games;deterministic network;gret learning algorithm;dual averaging;network;distinct objectives;algorithm;connectivity;teams;players;available observations;regret;analysis;case", "pdf_keywords": ""}, "341f6353547f4a58fdf11fbcc9de3a31083a619b": {"ta_keywords": "einstein equations;einstein field;dimensional harmonics;exact solution;energy;calculation;method;new method;combination", "pdf_keywords": ""}, "7a79099447bef9a3ea13b1dc409d04b3dff57320": {"ta_keywords": "underlying music;music;underlying track;generative structure;relative amplitude;structural properties;model;events;new model;relative velocity;novel set;sequence", "pdf_keywords": "automatic music composition;music information retrieval;music information processing;music modeling;music composition;new music compositions;music discovery;music vertex decomposition;musical score;music structure;music;art music transformers;automatic transcription;music performance;music bar;music style;expressive music performance;encode chord information;expressive piano music;supportive musical tokens;audio recordings;neural sequence models;language modeling;novel beat;information retrieval;sequence model;convolutional neural network architecture;note events;novel generative framework;novel neural network"}, "e9d8db4f5b5c106c43a268f635788c0a94b2916a": {"ta_keywords": "stochastic gradient descent;stochastic gradient;ascent methods;stochastic estimates;variational inequalities problems;optimization;unified convergence analysis;variance reduction;coordinate randomization;arbitrary sampling;variants;compression;several new variants;unified framework;approach;parametric assumption;variety;flexibility", "pdf_keywords": "stochastic gradient method;stochastic extragradient method;stochastic methods;stochastic gradient;stochasticity reduction;stochasticity reduction scheme;stochastic extragradient method iswe;stochastic method;stochastic generalization;stochastic variant;stochastic variants;stochastic learning;stochastic version;generalized gradient method;stochastic variational inequality;stochastic estimator;convex optimization;stochastic estimates;gradient methods;stochastic sampling;convex optimization problems;stochastic sampling framework;variance reduction;alternating gradient approximation;stochastic;sgw method;stochastic error;unbiased compression;stochastic variable gk;stochastic differential pricing"}, "b9913ddf94245c864509f0b94847bdbe77899b46": {"ta_keywords": "tonal prediction;connectionist temporal classification loss function;tonal transcription;neural network architecture;linguistic workflow;acoustic signal;language documentation setting;typographical errors;technology;novel method;use;promise;findings;efficiency;faithfulness", "pdf_keywords": ""}, "268d347e8a55b5eb82fb5e7d2f800e33c75ab18a": {"ta_keywords": "high speed vision;level convolution;parametric oscillator;level transformer;stage;level;shape;input data;generation;action;new method;method;novel;application;results", "pdf_keywords": "large scale image recognition benchmarks;deep images;attention weights;convolutional models;deep bidirectional transformers;representation learning;image recognition benchmarks;large scale dataset;large scale datasets;deep network;large scale tensor networks;convolutional local feature processing;attention;transformer models;accurate image embeddings;large dataset;large datasets;deep neural network;transformer;vision transformer;transformers;encoder;attention distance;large scale nonlinear models;novel encoder;natural language processing models;embeddings;neural architectures;positional embeddings;transformer model"}, "4533fd4cf13d2f4dd105edaf612934a1bd85ad5a": {"ta_keywords": "multichannel electroencephalogram;noise removal;probabilistic generative model;singletrial event;potential data;potentials;signal;simulated event;amplitude;method;coefficients;new method", "pdf_keywords": ""}, "b62430b9f8810da4d9f28842ac0ca899aa66d422": {"ta_keywords": "orbit coupling;zeeman field;electron;spin;strength;effect", "pdf_keywords": ""}, "d1c41eb99824e8f4752190da1b815378be23b4b9": {"ta_keywords": "autoregressive models;popular quadratic mean field model;quadratic mean field model;dependent exposure bias;model;sampling;accuracy;framework;smf;novel approach;psf;pmf;approach;output;several instances", "pdf_keywords": "sequence tasks;autoregressive models;rnns;scheduled sampling;neural machine;neural machine translation;reinforcement learning;imitation;input sequence;models;neural machine translation algorithm;inference time;term memory;sampling;sequences;catastrophic forgetting;sequence;sampling variant;convolutional neural model;neural networks;prediction accuracy;prediction;sampling leads;neural machine learning algorithm;model;language model;convolutionneural machine translation;model reliance;regularization techniques;memory"}, "175b58fe7e49bb5c0c771b73f8834bcff21b59c7": {"ta_keywords": "natural language inference;sentence complexity;encoder models;stress tests;sentence;evaluation;models;evaluation methodology;real inferential decisions;datasets;task;weaknesses;strengths;systems;future work;ability;different degree;directions;area", "pdf_keywords": "sentence encoder models;sentence encoder model;natural language inference;art sentence encoding models;natural language;natural language context;sentences;stressen models;textual entailment;nlp;specific linguistic phenomena;linguistic phenomena;distraction test;annotators;stress tests;lexical problems;stress test;word composition;neural models;entailment pairs;unstructured textwe;distraction iswe;unstructured text;nli;simple text;stress;tests;testing hypotheses;semantics;sentence"}, "49edf7f0dbad8b8c101af9ef95c72f62f545591e": {"ta_keywords": "compact topic embeddings;topic vectors;topic correlations;topic occurrence;topic size;variational inference;efficient inference;fast sampler;sparsity;quadratic time complexity;data;space;model;closeness;experiments;new model;method;approach", "pdf_keywords": "compact topic embeddings;topic inference;topic embeddings;topic modeling;novel topic model;compact topic vectors;conventional topic models;topic model;topic vectors;topic distributions;topic correlations;topic distribution vector;latent topics;document topic distribution;topic occurrence;topics;hard topic sample;topic size;document embeddings;large corpus;stochastic inference;dimensional topic;novel probabilistic variant;large document corpora;topic;variational inference;low dimensional document;accurate topic;noisy stochastic feature;novel topic"}, "db79a3e55690c5c86cfd0ec97712ed4ad1e47b3b": {"ta_keywords": "sequential ranking algorithm;active ranking;ranking;noisy pairwise comparisons;comparisons;items;probability;item;number;logarithmic factors;set", "pdf_keywords": ""}, "3c37b9ec2ff1828877575acc600b73c3bcde138f": {"ta_keywords": "recommender system;recommender systems;bandit setup;naive approaches;algorithm;novel approach;users;behavior;approach;types;challenging case;case;same type", "pdf_keywords": "stochastic bandit problems;bandits;optimal recommendation algorithm;armed bandits;optimal user rewards;optimal learning policies;optimal policy selection;stochastic programming models;stochastic programming model;optimal policies;recommender systems;recommendawe;optimal policy;novel learning problem;user rewards;recent optimal algorithm;stochasticwe;policydependent tradeoffs;learning framework;new online problem;optimality;selection;learning;simple threshold policy model;optimal performance;optimal planning policy;reward;dynamic programming approach;stochastic nature;many rewards"}, "f6f4d30e4740bd92b31acd297a15872d490e7f11": {"ta_keywords": "classification tasks;traditional supervised classification tasks;classification benchmarks;many novel domain;realistic domains;relation;specific heuristics;many domain;extraction tasks;classes;optimization methods;declarative language;link;results;modest improvements", "pdf_keywords": "text classification;relation learning;new supervised learning approach;relation extraction;learned classifier;classification;learning model;learning algorithm;classifiers;classifier;classification modelwe;new learning algorithm;learning process;relation trees;learning problem;training data;failure failure failure models;declarative language;label propagation;classical learning algorithm;deep learning;learninger;relation objects;failure model;failure failure failure;relation objectswe;deep learning paradigm;learning performance;failure examples;declarative representation"}, "dab261b25ff8ccd2c9144a5cb3a46b39ac0ac4bd": {"ta_keywords": "quantum information;quantum computers;quantum system;performance;current state;system;brief introduction;relevance;design;concept;summary;survey;problem;extent", "pdf_keywords": "counterfactual reasoning;machine learning systems;machine learning;deep learning;machine learning conferences;language inas machine learning;neural networks;reproducibility;neural network;generative models;imageneural networks;prediction;deep neural network;standard generative adversarial network;algorithms;mathematical physics;forward classifier;generative model;predictions;machine;prediction function;theory;current misrepresentations;misconceptions;speculative advances;few examples;common sources;precise technical meaning;future work;patterns"}, "b79dcc5304e557ce200b161d2a884c0ff77f34ec": {"ta_keywords": "additional spelling correction models;spelling correction;misspellings;platform;english;service;testing;utility;training", "pdf_keywords": "misspelling spelling correction system;spelling correction toolkit;hypothetical hypothetical word building misspellings;misspelling spelling counter;different spelling correction systems;neu spell recognition toolkit;other misspellings;spelling testing;misspellings;word spelling correction;misspelling dictionary;spelling correction;misspelt word pairs;spelling errors;correct spelling errors;negative spelling errorwe;word recognition tools;word recognition;word recognition tool;word recognition model;specific spelling problem;missing words;neural models;deep learning module;word list;short word list;word pairs;dictionary;deep contextual representations;words"}, "9b621c0bcd2029006b389bc51395fb6604f9a855": {"ta_keywords": "questions game;human answerers;question examples;questions;simple descriptive model;humans;dynamic model;goal;domain clarification questions;framework;model;manner;ability", "pdf_keywords": "human answerers;captioner utterances;natural language questions;image captioner;captions;questions game;answerer model;full caption model;natural language;dialogue games;random questioner;informative questions;questions;scenes;answerer;dialogue;images;machine interactions;questionswe;reasoning;unseen images;computational context;ms coco dataset;human;goal;candidate questions;robust information;context;component noun phrases;information"}, "a84c319fef32b2514af9541576189a1735aac507": {"ta_keywords": "network;construction;new method;method;introduction", "pdf_keywords": ""}, "f2de2c9d83b9058695e3272a4fbb7c30e04a1476": {"ta_keywords": "plausible word level predictors;english semantic density;second language;duolingo mobile app;lexical semantics;english;novel predictors;word;distributional models;ease;large scale;factors;large data;data;significant effects;users", "pdf_keywords": "second language learning;second language;duolingo mobile app;word recognition;semantic alignment;duolingo;duolingo users;english words;duolingo platform;different languages;language;word level;large corpus;native language background;languages;different language interface languages;higher accuracy;learning;semantic space;words;lower accuracy;word frequency;l2 direction;translation;english;many morphological variants;target word;accuracy;similarity;predictors"}, "5b6c1e9dddc4b55036a5629227ae2cc7d49eb6d0": {"ta_keywords": "first astronomical image processing tool;structured image finder;structured images;biomedical literature;images;subject descriptors;individual papers;literature;slif;tool;collection;representation;date information;type", "pdf_keywords": ""}, "a1babdf55a6bff96d533fd0c9bc44864283ec107": {"ta_keywords": "corporate governance variables;banks;particular bank;depositors;bank;external auditors;risk aversion;chairperson;account;decision factors;lower rate;simultaneous promotion system;president;following;service;existence;respect;paper;longer time", "pdf_keywords": ""}, "3af5e203368fa2c7959d035493571d181a8682af": {"ta_keywords": "classical music performances;midi format;musical score;audio recordings;individual tracks;audio;performances;novel dataset;dataset;level transcriptions;video recording;analysis;high quality;assembled mixture;note", "pdf_keywords": "novel music performance dataset;music performance analysis;multimodal music performance;music transcription research;music transcription;music transcription sequences;multimodal musicalthough;piano recording;individual individual recordings;music performances;individual drum recordings;professional chamber music performances;classical music performances;midi score;audio recordings;sheet music recordings;music audio;sound analysis;monophonic pitch analysis tool;audio modality;music production;collaborative music system;midi format;pitch analysis;sound recordings;individual instrumental parts;musical scores;polyphonic signal analysis;musical score;classical chamber music pieces"}, "8f43b63ca400a0ea1fdd272f8c83fd67f01d0182": {"ta_keywords": "talk features;talk signals;encoding;respective tags;tags;cross;features;data;sentences;competitive way;system;form;novel approach", "pdf_keywords": ""}, "14a09a04c5c295a93ff25492516112cd86fa0114": {"ta_keywords": "variational encoder;morphological inflection;languages;coderders;benchmark;model;new model;art results;majority;state", "pdf_keywords": "variational autoencoder;variational encoder;variational encoderdecoder framework;lemma representations;label transduction problems;unsupervised generation;encoders;underlying encoder;generative model;variational inference model;encoder;supervised learning;underlying supervised learning model;lemma embeddings;generative process;supervised learning model;decoder model;sequence transduction;deep learning;unlabeled data;decoders;decoder;latent representations;machine translation;deep learning paradigm;supervised learning problem;machine learningwe;recognition model;machine learning;incremental variational auto"}, "5e63e47cb3386b032ec43a92ce5980466228c761": {"ta_keywords": "new storage code;center cluster;cluster;erasure;recovery operations;multiple petabytes;data;center network;network traffic;data results;recovery;production;day;measurements;impact;significant increase;study;issue", "pdf_keywords": "data storage;storage code;storage systems;asymmetric storage systems;new storage code;storage optimality;storage;storage efficiency;significant storage resources;erasure codes;traditional erasurecodes;hadoop;availability;erasure;recovery algorithm;recovery operations;piggybacking framework;facebook warehouse cluster;network infrastructure;cluster;single disk failures;disk usage;memory;encode data;data;minimum download;network;smaller download;recovery;system design"}, "af44f5db5b4396e1670cda07eff5ad84145ba843": {"ta_keywords": "recursive neural network;neural networks;rnn;text;bowl quiz;questions;model;dataset;best human players;question;field;consistent manner;further applications", "pdf_keywords": ""}, "1bd43c91ecbf46098ef2b521c5367e849819960e": {"ta_keywords": "translation models;translation model;dynamic curriculum;machine translation system;domain adaptation;translation;synthetic data;back;selection;weighting;strategy;novel approach;variety;approach;use", "pdf_keywords": "neural machine translation;neural machine translation model;machine translation;translation baseline;translation quality estimation;translation quality improvement;translation quality improvement metrics;domain adaptation;translation quality;monolingual corpus;dynamic curriculum;domain adaptation models;language models;monolingual data;domain adaptation context;translation output;methods andmachine translation;parallel training;monolingual corpora;iterative back;translation;curriculum strategy;language pairs;translation equivalence equivalence;dynamic data selection;sentence representativeness;quadratic translation kernel;dynamic selection strategy;best static data selection strategieswe;language"}, "16f0c508aa54e26aa18e3b0f3c91b0c143c6a605": {"ta_keywords": "empirical risk minimization;robust optimization;representation disparity;fair models;worst case risk;machine learning;dro;erm;common problem;time;approach;status quo;significant amount", "pdf_keywords": "group risk minimization;subgroup fairness model;turk loss minimization;fair models;representation disparity;loss minimization;group risk;general lossminimization problem;robust optimization;new discrimination threshold;best representation;discrimination;robust stochastic optimization method;minority group user satisfaction inwe;robustness;representation representation representation;robust approach;disparity amplification;machine learning;common representations;groups;representations;group members;amazon mechanical turk;empirical distribution;disparity amplification problem;outliers;group;representation;similar risk"}, "5c159745fce2b87e8b00307b76f0948b9fa8b1d7": {"ta_keywords": "forest;string;tasks;novel architecture;system;combination;unprecedented performance", "pdf_keywords": ""}, "7a1a202e268ccc910e8044be556e56aa9eb5a94f": {"ta_keywords": "dependence plots;bias detection;datasets;model selection;sample behavior;raw feature space;model response;latent space;single features;example;multiple use;usefulness;arbitrary directions;method;cases", "pdf_keywords": ""}, "c674ce6454d69a87f00797f3ec90d1b38b451063": {"ta_keywords": "dual stochastic gradient oracle method;convex optimization problems;duality gap;networks;convergence;large deviations;new technique;rate;communication steps;method;probability;new analysis method;terms", "pdf_keywords": "stochastic dual oracles;dual gradient descent algorithm;convex optimization;convex optimization problems;dual accelerated gradient methodswe;stochastic oracle;accelerated dual gradient method;optimal oracles;optimal communication complexity;stochastic gradient model;dual approach;dual method;optimization;accelerated gradient method;stochastic problem;optimal estimate;networks;algorithms;global approximation;optimization problem;optimality;large matrices;optimal transport;oracle complexity;convergence rate ofwe;sum minimization;explicit oracle;neighboring agents;learning algorithm;duality gap"}, "2aecdd190066a57db8fea1e1143dc5fc288050e0": {"ta_keywords": "new privacy metric;privacy;adversary ability;private parameter;strong adversary model;realistic grid application;consumers;data;grid degrades;internet;performance;operational value;frequency;tradeoff;general framework;framework;things", "pdf_keywords": ""}, "242cf2e991f0eed4b1309a2a9dff548e8b95900f": {"ta_keywords": "robust speech recognition;beamforming methods;timefrequency mask;generalized cross correlation features;neural network;timefrequency distortionless response;maximum likelihood estimation;learning;features;weights;model;methods;paper;comparative study", "pdf_keywords": ""}, "b3bd90f630b2d19856ef031b3dddfcb9b041b243": {"ta_keywords": "learning strategies;examples;skills;feedback;problems;problem", "pdf_keywords": ""}, "dd3770b2dbc9668578fefdc078d37457ba9c0b9a": {"ta_keywords": "speech enhancement;statistical excitation prediction;un voice conversion;electrolaryngeal;hybrid enhancement process;eel;hybrid approach;effect;pass filtering", "pdf_keywords": ""}, "44cabe32482d4b622d9ca00bf23b3ee7950e2710": {"ta_keywords": "predictive decisions;higher quality decisions;judgments;criteria;complementary strengths;human;machine;optimal ways;variation;ml;unifying framework;taxonomy;underlying causes;conditions;context;wide variety", "pdf_keywords": "machine decisions;machine decision;predictive decisions;machine predictions;ml decisions;human decisions;machine cognition;human decision;decision maker;human expert;machine perception;ml complementarity;decision makinghuman decision making;hybrid decision;judgments;higher quality decisions;underlying decision making process;ml collaborations;same decision space;machine policy;machine learning systems;decision making;machines;prediction;artificial intelligence;decisions;unifying optimization;human perception;machine learning;algorithmic scores"}, "3ac59132297f4e50d5e83852555392f9ff05d8b4": {"ta_keywords": "quantum dot;particle excitation;spectral density;schrdinger equation;excitation;strong spin;orbit;calculation;new method;method", "pdf_keywords": "decentralized optimization problems;stochastic convex optimization;convex optimization problems;general convex optimization;convex optimization problem;composite optimization problem;free optimization problem;optimal optimization procedure;iterative minimization;gradient descent method;optimization method;optimization;stochastic oracle;optimal algorithm;convex;convex combination;optimization problem;convex functions;optimal point;dual method;convex sets;gradient term;stochastic;parallelization;new iterative algorithm;affine constraints;smooth part;smooth term;hamiltonian approach;convergence rate"}, "ff3b83ef0a153ed376556057269f3a61da3a103a": {"ta_keywords": "symbolic separation;multitrack music;part separation;sequential multiclass classification problem;symbolic mixture;instruments;separate parts;part labels;tracks;mixture;notes;models;heuristic approach;machine;keyboard;sequences;task;paper", "pdf_keywords": "instrumental instrumentation;music sequences;multiple instruments;instruments;voice separation;multitrack music;different instruments;symbolic music generation models;musicnet;automatic instrumentation;part separation;part labels;online automatic instrumentation model;music;string quartets datasets;separate parts;joint chorales;sequential multiclass classification problem;musician;term memory models;extensive empirical evaluation;notes;original instrumentation;data augmentation;music21;datasets;string quartets;comprehensive empirical evaluation;random data augmentation;inputs"}, "cb90d5ea3a95b4c6ec904f622f51d752f506636e": {"ta_keywords": "nodes;network;new metric;metric", "pdf_keywords": ""}, "d4f5f1a196e203226e4a69d52a04d46823f32fb3": {"ta_keywords": "multilingual nmt models;indic language pairs;indic languages;sentence pairs;languages;available benchmarks;available parallel collection;dataset;models;web;baselines;public domain", "pdf_keywords": "large parallel corpus;parallel machine translation system;large parallel corpora;parallel corpus;largest parallel corpus;high quality parallel corpora;parallel corpora;englishcentric parallel corpus;available parallel corpora collection;parallel translation system;multilingual machine translation;machine translation system;available monolingual corpora;monolingual corpora;centric corpus;parallel documents;parallel sentences;corpus;translation models;indic language pairs;neural machine translation system;neural machine translation model;many corpora;corpora;multilingual content;nonmachined corpora;high semantic similarity;translation data;monolingual sentences;pivot languagein"}, "d301054c2819e1a21480800fdabbe5ae909abe09": {"ta_keywords": "program search;natural language annotations;program;search models;libraries;search model;library;synthesis;joint learning;language;task;laps;method;technique", "pdf_keywords": "program synthesis;deep synthesis primitives;program abstraction;program synthesis problems;program abstractions;synthesis tasks;program learning;structure synthesis tasks;compositional generative models;learned library;hierarchical program induction model;generative models;abstraction;abstractions;joint abstraction algorithm;program samples;learned task;base programming language;abstract reasoning;generative model;synthesis;annotated training tasks;domain synthesis;program;program compression;joint generative model;structured image generationwe;abstraction domains;natural language models;neural search heuristic"}, "27b7489bd54dfd585edd2ba0da3920a31e7fd8b5": {"ta_keywords": "sensorimotor games;sensorimotor game problem;continuous learning;sensorimotor;transient play;teleoperation;learning;games;linear quadratic potential;gradient;predictions;empirical observations;context", "pdf_keywords": ""}, "af38829cdb55ee7b71d49399f71397d975e40a95": {"ta_keywords": "conditional answers;answer values;questions;hard questions;data;answer;maximum likelihood distribution;values;question;novel data release;process;range;set", "pdf_keywords": "conditional answers;novel conditional conditional conditional conditional learning condition;conditional conditional conditional knowledge condition;novel conditional conditional learning model;conditional conditional learning condition;conditional questions;waiting comprehension datasets;conditional conditions;conditional conditional version;conditionalqa;conditional condition;conditional values;original reading comprehension model;annotated questions;original comprehension model;unstructured text documents;conditional qr;complex documents;unstructured text;comprehension;exact text spans;documents;document;answers;long documents;challenging dataset;conditionswe;conditional qrqr;user scenarios;text"}, "2c2234548de4694b6455a19cd0d85a9d6c473456": {"ta_keywords": "similarity search methods;search quality;benchmarking;benchmarks;library;quality;evaluation;tools;applications;results;document;purpose;variety;goal;set", "pdf_keywords": "approximate similarity search methods;similarity search;approximate nearest neighbor search;nearest neighbor search;possible similarity measures;computing distances;sparse vector spaces;nearest neighbor;scalable scalable search method;scalable search method;distances;regularmnemonic distances;relative distance values;descent search algorithm;common distance;scalable algorithm;search algorithm;algorithms;metrics;high dimensional general metric spaces;sequential searching;optimal distance;distance;classic quadratic form distance;range search;nslib search;metric spaces;dimensional space;pivot neighborhood indexing algorithm;compact partitioning methods"}, "c00ba15810496669d47d2ed5b627e6c7d2b1f6aa": {"ta_keywords": "document translation tasks;document translation;sequence model;summarization;sequence;information retrieval;adaptive learning approach;tasks;high performance;model;new method;range;variety", "pdf_keywords": "large multilingual text;large multilingual corpus;relevance documents;crosslingual sentence retrieval;expensive machine translation training;document summarization;natural language reconstruction;document translation;information retrieval;large scale machine translation systems;paraphrasing;document reconstruction;large corpus;machine translation;underlying machine translation system;retrieval;encyclopedic task;level machine translation decoders;corpus;generative tasks;documents;new lexical aspects;paraphrase;document;news text;related batches;standard language classification algorithms;natural language understanding;scientific documents;translation"}, "4b890b6ded71f005414e55adb87c23efd437ef95": {"ta_keywords": "statistical parametric speech synthesis;voice conversion;various synthesizers;modulation spectrum;natural speech;postfilters;speech;tts;text;ssf;vc;segment;paper", "pdf_keywords": ""}, "1006d191e9eb5b4dbc35fc0bb389328ddc75cba7": {"ta_keywords": "byte;inference speed;t5 architecture;pronunciation;level models;level counterparts;spelling;training flops;tasks;parameter count;offs;terms;new set;trade", "pdf_keywords": "long text sequences;efficient machine translation architecture;arbitrary text sequences;neural language models;neural machine translation;unstructured text tasks;simple tokenizer;text sequences;language models;scalable machine translation architecture;performance machine translation decoder;language representations;unstructured text models;tokenization;natural language;contextcomputational linguistics;unstructured text;text preprocessing;text task;language model;arbitrary unstructured text structures;linear time machine translation techniques;corpus;deep learning;computational linguistics;raw text;free encoder;decoder;text;efficient machine learning architecture"}, "041b2510e54b2504890cb9f58b9bbc5601f35e3e": {"ta_keywords": "nonlinear dynamics;assignments;data;students;level course;level research;graduate;structure;field;set", "pdf_keywords": ""}, "399ab2a0eddf7a7abf776241d5c0a2c4cd5bf313": {"ta_keywords": "linear speech recognition;continuous speech recognition;speech model;linear graph;perceptron algorithm;weight parameters;parameters;representation;paper;mean;novel approach;problem;means;approach;experimental results", "pdf_keywords": ""}, "4b9bc5b5985bbfbc860039b98f233f8a43ad9171": {"ta_keywords": "parametric partial differential equations;artificial neural networks;pdes;pde;dependent coefficients;coefficient fields;few features;physical quantities;variability;time;examples;accuracy;simplicity;approach;observation;space;number", "pdf_keywords": "dimensional conductivity;nonlinear elliptic stochastic partial differential equations;nonlinear schrdinger equation;ground state energy;schrdinger equation;nonlinear pde;partial differential equations;neural network;pde;pdes;symmetric potential;nn approximation;symmetric background potential;wave functions;inhomogeneous media;random variables;inhomogeneous potential;uncertainty quantifications;random coefficients;networks;effective conductivity;convolutional neural network;symmetric spin;physical properties;wave function;simple convolutional neural network;effective conductance function;magnetic properties;physical quantities;periodic boundary conditions"}, "302face5b5a0944cab13665a2d4e07ef3aaf5240": {"ta_keywords": "open questions;question pairs;questions;strong baseline models;ambiguity;task;diverse types;original question;plausible answer;dataset;rewrite;domain question;data;set;half", "pdf_keywords": "domain question answering;annotated annotators;annotation process;natural language processing;ambiguous opendomain questions;ambiguous questions;multiple answer predictions;better knowledge extraction methods;natural language process;accurate knowledge extraction;ambiguous opendomain question;domain wikipedia;multiple answers;questions;structured data;multiple multiple answers;domain questions;opendomain questions;open domain question;ambiguity;unambiguous rewrites;partial labeling;new task;plausible answerswe;np extraction;new labeling;translation prediction;partial supervision;task;classification problems"}, "d86227948b6000e5d7ed63cf2054ad600b7994a0": {"ta_keywords": "natural language processing tasks;simple deep neural network;deep learning;simple network;inputs;essential features;model;variety", "pdf_keywords": ""}, "8c4b187bdaf91bf068adfe005a0463c4f9c36387": {"ta_keywords": "curvilinear structures;linear structures;higher order topological features;aerial images;art methods;delineation;data;wise loss;new method;wide range;approach", "pdf_keywords": "convolutional image network;powerful deep architectures;single deep network;deep learning;topological loss;image representation;biomedical images;random networks;networks;deep truth;wise losses;attention field;new loss term;linear structure;linear structures;automatic delineation;prediction accuracy;signal space;structured output;network;underlying structure;simple architecture;generative clustering;topological structure;delineation;thin structures;image;prediction;final prediction;architecture"}, "8147a495b9a933742f06458244f7c5df00767c4e": {"ta_keywords": "domain assertions;supervised open learning model;assertions;natural language sentences;sentences;aware learning;iterative rank;simple test;method;novel method;dataset;effectiveness;model;set", "pdf_keywords": "open information extraction;information extraction;iterative learning;natural language sentences;iterative learning approach;natural language;aware learning;supervised learning;iterative rank;binary classification;training samples;semantic representation;domain assertions;extraction;open ie2016 dataset;extractions;binary classification loss;large datasets;different sentences;reproducibility;iterationwe introduce;new method;available dataset;novel method;accuracy;ion sentences;open ie;information;wide web;novel approach"}, "43e8e371449aaef34c2f43ae90f2157fd5a617bd": {"ta_keywords": "selfish agents;convex equilibrium;social planner;optimal cost;pricing;feedback control;cost;game;fairness;riding;problem;methods;notion;impact;difference", "pdf_keywords": ""}, "830a396a4a77567caad1c155dd3b22597314e9f3": {"ta_keywords": "computational social choice framework;social choice framework;fairness;system outcomes;personalized recommendation application;aware systems;multiple perspectives;example;integration", "pdf_keywords": ""}, "5083d9e25113a09faeba7d56b7808e2f77b5c15e": {"ta_keywords": "large knowledge bases;templates;differentiable representations;neural model;several learning tasks;representations;order templates;performance;new method;idea;method;new operation", "pdf_keywords": "encode relations;relation matrices;knowledge bases;relations;relation;step reasoning operation;structured queries;knowledge base queries;second order reasoning operation;sequential generation;linguistic structure;models;entities;neural model;neural network;order reasoning;decoder model;natural languages;natural language;template instantiation;structure;sparse matrices;order templates;relationship;natural languagewe;hop encoder;entity;kbs entities;model;probabilistic memory"}, "4b762c0344f14bb00d590f5666c27b3aac7b0a7d": {"ta_keywords": "recurrent neural network language model;sentence completion;sentences;dependency dependencies;planck;model;test;art models;points;performance;state;approach", "pdf_keywords": "dependency neural language models;novel neural language model;syntactic dependency parse;simple neural language model;dependency rnn;dependency parse trees;neural word embeddings;syntactic dependency structure;recurrent neural network;sentence completion challenge;procssm drm sentence completion challenge;novel language model;dependency trees;training corpus;rnn;dimensional hidden word representations;sentence completion problem;output word probabilities;neural network;sentences;input words;natural words;learning process;languages;sentence;nonlinear models;deprnns;project gutenberg;novel algorithm;model"}, "bcd45c86e1bcf8d1411eb6704c4c58d0831b5b4f": {"ta_keywords": "multinomial distribution;statistical models;binomial distributions;text;classical models;models;random variables;description;behavior;approach;new class", "pdf_keywords": ""}, "0ad8284dbae11901a725cc71318a165c08852278": {"ta_keywords": "voice conversion;speaker model;joint density model;probabilistic formulation;few parallel utterances;nonparallel data;target;paper;novel approach;method;effectiveness;experiments", "pdf_keywords": ""}, "b21b927c251c415b601b6d7f785a42cc5c292635": {"ta_keywords": "scientific knowledge graph;scientific literature;task identification;tasks;knowledge;extraction;simultaneous identification;models;novel framework;ee;framework;construction", "pdf_keywords": "scientific information extraction;entity recognition;relation extraction;scientific knowledge graph;scientific knowledge graphs;single entity recognition;coreference clusters;scientific information extractor;scientific entities;semantic knowledge networks;knowledge extraction problem;coreference resolution;knowledge graph;crossreference information;large corpus;entity boundaries;annotation;semantic knowledge;scientific articles;knowledge discovery;entities;entity;multiple span representations;natural language;annotation constraintswe;expressive span representations;relations;task learning;computational linguistics;relation structure"}, "34fb3e21a63fb2987f7a87f88ecf49aea53cff36": {"ta_keywords": "photon process;photons;pairs;powerful mechanism;pair;mechanism;generation;fact", "pdf_keywords": ""}, "95cedaeb3178a4671703a05171a144e6b964a819": {"ta_keywords": "neural language model;hybrid language model;neural model;languages;mixture weights;count;vocabulary;neurons;hybrid model;words;dataset;novel formulation;accuracy;scalability;terms;method;experiments;advantages", "pdf_keywords": "stochastic language model;neural language models;nonlinear mixture models;language models;language modeling;gram models;firstneural language models;multidimensional mixture models;nonlinear mixtures;nonlinear mixture;gram lms;neural models;mixture weights;natural language processing;term memory;gram;neural nets;neural net nets;language;processing sentences;multiple distributions;neural networks;novel hybrid models;corpora;ngram distributions;vocabulary;probability distributions;distributions;deep neural network;hybrid models"}, "a13d8400813743adb22ba0bd0570c49af2675a39": {"ta_keywords": "continuous speech separation;level separation model;speech;variable nonlinear;baseline block;baseline model;path;online configurations;target;novel method;architecture;method;problem", "pdf_keywords": ""}, "54316d2861eb3d575a8c7d071f4cf7c2fc30be01": {"ta_keywords": "robustification;smoothing;classifier;accuracy;higher fidelity;inference time;image;novel approach;higher degree;approach;problem", "pdf_keywords": ""}, "73bbd0b53044e9f518a3596a3607521bbce12fc2": {"ta_keywords": "segmentation;gradient descent;novel local optimization method;gradient descent method;robustness;significant improvement;simplicity;different network architectures;art results;framework;method;state;standard approaches;ability", "pdf_keywords": ""}, "a5b1d1cab073cb746a990b37d42dc7b67763f881": {"ta_keywords": "dimensional natural language models;novel statistical model;joint training;nps;model;use", "pdf_keywords": "neural semantic parser;semantic parsers;semantic parser;semantic parsing;different semantic parsing paradigms;database description language;contextual representations;schema representations;language models;structured tables;large corpus;natural language;language model;description language;structured data;structured databases;utterances;table contents;joint representations;structured schema;tables;tableswe;unstructured queries;unstructured text;table schemas;unstructured data;database tables;underlying database;novel encoding;database"}, "7e870eb8d580fb1b8b7a8f97d94d67555a225635": {"ta_keywords": "intelligent message addressing problem;message addressing problems;recipients;recipient contact;potential recipients;email users;large corpus;formal models;completion;individuals;few initial letters;message;auto;composition;task;current contents;several techniques;problem;adaptations;quantitative evidence", "pdf_keywords": ""}, "267b94325028e0e2e6da1ae2cbe7f7a93284722e": {"ta_keywords": "walk similarity measures;extended similarity metrics;lazy graph walk;structural graph;graphs;email data;graph;content;documents;appropriate learning methods;detailed instantiation;further improvements;baseline methods;other objects;use;framework;schemes", "pdf_keywords": ""}, "a309ad4c4088843d230be1a85806960e633e1e46": {"ta_keywords": "data curation;models;data;performance;point;position paper", "pdf_keywords": "nlu machine learning;nonlinear linguistic theory;data universe;nlp;natural data;data curation;machine translation;task data;decoherence;human language;data;machine learning;linguistic theory;linguistic paradigms;neural networks;knowledge;models;lexical domains;lexical domain;corpora;large corpora;best data;data sets;privacy;nonlinear dynamics;extrinsic features;tasks;machine;task;neuron"}, "1be28ce9a1145c2cf4f78e6c494a4c15397fbac3": {"ta_keywords": "noisy biomedical knowledge graph;summarization graph;subgraph extraction module;adverse drug;drug interactions;neural network;channel knowledge;data integration module;attention;salient;detection;ddi;novel combination;new method;method;self;kg", "pdf_keywords": "drug interaction prediction;summarization graph;subgraph information;drug interactions;subgraph features;potential drug interactions;underlying knowledge graph;graph learning model;knowledge graph embeddings;drug interaction mechanism;deep representation;clinical prediction;drug interaction;deep learning;summarization scheme;knowledge summarization module;deep neural network;local subgraph approach;dangerous drug interaction;global knowledge graph;subgraph;external knowledge summarization;neural network;local subgraph;neural encoding scheme;novel graph;local subgraph encoding;novel drug pair;summarization;generalized neural network"}, "ce6143e24a455edc233f12933e9903426b963799": {"ta_keywords": "scalable implementation", "pdf_keywords": ""}, "51bf7a3aee6b1f61b902625f6badffedf200d31a": {"ta_keywords": "deep generative model;linear associative memory;deep network;associative memory;several interesting structural rules;layers;layer;specific rules;rule;formulation;algorithm;art;state;entry;new problem;problem", "pdf_keywords": "deep generative model;deep network;generative networks;deep networks;several visual editing effects;deep learning;deep convolutional learning;generative feature;generative models;convolutional layers;generative adversarial networks;linear associative memory;neural network representation;generative adversarial network;associative memory;generative model;deep learning approach;adaptive feature propagation;learned rule;supervised way;modeling;convolutional layer;neural networks;generative generator;annotations;more feature;nonlinear neural layer;feature space;pattern recognition;neural network"}, "990c7726dd31723f97a364828d5191080fe7ec2d": {"ta_keywords": "universal topological quantum computation;topological quantum circuit;chiral topological superconductor;correlated majorana edge modes;quantum circuit;majorana edge mode;quantum information;probabilistic gate;computation results;circuit;electric signal;matrix;method;use", "pdf_keywords": "fractional quantum hall;topological quantum computing;universal topological quantum computing;topological quantum computer;topological quantum gates;topological quantum hall effect;topological quantum computation;fractional quantum hall effect;topological quantum gate;quantum spin hall;universal quantum circuits;universal quantum circuit;quantum computation;topological superconductors;quantum circuit;chiral topological superconductor;topological superconductor;quantum circuit implementation;universal quantum gate;topological phase gates;quantum circuit model;dimensional topological superconductor;quantum gate;topological phase gate;conventional quantum circuit model;quantum network;modular quantum computation;quantum information processing;quantum;tolerant quantum circuit model"}, "6fae71765a5e86dfef2f93bbe03c4a2e20f827b5": {"ta_keywords": "english automatic speech recognition system;neural net;word error rate;lattice rescoring;system;ssp;evaluation;novel feature;combination;novel approach;derecoding;median;results;paper;points", "pdf_keywords": ""}, "b9e6c65aacfe8ecc1b7833b47803672273a918ec": {"ta_keywords": "orbit interaction strength;orbit interaction;zeeman field;spin;effect;significant change", "pdf_keywords": ""}, "cf08bef866885edb8b001deb18e582eec94c51de": {"ta_keywords": "spontaneous speech summarization;automatic speech summarization;talks;meaningful messages;domain talks;talk;acoustic features;new method;method;experimental results", "pdf_keywords": ""}, "b790c3e712c92065d596364af81a494adbc62c39": {"ta_keywords": "generative models;generative model;robust optimization;model selection heuristics;inner maximization objective;optimization;parameter search;large scale;realistic scenarios;gradient;relaxation;toy settings;dro;dro problem;framework;approach;efficacy", "pdf_keywords": ""}, "43d5c00938bd2acb1aca8e81a7d220025eddbc23": {"ta_keywords": "magnetic properties;magnetic field;spin;ground state;effect;interplay", "pdf_keywords": ""}, "6695d3b92e7cd7f2359f698a09c7b3dc37996329": {"ta_keywords": "pretraining;visual objects;reinforcement learning;gradient;rtg;novel label;model;ag;framework;lt;novel;purpose;method", "pdf_keywords": "multimodal pretraining model;label augmentation;bert representations;language modeling;pretraining model;visual objects;visual object;deep bidirectional transformers;recognition;stage pretraining;labels;visual image;language pairs;language;captions pairs;training;training data;vision;training approach;convolutional neural networks;label;downstream tasks;neural network;text;modal model;machine learning models;scene;tasks;auto;bert"}, "887d84c1310c6e71a0f89874ef9985b65a44c855": {"ta_keywords": "noisy speech recognition experiment;differenced maximum mutual information;feature transform;minimum phone error;superior recognition performance;dmmi;standard method;criterion;parameters;mpe;novel method;method;paper", "pdf_keywords": ""}, "46bf4bece58764d22764acfd3d232b50fb7767f9": {"ta_keywords": "mild cognitive impairment patients;mild cognitive impairment;deep net;structural neuroimaging data;deep convolutional neural networks;cnns;neuropsychiatric impairment;predictive regions;classification;3d;normal subject;characteristic curves;rocs;patients;application", "pdf_keywords": ""}, "6f173939f6defe3ebae8fb12f19349ba96b7b5c4": {"ta_keywords": "unsupervised clustering process;clustering process;end diarization;attractors;attractor;same recording;speakers;output speakers;end;number;large number", "pdf_keywords": "novel speaker diarization model;trainable diarization model;speaker embeddings;speaker diarization;conventional speaker diarization methods;speaker attractor;end speaker diarization problem;unsupervised clustering;circuit quantum electrodynamics;neural diarization method;independent discrete speaker separation;unsupervised clustering process;trainable representation;speaker extraction;clustering model;clustering;speech networks;speaker counting;vector clustering method;wise diarization loss;diarization;clustering method;wise embeddings;iterative encoder;diarization results;acoustic features;converted local attractors;encoders;end diarization;conventional clustering method"}, "1fb88c130bedcd2e75fd205b70af2999c6a8c49d": {"ta_keywords": "noisy environment;propagation;random path;neural network;network;single node;environment;interaction", "pdf_keywords": ""}, "6ad56b1b776a2c448fc90c543b50756941e5a119": {"ta_keywords": "utility interaction;incentive programs;energy providers;utility provider;utility;incentives;energy efficiency;graphical representation;consumer;random matrix;parameter model;parameter system;design;novel approach;approach;use", "pdf_keywords": ""}, "bb6317bbd2c4a81e94cf3d7eb1b73da246a022db": {"ta_keywords": "domain adaptation;neural language model;text sequences;natural language;nearest neighbor;nearest neighbors;text;rare patterns;new approach;large set;recent experiment;approach", "pdf_keywords": "neural language models;natural language models;language modeling;next word distribution;standard language models;language model;deep learning;corpora;nearest neighbors;deep learning environment;bert representation;recent words;training data;neural networks;accurate representations;knn;text collection;knnwe;knowledge;adversarial examples;pknn representation;word;pknn probability distribution;training examples;gaussian classifiers;representations;generalization;gram matrix;models;plentiful unlabeled data"}, "b31eb3428320342dfde042693ff2ca106dabed0d": {"ta_keywords": "abstractive summarization;sequence learning framework;contrastive learning;text generation;sequence;free evaluation problem;models;reference;large margin;powerful framework;smm;performance;art systems;gap", "pdf_keywords": "conditional text generation tasks;conditional text generation task;abstractive summarization;abstractive summarization frameworkabstractive summarization;text summarization;novel text generation model;text generation;sequence learning framework;summary quality;summaries;neural machine translation;contrastive learning;candidate summary;learning objective;nonlinear text analysis;sentence alignments;sentences;natural language processing;semantic similarity metrics;free evaluation problem;scoring model;evaluation metrics;sequence;contrastive loss;stage evaluation model;outcome;mle training;prediction;novel approach;quality estimation"}, "5b1516c87818084dc5d195cc274e1ee8923210d2": {"ta_keywords": "entity recognition;lexical items;languages;competitive ner performance;translation;word order differences;robust cross;attention;robustness;minimal resource requirements;respective domains;self;method;principles", "pdf_keywords": "bilingual word embeddings;multilingual word embeddings;unsupervised crosslingual nervation;unconstrained natural language extension;discrete word translations;project word embeddings;entity recognition performance;entity recognition;word representations;embeddings;linguistic labels;linguistic similaritieswe;lexical items;linguistic domains;simple translation method;target language;linguistic structures;linguistic structure;dictionaries;different languages;subword structure;resource languages;seed lexicon;unsupervised transfer;parallel tagging process;glove language library;languages;discrete dictionary;translation problem;dictionary"}, "1ce0664989e0b28ceea223cab68f885ed18c39c4": {"ta_keywords": "speaker cluster;speaker;discrete time scale;entire cluster;discrete time process;modelling;dynamics;model;theoretic approach;information;terms;novel approach;use", "pdf_keywords": ""}, "2154bdb9ce841eb98b9fd13bf7bf0a42f11f89a6": {"ta_keywords": "optimization protocol;averaging protocol;neural networks;nodes;algorithm;protocol;gossip;global average;accurate evaluation;practical applications;number;several respects", "pdf_keywords": "cloud training;networks;efficient communication;preemptible cloud machines;scalable computing;stochastic gradient compression;training protocol;stochastic gradient descent;imagenet training;parallel computing;cloud;network;data parallelism;asynchronous parallel computing;reduce nodes;other algorithms;training instances;sustained computing;training batch;optimal convergence rate;convex optimization;network topology;deep learning;single cloud server;gradient descent step;unreliable communication;nodes;deep learning systems;algorithms;clusters"}, "d9212b207e49a3aa6806fb2ddadb303b7b1d47a8": {"ta_keywords": "natural language;command systems;reactive baselines;alfred datasets;hierarchical structure;procedures;formalism;programs;framework;large margin", "pdf_keywords": "natural language commands;holistic procedural programs;hierarchical procedural knowledge;hierarchical modular network formalism;programming languages;executable procedures;simple procedural action library;hierarchical modular networks;program execution;procedural knowledge;procedure libraries;procedural library;procedural function representation;executable programs;automation;host programming language;program;procedure knowledge;arbitrary tasks;procedure library;agent command;programs;natural language input;natural language navigation instructions;annotated functions;level descriptions;natural language;level planning;agent control;more robust procedures"}, "8fcd012e8ed2ea8190163369c9f222178e70a19d": {"ta_keywords": "automatic speech recognition;learning architectures;special case;end;paper;novel approach;zhiguang tan;sss;approach;hybrid;ar", "pdf_keywords": ""}, "49418122bba375fa02907d38b0be80689f750b39": {"ta_keywords": "designing codes;image classifiers;neural network architecture;approximate reconstructions;unavailable computation results;unavailable predictions;codes;training methodology;functions;network;extensive experimental results;theoretic approach;approach;effectiveness;use", "pdf_keywords": "erasure codes;function encoding;encoding function architecture;designing codes;encoder;accurate coding;encoding function;code;low cost encoder;error correcting codes;neural networks;neural network architectures;backpropagation;encoding;computations;codes canwe;neural network;universal function approximation;training loss function;computation;functions;approximate unavailable outputswe;general computation;convolutional neural networkwe;unavailable computation results;linear functions;novel learning;arbitrary data;function;dimensional representations"}, "49f657d704a1b80ce3dba0d8a9e5479ec1d703d4": {"ta_keywords": "speech recognition;kolmogorov index benchmark show;kolmogorov index;state;paper;right;framework;preliminary results;strategies;method", "pdf_keywords": "speech recognition;deep transformer;automatic speech recognition;deep transformers;end speech recognition system;input speech;transformer network;nonautoregressive transformer structures;transformer;right decoding;more input speech;novel nonautoregressive training framework;language modeling;recognition;utterance;autoregressive model;entire speech;conditional language model;asr;natural language;predict;previous prediction;output sequence length;network;output;model;less tokens;sequence;large sequence;training"}, "a1321f4527559836509c27008329afaf11f8ea89": {"ta_keywords": "interleaved problem order;effective learning;problem order;simulation study;error detection;different problems;agent;effectiveness;results;better opportunities;domains;correction", "pdf_keywords": ""}, "da46a0b5ddf0f4bf4caad9d29d6b4a93dd2eb2d2": {"ta_keywords": "agents;agent;games;game;player;system;new approach;problem", "pdf_keywords": ""}, "8ff54aa8045b1e30c348cf2ca42259c946cd7a9e": {"ta_keywords": "novel dynamic neural semantic parsing framework;sequential context;reward;sequential question;complex questions;search;challenging question;principle demonstration;model;state;art systems;proof", "pdf_keywords": ""}, "aaaff6b99684cb5b5e0a68e214bd8bbd4bf2e231": {"ta_keywords": "entity recognition systems;syntactic parsing;data;hidden model;systems;intrinsic structure;predictions;unknown values;quality;comparative study", "pdf_keywords": ""}, "956e096b1e8422c91989938b9508272b956d3070": {"ta_keywords": "reranking approach;random graph;gradient descent algorithm;personal information management;paths;graph;transition probabilities;multiple corpora;approaches;features;tasks;methods;performance;domain;global properties;comparison;set", "pdf_keywords": ""}, "a4a8e91995ae8c8b203dd857bdc0915facddeebe": {"ta_keywords": "noisy data;noisy workers;labels;worker quality;minimization;data;algorithm;ground truth;new algorithm;quality;loss function;experiments;rounds;effectiveness;current estimate", "pdf_keywords": "binary classification;several multiclass classification datasets;generalization error classification method;supervised learning model;noisy annotations;classification problems;multiple noisy labels;object classification;ground truth labels;binary classification problem;supervised learning algorithm;sparse labeling system;noisy labels;optimal crowdsourcing task;individual labels;labels;machine learning model;crowdsourcing;classes;generalization hammer;annotations;minimax conditional entropy;learned predictor function;recognition;imagenet;noisy crowd workers;hypothesis class;less examples;scale crowdsourcingwe;more examples"}, "ca3535dcdda9849350ad7c991a60660b22844f2f": {"ta_keywords": "speech translation model;searchable hidden representations;sequence model;unsupervised learning;intermediate stages;intermediates;compositionality;systems;sub;benefits;framework", "pdf_keywords": "end speech translation encoder;sequence learning framework;standard speech translation encoder;decomposable speech recognition tasks;decomposable sequence tasks;decoder training;end speech translation;speech recognition tasks;next word prediction;decomposable utterance;speech translation;decomposable sequence task;decoder model;speech recognition;joint statistical machine translation;encoder decoder;performance translation system;human speech;searchable hidden intermediates;encoder;dimensional translation task;sequence task;multimodal utterances;translation system;utterances;decoder decoder;linguistic representation;advanced encoder;stage speech recognition problem;regressive encoder"}, "d56244c6abf3141900386d6911dd9097697a346b": {"ta_keywords": "simple text classifier;page classifier;text traffic;classifier;new web site;pages;single web site;wrappers;accuracy;bag;classic bag;documents;world;paper;novel;variant;half;system", "pdf_keywords": ""}, "42605c1ee030721cb38a3c225992d63297a6ace0": {"ta_keywords": "practical language revitalization technologies;language documentation;natural language processing;language community members;different linguistic;revitalization;conservation;workshop;paper;field;context;future;results;goal;contributions;strategy;coherent vision;aim", "pdf_keywords": "allomorphous speech recognition system;practical language revitalization technologies;speech synthesis;phonetic representation;phonetic models;speech recognition system;documentary linguists;automatic language processing tools;linguistic preservationwe;human transcription efforts;exemplary languages;endangered languages;online lexicon;interactive online dictionary;complex lexical documents;languages;novel chatbot;language documentation;language;present allosaurus;phones;dictionary materials;corpus search engine;orthography;prototypes;allosaurus;morphology;texts;language community members;different languages"}, "c8d0e13de2eaa09a928eff36b99d63f494c2f5ec": {"ta_keywords": "natural language descriptions;novel neural architecture;grammar model;code;target syntax;architecture;complex programs;generation;parallel;model;set;natural way", "pdf_keywords": "deep syntax;deep semantic parser;generative semantic parsers;nonlinear generative semantic parser;semantic parsing;abstract syntax trees;neural semantic model;code generation predictions;underlying syntax;language generation task;syntax;grammar model;code generation;target programming language;code generation examples;natural language descriptions;neural encoder;purpose programming language;target syntax;linguistic structure;arbitrary program structures;full parse tree levels;novel neural architecture;neural architecture;language;grammar rules;arbitrary generation rules;deep learning;semantic data;encoded knowledge"}, "9c03d14520c897ca8536e165507f568d1980dabd": {"ta_keywords": "lexical matching;strong lexical matching method;coreference resolution;document matching problem;text;question types;multiple context windows;novel method;process;approach;simple example;method;current state;state;art;account", "pdf_keywords": ""}, "2ea226a7fadde6a45f537c714e0832e83136f861": {"ta_keywords": "biomedical event extraction;structured prediction;new biomedical signals;joint inference;search;novel approach;accuracy;training strategy;combination;novel framework;decision;approach;estimates;methods;terms;use;cost;problem;power", "pdf_keywords": ""}, "708f8c0eb5032edd6f31663a27febbb0529cbcf3": {"ta_keywords": "neural neural neuraltax;syntactic representations;linguistic structure;text;explicit supervision;structure;patterns;several languages;svn;structures;predictions;method", "pdf_keywords": "neural syntax learner;syntactic representations;textual representations;explicit syntactic supervision;word embeddings;embeddings;constituency parse trees;natural language;visual representations;unsupervised parsing model;neural learner;image embeddings;constituency parse tree;parse trees;parsing;language modeling;novel syntactic distance prediction framework;language learning;text parsing;image captions;caption data;syntax;syntactic distances;captions;constituency trees;neural learning model;syntactic distance;multimodal data;explicit supervision;representations"}, "06e36261b21af2943e464a562c92c09dac292a82": {"ta_keywords": "reversecompiler analysis;free software tool;execution;program;tool;powerful tool;content", "pdf_keywords": "reverse engineering binaries;new reverse engineering tool;binary code;reverse engineering;new decompiler;decompiler output;important decompilation benchmark;decompiler;variable recompiler;many decompilation scenarios;reverse engineering context;code structure;binary programs;decompilation;decompiler traversalwe;new type prediction decoder;reconstructed abstract syntax tree;decoder;decompilers;precessing decoder;task decompilation;uncompiled code;code tokens;encoder;original code;precessing encoder;binaries;compilation;variable names;code"}, "54e7de06a97b4b6c41e185c0bee60c838a15265a": {"ta_keywords": "articulatory speech modification;unobserved articulatory parameters;speech waveforms;virtual articulatory parameters;articulatory parameters;statistical inversion;inversion mapping;speech;production mapping;generation;experimental results;framework;coalescence;report", "pdf_keywords": ""}, "7771aa7badc3375a31bfac8dc47755ff5d5c7780": {"ta_keywords": "subword level distributions;morphological complexity;subword levels;bpe merges;bpe;incremental process;terms;word;probability distributions;different types;specific levels;interaction;relationship", "pdf_keywords": "subword tokenization;suitable subword representations;subword regularization;subword entropies;original subword regularization;neural language model;internal word structure;subword types;word complexity;text entropy values;subword level;subwords;orthographic word types;linguistic complexity;specific subword levels;linguistic typology;computational linguistics;word level;corpus;word length;subword units;multiple linguistic typology;morphological complexity;subword;similar encoding efficiency;orthographic words;parallel corpus;word order;universal language discovery algorithm;entropy"}, "79c93274429d6355959f1e4374c2147bb81ea649": {"ta_keywords": "encoders;elementary objects;learning;modalities;framework;relationships;scale;goal", "pdf_keywords": "challenging visual reasoning dataset;challenging visual reasoning task;crossmodality representationswe;crossmodality representations;visual reasoning;attention graph;crossmodality encoder;novel crossmodality encoder;language understanding tasks;bert encoder;attention;natural language comprehension;object relationship encoder;joint attention;language encoder;relationship encoder;natural language;language task;encoders;natural language processing;language model;neural machine translation system;transformer encoder;vision;machine learning context;learning model;visual question;representation;neural networks;task"}, "03e4f33c0ccc4cb8c7e1589158a5377cdf5241d2": {"ta_keywords": "qualitative preferences;subjective preferences;corresponding human community;corresponding ethical principles;human agent;preferences;decision process;outcomes;agent;computational model;community;model;relations;chapter", "pdf_keywords": ""}, "d5f22dbc8f4b9e99f62e6ecf886bc4b9a0372e4d": {"ta_keywords": "large classes;web;algorithm;exploration;novel", "pdf_keywords": ""}, "680e61a17e27a1e8e121276c7ec53fc4fd40babb": {"ta_keywords": "uniform information density;acceptability data;uid regression;strongest predictor;hypothesis;document;mean surprisal regression;uid;sentence;mean surprisal;phrase;time;context", "pdf_keywords": "linguistic signal surprisal;linguistic signal;linguistic signals;linguistic process;language comprehension;linguistic unit;linguistic acceptability judgments;linguistic acceptability;language processing;linguistic analysis;utterances;lexical representations;language users;psycholinguistic system;uniform information density;word length;language production;language;human sentences;interlingual information exchange;utterance;language user;time information density;sentence variance;efficient probabilistic parser;corpora;words;uid hypothesis;cloze language model;spoken domain"}, "34f8214cbaa0655794c2c9570898abf15649b079": {"ta_keywords": "reverberant speech;reverberation time;adaptive training;dereverberation;relative error rate;recognition;dynamic mismatches;expectation maximization algorithm;model parameters;method;paper;novel method;experiment", "pdf_keywords": ""}, "3aba582b62d1abfcd95264e6c7b32aab4c9db4b8": {"ta_keywords": "explainability model;text classifier;simple algebraic continuation;model;predictions;novel self", "pdf_keywords": "neural text classifier;neural text classification approach;natural language explanations;explainable natural language explanations;natural language representations;text classification tasks;explainable ai;deep learning;global explanations;interpretable models;human annotators;aware encoder;interpretable layer;intrinsic description;natural language processing;natural language processing pipeline;model predictions;deep learning problem;explanations;concepts;learning;descriptions;text;ai;novel concept;level concepts;linguistic semantics;description;model self;local interpretability"}, "e5a5888966be6b5f9c0e8a82facd604086a1ee4c": {"ta_keywords": "term detector;tagging;gene names;syntactic parser;bionerner system;chimera detector;genes;reference reference reference reference reference;novel representation;reference reference reference;features;novel method;output;user feedback;fly project;systems;nonlinear force;mechanisms;identification;articles;new method;better understanding;first stepswe;context;accuracy;method;advantagewe;novel;performance;main systems", "pdf_keywords": ""}, "73271677da83a3f55523148d1b43a0501f0a35dd": {"ta_keywords": "online learning;invariant equilibrium;sum games;robustness;exogenous variations;game context;online;poincar\u00e9;behavior;variations;convergence;time", "pdf_keywords": "periodic zerosum polymatrix games;periodic game;online learning dynamics;learning dynamics;time learning dynamics;invariant nash equilibrium;periodic payoffs;generalized differential games;sum polymatrix games;sum bilinear games;sum bimatrix games;game formulation;time quantum learning dynamics converges;time quantum learning dynamics;sum polymatrix game;sum bilinear game;polymatrix game;sum games;sum bimatrix game;common interior nash equilibrium;sum game;evolutionary game;time gradient descent;seasonality;invariant equilibrium;linear dynamics;game converges;time behavior;evolutionary dynamics;quantum game"}, "74fb2834c820d2297b08201cb72de1c1d3d27f54": {"ta_keywords": "accent masking;privacy;side privacy;speech processing;cloud;gender;users;models;client;data;challenges;tasks;process;discussion;new class;future research;opportunities;principles;series;potential", "pdf_keywords": "differential privacy;side privacy guarantees;side privacy approaches;privacy;side privacy;uploaded speech data;voice conversion system;downstream automatic speech recognition systems;speech data;adversarial training;speaker identity;sensitive information;voice conversion literature;optimal voice conversion system;speaker information;false information sharing;speech recognition;gender masking;accent information;gender information;utterance;single utterance;encoding;optimal normalization;large encrypted clouds;noisy noisy datasets;deep learning;speaker;speech;cloud"}, "030d7d7ae48a9f81700b2c1f7cf835235777b8e7": {"ta_keywords": "retriever search;scalable model;large volumes;data;model;candidates", "pdf_keywords": "recent neural retrieval model;recent retrieval model;unsupervised retrieval;retrieval;accurate passage search;reader supervision;retrieval results;natural language processing;retrieval quality;text corpora;retriever generalization;large reader;passage encoder;loop retrieval model;retrieval datasets;relevance;reader;natural language;retriever;retriever string;text;retrievers;document encoder;retriever modeling;passages;extractive openqa systems;new extractive openqa system;simple supervision procedure;type reader;infrared supervised search task"}, "7c655ef6f0de8c1a219cdb796c77f4ae3c389b82": {"ta_keywords": "dna landscape;dna field;species;discrimination;interaction;method;model;new method;assumption;self", "pdf_keywords": ""}, "b2b0fbf9033f1c36bea8bb11c173f14378c60db9": {"ta_keywords": "translation system;speech;s2s;system;combination", "pdf_keywords": ""}, "262c0e54370dfc03a7ad53d79930568d18dd448c": {"ta_keywords": "machine learning algorithm;italic;algorithms;math notation;notation;inline;machine;data;formula;framework;case", "pdf_keywords": "parallel machine learning algorithm;other parallel matrix multiplication algorithms;novel parallel matrix multiplication algorithm;uncoded matrix multiplication;coded shuffling;coded shuffling problem;uncoded shuffling;shuffling algorithm;linear machine learning problems;uncoded index coding;coding;uncoded gradient descent algorithm;network nodes;network network nodes;efficient algorithms;algorithms;machine learning algorithms;computing cluster;optimal data storage;cluster;sparse data recovery;networks;computation systems;matrix multiplication;machine learning algorithm;nodes;algorithmwe;computing;erasure codes;storage codes"}, "14a058a1e41459a30327bb5fb480d51430b6a096": {"ta_keywords": "gene discovery;gene associations;novel method;novel approach;method;ner;use", "pdf_keywords": ""}, "a6d505a6e46c15ef0d213b9a4349ce2f852be894": {"ta_keywords": "partial partial mixture learning;mixture estimation;partial mixture;mixture proportion estimator;mixtures;supervised learning setting;classifier;algorithms;novel algorithms;examples;dimensional settings;number;performance;efficacy", "pdf_keywords": "mixture classifier;optimal classifier;negative classifier;convex loss function classifier;classifier;classification;supervised learning;classification space;suitable classifier;convolutional classifier learning;linear classifiers;partial learning;pvn classifiers;network classifiers;novel classifier;type learning algorithm;deep learning context;pvu classifier;probabilistic network classifier;learning objective;efficient estimation;dimensionality reduction;type classifier;mixture;true pvn classifier;unlabeled distribution;unlabeled data;partial convolutional neural networks;learning;labeling"}, "b08545e1281c1eb748e4474687eb61fd3b25d1a6": {"ta_keywords": "novel language discovery platform;novel words;novel dataset;sci;natl;proc;acad;course;years;practitioners;first time", "pdf_keywords": "novel word innovation corpus;new york times word innovation type dataset;linguistic novelty class annotations;novel word nodes;lexical contribution;novel linguistic structures;lexical normalization;linguistic structures;novel words;annotated text collections;novelty;computational linguistics;contextual prediction;unstructured text;novel word;lexical derivation;contextuality;word families;word pieces;novelty class;dialectal variation;semantics;different linguistic domains;word forms;uncontextualized models;common word forms;sentences;word;contextual model;novel dataset"}, "43fae0a7af211d91557d115d2f82e3c46d8bf022": {"ta_keywords": "natural language generation;nlg;interpretable metrics;output text;creation;metrics;intrinsic alignment;compression;input;human;key aspects;novel approach;novel framework;intuitions;transduction;comparable correlations;experiments;family", "pdf_keywords": "natural language generation;language generation tasks;general language generation benchmark;summarization tasks;story generation;summarization experiments;text alignment models;text paraphrasing;text summarization;summarization;summarization process;nlg;information alignment;interpretable summary;linear programming tasks;image captioning;creation tasks;word level alignment;novel alignment estimation model;human annotations;automatic metrics;dialog;accurate alignment estimation model;computational linguistics;text content;interpretable metrics;sentences;text pieces;generation;individual evaluation metrics"}, "b0894f5c914cd90cc3b3e16b15bec11efe317b14": {"ta_keywords": "peer assessments;strategic behaviour;assessments;principled test;test;strong detection power;patterns;data;series;practical setting;collection;presence", "pdf_keywords": "peer evaluation;peer review;academic peer review;blind peer review;peer grading;independent reviewers;crowdsourced review manipulators;peer submissions;reviewers;scientific evaluations;evaluation process;peers;statistical evaluation;assessment;evaluation;reviewer;peer;output rankings;strategic behavior;strategic manipulations;ranking;rankings;unbiased comparisons;review;strategic behaviour;test;novel test;standard ranking;participants;student peer"}, "f481d6dea08e348cecd5eb23a813d47373e62a94": {"ta_keywords": "natural language specifications;natural language;natural language elements;automatic explanation;programming;machine learning models;source code;programs;code;automatic generation;language;tutorial;similarities;communication;methods;analysis;tasks;differences;field;variety", "pdf_keywords": ""}, "c7424d651d60ef9f052e91bff18efd88782225a3": {"ta_keywords": "election;computational complexity;ties;result;np", "pdf_keywords": "underlying voting rules;edge election;single election;partial tele voting;election;stage voting rule;candidates;electionwe;voting;candidate;votes;partial winners;best candidates;ties;tie;complexity;vote;computational complexity;additional vote;breaking rule;unique winner;following decision problem;algorithms;contest;selection rules;second round;symmetric;theorems;version ofwe;first round"}, "19a6e362840d3a2d27d0fa5509eaa4d4597a2859": {"ta_keywords": "universal scaling law;scaling law;particles;mass;fluid;momentum;energy;simple model;calculation", "pdf_keywords": ""}, "b145a46718f293429054f0a9a4cdd2de94813b37": {"ta_keywords": "hyperlinked pages;wide web search;search algorithms;search;available links;web;algorithm;link;gaussian;results;method;new method;use;world;set", "pdf_keywords": ""}, "5ea3c08614e9673a109f581cf114af488f3aa601": {"ta_keywords": "automatic embryo staging;residual networks;downstream classifier;embryo;region proposal network;developmental stages;lapse data;decoder;predictions;smaller data sets;background;structure;likely monotonic sequence;programming;several sources;time;methods;best numbers;new method;vanilla", "pdf_keywords": "embryo detection;embryo stages;embryo staging;embryo;embryos;embryo initiation;embryo incubator;embryo development;large embryo incubator;videos;embryogenesis;reinforcement learning;deep learning;lapse videos;joint image classification task;morphokinetic embryo selection process;rnns;novel image annotations;region proposal network;developmental stage;deep learning dataset;morphokinetic embryo selection;developmental stages;explicit annotated boxes;stage;deep learning methods;video frames;weak supervision;video;downstream classifier"}, "6ab36d2577f7c9487b28b2bcdf236191ba901aad": {"ta_keywords": "dialog agent;dialogs;end learning;task;neural model;interaction;end;relevant interaction details;dataset;tod;novel problem;user;problem;way", "pdf_keywords": "dialogs;dialog system;dialog;such dialogs;dialogue systems;dialogue modeling;dialog paraphrasing process;dialogue;dialog outline;several typical dialog problems;conversational agents;end learning;conversation;natural language applications;dialogue thatwe;range dialogue rules;neural tod;user utterance;flowcharts;flowchart;large scale tasks;task prediction;neural task;flowchartswe;specific flowcharts;agent response;corpus;novel task;neural networks;tod dataset"}, "1b114486d67252ff83fc90d4a8607636045c54ce": {"ta_keywords": "data;high quality;method", "pdf_keywords": ""}, "518cb6d4247bdebf21e2811f296b0c7372602a0a": {"ta_keywords": "principled masking strategy;multilayer masking technique;random uniform masking;mask;mutual information;training time;prior approaches;tokens;strategy;drawback;performance;pmi;concept;half;version", "pdf_keywords": "random token masking;multiple subword tokens;large corpus;novel linguistic models;large corpora;random masking algorithms;random random masking;dimensional linguistic models;statistical mask;word spans;gram;vocabulary entries;natural language;collocations;linguistic structure;unmasked text;text recognition;masking;text;collocation;natural language understanding;vocabulary;words;input tokens;high collocation;mutual information;right vocabulary size;machine learning;bivariate collocations;effective representations"}, "aa9e0bf1e22563fca053578315b857688a0817cb": {"ta_keywords": "dialogue systems;user simulator;novel user simulator;simulator;agent;several agents;booking;interactions;task;movie;performance;procedure;paper", "pdf_keywords": "dialogue system;dialogue systems;dialogue state tracking;unstructured dialogue systems;spoken dialogue systems;machine dialogues;dialogue activity;dialogue agent;dialogue policies;dialogue state;dialogue actions;dialogue strategies;dialogue;dialog act level;short term memory;completion dialogue;natural language understanding;user simulator;recurrent neural network model;movie simulator;natural language generation;user simulation approach;user simulator class;completion dialogue setting;deep reinforcement learning;command line agent training;movie ticket;thedialogue systems;book movie tickets;agent training model"}, "6b4ca249b3b28d3fee65f69714440c08d42cee64": {"ta_keywords": "generative generative image models;more general generative image models;regularization strategies;simplified gradient penalties;distributions;local convergence;prototypical counterexample;generator;data distribution;convergent;realistic case;results;analysis", "pdf_keywords": "gan training converges;unregularized gan training;gan training dynamics;gan optimization;gan training;generative adversarial networks;gans;gan architecture;gan;generative adversarialwe;generative adversarial network;generative networks;gradient penalties converges;generative network;generative models;training converges;unregularized gradient;regularizer converges;regularization strategies;consistent learning models;new regularization strategy;regularization techniques;gaussian gradient descenttheorems;gradient descents;gradient penalty;local convergence;gradient penalties;regularization algorithm;gradient descent;wgan"}, "993c184553c41ca9134f149a3eb71b5bfab298b5": {"ta_keywords": "broader digital campaign;information operations;twitter platform;accounts;distinct strategic units;groups;distinct narrative;network maneuvers;mapping state;operation;novel computational framework;mvmc;methodology;complementary roles;analytical pipeline;novel method", "pdf_keywords": ""}, "6916118de98cb5293425c8f74919395a003e6076": {"ta_keywords": "text categorization;several ilp methods;propositional counterparts;new ilp system;order representation;flipping rule;methods;eeectiveness;study;performance", "pdf_keywords": ""}, "affdfafb0293b44412ec99ff39b114de5e83eb98": {"ta_keywords": "quenched approximation;variational method;numerical study;dynamics;strong attractive field;model;predictions;presence;results", "pdf_keywords": ""}, "88347f9f12b50590f50aefce4cf71b3a3f0bd138": {"ta_keywords": "language grounding;3d game engine;novel reinforcement;3d environments;imitation;language;unseen instructions;task;unseen maps;novel environment;environment states;challenges;instructions;model;novel approach;effectiveness;approach;rich set", "pdf_keywords": "visual language grounding;imitation learning;language grounding;imitation;perceptual knowledge;trainable neural architecture;optimal action;natural language instruction;reinforcement learning module;policy learning module;actions;reinforcement learning;optimal action givenwe;3d environment;3d environments;multimodal fusion unit;reinforcement;reinforcement learning model;robot;action;learning process;continuous agent positions;visual elements;reinforcement learning framework;language;image representation;specific tasks;meaningful representations;novel multimodal fusion mechanism;tasks"}, "5e10a61b34867c6e5b32ed7a1359bd47bbfb5e2d": {"ta_keywords": "multiple inconsistent explanation problem;calledabductive explanation;training example;learning;knowledge;explanation;negative examples;formalization;level explanation;ebl;techniques;possible explanations;certain type;set;paper;convergence properties;extension;tt", "pdf_keywords": ""}, "e3862b1ff18dbb6a421b9efd1c0db22e09644b6d": {"ta_keywords": "dimensional harmonic trap;trap;single particle;particle;dynamics;region;system;study;key;results", "pdf_keywords": ""}, "6f69fcacdf53a811ef18c5e9ac8ec58035dc43fc": {"ta_keywords": "connectionist temporal classification;recurrent neural network;rnn;standard rnn;different transducer losses;training lattices;transducer;novel transducer;graph representation;labels;loss;ctc;different transition rules;alignments;efficient framework;better results", "pdf_keywords": "recurrent neural network transducer;connectionist temporal classification;stateless prediction networks;training lattices;generatedneural networks;neural network;different transducer losses;learning trees;rnn;neural networks;ofneural networks;networks;transducer;label sequences;neural network model;graph representation;parametric neural networks;new transducer;label state;training data;speech recognition problem;novel graph;labels;nodes;generative model;convolutional neural networks;end speech signal;convolutional neural network;convolutional nn;label states"}, "31dc1e65d61a431964c75bf2eec167bcd9dca0fa": {"ta_keywords": "magnetic field;particle;suitable force;field;cavity;motion;effect", "pdf_keywords": ""}, "86471bf927401bf88af83626797228c2bf10a282": {"ta_keywords": "social attribution;social science;social behavior;causal model;human behavior;research;process;model;essential features;version;powerful tool;rich area", "pdf_keywords": "social attribution system;social attribution;causal attribution;predict models;artificial explanations;neural predictions;next prediction;prediction;faithful model;faithful interpretation;predictions;human explanation sciences;artificial models;meaningful evidence explanation;machine learning;behavior;faithfulness;single prediction;appropriate causalwe;explanations;natural conclusion;explanationer;interpretation;artificial intelligence;intent;new causal chain;causal chain;simple model;explainability;model"}, "aa30949af5b59624224980e7d741ad8c084271ec": {"ta_keywords": "majority ofvaccination messages;discussedvaccinationvaccination messages;structure ofvaccination messages;current coronavirus disease;conspiracy theories;unique tweets;days;videos;analysis;amplitude;term", "pdf_keywords": ""}, "d35534f3f59631951011539da2fe83f2844ca245": {"ta_keywords": "generative adversarial networks;generative adversarial network;generative networks;distinct photographs;latent codes;shelf face verification system;photograph;identity;identities;observations;algorithm;pairs;new approach;alternative approach;ability", "pdf_keywords": "conditional generative adversarial networks;generative adversarial networks;conditional gans;generative adversarial network;generative view;generative models;conditional generative learning;synthesisconditional generative adversarial networks;gans;generative model;face photographs;facial image pairs;celebrity face images;discriminative generation;generative process;gan;weface image synthesis;facial features;face image matching problem;recognition;individual image;images;encoder;face;multiple photographs;gradient generator;discriminative power;pairwise training scheme;learning;dataset"}, "61a07d1e4eaa831152e253b96b91808ef3a184b4": {"ta_keywords": "natural language data annotation;largest crowdsourcing marketplaces;data collection projects;labeling problem;data;crowd;marketplaces;challenges;use;tutorial;world experiment;practical solution;new class", "pdf_keywords": ""}, "aead4418733b998792deb9cbf198a834449e00d2": {"ta_keywords": "symbolic distribution;phylogenetic trees;symbolic models;symbolic mathematical integration;symbolic problems;distribution;test;utility;variety;approach;problem;methodology;performance;domain", "pdf_keywords": "systematic generalization;neural sequence integrator;generalization;sequence models;distribution generalization;neural sequence;genetic algorithms;primitive robustness problems;symbolic mathematical integration;adversarial problem sets;empirical compositionality;error propagation;symbolic integration;simple compositional model;models;general error propagation method;essential patterns;prediction;model deficiencies;iterations;robustness;patterns;exotic error clusters;genetic algorithm;distribution deficiencies;training distribution;problem types;heuristic computing;simple primitives;scalable method"}, "9f7e317c6ef0bb15aacc9b19f0f0d00fee6c9a36": {"ta_keywords": "memorization;memorization problem;dimensional data;algorithms;large computational overhead;algorithm;training example;fit data;computer science;new approach;theory;key problem;good results;propensity", "pdf_keywords": "memorization estimation;new memorization estimator;memorized examples;optimal memorization;memorization;memorization examples;memorization representation;memorization model;memorization memorization data;deep learning algorithms;deep learning;memorization task;convolutional learning rate;examplememorization;memorization value;natural image andmemorization;modern memorization research;memorization process;memorization problem;recognition;deep learning model;empirical evidence;correct memorization value;deep learning algorithm;prominent inmemorization;empirical data;training data;training task;influence training examples;examples"}, "a1a8eeb64c0846070b10531061c18fed6d566f8c": {"ta_keywords": "computational complexity;complexity;voting game;popular tie;manipulation;unique winner;vote;cowinners;opportunity;np", "pdf_keywords": ""}, "c4919feb50c514e32eb0f4131399180c6f9a0d7d": {"ta_keywords": "procurement cost function;polynomial procurement;online resource allocation;procurement cost;total allocation;multiple customers;optimization framework;dual algorithm;competitive ratio;surrogate function;incoming customer;seller;first design method;order;resources;problem;setting", "pdf_keywords": "optimal procurement cost function;optimal primal dual algorithm;generic procurement cost function;procurement cost functions;optimal primal;optimal cost;revenue maximization;dual algorithms;dual algorithm;online resource allocation problem;cost function;online resource allocation;procurement cost;cumulative procurement cost;convex optimization problems;greedy algorithm;dynamic resource allocation;pricing mechanism;optimization problems;natural optimization problems;squared cost functions;resource allocation;convex optimization problem;resource procurement;squared cost function;optimal value;total allocation;quadratic programming;convex objective;dual analysis"}, "df56ccda14b5bc255a07fc061c50839e75563c5a": {"ta_keywords": "parking;traffic;other traffic;account parking;seattle downtown area;game model;utility;practical examples;particular objective;subsections;model", "pdf_keywords": ""}, "4218563e1fe927440e00bf0abe5cb1e037deaf71": {"ta_keywords": "average thresholded confidence;model confidence;accuracy;threshold;world machine;deployments;unlabeled examples;toy models;novel method;atc;efficacy;method;art methods;set;fraction;state", "pdf_keywords": "target domain accuracy;threshold classifier;accurate prediction;accurate predictions;classifier accuracy;label shift estimation;accuracy estimation;classifier;target accuracy;unlabeled target data;natural shift benchmarks;prediction;average thresholded confidence;quantitative classification framework;arbitrary classifier;conditional classification;binary classification;accuracy;simple probabilistic labeling problem;high accuracy;unlabeled target;sigmoid classifier;unlabeled target points;validation source data;classifier classifier classifier;threshold;unlabeled data;model confidence;predictions;optimal predictor"}, "2d6d26c118f43f3ab314d07f58c20df6e89a13af": {"ta_keywords": "single dna;dna;large number;method", "pdf_keywords": ""}, "92a8f7f09f3705cb5a6009a42220a6f01ea084e8": {"ta_keywords": "actionable knowledge;large language models;level tasks;natural language;model;fact;llms;results;recent virtualhome environment;method;possibility;quality", "pdf_keywords": "large language models;semanticallylarge language models;language models;simple language models;language generation;conventional language generation tools;language model;scale language learning;language learning;actionable knowledge;natural language systems;high level instructions;simple language;language structure;actionable programs;many tasks;simple tasks;imitation tasks;natural language;level tasks;knowledge bases;task completion;language;implicit knowledge base;language languages;action translation;tasks collectedwe;human annotators;tasks;models"}, "59653e5cfa854a17c2ffcb86f2a454f27e12c716": {"ta_keywords": "real translations;pronouns;translation;bias;gender;salient source;differences;purpose;distributions;factor", "pdf_keywords": "machine translation;translationneural machine translation;translation accuracy;translation models;human translation;translation strategy;higher rank pronouns;encoder modelsneural machine translation;multiple gendered pronouns;pronouns;translation;neural machine conversation model;sentences;sentence lengths;higher sentence;truth candidates;search;conversation models;diversity diagnostics;collisional pronounncies;beam search;neural machines;language;diversity;sentence;gender frequency scores;punctuation subsets;collisional pronounfractions;bias;accuracy"}, "ac41e0ef30b6f9ee4930ac85dc46a9b50a1963d2": {"ta_keywords": "crowdsourcing data;suitable incentive mechanism;training data;preliminary empirical studies;workers;simple axiomatic characterization;mechanism;novel mechanism;right choice;approach;ability;problem", "pdf_keywords": ""}, "76f02d20e02c6baf39fee8f115cd94e4ceacf32b": {"ta_keywords": "reviewers submission;reviewers;evaluation;new submission;supervised learning framework;quality;recommendations;stream algorithms;alpha;stream;novel algorithm;recent graduates;trial;junior phd students;potential bias;relevant components", "pdf_keywords": ""}, "bf0beed35ea09aab56027d64c744098cc963fbde": {"ta_keywords": "quickest change detection;transient dynamics;transient phases;final persistent distribution;false alarm;cumulative;initial distribution;algorithms;durations;change;average run length;series;paper studies;objective", "pdf_keywords": "quickest change detection;dynamic cusum;cusum algorithm;transient changes;cusum test;abrupt changes;final persistent distribution;transient events;markov process;qcd;stochastic version;noisy noisy markov process;transients;detection;random processes;transient dynamics;stochastic systems;efficient algorithms;stochastic system;algorithms;noisy data stream;regenerative observation model;observations;global phase search;transient phases;shortest durations;stochastic model;algorithm;optimal asymptotic performance;initial distribution"}, "f1005edfa1fbc4ea0d9a90345388bda8a01e69ed": {"ta_keywords": "discrete tubular graphs;vessel trees;accurate bifurcations;capillary vasculature;curves;supervised learning;confluence;new method;new general concept", "pdf_keywords": "vessel graphs;vessel tree reconstruction;confluent vessel trees;discrete confluent vessel trees;standard tubular graph reconstructions;vessel trees;confluent vessel trees construction;unsupervised vessel tree estimation;such standard undirected tubular graphs;confluent tubular graph;tubular graphs;confluent tubular graphs;large vessel branches;tubular graph;geodesic tubular graphs;vessel contours;practical graphbased reconstruction method;small vessel branches;thin vessel branches;tubular graph weights;numerous nearcapillary vessels;vessels;vessel;volumetric data;graphs;graph;single vessel;confluent trees;tubular flow;irregular curvilinear structures"}, "794b0a1e9719d809ebdf2ef87ff84c2039bfdd52": {"ta_keywords": "wireless protocol stack;protocol stack;wireless protocol;wireless data;event;stack;data networks;state machine;architecture;active objects;reuse;paper;functions", "pdf_keywords": ""}, "ae30f8fc5a969d2d14ae066db4cd07d86fadbf42": {"ta_keywords": "absorption cross section;proton;plasma;maximum likelihood approximation;maximum likelihood;cross section;calculation;mechanism;method;results", "pdf_keywords": ""}, "ffe1416bcfde82f567dd280975bebcfeb4892298": {"ta_keywords": "automatic speech recognition;connectionist temporal classification;neural network architecture;language model;asr;network architecture;end;novel system;accuracy;training;combination;novel approach;problem;approach;state;recent literature;advances;art;goal", "pdf_keywords": ""}, "c50f98961c951fe3fbdb6f375beb28e40a6b0581": {"ta_keywords": "peer review auction;quality peer review;review process;review slots;quality submissions;quality reviews;bids;paper submission;mechanism ties;mechanism;generation;novel mechanism;authors;second stage;value;first stage;stages", "pdf_keywords": "peer review mechanism;peer review systems;peer review;stage ii peer review mechanism;scientific peer review;peer review process;peer review experiment;peer review processwe;abstract peer review;peer review couldpeer review;peer review data;online review engines;online peer review networkwe;reviewers reviews;reviewers;reviewers scores;online peer review website;independent reviewers;effortful reviews;european peer review auction;review slots;review slot;paper submission process;review processwe;peer;review;paper submission;review scores;reject decisions;machine learning tools"}, "3febb2bed8865945e7fddc99efd791887bb7e14f": {"ta_keywords": "deep contextualized word representation;bidirectional language model;large text corpus;linguistic contexts;word use;semantics;polysemy;uses;syntax;models;new type;bilm;model;complex characteristics;ii;state", "pdf_keywords": "deep contextualized word representation;bidirectional long term memory;deep bidirectional language model;short term memory;word representations;natural language tasks;bidirectional attention layer;deep context;bidirectional language model;large corpus;large text corpus;nonlinear stochastic memory models;natural language adaptation;word vectors;deep learning;deep model;character convolutions;natural language processing;encode context;linguistic contexts;deep bilstms;machine translation models;variational recurrent dropout;nlp;deep learning techniques;natural language applications;dependent representations;model polysemy;sentences;encoder"}, "aa2bd932a2ecb6e07c768bcf0dc119f0cd20f6e0": {"ta_keywords": "similarity measures;database;identification;method;traditional way", "pdf_keywords": ""}, "85a18aafcffdcc4eafcb9e5eda0abb8aa5cb8c3b": {"ta_keywords": "sdn;big data platforms;network architecture;networking;sdss;big data;network behavior;software;future;flexibility;approach;new approach;case;context", "pdf_keywords": ""}, "d89f4534d1a87005cdf470ec5d8154998d5abdc7": {"ta_keywords": "efficient resilience;parity models;erasure coding;neural networks;parm;prediction;machine learning;fast encoders;decoders;slowdowns;ideas;novel framework;failures;systems;resource;use;general framework", "pdf_keywords": "parity computations;parity models;parity model;parity;present parity models;prediction services;computation models;machine learning inference;parities;fast encoders;computations;prediction;decoder;computation;new encoder;parm;decoders;decoder model;new encoders;scalable computing resource;simple encoders;tail latency inflation;unavailable predictions;erasure codes;high performance computing;advanced prediction;specific encoder;available code;predictions;incremental processing"}, "e95a96dec775cc792b763f4eec13343c22e850e1": {"ta_keywords": "information theory;informatics;information systems;machine learning;advances;systems;workshop;annual activity report;new newsletter editor;6th international conference;report;june;bad honnef;parts;germany;july;period", "pdf_keywords": ""}, "49984bc327ef6952118c4b871eeef2a907f7a4ed": {"ta_keywords": "learning algorithms;normal form games;recency bias;faster convergence rates;games;multiplayer;equilibria;efficiency;natural classes;class;form", "pdf_keywords": "regret algorithms;regret algorithm;individual regret decays;game theory;smooth games;optimal strategy;continuous strategy games;optimality;faster convergence rates;faster convergence;regret curves;smooth game;continuous strategy space games;normal form games;main regret curves;regret;different regret;approximate optimum;sum game;pure strategies;adversarial players;splittable selfish routing games;online games;adversarial player;games;learning algorithms;learning problem;convergence rate;multiplayer;strategy space"}, "c460fd4a0dc86bc518f9a8e982bc48faf1efb942": {"ta_keywords": "social network;twitter;timeline information exchange process;parametrise social platforms users;broadcasting content;optimal time;collective attention;counterintuitive scheduling strategy;circadian rhythms;information overload;behavioural phenomena;monotony;phenomena;real data;experiments;objective function;problem", "pdf_keywords": "social media content;twitter;scheduling content;social network;social media;social media producer;social platforms;social timelines;microblogs;tweets;social networks;online social networks;social networking platform;consecutive tweets;timeline information exchange problem;consumption timeline;broadcast scheduling;instagram;online consumption;weibo platform;subscribers;bursty circadian rhythms;twitter0;novel scheduling algorithm;bursts;timelines;broadcast schedule;timeline;user behaviour;content producer"}, "1a9c89cb2e57e06dadd4c2fab5fae1bfdbb3b6d5": {"ta_keywords": "preference networks;collective reasoning tasks;aggregation;elementary nets;preferences;nets;agents;group;preferred alternative;collection;new approach", "pdf_keywords": ""}, "f5a0c6593ba95d23c025608ce9280848da8b929f": {"ta_keywords": "gene;recognition task;results;statistical analysis", "pdf_keywords": ""}, "92891a984b45df5fc764d81bf9bcd42e7e7ed1c7": {"ta_keywords": "local equilibrium;global equilibrium;differential game;epidemic spread;differential games;nonlinear dynamics;network model;quadratic pricing;loop;method;class;problem", "pdf_keywords": ""}, "3cc790174d138d7904189df997d5763f1793dedf": {"ta_keywords": "normalization task;normalization;text;agreement;certain characters;accuracy;characters;new method;method;degree;problem;interest", "pdf_keywords": ""}, "1d255aeabcb87929742280251007fd8c01bbe914": {"ta_keywords": "tin;synthesized iron;polyoxometalate;nanoscale cluster;po11fe;materials;synthesis;fabrication;new type;form;new class;ability", "pdf_keywords": ""}, "737f9a32d7f4007aa9526556c256ed4a182aec69": {"ta_keywords": "quadratic game;optimal price;convex function;convex functions;players;cost;game;design;small perturbations;parameters;number;notion;terms", "pdf_keywords": ""}, "4aa72e4232ae809ea1a9fe142275da25ba930655": {"ta_keywords": "linear quadratic games;quadratic games;convergence guarantees;quadratic game;gradient algorithms;equilibria;policy;gradient;conditions;sufficient conditions;counterexamples;sum;existence;large number", "pdf_keywords": "linear quadratic games;player quantum games;generalized game theory;linear feedback games;global nash equilibria;nash equilibrium;policy gradient dynamics;global nash equilibrium;sum lq games;linear feedback policies;linear policies;policygradient algorithm;linear policy;nash equilibria;lq games;sum games;lq game;game dynamics;optimal control;value games;natural games;reinforcement learning;play converges;local convergence guarantees;games;natural game;such games;strategies;strategy;equilibria"}, "b62ce3135ed6065863c0dec26037fd07c081abba": {"ta_keywords": "natural language inference tasks;causality;augmented data;intrinsic coherence;language;classification;data;documents;relative merits;novel representation;method;difference;use;simple method", "pdf_keywords": "natural language processing;natural language;sentiment analysis models;sentiment analysis;associative learning;classifiers;linguistic patterns;classification;linguistic structures;machine learning;sentences;domain data training;spurious associations;human judgments;term memory systems;simple machine learningwe;datasets;training data;causality;learning;dataset;unspurious associations;data augmentation techniques;human behavior;text;original dataset;languages;new dataset;term memory system;literature"}, "ca86a63362e51eea2e213ae2d3faed668ec1ad74": {"ta_keywords": "commonsense concepts;semantic similarity detection;concept extraction;natural language texts;natural language;world example;graph;detection;novel approach;approach", "pdf_keywords": ""}, "a1da1d600acd506b80c8870d293a756c70791683": {"ta_keywords": "bilingual lexicon induction;bilingual lexicons;unaligned word embeddings;distribution matching;languages;isometric assumption;isometry;spaces;assumption;novel method;bli;larger set;recent work;question", "pdf_keywords": ""}, "f184908270fc934ab74438a0aaac7a43a5eae6d2": {"ta_keywords": "single document summarization;single document summarization process;explicit sentence dependencies;sentence structure;document;aware encoders;spontaneous generation;joint distribution;structure;novel model;standard models;model;analysis", "pdf_keywords": "neural abstractive summarization;abstractive summarization models;new summarization model;abstractive text summarization;summarization models;simple abstractive summarization approach;abstractive document summarization problem;abstractive summaries;recurrent neural attention model;summarization;empirical summarization;structured attention;source document sentences;summaries;aware attention module;sentence structure;explicit document structure;probabilistic attention weights;training corpora;long source document;decoder;novel encoder;sentences;encoder;short documents;encoder model;semantic vectors;external linguistic structures;neural network;decoder framework"}, "99ac83b990af1fc591db5b676300a7c002905dae": {"ta_keywords": "multiple views;data view;linear programming;mixed integer linear programming;optimization;consistent class assignments;f1 score;em framework;baselines;scores;art performance;number;techniques;method;use;terms;state;presence", "pdf_keywords": ""}, "3f90668994d6e5949a530dfc84a10b492ff35cfa": {"ta_keywords": "shallow semantic parsing;semantic labels;distinct semantic labels;similar sentences;sentences;corpus;interesting data;accuracy;novel method;method;paper;methods;number", "pdf_keywords": ""}, "fce19dd512a82693ab9070049ed426179eca8856": {"ta_keywords": "argumentative structure;argumentative argumentative spectrum;natural language processing;successful nlp technique;nlp;argumentative spectrum;web content;content;chapter;ability;recent advances;success;area", "pdf_keywords": ""}, "0d22ce72a62419086fd4860a4671991846cd492b": {"ta_keywords": "lightweight block ciphers;global random attacks;implementation;throughput;design;speed;data;new generation", "pdf_keywords": ""}, "fb6ef2d6fbd1ea4905070077ab6c5b0108f2c38a": {"ta_keywords": "sarcasm detection method;sarcasm;tweets;large social network;detection method;detection;method;un;novel method;analysis;approach;measure;different strategies", "pdf_keywords": ""}, "b8e49216e5b4a017342b0be5f6fbbd79e690a1c7": {"ta_keywords": "auction;auctions;constrained learning approach;deep learning;neural network approach;machine learning approach;optimal mechanism;combination;design;novel approach;approach;use;recovery", "pdf_keywords": "optimal auctions;optimal auction;designing auctions;item auctions;auction;generic auction;suitable auction;allocation networks;auction design problem;wise auctions;parametric auctions;auctions;auction class;thesoupnet auctions;auctionswe;online auctions;new auctions;demand bidders;bids;regretnet algorithm;soup auction;bidders;jackets auctions;neural nets;combinatorial feasible allocations;utility gradient;combinatorial feasible allocation;bid;original regretnet algorithm;neural networks"}, "f297e939212780637705eba8798c9a386befd771": {"ta_keywords": "pivot phrases;pivot language models;translation process;data;input;novel method;power;method", "pdf_keywords": ""}, "8a902a848c3710290f04f2d59030f5670d3433f8": {"ta_keywords": "morphological complexity;morphological features;morphology;error prediction;error analysis;certain languages;language;classical models;accuracy;predictive power;study;importance;use", "pdf_keywords": ""}, "ab48fb72541653f40523caa9fcaac9cb84bf3373": {"ta_keywords": "neural source model;channel speech;joint source separation;conditional vector analysis;noisy data;recognition;model;novel frontend;system;dereverberation;framework;ap;performance;parameters", "pdf_keywords": "speech networks;speech sources;automatic speech recognition;joint source separation;neural source model;independent vector analysis;neural source;multiple sources;trainable neural networks;multichannel blindwe;deep learning;noisy input data;joint independent subspace analysis;neural networks;linguistic data network;noisy dataset;joint denoising;coherent k1 dataset;stable iterative source;iterative source steering;deep learning representation;joint separation;noisy data;recognition;separation;future neural network;vector;learning approach;dereverberation signal;linguistic input"}, "3c5d3bbb73aa0e3e969a25487a81b5b1f0c14044": {"ta_keywords": "knowledge graph;large datasets;scale knowledge graphs;80k facts;joint inference;facts;scale dataset;large number;structure;novel method;method;hours;incomplete fashion;ability", "pdf_keywords": ""}, "5ede529879d162d2779d410a5775d3f6cd6be3f4": {"ta_keywords": "generative models;robust optimization;model selection heuristics;machine learning models;inner maximization objective;optimization;parameter search;related data distributions;gradient;realistic scenarios;training;large scale;relaxation;dro;dro problem;toy settings;framework;collection;approach", "pdf_keywords": "generative adversarial network model;machine learning models;generative model;robust learning;adversary;generative modeling task;machine learning;empirical discovery;empirical risk;robust distributional robustness;training data;robust models;novel empirical approach;machine learning problem;dro regression;novel distributional framework;generative one;large scale generativewe;robust heuristic;models;nonparametric parametric neural models;training stability;robustness;neural network;powerful adversaries;optimization;parametrized empirical parametrizations;regularization;learning rate;dro game"}, "c6b462aaca52d0325db3118d2779865915b266c3": {"ta_keywords": "rule induction methods;rule induction;large training sets;new pruning techniques;asymptotic complexity;noisy data;accuracy;noise;methods;runtime;formal arguments;form;claim;experimental data;loss;solution;presence", "pdf_keywords": ""}, "dfb35ebe4fd754f59053d27c78f555bb5e7ccbff": {"ta_keywords": "thin structures;absolute curvature;curvature;optimization algorithm;surfaces;3d examples;estimation;orientation;lines;2d;center;block;novel approach;location;advantage;approach;range;idea", "pdf_keywords": "thin structure detection;absolute curvature regularization;curvature regularization;edge detection;curvature penalization;absolute curvature approximations;vessel detection;edge localization;curvature;absolute curvature;thin structures;sparse representation;subpixel delineation;variational inference;level vision framework;3d images;orientation estimation;vision framework;energy estimation;regularization;coordinate descent;regularization framework;boundary edges;stochastic variational inference;gradient vector;medial axis;sobel gradient operator;vision;shapes;detection"}, "cd5a9a0061de6a6841c63e60281133207b2d6763": {"ta_keywords": "description decoders;natural language;context encoders;global contexts;wikipedia term;unstructured data base;description;phrase;dataset;model;novel method;effectiveness", "pdf_keywords": "description decoder;natural language description;natural language;unfamiliar words;unknown words;unfamiliar phrase;polysemous words;global contexts;contexts;global context;decoder;dictionaries;news text;context encoders;phrases;immediate context;text;dict;foreign languages;unknown phrase;global contextthis paper;phrase;meanings;words;context;local context;massive text;specific context;unknown word senses;wikipedia"}, "89b8153a86708b411bd21357c5b6006142104fc9": {"ta_keywords": "ted talk corpora;annotated corpora;corpus;spoken quotes;sample;pros;properties;cons", "pdf_keywords": ""}, "91ef95907dc637ad3c29ac3cc0e682b9c1985a37": {"ta_keywords": "simultaneous speech translation;optimal segmentation strategy;algorithms;dynamic programming;greedy search;performance;astronomy;university;experimental evaluation;pittsburgh;pa;methods;physics", "pdf_keywords": ""}, "1f3c381eedfe8914b81e93070bfdb00cf86ac943": {"ta_keywords": "graphs;graph diffusion;structural views;views;learning;representations;structure;benchmarks;linear evaluation protocol;order neighbors;self;encodings;art results;best performance;performance;new approach;number;new state", "pdf_keywords": "graph representation learning;novel graph representation learning algorithm;graph level representations;graph representation representation learning algorithm;graph representations;graph learning;visual representation learning;graph views;graph representation;graph classification benchmarks;novel contrastive representation learning framework;graph encodings;graph classifications benchmarks;representation learning;learning representation;novel representation learning representation;deep representations;contrastive learning;graph levels;graphs;contrast nodes;general graph diffusion;graph diffusion;exact node representations;representation learning algorithm;large graphs;graph;graphwe;unsupervised learning;graph pooling layer"}, "4abdea830316d80ab0b29fb94ee0786216f6a1cd": {"ta_keywords": "joint phrase alignment;phrase alignment;machine translation tasks;inversion transduction grammars;phrase pairs;phrases;phrase;nonparametric methods;unsupervised model;itgs;extraction;model;competitive accuracy", "pdf_keywords": ""}, "d6b3effdeb3d38ac9ee43c3b8292b0937a295c30": {"ta_keywords": "multitask learning;hierarchical multitask learning;connectionist temporal classification;standard multitask training;hierarchical multitask;speech recognition;ctc;text words;performance;context;error rates;effect;factor", "pdf_keywords": "multitask learning;neural speech recognition;connectionist temporal classification;speech recognition;multitask model;multitask;hierarchical multitask;multitask learningwe;multitask approach;phonetic recognition task;automatic speech recognition;convolutional neural network;phonetic classification task;deep learning;decoder speech recognition model;phone multitask performance;temporal classification;neural network;hierarchical training;neural networks;phone softmax performance;novel attention model;cnn;free recognition;hierarchical learning framework;auxiliary loss;level phone ctc model;auxiliary phone ctc loss;other tasks;level encoder"}, "3c78c6df5eb1695b6a399e346dde880af27d1016": {"ta_keywords": "level sentence models;document reading;sentences;sentence;neural process;novel method;questions;previous approaches;method;approach;form;problem;several cases", "pdf_keywords": "neural reading comprehension model;reading comprehension model;paragraph models;paragraph predictions;neural paragraph;comprehension systems;multiple paragraph questions;individual paragraphs;paragraphs;paragraph selection;paragraph paragraphs;likely answer span;paragraph;sentence sentences;multiple paragraphs;softmax operator;sentence output;neural model;comprehension;good confidence scores;sample document selection methods;confidence scores;document retrieval system;questions;entire documents;level questions;training objective;models;sentence sentence sentence;simple model"}, "7c3a2e953d2c07ff4f150865112e4ceec14090ea": {"ta_keywords": "speech enhancement;statistical excitation prediction;noise reduction method;voice conversion method;excitation parameters;hybrid enhancement process;electrolaryngeal;spectral parameters;hybrid method;eel;pass filtering;effect", "pdf_keywords": ""}, "73fe797b4f4f2d18784246bb74626426a8fe108e": {"ta_keywords": "pointer graph;dynamic graph connectivity tasks;deep sets;data structures;graphs;neural networks;pointer;pgns;edges;cut trees;improved model expressivity;accurate models;disjoint;unions;link;augment;novel approach", "pdf_keywords": "pointer graph networks;efficient latent graph inference;dynamic graph connectivity tasks;polynomial learning networks;dynamic graph connectivity query;graph network;dynamic graph connectivity;vertex trees;nodes;networks;dynamic graphs;graphs;downstream query weights;novel data structures;dynamic connectivity;algorithmic queries;vertex functions;supervised operations;queries;parent nodes;vertex vertices;data structures;search trees;pgns;query data;graph;data structure;connectivity;neural network;relevant data flow"}, "8b652c4d7a8d5836925ce0fe28a91dc661778524": {"ta_keywords": "large neural language models;comparative questions;berts;text;recent results;entities;models;level performance;sort;difficulty;broad notion;study;ability;setting;different settings", "pdf_keywords": "ai conference;neural language models;nference conference;natural language;language model;large language models;large language model;human annotated questionswe;large corpus;semantics;human comparative questionswe;general comparative semantics;lexical domains;sentences;comparative semantics framework;training examples;entities;textual statements;lexical phenomena;large neural linear machines;comparative entity pairs;entity;entity mentions;domain training sets;domain examples;questions;natural comparative questions;learning;topic;results"}, "f53aa1d2676689c94429944f6a69431f96e05ae1": {"ta_keywords": "membership models;supervised learning;same mixed membership model;gibbs sampler;approximate inference;such models;supervision;topic;features;present methods;methods;good indicators;modification;stronger forms;technique;task;experimental results;injection;range", "pdf_keywords": ""}, "57676e07d66b102f3335a5c538735ebff9076623": {"ta_keywords": "zeeman field;effect", "pdf_keywords": ""}, "ba1823889a80c231966a0f24e57c6cf4a569ff8c": {"ta_keywords": "diverse multimodal fake news;multimodal entity inconsistency;multimodal fusion framework;visual entities;entity inconsistency;novel entity;celebrities;mutual enhancement;text complementation;images;level semantics;joint modeling;landmarks;model;cross;help", "pdf_keywords": "multimodal fake news detection;multimodal fake news;multimodal feature fusion;multimodal feature extraction;multimodal entities;multimodal entity;multimodal information;multimodal entity inconsistency;present inin multimodal news;multimodal correlations;multimodal clues;fake news detection;multimodal fusion framework;multimodal;false news detection process;multimodal methods;multimodal inconsistency;world news corpus;visual entity extraction;multimodal network;multimodal organization;textual features;singlemodality;important news elements;visual entities;crossmodal;highlevel semantics;multiple images;detection;attention transformer"}, "499ada382b7ce8f1cbd890e8c21500d95e20f2fe": {"ta_keywords": "purpose audio representation;human ear;models;hear;challenge;wide variety;strong basis;domains", "pdf_keywords": "generic audio evaluation challenge;audio representations;novel audio representations;flexible audio representations;new audio representation class;novel multiclass audio classification task;new sound processing tasks;audio community;hear challenge;novel music classification task;natural language representation;speech models;audio signal processing tasks;audio domains;natural language input data;novel learning tasks;hear datasets;deep learning;deep learning models;deep learning interface;holistic representations;natural language;natural language processing;parallel learning challenge;sound;text representation;audio signal;representations;audio signal processing;evaluation tasks"}, "aa2428e1c4ea6d6bb347cfa59beead8736e19c46": {"ta_keywords": "magnetic properties;magnetic field;ground state;effect", "pdf_keywords": ""}, "95ee674a03ad23eaaf4837121fc8aea30d885088": {"ta_keywords": "deep siamese networks;novel metric learning approach;compact preference representations;popular siamese formalism;metric function;deep neural networks;partial orders;distance;distance function;high accuracy;approximation algorithms;preferences;use", "pdf_keywords": "metric learning;novel metric learning approach;structured preference representation;tractable preference distance;preference distance;graph learning;network classifiers;lexicographic preference trees;popular conditional preference structure;preference trees;distance metric;metric;fast learning;neural networks;metric function;novel deep neural network;kernel learning;supervised learning setting;siamese autoencoder;classification tasks;novel neural network model;qualitative preferences;convolutional networks;structured representation;distance;novel autoencoders;individual preferences;salient features;best approximation algorithm;approximation algorithms"}, "683bbb665bdaea8688834e97559d63842242ee1f": {"ta_keywords": "reinforcement agent;reinforcement learning;reinforcement learning approach;intrinsic fear;agent;game;catastrophe;catastrophic mistakes;steps;approach;probability;dangerous states;new method;information;short number", "pdf_keywords": ""}, "13d9d24ff2ba69de4cedcebd8f59371a5c1de7ed": {"ta_keywords": "useful context words;data mining;knowledge;simple word distance;learning;framework;use;novel approach;approach;combination;identification", "pdf_keywords": ""}, "788aa828a194a6d6c4e5ab1d4b46fc5f987159b0": {"ta_keywords": "glass interaction;spins;spin;glass;classical system;new method;system;like state;application;method", "pdf_keywords": ""}, "3df97e8237c7d98c7343fc025eacbbc2b96a10ae": {"ta_keywords": "exosomes;strong extracellular field;dynamics;energies;particle;energy;model;self;presence;consistent description;number", "pdf_keywords": ""}, "a43d6fa0e96d56d0200e8d5e4407be8befc4e063": {"ta_keywords": "metamaterials;classical models;physical system;myopia;quantum;properties;models;factors;nature;behavior;predictions;paper;information;choice;loss", "pdf_keywords": ""}, "b2fac3812885e3c8101cc729b6846f9108ac4d70": {"ta_keywords": "estimators;mle;estimator;likelihood;mean squared error;maximum;bounding box;terms;new class;simple modification", "pdf_keywords": "optimal estimation;richardson equation;asymptotic bias;estimation;finite unconstrained mle;optimal selection rule;unconstrained mle;bias reduction;optimal decisions;optimality;estimators;maximum likelihood estimator;optimalwe;optimal sampling;parameter estimation;mle;supersymmetric optimal model;optimal performance;estimate;new estimator;jones function;new mean squared error estimator;optimal choice;supersymmetric optimal case;standard mle;optimal value;optimality condition;constrained minimal distance estimator;likelihood;estimator"}, "8e56db786a685b4b9c7f1b750f60a81baebff0b5": {"ta_keywords": "microphone;quiet environment;natural speech;voice;acoustic changes;speaker;talk;intelligibility;body conduction;signal;naturalness;certain intensity;nam;method;form;report", "pdf_keywords": ""}, "418349df9bf28e2b1290b758a4ebcf0d812c7288": {"ta_keywords": "blog network;blog;similarity;nodes;shape;method;approach;new method", "pdf_keywords": ""}, "e00f0a9e184a9d2afd8bb344908ca25d8bdc9e04": {"ta_keywords": "natural language computer;natural language language computer;natural language;natural language language language;matrix problems;voice inputs;user inputs;system;task;nl;design;development", "pdf_keywords": ""}, "692320cf5ae6980bc6b2b2d7bc48df961b545c22": {"ta_keywords": "channel speech enhancement;channel speech enhancement system;single microphone array;single acoustic reference track;conference;baseline system;proposal;task;context;challenge;tasks;development;paper;presence", "pdf_keywords": "single microphone array;microphone arrays;video conferencing rooms;video conferencing;chil audiovisual corpus;audiovisual presentations;automatic speech recognition;individual speaker;quiet meeting rooms;monaural speech separation;field speech;playback;room impulse responses;speaker;conference challenge database;real speakers;speech;noisy communication systems;noisy environments;noise data;database;receiver;sensitive noise;noise;conference;noisy environment;practical application;dinner party scenario;everyday home environments;signal signals"}, "87d50fc84c71ed9860ed02b0149266b74c446c9c": {"ta_keywords": "hidden model parameters;continuous speech recognition;nonparametric bayes manner;variational techniques;model parameters;linear regression;parameters;probabilistic treatment;generalizability;model topology;paper;approach;hyper;experimental results", "pdf_keywords": ""}, "dc3adb99f682a11fe0507dcbc5dc2958199c5af1": {"ta_keywords": "laser pulse;laser field;laser;frequency comb;atom interaction;atom;interaction;simple model;generation;model;analog;idea;concept;result", "pdf_keywords": ""}, "48685f26b32d199e6a4d80f6c61e62cc9738e403": {"ta_keywords": "npnpnp task;deep structural representation;vector machine classifiers;task;novel approach;essential features;approach;problem;form;power;natural way;state;art performance", "pdf_keywords": ""}, "e107beee5e84cd11d6460f7040676687a51a378b": {"ta_keywords": "end reconstruction operators;reconstruction network;unpaired training data;classical variational framework;variational problem;inverse problems;reconstruction;iterative unrolling;distortion;measurement space;distributions;end;method;convergence;new approach;output;combination", "pdf_keywords": "unrolled adversarial regularization;adaptive regularization;learned regularizer;unrolled reconstruction network;end reconstruction operators;regularization;reconstruction operator;adaptive regularizer;reconstruction network;optimal unrolled reconstruction;convex regularizers;adversarial learning framework;unrolled reconstruction;unrolled reconstruction distance;regularization penalty;adversarial setting;reconstruction algorithm;large unrolled reconstructions;variational loss;small unrolled reconstructions;corresponding variational objective;large scale reconstruction;reconstruction problem;regularizer;variational approach;quadratic regularizer;variational framework;reconstruction quality;reconstruction;iterative unrolling"}, "9f1d9dfb0b30d9fc5881d07b8e7f508815296c93": {"ta_keywords": "statistical parsers;new parser;language;other datasets;multiple domains;new domain;training strategy;model;process;paper;strategy", "pdf_keywords": ""}, "51ec4e93d8ae4c62453fdb34c6866696da0527b1": {"ta_keywords": "representative detection methods;fake news;visual content;effective visual features;detection methods;detection;multimedia;readers;role;recent developments;concepts;field;comprehensive review;chapter;brief introduction", "pdf_keywords": "fake news detection;false news detection;fake content;fake news events;fake images;fake news posts;fake tweets;false news dataset;novel method tofake news;fake events;sensational media content;multimedia content verification;cnn;false news claims;news event;social media content;fake news;visual content;false false false content;false false false false content;representative detection methods;fake newsin;false news;multimedia content;accurate detection;detection;news;social media;social media outlets;visual features"}, "a16cecbaf87d965e396e610f251f710a807b70ad": {"ta_keywords": "hearing impairment simulation method;hearing impairment simulation;hearing impairment level;individual hearing;measurable audiograms;audiogram;auditory charatecteristics;filter characteristics;accuracy;experimental results;persons;approximation;gain characteristics;method;individual", "pdf_keywords": ""}, "f49065750931c1c3c9edaf7d2f4bc8ea1342450a": {"ta_keywords": "neural autoregressive sequence models;neural machine translation;short sequences;regularization;model distribution;model;training;high probability;rate;performance;specific case;set;high degree;experiments;work;effect;issue", "pdf_keywords": "neural machine translation;neural machine translationwe;neural autoregressive sequence model;autoregressive neural sequence;machine translation;premature translation;translation quality experiments;translation quality;premature sequences;shorter sequences;neural machine model;short sequences;negative loglikelihood objective;regularization;language independent subword tokenizer;translation;target sentence;target language;sentences;natural language processing;learning;source sentence;sequence;loss term;optimization;machine learning;low resource datasets;gradient;learning process;conditional distribution"}, "b0b1112b06898733faefc32f54940aa4e84bc383": {"ta_keywords": "speech corpora;corpora;conversation corpus;digit corpus;parallel sentences;emphasis;languages;conversational setting;parallel digit strings;english;speakers;recording;study;collection", "pdf_keywords": ""}, "c55bc339122ad8cdba1ae74d1336be3d2f089699": {"ta_keywords": "stochastic convex optimization problems;affine constraints;smooth objective functions;optimization methods;special penalization technique;dual approach;inexact proximal step;similar triangless method;algorithms;primal case;algorithm;several methods;methods;same problems;initial problem", "pdf_keywords": "stochastic convex optimization;stochastic convex optimization problems;convex optimization;stochastic optimization;convex objectives;convex optimization problem;stochastic gradient estimator converges;stochastic optimization problem;stochastic composite optimization method;accelerated stochastic gradient methods;stochastic stochastic stochastic optimization problem;stochastic gradients;decentralized optimization problems;decentralized optimization problem;arbitrary stochastic gradient;stochastic dual oracle;stochastic gradient;decentralized optimization;biased stochastic dual oracle;stochastic dual coordinate ascent;stochastic average gradient method;convexity;strong convexity;affine constraints;stochastic estimators;convexity iswe;stochastic parameterization;stochasticity reduction;stochastic approachwe;parallel optimization"}, "6cf3bdcdee6236f9f04e7773e3601dbbb8fbc61e": {"ta_keywords": "entity recognition;natural language questions;simple natural language questions;natural language setting;entity;ner datasets;questions;ner;task;models;set;novel approach", "pdf_keywords": "entity recognition;new ner datasets;ner datasets;ner dataset;ner training sentences;natural language processing;ner data sets;disease entities;ner sentences;partial annotation;ner diseases;ner models;biomedical nerns database;entities;entity;phrase retrieval model;previous best weakly supervised model;ner benchmarks;domain models;natural language;shot ner models;phrase encoder;nerr database;accurate entities;simple natural language questions;nerns database;level annotations;ner model;domain resources;ner"}, "a0f00d5ea3727151b1c2fc8c407404f0c6641051": {"ta_keywords": "other parsers;parser;novel parser;simple parser;light pulses;lalarkov;sentences;shalzov style;parallel;motion;novel feature;accurate results;recent experiment;result;cost;ability;millions", "pdf_keywords": ""}, "3b7321832ba109cf47bfd13595c3b58acd4cb080": {"ta_keywords": "orbit coupling;orbit interaction;spin;electron;effect", "pdf_keywords": ""}, "3400b8bf1ffde3ef3d35dfcea893e6506427aa21": {"ta_keywords": "single speech sequence;speech recognition functions;multiple label sequences;joint recognition;multiple speakers;speech;source separation;sequence framework;hidden vectors;new sequence;end manner;mapping;model;end;new objective function;contrast;experimental results", "pdf_keywords": "end speech recognition task;input speech mixture;speech mixtures;speech mixture;end speech recognition model;standard speech recognition task;speech recognition;single speech sequence;multiple separate speech sources;independent speech separation;observed speech separations;speech recognition functions;multiple separate speechwe;joint utterances;dimensional speech;decoder networks;single utterance;spontaneous speech;utterances;long short term memory;speech;deep recurrent neural networks;speech types;input feature vector sequence;decode multiple label sequences;candidate utterances;large corpus;long short term memory network;art speech systems;encoder training"}, "3ce0f00d6c949192107f1bd6a167c03e1fb7393a": {"ta_keywords": "parsing models;parsers;novel deterministic dependency;dependency structure;first transition;easiest arcs;algorithm;nondirectional manner;performance", "pdf_keywords": "deterministic parsing algorithm;dependency parsers;novel parser;parsers;parser;informed parser;reduce parser;correct parses;right greedy malt parser;structured perceptron;malt parser;novel deterministic dependency;natural language sentence;dependency structure;sentences;linguistic constructs;linguistic input;learning process;sentence;right processing order;training algorithm;deterministic algorithm;more structure;easiest arcs;feature representation;algorithm iswe;structure;natural extension;root prediction;algorithm"}, "b9f5115b0353c268999fcc2f49c4b8e03a223994": {"ta_keywords": "evolutionary causal matrices;structuredsimulation module;interventions;simulation model;simulation;outcomes;social psychology;experiments;class;markov chain;testing;module;data;scale experiments;optimization", "pdf_keywords": "stochastic system;stochastic model;stochastic models;stochastic processes;population dynamics;simulation model;biological systems;stochastic nature;evolutionary patterns;simulation;stochastic exchange;general purpose simulation program;probabilistic model;multipurpose simulation program;computational models;populations;simulation program thatwe;mathematical models;underlying dynamics;simulation module;mathematical model;evolution;models;dynamics;markov chain;wascomputational predictive models;patterns;process;real experimental data;model"}, "cc4cc594c7dd38482c46a2db440135b8f26ff54f": {"ta_keywords": "proton exchange membrane fuel cells;pt skin;faceted pt skin;catalyst;catalytic oxidation;highefficiency catalysis;pt78zn22;metal zinc;zinc;structure;magic;theoretical studies", "pdf_keywords": ""}, "e2f015bbddd7bade7caca693e37f84c4cf70a7f5": {"ta_keywords": "speechindicator;binary masking;enhanced speech;mnmf;latent variables;effective initialization method;initialization method;nsf;clusters;method;paper;construction;experiments", "pdf_keywords": ""}, "f8d7b263e8d663583cd22d5988c8ea4a49ed2840": {"ta_keywords": "relation extraction;joint entity;entities;new relations;simple entity model;novel relations;novel approach;approach;approaches;accuracy", "pdf_keywords": "relation extraction;princeton university relations extraction system;relation extraction datasets;end relation extraction datasets;entity recognition;end relation extraction;information extraction;relation classification;natural language processing;unstructured text;entity mentions;relation models;entity information;informative entities;joint entity;relations;entities;relation modelwe;relation model;language models;parser;entity;extraction;relation;novel annotations;novel tagging scheme;distinct contextual representations;relation types;base encoder;joint models"}, "249b7517a746b1389991e10fd618cad62e66c4df": {"ta_keywords": "similarity extraction;similarity measures;word synonyms;corpus;different word types;parsed text;constrained graph walk variant;text;novel method;context;method;art approaches;problem;state", "pdf_keywords": ""}, "0718237a30408609554a0e2b90d35e37d54b1959": {"ta_keywords": "subword mining algorithm;meaningful subwords;mined subwords;downstream language;entropy;fasttext;representations;task;data;performance;model", "pdf_keywords": ""}, "771c1cb5fb161231e9aaa0a189caba672256a880": {"ta_keywords": "unsupervised morphological disambiguation;morphological analyzer;vocabulary language model;generative processes;full word forms;morphs;words;characters;mixture;model;context;sequence;experiments;use", "pdf_keywords": ""}, "7374494ee88608ef76f74b58a8f8c26ab06adfb9": {"ta_keywords": "end diarization model;diarization;diarization method;speech;clustering;speakers;clusters;end;region;novel method;method;number;post;results", "pdf_keywords": "speaker clustering challenge;unsupervised speaker;speech separation;speaker diarization;speech frames;speech challenge;speaker models;conventional diarization methods partition frames;speech;speaker;end diarization model;clustering;diarization;recognition;diarization results;speakers environment;classifier;frames;frame selection;deep search;clusters;frame;time kernel models;reference frame;representative encoder;several datasets;detection;deep neural network;speakers;dataset"}, "6276bbe6cc56234d430725a31a27939eeec88149": {"ta_keywords": "quotability identification problem;document;feature;novel feature;models;accuracy;model;experiments;superiority", "pdf_keywords": "quotability identification;passage ranking;passage quotability;influence passage quotability;quotability;quote counts;sentence features;source texts;text documents;linguistic annotations;texts;source document;many documents;best annotations;annotations;linguistic style;linguistic structure;sentence structures;literature;original text;document;similarity;documents;chroniccomputational linguistics;level sequence taggers;linguistic attributes;text;bert;passage labels;sequential passage prediction"}, "96d5e1f691397dfb51e8b818a21a2d11eee46a59": {"ta_keywords": "multicore processing;optimal runtime;computing systems;uncoded schemes;nlog;workers;scheme;order;times;recent advances;solution;design;number", "pdf_keywords": ""}, "75fe6c3ffdea2608794b4f21119c5a4dec07663a": {"ta_keywords": "generative flow;sequential latent variables;flow;complex distributions;neural networks;conditional density;benchmark datasets;model;novel approach;several layers;state;art;time;terms;approach;technique", "pdf_keywords": "nonautoregressive neural machine translation;nonautoregressive neural machine translation models;generative flow;generative flows;novel autoregressive flow;generative model;generative process;high quality translation outputs;novel generative method;generative;machine translation experiments;translation diversity estimation;high quality translation samples;sequential latent variables;translation outputs;novel flow architecture;source sequences;flow;noisy input sequence;floweq;autoregressive model;decoder;encoder;posterior flow;encoders;decoders;novel encoding;deep neural networks;neural model;complex distributions"}, "aa0b93501f79d57fe8542e72ccc8843ea50443c9": {"ta_keywords": "multilingual optimization;seq2seq models;target language;features;stack bottle;low resource scenario;conventional hidden model;various strategies;sbn;neck;systems;neck features;bottle;context;hmm;effectiveness", "pdf_keywords": "multilingual acoustic feature training;multilingual acoustic models;end speech recognition;multilingual features;monolingual features;automatic speech recognition;speech recognition system;attention networks;various multilingual approaches;attention;large scale feature processing;attention approach;languages;joint attention;neural network;feature;features;seq2seq models;novel feature extraction scheme;seq2seq model;optimal translation;neck feature extraction;sequence;machine learning;different training strategies;training andin;training strategies;asr;unified model;seq2seq"}, "de971e50d70bc4d66f7debfab242942b0d1cae34": {"ta_keywords": "cascade speech summarization module;automatic speech recognition;fusion module;depth information;bert;asr;depth;cascade approach;joint summation;noise;context;input;errors;signal;power;approach;variety;new public release;experiments", "pdf_keywords": "cascade speech summarization model;bert summarization model;speech summarization model;speech summarization;new speech summarization system;speech summarization system;summarization model;summarization performance;new summarization task;summarization accuracy;multiple speech recognition hypotheses;text summarization;speech recognition hypotheses;abstractive summarization taskwe;quality text summaries;summarization;abstractive summarization task;automatic speech recognition module;summarization version;bert models;quality summaries;attention fusion;automatic speech recognition;speech data;advanced speech system;bert module;speech encoder;speech translation;compact summary;sentences"}, "e0ab89821af308f51647bfe872f114d775fd8818": {"ta_keywords": "multilingual speech recognition system;multilingual medical data;finitestate transducers;translation;source language;target language;utterances;medical domain;network;system;recent development;paper", "pdf_keywords": ""}, "fba7ad8f63a42111b3618e51e3493ed70aafdcd0": {"ta_keywords": "conversation data;different word distributions;probabilistic model;word use;speaker;expectation maximization;influences;trust;meeting data sets;people;english;effectiveness;algorithm;method;model;experiments;japan;em;level", "pdf_keywords": ""}, "1ce96d8dbf69199ebd043de6cfa25d7e48b8ab03": {"ta_keywords": "linguistic properties;causal effects;causal quantity;bias;text;effect;writer;estimator;assumptions;observational data;intent;adjustment;results;algorithm;problem;setting", "pdf_keywords": "causal inferences;causal inference;linguistic property;linguistic properties;thecausal inference;arbitrary language representation;causal effects;causal effect;linguistic meaning;causal model;corpus;text;textcause;reader;bias;writer;sentence;sentence shape;proxy language;proxy representation;intervention;privileged entities;observational data;effect;empirical analysis;proxy label;outcomes;unlabeled data;arbitrary estimators;proxy treatment"}, "c37ecbccecab1774b545a5a5804b575718218f7d": {"ta_keywords": "emotional speech recognition;cnn bottleneck features;bottleneck features;deep neural network;features;bottleneck sturucture;other feature transformation methods;phonemes;essential features;sturucture;tandem approach;methods", "pdf_keywords": ""}, "ae5a34c20fee705ad7094c93a711d5f724d535f0": {"ta_keywords": "recommendation fairness;aware tensor recommendation framework;new sensitive latent factor matrix;sensitive information regularizer;fairness;novel fairness;sensitive features;other latent factors;sensitive information;recommendations;synthetic datasets;quality;framework;state;art alternatives;experiments", "pdf_keywords": "aware tensor recommendation framework;recommendation tensor;comprehensive tensor factorization approach;recommendation fairness;sensitive latent factor matrices;sensitive latent factor matrix;recommendation framework;aware tensor completion framework;better recommendation quality;traditional matrix factorization methods;new sensitive latent factor matrix;tensor completion methods;tensor completion;recommendation quality;tensor completion method;tensor completion task;tensor completion model;efficient recommendation;recommendation algorithm;tensor approaches;recommendation model;sensitive features;tensors;consistent fairness enhancement;novel tensor;tensor;recommender systems;tensor foundation;implicit recommender systems;optimal recommender systems"}, "ff187722c5b5462ac2066a737ae97650ffa177ed": {"ta_keywords": "novel automatic pronunciation assessment system;noisy classroom;noise reduction;sound processing;noise reduction technique;performance;system;combination", "pdf_keywords": ""}, "0f61621206e363367db85b39e8e4325e425afcb4": {"ta_keywords": "statistical singing voice conversion process;converted singing voice;source singer;waveform generation;conversion accuracy;method converts;direct waveform modification;singer identity;target singer;vocoder;spectral parameter trajectory;statistical characteristics;technique;method;techniques", "pdf_keywords": ""}, "94c3fd8eea08008cecd98f4aace024cf63954ead": {"ta_keywords": "malicious sensor;small false alarm rate constraint;estimation scheme;sensor observations;optimal filter;estimation;novel filtering;linear process;algorithm;squared error;novel approach;time;gain;paper;presence", "pdf_keywords": "secure estimation algorithm;secure remote estimation;secure estimation;linear attacks;attack detection;dynamic attack;stationary attacks;few unknown malicious sensors;data injection attack;false data injection attack;static attack;remote estimation;such attacks;attack class;robust estimation;attack;noisy observation;unknown sensor subset;state estimation;safe estimation;noisy noisy observation;estimation;anomalous observations;detection;noisy networks;various sensor subsets;noisy sensor;filtering scheme;unbounded noise;sensor network"}, "473021db54cbae9c4546597cd7e4b5d687a51c7f": {"ta_keywords": "crowdsourcing platform amazon;crowdsourcing problem;training data;results;preliminary empirical studies;proper scoring rule;experiments;new approach;approach", "pdf_keywords": "crowdsourcing items;amazon mechanical turk;crowdsourcing data;crowdsourcing;new crowdsourcing framework;novel incentive mechanism;incentive mechanisms;incentive voting;incentive compatible mechanisms;incentive mechanism;labeling tasks;incentive;generic incentive;incentive compatibility;novel labeling interface;reward;participants;labeling problem;rewards;input knowledge;public knowledge;task;payment mechanism;utility function;machine learning;certain choices;training data;optimal sampling strategy;voting;belief distribution"}, "042959b54176ad2c4f9d0966490ec407b6057527": {"ta_keywords": "asymptotic behaviour;expansion;parameter space;small region", "pdf_keywords": ""}, "90dd676184a796e3e5835c8e1f6a632985ce3e89": {"ta_keywords": "energy loss distribution;particle spectrum;energy loss;particle;fluid;analysis;method;fit;new method;one", "pdf_keywords": ""}, "80cb8981af401d9e4df0096626553c514d9e6600": {"ta_keywords": "pyrochlore lattice;optical properties;magnetic field;lattice;field;effect;presence;change", "pdf_keywords": ""}, "12b12ea73652da56023e0e4776211e4f4301f339": {"ta_keywords": "argumentation mining;argumentation;annotation;argument component;document;text;structure;method;novel method;observation;length;purpose;choice", "pdf_keywords": ""}, "77b919c4f4f37415d8f1019b1b04191d46de426c": {"ta_keywords": "fast query execution;unsupervised random walk;random walks;several retrieval problems;query performance;supervised learning;particle filtering approach;truncation approach;large scientific publications;path;speedups;distribution functions;novel approach;approach;accuracy;experiments;ability;advantages", "pdf_keywords": ""}, "e862e5f9a17938f1817017b2730e10463d94fb54": {"ta_keywords": "harmonic trap;dimensional quantum fluid;steady state;dynamics;ground state;system;square;form", "pdf_keywords": ""}, "2826ac3621fdd599303c97cb9e32f165521967b2": {"ta_keywords": "uncertainty scores;uncertainty score;uncertainty evaluation;patient cases;second opinion;data;dimensional representation;method;function;practice;new method", "pdf_keywords": "high expert disagreement instances;uncertainty prediction;expert disagreement;high expert disagreements;label uncertainty;human experts;true uncertainty scores;uncertainty models;machine learning models;uncertainty;uncertainty scores;uncertainty score;uncertainty functions;classification;medical imaging task;disagreement;medical imaging tasks;disagreements;deep learning algorithm;uncertainty inwe;medical imaging setting;high disagreements;high disagreement;single disagreement;noisy images;neural network;label disagreement;noisy image;imaging task;dataset"}, "a7f30bae9303825adbc333a8df8a03398dea5151": {"ta_keywords": "sentiment classification;different sentiment classification models;classification;explicit logic rules;complex inputs;models;visualizations;best models;baseline models;empirical data;accuracy;comparative study;project;efficacy", "pdf_keywords": "improved sentence classification rule;different sentiment classification methods;natural language representations;annotated word representations;word embeddings;natural language processing;neural networks;neural network;neural models;extended word embeddings;complex sentences;sentences;level sentence comparison;semantics;underlying logic rules;embeddings;logic rules;classification;entropic network;boolean versions;empirical improvement;accuracy;entropic network iswe;sentence;words;effectiveness;other models;network;improvement;knowledge"}, "203da29a37a983c487ce75a894b0d70698077bf5": {"ta_keywords": "joint information content;social networks;problematic content;news sources;joint patterns;world media;links;joint analysis;media;behavior;users;coherent pattern;method;list;new method;novel method", "pdf_keywords": ""}, "7f588b1d2a5b199a19a4c3bad6bd5154c7355817": {"ta_keywords": "protein corona;particulate media;protein;camouflage;specific biological properties;active biological activity;composites;blood samples;domains;active activity;ability;form;efficient utilization;widespread availability;widespread application;work;significant challenge;novel strategy", "pdf_keywords": ""}, "44aa9a79cfc9eef9ac3f861cfa58a172cb863bd2": {"ta_keywords": "electron gas excitations;electron gas;dimensional electron gas;orbit interaction;magnetic field;spin;dynamics;effect;interesting system;presence;suppression;case", "pdf_keywords": ""}, "aeffb61024e5ccac5021ca0bf9d199d9196a0521": {"ta_keywords": "coupled congestion costs;population distribution constraints;minimum toll value;toll;fidelity game model;tolls;stochastic dynamics;constraint;constraints;population;players;update algorithm;problem;enfinte authority;results", "pdf_keywords": "player congestion game;mdp congestion game;congestion games;tolling games;tolling constraint;tolling algorithm;coupled congestion costs;congestion costs;minimum toll value;identical congestion costs;tolling;feasible population distribution;game theory;population distribution constraints;average player distribution converges;tolls;optimal distribution;optimal strategy;stochastic transition rules;stochastic dynamics;tolling accuracy;transition probabilities;action costs;critical system constraints;optimal solution;optimal step;population distribution;share traffic;games;diffusive transportation"}, "dc1d1f64503578d9c5d906da4556f631d4178b04": {"ta_keywords": "vehicle collision events;collision data;cnn;prediction;prediction accuracy;joint distribution;rule;extensive data;novel idea;approach;one;approaches;new approach", "pdf_keywords": ""}, "de5057c1da9391269e926d4661d4558072db9f18": {"ta_keywords": "novel parallel classifier;automatic speech recognition system;parallel encoders;novel parallel cascade;parallel cascade;attention mechanisms;encoders;encoder outputs;stream model;datasets;stage training strategy;features;output;data;end;work;experiments", "pdf_keywords": "speech recognition;stream encoder;parallel encoders;automatic speech recognition;attention framework;data streams;end speech recognition problem;amplitude encoder;stream model;encoder features;encoder;decoders;decoder;encoders;data augmentation;attention mechanisms;neural network;signal processing system;parallel cascade;simple data augmentation technique;encoder family;signal sequences;microphone array;training strategy;novel training strategy;stage training strategy;level fusion;data;diverse information;signal"}, "281605579936538ee92bc4b0baad1b83c683c076": {"ta_keywords": "dependency graphs;dependency trees;dependency tree;high dimensional representations;languages;graph;generation;components;separation;novel approach;transformation;transition;approach;power;first stage;set", "pdf_keywords": ""}, "4ae0c4a511697e960c477ea3e37b3e11bf3e0e02": {"ta_keywords": "dimensional domain adaptation tasks;domain adaptation;robust convolutional neural networks;robust convolutional networks;domain transfer;local representations;domains;accuracy;essential features;earlier layers;predictive power;range;model;variety;new generation;cross;methods;method", "pdf_keywords": "robust convolutional networks;wise adversarial regularization;robust image classifiers;imagenet;deep learning context;deep learning;deep representations;wise regularization;convolutional neural networks;deep learning framework;domain classification performance;network classifier;classifiers;domain adaptation;neural networks;adversarial approach;image benchmark dataset;classification;linear classifiers;convolutional neural network;machine learning object;classifier;training epochs;predictive signals;receptive fieldswe;neural network;domain generalization tasks;wise convolutional filter;domain adaptation method;vision models"}, "ce4db7a32724e0abc8afe27f74d33e32e099b8e6": {"ta_keywords": "rich lipid;gene;fit1;gene family;fi1;many human diseases;droplet accumulation;transcript;mice;pigs;similar expression;misexpression;humans;member;useful model", "pdf_keywords": ""}, "7506626f776f211afac2c2d1138aca0e0479e5c3": {"ta_keywords": "gan approach;stochastic clipping;gan;unseen images;monte carlo;dimensional images;images;unique encodings;recovery;method;experiments;new method;usefulness;approach", "pdf_keywords": "generative adversarial network;gans;generative generators;generative;generative capacity;adversarial nature;stochastic clipping;deep convolutional neural networks;unsupervised learning;embedding;generators;gradient descent;images;standard clipping;supervised learning;stochastic;inversions;mapping;model;supervised learning problem;arbitrary precision;iterations;optimization;new technique;novel approach;maximum number;learning process;method;unseenwe propose;combination"}, "712cd873d7370db280f4ceaaf000dc49f76b59fe": {"ta_keywords": "adversarial perturbations;untargeted attacks;seq2seq models;attacks;robustness;source side perturbations;new evaluation framework;additional constraints;evaluation methods;output sequence;output;assumption;changes;meaning;more meaning", "pdf_keywords": "adversarial source word;adversarial source;adversarial training;adversarial perturbations;adversarial input;adversarial attacks;adversarial examples;adversarial loss function;adversarial input sequences;semantic similarity evaluation scheme;translation models;semantic similarity;automatic evaluation metrics;systematic attacks;natural language models;semantic equivalence;semantics;word substitutions;semantic models;seq2seq models;attacks;automatic metrics;deep unordered composition approaches;sentences;natural language;source side perturbations;robustness;equivalences;new evaluation framework;evaluation"}, "1c8d9d5558dc43f3505fa37fc50247e3ce0d2f54": {"ta_keywords": "small deconfounded observational dataset;deconfounded data;average treatment effect;cancer;genetic mutation;patient setting;world dataset;trial;samples;accuracy level;new approach;ate;approach;benefits", "pdf_keywords": "unobserved confounder;deconfounded confounder;confounders;binary confounder;confounder;single confounder;deconfounded data;causal effects;deconfounded samples;treatment effect;interventions;conditional data;average treatment effects;cancerwe;observational data;associated threshold value;predictors;specific effects;assumption;average treatment effect;estimation;genetic mutation;longitudinal data;intrinsic assumptions;theoretical data;estimators;dichotomic data;data;significant effect;estimation accuracy"}, "4d86b32ea80e2d9df2283fac39892d6dbd87ea87": {"ta_keywords": "support vector machine;error pattern recognition method;classifiers;pattern recognition problems;discriminant functions;geometric margin maximization;geometric margin value;pattern;general class;support;new minimum;prototype;novel approach;new method;wide range;effectiveness;experiments", "pdf_keywords": ""}, "8786ddc38ae0763e772337bf9331436252452918": {"ta_keywords": "fake news detection models;fake news detectors;entity bias;chinese datasets;entities;entity;extensive offline experiments;future data;online tests;english;contents;direct effect;cause;effect perspective;base;training;contribution;framework;performance", "pdf_keywords": "novel entity debiasing framework;fake news detection models;entity bias;fake news detection scores;unintended entity bias;various fake news detectors;fake news detection;synthetic fake news detectors;fake news detection methods;fake news detector;false news detection;biased information;fake news pieces;news detection;news veracity;online news ecosystem;news contents;news pieces;real world news;entities;fakefake news;entity;fake news;novel entity;false news;false information;real news;newscop;past news;diverse models"}, "9d0e4e9c9343b85311b1adff145fdbdfb69486ff": {"ta_keywords": "relativistic astroparticle emission model;cosmic ray emission;galactic center;resolution observation;observations;other theoretical models;framework;results", "pdf_keywords": ""}, "cc74ef901219dfd26efbbb8b7b87d1b7b7d38634": {"ta_keywords": "document;documents;information processing;brain;random variable;structure;definition;optimal way;hypothesis;study;field;method;particular importance;problem", "pdf_keywords": ""}, "cd06dfa789bfe491130ac7440e55d9d407396a43": {"ta_keywords": "accelerated methods;coordinate descent methods;optimization problems;acceleration strategy;original algorithm;conjugate directions;coordinate directions;big dimensions;methods;inexact variants;augmentation;variant;set;new class", "pdf_keywords": "stochastic coordinate descent method;stochastic descent methods;stochastic descent;stochastic spectral coordinate descent;stochastic descent method;stochastic descent algorithm;stochastic spectral descent;stochastic descent problem;stochastically convergent algorithm;coordinate descent;stochastic dual ascent problem;theorems theorems theorems;theorem theorem theorem theorem;theorems;underlying stochasticity;dual ascent algorithm;theorem proving theorems;theorem;stochastic;stochasticity;stochasticwe;algorithms;convex optimization problems;stochastic dependence;threshold theorems;stochastic parameters;iterations;acceleration strategy;theorems abouttheorems;partial spectral information"}, "aacaad6ab396e085799052b1a667c965d6465e32": {"ta_keywords": "protein emulsifying activity;protein interaction;emulsions;protein mobility;protein protein;protein;protein molecule;disulfide bonds;sulfhydryl bonds;mechanism;decrease;activity;addition", "pdf_keywords": ""}, "d5810f15cfdd59da549ffa648c5a05d806d94eb7": {"ta_keywords": "evidence supports;fact;user study;platform;claim;user friendliness;final verdict;transparency;predictions;system;architecture;time;piece;choices", "pdf_keywords": "journalistic corpus;factual claims;large scale fact;journalists;journalistic setting;large document collection;evidence;wikipedia;documents;content content;news room;russian federation;sentences;russian satellites;neural network;novel fact;dataset;checking;claims;fact;information;data;accuracy;relevant results;claim;platform;feature;fever;user study;useful examples"}, "a5148776955ef523de318a2fb45f8256e966b98e": {"ta_keywords": "labeling scheme;standard labeling scheme;labeling labels;labels;labeling;linear combination;collection;use;advantage", "pdf_keywords": ""}, "05c2bb89a5c42ad7932420bb39df2e566df6e1ec": {"ta_keywords": "java library;parallel version;friendly editor;development;new article;user", "pdf_keywords": ""}, "2127bea25859ba9c5997e2d15e17899a75ef6cb3": {"ta_keywords": "big code;data mining;data analysis;tools;development;short seminar;topic;community;process;results;talk;power;participants;part;set", "pdf_keywords": ""}, "3e2bac2abfb5b33a43fe56db5a868e17e38c616a": {"ta_keywords": "high performance computing classes;machine learning;project workshops;learning;project;examples;novel approach;results;approach;use;set", "pdf_keywords": ""}, "baf47cd0b471a9bb7b2230fec0b680fc9b3c4783": {"ta_keywords": "pragmatic speaker model;natural language instructions;speaker models;traditional speaker model;alternative descriptions;candidate descriptions;base listener;tasks;model uses;model;generation;interpretation;variety;state;art", "pdf_keywords": "explicit pragmatic inference;instruction generation tasks;pragmatic models;pragmatic inference procedure;pragmatic speaker model;pragmatic reasoning;pragmatic listener;human speech;input descriptions;instruction;rational listener instructions;linguistic structure;structured representations;rational speaker models;natural language;computational linguistics;coreference;important linguistic phenomena;linguistic phenomena;natural language processing;language;sound representations;alternative descriptions;sentences;decoder;navigation task;rational speaker;rational listener;primitive actions;human evaluation"}, "00cc6deb3cf2c9281ddcf4875aad3ee14c92e52f": {"ta_keywords": "entity recognition;machine translation systems;languages;distributional statistics;dataset;system;novel approach;matches;points;average;variety;methods", "pdf_keywords": "entity recognition;entity recognition problem;large corpus languages;machine translation;translation tokens;annotation projection;source entities;natural language processing;annotations;shelf machine translation systems;source entity;target entity span;target language;corpus;machine translation errors;entities;entity;linguistic data;rich target language;entity span;parallel corpora;nonlinear linguistic data processing;corpora;linguistic representation;nonlinear linguistic data;rich source language;natural language;monolingual baseline;ner languages;novel statistical parser"}, "be360de73689dc4af56f7adcee7e38d7acfed1e1": {"ta_keywords": "aggregate orderings;rankings;orderings;fairness;such lists;match;lower bounds;race;collection;new game;set;size;number;certain properties;problem", "pdf_keywords": ""}, "0f5bb9ae0c060b349597c0b2582bf271a5a2156a": {"ta_keywords": "long range tags;neural models;single text;entire sentence;model;terms;approaches;accuracy;other approaches;art model;speed;basis;new state;factor", "pdf_keywords": ""}, "ce268e0942ce0d1f6942e4d7e7e2aa6464f1b577": {"ta_keywords": "proficiency speakers;pronunciation;acoustic evidence;english proficiency show;recognition accuracy;same recognition accuracy;knowledge;study;method;improvement;experiments;degrees;system", "pdf_keywords": ""}, "cb53f9558bd13c853026f97dce3bbe3d989ca97d": {"ta_keywords": "serious game;fallacies;code;language;case;method", "pdf_keywords": ""}, "57dd2bd5fb6677191f9b36b589c91bb171e217ff": {"ta_keywords": "behaviour", "pdf_keywords": ""}, "a4dd375c18709b1554249cc5cb88d8ba6acfea10": {"ta_keywords": "machine translation;brief review;field;current state;factors", "pdf_keywords": ""}, "a95400c70c4beb609c77cc500677b2f1ed852e8e": {"ta_keywords": "automatic question generation;level inference;questions;motivated phrase;text;context;question;answer;prior work;information;discrete structure;approach;system", "pdf_keywords": ""}, "63d99a61e798d7cb714f336a8d581ae2b75672ee": {"ta_keywords": "machine learning system;novel autoregressive model;novel probabilistic model;model;behavior;system", "pdf_keywords": "contrastive language prediction;resource speech challenge;deep cluster network;deep cluster;deep cluster method;discrete speech representations;contrastive learning;discriminative representations;linguistic features;discriminative representation;clustering;label prediction;phonetics;bert kernel;neural network;clustering step;recognition;segmentation;novel autoregressive classifier;convolutional neural network;dataset;prediction;supervised supervised learning problem;learning;learning problem;kmeans;encoders;discrete representations;challenge;convolution modules"}, "d4305b3bf233e5f192a5d17dde114b771b621d92": {"ta_keywords": "power spectrum;free energy;background field;particle;estimation;field;length;method;new method;case", "pdf_keywords": ""}, "de3c3eb590065a6d78ec8566161f8236ab2a7435": {"ta_keywords": "parallel translation modes;parallel translation study;single ghz frequency;hybrid;flexibility;design;high degree;large number;results", "pdf_keywords": ""}, "ffb562d3ac7d86b5c527863f5a3e72e1aa22a809": {"ta_keywords": "heterogeneous rational agents;prediction process;physical system;agents;behavior;micro;system;novel approach;method;new method;idea;use", "pdf_keywords": ""}, "076b2ba158c35bd2941769864ce7455cf76ecd8e": {"ta_keywords": "discussion initiator;review discussions;causal effect;discussion;melix discussions;conditional causal effect;other reviewers;strong causal causal causal;trial;peer;outcome;testing;initiator;opinion;large scale;dynamics;goal", "pdf_keywords": "conference peer review;peer review;discussion process;social influence;group discussions;review discussions;discussion initiators;reviewers actions;discussion initiator;right peer review;consistent discussions;blind peer review;reviewers;authors discussions;discussions;subsequent contributors;discussion;peer;biases;strong influence;consensus;other reviewers;participants;bias;behaviour;positive reviewer;inherent bias;negative reviewer;mostthe discussion;complex research"}, "80b747af8d86541cf53198519c8fa51109eed4f9": {"ta_keywords": "unlabeled data augmentation;unlabeled supervised augmentation;data augmentation;complex data augmentation;data classification;ojta;uda;popular implementation;effective approach;monte carlo method;efficacy;contrast;simplest form", "pdf_keywords": "unlabeled data augmentation;unlabeled data augmentation method;sequence tagging tasks;novel data augmentation strategies;naive data augmentation methods;data augmentation;benchmark text classification;natural language tasks;uniform random word replacement;labeling tasks;translation machinery;supervised learning tasks;nonlinear data augmentation;unsupervised learning tasks;unlabeled data;text classification;classification tasks;word label distributions;deep learning;standard supervised learning;augmentations;supervised learning;empirical classification datasets;unsupervised learning;other words;supervised classification;supervised identification;text;natural language articles;significant improvements"}, "b033400e9a80915a928f4603582e5e8bf7656a85": {"ta_keywords": "multimodal navigation;multimodal navigation systems;multimodal data mining;navigation;unimodal baselines;datasets;data;performance;effective strategy;assessment;use;combination;single step;new method;ability", "pdf_keywords": "unimodal navigation models;navigation tasks;multimodal datasets;natural language navigation;navigation agent;navigation task;visual navigation;multimodal techniques;navigation benchmark;navigation subtask;navigation;multimodal domains;navigation success;scene;memory;agent;neural network;egocentric question;tasks;human brain;transverse navigationwe;common perception model;questions;benchmark datasets;natural language processing;performance;new evaluation;destination;representations;action"}, "c0099a15bd3251083c62ebd47c9705a16309b974": {"ta_keywords": "phase shift;algorithm;simple algorithm;image;point source;time representation;generation;large scale;combination", "pdf_keywords": ""}, "a16ae67070de155789a871cb27ecbf9eaa98b379": {"ta_keywords": "annotation separation;textual textual sources;textual sources;annotation;text;evaluator;end evaluator;art text;recommendations;method;best way;quality;state;problem;ability", "pdf_keywords": "text evaluations;many natural language generation systems;untrained evaluations;human evaluations;current human evaluation methodologies;intrinsic human evaluations;theart text generation;training evaluators;untrained evaluators;human text;empirical evaluation;novel evaluation setup;evaluators training;evaluator;upcoming evaluation task;evaluation;annotated news articles;quality text;textual identity;evaluation metrics;unstructured text;mechanical turk;evaluators;humanlikeness;text;large language models;texts;annotator agreement;human attributes;training method"}, "ece56ab633f11d1592a3d4f9386412d3f48fcf95": {"ta_keywords": "scalable crowdsourcing process;crowdsourcing;linguistic accuracy;implicit warrants;plausible warrants;warrants;authentic arguments;news comments;claims;novel approach;approach;process;2k;general context;variance;novel method", "pdf_keywords": "natural language argumentation;natural language arguments;argument reasoning comprehension task;argument reasoning comprehension;argument comprehension;annotated arguments;human reasoning tasks;large crowdsourcing study;crowdsourcing;natural language comprehension;argumentative comments;argument reasoning problem;novel crowdsourcing process;scalable crowdsourcing methodology;crowdsourcingwe;debates;scalable crowdsourcing process;reasoning comprehension;annotators;annotations;arguments;debate;annotated items;large corpus;argument;debate title;reasoning;challenging task;semantics;relevant premises"}, "2232808cf3161ca4c434126e35f47ee33c0c8219": {"ta_keywords": "teacher model;student model;explanations;intuition;student;mere value;training;value;approach;framework", "pdf_keywords": "attention student models;explanation generation;question answering;human reasoning;student models;argumentative models;explanations;machine learner;various explanations techniques;explanationswe;teacher model;student model architectures;student model;several candidate explanations;simple learning strategy;text classification tasks;explainability;prediction class;text classification model;learning;teacher frameworks;attention;multitask learning;attention regularization;efficient learning strategy;supervised learning;sentiment analysis tasks;text classification;student;different explanations"}, "228f2efe7b06b6db3b2c6c0a61d7b33daee1d641": {"ta_keywords": "word sense disambiguation;semantic similarity;synsets;unsupervised way;new unsupervised system;datasets;target word;sense;input word;different datasets;relevant sense;sparse mode;sentence;system;respect", "pdf_keywords": ""}, "301352755a94d7524312b7c7f2fab7d3fd3d334d": {"ta_keywords": "qualitative conditional preferences;agent aggregation;multiagent aggregation;probabilistic uncertainty;individual agents;single agent;agent;tractable approximation;dominance;formalism;experimental setting;context;concept;use", "pdf_keywords": ""}, "035595ebf6821031a543ee1c30386a6230fc7a41": {"ta_keywords": "novel speaker diarization algorithm;consistent diarization outputs;speaker;supervised self;attention mechanism;outputs;buffer;notation algorithm;current chunk;correlation;approach;experiments;efficacy", "pdf_keywords": "online speaker diarization system;online speaker diarization algorithm;end speaker diarization;blind speaker diarization problem;speaker detection;speaker diarization error;novel speaker detection;speaker tracing buffer;online speaker;speaker verification;speaker learning;speaker permutation information;online diarization;comparable diarization performance;speaker;proposed speaker;short utterances;variable chunk size training scheme;neural network;new speaker;chunk buffer;utterances;deep learning;active speaker;attention layers;standard speaker;several input frames;speech;utterance;chunk"}, "8328508dc12c295165f997e02d74d00a42971c01": {"ta_keywords": "semantic parser;typical context modeling methods;context modeling methods;domain datasets;grammar;datasets;best model;significant improvements;state;top;art performances", "pdf_keywords": ""}, "d36e39aedd802aea4be1ea303c70dc56e97dbc3c": {"ta_keywords": "consistent summarization summary;summarization summary;summary;methodology;hypothesis;questions;similar answers;source;creation;sound", "pdf_keywords": "conditional text generation;quality summaries;abstractive extractive summarization model;summarization dataset;neural text summarization;topical summaries;summarization;summarizationwe;abstractive text summarization;abstractive summarization;summarrecent advances;automatic generation;summaries;natural language inference;annotation interface;natural language sentences;new automatic evaluation protocol;standard automatic evaluation metrics;natural language processing;natural language;summary;sentences;factual consistency;factual inconsistencies;factual information;reference text;factuality;human evaluation method;semantic information;original text"}, "6c6975750207f787c318627ff7cb63a649165a8d": {"ta_keywords": "dimensional visual displays;intelligent tutoring agent;display representation;novel statistical learning algorithm;user interface layout;free grammar learner;learning algorithm;temporal information;context;algorithm;real world domains;synthetic domains;effectiveness;experimental results", "pdf_keywords": ""}, "1a671afdac8e7b759cf3b5ec7d03d485c76a989c": {"ta_keywords": "automatic speech recognition framework;novel encoder;encoder;decoder;mask;novel model;novel feature;end", "pdf_keywords": "end speech recognition framework;speech models;automatic speech recognition;speech recognition;decoder model;encoder;iterative decoding;decoder;decoder framework;mask ctc;speech;mask;predicting;standard autoregressive model;conditional uncertainty;nonlinear linguistic tasks;predict objectives;novel generative flow models;asr;classification scheme;output tokenwe;end model;novel model;novel generative flow;inputs;output;mollenauer machine learning algorithm;tokens;model;estimation"}, "38ff6cf441050a1db10df85ac0771ccc88dea748": {"ta_keywords": "review process;peer;conflict graph;strategyproofness;authorship;reviews;natural efficiency property;unanimity;theoretical framework;requisite property;flexible framework", "pdf_keywords": "efficient peer review;strategyproof peer review algorithm;competitive peer review;peer review;conference peer review;conference peer review data;reviewer assignments;review scores;more general connected review graphs;reviewer pool;paper reviewers;nips peer review process;simple review graph;ranking;academic paper review;reviewers;reviewer;total ranking;peer;ranking list;rank algorithm;aggregation algorithm;review article;aggregation;aggregation rules;strategyproof algorithm;standard peer;rank;evaluation;quality evaluation"}, "97ca917f66d60f5277651a74f233804b03cb5e3d": {"ta_keywords": "morphological segmentation;morphological segment;segmentation;morphologies;segment;deep convolutional neural networks;distinct segments;short time;examples;method;types;number", "pdf_keywords": ""}, "49989dc4d77b9df775b284ab7682ba76c080be12": {"ta_keywords": "spatiotemporal classification;spatiotemporal entropy;structured objects;hidden model;objects;object;intrinsic structure;underlying distribution;model approach;novel approach;approach;calculation;use;problem", "pdf_keywords": ""}, "51d735419392dbe961c60bff7eee95388b8d6d3d": {"ta_keywords": "grammar;dependencies;induction;method;novel method;problem", "pdf_keywords": ""}, "b26ca2bb882c2d3526fb4ac7f544fb87c39ded62": {"ta_keywords": "pursuit method;improved kernel gradient;handwritten image datasets;orthogonality constraints;recognition experiments;basis vector set;speech datasets;method;efficiency", "pdf_keywords": ""}, "2cd7c3ed5a06c461b259694376820dcfcfbe94a9": {"ta_keywords": "generative neural models;generative neural model;effective inference;search space;pruning function;novel approach;accuracy;charniak;choniak;approach;problem", "pdf_keywords": "neural generative parsers;generative parsers;generative parser;model parsers;constituent parser;treebank;parser;beam search;generative model;penn treebank;generative representation;level beam search;level beam search technique;level search;computational linguistics;search space;underlying language;large beam sizes;search procedure;constituent;pruning;level candidates;forward score estimation;successors;enhanced candidate selection strategy;beam sizes;prior state;powerful tool;candidate selection strategy;model"}, "19a3af37df22c7c646cc99efad3af96cda6e80f0": {"ta_keywords": "multimodal translation task;nihon;system", "pdf_keywords": ""}, "51546584aa394d159edcc08f2412ae30dd316f6c": {"ta_keywords": "prediction depth;deep models;prediction;interpretable groups;computational difficulty;complexity;difficult examples;accuracy;input;understanding;groups;relationship;problem;measure;framework", "pdf_keywords": "prediction depth;smaller prediction depths;lower prediction depth;deep prediction model;mean prediction depth;depth predictions;deep models;deep learning;deep model;deep learning models;deep network;prediction;deep neural networks;predictions;softmax;deep learning network;deep learning model;prediction entropy;inference learning curve;adversarial input margin;nonlinear predictability;depth;training algorithms;deep learning problem;validation depth;example difficulty;benchmark machine;machine learningwe;machine learning models;larger accuracy"}, "ca73cc17ca69fa0807e566c22c7c1711da916281": {"ta_keywords": "approximate nearest neighbor search;exact search method;high dimensional spaces;small world approach;popular small world approach;nonmetric spaces;best tradeoffs;empirical analysis;efficiency;different approaches;performance;effectiveness", "pdf_keywords": ""}, "56501a3441c2074bbbbe31015d6d41c57d9d285b": {"ta_keywords": "paraphrastic sentence representations;paraphrastic sentence models;inference;languages;language;tasks;models;code;parallel data;users;use;performance;system;variety;own state;art", "pdf_keywords": "paraphrastic sentence embeddings;own paraphrastic sentence representations;unsupervised english semantic similarity;unsupervised semantic similarity;semantic similarity tasks;sentence embeddings;universal sentence embeddings;semantic similarity;sentence similarity score;paraphrastic sentence;bidirectional sentence embeddingwe;sentencepiece models;sentence encoder;sentencepiece generation;embeddings;sentence pairs;neural machine translation;paraphrase;sentencepiece model;semantic inference;encode semantic information;parallel sentences;unlabelled sentences;sentences;similarity;sentence shapes;bilingual parallel data;intrinsic similarity;multilingual sentence;sentence structure"}, "ce458be308f2c75edc53366272fa6e744fda7902": {"ta_keywords": "word sense disambiguation;word sense;new unsupervised system;unsupervised way;similarity;unsupervised system;target word;different datasets;input;system;best features;ability", "pdf_keywords": ""}, "9195186cf44876d0d1d03b87756c464b760a7f4e": {"ta_keywords": "comprehensive research;results", "pdf_keywords": "voice activity detection;audio segmentation;short metaterm patterns;end speech recognition problem;decoder;metaterm patterns;long context modeling;search;deep learning;unified beam search approach;large corpus;decoder modules;search algorithm;subsequent subtask;modulation;speech translation;encoder;attention;several tasks;neural network;segmentation;dimensional pitch features;segmentation scores;overall st task;art segmentation models;level data;student model;context;dimensional data mining;novel machine translation system"}, "4fd6488e38043d680c592170bf7f651c079d0e98": {"ta_keywords": "small cell networks;macro base stations;micro base stations;novel network architecture;base stations;network;network structure;mobile users;larger cells;static users;macro;users;performance;presence;power", "pdf_keywords": "heterogeneous cellular networks;small cell networks;small cell network;cellular networks;heterogeneous network model;new network model;novel network model;different network models;network architecture;combat handoff traffic;network design;frequent handoff;tier network structure;network characteristics;same network model;tier network;lte;layer lte network;mean downlink throughput;macro base stations;handoff duration;hetnet network;network parameters;mobile users;micro base stations;underlying network;network;high mobility;user throughput;mobile"}, "4ab7b65e1a3b76eb3db064523c862f1325e04971": {"ta_keywords": "different speech recognition systems;parkinson;corpus;vowel;error rate;control speakers;disease;speakers;performance;systems;manner classes;errors;developmental stage;individuals;differences;people;results;rate;moderated correlation;work", "pdf_keywords": ""}, "3f79b71b887d2ccb733926867a62f69902fcbdab": {"ta_keywords": "adaptive ontology mapping approach;ontologies;constraint satisfaction solver;structural similarity;similarity;mapping accuracy;adaptive method;vector space model;artificial intelligence;interactive activation;neural network;gation theory;approach;paper;competition;estimator;harmonies;first measures;performance", "pdf_keywords": ""}, "7954b31ce1f6ad935808b7cf62c34bc118d20a9a": {"ta_keywords": "heterogeneity;correct heterogeneity region;decision process;high variance;world healthcare datasets;regions;decision;outcome;likelihood;assumed context;maker;novel method;method", "pdf_keywords": "causal inference framework;heterogeneity;causal inference problem;conditional bias;empirical evidence;large causal effect;underlying decision process;causal forests;heterogeneous preferences;causal perspective;empirical discovery;bias;collective decision problem;generalization;empirical data;empirical objective;bayesian inference;arbitrary conditional expectations;treatment decision;decisions;outcome model;prior knowledge;decision trees;large cohort;variation;different decisions;random forests;underlying condition;participants;hypothesis classes"}, "4bf5084d21f681c09409bd890daa4bf1c4f9b691": {"ta_keywords": "treatment platelet reactivity;periprocedural myocardial infarction;same average platelet count;effect;patients;occurrence;frequent events;statistical significance;sample;study", "pdf_keywords": ""}, "c3490ec9b8f695bed2187fb4a4164b1509389ca8": {"ta_keywords": "stochastic evolution;neural network;particle;simulation;nodes;new method;method;number;use", "pdf_keywords": ""}, "7d94d4c6b2db490e08beabd2661df009f1a06d6c": {"ta_keywords": "crowdsourcing project;source synsets;synsets;source database;data;topological information;world;web;project;collection;entries;results;task", "pdf_keywords": ""}, "02a757548da783d43ffcfd4b60f2cbb0ac71a4bc": {"ta_keywords": "subjective fairness elicitation;subjective fairness;fairness constraints;individual fairness;fairness;criminal dataset;efficient algorithm;individuals;accuracy;compas;task;sample;generalization theorems;behavioral study;provably convergent oracle;framework;preliminary findings;error", "pdf_keywords": "empirical fairness constraints;algorithmic fairness;fairness constraints;fairness elicitation;empirical fairness loss;fair machine learning;fairness preference principle;partial fairnessin;subjective fairness;metric fairness;fairness measurements;partial fairness loss;fairness loss distribution;fairness loss;classical fairness loss distribution;bipartite fairness measurements;fairness;fair distribution;fair machine;binary judgements;efficient oracle;binary classifiers;algorithmic framework;algorithms;fast oracle;learning problem;classification;elicitation;constraints;classifier"}, "f7247fefc9efb57ace33425a2981d6aba08da3b7": {"ta_keywords": "statistical dialogue management;statistical dialogue framework;dialogue management;dialogue process;dialogue behavior;intention dependency graph;observable decision process;deterministic graph structure;prdm;simulation model;simulation;framework;conventional rule;system;method;idg;optimization", "pdf_keywords": ""}, "23e42bc79f10234bdceef31441be39a2d9d2a9a0": {"ta_keywords": "knowledge base reasoning;logical rules;differentiable rules;learning problem;features;new feature;discovery;underlying model;structure;novel approach;novel novel feature;approach;approaches;first order;process", "pdf_keywords": ""}, "06064617f152f5032137204aec739c0c82dbb836": {"ta_keywords": "speech separation;second chime;recognition challenge;tracks;tasks;challenge;world dataset;task;set;adaptive approach;performance;baselines;novel approach;detailed comparison;scale;novel;power;approach", "pdf_keywords": ""}, "14047a24b23d9e392776229f9d40bee9f8243e4c": {"ta_keywords": "active sensors;connected sensor networks;tracking;algorithm;data representation;discrete variables;associated numerical model;accuracy;time;high performance;problem;method;paper;novelty;set;terms", "pdf_keywords": ""}, "0dd1b9ad5aeda250dc61f38cf7018e7a014e91c0": {"ta_keywords": "dimensional harmonic oscillator;harmonic field;dimensional gaussian field;dynamics;field;numerical study;law dependence;power;strength;effect;large value;results;agreement", "pdf_keywords": ""}, "a67face220a88b6b36f3343a6a017a3536562d5b": {"ta_keywords": "guessing game;brain;accuracy;patterns;game;agent;probability;search;problems;example;shape;fact;new class", "pdf_keywords": "challenging question generation task;specific question generation task;question generation problem;guessing games;visual questions;dialogue agents;deep learning;neural neural neural machine;dialogue;simulated game;neural network;semantic knowledge;deep predictive power;representations;current context;answer tokens;context;games;textual modalities;knowledge;encoder;agent;dimensional representations;task;representation;agents;questioner;harder tasks;input;gold dialogues"}, "970383c0a41d7ae1ec4b8abaa3033778203377b9": {"ta_keywords": "stochastic input;stochastic process;time stochastic processes;neural networks;accurate prediction;simulations;model;large number;combination;experiments;scale;number;ability", "pdf_keywords": "synthetic corpus;automatic speech recognition;noisy sentences;quizbowl;eval2000 transcription rate;speech;natural language processing;human corpora;deep learning;automatic data acquisition;deep averaging networks;deep neural network;noisy data;decoding;eval2000 test;test;interrogation;unknown words;text;source text;human questions;factoid question;words;questions;challenging task;confidence outputs;context word;neural model;supervised version;qa"}, "3193766c0439ff29a0a3d176628f8144d6e77231": {"ta_keywords": "behaviour", "pdf_keywords": ""}, "b38ec68c8bab031138606a9b00e9d817be3e1d22": {"ta_keywords": "topic models;mixed membership stochastic block models;entity link;topics;entities;entity;links;model;text;data;aspects;ppi;improvements;ability", "pdf_keywords": ""}, "3f256b31d446015d8cd0f9f3996009cdf2034c5e": {"ta_keywords": "joint language identification;speech recognition;connectionist temporal classification;several multilingual benchmarks;hybrid attention;different languages;independent network architecture;language;novel language;independent architecture;speech;ska data;architecture;ska;comparable performance;model;art performance;state;experiments", "pdf_keywords": ""}, "c56aced0f0c5cfebefadb530cb08d736c3ac5c05": {"ta_keywords": "retrieval;framework", "pdf_keywords": "new retriever code generator;code summaries;source code generation;code search;code generation;unstructured source code encoders;deep neural code prediction;deep bidirectional neural code prediction;source code;generative code generation problem;dense retrieval technique;retrieval architecture;retrieval database;sparse retrieval baselines;source codes;natural language sources;codebases;natural language descriptions;summarization models;lambda source code;code;retrieval;nonparallel code;redcoder;programming languages;popular programming languages;new retriever module;natural language description;codeswe;search engine"}, "a4ce6cd06bc73d81651f7888efa4337fd82a60f0": {"ta_keywords": "unknown word detection;spoken system;unknown words;system;systems;state;method;user;novel method;use;problem;fact", "pdf_keywords": ""}, "04b364d56995de2228cb1acfb320a935cbcf4440": {"ta_keywords": "weakly supervised segmentation;robust trust region approach;supervised training;large datasets;prior knowledge;loss model;quality;sense;order generalization;classic chain rule;order;approach", "pdf_keywords": "weakly supervised segmentation;supervised segmentation;semantic segmentation results;segmentation weights;semantic image segmentation;segmentation;deep features;supervised learning;deep learning;level segmentation model;supervised learning problem;supervised learning algorithm;regularization;gradient descent;new robust trust region approach;training algorithm;standard gradient descent;trust region approach;training data;neural networks;regularization model;loss optimization;neural network;trust region;salient features;computer vision;learning procedure;training;clustering;robustness"}, "fa774368fcf51cc0fa1bfda59b6a606e163c64b1": {"ta_keywords": "symmetric linear models;symmetric linear model;symmetric linear system;finite dimensional systems;asymmetric linear inequalities;linear inequalities;dimensional linear model;linear model;linear combination;synthesis;construction;rigorous approach;approach", "pdf_keywords": "symmetric counting constraints;symmetrical counting constraints;stochastic switching protocol;counting constraints;counting constraint;certain global counting constraints;discrete state count problem;dimensional switched system;symmetric constraints;finite transition system;state counting;arbitrary input states;global switching protocol;arbitrary state;dimensional transition system;discrete symmetry;deterministic discrete instances;global counting;counting technique;discrete symmetrywe;discrete state;dynamical systems;discrete states;counting function;counting problem;global constraints;natural systems;integer programs;assignment sequences;controllability"}, "9abf14d4f89bf6c297e1bbd637cd54e1a0335e71": {"ta_keywords": "bibliotor attributions;attribution;novel unsupervised model;copies;same document;classic paper;model;first edition;number;total number;different kinds;richardson", "pdf_keywords": "bibliographers;bibliographer;attributions;authorship;attribution;compositor attribution;literature;copy text orthography;shakespeare;modern literature;important publication;modern diplomatic spellings;original document;sources;compositors;text;analyses;refined spelling;manual judgements;compositor;compositor preferences;ancient documents;original word;manual transcriptions;book;diplomatic spacing preferences;general preferences;analysis;preference;accuracy"}, "ffdbd7f0b03b85747b001b4734d5ee31b5229aa4": {"ta_keywords": "text compression;backpropagation;soft prompts;learning process;performance;parameters;flow increases;proc;effective method;other methods;power;method;number", "pdf_keywords": "input representationwe introduce prompt tuning;large language model;language models;prompt tuning;prompt models;dimensional language models;language model;soft prompt;soft prompts;natural input text;prompt training;span corruption tasks;model tuning;myeidwe propose prompt tuning;task learning tasks;natural language;simple sentinel encoding;prompt tokens;prompts;shot prompts;text;input text;string representations;prompt data;single prompt;prompt;simple span corruption objective;input;backpropagation;fewshot prompt design"}, "1a3fcb1e2a416cbc79a011f1a1916aa53f7a2a09": {"ta_keywords": "human emotional states;body postures;stick figures;stick figure configurations;emotion;abstractions;representation;participants;particular action;specific action;effectiveness;intent;terms;number;meaning;configurations;fraction", "pdf_keywords": ""}, "e63c9eb5b623baad0a7805e839e5d9fabad37fce": {"ta_keywords": "descriptions;information;novel method", "pdf_keywords": ""}, "ab8174a1f1810c1122f90649276a552d2eb1ccd4": {"ta_keywords": "particle;deep state;particles;condensate;energy;bose;calculation;bec;method;new method;use", "pdf_keywords": ""}, "7618c65685c98fa88526555ae3f62cd5645066ad": {"ta_keywords": "relation entailment;relation hierarchy;database;new method", "pdf_keywords": ""}, "6e78e32481218e9391a88e6d0e30c0062ae71bec": {"ta_keywords": "gesture generation;unique gesture style;style transfer;generative models;pose;audio;speaker;mix;new dataset;stage;model;new model;mixture;conditioning", "pdf_keywords": "gesture generation;gesture style;gesture animation;gestures;audio poses;gesture detection;gesture;common gesture space;generative models;speech generator;style transfer;speaker models;generative adversarial networks;speech model;style embeddings;speaker styles;best gesture;appropriate gesture;generated videos;conditional adversarial network;specific speaker;pose;adversarial training method;unique styles;audio tasks;new speaker model;animations;audio;new speakerwe;speech"}, "f3bca263a92b69c6da872a9a3268f260ba43f690": {"ta_keywords": "recurrent neural network language models;large vocabulary continuous speech recognition task;discriminative method;discriminative criterion;entropy;rnn;training criterion;lm baseline;words;correct words;lm;training;paper;ce;method;effect;experiments", "pdf_keywords": ""}, "53880036fb85cc737103c480c613e1912c416010": {"ta_keywords": "structured learning system;structured learning systems;structured documents;framework;new framework;algorithm;construction;building;information;new construction;builders;system;single master", "pdf_keywords": ""}, "a2f4731258830c76af7e3bdb96c4488823219585": {"ta_keywords": "robust automatic speech recognition;blind source separation;frequency masking;reverberant environments;target speech;reverberant environment;recognition performance;sparseness;noise;interferences;asr;frequency;sss;technique;method;time;experiments;results", "pdf_keywords": ""}, "341c72f55a89572915aa476db7f525c2e0b60eba": {"ta_keywords": "cluster similarity;similarity measure;algorithms;correlation;characterization;arccosine;online experiment;novel method;practical importance;approach;requirements", "pdf_keywords": "cluster similarity indices;cluster similarity index;cluster similarity;clusterings;external cluster similarity;candidate clustering algorithm;similarity indices;similarity index;similar clustering;clustering algorithms;reference clustering;graph similarity indices;cluster;clustering;clustering algorithm;clustering process;clustering iff;cluster size;cluster sizes;proteinaceous clustering;cluster size baseline;cluster entropy;validation indices;cluster size specification;clusters;random clustering;similarity measure;algorithms;important indices;best index matters"}, "143183584a8ebaad93490f4550295a9cb6cf9817": {"ta_keywords": "scalable probabilistic logics;practical data storage;data storage;data representation;data representation problems;practical implementation;pseudo;form;topic;problems;methods;introduction;brief introduction;demo;hands", "pdf_keywords": ""}, "fc78af26fd7644867af1abb8fbf2c37b47ad8257": {"ta_keywords": "downstream lexicon induction;fidelity lexicon induction;dictionaries;shot tagging performance;standard evaluation dictionary;language pairs;hub language;fidelity cross;english;literature;hub;throughput;standard methods;new set;choice", "pdf_keywords": "multilingual refinement;large multilingual corpora;multilingual word pairs;lexicon induction performance;several new language pairs;several language pairs;arbitrary language pairs;multilingual systems;language pairs;diverse language pairs;natural languages;target language;morphological tagging;computational linguistics;new dictionaries;multiple languages;large parallel corpora;distant languages;language context;dictionaries;machine translation;languages;bilingual word translation;performance machine translation system;embeddings;different languages;distant languages experiment;language;iterative refinement methods;hub language"}, "682660c7a014e806b924fdf1a2a3d999a9ac13cf": {"ta_keywords": "abstractive summarization;summarization performance;neural language generation stage;standard abstract language;abstract words;standard parser;abstract;standard parses;decoder approach;source document;standard encoder;novel method;generation;paper;method;standard collection;set", "pdf_keywords": "abstractive summarization;summary text;seq2seq summarization;summarization resultswe;summarization;abstract meaning representation;explicit semantic modeling;probabilistic language model;neural language generation stage;unstructured source document;neural encoder;decoder;annotator;unguided nonlinear programming;summary;nlg;text;source document;full text;original source document;summarwe;underlying lemma representation;new probabilistic approach;lemma representation;seq2seq;encoding;manuscript;seq2seq framework;linear classifier;representation"}, "9d698e034d83eedc05237e629eaad1c0c4e5bbb9": {"ta_keywords": "learnable recursive clause;free recursive logic programs;clause programs;equivalence queries;polynomial time;depth determinate clause;additional oracle;nonrecursive clause;algorithms;certain classes;classes;depth determinate;function", "pdf_keywords": "recursive logic programs;learnable recursive clause;recursive logic program;logic programs;recursive programs;ary recursive clause;clause programs;logic program;ary recursive program;recursive clauses;recursive literals;recursive program;linear recursive programs;logic algorithm;recursive clause;generic logic operations;clause program;learning program;equivalence queries;learnability;subclause database;learnability model;logic literals;logic input;efficient algorithms;logic clauses;oracle;clause definitions;algorithms;equivalence query"}, "bfe6d67ed1c9119f91774e62fe0f4f328830526e": {"ta_keywords": "neural conversation models;conversation data;speaker data;speaker;exclusive speakers;task;model quality;baseline approaches;significant improvements;set;approach;hypothesis", "pdf_keywords": "neural conversation models;conversational models;personalized conversational model;novel conversational model;conversational data;conversational model;conversational personal data;conversational learning;conversation data;conversationality;conversation;conversations;machine translation;speaker roles;speaker role character;natural language processing;automated speaker;individual speaker;answerer;personalized information;speaker;responses;sequential generation;user response;linguistic diversity;nlp;models;richer personal information;information;context"}, "ca879ec1c04b94de274954dfd09dddfde6cbb4f3": {"ta_keywords": "perceptual age;singing voice;voice timbre;singer;individuality;age;method;novel method;experimental results;measure;adverse effect;paper", "pdf_keywords": ""}, "bd6c708a535af588d90025a0e6cf17407bf65434": {"ta_keywords": "words model;classifier;popular local explanation;model confidence;local explanation;bertarian;bert;bag;explanation;ability;paper", "pdf_keywords": "crowdsourcing study;machine learning models;attributions;model predictions;machine learning model;model confidence;machine learning;models;different attributions;model prediction;human understanding;generative models;hotel reviews;explanations;hotel review;model augmentation;poor prediction quality;classifier;model;examples;input examples;logistic regression models;deception detection task;bert model;predictions;intuitions;reviews;global cue words;model output;others"}, "bf7481685e63b85ef2586de3f6098f1a5fbe0e2d": {"ta_keywords": "novel active sampler;organic contaminants;active sampler;surface water;contaminants;active cartridges;active sampling mode;sampling;cartridges;osmotic pump;process;analysis;wide range", "pdf_keywords": ""}, "72579f6ce4a413585445c4ef8c8c2fa63ea1b8bc": {"ta_keywords": "gradient descent;optimal stochastic perturbations;deep learning problem;stochasticity;offline gradient;optimal sampling;depth;optimal parameters;accuracy;optimization problem;discovery;novel approach;machine;data request;significant loss;key idea;paper", "pdf_keywords": "robust feature library;smile detection;classifier;features;sifting features;target classifier;machine learning system;possible features;feature space;conducive features;input classifier;new classifier;target prediction task;unsupervised learning;noisy representations;inherent features;neural networks;input feature space;iterative learning;neural network;data representation;empirical neural network architecture;perturbation maximization method;novel algorithm;cloud;parallel neural networks;prediction model;trainable tensor;ml model;networkthe discovery"}, "80edd01d46228fac7ec0cd14aea1666253b28f4d": {"ta_keywords": "other voting profiles;voting settings;heuristics;several heuristics;preferences;others;information;people;ideas;paper;effectiveness", "pdf_keywords": "voting heuristics;approval voting;approval voting process;truthful whenin approval voting;voting behavior;voting strategy;other voter preferences;rational choice model;rational decision models;predetermined vote;user voting behavior;rational choice;behavioral decision;particular voting setting;behavioral decision making process;voter;voters;votes;many collective decision;heuristics;optimality heuristic;selection process;election;heuristic strategies;social choice setting;candidates;several rational choices;plurality election;popular election;optimality strategies"}, "1da3a9c194a01c0bff7b6ecda79db9d673810bee": {"ta_keywords": "level transduction tasks;transliteration;level transduction;phoneme conversion;grapheme;transformer;large format;neural network model;character;feature;other character;novel approach;data;strong baseline;performance;simple technique", "pdf_keywords": "neural machine translation model;morphological inflection generation;machine translation;strong recurrent baseline;morphological inflection;recurrent models;linguistic inflection;encoder;morphological inflection towe;level transduction tasks;deep machine;historical text normalization;decoder model;text editing;level transduction;level transduction model;includingcomputational linguistics;phoneme conversion;sub word forms;inflection;texts;text;normalization;morphology;character;model outperforms;transformer;new transformer model;level tasks;several common character"}, "28e81f96eab94e99febcaaee00637825c8a3e664": {"ta_keywords": "machine learning simulator;machine learning;data;machine;algorithm;parallel;speed;use;presence;novel method;method", "pdf_keywords": ""}, "faf494d0aa25a17aa25930ffb4c750fa59c44849": {"ta_keywords": "speaker verification problem;speaker;convolutional neural networks;encoder performance;image segmentation;representation;reconstruction;objective;combination;techniques;effectiveness", "pdf_keywords": "novel speaker encoder;better speaker embeddings;speaker verification tasks;speaker embeddings;speaker encoder;learned speaker;phonetic variability;phonetic information;deep voice;encoder;better speaker;end speaker;speaker;discriminative resnet;text encoder;neural networks;language recognition system;convolutional neural networks;neural network learning;neural network;human transcripts;convolutional neural network;several utterances;learning model;quality transcript;spontaneous symmetry;machinein;input texts;models;symmetric"}, "7efb1788b5e0fa3b4d9932722286ba1753b42f91": {"ta_keywords": "state tracking;effective description;schemata;shot transfer;description;data;process;system;performance;notion;meta;approach;impact;superiority", "pdf_keywords": "dialogue state tracking;dialogue state tracking technique;dialogue datasets;dialogue modeling;dialogue model;dialogue framework;state tracking framework;taskspecific ontology;natural language descriptions;state tracking;language descriptions;dialogues;dialogue;standard language description;label dialogue states;language model;semantics;shot state tracking;schema descriptions;natural language;tasks;conversations;state machine learning;schema items;task;powerful language model;schema;language language language;linguistic transformer;unseen tasks"}, "9688671a573651955c26d710c12617de26715e78": {"ta_keywords": "repair principle;codes;exact repair;systematic nodes;repair;storage;interference;alignment principle;miser;cut;first explicit construction;energy;minimum;paper;class;construction", "pdf_keywords": "bandwidth code;storage codes;simultaneous download;storage network;new interference alignment code;repair code;interference alignment code;data storage;nodes;explicit code constructions;optimal storage;bandwidth;parity nodes;exact repair;repair mechanism;systematic nodes;single node;storage;simple storage model;storage system;node;interference alignment;repair;systematic node;single node whilein;coupled parity nodes;miser codewe;code;repair properties;open source code"}, "148efaba70165d9faef0dac28d5fa2538cfa662d": {"ta_keywords": "collaborative decision;time allocation strategy;cognitive biases;second user experiment;first user experiment;human;time;effectiveness;resource;assumptions;role;making;explanation;setting;de", "pdf_keywords": "biased ai;cognitive biases;several cognitive biases;bias;ai model predictions;ai prediction;decision tasks;ai accuracy;novel probabilistic decision;human decision;predictive heuristic;confirmation bias;ai;ai model;cognitive resources;biased framework;collaborative decision;availability bias;artificial intelligence;time allocation strategy;allocation policywe;decision reduces;cognitive control;incorrect decisions;decision making;bias iswe;optimal allocation strategies;human accuracy;ai collaboration;decisions"}, "a6219725a9ad2079536c091f02fda2d4da6d62ac": {"ta_keywords": "exact regenerating codes;storage systems;multiple simultaneous node failures;mail servers;nodes;node failure;code;same node;codes;minimum bandwidth point;uniqueness;construction;scheme;explicit construction;applications;variable number", "pdf_keywords": ""}, "3c1001c04866647650216201feb54c927af3a05b": {"ta_keywords": "clauses;particular context;technique;applications;set;paper", "pdf_keywords": ""}, "60f0af1dbc2775a69f64e4351d969ac966659fb2": {"ta_keywords": "synonymy dictionaries;similar synset clusters;synsets;datasets;sparsity;russian language;preprocessing;graph;missing edges;second one;performance;first one;postprocessing;impact;approaches;different approaches;problem", "pdf_keywords": "synonymy dictionaries hamper performance;synonymy dictionaries;synset induction method;semantic similarity;lexical semantics;similar synset clusters;unstructured dictionaries;dictionaries;dictionary construction;word senses;linguistic similarity;symmetric semantic relations;dictionary;disambiguation;synsets;synonymy graph;information retrieval;synonyms;clustering;original lexicon;similarity;discovery;datasets forwe;nodes;automatic induction;possible words;unstructured patterns;antonyms;input graph;algorithm"}, "22616702da06431668022c649a017af9b333c530": {"ta_keywords": "natural language processing;computational neuroscience;claim truthfulness;task formulations;evidence;automation;accuracy;fact;research challenges;recent advances;methods;list;factor;recent development;use;opportunities;role", "pdf_keywords": "textual claims;natural language processing;fact checking;nlp;deceptive text detection model;fake news challenge;semantic information;text verification;politifact fact;text integrity;claim representation;feature checking;evidence;relevant knowledge graphs;verifying;veracity;false information;task formulations;fake news network competition;fake news news network competition;related tasks;fact;fake news stories;hypertextual database;articles;headlines;validity;news coverage;content;knowledge network"}, "6b7f2f30840b0d72484784a15b3be670868a9f95": {"ta_keywords": "distant languages;structured priors;new interlingual latent;unlabeled target data;syntactic tasks;log likelihood;source data;source model;target model;invertible projection;part;parameters;space;method", "pdf_keywords": "crosslingual parsing;crosslingual tagging;generative target language parser;crosslingual transfer;delexicalized dependency parsing;deep bidirectional adaptation;target language;target languages;distant languages;related language;source language;treebanks;accurate parsing;new interlingual latent;syntactic tasks;structured flow embeddings;languagespecific knowledge;fast text representation;several languages;languages;linguistic input;embeddings;language;unlabeled target data;parallel adaptation kernel;unsupervised adaptation;unsupervised learning;supervised flow model;discriminative baseline;supervised distribution"}, "eebc1811c55c2e5e8b3b78d0b0382ad50f22e32a": {"ta_keywords": "robust fact verification model;robust factual distortion;fact verification;contrastive evidence;distinct facts;false claims;vitaminc;belief;corpus;examples;checking;correction;resource;anomaly;document;possible alternation", "pdf_keywords": "fact verification models;scale fact verification dataset;fact verification;english wikipedia database;wikipedia sentences;small factual changes;wikipedia revisions;factual revision;empirical knowledge;factual revisions;inference tasks;wikipedia;english wikipedia;underlying knowledge;supplementary supplementary supplementary dataset;informative content;factual flagging;large corpus;encyclopedias;factual statements;semantics;wikipediawe;online encyclopedia;computational linguistics;adversarial inputs;knowledge;claim extraction;language model;evidence;semantic structure"}, "6dd1e4d97dbdb370a36c25f82a9a9baaa16c836c": {"ta_keywords": "mutant vesicular stomatitis virus;ebola virus;viral genome;pseudotyped virus;coil motif;gp2 protein;peptides;receptor;terminal motif;membrane fusion activity;infectivity;gp2;effect", "pdf_keywords": ""}, "7e122cc1a62e2f30951e14b91811896e1866dd7c": {"ta_keywords": "music;empirical loss;convolutional;generation;novel method", "pdf_keywords": ""}, "c7af06170f3d81ab761873a4c1fe0af2736eb0a2": {"ta_keywords": "natural conversation;dialogue system;emotional responses;automatic prediction performance;affective process;emotional triggers;prediction;emotion;random guessing accuracy;interaction;response;events;action;occurrences;study;analysis;ability", "pdf_keywords": ""}, "8dd3b88ac87372c9f4428029ac12288ff3405199": {"ta_keywords": "blood lipid variability;lymphocyte ratio;density lipoprotein cholesterol;neutrophil;ldl;risk factors;variability;nlr;multivariate linear regression;hdl;subgroup analysis;density;relationship;value", "pdf_keywords": ""}, "36b6abfb32ea56208a2858b558acbdd001c965e9": {"ta_keywords": "neural machine translation system;machine translation system;parallel machine;parallel;linguistic structure;performance;data;system;study", "pdf_keywords": "efficient neural machine translation;neural machine translation;open neural machine translation;translation task;translation model;parallel translation system;translation system;parallel translation process;lower translation accutranslation;neural attention model;memory efficiency;computational linguistics;natural language processing;memory;data augmentation techniques;attention mechanism;modeling evaluation;computational efficiency;performance;efficiency;tasks;new training data;task;logarithmic accuracy;npt task;like transform;models;abstracts;optimizations;nmt"}, "47b6023808002dfde031c17b34dcb1b522d3b326": {"ta_keywords": "dimensional electron gas;electron gas;spectral density;ultracold atoms;strong magnetic field;schrdinger equation;magnetic field;calculation;new method;method;presence;system", "pdf_keywords": ""}, "743d1aae44a12fb37b743ec947fad41cba9831b8": {"ta_keywords": "structured meaning representations;abstractive summarization;conditional text;computational pragmatics;text;distractors;informativeness;explicit models;generation;performance;methods;techniques", "pdf_keywords": "abstractive summarization;multisentence document summarization;rational language generation;structured meaning representations;summarization;pragmatic speaker model;neural machine translation model;neural attention models;structured sentences;successive successive unstructured text retrieval tasks;neural attention model;structured text;unstructured text;sentence descriptions;pragmatic distractor;minimal distractor;natural language;automatic generation;retrieval;distractor;speaker models;text;computational pragmatics;pragmatic system;theneural attention models;task;type generation dataset;language;context;informativeness"}, "ba5e3559a2d54bb0e8d7678c9905b4a77da63f71": {"ta_keywords": "simple incentive mechanism;incentive property;observational biases;objective evaluations;mild subjectivity;truthful responses;natural equilibria;responses;mechanism;payoff;many tasks;adoption;likelihood;presence;paper;large classes", "pdf_keywords": "incentive mechanisms;incentive agents;incentive systems;novel incentive mechanism;simple incentive mechanism;incentive mechanism;incentive mechanismswe;incentive matching;incentive models;incentive incentive models;strictlythe incentive design;incentive modelswe;incentive sharing;incentive structure;incentive behavior;incentive measures;incentive;incentive model;simple incentive policy;incentivewe;incentive feedback problem;novel incentive property;incentives;truthful equilibrium;truthful equilibrium payoff;strong incentives;fact incentive;incentive gain;simple reward mechanism;reward scalings"}, "52824fb6eb5d3b55fb6634c77dc80f5826964464": {"ta_keywords": "datalog code;inductive logic programming;software;database views;world software system;representative test cases;examples;techniques;specifications;method;behavior;machine;context;number", "pdf_keywords": ""}, "0cd693f1a1223f25e89c1f5efdedd7c3b7846691": {"ta_keywords": "overall traffic congestion;traffic flow;available parking;parking;parking contributes;density;data;new data;city;full city;dynamic response;simulation;network;level data;model;impact;evaluation;new kind", "pdf_keywords": "node queue network;parking queues;queue capacity;finite capacity queues;edge queues;queue;queues;parking spaces;parking space;flexible parking availability;traffic;parking lots;parking;pricing parking;neighboring parking lot;queuewe;street nodes;parking spot;network performance;street parking maneuvers;parking lot;network topology;nodes;parking garages;networks;curbside parking configuration;parking meters;flexible parking;network;node communication"}, "1f0524971c20a06d745ab784689eb8833435fde1": {"ta_keywords": "multiple systems;available systems;commonalities;system;discovery;fever score;comparative study;current state;exploration;problem;art;new avenues", "pdf_keywords": "natural language inference;successful natural language inference pipeline;natural language processing;fact extraction;sentence representations;first fact extraction;sentence selection;fever fact extraction;sentence selection pipeline;textual claims;sentences;entailment classifier;word embeddings;claim annotations;language modeling;task fever;entailment relations;novel labeling;lexicalization;sentence;task;verification challenge;wikipedia;fever;document;fever score;results;classifiers;onbiological documents;empirical discovery"}, "68731c68773b117250f04509103031109b222d27": {"ta_keywords": "distant supervision;relation phrases;local context signal;individual sentences;world corpora;global structural signal;remine;unified framework;novel open system;joint optimization problem;robustness;different domains;experiments;effectiveness", "pdf_keywords": ""}, "fd8e176087335355ff5e81821a616d15ec8d3346": {"ta_keywords": "indirect supervision sources;training labels;label relations;unseen labels;different output label spaces;text classification tasks;generalization;probabilistic modeling approach;image;mismatched output spaces;distinguishability;model;challenge;new research problem;framework;test", "pdf_keywords": "new weak indirect supervision;probabilistic label relation model;indirect supervision sources;indirect labeling functions;weak supervision sources construction;new indirect labeling problem;label relations;weak supervision frameworks;indirect supervision;label relation;weak supervision problem;comprehensive label relation;label relation structures;supervision sources;weak supervision setting;standard weak supervision setting;training labels;multiple supervision sources;weak supervision;supervised learning;labeling functions;manual supervision;label model;label data;supervised classifier;data annotation;label;training data;unseen label;labels"}, "e34f9e9163b13de00707157feda6a8b853c5c82d": {"ta_keywords": "deduplication;deduplication problem;reference data;scalable approach;data;crowd;knowledge;expert;fact;key idea;approach;need;approaches", "pdf_keywords": ""}, "e1d35deec12d18e53ca97a3cf4071526ad47968d": {"ta_keywords": "language model;linguistic knowledge;structural changes;models;training strategy;model;architecture;computational study;variations;scale;ability;set", "pdf_keywords": ""}, "e2ebf18e0b88752bd3ff905d2fba74213dcd2c51": {"ta_keywords": "excitation sounds;fundamental frequency;electrolarynx;statistical f0 prediction;f0 patterns;electrolaryngeal;patterns;prototype system;speech;f0;el;method;time;experimental results;paper", "pdf_keywords": ""}, "595a79ca667258ca2a4f5e7775e95a0fb0a0f024": {"ta_keywords": "stochastic gradient play;monotone game;free games;unconstrained optimization;games;efficiency;game;class;method", "pdf_keywords": "stochastic gradient play;single player optimization;gradient estimate;theemma game;monotone games;convex optimization;gradient update;free algorithm;nash equilibria;individual gradient;optimal performance;cost function;iteration complexity;continuous games;loss function oracle;dimensional optimization;games;unbiased estimate;players;multiplayer;algorithm;equilibrium;game;featureless algorithm;asymptotic behavior;player;iterations;gameswe;single player setting;cost"}, "36f7827bc344f9c2198dcb29732c525c68dc637a": {"ta_keywords": "tractable allocation techniques;cooperative game;cost;agents;game;tsp;locations;theoretical proxies;location;ts;solutions;number;problem;experimental evaluation", "pdf_keywords": "optimal transportation;optimal value;mean value;transportation networks;mean values;novel cost allocation;salesman problem;transportation;squared cost;skewley value;stochastic game;squared squared cost;symmetric vehicle;shapley value;representative allocation;cooperative game theory;least possible value;topological cost allocation problems;average distance;general delivery game;allocations;cooperative games;allocation;shapley values;cooperative game;optimal distribution;mean;common random walk models;topological cost allocation;value"}, "ead1e044d284f3deecd32c2d5cc89fe513195a0a": {"ta_keywords": "graph augmentation;synonymy relation;transitive edges;input graph;potential synonyms;equivalence property;datasets;addition;approach", "pdf_keywords": ""}, "7e63be5285e6596fbbc6c56bc89f7b6fd8bbe8c5": {"ta_keywords": "model debugging;model errors;textitbugs;data contamination;diagnose;time contamination bugs;training examples;spurious correlation artifacts;model;textitdata;test;source;ability;partiallylylylyly", "pdf_keywords": "debugging models;model debugging;debugging;model bugs;model attribution;feature attributions;explanation methods;bugs;user study;explanations;defective models;attribution methods;attributions;spurious models;data contamination tests;model learning methods;attribution;method effectiveness;interpretable neural networks;model predictions;models;devastating errors;attribution method;time contamination bugs;pattern attribution;tools;model;findings;wrong attributions;description"}, "bd8922f8cc8284553dc9e6db529af309298451fe": {"ta_keywords": "decoder networks;encoder;decoders;attention;target text;text;augmentation techniques;network performance;augmentation technique;translation;network parameters;training process;resource environments;joint knowledge;context;novel method;combination;approach", "pdf_keywords": ""}, "25ddddbd0bd1cfebf1548b2ee91bb1bbd05fdff1": {"ta_keywords": "neural agent;multimodal transformer;language inputs;neurons;encoding;challenging alfred benchmark;training;novel approach;behavior;single step;ability;art performance;large number;state;approach", "pdf_keywords": "language navigation tasks;language navigation agent;visionandlanguage navigation;natural language navigation tasks;language navigation;language attention;domain semantic navigation tasks;usual language encoder pretraining strategy;language encoder pretraining;unstructured navigation tasks;natural language navigation instructions;language pretraining;recurrent encoder;long term memory;language encoder;language demonstrations;action decoder;term vision;language inputs;synthetic language encoder;language decoder;compositional tasks;recurrent model;specific reinforcement learning pipeline;vision;synthetic language;natural language instructions;language;deep bidirectional transformers;complex human instructions"}, "0805cb1b26577f08f84190445992f7f0584e4742": {"ta_keywords": "complex systems;complex environment;dynamics;systems;molecular biology;numerical methods;system;evolution;analysis;novel approach;approach;variety;presence;study;results;combination", "pdf_keywords": ""}, "352ac73b7d92afa915c06026a4336927d550cec3": {"ta_keywords": "novel graph;parameters;neural network;supervised datasets;propagation module;gnns;natural language sentences;dataset;datasets;generator;model;extraction;data;gp;experimental results;significant improvements", "pdf_keywords": "relation extraction;natural language reasoning;natural relation extraction process;relationship extraction;relation extractionwe;relational reasoning;level relation extraction;relational reasoning task;natural relation paths;natural language processing;knowledge extraction process;unstructured text inputs;natural language;natural languages;relation paths;relations;classic natural language;neural networks;more relations;relation;gnns;convolutional networks;neural model;nodes;hop reasoning;graphs;relationships;reasoning;entities;embedding"}, "b53689b8c28353106f327f0981b108eb67816053": {"ta_keywords": "machine translation;preprocessing;novel preprocessing method;syntax;special preprocessing strategy;rule;tptf;quality;methods;proof;method;principle demonstration;efficacy;use;power;purpose", "pdf_keywords": ""}, "21d45b4923ad165fbb6612e08d06f9d786f9b4cc": {"ta_keywords": "symbolic knowledge graphs;commonsense models;commonsense model;symbolic knowledge;commonsense;general language model;teacher model;sharedsense;model;smaller models;ensemble;text;novel method;method;factor;approach", "pdf_keywords": "symbolic knowledge distillation;symbolic knowledge distillation model;symbolic knowledge graphs;symbolic knowledge graph;knowledge distillation;symbolic knowledge source;symbolic knowledge;automatic knowledge graph construction;natural language knowledge;high quality knowledge graphs;annotated knowledge;quality commonsense knowledge;causal commonsense;large commonsense models;commonsense model;general language models;linguistic models;knowledge sources;compact commonsense model;knowledge;general language model;current knowledge extraction methods;knowledge discovery;distillation;inference relations;critical knowledge;novel symbolic extension;large corpus inferences;biological knowledge;selective distillation"}, "53e0abebd9aef5915f72147d3674596a0051748c": {"ta_keywords": "personalized artificial intelligence services;data protection;personalized services;privacy;security;different protection approaches;artificial intelligence levels;services;system;edge research;management;ii;iii;context;different levels", "pdf_keywords": "personalized artificial intelligence services;personalized artificial intelligence;personalized services;data privacy;data protection;data protection mechanisms;data protection approaches;privacy preservation;personal data;user privacy;untrusted artificial intelligence;personalization;personal assistant personalization;untrusted data;artificial intelligence services;enhanced personalization;current privacy concerns;privacy;untrusted data sharing;privacy concerns;privacy implications;privacy paradox;user services;untrusted machine services;anonymous data;untrusted cloud;untrusted computing system;untrusted data flow;personalized models;untrusted untrusted machine clouds"}, "b6145cc19acfbec31373446a2dba210cc9b1eb7f": {"ta_keywords": "relation examples;precision data sets;learning problem;large collection;data;sized approaches;novel framework;framework;use;scale", "pdf_keywords": "structured disease corpus;relation extraction;structured corpus;small structured corpus;relation label propagation;structured corpora;unstructured testing corpus;corpus;novel labeling method;corpora;relation mentions;label propagation;distant supervision;efficient extraction;novel feature extraction method;accurate labeling;nonlinear labeling;disease articles;labeling;supervised learning;wikipedia dump;entity;relations;disease research team;relation examples;accurate extraction;large data sets;disease domain;discrete labeling;more patterns"}, "181e1d4b08dc62237277a6a743576facd8c5e572": {"ta_keywords": "posterior posterior estimations;speaker voice signal;speaker number;speakers;target number;target;downstream performance;number;signal;method;novel method;der;ability", "pdf_keywords": "speaker diarization;speaker diarization system;speech data;speech recording;speaker representation;speech features;speech segments;overlapped speaker diarization;meeting transcription systems;speaker;speech;decoding;style utterance;separate diarization model;recording;same speaker;diarization;multiple speakers;speakers;neural network architecture;recognition;vector estimates;noise ratio;network outputs;meeting;other fusion methods;feature;novel fusion method;fusion;eurecom rt rt11 library"}, "4236a5f650f5b7ced7512b5072a062b521220b31": {"ta_keywords": "transfer learning framework;temporal semantic features;urban areas;other dataabundant areas;historical data;traffic speed;target areas;classic regression models;various spatio;competitive performance;methods", "pdf_keywords": ""}, "e25ce2a7b28699e1d57803ef977175937ce50923": {"ta_keywords": "word segmentation;pronunciation estimation;natural language processing;annotation;pronunciations;quality documents;word;pointwise estimator;estimation process;splitting;method;context;particular sentence;novel method;decision;steps;other decisions;problem;strategy", "pdf_keywords": ""}, "0e532d1489d7420cff7ff8aa211ded08e7d57fe9": {"ta_keywords": "online stronglyconvex learning;convex loss functions;convex optimization problem;optimal hypothesises;algorithms;general framework;new approach;framework;paper;use;problems;problem", "pdf_keywords": ""}, "1109f787fc8d51feb3bae9bf6e1945dc4a1191e7": {"ta_keywords": "explainable ai;explanations;team performance;underlying team;underlying decision;task;humans;human;complementary performance;correctness;underlying structure;chance;impact", "pdf_keywords": "explanations increase team performance;ai;ai system;artificial intelligence;ai model;team task;expertise;participants responses;tasks;method user studies;informative brain;explanations;decision making;team performance;task;informative manner;local explanations;human performance;assistant;explanation sources;reasoning;experiments;tasks1;participants;human;decisions;accuracy;underlying behavior;brain;detailed analysis"}, "328a9fe143639810d6413c2cc901ec3afa6aa607": {"ta_keywords": "molecular dynamics;stochastic linear perturbation theory;continuum models;stochastic linear perturbation equation;model;novel approach;training;framework;ones;settings", "pdf_keywords": ""}, "350e5f5a89cbb3a23442c9d0d3e59fc50d665dbb": {"ta_keywords": "late time market;pricing;electricity;energy;distribution;model;notion;implications", "pdf_keywords": ""}, "2226f5a13e3e9faac2e228e95175d3e612b52395": {"ta_keywords": "artificial intelligence;special interest group;acm;annual activity report;group;annual report;monthly report;sigai;members;membership;report;june;units;july;period", "pdf_keywords": ""}, "c102ac8c779ee0a53dc8e4ee20b4088ac2c7e186": {"ta_keywords": "multidimensional data;complete distribution;data;rigorous distribution;end distribution;prior models;memory;predictions;model;joint;end;art model;state;significant reduction;error", "pdf_keywords": "joint attention decoder;end speech recognition problem;speech recognition;attention decoder;automatic speech recognition;spontaneous speech recognition system;attention model;joint attention approach;spontaneous speech recognition problem;joint nonlinear neural networks;deep convolutional neural network;convolutional neural network;joint attention;attention model iswe;decoder model;encoder;convolutional network approach;network encoder;attention;underlying speech signal;network decoder;decoders;cnn;recognition;same decoding;joint connectionist;featureless acoustic signal;same decoder;joint proc;joint joint joint"}, "1144cc3e86b1cc4160aedddb085d7861d4b528dc": {"ta_keywords": "softmax;corpus;vocabulary;accuracy;standard model;novel method;experimental results", "pdf_keywords": "end speech recognition;deep recurrent neural networks;neural machine translation;rnns;speech recognition;softmax;novel automatic speech recognition;automatic speech recognition;rnn;generative neural model;sample softmax example;encoders;encoder network;transducer models;tensor network representations;machine translation system;tensor networks;decoders;transducer;joint decoding;auxiliary ctc losses;own logit tensor;tensor networkwe;asr;sentence predictions;large vocabulary;large memory consumption;training;memory usage;model"}, "69e9a040ef633c60533843442529cc68c5f12932": {"ta_keywords": "iterative power iteration clustering;pairwise similarity matrix;spectral methods;dimensional embedding;truncated power iteration;power iteration;datasets;dataset;efficient method;igcc;technique;approach;art;state", "pdf_keywords": ""}, "3dc4580a154df87f3a56aa3d16b00c5a935ebe15": {"ta_keywords": "reviewer;bias;citation;own work;submission;observational study;model mismatch;analysis;effect;different modeling techniques;problem", "pdf_keywords": "citation bias;peer review;citations;valid peer review;potential reviewers;peer review process;citation;expert reviewers;reviewers;bias;review submissions;citation data;potential bias;review processes;expert reviewer;citation relationship;reviewer;review systems;review process;researchers;references;evaluations;inconclusive referee assignment;inaccurate referee assignment;evaluation;research journals;statistical study;journals;submissions;many anecdotes"}, "3cfdec4f1664fcdc20fd5a6d3f86e7b40cf19f70": {"ta_keywords": "neural encoder;decoder outputs;encoder;decoder model;text summarization tasks;summary quality;length;learning;method;methods;approach", "pdf_keywords": "text summarization task;decoder models;output sequence length;decoder;neural encoder;sentence compression;natural language caption sequences;decoder architecture;summarizationwe;encoder;sentence summaries;output length;neural encoderdecoder models;summaries;text;output sequences;generative recurrent neural network;output sequence;average output length;natural language;length;additional length;various lengths;bytes;various lengthsin;output;sentences;byte;input sequence;neural model"}, "7a070c558cdb9c525559d1ad48159551381750c9": {"ta_keywords": "gram machines;symbolic meaning representations;encode knowledge;encoded knowledge;sequence models;babi tasks;sequence;complexity;programs;novel approach;text size;questions;scalability issue;special version;approach;weston et al", "pdf_keywords": "knowledge storage;knowledge representation;knowledge encoder;large corpus;knowledge extraction;large web;structured knowledge;structured storage;complex semantics;symbolic machine;natural language text;unstructured data;gram machine;scalable storage system;linguistic representations;informative knowledge tuples;memory network;memory;symbolic representations;storage;unstructured representations;knowledgewe;schema;knowledge;neural network;babi tasks;wikipedia;neural network models;reasoning sentences;query query processing"}, "6516b800482100731f0eb348f678ad30799c839f": {"ta_keywords": "word emergence;distributional semantics paradigm;semantic neighbors;semantic sparsity;neology;new words;language;phenomenon;new statistical analysis;frequency growth rates;latter hypothesis;factors;importance;more support", "pdf_keywords": "word emergence;distributional semantics;neologisms;word distribution;neologism dictionary;semantic shifts;linguistic properties;neologism;new linguistic application;new words;semantics;neology;language factors;word embeddings;normal words;semantic neighborhoods;large corpora;corpora;word neighborhoods;emergence;words;language;stochastic word;semantic neighborhoodswe;dictionary;generalization;historical english;word;phrases;popularity"}, "ac03cf22e2a831ab030ae33b5ddf5f9864917a17": {"ta_keywords": "diverse student populations;students;student presentations;sample;academia;collection;media;world;industry;job;parts", "pdf_keywords": ""}, "0a7f95adbaf0e46c93b5f82c74a26f5874c861ac": {"ta_keywords": "traffic ramp metering systems;traffic ramp model;random phase approximation;feedback controllers;parameter approximation;parameter model;control law;simulation results;model;novel approach;paper;design;analysis;context;effectiveness", "pdf_keywords": ""}, "c5e4eafd85949e6aac9d8e98d5e03b2acf444046": {"ta_keywords": "other adversarial datasets;adversarial data;adversarial data collection;domain evaluation sets;datasets;outcome;models;diverse collection;qualitative analysis;future research;variety;efficacy;task;guidance;differences;key differences;question", "pdf_keywords": "other adversarial datasets;adversarial data collection;adversarial data;crowdworkers;natural language context;natural language processing;natural language;questions;training data;datasets;examples;empirical evidence;accuracy;empirical empirical evidence;linguistic aspects;empirical empirical empirical evidence;task;data augmentation strategy;novel empirical approach;workers;outperform models;semantics;machine learning systems;correct models;data;models;training;words;robustness;powerful tool"}, "575ac3f36e9fddeb258e2f639e26a6a7ec35160a": {"ta_keywords": "semantic language understanding;explicit syntactic knowledge;treebanks;downstream lh task performance;lh tasks;scale machine learning models;training;head;transformer;lu;level;context;ipt;model;couple;effect;tune;efficacy", "pdf_keywords": "downstream language understanding;natural language processing pipeline;explicit syntax knowledge;parser;downstream elliptic parsing;explicit syntactic knowledge;novel biaffine parser;original parsing;biaffine parser;natural language inference;semantic language understanding;neural language models;linguistic knowledge;cost machine translation pipeline;syntactic structures;downstream language transfer;natural language models;word representations;shot language transfer tasks;language data;art parsers;natural language;required syntactic structure;specific syntactic input;target language text;explicit knowledge;transformer networks;downstream lu task performance;downstream tasks;bidirectional lstm feature representations"}, "b437cc7c0ae672b188df078b5dd80f97e8dde978": {"ta_keywords": "lexical units;machine translation;speech recognition;unsupervised learning;fewer units;models;tasks;improved accuracy;thesis;methods;framework", "pdf_keywords": ""}, "69ba64b20d0a1849ef08d63c39bfafbaac909087": {"ta_keywords": "behavioral constraints;exogenous constraints;regret;learning;novel algorithm;bcts;world data;case study;application domains", "pdf_keywords": ""}, "40bbd3046f1fa86a50e526b3848b4f2bd3a1d873": {"ta_keywords": "perfluorinated polyelectrolyte;conventional electrolyte;polyelectrolyte solution;polyelectrolyte;lithiated ion;lithiated nafion;battery;performance;only alternative;work", "pdf_keywords": ""}, "059f515bf53bcddeca031fd4a4071c911999a3c6": {"ta_keywords": "similar clothes;different clothes;invariant feature learning;invariant person representation;apparel;cloth;person;images;several persons;baseline models;same person;framework;novel framework;aifl;cases;work;performance;experimental results", "pdf_keywords": "unsupervised apparelsimulation gan;synthetic person images;person image generation model;similar clothes;cloth image generator;different clothings;person image;invariant person representation;person identification algorithm;clothing;same clothing;invariant feature learning;same person image;synthetic apparel;apparel;invariant feature learning framework;person reid performance;person reid;cloth;discriminative feature;n1 apparel;adversarial way;deep convolutional model;gan;human body images;deep joint learning;feature representation;training dataset;recognition;feature extractor"}, "c96363c42bc8c465902c22b8c33c8704233f519e": {"ta_keywords": "natural language commands;code generation;code generation systems;natural language challenge;natural language pairs;languages;language;benchmark;dataset;performance;state;art;interest;methodology", "pdf_keywords": "language snippets;code generation;code generation task;specific seq2seq snippets;natural language commands;multilingual machine translation;code summarization;short code snippets;original code snippets;python programming language;programming languages;first comprehensive multilingual training dataset;various target languages;multilingual dataset;machine translation;languages;multiple languages;automatic translation;structured code;art seq2seq generationthis work;natural language domains;novel machine translation framework;available machine translation models;seq2seq;different languages;natural language;language;different language models;multilingual knowledge;typographic string literals"}, "70a321f12a655e305781e2de0ca9617d96e462c3": {"ta_keywords": "data market;multiple data aggregators;aggregators;equilibria;noncooperative strategic play;users data;competition;anarchy;game;interactions;much social welfare;price;preliminary model;model;continuum", "pdf_keywords": "strategic data buyers;data market models;data market;data aggregator;data buyers;strategic data sources;data buyer;pricing mechanisms;multiple data aggregatorswe;data;data sources;incentive mechanisms;generalized nash equilibria;data source;nash equilibrium problem;principalagent problem;optimal pricing parameters;market;pricing;optimal estimation;pricing parameters;strategic value;incentive;nash equilibria;buyers;estimators;estimation;estimation problem;competitive outcomes;competition"}, "bee52c51cbd37d0e48c3ea5f71a08f177d2aff73": {"ta_keywords": "binary classification;cipher;structured learning;weight vectors;conversion method;novel approach;method;adaptation;novel;art method;paper;state", "pdf_keywords": ""}, "7129b62be18487db5e9602e353bb10a4c79a9b92": {"ta_keywords": "executables;procedure names;neural textual;neural models;architectures;static analysis;representations;models;call sites;graph;evaluation;predictions;novel approach;methods;humans;power;art;state;approach;time", "pdf_keywords": ""}, "f4cf4246f3882aa6337e9c05d5675a3b8463a32e": {"ta_keywords": "patterns;natural language;understanding;method;novel method;action;simple strategy;hypothesis;task;need", "pdf_keywords": "visual language understanding models;annotated expert demonstrations;language navigation tasks;expert demonstrations;robotics tasks;language navigation benchmark;navigation tasks;level language;natural language instructions;level tasks;human language;natural language assistance;actions;reinforcement learningwe;language planning model;unstructured unstructured navigation task;natural language;natural language representation;tasks;simple navigation task;language;videos;vision;structured environment;scale navigation tasks;annotated language documents;action;exploration;expressive representation;imitation"}, "8f11643b42976433fc3a2ec19feef486929527a1": {"ta_keywords": "relativistic quantum field theory;relativistic field theory;electroweak interactions;lorentz;calculation;new method;method;context", "pdf_keywords": ""}, "d9e56aa9f69e18c9d37799b86b50d36709cbf711": {"ta_keywords": "same particles;different average numbers;same average number;ensemble;opposite average numbers;particles;same number", "pdf_keywords": ""}, "59a228f48a83eb0905391f7e454fde0eeb6680ee": {"ta_keywords": "automatic speech recognition;language model;acoustic lattices;continuous speech;joint learning approach;lexical units;lattices;linguistic constraints;lm;training;accuracy;class;novel method;method", "pdf_keywords": ""}, "8b7a8f9a27b8dc73a5b0b62ada14bbab047084fc": {"ta_keywords": "statistical voice conversion;silent speech enhancement;electrol speech enhancement;vcch;digital signal processor implementation;conversion accuracy;time vcch;computational cost;little degradation;experimental evaluations;time;several methods;paper", "pdf_keywords": ""}, "4100256a125d7b56cac693a436bba2b00fae3fa3": {"ta_keywords": "captioning;natural language descriptions;corpus;decoder;transformers encoder;acoustic signals;convolution;models;benchmark;model;high quality samples;model architecture;end;ned;end manner;fact;power", "pdf_keywords": ""}, "306c59458cebb35c2d520dd129f09d5c6cc2985f": {"ta_keywords": "behaviour", "pdf_keywords": "parametric paraphrase models;paraphrase database;bigram similarity corpus;paraphrases;lexical paraphrasability;gram embeddings;empirical phrase pairs;lexical paraphrasability problem;phrase pairs;phrase tasks;word similarity;bigram similarity task;semantics similarity;empirical short phrases;short phrases;paraphrase;new word vectors;plain phrase pairs;novel linguistic representations;sentences;word vectors;phrases;machine translation;linguistic semantics;word pairs;linguistic entailment method;dimensional word vectors;corpus;linguistic vocabulary;bigram tasks"}, "bba9b93ab8d9b98cd54001a5ba9673e513a35219": {"ta_keywords": "clinical data;neural network model;sensor data;data;diagnosis process;model;patient;algorithm;novel approach;course;form;state;visit;problem;set;presence", "pdf_keywords": ""}, "cc352ea39f820c3effc40ce09369cf59afe361df": {"ta_keywords": "cloud platform;cloud services;cloud computing environment;cloud sector;server architecture;such applications;novel approach;approach;house", "pdf_keywords": "cloudcloud computing;cloud platform;cloud services;cloud infrastructure;qualitycloud computing;cloud computing;cloud;windows cloud;efficient healthcare services;health care service delivery;clouds;iaas layer service;healthcare sector;data storage;iaas applications;healthcare industry;hospitals;service;virtual machine;it infrastructure;infrastructure;storage;servicein;iaas layers;applications;live video streaming services;networking equipment;medical practice;such applications;paas"}, "589e651c69251ee20a89e075d015eb03b35cf17d": {"ta_keywords": "speech encoder;speech translation;effective length prediction methods;speech inputs;decoders;nar;tt;large length beam;various length candidates;better translation;ar;negligible overhead;framework;effectiveness", "pdf_keywords": "end speech translation;speech translation;speech recognition;language model;novel encoder;encoder;novel decoders;decoder;end speech;encoders;novel nar framework;best nar e2e model;nar e2e;neural network inference;same encoder;neural models;e2e models;translation;ar decoders;nar;neural networks;neural network architecture;machine learning framework;representational model;models;text;learning;neural network model;tt;e2e"}, "d5123ab81f511027cbe11dc92d99e116fd193158": {"ta_keywords": "energy harvesting;remote sensing;timevarying wireless link;channel probe outcome;energy;source node;sink node;ed source;discrete time instants;multiple processes;process;ed;data;link quality;novel approach;status updates;paper", "pdf_keywords": "infinite horizon decision process;optimal source activation policy;simple threshold policies;energy harvesting sensor network;optimal decision;energy harvesting;optimal scheduling policy;optimal sampling policy;threshold policy;optimal action;optimal policy;optimal policy structures;channel state information;optimal channel;optimal single stage;decision process;harvesting source;observation packet;multiple processes;remote sensing setting;data transmission policy;single process case;optimal estimation;stage energy;energy buffer;optimal sampling;quantum information theory;stages action model;optimality equation approach;energy cost"}, "ddfd297531f56121b8383bd1eb2bb09189ab2e2b": {"ta_keywords": "emphasis translation task;speech translation;term memory;emphasis;neural networks;target sentence;transfer;lsm;distance dependencies;model;new model;participants;number", "pdf_keywords": ""}, "f0a498014c4ef67c0b72ceb18d95e0d25087fd57": {"ta_keywords": "neural machine translation systems;japanese bidirectional translation tasks;softmax;binary codes;binary code;output layer;vocabulary size;memory requirements;models;model;correct codes;word;computation;cost;robustness;error;method;new method;advanced approaches", "pdf_keywords": "neural machine translation systems;binary code prediction model;binary code prediction;machine translation models;binary code representations;machine translation;softmax;translation accuracy;binary code;comparative translation qualities;standard softmax prediction;binary codes;predicted bit arrays;output words;logarithmic logarithmic logarithmic approximation;gradient descent method;code output;binary binary tree;neural machine learning problem;output codes;vocabulary size;target language;computational linguistics;neural network;corpus;memory requirements;computational complexity;code;binarytree;binary classification problems"}, "88e2beccbc89b3e3dd793e2502b17c1fa551151d": {"ta_keywords": "generalizedbandwidth maps;data storage;storage network;maximum possible data size;data;maps;nodes;linear arrays;new type;form;maximum;way", "pdf_keywords": ""}, "f4c8539bed600c9c652aba76a996b8188761d3fe": {"ta_keywords": "accurate translation scheme;translation techniques;neural machine;translation;fidelity;performance gains;baseline;vanilla system;higher levels;context;specific methods;hybrid system;methods;experiment;variety", "pdf_keywords": "trainingneural machine translation;neural machine translation;machine translation systems;ofneural machine translation;translation systems;vocabularyneural machine translation systems;different translation models;multiple translation models;translation algorithm;parallel translation task;stochastic gradient descent;sde optimizers;level language;novel training algorithm;translation;new training algorithm;simple language;speech recognition;sde;language;nmt models;neural network;languages;natural language processing;regular annealing strategy;standard sde;stronger baselines;larger vocabularies;strong baseline;training"}, "3e3254bce9c321310d2e9825ed52b30da9879173": {"ta_keywords": "speech segment features;finite state machines;finite state machine;search network;unique arcs;linear representation;representation;arcs;new representation;boa;bag;number;elements;paper;idea;advantage", "pdf_keywords": ""}, "285c50d98dab741a82649b1abcaca8273cb8f253": {"ta_keywords": "binary classification;online discriminative training method;multiclass classification;margineme algorithm;phoneme;g2p conversion;conversion;g2p;art graph;massive error rate;novel approach;paper;sme;art approach;state;current state", "pdf_keywords": ""}, "55bdc4ad158e272ccf796ae52b0ab7086a834352": {"ta_keywords": "student models;models;instruction strategy;model;agent;art machine;new approach;human;system;higher quality;state", "pdf_keywords": ""}, "c3177616ad35ef7850ea1e62da1fa3be36943e8b": {"ta_keywords": "recursive neural network identification technique;dialog problem;right words;identification technique;identification;method;problem;new method", "pdf_keywords": ""}, "49a049dc85e2380dde80501a984878341dd8efdf": {"ta_keywords": "encoding;supervised learning tasks;discrete representations;speech;representations;latent space;input;proc;novel framework;set;inequalities", "pdf_keywords": "speech encoder;speech representations;latent speech representations;latent speech representation;speech representation;discrete speech representations;speech audio;speech recognition;low resource speech recognition;phonetic representations;speech recognition systems;convolutional feature encoder;novel feature encoder;input speech;latent representations;utterances;simple acoustic model;supervised pretraining;wav2;encoder;feature encoder architecture;encoders;representations;simple language model;convolutional neural network;recognition;language models;training data;context network;random natural language processing"}, "5f1bbc96a22a630d3662b3fceb3160091e4bd814": {"ta_keywords": "wise sequential processing;estimation method;distributions;frame;dominant distributions;pruning;extraction;model;method;local characteristics;paper", "pdf_keywords": ""}, "bf0105bdd5b0dfc09580697739fb84590d031d0b": {"ta_keywords": "cognitive aspects;cognitive abilities;cognitive context;student;method;novel method;problem;fact;potential;traditional way", "pdf_keywords": ""}, "8ec127925a8680928d546df7248963e772e07a5d": {"ta_keywords": "screening;aggregate noise level;job candidates;individual tests;noise;optimal strategy;candidate;group;choice;effects", "pdf_keywords": "fairness model;discrimination measures;randomized threshold policy;optimal employer policy;optimal threshold policy;fairness;threshold policies;threshold policy;classification;fair distribution;fair determination;conditional learning;discrete learning models;discrete learning;employees;classification error rate;unskilled workers;candidate interviews;optimal policy;fair prices;hiring;threshold function;candidates;greedy algorithm;employer;unskilled worker;groups;candidate;task;algorithms"}, "a34954d9e36ea6c57743f55124a6ae444b951c2c": {"ta_keywords": "activation predictions;deep neural networks;neural network;parameters;activations;training point influence;representer points;accurate understanding;linear combinations;decomposition;methods;method", "pdf_keywords": "deep neural network predictions;deep neural networks;neural networks;neural network;arbitrary loss functionals;training point;loss function;training points;training data;particular prediction;prediction;convolutional networks;neuron output;test point prediction;representer theorem;learning;generative representation;predictions;maximum likelihood activation;learning problem;empirical representation;training set;loss;learnedwe;activation function;machine learning;representer points;many machine learning methods;representation;weights"}, "ce4eadb324026191c075f1af876403a847329d5b": {"ta_keywords": "many learning problems;features;world learning problems;vectors;representation;new representation;problems;space", "pdf_keywords": ""}, "63bc09c11a792abfcbb2d9e2809aa67929f09262": {"ta_keywords": "word embeddings;hypernyms;machine learning framework;graph model;words;projection;method;new method;set", "pdf_keywords": ""}, "cfb1b39d1a6733f42cc5e8cfd60dc68cafa01d28": {"ta_keywords": "neural networks;nonlinear dynamics;relative motion;document;shape;novel approach;theory;position;pair;approach;results;problem", "pdf_keywords": ""}, "2b110fce160468eb179b6c43ea27e098757a56dd": {"ta_keywords": "sentences;paraphrase;adversarial examples;syntactic specifications;field models;complete networks;models;training set;data;new generation;target;problem", "pdf_keywords": "simple full syntactic parse model;generative parse;syntactic modifications;parses;textual entailment datasets;syntactic adversaries;syntactic transformations;natural language processing;syntactic variation;constituency parse;parser;input sentences;sentences;simple s3c parse generator;paraphrases;machine translation systems;natural language;accurate paraphrases;syntactic form;nlp;lexical constraints;interesting sentences;generative neural backtranslation baseline;adversarial examples;underlying text;valid adversarial examples;sentiment analysis;new sentences;linguistic style;simple paraphrastic sentence"}, "cf0860ab99c63cb7cbd5317fca7cf1fe70e8fb63": {"ta_keywords": "differentiable knowledge propagation;virtual knowledge base;sparse matrix index;kb index;kk index;kb;novel method;method;sum;use", "pdf_keywords": "large text corpus;nlp knowledge bases;large corpus;virtual knowledge base;text corpus;knowledge base;question retrieval problem;new reading comprehension model;efficient searchwe;contextual representations;9k subject entities;natural language processing;corpus;maximum inner product search;encode mentions;natural language;nlp;search;deep learning;corpora;knowledgewe;comprehension model;entity centric approach;bert model;encode relations;mention mentions;linguistic structure;knowledge;entity;entities"}, "024091a3c0223f27d6456b1a27db18fb08d41e5a": {"ta_keywords": "neural network;joint model;npt;noise;np;computation cost;new method", "pdf_keywords": ""}, "629323c5b9f7c64afac9300212538e488569bd1e": {"ta_keywords": "structured lexical knowledge;word embeddings;semantic relations;ontology;dictionaries;induction;approach", "pdf_keywords": ""}, "33aa6c70eac0e4b7eb28d8386e5e4113fdd55203": {"ta_keywords": "world history entrance exam;automatic question;multiple english questions;ntcir;veriable assertions;evaluation;accuracy;system;training set;performance;terms;set;description;use;real world examples", "pdf_keywords": ""}, "2e820673ca861a9ece8d36f2b93793b5d2c7e1da": {"ta_keywords": "lightweight block ciphers;encryption process;high throughput;speed data;throughput;randomization;modular structure;low cost;copies;algorithms;iteration;rate data;new class;design;message;number;generation", "pdf_keywords": ""}, "49d415cf593be38c6cd97a183dadc7d7b48bab72": {"ta_keywords": "innovations;productivity;firm growth;labor demand;artificial intelligence;recent rapid progress;empirical data;data;impact;literature;evidence;study;degree", "pdf_keywords": ""}, "2225950d1d3e02bc0d88a0c78325d00e0122b576": {"ta_keywords": "novel joint training framework;joint training framework;speech signals;joint training;recognition;world data;large collection;end;framework;realistic data;end fashion", "pdf_keywords": ""}, "05fb5a180214bf092eeda30baf9f16fb6bd15727": {"ta_keywords": "direct waveform modification;durational patterns;synthesis process;japanese language;quality degradation;english speech;segments;analysis;experimental evaluation;method", "pdf_keywords": ""}, "649c1148439a4e875dab414ba67bf8c80350af4a": {"ta_keywords": "neural semantic parsers;neural semantic parser;different semantic parsing tasks;formal meaning representations;parser;abstract syntax description language;natural language utterances;transition;transition system;new types;system;target mr;strong results;mrs;experiments", "pdf_keywords": "neural semantic parser;semantic parsers;semantic parsing;formal meaning representations;abstract syntax parser;different semantic parsingwe;parser;semantic representation;abstract syntax description language;natural language utterances;new abstract syntax description;neural encoderdecoder network;augmented recurrent connections;deep learning;code generation tasks;neural networks;neural network;unstructured representations;language;information flow;supervised learning tasks;transition;code generation;unstructured unstructured representations;languages;transition system;sentences;learning model;tranx;new types"}, "86d84c1c9b0a500f930696ab27c83a4b30477560": {"ta_keywords": "paraphrastic sentence embeddings;crosslingual tasks;corpora;text;sentence;art baselines;level document;word;parallel version;learning algorithm;model;art state;state", "pdf_keywords": "paraphrastic sentence embeddings;interlingual embeddings;semantic crosslingual tasks;semantic crosslingual task;cost parallel corpus mining tasks;sentence embeddings;parallel corpus;semantic similarity;parallel corpora mining tasks;parallel corpora;underlying machine translation task;paraphrastic sentence;machine translation data;sentence encoder;machine translation;semantic representations;corpus;neural machine translation model;natural language processing;sentences;deep nonlinear corpora;linguistic structure;novel statistical inference model;semantics;bitext mining task;sentence;parallel data;deep bidirectional transformers;semeval;examples"}, "65c53ed3575e160eb1e7d0a516353ba52de7e7e5": {"ta_keywords": "bid leakage;russian procurement auctions;bid auctions;auctions;bid;leakage;identification;detection process;price;data;machine;novel approach;approach", "pdf_keywords": "bid leakage estimation;russian procurement auctions;bid auctions;bid leakage;bid leakage inthe;fair auctions;auction;auctions;auction data;such auctions;bidders;auction prices;bidder;same auctions;bidding concentrations;bid;online auction market;online auctions;bids;tie tie auctions;final bid;time bid;procurement costs;leakage;corruption scheme;central lybcrn auctions;opponents;other related corruption patterns;participants;other participants"}, "a9e6222e71dd101d444b7192b3a0636c71edb0a4": {"ta_keywords": "content discovery engine;contextual content;novel knowledge base;knowledge bases;unstructured background;text;novel approach;approach;presence;france;paris;use;problem", "pdf_keywords": ""}, "6536f36648d39f0f9f6105562f76704fcc0b19e8": {"ta_keywords": "persistent spatial semantic representation method;semantic tasks;hierarchical reasoning;agent;alfred benchmark;construction;art results;step;approach;step instructions;state", "pdf_keywords": "hierarchical robotics task;persistent spatial semantic representation;hierarchical representation;mobile manipulation tasks;manipulation tasks;natural language commands;associated semantic voxel map;voxel map state representation;navigation tasks;natural language instruction;challenging mobile manipulation benchmark;persistent spatial representations;unstructured map representation;learned navigation model;language tasks;level language;natural language representations;hierarchical representation learning system;visuomotor language grounding;spatial state representation;interactive environment;natural language instructions;visual representation;task structure;hierarchical context;natural language;mobile manipulation instructions;maps;level navigation;voxel"}, "2c0f2a03c3a427cc61359b5e2c31cfefe9850a31": {"ta_keywords": "unsupervised information extraction;concept names;entities;hearst patterns;concept;clusters;instance pairs;several datasets;web;information;terms;sets;domain web server;large numbers;novel method;method;experimental results", "pdf_keywords": "nonlinear knowledge extraction tasks;unstructured web set;unstructured text clusters;term clusters;concept names;unstructured corpus;web sets;websets;information retrieval;html corpus;entity sets;clustering information;web queries;web entities;unstructured text;corpus;hypernym recommendation;novel clustering method;several different corpora show;structured documents;hypernyms;entities;clustering;novel clustering algorithm;concept;underlying database;datasets;meaningful clusters;meaningful sets;clustering performance"}, "ed2cc779c7eb0004bd6dd50538a2cafca092c94f": {"ta_keywords": "historical texts;tagging;15th century;evolution;data;analysis;methods;paper;important class;novel method;stage structure", "pdf_keywords": ""}, "5bcd9117899bc2c91db83532dcf587b9d8f8888b": {"ta_keywords": "laws;states;earth;united states;form;parts;teaching;effective way", "pdf_keywords": ""}, "3d1cfefdbe40f7535ada772c260c192bb63bb9fe": {"ta_keywords": "document similarity training;textual matching;document similarity;textual supervision;texts;aspects;novel approach;single aspects;distance;supervision;earth;novel form;source;optimal transport mechanism;model;combination;approach", "pdf_keywords": "aspect similarity tasks;document similarity evaluation;textual aspects;document representations;documentlevel similarity;whole abstract document similarity;high quality document similarity benchmarks;document aspects;aspect matching;document similarity;textual supervision;weak textual supervision;natural language supervision;multivector document representations;citation contexts;citation mentions;sentence models;abstract similarity datasets;structured documents;citations;natural language processing;textual descriptions;aspect;implicit natural language supervision duringwe;level similarity ranking;similarity patterns;close paper relatedness;similarity;corpus;underlying context"}, "5e74d4e041a25e7752a596e2891975df5ba65aa2": {"ta_keywords": "singlechannel speech enhancement;minimum variance distortionless response;beamforming;mask prediction;spatial covariance estimation;speech;spatial covariance estimates;deep network;masks;noise;mvdr;performance;novel method;reliable way;various ways", "pdf_keywords": ""}, "a0ab4106dabd6bc067c7b3e4db06807e2c0f6036": {"ta_keywords": "natural language;small scale queries;efficient learning framework;nonlinear dynamics;nonlinear dynamics problem;structure;large scale;high accuracy;framework;recent example;form;low cost;power;approach;problem", "pdf_keywords": "large language models;customized language models;large general corpus;language models;language modeling;scale language models;training corpus;language model;scalable bert framework;general corpus;natural language processing;natural language understanding;bioinformatics;nlp;learning;random retrieval;pretraining;efficient framework;nonclassification tasks;many models;models;task constraints;computational costs;bert approach;relevant training data;high resource resource models;tasks;domain adaptation framework;task data;classical finetuning models"}, "a4f2e6c38454c9e7b4068a456813d622b91f2663": {"ta_keywords": "speech diadochokinetic;pulse;ddk;short pulse;measurement;rate;data;protocol;method;form;problems;case", "pdf_keywords": ""}, "d408be961d0db8b97c0ca6b2fc7afd3c9dc914e7": {"ta_keywords": "modular multimodal mobility management;modular multimodal mobility platform;platform;manageability;extensible application;resilience;platform functions;architecture;capabilities;presence;system;patterns;guidelines;failure;principles;future practitioners;field", "pdf_keywords": ""}, "4fe70c172cc38c2eb15103f0f1eac4e6766c60e6": {"ta_keywords": "stochastic power spectrum;noisy noisy signal;noisy signal;power spectrum;estimation;method;new method", "pdf_keywords": ""}, "efe9fe804f34b18524708b18293508191bda78eb": {"ta_keywords": "redundant hardware;permanent faults;grid grid;grid;core nodes;energy consumption;tasks;faulty execution;novel approach;context;approach;paper", "pdf_keywords": ""}, "395044a2e3f5624b2471fb28826e7dbb1009356e": {"ta_keywords": "supervised textual embeddings;supervised embeddings;sentence similarity;sentiment classification task;tasks;comparative study;art architectures;art architecture;art;prior;art performance;state", "pdf_keywords": "paraphrastic sentence embeddings;word embeddings;universal sentence embeddings;textual similarity tasks;sentence similarity;textual similarity datasets;paraphrase classification;supervised hierarchical sentence model;strong word embeddings;semantic text similarity;recurrent neural networks;embeddings;annotated phrase pairs;sentence space;recurrent neural network;word paraphrases;word sequences;natural language processing;deep neural models;empirical recurrent neural network model;term memory network;sentences;compositional models;entailment models;natural language;entailment tasks;compositional architectures;learning models;standard neural architecture;neural networks"}, "14551d2bf2584bb1ea7ad69f9a64419bab82bb6e": {"ta_keywords": "sound events;audio data;data augmentation;convolution;detection;local features;transformers;d2020 task2020;kuramoto;novel method;experimental evaluation;method;various types", "pdf_keywords": ""}, "469ad889bd628e2cf46424f7097c4830719ec740": {"ta_keywords": "intelligibility space;vowel score;human transcription accuracy;intelligibility scoring;space representations;intelligibility score;talker;automatic estimation;unsupervised mappings;low dimensional space;approximate convex hull sampling;rankings;improved performance;correlation;paper;method;terms", "pdf_keywords": ""}, "d9944e13a38e5ca685985c9b5c050ec6d300e104": {"ta_keywords": "random walk;estimation;probability;method;new method;concept", "pdf_keywords": ""}, "ba602ea9aaecab5a3ad243211f110ae7db4cc66a": {"ta_keywords": "risk minimization;multiple local minimizers;performative risk;performative alignment;various equilibria;initial conditions;geometric condition;term behavior;convergence;sufficient conditions;real world situations;attraction;region;setting;notion;system;case;significant impact", "pdf_keywords": ""}, "ef6a4d8bf248944ca1d0cfdc107d3bb107f57bff": {"ta_keywords": "zeeman field strength;zeeman field;ep cluster;stability;field strength;field;standard model;framework;interplay;effect;suitable choice", "pdf_keywords": ""}, "6c68866e6486923d2e8b999de57d450c9d4febab": {"ta_keywords": "neural machine translation;translation quality;decoding;snp;phrase;different language pairs;nmt;nmt outputs;cost;outputs;method", "pdf_keywords": ""}, "c2b22a18ea2ed444c6c1f5bb27ab55bda2b44567": {"ta_keywords": "speech translation systems;language emphasis prediction;conditional random fields;neural network;long distance dependencies;long distance dependence;input;distribution;information;model;crfs;target;new model;previous model;experiments", "pdf_keywords": ""}, "b799d66c710dd82a1b925b9c31e55a0d2d99b624": {"ta_keywords": "urban dynamics;rich urban dynamics;life mobility dataset;human activities;hidden model;semantics;population;interest;type;different time slots;point;method;scale;volume;error;frequency", "pdf_keywords": ""}, "0c39c0dc296a902e4a5eb85182209f7b9e6053b0": {"ta_keywords": "dynamic network architectures;novel dataflow;dynamic network structure;dynamic nns;vertex function;backpropagation;different graphs;execution opportunities;cavs;execution;framework;art frameworks;state;approach;exposes;efficacy", "pdf_keywords": "deep dataflow graphs;dynamic dataflow graphs;dynamic deep learning;graph computing;vertexcentric programming interface;dynamic networks;static dataflow graphs;dataflow graphs;dynamic neural networks;new graph computing application;novel graph execution logic;deep learning;different dynamic neural networks;novel deep neural network architecture;other graph computing applications;deep neural network;deep learning framework;deep learning models;variable vertex function;dynamic nns;vertex function;symbolic programming;dynamic network structure;complex nn architectures;vertex;network networks;graph construction;deep batching;input graph;arbitrary graphs"}, "57026b2d45fa59c6326b5a1d2e27626403f083ba": {"ta_keywords": "artificial intelligence ethics;artificial intelligence educators;artificial intelligence course;general artificial intelligence course;practical case studies;resources;links;use;suggestions", "pdf_keywords": "artificial intelligence ethics;alone artificial intelligence ethics course;robotic intelligence;artificial intelligence educators;artificial intelligence courses;ai;artificial intelligence;ai systems;artificial intelligence systems;general artificial intelligence course;killer robots;ethics;associated ethical responsibilities;autonomous systems;robot;intelligence;robotic projects;robotic development;ethical issues;normative tools;machine learning system;algorithms;agents;autonomous vehicle industry;tasks;simple algorithms;utilitarianism;responsibilities;human beings;technology"}, "653add540adae12491fade7e18ec4e1e4288b4a7": {"ta_keywords": "course planning;software tool;learning process;software;teaching;undergraduate students;study;electronic setting;performance", "pdf_keywords": "academic advising;advising;recommendation system;student goals;recommendation systems;academic advisors;course selection;student performance;students;academic advisor;academic advice;student;other students;theyacademic advising;many students;advisors;graduation requirements;academic course;grades;large user study;course requirement checking;graduate courses;advisor;student survey;computer science students;meeting course requirements;semesters;recommendation;academic performance;recommendations"}, "6fe62b967376361d7cd55e1033ab968895841d67": {"ta_keywords": "interpretable deep learning text mining algorithm;focused concept miner;coherent corpus;text data;online newsgroups;predictive performance;crowdfunding platform;fcm;level concepts;outcome;constructs;user", "pdf_keywords": ""}, "29001ac04e61dfffb8e24ffd3e351ece12ce44af": {"ta_keywords": "mixture signal;source separation system;domain masks;2mix dataset;signal;estimation;discrete representations;inference schemes;phase;estimates;representations;several training;performance;novel approaches;efficacy", "pdf_keywords": "speech enhancement;speech separation;magnitude mask;softmax activation;softmax layer;deep learning;complex mask;frequency masks;phase mask;convex softmax activation;amplitude mask;source separation;complex ratio mask;ideal amplitude mask;various magnitude representations;magnitude estimation;deep clustering;classical sigmoidal units;phasebook representations;deep neural network framework;acoustic signal;training phasebook;mask;band phase reconstruction;speech;frequency signalwe;noisy noisy noisy phase;training data;acoustic sound field;magnitude"}, "5dce0fd43a21825bebd8121fd0a28155d524c44c": {"ta_keywords": "gaussian noise;electron density;dimensional electron gas;strong magnetic field;dynamics;field;effect;expense;presence", "pdf_keywords": ""}, "4b2d583e22f378f9104814d9f63cda411ddd5825": {"ta_keywords": "sememe prediction;multilingual words;dimensional semantic space;sememes;model correlations;world datasets;baseline methods;model;significant improvements;novel framework;experiments", "pdf_keywords": ""}, "f1513d72cb5dd6d70541cce0da36b77467128d13": {"ta_keywords": "entrance exam;novel method", "pdf_keywords": ""}, "838fbfd9066dbbac6c10059c5b183046fb1cd9d1": {"ta_keywords": "deep active learning;natural language processing;uncertainty estimates;dataset;classic uncertainty;multiple tasks;empirical study;bayes;dropout;multiple models;large scale;acquisition functions;full suite;settings", "pdf_keywords": "active learning;active learning approaches;joint learning;deep learning;large sparse classification datasets;learning framework;reinforcement learning;natural language processing;bayesian uncertainty;novel empirical representation;bayes;classifying;novel partial feedback;uncertainty estimates;random acquisition;classic uncertainty;image classification;regularization technique;variational model;models;multiple datasets;tasks;training pool;uncertainty;accuracy;task;bias;acquisition functions;salient features;multiple tasks"}, "5693c74eb8ffde1490ba480fdc963f008243906a": {"ta_keywords": "tags;tag tag;collection;local knowledge;weight;framework;user", "pdf_keywords": ""}, "a8239258abded4f08d1bf270c2e86662f4dc1760": {"ta_keywords": "student learning;learning;prior knowledge;synthetic student;student errors;knowledge;weak prior knowledge;student;computational model;sim_the student;challenging problem;problem;accuracy;examples;complex problem;errors;error;prone process;performance;domain;innovative application;impact", "pdf_keywords": ""}, "a182a8a0678857df5c513d52469fa707c32e69ec": {"ta_keywords": "underlying translation rules;continuous space rule selection;rule selection;different translation tasks;syntax;previous minimum entropy;sentence context;global data;csrs;domains;score;dis;points;msm;model;set", "pdf_keywords": ""}, "4e3935ef7da6bcbb202ec7f8b285c313cadcd044": {"ta_keywords": "information;questions;document;papers;evidence;performance;new study;model", "pdf_keywords": "natural language queries;natural language processing;large corpus;nlp;underlying text;computational linguisticswe;computational linguistics;research papers;document;questions;documents;papers;queries;search;computational computational computational linguistics;deep networks;regular readers;linguistic complexity;text;linguistic similarity;dimensional documents;paper;neural network;large sample;lexical representation;task;evidence prediction scaffolding;entire paper;research;training data"}, "85e148ac629b1b38556c5fe5f8d657f2eb01a701": {"ta_keywords": "orbit interaction;spin;orbit;coupled system;dynamics;effect;suppression;enhancement;specific way", "pdf_keywords": ""}, "a997d6e253f08a3e589432c611d6d2a3097d7629": {"ta_keywords": "collaborative research;collaborative document repository;novel research paper tool;research papers;annotated users;paper;tool;collection;style", "pdf_keywords": ""}, "3ec37205c9201fc891ab51da200e361fdc34bfb3": {"ta_keywords": "deep learning architectures;documents;performance;best strategies;best strategy;different strategies;construction;purpose;fundamental aspect;comparative study;process;list;success;choice", "pdf_keywords": "word embeddings;novel deep learning architectures;deep learning;contextual embeddings;embeddings;random document encoder;random random corpora;reader;stanford linearized corpora;cognitive learning;natural language;neural network;corpora;documents;vocabulary;comprehension;document;text;cbrn2 library;different crc benchmarks;content;document tokens;linguistic structure;crc models;encoding;mathematical representations;brain;random variable encoding;natural words;models"}, "cfad4dc5f1f7fcaf7ca318acf672ad92d47f8413": {"ta_keywords": "many hiring decisions;hiring function;economics;market;paradox;empirical measures;rate;intrinsic conflict;conflict;fundamental aspect;significant tension;fact;field;practices", "pdf_keywords": "novel algorithmic hiring algorithm;algorithmic hiring;hiring process;hiring decisions;usual hiring process;employment pipeline;hiring;algorithmic bias mitigation;candidate evaluations;ordering hiring;hiringwe;recruitment;job seekers;job ad;biasaware methodology;biases;bias;employment context;employment;workforce imbalance;screening stage;workforce;screening;employment opportunity;selection process;employment dynamics;discrimination;fair machine learning algorithm;biased results;employer"}, "f889723a4427e914e4e32547dfd0ca4996170180": {"ta_keywords": "voice conversion challenge;target text prediction;prosody;subjective evaluations;conversion process;pk2020 benchmark;target;vcc2020;quantitative evaluation;latest generation;tp;results;effectiveness;dependent manner", "pdf_keywords": "speech prediction;new prosody modeling technique;advanced speaker learning;latest voice conversion challenge;voice conversion;target text prediction;prosody model;quality speech;target speaker;standard european voice conversion;quantitative speech recognition;input utterance;prosody;novel speech;speech;neural vocoder;linguistic model;prosody pattern;linguistic modelwe;different linguistic representations;linguistic representation;different speakers;correct speech;advanced knowledge acquisition;text;vcc2020;benchmark;automatic recognition;model;tesi"}, "f4cca8ea79e26fa20a91c3d3b769c9f7b82a6207": {"ta_keywords": "virtual spherical microphone array;spectral notches;spherical array;median plane;icr pinna wall;pinna;bessel series;signal;extraction;experimental results;available database;method;novel method;use;form;paper", "pdf_keywords": ""}, "8c38bffc058d558e7c734032ba63942865e05ae4": {"ta_keywords": "incomplete knowledge bases;deductive reasoning;complex queries;novel method;substantial improvements;new method;better performance;art", "pdf_keywords": ""}, "e4a6bc3ac385b8982bbbe0a2a5ac0c79101ec979": {"ta_keywords": "gaussian wave distribution;gaussian wave;gaussian noise;wave distribution;propagation;wave;region;space;medium;effect", "pdf_keywords": ""}, "9bbeb4f0e48032df19f9f6a08839da5d2e60e8eb": {"ta_keywords": "multimodal broadcast speech recognition;binary masking algorithm;discriminative language modeling;second chime challenge;prior distributions;minimum bayes risk;arrival;efficient method;time difference;novel method;method;effectiveness", "pdf_keywords": ""}, "8f963beca679cb1129df0a944c6de4b126e20fd5": {"ta_keywords": "seq2seq decoder;external seq2seq decoder;seq2seq models;long short term memory;language model;lstm;memory cell state;hidden state;several methods;methods", "pdf_keywords": "seq2seq decoder;seq2seq model;seq2seq;seq2seq model output;term memory;seq2seq seq decoder givenwe;cell control fusion decoder;memory cell state;memory cell states;seq2seq externalthis paper;fusion;rnnlm;encoder;decoder;rnn;sequence;several new fusion methods;new fusion cell control method;large vocabulary speech recognition;end speech recognition;encoder units;cost fusion model;language model;several fusion schemes;decoder problem;deep neural network;neural network;genome;lsm;lsr"}, "b2c47dd46bf7087b754aed45f06b6196cf2b1c28": {"ta_keywords": "acute abdomen;radiographic technique;diagnostic imaging;smirnov equation;diagnosis;motion;equations;chapter", "pdf_keywords": ""}, "46d87d4614d9353f1b7d527333073ef9109bfaea": {"ta_keywords": "item labeling problem;crowd;new method", "pdf_keywords": ""}, "82ae0d4b41046ccedb435ece08a61f198cf77bb9": {"ta_keywords": "basketball game report corpus;unsupervised generation;text content;large corpus;sentence transitions;full content;sentence;games;reference sentence;explicit content coverage constraints;same writing style;neural method;objectives;record;novel method;approach", "pdf_keywords": "text generation;natural language generation;hybrid attention;document generation;stylistic control;writing style;structured content;style sources;hybrid attentioncopy approach;style representations;style transfer;style imitation;novel style transfer model;novel attention;sentences;text;exemplar sentences;source content;content fidelity;content;automatic andcontrolled generation;restaurant recommendation;weak supervisions;rich weak supervisions;restaurant reports;corpora;automatic evaluation;restaurants;restaurant;neural approach"}, "bc1832e8b8d4e5edf987e1562b578bd9aa5e18a9": {"ta_keywords": "neural networks;acoustic conditions;similarity;high accuracy;mismatched training;mismatched condition;comparison;test set;data;sequence;sample;words;results;method;respect;new method", "pdf_keywords": ""}, "e212f788c701370af02b138d2a61e180cddfb138": {"ta_keywords": "single target language;multiple target languages;single source language;target language;best translation;language;original language;general method;framework;information;method;interest", "pdf_keywords": ""}, "c5dfc5fe7102fd8647edd1c9483aded82557e544": {"ta_keywords": "global counterfactual explanations;overall recourse costs;recourses;predictive model evaluation;interpretability;explanations;accurate summary;comprehensive summary;novel objective;novel model;correctness;individuals;entire population;novel approach;population;agnostic approach;framework", "pdf_keywords": "counterfactual recourse generation;global counterfactual explanations;counterfactual prediction;recourse counterfactuals;elementary counterfactual explanations;global counterfactual explanation;actionable recourse;accurate recourse summaries;recourse accuracy;recourse rules;several prior recourse finding techniques;recourse correctness;recourse summaries;interpretable explanations;recourses;recourse discovery;individual recourse;recourse;recourse rule;recourse sets;recourse generation;level recourse finding techniques;recidivismrecourse;recourse search;recourses output;recourse problem;predictivetheorems;recourse errors;predictive models;judicial bail decisions"}, "90ed32fa521b9e85f1c9efe356619814a2e79961": {"ta_keywords": "spectral density;quantum system;exact diagonalization;calculation;method;new method", "pdf_keywords": ""}, "a75869d69cc86f501939c237ae4711aa2885f6a6": {"ta_keywords": "diverse languages;different languages;universal lexical representation;learning problem;strategy;approach", "pdf_keywords": "low resource machine translation tasks;resource machine translation;low resource translation;neural machine translation;neural machine translation system;low resource translation scenarios;new machine translation;future neural machine translationwe;machine translation;machine translation system;resource translation;single translation kernel;resource language pairs;resource language;resource languages;language pairs;metan;translation;transfer learning;multiple languages;multilingual scenario;low resource;translationwe present;different languages;generalization performance;agnostic metalearning;diverse languages;gradient descent;inmachine translation;neural machine"}, "18e70ad07561cf09a2d7f0da992a0e87a5e5c0a8": {"ta_keywords": "recognition;speech;method;new method", "pdf_keywords": ""}, "8cebfae7cd436241eb5c3442e687a913a75a5531": {"ta_keywords": "sense induction;same sense granularity;russian language;disambiguation;contexts;context;words;task;new datasets;word;group;simo;participants;evaluation results;results;advance;unknown;form", "pdf_keywords": "word sense induction;word sense induction task;standard word sense induction task;english semantic processing;semantic words;standard word sensethis paper;linguistic properties;linguistic similarity;homonymous senses;linguistic documents;linguistic corpora;different sense inventories;linguistic features;semantic structure;complex words;polysemous senses;sense inventories;polysemous words;various senses;russian language;sentence representations;semantic similarity systems;senses;novel sense;russian national corpus;large corpus;words;interaction;english workshop;repulsive interactions"}, "c6bb04f3d8000b7e800f6359082de39548c7da79": {"ta_keywords": "nonparametric language models;locality information;locality features;structural locality;local neighborhoods;such models;features;learned parameters;models;web;examples;likelihood;model efficacy;different domains;world;paper;effective approach;use;experiments;access", "pdf_keywords": "structural locality;locality information;relevant locality information;locality features;specific locality;locality;domain adaptation;own locality;datastore context representations;local hierarchies;natural language;programming language domain;common hierarchical attributes;language source code;relevant contexts;natural language processing;word prediction;unstructured text;ubiquitous feature;multidimensional learning trees;scale machine learning;machine learning;domain entities;local neighborhoods;natural language wikipedia articles;language;learned parameters;neural models;attributes;features"}, "97846070369f66c3080a0803be58e96963dec581": {"ta_keywords": "social networks;epidemics;harmful websites;online misinformation;pandemic;internet;internet infrastructure;widespread use;use;related attacks;major worldwide effort;current evidence;study", "pdf_keywords": ""}, "08f6819e66318cd49cddefd5d690a752d1098da7": {"ta_keywords": "argumentative argumentative component;claims;different datasets;different conceptualizations;lexical level;properties;comparative study;gaps;system configurations", "pdf_keywords": "argumentative argumentative dataset;argumentative discourse;argumentative sentences;claim detection;rhetoric complexity;discourse relations;discourse;arguments;sentence labeling;assertions;text classification;claim identification;claim claims;lexical features;lexical indicators;famous corpus;argument;corpora;conversational structure;sentences;claims;lexical version;few lexical clues;individual words;informative portion;learners;empirical similarity;classifier;embeddings;such different conceptualizations"}, "9f73c3f86026c21d0e5e55c70462952c6ada1175": {"ta_keywords": "deep neural networks;image models;training;high loss;event;examples;iteration;error rates;evaluation;technique;approaches;paper;variety", "pdf_keywords": "expensive backpropagation steps;deep learning;deep neural networks;traditional deep learning;deep learning dataset;training iterations;deep learning problem;neural networks;low loss;training images;loss exampleswe;learning;high loss;bottleneck;batch;lower losses;empirical image classification models;simple learning rate schedule;learning rate schedule;training;training rates;regular batch;backward pass;selection minibatch;machine learning;future examples;speedup;brain;training time;training process"}, "dc984ea8be018a0244b40468d13f7b734ab55bac": {"ta_keywords": "neural machine translation;discrete translation lexicons;encode translations;standard nmt probability;nmt;frequency words;corpora;performance;improvement;linear interpolation;lebesgue score;probability;bias;convergence time;systems;methods;method;experiments", "pdf_keywords": "lexical translation probabilities;neural machine translation engine;neural machine translation;machine translation;machine translation engine;attentional nmt models;probabilistic lexicons;attention vectors;discrete probabilistic lexicons;translation model;content word translation;translation accuracy;translation process;large target vocabulary;sequence learning;linguistic models;large corpus;next target word;encode translations;frequency content words;lexicons;attn lexicon;lexicon;nt lexicons;target word;handmade lexicons;source word;frequency words;neural machine;translation"}, "0533ccdc4840eed0fe1769b5e78da912631be609": {"ta_keywords": "nonlinear optical solitons;consistent nonlinear optical field theory;nonlinear ordinary differential equation;phase;space representation;self;theory", "pdf_keywords": ""}, "4b73f4956c31cd10994c73b21e2c38a60a68d03e": {"ta_keywords": "assignment;weighted average;disjoint preferences;polynomial time complexity;averages;objects;order;set;problem;novel approach", "pdf_keywords": "optimal assignment;maximal assignment;sided matching problem;novel assignment formulation;paper assignment problem;utility assignment;agent evaluation algorithm;transitive preference relation;optimality;egalitarian assignment;egalitarian assignments;matching process;algorithm;agent capacities;poly programming problem;agent systems;assignment;multiple agents;utilitarian assignment;agent system;winner election problem;assignments;poly programming;certain utility;more equivalence classes;owa assignment;novel algorithm;constraints;equivalence classes;classical assignments"}, "b131cf78363993e4126b2562a156bd9d046c8bc4": {"ta_keywords": "pivot languages;pivot language;syntactic roles;languages;bari languages;syntactic properties;constituent words;translation;interoperability;strumicarus;new method;method;united states", "pdf_keywords": ""}, "18e5fb8cec55a75b288a499c57d77ede541dc049": {"ta_keywords": "commonsense;knowledge sources;language models;tasks;shot question;novel neuro;symbolic framework;benchmarks;training regimes;data;empirical results;data generation strategies;framework;form;set;impact", "pdf_keywords": ""}, "5b1bb1f6ed091dfd53adf7ebbcda2c48a3b67c2c": {"ta_keywords": "supervised semantic frame induction;semantic frame induction task;context labeling;word;subtask;estimatively;noisy environment;message;structure;presence;approach;method;best performance;novel method", "pdf_keywords": "verb clustering;unsupervised semantic frame;word sense induction approach;context embeddings;semantic role labels;semantic labeling;agglomerative clustering;verb roles;syntactical features;verbs;verb senses;embeddings;semantic role labelswe;sentence ordering;verb;word2vec models;machine learning tasks;role labelling;generic tasks;clustering;specific representations;role induction;generic clustering model;tasks;subtask;unsupervised machine learning;task;best clustering results;vectors;word"}, "384bf224d91a1691c9e6384201483121e2e7ddab": {"ta_keywords": "exact subspace;community recovery;novel algorithm;algorithm;equivalence principle;concept;fundamental limit", "pdf_keywords": ""}, "e01aa6f8ce625469b6f161d7ab9e61a60ac33798": {"ta_keywords": "latency streaming codes;optimal online code constructions;optimal offline rate;online schemes;rate;settings;constructions;broad parameter regimes;setting;fact;analysis", "pdf_keywords": "optimal streaming code;online streaming codes;low complexity streaming codes;streaming codes;size streaming code;optimal streaming;optimal online code constructions;lossless online communication channel;live video streaming;size message packets;low complexity streamingwe;streaming model;online communications;packet loss channel;channel packets;message packets;channel packet;optimal offline rate;single message packet;communication schemes;new communication scheme;message packet;message size;communication channel;coding;channels;channel;communication networks;point communication channels;generalized code"}, "6b7004138ee2de5ec52e500cae4e65390e961e16": {"ta_keywords": "kernel cut solvers;kernel cutting;segmentation;spectral relaxation;new linear kernel;standard regularization;optimization;cross validation;joint energy;tasks;data;new approach;problem;approach;application;power;variety;combination", "pdf_keywords": ""}, "e0236106e51984e4ea6bbbd1fb5ce57abf3e4e5e": {"ta_keywords": "worldwide pandemic;facial mask;social distance measure;masks;mask;virus;new viruses;risk;appearance;images;global economic crisis;people;emergence;person;use;order", "pdf_keywords": ""}, "af92dd61340808f3008a84ae57803bb4aa57d03b": {"ta_keywords": "pose;adaptive attention;neural architecture;audio;interlocutor audio;avatar;refinement model;model;participants;monadic;body;dyadic data;novel model;interlocutor;set;dram;importance", "pdf_keywords": "avatar pose;attention model;pose;nonverbal representations;nonverbal objects;nonverbal object;audio conversation;recurrent neural network;temporal convolutional network;adaptive attention betweenwe;future body;like avatar;avatar;person;face interactions;intra personal dynamics;human;neural architecture;conversation;neural networks;novel autoregressive temporal model;selective attention;models;audio;neural network;joint adaptation;interpersonal dynamics;model;dyadic conversations;dyadic interpersonal dynamics"}, "5b1c0152bbb12ece2a8817c727e33e6d5c503065": {"ta_keywords": "machine learning algorithms;robustness;system noise;codes;theoretic perspective", "pdf_keywords": ""}, "c395595cf7be23f7d90cbca98d8c7861ebfd884d": {"ta_keywords": "classification metrics;classifiers;machine learning tasks;comment toxicity;metrics;annotator;performance measures;datasets;current metrics;roc;performance;misinformation;machine;values;transformation;methods;paper;practice", "pdf_keywords": "machine learning tasks;classifiers;social computing tasks;comment toxicity;machine learning classifiers;popular social computing tasks;classification metrics;classifier;classic social computing tasks;classifier performance;classic machine learning tasks;crowdsourcing;annotator classifiers;machine learning;social computing;new classifier;disagreement deconvolution;classification decision;empirical accuracy;crowdsourcing process;social computing context;annotators;disagreement deconvolution iswe;face datasets;outliers;oracle classifier;procedure towe introduce disagreement deconvolution;performance metrics;accuracy;training data"}, "37074b2b9cebd89e4a92d20f41eec7360e11fe5a": {"ta_keywords": "better online speech recognition performance;topological maskpredict;speech recognition system;connectionist temporal classification;maskpredict;low latency regime;nar;significant improvements;system;method;approach;experimental results", "pdf_keywords": "e2e streaming speech recognition system;speech recognition;automatic speech recognition;speech recognition system;automatic speech recognition system;attention encoder;encoder;ml decoder;input audio;connectionist temporal classification model;amplitude prediction;speech;input tokens;attention;next token;recognition;random access control;last token;input data;blockwise;collisionless kernel representations;output data;amplitude amplitudes;block;amplitude;novel end;novel e2e;amplitudes;coherent sentences;tokens"}, "bd6018632a360cb567da8e50e1717ff526503845": {"ta_keywords": "novel probabilistic conditional stochastic beam search;optimal conditional stochastic beam search;noisy noisy beam search;noiseless partial differential equations;conditional stochastic sampling;conditional conditional stochastic sampling;pdfs;pdf;pmsn;method;design;class", "pdf_keywords": "conditional stochastic beam search;standard conditional stochastic beam search;stochastic beam search method;stochastic beam search algorithm;stochastic beam search problem;beam search;noisy beam search problems;conditional sampling;rigorous stochastic generalization;sample sets;optimal weights;standard quadratic search algorithm;flexible sampling principle;new sampling strategy;importance sampling strategy;search;consistent estimators;stochasticity;beam;faireq algorithm;stochastic version ofwe;neural machine translation algorithm;sequence models;optimization;algorithm;nodes;standard inclusion probability estimator;successful algorithm;weights;neural machine"}, "6494cd26511c076186673c9a636d21d1dfed8d5a": {"ta_keywords": "student network;teacher network;student;array;features;network;conventional student;soft targets;original noisy signals;ss;input;method;significant improvement;same task;framework;paper;experiments;approach", "pdf_keywords": ""}, "c8f9313ce8416a7be079935d1cbb637705f75182": {"ta_keywords": "translation model probabilities;gram similarity;gram statistics;language model;thesaurus;translation;individuality;automatic construction;significant improvements;estimation;quality;experimental evaluations;method", "pdf_keywords": ""}, "8c7628641450203b0aa959b5a69729ff906760ff": {"ta_keywords": "neural diarization;attractor calculation module;sequence encoder;speaker overlaps;speaker;speakers;attractors;encoder;wise attractors;embeddings;decoder;eend;sequence;unknown number;eda;end;method;paper", "pdf_keywords": "speaker diarization systems;end speaker diarization method;speaker diarization;neural diarization;speaker vector;speech processing;speaker attractors;simulated speaker;acoustic feature sequence;speech separation method;diarization accuracy;speech separation;novel speech separation method;independent speech separation;separated speaker;speaker identification;speaker reference record;acoustic signal processing;neural networks;neural network;attentive eend model;diarization results;diarization context;deep network;diarization protocol;channel speaker;speaker;speaker overlap;novel speaker;attractor calculation module"}, "5aea95e1ae78a66474051a330ded374e199b658c": {"ta_keywords": "graph models;citation networks;neighborhood aggregation;knowledge networks;neighborhood representation;graph;representation learning framework;bioinformatics;representations;structure;aware behavior;essential features;dependent behavior;state;art performance;experiments;number", "pdf_keywords": "graph representations;underlying graph structure;subgraph features;graph structures;neighborhood aggregation;neighborhood aggregation models;local subgraph;complex networks;graphs;different subgraph structures;large graph;citation networks;jumping networks;partial graph representation;nodes;neighborhood aggregation procedure;knowledge networks;node representation;networks;graph;unseen graphs;important nodes;local aggregation;lazy aggregation;large scale data;different neighborhood structures;networkswe;aggregation;aware representations;node features"}, "564dec6eab6115ecd604f22738ce0b47777f6e17": {"ta_keywords": "variational bayesian estimation;speech recognition;speech recognition procedures;speech classification;probabilistic bayesian estimation;variational posterior distribution;acoustic modeling;variational;clustering;total probabilistic framework;maximum likelihood;vbec;ml parameters;ml;framework;approach;paper", "pdf_keywords": ""}, "8c5465eb110d0cab951ca6858a0d51ae759d2f9c": {"ta_keywords": "interpretable neural predictions;rationale extraction;rationale model;rationale;neural network models;attention mechanisms;classifier;joint training;novel approach;approach;previous work;further uses", "pdf_keywords": "text classifiers;text classifier;text classification;sentiment classification problem;text classification problem;level sentiment annotations;text rationales;sentiment analysis;natural language processing;classifier;natural language representations;regularization;interpretable classifier;double neural network model;large scale word embeddings;neural networks;stochastic classification;natural language inference;natural language;natural languagewe;double feature;sentence annotations;neural network;classification problems;text;sentiment;automatic extraction;pattern recognition;computationalneural networks;double feature feature"}, "2660dbba723573266edb2a0a4929e6847ae83212": {"ta_keywords": "vector speaker adaptation technique;density ratio language model fusion technique;conversational systems;conversational model;conversational tasks;word error rate;approach;llllllllllllllll;effectiveness;factor", "pdf_keywords": "acoustic feature sequences;density ratio language model fusion;external language model fusion;prediction network embeddings;speaker adaptation;conversational architecture;prediction network vectors;conversational systems;prediction layers;neural networks;automatic speech recognition;bidirectional rnn;random utterances;neural network;novel multiplicative integration;target domain corpus;multiplicative integration;encoder;joint network;multiple sources;model regularization;model combination;machine learning model;iterative integration;data augmentation;character outputs;different languages;simple architecture;input;end modelwe"}, "94f22d7a8b48784b3d8975616e20d8028a08162f": {"ta_keywords": "model;dynamics;generalization;natural generalization;construction", "pdf_keywords": ""}, "d119cc4051ed1206a0dac963cd23a84acf77fea7": {"ta_keywords": "uncalibrated forecaster;probabilistic forecasts;threshold calibration;threshold decisions;decision loss;calibration;decision rule;decision makers;different decision rules;calibration procedure;deployment;loss;efficient algorithm;setting;outcome;practice;stronger notion;condition", "pdf_keywords": "uncalibrated forecastwe propose threshold calibration;threshold calibration;uncalibrated forecaster;threshold losswe;threshold loss function;threshold decisions;threshold decision;threshold calibration error;accurate decision loss prediction;forecaster;threshold;new calibration notion;forecasters;forecasting;prediction function;distribution calibration;forecast;probabilistic output;prediction;decision loss;calibration function;reliability gap;calibration;nonlinear decision;predictive functions;noisy noisy noisy forecast;new calibration method;reliability;stochastic estimation procedure;tf calibrations"}, "6a9795853e5f39325deb0d916fe22d9e5a202a9f": {"ta_keywords": "early ampres printers;successful publishers;early ampres;prints;printers;early identifications;original sloan gemindrome;books;papers;sloan gemindrome;identification;article;novel method;method", "pdf_keywords": ""}, "5270b626feb66c8c363e93ba6608daae93c5003b": {"ta_keywords": "knowledge modification;specific factual knowledge;transformers model;knowledge;unmodified facts;transformers;models;model performance;key components;discovery;privacy;many scenarios;new task;unintended biases;several approaches;task;natural baseline performances", "pdf_keywords": "learned language models;memorization;language models;implicit memorization;language model;autoregressive language models;new knowledge modification benchmark;language models canwe;scale language models;explicit memory modules;memory modification function;memory;explicit memory modulesin;knowledge modification task;deep bidirectional transformers model;implicit factual knowledge;unstructured knowledge;factual knowledge;specific factual knowledge;unmodified facts;tuning datasets;deep learning machine;constrained gradient descent algorithm;knowledge;iterative minimization;large models;sentences;model performance;implicit memories;models"}, "7b99c51d562e33309a46601c846abbe72a65c6a4": {"ta_keywords": "best intermediate transfer tasks;tuning approaches;respective datasets;computational cost;accuracy;methods;scenarios;different methods;estimates;comparative study;shot fine;variety", "pdf_keywords": "intermediate transfer learning;transfer learning;intermediate task selection;intermediate task data;best intermediate task;commonsense reasoning tasks;intermediate tasks;target tasks;machine learning pipeline;task selection;task selection problems;intermediate task;beneficial intermediate tasks;computational tasks;target task;sequential adapter training setup;challenging tasks;task selection problem;fewshot transfer;transfer;fewshot transfer model;tasks;machine learning;training adapters;beneficial tasks;empirical datasets;training;sentence encoders;task;computational challenges"}, "c47c8c2527bf2ca8339c342f44db2218a0cbcbbd": {"ta_keywords": "knowledge graph identification;knowledge graph construction;probabilistic soft logic;knowledge base construction systems;information extraction;art results;key problem;ppl;methods;order;present state;problem;magnitude", "pdf_keywords": ""}, "4eb22b488052c430170139c492674aa05512f7bf": {"ta_keywords": "molecular lennard;force optimization;final forging;jones;strong force;msc;process;paper;results;presence", "pdf_keywords": ""}, "f394c5101d7bfc3d8055f9391a83f7e2395dec4a": {"ta_keywords": "supervised programs;openmp parallelization directives;supervised learning algorithm;underlying program;npb code;algorithm;performance;time structure;underlying message;approach;version", "pdf_keywords": ""}, "f41e6c832c9e0d5360b66ee7681d3b1ffd2d9c3d": {"ta_keywords": "hierarchical structure;scene navigation;object manipulation;task;deeper understanding;input data;model;high accuracy;alfred benchmark;ability;paper;problem;nature", "pdf_keywords": "hierarchical task structure;hierarchical task;task manipulation;task hierarchy;annotated tasks;prior task knowledge;object manipulation;tasks;planning;language navigation;scene navigation;task;accurate tasks;action manipulation;learning;level goal;navigation;navigation model;semantic structure;hierarchical manner;actions;vision;object detector;visual representation;action action;visual observations;action;detailed plans;higher generalization ability;agent"}, "c54ad6e29f3e516eecf0a72bd1f95b80e8617116": {"ta_keywords": "compressive phase retrieval problem;phase retrieval problem;memory complexity;phase;retrieval approach;complexity;algorithm;signal components;complex vector;few measurements;construction", "pdf_keywords": "compressive phase retrieval;sparse phase retrieval;phase retrieval algorithm;sparsegraph code;compressive sensing;phase retrieval;compressive sensing layer;compressive sensing problem;sparse signal;original compressed phase;sparse graph;sparse complex vector;phase information;phase retrieval stage;sensing codes;code phase;degree sparse;graph codes;sparse;code matrix;arbitrary measurement complexitywe;arbitrary measurement complexity;noisy quadratic measurement;irregular phase;phase;measurement matrix;degree phase;graph measurements;signal components;code algorithm"}, "044b502e5a00b5eeff1dd078ea03f491ca2c37bf": {"ta_keywords": "continuous phoneme recognition task;structural classification methods;state transducers;structured perceptron;conditional random field;features;wfsts;paper;baseline;methods;experiments", "pdf_keywords": ""}, "b9057dce43181a30aa3e0435c8ffc4c0b6f8f127": {"ta_keywords": "defeasible inference tasks;predictive graphs;meaningful graphs;defeasible inference task;graphs;such graphs;human accuracy;method;art performance;transfer;problem;state", "pdf_keywords": "inference graphs;such inference graphs;defeasible inference;graph generation;defeasible reasoning;defeasible inference problem;logic literature;graph representation;reasoning;graphs;autoregressive language models;related reasoning task;meaningful nodes;language model;generative model;linguistic structures;linguistic structure;conjectures;defeasible tasks;mechanical turk;nodes;empirical discovery;nonlinear mathematical linguistics;conclusions;artificial intelligence;language;human knowledge;languages;cognitive science;knowledge"}, "b143ee344fe3af4169bde8af8b682a2835dae4a4": {"ta_keywords": "novel task furn move;available agent models;agents;complex tasks;furniture;novel task;simple tasks;task;living room;novel feature;right strategy;piece;available dataset", "pdf_keywords": "agent furn move task;novel task furn move;coordinated navigation;cooperative tasks;unstructured navigation agents;coordinated actions;agent actions;optimal coordinated navigation;cooperative agent;visual navigation;optimal coordination;cooperative learning;planning trajectories;training agents;agent system;agents;robot motion;reinforcement learningwe;close coordination;heterogeneous actions;agent;visual environment;motion;agent models;optimal joint policy;multiplereply agents;uniform coordination;reinforcement learning;independent actions;unstructured learning"}, "c3930cb34241a42e03ed02cbc83a3c87dddd60cc": {"ta_keywords": "heterogeneous scoring systems;stories;reinforcement learning approach;quality;relevant stories;story;collection;set;problem", "pdf_keywords": ""}, "c9d7b1f9b13d6ea4ff45b908285cc65af959cc5b": {"ta_keywords": "languages;probability;grammars;language;particular measurable features;assignment;word;methods;conditions;method;basis;other method;problem", "pdf_keywords": ""}, "3050735eb35af3527276aa1952f79eb2483df3f0": {"ta_keywords": "conversational process;utterance;valid representation;belief;unconstrained context;performance;process;proof;principle proof;level", "pdf_keywords": ""}, "a556914c1b32372d47a36f2826cbe143ddae95ca": {"ta_keywords": "node attachment prediction task;taxonomy expansion model;natural supervision;query terms;feature representations;taxonomy;supervision signals;expansion;prediction;steam;node attachment task;self;natural self;multiview", "pdf_keywords": ""}, "d85c0032d7bb0bd220eb2df8ba6d2130bc87e79e": {"ta_keywords": "neural diarization;iterative pseudolabel method;committeebased training method;unlabeled data;eend;end;model;third dihard dataset show;target condition;paper;performance;experimental results", "pdf_keywords": "unsupervised speaker diarization;end speaker diarization;speaker diarization;traditional speaker diarization approach;speaker diarization scenarios;speaker diarizationwe;speech segmentation;speaker activity;neural diarization;voice activity;joint speech activities;audio mixtures;diarization system;training method;recognition;multiple speakers;training approach;single neural network;label information;clustering;unlabeled data;callhome dataset;training set;dimensional embeddings;modelin;eend;training;overlap detection;orthogonal representations;committee member"}, "571b4425498549c56c0828a824dc453ff6f482fc": {"ta_keywords": "delay tdma protocol;hybrid medium access control;delay;knowledge scheduler;protocol;mean delays;scheduler;tdma;qzmac;networks;zmac;qz;scheme;self;novel", "pdf_keywords": ""}, "0823f2187eeed53be8fd452decf6ed9a6a6cd124": {"ta_keywords": "semantic interactions;information processing;interactions;particles;mechanisms;understanding;context;study;particular interest;fundamental problem", "pdf_keywords": ""}, "5de24203bf98ae7f4c514bc0bd2a310caa47a047": {"ta_keywords": "efficient train coordination;vehicle re;flatland competition;traffic networks;competition;grid;agents;model;paper;problem;novel approaches;environment;methods;goal;results", "pdf_keywords": "deep reinforcement learning;reinforcement learning systems;reinforcement learning;reinforcement learning problem;train;scale rail grid learning model;autonomous agents;flatland challenge;flatland competition;railway switches;autonomous agentswe;vehicle re;neurips2020;learning environment;vehicles;optimal strategy;agents;popular neur circuit;spatial planning;traffic networks;vehicle;game;vrsp;rns;theneur circuit;term planning problem;decision cells;agent;competition;strategy"}, "254d1b8cf247ae8b19e017f7ba758d670207ddda": {"ta_keywords": "optimal beamforming parameters;automatic speech recognition;spatial filtering;generative model;array signals;network;array geometry;frequency domain;accuracy;task;cost;terms;chapter;results;experiments", "pdf_keywords": ""}, "1f5a1e959147e989e12846a5bd1d20234ef667d7": {"ta_keywords": "direct oral proteases;severe bleeding events;prothrombin region;prothrombin complex concentrates;proteases;plasma concentration;rivaroxaban;patients;dabigatran;apixaban public registry;sample;study;number", "pdf_keywords": ""}, "148f055083666c72945eea79833a19494f5f57c0": {"ta_keywords": "significant objects;objects;property;object;certain class;class;number;simple example;members", "pdf_keywords": ""}, "924ce584acc148be29ef905c228fda7fe552c0c2": {"ta_keywords": "stochastic logic programs;new probabilistic language;personalized pagerank;earlier relational learning algorithm;logic;short derivations;proof space;algorithm;proppr;correct approximate grounding scheme;proofs;extension;computation;linearized version;path;connection;problem", "pdf_keywords": "probabilistic logic network program;relational logic network;markov logic networks;probabilistic logic;statistical relational learning solutionwe;large knowledge bases;large scale knowledge base;personalized pagerank;knowledge graph;knowledge base;probabilistic language;stochastic logic programs;pagerank;noisy knowledge bases;multiple knowledge bases;database predicate;underlying database;large databases;relational database;faster inference;probabilistic structure;new probabilistic inference method;efficient inference;correct approximate inference scheme;new probabilistic learning method;sports knowledge bases;inference scheme;proveablycorrect approximate groundingwe;inference;discrete clause search program"}, "8c4d1e81c277f71cd9e3c9a0af356203c7948dca": {"ta_keywords": "transcription correction task;transcription transcription transcription;transcription process;predictive linguistic model;end documentation document;yoloxchi;shortage;novel method;essential features;source;end;quality;model;novel;use;approach", "pdf_keywords": "transcription document;tonal phonology;transcription;novice transcriptions;corpus;transcription style;bimoraic lexical;source language corpus;language lexicons;transcription settings;transcription correction;language processing;phonological processes;transcription process;unstructured transcription system;corpora;speech recognition;novice transcriptioner;homogeneous transcription format;transcription bottleneck;elliptic transcription;tonal variation;language documentation;languages;transcriber shortage;target language;standard language;conventional hmm approaches;language;natural language"}, "c5ed3d1a2ce418610a6fc9b5520a4f845279969a": {"ta_keywords": "parity queries;parity models;parity query;parity;paritym;queries;decoder;decodes;erasure;multiple queries;prediction;predictions;resilience;models;inference;systems;system;new approach;form", "pdf_keywords": ""}, "657329c633709dd1ac34a30d57341b186b1a47c2": {"ta_keywords": "dynamic sparse attention patterns;large search;language modeling;attention;sequence length;particular query;overall complexity;comparable models;time window;module;model;means;new approach;problem", "pdf_keywords": "sparse attention matrix;sparse attention module;dynamic sparse attention patterns;comparable sparse attention models;sparse attention;attention matrix;conventional attention matrices;efficient attention;attention matrices;sparse representations;recurrent attentionwe;attention;sparse;large long sequence benchmarks;minimum attention weight;language modeling;local attention;sparsewe;neural machine translation model;deep memory;range attention;attention heads;input vocabulary;high dimensional representations;long sequences;sequence length;short elementary sequences;neural models;deep learning representations;pattern recognition"}, "ba4a34680e09e77984624c95f5245d91b54373f6": {"ta_keywords": "multilingual machine learning models;benchmark;high accuracy;model;tasks;performance;large set;set", "pdf_keywords": "crosslingual language model;crosslingual generalization;crosslingual natural language inference corpus;multilingual learning;multilingual encoders;crosslingual data;translation tasks;robust crosslingual model;lingual models;multilingual;target language;multilingual documents;diverse languages;translation encoder;crosslingual question;translation data;translation system;languages;translation translation translation experiment;linguistic representation;linguistic representations;language structure;many translation;translation quality;language;crosslingual constraint constraint constraint;machine learning tasks;language families;structured prediction tasks;word representations"}, "927ff874d3ed9307356d256c31b79a0624b3c9d5": {"ta_keywords": "speech diarization;jhu multi;recognition;challenge baseline;dynamic models;second challenge;challenge;models;everyday setting;use;performance;feasibility;results;experiments;series", "pdf_keywords": "speaker recognition;speech recognition;speech activity detection;speaker diarization system;simultaneous speaker diarization;speaker diarization;speech stream;suppress speaker permutation errors;multimicrophone setting;speaker signals;deep deep learning;deep learning;simultaneous deep learning;speaker;sixth chime challenge;deep neural networks;acoustic model;acoustic reference field;deep neural network approach;common common speech model;neural networks;neural network;neural network performance;high diarization performance;simultaneous deep learningwe;speech;speakers;convolutional neural network;recognition;utterance"}, "c4efaeccd7f0d900b1df95dadf51bad74264f613": {"ta_keywords": "pure nash equilibrium profiles;pure nash equilibrium;probabilistic serial;optimal manipulation;time algorithms;utility;computation;rule;experiments;best responses;problem;existence", "pdf_keywords": "preference allocation;preference mechanism;nash equilibrium profiles;nash equilibrium exist;pure nash equilibrium;boundedly rational decisions;nash equilibrium;probabilistic agent;allocations;utility theory;allocation;allocation rule;preference rule;optimal manipulation;fractional allocation;probabilistic serial rule;new allocation rule;stochastic serial rules;expected utility;random assignment;computational complexity;random assignment problem;canonical allocation;utility;probabilistic serial;selection mechanism;complexity;selection policy;powerful probabilisticwe;more utility"}, "605bae6c397e4829dde7ff7b8ddb84782ec6e607": {"ta_keywords": "virus replication cycle;stochastic search;influenza;connected stochastic equations;selection;selection selection selection problem;infectious diseases;selection problem;virus;potential targets;possible targets;comprehensive map;set", "pdf_keywords": ""}, "a18b49fae647ae08711c2384611b3537485e8408": {"ta_keywords": "speech translation system;machine translation system;simultaneous interpretation data;translation style;simultaneous interpreter;performance;sources;system;experience;year;variety", "pdf_keywords": ""}, "417259d40d0d8b3ca7ebdcf811aa9f7814d5c0c5": {"ta_keywords": "saxophone model;saxophone model couples;saxophone;reed model parameters;tone hole configuration;reed stiffness;filter frequency responses;control parameters;several possible frequency response pairs;mouthpiece;input pressure;pitch;convex optimization;pressure;fingering;measurement;method;bank;work", "pdf_keywords": ""}, "4302e981e3ec118b68e0b3fcf1820b3f6ecfa988": {"ta_keywords": "argumentation theory;arguments;relative performance;practice;theory;quality;relationship", "pdf_keywords": ""}, "15251fa3a3bcf695bf153d0856886cab9a3145ea": {"ta_keywords": "text summarization;comprehensive evaluation;datasets;stack;improvement;unified view;quality;depth;method;effective way;new framework;variety;problem;current state;art methods;state", "pdf_keywords": "empirical neural extractive summarization;summarization system;text summarization systems;summarization accuracy;text summarization;summaries combination;meta learning gap;modern text summarization systems;summarization;candidate summaries;document sentences;machine translation;best candidate summaries;systemtext summarization;text representations;abstract sentences;candidate summary;new candidate summary;stage learning;meta systems;natural language;sentences;meta system;summary;induction training;stage learning problem;refactor model;refactor;semantic structure;neural attention"}, "f9e3b7c6ca7d534694148bd0c7c37c1ef896a784": {"ta_keywords": "end speech recognition system;channel speech speaker;mixed speech speaker;different languages;acoustic features;various linguistic objectives;speakers;channel;mixture;system;sequence;end;capabilities;variety;results", "pdf_keywords": ""}, "400e083a18ab94bbf45b0820693fb5035684dd7c": {"ta_keywords": "semantic similarity network;semantic similarity networks;semantic description;sentence;recognition;context;construction;method;problem;results;use;experiments", "pdf_keywords": ""}, "71a85e735a3686bef8cce3725ae5ba82e2cabb1b": {"ta_keywords": "underspecified behavior;clinical risk prediction model;machine learning;training;empirical data;models;data sets;deployment domains;distinct failure mode;domain;significant feature;structural mismatch;issues;variety;problem", "pdf_keywords": "underspecified machine learning pipeline;deep learning case studies;deep learning models;deep learning;deep image classification tasks;ml pipelines;machine learning models;deep neural networks;ofmachine learning;underspecification;encode generalizable inductive biases;optimal underspecification manifest;common machine learning practice;training domain;inception;machine learning;robust generalization;machine learningwe;machine learning framework;machine learning methods;models;pipelines;credible inductive biases;poor model behavior;learning;shortcut learning;imagenet;training distribution;training;shallow random feature model"}, "dd961bb9e2a70f3819a13b13402fe585ae384226": {"ta_keywords": "pure nash equilibrium;pure nash equilibria;probabilistic serial;indivisible goods;equilibria;agents;truthful profile;same assignment;time algorithm;rule;profile;verifying", "pdf_keywords": "pure nash equilibrium;pure nash equilibria;perfect nash equilibrium;random assignment rules;nash equilibrium profile;probabilistic serial mechanism;ordinal preferences;truthful preferences;probabilistic serial;randomized way;utility distribution;assignment process;truthful preference profile;deterministic way;incentive;cardinal utilities;utility;lexicographic preferences;nash dynamics terminate;agents;equilibria;indifferences;second agent;divisible goods;market models;preference profile;voting rules;node equilibria;social choice;agent"}, "86d55c5a098689438ceb1d52bdd768da3b47f55f": {"ta_keywords": "networked sensors;sensors;supervised learning;dynamics;algorithms;filtering;accurate characterization;process;design;class;paper;parts", "pdf_keywords": "suitable stochastic markov decision process;stochastic sensor;stochastic markov decision process;dynamic sensor subset selection;stochastic markov chain;optimal sensor subset selection;stochastic sampling algorithm;sensor subset selection;stochastic markov decision process inwe;centralized tracking;markov process;stochastic samplingin;sensor subset;markov chain;stochastic propagation;stochastic process;novel partial consensus approach;stochastic approximation;stochastic processes;linear sensor network;optimal sampling;discrete time thresholding;sensor observations;optimal estimation;transition probability matrix;probability transition matrix;tracking;sensor configurations;stochastic partial differential equations;sensor"}, "2c0ebf5479db7f76c1e15512676c16b9032343fb": {"ta_keywords": "transmission line;performance transmission line;shift;front edge;position", "pdf_keywords": ""}, "0d360a1256ccdfca58cf98d12243df8407fd442d": {"ta_keywords": "untrusted link;attacks;new attack;attack;weights;door attack;models;model;link;users;fact;tune;value", "pdf_keywords": "novel adversarial perturbation attack;deep learning models;classical deep learning models;large natural language models;strong fine tuning loss;deep learning;vulnerabilities;attacks;random attacks;undesirable models;novel attack;new attack;data poisoning;spam detection tasks;tuning kernel;attack fronts;attack;models;leak;arbitrary keyword;spam detection;toxic filtering;neural network;viable attack;defense;limited knowledgewe;large class;machine learning;weight poisoning;model"}, "c14254fd285706e549d0dcc57ae74680164c9afc": {"ta_keywords": "inverse reinforcement learning algorithm;passengers decisions;reinforcement learning;risk;loss function;gradient;sensitivity;observed behavior;canonical grid world example;model;examples;technique", "pdf_keywords": "inverse reinforcement learning algorithm;sensitive reinforcement learning;sensitive reinforcement learning algorithm;inverse risk;sensitive reinforcement learning procedure;convex risk metrics;sensitive reinforcement learning approach;novel inverse risk;reinforcement learning;sensitive reinforcement learning problem;reinforcement learning model;reinforcement learning algorithm;optimal policy;loss aversion;optimal policies;risk metric;reinforcement learning data;risk;forward risk;risksensitivity;learned value function;true agent policies;greedy policy;human decision making;reward;rich behavioral models;loss function;behavioral economics;prospect theory;learning problem"}, "e10dba1d4a56a81429d6ec4c9b7bdc15ea75474b": {"ta_keywords": "malicious sensor;secure estimation;sensor observations;estimation scheme;multiple sensors;other sensor measurements;optimal filter;detection;novel filtering;linear time;observations;algorithms;algorithm;process;time;efficacy;presence;paper deals", "pdf_keywords": ""}, "5403fd71810d098e572d9bd0f9ec10e96d6b6336": {"ta_keywords": "point transmission control problem;noisy network;probability transition graph;optimal policy;wireless system;large state space;level system;point;complexity;performance loss;method;paper;problem;representation;structure", "pdf_keywords": ""}, "967b2d10b8b378f1da43fd4d9107826e540e1112": {"ta_keywords": "joint language;joint embedding;motion;languages;language;curriculum;easier sequences;model;novel architecture;space;end;novel approach;harder ones;approach;problem", "pdf_keywords": "natural language animation;joint language embeddings;conditional pose sequences;pose sequences;joint embedding;animation;human motion;next few poses;joint language;video descriptions;annotated english sentences;motion;natural language description;joint representation;human behaviorwe;whole body motion;robot action;sentences;natural language sentence;available corpus;supervised model;language;humanoid robot;inference stage;language concepts;joint positions;text;movement;joint space;video"}, "0bdf1f3b79f4df5d5e11af1ea00379e1461e22fa": {"ta_keywords": "interesting partial dependence plots;interesting plots;machine learning applications;datasets;single features;raw feature spaces;generalization;models;generative model;selection;pdps;example;usefulness;multiple use;latent space;model response;arbitrary directions;cases;method;use", "pdf_keywords": ""}, "d95973f0f0d86b758154e9a5f3d7434430d7856c": {"ta_keywords": "proton cross section;proton densities;deuteron collision;proton;gaussian proximal operator;gradient;free method;calculation;method;large range;use;novel", "pdf_keywords": ""}, "194c5644c49e9e1b87990439fae05c98ba8b4fbb": {"ta_keywords": "materials synthesis;synthesis sentences;synthesis procedures;semantic noise;semantics;materials;resource;novel resource;graphs;novel dataset;design;representations;original ones;analysis;comparison;presence", "pdf_keywords": "synthesis annotations;annotated materials synthesis data;synthesis extraction models;synthesis text;annotation process;new resource inthe synthesis;synthesis procedures;materials synthesis procedures;materials synthesis;annotation;annotated entities;semantic structure;synthesis;annotations;synthesis planning;entity annotation;unstructured natural language text;structured representations;best annotations;synthesizingwe;semantic dependencies;annotated states;shallow semantic structures;different annotations;materials knowledge;syntactic dependencies;annotatedwe;materials science literature;novel materials;molecule"}, "03b68259f9e70d2007d40e5331c9ff31f2bb46b9": {"ta_keywords": "activity recognition method;unlabeled acceleration sensor data;activities;sensor data;appropriate training data;physical characteristics;height;end user;information;user;gender;methods;effectiveness;method;paper", "pdf_keywords": ""}, "e11d6a031d5f85f372b0fda3ab62ca4ce2d89f2c": {"ta_keywords": "transition logic circuits;transition logic circuit;transition rules;transition;optimization;rule;design;designer;system;method", "pdf_keywords": ""}, "90fbeb4c871d3916c2b428645a1e1482f05826e1": {"ta_keywords": "image captioning;source code captioning;decoder learning framework;encoder;art encoder;novel encoder;decoder model;decoder systems;fact vector;tasks;framework;review step;review steps;number;state", "pdf_keywords": "sequence learning;recurrent neural networks;rnns;captioning task;captioning tasks;attentive encoder;machine translation;image captioning;encoder;attention;novel encoder;decoder model;encoders;new encoder;decoder;decoder architecture;conventional encoder;decoders;captioning applications;neural network;source code captioning;attention mechanism;representations;generative model;review network;discriminative supervision;decoder bottleneck;generic encoder;decoder framework;encoding"}, "ddd74358d7e11535ee77e2c323dd662d115a0f20": {"ta_keywords": "object grounding;quadcopter motion;new objects;object;essential features;presence;strategy;novel approach;approach", "pdf_keywords": "shot language;shot segmentation component;natural language instruction;shot representation learning system;segmentation agent;natural language input;robust instruction;visual representation;natural language instructions;deep learning pipeline;monocular images;vision;natural language mentions;objects;language instruction;natural language;demonstration dataset;video camera;quadcopter navigation task;natural language processing;deep learning;segmentation component;physical quadcopter objects;object;allocentric object context grounding map;level annotation;new objects;deep reinforcement learning;unseen objects;training"}, "97943a5dee3c6e36d01a6099acb9ec360ad0ee19": {"ta_keywords": "portmanteau generation problem;neural sequence;unsupervised candidate generation strategy;sequence;channel model;s2s;combination;novel approaches;art benchmark;character;level;approach;state;concept;art", "pdf_keywords": "novel language model;novel word formation phenomenon;language representation;neural sequence;language model;large corpus;portmanteau generation;input words;new words;linguistic inference;linguistic semantics;words;ground truth accuracyportmanteau;next character;portmanteaus;roots words;multiple root words;broader semantic representation;portmanteau;data encoding;tosequence;sequence;vocabulary;reverse direction models;best word;theportmanteau;semantics;text;character;channelstyle model"}, "30f86d38f0660af5ea2e16d996434c72eee8c5ee": {"ta_keywords": "data mining;toolkit;new open source platform;types", "pdf_keywords": "speech recognition;end speech processing;dynamic neural network toolkits;speech recognition tasks;end speech recognition problem;speech recognition problem;new deep learning engine;deep learning system;other speech processing experiments;pattern recognition;neural network structure;advanced pattern recognition;deep learning;advanced signal processing;kaldi toolkit;new deep learningwe;advanced neural machine translation;feature extraction;software platform;joint language identification;signal processing;recurrent neural network encoder;speech;novel software architecture;data processing;encoder;new open source platform;espnet;improved accuracy;generator network"}, "36bca9d41de386fce5dce06999a45a802a7c4f41": {"ta_keywords": "acyclic preference networks;acyclic bp networks;networks;random generation;bp networks;rank;general method;novel method;method", "pdf_keywords": ""}, "0c5bfa2d4bb351a479073cb358c3ae6f7ecf0476": {"ta_keywords": "nonlinear languages;novel nonlinear languages;nonlinear language class;novel nonlinear data;feature extraction;novel data structures;annotators;npnpnp;feature comparison;features;analysis framework;framework;structure;extraction methods;representation;class;use", "pdf_keywords": ""}, "a2aa642db090b3aa28a44ccbc3c51fdb0be8335b": {"ta_keywords": "benchmark treebanks;domain corpora;new domains;performance;shot setting;comparative study", "pdf_keywords": "neural parsers;constituency parsers;recent nonneural parsers;benchmark treebanks;parsers;novel parser;domain treebanks;domain parsers;structured output parsers;standard parser;parser;statistical parsers;bert parser;computational linguistics;chinese corpora;higher generalization performance;word embeddings;unstructured neural models;neural models;encoder representations;novel encoding;bert encoder;input representations;domain generalization;natural language;bertarian encoding;bertarianian encoding;domain test sets;better structures;same benchmarks"}, "de5834305ea419c25b17f0c8d27bad6a5feb311a": {"ta_keywords": "scale chess commentary;natural language descriptions;truth commentary texts;chess game;trainable neural model;individual moves;multiple pragmatic aspects;game state;move;models;new end;outputs;end;methods;account", "pdf_keywords": ""}, "47234fca1b14666d72bc5df0e2d911ff7cdea688": {"ta_keywords": "stochastic block models;classical stochastic block models;hypergraph objects;hyperedges;edge weights;stochastic perturbations;optimal partitions;nodes;stochastic perturbation;multiway measures;weights", "pdf_keywords": "hypergraph clustering;stochastic hypergraph model;weighted hypergraph;spectral clustering;weighted hypergraphs;spectral clustering algorithm;new spectral clustering algorithm;new hypergraph model;hypergraph;heterogeneous hypergraph;hypergraph completion problem;hypergraphs;sparse graphs;cluster theory;clustering;community detection problem;robust clustering algorithm;weighted stochastic block model;cluster;subspace clustering;robust clustering problem;graph structure;graph entries;clustering algorithm;subgraph;clusters;cluster size;hyperedges;cluster growth;nodes"}, "9a41111cf881b052555985bd8cf304ef9fc4f6d5": {"ta_keywords": "information extraction;list;items;task;constraints;identification;performance;novel method;method", "pdf_keywords": "corresponding structured corpus;structured corpus;structured corpora;large corpus;target corpus;labeling relations;information extraction;novel labeling method;label propagation;labeling data;supervised learning;more entity mentions;drug nodes;structured database;potential instance labels;supervised learning process;disease nodes;entities;drug domains;document structure;classifiers;large scale evaluation;training dataset;structured systems;machine learning framework;relations;svm;extraction;standard classification learner;text"}, "c8a95217cde1bc893b230297250918818aa01dd7": {"ta_keywords": "coherent coherent state;coherent state;dimensional optical lattice;single photon;lattice;photon;single atom;excitation;combination;generation;method;new method", "pdf_keywords": ""}, "71cdf94d13cc6c497dcc2dcb20893fe64cfaf62e": {"ta_keywords": "text generation;word clusters;unlabeled text;topics;controlledlable language;self;centers;novel;possible continuations;framework;components;scale;quality versions;experiments", "pdf_keywords": "text generation;conditional text generator;interactive writing assistant;current interactive writing assistants;prompt topics;several text option generators;conditional generation;interactive writing applications;training corpus;upcoming topics;next topics;input words;semantic representations;level story representations;topics;prompt text;option generation;topic list;candidate topics;future words;novel option generator;option generators;sentences;word clusterswe;conditional transformer language model;option generator;linguistic structure;text;prompt;next paragraph"}, "edb49aa423afc210facec998277923c4b75e4648": {"ta_keywords": "antiferromagnetic phase transition temperature;structural phase transition;neutron diffraction;structural properties;photoemission spectroscopy;orthorhombic symmetry;temperature;angle;arpes;function;means", "pdf_keywords": ""}, "4e749b2e0728044af44d50a708fc99d49359ea0b": {"ta_keywords": "unsupervised learning models;neural networks;comparable performance;accuracy;performance;substantial advantage;terms;context;variety;sense", "pdf_keywords": "related language translation tasks;translation tasks;romanization decipherment;computational linguistics;unsupervised character;informal romanization decipherment;universal translation accuracy;different languages;transliteration;related language translation;gram language model;languages;level transduction tasks;different language types;translation task;universal universal translation accuracy;unsupervised learning models;linguistic correspondence;phonetic translations;universal universal translation translation accuracy;unsupervised tasks;universal characterisation task;language;different alphabets;unsupervised model;universal script conversion transducer;kannada;translation cross section;corpora;lexical aspects"}, "9700940262cd5e797ab81eee464c3b3a16295cba": {"ta_keywords": "speech enhancement;speech dereverberation;variance adaptation;static adaptation scheme;adaptive training scheme;novel parametric variance model;relative error rate reduction;dynamic capabilities;dynamic components;dynamic component;evaluation;preprocessing;model;parameters;method;paper", "pdf_keywords": ""}, "0d2a1c0724743de0cb74463466b075598ba36c45": {"ta_keywords": "periodic potential;particle motion;particle;motion;potential;time;study;case;results", "pdf_keywords": ""}, "f01f4808263ecfa221f856c34d3420166dbf5930": {"ta_keywords": "car navigation system;confusion level;multimodal sensor data;driver;convolutional neural network;classifier;lstm;neural network;long shortterm memory;benchmark benchmark model;performance;method", "pdf_keywords": ""}, "ff7b5379641875be7357766af0b1e2bd55c74cc8": {"ta_keywords": "differentiable search index;retrieval;text model;relevant docids;documents;text;generalization capabilities;models;dsi;terms;maps string;performance;new paradigm", "pdf_keywords": "document retrieval tasks;standard document retrieval task;information retrieval;structured semantic docids;docid representation learning;retrieval tasks;unstructured indexing system;docid representations;document representations;document representation;docid representation;retrieval;novel indexing;unstructured data;retrieval process;whole retrieval process;end search system;indexing model;documents;text model;relevant docids;structured identifiers;document;docids;differentiable search index;candidate docids;structured structured examples;neural language models;index;natural questions data"}, "3cd4ae1cac866f853bb3276d215cff18df371b67": {"ta_keywords": "background noise;feature transformation;signal;minimum bayes risk;feature;novel approach;efficient combination;approach;form;problem", "pdf_keywords": ""}, "ceef266c59698999c9283a0cda852d8bc1ce27ea": {"ta_keywords": "contextual word representation;token types;space changes;isotropy;space;tuning;local structures;effect;changes;study;fine", "pdf_keywords": "semantic downstream tasks;semantic regression task;word representations;linguistic representations;language models;linguistic knowledge;large word frequency;natural language processing;word frequency;sentences;several downstream tasks;nonlinearity prediction models;isotropy enhancements;rich conditional conditional sentence sequence;words;tasks;isotropy;cwrs;bert;frequency bias;finetuning;models;nllp;sentence;performance;such models;tuning;context;pipeline;tunedwe"}, "b719fc66b173f8e9e0624317bb00abf10a4d5606": {"ta_keywords": "basketball games;basketball game;video shot;temporal segmentation algorithm;temporal segmentation algorithms;continuous frame difference;deep learning;novel deep learning model;histogram;frame difference;multimedia research;window frame difference;dimensional feature vector;much attentions;superaverage ratio;model;recent development;paper", "pdf_keywords": ""}, "18ef33a6e040b49ba475e586202932cecbafba0d": {"ta_keywords": "event influence generation challenges;event influences;knowledge;context;human judgments;strong baselines;reference;closeness;evaluation metrics;large dataset;distance;terms;new set;method;performance;new method", "pdf_keywords": "event influence generation datasets;event influence generation;new event influence evaluation system;event influence graphs;source events;standard event influence evaluation system;commonsense knowledge generation;influence reasoning;influence generation;event influences;source event;language models;standard event influencewe;natural language;events;commonsense knowledge bases;event;generative knowledge;reasoning chain;language model;target events;machine translation system;linguistic structure;influence relations;context;influence graph;background knowledge;synthetic text tokens;questions;hop information"}, "e5acad5bba23a8c3a9f7cd24f7694ab10357ebc7": {"ta_keywords": "speech recognition system;voice activity detection;decent speech dereverberation;reverber;reverberant condition;end dereverberation;multichannel;2mix corpus show;separation performance;wsj1;relative reduction;system;performance;end;numerical stability;work;experiments", "pdf_keywords": "advanced advanced voice activity detection;advanced advanced advanced speech processing;voice activity detection;multichannel speech recognition;speech recognition;neural beamformer;speech channels;beamforming approach;beamforming process;advanced signal processing;beamformers;convolutional beamformer;mask estimation;maskformers;different beamformer variants;neural beam;reverberant wsj1;th speaker;advanced subnetworks;end training;encoder;multichannel environment;2mix corpus show;advanced feature extraction;end optimization;advanced end;input signal;spatialized wsj1;advanced network;final output"}, "4b9b7240ef9b6bc442044684ed5646ef02897d87": {"ta_keywords": "planning competition domain;concurrent actions;domain;domain variables;true stochasticity", "pdf_keywords": ""}, "c3aa698b562e91f78a042b938ffce1877b6e859c": {"ta_keywords": "artificial", "pdf_keywords": ""}, "728a6850882a0d8ef5551949cc2baee1e1667cd8": {"ta_keywords": "optimal bribery schemes;voting domains;algebraic complexity;optimal set;agents;selection;maximal number;cases;problem", "pdf_keywords": ""}, "df8ae2068d17d969db6ab2d27108776e99413975": {"ta_keywords": "natural language inference;linguistic data;inference;andgraph data;graph data;np inference problem;graph;text;novel method;method;np;approach;power;performance;art performance", "pdf_keywords": "entailment model conceptnet;natural language inference;natural language inference tasks;textual entailment;entailment relationships;external knowledge source;external knowledge sources;entailment model;knowledge bases;knowledge graph;large corpus inference;natural language model;right external knowledge source;entailment;external knowledge;natural language queries;available knowledge source;conceptnet;natural language sentences;inference;knowledge structures;knowledge structure;large corpus;unstructured text representation;knowledgewe;embeddings;knowledge;sentences;structured information;other sources"}, "a7abd783de8d21d640e41d31ec89f2c1caec4e42": {"ta_keywords": "unsupervised word sense disambiguation;disambiguation scores;disambiguation;sense predictions;machine learning model;mining framework;texts;art machine learning model;novel application;data;system;state;question;correct answer", "pdf_keywords": "unsupervised word sense disambiguation;first unsupervised word sense disambiguation system;novel word disambiguation system;interpretable disambiguation;interpretable word sense predictions;free disambiguation;disambiguation models;word clusters;interpretable sense representations;disambiguation;word senses;semantic network;intrinsic linguistic similarity;text corpus;possible linguistic structures;corpus;input corpus;rich language structure;precessing hypernym representation;python language;super sense inventories;babelfy system;knowledge;clustering;language;word;context;unsupervised way;super senses;words"}, "58961f0ea3291ddab697fbe5be999a0793b0efaf": {"ta_keywords": "storage codes;storage systems;dss;failures;storage;codes;cd;networks;error;form", "pdf_keywords": ""}, "f75e691daae9133941c9a083e319b39bd837d456": {"ta_keywords": "joint knowledge embeddings;knowledge graph completion performance;entity alignment;various knowledge graphs;entities;dimensional semantic space;relations;alignment performance;realworld datasets;parameter sharing method;small seed;novel approach;significant improvements;method;experiments", "pdf_keywords": ""}, "80a085a79ac6cee94f21d21ab8ca302458c4e131": {"ta_keywords": "cloud;parametric loss function;loss function;parametric loss;loss;network;data;information;inference accuracy;information content;comprehensive analytical framework;dw;framework;paper;version", "pdf_keywords": "noisy data;data sharing;cloud;intelligent encryption mechanism;autonomous noise distributions;untrusted cloud;privacy;many machine learning;encrypted graph;noise distribution;expensive edge computing;noise tensor;machine learning environment;deep learning;data storage;encrypted basis;training data;collective attacks;only cloud;personal data;cloud devices;mobile cloud;noise;noise injection;accurate storage;inherent privacy constraints;secure separation;deep learning process;datasets;secure network"}, "464b47a6a395fa1338e230254965cf5f669e715c": {"ta_keywords": "sparse discovery;sparse discovery strategy;polysynthetic languages;novel empirical discovery;novel babbling stream model;babbling model;neurons;model;stream;presence;success", "pdf_keywords": ""}, "ca57443fcb87f03267fccee162a4924c56062c6f": {"ta_keywords": "gaussian curvature;curvature;dynamics;good model;system;effect", "pdf_keywords": ""}, "d3304b926cfcd91110bd5ba01db21d26ce5fca2d": {"ta_keywords": "paraphrastic sentence embeddings;type sentence embeddings;neural machine translation;type pairs;data quality;training data;par;pairs;bitext;prior work;ability;setting", "pdf_keywords": "paraphrastic sentence embeddings;neural machine translation;paraphrastic sentence embeddingswe;neural machine translation model;bilingual sentence pairs;sentential paraphrases;paraphrase pairs;word embeddings;paraphrastic sentence;paraphrastic word;word paraphrases;paraphrase task;simple paraphrastic sentence;translation quality;english sentences;translations;sentences;sentence fragments;softmax;softmax systems;output vocabulary;linguistic structure;english pairs;softmax layers;translation;simple sentences;corpora;target vocabulary;output output vocabulary;languages"}, "305a1251a68fb16835876d8c99de498472c0cd8f": {"ta_keywords": "computation schemes;reed muller code;computation scheme;code;locality;multivariate polynomial functions;computation;codes;schemes;function;scheme;lens;lower resource;viewpoint;new approach;approach", "pdf_keywords": ""}, "3332dc72fbe3907e45e8a500c6a1202ad5092c0f": {"ta_keywords": "spectral clustering;source separation;target spectrogram;channel mixtures;rank pairwise affinity matrix;ideal affinity matrix;multiple speakers;deep network representations;mixtures;signal quality;context;state;model;art;novel approach;preliminary experiments;use;terms;approach;problem", "pdf_keywords": "acoustic source separation;deep clustering;speech separation task;deep clustering system;speech separation;scale speech separation experiments;deep learning framework;deep learning;deep networks;spectral clustering;deep representations;clustering;segmentation;spectral clustering model;embeddings;clustering framework;audio;simple clustering methods;training data;convolutional neural networks;convolutional neural network;network input features;segmentation problem;underlying clustering process;speech;partition;speaker;speakersspeech;partitions;masking functions"}, "6e7cfed8815cce163efac9d17b1109849c050c6b": {"ta_keywords": "spin dynamics;spinor;orbit coupling strength;spin;zeeman field;orbit interaction;zee;effect", "pdf_keywords": ""}, "7038b181f776e9cd587d4d61cb68692fdac8ec26": {"ta_keywords": "dynamical mode decomposition;traffic flow networks;oscillatory modes;operator;control;analysis;dm;model;generalization;free approximation;method;novel approach;approach;use;paper", "pdf_keywords": "dynamic mode decomposition;dynamic mode decomposition method;dynamical mode decomposition;traffic flow data;measured vehicle flows;phase sequence estimation;traffic flow;traffic data;traffic flow networks;vehicle flows;traffic queues;nightly traffic flow;signal phases;traffic control systems;temporal phase separation;traffic flow network;original queue dynamics;dimensional fluid data;flow data;abnormal traffic events;temporal phase separation sensor;koopman operator;signalized network;instability analysis technique;traffic;complex oscillatory dynamics;oscillatory modeswe;coherent structures;unstable queuing dynamics;dimensional phase"}, "1e58c9d1153d2f25d94b3a12b785bd7abe43bd1c": {"ta_keywords": "change point detection methods;classification;autoencoder;improved representations;neural network;likely class changes;unsupervised manner;representations;dissimilar sequences;sequence;points;examples;extensive synthetic simulations;novel framework", "pdf_keywords": "unsupervised change point detectionwe;change point detection;change point detection methods;change point detection procedure;change point detection method;unsupervised classification;supervised classifier;change points;classification;supervised learning;unsupervised detection;classifier;supervised setting;false change points;change statistic;sequence data;sequential data;classification results;likely class changes;classification network;unlabeled data;changes;detection;data sets;discrete data structures;detection procedure;deep learning;layer classification network;gesture recognition;recurrent networks"}, "5bad092098ba7400e19468a06cb8b238c43b7637": {"ta_keywords": "orbit coupling strength;orbit coupling;electron;spin;zeeman field;field;effect", "pdf_keywords": ""}, "c81bb5ff79e8c7f65a3e28b7ba52d90deaa32fde": {"ta_keywords": "online collaborative editing;collaborative editing;social influence;code;switching;social effect;different languages;technical language;performance;study;use;success;predictor", "pdf_keywords": ""}, "f26f17ec49f2593bcc926051394871480a80c0c2": {"ta_keywords": "crosslingual word similarity;bilingual lexicon induction;crosslingual word;normalizing flow;densities;mixture model;probability densities;benchmark data sets;spaces;formulation;competitive performance;novel approach", "pdf_keywords": "bilingual word embeddings;learned bilingual embedding;bilingual word mapping;supervised bilingual mappings;target word embeddings;learned bilingual lexicon;bilingual mappings;bilingual dictionary induction;bilingual lexicon induction task;bilingual words;bilingual dictionary induction task;bilingual word;crosslingual word similarity;unsupervised language induction;large corpus;large multilingual metric space;embeddings;word translation accuracy;language variation;difficult language pairswe;linguistic structure;unsupervised dictionary induction model;gaussian mixture model;conditional translation;standard gaussian mixture;languages;richer language;normalizing flow;density matching;novel supervised measure"}, "28028458d75bf9281200389a880741eb6d06a3a4": {"ta_keywords": "datalog specifications;inductive logic programming;benchmark modules;large software systems;test suite;recall;precision;recovery;performance;specifications;extensions;high rates;method;novel method;shelf;third", "pdf_keywords": ""}, "436380dd75d8ff3f2debb29913bd2fe8dde0b684": {"ta_keywords": "speech mixtures;source separation;complex matrix factorization problem;gaussian noise benchmark;mixture;speaker;image quality benchmark;phase;context;conventional approaches;presence;novel approach;approach;paper;problem;use", "pdf_keywords": "complex matrix factorization;negative matrix factorization;multi channel source separationwe;single channel source separation;input matrix;channel source separation;complex matrix;source separation;step speech separation problem;individual speech signals;mixed speech signal;successive matrix representations;supervised speech enhancement;speech signals;speech signal;music transcription;individual signals;spectral magnitude;nmf framework;individual signalin;phase reconstruction;mixed signal;decomposition process;cmf approach;signals;nmf;target signal;signal;signal amplitude;phase"}, "6992f54509c139455c3cffa9b0e4ae5c19ebff82": {"ta_keywords": "effect", "pdf_keywords": ""}, "30c6be4c7f549a2ec7328d24ecc0a54fbf90d41c": {"ta_keywords": "optimal control policies;optimal control;partial differential equations;pdes;stochastic process;wireless networks;large state space;simulation;parameter space;method;process;problem;new method;paper;use;convergent;significant fraction", "pdf_keywords": ""}, "4da018847a0f44378e6a1ded93fee672a3c7c370": {"ta_keywords": "end speech recognition;field model;mask;novel network architecture;model;conformer;tokens;tribov;parametric fashion;remarkable performance;generation;recent experiment", "pdf_keywords": "end speech recognition;automatic speech recognition;speech recognition;connectionist temporal classification;fast inference speed;decoder model;asr models;novel encoder;novel inference framework;encoder;asr network;real time factor;novel generative flow models;parametric model framework;networks;ctc;prediction;predict;convolution representation;text;mask;generative flow;data augmentation;nonautoregressive model;modeling;asr;network;multilayer network;parametric model;conditional probability"}, "abaf39dc9d1b156ddf387230611f5102378d052c": {"ta_keywords": "large corpora;input correction;word error rates;attention;new decoder;sequence;single inputs;supervised methods;data;novel approach;half;character;power;approach;methods;ability", "pdf_keywords": "ocr input;text correction;optical character recognition;transcription error correction;ocr;unsupervised correction;robust text;document recognition;input decoding;decoder;correction tasks;input correction;un transcribed lines;attention combination;corpora;large corpora;hierarchical attention combination;text;free text;attention;texts;new decoder;encoding;neural translation models;encoder;accurate reading;error correction;human language;unsupervised training;correction performance"}, "63bfe58735f44b0af24da3c2cb6b1651b001b83c": {"ta_keywords": "secret sharing;communication complexity;secret;shares;general network;networks;dealer condition;dealer;network;lower bounds;information;algorithm;condition;participants;problem;theoretic", "pdf_keywords": ""}, "5d69380565aa258bfa54005c9ba05e30675be227": {"ta_keywords": "exploratory learning;novel classes;discovery;novel method;tasks;novel;method;context", "pdf_keywords": ""}, "62d6ccd01c2e022a385add5e689b4561b0fbfd88": {"ta_keywords": "speech segment proposals;speaker diarization;novel speaker diarization method;speaker embeddings;diarization datasets;region proposal;remarkable improvements;rsp;state;method;experimental results;experiments", "pdf_keywords": "speech segment proposals;speech segmenting;speaker diarization;novel speaker diarization framework;speaker diarization system;speaker classificationspeaker recognition;speaker recognition;novel speaker diarization system;novel speaker diarization method;speech prediction;speaker classification;dependent speaker verification;speaker embeddings;overlapping speech;term speaker identifications;diarization datasets;acoustic features;region proposal network;deep neural networks;speaker;exotic speaker models;speechwe;speech;convolution layers;novel exotic speaker model;term speaker denoise;true speaker;standard diarization systems;neural network;boundary refinement"}, "9ba545841b837fa077579290e252eb00351ebeb0": {"ta_keywords": "convex problems;convex;convergence analysis;methods;method;class;art methods;terms;previous state;novel;time", "pdf_keywords": "novel communication compression strategy;compression network;compression networks;online nonconvex optimization;convex loss;nonconvex optimization;communication networks;compression;nonconvex optimization problems;stochastic nonconvex optimization algorithm;minibatch stochastic gradients;stochastic optimization;gradient descents;packet communications;optimization;stochastic optimization problems;optimization method;compression pers;strong complexity rates;communication networkswe;stochastic optimization problem;small complexity;stochastic gradient method;federated learning;arbitrary complexity;communication rates;packet framework;learning algorithm;communication cost;stochastic gradient estimator"}, "ea1f61270480a8dec54ec571c0e6ce116d096241": {"ta_keywords": "novel polyphonic sound event detection method;polyphonic polyphonic model;sound event;framewise detection methods;multilabel classification problem;multiple frames;detection results;term memory;processing;hybrid system;segment;model;smoothing process;post;study", "pdf_keywords": ""}, "598321d9c3eb5c035b449e19e539b6fa04b3802a": {"ta_keywords": "maximum entropy;binary system;single copy;copies;data set;number;set;observation;method;square", "pdf_keywords": ""}, "8f1f43408baf1ccb0ec3e7985592326c83ee276d": {"ta_keywords": "dialog system;statistical machine translation;chat;smdm;approaches;combination;best performance;paper;experiments", "pdf_keywords": ""}, "07a9f47885cae97efb7b4aa109392128532433da": {"ta_keywords": "high translation quality;noisy input;attention;noisy output;noisy noisy noisy;translation;models;original models;original ones;new class", "pdf_keywords": "neural machine translation systems;neural machine translation system;simpler attention distributions;convolutional neural machine translation model;convolutional neural machine translation;attention variant;attention head distributions;translation quality;random attention;performance machine translation systems;attention;standard attention;quadratic quadratic quadratic neural machine translation model;quadratic quadratic neural machine translation;attention heads;single cross attention head;cross attention head;cross attention;encoder;neural machine;decoder model;decoder;decoders;final decoder layer;high memory efficiency;elementary sentencepiece vocabulary;performance nlp systems;elementary text;elliptic encoder;sentences"}, "8afbc4188be9e9452ce1fe868ebe217179d36793": {"ta_keywords": "adaptive speech system;adaptive adaptive speech system;linear regression;naimark speech;speech exemplars;background noise;automatic recognition;speech;noise;recognition;novel method;method;problem;paper;presence", "pdf_keywords": ""}, "3426fadf73a5ce418486e640b26b3d2470d932b5": {"ta_keywords": "end speech recognition models;multilingual end;adversarial classification objective;languages;language;context;independent objective;objectives;effectiveness;results;large number;experiments", "pdf_keywords": "multilingual speech recognition;multilingual convolutional neural network;target language adaptation;end speech recognition models;multilingual models;multilingual knowledge transfer;language prediction models;language prediction;language prediction model;scalable multilingual model;domainadversarial classification objective;multilingual seed model;speech model;monolingual model;less target language data;target language;adversarial classification objective;adversarialwe;language adaptation scenario;multilingual setup;explicit phoneme objective;adversarial objective;high resource languages;independent phoneme objective;new language;independent encoder representations;single input speech;different languages;multilingual users;vocabulary learning"}, "8de431e0e62653711136836642af38179731c2f0": {"ta_keywords": "secure codes;theoretic code structure;storage systems;erasures;erasure;codes;security;high reliability;data;algorithms;iterative structure;distribution;information;nodes;construction;paper;class;number;efficiency;terms", "pdf_keywords": "theoretic erasure codes;explicit storage codes;storage codes;secure codes;optimal regenerating codes;storage systems;storage system;optimal codes;optimal storage;storage;minimum possible storage;repair codes;dynamic storage model;storage device;codes model;storage devices;secrecy capacity;minimal possible storage;density codes;storage capacity;repair code;erasures;high availability;passive eavesdroppers;present explicit codes;storage connections;eavesdroppers;security requirements;eavesdropper;repair algorithms"}, "fdf1aec2da3597010c31138159574b1016019f73": {"ta_keywords": "preference reasoning communities;preference reasoning;computational social choice;social choice;preference;decision making;decision making process;empirical impact;empirical contributions;theoretical results;case study;paper", "pdf_keywords": ""}, "538deb39d57bef62833c492a56c796a2bafa340f": {"ta_keywords": "active learning;support vector machines;linear polynomial basis function kernel;scale data mining;kernels;qualitative features;essential features;data;powerful approach;method;problems", "pdf_keywords": ""}, "63e7e3b16e03da62a2c535ac9cfccfa3ae48b292": {"ta_keywords": "team utility;artificial intelligence;arbitrary team;ai;team decision;individual accuracies;team;human;accuracy;utility;performance;experiments;linear models;final decision;system;mate;measure;cost;world;linear;method;terms", "pdf_keywords": "new teamcentric optimization;ai teams;team utility maximization problem;team predictions;accurate teammate;metadecision prediction;team utility;artificial intelligence;higher team utility;team behavior;best empirical classifier;classifiers;online team;team performance;team scenario;additional human effort;classifier;machine learning;accurate classifier;highest team performance;generalized machine;teamwork;human decision making;prediction;team;task;empirical selection;optimization framework;team utilities;joint teams"}, "b2baf9e053c32abfb3c8658b9bc6d6790ae671cb": {"ta_keywords": "eye movement features;novel eye movement features;natural language;unknown words;novel machine learning method;detection;natural context;accuracy;approaches;approach;method;terms;use;paper;experimental results", "pdf_keywords": ""}, "15931520cce546bbf19b4cebeb4161c4debeabe7": {"ta_keywords": "mechanical turk;human cognitive constraints;vote;uncertainty;optimal manipulation;behavioral data;behavior;better outcome;world behaviors;winner;scenarios;people;account;model;size;novel model;degrees", "pdf_keywords": "human voting behavior;voting behavior;voter behavior;user voting behavior;voting process;approval voting;election systems;single winner approval voting election;voting;plausible heuristics;polling;ballots;mechanical turk;strategic election;particular vote;voter;votes;heuristics;voters;ballot approving;ballot;human decision making;presidential election;most voters;majority elections;simple heuristics;particular candidates;election;vote;candidates"}, "931cbd9d689e9fd6bd91f4e8e1dbdd7fbb6df9de": {"ta_keywords": "end speakerbeam;novel speech recognition system;target speaker;promising target speech recognition results;speech enhancement;speakers;recognition performance;e2e;mixtures;mixture;diarization ability;system;network;end;terms;interesting properties", "pdf_keywords": ""}, "a445adf335aa5212f929f67c1ca56a62c221b43a": {"ta_keywords": "unknowns;discovery;agnostic methodology;oracle;framework;various applications;underlying causes;feedback;efficacy", "pdf_keywords": "informed discovery;predictive models;unknowns;training data;discovery;predictive model;feature network;predictions;systematic biases;knowledge;univariate predictive model;datasets;training data ofwe;bandit algorithms;large datasets;generic feature network;feature similarity;novel bandit algorithm;similar feature values;conceptual clustering;new bandit framework;feature space;bandit algorithm;underlying model;novel approach;subjectivity snippets data sets;descriptive clustering;modelagnostic methodology;feedback;varietyin"}, "947750c717a5fbd17fc52758322d1ca201c4c6bc": {"ta_keywords": "unorganized information extraction;unstructured text file;text;nonlinear systems;disaster;central database;system;japan;phrases;aftermath;collection;key words;paper;creation", "pdf_keywords": ""}, "c45a23e7c565169c5a55898683aceac458c116bb": {"ta_keywords": "3rd chime speech separation;advanced speech recognition;front speech enhancement;robust feature extraction;recurrent neural networks;high recognition accuracy;recognition challenge;language modeling;sri system;merl;model;noisy environments;multi;system;different types;advantage;paper", "pdf_keywords": ""}, "2b0aa68ef2c1773642ca91627a4fc03f536cc5fc": {"ta_keywords": "orbit interaction;spin;orbit;coupled system;dynamics;effect;suppression;enhancement;specific way", "pdf_keywords": ""}, "f762ce106b37728df1126375981a02a589e0497c": {"ta_keywords": "gradient descent;proximal stochastic;coordinate descent;importance sampling;variance reduction;linear complexity;variants;concepts;unified theory;tt;new family", "pdf_keywords": "stochastic gradient methods;stochastic gradients;stochastic gradient;stochastic gradient learning;stochastic optimization;stochastic methods;novel stochastic approximation method;stochastic approximation method;stochastic finite difference approximation;stochastic weak convergence result;stochastic reformulation;novel stochastic optimization algorithm;gradient descent;proximal stochastic;stochastic version;stochastic theory;stochastic optimization problem;stochastic;linear stochastic differential equations;stochasticity;simple stochastic differential equation framework;stochastic processes;stochasticthis paper;stochastic models;quadratic gradient estimator;convex convergence;stochastic process;nonlinear stochastic differential equations;gradient estimator;stochastic partial differential equations"}, "ec084dac14a069da2e924ff7f3d5d2fb75b9b39a": {"ta_keywords": "multinomial inverse regression;dimensional logistic regression coefficients;dimensional logistic regression;effective estimation;dimensional response;novel estimation technique;estimation;estimation properties;prior specification;guidelines;methods;framework;method;article;detailed examples", "pdf_keywords": "text regression;multinomial inverse regression;multinomial inverse regression coefficients;multinomial inverse regression model;multinomial logistic regression;text regression scores;multinomial likelihood framework;low dimensional topic model;supervised document reduction;crowd sentiment distribution;high dimensional logistic regression;sentiment;phrases;candidate speeches;simple multinomial model;informative language tokens;representative political speeches;text data;efficient estimation;lasso regression;lasso;dimension reduction;logistic regression;predictors;novel estimation procedure;fisher information;dimension response;text;lasso penalty;generalized log"}, "bc251481aa5566b1e86a8dbd0417cdf858205e3b": {"ta_keywords": "aware fake news detection framework;fact;preference;preference differences;models;joint detection;respective preferences;pref;respective preferred parts;post;mitigate interference;framework;fend;end;problem", "pdf_keywords": "fake news detection tasks;aware fake news detection framework;fake news detection systems;joint fake news content;false news detection;fake news posts;fake news pieces;unseen news posts;fake news;social media posts;text mining;text classification;false posts;natural language processing;fact;textual contents;preference learner;jointfake news;content;news article;real article;twitter datasets;online social media;feature extractor;synthetic fake version;twitter;pattern prediction;articles;text;sources"}, "dc2f6f092fa04e334dfe2e8592b6d597e00b97ca": {"ta_keywords": "aware semantic classes;hypernymy extraction;semantic classes;distributional semantics;domain taxonomy induction task;hypernyms;noisy hypernyms;scale crowdsourcing study;recall;terms;sense;processing;utility;method;approach;precision;quality;state", "pdf_keywords": "word sense clustering;hypernymy extraction;usedword sense induction;aware semantic classes;distributional semantics;noisy hypernymy relations;quality lexicalwe;semantic similarity;lexical resources;text corpus;semantic classes;semantic semantics;semantic role labelling;word sense;hypernyms;large corpus;lexical quantities;noisy hypernyms;linguistic datasets;semantic classeswe;normal semantic classes;crowdsourcing tasks;sense clusters;question answering;semantics;global sense graphs;word senses;semantic networks;applicationhypernyms;domain taxonomy induction"}, "02a83a01d6236149e4ead01e202b2453f9590e9e": {"ta_keywords": "group fairness;fairness objectives;different groups;confusion matrix;attributes;individuals;several optimization problems;tensor;attribute values;accuracy;multiple trade;offs;measure;trade;elements;general perspective", "pdf_keywords": ""}, "c263507db2c15a8b2e3c955bda7b3c29a1ebd106": {"ta_keywords": "nonlinear learning;supervised learning;dimensional data;large data sets;benchmarking;data;structure;large set;context;method;use;particular interest;new method", "pdf_keywords": "intent detection;natural language;fine grain intents;live chat system;knowledge base;queries;bert;semantics;scope queries;training data;nlu platforms;new datasets;datasets;dataset;classifier;accuracy;machine learning;machine learning system;simple text;results;diverse domains;source dataset;test sets;unintended patterns;performance saturates;bioinformatics;natural systems;large portion;literature;system"}, "9bd170248355047067f05349d57110cc8e4de5cf": {"ta_keywords": "maximum entropy;single copy;copies;distribution;large number;new method;method;number;observation;square", "pdf_keywords": ""}, "9952fe8cbd09e4fc89dc7d76595d138e36c7d7b5": {"ta_keywords": "ranking models;bert;benchmark datasets;transferability;transfer;evaluation;systematic evaluation;datasets;models;case;modes", "pdf_keywords": "shot transfer learning;annotated queries;shot queries;neural ranker;traditional retrieval model;bert;natural language content;shot models;rankingwe;transfer models;bert25;transferability;shot training set;shot training;few shots;deep learning models;systematic evaluation;natural language;results;diverse datasets;bertarian;transfer;datasets;accuracy;art bertarian;passage collections;wikipedia;queries;query types;models"}, "1f76ee8472ec3a41511540f62e4676317df14ea5": {"ta_keywords": "singing voice timbre;singing voice;voice conversion;arbitrary target singer;arbitrary source singer;perceptual age;singer;listener;song;perceptions;method;novel method;paper;notable characteristics", "pdf_keywords": ""}, "5a5fb155b5fc518389a7fe67b55271e143ad695d": {"ta_keywords": "censorshipsored block model;hypergraphs;community recovery;maximum likelihood estimator;observations;mlf;node;general results;information;theoretic limit;problem;results;cbm;case;number;study", "pdf_keywords": "minimizes sample complexity;sparse random graph;large sample complexity;sample complexity;sparse sparse sparse hypergraph;community detection rate;hypergraphs;hypergraph;community detection problem;complexity;random constraint satisfaction problems;sparse cluster;combinatorics graphwe;sample complexity limit;subgraphs;subgraph;communities;nodes;order sample complexity;combinatorics graph;standard graphs;parity measurement;cluster data points;social network elements;successful clustering;recovery algorithm;parity measurement case;community recovery;graph;efficient algorithms"}, "17c7c92db1f4ace842f9db6b44bfce264308b628": {"ta_keywords": "span labels;phrase vectors;parsing;diora;dataset;df;wfsm;wall street journal;prior state;novel approach;versions;art approaches;approach", "pdf_keywords": ""}, "753fd6952c9f06f3bbd46e37129acc3f7a984896": {"ta_keywords": "text generation system;text;generator;guide;passage;input;performance;relevant passages;characteristics;standard", "pdf_keywords": "many text generation systems;novel language generation tasks;generation tasks;textual knowledge corpus;generation task;retrieval;many generation task;natural language systems;unstructured corpus;retrieval quality;retriever supervision;retriever passage;informative conversations;structured text;better generator;novel retriever training system;best retriever passages;retriever models;better retriever;retriever;language model;generator;retriever model;conversations;machine learning tasks;generator underperform;generation;best matching generation;good retriever;passage index"}, "7fa3a5318ac45b2fd93a0130f0ceba9995ffa3c0": {"ta_keywords": "slang;languages;similar terms;similarities;entities;novel framework;framework;baseline methods;tasks;novel application;first mining;number;experimental results", "pdf_keywords": ""}, "8306e4a566e2b1279d5d67b40facc8e1e345c4e3": {"ta_keywords": "strong convergence theory;smooth nonconvex regime;stochastic approximation;convergence rates;bidirectional compression;error feedback;variance reduction;momentum;ef21;techniques;practical extensions;proximal setting;partial participation;approaches;extensions;cases", "pdf_keywords": "new stochastic approximation framework;stochastic gradient descent method;stochastically convergent approximation;novel stochastic gradient method;layer stochastic optimization;stochastic approximation;convex optimization;compressed gradient computation;stochastic gradientswe;optimal error tolerance;composite optimization;composite linear optimization;stochastic gradient learning;stochastic gradients;composite optimization problems;proximal error feedback;bidirectional compression;global optimization;state optimization;optimization;composite optimization problem;optimization method;optimization problems;stochastic gradient model;stochastic momentum representation;systematic error feedback method;error feedback mechanism;stochastic oracle;stochastic version;novel stochastic representation"}, "bad5d2d6d1f3282ebbcb602a6f3a5dd9488fd713": {"ta_keywords": "multilingual phoneme recognition;allophone annotation;allophone utterances;phonemes;phone error rates;universal narrow phone;several datasets;language;set;tuning;performance;significant improvements;method;different sizes;fine", "pdf_keywords": "previous multilingual phoneme recognition models;phoneme recognition model;phoneme recognition;universal phoneme models;phone recognition;universal phone representations;reasonable phone recognition;speech recognition;specific phonemes;phone recordings;acoustic models;allophone modelwe;allophone model;acoustic model;universal phones;allophone layer;specific allophone layer;individual utterances;utterances;new languages;shallow acoustic models;popular allophone;new language;phone;several languages;languages;phone inventory;recognition;common natural language structures;utterance"}, "c64843e51f24773c895511ba9befa8a9bc4924a9": {"ta_keywords": "statistical voice conversion;singing voice;perceptual age;arbitrary target singer;acoustic features;source singer;listeners perception;singer;actual age;age;experimental results;factors;part;paper;investigation", "pdf_keywords": ""}, "8ef0ca924ae88ac2bb2803c49589722b52efc5b4": {"ta_keywords": "affective interaction corpus;affective interactions;corpus;languages;expressive datasets;interactions;indonesian;icra;data;novel study;novel;study", "pdf_keywords": ""}, "192fc995631e3443bb7f291a971089bd06e61017": {"ta_keywords": "human annotation;open problems;new questions;classification schema;open problem;classification;participants;interface;novel interface;corresponding dataset;computer science;prior work;recent work;subset;effectiveness;order", "pdf_keywords": ""}, "39ffb5e9f2f36df42ef8ea010499e484c913e79e": {"ta_keywords": "historical language corpora;normalization;normalization algorithm;corpora;different algorithms;data;novel method;method;use;paper", "pdf_keywords": ""}, "26f427d2b27828f2893e95344342570699e9c589": {"ta_keywords": "code conversions;convertible codes;code pairs;decodability properties;code;data;distance;ki;kf;nf;mds;new framework;resource;property;efficient manner;new class;work", "pdf_keywords": ""}, "5c333f11431d1f0d04ced62b712c8d05ebac0891": {"ta_keywords": "speech enhancement experiments;speech enhancement;gaussian noise;speech signal;conditional diffusion signal;noise ratio;noise characteristics;novel diffusion;diffusion;probabilistic model;original signal;high signal;generalization capability;model;other datasets;performance;training;processes", "pdf_keywords": "novel generative speech enhancement model;speech enhancement;novel speech enhancement algorithm;speech synthesis;world speech enhancement problems;noisy speech;noisespeech enhancement;noisy speech signal;conditional diffusion model;speech data;conditional conditional diffusion process;speech networks;conditional diffusion;conditional diffusion process;noisy distribution;conditional diffusion probability density matrix;noisy data;noisy regime;noisy medium;speech recognition systems;generative models;original diffusion process;reverse diffusion process;original diffusion;noise characteristics;generative model;probabilistic models;diffusion;diffusion process;probabilistic model"}, "c82854b8d4c715da141d34c73bf9bda67adf307c": {"ta_keywords": "sentiment polarity;semisupervised latent variable model;latent variable models;news topics;polarity;polarization;community responses;topics;comment volume;blog;different political beliefs;communities;comments;mcr;multitarget;interest;computational methods;targets;la;proxy;level;degree", "pdf_keywords": ""}, "30a30781c66c758e8e59cdb00c368f3add99768b": {"ta_keywords": "speaker recognition;speaker interface;speaker information;speaker;contrastive learning;supervision models;supervision training strategy;supervision model;acoustic characteristics;augmentation;previous self;training;self;significant improvements;novel self;manner;experiments;recent experimental results", "pdf_keywords": "unsupervised speaker recognition;effective speaker embeddings;speaker embeddings;bias speaker representation models;supervised speaker recognition;speaker recognition;scale speaker identification dataset;speaker verification;speaker training method;speaker discovery;speaker verification performance;speaker speaker training;novel speaker recognition system;augmentation classifier;novel deep learning framework;adversarial training strategy;contrastive learning;data augmentation;speaker;supervised training;augmentation algorithm;novel speaker;acoustic characteristics;parallel training;gradient reversal layer;human benchmark;supervised learning;channel information;supervised learning task;representations"}, "84370f2fa3ac21ed3c6a30144fbdb377157b8853": {"ta_keywords": "conditional preferences;preference reasoning;preferences;elementary operators;agents;single agent;nets;pcps;representation;group;approach;new approach;terms", "pdf_keywords": ""}, "d66110a315f5f216b42d99cfafec31e8e30a03ea": {"ta_keywords": "speaker extraction;extraction extraction signal;extraction target;extraction signal;extraction system;extraction;transducer;stochastic process;target;estimation;feasibility;presence", "pdf_keywords": ""}, "05d1e21f5f0c8209cc125f2e9ccd3a62d6479114": {"ta_keywords": "document similarity metric;similarity;database tables;database;documents;tables;pairs;paper;names;authors;use;method", "pdf_keywords": ""}, "f9409302c4d8201481fe65675bc6f0fa32e01df7": {"ta_keywords": "gaussian noise;light;propagation;effective energy;energy;medium;effect;dominant factor", "pdf_keywords": ""}, "59d3f6a14e20efdf54216188e227e58a351237e5": {"ta_keywords": "generative fingerprint fingerprint network;generative fingerprint;generative fingerprint pattern;novel generative fingerprint architecture;fake image attribution;fingerprint;generative model;ir feature extraction;novel feature extraction;discriminability;content;constraints;experiments;stability", "pdf_keywords": "gan fingerprint;gan images;gan networks;fake image attribution;fake images;different gans;adversarial network;fake image detection;gans;gan;learned fingerprintin;learned fingerprint;violating fingerprint;fake image detection experiment;generative adversarial network;adversarial classification loss;adversarial learning;generative adversarial networks;new fingerprint;novel fingerprint;false images;fingerprint;fingerprint features;generative deep neural networks;irrelevant fingerprint;fake traces;scalable fingerprinting method;digital copyright infringement;generative models;generative model"}, "207c64b36fbd6accf7067366a251d071e8dd03a7": {"ta_keywords": "mountain flora;complex dynamics;complex multispecies system;unbound species;consistent multispecies model;mountain;unbound range;model;unboundest part;self;detailed investigation;means;results", "pdf_keywords": ""}, "4f6d64eec6eaa38177ae45ad6315cf25d1535294": {"ta_keywords": "attentive history selection;history attention mechanism;encode conversation history;conversation histories;conversational questions;soft selection;language;extensive experimental evaluations;position;ham;sci;acad;effectiveness;novel method;model", "pdf_keywords": "history attention model;conversational search tasks;conversation history modeling;conversational search;history attention mechanism;conversation histories;conversation history;conversational task;machine reading comprehension;history selection module;answer span prediction task;attention weights;machine comprehension task;dialog act prediction task;answer span prediction;history modeling module;recent qcd document quality assessment;dialog act prediction;conversations;dialog;helpful history;conversation;machine comprehension problem;natural language;certain dialog behaviors;language modeling;subsequent dialog follow ups;bert model;cnn;available history"}, "e8135016ff3bd33ace936e50247fd650fcc58a7a": {"ta_keywords": "translation accuracy;translation evaluation;discrete translation lexicon;neural machine;minimum risk training;background noise;method;task;novel method;use;presence", "pdf_keywords": "attentional neural machine translation model;attentional neural machine translation;machine translation task;lexical translation probabilities;translation accuracy;scientific paper translation task;discrete translation lexicons;translation;asian translation;translation signal;target language;language pair;neural models;neural machine;natural language processing;minimum risk training approach;best prediction;accuracyneural networks;english;sentences;neural network;minimum risk training;discrete lexicons;predictions;risk training;novel ensemble model;search;nmt;model;tasks"}, "7847419becbc04596b79f804f844cf9719e875ea": {"ta_keywords": "unannotated demonstrations;level subtask descriptions;sparse natural language annotations;level actions;level subtasks;action sequences;hierarchical policies;demonstrations;autonomous decision making;reusable skills;descriptions;generative model;discovery;goals;sequences;model;framework", "pdf_keywords": "partial subtask descriptions;level subtask descriptions;sparse natural language annotations;subtask descriptions;training demonstrations;unannotated demonstrations;supervised reinforcement learning;latent natural language descriptions;subtask abstractions;structured tasks;level subtasks;level tasks;unlabeled demonstrations;level actions;subtasks;hierarchical training strategy;action sequences;policy sketches;level plan inference;tasks;subtask;conditional language models;natural language descriptions;unobserved subtasks;actions;related tasks;hierarchical policies;supervised training;level plans;compositional descriptions"}, "ad734a3f530a6c338af6bf2bf678e5af05477c1a": {"ta_keywords": "secret sharing;communication complexity;networks;general networks;network;lower bounds;algorithm;communication;dealer condition;constant factor;problem;solution;contrast", "pdf_keywords": ""}, "11154216ca898590e7b2f0339587e3378c2c646c": {"ta_keywords": "expressway work zone;connected vehicle environment;connected vehicle technology;drivers;psychological characteristics;work zone;psychology;confidence;machine interface;information;study;results;board;difference;method", "pdf_keywords": ""}, "87951cea6573eed827986371a35025e478d3c184": {"ta_keywords": "novel stochastic gradient;stochastic gradients;stochastic stochastic gradient method;importance sampling;several variants;variants;rates;algorithm;new variants;strategies;sg;monotone;art estimates;state;unified framework", "pdf_keywords": "stochastic extragradients;stochastic extragradient;stochastic extragradient methods;stochastic variational inequality problems;stochastic gradient descents;stochastic gradient descent;stochastic version;stochastic optimization methods;stochastic extension;stochastic estimator converges;stochastic coordinate descent;stochastic estimator;stochastic realizations;stochastic operator;stochasticity;stochastic;stochastic consensus optimization;stochastic coordinate descent method;variational inequality problems;stochastic momentum;variational inequality;optimal stochastic selection;differentiable game formulations;stochastic selection;stochastic noise;monotone optimality problems;stochastic differential equations;stochastic superposition;extragradient;monotone regularization"}, "dfa7bdea128b899d348ed32a84a7ccb1da4340e4": {"ta_keywords": "robust dialog systems;dialog response;long short term memory;retrieval tasks;retrieval task;neural networks;ssr decoder;utterance;retrieval process;new statistical model;data sources;ls;model;identification;generation;user;approach;work", "pdf_keywords": ""}, "3675958405f3ad1633d565efa36b4eb3004bcf59": {"ta_keywords": "social live streaming applications;novel video upload solution;video upload;intermediate video;mobile network;bandwidth;vlt mobile network;quality;benchmark data;performance;viewing time;eso;average increase;world;approach;traditional methods", "pdf_keywords": ""}, "0ca2a7465fe88f1f4912b8dd7b4b0db69a268b0b": {"ta_keywords": "neural lattice language models;language model;multiple language modeling tasks;new language modeling paradigm;lexical items;linguistic intuitions;multiword;information flow;lattice;multiple granularities;polysemy;models;prediction;sentence;possible paths;moderation;approach;experiments;ability;existence", "pdf_keywords": "neural lattice language models;neural lattice language model;neural language models;neural language model;language modeling tasks;language modeling;neural machine translation;language model;vocabulary models;new language modeling paradigm;natural language processing;lexical content;vocabulary structures;standard language models;new language modeling approach;polysemous words;vocabulary embeddings;sentences;art speech lattices;neural networks;neural activity;latent segmentations;neural network;simple neural network architecture;input vocabulary;linguistic intuitions;lattice;polysemy;lattices;segmentation"}, "4786e10003655be97feee21b9d9894a88a62885f": {"ta_keywords": "multilingual transfer;unsupervised truth inference;unsupervised method;transfer efficiency;limited supervision;strong baselines;novel method;art methods;method;distance;popular tool;state;art;terms", "pdf_keywords": "new multilingual transfer models;noisy annotations;source models;novel multilingual transfer model;several unreliable annotators;supervised transfer;multiple source models;natural language networks;entity recognition;multilingual transfer;novel unsupervised transfer approach;source language;bilingual dictionary induction;tiny corpus;supervised batch;target language;corpus;propose supervised models;entity recognition performance;supervised models;single source;target languages;generative model;novel multilingual method;naive supervised low resource settings;generative modelwe;generative setting;unsupervised transfer;multilingualization;linguistic text"}, "4a06bfa86cbccdf5e55dcec3505cdc97b8edb288": {"ta_keywords": "parallel graphs;graphs;relative probabilities;cross sections;parallel;algorithm;new algorithm;vertices;evaluation;context;set;problem", "pdf_keywords": ""}, "622f980030f766e5eb3989f36eea4459ccc948bf": {"ta_keywords": "incremental adaptation;novel incremental adaptation framework;incremental adaptation framework;acoustic model parameters;macroscopic time evolution system;stable adaptation characteristics;acoustic noise;posterior distributions;speech;recognition;variant characteristics;combinatorial approaches;time;key technique;presence", "pdf_keywords": ""}, "ae06bc1e8e67c27b89329ebcfe61b71625d853f6": {"ta_keywords": "diverse explainability techniques;explainability techniques;text classification;downstream text classification tasks;saliency scores;salient input regions;human annotations;diagnostic properties;neural network architectures;comprehensive list;performance;list;human ones;model;rationales;relations;agreement;set", "pdf_keywords": "saliency explanations;explanationability techniques;novel explainability technique;explainability methods;explainability techniques;explainability technique;explainability;downstream text classification tasks;text classification tasks;salient features;saliency models;saliency prediction;saliency model;explanations;saliency;machine learning models;deep learning;saliency scores;cnn;neural network architectures;human annotations;similar explanations;partial description;saliency score;convolutional neural network model;neural model;language model;different supervised learning tasks;supervised learning task;diagnostic properties"}, "43896ea7d488100d135645fbb4be6e7eb2e7f4e2": {"ta_keywords": "many decision tree;features;feature;vector representation;strings;algorithms;world learning problems;set;value;rule;example", "pdf_keywords": ""}, "adb80a4190fdb6a019d57f18ae072ca93f494b7e": {"ta_keywords": "shallow shallow shallow acoustic model;low rank approximation;noisy speech recognition task;rank approximation;weight matrices;discriminative training;rank model;network;novel method;method", "pdf_keywords": ""}, "f3762141fd64bee8d09e55ad4c83057cd4e002d4": {"ta_keywords": "coalition utility learning;cooperative agents;correlation utility function;own estimated utility function;correlation;constrained generalized least squares;correlations;noise estimation;estimation schemes;utility;agents;game;agent;weighted sum;approaches;efficacy", "pdf_keywords": ""}, "388d41b99c9c0867301f345c65877a2796225ead": {"ta_keywords": "transcribes target speaker;automatic speech recognition;multiple speakers speech;target speaker;novel auxiliary loss function;interference speaker accuracy;speaker;utterances;asr;loss attempts;loss;short sample;monaural mixture;training;method extracts;apsa", "pdf_keywords": "speaker extraction model;autonomous speech speakers;complete speech separation task;speaker separation;interference speaker accuracy;speech networks;automatic speech recognition;transcribes target speaker;auxiliary interference speakers;interference speaker;acoustic models;multiple speakers speech;novel auxiliary loss function;target speaker;acoustic model;speaker;time speech recordings;auxiliary input data;deep network;monaural deep network;deep neural networks;speech;more speakers;convolutional neural network;utterances;long short term memory;signal processing;auxiliary output branch;utterance;free maximum mutual information"}, "7a8f8109e65ed9a6048859681a825eb5655e5dd2": {"ta_keywords": "word embeddings;sentence representations;random parameterization;new parameterization;sentence;matrix;experimental evaluation;new method;combination;methods", "pdf_keywords": "word embeddings;empirical sentence encoders;random sentence encoders;sentence representations;sentence embeddings;sentence encoders;sentence encoder;sentence classification;sentence classification evaluation;words model;embeddings;recurrent neural networks;word sequences;autoencoders;empirical sentences;many semantic information tasks;sentences;natural language;random encoders;sentence structures;supervised learning task;semantic information tasks;other encoders;encoders;sparse representations;neural networks;input word;multidimensional sentences;representations;classification task"}, "03058f9a39d37a8bee635969eed227d59bbc8152": {"ta_keywords": "stochastic differential equations;adjoint sensitivity method;partial differential equations;gradients;model context;method;transition;new method;hypotheses;problem", "pdf_keywords": "scalable stochastic adjoint sensitivity method;backward stochastic differential equations;stochastic adjoint sensitivity method;stochastic differential equation representation;stochastic differential equations;backward stochastic differential equation;latent stochastic differential equations;linear stochastic differential equations;gradientbased stochastic variational inference scheme;simpler stochastic differential equation;stochastic differential equation;stochastictorch;stochastic adjoint framework;stochastic partial differential equations;new stochastic adjoint framework;stochastic adjoint;stochastic flow;stochastic dynamics;stochastic equations;parameterized stochastic;arbitrary stochastic noise;stochastic noise;stochastic;stochastic versions;stochastic encoder architecture;stochastic attractor;stochastic generator;adjoint sensitivity method;backward sdes;stochastic maximum principle"}, "4eb62ee328ceac9976c72bca65570d73ca0e8b64": {"ta_keywords": "einstein condensates;repulsive interaction;repulsive attractive interaction;attractive interaction;bose;dynamics;becs;2d;presence", "pdf_keywords": ""}, "30a6a5614727017e7d7981f87df57d17713501a0": {"ta_keywords": "incentives;digital social networks;bandits;novel algorithm;users;algorithm;models;idea;novel approach;approach;realistic examples;features;design;performance;class", "pdf_keywords": "bandit algorithms;novel bandit algorithm;bandit analysis;symmetric bandits;traditional bandit approaches;dimensional bandit algorithm;bandit technique;optimal incentives;personalized incentives;incentive selection;empirical rewards;random rewards;greedy matching;incentive design scheme;greedy algorithm;matching strategy;heterogeneous incentives;optimal matchings;incentives;incentive;optimal payoffs;specific incentives;stochastic payoff;stochastic agent;new greedy algorithm;evolutionary rewards;greedy algorithm thatwe;rewards;optimal strategy;greedy approach"}, "357ff26120dd220d7132f8083697d54b007ef260": {"ta_keywords": "model framework;novel framework;models;framework;model;performance;collisionless;top;results;domains;variety;approach", "pdf_keywords": ""}, "9f24b8f93ed00a1592e02fdb0edf5ebf0d8752ff": {"ta_keywords": "assignment algorithm;conference peer review;reviewers;review quality;fairness objective;assignment;disadvantaged paper;papers;incremental max;flow procedure;fairness;statistical accuracy;total quality;objective;analysis;contrast;problem;focus", "pdf_keywords": ""}, "6a4deeb40aed8a4d56c8d9401c94b6c7a769e8c3": {"ta_keywords": "faceted query;scientific literature search;query documents;input query document;machine learning venues;models;novel application;aspects;task;example;finer;users;set", "pdf_keywords": "faceted literature search;faceted search;scientific literature search;information retrieval;query facets;facet queries;empirical search data;other search tasks;information retrieval task;open research corpus;query facet;empirical search;document matching;facet selection;diverse research paper abstracts;faceted corpora;document representations;parallel search engine;facet example;unstructured documents;unstructured literature;unstructured text;informative documents;facets;interesting documents;model facets;facet;research papers;relevance ratings;scientific papers"}, "356ea9b29101ec6974a7a97b62266b0e7e58d6bf": {"ta_keywords": "strong magnetic field;field;effect", "pdf_keywords": ""}, "9e0d3161b13481418b7e85e3a691d23d67cf1e68": {"ta_keywords": "privacy;convolutional network;several regionaware feature maps;feature channels;dynamic regionaware;images;correlation;crucial regions;input image;attention mechanism;novel approach;drag;self;objects;paper;other important elements", "pdf_keywords": "image privacy dataset;picalert image privacy dataset;object detectors;private images;image sharing;social network;privacy;public images;image detection;visual features;automatic identification;convolutional features;deep learning;scene features;online images;neural networks;privacy risks;convolutional neural network;network;visual information;images;dynamical learning;unawareness;crucial regions;graph;features;sharing process;single image;attention mechanism;correlations"}, "a3c9d1c5e403f35e5694778b86832f0f9a7d87e6": {"ta_keywords": "fast similarity search;large speech data;neighborhood graph indexing;nearest neighbor graph;similar query;utterances;index;graph;significant computational cost;degree;first manner;novel method;method;experimental evaluations", "pdf_keywords": ""}, "77910e51a40d17157fc798325d06edfa6cff18d6": {"ta_keywords": "structured external knowledge;code pairs;natural language;domain code;data augmentation;online programming forum;languages;testbed;retrieval;nl;resampling;new method;pairs;generation;method;order;quality", "pdf_keywords": "mined code snippets;code generation models;programming language reference system;domain code generation task;code generation;apis;domain code generation;relevant apis;semantic parser;semantic parsing;natural language;gnu language;documentation;general language;code;external knowledge;code pair;external sources;source document;semantics;art syntax;retrieval;document document document reference library;other gnu classes;sources;rare data;succinct documentation;partial knowledge;python document;useful representation"}, "948b68677c4f3bcbb1bae7f1d4e1fd5a103f03d4": {"ta_keywords": "speech enhancement quality;speech enhancement;automatic speech recognition;multichannel end;dereverberation model;dereverberation;me2e;signal level metrics;subnetworks;quality;asr;end;system;combination;experiments;paper investigates", "pdf_keywords": ""}, "5edaab1fa078a5c468e3fb26d267ca49be32e70e": {"ta_keywords": "preference elicitation questions;preference elicitation;amazon mechanical turk;effective elicitation questions;elicitation;luce model;aggregation;better group decision;features;plackett;budget;cost;experiments;framework", "pdf_keywords": "preference elicitation;preference elicitation procedure;elicit preferences;randomized voting rules;ranked top alternatives;single agent preferences;luce model;random voting rule;elicitation;group decisions;random market;aggregation;information retrieval;preference;voting rules;discrete decision making systems;social choice;economics;full rankings;single agent;alternatives;queries;online market;preferences;agents;dimensional decision;probabilistic model;optimal design;experiments;information gain"}, "98caf4eb79208cf4bbfe20bde37bc1b6ded6d6de": {"ta_keywords": "entity recognition models;soft gazetteers;resource languages;literature;available information;context;outperforms;cross;factor;method;experiments", "pdf_keywords": "entity recognition models;entity recognition;soft gazetteer features;novel soft gazetteer model;natural language processing;entity mentions;natural language;knowledge bases;limited gazetteer;gazetteer;resource knowledge bases;crosslingual entity;sophisticated ner resources;entity;candidate retrieval algorithm;entities;resource languages;wikipedia;ner mentions;neural network;text;sentences;features;document;resource;suitable feature;ner;input text;ner performance;languages"}, "55b61befce42280c3d57331121c7d349dd8be4cf": {"ta_keywords": "simultaneous speech translation;speech translation;speech;delay;accuracy;speed;text;evaluation;human judgements;results;higher relative importance;users;correlations;trade", "pdf_keywords": ""}, "a949ba38194ad43c86925acec6705b434d5a920f": {"ta_keywords": "counter;analysis;number;method;context;new method", "pdf_keywords": ""}, "22b6e88a2f234fc5646f6239f9040a776e841a97": {"ta_keywords": "bilingual lexicon;translation accuracy;transcriptions;corpus;induction process;induction;sentence;second step;novel method;first step;steps;method;small quantities", "pdf_keywords": ""}, "9abd13caa32b1a90e32462a884a512f8666e80cc": {"ta_keywords": "independent parsing problem;natural language setting;context;multifarious follow;phase process;scenarios;novel approach;phase approach;approach;experiments;experiment", "pdf_keywords": "dependent semantic parsing;semantic parsers;learnable intermediate structure span;independent parser;natural language front;natural language sentences;intermediate annotation;reinforcement learning;attention layer;contextindependent parser;reinforcement learning processes;linguistic structure;sentences;queries;biological sentences;end robotics;corpus;training data;splitting spans;language front;end robotics pipeline;precedent query;several spans;spans;splitnet;query analysis;knowledge;text;splitnet splitnet;query"}, "3c8853d4ae3ad2633c47e840a48951d62b64a5b4": {"ta_keywords": "adaptive label propagation;graph sparsification;label propagation;latent factor extraction;graph construction;view;benchmark datasets;unified framework;remarkable improvements;model;baselines;novel approach;experimental results", "pdf_keywords": ""}, "df29486c04eafd004f2f0816e84c798783802cdf": {"ta_keywords": "visual representation;text;conversion;specific language;improvement;new method;method", "pdf_keywords": ""}, "298a68153859303ee70b3ef1525ee9c7031e32f5": {"ta_keywords": "p2 bot;mutual persona perception;chats;modern cognitive science;chit;key feature;understanding;tools;data;structural information;behavior;novel tool;receiver;valuable tool;transmitter;framework", "pdf_keywords": "interlocutor dialogue generation dataset;novel mutual persona perception method;conversational systems;mutual persona perception;conversational agents;dialogue construction;human interlocutor;dialogue generation;engaging dialogue agents;dialogue agents thatwe;conversations;future dialogue process;dialogue responses;person;human interactions;dialogue;dialogue response;filtering paradigm;interlocutors;persona;natural language processing;novel dialogue responses;interlocutor interaction;chatbot;candidate dyadic utterances;best interaction;interaction;human perception;agent;utterance"}, "251a80dd4126fed3d6ae64f00dc24479f0ba5662": {"ta_keywords": "dimensional electron gas;magnetic field;dynamics;introduction;pedagogical introduction;2nd igfap colloquium;lecture;problem", "pdf_keywords": ""}, "46dab5eb9c11bd49893e2dafa7d1b720a0aa2b3d": {"ta_keywords": "comprehension tasks;paragraphs;social media content;social media tag;words;questions;gating mechanism;interaction;novel approach;approach;art results;properties;state", "pdf_keywords": ""}, "e92677eb974a2814d57de54e2c3733cbd92e2c00": {"ta_keywords": "efficient parallel decoding;latency computation;computing systems;computing;workers;architectures;groups;cost;costs;scheme;structure;orders;need;time;goal;problem;magnitude", "pdf_keywords": "quantum computation systems;efficient parallel decoding;matrix multiplications;hierarchical computing system;hierarchical coding;hierarchical computational structure;matrix multiplication matrix multiplication;matrix multiplication;computing systems;matrix;computation;k2 submatrices;computation time;total computation time;crossgroup code;hierarchy;processors;deterministic quantum system;such processors;computation results;codewe;replication code;architectures;code;parallel;small clusters;clustering;workers;nodes;data processing"}, "0e9e334e2647307f8fa7f9937d93f3ca9095e351": {"ta_keywords": "variational inequalities problems;variational inequalities;saddle point problem;alpha method;saddle point;lipschitz;alpha;new method;variant;method;monotone;version;special cases;use", "pdf_keywords": "monotone operator;optimal point method;smooth convex optimization problem;optimization method;extragradient method;extragradient methods;stochastic extragradient methods;monotone version;optimization problems;optimality;stochastic extragradient;stochastic proximal point method;monotone;convex;subexpansive operator;lipschitz variables;optimal sub;stochastic games;lipschitz index;numerical problemwe;linear convergence resulttheorem theorems;standard mann iteration method;iterate convergence rate;iteration;optimal connection;underlying operator;constraint;iterative propagation;stochastic method;bilinear combination"}, "75ba422d90c488b1388345865e0525208331bb3d": {"ta_keywords": "private data;markov chain monte carlo;typical markov chain monte carlo;mcmc models;mcmc;markovian dynamics;models;efficiency;practical problems;power;good performance;problems;variety", "pdf_keywords": "nonlinear language processing;gradient descent;differential privacy;softmax layer;deep learning;sgd model;gradient models;natural language tasks;private stochastic;learning rate scales;bert;private model;sgd;differential tag;privacy;nlp;parallel learning steps;supervised learning setup;private differential equation;computational linguistic features;partial differential equation models;privacy requirement;partial characterisation;tag;good predictor;law noise reduction;model;rabi modelwe;tuning approach;partial differential equation"}, "9d3e33875ec39001e72313fb919f66242ee97880": {"ta_keywords": "linguistic units;languages;language;discovery;computational techniques;data;method;variety;combination;use", "pdf_keywords": "unsupervised phoneme discovery;acoustic unit discovery;low resource speech features;speech units;acoustic data;linguistic feature representation;spoken language;language reference signal;linguistic systems;linguistic units;orthographic transcriptions;language input;orthography;new acoustic units;utterance;discovery;language;rosetta project;deep learning approach;translations;intermediate symbolic units;neural networks;national security agency;text translation;subwords;encoder;words;image2text system;sequence encoder;national securitythis paper"}, "7d7469e059c6890c24d42931c697df835329f26a": {"ta_keywords": "noise mixture model;signal model;squared error;word error rate;maximum mean;method;art approach;terms;novel method;state", "pdf_keywords": ""}, "f7f6160d4e9e3bf7f36bacbc9f15e916a6f226de": {"ta_keywords": "multichannel speech enhancement;adequate speech enhancement ability;automatic speech recognition;speech recognition;novel multichannel;advanced signal processing;available corpus;end architecture;architecture;network;processing;asp;systematic evaluation;end;components", "pdf_keywords": ""}, "99c87e16c56b8a113124779734951f11bd662d5d": {"ta_keywords": "energy efficiency;energy consumption;social game;utility function;significant energy reduction;energy harvesters;game;building;occupants;performance;actual behavior;desire;good predictor;model;aoccupant;importance;parameters", "pdf_keywords": "social game;social lottery;occupancy votes;occupancy occupancy occupancy occupancy occupancy occupancy players;social database;energy consumption;smart energy consumption;occupants behavior;game;actual occupants behavior;continuous game;energy savings;energy efficiency;smart buildings;games;other occupants;potential energy savings;energy;vote level;energy landscape;occupants;building;convex game;behaviors;utility;nash equilibrium;players;utility estimation problem;utility functions;individual user"}, "4697ef43450f173e12b1e22b77e976dc56fdf5fe": {"ta_keywords": "novel compressive sampling matching pursuit;novel compressive sensing;compressive sensing;art white box attacks;basis pursuit;norm attack;adaptive defence;attack;algorithm;mnist;exploitation;cosamp;public domain library;exploration;cad;experimental results;paper;nasa;state", "pdf_keywords": "compressive sampling matching pursuit;compressive sensing;novel compressive sensing;basis pursuit;norm attacks;norm attack;l0 norm attack;novel adversarial training technique;adversarial training;adaptive defense;adversarial images;art white box attacks;adaptive defence;adversarial examples;adversarial perturbation attacks;iterative attacks;attack;recovery algorithms;robust recovery;attacks;adversarial perturbations;reverse engineering attack;art attacks;compressible signals;defenses;main challenge;defense;recovery method;optimal strategy;bandit theory"}, "ce3d6673b7eebdd0198940316d18e383e9597c9a": {"ta_keywords": "contextual bandits;optimal regret;loss predictors;stochastic environments;predictor;lower bounds;sqr;multiple predictors;total error;help;various settings;problem", "pdf_keywords": "bandit algorithms;contextual bandits;bandits;empirical regret;online learning;learning rate;learning problem;stochastic context;learning process;losspredictors;learningtheorems;learning;stochastic attacks;adversarial stochastic environment;optimality;optimal selection rule;loss estimator;stochastic setting;stochastic settings;losses;randomized attacks;strategy;best strategy;novel strategy;predictor calls;learning model;minimax optimality assumption;exploration overhead;regret;adversarial setting"}, "29b3a609f2b5cb10cffb80a6aaf96413a4a9998e": {"ta_keywords": "compression schemes;efficient compression;art compression ratios;distribution algorithm;invertible flow transformations;iflows;numerical;paper;terms", "pdf_keywords": "lossless image compression;efficient lossless compression;lossless compression;efficient compression;image compression;invertible flow transformations;compression;discrete generative flow;modular coding;new neural compression architecture;compression performance;invertible flow;discrete generative flows;normalizing flows;generative flow;inverse flow;flow layers;uniform entropy coder;binary encodings;compression ratio;comprehensive encoder encoder;high compression ratios;dynamic uniform entropy coder;convolutional flow;network coding;art compression ratios;fidelity images;novel algorithmic scheme;convolutional flows;modular algorithm"}, "24ee54c8d5a01197e015d40be4277cfbb727394f": {"ta_keywords": "bikes traffic;bikes;bikes setup;riders;sharing;city;data;scale datasets;best performance;area;system;set;baselines", "pdf_keywords": ""}, "ea5cfce90444b17b36da07840b2f0cafb54ab0a7": {"ta_keywords": "deceptive corpus;deception;similar english corpus;original corpus;automatic detection;detection;comparison;accuracy;questions;different questions;features;novel method;order", "pdf_keywords": ""}, "e3a1b2a19356dc685d78630ae2a8852ad6c86200": {"ta_keywords": "aware ranking function;aware ranking functions;proximity information;relevance features;search engine;proximity;pruning procedure;variant;performance;method;linear combination;use;experiments", "pdf_keywords": ""}, "5e24aa9fdf5466e96d314dfcde973fccec02995d": {"ta_keywords": "online learning;line learning algorithm;batch learning methods;winnow algorithm;popular voting scheme;features;algorithm;pass;accuracy;margin;data;fly;novel modification;novel", "pdf_keywords": ""}, "e2854bf66ed86a5dc74183bae5fde18e65699833": {"ta_keywords": "polyphonic sound detection;speech recognition;neural network;neuronal network;memory model;lagrangian;lagrangian lagrangian;model;art performance;method;new method;field;state", "pdf_keywords": ""}, "21c9c624bc328686cef4bb1f80a786a5027d8886": {"ta_keywords": "aware scientific information extraction;different scientific information extraction tasks;citation graph;referential links;citation;scientific documents;individual papers;extraction;software tools;papers;information;content;significant gains;structure;sie;approach;novel method;simple ways;development", "pdf_keywords": "salient entity classification;citation graph information;relation extraction models;different scientific information extraction tasks;relation extraction;aware scientific information extraction system;citations;citation graph;referential links;information extraction;salient entities;saliency classification;saliency extraction;mention identification;textual information;end information extraction;aware saliency dataset;level relation extraction;saliency extraction pipeline;natural language processing;saliency extraction problem;best annotations;scirex datasets;scirex dataset;entity extraction problem;citation;scientific documents;salient;saliency;semantic structure"}, "50c651e9f94f9d4927a726af0ef44818179d87da": {"ta_keywords": "multilingual geo query corpus;semantic parser;machine translation applications;machine translation tasks;parser;corpus;semantic information;texts;general context;tasks;use", "pdf_keywords": ""}, "cd0702deabaa8b7ccfba077f89dcc24e48ae1d47": {"ta_keywords": "novel query likelihood relevance ranking;relevance ranking;baseline relevance;maximal marginal relevance;ranking;statistical language models;documents;document;mmr;mixture model;intrinsic structure;data;novel approach;methods", "pdf_keywords": ""}, "c1d0e73ec3aaf7ffdcbe41835d649d638cbc2f2d": {"ta_keywords": "natural language processing;meta consistency classifier;inference;regression tasks;several classification;prediction process;motion;collisionless representation;novel approach;expensive multilayers;approach;effectiveness;use;equation", "pdf_keywords": "novel meta early exit classifier;meta early exit classifier;meta early exit classifierwe;faster classifiers;early predictions;classifier;early exiting;predictiondeep learning;nonlinear realistic classification tasks;meta early exit structure;simple classifiers;backward training strategy;machine learning model;current prediction;classification;high accuracy;forward training strategy;accuracy;predictions;early exiting rate;deep neural network classifier;deep learning machines;forward neural network;elementary layer classifier;neural networks;confidence;bert pretraining algorithm;adaptivewe;model;strong statistical efficiency"}, "77cd3ae8a0b9ef6865d5324a4d62280e6f7a1053": {"ta_keywords": "data augmentation methods;data augmentation;different data augmentation methods;automatic speech recognition;augmentation;generative adversarial network;distanttalk scenarios;e2e;different audio characteristics;data;end;tts;cycleconsistent;module;study", "pdf_keywords": "automatic speech recognition;conventional speech recognition;data augmentation methods;data augmentation;deep learning;input speech;label augmentation;augmentation methods;robust speaker recognition;augmentation;neural networks;deep hybrid;speech;distanttalk scenarios;advanced impulse approximation;recognition;machine learning framework;impulse approximation;neural network;pseudo labels;speaker;noisy data;distance segmentation function;e2e;learning framework;pseudo;generative model;predictingwe;artificial neural network;type segmentation function"}, "3b3a5a9c7352b74e8377ce3182ea646b0bed5b4c": {"ta_keywords": "neural semantic parser;competitive neural semantic parser;semantic parsing tasks;semantic parsers;strong baseline parser;reranker;django;best list;performance;accuracy;bleu;la;test;paper;simple approach;co", "pdf_keywords": ""}, "89d15c9de3608157ff746af7368556149b50e037": {"ta_keywords": "language modeling;textual context;sememes;sememe distribution;next word;language;new framework;framework;power", "pdf_keywords": "language modeling;lexical sememe knowledge;word prediction;atomic semantic units;language model;level language modeling;neural language;minimum semantic units;most language modeling methods;linguistic representations;scale language modeling dataset;atomic language units;sense generation process;sense prediction framework;human languages;word classifiers;corpus;word decoder;linguistic meaning;semantic information;neural attention model;implicit semantics;novel statistical translation framework;language;softmax models;probable senses;hierarchical softmax decoder;words;hierarchical sememe;languages"}, "9dbd86f089c2132dc46d316750d9786d60d5d720": {"ta_keywords": "historical language data;annotation tool;annotation;standard language data;web;tool;anyone", "pdf_keywords": ""}, "6f1ca0249eafa36a5762ac53f6ba2a4ee2133456": {"ta_keywords": "speech recognition;new approach;paper", "pdf_keywords": "transcription error detection;opus codec;human transcripts;transcript;end speech recognition system;transcription errors;novel speech recognition system;popular speech recognition toolkits;theart speech recognition;decoder;corpus;alignment pipeline;audiobooks;audio;speech;encryption protocol;alignment;recognition;reference sequence;input data;alignment graph;deep neural networks;deep neural network;wavefunction extraction;other toolkits;input;original document;tokenized subword;gramwe present;accuracy"}, "22b7a7c9faa8f340520ae1418c9cf8d960aaeec0": {"ta_keywords": "symbolic logic;reasoning system;symbolic knowledge;symbolic language;natural language questions;neuro;data;system;practical implementation;set;complexity;powerful tool;paper", "pdf_keywords": ""}, "eeec05fc11b2e0b40b3b0800bc50930e240cafeb": {"ta_keywords": "corpus;background information;intrinsic background information;context;features;analysis;process;terms;set;novel method", "pdf_keywords": ""}, "525b7f73744f5650391be4678d6d51ddaf23ed72": {"ta_keywords": "detector noise detectors;detector noise;detector detector detector;detector detector;detector;hypothesis", "pdf_keywords": ""}, "d338bcd1e34a8259e123465203b05c5bf21aa12a": {"ta_keywords": "speaker individuality;featureless sound modes;acoustic mode;erj modes;speech;erj;shiba library;power;context;method;use", "pdf_keywords": ""}, "095bc69eddbf73fabf58a929d2be9a99c1b533a6": {"ta_keywords": "preference reasoning;computational social choice;computational preference;specific preferences;social choice;preflib;data points;community;platform;repository;collection;form;new repository;power", "pdf_keywords": ""}, "85099e075880a4844f3de77006a80c73daf99a4c": {"ta_keywords": "metastable metastable state;metastable state;metastable states;different metastable states;simultaneous creation;efficient method;generation;ss;method;use", "pdf_keywords": ""}, "f7ce4c7ec30c846cc122393deee98f1eacd24049": {"ta_keywords": "dialogue state tracking challenge;improved dialogue state tracking accuracy;dialogue state tracker;long short term memory;dialogue;neural networks;state;ldss;dstc;nodes;linear array;set;baseline system;experiments", "pdf_keywords": ""}, "bd24b47165407a8b2d32016645ca71f7c9213636": {"ta_keywords": "intended sender;sender;email;intended reader;receiver;message;ability;best strategy;problem;experimental results", "pdf_keywords": ""}, "b54d49cdf57abf7f7e7bcc0e946f450fd807f829": {"ta_keywords": "reinforcement learning;expert agent;inverse;agent;constraints;behavior;irl;novel approach;set;problem;idea", "pdf_keywords": "constrained demonstrations;reinforcement learning;human obstacle avoidance;constraint inference;maximum entropy framework;reinforcement learning algorithm;future demonstrations;demonstrations;maximum causal entropy formulation;inverse;maximum entropy;more general constraints;maximum entropy setting;unconstrained policy;agent;constraint inference iswe;learning;agents;constraints;obstacle;constraint constraints;human trajectory;observed behavior;likely constraint;demonstrationswe;constraint hypothesis space;robots;inference;constraint;specified task specification"}, "1a2410823486613e327892f05b38d3070f2d712c": {"ta_keywords": "harmonic trap;traps;dynamics;dimensional lattice;particles;lattice", "pdf_keywords": ""}, "3bb1e24eb3429f807397833105d1e137d9927767": {"ta_keywords": "active sequence labeling;label efficiency;effective data augmentation method;active sequence;sequence mixup;sequences;augmentation;discriminator;samples;iteration;efficiency;eq;method", "pdf_keywords": "sequence labeling tasks;active sequence labeling;current active sequence labeling methods;resource sequence labeling tasks;efficient labeling;human annotations;entity recognition;deep learning;effective data augmentation method;simple data augmentation method;active learning;sequencesin;generated sequences;label efficiency;sequence;training data;subsequence mixup;input samples;mixed label algorithm;active learning rounds;proved labeling;sequences;mixed labels;label selection;empirical representation;sample;samples;label selection algorithm;seqmix;level labels"}, "17c9a0f1a287c08bb2c1c1df47fa51ce1e428c4e": {"ta_keywords": "nonlinear dynamical system;shape;accurate prediction;object;method;new method;use;combination", "pdf_keywords": ""}, "a064010cf6fe594b2506a8fecd16dc0040211daa": {"ta_keywords": "multilingual data;translation channel;higher translation gains;better translation gains;resource language;translation;underlying language;data;correspondence;novel approach;approach;methods;method;idea", "pdf_keywords": "multilingual training;neural machine translation;multilingual word;linguistic resources;encode words;translation accuracy;multilingual character;resource languages;subword length encoding;multiple target languages;language generation task;decoder;vocabulary size;linguistic discovery task;low resource models;different encoding strategies;encoder;decoder layers;encoding;decoders;vocabulary;low resource;english;translation;powerful transformer encoder;side vocabulary;gram;standard sde;sde;words"}, "2fbb75d7947808698f1554e4d400ec5ecb5ef998": {"ta_keywords": "typical reading comprehension task;global prediction model;long document;performance;model;novel method;magnitude;order", "pdf_keywords": "reading comprehension model;answer selection task;answer selection;challenging reading comprehension;reading challenge;context retrieval;multiple answer candidates;long summaries;span prediction models;span prediction model;narrativeqa dataset;comprehension;answer candidates;long documents;text;global normalization;query document;documents;document;global normalization method;project gutenberg;sentence;selection;predictions;context;answer;neural network;model performance;intrinsic context;performance"}, "c7c93601b52b1bcc68ec1f8b2c77c54f1b358ab9": {"ta_keywords": "pairwise comparison models;pairwise comparison data;sample tests;lower bounds;test;sample test;assumptions;information;samples;range;distributions;theoretic;state;art;role", "pdf_keywords": ""}, "e59adee86b666ad76164b3446cfee5068a15e5c9": {"ta_keywords": "fault tolerance;nn layers;neural networks;abft;nns;many nns;adaptive approach;intensity;time overhead;execution;traditional approaches", "pdf_keywords": "deep computations;available deep learning pipeline;deep learning algorithms;available deep learning pipelinewe;efficient fault detection;nn inference;deep learning;deep connections;deep networks;deep learning context;high compute;deep inelastic neural networks;redundant computations;gpus;impart fault tolerance;deep learning scenario;deep learning train;fault tolerance;bottleneck;machine learningchecksum;tensor network architectures;efficient redundant execution;machine learning tasks;unused computation cycles;gpu;general purpose tensor network;graphics processors;faults;nn layer;nns"}, "e6fa88f4af68aa7be4ae91940892eee52571997c": {"ta_keywords": "shot object detection accuracy;shot object detection;rare objects;tuning framework;tuning;tuning technique;challenging task;new implementation;new approach;method;fine;new state", "pdf_keywords": "shot feature extractors;shot object detection;shot detection performance;shot classification;shot learning;shot detection;shot object detection model;novel object detection;shot object objects;shot image classification problem;shot objects;higher detection accuracy;object detection;training shots;stage object detector;detection accuracy;cnn;shot;shots;detection;object representation;low shot regime;rare objects;rare object;new benchmark;classifier;new objects;objects;learning;machine learning"}, "db0a3ce9f315f650fe5220101c5677778de39fee": {"ta_keywords": "discriminative parser;machine translation;translation accuracy;parallel text;parser;margin training;order;model;accuracy;input;paper;method", "pdf_keywords": ""}, "06f4de06fc37576e1e381cd76e375d57852047b9": {"ta_keywords": "neural machine translation;efficient machine translation;nmt;text;efficient transformer;task;reliability;novel approach;novel method;systems;combination;power;method;date", "pdf_keywords": ""}, "cd3595f65519e4af6bcd073790ac32acdafadf55": {"ta_keywords": "shot image generation;adaptation;regularization;variable weights;weights;different target domains;models;few examples;novel method;quality results;algorithm;changes;effectiveness", "pdf_keywords": "shot image generation;multimodal image generation;unconditional image generation;generative adversarial networks;generative models;generative matching networks;artistic domains;multimodal image translation;style transfer task;unsupervised generation;generative model;photo domain;shot model;adaptation;images;realistic adversarial learning model;new generation method;multiple images;emojis;minegan approach;effective adaptation technique;generator;other layers;source domain;diverse results;joint convolutional neural network;machine learning;variations;target domain;convolutional neural network"}, "6887537de3655a25c75bf4d0833f51e72331bdad": {"ta_keywords": "extended signal;noisy signals;noisy signal;signal;signals;detection;noise;estimation;mask;estimate;method;phase;environment;variety;size;problems", "pdf_keywords": ""}, "a8fc183c089bd596ccc48b3d666f8814e1b41e55": {"ta_keywords": "generative code models;program synthesis;comment generation;type inference;bidirectional context;code;large corpus;right generation;inexact;challenging tasks;tasks;models;performance;shot setting;ability;standard", "pdf_keywords": "code generation;generative code models;other generative code models;code generation system;binary code generation tasks;generative code model;program synthesis;comment generation;generative language model;binary generative code engine openai;novel structured generation method;practical code infilling;shot program synthesis;standard code generation benchmarks;arbitrary code;generative models;synthesis tasks;causal masking;type writer approach;causal masking objective;autoregressive language model;program;code;binary code files;generation;language models;editing;type inference;large corpus;type annotations"}, "9a43dda4b01dde5d513c431564098e4d8794a7a5": {"ta_keywords": "level sentiment;supervised classification;product reviews;semantic spaces;clusters;art clustering approach;public taxonomy;words;large datasets;document;terms;method;accuracy;current state;robustness;new method;use;art;state", "pdf_keywords": ""}, "2f201c77e7ccdf1f37115e16accac3486a65c03d": {"ta_keywords": "optimal pruning;quantization;pruning rate;small pruning rate;partial differential equations;nodes;network;classic lemma;perturbation;problem;version", "pdf_keywords": "adversarial attacks;adversarial example;adversarial examples;dynamic networks;noisy adversary;deep network;learning structure;neural networks;deep learning;neural network;high generalization accuracy;stochastic activation;networks;rapid learning;deep features;models;random activation;robustness;stochastic reinforcement learning model;model;reinforcement learning agent;higher accuracy;dynamical structure;network;activations;dataset;accuracy;reinforcement learning model;random perturbations;dropout technique"}, "136235d2a3dc4f1c995eaf977aec9c42114da850": {"ta_keywords": "dyadic morphosyntactic features;uncertainty resource;new empirical resource;description;first results", "pdf_keywords": ""}, "8a09c90f6e9a3f6c3b172e5059c7af47f528f66b": {"ta_keywords": "semantic reinforcement;reinforcement;reinforcement strategy;classical reinforcement strategy;letters;latent space;unsupervised approach;word;similarities;compute similarities;participants;theme;computational approach;message;outputs;way", "pdf_keywords": "letter images;generative way;logograms;semantic reinforcement;letter image;text;logo generation;tunable generative image;generative model;linguistic cues;google doodles;input word;letters;creativity;fascinating linguistic patterns;visual cues;images;inherent similarity;creative process;unsupervised approach;word;word recognition accuracy;characters;many words;statistical neural network;interesting images;logo;latent space;relevant pngs;word recognitionpictogram implementation"}, "1022696090666eab5c82ebc07d63c0de2fca2521": {"ta_keywords": "soft joins;relational databases;new relational database;inductive classification tasks;wide web;atomic values;data;extension;equivalence;new system;performance;system;world;number", "pdf_keywords": ""}, "71124b00b873e85aa55b07100cd5b492e5b1d73d": {"ta_keywords": "quantum key distribution network;data integrity certification;data integrity;key distribution network;japanese quantum;universal2 hash function;untrusted users;integrity;data owner;party verifier;random number;data;scheme;data reconstruction;end user", "pdf_keywords": "verifiable secret sharing scheme;secure sharing scheme;arbitrary secret sharing scheme;secret sharing scheme;data integrity guarantee function;integrity protection;secret sharing;quantum cryptography;quantum key distribution networkin;data integrity;data integrity applications;data integrity certification;data sharing;authentication;quantum key distribution;sharing scheme;cryptographic tools;storage system;integrity check;verification scheme;qkd network;universal2 hash function;secure communication line network;integrity;actual qkd network;longterm security;leftover hash lemma;hash function;party verification scheme;security"}, "eb7a64195ef4a268f79fa6740f128387f2696c65": {"ta_keywords": "autonomous reinforcement learning;reinforcement learning;contextual bandit technique;environmental rewards;reward;agents;constraints;implicit constraints;algorithms;demonstrations;environment;unspecified constraints;pac;man code;novel approach;approach;society;set;problem;functions", "pdf_keywords": ""}, "6cddfbed35c46937588bd9d6b846ca2855953cea": {"ta_keywords": "neural lattice;lattice;lattices;discrete lattice;posterior probabilities;neural neural network;posterior score;model;architecture;words;number", "pdf_keywords": "lattice encoder;performance lattice encoders;novel lattice encoder;attentional encoder;machine translation model;lattice models;lattice model;lattice scores;parent lattice;lattice score;lattice;lattice nodes;lattice weights;lattices;strong attention mechanism;translation accuracy;underlying lattice structure;attention mechanism;fine tune lattices;term memory;parallel corpus data;decoder model;sequence model;encoder;speech translation corpora;deep learning model;speech translationwe;encoding;neural network;translation"}, "58a2e825884bc86e650fffafb86a2833117852c5": {"ta_keywords": "model hub;model hubs;label evidence;models;target ptm;ptms;ptm;model architecture;features;tasks;tuning;new paradigm;maximum value;preference;various types;fine", "pdf_keywords": "transfer learning;pretraining algorithm;deep learning models;deep learning;benchmark models;efficient transferability metric;partial learning;transferability metric;transferability estimation;language models;high transferability;learning;models hub;transferability assessment;supervised regressionwe;transferability;models;benchmark;neural networks;structured learning;neural network model tuning technique;exclusive learning models;model hub;training data sets;networks;features;possibledeep learning;training process;feature;prediction"}, "e6cec3044688f1701b4b72b4b2189f215abc3759": {"ta_keywords": "crowdsourcing experiments;new reward mechanisms;agents;parallel data sets;data sets;truthful behavior;agents abilities;mechanisms;beliefs;large numbers;heterogeneity;simple structure;existence", "pdf_keywords": ""}, "549df5fc83c382cbdf633dc782fa67bf2f983f2c": {"ta_keywords": "data breach threats;database;storage system;agents;data;activations;large number;simple way;fact", "pdf_keywords": ""}, "f61886d138497431cbeaa7bb73051bfb7a745026": {"ta_keywords": "captions;linguistic structures;sequential context;level supervision;objects;context;subjects;image;localization;generation;prior methods;relations;approach leverages phrasal;individual triplets;information;method;performance;extensive experimental comparisons;combination;novel method", "pdf_keywords": "scene graph generation;specific scene graph representation;scene graph;scene graphs;visual relation detection;truth scene graphs;vision language;captions;understanding objects;scene;supervised supervised search pipeline;image descriptions;object detection;linguistic annotated regions;actual captions;holistic representation;supervised search pipeline;linguistic context;global context;text graph;linguistic representation;text graphs;supervised learning;context;joint detection;deep network;new joint classification;text entities;computer vision;objects"}, "275aaa20ba853c40a461f224eefbf06730bf03a9": {"ta_keywords": "nonconvex optimization;negative curvature;saddle points;simple gradient;algorithm;descent;numerical experiments;art algorithms;results;literature;state", "pdf_keywords": "quadratic gradient descent algorithm;quadratic gradient descent;quadratic gradient descent method;gradient descent;linear gradient descents;gradient descent methods;nonconvex optimization;gradient descents;hessian power method;alternating gradient approximation;gradient descent problem;nonconvex optimization problem;hessian matrix;robust gradient;negative curvature saddle points;gradient method;stochastic gradient descent setting;hessian representation;stochastic gradient descent sequence;entire hessian;gradient calls;gradient;stochastic gradient descent settingwe;saddle points;simple gradient;gradient function;stochastic gradient;nonconvex test function;quadratic approximation;polynomial speedup"}, "e6924d247b56980260e4c68dbc51b947409e4764": {"ta_keywords": "chiral symmetry breaking;chiral phase transition region;chiral dynamics;chiral symmetry;effect;consequence", "pdf_keywords": "stochastic gradient descent;arbitrary local stochastic gradient estimators;local stochastic gradient estimators;local stochastic gradient algorithms;local stochastic gradient refinement;local stochastic gradients;local stochastic gradient method;local stochastic gradient;stochastic gradients;stochastic gradient method;stochastic gradient;stochastic gradient propagation;global stochastic gradient;standard stochastic gradient method;stochastic optimization problems;stochastic average gradient;local stochastic solver;local gradient estimator;stochastic methods;generalized gradient models;local gradient methods;local stochastic;stochastic differentialtheorems;efficient learning;partial gradient computations;new stochastic version;optimal drift;gradient methods;iterative minimization;local stochastic geometry"}, "a1fc0041ef89ed5371317c8e2cc5effa8f38ae48": {"ta_keywords": "local clusters;local search strategy;nodes;superstructure;exact search;algorithm;hops;neighbors;sub;high accuracy;methods;hundreds;assumption;variable;faithfulness;new class", "pdf_keywords": "reliable causal discovery;optimal sparse causal graphs;probabilistic network;noisy noisy noisy data distribution;noisy noisy noisy signal;optimal learning;linear hierarchical structure;redundancy estimation;correct substructure estimation;inverse covariance matrix;random matrix theory;detection;noise distribution;local clusters;parent graphs;estimation;local search strategy;global optimum;noise peaks;nodes;exact search algorithm;accurate estimation;search algorithms;parent graph;exact search;faithfulness search;data structure;search;network;search space"}, "0a227a21172f7344ad911aeefc40ae4ec82d7cac": {"ta_keywords": "metaphor identification;metaphor;corpora;datasets;novel;new features;creation;identification;methodology;collection;papers;paper", "pdf_keywords": ""}, "178f424d0f156cbf5b35eb241fc00b27a0a3808b": {"ta_keywords": "2nd chime speech separation;speech enhancement;sequence training;term memory recurrent;neural network architecture;term memory;neural network;ssr;hybrid ssr;recognition challenge;recognition;architecture;paper", "pdf_keywords": ""}, "317ed59456d76b500a7eb63b181df9e8b795976b": {"ta_keywords": "parking;urban environments;new framework", "pdf_keywords": "parking congestion;optimal queueing equilibria;parking demand;optimal parking infrastructure design;parking availability;parking managers;parking infrastructure;parking;parking area;utility maximizers;parking spot;parking region;queue;congestion;optimal pricing mechanism;parking ring;queue length;optimal rate;optimal strategy;optimal social welfare;optimal driving strategy;queueflow simulator;parallel queues;weparking demand;park;global optimal strategy;certain optimal optimization problem;equilibrium;random pricing models;utility functions"}, "56823e326f2515f73662b176054fbee0895e0c44": {"ta_keywords": "web form;navigation;assistant;requests;form;machine learning;users;behavioral experiments;alternative;interaction;ambiguous communications;user;usefulness;stream;method;new method;specialist;experimental results", "pdf_keywords": ""}, "7891ec1d8ba2abf238326dc6e8862cc4431a6f5c": {"ta_keywords": "wireless relay node;random lattice path;convex hop costs;multihop network;network;nodes;priority;deployment;fringes;measured signal;power law;objective;sum;number;model;problem", "pdf_keywords": ""}, "cdf5eb63e9c2434073e811aba50ae80ede9d15f6": {"ta_keywords": "focused retrieval documents;retrieval search service;document corpus;search;new collection;focus", "pdf_keywords": ""}, "1afe82d34c182d43cbcc365d26e704058aa32351": {"ta_keywords": "voice conversion;speaker model;dynamic features;parameter generation algorithm;parallel corpus;model optimization approach;joint density model;model;parameter trajectory;mixture;high accuracy;integration;new approach;paper;use;requirement;approach;first problem;problems;experiments", "pdf_keywords": ""}, "2c871df72c52b58f05447fcb3afc838168d94505": {"ta_keywords": "knowledge neurons;bert;such knowledge;factual knowledge;language models;knowledge attribution method;neurons;blank cloze task;language;concept;context;storage;fill;light;preliminary results", "pdf_keywords": "knowledge neurons;such knowledge neurons;exclusive knowledge neurons;corresponding knowledge;knowledge expression;text knowledge base;knowledge prediction;knowledge;unstructured knowledge;underlying knowledge;factual knowledge;knowledge attribution method;structured knowledge;language models;specific factual knowledge;training corpus;relational information;neuron;neuronal activation;neurons;relational fact;deep structure;texts;natural language processing;neuronal state;attention;neural networks;neuronal dynamics;relational system;attribution score"}, "26c2aad87810418b09e0f5b80352dd4d2536afe3": {"ta_keywords": "social skills training;social skills;social interaction;sst;training system;human anxiety;user speech;conventional sst;human participants;target skills;virtual avatar;reinforcement;play;role;computer;language information;feedback;system;discomfort;design;users;method;procedure;article", "pdf_keywords": ""}, "07cedc7899497f2f4ee6f4736e03b78accb47b74": {"ta_keywords": "relational neighbor classifier;random graph walk;label;few labels;authoritative instances;several benchmark datasets;same classification accuracy;large margin;training seeds;data;amount", "pdf_keywords": ""}, "d4d26ccbf1e64e725b5bffc08ab28a72e271facb": {"ta_keywords": "comprehensive analysis", "pdf_keywords": ""}, "26d5c7ad2778c77a1b8734dceb34fe38a1179e2f": {"ta_keywords": "malwares;malicious application;mobile devices;detection;m1 model;rnm;time m1 model;model;novel model;framework;presence;time;version;modification;paper", "pdf_keywords": ""}, "9045bf2a9c1e2b9621c69c57f991d10880e91f18": {"ta_keywords": "submatrix detection;submatrix detection problem;computational hardness;randomness;secret leakage;sparse;clique problem;statistical problems;reduction;variety;problem;input;useful tool;context", "pdf_keywords": "worst case logspace algorithm;computational hardness;logspace algorithms;computational complexity;optimal logspace algorithm;logspace algorithm;randomized logspace;best logspace algorithm;new space complexity analysis;optimal logspace;clique conjecture;complexity;computational efficiency;many statistical problems;lower complexity variants;clique hypothesis;various statistical problems;randomized algorithm;other statistical problems;several statistical problems;case complexity;multiple access logspace algorithm;computational tradeoff;logspace hypergraph;clique problem;logspace;statistical problems;interesting computational phenomena;logspace size;random graphs"}, "24a2f68cf81ba3ee55e7a87d0770374ab8e99858": {"ta_keywords": "recursive programs;equivalence queries;diicult learning problem;predictability model;constraints;class", "pdf_keywords": ""}, "5ed4b17a4b7932619f0969e1f5acae76e90f7bdd": {"ta_keywords": "novel recursive semantic parsing framework;query generation problems;query generation problem;sql query layer;different query layers;recparser;novel question denest module;recparser module;utterance;spider dataset show;layer;different components;approach;experimental results", "pdf_keywords": ""}, "feb403bb5a064ab68b2db655b80a7417f7cfc9f3": {"ta_keywords": "novel relational learning approach;relation tree;relational networks;relational data;contrastive induction;induction;pairwise pairwise pairwise features;hidden variables;tree;efficient algorithm;snd;cvi;approach;technique", "pdf_keywords": ""}, "2fb44f1317bc51a1e011a5a44d817ad9104e29e8": {"ta_keywords": "private text;tight privacy guarantees;private mechanism;encoder;private auto;descrip code;small encoder dimension;text;true sensitivity;new analysis;least factor;optimistic case;results;downstream tasks;impact;error", "pdf_keywords": "differential privacy;privacy;private mechanism;randomization process theorems;privacy problems;private access;private process;private domain;randomization process;autoencoder;uniqueness;individual records;individual utterance;theoremstheorems;theorems;large scale machine learning;database;characterization;sharing;curator;theorem;machine learning;other individual;large scale data sets;formalin;text;nonlinear hypothesis;domain;powerful tool;violations"}, "b3848d32f7294ec708627897833c4097eb4d8778": {"ta_keywords": "neural language models;language models;dialog;noise ratio;signal;annotated data;model;safety;factual grounding;tuning;external knowledge sources;lamda;significant improvements;new aspect;ratio;key challenges;family", "pdf_keywords": "dialog models;future dialog models;neural language models;dialog response generation tasks;ended dialog models;neural language model;dialog responses;large language models;dialogs;dialogue evaluation;ended dialog modeling problem;dialogue response;generative language models;stochastic language models;dialog;dialog applications;safe conversations;sentence representations;natural language;generative language model;scale language models;contextualized word representationswe;safe topics;contextualized word representations;dialogue selection;unsafe responses;dialogue agents;dialogue;traditional generative language model;dialogue comparison"}, "17c5e16d16585a01fbfd90ff39f6799952675b21": {"ta_keywords": "bilingual speech recognition;final bilingual output;monolingual information;corpora;novel joint modeling framework;code;model;efficacy", "pdf_keywords": "bilingual speech recognition;novel bilingual conditional likelihood model;bilingual task;bilingual model;bilingual systems;bilingual label sequence;bilingual situations;monolingual speech;monolingual classifiers;monolingual conditioning;monolingual label;bilingual andwe;novel conditional conditional conditional model;language;different conditional probabilistic graph;sentence sequences;corpora;utterances;cnn models;single sequence transduction task;conditional conditional model;speech signal;conditional conditional conditional model;conditional model;encoder;speech;cnn;convolutional neural network;conditional data;subtasks"}, "c204d40384d39c59cd7249bde4cd8615972acaac": {"ta_keywords": "current machine translation systems ability;machine translation;language pairs;robustness;languages;challenges;task;results;fewshot variants;second edition;real world;mt;white paper;form", "pdf_keywords": ""}, "024aa0b78e2a29d07533ee1c6e3b2e875ae45618": {"ta_keywords": "conversation data;influence;expectation maximization;probabilistic model;estimation;influences;meeting data sets;own earlier word use;people;english;companions;model;effectiveness;algorithm;experiments;method;em;japan;applications", "pdf_keywords": ""}, "ffc211476f2e40e79466ffc198c919a97da3bb76": {"ta_keywords": "decentralised learning;generative reinforcement learning tasks;agents;agent;decentralised version;value;novel value;consistency;policies;centralised structure;methods;intrinsic structure;method;action;set;use", "pdf_keywords": "reinforcement learning;starcraft ii learning environment;multiagent reinforcement learning system;optimal actions;reinforcement learning framework;decentralised learning;arbitrary strategy;agents value functions;decentralised unit micromanagement;value decomposition network;rank ii micromanagement tasks;starcraft ii;optimal strategy;micromanagement tasks;heterogeneous agents;joint actionvalue function decomposition;joint actions;competitive games;agents;learning;decentralized linear combination;centralised methods;centralised representation;decentralised state information;representational network;starcraft ii1 system;deep learning network;learning method;micromanagement problem;agent"}, "a6b431df3b3d40c98d8d623cab559a9cddd41662": {"ta_keywords": "generative dialog model;generative schema;dialog system;generative connective tissue;learning model;schema;learning process;training data;specific dialog policy;connective tissue;protein;data;algorithm;task;article;experiments", "pdf_keywords": "shot dialog;dialog task;rich dialog structure graph;dialog data;dialog systems;dialog;dialogue response graphs;dialog generation problems;dialog policies;dialog policy;dialog history;specific dialog policies;ravenclaw dialog model;shot generalization;zeroshot task transfer;attention weights;meaningful schemas;next action prediction model;conversational agents;training data;novel schema;schema representation;schemas;schema;unseen tasks;new task;novel attention mechanism;shot setting;schema graph;task"}, "62763dbdd47f144c73663b6c6b5d95caeb318e43": {"ta_keywords": "structured matrix;classical low rank assumptions;rank model;underlying matrix;estimation;low rank;rank;matrix;information;noise;permutation;entries;lower bounds;problem;theoretic;rates", "pdf_keywords": "rank matrix completion;rank matrix completion problem;matrix rank decompositions;noisy noisy matrix completion;low rank matrix;matrix completion;rank matrix;positive rank matrices;positive rank matrix;matrix completion problem;noisy matrix completion error;arbitrary permutation rank;rank model;permutation rank;new rank model;low rank;rank;low matrix;underlying matrix;underdetermined inverse problem;matrix theory;least squares estimator achieves;noisy observations;least squares estimation;structured matrix;least squares problem;user views;other submatrix;minimax;matrix"}, "bd49e66af9755e6138967eba6aeb37d8190d2b4f": {"ta_keywords": "couple;spouses;text;pairs;bias;parents;argument;method;end", "pdf_keywords": "relation extraction tasks;relation extraction datasets;natural language inference datasets;semantic parsers;natural language;natural language explanations;computational semantics;underlying explanations;sentences;bert;spouses;nln;relationship;inductive biases;couples;inductive bias;patterns;models;explanations;text;datasets;language;family;input sentence;model developers;prior knowledge;only explanations;empirical evidence;representations;empirical representation"}, "5e51edfcef2b28594c63cce97c08752dfd438af0": {"ta_keywords": "structured soft margin confidence;soft margin support vector machines;structured learning;multiclass confidence;phoneme conversion task;novel regularization;learning;margin error;novel method;graph;method;conventional methods;best hypotheses;terms", "pdf_keywords": ""}, "eebfece29b7a5c2202f1ec53ef49d6fdb75ce0ea": {"ta_keywords": "teacher neuron;fire neuron;sparse feedback signals;online learning;synaptic weights;complex temporal functions;intrinsic parameters;fire;novel approach;parameters", "pdf_keywords": "neuronal dynamics;neural dynamics;neuronal network;neuron;neurons;neural computations;learning dynamics;synaptic connectivity;exact neural computations;synaptic weights;learning time;gradientneural networks;fire neurons;spike patterns;intrinsic electrophysiological properties;neuroscience;fire neuron;simulated neural networks;novel learning rule;potential spike;neural network;neural networks;spike times;learning rule;learning rule scales;learning;spike train;learning paradigm;demonstrateneural networks;spike"}, "e31efa7295e5d6681607ed8ef9c45300d64227aa": {"ta_keywords": "optimal manipulation;voters;partial information;candidates;vote;highest utilities;better outcome;scenarios;effect;people", "pdf_keywords": "strategic voting behavior;voting strategy;approval voting;simple voting heuristic;voting process;decision heuristics;truthful voting;voters preferences;voting behavior;voting pattern;single transferable vote;user voting behavior;approval elections;optimal heuristic;voting rules;voting;winner conditions;heuristics;truthful vote;optimal choice;optimal manipulation;take best heuristic;preferable strategy;voting profiles;best heuristic;votes;optimal outcomes;winner elections;heuristic;voters"}, "99c4007b1f6cb905788479db7fc886168f05e57c": {"ta_keywords": "robust automatic speech recognition;semilocal deep neural networks;recurrent deep neural network;rnn;asr;snns;hybrid system;hybrid;system;set;novel;work", "pdf_keywords": ""}, "c783e1fb3ce8514f981925ee590c00884660ee4e": {"ta_keywords": "structured multimodal documents;generative models;large corpus;image representations;wide web;text;models;new models;world;internet;new family", "pdf_keywords": "bidirectional context control;multimodal tasks;generative models;large corpus;generative modeling;text language models;bidirectional context;full generative modeling;deep link;generative adversarial networks;decoder;hypertext;unstructured text;nontrivial image generation;coherent image captions;image tokens;encoder;text languagewe;rich text;text;image captionswe;language modeling objective;encoding;pure image documents;linking;captioning;long token spans;direct supervision;representations;scale language models"}, "80b92f762e116d4513da27792822897ca3915247": {"ta_keywords": "strong privacy;privacy;intrinsic clustering;discrete models;nodes;edges;graph;datasets;optimizers;guarages;novel method;method;context;approach", "pdf_keywords": "discrete graph training;graph networks;graph representation;large privacy;strong privacy guarantees;discrete graph representation;private training data;privacy;baseline privacy;training graph;deep learning;large graphs;privacy guarantee;representation learning;privacy budget;deep learning networks;deep learning paradigm;privacy tradeoff;scalable graph splitting algorithm;large graph clustering;private stochastic;networks;natural language processing;discrete graph structure;random graph splits;convolutional networks;graph;social networks;graph towe;graph splitting approach"}, "3d5b51fc30ffacdcc8424618555accb36756ccc9": {"ta_keywords": "unconstrained minimization problem;several stepsize selection schemes;iteration complexity;stochastic;novel stochastic;smooth function;points;nonconvex cases;method;space", "pdf_keywords": "stochastic direct search;stochastic descent;stochastic optimization problem;random vector search method;random search method;random pursuit method;simple randomized algorithm;derivative free optimization;stochastic variant;stochastic derivative;convex optimization problems;stochastic version;unconstrained optimization;deterministic direct search;free optimization setting;quadratic quadratic derivative free optimization;random vector directions;quadratic quadratic quadratic derivative free optimization;stochastic;direct search methods;point method;unconstrained optimization problem;points method;descent method;free algorithm;descent descent method;empirical derivative;random directions;minimization;iteration complexity"}, "845aad7b99f48526fe003c775836091521624471": {"ta_keywords": "russian lexicographic network;lexicographic model;bilinear classification problem;week articles;word;average recall;slava;recall;week;model;weighted average precision;available data;measure values", "pdf_keywords": ""}, "a3cd9c4f8fa52c5e23885c2f82931d7e0f7d4b45": {"ta_keywords": "dimensional barcode symbology;linear barcode;new checking system;dispensing;powder;symbology;items;generic name;data;generic dose;system;number;order parameter;terms", "pdf_keywords": ""}, "697e6eecb0e77ba56c685bb99b221d959739d13b": {"ta_keywords": "latent dirichlet allocation;tagging;flickr archive;automatic geo;separate ld models;single ld model;images;various likelihood metrics;city location;city;approaches;estimate;second approach;approach;first approach;novel", "pdf_keywords": ""}, "e42b3ead5ff04adfa95c87e0180561f0c3ba4af4": {"ta_keywords": "vanilla reinforcement learning;reinforcement learning;action constraints;convex combination weights;benchmark control tasks;convex set;convex combination;vertex weights;hard state;weights;action;vertices;geometric property;systems;algorithm;new approach;examples;approach;paper;variety", "pdf_keywords": "novel vertex policy network;generalized safe reinforcement learning algorithms;discrete time affine control systems;model predictive control;convex polytope;convex polytopes;robust control;policy network architecture;convex optimization problems;safety constraints;convex combination weights;control policies;convex combination;convex combinations;reinforcement learning;convex hull representation;constraints;constraint satisfaction;convex set;convex hull algorithm;convex hull;benchmark control tasks;feasible space;softmax layer;convex inequalities;state safety set;safety;polytopes;polytope;policies"}, "e54a4e49917eb3da18c2f239be70a68fbd3274c3": {"ta_keywords": "technical debt;scientific software;scientific packages;rsci;replication;documentation;reviewers;package;findings;study;authors;relevance;prevalent type", "pdf_keywords": "documentation debt;standard software reviews;usability debt types;technical debt types;technical debt;scientific software;review documentation;technical debts;documentation;scientific packages;underlying software;software products;software;requirements debt;software projects;requirements design debt;software industry;debt;software package;software development;review process;design debt;recurrenttechnical debt;software development process;open source;researchers;rsci package library;large software project;peer;reviewers"}, "59d225fcb08ce66935e0285a9936ee158c4fdb97": {"ta_keywords": "entailment trees;multistep entailment trees;entailment steps;strong language model;explanations;intermediate conclusions;trees;hypothesis;questions;facts;tasks;model;multistep;new approach;approach;interest;form;skill", "pdf_keywords": "entailment trees;entailment tree;generative reasoning;discrete entailment trees;multistep entailment trees;entailments;entailment treewe;systematic explanations;deductive proofs;natural language documents;inference;generative models;semantic structure;natural language;rigorous explanations;explanations;generative model;reasoning;natural language theory;input sentences;sentences;reasonable trees;automatic evaluation;general knowledge;deep explanation;corpus;explanation authors;large corpus;hypothesis;trees"}, "deedb9b61a01d686b28e6034770fccc142e77fab": {"ta_keywords": "natural language processing experiment;meaningful predictions;predictive model;unseen languages;experimental tasks;predictor;experimental task;human experts;features;performance;input;model;reasonable baselines;representative;different modeling architectures;set;small subset", "pdf_keywords": "natural language processing tasks;machine translation task;same machine translation task;machine translation machine;natural language processing;new nlp;translation task;target language;computational linguistics;nlp;nonlinear machine learning tasks;multilingual word embeddings;deep learning;tasks;plausible predictions;model training;prediction;languages;novel regression model;multilingual languages;corpora;target subword;nonlinear prediction;high accuracy;task;recent experimental task;important sub task;theoretical predictions;prediction model;dimensional model training"}, "4cfbd97a5b42695697f70a9f28ee29711f6ca433": {"ta_keywords": "trustworthy predictions;adversarial attacks;similar driving dataset;novel inputs;indoor driving environment;autonomous systems;novel scenarios;training dataset;prediction model;datasets;scenarios;novel image;similar features;aware manner;house;information;task;world;method;experiments;efficacy;paper", "pdf_keywords": ""}, "10e88416035a8a3cbef0e65f8967df650abd0a00": {"ta_keywords": "novel unsupervised word sense disambiguation system", "pdf_keywords": "new unsupervised word sense disambiguation system;supervised word sense disambiguation system;word sense disambiguation;disambiguate word;disambiguation;semantic similarity;word sense inventory;semantic representation;dense word embeddings;russian language;traditional sparse context;dense sense representations;new unsupervised system;sense induction;language;different sense inventories;target word;python programming language;synset;unstructured data;context;unsupervised model;synsets;sentences;words;unstructured unstructured data;sparse;knowledge;dataset;sense"}, "4fffa5245d3972077c83614c2a08a47cb578631e": {"ta_keywords": "speech representation learning;language model;bert;clustering step;continuous inputs;teacher;input;model;novel self;accuracy;approach;ability;advantages", "pdf_keywords": "speech representation learning;speech recognition;discrete speech representations;supervised learning;unsupervised learning;speech systems;unsupervised clustering step;novel unit discovery algorithm;learning;clustering;learning approach outperforms state;audio;deep markov models;learning approach;hidden unit discovery model;machine learning;phonetic phonetic phonetic speech models;speech;standard speech pair;acoustic unit;audio signal;structured noise;learning speed;multiple clustering models;cluster assignments;neural networks;feature extraction;learning rate scales;learning process;predictive coding"}, "520e82c0f35a14ecf78b93de3673bb8b2a3212fc": {"ta_keywords": "timeline;temporal expressions;noisy training data;events;training data;supervised approach;joint inference;extraction;predictions;documents;models;state;art performance;experiments;approach", "pdf_keywords": ""}, "0115d5d37f7cdc7b8d2147c0bb348e714432e899": {"ta_keywords": "channel speech enhancement;telephone audio domain;language identification;domain adaptation;neural network;identification performance;noisy signal;rl;sse;significant improvement;systems;magnitude;paper;experimental results;approach", "pdf_keywords": ""}, "cc2c3df6b09166c54e670d347bfe26dae236ac73": {"ta_keywords": "2d;learning;systems;global aspects;general framework;problem", "pdf_keywords": ""}, "f7979c6690562c5f8bf700e3fd184c4d1df0a54c": {"ta_keywords": "neural entity;resource languages;resource language;entity;baseline systems;total of54 languages;pivot;source;character;information;models;shot scenario;level;accuracy;shot manner;framework;experiments", "pdf_keywords": "crosslingual knowledge linking;neural machine translation system;bilingual lexical resources;neural entity;linguistic entities;crosslingual text;crosslingual name tagging;bilingual lexicalwe;neural machine translation algorithm;disambiguation;machine translation;crosslingual knowledge;entity mention;entity mentions;multilingual multilingual encoders;dicted entity link;wikipedia knowledge bases;linking;crosslingual version;entity;shot entity;resource language pairs;structured knowledge base;entities;single language;target languages;parallel entities;semantic structure;resource languages;lexical data"}, "3b0a1a10d8f7496226635c5c3b8475fcd10d890d": {"ta_keywords": "redundant requests;storage systems;latency performance;redundancy;service times;requests;memoryless;policies;scenarios;primary goals;study", "pdf_keywords": ""}, "f826381aea632791b6007e427a9587c11b239b6a": {"ta_keywords": "dialogue tasks;neural network;exploration efficiency;novel strategies;rich structure;process payoffs;novel method;approach", "pdf_keywords": ""}, "f07a326e21395f025a87b2d77cac7e8ca502f002": {"ta_keywords": "multimodal inference problem;inference framework;medical domain;knowledge;domain;data;structure;representation;novel method;parts;problem;sum", "pdf_keywords": "deep neural models;large corpus;deep learning;nli models;domain knowledge;neural network prediction;deep neural network model;textual inference;sentences;art language understanding models;novel neural network model;neural networks;neural network;hypothesis extraction;medical domain;data augmentation;ground truth prediction;underlying description;dependent description;task;linguistic aspects;entailment;tasks;specialized domainswe;large datasets;metastable domain information;specialized domains;question entailment;modelin;nhnhnhnhnhnhnhnhnh nhnh nhnh"}, "d95aafa571e9cb6795cc28ecf257ead123664e3c": {"ta_keywords": "mrf segmentation potentials;common regularization energies;standard pairwise clustering criteria;regularization functionals;new segmentation model;optimization;average association;continuous solvers;aa;techniques", "pdf_keywords": "kernel clustering;dimensional image clustering;image clustering;optimal segmentation;dimensional image clustering problem;image segmentation;segmentation;elementary pairwise clustering;new segmentation model;pairwise clustering objective;pairwise clustering;new pairwise clustering method;standard pairwise clustering objectives;standard pairwise clustering criteria;clustering;robust segmentation;texture segmentation;pairwise clustering objectives;uniform segmentation;many standard pairwise clustering objectives;clustering constraints;pairwise clustering criteria;pairwise clustering criterion;common pairwise clustering criteria;pairwise clustering problems;standard pairwise clustering criterionwe;many standard pairwise clustering criteria;discrete point clustering;object segmentation;video segmentation"}, "250e4a8f5155f1f9f60b2dee3e8da8024338db4d": {"ta_keywords": "unsupervised clustering;clustering;classification;sentiment target;global similarity;document documents;document;global sense;novel method;means", "pdf_keywords": ""}, "8eda71ecad19cdef6092e76276eba48312ec7063": {"ta_keywords": "dense data;discrete representations;query encoders;time models;models;dynamics;representations;level topics;document;input;quantitative results;different aspects;description;output;quantitative study;work;experiments", "pdf_keywords": "dense retrieval models;query representations;dense retrieval;document representation;natural language processing;dense embeddings;discrete representations;document encoders;deep learning;deep learning track;discrete query;level topics;input tokens;attention;document;query;word importance;diffusion matrix;par;attribution scores;task;topic;input;words;matrix;benchmark datasets;results;paper;output;data"}, "e0c66240239263f16159eef166a391d3939ae2d5": {"ta_keywords": "shortest passage;benchmarks;models;performance;comparative study;sbc;overlap;degree;variety", "pdf_keywords": "question attention layer;comprehension tasks;attention readers;scale reading comprehension dataset;machine reading evaluation;much reading;comprehension task;machine reading problem;value memory networks;natural language processing;comprehension;babi task;random encoders;many popular benchmarks;tasks;entity;text;questions;task;popular benchmarks;best models;randomization;models;encoders;performance;topic;model;intense interest;rapid progress;outcome"}, "3105b5863d4597058bf51aeda40db53394075784": {"ta_keywords": "ultracold atoms;atoms;tournaments;level system;same level;accuracy;new method;generation;target;method;number;use;assumption", "pdf_keywords": ""}, "446efa0bcf3528b51332a12495cb56784dd8bad3": {"ta_keywords": "novel transfer learning framework;unsupervised learning;different embeddings;glove embeddings;elmo embeddings;unary features;graphs;specific lvo;lvo;unit;ability;advantages", "pdf_keywords": "graph predictor encodes task;novel deep transfer learning framework;latent relational graphs;novel transfer learning framework;traditional feature transfer learning framework;hierarchical structured graph representation;graph predictor;latent graphs;deep learning framework;universal graph structures;deep learning;structured context prediction objective;graph structures;natural language tasks;structured features;generic graphs;unsupervised learning;different embeddings;supervised learning framework;graphs;supervised learning;deep learning strategy;unsupervised learning objective;unlabeled data;glove embeddings;natural language representation;downstream tasks;graph;subgraphs;encode information"}, "549dae68d04eefad88885c64a4d946205e524b79": {"ta_keywords": "document clustering;sentiment classification;document data;representations;vectors;invariant mappings;document;analysis;geometry;variety;utility;powerful tool;respect;new class;problems;set", "pdf_keywords": "topological representations;binary sentiment classification;text classification tasks;text datasets;embedding;persistence diagrams;topological signatures;intrinsic similarity;natural language processing;text documents;patterns;similarity;compact metric spaces;stochastic language;art sentence classifiers;discrete representations;word vectors;compact metric space;persistent homology;clustering;representations;discrete patterns;aforementioned datasets;haussdorff distance;documents;document;standard document distances;finite point cloud;clustering behavior;data"}, "35c710f5fdacc71a675832f6beaa2dbfe301d0ce": {"ta_keywords": "dialog systems;active learning framework;system utterances;random selection;inputs;uncertainty;example;complexity;framework;good alternatives;domains;strategies;evaluation results;construction;different domains;paper", "pdf_keywords": ""}, "25efc17ba82ba4af29f2e03868de74e1ea66d025": {"ta_keywords": "contextual multilingual embeddings;new instructional video dataset;video search;multilingual text;english search;vision models;video;models;essential features;model;problem", "pdf_keywords": "contextual multilingual multimodal representations;multilingual video training;multilingual multimodal video frames;multilingual representation learning;multilingual instructional videos;multimodal embeddings;multilingual videowe;multilingual representations;multiple multilingual representations;multimodal representations;underlying machine translation models;multilingual models;multilingual representation;multilingual features;multimodal representation;multilingual transformers;tasksneural machine translation;multilingual text;multilingual language;video search tasks;deep linguistic structure;language models;video search;speed video search tasks;machine translation;large corpus;unsupervised word translation;video training;multilingual version;visual context"}, "e60b88313fad52c1ef8dd02b482785651d09ad66": {"ta_keywords": "neural network;neural networks;separation;networks;weights;sub;method;problem", "pdf_keywords": "hidden neuron;neurons;size depth;small depth;neural networks;approximation;networks;neural network;theorems;neuorons;depth;polynomials;low degree polynomial;separable functions;universal network;network;convolutional neural network;universal quantum computation;weights;unit cell;function;cnn;2dwe;linear optical circuit;wellwe;existence;uniform distribution;linear optical elements;norm;bias term"}, "a1c5af2a531c64f1c06e806d7986cd878ec3c33a": {"ta_keywords": "fraud analysts;popular explainers;explainer;xai test;world fraud detection task;evaluation methodology;model score;model model score;model;application;study;experiment;data", "pdf_keywords": "explainable ai methods;explainability methods;explainability;explanations;different explanation methods;explainers;best explanation method;ai;explainer;best explainer;interpretability;ai systems;evaluation;evaluation methodology;different explanations;simplest explanation methods;artificial intelligence;evaluation practices;evaluation approach;comprehensive evaluation;xai methods;decision task;explanation;xai;interpretation;itinterpretable machine learning;computing environment;methodology;machine learning systems;decision process"}, "4fa4e39ade763085a75146392b997b7d4da49725": {"ta_keywords": "text classification;weak supervision;seed words;classifier;text;datasets;seed;novel framework;framework;terms;use;case studies;number;problem;efficacy", "pdf_keywords": "classical disambiguation system;text classification tasks;seed words;text classification;unstructured text corpora;appropriate seed words;document classification tasks;indicative keywords;weakly supervised setting;corpus;group word occurrences;word occurrences;plain corpus;initial seed words;input corpus;weak supervision;supervised classification;supervised learning;context;unusual words;same seed word;top few words;classification;deep learning;words;classifier;unlabeled documents;unsupervised ones;seed information;text"}, "462e36e5e296900c80dcd36173340f9c29e36c80": {"ta_keywords": "sharedstate triphone hmm;probabilistic probabilistic criterion;tree structure;sst;hmm;criteria;data;probability density function;criterion;method;former criterion;minimum;maximum", "pdf_keywords": ""}, "f05741b65a1d644f2fae4c654dae315a7451ee85": {"ta_keywords": "heterogeneous topic web;such heterogeneous topic web;text network exploration;topic atlas;life text networks;probabilistic generative model;text;links;paper;model;extensive qualitative analyses;task;different relationships;effectiveness;prototype demo system;system", "pdf_keywords": "heterogeneous topic web;topic modeling;topic model;text network exploration;topic models;topic atlas;document networks;standard topic models;topic model selection;academic citation networks;topic database;topic relationships;topic topic connection;hyperlinked documents;word topic equivalence;topic topic equivalence equivalence;topics;topic space;topic equivalence;common topic basis;topic databasewe;hyperlinked webpages;topic coherence;topiceerwe;new text network;large document collections;document level;text network;probabilistic generative model;heterogeneous web"}, "fe54832083f65eade8e2847627d330a24df22488": {"ta_keywords": "channel electroencephalogram;spectral transform;background noise;channel signal;covariance matrices;frequencies;frequency bins;stft;potentials;different spatial spread;trial event;eweg;time;experiments;effectiveness;new method;method", "pdf_keywords": ""}, "d7a7ebd1565c3795bc2bcdec4334d42a65ad17c5": {"ta_keywords": "text generation;language models;text;different input data;several future directions;information perspective;challenges;survey;general task definition;mainstream architectures;special properties", "pdf_keywords": "many text generation tasks;text generation models;text generation task;text generation;neural generation models;robust text generation;natural language processing tasks;generative pretraining tasks;machine translation;language models;natural language;deep learning;text;generative model;semantic models;human language;comprehension tasks;textin;structured input;multilingual machine translation;dialogue systems;unstructured input;unsupervised summarization;generation;decoder model;encoder;learned structure;model toin;input;conversational response"}, "a2221b03211408ac2db0559b9a54c1d72b5f560c": {"ta_keywords": "downstream music annotation tasks;music genre classification;musicoder;tagging tasks;learning approach;art models;auto;new self;approach;performance;paper;results;development;state;focus", "pdf_keywords": "specific downstream music annotation tasks;unsupervised acoustic model training;acoustic encoder;acoustic music representation;acoustic encoders;music auto;music genre classification;massive unlabeled music;acoustic data;input music;acoustic music analysis library;attention transformer encoder;encoder;encoders;continuous acoustic frame domain;acoustic frames;music genres;music;universal music;supervised learning framework;musicoder;neural network;convolutional neural network;supervised learning;taggingin;tagging;supervised learning problem;language processing;layer self;supervised version"}, "74e9053d6f44f4507bd40bbea999ee65f0cbefb2": {"ta_keywords": "nonsensical statements;speech;prediction;words;input;essential features;word;function;method;new method;idea", "pdf_keywords": "natural language classification tasks;natural language processing tasks;natural language processing;text classification;natural language sentences;nlp;neural models;model predictions;more accurate predictions;deep learning;unordered composition predictions;input examples;high entropy outputs;model overconfidence;neural model;deep neural networks;input tasks;nonsensical examples;predictions;prediction;overfitting;unimportant words;random inputs;deep unordered composition approaches;interpretability;human accuracy;examples;unstructured text;text;input reduction"}, "01a21d74fb7414404851872f23cdca42243ab6a8": {"ta_keywords": "batch;dataset;convolutional cell;aggregation;global information;feature;cell;cells;novel network structure;tuning;model fine;bconv;utilization;latent state;perspective;paper;problem", "pdf_keywords": "batchrelated convolutional;convolutional cells;convolutional cell;new deep learning method;batch;subsequent batches;image classification tasks;general image classification tasks;deep learning method;promising image classification task;previous batches;social learning;identification model;person reid;dataset;surveillance;person;convincing datasets;cell;multiple datasets;evolutionary learning network;cells;network;different datasets;training;feature extraction;feature extraction model;tasks;aggregation;novel network structure"}, "c2dd1c332f65fea3a66f4a982428f31ce1a9dc70": {"ta_keywords": "deductive databases;structured information sources;common database representation;knowledge integration;information retrieval;ranked retrieval methods;queries;novel logic;information;integration;inference;multiple web sites;object identiiers;extraction;keys;whirl;novel method;method;style;malization", "pdf_keywords": ""}, "eca07d2b351d81719b33c913a87c63d6930ee7f5": {"ta_keywords": "einstein condensate;component bose;effect", "pdf_keywords": ""}, "3a95fab610d5ff49fbdb7a4d8760b02c51df0013": {"ta_keywords": "private health information;new privacy paradigm;sensitive data;clinical notes;family history;classification tasks;algorithmic approach;secure paradigms;new approach;diverse set;utility", "pdf_keywords": "data anonymization;novel anonymization technique;obfuscation;anonymization;anonymization technique;clinical note documents;sensitive data;sensitive information;new obfuscation method;partial informative clinical text;word embeddings;obfuscation phenomenon;clinical notes;natural language processing;clinical note;text data;privacy;clinical data;corpus;tokens;nlp;clinical narratives;partial informative text;embedding;text;corpora;data holders;random token;new token;semantic relevance"}, "76b95833fd0e242896d231abdea8dc01a167c7a6": {"ta_keywords": "stochastic differential equations;drift function;stochastic;approximate equations;state vector;estimation;nonparametric approach;posterior;simulation;novel approach;systems;approach;class;use", "pdf_keywords": "stochastic differential equations;stochastic differential equation;stochastic process;general stochastic model;drift function;drift functions;wiener process;approximate expectation maximization;sparse approximation;markov processwe;markov processes;markov process;random walk models;probabilistic model;random matrix approach;posterior process;sparse observations;random process;expectation operators;observation noise;empirical representation;sde;path probability density;drift;noisy trajectories;sdes;noisy noisy noisy noisy environment;noisy environment;noisy noisy noisy environments;nonparametric model"}, "a660429b77e932af1c1d7d3f0554f4b17c044082": {"ta_keywords": "similar terror groups;latent clusters;von neumann entropy;entropy;patterns;baseline empirical approach;data;mode;ideology;weighting;structure;novel approach;formation;approach;procedure;application", "pdf_keywords": "global terrorism database;similar terrorist groups;terrorist networks;terrorist groups;terrorist attacks;terror groups;terrorist events;global terrorist scenario;terrorist global scenario;terrorist scenario;latent clusters;social network analysis;latent patterns;complex networks;ideological features;clustering;social networks;hidden similarities;similarity;such networks;scale networks;complex network approach;networks;novel clustering architecture;groups;network structure;attacks;different groups;patterns;data structure"}, "8512718bafa447f9b433da9e809215dfc28b6b28": {"ta_keywords": "performance prediction;performance prediction model;natural language processing;different nlp tasks;nlp;performance;confidence intervals;confidence interval;reliability;system;calibration;feasibility;methods;problem;angles;purpose", "pdf_keywords": "natural language processing tasks;performance prediction task;holistic performance prediction model;performance prediction;novel performance prediction model;performance prediction models;natural language processing;performance prediction model;different performance prediction models;neural machine translation;word representations;nerns tasks;machine translation;learning tasks;nernstrograph systems;nerns task;prediction;classification task;reliability analysis;accuracy;tensor regression;linguistic structure;holistic evaluation method;tensor networks;holistic evaluation setting;nernstrograph;performance;reliability;sentences;nerns"}, "84702b091af8842b6bbe457e5435c343a9824693": {"ta_keywords": "repulsive condensate;repulsive interaction;einstein condensate;condensate;component bose;dynamics;bec;presence", "pdf_keywords": ""}, "54e7209e692ca4f5c85f0e68df34040b3cfa8bad": {"ta_keywords": "coded matrix multiplication;new encoding scheme;mds matrix;matrix;larger sparsity;sparsity level;maximum distance separable;schemes;mds;information;work;fundamental limits", "pdf_keywords": ""}, "1acbfc7d3e245bd3146e9e24eae7550aa2d03482": {"ta_keywords": "stochastic dynamics;stochastic process;neural network;network;models;salient features;class;sequence", "pdf_keywords": "deep linear neural nets;cheap rank preservation operator;deep network;training networks;rank collapse;deep networks;linear networks;vanilla linear networks;relu networkswe;initial rank magnitude;gradient network;deep neural networks;arbitrary input rank;random matrices;hidden layer activations;nn matrix;naturalizable network;simple linear neural network;batch normalization;random initialization;random matrix;rank vanishing problem;random network;novel batch normalization method;networks;rank vanishingwe;neural network;neural networks;hidden representations;hidden layers"}, "796f29cee975603c7a1469df1eb21ed5142ecff5": {"ta_keywords": "literary evidence retrieval;literary documents;dense retrieval;literature;random passage model;passage;quotes;collection;novel approach;field approach;results;problem", "pdf_keywords": "literary evidence retrieval;scholarly excerpts;literary texts;novel retrieval dataset;lexical sources;literary literature;literature;dense retrieval model;literary claims;challenging literary domain;scholarly claims;literary analysis;literary manuscript;literary quotations;english texts;literary claim;tothe empirical corpus;project gutenberg consortium;secondary sources;primary sources;annotated datasets;early texts;humanities researchers;data retrieval;empirical neural model;sentence tokenization;sources;sentences;relativistic retrievers;excerpt"}, "d10e410765699a75628a1437b93f0d0fc3dc0aa6": {"ta_keywords": "unlabeled instances;labels;seed instances;page seeds;large datasets;graph;link structure;method;new method", "pdf_keywords": ""}, "48aa33ad92566cb60ef348ffa438e4712f618b03": {"ta_keywords": "clinical probe;reflectance;transillumination images;lesions;tooth;teeth;swir images;multispectral short wavelength;microct measurements;penetration depth;occlusal surfaces;swir;diagnosis;depth;potential false positives;model;mathematical model", "pdf_keywords": ""}, "3fb78bee6cb39588a1a4cbb4e0abce5e362aa130": {"ta_keywords": "adversarial bandits;regret bounds;mirror descent;regret;horizon;actions;variant;time;note;comparison;number;recent investigation;kt", "pdf_keywords": ""}, "e6ffeb4b9d808d6c9b8d388a7cbb431ac96bf194": {"ta_keywords": "proton;effect;results;study", "pdf_keywords": ""}, "99053e3a708fc27709c9dab33110dc98b187c158": {"ta_keywords": "finance knowledge;gold reasoning programs;financial data;financial reports;large datasets;dataset;large corpus;finqa;models;questions;expert humans;scale dataset;different models;results;novel platform;comprehensive experiments;numerical analysis;aim", "pdf_keywords": "financial valuation tasks;scalable financial valuation models;financial data;financial reports;reasoning programs;financial experts;numerical reasoning programs;nlp program;modeling corpus;financial markets;financial options;static reasoning program;annotated trees;financial analysis;financial companies;large datasets;expertannotated version;benchmark dataset;large datasetsthe;dataset;financialwe;successful reasoning;questions;data;tax positions;gold program;program generators;new datasetwe;quantitative inference;empirical results"}, "0acbdcac9edf74cc2c1e98bd59e301c9300977d0": {"ta_keywords": "informative unlabeled instances;tags;tag tag;weight;collection;framework", "pdf_keywords": ""}, "ee2e171d6a897ee5d0b0bde2d5f2548b52d3a840": {"ta_keywords": "cognitive help;tutoring service;classroom study;students;performance;better performance;effect;context;paper", "pdf_keywords": ""}, "9633928f72cda45d102fb6740291d47137d0a5ca": {"ta_keywords": "federated learning system;backdoor attacks;learning system;learning environment;pruning method;learning;validation;average attack success rate;key features;test accuracy;ability;model;tune;tasks;paper", "pdf_keywords": "federated neuron pruning method;backdoor attack performance;federated learning scenario;global model aggregation attack;backdoor attacks;backdoor attack success rate;pruning neurons;backdoor attack;backdoor attack model;pruning;backdoor data;new attack mechanism;pruning algorithm;50federated learning;backdoor patterns;pruning method;private dataset;backdoor model;pruning process;redundant neurons;pruning sequence;attacks;attack success rate;unnecessary neurons;cm attack;attacker;available benchmarks;attackers;networks;attack"}, "7731e3dec97c48498b585408d44615346ade144a": {"ta_keywords": "language variation;language specificity;social networks;community;rabi frequency approximation;group;variance index;words;variance;senses;context;activity;good measure;bert;sense", "pdf_keywords": "subreddits word definitions;reddit communities;online communities;social media platforms;cluster word clusters;different communities;communities;popular communities;social media;popular subreddits;lexical semantic change;distinct communities;social network;different subreddits;semantic change;communitywe;community;social groups;subreddit subreddits;subredditswe;community members;subreddits subreddits;online language;subreddit compilation;semantic variation;new subreddits;language use;subreddits;subreddit subreddit subredditroz;community size"}, "b116e5044fe047fc48307795af1f3e11b3a9401c": {"ta_keywords": "variational bayes algorithm;bayes;laplace algorithm;model;independent setting;calculation;performance;popular version;approach;degree", "pdf_keywords": ""}, "4b18303edf701e41a288da36f8f1ba129da67eb7": {"ta_keywords": "shot learning;domain adaptation methods;learning;linear layers network;attributes;classes;standard real datasets;weights;features;general framework;top layer;generalisation error;new approach;environment;approaches;relationships;kind;experiments", "pdf_keywords": "zeroshot learning;shot learning method;shot learning approach;shot learning;attribute predictions;random forests;unbalanced training sets;classification datasets;attributes;classification;transfer learning;visual attributes;attribute description;attribute;attribute signatures;real datasets;training set;domain adaptation;classes;large datasets;arbitrary data sets;training;classification accuracy;supervised fashion;features;training error;class;informative vector machine;kernel learning;key features"}, "b29dd2c50da0dc4589eafac58007f6be7e13c501": {"ta_keywords": "indoor environments;3d geometry;camera model;generative statistical model;objects;room;relative dimensions;cabinets;scene;beds;tables;locations;own prior size;parameters;complex;method", "pdf_keywords": ""}, "873b83326ad1f98549beb85bdb130a40a61e1f9b": {"ta_keywords": "uncalibrated scoring functions;new scoring function;scoring function;language model;surface form competition;predictive function;multiple choice datasets;specific task;priori likelihood;consistent gains;option;context;variety;effect;problem", "pdf_keywords": "generative models;shot inference;shot learning strategy;generative model;probability scoring methods;shot model;uncalibrated scoring functions;multiple choice tasks;language models;small language models;shot questions;raw probabilities;shot performance;large text models;surface form competition;models;conditional conditional conditional conditional probability;learned affine transformation;scoring;normalizing log probabilities;language model;probabilities;prediction;tasks;multiple choice datasets;learners;learning process;datasets;specific task;shot"}, "9fe579e54712ba82c4f1c93e46409613f592df16": {"ta_keywords": "orbit interaction;spin;magnetic field;coupling;overall dynamics;particle;motion;effect;system;significant improvement", "pdf_keywords": ""}, "178d51c35c03e3ccaae2409c32a3c2001cefe7eb": {"ta_keywords": "incremental estimation algorithm;new incremental estimation algorithm;posterior refinement;posterior distributions;time evolution system;model parameters;discrete stochastic process;propagation mechanism;algorithm;process;general solution", "pdf_keywords": ""}, "415d4231cab5ddee73e2ed536d033d5c31f24b4a": {"ta_keywords": "open information extraction system;biomedical text;initial ontology;text;ambiguous seeds;seeds;new facts;approach;small number;nell", "pdf_keywords": ""}, "8163c4010fc103343518d49db5974577593972f6": {"ta_keywords": "liar detection;liar partner;deception;automatic detection;detection;accuracy;questions;novel approach;concept;similar question", "pdf_keywords": ""}, "fa5c7406d09af3f06a3a7ead49975e3ee90ed584": {"ta_keywords": "shared autonomy;autonomy;robot;efficiency;lens;process", "pdf_keywords": "autonomy system;human robot;robot;robots;possible robot responses;human language;robot communications;autonomy;tasks;natural language processing;human skills;interaction;human teammate;linguistic strategy;task;planner;human;good motion plans;linguistic processing;communicating;simultaneous knowledge;language;nlp;humans;simple table clearing task;constraints;participants;subject allocation problem;humanwe;same task"}, "9527352b925f9fa36c40966ed755afd22301b0aa": {"ta_keywords": "decision makers;decision;theoretical tools;quality;experimental data;constraints;method;combination;generic system;group;system;impact;set;introduction", "pdf_keywords": ""}, "74c80622b91894efbe4ae9ce1428e4d699b05516": {"ta_keywords": "dual stochastic gradient oracle methods;convex optimization problems;dual oracles;networks;duality gap;large deviations;convergence;rate;methods;probability;new analysis method;communication steps;terms", "pdf_keywords": ""}, "c00e4564ea054c14c83cb564af6c37e47c8ab367": {"ta_keywords": "centralized active tracking;neighboring sensors;single sensor;sensors;trajectory;model model model;model;novel approach;paper;problem;large number", "pdf_keywords": "online iterative state estimation;stochastic control;stochastic process model;state estimation;process estimation;stochastic process;stochastic dynamics;markov chain model;like state estimation;active sensing strategy;active sensor selection algorithm;markov chain monte carlo;unknown transition probability matrix;active sensing;multitimescale stochastic approximation;centralized tracking;transition probability matrix;markov chain;stochastic model;markov chains;stochastic update;online estimation;state estimator;finite state model;parallel markov chains;online algorithm;noisy sensor data;reliable remote estimation;unknown topological point process;expectation maximization"}, "98ef0db84e62aef969629264c9de1f4d0013f3b9": {"ta_keywords": "knowledge composition;knowledge extraction;multiple tasks;knowledge;classifier;task;representations;adapters;nondestructive manner;new method;specific parameters;method;performance;stages", "pdf_keywords": "diverse nlu tasks;natural language tasks;task learning;knowledge composition;multiple source tasks;multiple tasks;several tasks;separate knowledge composition step;new tasks;other tasks;stage transfer learning strategy;knowledge extraction;more tasks;stage transfer learning process;different tasks;tasks;target task;task;sentiment analysis;catastrophic forgetting;training adapters;knowledge;deep learning models;small training datasets;fewer training instances;adaptation;classifier;natural language;machine learning;adapters"}, "43953a051b6518f32fc37734cfc49942baeac5a1": {"ta_keywords": "prosodic parameter differences;utterances;spectral variation;cepstrum;spectral parameter;mel;experimental evaluations;same sentence;effectiveness;method", "pdf_keywords": ""}, "4077c1986f32817801b3082ce8dde514424f71a1": {"ta_keywords": "synset cleaning;synset cleansing;nodes;empirical validation;novel workflow;remove;list;scale;paper", "pdf_keywords": ""}, "2344cca985dd4e2e2519838b2353b5c295e73036": {"ta_keywords": "neural machine comprehension;reasoning types;questions;knowledge;pattern recognition;dataset;sentences;challenge;labels;arc;comprehensive set;original query;definitions;model;distribution", "pdf_keywords": "annotating knowledge corpus;sophisticated annotation interface;annotation system;annotations;novel annotation interface;reasoning labels;annotators;reasoning type labels;question analysis;reasoning types;annotator;natural language;linguistic semantics;semantics;questions;machine comprehension;knowledge;high dimensional science questions;natural language processing;complex science questions;arc challenge;unstructured labeling interface;natural language representations;reasoning;same underlying knowledge;questions consensus;corpus;dataset;novel challenges;challenge"}, "05fb1eea6381ccd21bde53495c7707546aa234c7": {"ta_keywords": "social media messages;novel pattern recognition features;entities;neural network;nodes;entity;emergence;novel approach;depth;novel;form;comprehensive form", "pdf_keywords": ""}, "3132a18a441ab6067066e4d4d85608b058c9ed33": {"ta_keywords": "time series data;autocorrelation statistics;autoencoders;abrupt changes;detection;data;change points;novel loss function;invariant representation;changes;life data sets;time;novel method;use;method;results;large variety", "pdf_keywords": "novel change point detection;free change point detection method;autoregressive breakpoint detection procedure;continuous variable detection;continuous feature extraction;dynamic feature extraction;change points;autoencoders;autoregressive data;time domain;recursive kernel learning framework;autoencoder;invariant features;time series data;real world data;vanilla autoencoder;time series;convolutional learning code;convolutional learning;domain autoencoder;features;finite dimensional data sets;novel feature;noisy data;feature encoding;dissimilarity measure data;benchmark data sets;feature;detection;changes"}, "91e605a125f64207a242693d0dc1c862080f6c27": {"ta_keywords": "automatic phoneme recognition;phoneme lattices;phoneme;important lexical entries;lattices;major language;translation;speech;factors;method", "pdf_keywords": ""}, "b953a582cc79c33054b295c20c1201e8d5bd8243": {"ta_keywords": "student models;automaticallygenerated problem content features;clustering algorithm;knowledge engineering effort;algebra dataset;models;model;approach;experimental results", "pdf_keywords": ""}, "d9d0d908e3f652ee350f4919d4c2ab972ada1ca4": {"ta_keywords": "new coreference annotation framework;new coreference dataset;coreferences;coreference questions;coreference structure;bowl quiz community;text data;bowl community;simple classifier;data;information;complete set", "pdf_keywords": ""}, "f18ec4e0bce2e4d847954c9692959d88ba8a9b66": {"ta_keywords": "novel adaptive filtering algorithm;secure estimation;stochastic process;joint filtering;complex networks;filter;communication system;estimation;vehicle;random variables;accurate estimation;accurate modeling;algorithm;slow;v2x;data;prerequisite;efficacy;paper;amount", "pdf_keywords": ""}, "5e27712db641bc8f16c510292f7fd5440acd563d": {"ta_keywords": "graphical learning features;graphical learning;graphical learning algorithm;relational datasets;base learner;other related instances;features;data;instance;algorithm;predictions;scheme;set;art;state", "pdf_keywords": ""}, "73e868f74376814a4c08eca6ce043fe7c7aefeed": {"ta_keywords": "graphs;page fms;modeling;discrete;popular model;fms;good model;connection", "pdf_keywords": ""}, "f6160c3196288b9e435dc6f86024f56e6b5ab722": {"ta_keywords": "tractable fairness concepts;allocations;computational complexity;social welfare;envy;freeness;utilities;agents;proportionality;sum;item;problem", "pdf_keywords": ""}, "579095d50eab27ace24a1ea0e97af2f70191dc7c": {"ta_keywords": "graphical model;biological tissue;different tissue types;graphical properties;tissue;new cells;simultaneous description;features;proliferation;view;particular interest", "pdf_keywords": ""}, "0c61265a4325df4b97389f92e5e4f5df412f8e97": {"ta_keywords": "partial discharge localization model;partial discharge localization;power transformers;accurate localization;electrical joint method;transcendental error probability;full consideration;model;solution;extensive experiments;problem", "pdf_keywords": ""}, "46a2960e409c39901c1efd07a6adfc5f26e22ee8": {"ta_keywords": "popular email client;typical email client;thunderbird;data mining techniques;clients;permanent additions;case study;results;techniques;deployment;benefits", "pdf_keywords": ""}, "d6a7d2e9f2caf3e8eb615580f4ee8329ff9a271d": {"ta_keywords": "parking;finite capacity queues;curbside parking spot;drivers;dimensional network;travel time;neighborhood;population;mean rate;percentage;model;significant savings", "pdf_keywords": ""}, "4d01d6b445077ad0f1c9d85af93f9ed9239f3c33": {"ta_keywords": "speech tagging;tagging accuracy;tagger;historical texts;historical german;texts;different corpora;method;use;paper;part;ability", "pdf_keywords": ""}, "189e6bb7523733c4e524214b9e6ae92d4ed50dac": {"ta_keywords": "neural sequence taggers;deep hierarchical recurrent networks;tagging;fewer available annotations;microblogs;plentiful annotations;source task;target task;transfer;languages;spread;significant improvement;point;performance;domains;effects;applications;problem", "pdf_keywords": "sequence tagging;deep hierarchical recurrent networks;transfer learning;novel transfer learning framework;thenamed entity recognition;efficient recurrent neural network;different transfer learning architectures;tagging;natural language networks;deep hierarchicalwe;penn treebank;machine learning tasks;fewer available annotations;deep neural networks;rnn;character embeddings;level rnns;source task;transfer framework;text;languages;different languages;transfer;neural network architecture;neural networks;target task;novel learning method;learning;tasks;neural network"}, "58b628792d3eb22a034a871ed3cf373afe591928": {"ta_keywords": "erasure codes;solomon codes;new codes;higher reliability;functional theory;systems;high density;hep;sudarshan;lma;new family;fashion", "pdf_keywords": "erasure codes;storage codes;repairable codes;storage blocks;data storage;standard replication schemes;solomon code;hdfs;hypergraph storage;storage system;replication scheme;suboptimal storage;data storage framework;logarithmic locality;block locality;nonlinear codes;code distance;storage;linear codes;same data storage framework;replication;storage requirements;explicit linear codes;linear codeswe;encoded block;codeswe;hyperarea code;such codes;hydrogen storage;systematic code"}, "58174f5bb9f9815b52a99fa03ec42f2b44f2d550": {"ta_keywords": "automatic set acquisition;large corpus;text documents;lexicons;gnu documents;search;language;independent sets;asia;instances;web;systems;dependent patterns;expansion;new system;recent advances;approach;form", "pdf_keywords": ""}, "2bafaffe45ba66685c87e2d0a598222a9a68ae13": {"ta_keywords": "quantum information processing;progress;survey;field;catalogue;author;work", "pdf_keywords": ""}, "108ec3512cdf2e89ba3067f5b10eaa4a96df9347": {"ta_keywords": "standard transfer vector adaptation scheme;continuous speech recognition;transfer vectors;transfer vector;acoustic modeling;novel adaptation scheme;adaptation scheme;directional statistics;directional data;unit direction vector;novel statistical approach;fine training;probabilistic version;cascades;scaling factor;discrete;class;scheme", "pdf_keywords": ""}, "b575d272036740e03fcf67d64db969557843f629": {"ta_keywords": "hashtags;social media posts;character composition model;statistical mechanics;novel approach;approach;user;use;art methods;problem;state", "pdf_keywords": "associated hashtags;hashtags;social media posts;tweets;embeddings;deep learning;right hashtag;character sequences;deep deep deep learning model;new hashtag;recurrent neural network;embedding;vectorspace representations;tweet;unusual character sequences;deep learning pipeline;character lookup table;character level encoder;representations;softmax output layer;tags;neural architecture;posts;representation;original tags;target tags;novel character;words;models;vocabulary words"}, "d34712c217046ccf8063efe083fbb1e6cbfc0340": {"ta_keywords": "content delivery networks;cdn architecture;higher availability;high availability;cdns;shortest latency path;servers;write load;c2dn;deployment;erasure;new design;design;miss ratio;goal", "pdf_keywords": ""}, "f8e580fcf34ee6da50989bbde685634018cbe224": {"ta_keywords": "advanced dialog state tracking system;5th dialog state challenge;dstc5 challenge;tracker;trackers;attention;slot level performance;topic;combination strategy;score;dstc5;system;rule;test;results;technique;efficacy", "pdf_keywords": ""}, "8a6d2e134b3b2df6291af8e36e126ae55d50649c": {"ta_keywords": "paraphrastic sentence embeddings;sentence embeddings;recurrent network;models;novel approach;supervised settings;art model;underlying structure;transfer;art methods;state;approach", "pdf_keywords": "long short term memory;word embeddings;recurrent networks;term memory recurrent;paraphrastic sentence embeddings;new recurrent network;text transfer learning;sentence compression;sentence model;word sequence;sentences;sentence pairs;embedding;natural language;accurate semantic similarity;natural languagewe;phrase pairs;sentence matching;short words;linguistic composition;neural networks;deep neural networks;paraphrastic sentence;neural network;intrinsic semantics;semantic structure;length vector;sentence;deep unordered composition model;semidefinite programming approximation"}, "3911b13a61a3a57674cc8c70c760f545de8aeea2": {"ta_keywords": "truthful equilibrium;truthful information processing;new truthful payoff measure;payoffs;agents;evaluations;strict equilibrium;scale evaluations;game;same evaluation;mechanism;symmetric equi;terms;new mechanism;particular answer;number", "pdf_keywords": ""}, "f335e2256b31d9458c10c61e60bb8bed9dcaf1d9": {"ta_keywords": "language navigation;training approach;novel training paradigm;limited training data;generalization;guide;unseen environments;path", "pdf_keywords": "visual agent;constrained navigation;language navigation;multidimensional reinforcement learning;navigation;learning paradigm;reinforcement learning;visual environment;learning;natural language instructions;learned policy;common navigation trajectory;novel training paradigm;new learning paradigm;joint instruction training;action selection;level learning approach;single instruction;instruction variants;new training paradigm;joint representation learning;local action generator;agent;baseline agent;action predictor;instruction;multiple instructions;deep learning;greedy agent;unseen environmentswe"}, "5f23482a8c06ca1ae3e4577e3fdd9213884dac85": {"ta_keywords": "probabilistic trees;probabilistic algorithms;probabilistic process;trees;dynamics", "pdf_keywords": ""}, "1e638d235a512cc76d00713639259540342c6fbe": {"ta_keywords": "relation extraction;end relation extraction model;semantic information;parallel processing;classification tasks;data set;information;model;novel method;end", "pdf_keywords": ""}, "c07fdc95bbf533f8709f8e39c069c1e22b73a7dc": {"ta_keywords": "cavity;bearing;dynamics;mode;nc;field;ec;cc;study;results", "pdf_keywords": ""}, "26c65dad79da20aa67df21a6c10e509a964f0841": {"ta_keywords": "unstructured database;database;entity mentions;entity statements;validation system;available database;novel validation system;data;public domain;system;hypothesis;university;urbana;champaign;set;illinois;ability", "pdf_keywords": ""}, "d4d25eaa373087ac80810d79afff863ef1bae3c3": {"ta_keywords": "wireless weight identification system;wheelchair application;wheelchair users;seat activity;seat movements;weight shifts;wisat;resolution interface pressure mat;method;novel method;study", "pdf_keywords": ""}, "a4cd428d196bf041c22592216f15246b98b91915": {"ta_keywords": "dynamics", "pdf_keywords": ""}, "8a94106364576f0aa79dccfb30f0536514408249": {"ta_keywords": "accurate rank learning;linear baseline rank learning;document pairs;learning algorithm;outlying pairs;learning process;algorithm;undesirable eects;performance;problem", "pdf_keywords": ""}, "abcaec70b463ed925c29180437ed581c971952cf": {"ta_keywords": "plural response candidates;adaptive response selection method;single response;user satisfaction;collaborative filtering;example database;appropriate response;experimental results;framework;paper", "pdf_keywords": ""}, "f46a3a5dc70a70292175e6c7ad505b8206cb070c": {"ta_keywords": "speech separation;step speech separation;speech fragments;speech recognition systems;talker;recognition challenge;recognition;vocabulary;challenge;final collection;power;future generation", "pdf_keywords": ""}, "1756376bf7cf0d0a7bec881d663b57907a361ecf": {"ta_keywords": "incremental tree edits;incremental editing;structural edits;consecutive incremental tree transformations;novel edit encoder;structured data;entire editing process;editor;edits;tree;imitation;data;generic model;method;unique benefits", "pdf_keywords": "abstract syntax trees;incremental editing;tree edit;new tree editor;neural editor;tree edit actions;optimal editing strategy;data editing;program editing;structural edits;source code edit datasets;flexible editor;structured data;editing;stochastic editor;editor;edit sequence;different source code edit datasets;new subtrees;multiple edit patterns;program revision;unsupervised editing;subtree;subtrees;tree;new editor;syntax;precision sentences;new edit encoder;natural language processing"}, "5eba3e525056cac6112cf0b13b62d86ba66661d9": {"ta_keywords": "active learning heuristics;active learning approach;syntactic features;distinct sub;training data;different languages;instances;samples;models;empirical study;novel method;method;confusion;new analysis;importance;proper calibration;amount", "pdf_keywords": "output tagsactive learning;novel active learning method;active learning;uncertain training instances;arbitrary machine learning tasks;annotated tokens;powerful linguistic annotations;confusion reduction strategy;representative training instances;oracle annotations;annotations;linguisto annotations;machine learning;annotation;machine learning experiment;semantics;arbitrary words;annotation tool;select word types;machine learning framework;linguistic tasks;tags;output tags;robust translation;auxiliary prediction modules;word types;target language;word type;tagger;standard crosslingual annotations"}, "84bc74d875e748aa0f11ac0c5e3000b16484b053": {"ta_keywords": "instances;training data;classification accuracy;sample submission;task;common patterns;systems;absolute losses;perturbations;number;simple method", "pdf_keywords": "adversarial instances;adversarial attack;adversarial dataset;adversarial examples;simple adversarial attacks;adversarial claims;fever framework;fever;first fever;novel counterintuitive predictions;annotators;counterintuitive predictions;deep learning problems;machine learning;robustness;task;accurate neural networks;robust natural language processing;classification accuracy;models;neural networks;dataset;instances;original task;optimal hypothesis candidates;biases;participants;natural language inference;hypothesis candidates;nlci"}, "ecab8208e5182d4b3b0d6183928e816301d2366d": {"ta_keywords": "stochastic gradient descent;stochastic updates;proper elastic net update;forward backward splitting;dynamic programming algorithm;sgd;time subproblem computation;nonzero features;algorithm;update;fos;bag;previous work", "pdf_keywords": "sparse stochastic gradient models;sparse optimization;elastic net regularization;novel sparse stochastic;stochastic gradient descent;new regularization method;regularization;sparse linear models;gradient descent;elastic net regularizers;stochastic gradient updates;sparse datasets;stochastic gradient optimization problems;stochastic gradient update;sparse structured models;sparse;gradient descent method;forward backward splitting algorithm;backward splitting algorithm;efficient training;new learning algorithm;learning rate;elastic net;backward splitting;lazy updates;fast training;lazy update method;machine learning;stochastic models;nonzero features"}, "02cc92287c6614b6a2aa982007471f16b3450013": {"ta_keywords": "natural language processing;natural language;open problems;problem formulation;actions;plans;dataset;empirical approach;meaningful steps;relation;work", "pdf_keywords": ""}, "ddc502b6c0d08fefe5b77639e4737cd8c7bce25c": {"ta_keywords": "similarity information;meaningful structure;web pages;unwrapper;anwrapper;variety;context;method;time;half;use", "pdf_keywords": ""}, "ab42ad9698386cc15a30a8c7885fa82b260f537b": {"ta_keywords": "parking spot;bayesian markov process;bayesian approach;probabilistic estimate;appropriate location;location;data;method;time;analysis;novel method", "pdf_keywords": ""}, "3abcd0ffc54c3a16c9dc5e5d3ea59eaa43070127": {"ta_keywords": "algorithms;dangerous behavior;undesirable behavior;flexible framework;framework;machine;framework simplifies;experiments;standard machine;problem", "pdf_keywords": "machine learning algorithms;machine learning algorithm;machine learning;machine learning model;fair machine learning;linear machine learning;regression algorithm;classification;data mining framework;reinforcement learning framework;aware algorithms;regression;reinforcement learning policy;learning framework;reinforcement learning;aware algorithm;reinforcement learning problems;reinforcement learning approach;unstructured learning;regression models;behavioral algorithm;reforcement learning;learning;simple reinforcement learning example;batch reinforcement learning;predictive control algorithm;algorithmic fairness;reinforcement learning problem;algorithms;linear regression"}, "37241cdc693b9c2daf49557f18c1ad6a15247239": {"ta_keywords": "new document binarization scheme;degraded color document images;art binarization solutions;thresholding method;segmentation;image;popular niblack;art version;different scales;par;range;solution;state;use", "pdf_keywords": ""}, "f66a17836380c0c79c1b42a9219cf8fde6524287": {"ta_keywords": "occurrence model;corpus;search engine;entity;common source;questions;model;case study;set", "pdf_keywords": ""}, "a817785f0100f3fadc5c1203974d151d5b093310": {"ta_keywords": "parallel programming library;traditional parallel programming library;parallel computer;performance;single node;comparison", "pdf_keywords": ""}, "9336a2ff833d0b4bc914e2282ad04e19d27bc2be": {"ta_keywords": "copper ground grid;flat steel ground grid;ground grid points;flat steel grid;potential difference;ground;potential rise;220kv kv;copper;calculation;significant effect;same amount;results;time;paper", "pdf_keywords": ""}, "4d5f9a0aba65ba6294c543ba5e6108e6d690f133": {"ta_keywords": "extractors;domains;document;domain;comparative user preference studies;robust features;dependent data;classes;conditional distributions;different sections;data;section;algorithms;explicit structure;shape;implicit common structure;changes;presence", "pdf_keywords": ""}, "c69da8266e2f3f67febf22b8f2bf91623346d283": {"ta_keywords": "tweets;vaccination;inauthentic propagation;websites;common patterns;messages;bursty pattern;period february;network;topic;days;structure;majority;june", "pdf_keywords": ""}, "cd9bfa6266cab4bf4b04c82746a5b650f83b57e4": {"ta_keywords": "global minimizer;optimization algorithms;global performance guarantees;recent theoretical results;reasonable time;general classes;problems;classical arguments;problem;overview;list;structure", "pdf_keywords": "convex optimization problems;convex optimization;smooth optimization;smooth optimization problem;gradient optimization problems;nonconvex optimization methods;nonconvex optimization;convex optimization problem;nonconvex optimization problem;convex optimization strategy;nonconvex optimization algorithm;stochastic optimizationin;other optimization problems;convex iterative gradient estimator;free optimization algorithm;standard convex optimization technique;underlying optimization problem;global optimization;global minimizer;minimization type;stochastic nonconvex optimization;unconstrained optimization problem;optimal stochastic gradient method;optimal stochastic gradient;iterative minimization;simple convex constraints;hessian complexity;stochastic convex optimization;linear optimization;composite optimization"}, "d385d8563192569b229bde762fcd4d57ce2b3ee2": {"ta_keywords": "definitions;technical terms;large corpus;language model;domain;essential features;method;set;user", "pdf_keywords": ""}, "63d7e40da7f0d37308b8e97fca4a14a26a6b52ea": {"ta_keywords": "data excellence;data quality;data practices;data cascades;ai;stakes ai;empirical evidence;more robust systems;india;interviews;citizen;opportunities;practitioners;negative impact;us", "pdf_keywords": "poor data practices;data practices;data quality;data quality indicators;data issues;data management;data;data literacy;data sense;data standards;data science;data integrity;data lifecycle;data cascade;data cascades;big data;data disparities;data documentation;data collection practices;representative data;data management systems;way data;structured data cleaning;data metrics;data system;structured data;data collection;generic data;data pipeline;data set"}, "7707af52b3e19bfd3fc07c2be5aed044e5d7953a": {"ta_keywords": "persistent persistent connectivity;persistent persistent persistent connectivity;persistent connectivity;persistent persistent filtration;persistent filtration;connectivity;topological loss functions;deep networks;originals;pair;new method", "pdf_keywords": "topological loss functions;topological classification;topological features;global topological descriptor;lossy topological constraints;dimensional images;topological mask;persistent homology;persistent persistent homology;topological structures;deep networks;deep learning;topology;deep learning pipeline;large scale image;recognition;connectivity;homology;topological correctness;images;road networks;deep learning model;maps;topological discrepancywe;loss yield reconstructions;microscopy scans;aerial images;feature map;homology classes;neuronwe"}, "f74ccbc8988b7f0b847c480d4e8bea3082f4f931": {"ta_keywords": "optimal strategy;player selection;generative models;generative model;search process;search;game;sample complexity;novel approach;model;data;approach;environment;use", "pdf_keywords": ""}, "3f5b7fcb6fc50ba80318ab959f3d63253cd0ef6b": {"ta_keywords": "acoustic event detection;classifier chains;iterative binary detection;multiple binary classifiers;new classifier;probabilistic chain rule;events;event;classification;recurrent unit;aed;method;paper;superior performance;interdependence;same task;conventional methods;experimental results", "pdf_keywords": "acoustic event detection;sound event detection;sound events;acoustic events;acoustic scene classification systems;acoustic activity;recording events;acoustic recordings;speaker diarization;classifier chains;iterative binary detection;event classes;classifier;polyphonic region;recordings;classification;large scale audio signal;coherent phase;gru classifier chain;coherent phasein;multiple events;gru classifier;event;events;phase;probabilistic chain rule;feature extractors;classes;convolutional neural networks;activity"}, "e6beab7c192d7fb04c8bfb0886464fd719cd3421": {"ta_keywords": "model prediction accuracy;machine learning model;unlabeled target data;average thresholded confidence;model confidence;accuracy;source data;threshold;certain threshold;atc;several machine;performance;novel method;deployments;method;efficacy", "pdf_keywords": ""}, "d25c4bf23b4b951f2417e4a8a44574c99608e9d7": {"ta_keywords": "general linear chirplet transform;seismic data;frequency analysis;frequency analysis method;general linear cascade;cascades;spatial distribution;novel time;data;features;time;flexible approach;method;paper", "pdf_keywords": ""}, "4a160efbe80c38cd5eb2f92c7c095b49b113397d": {"ta_keywords": "code generation;natural language queries;new code;retrieval;plugin;functionality;creation;performance;machine;impact;context", "pdf_keywords": "new python code search engine;code retrieval engine;new python code generation;ide code snippets generator;python code snippets;code generation engine;code retrieval;code retrieval plugin;ide code generation framework;code snippet generation;code retrieval results;natural language snippets;code retrieval model;ide code generation;code snippets;python programming tasks;code generation;code snippet library;python language;new python programming task design framework;intelligent code;python programming;natural language programming techniques;python programs;ide developer assistants;code generation model;common code snippets;nl2code ide plugin;pycharm ide;code snippetswe"}, "e7ce1b01d2928514710bba044ac2af758c975d99": {"ta_keywords": "selfish routing game;congestion costs;network congestion;equilibrium quality;equilibrium;uncertainty;overestimation;costs;users;user;variety;user type;multiplicative constant;different level;theoretical results;many types;new model", "pdf_keywords": "selfish routing networks;selfish routing game;selfish routing;user uncertainty;congestion costs;uncertain users;maximum uncertainty;aggregate congestion costs;urban transportation networks;uncertainty;network costs;uncertainties;simple routing;commodity network;dependent uncertainty;equilibrium cost;efficient transportation;uncertainty level;generic multicommodity network;congestion games;uncertain parameters;nash equilibrium;congestion level;congestion;edge cost functions;optimality;infinitesimal users;user traffic network;independent network;generic pricing model"}, "caf40157a7a1d72ae3a6946169c992d8c973b743": {"ta_keywords": "gingival regression;gingival diseases;several epidemiological models;dental problems;early stage diseases;dental implants;disease;popular models;diagnosis;problem;cell division;early stage", "pdf_keywords": ""}, "1b57ffe73ae95f339015c174ec574b59f99ea553": {"ta_keywords": "perturbation maximization method;features;preservation;input feature space;service provider;gradient;provider;search process;search;uncertainties;subset;novel approach;scenarios;presence;approach", "pdf_keywords": ""}, "c6048cd0b1368be0e62633ef723f9d691323102c": {"ta_keywords": "continuous multiscale mixture model;novel sampling method;sampling;conventional sampling;clustering experiments;clustering performance;speaker;model estimation;nonstationary noise;gibbs;average;methods;method", "pdf_keywords": ""}, "40e292d16168fcb8ac87c20682b827ad17a999dd": {"ta_keywords": "user experience index;user experience;mcao;entropic index;framework;use;novel method;machine;combination;method", "pdf_keywords": ""}, "6332d5bb0e6af89471ffc6157e3816c029b3ae83": {"ta_keywords": "dimensional electron gas;electron gas;strong magnetic field;transverse magnetic field;dynamic behavior", "pdf_keywords": ""}, "8b3c0dd95167d4d63161038493a691ee5cdc76b3": {"ta_keywords": "monolingual sentence matching;similarity;convolutional neural networks;knowledge;learning;high performance;competitive manner;novel method;performance;system;model;small hand", "pdf_keywords": "natural language processing;text simplification;automatic text processing;sentence level alignment;large corpus;sentence alignment;natural language;corpus;sentence pairs;novel semantic similarity metric;sentences;wordnet;grammar;natural human languages;human text;texts;standard wikipedia;sentence;similarity induction;phrase;convolutional operation;latent semantic vector representations;wikipedia;similarity;annotated dataset;significant improvement;semantic data;simple ones;empirical encoding;extraction"}, "e4de1009eb7b3524bf7d19bdcebced80035a47cf": {"ta_keywords": "asynchronous neighbor discovery protocol;novel asynchronous group testing scheme;active neighbors;iot;level synchronization;complexity;network;log;scheme;codeword length;frame;assumption;internet;things;set", "pdf_keywords": ""}, "dbb159b288930c6be32c2d5b91373ca1e341e633": {"ta_keywords": "dictionary learning;speech feature;stereo;large scale speech processing;inputs;enhancement;input;signal;output;new approach;mixture;method;domain;advantage;approach;fact", "pdf_keywords": ""}, "2f3ec666ba50c6a9ce74abad6a5127ea38a05bca": {"ta_keywords": "storage nodes;simultaneous storage;storage space;nodes;repair;error detection;erasure;decoding;codes;network;constituent codes;low complexity;simultaneous minimization;operation;correction;framework;simple framework;problem;manner;advantages", "pdf_keywords": "storage networks;storage network;storagea novel framework;erasure codes;storage;storage setup;storage medium;codes framework;repair bandwidths;new generation codes;arbitrary linear block codes;availability;collector node;nodes;node;new codes;codes;code;code framework;network scenarios;reconstruction process;network;network situations;reconstruction;repair;data;digital media;collector;simultaneous construction;deployment process"}, "8f2182846d5d4cfbc216b5e4c00411e021dc4776": {"ta_keywords": "multilayer perceptron;diagnoses;simple empirical data;features;clinical measurements;model;several strong baselines;novel method;targets;hand;step", "pdf_keywords": "recurrent neural networks;recurrent nets;recurrent neural network;sparse clinical data;deep learning;term memory network;deep learning representation;neural networks;neural network;rnn;neural multilayers;clinical time series;term memory;multilayer perceptron;length sequences;homogeneous neural network;memory;learning;convolutional neural networks;novel feature;diagnoses;regularization;early diagnosis codes;sequence;features;late diagnosis codes;strong baselines;diagnosis codes;neurons;hidden layers"}, "bcffee102a99f726ddfe765906babb01b8226269": {"ta_keywords": "particle acceleration;collision;many collision;particles;dynamics;mechanism;common phenomenon;processes;study;key", "pdf_keywords": ""}, "322ef476e90a487c8f9797bece7799b69af9e5c1": {"ta_keywords": "dimensional coded matrix multiplication;matrix multiplication;computation scheme;computing;backup nodes;time performance;performance;product;size;scheme;number;interesting insights;sub;context;minimum number;analysis;regimes", "pdf_keywords": ""}, "822395760906f4940df68aa33925b6bf9123bac2": {"ta_keywords": "data set;data;noise source;analysis;power;method;new method;presence", "pdf_keywords": ""}, "24d28783f6061bd1e91fb60417ac8b3646305a49": {"ta_keywords": "scale information extraction systems;weight blackboard system;declarative control system;architecture;components;feature computation;classification;level tasks;extraction;component;communication", "pdf_keywords": ""}, "c8648d04f52e49167d1a4443a4830709cf3331ff": {"ta_keywords": "optimal strategies;defender;suitable strategy;security;polynomial time;complexity;game;theoretic model;target;deployment;model;problem;context", "pdf_keywords": "optimal strategy;optimal strategies;polynomial programs;game theory;symmetric strategy;linear programs;security games;sum game;twoplayer games;polynomial time;allocations;linear program;multiple players;pure strategies;follower strategy;weighted matching strategy;defender;arbitrary complexity;strategy;polynomialtime constraint generation technique;mixed strategy;games;various potential targets;polynomial size;stackelberg model;player game;optimalwe;target resource;attacker;new strategy"}, "651468a69da74dab716cebbd179a5cbb8e672c14": {"ta_keywords": "reinforcement learning;demonstrations;replay buffer;training;arbitrary setting;algorithm;novel;influence", "pdf_keywords": "imitation learning;expert imitation;suboptimal demonstrations;noisy demonstrations;synthetic demonstrations;useful demonstrations;intrinsic reinforcement learning model;demonstrations;reinforcement learning;reinforcement learning protocol;efficient exploration;reinforcement learning framework;reinforcement learning agent;sparse environments;reinforcement learning context;reinforcement learning problems;sparse environment;reinforcement learning model;reinforcement agent;exploration;demonstrating;demonstrations thatwe;training strategy;arbitrary hyperparameters;prior knowledge;art lfd algorithms;selection;motion;lfd algorithms;robot"}, "1cb7015c0a8015c65844876459809ecac917ec02": {"ta_keywords": "tal speech enhancement;channel am;channel;training pipeline;input;model;complemen;module;power;paper", "pdf_keywords": ""}, "899055ad2f0863cf1931c41f04da8b1dd7382607": {"ta_keywords": "lipid variability;hemoglobin a1c levels;density lipoprotein cholesterol;lipid profiles;cardiovascular disease;elective coronary intervention;variability;ldl;risk factors;retrospective study;pci;patients;relationship", "pdf_keywords": ""}, "173c73077a421680f12576524e85dff4b890c17e": {"ta_keywords": "vernet theorem;distance;projection;distance function;distributions;wheeler;numerical algorithm;linear regime;method;utility;version;sense", "pdf_keywords": "robust wasserstein distance;wasserstein distance;reproducing kernel space;empirical ksp distance;dimensional testing;empirical kwp distance;empirical distributions;nonlinear dimensionality reduction;high dimensional data analysis;deep kernels;dimensional testing problem;dimensional data;distances;synthetic datasetsin;multivariate distributions;synthetic datasets;distance;possible distributions;distributions;empirical verification;real datasets;data space;observed samples;machine learning;projection;dimensionality;sample complexity;kwp distance;large datasets;uncertainty quantification"}, "4ae15dbb068cc962b39dca07d87b22fe5dcd5f6a": {"ta_keywords": "smart grid operations;smart grid;private parameter;new privacy metric;privacy;adversary;electricity;consumers;algorithm;frequency;data;generation;transmission;problem;paper;performance;tradeoff", "pdf_keywords": "smart grid;smart grids;dynamic nonintrusive load monitoring;smart grid operations;new privacy metric;variety ofintrusive load monitoring applications;inferential privacy;forintrusive load monitoring;privacy;energy consumption data;data privacy;smart meters;power consumption parameters;aware load management;energy generation systems;energy consumption model;electric grid;privacy threat;privacy risk;local power plants;direct load control simulations;power consumption;energy density matrix;demand response;overall electricity budget;direct load control;energy efficiency;load load load potential;electricity;network models"}, "513552a56668279d6cd0857a4399fe8a63d92145": {"ta_keywords": "chiral condensate;lorentz force;thermalization;lorentz;phase;effect;presence", "pdf_keywords": ""}, "8113f05360c6483e52b3e261fc9efce671e0aaa6": {"ta_keywords": "unsegmented speaker recordings;recognition components;recognition;diarization;speech;separation;downstream wer;sdr;modular system;stage;pipeline;specific metrics;der;different state;art methods;problem;results;effect", "pdf_keywords": "unsegmented speech recordings;speaker separation;speech recordings;speech processing;joint speech recognition;unsegmented conversations;novel sequential separation system;automatic speech recognition;speech diarization;signal separation;clean utterances;speaker diarization;audio recordings;novel separation scheme;separation;speech sources;downstream pattern recognition;utterances;speech;audio;deep learning;deep network;original mixed recording;recognition;speaker;pattern recognition;deep recurrent neural network;audiowe;deep neural network architecture;diarization"}, "29437d98b9e6f45bef7029f3ce1237b8b284464f": {"ta_keywords": "text generation;hybrid attention;content record;soft templates;new content coverage constraint;weak supervisions;sentence;data;restaurants;copy mechanism;style;automatic adaptions;stronger performance;sports domains;comparison methods;new approach;approach;experiments;range;problem", "pdf_keywords": ""}, "d63edef60d674408819bb015b64b7f42470e151b": {"ta_keywords": "denovo molecule design problem;druglike molecules;target molecules;conditional rnn;novel generative model;pocket descriptor;protein;model;structural information;3d information;3d;descriptor;pocket;generation;performance", "pdf_keywords": "de novo molecule design;protein structures;dimensional structure information;molecular level structure;molecule generation;molecular generation;proteins;druglike molecules;conditional rnn model;target proteins;protein pockets;protein protein;interesting macromolecules;protein;structural information;target molecules;normal rnn model;conditional rnn;molecules;binding;novel generative model;rna;interesting molecules;generative model;generative models;accurate generative model;novel descriptor;egcm descriptor;ligand;neural network"}, "1ddf9d306ae27113f55ea3d4eee12c8441235656": {"ta_keywords": "hybrid josephson junction;josephson junction;mode waveguide;coupling;resonators;parallel translation translation study;hybrid;input;results", "pdf_keywords": ""}, "b03feec6f5b898484fdfdc3cd12f084afbe77036": {"ta_keywords": "outlying document pairs;pairwise preferences;rank;significant performance gains;algorithm;paper;art methods;undesirable effects;effects;state", "pdf_keywords": ""}, "f388c2be45e4415fcb59cf43a3b29463cf7e7940": {"ta_keywords": "natural language processing tasks;journalists;statements;fact;truthfulness;claim;task;available dataset;assessment;construction;challenges;further research;baseline approaches", "pdf_keywords": ""}, "9352dfd127dcfce8013eb350e0229cc72b9bd203": {"ta_keywords": "panchromatic images;attention;resolution;convolutional network;image results;novel method;backbone;ss;fcn;method;mechanism", "pdf_keywords": ""}, "4104d632d1cff0c9314cde344e2b1da06e662c5b": {"ta_keywords": "asr loss;text loss;corpora;differentiable speech;text modalities;speech;extensive results;end;data quantity;impact;consistent gains;performance;approaches;method", "pdf_keywords": ""}, "7e43dad7fbae3a7db47adc6b89c76acbd2fb225f": {"ta_keywords": "instructional video videos;instructional articles;wiki;hierarchical structure;project;website;evaluation;method;strong baselines;other methods;procedures;substantial improvements;design;new method", "pdf_keywords": "wikihow corpus;simple hierarchical knowledge;hierarchical knowledge;natural language processing;natural language processing task;natural language;hierarchical relations;automatic evaluation;natural language theory;nlp;instructional articles;single text goal;procedural knowledge;text goal;instructionalwe;hierarchy;wiki;structured data;sentence embeddings;schemas;knowledge;procedures;crowdsourcing annotations;several strong baselines;complex procedures;tasks;steps;task;texts;other procedures"}, "15a6c3d32ae1daefba3c4b40146de8efdf16ec8d": {"ta_keywords": "harmonic oscillator;energy density;particle;calculation;analysis;method;new method", "pdf_keywords": ""}, "950c2c041db52c416e49fb0945078f6463c501b8": {"ta_keywords": "energy consumption data;energy consumers;aggregated data;data mining;incentives;utility company;iterative algorithm;data;algorithm;design;new approach;approach", "pdf_keywords": "utility learning;utility models;incentive design;incentive design problem;polynomial utility models;utility function;utility functions;energy consumers;reverse stackelberg game;reverse stackelberg game framework;incentive;stackelberg game;incentives;demand response program;utility;level consumption data;consumers;consumption data;consumption;compatible demand response program;utility company;consumer;efficient behavior;energy;iterative optimization problem;highest consumption;power grid;power delivery;concave function;game"}, "f637d061704579531a8b8e03ef6e8331ba117490": {"ta_keywords": "math notationtex;notation tex;notation inline;math notation;math;probability;items;item;problem", "pdf_keywords": ""}, "7626f73c3b013b5b7bf293c1cc22d2835b6579b3": {"ta_keywords": "production;model", "pdf_keywords": ""}, "bf0b66e0e328df1df42b075422c8fecdd95736c0": {"ta_keywords": "student learning tool;supervised learning project;linear algebra project;students;student;test problem;test case;problem;results;paper;choice;shape", "pdf_keywords": ""}, "4ffca5d623950e2396089e7fc1621b4a477436cb": {"ta_keywords": "stylistic control;stylistic structure;text;style;underlying sentence;restaurants;sentence;automatic adaption;high fidelity;model;fidelity;experiments;approach;sports domains;new approach;strength;problem", "pdf_keywords": ""}, "97e033d79b6aebab1927ab9232afa8268e198481": {"ta_keywords": "cache content placement;cache;network codes;allocation;combinatorial problems;optimization;algorithm;video;design;tractable ones;scale;role", "pdf_keywords": ""}, "f5ca46585818771e64ee9449c930748fbee35cba": {"ta_keywords": "reasoning task;explanation structures;explanations;original explanations;accuracy;new system;approach;users;original ones;different scenarios", "pdf_keywords": "inference graphs;defeasible inference task;defeasible inference query;inference graph;defeasible reasoning;explainable natural language processing;reasoning task;explanation structures;incomplete knowledge representationwe;natural language;expressive human feedback;explanation structure;reasoning;explanations;defeasible query;human feedback;models;graph generator;data representation;unstructured learning;reproducibility;nlp;feedback;loop translation platform;knowledge;linguistic level;graphs;fewer inconsistencies;model;language"}, "97db55b196cf0c768644a392a7e6c79d1c65207e": {"ta_keywords": "online speech enhancement algorithm;reverberant speech enhancement task;add algorithm;time time fluctuation theory;current frame;istft;frame;overlap;inverse;neural network;algorithm;ability;power;study;results", "pdf_keywords": "online speech enhancement;monaural speech enhancement task;noisyreverberant speech enhancementwe;deep neural network;deep learning;past frames;neural network;frame;convolution network;deconvolution layer;novel nonlinear nonlinear generative model;noise ratio;prediction;dnn;synthesis window;algorithmic latency;noisy environment;sparse representation;nn layer;nonlinear dynamics;analysis window;inelastic nonlinear dynamical system;global network model;nonlinear system;models;quantitative improvement;input signal;future context;signalwe;sparse nonlinear media"}, "45ce9fce4a4eea9f72688885182aee0c84786fab": {"ta_keywords": "musical data;musical instruments;musical music;novel encoder;encoder;genres;unconstrained waveforms;datasets;training;translation;styles;set;supervision;end;pattern;method", "pdf_keywords": "audio style transfer;musical domains;music synthesis;variational autoencoder;musical domain;music domain;generative audio synthesis experiment;music source;generative style transfer experiment;multiple autoencoder pathways;audio conversions;several classical music domains;audio;encoder;music;musical instruments;novel encoder;cross domain translation;wavenet;deep learning;single encoder;convolutional network architecture;genres;novel latent representation;musical pieces;pitch information;deep learning paradigm;professional musicians;decoder architecture;speaker identity"}, "782a50a48ba5d32839631254285d989bfadfd193": {"ta_keywords": "performance entity representations;entity;finegrained entity types;entity identity;embeddings;learning models;representations;high performance;tasks;performance;vectors;posterior probabilities;novel approach;post;values;box;small number", "pdf_keywords": "entity representations;interpretable entity representations;entity disambiguation;coreference arc prediction;entity mention;knowledge base entities;entity mention spans;natural language processing;entity descriptions;entities;entity;simple entity encoder;embeddings;entity types;different embeddings;domain prediction;linguistic specificity;large type vectors;downstream tasks;mentions;representations;downstream models;memorization;priori knowledge;text;type;bert;knowledge;prior knowledge;tasks"}, "d26683135c70d7b2a61ce5f70fb49b4fa22cf9c4": {"ta_keywords": "conditional mallows models;mallows models;markovian;mallows model;arbitrary ranking distributions;insertion model;pairwise comparison data;approximate samplers;algorithms;many important special cases;new class;approach", "pdf_keywords": "pairwise preference elicitation;preference distributions;preference distribution;arbitrary ranking;pairwise preferences;partial preference profiles;conditional preferences;matching preferences;arbitrary pairwise comparisons;partial rankings;noisy pairwise comparison data;aggregate preferences;popular partial rankings;optimal rankings;partial ranking;ranking model;popular ranking model;ordinal preference;partial preferences;probabilistic models;pairwise comparisons;usual insertion model;ordinal rankings;popular ranking algorithm;realistic preferences;pairwise comparison data;hierarchical insertion model;voter preferences;ranking algorithm;popular popular popular ranking model"}, "205d67dfe0112df846bc4b221fa2665b0434d441": {"ta_keywords": "liquid phase transition;spin;transition;critical value;dynamics", "pdf_keywords": ""}, "7df95dceaba3f4fb45e2b9de29caf7fbce20e25c": {"ta_keywords": "dimensional bose;gaussian noise;einstein condensates;einstein condensate;strong magnetic field;bose;critical exponents;condensates;dynamics;2d;expectation;field;effect;presence;agreement", "pdf_keywords": ""}, "83a2582b94aeaaa97b2f52af8d827d28dc4690bf": {"ta_keywords": "new speech recognition algorithm;speech recognition;quiet quiet pauses;quiet pauses;speech;necessary steps;active individuals;active life;principles", "pdf_keywords": ""}, "930445d9cda71d6ff857e69aa5bb4b1bef7d31e5": {"ta_keywords": "robust reinforcement learning;robust reinforcement learning framework;quantization;reinforcement learning;stationarity;particular interest;approaches;improved results;framework;comprehensive approach;case;context", "pdf_keywords": "reinforcement learning;state bandit models;stochastic bandit feedback problem;sequential decision;reward distributions;bandit algorithm;state learning algorithms;bandit framework;bandit problem;stochastic decision processes;homogeneous reward function;ad;stochastic decision process;sequential observation;online auction;ad display;optimal mean rewards;low dynamic regret bounds;reward;stochastic decision processeswe;stochastic inventory control;decision processes;rewards;state action;dynamic regret;underlying decision process;general stochastic learning problems;stochastic control andwe;optimistic exploration;optimal exploration"}, "d3231772937a2182b2377d028417245c49868dd1": {"ta_keywords": "neural machine translation;neural sequence models;transformer base model;global best model scores;beam search;search errors;exact search;nmt;german test;model errors;first search;large beam;exact inference procedure;depth;combination", "pdf_keywords": "neural machine translation;neural sequence models;powerful machine translation technique;deep neural model;accurate translation results;theneural machine translation;translation results;best translation score;translation process;long sentences;exact beam search algorithm;novel nnt decoder;beam search;global best model scores;empty translation;best translations;best model score;previous nmt experiments;translation;exact inference;best beam score;nmt;models;exact inference method;exact search error;exact inference procedure;memory;global best score;search errors;exact inference scheme"}, "6b2b5d3d9a2ca4bc4fbd81551a62370be2fbff1b": {"ta_keywords": "linearized neural networks;law scalings;neural networks;empirical power;related scaling regimes;models;dataset size;infinite data;smooth data;infinite width limit;model;algorithms;network;model parameters;resolution;feature covariance;power;large range;data;kernel;theory;world data;laws;performance;variance;regime;number;problems;method;existence", "pdf_keywords": "neural scaling laws;deep learning models;universal scaling behavior;training loss scales;deep models;limited scaling;scaling regime;deep neural networks;scaling behavior;deep learning networks;scaling regimes;scaling behaviors;scaling properties;linear neural models;neural models;large dataset resolution;dataset scales;scaling;relu networks;dataset scaling exponent;learning modes;average loss scales;neural language models;scaling framework;feature loss;features scale;asymptotic loss;neural power;arbitrary loss;neural networks"}, "4e1b16fd719354b0a9e92075be66c85d4b95082c": {"ta_keywords": "electron gas;spin dynamics;spin;zeeman field;effect", "pdf_keywords": ""}, "713844009469478141671c53a3b73cd12caf9df0": {"ta_keywords": "zeeman field strength;zeeman field;ep cluster;stability;field strength;field;standard model;framework;interplay;effect;suitable choice", "pdf_keywords": ""}, "810420af4fa5f3ed932724aea5f7b66d3bd592b2": {"ta_keywords": "wide web;web;queries;www;unknown concept;resource directory;real data;new instances;examples;machine;methods;world;list;positive examples;definition;experiments;problem", "pdf_keywords": ""}, "e5e74d312679eae8f2a2943e16f2efebcb5cc50f": {"ta_keywords": "surface wind speed;wind speed;surface;wind;speed;key quantity", "pdf_keywords": ""}, "e35357ac461a669fe7e4b877ee1fad0dfda26303": {"ta_keywords": "shot style transfer context;shot style transfer;shot style transfer problem;several attribute transfer tasks;gender neutralization;corpora;text anonymization;languages;style;code;statements;simplification;results;new model;accuracy;difference", "pdf_keywords": "multilingual style transfer;style transfer;supervised machine translation;effective style transfer systems;meaningful style representations;accurate style representations;style transfer model;text generation;natural language generation task;effective style exemplars;different style transfer systems;style transfer performance;complex style transfer model;new style transfer system;first formality transfer evaluation dataset;style vectors;style classification;effective style classifier;style exemplars;automatic formality evaluation;semantic similarity annotations;semantic similarity annotation;new translation translation model;shot style transfer systems;shot style transfer system;natural language;corpora;high quality oo sentences;multilingual language models;paraphrases"}, "e97c5b206c1f308b821917bc2f584b5f1faad547": {"ta_keywords": "cardinal scores;ranking;empirical bayes;possible estimators;shrinkage framework;method;novel method;variety;plug;proof;applications;paper;concept;problems", "pdf_keywords": "ranking estimator;ranking;rankings;ranking algorithm;new ranking algorithm;partial ranking;cardinal scores;generalized pair rankings;cardinal estimators;total ranking;review scores;ordinal ranking;cardinal score;cardinal estimator;ordinal estimators;ratings;uniformly optimal ordinal estimator;rating;scores;median scores;reviewers;noisy peer review;ordinal estimator;ranks;rank;arbitrary miscalibrations;rank one;optimal estimators;possible estimators;reviewer"}, "2d3fcbaf28e650471b942f221c5fa3c178b1b72a": {"ta_keywords": "accelerated directional search method;unconstraint optimization problem;constrained optimization;constraint;unconstraint;structure;method;context;dimension;paper", "pdf_keywords": ""}, "b345057638e60eee581fea6c7110a98e3b9ebe61": {"ta_keywords": "contrastive topic modeling;text streams;topics;early detection;text documents;events;new document;content;patterns;likelihood ratio;novel;novel method;precise characterization;art methods;time;method", "pdf_keywords": "topic dynamics;online topic modeling;novel contrastive topic model;novel contrastive topic modeling framework;topic model;new topics;undiscovered topics;topics;spatiotemporal free text data;spatial event detection;novel contrastive topic;contrastive topic;semantic scan;semantic scanning;semantic semantic scanning;novel topics;novel semantic scan framework;unstructured text;unstructured text documents;text streams;semantic scanning scanning;semantic retrieval tool;semantic scan framework;semantic documents;topic;world detection tasks;semantic information;keywords;semantic search method;robust online document assignment"}, "2038086c604f1f8841d086cd5cc6052e546ffc24": {"ta_keywords": "spin dynamics;spin;orbit interaction;valve system;orbit;system;effect", "pdf_keywords": ""}, "9850d2b41c6c5be039649d6422306121b760169d": {"ta_keywords": "oblivious update algorithms;data storage schemes;data storage systems;data storage system;nodes;schemes;data;networks;new class;amount", "pdf_keywords": ""}, "71bcdfe5b6be3a0d08ce4bde45acdfd0f738e2f7": {"ta_keywords": "inverse reinforcement learning algorithm;passengers;ride;loss function;gradient;algorithm;observed behavior;sharing;canonical grid world;data;model;decisions;company;examples;technique", "pdf_keywords": ""}, "1843c91e9692484b574ef40961f1d0443a56ddf4": {"ta_keywords": "simple incentive mechanism;equilibrium;strict equilibrium;objective feedback;incomplete feedback;symmetric equilibria;mechanism;agents;constraint;theorem;game;answer", "pdf_keywords": ""}, "4c0a915b9389e6489753a968085ee12833131d0a": {"ta_keywords": "peer review process;systemic challenges;important open problems;oprtihm;community;brief introduction;tutorial;relevant references", "pdf_keywords": ""}, "df53aabeca68a8c0076f7e110f2cc7df7d010e7a": {"ta_keywords": "split;perturbative expansion;separation;numerical study;particles;level system;system;configuration;behaviour", "pdf_keywords": "sound source localization;speech networks;independent speech separation;audio mixture;source splitting;source split;end speech models;source splitting model;separate sources;source mixtures;speech recognition;source mixture;end speech recognition problem;speaker sample;multiple sources;source splitting mechanism;deep models;single source;single speaker;unsegmented speech recordings;deep learning;deep learning models;novel source splitting mechanism;target speaker;speaker;deep neural networks;input mixture;novel deep learning architecture;simultaneous estimation;doa estimation"}, "b03c7ff961822183bab66b2e594415e585d3fd09": {"ta_keywords": "multiple attention heads;attention heads;neural models;memory efficiency;multiple heads;models;greedy algorithms;accuracy improvements;performance;test time;large percentage;practice;potential speed;state;variety;art;obtainable therefrom", "pdf_keywords": "multilayer attention models;multilayer attention multilayers;rank attention model;unsupervised clozestyle attention model;decomposable attention;multiple attention heads;attention heads;attention head;single attention head;attention;attention mechanism;neural machine translation model;many natural language processing;machine translation model;deep learning;neural networks;neural network;natural language inference;head pruning;encoder;predictingwe;best heads;layers;encoders;translation model;learning;models;high accuracy;pruning;important heads"}, "315432474166ff7abbc6351e8ff07fcccbd68458": {"ta_keywords": "low quality misinformation sources;high quality health information websites;low quality url;tweets;high quality health sources;traditional news sources;misinformation;urls;article;prevalence;users;higher rate;overall conversation;small proportion;terms", "pdf_keywords": "social media;social media platforms;misinformation sources;social media sites;other social media platforms;online social media community;fake news virus;informationsocial media platforms;misinformation consumption;retweetssocial media;low quality information sources;social network;news media;tweets;misinformation web pages;twitter;information sharing;daily tweets;information communities;information dissemination;social networks;social bots;media community;information sources;facebook;misinformation;traditional news sources;viral epidemics;low quality links;retweets"}, "92acaf505a9c738e56ed70759e8d0062f3c520d6": {"ta_keywords": "automatic speech recognition;audio segmentation;segmentation;neural network;asr;high accuracy;english dataset show;rtf;low rtf;accuracy;system;baseline;method;reasonable trade;experimental results", "pdf_keywords": "audio segment;automatic speech recognition;segment audio;acoustic feature sequences;speech recognition;audio;unsegmented sequences;whole acoustic feature;neural network;autoregressive model;binomial neural network;segmentation;segmentation part;attention weights;length token sequence;end model;attention;classifier;insertion;new mathematical model;insertion operations;modeling;sequence;segment;amplitude;model;higher accuracy;asr;joint modeling;joint attention"}, "f75d05e759447c2aedb7097728f29f9a520d9bc1": {"ta_keywords": "range language models;range context;sequence benchmark dataset;routing model;models;tokens;predictions;results;small set;state;analysis;art perplexity", "pdf_keywords": "range language models;large language models;natural language transformers;linguistic models;longrange models;attention;longer sequences;range models;local attention head;machine translation;random sequence replacement;range context;distant context;natural language;range;random walks;new words;sentences;random prefix;random walk;random random walks;random sequence;models;high accuracy;random walkswe;such models;distinct subword clusters;random chunk;possible suffixes;random random sequence"}, "1dfa71ecab0c25c5fdd6b2df83a41e944ffa5d58": {"ta_keywords": "word sequence;multinomial distribution;text;occurrence;statistical models;words;nodes;models;description;graph;higher frequencies;behavior;number;class;essential features;small number;fact;case;advantage", "pdf_keywords": ""}, "0c47eb31b2dd76d8dc986173a1d3f00da1c9c74d": {"ta_keywords": "nearest neighbors language model;neighbors language models;inference speed;tableaux;sample representations;nonparametric;lefschetz type;representations;comparable performance;model;novel approach;6x speedup;applications;form;approach;deployment", "pdf_keywords": "language modeling benchmark;adaptive retrieval;machine translation tasks;adaptive retrieval paradigmin;deep nonlinear learning;nearest neighbors model;adaptive retrieval analysis;adaptive retrieval paradigm;extensive monte carlo data;present extensive monte carlo data;inference speed;efficient inference;parametric linear optical models;memorization;language classifier;parametric nonlinear programming model;parametric models;language model checkpoint;nlm;adaptive retrieval system;text generation;nearest neighbors;linguistic features;benchmark;dimensional data mining;adaptation datasetswe;unnecessary retrieval operations;conventional retrieval systems;nlps;training data"}, "83cbe142d445a521aefa11acbd184e176085e7c7": {"ta_keywords": "trusting voters;biased sources;trusting voter;unbiased sources;suspicious voters;mistaken voters;information sources;anomalous updates;information source;voters;candidate;decision making;new data;audience;model;types;classes", "pdf_keywords": "biased voter model;biased information sources;biased communication;voter biases;biased sources;biased agents;trusting voters;voter model;trusting voter;rational voter;unbiased sources;biased media choices;voter behavior;biases;deceptive voters;suspicious voters;bias;rational agents;political polling;political decisions;political systems;information sources;candidate voters;showmotivated reasoning;information gathering;biasedtransitive relationship;simple probabilistic description;voter;probabilistic model;social learning"}, "6a79ff7465d8249d9c8a50fa5f2e0a3e308b436d": {"ta_keywords": "novel unsupervised domain adaptation method;domain adaptation;dense retrieval approach;query generator;retrieval tasks;improved results;pseudo;box state;role;method;scenario;art", "pdf_keywords": "novel unsupervised domain adaptation method;unsupervised domain adaptation;dense retrieval models;new dense retrieval model;dense retrieval tasks;domain adaptation methods;dense retrieval corpus;dense retrieval;sparse retrieval models;query generator;query generation;dense retriever mining;unstructured queries;unsupervised training;dense retrievers;dense dense retrievers;supervised learning training;supervised training;minimal training data;dense passage retrieval;retrievers;passage retrieval;large corpus;supervised learning;target domains;retriever field;supervised learning model;query performance;performance learning modelwe;beir datasets"}, "3e3f55cb25b919c4e8158195fd3ce2f23cfa7723": {"ta_keywords": "novel counterfactual inference framework;language bias;vqa dataset;bias;cp dataset;language;knowledge;data;competitive performance;analysis;methodology;answer;idea", "pdf_keywords": "visual answering;counterfactual inference framework;novel counterfactual inference framework;counterfactual representation;counterfactual inference;counterfactual model;counterfactual samples;counterfactual patterns;visual attention;language bias;recent debiasing;recent debiasing methods;bias;vqa models;generative language;vqa framework;different baseline vqa architectures;quantumqa;novel causality;direct causal effect;natural language approximation;vqa;visual perception;vqa test sets;vqa works;vlq;context;causal effect;vlq framework;joint vlq framework"}, "7b29f45df975ed1e4c3864b6ab4483f11086aa76": {"ta_keywords": "word embeddings;data induction;such embeddings;cases;context;points;favorable setting;utility;gains;problem", "pdf_keywords": "multilingual neural machine translation;neural machine translation;word embeddings;multilingual translation system;translation task;embeddings;multilingual tasks;such embeddings;multilingual translation;sequence tagging;high quality words;sequence labeling;layer encoder;independent embeddings;decoder;parallel neural machines;pretraining;neural network models;translation;training data size;bilingual scenarios;single source language;multilingual version;representations;text classification;multiple languages;pre;sentences;high accuracy gains;words"}, "7a684045afae2ccf40338ff07b8fa429bad93a57": {"ta_keywords": "novel scientific insights;discovery;scientific content;mining framework;novel insights;novel features;pages;novel;data;framework;accelerates;ability", "pdf_keywords": ""}, "fc5d79301a0876201c95954a764ec374b8eb236e": {"ta_keywords": "domain target sentences;neural machine translation;domain sentences;domain corpus;translation;adaptation;adaptation performance;unadapted models;nmt;substantial improvements;pseudo;novel method;efficacy;method", "pdf_keywords": "neural machine translation;neural machine translation models;neural machine translation model;parallel translation model;translation model;translation accuracy;translation lexicons;translation strategy;parallel translations;monolingual corpus;domain corpus;domain source words;parallel corpus;large corpus;monolingual source sentence;domain lexicons;translation;parallel adaptation;monolingual source;monolingual data;unadapted sentences;domain words;unsupervised adaptation method;domain language;corpus;lexicon induction;target sentences;adaptation;conventional lexicon;parallel sentences"}, "75abecb4568366d89e89c3c9d39574b9c1c028a5": {"ta_keywords": "document retrieval;navigational performance;best document;document;navigational mechanism;novel navigational mechanism;original document;informative portion;form;later use;constraints;original one", "pdf_keywords": ""}, "ec9367ab933a142124eecd3232fe2d933d93a144": {"ta_keywords": "dimensional bose gas;harmonic trap;bose gas;harmonic field;dynamics;steady state;condensate;dependent density;states;interaction;system;time;result", "pdf_keywords": ""}, "51c2321244b0a489970e1b52c59b049fdcc5cd46": {"ta_keywords": "machine translation accuracy;machine translation;clqa;accuracy;manual analysis;data;words;frequency;result;factors;paper;relationship", "pdf_keywords": ""}, "bb6c2a64ecb6e4c9f3f5720d53cca76a2c37505d": {"ta_keywords": "contextual language understanding;contextual information;framing;representations;brief history;roadmap;possible progression", "pdf_keywords": "large text corpora;natural language representations;neural language models;natural language models;large context;deep representations;natural language;language learning;natural language descriptions;natural language processing;nlp;contextualization;multilingual data;deep learning;natural language interaction;linguistic semantics;language use contexts;semantic structure;language domain;contexts;semantic aspects;shorter representations;unsupervised representations;semantics;natural words;language;supervised tasks;includingneural networks;world scope;object representation"}, "6b98bef930182a848c027dece1bfb58ca706449d": {"ta_keywords": "pronunciation information;speech recognition system;pronunciation;segmentation;word;text;ppa;accuracy;novel method;performance;method;high degree", "pdf_keywords": "hybrid speech recognition;pronunciation dictionary;subword model;pronunciation information;speech recognition tasks;new subword model;many speech systems;pronunciations;pronunciation;dictionary model;tokenized text corpus;speech translation;letter sequences;word unitsin;utterance;words;speech;word;model;neural networks;fast align;signal processing;hybrid approach;modular learning approach;neural network;hybrid approachwe;accuracy;text;lists;terms"}, "8d7db1b1290e5d6f802e9f1075ef197cb55d754f": {"ta_keywords": "front recognition;front models;front model;wave;front;early warning;lens;novel approach;approach;problem;area;large class;equivalence", "pdf_keywords": ""}, "acbf4f9a4457cf2884e6018e4653519beef2833a": {"ta_keywords": "guinea pig cytomegalovirus;human congenital cytomegalovirus;epithelial cell infection;toalanine alteration;gpcmv;infection;pentameric complex;mutations;amino;macrophage;primary cell type;cell types;peclide;effects;homologue;regulation;model", "pdf_keywords": ""}, "790d3503fa95ec32f04c280bd9a52fef6bf1e874": {"ta_keywords": "finite differencing methods;finite differencing schemes;traffic flow;numerical simulations;upwind;macroscopic models;accuracy;friedmans;validation;paper;work", "pdf_keywords": ""}, "0fcfa0ef253a81c103854e1dc123d90e7310a0e1": {"ta_keywords": "private deep learning;deep learning;training models;model performance;dsg;popular method;other approaches;significant impact;lower impact;impact", "pdf_keywords": "private deep learning;private deep learning algorithm;differential privacy;private dgg algorithms;privacy;large scale machine learning problems;deep learning;deep learning data;privacy preservation;differential data;model accuracy tests;machine learning experiment;fairness guarantee;model utility;differentiallywe;suitable dualization;dgg;fairness lens;model;different classes;teachers;accuracy;data;utility;pate;dp;majority;disparate impact;different patterns;powerful method"}, "634bbe75c34b82e664f1e9f083314b5bdb6ba187": {"ta_keywords": "eye blinks;classical attention task;ocular artifacts;probabilistic generative model;event signal;target signal;prior information;signal;contamination;data;multichannel;experimental evaluation;novel method;model parameters;method;results;excess", "pdf_keywords": ""}, "79a6f290cfe8652575e7bb65cfed519bca8f3bd3": {"ta_keywords": "dimensional lattice gas;gaussian noise;lattice;harmonic potential;dynamics;presence;case;effect;factor", "pdf_keywords": ""}, "9cda754187545c3cc8f9e9f134c08707269d0fae": {"ta_keywords": "encoder;single encoder;bert objective;encoders;speech;unlabeled text;language;downstream performance;advantages;novel approach;same task;combination", "pdf_keywords": "speech translation tasks;unannotated speech data;multimodal speech;unannotated speech;speech encoder;speech translation;speech data;unannotated text data;speech normalization;text models;text self;deep encoder;deep learning;joint speech;training models;speech recognition;large scale machine learning models;text representations;quality speech;text encoders;text encoder;speech;large scale tasks;learning;text;text data;deep interactions;encoder;unsupervised learning;natural language systems"}, "b50d03ecd9f2055b32451e3c04138a0da07b0f69": {"ta_keywords": "meeting monitoring;meeting;speech recognition system;monitoring;timelatency threshold;participants;thelatency threshold;wireless sensor;system;onset;approach;experimental results", "pdf_keywords": ""}, "2bd54adb3b5588281396a4b5dae7db09496b2c61": {"ta_keywords": "proton interaction;lhc energies;proton;perturbative qcd approach;nucleus cross section;nucleus;cross section;calculation;description;different values;available experimental data;results;analysis", "pdf_keywords": "language model predictions;language model checkers;language semantics;natural language processing;multilingual machine reading comprehension model;word spelling problem model;lexical aspects;sentences;language;machine learning models;lexical overlap;unstructured text;comprehension task;questions;word spelling problems;neural networks;neural neural neural qa model;automatic way;text;model predictions;additional questions;document;spelling problems;answer length;accuracy;knowledge;russian language;paragraph;reading process;spelling"}, "5e657bc8097c12649d027ca3c16ff7d37df1354d": {"ta_keywords": "machine translation settings;test languages;heuristic baselines;weight training data;data scorer;languages;performance;sets;experiments;method", "pdf_keywords": "multilingual machine translation model;multilingual translation;different multilingual optimization objectives;multilingual training;multilingual data;translation methods;multilingual data usage;different multilingual objectives;translation pipeline;multiple languages;multilingual systems;different languages;many languages;task translation system;language scorer;learned language score;diverse language group;different translation directions;suitable linguistic objectives;machine languages;languages;linguistic models;translation;language groups;language group;training data;base language group;differentiable data selectionwe;best model performance;differentiable data selection"}, "302ae0d991d62dee82b63530b487a50469810af4": {"ta_keywords": "interpretable spatial operations;spatial operations;complex spatial actions;rich natural language descriptions;3d blocks world;new neural architecture;new dataset;competitive results;problem", "pdf_keywords": "language representation framework;natural language;annotators;richer natural language;interpretable language;new interpretable model;machine learning tasks;interpretable internal representation;annotator;semantics;realistic representations;spatial reasoning;deep learning;language;interpretable models;natural context;linguistic properties;new corpus;representations;natural scenes;new dataset;artificial intelligence;physical world;models;corpus;sentences;language encoding;patterns;words;convolutional model"}, "a73d83e50b5687455336a2adce32a069c77ba163": {"ta_keywords": "heisenberg antiferromagnet;antiferromagnetic heisenberg;spin dynamics;antiferromagnetic order parameter;spin;magnetic field;flop model;field;effect;strength;context", "pdf_keywords": ""}, "13608821aa3b369526221182dfbd3a8842549652": {"ta_keywords": "optimal power allocation;relays;wireless networks;placement policy;total cost decision process;network;deployment;distance;numerical exploration;value function;sink;performance;number;problem;source;insights;results", "pdf_keywords": "wireless relay networks;relay channel;wireless relay network;relay nodes;wireless relays;complete relaying scheme;relay deployment;relay assignment;relay placement;relay network;wireless relay position;loss model iswireless relay networks;optimal power allocation;relay design;optimum power allocation;relay node;relay locations;relay;wireless networks;relay price;relay position;next relay;relays;optimal power split;relay gain;wireless network;relay pair;relays offline;communication network;optimal distance"}, "81e684d01bbfb1f4143bb2ffea36cc4791f0530c": {"ta_keywords": "reverberant automatic speech recognition;reverberation time estimation;channel dereverberation method;feature transformation techniques;art dereverberation techniques;asr;art feature transformation techniques;multichannel;system combination;system;combination;novel approach;approach;task;paper;efficacy;experiments", "pdf_keywords": ""}, "14dddd1d8cb2e8c5f4e9998fef84e715cb321ac9": {"ta_keywords": "interpersonal trust;trust;contractual trust;sociologists;artificial intelligence model;formalization;vulnerability;model;decisions;people;user;ability;key properties;impact", "pdf_keywords": "ai trust;human trust;interpersonal trust;trustworthiness;trust;ai;extrinsic trust;warranted trust;artificial intelligence;trustor;unwarranted trust;intrinsic reasoning;formalization;formal perspective;automation;sociologists;semantics;natural language processing interpretability;social processes;inherent inherent inherentity;interaction;formal definition;sociology;social science;natural language;unified description;logic;relationships;model capability;methodology"}, "941e42ee75fc2bf07078bcfbd14bdf9ca7fe99ff": {"ta_keywords": "language models;threatened language;language;cross;accuracy;use;results;terms;method;magnitude;order", "pdf_keywords": ""}, "981152cb3f1e11c9cee6af2275b57ef79c621934": {"ta_keywords": "text generation;human text;neural network;inverse probability;repetitive candidates;samples;probability;diversity;higher probability;close resemblance;novel method;algorithm;results;method", "pdf_keywords": "neural text generation;word distribution;repetitive sentences;repetition;less repetition;repetitive candidates;simple stochastic sampling method;stochastic sampling;language models;repetition loops;large vocabulary;dynamic vocabulary filtering;traditional stochastic sampling methods;sample;fluent samples;samples;texts;text degeneration issue;text;vocabulary;unsupervised learning;probable distribution;distribution;small text;high probability;probability distribution;extreme distribution;high diversity;samplewe;inverse probability weighting"}, "1ea337ac24503d9da8dd9bbf98aac0bfd5920834": {"ta_keywords": "speech results;clean transcripts;faithful transcripts;speech recognition results;human stenographers;lecture speech;style transformation;common corrections;corrections;sst;evaluation;human consumption;types;method;detailed analysis;model;paper;framework;majority", "pdf_keywords": ""}, "b8b5b95a0471e0553a0e6cd5086f384cf0f4d4d8": {"ta_keywords": "speech translation system;paralinguistic information;translation process;target language;translation module;emphasis sequence;speech;utterance;emphasis value;emphasis level;regression;word;steps;sequence;paper;values;method", "pdf_keywords": ""}, "0f2ea810c16275dc74e880296e20dbd83b1bae1c": {"ta_keywords": "random matrix;list;distribution;data;computer program;novel attention mechanism;query;document;paper;method;form;problem;primary goal", "pdf_keywords": "recurrent neural network document reader;query attention mechanism;query representations;generative attentive reader;comprehension tasks;type attentions;attention;contextual embeddings;contextual representations;semantic representations;machine reading;specific attentions;accurate answer selection;novel attention mechanism;attention mechanism;reader;final attention;structured corpus;queries;relational learning problem;massive corpus;reader1;ga reader;query;text;memory;neural network;document;document tokens;corpus"}, "e58edbeb41f3d2d24832e6e3abb94baac754e3f7": {"ta_keywords": "automatic text summarization metrics;automatic evaluation metrics;extractive evaluation settings;evaluation metrics;summary;most datasets;popular datasets;reliability;level evaluation settings", "pdf_keywords": "abstractive summarization systems;extractive summarization system;multiple text summarization datasets;text summarization;extractive summaries;summarization;abstractive summaries;evaluation metrics;automatic metrics;summaries;different automatic metrics;semantic matching metrics;most metrics;diverse metrics;text embeddings;natural language representations;different metrics;linguistic content;large benchmark;metrics;unstructured text;traditional metrics;text generation;recent cnndm evaluation;current metrics;intrinsic similarity;similarity;evaluation;summary;mean semantic unit"}, "0180c56bfbfb21243f8605e4c6f6aab2779d3ef0": {"ta_keywords": "natural language explanation system;natural language explanations;specific explanation system;explanation system;ap decision process;explanation logic;mdp;real time;mdp policy;optimal action;incremental upgrades;system;robust system;specific data;user;novel system;domain", "pdf_keywords": ""}, "197fcdfe05d0892ee7b4a98ef6fa74dfbcd14b48": {"ta_keywords": "crowdsourcing inference problem;inference;joint inference;inference problem;axioms;convexity;natural assumptions;objective function;worker;absence;set;abilities;other hand", "pdf_keywords": "crowdsourcing tasks;convex inference;reliable crowdsourcing;crowdsourcing;crowdsourcing setting;aaccurate crowdsourcing;scale crowdsourcing platform;human computation;estimatingtheorem theorem theorem theorem;convexity;crowd information;theoremswe;convex optimization approach;reasonable objective functions;inference problem;novel probabilistic model;algorithms;theorems;models guaranteeingwe;bias;participants;optimization;probabilistic versionwe;convex subset;workers;model spammers;information retrieval;axioms;objective functions;axiomatic approach"}, "18c00a9b1e6fde799ec5100cf0b1f37c306d061f": {"ta_keywords": "web search;database;data;web;information;case studies;particular topic;hypothesis;use;method;first case studies;approach;ii;way", "pdf_keywords": ""}, "5c283474bbb4838160410e24d33ce89ebaf32c07": {"ta_keywords": "speaker;multiscale;multiscale version;variational method;maximum likelihood estimator;optimization methods;probabilistic version;comparative study;standard model;comparison;different approaches;difference;impact;available data;literature;predictions;results", "pdf_keywords": ""}, "2acc25a01a7ab7cd6b1a75d534ad29ea7d26f92d": {"ta_keywords": "subtopic retrieval;query likelihood relevance ranking;baseline relevance;query topic;maximal marginal relevance;many different subtopics;statistical language models;documents;tre;mmr;mixture model;several methods;data;strategy", "pdf_keywords": ""}, "0d3baef146655c5727452ccc0dd680d21d92ae4e": {"ta_keywords": "model consumer heterogeneity;gaussian distribution approach;hierarchical structure;hierarchical version;model;following characteristics;approach;paper;novel approach;kind", "pdf_keywords": ""}, "0735fb79bf34698c1df4461a05ed51c232c412e4": {"ta_keywords": "encoder;transformer;programming language;languages;specific attention head;histograms;computational model;task;examples;model;problems;form", "pdf_keywords": "neural transformer;programming language;random variable generator;rnns;encoder;efficient computation;nonlinear rnns;transformers;program;random sequence approximation;transformer;transformer architecture;recurrent neural networks;representation;programs;task shuffle;random variable selection operator;aggregate operations;recurrent neural network;natural language processing;simple operations;random access method;aggregations;compositional tool;random variables;elementary representation;nonlinear recurrent neural networks;computation;random numbers;random variable"}, "6fea118a29d78340ae26c465ff06e80e55efbe3b": {"ta_keywords": "incremental reading model;unidirectional language models;language models;text;model;answer;new approach;question;problem;effects", "pdf_keywords": "text comprehension;continual learning;reading comprehension problem;incremental models;learning model;comprehension;questions;next slice;text;sliced prediction layer;slice;input;models;answer;first slice;information;model;context;docqa;global prediction layer;concept;question;goals;new framework;goal;early stopping strategy;humans;squad;architecture;system"}, "92731a953ad063eab1bc90dc541fb956f147a6ba": {"ta_keywords": "preference processing system;dining;preference;table;processing;such processing;graphical representation;process;experience;strong motivation;use;study;problem", "pdf_keywords": ""}, "0791fe161d947d1e4d3af279b261155b88bc9ddf": {"ta_keywords": "temporal clustering;deep neural networks;time series;partial differential equations;sequences;sequence;partial differential equation;invariance;cell;novel approach;proc;initial condition;outcome", "pdf_keywords": "temporal clustering;temporal clustering invariance;temporal clustering framework;deterministic temporal coarsening;irregular time series;data augmentation;clustering;new time series;time series;group convolutions;simulated data augmentation;cluster data;clinical time series;cluster;timestamps;large convolutional neuralin;convolutional sub;cnn;clusters;gaussian clustering approach;convolutional learning model;training data;convolutional neural network;approximate invariance;deep convolutional neural networks;novel ensemble learning approach;multiresolution ensembles;cluster members;extra feature;novel encoding"}, "8d35230fec724398bed3f5939e9fa6a94f55a785": {"ta_keywords": "differential privacy;private datasets;private algorithms;private machine;public data;machine learning;loss functions;algorithms;theoretical results;utility;open questions;cost;samples;nondifferenti;number;upper bounds;interplay;comprehensive study", "pdf_keywords": "private machine learning algorithms;private machine learning;differential privacy;private dataset;private learning;private algorithms;private algorithm;private data;private data processing;private learning mechanisms;private clustering;privacy;private data release mechanisms;private data release mechanism;private mechanism;private kernel;private model;private mechanisms;privacy budget;private machine;unsupervised machine learning;machine learning algorithms;exclusive learning;private versions;kernel learning problems;machine learning tasks;machine learning models;machine learning;learning algorithm;generalization"}, "6eae6230ae277b6915706ec05241c8db6b9fab86": {"ta_keywords": "new similarity search library;similarity search;search;library;competitive baselines;library development;design issues;variety;preliminary experimental results;new method", "pdf_keywords": ""}, "8872e32284467fcbeadd1edd2f11aff077de4ccf": {"ta_keywords": "new rule;algorithm irep;rule;benchmark problems;improved error rates;algorithm;irep;large collection;several modicientications", "pdf_keywords": ""}, "47442ea4c28d631a9d46a9c23454684b834e49ea": {"ta_keywords": "essential distributional semantics;unsupervised neural networks;corpus;semantics;new dataset;prior knowledge;use;novel approach;approach", "pdf_keywords": ""}, "df873bde0b44e543634d109a7a8b1ba7dfaa8187": {"ta_keywords": "topic models;latent ontological structure;input corpus;corpus;unsupervised model;stochastic block models;learned structure;structure;model;facts;mixture;case study;accuracy", "pdf_keywords": ""}, "22dd93fe1a0b8e9cb83eaff6e2ecca0cd6693294": {"ta_keywords": "voice activity detection;connectionist temporal classification;automatic speech recognition;online speech;ctc;vad;end;segments;pid;architecture;low threshold value;interest;novel approach;novel method;approach;method", "pdf_keywords": "voice activity detection;novel voice activity detection method;online speech recognition;automatic speech recognition;voice segments;online speech interface;offline recognition;sequence sequence speech signal;connectionist temporal classification;short audio recordings;long audio recordings;audio recordings;lower minimum blank duration threshold;audio recording;attention model;acoustic signal;recognition;hybrid attention;speech;frame prediction;softmax output;decoder model;encoder;heuristic thresholding;neural network model;path decoding;background signal;ctc framework;spontaneous talks;online sequence"}, "9d332ad27bfce66ee725b413aa07bd93c355efdf": {"ta_keywords": "natural language processing;novel data augmentation techniques;popular natural language models;filters;transformations;robustness;creation;framework;manner;literature;efficacy", "pdf_keywords": "critical noise amplitude;coupled pulse sequences;spin;critical noise;simultaneous synchronization;orbit coupling;phase information;computational linguistics;orbit coupling strengths;pulse sequences;phase;natural language models;level machine translation;orbit interaction;sequencewe;machine translation;natural language tasks;linguistic structure;signal generation;noisy data;linguistic structures;test machine translation models;nonlinear interaction models;reference sequencewe;large corpus;testing machine translation models;popular natural language models;quadratic chirp generation models;comprehension systems;sentence compression"}, "8057a5e7bcb0be7059a6e632124bc861b533c794": {"ta_keywords": "4th chime challenge;channel track task;chime4;neural network;pattern recognition;acoustic model;forward;adaptation strategy;nonlinear dynamics;fnm;system;development;available data;robustness", "pdf_keywords": ""}, "88167f36dced91c279162d68af7225f2b4e2091c": {"ta_keywords": "human language data;language model;intrinsic data;models;data;data sets;intrinsic nature;downstream performance;performance;model;same data;class;effect", "pdf_keywords": "downstream natural language tasks;transformer language models;language models;transformer language model;dimensional natural languagewe;deep transformer models;human language corpora;deep bidirectional protein representations;natural language processing;performance machine learning models;deep bidirectional transformers;deep bidirectional protein representation;unstructured structured corpora;natural language;training data;corpora;several corpora;models;downstream tasks;generalized gradient approximation;training;downstreamwe;transformer;unstructured data;intrinsic structure;better downstream performance;tasks;lsp;certain features;topological insulators"}, "f752bf6f8c1502b8cb58aa1483ef598f9fc0d44c": {"ta_keywords": "random matrix elements;arbitrary net;nodes;nets;rank;complete set;number;method", "pdf_keywords": ""}, "5ebe542ee1a7eab7aad8e36ed53dbdd7ebd98c8d": {"ta_keywords": "neural network architecture;neural network;high accuracy;novel method;method;paper", "pdf_keywords": ""}, "73484141ca58d9714ac592e3667de416322b51eb": {"ta_keywords": "wavelet transform;iterative linear operations;actual image;reconstruction;new representation;conventional method;representation;dimensional case;method;lower computional complexity;paper;convergence;theoretical guarantee", "pdf_keywords": ""}, "29263fa3632951be0ca617988d7c9ce651e74393": {"ta_keywords": "multilingual training;multilingual model;machine translation systems;resource languages;encoder;encoders;decoders;performance;lrls;different varieties;effects", "pdf_keywords": "multilingual machine translation models;multilingual translation models;multilingual neural machine translation;multilingual machine translation;multilingual encoders;multilingual models;multilingual training;machine translation model;multilingual nmt models;neural machine translation model;multilingual training benefits;different multilingual training settings;optimal translation strategy;machine translation;smaller multilingual data;multilingual data;multilingual setting;different target languages;language pairs;language pair;linguistic adaptation;decoder performance;powerful language processingthis paper;betweencomputational linguistics;languages;related language clusters;language clusters;language;encoder;decoder"}, "8b1be80cc1fabcd9ccea76d9a8830e2b07e71f0c": {"ta_keywords": "optimal raspberries;optimal packing;metamaterial analysis;optimal shape;optimal vibration;raspberries;optimal detuning;bouquet;detuning;combination;action", "pdf_keywords": ""}, "76b36a059c0d8d66a1bf910de32b34dba19482fa": {"ta_keywords": "encoder;decoder;latency reduction;endpoint prediction;high recognition accuracy;novel blockwise;algorithm;ms;evaluations;efficiency;90th percentile;several tasks", "pdf_keywords": "endpoint encoder;endpoint prediction search;encryption system;hybrid beam search algorithm;decoder;endpoint prediction;encoder;endpoint endpoint prediction;beam search;end speech recognition;encoded features;continuous block synchronization;nonlinear network decoder;back stitch search;endpoints;endpoint;amplitude search;novel algorithm;amplitude amplitude search;amplitude amplitude amplitude search;synchro;algorithm;amplitude searchwe;st attention;stream;attention;field algorithm;target attention;last steps;backward jump"}, "caabc3d0c5ece9d44fb2216a347362d4609934c1": {"ta_keywords": "programming languages;large language model;polysourcer;code;source model;models;art codex;new model;sources;single machine;state", "pdf_keywords": "new large language model;source language model;large language models;language models;source languages;several programming languages;various programming languages;programming languages;popular programming languages;different programming languages;multiple programming languages;art code models;generalized languages;code completion benchmark;code preparation tool;code completion tasks;languages;natural language descriptions;natural language;language;corpora;coderot model;more code;decoder;codex model;polycoder;code;sized code;source model;codex"}, "3a8129e6fe3ad9bc3a51e44da32424e38612e4cc": {"ta_keywords": "belief propagation;proof;formulas;quality;depth manner;method;depth;length;new method;certain class;bp", "pdf_keywords": "tensorial deductive database;stochastic logic programming model;several other probabilistic logic programming models;deductive database;popular stochastic logic program;probabilistic logic programs;large knowledge bases;probabilistic logic program;probabilistic logic;logic programming model;new logic model;relational logic;clause models;deep learning;deep learningwe;tensor product representation;tensor representation;tensor productwe;big data;tensor;knowledge;belief propagation;neural networks;neural network;exact semantics;specific predicates;probabilistic model;learning;learning process;many tasks"}, "891fd2690a21f29b2ab54ee2249261d93c8cbc5c": {"ta_keywords": "crowdsourcing;answers;workers;reference answer;questions;worker;process;mechanisms;stage setting", "pdf_keywords": ""}, "0ce6db2fb8c691ff8a89bd01f379ce92b1d248d0": {"ta_keywords": "multinomial distribution;classification;clustering;probabilistic models;statistical learning tools;contagion;occurrence;words;classical nomial;comprehensive set;higher frequencies;beta", "pdf_keywords": ""}, "6994b9860248aea10f8b8bac74e87afd3fcdc842": {"ta_keywords": "semistructured document;entities;benchmark sets;documents;sets;languages;human language;web;novel method;method;system;experimental results;utility;use", "pdf_keywords": ""}, "49f9afa4d0405019d01b55529ce4167380acc103": {"ta_keywords": "direct waveform modification methods;direct waveform modification;synthetic speech;unobserved articulatory movements;vocoderbased excitation signal generation;input speech;modified spectral envelop parameters;spectral differential parameters;novel method;larger quality improvements;novel approach;time sequence;generation;method;capability;paper;experimental results;order", "pdf_keywords": ""}, "e1bb329621de73d08c47beae9b5439a1c244eb1a": {"ta_keywords": "novelty detection;various novelty detection scenarios;novel contrastive learning method;various image benchmark datasets;instances;other instances;training scheme;sample;similar fashion;superiority;method", "pdf_keywords": "novelty detection;contrastive learning;novel contrastive learning approach;novel contrastive learning method;new detection score;novel representation learning method;contrastive training;contrastive data;contrastive representation;detection;classification;detection score;robust anomaly detection;supervised classification;supervised learning;detection method;detection scenario;underlying distribution;classifier;classifiers;visual representations;novelty;anomaly detection dataset;pattern recognition paradigm;type detection;detection efficiency;deep learning;distribution inputs;intrinsic similarity;outliers"}, "931a103258c96a1230dc5c7e38a1cd0b095b9d62": {"ta_keywords": "language model;language model construction;gram language model;vocabulary speech;acoustic model scores;prior linguistic knowledge;word boundaries;learning;novel sampling technique;paper;techniques;new approach", "pdf_keywords": ""}, "a5690b0a514a7cbc913871e41e54c9ad4f6362db": {"ta_keywords": "machine translation;language pairs;noisy comments;robustness;best improvement;blind test;reddit;task;baselines;mt;systems", "pdf_keywords": "machine translation systems;machine translation;neural machine translation;translation accuracy;translation system;translation data;noisy source sentences;translation output;different translation directions;monolingual data;original translators;noisy text;backtranslation;machine vision systems;target language;data augmentation;grammar errors;high robustness;orthographic variations;other linguistic phenomena;informal language;sequence models;strong baseline systems;robustness;level denoising decoder;text;adversarial inputs;training task;improvements;mt models"}, "6aecc93c2d61da073b70dec19795172ca1ff3405": {"ta_keywords": "counterfactual invariant predictors;counterfactual invariance;counterfactual examples;model predictions;domain model performance;input;formalization;irrelevant parts;practical schemes;requirement;access", "pdf_keywords": "counterfactual invariant predictors;counterfactual invariance;counterfactual data;counterfactuals;certain counterfactual input data;counterfactual invariancewe;underlying causal structure;true underlying causal structure;causal structures;complicated causal structures;counterfactual desiderata;predictor;invariant predictor;review data;product reviews;unregularized models;ratings;optimal predictor;good predictor;popular reviews;sentiment;empirical classifiers;wise robustness;movies;predictions;conditional regularization;natural language;invariant risk minimizer;models;star rating"}, "dfd4beb1ecf70b07eb4a52e6ae58f3357e66f478": {"ta_keywords": "robust speech recognition;text recognition;empirical representation;noise;representations;error rates;model;performance;class;settings;novel method;method;variety", "pdf_keywords": "robust deep recurrent autoencoders;robust speech recognition;deep learning;noisy speech;deep data;speech recognition;data augmentation;generic data augmentation models;speech recognition systems;domain noise;regularization;adversarial examples;encoder;neural networks;noisy noisy signals;noisy noisy environments;encoder architectures;robust models;random data augmentation procedure;synthetic neural network;noise;encoder layer;weight decay methods;neural network;learning;augmentation;noise conditions;input features;random noise;models"}, "a427334e296b6be27c3a9c7d6b942d6468e487b8": {"ta_keywords": "optimal deployment policy;network deployment;relay placement problem;network;nodes;high connectivity;numerical exploration;objective;performance;cases;problem", "pdf_keywords": ""}, "823956ee7b994735f3605f426a71e7f85d86f1d4": {"ta_keywords": "unsupervised frame induction;unsupervised task;dependency triples;triclustering problem;graph;task;vogel;state;schneider;art results;approach;fulcher;problem;efficacy", "pdf_keywords": "semantic frames;frame induction framework;unsupervised frame induction;unsupervised frame induction techniques;frame induction task;frame induction;corpus;text annotation;natural language;frame induction problem;graph clustering;polysemous verb classes;stochastic graph clustering;underlying text;unsupervised task;clustering;verb class;triclustering algorithm;purpose text;predicates;large similarity;triframes;context;triclustering problem;triadic data;triclustering method;frame;novel graph;clusters;induction"}, "6eb5029dabd60eb47fddebb5919c613d399fddc6": {"ta_keywords": "sequential discriminant analysis;speech recognition;several speech recognition tasks;discriminability;sequential maximum mutual information;training criterion;speech;mmi;simple extension;method;paper", "pdf_keywords": ""}, "edca37b2004861513c54e7e97b64d4e00e72003f": {"ta_keywords": "polynomials;polynomial;single clause;graph vertices;reducibilities;graph;method;result", "pdf_keywords": ""}, "49775f20431c4a605c5dcc7111c9fe785bf00c62": {"ta_keywords": "partial differential learning;risk aversion objective;strategy;pde;data;novel implementation;sense", "pdf_keywords": "policy gradient methods;policy gradients;policy gradient;sensitive policy gradient;policy gradient algorithm;sensitive policy gradients;optimal policy;coherent risk functionals;risk functionals;critic algorithm;coherent risk objectives;cumulative prospect risk;risk allocation objectives;markov risk;risk aversion;constrained policy classes;risk allocation objective;risk objective;reinforcement learning;reinforcement learning framework;direct policy parameterization;learned state distribution correction;gradient descent;markov chain learning;global optimality;nonlinear risk criteria;risk envelope;powerful reinforcement learning approach;coherent risk;bandit problems"}, "4350ce87dd3ec067f1e583ad415f71ef4ba6075e": {"ta_keywords": "novel dependency parser;original dependency parser;domain adaptation;annotated data;useful linguistic information;training;same data;approach;advantage;use", "pdf_keywords": ""}, "598e2d69d3573adae1f0e3bbe54d10c43f48e0b0": {"ta_keywords": "proximal stochastic gradient method;stochastic tracking method;stochastic optimization;distributional drift;gradient noise;asymptotic convergence guarantees;time drift;optimization error;iterates;efficiency;sense;effects", "pdf_keywords": "stochastic optimization;stochastic gradient;stochastic approximation;stochastic algorithms;stochastic learning algorithm;stochastic approximation algorithm;stochastic graph representation;stochastic optimization framework;stochastic graph model;stochastic gradient method;stochastic approximation methods;proximal stochastic gradient method;optimal learning rate;stochastic noise;optimal learning;stochastic tracking;stochastic optimization problem;asymptotic tracking;stochastic tracking problem;stochastic;convex optimization;stochastic dynamics;continuous learning algorithm;exponential learning rate;asymptotic tracking errors;stochastic version;stochastic parameter;parallel stochastic optimization problem;dependent gradient descent;asymptotic tracking error"}, "22f93927c487e0e0e0d2844489423bcb5d21b45c": {"ta_keywords": "harmonic trap;dimensional lattice gas;phase transition;dynamics;high density regime;low density;density;finite set;states;regime;rich structure;system;parameters;effect", "pdf_keywords": ""}, "19803adec3b97fb2e3c8097f17bf33fabf311795": {"ta_keywords": "contrastive regularization;strongest baseline;sentence pair;training tasks;sequence;tuning;error propagation;novel framework;model;confidence;large margins;lps;reweighting;framework;experiments", "pdf_keywords": "contrastive learning;front learning benchmark;weak supervision sources;many natural language processing;natural language processing;novel contrastive learning method;usual supervised learning problem;weak supervision;language models;novel contrastive learning algorithm;shallow learning;deep learning;deep coding;successful deep learning;only weak supervision;contrastive representation;label misclassifications;supervised learning model;training framework;regularization;unsupervised learning problems;classification task;learning;training data;supervised learning problem;deep nonlinearity;noisy labels;deep neural networks;labels;clean labels"}, "53dc99155c52979e311a403571f1b1d57ff73b48": {"ta_keywords": "displacement field;digital image correlation measurements;generative data;material microstructure;finite element analysis;material;predictive;predictions;phenomenological model;specific constitutive model form;good generalizability;knowledges;novel approach;framework;way;need", "pdf_keywords": "soft tissue modeling;biological tissue modeling;tissue mechanics;neural operator learning model;implicit neural operator learning approach;soft biological tissue;neural operator learning;neural operator model;soft tissue;neural operator learning approach;tissue tissue response;theneural operator learning;tissue tissue;implicit neural operator;neural operator;material model;material modeling framework;materials modeling;tissue;heterogeneous material modeling tasks;tissue types;tissue response;soft tissue response problem;model training;material displacements;constitutive modeling;neural network;material database;conventional nnn models;neural networks"}, "6bca949b7ce69d6a43120d75e65f43d4c5a80ed4": {"ta_keywords": "incentive design;incentive design techniques;incentive design tools;incentive design problems;machine learning;interested parties;exotic outcomes;combination;self;new class;new tool;need", "pdf_keywords": ""}, "0ee468b9b709a2610c4b574d67218e7960350224": {"ta_keywords": "novel data augmentation strategy;neural machine translation;scalable data augmentation algorithm;translation datasets;augmentation;text;single text message;nmt;other words;random number;set;strategy;method", "pdf_keywords": "neural machine translation;machine translation;novel data augmentation strategy;data augmentation;translation datasets;augmentation framework;data augmentation model;translation task;word dropout algorithm;augmentation;word dropout;augmented target sentence;deep learning data;deep learning models;other words;empirical data;target sentence;deep learning process;sentences;vocabularies;adaptation;source sentence;single word;text;deep learningwe;empirical observations;words;batch;machine learning;mle"}, "4b22503d6da9ff3222d94106cc7425ea4fea43af": {"ta_keywords": "dependency parsing;social media data;efficient library;languages;efficient model;entropic;shelf;model;novel", "pdf_keywords": ""}, "1fa69608666e66452df56b1f71282def7ac16035": {"ta_keywords": "bribery problem;bribery;more voting agents;computational complexity;manipulation problems;social choice;budget constraints;votes;manipulation problem;large source;preferred outcome;outside agent;scenario;problems;results;dissertation;context", "pdf_keywords": ""}, "2f7c03f0d3c6f51728e925a874c49a25559cc6b3": {"ta_keywords": "compositional questions;query;long documents;neural representation;document;art models;novel method;result;method;approach;state", "pdf_keywords": ""}, "c13c400d1f481863d57ec265d296b0a08ec77876": {"ta_keywords": "automatic speech recognition;audio segmentation;connectionist temporal classification;unsegmented evaluation;asr;segmentation;ctc;novel insertion;loss;pipeline;model;end;single model;approach;experimental results", "pdf_keywords": ""}, "0a2ba7b1c05062d2bb7cd35e218fe08d6ea29488": {"ta_keywords": "meeting;relevant attendees;email;addresses;graph;information;tasks;framework;new approach;full set", "pdf_keywords": ""}, "fd54706252a094d592feadf53a0a3ffed4af9295": {"ta_keywords": "far field speech recognition systems;3rd chime challenge;underlying speech recognition problem;speech separation;recognition;underlying signal processing;essential features;systems;challenge;performance;design;empirical simulations;context;world scenario;paper;problem;results", "pdf_keywords": ""}, "142407d3cb61067e88d385f95ae238c74b19d554": {"ta_keywords": "social network structure;social network;graph structure;nodes;graph;common feature;statistical analysis;behavior;structure;users;ability", "pdf_keywords": ""}, "75983c55a489d526427fe399ce2670376168a2f0": {"ta_keywords": "parallel corpora;statistical machine translation;paraphrases;domain corpora;paraphrase;domain domain corpora;phrase;parallel extraction;lemma;context;method;new method;process;definition;use", "pdf_keywords": ""}, "5bc188b4ab7b27649236fad6a686b2cfe6368219": {"ta_keywords": "document topic modeling;commonsense;clustering technique;similarity;document;texts;algorithm;composition;concept;view;underlying point;measure;important aspects;length", "pdf_keywords": ""}, "d8d49cc56b303d6ed0e821f8593e2f7acd1b4fb4": {"ta_keywords": "benchmark sound detection systems;sound events;collisionless sound;detection;learning architecture;conformer;validation set;system;set;performance;method;novel method;comparison;end", "pdf_keywords": ""}, "a61ef7be5b5c9fbc6654f7c17fa595976652416b": {"ta_keywords": "matching;organ;tissue;available data;patient;new mechanism", "pdf_keywords": ""}, "ff1a1e39a94b9ca31e6013d12bc2d27f7a31567c": {"ta_keywords": "end speech recognition problem;attention model;separate attention function;multiple attentions;conventional encoder;decoders;decoder;decoder framework;slacian database;new network architecture;final output;outputs;end;head;proc;method", "pdf_keywords": "attention models;attention model;end speech recognition;multiple attentions;different attention functions;speech recognition;effective speech recognition;single attention;large corpus;complex speech dynamics;attention;multiple decoders;andlinguistic processes;speech;encoder;multiple characters;japanese data;decoder framework;recognition performance;head;network analysis;encoders;network analysis method;head toattention;final output;training data;new network architecture;spontaneous translation;linguistic aspects;outputs"}, "3b4b5e72a2f84d079d0d1d825309c2f6ded76539": {"ta_keywords": "transfer vectors;gaussian class estimation;transfer vector;variational bayes method;estimation;approximate estimates;standard variant;variant;novel approach;approach;performance;steps", "pdf_keywords": ""}, "77e5c4fa595466aa51d29327a60f9d4af4436876": {"ta_keywords": "learnability;learning;knowledge;empirical discovery;abductive explanation;novel approach;algorithm;ebl;approach;experimental implementation;article;mistake;theoretical performance;art;state", "pdf_keywords": ""}, "3c0d4dc4237934e37467f4ede3af859bcb140abf": {"ta_keywords": "transformer;transformer layers;pure transformer;features;feature extraction method;total person;tokens;large scale dataset;image patches;token;regression module;global information;information exchange;art performance;state;problem;method;extensive experiments", "pdf_keywords": "supervised crowd counting;crowd counting tasks;crowd counting;cnns;crowded scenes;crowd density map;crowd event;crowd;convolutions;sparse representationwe;global receptive fields;semantic features;global image;sparse representation;global context;popular benchmark crowd;intrinsic labeling;challenging datasets;people;encode features;visible people;context;persons;point supervision;features;labeling;images;objects;high level features;scene"}, "9f49ed155d7575d181d16dd5bc92b754cae0bea9": {"ta_keywords": "novel blind identification method;identification problem;convex relaxation;optimization;subspace;rank;inputs;formulation;numerical example;efficacy", "pdf_keywords": ""}, "91a2d496553cfee2b66906f704b8e3d081e2d1bf": {"ta_keywords": "inductive logic programming;languages;fault density;ilp systems;clauses;ilp;evaluation;bias;comparison;extensions;number;performance;extension;class;user", "pdf_keywords": ""}, "2bbb33ab8124e5078ec39e821a25c24c20a31b9b": {"ta_keywords": "crowdsourcing;second workshop;parallel workshop;first workshop;open research;47th international conference;large data bases;vldb;technology;science;budapest;june;future;hungary;place;conjunction;series", "pdf_keywords": ""}, "0e96925c57b3325e7e37c1964b518e9276024cbf": {"ta_keywords": "disordered phases;phase transition;liquid phase;spins;spin;magnetic field;critical value;disorder;transition;dynamics;finite number;field;interplay;large number;presence", "pdf_keywords": ""}, "7c2ff8fa0d24ed712e4bc2dbdb370a1cd62c965b": {"ta_keywords": "harmonic trap;particle;dynamics;motion;time;dependent density;states", "pdf_keywords": ""}, "3d846cb01f6a975554035d2210b578ca61344b22": {"ta_keywords": "graph embeddings;supervised entity extraction;entity classification;embeddings;class label;graph;instance;neighborhood context;benchmark tasks;instances;models;inductive variants;large set;improved performance;method", "pdf_keywords": "graph embeddings;accurate graph embeddings;graph context prediction;graph context;graph induction;unobserved graph;embeddings;embedding;transductive learning;unsupervised learning;laplacian regularization;input graph;supervised learning system;graphs;conditions embeddings;embeddingwe;graph;feature vectors;class labels;learning methods;class label;input feature vectors;regularization;nearby nodes;classification;learning methodswe;labels;entities;neighborhood context;unobserved instances"}, "b6de9d0ca42a03967287aa7abfd59479e086a35a": {"ta_keywords": "automatic gloss finding;knowledge bases;hierarchical clustering;world datasets;algorithm;robustness;wide variety;extensive experiments;effectiveness;experiments;user", "pdf_keywords": ""}, "d39478dd8d825bbd6c963d6a5ef2cee6857f6c21": {"ta_keywords": "argumentation;statements;difficulty;talk;powerful tool;recent work", "pdf_keywords": ""}, "358d7d6333d3edd530e37efd8004cb9da8cfd5d4": {"ta_keywords": "structured procedural knowledge extraction;instructional cooking videos;semantic role labeling;visual action detection;structured form;unsupervised segmentation;essential structural information;task;benchmark;action;tuples;lambda;more elaborate models;form;performance;standard modeling approaches", "pdf_keywords": "structured procedural knowledge extraction;semantic role labeling;novel procedural knowledge extraction task;narrative videos;annotated videos;procedural knowledge extraction;semantic role labeling model;instructional video analysis;instructional cooking videos;sentence classification dataset;structured procedural knowledge;understoodinstructional videos;recipe videos;supervised multimodal methods;structured knowledge;natural language;large corpus;key sentences;annotation system;dense procedural knowledge;instructional videos;structured procedures;kitchen videos;video clip;utterances;recipe texts;videos;un annotations;sentences;open extraction"}, "adf726bdcdddacee1c70d911b8f84b6a16841a32": {"ta_keywords": "textual reviews;different product types;product type;prominent aspects;user experience;dataset consisting;novel framework;framework;experiments;ability;art performance;user;state", "pdf_keywords": ""}, "0dff2b00fd6e8e7b5f3c0707b0e51e3628988420": {"ta_keywords": "aware backtranslation methods;new privacy;aware text;sensitive attributes;privacy;task;world datasets;extensive experiments;methods", "pdf_keywords": "sensitive attributes;sensitive information;novel privacyaware model;obfuscation;obfuscation model;unstructured text representations;aware text;higher obfuscation scores;end obfuscation model;implicit obfuscation;obfuscation performance;underlying text;novel privacy;machine translation;privacy;demographical attributes;unstructured data;natural language;natural language languages;text;forward translation model;semantic quality;linguistic accuracy;linguistic quality;demographic groups;datasets;new research challenge;robust form;leakagewe;leakage"}, "694c0c5a4d0176e29bb85e1b9ca8ea84075fbbbb": {"ta_keywords": "reconfigurable neural network;neural network;dimensional neural networks;softmax;rnn;nns;hybrid feed;architecture;hidden states;hnn;family;comprehensive empirical study;choice;existence", "pdf_keywords": ""}, "194a1e5f9af0ea00b22def879d90b926187fbb64": {"ta_keywords": "spin dynamics;heisenberg model;spin;orbit interaction;model;effect", "pdf_keywords": ""}, "c02da00857c33fa39b115c0eb6c655ff6cf96878": {"ta_keywords": "harmonic trap;harmonic oscillator frequency;harmonic oscillator problem;energy density;particle;exact solution;method;new method;order;case", "pdf_keywords": ""}, "ca201db9980e49647feedf39eb30b19f074bf68a": {"ta_keywords": "transcription signal;convolutional;novel approach;shape;machine;ability;approach;power;use", "pdf_keywords": "end speech recognition task;spontaneous speech transcription theory;underlying speech corpus;transcription;speech recognition task;automatic speech recognition;corpus;neural transcription;orthographic speech recognition;igws transcription;spontaneous speech;standard corpora;utterance number;utterance;corpora;elementary elementary elementary utterances;convolutional parsing;earnings calls;record length;simple tokenizer;formatted text;standard words;baseline conformer;end;target labels;complete english orthography;english documents;sequence model;standard expression expression expression;acoustic model"}, "48530f3d6425f2f150f07ccdd61ba951951a0a7d": {"ta_keywords": "domain adaptation;multilingual domain adaptation;domains adaptation;neural machines;new domains;specific layers;tasks;small task;tuning;scalable approach;model;approach;par;experiments;full fine", "pdf_keywords": "multilingual adaptation;domain adaptation;multilingual models;multilingual translation task;multilingual nmt;multilingual dataset demonstrate;multilingual model;neural machine translation models;translation models;high resource languages;novel translation model;multilingual baselines;neural machine translation system;low resource languages;different adaptation tasks;translation model;new languages;adaptation;bilingual baselines;empirical translation properties;language pairs;corpus size;translation dynamics;languages;target task complexity;large models;capacity domains;tuning;nmt model;multiple target tasks"}, "e3a85c5defe60f1f394fc4e7245fc071a249cf5b": {"ta_keywords": "behavioral model;occupants behavior;social game;learning framework;occupants;statistical learning;behavior;office building;game;game site;neighborhood;person;method;goal;parameters", "pdf_keywords": ""}, "c99e050b83360e5cbeee8fd2957aaab5b31aa638": {"ta_keywords": "generative sequence model;language models;memory;art language models;term discrepancies;prediction;model;discrepancies;calibration;true distribution;state;amount;approach", "pdf_keywords": "language models;language model training;neural language models;language model;generative sequence model;box language model;generative models;human languages;learned modelwe;generative model;art language models;model calibration procedure;entropy rates;dimensional memory model;entropy;entropy rate;iterative generation;calibrations;accuracy;predictions;empirical data;term generations;memory;calibration;supervised learning task;small cross entropy;model;rate reference;classifier scores;same calibration approach"}, "7ad913d1c6eddbdad1ab4571ab91f00f055ab735": {"ta_keywords": "parallelization;fast recurrence;attention mechanisms;form speech;architecture;novel architecture;sci;context", "pdf_keywords": "parallelizable recurrent neural network structure;parametric speech recognition;deep recurrent neural network;parametric speech sensor;longform speech recognition;attention weights;trainable vector representations;encoder model;novel encoder;encoder structure;nonlinear unitarity unit;encoder;bidirectional recurrencewe;art convolution encoder;scalable representation;natural recurrence kernel;scalable unitarity group;attention;parallelization;parametric model;attention mechanism;nonlinear models;nonlinear;bidirectional recurrence;model;recurrence;representation;compute efficiency;novel network architecture;natural language processing"}, "40947612162cc4644f9489721ec1ca94fe7e765c": {"ta_keywords": "semantic relatedness;thesaurus;scale crowdsourcing study;russian;open sources;resource;evaluations;evaluation;sources;native speakers;coverage resource;first open distributional;high accuracy;collection", "pdf_keywords": "russian semantic similarity evaluation;semantic relatedness measures;semantic similarity;semantic relatedness;semantic relations;thesaurus;large semantic information;word pairs;similarity;larger dataset ofsemantic relatedness;semantic information;unrelated source words;russian language;natural language processing systems;natural language processing;scale crowdsourcing study;similar documents;language resources;word pairswe;large corpus;word collection;source words;large scale crowdsourcing campaign;continuous relatedness scores;language;source word;tokens corpus;words;languages;hypernyms"}, "69379f55de081938ae9d8b91ef549542ed78f5f0": {"ta_keywords": "orbit interaction;zeeman field;spin;interaction;effect", "pdf_keywords": "speaker diarization systems;speaker diarization technologies;neural speaker diarization;speaker diarization research;speaker diarization performance;novel speaker diarization system;speaker diarization techniques;automatic speaker diarization;joint speaker diarization system research field;speaker diarization system;future speaker diarization research activities;speaker diarization technology;speaker diarization;several speaker diarization systems;novel speaker diarization method;joint speaker diarization;speaker diarization method;speaker diarization output;speaker recognition;speaker diarization process;spontaneous speaker diarization;speaker representation;second speaker recognition challenge;standard speaker diarization error rate;speaker verification;speaker representations;novel speaker representation;speaker verification method;dependent speaker verification;speaker identification method"}, "d74c5b5ed8eb467dc7f313b70a08880fcd74c39d": {"ta_keywords": "local electronic tax information system;local electronic tax system;local government;local governments;local residents;citizens;data;dimensional data set;credibility;public opinion;quality;ability;good indicator;study", "pdf_keywords": ""}, "625764f8e3e1334ffbfe5b3139e555499e6df4d5": {"ta_keywords": "natural language websites;natural language documents;linguistic content;corpus;requests;specific features;pages;updates;subject experiments;specific data;request;user;system;performance", "pdf_keywords": ""}, "b5241fcbfbf30f6fd8ff1ae19d947dd2ca23244f": {"ta_keywords": "emotion;rich events;dataset;events;large collections;personal images;collection;small dictionarys;web;subjects;trees;survey;series;set;acquisition;process;method", "pdf_keywords": ""}, "21c39ce886dc38dd2006ea25d6bd1eff4cdba0b8": {"ta_keywords": "online political blogs;blogs;blog;social media;prediction;posts;content;behavior;users;analysis;type;model;new model;function;number;brief qualitative discussion", "pdf_keywords": ""}, "51321a60f5ec2c80253394ef86e8b5fcc768f52a": {"ta_keywords": "dimensional databases;identifier names;database;similarity;names;objects;large number;method", "pdf_keywords": ""}, "6954a6bb9d6f3e365b26b694c963ae1d62a03444": {"ta_keywords": "additive attention;frame;successive sequences;multiple sequences;effective context;efficient model;linear complexity;fastformer;accelerated acceleration;pairwise interactions;efficiency;model;new model", "pdf_keywords": "attention query;attention keys;global context representations;aware attention value;additive attention;additive attention mechanism;entire query sequence;window attention;query sequence;linear attention;global query vector;novel additive attention;accurate context information;attention networks;contextual information;global query;context information;aware representations;aware key;query matrix;key vectors;global attention;global context;query;text summarization tasks;natural language inference;token representation;benchmark tasks;term learning model;news recommendation tasks"}, "03006aefccdd0c5c6736ab11ed574d02ba1cc086": {"ta_keywords": "machine translation problem;language sentences;target;resource scenarios;order;source;significant improvements;effective solution;experiments;time;problem", "pdf_keywords": "neural machine translation;low resource machine translation data;neural machine translation model;machine translation;translation strategies;translations;translation quality;target sentences;language sentences;many language pairs;divergent language pairs;word translation;english language pairs;translation;trainingtime supervision;standard statistical alignment models;resource uhur;high resource data sets;language;novel statistical reordering rule;parallel data;resource scenarios;english;additional source;natural resource allocation;common resource;effective semisupervised learning framework;large data sets;standard reordering rule;augmentation"}, "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269": {"ta_keywords": "codexpilot project;scale languages;available code;software;open problems;writing;new evaluation;github repository;quality solutions;quality;available version;variety;model", "pdf_keywords": "code generation models;code generation tool;accurate code samples;code generation;code generation systems;program generation;generative programs;code synthesis;large language models;language engine;arbitrary code;other large language models;several large language models;large language model;dominant programming language;worstcode generation;generative code;natural language specifications;language models;generative languages;natural language docstrings;software generation capabilities;underlying language;code task;github corpus;python language;robust evaluation framework;powerful languages;evaluateability;natural language models"}, "b5a667bf189a0cfda22bac702d97b601ae6adb6f": {"ta_keywords": "free ascent algorithm;gradient;robust models;concave;random reshuffling;distribution shifts;finite sum structure;algorithm;strategic decisions;data sources;data", "pdf_keywords": "dependent distributional robust risk minimization problems;adversarial distribution shifts;robust optimization;robust optimization problems;robust learning problem;stochastic optimization;convex optimization problem;robust strategic classification problem;robust strategic classification;optimal optimization;generalized linear loss;stochastic optimization problem;robust formulation;optimal strategies;adversarial perturbations;quadratic gradient estimator;order gradient estimator;approach learns models;optimization framework;gradient estimator;optimization problems;distributions;random reshuffling;learning problem;dependent supervised learning;synthetic datasets;stochastic algorithm;weak constraints;overall loss function;optimization problem"}, "26cc9e13a7a76e3cf5f9885d08cdafabd6fbd7ec": {"ta_keywords": "realistic tournament data;tournaments;randomization;empirical properties;atp world tour;english premier league;german league;real data;log;popular model;characteristic", "pdf_keywords": "synthetic tournaments;particular tournament;balanced knockout tournament;tournament;tournaments;tournament tournament rule;knockout tournament;tournament fix problem;best tournament players;single tournament player;likely match probabilities;computational complexity;real world instances;seeding;seedings;players;particular player;selection ruleswe;popular condorcet;selection rules;node nodes;international league;search tree;real world;possible candidates;bundesliga data;datasets;choice points;artificial intelligence;artificial intelligence framework"}, "ac46562e61cfef6213a915bbb80d1a1a2901542a": {"ta_keywords": "technical paper recommendation;reviewer interest information;information retrieval;preference data;web;other methods;novel approach;novel autonomous procedure;members;approach;principles;problem", "pdf_keywords": "reviewer interest data;evaluative paper recommendation;reviewers interest data;technical paper recommendation;paper recommendation;information retrieval;recommendation algorithm;paper information sources;collaborative algorithms;reviewer interest;reviewer preferences;conference paper submissions;multiple information sources;conference papers;reviewers preferences;collaborative approach;information sources;reviewers;reviewer;reviewer results;technical papers;paper sources;recommendation;collaborative ltering;information integration system;text sources;reviewer source;conference abstracts;suitable papers;paper contents"}, "559fdae33f0b7733b80a7dbcb902c79598a0d26e": {"ta_keywords": "treatment effect estimation;propensity score network;treatment effect estimation problem;continuous treatment effects;causal effects;adversarial manner;covariate;tpa;methods;novel approach;settings;independence;variety;architecture;framework;problem", "pdf_keywords": "treatment effect estimation framework;treatment effect estimation;generalized treatment effect estimators;treatment biases;treatment effect estimators;alternative treatment effect estimatorswe;generative treatment model;treatment selection bias;treatment effect models;treatment predictor;treatment embeddings;treatment effect estimation problem;underlying treatment variables;structured treatment interventions;causal inference;treatment distributions;inductive biases;adversarial domain adaptation approach;counterfactual outcomes;generic treatment effects;arbitrary treatment groups;treatment response curves;bias reduction;counterfactual representations;treatment effect estimation ofwe;treatment effects;treatment modeling community;treatment effect;binary treatment;adversarial training algorithm"}, "bf63276c90a803fe0d069ce0a3a4a8236e756363": {"ta_keywords": "predictive inference;gutters;predictive models;models;neural architectures;predictive prediction;gutter;inferences;comic book stories;novel architecture;predictive power;architecture;novel tasks;panels;power;large number", "pdf_keywords": "comic book panels;comic books;text models;popular comic scene;available comic books;comic book;comics;text story;comics domain;contextual information;neural image caption generator;manga text;few context panels;character coherence;simple text model outperformswe;literature;text;dialogue;high dimensional manga;visual storytelling system;text cloze;characters;machine learning models;manga;style tasks;machine learning;textboxes;colloquial dialogue;manga extraction;deep learning"}, "83165cf62e62a013c2bad61c98120ccb9a0087ae": {"ta_keywords": "tutorial;relevance", "pdf_keywords": ""}, "e8deeebc7ff6315115f01fd70a343d62db202888": {"ta_keywords": "high quality synsets;synsets;novel crowdsourcing system;compilations;noisy pairs;corpora;genus;high accuracy;generation;cambridge;system;performance;f1 score;experiment", "pdf_keywords": ""}, "b778a7c4001898a1c3888577154d747522f16db4": {"ta_keywords": "different adversarial losses;adversarial loss;discriminative adversarial networks;generative adversarial networks;regularization approaches;regularization terms;divergence;model distributions;component functions;networks;different component functions;data;like measure;new framework;sufficient conditions;paper;relationship", "pdf_keywords": "different adversarial losses;adversarial losses;adversarial loss;adversarial loss test;discriminative adversarial networks;discriminative adversarial network;discriminative adversarialwe;discriminative gans;generative adversarial networks;conditional gan;unconditional gans;gans;loss functions;discriminator loss function;discriminative counterpart;generalized gradient penalties;gan;regularization;discriminative model;regularization terms;regularization approaches;gradient penalty;asymmetric loss;global regularization;neural networks;coupled gradient penalties;gradients;regularization problem;neural network;supervised learning task"}, "0453bab552e83f19dd6ba12061949f128fa9b045": {"ta_keywords": "probabilistic learning method;datasets;classes;unknown classes;conditional value;data;em;unknown number;formalism;rpt;method;novel;efficacy", "pdf_keywords": "supervised generative learner;safe learning;exploratory learning;structural learning;supervised algorithm;novel multiclass;classes;large class;generative model;best exploratory model;popular deep learning;models;novel probabilistic approach;class;deep learning;few seed classes;multiclass problems;new classes;semisupem;model selection;novel algorithms;unstructured data;deep learning approach;model;nonparametric models;robust method;robustness;structure search;search;examples"}, "b36dc8db9930a785edd55ca30328ace2896523e6": {"ta_keywords": "semantic annotation;annotated semantic corpus;different semantic teams;available atis corpora;corpora;tools;knowledge;polish;results;epsz;purpose;set;article", "pdf_keywords": ""}, "87eece8d39d1e25ba87550be8b01af32738cbf2c": {"ta_keywords": "speech datasets;speech;corpus;wsj0;enhanced end;basic end;architecture;architectures;training strategy;end;article;experiments", "pdf_keywords": ""}, "e2ece7ea0924b4f95f65587973118bea9a44a3d2": {"ta_keywords": "software domain;similarity measure;java classes;entities;classes;relationships;dataset;identification;accuracy;approach;method;measure", "pdf_keywords": "fermion materials;text entities;coordinateterm relationships;semantic relationships;functional materials;fermion superconductor;semantic relationship;corpus;components;generic text corpus;fermion;java code;matter interactions;entities;similar entities;specific knowledge base;detection;contexts;coordinate term relationships;matter interaction;coordinate term prediction task;cross validation;java classes;underlying class;testing relationships;lexical representations;libraries;software;java class;standard libraries"}, "bbc7e533e5bfb388af1afd85bfb7ba17330cae76": {"ta_keywords": "high voltage shore power supply;bridge inverter current;converter chain;dc bias suppression;output voltage;transformer;dc voltage component;control strategy;novel control strategy;real time control;pulse width;principle;width;experimental results", "pdf_keywords": ""}, "92cee1e209f2d9a311416b0d9fd8a49b0fbe7df2": {"ta_keywords": "symbolic learning methods;several learning methods;several different learning methods;text categorization;feature selection method;generalization performance;filters;features;collaborative setting;transfer;direct transfer;stability;data;several concrete proposals;beneficial variation;setting", "pdf_keywords": ""}, "956aa64b0d5f5802b98bf551d5bab8993b114fd0": {"ta_keywords": "new similarity function;similarity;large database;data;use;series", "pdf_keywords": ""}, "f52f7964febd6d6d72aa23505b50d33e1d4ce0aa": {"ta_keywords": "novel supervised rule discovery;weight learning;boosting;iterative discovery;learning;candidate rules;models;data;model;tasks;lds;gaps", "pdf_keywords": "novel labeling rule framework;rule discovery framework;rule discovery;labeling rules;novel rule discovery;weak label generation;weak supervision source;soft label generation;novel rule representation;supervised learning;supervised learning model;relation extraction;language models;rule templates;protein interaction classification;new weak labels;effective rules;human annotators;learning results;discrete classification tasks;classifiers;cost model learning;level annotation;annotation agreement;rule candidates;quality labeling rules;language model;weak labels;new rule proposal;learning model"}, "60f3e69e4f18e8e8e7dcc4ba66c1e216b49ad982": {"ta_keywords": "sense knowledge acquisition;theoretic knowledge acquisition;game engine;novel game;game;java application;players;acquisition;gecka;novel concept;novel application;large collection;development;application;form", "pdf_keywords": ""}, "09b87b6e7bfbf66d355574d292586595e0185d6e": {"ta_keywords": "design", "pdf_keywords": "new phylogenetic feature prediction system;global language network;phylogenetic surface features;phylogenetic trees;multilingual linguistics;phylogenetic relationships;lingference;language structures;languages;classification tasks;human language;linguistic typology;language;linguistic specificity;interlingual text input;typological features;classifiers;hybrid feature classifier;multilingual syntax;linguistic parsers;linguistic redundancy;neural network;partial feature information;training data;feature;features;task dataset;geographic proximity;geographical proximity;world atlas"}, "5b16d138bf16762d43b55b6e21d9b0b61021180e": {"ta_keywords": "dimensional electron gas;electron gas;interacting bosonic system;dimensional square lattice;interacting system;dynamics;system;predictions;analysis;good agreement;results", "pdf_keywords": ""}, "b15ea460c77a4ee8aa159a30ab0331deedfcf392": {"ta_keywords": "large language models;specialized expert modules;high capacity sparse layers;auxiliary expert;experts;available experts;balanced routing functions;training;heuristics;inference;base;model parameters;new balanced assignment;full use;token;small fraction;loss;efficiency;approaches", "pdf_keywords": "sparse expert layers;large language models;previous sparse expert approaches;large expert training set;expert layers;large sparse models;parallel training;language models;high capacity sparse layers;sparse models;sparse approaches;language model;sparse base layer;sparse parameter models;new conditional compute layer;large scale models;expert features;low computational cost;new sparse layer method;training loss modifications;optimal assignment;scalable scalable learning paradigm;single expert;best assignment strategy;expertwe;expert;computation budget;machine learning;experts;redundant redundant knowledge"}, "bbb7eb10c45cabaee6e427242fce7180c0217ef1": {"ta_keywords": "different programming languages;variable names;programs;discriminative models;general path;different prediction tasks;method names;representation;full types;tasks;approach", "pdf_keywords": ""}, "cbf941fef87830efa4de98455cfe943917909b66": {"ta_keywords": "superlinear convergence;local superlinear convergence;hessian approximation;inverse hessian approximation;convex broyden class;new theoretical analysis;baxter;methods;potential function;analysis;trace;de vosibirsk;logarithm;rates", "pdf_keywords": "superlinear convergence algorithm;superlinear convergence rate;superlinear convergence;superlinear convergence rate iswe;linearly convergent methods;superlinear optimal solution;hessian approximations;convex broyden class;smooth unconstrained optimization;positive definite hessians converges;linear convergence rate;nonlinear optimization;convergence algorithm;convergence rate;classical methods;quadratic minimization;gradient method;same convergence rate;bfgs methods;optimization;variational method;asymptotic behaviorwe;broyden family;convex combinations;convergence;iterations;explicit bounds;line search;diagonalizations;general nonlinear functions"}, "d8551a4b49aa547ad8884ba9f545480860fcadd1": {"ta_keywords": "novel deep neural operator architecture;implicit neural operator;neural network;mechanical responses;conventional constitutive models;integral operator;modeling;materials;ifno;feature space;data;integration;discrete equations;range dependencies;approach;work", "pdf_keywords": "deep neural operators;deep neural operator;material response prediction;neural operator learning approach;novel deep neural operator architecture;material modeling tasks;material responses prediction;neural operators;novel integral neural operator learning architecture;shallow neural network;heterogeneous material modeling;learned solution operators;neural network;material modeling;material models;neural networks;implicit neural operator;shallow neural network model;neural operator;deep layers;integral neural operators;digital image correlation;shallow neural network modelwe;neural network representation;integral neural operator;known partial differentialneural operators;fidelity material responses;independent inspection network;digital image corrections;neural network model"}, "cee25a535ec7165eae38f498a391050077ad9f65": {"ta_keywords": "speaker clustering;dirichlet process mixture model;conventional hierarchical agglomerative clustering;scale speaker;utterances;novel nonparametric model;utterance;scale data;model;case;large amounts;problem", "pdf_keywords": ""}, "7668b23aadf43bebe5e2d3abf37938b44bd16200": {"ta_keywords": "new web search benchmark;new web search problem;webqa benchmark;new benchmark;web;multimodal;connected world;significant improvement;opendomain version;cost;art;state;problem", "pdf_keywords": "wave turbulence;partial wave;waveforms;wave;waveform waveforms;wavelet;coarse filtering;turbulent flow;multichannel signal processing;stream;single event;nonlinearwe;spectra;streamers;sound sound sound representation ofwe;multiple input multimodality;convolutional approach;fc1 layer;particle dispersion;convolutional fpn;background;events;nonlinearity;fc2 layer;nonlinear systems;multimodal representation;patterns;pattern;distortion;interrelated patterns"}, "c688e187cede868e35fc1b53913e0fbbe6e38ea0": {"ta_keywords": "structured prediction;structured prediction tasks;dataset aggregation algorithm;algorithms;latter algorithms;dagger;searn;variety;comparative study;parameter;advantage", "pdf_keywords": ""}, "2448e63a7bb626d09001fe37e60befdb2919f6e6": {"ta_keywords": "knowledge extraction;accurate commonsense property lookups;underlying knowledge;robust extractions;time machine learning system;graph;novel approach;representation;method;paper;experiments", "pdf_keywords": ""}, "cee96ee69adacfdeb648c230d2c9b01011724724": {"ta_keywords": "repulsive force;repulsive potential;dynamics;random potential;particles;interaction strength;random fashion;level system", "pdf_keywords": ""}, "007371feab4af758b74580c43e74827b3500c67e": {"ta_keywords": "demand streaming system;scalable video;simple fractional storage architecture;convex content placement problem;user demand;content;demand;implementation;codes;changes;general optimization problem;formulation;approach", "pdf_keywords": ""}, "2a0cb1a1e78b77fe9981e4935410cf3ea900e370": {"ta_keywords": "spin;electron;electrons;orbit interaction;dependent scattering;polarized background;magnetic field;background;effect;presence;powerful tool", "pdf_keywords": ""}, "d60b4594fb0404329d9ebf6fd88702ca3479e904": {"ta_keywords": "abbreviations extraction algorithm;biomedical text;abbreviations;visual recognition machine;alignment;algorithm;translation;standard data;improvement;methods;method", "pdf_keywords": ""}, "99546b4d1f2547095bb15eec36e03f64b74a78d4": {"ta_keywords": "popular stock market market;popular market;popular community;market;users;epicentres;activity;number;strong connection;end;post length", "pdf_keywords": ""}, "5e3d1bece9dd2356fd2b31312bd62c8f7126882d": {"ta_keywords": "entangled states;gaussian state;gaussian transformation;states;transformation;new method;large number;method;application", "pdf_keywords": ""}, "b990331a5394f3642a1fd1791d70bfa2d85d9d1d": {"ta_keywords": "dissemination ofvaccination patterns;patterns ofvaccination;patterns ofvaccination patterns;social media patterns;vaccine;social media;dissemination;traditional media;mechanism;messages;many researchers;mechanisms;combination", "pdf_keywords": ""}, "8b98f7ff3bb1b199db85fc219a5c27b355adf1be": {"ta_keywords": "erbium laser;osseous crown restorations;invasive osseous crown;osseous crown;free erbium imaging modality;radiation;lengthening;procedures;treatment;light;need;cost;study", "pdf_keywords": ""}, "a604ad4654f31d325b888806e276123a704cb5c8": {"ta_keywords": "support vector machines;novel minimum error classifier;classifiers;geometric margin maximization;svms;new mce training method;geometric margin;geometric margin value;discriminant functions;general class;new method;prototype;effectiveness", "pdf_keywords": ""}, "652e3c774da47c0c8788111ec886a00d3b8fc637": {"ta_keywords": "brain deformations;biomechanical modelling;biomechanical modelling approach;deformations;total lagrangian explicit dynamics;mri;tumour;patientspecific magnetic resonance imaging;skull;computer simulation;mesh;resection;cerebrospinal fluid;parenchyma;problem geometry;solver;data;approach", "pdf_keywords": ""}, "d0ea87ce3bcd86428d379fd478c365c64f870200": {"ta_keywords": "schema robustness;dialogue systems;dialogue system;collaborative service systems;schemas;unseen services;beta;simple modela;survey;sensitivity;augmentation method;use;novel method;novel measure;method", "pdf_keywords": "dialogue state tracking models;dialogue state tracking;dialogue systems;dialogue model;dialogue state;dialogue;schema benchmark;crowdworker paraphrase tasks;schema variants;schemas;schema sensitivity;linguistic styles;schema;different schema variants;different schema augmentation methods;schema augmentations;different training schemas;original schemas;schema repository;prediction consistency;agnostic schema;ss benchmark;text;deep learning;models;services;generalize services;systems;novel models;training data"}, "429b65937d4922578a81e1f0ef5aeab7361ae36b": {"ta_keywords": "corresponding text;sentences;mapping;text;tools;search;new class", "pdf_keywords": "natural language translation tasks;language bash;bash commands;natural language snippets;specific scripting;longer shell scripts;bash;parsers;natural language;tl scripts;bash command;semantic parsers;large corpus;mapping sentences;higher full command accuracy;style commands;code snippets;command;semantic parser;corpus;syntax;single command;nl2bash;neural semantic parsers;neural machine translation models;command structure errors;nl sentences;programming challenges;line translation;translation accuracy"}, "ba45a346690f3c5b6f8c371b5c6cf1d7cce5619d": {"ta_keywords": "blind system identification problem;physical system;measurement output;identification;measurements;method;new method;problem", "pdf_keywords": "constrained rank minimization problem;regressive exogenous input;rank minimization problem;relaxed convex formulation;input realizations;quadratic input nonlinearity;arxiv model;convex optimization program;identification problem;noisy vector;maximum likelihood methods;blind identification problem canwe;global optimization problem;quadratic minimumrank approximation;blind identification;information theory;unknown input;only output measurements;single input;arx model;single output;impulse response;subspace;input;identification;feasibility problem;model;outputswe;constraints;unknown inputwe"}, "65c2a39f1579a947926ac5746888445ea4afdf6e": {"ta_keywords": "free grammars;novel neural models;pcfgs;formalism;representations;single model;context;unified framework results;dependencies;sparsity problem;constituents;stronger results;experiments", "pdf_keywords": "generative grammars;generative grammar;unsupervised grammar induction;grammar induction;generative constituent context model;free grammars;unsupervised parsing;dependency parsing;grammar induction benefit;free grammar;lexical dependencies;rich phrase structure;conditional grammars;constituency dependency trees;phrase structures;grammars;grammar formalism;rich lexical representations;phrase structure;grammar;novel neural factorization;grammar rules;computational linguistics;parse trees;likely parse trees;constituent;large corpus;neural representations;sentences;natural language text"}, "562f33611cdc0d8ed6609aa09f153e6238d5409e": {"ta_keywords": "clinical time series;diagnosis;data;predictive power;tests;accuracy;model;outcome;novel method;method;ability", "pdf_keywords": "clinical time series data;rnns;novel imputation strategy;recurrent neural network;recurrent neural network model;missingness indicators;neural network;rnn;clinical information system;missingness patterns;patient records;neural networks;imputation;clinical staff record measurements;patient logistic regression;missingness;diagnoses;data indicators;supervised learning;multilabel classification;predictive models;large dataset;clinical staff;data;diagnosis process;database;diagnosis;episodes;patterns;clinical decisions"}, "a36f7d5d8f724168e534925edff97b3680e545c9": {"ta_keywords": "activation tensors;trainable neural network layers;tensor contractions;contraction layer;significant model compression;image recognition;alexnet;model;popular networks;tcl;accuracy;new representation;data;performance;vgg;task;use;terms;cases;significant impact", "pdf_keywords": "large tensor networks;tensor contraction layer;tensor compression;trainable neural network layers;convolutional neural networks;deep learning;tensor contractions;tensor contraction;large scale image recognition;activation tensor;convolutional layer;tensor decompositions;tensor;neural networks;image recognition tasks;significant model compression;image recognition;low dimensional representation;supervised classification;convolutional transformation matrices;alexnet;tensor factorisation techniques;networks;alex network;convolution;more compact representations;classificationwe;essential features;accurate representation;training"}, "c2ff76c75acc777e005360e9d4c4d928d95c0432": {"ta_keywords": "reliable storage systems;optimum regenerating codes;storage systems;information storage;codes;networks;design;paper;field;central issue;fundamental task;selection", "pdf_keywords": ""}, "6c3b8e65dc45cb62172f9425dcff4c48055d47eb": {"ta_keywords": "textual features;visualization;language;food;social media;geographic locale;community characteristics;datasets;query;time query;predictive power;online system;analysis system;greatest predictive power;terms;insight;majority;baselines;connections", "pdf_keywords": "personal twitter feeds;social media posts;tweets;twitter;unstructured social media content;social media;topic models;food topics;accurate prediction;complex natural languagewe;lexical features;best prediction;hashtags;large corpus;temporal trendswe;linguistic patterns;temporal content;machine learning;textual information;corpus;text content;language;food;posts;unstructured text;political activity;many latent population characteristics;languages;machine learning framework;knowledgewe"}, "5e00596fa946670d894b1bdaeff5a98e3867ef13": {"ta_keywords": "language modeling;single prefix language modeling;textual representations;vision;weak supervision;training complexity;joint modeling;framework;end;scale;work", "pdf_keywords": "bidirectional attention;supervised captioning task;simple visual language model;captioning task;new representation learning model;representation learning;language models;novel generative representation;natural language supervision;image captioning;unidirectional language modeling;shot generalization;shot transfer tasks;vision tasks;deep bidirectional transformers;language benchmark;text tasks;pretraining;contrastive learning model;generative encoder;single prefix language modeling;unsupervised learning;single prefix language modeling objective;generative vqa representations;text benchmark;natural language;text captioningwe;language understanding;linguistic representation;supervised learning"}, "96bb4b49f69419c31857e928969fcaa137e15060": {"ta_keywords": "amazon product review system;review;reviews;answer;923k questions;generation;novel dataset;question;new task;models;novel method;number;strong baselines;approach;challenging nature", "pdf_keywords": "answer snippets;review document;form answer samples;answers;answer synthesize;machine reading comprehension systems;natural language processing;review;corpus;reviews;heuristic corpus;language models;knowledge;knowledgeable customer;relevant documents;relevant snippets;answer lengths;single document;helpfulness;classifier;semantics;evaluation;large dataset;empirical literature;task;empirical evidence;answer;novel probabilistic classifier;novel method;questions"}, "2873f78efd7adcb118a70f8ea3ca7fa1501e320a": {"ta_keywords": "shot relation classification dataset fewrel;shot relation classification models;above relations;domains;few instances;none;experimental results", "pdf_keywords": "shot relation classification;shot relation classification model;shot queries;shot classification task;relations;relation structure;relation patterns;relation;shot learning;shot learning model;shot models;relation extraction rate;shot domain adaptation;shot tasks;training domains;fewshot nota;annotations;shot nota;shot none;shot version;fewrel;new extra class;deep link adaptation;open challenge;query;data;domains;aspects;domain;biomedical domain"}, "3f311aee9d25b0284d21274cfc8706d6f0277f87": {"ta_keywords": "deep neural networks;quantization;policy gradient;several dnns;reinforcement learning;bitwidth;dnns;memory;operations;accuracy;network;nodes;algorithm;data;sensitivity;amount;effectiveness;method;novel method", "pdf_keywords": "deep quantization;optimal deep quantization levels;deep quantization framework;quantization;quantization levels;quantization bitwidth;deep learning;wise quantization bitwidths;deep learning pipeline;deep neural networks;deep neural network;deep reinforcement learning framework;quantization technique;quantization process;deep learning models;deep learning model;underlying network training;deep structured networks;convnets;neural networks;neural network;reinforcement learning;reinforcement learning episodes;specified training;networks;network datasets;large scale networks;learning problem;policy optimization methods;efficient heterogeneous bitwidths assignments"}, "bcd6cd7bdd661bd86c58b7251ae4633a6ba9979e": {"ta_keywords": "information retrieval;actual reviewing preferences;reviewers;recommendation;conference paper committee members;multiple information sources;papers;small focused sets;database technology;abstracts;web;committee members;combination;approach;method", "pdf_keywords": ""}, "705794a57cca12c2e58b2d77ac32bd4f92ed31ab": {"ta_keywords": "random graph models;random graph model;use crowdsourcing;gaussian;models;generalization;new generation;purpose", "pdf_keywords": ""}, "fd306df2809c7acc19dd1994e8ecb11caa33290d": {"ta_keywords": "coherent state;laser beam;laser;light;states;state;generation;frequency;method;new method", "pdf_keywords": ""}, "99e56ebc2f3739dfca93d5a92ebc1e6e2a3050d2": {"ta_keywords": "grading;online courses;machine learning;statistics;auto;hybrid approach;classical technique;problem;approach;setting", "pdf_keywords": ""}, "65b226f71faaac9b8a4d63445c85601a16635464": {"ta_keywords": "stochastic gradient descent;nonconvex optimization problems;gradient descent;convergence rate;stationary points;stochastic;algorithms;order;versions;problem;dimension", "pdf_keywords": "stochastic gradient descent;stochastic gradient descent algorithm;stochastic gradient descent method;gradient descent;nonconvex optimization;nonconvex optimization problems;gradient descent method;classical gradient descent algorithm;nonconvex optimization problem;stochastic gradients;stochastic gradient descent algorithm canwe;large gradients;hessian;gradient queries;stochastic gradient maps;nonconvex;stochastic approximation;iteration complexity;nonconvex problems;scale machine learning;gradient;saddle points;optimal points;semidefinite programming;convergence rate;markov learning;simple stochastic algorithm;global learning;gradient function;vector product"}, "c6bb9e4a9eaa0f0f8309597af2cefe03bd3f1bb5": {"ta_keywords": "chaotic forecasting models;chaotic systems;chaos;datasets;forecasting performance;novel dataset;dataset;dynamics;astro;feature analysis;fields;climatology;physics;properties;biochemistry;variety;degree", "pdf_keywords": "reproducible chaotic dynamics;available chaotic systems;nonlinear time series prediction;chaotic dynamical systems;chaotic systems;chaotic attractors;chaotic attractor;time series attractor;chaotic dynamics;nonlinear time series classification;nonlinear chaos;nonlinear forecasting;nonlinear dynamical models;chaotic system;classical chaos;single chaotic systems;novel time series encoder;nonlinear time series;time series models;single chaotic system;forecaster;symbolic regression;forecasting;dimensional attractor;forecasting models;multivariate time series prediction problem;neural ordinary differential equations tasks;unsupervised time series classification;simple chaotic system;time series model"}, "ede108538033ae00d1667685afbd488380020613": {"ta_keywords": "coronavirus;acute respiratory syndrome;viral epitope;omicron subvariant;viral population;ss;oc;subvariant;spread;significant factor;several variants", "pdf_keywords": ""}, "f9f862f48599526147bbb110ba986ff6872ef4b0": {"ta_keywords": "indoor trajectories;uncertainty;random process;trajectories;random nature;process;model;assumption", "pdf_keywords": ""}, "be0c64252a2c3071236d88feeab47d06ef6e0fb7": {"ta_keywords": "email header;recipient;email message;recipients;information;current contents;novel method;method", "pdf_keywords": ""}, "b661520bf0061b7d96ccf12016e351dd3a6ee780": {"ta_keywords": "deep linear networks;importance weighting;digital sky survey;network;sloan;sdss;sds;subtle effect;effect;efficacy", "pdf_keywords": "importance weights;deep nets;deep learning;typical deep networks;deep networks;deep learning models;importance weighting;deep network;unregularized neural networks;modern deep neural networks;regularization;importance weighting impacts;gradient descent;importance;training epochs;regularization methods;normalization;convolutional network;simple convolutional networks;attention;alsoimportance weighting;training examples;weighting;weight decay;optimizer;training data;parameterized models;weighting impacts;nets;accuracy evenwe"}, "2cd2df06d488565063e0600ff840d293be2eaf31": {"ta_keywords": "adaptive procedure;play;matching;game;new simple adaptive procedure;correlated equilibria;gretre;asymptotic behaviour;empirical distribution;set;simple model;procedure", "pdf_keywords": "stochastic game;stochastic games;stochastic players;strategic games;game theory;optimal strategy;random game;optimal payoff;optimal payoffs;simple game;diagonal strategy;strategy;games;play converge;past play;adaptive procedure;quadratic optimal payoff;strategies;positive regret;simple adaptive procedure;payoffs;play;game;payoff distribution;regret;payoff;intuitive observation;equilibria;average payoff;correlated equilibria"}, "372657f609f5a95b378a1aad7b08deb9b9b510c0": {"ta_keywords": "unsupervised adaptation;model adaptation;reinforcement learning framework;feature distributions;real data distributions;simulated data;novel model;return;integral probability;maximization algorithm;ipm;amp;framework;end", "pdf_keywords": "policy optimization;reinforcement learning;several benchmark continuous control tasks;continuous control benchmark tasks;policy gradient updates;model adaptation framework;model learning;model adaptation;arbitrary learning strategy;model predictive control;model adaptation procedure;model adaptation method;continuous control tasks;deep reinforcement learning problem;reinforcement learning problem;optimal learning;predictive control;dynamic models;stochastic actor;robotic control;learning;learning process;art model adaptation methods;large learning;behavioral policy;optimal markov chain length;probabilistic dynamics models;models;action data;dynamic model"}, "a0511f02a867bf19e2fa01e6cbd3663f4bd1b953": {"ta_keywords": "harmonic trap;dimensional harmonic oscillator;dependent schrdinger equation;dynamics;particles;interaction;system;time;case;results", "pdf_keywords": ""}, "1ae1850bcfa3c31d7bc828cc33f7dd3926cee26f": {"ta_keywords": "common knowledge base;entity associations;entities;random walk approach;links;novel system;system;presence", "pdf_keywords": ""}, "30cf652bd33049aaf111a5f84eb262a87c045bdb": {"ta_keywords": "worldcheckers check;timecheckers;features;subtle features;document;claims;world claims;world claim;claim;necessary information;performance;system;simple experimental setup", "pdf_keywords": "recent debates;debates;false claim list;natural language documents;assertions;claim validation;sentences;false claims;accurate evaluation;input sentences;journalists;politifact;debate;claim pairs;claims;past claims;several strong baselines;speeches;true content;new evaluation measures;task formulation;topics;accuracy;long document;documents;new dataset;comprehensive dataset;sentence;task;dataset"}, "31412f9b23511e212895305927d9ccddb445bcbc": {"ta_keywords": "voice timbre control parameters;voice voice timbre words;voice voice voice timbre words;voice timbre;acoustic basis vectors;voice;annotation;method;paper;choice", "pdf_keywords": ""}, "633ee881c594cface387557359ef13613d8eaef0": {"ta_keywords": "random assignment;egalitarian value;unrestricted cardinal utilities;finite objects;achievable value;optimal value;ordinality;agents;objects;properties;tradeoffs;value;accuracy;detailed experiments;bounds;truthfulness;problem;case;effect", "pdf_keywords": ""}, "1ccd031f28dccfb226f6c0c588c93a97a50bf95f": {"ta_keywords": "automatic code generation;arbitrary natural language specification;parallel machine learning models;apps;satisfactory python code;code;benchmark;introductory problems;models;test cases;machine;ability;set", "pdf_keywords": "code generation;code generation ability;code generation performance;natural language specifications;unstructured code;large scale language modeling;unrestricted natural language;access code benchmark;task descriptions;code;text generation;code performance;python programming language;machine translation techniques;scale language modeling;programswe introduce apps;scale language models;machine learning models;apps;programming exercises;programming progress standard;benchmark;languages;machine learning;python;python code;correct programs;codeforces;benchmark datasets;machine learning applications"}, "ac5e7f9bbc5d46bebc4ec5616aba9d014a6d237f": {"ta_keywords": "introductory computer science courses;science fiction;student guides;student discovery strategy;student guide;curriculum;artificial intelligence;subject;reading;field;tool;set;recent research;use", "pdf_keywords": ""}, "615b823d1fc9548ce384f1bb4f544445175e8537": {"ta_keywords": "univalent polycrystalline;polycrystalline layers;layered crystal;monovalent", "pdf_keywords": ""}, "556a4a0b5fcda4d9f9fad637f2655aeb1b1a00b2": {"ta_keywords": "paralinguistic information;input speech;target language speakers;speech;emphasis;information;method", "pdf_keywords": ""}, "d6fc0fcf0764065f6e58c57ca850abfdd918504b": {"ta_keywords": "minimum error rate training;large corpus;sentences;intersentence features;linear scoring model;high quality documents;log;central engine;mst;shannon error;paper;context;monte carlo method;model;system", "pdf_keywords": ""}, "7de4a82edf68b69a9c007fe8e840edf4ade1171c": {"ta_keywords": "enzyme ficin;arginine derivatives;other enzyme sulphydryl enzyme;arginine;enzyme;acetylation;free arnino;kinetics;stability;optimal conditions;activity;effect;results;groups;use", "pdf_keywords": ""}, "2cae732250b59f9e2238626d8d7e0064b97de3c9": {"ta_keywords": "wavelet transform domain;signal reconstruction;reconstruction;iterative algorithm;linear simultaneous equation;new representation;representation;image;dimensional case;applicability;terms", "pdf_keywords": ""}, "616c15dd765c36c21efc75c7ed52e5af81c21053": {"ta_keywords": "quantum computing architecture;quantum system;universal;architecture;generation;combination;new method;use", "pdf_keywords": ""}, "bf9b069242f0af129c2aad8430a52454b008c327": {"ta_keywords": "stochastic gradient descent;learning rate;dependent stochastic differential equation;sgd;linear rate;dependent sde;learning;rate;objective functions;time formulation;convergence;surrogate;analysis;broad class;general theoretical analysis;effect;use", "pdf_keywords": "stochastic gradient descent;stochastic learning rate;generalized gradient learning rate;stochastic gradient model;arbitrary learning rate converges;stochastic gradient algorithm;stochastic gradient dynamics;stochastic optimization;arbitrary learning rates;learning rate;gradient descent;stochastic optimization parameter;stochastic optimization problems;stochastic optimization technique;discrete stochastic optimization methods;optimal stochastic partial differential equation;stochastic optimal solution;stochastic approach;novel stochastic model;stochastic differential equation;novel stochastic differential equation;sgd;generalized gradient method;stochastic iterative algorithm;convergence rate;accelerated gradient methods;nonlinear stochastic differential equation;convex optimization;stochastic partial differential equation;stochastic noise"}, "250f8f71f7cff972a70482229ca9053b356217cd": {"ta_keywords": "online unsupervised voice activity detection;bayes model comparison;phenomenological model comparison framework;conventional statistical methods;statistical scheme;slac national database;vad;experimental evaluation;significant improvement;method;paper;conventional method", "pdf_keywords": ""}, "3d2ceea5dea234ae9a20f8e1c9e558735757e90e": {"ta_keywords": "multilingual acoustic texts;dependent phoneme distributions;allophone recognizers;accuracy;model", "pdf_keywords": "multilingual acoustic classification;multilingual acoustic recognition;multilingual acoustic modeling;multilingual acoustic models;acoustic phone recognition;automatic phone recognition;phone recognition accuracy;specific phonemes;scalable phoneme model;speech recognition;phone utterances;phoneme assignment;automatic speech recognition;phoneme model;phonology;phoneme inventory;spoken languages;allophone mappings;phonemes;acoustic maps;allophones;dependent phoneme inventory;allophone layer;different languages;phoneme;private phoneme model;unseen languages;languages;language specificity;phone distributions"}, "8234049255a0e03fc745457de456634d1aab214b": {"ta_keywords": "wrapper learning;web pages;web page;structure;information;simple method;method;use;user", "pdf_keywords": ""}, "e11b4750e288785134f042c144f057a11dc0180a": {"ta_keywords": "elections;several representative systems;dynamic representations;representatives;dynamic representation;voters;dependent weights;model;performance;novel form;hybrid;issue;significant improvements;set;context", "pdf_keywords": "flexible representative democracy;interactive democracy;finite voting;voting systems;interactive democracy proposals;optimal voting scheme;direct democratic system;representative choices;linear voting;representative democracy;election process;democratic voting;democratic election rule;voter castigation process;voting process;election rules;direct democracy;democratic process;local elections commission;local election commission;flexible delegations;traditional direct democracy;voting;expert representatives;representative systems;representatives;representative;election;democracy;voters"}, "b8b813111c411ae61881ab9cd25707d9de6444ec": {"ta_keywords": "compositional attention outperforms;compositional attention;novel attention mechanism;attention blocks;attention;transformer model;tasks;task;standard head structure;key combination;value pairing;mechanism;query;manner;variety;soft competition stage;series;numerical experiments", "pdf_keywords": "compositional attention;head attention model;novel attention mechanism;attention;retrieval tasks;attention mechanism;key attention;attention mechanism decomposewe;search retrieval tasks;popular attention mechanism;retrieval task;several tasks;task structure;retrieval operations;multiple parallel retrievals;multiple tasks;retrieval mechanisms;retrieval coupling;novel compositional model foremergent objects;flexible search;retrievals;retrieval;retrieval features;various tasks;multiple search;retrieval pairings;tasks;search redundancy;retrieval pairing;dynamic retrieval"}, "a469f2ec3ab15f20f06d95aea1839b1263d3385e": {"ta_keywords": "random assignment mechanisms;different random assignment mechanisms;egalitarian welfare aspects;unrestricted cardinal utilities;achievable value;ordinality;optimal value;freeness;envy;agents;properties;objects;truthfulness;bounds;effect", "pdf_keywords": "random assignment mechanisms;different random assignment mechanisms;arbitrary allocations;symmetric symmetric utility mechanism;optimal allocation;random allocation;fair allocation;utility allocations;allocation;random assignment;utility maximization problem;optimality;indivisible goods;indivisible objects;least utility;egalitarian welfare aspects;random serial dictator;deterministic mechanism;unrestricted cardinal utilities;utility;assignment problem;preferred object;preferred objects;ordinal preferences;envy;probabilistic serial;egalitarian value;proof allocation algorithm;cardinal utilities;random permutations"}, "18f4ec53a4221a97e1482f091f41a23f3d873cf2": {"ta_keywords": "few evidence annotations;supervision task;supervision tasks;supervision;abundant labels;evidence;robustness;document;existence;new approach;method;efficacy;approach", "pdf_keywords": "evidence sequence labeling tasks;evidence extraction;text sequence classification;rapid evidence extraction;abundant evidence annotations;evidence annotations;text classification tasks;text classification;generating evidence;thebrilliant evidence thatbrilliant text documents;prediction tasks;natural language processing;supervised approaches;annotations;text documents;supervised attention;natural language;interpretability literature;weak supervision;extraction;thatbrilliant text documents;extraction modules;extraction model;supervised manner;strong supervision;evidence;ofbrilliant evidence;propaganda detection;extraction process;document"}, "c4536a5c7f47bfc48df202ba882002531248f955": {"ta_keywords": "acoustic excitation sounds;fundamental frequency patterns;actual physical electrolarynx;electrolarynx;control method;prototype system;statistical prediction model;method;performance;time;paper", "pdf_keywords": ""}, "f43ae70242aea3dbb80b7c3b5474356e9ee9079b": {"ta_keywords": "natural language;complexity;random variables;large number;small number;evaluation;method;new method;presence", "pdf_keywords": ""}, "72c9663494827b2e87ad5a65a6ff7e769eb15a57": {"ta_keywords": "new reinforcement learning models;novel evaluation criteria;generative data;story;loop;model;art baselines;criteria;better performance;experiments;essence;traditional metrics;demonstrate;state", "pdf_keywords": "visual story generation;story generation quality;visual storytelling task;text generation;unstructured text stories;stories;natural images;imitation learning;photo stream;bidirectional neural networks;convolutional neural network model;sentences;description;policy gradient;semantic structure;coherent story;human evaluation;story;reference sentences;reinforcement learning framework;neural network;neural networks;relevant stories;generation;images;gradient descent;good story;quality;quality evaluation criteria;image"}, "15bb07d0996ece844de8cae24d3dc15972e6841a": {"ta_keywords": "popular summarization datasets;art summarization systems;automatic metrics;datasets;reliability;quality;models;insights;performance;system;framework;impact;samples;state", "pdf_keywords": "summarization datasets;automatic summarization metrics;popular summarization datasets;summarization system;text summarization process;summarization;abstractive text summarization;abstractive sentence summarization;summaries;machine translation;semantic content;automatic metrics;annotated samples;news articles;natural language processing;text;neural attention models;data quality;metrica novel approach;metrics;cnn;text matching;available metrics;datasets;dataset;empirical similarity;document;computational linguistics;complete context;modelcentric evaluation"}, "674833d48a77ef009f751a66988590592dd5d996": {"ta_keywords": "orbit coupling strength;orbit coupling;zeeman field;electron;spin;effect", "pdf_keywords": ""}, "49ee2270f3265ee27b36e05e130be79e05d5ba29": {"ta_keywords": "learning problem;textbooks;parallelization;parallel algorithm;text;novel approach;approach;machine;problem;technique;use", "pdf_keywords": ""}, "68ea2572584068befd441dccf461f3444ff14f4a": {"ta_keywords": "physical student agent;human learning process;skill knowledge;toe game;student;physical world;method;ability;users", "pdf_keywords": ""}, "4cfc7d3c6a61f6db48b1f3c75235592c1609a54f": {"ta_keywords": "transcription interface;iterative interfaces;interface design;effects models;considerable quality gains;user study;quality gains;variations;conclusions;choice", "pdf_keywords": ""}, "e2ac96254d7e9ec0dde882e3a09797d00f26220f": {"ta_keywords": "spin dynamics;spin;orbit interaction;valve system;orbit;system;effect", "pdf_keywords": ""}, "757acf616a38422c7186952e1075a28fed1a07c0": {"ta_keywords": "amino acid neurotransmitters;anesthetic xylazole;fetal rat nerve cells;nitric oxide;signaling pathway;cgmp;ferromagnetic component;pathway;optimization;likelihood approximation;concentrations;effects;study", "pdf_keywords": ""}, "fc912e9af47bf10428396b687b2bfb1e5832fcb1": {"ta_keywords": "connectionist temporal classification;automatic speech recognition;efficient auxiliary loss function;gc encoder network;word error rate;intermediate loss;various corpora;asr;gc;intermediate layer;training;code;performance;inference;method;objective;small overhead;small modification", "pdf_keywords": "connectionist temporal classification;network learning;automatic speech recognition;nonlinear speech recognition;end speech recognition problem;efficient auxiliary loss function;ctc encoder network;stochastic depthwe;autoregressive decoder;networks;simple intermediate losswe;stochastic depth;external language model;stochastic approximation;intermediate ctc loss;network;new intermediate loss;layers;nonlinear attention pulses;training objective;network science;wsj corpus;intermediate layer;ctc;training;sequence model;non autoregressive intensity;word error rate;loss;losses"}, "f3132572bb3870dbe99b2d1c01ce17fa38783a2f": {"ta_keywords": "power spectrum;power pulse;generation;method;new method", "pdf_keywords": ""}, "213e471bacff5c0852943988fcb955797f1e591f": {"ta_keywords": "automatic reference translation;effective reference formulation;human evaluation;references;metrics;systems;value;methodology;variety", "pdf_keywords": "machine translation evaluation;machine translation research;human reference translations;single unbiased reference translation;machine translation;reference translations;reference quality;translation qualities;human translation;parallel machine translation;reference sentences;original reference translations;translation model;translationese artifacts;several automatic metrics;automatic metrics;translationese language;automatic metric;english german news translation task;paraphrasing;human evaluation;german evaluation campaign;translation;empirical evaluation;evaluations;diverse references;more natural language;reference sets;standard references;evaluation"}, "77568c594470f9aa029f92774e2c12ab0451d9bb": {"ta_keywords": "robust language models;topic cvar;topics;mixture;topic;novel statistical method;news;reviews;conditional value;risk;loss;training;model;idea;novel approach;approach;factor", "pdf_keywords": "robust language models;robust language;robust learning model;training distribution;arbitrary test distributions;robust languages;standard maximum likelihood training;test distributions;novel language model;language model;potential test distributions;test distribution;yelp review corpus;stochastic gradient descent methods;new robust objective;conditional distribution;robust optimization;robustness;mle;corpora;machine learning;test performance;test mismatch;case distribution;sentences;baseline distribution;text;risk;topic cvar;distributions"}, "89c64fd60ca58f4753a818cd0923f5041b51a807": {"ta_keywords": "relay network cost objective;sink interconnection;routing;sensor;optimal solutions;sink;available paths;line;decision process;location;goal;point;problem;start", "pdf_keywords": ""}, "612d577534dbbf546405d4036d912666523a8164": {"ta_keywords": "polynomial learnability;description logics;learnable sublanguage;tractable learning;expressive power;examples;vocabulary;order logic;positive examples;restricted subset;subset;size", "pdf_keywords": ""}, "0d516b476559485e04290e859ca59101c0a91ae1": {"ta_keywords": "millimeter wave;partial observable decision process;optimal threshold policy;mm wave;stationary threshold policy;optimum distance;relay;uncertainty;wave;relay link;pdm;state;ues;framework;user equipments;approach;novel approach;quality;actions", "pdf_keywords": "millimeter wave networks;relay link selection;relay selection problem;relay;relay link design;relaying link;novel relay link design;relay link;relay link design model;novel relay scheme;relay region;wireless networks;quantum networks;wireless links;additional exploration probe packets;new relay links;millimeter wave;wavelength barrier;dynamic obstacles;directional beamforming technologies;mmwave signal;dynamic obstacle exploration;obstacle blockage;network;mmwave;wireless sensor networks;dynamic obstacle;obstacles;data transmission;data packets"}, "99fe5475ab28fa7ad4bce51d7b294b3f40caad4d": {"ta_keywords": "harmonic trap;einstein condensate;dimensional bose;level atom;condensate;trap;atom;dynamics;gaussian field;interaction", "pdf_keywords": ""}, "31392ad8722d9c66181b621936e2013199e02edc": {"ta_keywords": "language model knowledge;nlu tasks;model ability;semantic features;models;words;relative acceptability judgments;study;data volume;tuning;maximum number;growth;respect;different approaches", "pdf_keywords": "large pretrained language models;language models;normal language approximation;linguistic representations;linguistic generalization;linguistic features;linguistic representation;language understanding;minimum description length;semantic featureswe;supervised learning tasks;nlu task performance;classification tasks;nlu tasks;nlulp;learning;linguistic extension;dimensional nonlinear programming;computational challenges;complexity;natural words;simple decoder;simple nonlinear programming;simple classifier;linguistic research;computational models;textuality;pretraining;large class;features"}, "6c82727731955a2332a0cc38ec56b35a971061eb": {"ta_keywords": "machine translation;speech recognition;toolkit;tasks;experiment configuration system;design;utility", "pdf_keywords": "neural machine translation;standard machine translation benchmark;machine translation;novel open source toolkit xnmt;recurrent neural networks;deep learning;speech recognition tasks;scale machine learning tasks;transcription;speech recognition;coherent language processing tasks;lium corpus;simple simple sequence encoding;neural network;xnmt;toolkit;nmt;bidirectional encoding;training framework;patternsin;inference framework;standard modeling tools;simple patterns;inference;pattern matching;learning rates;translation;extensibleneural information processing;neural signal processing architecture;new models"}, "7e406537f52528527d10872d1807ad974599b13a": {"ta_keywords": "neural network;patterns;parallel search;pattern;input text;signatures;results;form;scale", "pdf_keywords": ""}, "5babe5334c6867db13fa7e6943f64059c7cba6ce": {"ta_keywords": "harmonic potential;wave function;particle;dynamics;simple model;model;function;concept", "pdf_keywords": ""}, "c0cce8955bf10b21753161ffaa1978a7c8b78a16": {"ta_keywords": "special bodyconductive microphone;nam microphone;statistical voice conversion;conductive microphone;optical network;signal;estimation;enhancement;model;air;input;nam;framework;vc;north american;use", "pdf_keywords": ""}, "6027ef3b4e5585b45db0b9d333956425d3972351": {"ta_keywords": "commonsense reasoning benchmarks;commonsense reasoning;knowledge facts;multiple new answers;sourcing;crowd;strong baseline methods;approach;large margin;test question;novel model", "pdf_keywords": "open commonsense reasoning;commonsense reasoningwe;commonsense reasoning research;commonsense reasoning;commonsense knowledge;knowledge graph;reasoning paths;neural query language;commonsense datasets;neural fact index;underlying knowledge;common reasoning;commonsense;domain reasoning;structured knowledge;sense reasoning;reasoning process;reasoning;ended reasoning;fact vectors;commonsense question;many questions;dense fact index;knowledge;natural language processing;knowledgewe;large decision space;differentiable reasoning;complex open domain questions;multiple answer lists"}, "a38e0f993e4805ba8a9beae4c275c91ffcec01df": {"ta_keywords": "program synthesis;general purpose programming languages;large language models;program;models;tuning regimes;output;best models;specific input;good performance;shot;others;collection", "pdf_keywords": "program synthesis tasks;program synthesis;program synthesis datasets;program synthesiswe;general purpose programming languages;new programming language;programming languages;purpose language models;short computational programs;large language models;generative language models;short programming functions;language models;synthesis tasks;short programs;simple programs;short natural language descriptions;statistical language models;language model;synthesis task;natural language descriptions;program;user programming;synthesis performance;natural language;natural language systems;optimal synthesis performance;thewe examine synthesis performance;small programs;software engineering"}, "4f4da6fdb9496b0295764b2db11381dd390de02d": {"ta_keywords": "speech transcripts;sensitive manual correction;higher human supervision efficiency;segmentation;fly user modeling framework;faster algorithm;cost;segments;fly updates;previous approaches;baseline method;framework results;sensitive fashion;experiments", "pdf_keywords": ""}, "c581686edbd7227e9eb4a0841cce16728ca27369": {"ta_keywords": "machine comprehension system;instructional recipes;comprehension;recipeqa;linguistic structure;linguistic aspect;task;text;questions;process;dataset;modalities;goal", "pdf_keywords": "multimodal machine comprehension;recipe descriptions;synthetic natural language questions;comprehension tasks;recipe description;recipe images;modal descriptions;natural language;recipes;multiple descriptions;recipe titles;comprehension task;recipe;recipeqa;machine reading tasks;descriptions;cooking recipes;comprehension;recipeqa dataset;20k recipes;procedural knowledge;recipewe;perfect recipe;single recipe;natural context;questions;linguistic structure;lexical representations;task;modality"}, "ad48174ccbff6259a7d3cb0d0985e5aefa314b84": {"ta_keywords": "different machine translation systems;level machine translation systems;machine translation;morphological properties;natural language processing;mtm;underlying machine;systems;different character;performance;same input beam;comparative study;domain;context;comprehensive survey", "pdf_keywords": "characterlevel natural language processing;level machine translation;computational translation;character processing architecture;machine translation tasks;computational translation task;neural machine translation system;current character models;wecomputational linguistics;machine translation system;novel character processing architecture;machine translation;neural machine translation;automatic translation;subword systems;dimensional character processing;givenneural machine translation;subword models;deep bidirectional machine translation task;subword model;evaluationneural machine translation;decoder;character level;dimensional electron gas;elementary particle character;level sequence processing;translation quality;electron gas;next decoder state;text transformer"}, "1e3e2b03e28f48bb4d48154992cd6b62969c643e": {"ta_keywords": "protein characterization;characterization;future;domain;challenges;brief overview", "pdf_keywords": ""}, "7b51209e7d9cbedc18b6ab202e6fcdabaebbb088": {"ta_keywords": "discriminative language model;discriminative language modeling problem;linear neural networks;log;word;accuracy;essential structure;application;advantage;approach;new approach;use;experiments", "pdf_keywords": ""}, "795aca47df94300fa6bfd464e6873aef56c7f3ae": {"ta_keywords": "multilingual semantic network;semantic relations;multilingual version;synsets;individual word senses;tool;effective extraction;large scale;output format;release;architecture;use cases", "pdf_keywords": ""}, "4cd66273298128dfb5be290e891870085ecfc455": {"ta_keywords": "joint np;algorithm;np", "pdf_keywords": ""}, "7099d5a4b2d4ed47905071fc23aff08580401e42": {"ta_keywords": "echo state network;recurrent neural networks;automatic speech recognition models;rnns;efficient training;esn;models;storage;subset;counterparts", "pdf_keywords": "deep echo state network;recurrent network;recurrent neural networks;deep encoder;deep decoder;deep hybrid neural network;simple recurrent layer;deep neural networks;echo state network;novel automatic speech recognition models;deep reservoir computing;encoder;encoders;rnn;echo state;neural network;standard speech utterances;random matrix network;esn models;decoder;random matrix model;deep reservoir;random weight matrices;esns;esn;gaussian states;neurons;random generating;prediction;hidden state"}, "af460a6b3ecaddd4015b34255564c366ecfef802": {"ta_keywords": "probabilities;open problems;algorithm;closed system;simple algorithm;random variable;classical computer system;generalization;problem;problems;evaluation;wide class;fact", "pdf_keywords": ""}, "5861dbfcb253ca02067dd182d42b7d567433c834": {"ta_keywords": "frontdoor estimators;confounders;estimators;estimator;mediators;optimal estimator;backdoor;data;unbounded constant factor;sample variance;model parameters;variables;methods", "pdf_keywords": "causal inference;frontdoor estimators;optimal confounders;causal models;confounders;backdoor estimators;frontdoor estimator;classic frontdoor estimator;new confounder;backdoor estimator;measurement confounder;causal model;classical frontdoor estimator;causal graph;econometrics;backdoor effect;empirical prediction;frontdoor;estimators;sectional estimator;backdoor;empirical data;mediators;estimatorswe;skewness estimators;graphical models;regression;additional conditional independences;mediator;estimator"}, "0f655f0e1937ad19b038952e2df69e30d447aac8": {"ta_keywords": "clinical time series;missingness;recurrent neural network model;time series;novel statistical model;disease;data;binary model;model;information;presence;problem;concept;combination;utility", "pdf_keywords": ""}, "4857e0e3d720b87b4523a6435cc166bcb7ae328a": {"ta_keywords": "artificial neural networks;single neural network;neural networks;networks;new method;method;generation;analysis;large number;results", "pdf_keywords": ""}, "8ff620f704a4151fd7abba1db792463fbd32bfe5": {"ta_keywords": "short documents;abstractive summarization problem;salient sentences;long documents;salience detection baseline;sentences;perplexity scores;reduction;novel algorithm;significant improvement;novel method;improvement;source;identification;approach;number;conventional methods;model", "pdf_keywords": "novel summarization model;abstractive summarizers;long legal briefs;coherent summary sentences;complex legal documents;summary sentences;abstractive source document;salient sentences;sentence complexity;document documents;average source document length;sentences;documents;coherent summary;source document;language model perplexity;sentence salience;complex legal case;document;salience classifier;natural language understanding;complex legalwe;quantization;summary;large collection;novel quantization approach;novel extraction;nlo datasets;court;case cases"}, "baf34ac4080a365a7cec30b6877fa1a018eb31cf": {"ta_keywords": "joint passage retrieval;novel autoregressive reranker;multiple passages;better answer coverage;passages;joint modeling;datasets;prior approaches;combination;efficient manner;model;set;state;new model;same question;art approach", "pdf_keywords": "joint passage retrieval retrieval;joint passage retrieval model;joint passage retrieval;passage retrieval;answer generation model;retrieval recall;autonomous passage retrieval problems;retrieval;distinct answers;natural search tasks;multiple answers;answer generationwe;passage encoder;reranker decoder;multiple passages;passage reranker;passage encoder fp;recall;answers;decoder;passages;natural language question;passage;single answer;relevance;new answer;dynamic oracle training;positive passages;task;reranker training technique"}, "f4465442a9b850a2c5b71a63fff0d24396b15f2c": {"ta_keywords": "dimensional electron gas;electron gas;harmonic trap;electron interaction;electron density;electron;dynamics;terms", "pdf_keywords": ""}, "7fb1262d4484732c8f7295fa5fb5e6ed6eabb6a0": {"ta_keywords": "reserve electricity market model;reserve electricity markets;transmission representation;detailed transmission representation;energy;simulation;methodology;novel;paper;use", "pdf_keywords": ""}, "9f208842f70503e8b71fd4c34ba682dcd0ea4788": {"ta_keywords": "agents decisions;incentives;strategic decision makers;adaptive control theory;adaptive design;strategic interactions;agents;other agents;noisy cases;data;design;novel approach;approach;response;convergence results;use", "pdf_keywords": "adaptive incentive design;adaptive incentive design problem;incentive game;incentive design;incentive design problem;incentive design problems;incentive parameters;incentive design step;incentive design process;asymptotically incentive compatible;utility learning problem;agents utility functions;utility learning algorithm;incentive;utility learning;adaptive players;nash equilibrium concept;incentive basis functions;incentives;game theory;optimal strategy;evolutionary game;game theory framework;stable differential nash equilibria;noncooperative game;agent responses;strategic decision makers;games;strategic interactions;utility"}, "f136a0fdc2065485c83396ae41d431395de51af4": {"ta_keywords": "candidate reviewers;icml reviewers;reviewers;suitable reviewers;large conferences;review pipeline;principled selection;attractive features;comparative study;novel approach;pool;principled features;small number;approach;problem", "pdf_keywords": "novice reviewer selection;qualified reviewers;icml reviewer pool;novice reviewers;reviewer pool;main reviewer pool;traditional reviewer pool;reviewer groups;new reviewers;reviewers;experimental reviewers;nominated reviewers;experimental reviewer;reviewer;peer review;reviewer pairs;artificial intelligence conferences;review process;conference discussion process;icml conference;real ml conferences;participants;submissions;conference;review;large conferences;mentoring mechanism;research participants;evaluation;mentoring"}, "b24e2c3983c3207b1c7124c48d691cf459a3197b": {"ta_keywords": "discrete models;discrete equations;machine learning;information processing;signal processing;applications;examples;technology;comprehensive set;comprehensive guide;many areas;use;science", "pdf_keywords": ""}, "0c0d9ecde0efead75e15353ac6c179c4fc22bdda": {"ta_keywords": "dimensional nonconvex strategy spaces;local equilibria;continuous games;differential equilibria;such equilibria;strategies;sufficient conditions;tutorial examples;unified framework;results", "pdf_keywords": "nondegenerate differential nash equilibria;local nash equilibria;local differential nash equilibria;differential nash equilibria;strict local differential nash equilibria;continuous games;dimensional strategy spaces;differential nash equilibria andwe;local equilibria;nondegenerate differential game forms;degenerate differential game forms;differential game forms;degenerate differential equilibria;local optimality;global equilibria;differential equilibria;equilibria;optimal control;playera game;equilibrium;games;different equilibrium behavior;nonlinear evolutionary dynamics;player costs;strategies;players;strategy;game;thermodynamic equilibrium;dimensional dissipative interaction"}, "e8b026b36d8be73ed428f7e4e55c26b27c34a544": {"ta_keywords": "bifunctional electrochemical sensor;electrochemical sensors;electrochemical sensor;electrochemical response;electroactive responses;periodic potential;colloidal crystal;molecule;macromolecule;simultaneous measurement;performance;such devices;target;strength", "pdf_keywords": ""}, "f20d7185c47ce55cdcd9b839ef6fce595baba029": {"ta_keywords": "lorentz force;presence;effect;ones", "pdf_keywords": ""}, "3ebed41fa35e5902b692a3e380c7c9a035c04426": {"ta_keywords": "modern artificial intelligence;machines;science fiction;advanced control systems;teaching strategy;creep;image processing techniques;students;principles;strategy;approach;tool;relevance", "pdf_keywords": ""}, "6b02fe6e0f6b2120a08e098513511e15a05f9073": {"ta_keywords": "datasets;data;contamination;qualitative behavior;shifts;shift;novel approach;approach;level;ability", "pdf_keywords": "dimensional datasets;dimensionality reduction methods;dimensionality reduction;dimensional data;domain classifier;anomaly detection;classifiers;classification;principal component analysis;shift malignancy;shift detection accuracy;shift detection;datasets;box dimensionality reduction;machine learning;classifier;dataset;shift intensities;modern machine learning models;large shifts;benign shifts;image datasets;detection;thresholding;large shift;domain classifier classif;original dataset;empirical data;distribution detection;sparse random projections"}, "3483d04a89dd69afd7b1393eadd8e8e4c5376d59": {"ta_keywords": "unsupervised conceptual clustering algorithm;classification;algorithm;coupled trees;trees;small domains;approach;new approach;system;accuracy;problems;period;variety;use", "pdf_keywords": ""}, "b1a8c6de4fbfe485c8f1c7723404467b72788ff2": {"ta_keywords": "machine learning;modern medical treatment paradigm;patients;treatment;machine;recent progress;tremendous progress;mechanisms;better understanding;many fundamental questions;recent years;challenges;tremendous success;past decade;context;need;talk", "pdf_keywords": ""}, "b6b76f529d273a35180d0dc65912db1538539067": {"ta_keywords": "metadata;same semantic space;text;label scarcity;datasets;training samples;original document;generative process;bottleneck;framework;effectiveness;experimental results;development;approach;wide range", "pdf_keywords": "metadata;various metadata;supervised categorization;useful metadata;metadata metadata metadata repositories;topic label;text classification problem;training data generation;corpus;generative embedding;generative data;tags;unstructured data;embeddings;same semantic space;embedding;training data;training data generation module;text content;unstructured database;unstructured data set;labels;classifier;generativewe;neural classifier;label scarcity;metacat;heterogeneous information network;minimal supervision;label scarcity problem"}, "ac713aebdcc06f15f8ea61e1140bb360341fdf27": {"ta_keywords": "model extraction;language model;adversary;bert;api watermarking;natural language processing;real training data;victim model;model;classification;defense strategies;attack;only query access;local copy;problem", "pdf_keywords": "model extraction;attacker model;natural language processing tasks;language model;api watermarking;random query generators;bert;public bert repository;adversary;underlying knowledge representation;natural language processing;query generation model;deep neural models;metastable knowledge representation;real training data;extraction;random queries;bidirectional representations;victim models;simple defenses;models;original model predictions;nlp;membership classifiers;deep networks;victim model fine;query generation process;corpus;good models;attacker"}, "3671dabbfd2e854060e1e382bad96b6bb00fcb46": {"ta_keywords": "dictionary adaptation approach;noise exemplars;speech recognition;speech exemplars;target noise;unknown noise;context;approach", "pdf_keywords": ""}, "6bea71fa6deb19c67e9586428f8f240e789fb3df": {"ta_keywords": "stochastic optimality condition;stochastic optimality conditions;linear stochastic;auer;empirical analysis;algorithm;powerful tool;sfb;novel modification;construction", "pdf_keywords": "stochastic bandit problem;various stochastic bandit problems;bandits;optimal decision;stochastic;confidence sets;stable random noise model;learning process;learning model;stochastic models;random noise models;uncertainty;stochastic regression models;supermartingale;constant regret;optimism;martingale;optimal actions;continuous variable learning;optimal control;empirical performance;martingales;novel tail inequality;stochastic regression parameters;regret;algorithms;algorithm;high probability;probability;threshold"}, "2f0221142db900e75bd9c54fa153fb770a72f672": {"ta_keywords": "shape;powerful method", "pdf_keywords": ""}, "7dce2877758b0103d1f7a454c184dc641e123359": {"ta_keywords": "document retrieval;structured queries;scale document processing;new software suite;performance evaluation;processing;software;report;context", "pdf_keywords": ""}, "309b2c75dcdafea19a053876e56cef9747d428fb": {"ta_keywords": "lattice point source;attention model;speech translation task;lattice inputs;new lattice model;lattice structure;lattice algorithms;lattice;positional embeddings;positional inputs;position;model;task;self;extension;design;method;important step", "pdf_keywords": "attentional lattice encoders;attention lattice model;new neural lattice model;lattice inputs;model lattice inputs;lattice model;lattice;lattice structure;simple lattice model;attention models;different lattices;lattice data structures;lattice positions;attentional models;global lattice structure;lattice scores;generative lattice model;lattice reachability;lattice nodes;generalized attention models;sequential sequential lattice rhodes;attentional encoders;focus lattice model;latticewe;lattices withwe;recurrent neural networks;identical lattices;sparse lattice model;different lattice scores;sparse lattices"}, "975551547fef77605fb85a551bbd7523b77746b7": {"ta_keywords": "supervised hierarchical classification;hierarchical classification framework;hierarchical classification methods;topic label modeling;underlying supervised classification framework;higitclass;structured data;framework;novel framework;features;idea;available repository;novel;work", "pdf_keywords": "novel repository classification task;keyword enrichment;repository classification task;keyword enrichment module;supervised hierarchical classification;hierarchical classification;unstructured data;word embeddings;semantic contentwe;encode structured information;supervised classification;dataless text classification systems;github repositories;keywords;structured data;word nodes;structured information;hin embeddings;classification;heterogeneous information network;homogeneous information networks;topic distributions;categories;large scale data;supervised classification algorithm;embeddings;hin repository;single keyword;corpus;spherical topic modeling"}, "46ed42e4318e1363a0ec3dde195422cdfecf2017": {"ta_keywords": "phrase embeddings;empirical value;quality;novel approach", "pdf_keywords": "accurate phrase embeddings;powerful phrase embeddings;dense phrase representations;paraphrase generation model;semiparametric phrase embeddings;diverse phrasal paraphrases;sentence embeddings;sentence representations;phrase similarity;underlying sentence representation;complex phrase semantics;phrase semantics;lexical phrase types;semantic relatedness tasks;phrases;neural topic models;sentence representation;accurate topic embeddingswe;negative phrases;wordlevel models;length phrases;natural neural topic model;sentence semantics;natural language processing;language models;neural topic model;embeddings;phrase cues;semantic relatedness;meaningful vocabulary"}, "d4af2654f97c09741aba9f0da9ace7bc84b9a63f": {"ta_keywords": "probabilistic network;model paths;model situations;citylink center;social service provider;novel event;poverty;paths;continuous time;clients;ectbn;occurrences;cincinnati;system;various types;representation;power;usa", "pdf_keywords": ""}, "73a6e4574de038878be1bbb5985400998e420a5b": {"ta_keywords": "strategyproof assignment;assignment quality;strategyproofness;conference peer review;assignment;quality guarantees;time algorithms;methods;dataset;order;compromise;price;amount", "pdf_keywords": ""}, "78438b61afc2c9123c28ca4d6b58e598462ae9be": {"ta_keywords": "adversarial training process;adversarial classifiers;target representations;benchmark datasets;toy dataset;mda;sentiment analysis;clustering;min optimization;empirical results;art methods;method;approach;state;max", "pdf_keywords": ""}, "1f0446dddd192e94f3930a3a449bd89796f4200f": {"ta_keywords": "single feature transformation matrix frame;multiple transformation matrices;general linear regression;linear regression;matrices;fmllr;feature;gmm;alignments;frame;space maximum;novel method;framework;weighted sum;pass;second problem;paper;method;results;addition", "pdf_keywords": ""}, "2226560f94c1e90d6900d4674b649cc5522b78cc": {"ta_keywords": "multilingual model;improved translation accuracy;different target languages;different languages;parameters;model;methods;effect", "pdf_keywords": "multilingual neural machine translation;multilingual translation tasks;multilingual translation task;multilingual machine translation;neural machine translation system;translation models;many translation task;multilingual translation;multilingual encoders;multiple separate translation models;multilingual parameter sharing approach;multiple target language translations;attentional encoder;machine translation;multilingual translationwe;decoder attention;attention parameters;attentional model;translation task;translation accuracy;target language pairs;multiple target languages;language pairs;target language;neural machine;translation problem;target languages;shareable sublayers;arbitrary target languages;translation"}, "04b91791225a4f86b0715b41c6f56c00c197d810": {"ta_keywords": "storage systems;codes;conversion;code;data;new method;method", "pdf_keywords": "optimal convertible codes;storage codes;conversion bandwidth;code conversion;bandwidth conversion;stable convertible codes;storage space consumption;scalable storage;bandwidth optimality;fundamental bandwidth requirement;convertible code;bandwidth requirement;convertible codes;bandwidth cost;symmetric codes;significant bandwidth savings;lossless code;network storage system;code conversion problem;storage space;storage nodes;recoverable codes;convertible storage systems;erasure codes;storage;vector codes;convertible codewe;optimal access cost;base code;minimum conversion"}, "43fe2d8781473360eeaae7a3284169a303200846": {"ta_keywords": "fake news challenge;novel ensemble method;classifiers;classifier;ensemble;fake news;accuracy;detection;challenge;method;results;significant improvement;aim;state;context;pilot study;art;study;use", "pdf_keywords": ""}, "d9b89de5c2a39479768c6e32f13ac3e816635cc1": {"ta_keywords": "lexical translation parameters;translation model;word lattices;consistent word error rate reductions;transcription;finitestate transducer;lattice;speech input;datasets;model;minutes;inference;composition;hours", "pdf_keywords": ""}, "44e24aabd05bef8cb45646486f1a24b7caecee45": {"ta_keywords": "multilingual seq model training;prior multilingual seq model;recurrent neural network language model;seq2seq model;resource speech research;other languages;seq2s;different languages;model;data;prior model;context;novel approach;different architectures", "pdf_keywords": "recurrent neural network language model;attention decoder model;multilingual seq2seq model;joint attention decoder;attention decoder;multilingual model;speech recognition;language model;monolingual models;hybrid rnn;location attention function;attention;hmm systems;attention mechanism;target languages;joint attention;target language;rnn;resource language;deep neural network;neural network;neural networks;joint decoding;seq2seq model;encoder;joint decoder;beam search decoder;language;decoder;end speech"}, "8b468872cf915c98ff46a2bea4d2a34112b7b0b0": {"ta_keywords": "entity extraction;knowledge base inference;relational rules;first order rules;parsed text;backward random walks;person;search;scalable approach;features;model;graph;tasks;approach;constants;system;problem;class;comparative study", "pdf_keywords": ""}, "d8704a63517868475b3af7ec25eaa2fb2a44362b": {"ta_keywords": "stiff refractive index;segmentation;loss;novel approach;model;approach;combination", "pdf_keywords": "deep cnn segmentation;cnn segmentation;shallow image segmentation;regularization loss;regularized losses;network output segmentation;network training;weakly supervisedwe;gradient descent;regularization approach;segmentation;new regularization method;dense network optimization;regularization solvers;grid loss;networks;regularizer;deep learning problem;loss;deep structured network approach;loss approach;loss function;grid losses;dense network parameters;edge;network;edge structure;random edge;forward discrete energy minimization problem;grid crf loss"}, "6dd6d4dfc3cf9ff41aad7e903cf1294de2ac5629": {"ta_keywords": "world traffic data;japanese highways;traffic data;deceleration scenarios;scenarios;reasonable risk acceptance thresholds;foreseeable parameter;ranges;international environments;contextualization;sakura initiative;distribution;applicability;sets;method;definition;cut;different fields;correlation;new set;study", "pdf_keywords": ""}, "1288d6570085a28518a9f3495e77dbb75899421c": {"ta_keywords": "entity recognition;active annotation method;human annotators;unsupervised learning framework;biomedical domain;unknowns;data;framework;encouraging results;errors;paper", "pdf_keywords": ""}, "163a67b5b0371035fa6e0f88b36ba97a32e735bc": {"ta_keywords": "erasure code;code conversion;storage systems;conversion;data;process;notion;efficiency;framework", "pdf_keywords": "storage code;optimal erasure codes;erasure codes;storage systems;erasure code;stable convertible codes;recoverable codes;minimum storage;optimal convertible codes;storage system;storage;code conversions;decodability properties;storage space requirement;conversion codes;storage devices;linear mds codes;code conversion;code pairs;systematic convertible code;digital codes;convertible code;convertible codes;code symbols;check code;redundancy configuration;projective code;code;encoding vectors;dfs"}, "027c5e44164a2ee3543ecdff73cd4d7888a42a90": {"ta_keywords": "selection networks;machine learning systems;preferences;large class;metric;class;post;representation;method", "pdf_keywords": ""}, "c17ccb7f0372ec98b7e070b0f70518f28516ecd5": {"ta_keywords": "stochastic processes theory;khoze;introduction;paper", "pdf_keywords": "magnetic field dynamics;particle dynamics;magneticthe dynamics;bosonic einstein equations;wave interaction;magnetic excitations;bosonic fieldthe dynamics;symmetric particles;schrdinger equationthe dynamics;magnetic field;plane wave dynamics;particle distance;problemthe dynamics;bosonic field theorythe problem;einstein interaction;interaction onthe dynamics;quantum dynamics;scatteringthe problem;stable magnetic field;particle inthe;bosonic wave function;bosonic particles;einstein equations;corresponding particle;weak magnetic field;relativistic wave equation;dynamicsthe problem;spin dynamics;particle;magnetic properties"}, "23e03cd57b5d75993545127f3fecf99d25021583": {"ta_keywords": "cancer genome atlas;multiple cancer phenotypes;multiple cancer;somatic genomic alterations;gene;tumors;novel model;functional impact;model;vector;representation;sgas;tcga;project", "pdf_keywords": "gene embeddings;tumor embeddings;similar gene embeddings;cancer genomes;cancer phenotypes;gene2vec;genetic information network;cancer biology;bioinformatics tasks;genes;bioinformatics;driver genes;gene ontology;gene;genetic gradients;bioinformatics machinery;gene clusters;individual genes;encoder;generalized neural network;functional similarity;driver gene;cancer development;deep learning;transcriptomic database;different cancer types;cancer omics data;cumulative genetic information;discrete binary representations;genomic alterations"}, "1d3539a8d94bd3ab78993d7cc584efc06ed0e460": {"ta_keywords": "feature attribution;powerful synthetic dataset library;machine learning;realworld datasets;popular explainability techniques;library;efficient computation;several evaluation metrics;code;synthetic platform;variety;ability;power;context;values;approach;settings", "pdf_keywords": "realistic synthetic datasets;synthetic datasets;explainability algorithms;synthetic dataset library;explainable artificial intelligence;popular explainability techniques;new explainability technique;explainability techniques;explainability methods;new explainability method;synthetic feature distributions;explainability mechanisms;explainers performance;interpretability;explainability;fair benchmarking;reproducibility;model predictions;novel feature attribution algorithm;feature attribution;quantitative predictions;several popular explainers;explanations;quantitative prediction;popular explainers;explainers;datasets;noisy noisy datasets;nonlinear additive dataset;machine learning"}, "8a14b3a9e642f4ca7fad4df997fc1941bdcfb935": {"ta_keywords": "entangled states;representation;states;state;mathematical description;system;set;new method;concept;art;method;large number;terms", "pdf_keywords": ""}, "ffecb8b8b415149f5351b64a2dbb1a1fa64219f0": {"ta_keywords": "end speech translation;multilingual end;multilingual ones;target languages;source languages;many translations;speech utterances;sequence architecture;universal sequence;generalization;model;st;effective framework;cases;case", "pdf_keywords": ""}, "9c78481004b7dbb601b83cc081ec23c02e6f5270": {"ta_keywords": "scalar game dynamics;scalar games;continuous games;stable equilibria;stability;equilibria;scalar;formal guarantees;game;rates;model;variations;set;comprehensive understanding;context", "pdf_keywords": "game dynamics;nash equilibrium;game theory;continuous games;nash optimality;scalar games;sum gradient learning dynamics;games;stability;equilibria;local game;learning rate ratio;learning rate;time games;dynamics;stability properties;nash;time dynamics;differential equilibrium;game;learning;spectral properties;players;robust prediction;matching dynamics;strategy;nonlinear systems;player;competitors;nonlinear interactions"}, "8b231737e0048a400527d89aa56c712e8b9bc690": {"ta_keywords": "end speech translation;multilingual end;target languages;source languages;speech utterances;many translations;sequence architecture;universal sequence;tt;high accuracy;available data;generalization;effective framework", "pdf_keywords": "end speech translation;multilingual speech translation task;multilingual speech translation;pipeline speech translation;multilingual end;neural machine translation;speech translation;multilingual encoder;resource speech translation;machine translation;multilingual models;parallel machine translation system;parallel machine translation;speech encoder;different multilingual models;automatic speech recognition;multilingual training;multilingual;multilingual one;speech utterances;target languages;input speech;sequence architecture;universal encoder;different languages;attention;source languages;language combination;corpus;encoder"}, "da20ab7724335eb48bcd0e9be30f0ac4b6a464c6": {"ta_keywords": "indoor racing environment;novel scenarios;autonomous systems;prediction model;detection;vision;novel scenario;dataset;world;house;information;novel method;method;experiments;presence;efficacy;ability", "pdf_keywords": "new image similarity metric;training images;image reconstruction networks;convolutional neural network;cnn;convolutional similarity index;deep neural networks;training data;novel images;autoencoder;vision;different driving datasets;image reconstruction;neural network;input image;standard convolutional loss function;neural networks;indoor racing environment;machine learning architecture;visual saliency preprocessing;visual navigation;features;steering angle;detection;novel loss function;machine learning;oneclass classifier;image;novel scenarios;dataset"}, "e54e0d9eaa922cefb1c69e105979399fd34497b1": {"ta_keywords": "fairness;group labels;aware learning;aware learning method;several benchmark datasets;simple onfidence;strategy;pg;vanilla;paper;method;problem", "pdf_keywords": "algorithmic group fairness;group fairness;optimize fairness constraints;fairness methods;fairness method;fair representations;fair training;other fairness baselines;fairness loss;fairtraining method;annotated group labels;fairness;group label assignment;random label assignment;random label assignment isin;group classifiers;dataset biases;group labels;fair distribution;group classifier;target labels;fair model;label assignment;unlabeled samples;label;random labels;training samples;sensitive groups;bias;label information"}, "dcb28c8ba94434eb8a06e81eb55bfdbc343d2340": {"ta_keywords": "ary relations;joint extraction;multiple representations;dimensional document;document;distant supervision;weak signals;high precision;noisy labels;novel approach;context;novel method;previous approaches;novel architecture;system;presence;approach", "pdf_keywords": "ary relation extraction;ary relation extraction methods;biomedical text corpus;unstructured text;text corpus;document learning;relation;maximum recall;semantic content;short text span;level extraction;text spans;novel multiscale neural architecture;ary relation;level representations;machine learning tools;multiscale learning;relation candidates;large benchmark dataset;multiscale learning system;discrete relation relationships;relation type;linguistic structure;standard biomedical reading task;discrete relations;subrelation hierarchy;maximum potential recall;document;text;mention tuples"}, "900b785dbbea7ccd5846eafb14c6715f76fe5e00": {"ta_keywords": "malicious content;malware;mobile devices;security;backpropagations;backpropagation;modern wireless networks;network;devices;traffic;risks;personal information;dynamics;novel method;user;challenge;presence", "pdf_keywords": ""}, "2a81081c987da2bb8184b8e9a884cf6a73712ee8": {"ta_keywords": "zeeman field strength;zeeman field;ep cluster;stability;field strength;field;standard model;framework;interplay;effect;suitable choice", "pdf_keywords": ""}, "126be977c03d732fbef2381565a41b957d41a2cc": {"ta_keywords": "book narrative modeling;natural language processing;discourse;nlp;fictional relationship understanding;linguistic structure;novel representations;text;unsupervised learning;language;novel approach;models;problems;approach;problem;methods;question;combination;techniques", "pdf_keywords": ""}, "aae8d332c3ff9d081ae36967d3d7b5f394b51bcc": {"ta_keywords": "entity recognition;stackelberg game;classification;differentiable teacher;teacher;training process;training;self;follower;semiand;model;student;game;new method;leader;approaches;winner;experimental results", "pdf_keywords": "differentiable selftraining framework;training framework;training method;learning model;training approach;training instability;supervised learning model;learning problem;standard learning framework;training instability issue;training problem;supervised learning library;supervised learning method;learning;supervised learning;supervised learning algorithm;student predictions;student model;knowledge distillation;training;stackelberg game formulation;classification tasks;student framework;teacher;large scale machine learning;machine learning;stackelberg game;deep learning;classification class;supervised text data"}, "2270b8628fd8ca67ae39d277f45bc3c38ac63d5f": {"ta_keywords": "large models;vertex splitting algorithm;parallel version;mesh;scale models;neural networks;accurate representation;novel approach;data;context;power;approach;problem;ability", "pdf_keywords": "tensorflow;tensor computations;new parallel computing framework;data parallelism;parallel processing;tensor compression library;parallelization;tensors;tensor;new tensor;parallel algorithms;parallel data;parallel implementation;deep learning models;tensor2tensor cluster;deep learning;parallel version;large datasets;allreduce library;parallel;batch;splitting algorithm;single point processing unit;cores;processing graph;processing operations;single processor;allreduce;sequence model;new framework"}, "e2bd274c8dd2a3b2a0a6f5d8a29baee07df34eb9": {"ta_keywords": "machine translation systems;translation process;t2s;tree;par;string;accuracy;systems;basic system;phrase;bleu;peripheral elements;stateof;number;paper;art methods", "pdf_keywords": ""}, "98b7d5611c0a128f45db100cc796b981573adcc5": {"ta_keywords": "coupled harmonic oscillators;asymptotic behaviour;spectral function;asymptotic behavior;closed system;cycles;system;problem;number", "pdf_keywords": ""}, "bcd4c46e4d75ddedb6138cfd77600c6d964a9aa8": {"ta_keywords": "code models;program selection;tocode tasks;natural language;minimum bayes risk;machine learning;translation;computational science;mbrexec;algorithms;many machine;work;key problem", "pdf_keywords": "code models;code datasets;large language model;program generation;language models;program synthesis;program selection;tocode tasks;programming languages;representative programming languages;natural language;code;code snippet;text programs;natural language processing;language;semantic programming;computational linguistics;level code;python;best program isin;program;best program;programs;large corpora;minimum bayes risk;level code forthis paper;moderate minimum bayes risk;translation;bayes risk function"}, "8688169ad5701e726968e293ff7dc53d76dd8007": {"ta_keywords": "electron gas;magnetic properties;electron density;magnetic field;magnetic field strength;2d;field;effect;presence", "pdf_keywords": ""}, "4f78624defde3b60551cfeb37e3943b267ea704a": {"ta_keywords": "large machine learning models;quantization;compression;gradient differences;learning problem;large machine;block;convergence properties;nonconvex settings;methods;analysis;method;problems;differences;novel", "pdf_keywords": "arbitrary closed convex regularizer;large machine learning models;stochastic gradient;nonconvex minimization;linear stochastic gradient;convex minimization problemwe;convex minimization problem;smooth nonconvex minimization problem;nonconvex optimization problems;empirical risk minimization;structured machine learning;gradient ofwe;learning problem;optimization framework;optimization;convex;stochastic update;quadratic gradient model;quadratic quadratic gradient model;linear programming;learning;gradients;markov approximation method;machine learning model;nonlinear generalization;convex functions;deep neural network;traditional linear programming;iterative process;cluster"}, "dbdefb498b619912a726fec7c85533594a1c6a1b": {"ta_keywords": "minimax optimization;full maximization;smooth algorithms;nonconcave;nonconvex;adversary;max;algorithm;challenging setting;monotonic progress;limit cycles;player;new framework", "pdf_keywords": "nonconcave minimax optimization problems;nonconcave minimax optimization problem;adversarial training problems;stochastic gradient descent;minimax optimization problem;gradient descent;nonconcave maximization;minimax;nonconcave games;proximal optimization problem;stochastic subgradient descent;gradient complexity;gradient ascent;proximal algorithm;adversarial networks;standard adversarial training method;gradient computations;adversarial training;convex optimization problem;deterministic subgradients;alternating gradient ascent;generative adversarial networks;stochastic gradient oracles;optimization;stochastic subgradients;inner optimization loop;computable optimality notions;generative adversarial network;gans;improved gradientwe"}, "a3ce3004a0eade48a3ae652dbf5c04a60c2416aa": {"ta_keywords": "personalized dialogue generation;dialogue generation process;personality traits;explicit personality;sequence;model;novel approach;framework;problem;paper", "pdf_keywords": "personalized dialogue generation;personalized dialogue generation models;personalized dialogue generation model;aware dialogue generation models;personalized dialogue responses;personalized dialogue model;dialogue generation;conversation generation;dialogue generation process;dialogue models;diversified dialogue responses;dialogue data;dialogue responses;dialogue conversations;first dialogue corpus;scale dialogue corpus;personal dialogue;human conversation;dialogue texts;scale dialogue dataset;dialogue task;dialogue;dialogue corpora;domain dialogue system;persona information;conversation;persona representation;explicit personality traits;personality;personality traits"}, "54a13bcc9613dcaa76fb25fbe96572f376cfcca9": {"ta_keywords": "decay rate scheme;lnlap;adesire;matrix;column sums;row;model;second moments;parameters;publisheddesire;comparable results;per;sums;scale;parameter;scheme;schemes;regime", "pdf_keywords": "stochastic optimization;novel stochastic optimization method;deep neural networks;squared past gradients;adaptable gradient;gradient model;gradient;adaptive learning problem;noisy objective function;minimization;gradients;matrix factorization techniques;optimization;minimization procedure;stochastic matrix;memory;auxiliary hyperparameters;learning process;emma;rank approximation;sublinear memory cost;arbitrary weights;training run progresses;generalization;models;learning problemswe;machine translation model;training rate;training step;algorithms"}, "957e3ec3c722f5cb382fe8ac54fc846ee772a95f": {"ta_keywords": "bandit problem;peer game;online peer;several new regret;novel algorithm;algorithm;exploration;model;parameter space;previous works;generic framework", "pdf_keywords": "stochastic bandit learning;stochastic bandit algorithms;stochastic bandit problem;adversarial bandits;bandit feedback;stochastic banditwe;adversarial banditswe;stochastic bandits;dependent regret bounds;bandit setting;optimal regret;stochastic optimization;bandits;new adaptive regret;quadratic learning rates;partial information feedback;quadratic learning rate;learning rate;uncertain parameterized learning;quadratic learning problems;learning rate parameter;adversarial setting;online mirror descent framework;optimalemma;scalable learning model;global optimality;optimal choice;adversary;entropy learning;quadratic learning algorithms"}, "c4ce6aca9aed41d57d588674484932e0c2cd3547": {"ta_keywords": "scientific literature;knowledge base;scientific papers;natural language;search engine;mechanisms;literature;content;novel approach;relevant information;utility;approach;problem;bp", "pdf_keywords": "biomedical ontologies;usual nrf ontologies;novel semanticwe;semantic information;type knowledge base;entity mention extraction;knowledge base;novel schema;unified knowledge base;ontologies;biomedical knowledge;annotated data;annotators;mechanism relations;entity annotations;natural language texts;biomedical sentence pairs;annotations;deep semantic knowledge;unstructured text;schema;knowledge basewe;natural language;annotated dataset;human annotators;scientific texts;corpus;structured search;biomedical search tasks;unstructured database"}, "73b22457a2f52a834d73d73a76b4124c1cb326be": {"ta_keywords": "performative prediction;nash equilibria;stable equilibria;stochastic;game;new game;algorithms;retraining;gradient method;theoretic framework;distinct solution concepts;variety;mild assumptions;ii", "pdf_keywords": "performative prediction game;performative prediction games;stochastic game;stochastic games;stochastic gradient play;continuous games;unique nash equilibrium;nash equilibrium;arbitrary games;dependent games;performative prediction problems;stochastic gradient bias;game theory;stochastic gradient theorem;stochastic optimization;stochastic gradient methods;performative prediction;multiplayer prediction;stochastic control;optimal strategy;stochastic gradient method;nash equilibria;dependent learning;stochastic stochastic computation;dependent learning problem;stochastic gradientwe;stochastic;games;natural semilinear stochastic gradient problem;gradientwe study stochastic"}, "6eb974721719056ba8dc74a898c64ae1d081e0ae": {"ta_keywords": "univariate diagnostic curve;diagnostic curve;diagnostic curves;black box model;models;undesirable features;qualitative properties;sample behavior;datasets;monotonicity;usefulness;output;input space;cases;multiple use;property;presence", "pdf_keywords": "partial dependence plots;dependence plots;directional dependence plots;familiar pde plots;standard pde plots;ice plots;interesting plots;graphical representation;multivariate graph models;multivariate models;visual tool;machine learning;directional plots;multivariate model;model contrast utility;plot;empirical data;different models;specific pdps;multiple features;arbitrary curves;best qualitative model;various qualitative properties;unsupervised learning tasks;utility;black box decision systems;box models;dependencewe;unsupervised learning;counterfactual explanations"}, "4e2c41466c8246af0a563ea36fbe80c896bbab2c": {"ta_keywords": "neural machine translation;word sense;ambiguous words;global sentential context;input word;translation;context;accuracy;nmt;terms;system;systems;method;novel method;use", "pdf_keywords": "neural machine translation;machine translation;translation performance;word sense disambiguation literature;sense disambiguation;machine translation problem;machine translation framework;best translation strategy;sample translations;aware word embeddings;word sense;translation;ambiguous words;directional context network;different language pairs;target language;natural language processing;aware word;deep learning;aware embeddings;neural model;deep learning pipeline;target languages;neural network models;encoder;target word;word increases;deep neural network;text network;input word"}, "e9c52a3fac934919eca036909cc18d909db0d467": {"ta_keywords": "molecular fluid;continuum model;continuum continuum;flow;like continuum;peridynamic;fluid;scaling;model", "pdf_keywords": "molecular dynamics simulations;molecular dynamics simulation;molecular dynamics;molecular displacements;linear peridynamic solid model;peridynamic solid model;linear peridynamic continuum models;recent molecular statistics simulation;multilayer graphene problem;graphene;multilayer graphene;single layer graphene;nonlocal dynamical model;nonlocal models;nonlocal model;material model;optimal nonlocal operator model;heterogeneous materials;isotropic elastic solids;computational mechanics;nonlocal operator model;microscopic model;linear elasticity;dimensional quantum network science;peridynamic model;md displacements;material parameters;nonlocal framework;stochastic refinement;dimensional ising model"}, "52ec4713343083e69b87e36a7a12c7b5898e2780": {"ta_keywords": "unprocessed noisy speech;automatic speech recognition;adaptive generation;nonlinear partial differential equations;path optimization scheme;asr;snp;hidden model;speech;previous end;adaptation;end approaches;novel approach;systems;pathway;end;nodes;message;approach", "pdf_keywords": ""}, "4d1a14352ffb526a1fa0e1cd90e2484e188cddc0": {"ta_keywords": "transferable dialogue systems;user interoperability;natural language;reinforcement learning;agents;structured reward functions;source domain data;dataset demonstrate;modelling framework;behaviors;framework;area;wft;aim;collection;experiments", "pdf_keywords": "transferable dialogue systems;dialogue model;dialogue systems;dialogue system;dialogue representation;human dialogues;dialog systems;end dialogue systems;dialogue interaction;level dialogue model;end dialog agents;user dialogue;source domain dialogues;dialogue encoder;dialogue pairs;dialogues;dialogue;dialogue actions;policy learning;user simulator;natural language;learning system;novel joint learning framework;language models;joint learning framework;user learning problem;reinforcement learning;trainable task;novel reinforcement learning approach;transfer learning"}, "6d00b1024298e5b64ee873028385f7bb4396b05d": {"ta_keywords": "compositional generalization;algebraic recombination;comprehensive compositional;semantic operations;explicit representations;neural model;utterance;text;novel algorithm;model;form;paper;experimental results", "pdf_keywords": "compositional generalization tasks;compositional semantics;neural parser;compositional generalization;limited compositional generalization;compositional learning;compositional generalization model;semantic parsing tasks;compositional generalization benchmark;latent syntactic operations;comprehensive compositional generalization benchmarks;compositional generalization problem;semantic operations;lexical recombination;parsing;semantic representations;compositionality;semantic primitives;semantic prediction;neural encoding;compositional model;compositional process;underlying semantics;composition;syntax;latent syntax;natural language processing;interpreters;novel compositional induction model;algebraic recombination"}, "6ccac8a95bc77549b98d045db6d5e0de3d356ba4": {"ta_keywords": "document ranking;document documents;bert;document;novel neural model;neural variant;characterization;model end;model;context;end;utility;system;combination", "pdf_keywords": "english text retrieval;lexical translation model;document retrieval tasks;text retrieval;document embeddings;document retrieval task;passage retrieval;information retrieval;dependent embeddings;optimal translation rate;free embeddings;optimal translation;model1 model outperforms bert;neural models;embedding;contextualization;natural language processing;natural language processing pipeline;translation;linguisticwe;neural model;text documents;novel neural model;natural language;ranker model;ibm model;baseline models;efficient neural network;document tokens;language"}, "69c515a62403fcc19125d3a6dd8e878aa5cde604": {"ta_keywords": "utterance;faster inference speed;model;state;art models;experiments", "pdf_keywords": "automatic utterances;incomplete utterance restoration;text generation;sequence labeling;text editing;bert encoder;unstructured unstructured utterances;novel semi autoregressive generator;autoregressive generation;semi autoregressive generator;utterances;spontaneous utterances;simple utterance;effective lexical representations;utterance;typical utterance;autoregression;entire utterance;next generation machine learning;attention matching network;encoder;deep conversation;novel deep learning;simple sentences;sentenceswe;sequence;deep learning;generator;unnecessary tokens;generative model"}, "d2f327736c9b68f68ad64d0b1cefed9b4dd83313": {"ta_keywords": "structured prediction;natural language;imitation;incremental model;datasets;automatic measures;unaligned data;large search space;explicit enumeration;generation;human judgements;outputs;new method;results;approach;state;art approaches", "pdf_keywords": ""}, "e37fb85e8869d464bae8eeebf4cd9321ec8c70ad": {"ta_keywords": "interpretability;interpretable hypotheses;empirical risk minimization;machine learning;statistical learning theory;accuracy;statistical implications;formal study;model;results;offs;reason;set;paper;trade", "pdf_keywords": "interpretable classifiers;empirical classifiers;interpretability hypotheses;classifiers;interpretability;interpretable hypotheses;interpretationability;empirical risk minimization;classifier;deep learning;supervised classification;machine learning;empirical risk;statistical learning theory;binary classification;output classifier;accuracy;generalization;dangerous data sets;interpretation;generalization error;risk;black box;generalizationthis paper;training sample;statistical cost;learning process;larger hypothesis space;risk terms;attention"}, "8b0b2b69657076fc1ce7cce75a6d69e3e5ba2d63": {"ta_keywords": "memorylesscapacitors;layer memory;layer dielectric;memory;dielectric;double layer;layer structure;layers;new type", "pdf_keywords": ""}, "6a173e22819480b891306eac65fd44be010dfca8": {"ta_keywords": "influenza virus;immune cell signaling;zinc finger transcription factor;inflammation;macaque lungs;cell cycle;virus;host functions;network;regulation;mechanism;novel model;model;novel role;dyregulatory;novel strategy;analysis", "pdf_keywords": ""}, "1d2a2b14ef14eeaf89169f738f7634cdc685c785": {"ta_keywords": "relational database;similarity;database;measurement;trivial way;queryd;degree;terms;method", "pdf_keywords": ""}, "c968e8dc442102b38b134b1afadc7cc78fc5b5fb": {"ta_keywords": "entity recognition;interpretable evaluation;datasets;task;models;ner;evaluation method;current systems;weaknesses;interplay;general methodology;strengths;differences;method", "pdf_keywords": "entity recognition model;entity recognition;ner models;entities;different ner datasets;short entities;ner systems;interpretable evaluation;nernstronomic classification;entity length;einstein condensate;models;nernet model;holistic metrics;aglove model;dependent bose;word representation;ner task;empirical relations;holistic metric;grained description;model;test entities;different decoderswe;natural language;evaluation attributes;representation;encoders;bose;different models"}, "7a56aba1a4d4020c4933319588b9ed2b34d51125": {"ta_keywords": "secure data;peer peer peer;noisy data;peer;security;data;connectivity;generation;model;measure;framework", "pdf_keywords": "secure linear regression;malicious algorithms;secure computation;gradient descent;stochastic gradient descent;scalable machine learning;abstract multiparty computation;algorithms;iterative computation;logistic regression;spdz;linear regression;previous implementations;empirical evaluation;iterative approach;implementation;novel method;polynomial activation function;other methods;efficiency;stochastic;new method;standard analytic methods;simple method;spz;method;data;methods;framework;generalization"}, "e02f79b710cdcaa9135b835fad964f6f2c78b1a7": {"ta_keywords": "neural encoder;character sequences;encoder;challenging multilingual benchmark;characters;dog;input sequence length;subwords;novel training strategy;soft inductive bias;accurate training;input", "pdf_keywords": "deep encoder;new neural encoder;encoder;universal tokenization;encoders;explicit tokenization;vocabulary predictions;tokenization;language modeling;nonlinear linguistic processing;subwords;scalable scalable character learning model;new linguistic structures;universal encoding strategy;heuristic tokenizers;simple words;deep transformer stack;extra vocabulary knowledge;words;standard vocabulary;subword;external linguistic rules;deep transformer stacks;representations;vocabulary;encoding;language understanding;memorization;classifiers;nonlinear classifiers"}, "4572ded23106285cbd8ebbe6c3b354973ac06ff7": {"ta_keywords": "particles;random environment;particle;dynamics;interaction;system;set", "pdf_keywords": ""}, "e214d2a6399925ce60fa5ce90c0374127a32b47e": {"ta_keywords": "optimal policies;quadratic decision process;wireless sensor networks;outage probabilities;light traffic regime;natural heuristic;relays;nodes;forward approach;explore;approach;unit distance;power;problem;links;number;structures", "pdf_keywords": "relay placement decision;relay deployment;arbitrary network deployment scenarios;wireless relay networks;impromptu deployment strategies;optimal deployment;wireless networks;network deployment;network optimization;optimal deployment cost;relay propagation;relay placement rate;deployment algorithm;wireless relays;several deployment algorithms;wireless network deployment problem;wireless search algorithm;relay nodes;places relay nodes;wireless sensor networks;impromptu deployment;novel deployment strategy;relay placement problems;learningwireless relay networks;scalable deployment strategy;stationary deployment policies;network coverage;next relay;relay policy;sensor network"}, "7ee55c115470e1b86e552c5594e2e4258b4ccefb": {"ta_keywords": "proton interaction;proton;chiral dynamics;important vertex;framework", "pdf_keywords": ""}, "a7766d4c41df235764dfaa9971ce861f6120ac27": {"ta_keywords": "group meetings;ourlatency interface;ourlatency threshold measurements;sixlatency sensors;timelatency interface;thelatency threshold;novellatency protocol;table;performance;velocity;system;threshold;center;information;set;results;context;benefits", "pdf_keywords": ""}, "252ef125a8874fe8face4540f87f2e000275cc96": {"ta_keywords": "active terrorist groups;novel clustering algorithm;communities;similarity measure;dimensional weighting;groups;unweighted algorithm;algorithm;interesting patterns;different ideologies;task;data;promising result;combination;natural way", "pdf_keywords": ""}, "803c7fdd6e01e1ff8cd43297f4e052078409456d": {"ta_keywords": "social networks;continual learning;communication;conventions;probabilistic model;central computational problem;computational foundation;new empirical data;interaction;several phenomena;expressions;adaptation;multiple timescales;transmission;classical formulations;simulations;previous accounts;formation;challenge;model;convergence;same partner", "pdf_keywords": "stable social conventions;convention formation;common conceptual motif;social interactions;social communication;coordination;conventions;new conventions;language formation;social phenomenon;human communication;hierarchical generalization mechanisms;human generalization behavior;linguistic organization;general cognitive mechanisms;communication;different conventions;communities;linguistic repertoire;communication game;specific conventions;generalised concepts;social inference;level conventions;cognitive mechanisms;social context;convention names;natural language communication experiment;community;hierarchical cognitive model"}, "c6488f0c62ee4a4d48d0fbf8e8185655226294c1": {"ta_keywords": "controller manipulation attack;reconfigurableconfigurableconfigurable surface;ris controller;attack detection probability;attacker;receiver;transmitter;communication system;new attack;phase shift;constraint;cma;ris;goal;data rate;ris elements;potential", "pdf_keywords": "attack detection;novel nonlinear phase shift matrix attack;novel controller manipulation attack;new controller manipulation attack;single antenna transmitter communicates;wireless fading channels;different attack detection models;wireless communications;attacks;wireless communication system;new attack model;modern communications systems;single antenna receiver;novel detection strategy;attacker;interference pattern;multiple antennas;attack;wireless networks;communication system;fading block;data transmission phase;receiver;wireless network;transmitter;ris controller;performancewireless communications;channels;threshold detectionwe;channel"}, "2e4cdd36d77b9d814638fc2cd6c703535cb1d2f7": {"ta_keywords": "frozen language model;natural language generation;language models;unfamiliar inputs;input representations;input;continuous prompts;nlu;tuning;tasks;context;pd;paper;effective way;problem", "pdf_keywords": "machine translation tasks;machine translation task;machine translation;small language models;natural language models;generative pretraining;language models;prompt tuning;language model;text generation;natural language processing;prompt learning;input embeddings;certain natural language understanding;pretraining;unified text;nlgs;soft prompts;numerous downstream tasks;soft prompt;task inputs increases;soft prompt designing;prompt;translation;embeddings;multiple downstream tasks;encoding;unfamiliar inputs;tuning;tasks"}, "4c2d9136c579a0393d4f50bbbbc6f8dab43c38e9": {"ta_keywords": "velocity dispersion;velocity prediction;novel shape parameter estimator;velocity;prediction;prediction paradigm;robustness;source;variety;novel method;ability;introduction;context;method", "pdf_keywords": ""}, "6f49026ff623c64ce6de81fd04cf6e1ffe7dd6d9": {"ta_keywords": "binary neurons;binary neuron model;convolutional generative adversarial network model;piano;output layer;generator;rolls;additional refiner network;network;experimental results", "pdf_keywords": "binary neural network;generative networks;stochastic binary neurons;binary neurons;random binary stochastic neurons;novel convolutional gan;generative adversarial network;stochastic generator;deterministic binary neurons;generative framework;track piano;music generation;piano;convolutional neural network;convolutional neural networks;music transcription;gan;musical model;neural networks;generator;output layer;binarization strategy;refinement network;tracks;music;networks;training data;binary;refiner networks;outputs"}, "57e7be6b404abfd7a56a73c0ff9bccc5b27ad7ae": {"ta_keywords": "neural machine translation models;aware feature embeddings;back translation;auxiliary language;adaptation;domain;consistent improvements;model;task;specific data;empirical results;performance;multiple experimental settings;strategy;effectiveness;approach;work;method", "pdf_keywords": "domain machine translation;neural machine translation;neural machine translation system;domain feature embeddings;domain adaptation;machine translation data sets;effective unsupervised domain adaptation technique;embeddings domain;translation model;natural language domain;translation accuracy;aware feature embeddings;output feature embeddings dooumt ain;translation data;input domain;output domain;translation results;conditional translation;input task;tasks;translation;novel task;simple adaptation;task;auxiliary language;neural machine;domain;learner;transformer;representations"}, "2b3ab7e9c66bffc7af9e4413036e7bba686a7734": {"ta_keywords": "novice reviewers;reviewers;bias;reviews;machine learning;papers;junior phd students;submission;paper;resubmission;recent graduates;trial;analysis;master;relevant components;signal;top us universities", "pdf_keywords": "reviewers evaluations;information reviewers;peer review;competent reviewers;resubmission biases;reviewers;recruiting reviewers;novice reviewers;resubmission bias;individual peer review;biases;bias;major machine learning conferences;machine learning conferences;significant bias;negative bias;several biases;several common biases;review system;individual peer review articles;review process;potential bias;own review;hypercritical past reviews;underlying confidence;empirical evidence;review pipeline;informed decisions;reviews;review"}, "536ce077f08886b5834b639da25068d877c98b2c": {"ta_keywords": "gaussian curvature;electromagnetic spectrum;nonlinear medium;wave function;wave;propagation;particle;energy dependence;effect;terms", "pdf_keywords": ""}, "b54efe01969adaa1c623331d5791897a4dd9f886": {"ta_keywords": "fruit fly genomics;flybase database;interactive information extraction system;search;curators;novel tools;tools;articles;novel tool;curation;cross validation", "pdf_keywords": ""}, "0e8da301b098f96fed39c5aa8e2194f678690a16": {"ta_keywords": "sharing;deterministic algorithm;general network;network;direct communication link;secret;communication;dealer;common distribution function;parties;participants;problem;example", "pdf_keywords": ""}, "73569460b023f9ac1fe5a1876c3401460d2fc15d": {"ta_keywords": "source code understanding;code clone;code;curriculum;downstream data diversity;code problems;algorithm classification;models;tasks;transformation;data;approach;paper;experiments;tune;range;use;hard manner", "pdf_keywords": "code discovery;code generation;downstream tasks;code clone detection task;downstream data diversity;specialized datasets;various downstream tasks;semantic knowledge;code augmentation method;code clone detection;code search task;semantic features;semantics;source code;code search;code detection;clone detection tasks;data augmentation;target dataset;new task;lightweight approach;training;multiclass classification tasks;learning;training process;code;augmented dataset;tasks;natural language;training set"}, "23f1d4b46bc7c8f357a5a89144d5d32af7be13a5": {"ta_keywords": "news summarization;generation models;generation process;text;news;tuning process;large datasets;tuning;quality;main aspects;dynamics;fine;use;work", "pdf_keywords": "large abstractive summarization models;summarization models;summarization model;summarization datasets;abstractive summarization;multisentence media interview summarization;media interview summarization;language models;summarization;text generation;abstractive summaries;summaries;classification task;learning;learnt behavior;abstraction;examples;learning dynamics;basic tasks;training strategy;extractive datasets;training process;novel training strategy;brief overview;models;datasets;prediction;sentence;intrinsic factuality;new factuality"}, "9389af659f14239319186dff1cef49e8ece742c8": {"ta_keywords": "core graph learning tasks;expressive graph;graph regression;scale graph data;graphs;node classification;expressive models;simple scalable baselines;andlink prediction;ogb;datasets;discovery;scale challenge;challenge;efficient machine;machine;dedicated baseline experiments;collection;potential", "pdf_keywords": "graph learning framework;scale graph machine learning challenge;graph learning;scale graph learning;available large scale graph;graph machine;expressive graph models;large graphs;large graphswe;intensive graphs;core graph tasks;molecular graph learning;graph models;graph regression task;heterogeneous academic graph;graph regression;powerful graph representation representation framework;advanced graph models;powerful graph;academic graph;graphs;scale graph ml;graph representation;link prediction;heterogeneous graph;massive modern datasets;graph;molecular graphs;synthetic graphs;largest gw datasets"}, "c68349ba142be731d6f3339d894764921c69b774": {"ta_keywords": "training data;early detection systems;early detection process;diabetes;like quiz;data;health system;massive data sets;dangerous diseases;risk;questions;game;cancer;type;individuals;method;advances;design;potential;proliferation", "pdf_keywords": "like quiz;public health information survey;twitter;local tweets;participants weight;obesity;natural language questions;social media;public health research;questions quiz;training data;quiz;body mass index;diabetes;questions;social events;public health question;data;respondents;overweight;diabetes rates;health model;weight;large scale data;participants;relevant data;available data;topics;food;survey"}, "6665e03447f989c9bdb3432d93e89b516b9d18a7": {"ta_keywords": "dynamics", "pdf_keywords": ""}, "e92de0c4ef62a84201fac284eb66c37330b5fe1c": {"ta_keywords": "accelerated code generation;machine translation;code query;semantic statements;code;translation;several open source projects;bugs;novel method;quality;method;paper;set;effectiveness", "pdf_keywords": "software bug fixing;bug fixing;bug fixing statements;bug fixes;software bugs;automatic patch generation;dangerous software bugs;bug lists;fix code;patch suggestion;patch recommendation;neural machine translation technique;code repositories;source code changes;automatic software modeling;neural machine translation;bugs;repetitive code changes;code changes;neural machine translation lexicon;source code;patch generation model;prone software;weautomatic program repair;several large software projects;code query;data mining tasks;fixing;future code changes;long period software evolution"}, "90848c88f56fcd421ac3cfd2c87d3e61211103ea": {"ta_keywords": "orbit interaction;magnetic field;spin;dynamics;static field;interaction;field;presence", "pdf_keywords": ""}, "051a85bd1384767ea5882dcefa98aee5664aa2cf": {"ta_keywords": "deep network;deep neural networks;speech processing;novel networks;inference iterations;networks;conventional networks;layers;knowledge;powerful network;new understanding;such frameworks;novel approaches;parametrization;problem", "pdf_keywords": ""}, "6fde1c63b1a353cf539d319341ae9396000660ed": {"ta_keywords": "automatic pronunciation evaluation;noise reduction technique;noisyenvironment;background noise;other methods;performance;method;spice;model;gop;paper;effect;literature", "pdf_keywords": ""}, "e03d9684a19c8f8e29ee97b347d4f1e280a88e44": {"ta_keywords": "spoken language representations;language grounding;gesture space;gestures;language representations;crossmodal grounding relationships;novel contrastive learning approach;spoken language;novel contrastive loss function;available dataset;similarities;novel approach;approaches;nce;model;approach;effectiveness", "pdf_keywords": ""}, "d31b5b60e1b3af84cd977da8db0ed4faeb79e7f7": {"ta_keywords": "text style transfer;emotive defective texts;emotive regular text sequence;individual text texts;texts;perfect matching;style;novel method;targeted remodel;features;art methods;other features;collection;method;state", "pdf_keywords": "text style representations;text style transfer;style transfer tasks;tunable text style transfer kernel;style transfer;sentiment transfer;textual style;style vectors;informal vocabulary transfer;arbitrary styles;style attributes;text model;word transfer;diverse corpus;linguistic information;shot style extractor;styles;shot style transfer;style vector;novel automatic inference mechanism;text;conditional transformer language model;text processing;style vector input;language model;art encoder;style;adjacent sentences;texts;text processing research"}, "5812e30eb4756aeaf0b013a65b98f8f8aa0f8315": {"ta_keywords": "domain adaptation;abstraction;standard model;public domain;dsm;novel approach;method;approach;context;novel pre;essential elements;ability;problem;use;power", "pdf_keywords": ""}, "cf2a953dc82115d34de51737fef46bf3ff4cd5a6": {"ta_keywords": "lorentz force;suppression rate;effect", "pdf_keywords": ""}, "c44addf352f25f28f69ca9f9422c0e463783206f": {"ta_keywords": "parsed text corpus;similarity measures;specific similarity measure;parsed text;graph walks;graph walk process;meaningful edge sequences;walk process;task;graph;level knowledge;new learning method;novel framework;derivation;function;framework;use", "pdf_keywords": ""}, "9c16dcbcdfe6991f5d448543e6f4cbdf37149883": {"ta_keywords": "multimodal tasks;additive function projection;unimodal signals;visual question;new diagnostic tool;emap;box algorithms;performance;task;tool;model;data", "pdf_keywords": "multimodal classification tasks;multimodal machine learning;multimodal classification;real multimodal classification tasks;multimodal interactive models;multimodal corpora;multimodal interactions;multimodal data;multimodal systems;multimodal system;crossmodal interactions;classification tasks;interactive models;natural language output;textual features;unimodal models;supervised search tasks;expressive interactive models;new classification task;deep learning paradigm;feature networks;essential features;best interactive model;necessary computational tasks;interactive task;text representations;deep learning dataset;features;machine learning;prediction metrics"}, "335bf6f23ccdae43e45a7c12f33bc4f3488e3762": {"ta_keywords": "such incomplete corpora;single source;additional auxiliary language;auxiliary language sentence;original source;standard nmt;methods", "pdf_keywords": ""}, "82459c972cc1e439c759010acf7ddce1a89b66e0": {"ta_keywords": "fuzzy graph clustering;algorithm;new algorithm;networks;computational analysis;meta;global context;domains;data;variety", "pdf_keywords": "aware graph clusteringwe;fuzzy graph clustering;linguistic graphs;node sense disambiguation;sense embeddings;sense graph;lexical clustering;word sense induction;node sense induction;senseaware graph;aware graph construction;aware clustering algorithm;natural language processing;linguistic structure;linguistic structures;clustering structure;clusteringwe;structured lexical database;fuzzy clustering algorithm;semantic lexicon;global graph clustering problem;lexical semantic resources;semantic relatedness;disjoint clusters;semantic task;semantic structures;computational linguistics;node neighborhood clustering approach;clustering framework;soft clustering"}, "684821e2459c7fc3ef8a2ec8102678af3613a962": {"ta_keywords": "speech representations;specialized prediction heads;multiple speech tasks;unlabeled data streams;generalization abilities;self;available self;performance;models;task;network;minimal architecture changes;results;sb;framework;large number;complete framework", "pdf_keywords": ""}, "2467b2daea0398709d7ea57d084cc1f00f9d168f": {"ta_keywords": "conditional speaker network;novel conditional speaker model;automatic speech recognition;speaker;speakers;nar;rnnc framework;asr;mixture;model;framework;novel approach;problem", "pdf_keywords": "conditional speaker chain module;conditional speaker chain;speaker speech datasets;automatic speech recognition;connectionist temporal classification encoder;global acoustic dependencies;novel conditional chain model;conditional chain;conditional conditional network;speaker;encoder;encoder network;encoders;conditional network;advanced conformer encoder architecture;convolution network;joint recognition;end models;speech;mixed waveform;decoder framework;convolutional network;joint classifier;network representations;approximate segmentation;models;output;asr;additional intermediate lossin;end"}, "a1588ac6d582d30742f998464500bb5ead125dc6": {"ta_keywords": "auction network;regret protocol;new deep learning architecture;auctions;regret;alternative loss function;attention mechanism;revenue;new network architecture;hyperparameters;performance;variety;settings;modifications", "pdf_keywords": "optimal auctions;simple auction mechanism;auction design;auction;price auction problem;modern auction systems;auction network;auctions;unconditional auctions;optimal valuation distributions;financial auction channel;player auctions;modern auctions;auctioneer;student auction;optimal revenue maximization;regret maximization algorithm;revenue maximization;wish mechanism;explicit regret budget;optimal value;regretnet;wish networks;regret budget;optimization;incentive compatible;regret regret architecture;regret values;regret problem;neural network"}, "1c682dca13e47e6e1ee3c8db54af631a8e5e5792": {"ta_keywords": "cost markov process;optimal policy;probability transition matrices;policy iteration;direct spectral decomposition;large state space;wireless system;algorithms;best performance;outer product;projection;dominant subspace;paper;method;value function;performances;numerical results", "pdf_keywords": ""}, "48e32ba9a891f36183a26f35316e8906d14d83c0": {"ta_keywords": "computational quality control toolkit;novel computational quality control toolkit;computational quality control methods;crowdsourcing;toolkit;computational science;computational sciences;capabilities;purpose;use;context", "pdf_keywords": "crowdsourcing;computational quality control toolkit;pair aggregation;continuous answer aggregation;datasets;toolkit;several computational quality control tasks;crowd;feature;aggregated response;quality control algorithms;consensus methods;benchmark;discovery;data mining;scalable methodology;world data sets;data;python framework;efficient implementations;typical selection algorithms;quality;empirical evidence;kit;computational science;selection algorithm;specific data;uncertainty measures;predictions;subset"}, "d4756d1a7b81f53e71f939ab387cad5f0a4a13b7": {"ta_keywords": "speech recognition systems;sound events;term memory recurrent;neural network;duration;detection;new hybrid approach;modeling technique;evaluation;approach;conventional methods", "pdf_keywords": ""}, "19418493b1f9c82809fe4584af427b8807b8ae2d": {"ta_keywords": "similarity information;large corpus;sentences;information;novel method;method", "pdf_keywords": "relation extraction task;textual corpora;relation classication task;project gutenberg corpus;relation classication;corpora;relations;relation;relation graph;object pair;nonlinear neural architecture;deep recurrent neural networks;sentence;level relation classication problem;commonsense;raw text;structure;objects;physical objects;neural network;physical object;automatic framework;feature;wikidata;neural network regularization;simpler feature;literature;predictwe;novel tasks;novel framework"}, "298ddceada580c46e40e2a0323c0e3b16ed5f3c9": {"ta_keywords": "estimation;distribution;probability density;parameter;values;method;value;new method", "pdf_keywords": ""}, "562fbb5d706d46f3e250429ac48e6acd2bf18cb1": {"ta_keywords": "speech enhancement;optional downstream speech recognition module;speech recognition;separation toolkits;toolkit;toolkit design;separation systems;end toolkit;functionality;benchmark datasets;quick development;single framework;implementation;aim;set;design;results;experiments", "pdf_keywords": "automatic speech enhancement modelswe;speech processing toolkit;automatic speech enhancement models;programmable speech models;source speech processing toolkit;automatic speech enhancement;advanced speech recognition pipeline;end speech enhancement;speech enhancement;speech evaluation;speech enhancement problem;speech recognition;speech separation;channel speech separation algorithm;art speech enhancement;speech recognition tasks;target speaker extraction;speaker generator;neural beamformer;encoder;enh3;separation toolkit;target speaker;toolkit;enh1;source toolkit;enh2;software;deep learning;separation models"}, "2dbe78aa516cc911a71ff333a35a5ce0b1a49640": {"ta_keywords": "baselines;longer future context;higher accuracy;novel approach;approach", "pdf_keywords": "deep learning model;common common deep learning model;deep learning;ducer sensor network;neural network;novel adaptive model;additional training;streaming encoders;multiple latency modes;encoders;snn;train models;specified future context;future context;future context size;actual speech recognition service;total future context size;multiple models;speech recognition;modelwe;large datasets;future context size constraint;model;machine learning problems;automatic speech recognition;speech recognition systems;multiplewe;language models;data;sequence"}, "0dc379de3a613110c5fdc9c0361372c1114ee18d": {"ta_keywords": "electron beam transport;principle ion source;stanford electron storage ring;ion source;purity;energy;proof;ssr", "pdf_keywords": ""}, "c6c6b4d328381a530e933c208bb43db2a7fa93c8": {"ta_keywords": "orbit coupling;orbit coupling strength;orbit interaction;spin;coupling strength;coupling;atom;polarized background field;interaction;effect;basic example", "pdf_keywords": ""}, "7225c2a42990f850f692f8d82e7f3bfaf312145c": {"ta_keywords": "recurrent neural network models;models;training samples;model;training time;nmt;order;training;transformers;next;novel framework;performance;sample;principled way;current competence;difficulty;framework;different times", "pdf_keywords": "neural machine translation models;neural machine translation;training models;machine translation;training data pipelines;training corpus;learning models;training examples;curriculum;training data;training samples;training system;novel curriculum;training transformers;learner;training;machine learning model;model trainer;scale learning approach;models;neural networks;elementary acquisition examples;neural network;training time;model;large batch sizes;natural language processing;simple model;nmt;competence model"}, "4a76869cda286efb20eb78cc6adb13daab37a0d1": {"ta_keywords": "fokker;particle density;accurate simulation;simulations;particle;accurate particle;optimal control;novel statistical estimator;equations;models;time evolution;novel method;logarithm;gradient;method;moderate dimensions;performance;context", "pdf_keywords": "stochastic particle dynamics;stochastic particle systems;stochastic particle system;stochastic differential equation;particle approximations;stochastic propagation;langevin equation;deterministic particle simulations;direct stochastic simulations;second order langevin dynamics;stochastic simulations;particle densities;stochastic stochasticity;fokker equation;planck equations;particle density;approximate particle trajectories;stochastic version;stochastic partial differential equations;particle dynamics;stochastic process;stochastic processes;particle method;particle distributions;stochasticity;stochastic;stochastic counterparts;dimensional stochastic process;stochastic response;stochastic systems"}, "674f892caa52fa400109defa1773a10088918124": {"ta_keywords": "underlying predictive control problem;predictive control;optimal control problem;input constraints;input constraint sets;constraint violation converges;finite horizon;dual method;mpc;gradient representation;projections;model;state;distance;terms;sequence;rate;method", "pdf_keywords": ""}, "cc549a11d277d86f6228443cb16c231c9bda6c96": {"ta_keywords": "continuous emphasis modeling;adaptive training;state clustering;hybrid system;systems;approach;effectiveness;experiments", "pdf_keywords": ""}, "c1546da843be7ea3e0adfb85b69a0b08d41c7159": {"ta_keywords": "spectral function;asymptotic behaviour;problem", "pdf_keywords": ""}, "5b3ca06a7673e2bf372d5f89afb15ae1eb714075": {"ta_keywords": "probabilistic model;novel probabilistic model;discrete time version;random events;discrete network;discrete data;random variables;continuous time;dynamics;systems;model;class;power;impact;set", "pdf_keywords": ""}, "594827fdb2047bc7be4ea2f0d2364f46d187247e": {"ta_keywords": "speaker verification;automatic speech recognition;subtitles;speech;youtube;world data;network structure;knowledge;data;sequence;method;relevant parameters;same tasks;conventional approaches;novel method", "pdf_keywords": "new japanese speech corpus;japanese speech corpus;japanese asv corpus;short speech videos;automatic speaker verification;large speech datasets;spontaneous speech corpus;speaker verification;speech corpus construction strategy;voice recognition;new speaker videos;subtitles;large corpus;speech recognition tasks;automatic speech recognition;autonomous speech;jtt corpora;advanced speech recognition;japanese asv;speech languages;novel deep learning framework;speaker alignment;scalable speech;new japanese version;utterances;youtube videos;machine learning framework;utterance;new speaker;cnn"}, "539631a828bf0badd20d2241784b4e06c223250e": {"ta_keywords": "continuous multiscale mixture model;novel sampling method;sampling;conventional sampling;clustering experiments;clustering performance;speaker;model estimation;nonstationary noise;gibbs;average;methods;method", "pdf_keywords": ""}, "d5181375d242ed181bcde0d682a3c7ec4c4c6102": {"ta_keywords": "improved communication skills;social communication;early communication;communication;late communication;communication environments;training;people;method;types;population", "pdf_keywords": ""}, "562fe9b2f5e7ede128dd9a93edc3971c5e0a2394": {"ta_keywords": "artificial dialog acts;dialog acts;dialog act;future dialog act;generative model;interlocutors;prediction;knowledge;performance;new approach;equations;approach;set;factor;use;results", "pdf_keywords": ""}, "05710169c48ac1ffe6af514cc10e72d025023343": {"ta_keywords": "random walk model;spin;dimensional gaussian;strong coupling;electric field;coupling;current;numerical study;predictions;model;limit;effect;results", "pdf_keywords": ""}, "14a6452d6d026a3f384e425add6ab68f8e65037f": {"ta_keywords": "public crowdsourcing;efficient data labelling process;data labelling;crowdsourcing;real label collection tasks;data;participants;practical implementation;tutorial;practice session;concept", "pdf_keywords": ""}, "ea1ba6f5e5852e38ace7bbae4e4f60ffeeabe5b1": {"ta_keywords": "microresonators;shear stress;such pairs;formation;phenomenon;simultaneous action;important factor;important ingredient;form;consequence", "pdf_keywords": ""}, "70a2a554829f2cebb9fa89829994444fa1ec5a7b": {"ta_keywords": "collective decision maker;collective decision makers;collective decision;gaussian norm;distance;time;features;notion;number", "pdf_keywords": ""}, "39c5740304b5f4072f92e4e012a4b57e7bc2e817": {"ta_keywords": "channel speech separation;channel separation;noise ratio;novel signal;evaluation;validity;metrics;verification;standard approaches;sr;fidelity;method;terms;novel method", "pdf_keywords": ""}, "470fd4faf9b8499e8bf21c5d143145305d07fe83": {"ta_keywords": "tweets;useful collective entities;collective entity;space;time;fact;large number;method;new method", "pdf_keywords": ""}, "5667f934c5bc008d0464878729eed34cbf7ec1df": {"ta_keywords": "unlabeled data;supervised learning tasks;graph classifiers;modeling capability;possible outcomes;set;novel approach;approach;experimental results;use", "pdf_keywords": ""}, "2f1743d1a1be46452ab90691ead8bf916ffd912b": {"ta_keywords": "parametric oscillator;parametric parametric oscillator;parametric noise;parametric gain;oscillating field;stationary state;stability;amplitude;simultaneous measurement;phase;state;gain;method;loss;new method;results", "pdf_keywords": ""}, "402ab2adcf9da95e6aad9884b1ec53271f39cd32": {"ta_keywords": "kalman filter;novel inverse filters;inverse state;inverse;nonlinearity;theoretical stability guarantees;ekf;time systems;unknown matrix approaches;space models;extended versions;unknown input;class;formulations;order;case", "pdf_keywords": "extended kalman filter;several inverse filtering systems;kalman filter;inverse filter;inverse state;nonlinear filtering;nonlinear system model;linear filtering;dithered inverse;forward filter;state estimation;inverse;stochastic filters;forward filters;new forward filter;different forward filter;forward estimation procedure;inverses;iterative filtering;ekf;dynamical systems;noisy system dynamics;extended kf;same initial state estimate;new filter;filter;linear systems;nonlinearity;state transition function;filters"}, "f82ae0a87cae2f3a43d4c0289d0cdf7ca57461d0": {"ta_keywords": "orbit coupling strength;orbit coupling;zeeman field;spin;effect", "pdf_keywords": ""}, "217074971e3dfdbfab3a8c3819cd7953ae666da4": {"ta_keywords": "genetic association database;genes;genome;complex traits;researcher;researchers;stable text;mining system;database;articles;knowledge;features;modularity;implementation;effec;behavior;generic framework;system;tiveness", "pdf_keywords": ""}, "1b759204e7c13f0e4af9fe00b052af4456ac3669": {"ta_keywords": "reinforcement learning;reinforcement learning algorithm;smaller task horizon;skipping;repetition;gain;frame;action;performance;task;inertia;loss;dependent quantity;terms;price;effect", "pdf_keywords": "reinforcement learning;shorter task horizon;reinforcement learning problem;simple reinforcement learning;optimal strategy;game theory;effective task horizon;policy gradients;task horizon;optimal learning parameters;greedy policy;field game;defensive play;skipping;learning;learning process;repetition horizon;strategy;reinforce;game;soccer;payoffs;smaller task;effective strategy;control;frame;decision processes;infinite experience;action sequence;actions"}, "c1125fa33a239ef4fd3378ccd46b2a0a0cf79a15": {"ta_keywords": "data storage system;novel storage system;asynchronous cascade cascade;arbitrary data structures;modular format;data;modular distribution;modular modular distribution;erasure;system;hitchhiker;flexibility", "pdf_keywords": ""}, "ac8d33e4c0a45e227a47353f3f26fbb231482dc1": {"ta_keywords": "joint modeling;apjstion suite;text;associated proc;aps;aps division;computer science;novel method;lond;baltimore;maryland;64th annual meeting;november", "pdf_keywords": "aware language models;language models;temporal knowledge;language modeling objective;scale news corpus;language model;complete temporal spans;knowledge probes;temporal context;natural language systems;scale language models;knowledge representation;aware model;different time slices;factual queries;unified text;linguistic structure;timestamp;knowledge graphs;neural sequence model;computational computational linguistics;training models;sequential versions;snapshots;knowledge;noisy data;salient span;language;text;articles"}, "9b1057e1f6eb17abf3962d6cd2f49468d27b94c6": {"ta_keywords": "pause prediction model;target speech;emphasis;speech;translation;paper studies translation;words;model;set;experiments", "pdf_keywords": ""}, "c985600f0aa223ddc76a2ea628f1fa23504dcbcd": {"ta_keywords": "target speech extraction module;speech recognition module;target speech recognition performance;automatic speech recognition;target speaker;training objectives;asr;minimum training requirements;location;signal;parallel clean data;system;novel method;end;presence;paper", "pdf_keywords": ""}, "0e141942fa265142f41a2a26eb17b6005d3af29e": {"ta_keywords": "languages;representation;dynamics;rle;qualitative analysis;relevance;art;current state;problem", "pdf_keywords": "language taxonomy;language resource availability;linguistic diversity;computational linguistics;language resources;language discovery;different languages;language inclusion;language domain;languages;example languages;language representation;linguistic disparities;language;language structure;rich languages;functional languages;natural language processing;poor languages;taxonomy;popular languages;language discrimination;nlp;european language discrimination;many resource;resource;digital resources;resource availability;taxonomical hierarchy;data availability"}, "683e201783bf76ab99791a02e3763fd3ab8dad96": {"ta_keywords": "entity recognition;active learning;labeling;deep learning;training data;data;classical methods;amount;method", "pdf_keywords": "deep active learning algorithms;cnn word encoder;entity recognition;best deep models;novel deep learning model;other deep models;deep learning;deep bayesian active learning;long short term memory;active learning;practical active learning algorithms;deep neural network;cnn;active learning algorithms;standard deep learning representation;convolutional word;sequence tagging;active learning algorithm;recurrent neural networks;standard deep learning module;ner task;labeling efficiency;labeling task;cnn cnn cnn target encoder;entity;encoders;ner;training data;shortest sentences;standard supervised model"}, "e318e554098224c9475dfc80765cbbb82fa4a409": {"ta_keywords": "efficient peer review;conference organizers;algorithmic side;algorithms;fairness;th aaai conference;strong theoretical guarantees;world experiment;experimental procedure;work;issue;novel", "pdf_keywords": ""}, "8c9033f976e7787dde9af5ba952d7f9ac9c34496": {"ta_keywords": "outlier detection;data streams;novel streaming algorithm;streams;large data volumes;feature;life datasets;density;robustness;row;algorithm;accuracy;art detectors;variety;state;approach;terms", "pdf_keywords": ""}, "36cbc3c24429ba3b69def38e6e64b41485b0a023": {"ta_keywords": "maximum entropy;data;subset;measurements;new method;set;method;collection;ability", "pdf_keywords": ""}, "119c33321fc0e1db837ce293f1b65cc26c1cc34e": {"ta_keywords": "novel reranker;key sentences;sentences;articles;event;mtm;pattern information;world datasets;sound;novel;ethyl airliner;claims;experiments;methods", "pdf_keywords": "claim detection;sentence vectors;lexical similarity;debunk claims;false claims;novel reranker;sentence templates;candidate articles;recent tweets;news stories;whole candidate articles;lexical information;claim quality;feature matching;recent claim;tweets;sentence relevance;claims;similar sentences;feature extraction;semantics;feature;recent tweetswe;semantic information;key sentenceswe;articles;key sentences;claim;fact;content"}, "708dcd8456426cd609c89a86344e0007c04c80bf": {"ta_keywords": "multilingual benchmark;several benchmark languages;multilingual knowledge access;languages;cloze;code;type probes;algorithm;switching;novel;method;effectiveness", "pdf_keywords": "novel multilingual benchmark;multilingual language models;multilingual natural language processing;multilingual np;language model;monolingual bert;resource languages;syntactic prediction tasks;languages;syntactic prediction;slo languages;token entities;language;linguistic similarity prediction;token counterparts;natural language;linguistic similarity;object benchmark;knowledge bases;multiple tokens;tokens;np benchmark;mask tokens;multiple mask tokens;entity;many popular entities;underlying words;queries;many popular entities consistwe;fr nl"}, "db0c587111cfed85dcea413e385b17881e6e0cbb": {"ta_keywords": "neural network language models;covariance matrix adaptation;structure discovery;parameter tuning;parameter optimization problems;parameter optimization problem;recognition performance;training configuration;black box;other black box;method", "pdf_keywords": ""}, "3b30422b372040ad19a713b35006c21808287720": {"ta_keywords": "erasure code design;erasure code;storage;network bandwidth;codes;reconstructions;regeneration;network;optimality;os;msr;reduction;design;evaluation;typical parameters;respect;number;terms", "pdf_keywords": ""}, "6fa7de6f3ce3a599de6fab273a0d43939e176e9d": {"ta_keywords": "agent;simulated human;assistant;navigation problem;policies;decision process;responses;useful information;framework;rich forms;practicality", "pdf_keywords": "agent communication;navigation agent;navigation task;imitation learning;human communication;human assistant;assistant;reinforcement learning;reinforcement learning systems;task request;dialogue;training agents;agent;human assistance;reinforcement learning framework;reinforcement learning approach;natural language explanations;novel reinforcement learning framework;tasks;learning;information;assistance;agents;communication;requests;arbitrary request;simple reinforcement learning approach;intentions;navigation;natural language"}, "48e4ba2d04bd98843d5aab6e227b29584d63f7b6": {"ta_keywords": "nonlinear crowdsourcing;linguistic cooperation;nonlinear linguistic interface;existent linguistic genres;linguistic resources;cooperation;existent genres;annotated bibliography;interface;dissemination;survey;use;results;standard tool;predictions;performance;article", "pdf_keywords": "crowdsourcing taxonomies;crowdsourcing;crowdsourcing methods;linguistic content sharing;popular russian linguistic resource;collaborative resource;linguistic resources;annotated examples;datasetthe opencorpora collaboration;teammates;participants;resources;cooperation;games;teams;opencorpora;literature;cooperation patterns;other teams;lexical substitutions;team;text;cooperationwe;genres;source code;social networks;dataset;examples;human networks;opportunity"}, "43a87867fe6bf4eb920f97fc753be4b727308923": {"ta_keywords": "efficient transfer learning methods;tuning methods;models;parameters;new parameter;art parameter;efficient fine;specific hidden states;tasks;modifications;unified framework;methods;design dimensions;different methods;set;state;paper;design;comparable results", "pdf_keywords": "efficient transfer learning methods;language models;machine translation;deep bidirectional transformers;frozen transformer models;attention;transformer network;attention module;retrieval tasks;general language understanding;natural language processing;transformers;efficient tuning methods;efficient tuning;downstream tasks;novel tuning model;attention points;hidden representations;tuning;tuning process;pretraining;tasks;tunable prefix tuning;models;universal prefix tuning;translation;various hidden representations;natural language understanding;text summarization;tunable parameters"}, "3e24375a1810375183d47ceadc7418e94533ba5f": {"ta_keywords": "novel online scheduling algorithms;perishable allocation;online allocation;perishable resources;efficiency;offline setting;computational power;algorithms;energy;empirical performance measurements;resources;fairness;objectives;better performance;range;ones;mechanisms;problem;novel approach", "pdf_keywords": ""}, "b63b698d177ba2861fe97d23763d66324bb1236a": {"ta_keywords": "mutant replicates;hemagglutinin glycoprotein ligand;chimeric mutant;m2 protein;m2 transmembrane domain;transmembrane domain;ion channel activity;replication;multiple cycles", "pdf_keywords": ""}, "dcd39e2eb27d17c369f3bf7a5a7a2a30bb9201c8": {"ta_keywords": "language models;unlabeled text data;semantics;transferability;datasets;downstream tasks;dataset;benchmark gglue;knowledge;models;model;scale;effect", "pdf_keywords": "long shortterm memory;language models;downstream tasks;deep learning;deep linguistic modulation;better thandownstream tasks;natural language processing;better downstream performance;natural languages;downstream performance;learning;natural language;human language;underlying language;accurate representations;semantics;linguistic linguistic task;high lexical overlap;tasks;linguistic structure;synthetic datasets;powerful representations;nlp;tune transformer models;explicit token dependencies;baseline models;representation;models;artificial datasets;task"}, "c6af1ad95917badd7bc65b303a40f54950360279": {"ta_keywords": "statistical dialog manager;conventional statistical dialog managers;dialog manager;automobile dialog;bayes decision theory;penalizes system actions;inefficient actions;rule;destination;efficiency cost;regularization cost;system;penalizes;task;parts;method;experiments;paper", "pdf_keywords": ""}, "595306f993993e44e2c2f674367103f44df03d9b": {"ta_keywords": "resource machine translation;unsupervised machine translation framework;translation quality;translation baselines;data augmentation;resource words;resource datasets;resource data;general framework;experiments;methods", "pdf_keywords": "translation augmentation;resource machine translation;neural machine translation;current neural machine translation systems;machine translation;word translation strategy;monolingual translation;resource translation;multilingual translation;parallel translation;forward translation;translation accuracy;low resource languages;monolingual data;translation process;generalized data augmentation framework;standard supervised backtranslation;backward translation;data augmentation methods;data augmentation;bidirectional word induction;large parallel corpora;language pairs;resource language;word substitution;translation;bidirectional encodings;augmentation;language model;shot translation"}, "5d6f87e31d806a77d22e344106d0310be3342259": {"ta_keywords": "review assignments;peer review;optimal assignment;assignment probability;reviewers;reviewer;peer;algorithms;certain papers;arbitrary constraints;evaluation;particular paper;datasets;past conferences;small probability;practical special cases;problem;framework", "pdf_keywords": "peer review algorithm;conference peer review;peer review;randomized paper assignments;dishonest reviewer assignments;peer review process;decentralized assignment;reviewer deanonymization;malicious paper assignments;reviewers;reviewer set;optimal assignment;reviewer;peer review editor;implementable assignment probabilities;optimal assignment subject;deterministic assignment;optimal assignment problem;assignment process;sparse assignment;certain papers;assignment algorithm;truthful favorable reviews;review score;assignment;sparse assignments;dimensional symmetric assignment problem;dimensional symmetric symmetric assignment problem;ratings;optimal stochastic fairness objective"}, "68d0b245e9754de9f36cba305e4ce50ff868cb6a": {"ta_keywords": "effective grammar;grammar;sentences;elementary words;simple algorithm;arbitrary structure;algorithm;description;induction;context;principles;set", "pdf_keywords": ""}, "7cc74ffa1215321712d4a830bb9dee19d9f0fb47": {"ta_keywords": "compositional generalization problem;structured graph;generalization;composition;complex test structures;query clauses;complex inputs;conjunctions;graph;freebase questions;permutation;input;first scalable model;invariant representations;novel method;method;ability;form", "pdf_keywords": "semantic compositions;compositional freebase query dataset;compositional generalization;compositional generalizations;query representations;syntactic compositions;challenging compositional freebase questions;natural language questions;syntactic composition;semantic structure;syntactic structures;parsers;conjunctive query graphs;language representations;linguistic structure;structured predictions;syntax information;semantic predicates;graph decoding;linguistic features;deep representations;conjunctive queries;graph decoder;semantics;compositional vectorwe;sentences;query clauses;structured graph;query patterns;generalization"}, "ee3ce47b79917974d30b6eeaaeeba99f1b1a5c59": {"ta_keywords": "network;models;kolmogorov;speed;performance;rpt;end;archi;smirnov;type;novel set;tectures;framework;study", "pdf_keywords": "deep learning network;neural network model;deep learning tools;new deep learning toolkit;encoderde training architecture;pattern recognition;transfer learning;matrix;network;global network network design;decoder training;multiple global network connections;multiple sensors;rnn;neurons;sensor identification;speech recognition;global network design problem;encoder;new algorithms;end speech recognition;labels;usual rnn;inputs;models;remote control;higher accuracy;nodes;wireless network;recurrent transformer model"}, "5f96e3e00b36c5eeebff09a1bf4c804bd4ce4620": {"ta_keywords": "quickest attack detection problem;false data injection attack;remote state estimation;attack;probability belief;quickest detection;optimal policy;estimator;algorithms;algorithm;state;decision process;system;numerical results;significant performance gain;paper;structure", "pdf_keywords": "discrete time attack detection problems;quickest attack detection problem;attack detection problems;quickest attack detection;attack detection cost;attack detection problem;attack detection;linear attack scheme;malicious sensors;linear attack parameters;attacks;observable probabilistic decision process;false data injection attack;discontinuous attack;remote state estimation;attack scenario;attack strategy;remote estimator;noisy sensor network;discrete time sensor systems;noisy linear observations;sensor detection;attack initiation;remote estimation;attack;estimation process;quickest detection;physical attacks;optimal estimation method;attacker"}, "64a106b707586345a055aa22c3356c4dc3d01877": {"ta_keywords": "complex networks;network connectivity;connectivity;network;dynamics;interaction;particles;random variables;model;concept;set", "pdf_keywords": ""}, "d5a95567e079685322cd485033d334284c4b0a62": {"ta_keywords": "reversible polymerization systems;reversible polymerization;new polymers;polymer science;polymer materials;small monomers;macromolecules;such systems;recent developments;properties;understanding;potential;perspective;new insights;role", "pdf_keywords": ""}, "f16cf130ae75d1ea1ad3b926f605adef41af4af1": {"ta_keywords": "uncertainty propagation;uncertainty;dynamic systems;propagation;conservation laws;conservation;generalized theory;method;small group;study;first steps;results", "pdf_keywords": ""}, "b13e9d23983273c0c67b91ae70c55d4c3f745b8b": {"ta_keywords": "simultaneous translation;art machine translation methods;translation;languages;nodes;novel framework;framework;predictions;presence;state;large number;efficacy", "pdf_keywords": "neural simultaneous machine translation;neural machine translation;simultaneous machine translation optimization strategy;simultaneous machine translation;simultaneous translation task;machine translation;translation agent;simultaneous greedy machine translation algorithm;simultaneous speech translation;simultaneous translation;translation process;simultaneous mt translation;translation quality;meaningful translation results;simple machine translation algorithm;speech translation;efficacysimultaneous machine translation;translation signals;translation times;incomplete source sentences;target language pairs;delay;trainable encoder;reinforcement learning;input words;language pairs;next source word;nmt;neural network;novel reinforcement learning algorithm"}, "ed1c17451a23471afde91c109ecadc6aab8b2ba6": {"ta_keywords": "multimodal disinformation detection;disinformation;misinformation;effective detection;temporal information;detection;information;network structure;propagation;different types;types;ii;survey;use;effort", "pdf_keywords": "multimodal fake news detection;false news detection;disinformation detection;multimodal disinformation detection;truthful detection;disinformation detection lack suchdisinformation;fake news research;user credibility;credibility;deceptive behavior;multimodal social media;false detection;veracity;rumorthe;false information;fake news;spontaneous belief propagation;false news;multimodal social media users;false claims;belief propagation;digitalcyberbullying;harmful content;negative viral memes;underlying message;false memes;multimodal labeling;social media;factuality;digital cyberbullying"}, "8c838c7631a7408d5ea5801c9360213782665c9c": {"ta_keywords": "spin dynamics;planar spin;spin;orbit interaction;type polymer;planar geometry;valve;theoretic method;model;field", "pdf_keywords": ""}, "db1dafd0c356491cbbf53338b9984de324e7239c": {"ta_keywords": "bilingual lexicon induction;minimal supervision;isometry;learning procedure;spaces;supervision;novel method;accuracy;method", "pdf_keywords": "bilingual lexicon induction;bilingual word embeddings;bilingual dictionary induction;bilingual lexicons;language matching;distant language pairs;optimal word embeddings;bilingual dictionaries;language pairs;unlabeled language model;language pair;computational linguistics;large multilingual datasets;monolingual dictionary resources;word pairs;distribution matching;embeddings;similarity;orthogonality breaks;isometric assumption;unsupervised mappings;weak orthogonality constraint;languages;isometry;isometric transformation;orthogonality assumption;hausdorff distance;iterative dictionary refinement;flexible orthogonality constraint;language"}, "9bb9b23823b45ba7521d872bb3e970ede4aafb8a": {"ta_keywords": "random walks;random walk;network;method;large number;set", "pdf_keywords": ""}, "41675d91ad815f64b0df382c0944247811a62cc9": {"ta_keywords": "machine translation systems;nonparametric inversion transduction grammars;single machine translation system;phrase tables;phrase extraction approach;phrase table;step word alignment;corpus;novel bayesian inference framework;several language pairs;phrase;words;novel method;model;accuracy;experiments;use;method", "pdf_keywords": ""}, "1fc6290fa3e6784501ca67cbef33a6a8edcbdb9e": {"ta_keywords": "empirical measure;empirical measures;general measures;general compact metric spaces;sample;convergence;independent samples;sample rates;sample results;size", "pdf_keywords": "empirical measures;empirical measure;general measures;measures;underlying measure;convergence rates;probability measures;measure;metric increases;asymptotic size;convergence rate;planar probability measure;asymptotic behavior;general compact metric spaces;metric;optimal quantizationwe;large distance;planar measure;compact metric space;scaling regime;convergence;isserstein distance metric;same convergence;kolmogorov;wasserstein distance;quantization;converges;optimal transport bounds;large particles;relative size"}, "f5b6819e05e087d2bbae3ddb6d58d2ab4e1d7ca2": {"ta_keywords": "inverse reinforcement learning;reinforcement learning;autonomous cyber;policy orchestration;contextual bandit technique;physical agents;agent;constrained policy;environmental rewards;reward;demonstrations;orchestration;algorithms;policies;pac;novel approach;best actions;approach;set;novel ways;unspecified constraints;man", "pdf_keywords": ""}, "bfb13c6889626e833bf449fdb361d186467919af": {"ta_keywords": "feature feedback;sentiment analysis;natural language inference;domain evaluations;text;novel methods;datasets;experiments;methods;domain;sample gains;efficacy;analysis", "pdf_keywords": "feature feedback;feature feedback model;natural language inference;natural language processing;underlying sentiment;sentiment analysis dataset;domain evaluation;phrases;sentiment;natural languagewe;text mining;deep learning tasks;classification models;classification;classification accuracy;sentences;classify framework;domain performance;intuitive appeal;domain datasets;domain benefits;examples;performance gains;classify;evaluation;yelp reviews;training data;results;model trainedwe;text"}, "df9949abc06cb4f0f4c0ac1eb7ce0bc62ed5ec02": {"ta_keywords": "pointwise approach;art;problem;state", "pdf_keywords": ""}, "5331a846c854c3ecedf9ecf3ea516cb6dcaba4c8": {"ta_keywords": "saliency benchmark framework;saliency methods;model reasoning;empirical validation;performance;novel framework;framework;methods;complexity;synthetic tool;development;set", "pdf_keywords": "saliency methods;new saliency methods;saliency applications;new saliency method;saliency information;saliency library;real object detection scenarios;elliptical reasoning framework;world object detection tasks;model reasoning;models;object detection problems;synthetic framework;relevant object;reasoning;objects;model;more objects;relevant features;elementary reasoning;benchmark;synthetic evaluationwe;truth reasoning;deep network;complex reasoning;object;prediction;feature;context;simple model"}, "807600ef43073cd9c59d4208ee710e90cf14efa8": {"ta_keywords": "neural information retrieval model;information retrieval;heterogeneous evaluation benchmark;type models;type;interaction;systems;number;work;framework", "pdf_keywords": "neural information retrieval models;different retrieval models;diverse text retrieval tasks;text retrieval tasks;many different retrieval strategies;information retrieval;retrieval datasets;dense retrieval;different retrieval tasks;theart retrieval systems;heterogeneous evaluation benchmark;heterogeneous benchmark;novel evaluation benchmark;query classification;beir benchmark;neural query;benchmark datasets;new benchmark;comprehensive evaluation benchmark;medical information retrieval;evaluation benchmark;benchmark;major neural models;document embeddings;annotation selection bias;neural models;accurate queries;unstructured datasets;lexical matching;diverse datasets"}, "3a40cdd82f0706cda6c247e586d5054abeab4e1f": {"ta_keywords": "list expansion;list questions;new list;original list;expansion;candidate answers;original answers;sde;answers;hybrid approach;hypothesis;output;context;paper;use", "pdf_keywords": ""}, "2c3d02ce8780cc6648caf4ee996d9628c6388751": {"ta_keywords": "real crowdsourcing datasets;unique probabilistic labeling model;minimax conditional entropy principle;labels;objective measurement principle;ordinal labels;ground truth;item difficulty;worker ability;variety;method", "pdf_keywords": "real crowdsourcing tasks;probabilistic labels;real crowdsourcing datasets;crowdsourcing;unique probabilistic labeling model;minimax conditional entropy principle;generalized labeling model;deterministic labels;conditional entropy;labeling;binary labeling task;multiclass labels;label assignments;multiclass labeling problem;noisy labelers;maximum entropy;true label distribution;labels;conditional entropy method;labeling process;dimensional empirical confusion tensors;noisy labels;label;crowds;theconditional entropy;empirical representation;entropy;ordinal labels;prescribed discrete discrete discrete label;true labels"}, "ce5ff42d629e67a84731b3c62b57b47fc7f2b20d": {"ta_keywords": "autonomous speech recognition problem;encoder;novel network learning technique;novel streaming;attention;network;network data;input sequence;aware inheritance mechanism;self;blockwise;several online tasks;novel context;significant accuracy;structure;computation;process", "pdf_keywords": "end speech recognition;synchronous decoding;synchronous beam search algorithm;synchronous beam search;contextual block encoder;synchronous beam search technique;automatic speech recognition;neural transducer;beam search;encoder;decoder;decoders;theconventional beam search;novel encoder;scale speech corpus;streaming processing;unconstrained beam search;decoder layers;encoders;chunkwise stream;conventional beam search;optimal attention allocation;rnns;encoder layers;noisy encoder frames;attention allocation;streaming;message generator;synchronous node;attention"}, "945d4addf8e94487f6199af71dc15a298791c1b4": {"ta_keywords": "channel fading process;energy arrival process;infinite horizon decision process;energy harvesting source;wireless channel;remote sensing;mdp;time;information;ii;scenarios;age;problem;paper", "pdf_keywords": "energy packet arrival probability;energy arrival process;energy harvesting processes;energy harvesting server;energy harvesting problem;optimal channel;optimal transmission model;optimal channel activation policy;energy arrival;energy harvesting efficiency;energy harvesting process;energy harvesting;optimal source node;optimal sampling policy;observation packets;finite energy buffer;fading channel;optimal sampling;energy buffer;data transmission policy;packet generation process;markovian scheduling policy;arbitrary channel state action;packet channel;channel state information;generic packet channels;markovian error correction;optimal estimation;packet packet channel;sampling strategy"}, "27df24c537b2d3c2a769d917adf92a6a059c5917": {"ta_keywords": "neural operator methods;neural operator representation;multiscale problem;finite element methods;continuum model;particledynamics;particle system;fidelity model;empirical accuracy;underlying model;novel approach;computational efficiency;approach;response;framework;problem;efficacy", "pdf_keywords": "multiscale modeling;multiscale coupling framework;new multiscale coupling framework;neural operator representation;linear multiscale model parameters;multiscale dynamics;multiscale problems;neural operator;efficient computational surrogate;novel coupling framework;discretized functions;computational modeling;surrogate model;particle model;coupling method;numerical model;network approximation;sparse features;collective collective mode approximation;neural networks;empirical coupling;neural network;standard finite element method;rigorous continuum model;continuum model;tractable deep neural network;microscopic particle system;stochastic partial differential equations;coupling framework;numerical simulations"}, "46ef61536a01578e79b6d4e35e803a914afeb629": {"ta_keywords": "2d electron gas;inelastic scattering;electron wave;electron gas;resonant emission;2d;new type;observation;ins", "pdf_keywords": ""}, "65ee083ce61576955d76b36819bf3ac271335597": {"ta_keywords": "exact regenerating codes;storage systems;peer systems;linear code;codes;uniqueness;explicit construction;parameters;sufficient conditions;paper;construction;property", "pdf_keywords": "exact regenerating codes;data storage repair;code construction algorithm;codesregenerating codes;minimum storage point;data storage;storage systems;efficient codes;code construction;network code;minimal field size;nodes;field size requirement;original code;code;minimum bandwidth point;such codes;subspace approach;subspace;node;other nodes;codes;code problem;single parity check code;same node;network density;mail;repair;regeneration;minimum field"}, "092ee3a32b6cd951da971124a24872c7cccf3a9f": {"ta_keywords": "unsupervised transductive transfer learning;protein name extraction;iterative feature transformation;transfer;novel maximum entropy;target domain;training;data;models;process;ift;current state;approaches;technique;comparable performance;problem;art", "pdf_keywords": ""}, "fce10a1a9727cbda33d44b62409e303f1009417a": {"ta_keywords": "recurrent neural network grammars;natural language;rnngs;rnng;rules;model;novel approach;process;problem;head", "pdf_keywords": "recurrent neural network grammars;novel syntactic model;phrase representations;recurrent neural network;word phrase representation;natural language network;language modeling;parsing;simple rnn word;generative neural networks;language model;novel generative model;phrasal representations;discourse parsingwe;new generative model;generative model;probabilistic generative modeling family;novel neural model;syntax;attention weights;recursive tree structure;compositions;constituent;rnn;deep learning;rnngs;rnng;attention mechanism;attention;parsen accuracy"}, "49af035c598901fbf766da2cfb040cca7336a8ac": {"ta_keywords": "semantic parsers;semantic representation;abstract meaning representation;imitation;text;task;exploration;information;graph;reduction;noise;field;most information;novel method;method;areas;amount;recent problem", "pdf_keywords": ""}, "d47ad0a606bedf41dcea614bfa7b7494879c7ba0": {"ta_keywords": "arbitrary text;state changes;state change tuples;crowdsourcing;text;dataset;open domains;entity;state value;attribute;accuracy;generation;set;first time;method;novel method", "pdf_keywords": "natural language processing;new task formulation;state changes;present open source;unstructured datasets;open state change;state change tuples;state changeswe;procedural text;unstructured data;open vocabulary;nlp;language;unseen entities;unmentioned entities;open attributes;entities;changes;text;tasks;available dataset;attributes;novel task;state values;entity;task;attribute;project;large dataset;unseen states"}, "188928df74f9ce00bd1b58686db93ac8cdd07275": {"ta_keywords": "parallel beam search algorithm;multiple speech utterances;multiple utterances;line recognition use;line recognition;search process;loop program;multiple hypotheses;traverse;technique;regard", "pdf_keywords": ""}, "2f369845ae7191196d65310210db2485feb3aa86": {"ta_keywords": "new g2p conversion training method", "pdf_keywords": ""}, "bd2f3822801a7e2f933d06c261b8783764d8ce18": {"ta_keywords": "text classification samples;classes;aberration;deeper layers;samples;commonalities;example;observations;model;adv;distribution;differences;first layer;simple method;attempt", "pdf_keywords": "text classifiers;text classification model;adversarial attacks;binary sentiment classification;classifiers;classifier;optimal text;natural language processing;deep learning models;natural language;deep learning;neural dependency parsers;text;datasets;hidden representations;novel class;word vectors;sentences;novel feature;attack;nonlinear neural networks;class;words;bert;similarity prediction;different datasets;common target word;example;word;representations"}, "576860f910ea8fde366deb03c910ab30cd776966": {"ta_keywords": "classical entanglement distribution;entangled states;quantum field;quantum;particles;separation;collective motion;field;states;large collection;method;simple method;large number;mechanical version", "pdf_keywords": ""}, "95a35473fd1936927dd4a53fe0a5d2d6762d99b3": {"ta_keywords": "musical instruments;musical notes;musical performance;expressive performance;interpretable differentiable digital signal processing;detailed control;synthesis;synthesis parameters;articulation;notes;hierarchical model;timbre;performance;level hierarchy;level properties;dynamics;priors;dsp;level;individuals;option", "pdf_keywords": "realistic neural audio synthesis;hierarchical audio synthesis module;conventional audio synthesis;hierarchical music modeling system;novel music modeling system;synthesizer modules;expressive musical terms;midi;synthesis model;musical notes;realistic audio;synthesis levels;synthesis generator;synthesis;musical instruments;sound instruments;level note transcription;synthesis parameters;hierarchical generative model;musical performance;synthetic neural networks;synthesizer coefficients output;generative model;realistic automatic generation;music;instruments;audio;modeling note expression;acoustic waveforms;real violin recording"}, "55a3b36fd21dbbe9384ab3ba1bcf901235d95f47": {"ta_keywords": "hotpot;large improvements;strong baseline;algorithm;domain;model;final answer", "pdf_keywords": "hard question corpus;simple question corpus;unsupervised decomp;unsupervised learning;extractive decompositions;hard questions;unsupervised training;decoder training;machine translation;questions;supervised learning tasks;unsupervised encoder;high learning rates;common crawl questions;many simple questions;computational linguistics;more accurate questionswe;autonomous learning tasks;level questions;supervised learning pipeline;decompositions;learning methods;supervised learning run;abstractive language model;learning;decomposition models;decomposition;generative model;transduction;simple questions"}, "e153713b0423b4bae325340b2211e704effd5252": {"ta_keywords": "inductive logic programming;fault density;calling tree representation;algorithms;automatic generation;ilp systems;ilp system;ilp;numeric metrics;class;methods;systems;extensions;clauses;performance;reasons;task", "pdf_keywords": ""}, "e718ffe247a61a77b45953a7e8a5b86a45ed579f": {"ta_keywords": "training dependency parsers;corpora;maximum spanning tree;data;mst;training cost;novel approach;approach;advantage", "pdf_keywords": ""}, "84e50df18c284b985d287b462c63c20186cc5da1": {"ta_keywords": "programming;models;novel classes;teaching;demonstration;novel method;method;computer;technique;utility;use;case;means", "pdf_keywords": ""}, "efada589efdb0adf3aa9dc2b6cb6979a50658276": {"ta_keywords": "novel news reporting project;news articles;news article;news;headline;statistical prediction;article;novel data;hypothesis hypothesis hypothesis hypothesis hypothesis hypothesis;statistical analysis;novel feature;data;analysis;claim;agreement;set", "pdf_keywords": ""}, "3c6407554fb4ee599f42501cf5cba8fcefa88783": {"ta_keywords": "automatic speech recognition models;encoder reconstruction error;unpaired data;corpus;consistency training;word error rate;text;words;cycle;end;paper;method;experimental results;approach", "pdf_keywords": "semisupervised speech system;training speech networks;speech network;end speech system;end speech recognition problem;speech networks;automatic speech recognition;dimensional speech model;deep encoder;encoder;standard hidden state encoder;audio data;end speech;end encoder;semisupervised training condition;transcriptions;unpaired data;text networks;speech;decoder;unpaired data scenariowe;textonly data;data augmentation model;consistency training;text synthesis;deep learning experiment;cycle consistency;neural network;multidimensional output encoder;consistency training approach"}, "b568c562fcfad8d7a943a9ea63aca36c487b6d7d": {"ta_keywords": "classifier;relative error;document;documents;background signal;simple analytic formulae;data;own data;method;large collection;new method;form;presence;use", "pdf_keywords": ""}, "afa9364ec48e38d19099cfc22ac9cb679c4baa39": {"ta_keywords": "gender biases;stereotype;gendered entities;intraandity;visual scene;language;scene;vision;context;role;findings;dynamics;larger set", "pdf_keywords": "gendered entities;multimodal language models;gender information;gender bias;intermodality associations;multimodal language model;gendered agent;gender association;genders;linguistic models;linguistic transformer;gender;language bias scores;language transformers;language models;female terms;stereotype;female agent;biases;bias;visual context;language;male agent;sexual interaction;deep bidirectional word representations;male agentwe;sexual agent;recognition;captioning tasks;visual scene"}, "8b5071a38718194063cf17ca446ba8d9f4907a18": {"ta_keywords": "natural language processing tasks;sentiment analysis;simple neural network;dropout;factoid question;tasks;essential features;models;model;network;novel variant", "pdf_keywords": ""}, "4240d8e1e5c2ef82d62ba9d7bb323c357c718c1c": {"ta_keywords": "persistent topological structure;novel topological layer;topological features;general persistent homology;persistent data;layer;deep learning;data structure;classification;arbitrary filtration;filtration;dmpa function;novel adaptation;structured fashion;differentiability;effectiveness", "pdf_keywords": ""}, "1678eccf0f3895dbea6dfac44fc9d4f86de15ff6": {"ta_keywords": "affective communication;emotion trigger;emotion;response;person;ability;problem", "pdf_keywords": ""}, "751816df0027c0ae6c337ba392a5447bef86ca77": {"ta_keywords": "electronic excitations;polymers;state energies;transfer learning models;excited state;efficient transfer;transferability;p3ht;oligomers;crystal;hexylthiopnene;solution phases;novel approach;relative locality;approach;information", "pdf_keywords": ""}, "a4577911d247e472772e2101d21aeaf8f46053cc": {"ta_keywords": "semantic parsing;ambiguous natural language sentence;ambiguous input;explicit paraphrase;free grammars;ambiguity;synchronous context;meaning representation;input;problem;new class;new method", "pdf_keywords": ""}, "fdb3969b654ab01be1807bbf84707a80e6283a52": {"ta_keywords": "materials science journal articles;structured representations;synthesis procedures;procedural text;experimental syntheses;structured representation;scientific entities;inorganic compounds;unsupervised approaches;texts;generative model;articles;supervised models;structures;strong heuristic baseline;expert;events;variety;system;set", "pdf_keywords": "synthesis text;inorganic synthesis procedure text;materials science synthesis text;nlp library;natural language processing;inorganic syntheses;reaction databases;inorganic materials synthesis;chemical synthesis;annotated materials science journal articles;nlp;annotated text;structured representations;material science text;machine learning;action graph structures;nonlinear linguistic structures;linguistic entities;action graphs;synthesis procedure;neural network models;extraction task;complex materials;learning process;generative model;text;linguistic features;annotated articles;newswire text;extraction"}, "19f727b7a42a21bc3f99536e8368029f4b9b8e14": {"ta_keywords": "noun images;lexical resource;word senses;visual world;foreign language;natural language processing;mapping;annotated collection;context;study;paper", "pdf_keywords": ""}, "97ef5081aa4e2984c16ea78b862266e4852c7faf": {"ta_keywords": "search;email;space model;activity;space models;center;content;model;vector;results;future involvement;relationship;data;novel approach;domain;approach;persons;literature;novel problem;use", "pdf_keywords": ""}, "af034b0e893a0a24e41cdb54afb35d4250407f50": {"ta_keywords": "orbit interaction;zeeman field;interaction strength;spin;factor;strength;field;effect;value", "pdf_keywords": ""}, "7657b56d2ac9269b32e8bcbe2a20f99ea17afe09": {"ta_keywords": "converted singing voice;age control;direct waveform modification;dependent modeling;age;spectrum;gender;conventional method;range;method;quality;paper", "pdf_keywords": ""}, "869d53277b0ec5e47a30b874aeb157df88649ea0": {"ta_keywords": "zeeman field strength;zeeman field;zeeman;ep cluster;splitting model;stability;field;framework;interplay;suitable choice;effect", "pdf_keywords": ""}, "bff4d630cbea6a90b149b28caff5489c1a4ccaad": {"ta_keywords": "stochastic differential equation;stochastic process;statistical mechanics;bosons;quantum;particle;statistical properties;coupling;transition;bath;classical;model;state;system;value", "pdf_keywords": ""}, "7e0342b304ca8ce564a664eb17e85358b07488fe": {"ta_keywords": "noise mixture model estimation;only noise mixture model estimation;speaker adaptation;clean speech;joint processing method;squared error estimates;noise;method;improvement;evaluation;minimum mean;paper;results", "pdf_keywords": ""}, "fcdac45272543b4f8b8eaa59d66044d1b7018494": {"ta_keywords": "translation model;pseudoparallel data;multilingual translation setting;validation set;algorithm;novel method;paper;method", "pdf_keywords": "backward translation model;iterative translation;translation model;backtranslation;joint translation;good forward model;translation rate;forward model;new forward model;higher word prediction accuracy;translation protein;translation setting;pseudoparallel data;low resource learning;backward models;backward model;training data;effective training data;translation;ground truth parallel;multilingual nmt;multilingualin;target language;large models;learning technique;learning;generation techniques correspond;resource language;learning rate;training stages"}, "e8f42dd98d7f546036fa4a1109c3fe3dd98f9647": {"ta_keywords": "natural language argument;argument reasoning comprehension;argument;scalable crowdsourcing process;authentic arguments;news comments;correct implicit reasoning;neural attention;language models;claim;dataset;new task;capability;experiments;options;current methods;several systems", "pdf_keywords": ""}, "4358335263622fe189cf95c613f4d6fdcb67fbea": {"ta_keywords": "orbit coupling strength;orbit coupling;electron;zeeman field;magnetic field;spin;field;strength;effect", "pdf_keywords": ""}, "03fff40cff6ac531e340f6ffb376e34609770846": {"ta_keywords": "information warfare scenarios;coordination networks;online crimes;coordination;general network;protests;accounts;groups;spread;recent campaign;framework;context;different kinds", "pdf_keywords": "suspicious social networks;coordinated tweets;social media platforms;influence campaigns;information warfare scenarios;coordinated campaigns;suspicious traces;erratic tweets;viral webthe recent announcement;twitter;social media;tweets;inauthentic accounts;social networks;insocial media platforms;attacks;coordinated accounts;twitter handles;bots;other online platforms;coordinated behavior;detection;hashtags;twitter handleswe;spreading;machine learning;security;suspicious handles;information content;accounts"}, "eb07ff030df4c3dc20e85d89c2e0d0cc730918a0": {"ta_keywords": "unrolled speaker separation;unrolled speaker separation performance;long reverberant conversations;unrolled speaker;joint speaker inventory;unrolled speaker signal;talkers;input signal;parameter;novel method;method;variety;approach", "pdf_keywords": "novel continuous speech separation method;continuous speaker inventory;continuous speech separation;longitudinal speaker separation;speaker inventory construction;biased speech separation methods;target speaker separation;speaker inventory;informed speaker inventory;global speaker information;unstructured speaker content;novel speaker profile selection;target speaker extraction;speaker information;independent acoustic speaker candidates;unstructured speakerwe;additional speaker information;speaker models;speaker enrollments;separation performance;source separation;voigt speaker system;speaker embeddings;speaker;separation;separation task;clustering;unrolled speaker signals;target speaker discrimination;audio sequence"}, "4c66979a31fa4be5b5814fdb5cb8411572d61da8": {"ta_keywords": "normalization;historical data;historical text;linearization;high accuracy;baseline;standard;better accuracy;rule;data;accuracy;novel approach;respect;approach;copies;little loss;small number", "pdf_keywords": ""}, "729260566c7fdf689bb04eaaecef59d40da93ef7": {"ta_keywords": "adversarial images;deep neural network;image classification;detection;principal component analysis;preprocessing algorithm;preprocessing technique outperforms;threshold policy;novel preprocessing technique;random perturbation;stochastic approximation;dnl;threshold values;adaptive version;algorithm;certain combination;paper", "pdf_keywords": "adversarial perturbation;adversarial image;novel adversarial detection algorithm;adversarial images;adversarial image detection;adversarial attacks;image classifier;deep learning models;deep neural network;unsupervised learning;classifier;attacker model;image classification;detection;classical detection algorithm;probabilistic classification algorithms;image classification algorithms;attacker;classifier models;random perturbation;dnn;principal component analysis;principal components;false detections;classifierwe;probabilistic model thatin;nonlinear parametric model;test image;nonlinear iterative filtering;image"}, "ede8ba65c4db10d357d9c3bf8e75b092f536fc84": {"ta_keywords": "panoramic action spaces;language navigation;baseline instruction follower;novel speaker model;new speaker model;standard speaker models;new speaker instructions;data augmentation;pragmatic reasoning;model;benchmark;performance;design;approach;success rate;integration", "pdf_keywords": "language navigation tasks;natural language instruction;navigation tasks;language navigation;navigation task;baseline instruction follower;navigational instructions;linguistic tasks;vision navigation challenge;panoramic action space;navigation;data augmentation;natural language;deep learning;final navigation performance;instruction;language;synthetic instruction data;panoramic representation;new scenes;navigation routes;semantic structure;action space;vision;pragmatic inference;unsupervised learning;supervision;scene;linguistic structure;route description"}, "d5634a21b3727258822b78f5c5ababf7261a5c79": {"ta_keywords": "speech enhancement;learning;rydalis representations;baseline features;downstream tasks;separation;self;nir;fbank;log;methods;experimental results;sph", "pdf_keywords": "comprehensive speech separation;speech separation;speech enhancement;speech enhancement model;speech prediction;speech enhancement mask downstream;robust speech processing;generative adversarial networks;gan;generative adversarial network;speech synthesis model;generative networks;separation techniques;discrete speech representations;speech representations;speech mixtures;unsupervised pretraining;separation downstream;separationwe;underlying speech;former suppresses background noise;parallel speech models;separation;clean signal;unsupervised learning;speech;unlabeled data;noise;speaker;noisy mathematical signal"}, "3813627f7fec57aa4c15b791e36912f470273bb1": {"ta_keywords": "discussion hashtags;twitter content;topical clusters;hashtags;online social media users;hashtags shift;topics;content sharing;pandemic;content;quality clusters;others;results;hundreds;millions;course;study;form", "pdf_keywords": ""}, "3e8420d1bebb3f93d285da2de801d2e43b290880": {"ta_keywords": "neural sequence taggers;slot tagging datasets;massive multilingual ner datasets;noisy pseudolabels;benchmark datasets;shot training;learning;unlabeled data;training;meta;self;techniques;error propagation;art systems;state;large amounts;effectiveness;experiments;method", "pdf_keywords": "neural sequence labeling models;neural sequence taggers;training data;training framework;deep learning;available training data;labeling;deep learning models;language models;training parameters;supervised bert;supervised bert model;supervised tasks;learning;language model;supervised learning;labels;label scarcity challenge;deep neural networks;training;supervised model;limited labels;single label;few labels;training process;learning objective;benchmark datasets;active learning;token classification layer;unlabeled data"}, "e51bca890c004c43b25c5a5e7aa968fe70ec2668": {"ta_keywords": "graphical models;graphical model;hyperlinked web page;graph;structure;thesis;task;more features;class", "pdf_keywords": ""}, "622e05f5d3dd430644288d5048f6050f37947de7": {"ta_keywords": "entity recognition;similar hierarchical priors;natural language data sets;novel hierarchical prior structure;feature spaces;transfer;common structure;task;model;groundwork;class;future exploration;degrees", "pdf_keywords": ""}, "146b84bdd9b9078f40a2df9b7ded26416771f740": {"ta_keywords": "inverse reinforcement learning algorithm;inverse reinforcement learning;loss function;risk;decision processes;grid world example;gradient;agent;decision;observed behavior;novel approach;canonical benchmark problem;model;work;technique", "pdf_keywords": ""}, "77899bac8f463b7a77c0c282748e989d419386e7": {"ta_keywords": "possible ld formulations;ld formulations;discrete difference;formulations;ld;improvements;set;method;suite;full set;advantage", "pdf_keywords": "supervised classification;hyperlinked text classification;supervised learning setting;classifier;network classifiers;learning results;machine learning;text classification;neural network classifier;nonlinear learning;entropic regularization;supervised version;entropic regularization constraints;supervised information extraction pipelines;bioinformatics;learning;feature classwe;learning algorithm;feature class;regularization rule;new learning algorithm;neighbour entropy regularization;information extraction;ensembles;feature space;probabilistic prediction;benchmark tasks;unlabeled examples;relation extraction;hyperlinked text"}, "1a53e7446274016f737236bdd48e3ff05d966384": {"ta_keywords": "code snippets;code pairs;natural language;correspondence features;code;classifier;python;features;mined nl;nl;quality;neural networks;data;structure;correlation;probabilistic model;novel method;sets;method;hand", "pdf_keywords": "single code mining classifier;accurate code predictions;natural language pairs;structured code snippets;code fragments;linguistic annotations;code snippets;code snippetes;natural language;original natural language query;code segment candidates;relevant code snippets;natural language intents;candidate snippets;annotated data sets;code segments;annotated examples;annotation;code snippet;annotators;snippet pairs;code blocks;mining intent;candidate snippet;classifier;linguistic input;code;informative features;open source;training data"}, "d0e9c5cb669dec908a38eab4315cbf101bc4b0a0": {"ta_keywords": "domain adaptation;machine translation;neural language models;data selection;large language pairs;selection;adaptation;comparative study;use", "pdf_keywords": ""}, "308eb6751a3a1da0f64f291366c8ee27f84b3f16": {"ta_keywords": "einstein condensate;dimensional bose;repulsive attractive interaction;condensate;interaction strength;magnetic field;ground state;interaction;presence;effect;strength;wide range;values;factor", "pdf_keywords": ""}, "2550fafc0cbd8bbf7aadd864ac569596d33db038": {"ta_keywords": "text;data;definition;context;notion;few examples;powerful tool;use;future research;way", "pdf_keywords": "knowledge grounding;network science;grounding;grounded interactions;cognitive science;grounding model;ground;cognitive sciences;cognitive science literature;cognition;ground phenomena;successful grounding;grounding signal;cognitive neuroscience;human language semantics;novel network;ground signal;semantics;underlying language;common ground;grounding problem;coordination;computational neuroscience;concepts;mental context;natural language;neuroscience;language processing communities;nodes;language processing research"}, "7ce80c7df1774e4483b32a813d54a8ff35dd0163": {"ta_keywords": "hierarchical game;stackelberg equilibria;simultaneous gradient descent;sum games;equilibrium concepts;gradient;games;best response strategy;twotimescale algorithm;follower;critical points;leader;continuous action spaces;update rule;nash;conditions;class;connections;number", "pdf_keywords": "stackelberg game;differential stackelberg equilibria;stackelberg equilibria;differential stackelberg equilibrium solution concept;stackelberg equilibrium concepts;stackelberg game setting;differential differential stackelberg equilibria;differential stackelberg equilibrium;continuous action space games;hierarchical games;hierarchical game;differential differential differential stackelberg equilibria;learning dynamics converges;nonlinear stochastic games;continuous games;learning dynamics;continuous strategy space;nonlinear generative games;hierarchical play game;simultaneous play gradient descent;continuous game;stochastic games;differential nash equilibria;predictive game;hierarchical dynamics;hierarchical decision;continuous gradient dynamics;continuous game setting;hierarchical decision problem;generative games"}, "203636315f7c9526189d88c541bedf623d63ea7c": {"ta_keywords": "new evaluation measure;questions;task;answer;summary;measure;performance;quality;question;form;version;form version", "pdf_keywords": "novel disambiguation task;ambiguous factoid questions;disambiguation task;ambiguous factoid question;factoid question;new disambiguation system;adisambiguation tasks;disambiguation metrics;automatic assessment;disambiguations;disambiguation;annotation process;annotators;annotated samples;answer length;retrieval aspects;novel short answer summaries;quality quality assessment task;empirical evaluation;annotations;different disambiguations;markov chain disambiguations;annotation pipeline;new evaluation measure;retrieval model;natural language documents;knowledge;task;annotation instructions;quantitative assessment"}, "018bf5da2ba1f1901e98f72c7eedbf6b91967192": {"ta_keywords": "text structures;privacy;text structure;preservation;new approach;variant;application;approach", "pdf_keywords": "private text models;private stochastic gradient descent;private text corpora;privacy adaptation;private text data;new privacy;private data;private representation;privacy constraints;private text;privacy preservation approaches;privacy;natural language processing;users privacy;privacy protection;private text provider;deep learning;private access;natural language understanding;language models;deep learning models;private input;deep learning methods;adversarial examples;adversarial approachwe;generalization;adaptive deep learning;collective learning approach;learning;text representation"}, "a6a7374c5ddac1446ceab9d7cbe5a3305238d0ee": {"ta_keywords": "trigram conversation turn;conversation corpora;conversation filtering;conversations;conversation;movie scripts;speakers;television;unit;various types;work", "pdf_keywords": ""}, "963c4c34f1292f64a6e9fc04428fc7a0893b8ef3": {"ta_keywords": "novel residual deep attention mechanism;residual attention;channel attention processing module;adaptive reconstruction network;novel spatial;ssrs;pixels;hierarchical features;raan;spatial domain;scam;distance dependencies;paper;different weights", "pdf_keywords": ""}, "53feb3b34425ea95c259e8d0693edd490d6b470f": {"ta_keywords": "back correlation;online social network;mistaken words;semantic processing;back;quantitative quantitative quantitative quantitative quantitative analysis;comparative study;framework;form", "pdf_keywords": ""}, "a792d5a1e9a6a53edd8cbc00e387bc07c54e423c": {"ta_keywords": "crowdsourcing;agents;truthful responses;answers;evaluations;accuracy;simple intuition;mechanisms;scale tasks;new mechanisms;possibility;absence", "pdf_keywords": ""}, "10efdde1ae3a9d359ac1aae0bd5ef7bfd68810dd": {"ta_keywords": "multilingual representation learning;multilingual language modeling;novel multilingual representation;dict;languages;multiple downstream tasks;mlm;model;original word;domain;method;key requirement;approach;efficacy", "pdf_keywords": "multilingual language models;multilingual encoders;bilingual dictionaries;scalable machine translation alignment method;language embeddings;machine translation;expensive translations;scalable machine translation system;language corpora;crosslingual identification;large corpus;sequence labeling;several languages;languages;dictionaries;key language;language;dict;potent language;deep models;textual entailment;machine learning tasks;dictionary;sentence retrieval;deep representation representation;parallel text;semantics;deep learning framework;deep bidirectional transformers;mlm"}, "ef1d93b03c20b2f488b66e8e2c24fceb2105d58f": {"ta_keywords": "metaphoric expressions;different languages;several languages;equivalence principle;characterization;lemma;new test sets;model", "pdf_keywords": ""}, "3993788eb252f5eb7fc19e9f98357a72f9f0476d": {"ta_keywords": "membership network model;mixed membership;balanced clusters;generative models;membership;novel regularization method;model;method;degree;applications", "pdf_keywords": ""}, "71c7104eaed93497824cf197949c77e7d6cb36d3": {"ta_keywords": "knowledge bases;iterative retrieval;framework pullnet;open domain question;text;domain question;context;framework;experiments;prior state;art methods", "pdf_keywords": "complex web questions dataset;knowledge bases;incomplete knowledge base;knowledge base;knowledge sources;novel iterative retrieval algorithm;iterative reasoning methods;question subgraph;complex web;knowledge;questions;unstructured text;semantics;subgraph construction method;text corpus;corpus;search;similaritywe;content;natural language;heterogeneous information;search graph;relevant entities;orneural networks;related tasks;qaq;information;empirical discovery;linguistic complexity;novel iterative question"}, "911536dc3dfbbbf2bb8d71181b31e0aa7920b9f6": {"ta_keywords": "financial market;discrete time;algorithm;prediction;discrete mathematics;dd method;method;values;numerical experiments;ds;solutions;dd;experts;power", "pdf_keywords": ""}, "798e45ea830884be36c3f526d3b169eaba95f989": {"ta_keywords": "local nash equilibria;local equilibria;sum games;result;stronger results;setting;previous results;family", "pdf_keywords": "nondegenerate differential nash equilibria;local differential nash equilibria;local nash equilibria;continuous zerosum games;differential nash equilibria;differential game forms;differential nash equilibria andwe;differential game form;global game theory;local equilibria;continuous games;possible local equilibria;degenerate equilibria;differential nashwe;hessian game;sum games;game theorywe;continuous gameswe;sum game;equilibria;generic existence;genericity;player games;games;complementary cost function;robust policies;agent policy gradient;stability;rigorous characterization;smooth functions"}, "8a0a8568acf2b95c9cb471e28ee6b25c5e4fe186": {"ta_keywords": "stochastic process;stochasticity;decision making;data;process;analysis;relevant information;essential features;system;approach;novel approach", "pdf_keywords": ""}, "e75c388b60cf447be7148be25feeee3e10d12cf4": {"ta_keywords": "dimensional data;dimensional information;training data;dataset;data;interpolation;method;new method", "pdf_keywords": "dimensionality reduction;dimensionality reduction techniques;dimensional data;dimensional data representation;dataset interpolation;intrinsic dimension;generalization performances;dimensional representation;synthetic data sets;interpolation regime;dimension;interpolation;dimensionalwe;machine learning;underlying data;extrapolation;data;suited geometrical definitions;algorithms;samples;random points;linearity;nonlinearity increases;model;representation;performance;nonlinearity;notion;usefulness;geometrical interpretation"}, "42c3c50b8e368ee2e1b52d010b6c53b3d732770c": {"ta_keywords": "sentiment information;end speech sentiment approach;unlabeled speech;large unlabeled speech;language models;language model;texts;pseudo label;human supervision;paper;novel approach;framework;end;advantages;approach;advantage", "pdf_keywords": "speech sentiment analysis;e2e speech sentiment analysis system;end speech sentiment approach;speech sentiment classifier inwe;sentiment classifier;sentiment analysis;relevant speech classifiers;automatic speech emotion classification;natural language processing;sentiment;speech dataset;text tokens;language models;bert model;speech emotion recognition;human annotation;automatic speech recognition;human sentiments;deep learning;language model;pseudo labelers;e2e speech equation;transcripts;training data;speech signal;simple neural net architectures;text;deep neural networks;deep learning model;pseudo labels"}, "5dcbdb9bf80575953b5d21f378d8139f0a44168b": {"ta_keywords": "user emotion;support vector machine;svm;svms;emotion triggers;automatic recognition;recognition;sequential identification;object;objects;high accuracy;scene;subsequent filtering;method;result;means;pair", "pdf_keywords": ""}, "bc632f81dab322ac610a8d11463cc1bba6130eda": {"ta_keywords": "historical text normalization;shot learning outperforms;training data;languages;target task;datasets;sequence;identity baseline;significant improvements;improvements;data;paper", "pdf_keywords": "historical text normalization;normalization tasks;task learning;neural machine translation;autoencoding;normalization;auxiliary tasks;normalization accuracy;translation system;auxiliary data;phoneme mapping;shot learning;training data;tasks;shot learning outperforms;text;task model;small training data variations;novel linguistic feature;neural encoder;task;generalization;neural machine;training epochs;historical text;different tasks;target task;phoneme;computational linguistics problem;historical documents"}, "13b674bb3078623608045a18570b47f6e49a8358": {"ta_keywords": "art results;paintings;artistic work;sculptures;textual description;artistic movements;images;bipartite matching;visual question;descriptions;tapestries;task;dataset;preliminary system;computer;countries;method;dozens;centuries;state;new instance", "pdf_keywords": ""}, "4b9795493a937b9034be9c26afab23f6dc751f62": {"ta_keywords": "retrieval automaton;language models;natural language text;standard language model inference;retrieval;datastore search;novel probabilistic model;automaton;probability;novel method;method;ability;parallel", "pdf_keywords": "neural language models;neural language model;simple unstructured data;natural language text;natural language processing;unstructured data sets;unstructured data;novel autoregressive language modeling;unstructured unstructured data sets;unstructured database;unstructured unstructured unstructured data sets;symbolic language model;alsoneural language models;standard language model inference;corpus;neural models;linguistic structure;text collection;automaton;unstructuredwe;original training corpus;symbolic language;nearest neighbor searches;search database;simple language;search engine applications;retrieval;neural learning algorithm;neural networks;relevant linguistic features"}, "a7b6802f20c399615dbac161678cd6a6d2df5a97": {"ta_keywords": "rank models;quality data;hybrid machine learning;quality quality control;learning;planck;social impact;data;strasbourg;germany;mining pipeline;human;significant business impact;case study;loop;online setting", "pdf_keywords": ""}, "7df6aa19f50c8ec5f12d58e0685ed5c6e9a08bb2": {"ta_keywords": "stochastic models;preferences;text;users;information;efficient algorithm;model;possible outcomes;product;user;output;method", "pdf_keywords": ""}, "a3e3a9d878999c7038c275e75f5cd8a232aa4999": {"ta_keywords": "task diversity;generative capabilities;new benchmark;tasks;models;specific trainable heads;robustness;simple task;model parameters;superb;shifts;different types;lightweight methodology;sg;difficulty;data domain;quality", "pdf_keywords": "speech enhancement tasks;speech representation;domain speech conversion;discrete speech representations;speech processing;speech enhancement;speech separation;deep models;speech translation;automatic speech recognition;neural vocoder;speech segmentation;independent speech separation;limited task supervision iswe introduce superb;language modeling;new benchmark;spontaneous speech;speech;generative capabilities;generative capability;sg robustnesswe;new tasks;benchmark;task diversity;neural networks;learning;speaker;decoding;tasks;downstream tasks"}, "b5002aa334f8d0c0e1a4dedad79580e10a928c30": {"ta_keywords": "spectral feature;low resource speech;low resource datasets;signal processing tasks;representations;learning;interpretable framework;experts;combination model;framework;relative;mixture;last model;baseline", "pdf_keywords": "hybrid attention model;layered attention model;low resource speech tasks;attention model;speech processing tasks;speech representations;common voice tasks;attention;spectral features;supervised learning;filterbanks;essential features;attention mechanisms;fusion;recognition;novel attention;learnable transformations;extractors;features;learning;learning framework;representations;speech;dimensional features;convolutional mechanisms;convolutional layer;strong background data;fusion method;feed;learning approach"}, "8809d0732f6147d4ad9218c8f9b20227c837a746": {"ta_keywords": "end speech processing problem;powerformer;convolution;entropic model;input data;power;end;information;architecture;novel approach;approach", "pdf_keywords": "end speech processing applications;end speech processing applicationswe;convolutional learning model;end speech;convolutional learning convolution;convolutional learning;convolutional detector;convolutional neural network;speech;convolutional network;convolutional layer;utterance;frequency mask;detector;encoder;novel generative network;power;detector sensor;power grid;conformer;output;novel generative adversarial network;sparse representations;models;sensor;kernel;feature;term memory;global context;model"}, "d8252e24b6036ca895800b547698ab44d09ae350": {"ta_keywords": "personal information management tools;personal information;machine learning;searches;performance;experimental evidence;circumstances", "pdf_keywords": ""}, "f91c24b0dc56a6b377e99e046d7540e5bb7aa46e": {"ta_keywords": "college student information management system;design;paper", "pdf_keywords": ""}, "a6a7724763d8adba466519489b0b9d209e7f2d15": {"ta_keywords": "text generation problem;novel text evaluation method;text;sequence;sequence model;description;metric;metrics;novel;test settings;different test settings;method", "pdf_keywords": "supervised text generation;text generation evaluation;text generation;empirical neural summarization framework;neural summarization evaluation;natural language models;text generation problem;machine translation;natural language data;machine translation tasks;quality sentences;natural language representations;natural language processing;computational summarization;computational linguistics;sequence models;natural language;linguistic similarity;text tasks;unified text;source text;summarization;sentences;text;abstractive summarization;linguistic structure;sequence;machine learning tasks;supervised learning;semantics"}, "e3f1a9c3d87e9828cdeb08ba90a260c69e974a75": {"ta_keywords": "basic dance generation;target dance sequence;deep recurrent method;basic dance steps;deep lds layer;audio input;audio power spectrum;convolutional layers;low cross entropy;term memory;training;input;model;network;lds;time;study;experimental results", "pdf_keywords": "long dance sequences;deep encoder;deep recurrent neural networks;generated dance;motion beat;music track inputswe;dance generation tasks;deep neural network;deep decoder;music tasks;music input;deep neural networks;dance;neural network;basic dance steps;tasksmusic;music;motion generation;deep convolutional neural networks;novel encoder;dancer;consecutive audio input frames;untrained noises;encoder performance;hip hop bounce data;motion control;motion;scale quantum network;decoder;quantum system"}, "78d57a1ecd724c5f8b1534372969d5b35daa6d4b": {"ta_keywords": "generative neural models;constituency;direct search;pt;explicit model combination;algorithm;art numbers;state;performance;art results;new state", "pdf_keywords": "base parsers;novel constituent parser;constituent parsing;hybrid parsers;parsers;generative neural models;generative models;generative model;single generative model;synchronous beam search;candidate outputs;generative model canwe;constituent;tree;natural language;reranking results;constituency;models;trees;reranking procedure;candidate scores;explicit model combination;model;evaluation performance;evaluation;model combination effects;implicit model combination;performance;training data;scores"}, "b6c6e06b4bc68349845b30e01e01d7603f468547": {"ta_keywords": "optimal relay location;relay nodes;relay node;optimal placement;single relay case;individual power constraints;sink node;theoretic achievable rate formulas;source node;information;high attenuation;line;straight line;explicit formulas;problem", "pdf_keywords": "optimal relay placement problem;optimal relay location;optimal relay placement;optimal relay locations;optimum relay location;optimum relay locations;single relay placement;relay node placement;relay placement;optimal power allocation problem;optimal power allocation;optimal power allocation towe;relay nodes;optimal source power split;relay propagation;relay network;multiple relays;single relay node;relay relay relay power consumption;relay power consumption;pure relay channel;relay location;power allocation;power allocations;single relay;total power constraint;relay node;same relay position;relay scenario;wireless networks"}, "7729fbebff327bebb9292dc1c51c51dd55390954": {"ta_keywords": "transitive verb construction;sentence similarity;rank tensors;full tensors;performance", "pdf_keywords": ""}, "f3271e61dc0507183ee399393129d7888c2f82b9": {"ta_keywords": "supervised quality estimation;automatic quality estimation approaches;annotated scores;annotation effort;quality;evaluation;full references;framework;particular domain", "pdf_keywords": ""}, "68aa7c7b65365c3303d5024b1273408fb435d178": {"ta_keywords": "dialogue;dialogue acts;entrainment;entrainment changes;lexical choice;like interaction;effect;machine;various types;fundamental aspect;choice;novel measure;users", "pdf_keywords": ""}, "b0ddd849c5ae0004678fa483908c06d87894f3ab": {"ta_keywords": "isolated word recognition;new probabilistic selection criterion;variational probabilistic approach;speech;models;spread function;social model;standard model;model;criterion;ss;wide class;hmm;analysis;practical implementation;data", "pdf_keywords": ""}, "b0d644277933988c00b22d8ae012512fe498ad62": {"ta_keywords": "word sense inventories;sense inventories;disambiguation task;languages;generation;grave et al;collection;basis;model;method", "pdf_keywords": "unsupervised word sense disambiguation system;unsupervised word sense induction;multilingual word sense disambiguation system;word sense induction;word sense disambiguation;multilingual word sense disambiguation;sense embeddings;word sense inventories;word sense inventory;word embeddings;word relatedness dataset;sense induction;word vectors;linguistic feature extraction network;linguistic linguistic feature extraction network;word sense;learnt sense inventory;word similarity benchmarks;sense inventories;linguistic linguistic similarity prediction;linguistic similarity;model clusters clusters word clusters;linguistic structure;embeddingswe;appropriate sensewe;linguistic description;embeddings;corpus;natural words;word pairs"}, "ebc64974e9e0021984a0158b3c04b60327730a88": {"ta_keywords": "large scale knowledge;relevant retriever items;approximate data query;database;retriever;data stream;functionality;like transducers;like framework;base question;framework;novel", "pdf_keywords": ""}, "f0cd4de3cdf547dcdcc6995dca9ab3f65955b324": {"ta_keywords": "recurrent highway networks;lattice recurrent unit;accurate language models;neural networks;models;better empirical computational convergence rates;lru;available datasets;performance;limited resources;challenge;statistical efficiency values;new family;family;experiments", "pdf_keywords": "recurrent network;recurrent neural network;generalized linear neural networks;deep neural networks;linear neural networks;generalized linear recurrence units;linear neural networkswe;scalable scalable representations;lru models;recurrent gate;term memory;language models;natural language networks;lru model;level language models;character level language modeling;flow;models;depth dimension;linear response models;large dataset;unstructured data;lru;depth dimensions;depth;time dimension;model;dataset;small datasets;best models"}, "485b3f77b9913e151e7ca897d99497e70e7f30d1": {"ta_keywords": "aexhaustive vocabulary;new vocabulary;general subwords;rare words;training efficiency;way embeddings;larger units;training pass;validation loss;granularity;training;element;smaller units;performance improvements;specific units;course;method;benefits", "pdf_keywords": "neural machine translation vocabulary;neural machine translation algorithm;typical model training pipeline;deep translation;high translation accuracy;nonlinear machine translation;vocabulary synthesis system;translation accuracy;segmentation granularity;learned bpe embeddings;new vocabulary online;new vocabulary items;new vocabulary;general subwords;new vocabulary item;autoencoder;segmentation;vocabulary;new vocabularywe;subword units;translation;additional training time;hybrid word;larger datasets;incremental bpe method;bpe models;target language;training;new rare words;word"}, "1e4e2aceed87febcc643f1473507c9535ba5c19a": {"ta_keywords": "field model;volume;blockwise;paper;version", "pdf_keywords": "simultaneous speech translation;novel simultaneous speech translation;simultaneous translation model;end speech translation;encoder;encoder convergence;streaming models;attention;joint attention;contextual block processing method;reference beam search scores;streaming;attention mechanism;volume;faceted lp representation;translation;lp representationwe;simultaneous sst;intermediate ctc loss;sst;blockwise;transformer;novel representation;extra ctc branch;tasks;denoise;intent;output states;slab denoise;feature"}, "6e24bcfcdb31afbb313a13c1c84cb779ceb17500": {"ta_keywords": "stochastic algorithm;taxi driver data;decision maker;stochastic processes;many decision making processes;person;algorithm;actions;hidden model;reference point;basis", "pdf_keywords": ""}, "0d20360c5d533760d97d7ce19b78d4791a5173cb": {"ta_keywords": "terrorist attacks;terrorist targets;terrorist events;inference algorithm;broader machine learning models;network;algorithm;accuracy;recall;method;precision;new method;results;heterogeneous dynamics;high levels;power;work", "pdf_keywords": ""}, "5d9fe38b750f59b4ef0a2b58fde0f60d4317c7ad": {"ta_keywords": "simultaneous multidomain learning;multiple metadata attributes;data classification;multidomain;several attribute attributes;domains;data;performance;novel approach;approach", "pdf_keywords": ""}, "242c35b91fe1d7aedab9d1da7652aad2219d4784": {"ta_keywords": "dimensional models;training;novel approach", "pdf_keywords": ""}, "924e43c4de98743d2e7c14c241b03b2109325b90": {"ta_keywords": "efficient method;multiple processors;memory;variant;method;text;part;problems", "pdf_keywords": ""}, "030fade3049e0847702393dde3100ecc41a5e86a": {"ta_keywords": "optimal performance element;local optimum;performance element;performance elements;random test cases;algorithm;performance;utility values;high probability;discrete space;basis;problem;settings", "pdf_keywords": ""}, "bea54062d105b9fe3250ce3569cf817e54772894": {"ta_keywords": "probabilistic parser;historical corpora;sequence labeling approach;unlexicalized parser;historical german;longer phrases;phrases;automatic recognition;german;trees;sequence;accuracy;evaluation;errors concern;study;paper", "pdf_keywords": "phrase recognition;phrase annotations;neural sequence labeling tool;sequence labeling approach;probabilistic parser;historical corpora;corpora;sequence labeling tool;parser;phrase types;unlexicalized parser;different historical corpora;standard parser;linguistic structure;historical german dataset;phrase output;annotation;longer phrases;linguistic content;linguistic similarity;annotationthis paper;phrases;novel labeling strategy;calucleon parser;historical german;historical text;topological field annotationsthis paper;constituency trees;automatic recognition;standard gold standard annotation"}, "38705aa9e8ce6412d89c5b2beb9379b1013b33c2": {"ta_keywords": "semiparametric inference;semiparametric situations;deep networks;deep neural networks;deep feedforward;neural networks;empirical application;concreteness;convergence;effectiveness;treatment effects;new bounds;context;theory;new rates;variety;method;particular emphasis;use", "pdf_keywords": "semiparametric inference;nonparametric estimation;semiparametric causal inference;general nonparametric regression;deep network estimators;nonparametric regression;semiparametric inference problem;asymptotic inference;valid asymptotic inference;nonparametric regression style problems;deep nets;nonasymptotic bounds;asymptotic estimates;deep feedforward;fast convergence rates;deep network estimator fmlp;neural nets;deep learning methods;deep networks;smooth loss functions;deep neural networks;generalized linear loss functions;deep learning;deep relu networks;empirical bound;most nonparametric practice;feedforward networks;generalized linear loss function;convergence rates;deep network"}, "4dd85ae17a5fd0bce09ffef0455b6e827d7e1e2b": {"ta_keywords": "networked cyberphysical systems;attack performance;optimal design;attack;noisy model;network;noisy noisy model;constrained optimization problem;novel method;physical system;process parameters;paper;use", "pdf_keywords": "secure estimation;popular linear attack scheme;linear attack;attack algorithm;attack detection probability;attack detection;attack scheme;novel attack scheme;new attack scheme;remote estimation problem;attacks;networked control systems;false data injection attack;consensus filtering;new attack;estimation process;attack;novel attack;estimation;estimators;state estimation;attacker;external attacker;networked physical systems;stochastic gradient descent;estimationwe;sensor observations;physical attacks;simultaneous perturbation stochasticwe;multiple agent nodes"}, "5143ebd23322fe805bed2667fcfb70920c105f7f": {"ta_keywords": "cognitive modeling;effective demonstrations;cognitive model;demonstrations;simulated student;training sequence;effectiveness;efficiency;detail;characteristics;ordering;level", "pdf_keywords": ""}, "cfbe9183f2fe2847f7a3c811f6309a2cab3f85cf": {"ta_keywords": "scientific summarization dataset;summarization datasets;extractive summarization system;word embeddings;bert;scientific documents;intermediate pretraining;significant performance improvements;art results;step;influence;interacts;different domains;state", "pdf_keywords": ""}, "20086d6a9fab6081f300e08d3f952cb9b16e6de8": {"ta_keywords": "reference database;search;database;perfect hash function;benchmarks;hash function;residual strings;implementation;less space;space;set;traditional approaches", "pdf_keywords": ""}, "693f5d55e0561099944f5e00e301bf26db0b972d": {"ta_keywords": "semantic analysis;stochastic models;stochastic model;available corpus;evaluation;task;training;thesis;input data;noisy system;output", "pdf_keywords": ""}, "939a149f156425b83e48ea72e9e09a55ea33b8d7": {"ta_keywords": "wavelet transform domain;signal reconstruction;convex sets;projections;iterative solution;novel stabilization mechanism;linear simultaneous equation;novel formulation;algorithm;representation", "pdf_keywords": ""}, "0fbb90b8fe1d02a4f0f616df9a09ec42eace53bd": {"ta_keywords": "online unsupervised classification;remote recording condition;mean value;noise type;online;new method;conventional approaches;experimental evaluations;method;scheme;changes;respect;approach", "pdf_keywords": ""}, "e21633b0b5e55dce56bc07e919c6d12ecf8cef0c": {"ta_keywords": "logic programs;logic systems;determinate clauses;logic;locality;constant depth;new restriction;classes;language;pac;useful resource", "pdf_keywords": ""}, "c305e3314c0853b14911f704c68b04cfc9ea7aa1": {"ta_keywords": "corpus;baryon;languages;baryons;conversion;dependency;family;accuracy;principle;same degree", "pdf_keywords": "dependencies treebank;hindi treebank;syntactic dependency relations;syntactic annotation scheme;novel automatic treebanks;corresponding syntactic structures;syntactic trees;linguistic annotations;syntactic structures;same treebank;phenomenological dependency analysis framework;parsing;linguistic dataset;languages;language;nonlinear linguistic evolution;structural trees;morphological information;constructions;long distance dependencies;text tags;trees;structural differences;complement constructions;terms;elliptical constructions;ellipsis;labels;literature;conversion processwe"}, "652579315d767331d8e05ea46489e6bd081ef48a": {"ta_keywords": "pairwise comparisons;evaluation;students;evaluators;classroom;conventional cardinal approaches;approach;new approach;expertise;lack", "pdf_keywords": ""}, "b293e4659e20815bcf0b6d31ce46b8bd9437c1fa": {"ta_keywords": "private machine learning;privacy attack;privacy protection;privacy;privacy issues;machine learning;corresponding protection schemes;paper surveys;survey;key challenges;machine;ii;category;categories;iii;solutions;state;interactions;current research progress;art", "pdf_keywords": "private machine learning;private deep learning;machine learning attack;privacy management;data privacy;data privacy attacks;privacy attack;privacy protection;privacy protection problems;new privacy protection schemes;privacy;model privacy;privacy managementas;privacy preservation;privacy management systems;user privacy;privacy risk assessment;privacy preservation schemes;users privacy;own privacy;unique privacy policy;structured privacy model;privacy configurations;privacy management transfers;privacy policy evaluation;privacy issues;data attacks;feature estimation attack;private data release;thefeature estimation attacks"}, "8d17543c20f23b6a40bec9334d50e9c15a08c1c4": {"ta_keywords": "specific subgraph;answers;text;methods;novel model;question;model;art;state;setting", "pdf_keywords": "underlying knowledge base;knowledge bases;knowledge base;large text corpus;rich knowledge representation;domain knowledge extraction;knowledge representation;knowledge base facts;natural language;text sentences;relation extraction;heterogeneous update rules;question subgraph;relation extraction rule;graph learner;relation embeddings;linguistic structure;heterogeneous graphs;semantics;update rules;rich relational structure;information sources;search network;nodes;node nodes;embeddings;neural structured learning;simple entity links;knowledge;text document"}, "1808b64aec21863489f0fe66f250890a3ac2b843": {"ta_keywords": "verifiable secret sharing;many unbiased random coins;random noise;unbiased random bits;malicious participants;distribution;shares;shallow circuit;generation;method;technique;set;use;fewer executions", "pdf_keywords": "new privacy mechanism;randomization;privacy;unbiased random bits;randomness extractorswe;randomization problem;random noise;unbiased bits;noisy source distribution;secret sharings;unbiased input bits;noise generation;statistical databases;random inputs;noisy sums;random distributions;exponential noise;malicious participants;stochastic implementation;noisy source;own data;algorithm;databases;random network;noisy shares;malicious parties;noisy version;noise;computation;database"}, "c3a662b864673d8cc7469051419ab8819926d4b0": {"ta_keywords": "bert;languages;linguistic properties;architectural properties;natural data;mix;findings;experiments;small setup", "pdf_keywords": "small bert models;bert models;multilingual machine;multilingual tasks;multilingual model;bert representation;stronger multilinguality;multilingual languages;language models;multilinguality;language model;multilingual pretraining setup;language representation;machine translation models;synthetic languages;train language model;multilingual setting;multilingual settings;machine translation;bert;languages;language spaces;linguistic models;parallel corpus;language;small corpus;corpus;multilayer representations;computationalmultilingual;same language"}, "b1d24e8e08435b7c52335485a0d635abf9bc604c": {"ta_keywords": "textual sources;fever;assertions;new dataset", "pdf_keywords": "fact extraction;fake news dataset;textual evidence;empirical fact extraction;claim verification;claim validation;false news challenges;entity claim;corpus;politifact database;annotators;textual sources;more accurate claims;wikipedia;false detection;textual entailment;natural language similarity;evidence;level evidence selection;more complex claims;validation;wikipedia page;unstructured unstructured data;verification;facts;simple factoid sentences;empirical discovery;new empirical dataset;claims;dataset"}, "4a4bc9f6c5ec76b0d501b641d3092aceb2e083bd": {"ta_keywords": "modal modal user interface;user interface;user interface pages;bar;restaurant;dynamics;world business pages;person;specific preferences record;user;use;novel method;set", "pdf_keywords": ""}, "d5eeaac5c5e524ad05d9b1f3f3f41aece082955a": {"ta_keywords": "robust probabilistic predictive classification method;speech recognition;variational bayes;speech recognition error;predictive classification;novel probabilistic framework;probabilistic framework;robustness;training problem;variance;bpc;method;theoretical framework", "pdf_keywords": ""}, "04d96a75b4383240cb15fb729b29f5775219d724": {"ta_keywords": "neural automatic speech recognition;decision trees;underlying decision trees;data mining;library;asr;data;tools;common distribution function;underlying decision;use", "pdf_keywords": "deep learning decoder;deep learning library;deep learning model;parallel machine translation systems;speech encoder;deep learning;deep learning framework;machine translation;unstructured neural network;novel speech encoder;encoder;decoder model;decoder;neural network model;neural networks;language model;speech recognition;decoder system;speech recognition system;neuronal network;equation ofspeech recognition systems;parallelization;bioinformatics;encoding;linguistic data;nonlinear sentences;automatic speech recognition;learning;equation;speech"}, "312b12dd6aa558b92df3ddd9b1057aa80a0ad718": {"ta_keywords": "shot relation classification;relation classification systems;textual entailment models;available textual datasets;shot classification performance;datasets;conditional encoding;f1;novel approach;approach;use;leverage;performance", "pdf_keywords": ""}, "5885625fac055f4f8f47b0d6b5c026c8806896f0": {"ta_keywords": "stochastic process;stochastic processes;process;dynamics;chunk;similarity property;concept;self;method;study;new approach;results;use", "pdf_keywords": ""}, "098076a2c90e42c81b843bf339446427c2ff02ed": {"ta_keywords": "influence functions;influence estimates;neural network models;deep learning;snp benchmark;regularization;np benchmark;datasets;accuracy;model;depth;extent;network architecture;model parameterization;failures;comprehensive empirical study;width;successes;paper", "pdf_keywords": "deep learning models;deep models;deeper networks;deep learning;deep networks;deep network;small cnn;deep learning tasks;deep learning framework;small neural network;hessian matrixin;neural networks;debug machine learning models;influence functions;hessian;gradients;cnn architecture;hessian vector product;gradient;exact hessian approximation;empirical influence function;underlying loss function;influence function quality;networks;empirical representations;influence function;empirical risk minimization technique;influence estimates;mnist dataset;exact hessian matrix"}, "446f1eaec22a90574670491073cd5b03bfa1e273": {"ta_keywords": "simple statistical claims;statistical properties;knowledge base;data;numerical information;simple claims;properties;property;text;countries;raw text;verification;identification;baseline approach;experiments;problem;approach;interest", "pdf_keywords": ""}, "76468db928e18f97dadbd25c04c80ebd491fec9b": {"ta_keywords": "transition metal pyrochlore;crystal field splitting;nd0 transition metal;rare earth;splitting;order;influence", "pdf_keywords": ""}, "aaf7e94e1a2f8891e5c5f4d11d4f135a1687bb0a": {"ta_keywords": "mechanical oscillator;simple mechanical model;mechanical model;mechanical mechanical mode;mechanical mode;macroscopic structure;assembly;mechanical version;capture tabs;dynamics;motion;spring;model;pair;simple modification", "pdf_keywords": ""}, "c589c4ec7247980f38a6bd22f215fea8028a0f66": {"ta_keywords": "annotations;human systems;accurate decision making;reasoning;accuracy;ground truth rationales;accurate understanding;decision making;decisions;workers;data;results;errors;qualification;careful analysis;comprehensive study", "pdf_keywords": "amazon mechanical turk;human annotations;annotation quality;human annotators;high quality annotations;annotated learning;accurate annotations;annotation results;annotations;annotators;annotated ones;annotation process;annotation;annotation experiment;aforementioned annotations;interpretable natural language processing;explainable natural language processing;turkers;nlp;machine learning;online crowd;interpretable predictions;datasets;dataset;accuracy;workforce;crowd;tasks;examples;task"}, "ecc520794da34d2b141235002c70b06c999bda73": {"ta_keywords": "sense embeddings;interpretable sense representations;new coherence evaluation;text embeddings;minimal model", "pdf_keywords": ""}, "c12e46d7d0bb9fa062bce0549f2a6a9de00758d5": {"ta_keywords": "novel sound event detection;transformer encoder;input feature sequence;attention mechanism;global context information;attenuate modules;sase method;sase;self;multiple self;method;experimental evaluation;paper;account", "pdf_keywords": ""}, "f2fb4a931580c4f1c5bdb47ebc80c801b422cd1a": {"ta_keywords": "neural network;logic tasks;dynamics;logic;network;language;method;task;methods", "pdf_keywords": ""}, "57b972ebe314cfe8e57fd6b9f9239123eb70e979": {"ta_keywords": "free automatic speech recognition problem;deep convolutional neural networks;convolutional neural networks;cnns;encoders;use;comparative study", "pdf_keywords": "connectionist temporal classification;recurrent neural networks;conversational telephone speech;speech recognition system;automatic speech recognition;convolutional neural networks;deep convolutional neural networks;convolutional ctc models;convolutional neural network;deep network;rnns;neural network architecture;convolutional layers;cnns;neural network;convolutions;forward speech;encoders;convolutionalwe;scalable forward neural network;convolutional block ciphers;gram models;text;beam search;elementary learning;talk;ctc;network;search;empirical pattern recognition algorithm"}, "6f79cbe893ae46a6f97617d14656ab57c26e6faf": {"ta_keywords": "directional automatic speech recognition;explicit speaker locations;source speaker locations;neural network manner;e2e;asr;end;explainability factor;better performance;advantage;methods;new paradigm;paper", "pdf_keywords": "channel overlap speech extraction network;field multi speaker data;directional automatic speech recognition;end speech recognition objectives;field speech recognition;source speaker locations;speech recognition;explicit speaker locations;advanced speech recognition;noisy noisy speech data;robust robust speech extraction algorithm;speech sources;traditional multichannel filtering;source localization;deep learning;noisy speech;deep learning network;neural networks;asr network;advanced robot audition system;neural network manner;neural network;machine learning architecture;convolutional neural network;cnn;asr method;reference network;recognition;advanced signal processing;wideband problem"}, "5cb74e269c57263d475734a66d34e4d2d2f9e1ac": {"ta_keywords": "finite element method;vibration characteristics;ffm method;modal test;experimental values;ffm;sample;good agreement", "pdf_keywords": ""}, "cefd3993db4d065b95ab8f105452fb728c02b60e": {"ta_keywords": "scientific reviews;pass peer reviews;author reviews;natural language processing models;scientific paper;text;papers;lower constructiveness;core ideas;quality;challenges;aspects;explanation;system;collection;pursuit;train;art;state", "pdf_keywords": "automatic review generation;automatic review generation task;automatic scientific reviews;automatic review generation system;structured scientific reviews;automatic review system;scientific paper summarization;structured reviews;automatic summarization system;summarization model;empirical summarization;accurate review reviews;review reviews;scientific review;text review;review system;summarization process;abstractive summarization;peer review reviews;annotated human papers;reference reviews;randomized peer review;human reviewers;review;reviews;reviewers;sentence extraction;annotations;detailed reviews;annotation platform"}, "f491a5f09ee01436d772a6cff25f22d700d8c9c0": {"ta_keywords": "power flow analysis;nodal analysis method;nodal analysis;generator;voltage support;dg;mana;approach;method;side management;novel approach;example case studies;robustness;formulation;solvability;performance;challenges;characteristics;paper", "pdf_keywords": ""}, "72213e24713264da816f43a42d606f115998fe7b": {"ta_keywords": "stacked graphical learning algorithm;collective classification;stacked algorithm;graphical learning algorithm;graphical learning;online learning algorithm;graphical model;information extraction system;neighbors;instances;local features;predictions;convergence;application;idealized version;pass", "pdf_keywords": ""}, "c04c865c8b33ce0251c9f37d0cccf2b3b1e4fd34": {"ta_keywords": "network reinforcement learning problems;network nodes;nodes;wide navigation tasks;agents;network;evolutionary status;rich observation spaces;policies;novel approaches;approaches;variety;performance;methods;problem", "pdf_keywords": "approximate causal states representations;predictive state representations;partial observable policy network;observable policy networks;causal state representation;partiallyobservable models;new causal state representation;causal states representation;underlying state representation;empirical bayesian representations;predictive learning;state inference;state abstraction;partial observability programming;state representation approach;conditional entropy;partial observables;learning representation;causal statewe;finite causal states;reinforcement learning;pnp predictive decision processes;discrete causal states;world navigation tasks;causal states;conditional states;invariant prediction;continuous state space environments;latent representations;observable states"}, "8504a5eb4638aeb2f61f8b7f93440b9e495b443b": {"ta_keywords": "centralized tracking;unknown parametric distribution;tracking;optimal algorithms;algorithms;analysis;paper deals;problem;author", "pdf_keywords": ""}, "8e7d063c681557c94382ff3da6415d3720fe11a7": {"ta_keywords": "conditional encoding;twitter task;bidirectional encoding;tweets;text;scalar signals;encoding;corpus;detection;signal;conditional model;approach;state;art results;method", "pdf_keywords": "stance detection;unlabelled tweet corpus;twitter sstance detection corpus;social networking task;tweets;target stance;weakly supervised way;tweet;weakly supervised learning;stance;natural language;social networking;unseen target stance;supervised learning approach;joint task;supervised manner;training data;corpus;particular target;dependent embeddings;bidirectional encodings;text;detection;task;word vectors;neural network;targets;hillary clinton;term memory;target"}, "8568f6eda2e4cb7921fe175ab44b2f5ecbb2b870": {"ta_keywords": "more reading difficulty;human readers;error words;misspellings;unimpaired comprehension;eye;words;error rates;correct words;error types;surprisal;errors;character;tracking study;transpositions;computational model;traditional word;spite", "pdf_keywords": ""}, "ae77189921ffade5ee4c4d4a0e93e879d7280b80": {"ta_keywords": "virtual avatars;multimodal body;pose;generative model;body;model;data;language;ability;power", "pdf_keywords": ""}, "93e012cbf8e29aacb9654313250a81d53bbcbdf2": {"ta_keywords": "email leaks;leak detection task;leak detection technique;real email examples;outlier detection task;enron corpus;leak;outliers;unintended recipients;recipients;detection;network patterns;new method;method", "pdf_keywords": ""}, "df99459a75328393a9a989498db46ec445335724": {"ta_keywords": "privacy;review data;peer;data;release;utility;utility tradeoff;several theoretical results;framework;manner", "pdf_keywords": "differential privacy;privacy mechanism;privacy;privacy considerations;peer review process;public data;reviewer identities;data release mechanism;peer;reviewers;polynomial time;cryptographic protocols;random projections;review data;review paper;assignment process;polynomial time complexity;multiple conferences peer review;new algorithm;arbitrary weights;noisy data;time computable algorithm;reviewer;datathis paper;convex optimization problems;review process;algorithm;guarantees;scientific publication;papers"}, "c4bc2f7e04e02107aa6eaa0c811c3c046efbbc14": {"ta_keywords": "simple learning problem;hardness;strings;cryptographic assumptions;alphabet;nontrivial structure;examples;result", "pdf_keywords": ""}, "69b184f62c97513b03deed96a1443f79b34af0d7": {"ta_keywords": "paper bidding;assignment attacks;review process;integrity;assignment;such attacks;novel approach;current approaches;efficacy", "pdf_keywords": "adversarial bid manipulation attacks;adversarial reviewers;malicious reviewers;bid manipulation attacks;such bid manipulation attacks;malicious reviewer;paper bidding;candidate reviewers;malicious actions;robust bid;bid manipulation problem;dishonest reviewerswe;reviewers;paper reviewing;bidding model;robust relevance scoring model;bids;reviewer;automatic paper assignment system;adversary;robust assignment framework;bid;robust assignment algorithm;other reviewers;paper relevance score;review process;submissions;attacks;assignment system;box attacks"}, "b21fa4f31c4813444e50259dfbe2c56660161174": {"ta_keywords": "statistical distributions;components;analysis;new method", "pdf_keywords": ""}, "89fd287f7eacc7a40d0216ba3b919812da658b94": {"ta_keywords": "spatiotemporal superresolution;latent dynamics;empirical pattern formation approach;selective backpropagation;multidimensional pattern recognition problem;sparsity class;adaptation;accurate inference;data;inference;dsr;novel method;lsda;scale framework;time;method", "pdf_keywords": "neural dynamics;neural dynamics models;relevant neural dynamics;learned quadratic time;neural activity;selective backpropagation;nonlinear latent dynamics;time stochastic representations;latent dynamics;neural population dynamics;neurons;neural network;real electrophysiology data;interrelated neural networks;neural networks;deep generative models;neural population;hidden state dynamics;macaque motor cortex;neuroscience;dimensional dynamics;calcium imaging data;theneural networks;training data;dynamical systems;brain;aneural networks;complex network;fidelity inference;linear dynamical systems"}, "c5950fa3ee124cf2dcb8783db6f582f49170fb45": {"ta_keywords": "generalization gap;generalization;empirical validation;unlabeled data;models", "pdf_keywords": "empirical classifiers;deep learning;threshold learning;supervised learning;training data;deep nets;accurate binary classification;practical deep learning settings;deep learning model;neural network training;accurate binary weight classification;regularization;independent regularization;unlabeled data;early learning error;empirical data;deep neural network;classifier;binary classification;stable learning rule;nonlinear classifiers;general learning algorithms;weight decay regularization;training error;generalization bounds;benchmark dataset;machine learning;gradient descent;stochastic gradient descent algorithm;empirical error"}, "a7822238f5db7d62731eaeabf9725a65f4edf893": {"ta_keywords": "dft model;dependent density;functional theory;ground state;numerical simulations;thermal equilibrium;gft;numerical simulation;model;predictions;calculations;framework;time;concept;results", "pdf_keywords": ""}, "9cdf512f273083efa1ea01f7b31daa97a7bbe884": {"ta_keywords": "muller codes;computational locality;computation schemes;multivariate polynomials;computation;locality;codes;schemes;reed;workers;new approach;recent results;model;lens;number;leverage results", "pdf_keywords": "coded computation;multivariate polynomial computations;certain computational locality;computational locality;computation schemes;computational locality result;computation scheme;muller codes;multivariate polynomial functions;multivariate polynomials thatwe;multivariate polynomials;computations;locality;computation;locality properties;arbitrary multivariate;computational complexity;polynomial functions;polynomial technique;algorithms;codes;multivariate;polynomial function;polynomials;local operation;algorithmswe;local recovery schemes;iterative minimization;large linear transforms;polynomial"}, "6aeb477e5f0882f8363a3a8e5e6f83962d91edc6": {"ta_keywords": "future learning;feature recognition;instructional treatments;demonstrates;computational model;other models;model;machine;techniques;use;literature;variety", "pdf_keywords": ""}, "39ab4b9cdeb4a71b3e25fe5339962654c7c9ff8c": {"ta_keywords": "false information spreaders;social network;credibility;spread path;spreaders neighborhood;nodes;trust;nodes neighborhood;network structure;real world data;accuracy;historical data;model;action;features", "pdf_keywords": "node credibility features;false information spreaders;other false information spreaders;other false information spreaderin;social networks;trust dynamics;global trust scores;social media algorithm;trust;credibility;new spreader prediction model;followee network;trust features;networks;novel graph;false information;spreader;nodes;future spreaders;neural network model;spreaders;node nodes;graph;social media;spread;neural networks;node;symmetric adjacency matrix;node embeddings;generalized symmetric adjacency matrix"}, "00799dceb9e7209bb9d71b38fa5b49483e886978": {"ta_keywords": "dynamic neural networks;dynamic instance;static vertex function;dataflows;specific graph;accurate batched execution;accurate training;training;inference;complexity;arbitrary size;multiple samples;cavs;use;magnitude speedup;order;novel method;approach;paper", "pdf_keywords": ""}, "ab57c70c14b82d07c40c75fecaac98b0e2dc0510": {"ta_keywords": "hierarchical graphical model;linkage problems;linkage problem;novel methods;approach;framework;novel framework;experimental results;efficacy", "pdf_keywords": "unsupervised probabilistic record;record linkage;record linkage records;supervised classification;supervised classification tasks;linkage methods;supervised approaches;supervised setting;field classifiers;unsupervised methods;probabilistic probabilistic model;probabilistic model;record;hierarchical graphical model;hierarchical graphical model framework;pair feature vectors;records;hierarchical latent;database systems;match class;verifying linkage;hierarchical model;data sets;large record;label;linkage problem;linkage problems;pair feature vector;data;database"}, "6413e6a4f68be0ea6aed0082b205147d9f893699": {"ta_keywords": "inflection generation;inflections;encoder;decoder model;machine learning systems;novel method;generation;method;form;lemma;design;problem;paramount importance", "pdf_keywords": "inflection generation;inflection generation model;inflection;inflections;neural encoder;inflection table;other possible inflections;inflection type;decoder translation model;inflectional forms;encode rules;character sequences;character sequence;input character sequence;output character sequenceswe;sequence string transducer;encoder;large corpus;morphological analysis;decoder models;linguistic structure;linguistic structures;neural network;neural network model;decoder model;new encoder;simple encoding;deep learning;language;decoder"}, "8dd85c3a3700d0d282ddbd4dff5e238f24c00676": {"ta_keywords": "strattal spectral envelope;strattal noise;mixture modelling;spectral envelope;planar distributions;strattal;variational framework;histogram;mixture;like peaks;2d;3d;planar;method;novel method;paper", "pdf_keywords": ""}, "74dccd379776bbb50b352c19b8caf2a7896d58ee": {"ta_keywords": "coherency analysis method;coherent constrained variables;coherency analysis;large power system optimisation problems;simple power system optimisation problem;variables;mathematical formulations;sensitivity;method;model;illustrative examples", "pdf_keywords": ""}, "6a3cc30d5d6342d912851deb4362b8c47fa5ede3": {"ta_keywords": "debiasing methods;nlu models;biases;certain biases;challenge datasets;models;examples;evaluation;methods;new framework;improvement;sets;advance;reliance;framework", "pdf_keywords": "natural language inference;artificial bias datasets;natural language representations;synthetic bias dataset;new debiasing framework;debiasing methods;debias accuracy;fast performance prediction;novel debiasing framework;nlu tasks;intrinsic textual entailment;synthetic bias data;debiasing;iterative debiasing;bias;biased examples;unknown biases;shallow learning model;biases;lexical overlap bias;debiased model;linguistic entailment;deep neural networks;natural language understanding;specific biases;specific bias;correct bias;natural language;large scale dataset construction;debias approach"}, "51f6654b9d5925002ccaa5cd339b4377b96719ce": {"ta_keywords": "clustering;workstations;clusters;activities;emails;computer;users;approach;user;novel approach;information;experimental results", "pdf_keywords": ""}, "f4906089f0720c83e57e4a46ae75283df4d67e5a": {"ta_keywords": "own reviewers;reviewers;peer selection problem;strategyproof mechanism;own thinking;thensf;mechanism;ability", "pdf_keywords": ""}, "05f8dd59d4184d38e240bdea4d58e424b8cd055c": {"ta_keywords": "persuasion;persuasive power;online debate;successful persuasion;reputation;argument content;influence;cognitive overload;heuristic information;platforms;additional reputation points;managerial implications;manner;probability;individuals;impact;average;characteristics;processing;theoretical model", "pdf_keywords": "online debate;online argumentation platform;single online debate;observed debate outcome;deliberation;persuasive power;debate text;persuasion;debate outcome;debate posters;opinion exchange;debates;debate success rate;debate success;persuasive messages;persuasion settings;persuasive message content;endogenous opinion selection;deliberation process;reputation indicators;successful debate;successful persuasion;debate;persuasivewe;american political debate;influential posters;future debate;reputation;recent debate;conversation trees"}, "59f41c5024a238ae8843f3dd67692961ecc63e75": {"ta_keywords": "deep neural networks;phoneme recognition;evolutionary algorithms;digit detection;efficient optimization strategy;evolution strategy;discovery;simple binary vector;acyclic graph;above binary vector;other tuning parameters;dms;paper;approach;performance;respect;experiments;effectiveness", "pdf_keywords": ""}, "f9e32b30fd9ad50cce12ffb753c7be88100b6dc2": {"ta_keywords": "crowd;efficient estimator;permutation;data;significant generalization;global minimax rates;minimax;different estimators;new error metric;lower bounds;model;logarithmic factors", "pdf_keywords": "crowdsourced labeling;crowdsourcing data;crowdsourced data;many crowdsourcing applications;crowdsourcing;labeling models;efficient estimators;crowd;efficient estimator;estimation;labeling problems;node estimator;minimax lower bounds;such estimators;estimators;noisy labelers;noisy representations;information models;minimax;sample complexity;efficient algorithm;sparse representations;wan estimatorwe;labels;permutationwe;nodes;optimal constraints;workers;global minimax rates;approximate ordering"}, "15ac2d8629ca9241ea558eb2b816272d82447ac7": {"ta_keywords": "statistical bias;models;estimators;variance tradeoff;classical parameter;people;class;surprising win;second challenge;art methods;win", "pdf_keywords": ""}, "18ddcd250bbbe716a0616412ea329a8343f60542": {"ta_keywords": "crowdsourcing;theorems;economic documents;social nature;proofs;problem;generic setting", "pdf_keywords": ""}, "c18700ed4ef07dd85ba8bceab3b9584c6e6af49c": {"ta_keywords": "maximum entropy;data;measurements;copies;observation;set;new method;number;method;square", "pdf_keywords": ""}, "af4e11436268cf68505f1caee5d6f7ff0df9c99a": {"ta_keywords": "natural language processing models;causal inference;natural language processing;causal effects;text;fairness;interpretability;outcome;statistical challenge;estimation;robustness;performance;unified overview;treatment;survey;use;addition;means;potential uses;settings", "pdf_keywords": "causal inference;statistical causal inference;natural language processing;causal reasoning;natural language;causal graphs;counterfactual outcomes;causal conclusions;natural language classifiers;causal effects;causality;counterfactual data augmentation;nlp modelswe;nonlinear inference;causal structure;language models;causal mediation;counterfactual examples;confounders;linguistic structure;confounder;inference;counterfactual data substitution;interpretability;language processing;text data;bias;text;supervised models;predictors"}, "f8b32c2edcd7ef098ce40b7fd2e68448ac818191": {"ta_keywords": "eye movement features;support vector machines;novel eye movement features;svm;natural reading;tracking features;eye;unknown words;detection performance;personalization;measure;experimental results;method;paper;previous work", "pdf_keywords": ""}, "11e4346e60ac76ad018231a851fbbdb2112044d2": {"ta_keywords": "claim phrase veracity;novel claim verification benchmark;claim phrase;claim verification;claim;neural model;validity;interpretationability;novel feature;few assumptions;predictions;final verdict;model;novel approach;approach;other approaches", "pdf_keywords": "claim phrase detection modules;interpretable fact verification;claim phrase veracity;claim veracity prediction;claim retrieval;claim phrase predictions;phrase veracity prediction;claim phrases;phrase veracity predictions;claim phrase samples;largescale fact verification benchmark;textual claim;claim veracity;claim verification;evidence retrieval;claim veracity zi;claim sentence;evidence sentences;novel evidence retrieval method;trustworthy knowledge source;fact verification problem;final claim verification;novel interpretable method;claim phrase premiseswe;evidence sentence pairs;veracity;most verification systems;probabilistic soft logic;claim levelthis paper;logical knowledge"}, "bcbac71ac64cd6a6aaae41e37ebe960f508ab741": {"ta_keywords": "symbolic representations;symbolic representation;information;language;questions;additional information;model;earlier models;novel model;world;performance;process;inclusion", "pdf_keywords": "symbolic knowledge representations;neural knowledge bases;fact memory;knowledge bases;contextual embeddings;knowledge base;language models;external entity memory;embeddings;context embeddings;knowledge graph;language model;embedding;context entities;natural language;new knowledge;memory;knowledge;freebase entities;structured data;entities;deep learning;entity pairs;computational linguistics;domain questioning task;entity;linguistic structure;accurate representations;corpus;subsymbolic representations"}, "1ca247158522991ad54cccaac6c6938576a8bd26": {"ta_keywords": "rna transcription;rna transcription region;transcription signal;neural model;nonlinearity;model;accurate evaluation;translation;field;problem;new approach", "pdf_keywords": "lexical translation model;efficient information retrieval;parallel corpus;lambda translation model;marco document;deep learning transformers;stage retrieval system;rank;marco model;task;lambda;lambda model;other baselines;results;lambda framework;large class;m1;vertex representation;hidden validation;software;training;target machine;information;implementation;untuned variant;novel method;data;help;method;availablewe"}, "2eef9173946078c402596b9b080b6878db00b8ac": {"ta_keywords": "social media data;social media content;social media site;social media;obesity;type ii diabetes;available twitter archive;early detection;analysis;results;community;individual users;users;method;predictions;language", "pdf_keywords": ""}, "38bd034e6a0589bf1132d3e8c79818b271377290": {"ta_keywords": "discriminative training;discriminative training problem;margin;hinge loss;minimum;information;unified framework;gin term;derivative;error;theorem;respect;new theorem;mar;fundamental observation;article;integration", "pdf_keywords": ""}, "3429a6b440fb6f71990bbeda9d097d709634a913": {"ta_keywords": "parser selftraining;machine translation;parse tree;parse trees;automatic evaluation metrics;training data;translation;self;trees;effectiveness;variety;method;paper;experiments", "pdf_keywords": ""}, "9ce09b03f056253252f3e8c0c65d86a27117a0ac": {"ta_keywords": "image classification;adaptation;unlabeled domain;new data;public domain standard;nasa;automatic data analysis;robustness;model;novel approach;feasibility;target;public;approach;problem", "pdf_keywords": "tuning domain adaptation test;domain adaptation;novel adaptive classifier;adaptation framework;adaptive training;online adaptation;large scale image classification datasets;imagenet;adaptation;domain classification;training hyperparameters;deep learning;image classification;training data;robust classifier;supervised classification;supervised learning;time adaptation;supervised learning model;simple classifier;adversarial noise training;time adaptation source data xs;supervised task;target image;standard image corruptions benchmark;deep learning kernel;empirical empirical image corruptions;convolutional residual networks;benchmark;art classifiers"}, "2db020e3398c06e3a22f12d8caffe76b0d9d1dda": {"ta_keywords": "commonsense tasks;language models;tasks;novel neuro;shot question;hypotheses;symbolic framework;benchmarks;training regimes;data generation strategies;empirical results;form;set;impact", "pdf_keywords": "commonsense tasks;question generation techniques;choice commonsense tasks;linguistic tasks;semantic relations;commonsense questionwe;commonsense models;knowledge graphs;knowledge injection systems;current knowledge knowledge;novel knowledge structures;unstructured questions;knowledge sources;knowledge models;knowledge structure;tasks;natural language;language models;challenging questions;knowledge source;knowledge;questions;task;zeroshot;additional distractors;zeroshot evaluation;distractors;data construction;more challenging representations;symbolic framework"}, "68dca6ee694f22e2af66b56e60fdfa74041242e6": {"ta_keywords": "magnetic field;large magnetic field;valve junction;variational method;spin;valve problem;dynamics;few tesla;numerical simulations;simple model;model;description;exact solution;limit;results", "pdf_keywords": ""}, "a0f8733dd84608b3cad97904624f8bfdc2d2fcbf": {"ta_keywords": "magnetic properties;magnetic field;effect", "pdf_keywords": ""}, "890317710697a9e41d0d9961d99986c4d865393f": {"ta_keywords": "aware mobile apps;novel representation learning model;representations;novel spatio;meta path;world dataset;temporal pattern;centric task;graph;attribute;units;objective function;performance;arbitrary user;structure;scale", "pdf_keywords": ""}, "b6c4a96e09b9f11e7c70e7f1fbe3f3971b92762d": {"ta_keywords": "text generation;language generation;machine translation;fudge models;text;topic control;fudge;conditional distributions;poetry;conditional probabilities;future discrete data;formality change;tasks;couplet completion;interest", "pdf_keywords": "formalcontrolled text generation;text generation;natural language models;future word prediction;natural language;natural language networks;natural language network;large pretrained language models;machine translation;language model generations;conditional factorization;bayesian generalization;linguistic output;text;machine translation pipeline;topic control methods;simple probabilistic factorization;future discriminators;bayesian conditioning model;conditional distribution;linguistic structures;simple conditional data;novel topic control problem;original machine translation pipeline;sentences;text style transfer;simple machine translation problem;attribute predictor;generation;rhyme prediction"}, "97e8430fe01394ea9a49fd841d9aecdfc294a796": {"ta_keywords": "fuzzy rules;genetic algorithms;ultrasonic image;ultrasound images;myocardial heart disease;classification rate;optimization;diagnosis;abnormal cases;physician;sets;method;computer;problem;results;potential utility", "pdf_keywords": ""}, "33206a493e3519d27df968e98eb7fe6af14ef985": {"ta_keywords": "causal relationships;causal model;events;memory;process;model;dataset;examples;method;review", "pdf_keywords": ""}, "89440a4cb27d17ced5d54bbe0f81f3477bf16404": {"ta_keywords": "particle;motion;measurement;transverse confinement;simultaneous measurement;velocity;momentum;shape;method;medium;sum", "pdf_keywords": ""}, "27dafb0b6076e050c31ede8d3e0184ef3592b364": {"ta_keywords": "magnetic properties;magnetic field;effective field theory;field;effect;framework", "pdf_keywords": ""}, "591ebe6dccb388d041623840db29a5e58824b4b0": {"ta_keywords": "compact preference models;preference statements;preference orders;preference variables;graph statements;graph structure;uniform generation;graph;new generation algorithm;algorithm;flipping length;usability;number;positive implications", "pdf_keywords": ""}, "7f1a6c67d03de88b898271d52dd2e51907d5b615": {"ta_keywords": "natural language processing tasks;dependency parsing;natural language analysis;labeling spans;spans;semantic role;disparate tasks;relations;unified format;models;essential features;wide variety;novel method;model;art;state;efficacy", "pdf_keywords": "natural language processing tasks;natural language analysis tasks;natural language representations;natural language processing;semantic role labeling;natural language processing pipeline;dependency parsing;semantic parsing;different language analysis tasks;natural language analysis;natural language;task learning;parsing;relation extraction;unified span;other tasks;sentiment analysis;nlp;labeling spans;tasks;single task;disparate tasks;machine translation;specific tasks;span representation;linguistic structure;target tasks;spans;underlying source task;span"}, "a9c0ffc760f65ccaa99a08fc66b31653fd4a5bd7": {"ta_keywords": "organ donation;quality;review;young people;field;life;important aspect;importance;important issue;literature;aim;last few years", "pdf_keywords": ""}, "a03675379685d88c727bc985a323cc71d06f2514": {"ta_keywords": "structured generative;discrete syntactic structure;novel generative model;continuous word representations;invertible neural network;efficient exact inference;prior;dependency;unsupervised fashion;marginal likelihood computation;part;model;induction;art models;invertibility condition;state;experiments;demonstrate", "pdf_keywords": "unsupervised dependency parsing;structured generative;simple generative process;generative model;novel generative model;generative process;discrete syntactic structure;novel generative approach;structured syntax models;continuous word representations;dependency parsing;unsupervised learning;novel unsupervised learning model;syntax model;structured syntax model;latent embeddings;discrete embeddings;standard speech induction;embeddings;syntactic aspects;word language modeling benchmark dataset;supervised learning;unsupervised pattern recognition;efficient exact inference;structured syntax;training data likelihood;sentences;empirical parameterization;hidden markov model;deep learning"}, "7f817600b612aab6039dfba576ae8e8e7460d8f1": {"ta_keywords": "initial pronunciation dictionary;word pronunciations;textual data;dirichlet process;wsj data;learning;text;discrete version;model;phone;new framework", "pdf_keywords": ""}, "60a9438c24847a949419e0350a61fc2a330e4a09": {"ta_keywords": "many popular kernel clustering criteria;kernel clustering;density mode isolation bias;density biases;kernels;histogram mode isolation;empirical artifacts;bias;similarity;significant artifacts;common class;decision;conditions;past;breiman;principled solutions", "pdf_keywords": "kernel clustering criteria;kernel clustering;new pairwise clustering criterion;new clustering criterion;clustering;small bandwidth kernels;tight dense clusters;clusters;continuous domain clustering problem;continuous clustering problem;kernel functions;density biases;adaptive kernels;data density;bandwidth kernel;classification;many popular kernel;kernels;many machine learning tasks;feature space;decision tree;empirical observation;density mode isolation bias;data embeddings;data density mode;image classification;machine learning;data density equalization;accurate density estimation;novel continuous gini criterion"}, "11db042ed2264f3ea1b8f20151adf725ec3461e8": {"ta_keywords": "neural networks;partial training;error surfaces;convergence properties;apparent convergence;critical points;weights;large movements;nonconvex;random initialization;weight space;loss;flat regions;symmetry", "pdf_keywords": "neural networks;error surfaces;partial training;training paths;local minima;random initialization;weights;critical points;large movements;symmetry;flat regions;apparent convergence;weight space;loss;nonconvex;good ones;observations;regions;novel approach;analysis"}, "201b79be15b6b01e62a82b29ac4d30d3e6a11799": {"ta_keywords": "channel speech signal;speech signal;recurrent neural network;law noise;rnn;variance;encoder;decoder;law reduction;power;effective estimation;law dereverberation;model;novel approach;use;presence;case;approach", "pdf_keywords": "end speech recognition model;channel speech recognition case;speech recognition module;speech recognition;separate speech streams;decoder network;level speech recognition problem;encoder;speech features;recurrent neural network;encoders;novel encoder;input speech;multisource encoder;speech separation;decoder;encoderwe;communication channel;channels;speaker;primary neural beam;channel;channel ones;2mix corpus;channel ansj1;generative pretraining;reverberant wsj1;channel dynamics;entire input sequence;hidden power relations"}, "62606fbb3aa3ffb17c5427b3652c18a81425cd65": {"ta_keywords": "training data;annotated data;data;new class;conversion;novel method;creation;large amounts;method;use;effectiveness", "pdf_keywords": ""}, "99848c6424556bce427d621e89b6d05dac131910": {"ta_keywords": "unique subsumption pairs;decentralized answer aggregation mechanism;crowdsourcing;human intelligence task;loka microtask platform;pairs;pair;microtask platform;project;meaningful relation;aim;first report;question", "pdf_keywords": ""}, "7a1bcf3c84607f7aeb0601658845ca2083059f43": {"ta_keywords": "supervised learning model;multidimensional model;linear programming language;learning;input data;scale dataset;discrete objects;context;novel approach;np;set;approach", "pdf_keywords": ""}, "ef09dd6f5615e2b937d3f9dd555c2daafb4c4f4b": {"ta_keywords": "parallel virtual machine library;parallel virtual machine;traditional parallel virtual machine;performance;memory nodes;processors;libraries;library;memory;code loops;typical application;terms;context;number;paper", "pdf_keywords": ""}, "aa0d4f7cfa13758a02d248fd607547f045306519": {"ta_keywords": "standard email performance measure;email content;document recognition;email;document;performance;time;simple statistical model;presence;function;factor;question;method;relationship", "pdf_keywords": ""}, "36db0616e59c8ac5e9ba8ded820ef6c969f068c1": {"ta_keywords": "stationary optical fiber;stationary optical spectrum;dimensional optical fiber;optical fiber;optical elements;spectrum;fiber;symmetry;construction;new method;high degree;set", "pdf_keywords": ""}, "78f1eef6d79a129f59b977a5037f5fc9cc7fda90": {"ta_keywords": "submissions;peer;algorithms;new algorithms;review;statistical physics;contributions;content;web;evaluation;quality;new tools;process;collection;variety;applications;new representations;construction;principles;development;set", "pdf_keywords": ""}, "afdae523d420278670c30f45c015cc5860a0de22": {"ta_keywords": "adaptive gradient methods;adaptive methods;stochastic line;search methods;constant step;parameterization;search method;optimal rate;convergence;step;polyak;size variants;size;sizes;problem;context", "pdf_keywords": "adaptive gradient methods;adaptive gradient method;adaptive gradient descent problem;stochastic gradient method;gradient descent;adaptive step;stochastic line search method;gradient methods;parameterized gradient model;stochastic search algorithms;adaptive preconditioner;stochastic polyak step;adaptive preconditioners;large parameterization;stochastic weights;optimal step size;gradient method;stochastic search algorithm;gradient norm;scalable machine learning framework;convex optimization problems;adaptive version;noisy learning environment;stochastic line;adaptivewe;better convergence;optimization;learning model;fast convergence points;parallel parallel parallel sgw method"}, "656aedc681975c3c97b1764466832de537358150": {"ta_keywords": "hybrid automatic speech recognition;auxiliary feature extraction module;available recognition tasks;sequence summary network;auxiliary feature;recognition performance;adaptation scheme;asr;e2e model;models;e2e;systems;end;computational graph;part;scheme", "pdf_keywords": ""}, "2ccfa631708b78130b1ea1b8ae3c2b688caf3938": {"ta_keywords": "surgery parameters;surgery patients;heteroscedasticity;surgery;optimal timing;optimal trade;booking;specific uncertainty;field models;corpus;case;problem;current practice;issue", "pdf_keywords": "surgery duration data;optimal surgery duration;surgery case durations;neural regression algorithms;surgery duration;surgery durationwe;surgery records;partial learning regression;hour surgery duration;surrogate model;partial learning;supervised learning model;generalized multilayer perceptron;electronic health records;novel surrogate;surgery task;minutes;durations;large national hospital;surgeries;surgery operations;duration;underlying predictive distribution;clinical record;optimal time;neural network model;prediction;scheduling;dataset;surgery"}, "58777f0af009a225e315b7240db20ba545207702": {"ta_keywords": "preference aggregation;deterministic preferences;aggregation schemes;individual agents preferences;social choice function;aggregation;probability distributions;agents;manipulation;theorem;theorem theorem;large numbers;ability;set;problem", "pdf_keywords": ""}, "309fb4d4d0946ac746f352c13cd3be4e2cd86dae": {"ta_keywords": "probabilistic model complexity control method;stochastic model complexity control method;acoustic modeling;posteriori approximation;variational approximation;speech;underlying probability distribution;maximum;applications;related applications;versions;paper;hypothesis;sum;inequalities", "pdf_keywords": ""}, "ce17dab00ddd2c86da508fc0502247f9b18a570f": {"ta_keywords": "proportional approval voting;satisfaction approval voting;approval voting;prominent voting rules;approval ballots;multiple winners;winner;strategy;rules;proof;open problem;np", "pdf_keywords": "proportional approval voting;prominent approval voting rules;prominent voting rules;voting process;approval voting rules;popular popular approval voting rule;stochastic voting;approval voting;voting rules;voting rule;satisfaction approval voting;winner determination;approval ballots;binary winner;computational complexity;votes;complexity;multiple winners;votingwe;candidates;single winner;approval;third approvalwe;candidate;binary preferences;single agent;winning set;agents;proportional representation;resource allocation problem"}, "1d1bbed89882ac1001b915ee73199a919aa0d13c": {"ta_keywords": "effective language models;vocabulary language modelling;training languages;universal linguistic knowledge;languages;prior;posterior;laplace;network weights;character;sample;task;data;method;level", "pdf_keywords": "language models;prior outperforms baseline models;neural recurrent models;language prediction;level language modeling;language modeling;conditional language models;term memory;human languages;strong priors;conditional language modelswe;outperform baselines;uninformative priors;new languages;diverse training languages;linguistic adaptation;available training languages;languages;heldout languages;universal linguistic knowledge;deep learning;language;neural weights;posteriori inference;linguistic domains;unnormalizable priors;learning;corpus7;neural networks;prior knowledge"}, "7b7f8fb08262fce3da64c09788fd4b595408e4e6": {"ta_keywords": "harmonic oscillator frequency comb;harmonic oscillator;spectral density;frequency comb;particle;calculation;power law;method;analysis;new method", "pdf_keywords": ""}, "d940e0192a6cc1ddd6288239b77b06e50f042114": {"ta_keywords": "speech representations;automatic speech recognition;available corpora;asr;e2e;representations;advanced end;sev;eral;tasks;end;source;more scenarios;experimental results", "pdf_keywords": "end speech processing;speech representation;speech representations;automatic speech recognition;speech segmentation patterns;advanced speech processing models;speech recognition;advanced speech recognition systems;various speech representations;overlapped speech corpora;speech signal;learning representation;attention model;deep learning;learning representations;encoder;speech separation;corpus;spontaneous speech;deep bidirectional transformer encoders;untran speech;convolutional neural networks;filterbank;deep clustering;sslrs;segmentation;learning rate;recognition;utterance;generative encoder"}, "d5924c8cdef6270a955ba82c2b07a8282d869744": {"ta_keywords": "gesture generation;gesture models;acoustic cues;partial speech;spoken language;partial acoustic wave;language;novel class;novel method;approach;power;relationships", "pdf_keywords": ""}, "b82e9b84cb639f6bb061c8a43b97986ecfec00ea": {"ta_keywords": "partite graphs;similarity;column classification;comparable precision;query response time;automatic set instance acquisition;task;dimensions;representation;magnitude improvement;specific baselines;new representation;variety;approach results;number;orders;terms;expansion;respect", "pdf_keywords": ""}, "ee33d61522fd70fa4e6470decbdac6c17f8b4fdb": {"ta_keywords": "attractor calculation;neural speaker diarization;multiple attractors;encoder;attractors;decoder;speech;attentive end;eend;sequence;flexible number;end;conventional self;paper;ed;method", "pdf_keywords": "deep attractor network;end speaker diarization;speaker diarization;attractors;speaker mixture withspeaker diarization;multiple attractors;speech mixtures;attractor calculation;end encoder;encoder;neural speaker diarin;speaker diarisation;attractorwe;speaker mixture;embeddings;speakers activities;speaker interaction;independent speaker separation approach;speaker system;independent speech separation problem;speaker;decoder;speech;speakers;noisy environment;invariant learning;diarization;automatic recognition;conversations;attentive end"}, "d6d2003d112e3d9d93edd4920436fe2fe879eb87": {"ta_keywords": "large scale text mining;fast clustering;large scale text datasets;pairwise similarity;similarity matrix;linear adaptation;data;context;power;previous methods;method;new method", "pdf_keywords": ""}, "3b8b6f27a5df5dc2c231d0fa1e471887e4583466": {"ta_keywords": "publication networks;social networking content;social information;genes;author;novel approach;recent work;knowledge;future;baselines;use;approach;problem;variety", "pdf_keywords": ""}, "4bfc185dcc67b3eddfa059fc5446f4df844a0728": {"ta_keywords": "orbit interaction;spin;orbit;coupled system;dynamics;enhancement;effect;suppression", "pdf_keywords": ""}, "9635e3c008f7bfa80638ade7134a8fb0ef1b37e1": {"ta_keywords": "language model;tokeniser uncertainty;tokeniser entropy;marginal likelihood;domain data;perplexity;difference", "pdf_keywords": "language models;modern language models;language model robustness;best tokenisation metric;best tokenisation;language model performance;sampled tokenisations;single canonical tokenisation;language modelling;language model;tokenisations;dominant language modelling paradigm;tokenisation;best tokeniser;multiple tokenisations;tokeniser distribution;unstructured language representation;tokeniser entropy;high tokeniser entropy;thecomputational linguistics;tokeniser;input tokens;underlying language;natural language;different token sequences;marginal likelihood estimate;marginal likelihood;new evaluation metric;stochastic beam search;structured model"}, "3f2821dd40c12da560c89b9dad7f95cd4ad9354f": {"ta_keywords": "adaptive redundancy protection;ciphers;cipher;storage capacity;clusters;disk;overload;novel architecture;architecture;exponential growth;imminent heart attack;significant challenges;paper;size;significant cost", "pdf_keywords": "scalable storage implementations;scalable storage;reliable storage systems;extensible storage system;novel storage system architecture;scale storage systems;novel storage system;storage clusters;scale storage clusters;storage cluster;storage nodes;cluster storage system;performance storage systems;adaptive redundancy management;redundant disks;storage systems;robust redundancy management;new storage system;storage system;asynchronous storage environment;adaptive redundancy schemes;suitable redundancy scheme;data redundancy;world storage clusters;scale storage devices;adaptive redundancy;high density storage;storage devices;redundancy scheme;simple network storage model"}, "ff6ddbd7ba59e0fd4a74748942083391d6e9a666": {"ta_keywords": "metastable state;metastable process;metastable system;particles;similar process;state;information;analysis;system;novel approach;approach;results;use", "pdf_keywords": ""}, "9fe09ca520cb7ce106e65b39c455777d18ec6efe": {"ta_keywords": "arborist taxonomies library;taxonomies;taxonomy;recall;reciprocal rank;mean reciprocal rank;texts;algorithm;contributions;margin function;higher quality;art;principled formulation;approach;state;innovative approach", "pdf_keywords": ""}, "ef4166a7fb2c40ca87b0ebb253e8ba1e80c09fd7": {"ta_keywords": "speech recognition challenge;augmented discriminative feature transformation;advanced speech recognition baselines;discriminative feature transformation;feature transformation;chime;vocabulary subtask;arbitrary features;training code;task;novel approach;paper;approach;experimental results", "pdf_keywords": ""}, "5db0fa82c322bb7d9f60109294d088ff139eebf3": {"ta_keywords": "gan manifold;noisy image;bayesian markov chain;unseen images;image space;images;projection;quality;nearest point;novel method;identification;method;key ingredient;mss", "pdf_keywords": "gan manifold;noisy images;gans;generative adversarial networks;gan;real images;convolutional neural networks;deep learning;gradient;generative models;image space;images;neural networks;latent vectors;bm3d;image;generative counterparts;latent vector;noise;higher quality;learning;novel method;learning rate;different noise variance;technique;dataset;corrupted version;nearest point;method;paper"}, "1d05e91b6d94f06439b2b41291a8dcc3d8064149": {"ta_keywords": "image grid regularization;segmentation;graph cuts;kernel bandwidth;general pairwise feature;standard color model;adaptive bandwidth strategies;energy;gini bias;novel approach;techniques;means;formulation;methods;extreme cases;effectiveness", "pdf_keywords": ""}, "da1f22dd6d834e031eb733d2b70320f34ef9458f": {"ta_keywords": "parametric ordinal models;cardinal measurement setting;quality score;pairwise comparison;ordinal settings;cardinal;optimal error;error rates;identical scalings;lower bounds;items;constant prefactors;class;problem", "pdf_keywords": "crowdsourcing;commercial crowdsourcing platform;pairwise comparisons;quality vectors;pairwise comparison;quality scores;quality score;rankings;comparison graph;weighted;minimax estimation rates;aggregation;single personrank ordering;preferenceelicitation models;estimation;noisy data model;pwk quality vectors;mle;rank ordering;pairwise;comparisons;weighted laplacian;data elicitation;luce model;parametric models;negative log likelihood;noisy data;comparison;algorithms;rank"}, "b07b124852f897823490db0a04ea6e411bb77f00": {"ta_keywords": "binary classification;optimal threshold;classifier;classifier outputs;f1 measures;f1 measure;optimal value;conditional probabilities;half;context", "pdf_keywords": ""}, "b6502b61bf8f0332c6caa30198cff3619a9790aa": {"ta_keywords": "redundant requests;storage systems;latency performance;policies;rigorous analytical study;situations", "pdf_keywords": "redundant requests;memoryless service time;arrival service;batch servers;certain request process;memoryless service;latency performance;availability;optimal timing;independent batch processing systems;service time;multiple servers;many servers;service times;latency;servers;large batch operations;scheduling policies;centralized buffer;memoryless batch processing system;average latency;batch processing system;optimal buffer;pid process;heavy everywhere distribution;requests;processing process;storage system;service;minimum distribution"}, "b72c5236dacf2b958ebcf427d17a100bc54af504": {"ta_keywords": "encoder;denotating;context inheritance;hughes dataset;context;shannon;aware inheritance mechanism;inheritance mechanism;novel mask technique;block;computational load;vector;novel approach;evaluations;approach;problem", "pdf_keywords": "end speech recognition problem;long speech feature sequences;automatic speech recognition;novel encoder architecture;encoder;aware encoder;novel encoder layer;novel encoder;de encoder;encoders;encoder problem;transformer encoder;context embeddings;novel decoder layer;global speaker;local acoustic information;decoder;novel context inheritance mechanism;block encoding;neural network;block input sequence;deep learning pipeline;context information;global context information;novel encoding;encoding;attention;context inheritance mechanism;context space;conventional encoding"}, "6dfc2ff03534a4325d06c6f88c3144831996629b": {"ta_keywords": "quality commonsense problems;unstructured problems;simple movies;unstructured unstructured unstructured problems;recognition;low bias;high accuracy;simple algorithm;novel method;quality results;approach", "pdf_keywords": "visual commonsense reasoning;large scale movie description challenge;visual commonsense;natural language processing videos;annotated images;unstructured images;unique movie scenes;image interestingness;recognition;video commonsense;unstructured natural language responses;pattern recognition pipeline;unstructured video frames;scene descriptions;unstructured unstructured images;crowdsourcing task;movie images;unstructured unstructured unstructured images;powerful vision representations;deep model;pattern recognition;robust answers;art vision systems;computer vision;mechanical turk;images;scene captions;unstructured multiple choice problems;dimensional images;videos"}, "df43f6ff7c66d39240235af3052be55222bef80d": {"ta_keywords": "mixture models;speaker clustering problem;nested gibbs;mixture;sampling;novel sampling method;method;paper", "pdf_keywords": ""}, "964293c1cb1b619eb9b474381d2ba60cf44fcc2d": {"ta_keywords": "distribution facility management;distribution facility;distribution line maps;distribution facilities;mapping system;tpi database;planning;maintenance;construction;design;data;system;input;corrections", "pdf_keywords": ""}, "d389d8c2e15f9e9269c17fe6f960f70559eee840": {"ta_keywords": "unsupervised word segmentation;text corpora;word embeddings;morpheme patterns;downstream language modeling;parsimony criterion;novel method;quality;use;state;method;experiments;art approaches", "pdf_keywords": "word embeddings;unsupervised morpheme segmentation;meaningful morphemes;morpheme vocabulary;interpretable morphemes;word representations;morpheme predictability metric;morphemes;morpheme segmentation algorithm;continuous word representations;morpheme boundaries;morpheme patterns;candidate morpheme patterns;new linguistic datasets;quality morphemes;candidate morphemes;word segmentation tasks;downstream language modeling tasks;enriched embeddings;word similarity;segment words;language discovery;new language discovery datasets;new language models;corpus;transformational linguistics;text domains;most subword;vocabulary;words"}, "3d1318bc66d534eefac7c665fd7cc891fba27b87": {"ta_keywords": "zeeman field;effect;magnitude;order", "pdf_keywords": ""}, "33972d9e9a102f9388e5850d8aed3d1aefc9d2e5": {"ta_keywords": "fallacious argumentation;argumentative discourse;logical arguments;logical network;approach;novel approach;nature", "pdf_keywords": "fallacious argument game argotario;software argotario;new open source architecture;argumentative discourse game;games;computational argumentation;argotario;fallacious argumentation;fallacious arguments;software;game;fallacious actions;rich dataset;argumentative discourse;dataset;annotated documents;argumentative assignment;first beta;tool;argument;demo;novel methodology;essential aspects;source;computational linguistics research;workflow;feedback;qualitative criteria;view;methodology"}, "04138da3bac26f83a9d57152118d4cd5cc8c717d": {"ta_keywords": "adaptive similarity measure;adaptive graph walks;adaptive graph walk;similarity measures;relation graphs;personal information management;structured text;search queries;entity;intelligent message threading;person name recognition;parsed text;corpora;graph;alias finding;processing;inter;different tasks;different domains;framework;thesis", "pdf_keywords": ""}, "395aae6e7a79e5760457ca38e868acc970016230": {"ta_keywords": "attention architecture;table reasoning datasets;large tables;tabular data;memory;documents;current accelerators;architecture;speed;appropriate inductive bias;respect;art;new state", "pdf_keywords": "table reasoning tasks;textual table reasoning;table transformers;attention;attention model;novel attention model;attention pattern;tables;table;attention flow;attention patterns;tabular;table table;table cell;memory;rows;efficient representation;table data;learning tasks;computing;columns;machine learning pipeline;novel architecture;reasoning;large transformers;transformers;tensorflow;novel computational transformer;representation;predictingcomputational neural machine learning"}, "e07c2e66dab7b61091bb8a4ad132bf279c233027": {"ta_keywords": "stochastic topic modeling;citations;citeseer data;link structure;joint modeling;single graphical model;bipartite graph;novel models;ldda;text;ideas;context;first model;lpda;second model;experiments;subset", "pdf_keywords": ""}, "aeb4478619461ca25592e6d692f3591ec8c4091b": {"ta_keywords": "semantic collisions;filtering;texts;other potential mitigations;same underlying principles;gradient;approach", "pdf_keywords": "adversarial semantic collisions;semantic collisions;adversarial attacks;paraphrase identification;nlp;machine learning tasks;natural language processing;useful toolwe investigate collisions;nonsensical collisions;deep learning;input semantics;word approximation;collision prediction;nonlinear machine learning;semantic relevance;more natural collisions;adversary;nonlinear generative model;high similarity;collision generation;collisions;sentences;collision examples;language model;transferable collisions;word;document retrieval;collision;attacks;machine learning"}, "3a3fb140890dbba93290e358af700f9a5c8bcc7a": {"ta_keywords": "symbolic meaning representations;natural language;gram machine;deep learning;gram;synthetic text;entire text;wikipedia;encyclopedia;babi problem;bab;domain questions;scalability issue;different documents;problem;novel method;approach", "pdf_keywords": ""}, "2f780a18d44f4e3c5c4c74d4060b8dfd542a778d": {"ta_keywords": "sports broadcasting;broadcast;public broadcast system;players trajectories;american football;individual players;players;narrative;dynamics;time;impact;different points;factors;amount;available database;variety", "pdf_keywords": "racial bias;broadcast football transcripts;bias;racial metadata;white players;football broadcasts;unbiased descriptions;black players;nonwhite players;racial composition;major offensive positions;unconscious racial boundary;player attributes;players;level gender bias;race;statistical contributions;nlp;football;ncaa;football team;natural language processing;playerswe examine;natural language corpora;last name players;250k player mentions;empirical data analyses;many mentions;statistics;sportswe"}, "34fc6da7a88433478fd976fd0b9de3cf7134e652": {"ta_keywords": "quantum numbers;quantum field;random variables;particles;distribution;set;variables;field;number;new method;method;analysis", "pdf_keywords": ""}, "df1d89f4ca9c20e2c6703cdbf26a62f2b50ac71c": {"ta_keywords": "equilibrium concepts;particle physics;equilibria;simultaneous gradient descent;sum games;dynamics;critical points;convergence;standard model;conditions;connections;context", "pdf_keywords": ""}, "02980e5ba847282b683a85e7a8862c6c1b6e0d94": {"ta_keywords": "magnetic properties;magnetic anisotropy;local magnetic moments;magnetic field;strength;effect", "pdf_keywords": ""}, "73aa33fd469b171d50c452c5e3fe0e9e03520520": {"ta_keywords": "public domain knowledge exchange;knowledge exchange method;public domain;proceedings;iwisps meeting;evaluation;systems;lds;snn;pdr;method;set", "pdf_keywords": ""}, "c330ec2047d019d98233abb59d13b3256c662cc7": {"ta_keywords": "counterfactual distributions;counterfactual queries;counterfactual probabilities;experimental counterfactual distributions;counterfactual query;arbitrary structural causal model;counterfactuals;counterfactual variations;discrete structural causal models;structural causal models;arbitrary causal diagram;partial identification algorithms;causal model;same causal diagram;partial identification task;equivalent polynomial program;target;effective montecarlo;canonical family;context;finite support;program;variables arewe;world;bounds;special family;result", "pdf_keywords": "counterfactual identification;counterfactual distributions;counterfactual distribution;arbitrary counterfactual probability;counterfactual queries;unknown counterfactual probabilities;counterfactual probabilities;counterfactual uncertainty;discrete structural causal models;counterfactual variations;counterfactual probability;discrete microscopic causality;arbitrary causal diagram;causal model;classical counterfactual equivalence;interventional data;interventional probabilities;interventional distributions;causality;observational constraint;causal diagramwe;novel partial identification algorithms;empirical evidences;interventional distribution;interventional constraints;observed behavior;partial identification;observational datasets;observational data;causal effects"}, "0a3caecce668731efe7abf37720793eed1fb951a": {"ta_keywords": "political messages;twitter user;tweets;accurate classifier;classifier;political affiliation;precision classifier;information sharing;large corpus;sender;presidential election;information;message;result;analysis;receiver;impact;systematic study", "pdf_keywords": ""}, "5bca90a331417402f5018f552e1a62656dd7fc5b": {"ta_keywords": "noisy noisy graph;community structure;noisy nodes;noisy observations;nodes;community;graph;efficient algorithm;information;features;problem;theoretic limit;simple linear combination;number", "pdf_keywords": ""}, "5166c0e04d77ac0f7969c49c0f8f18129a114198": {"ta_keywords": "motion;measurement;central patient;clinical environment;patient;vicinity;method;novel method;target;time", "pdf_keywords": ""}, "205ff5dae21ca44c15d3b7d7a9febb7d84b47bc4": {"ta_keywords": "translation systems;new language;resource languages;neural machine;resource model;lrls;lrl;fidelity performance;new method;method;case", "pdf_keywords": "neural machine translation systems;multilingual seed models;multilingual training;translation process;new languages;source language;target language;multilingual nmt systems;more general language pairs;similar languages;best adaptation;different adaptation paradigms;languages;source training;translation;adaptation;language;original language;parallel training;coldstart scenario;coldstart;training;source scenario;natural language processing;rare words;small variations;words;strategies;lrl;universal model"}, "488b1849dd81e63aae2cd327564077ae123c0369": {"ta_keywords": "generalized gradient models;absolute compression;optimal compressors;convex problems;compensation class;compensation;sgl;novel analysis;numerical experiments;class;new analysis;error;theoretical findings", "pdf_keywords": "absolute compression;stochastic gradient descent;stochastic gradients;stochastic optimization;absolute compressors;stochastic gradient descent method;stochastic reformulation;different stochastic estimators;stochastic input;generalized gradient approximation;gradient descent rate;generalized gradient descent;global gradient discretization;estimation;noisy channels;stochastic;output gradients;convergence rates;global discretization;noisy channel;error rate;optimal rate;convex function;convergence rate;maximum entropy;error feedback;generalized inequality;linear response;compensation;telecommunication"}, "4cd92a56dca741190e453b4229eb9851abf6944c": {"ta_keywords": "smooth convex stochastic optimization;accelerated stochastic gradient;new accelerated stochastic;stochastic gradients;convex case;tails assumption;probability complexity;sigma;noise;order method;special variant;method", "pdf_keywords": "stochastic convex optimization;smooth convex stochastic optimization;stochastic convex optimization method;accelerated stochastic gradients;convex stochastic optimization problem;stochastic convex optimization problem;stochastic gradient method;stochastic gradients;stochastic gradient;stochastic optimization;general linear stochastic gradient problems;standard stochastic gradient method;smooth stochastic gradients;new accelerated stochastic;stochastic optimization problems;stochastic gradient model;stochastic optimization problem;stochastic programming;convex optimization;stochastic neural networks;stochastic noise;stochastic stochastic;optimal stochastic solutions;convex optimization iswe;convex objective function;stochastic signalwe;noisy stochastic signal;convex optimization problem;stochastic function;stochasticity"}, "40848b41ed8c9c255ecd8a920006877691b52d03": {"ta_keywords": "wild distribution shifts;machine learning tools;new benchmark;benchmark;datasets;communities;data sets;baseline models;challenges;variety;collection;set;future work;applications;directions", "pdf_keywords": "realistic distribution shift;distribution shifts;shift models;machine learning systems;machine learning applications;rigorous distribution shift benchmark;robust distribution shifts;world distribution shifts;machine learning;specialized training algorithms;different distribution shifts;empirical distribution shift;empirical discovery;deep learning;realistic dataset;machine learning models;classification;synthetic data sets;study distribution shifts;random shift;classifiers;adaptation;deep learning data;deep learning models;unsupervised domain adaptationwe;deep learning pipeline;real data distributions;synthetic data;synthetic benchmarks;label shift adaptation"}, "2e492af839e971d05592df1c76d4878908e1d4c0": {"ta_keywords": "factor graph grammars;hyperedge replacement graph grammars;factor graphs;tractable inference;plate notation;fggs;sets;use;class", "pdf_keywords": "stochastic computation graphsa factor graph grammar;stochastic computation graphs;factor graph grammars;hyperedge replacement graph grammars;factor graphs;random graphs;other random graphs;hypergraph models;factor graph;stable hyperedge replacement graph;graphs;probabilistic networks;dense random networks;factor diagrams;dimensional graph model;dynamic graphical models;dimensional hypergraph;subgraph;hypergraph fragment;product networks;graph;nodes;probabilistic context;combinatorial model;stable graphical model;free grammars;fgs generate sets;derivation trees;patterns;difference graph"}, "c66b59394f99d639b277a54ad357d20de30285bd": {"ta_keywords": "biomedical literature;structured document retrieval;new biomedical literature;text mining;searchable database;database;image processing;new database;researchers;information;goal;combination", "pdf_keywords": ""}, "8fc728b71f9e92f91455f957f10c7e496cbe4772": {"ta_keywords": "language model enhancement;entity typing process;language model;entity;benchmark datasets;art baseline;quality;method;state;experiments", "pdf_keywords": ""}, "208e5c187e81f63024ece8e2003dbaef094703cb": {"ta_keywords": "quantum memory;fidelity quantum memory;photon laser pulse;quantum teleportation;quantum;quantum logic operations;dense coding;pulse;generation;device;application;new method", "pdf_keywords": ""}, "7dadf1e4f6f7a6966d5f691c3707fe221038528b": {"ta_keywords": "tractable fairness concepts;allocations;computational complexity;social welfare;envy;freeness;proportionality;agents;utilities;item;sum;problems;number;prop1;fre1", "pdf_keywords": "fair allocation;allocations maximizingtheorems;efficient allocation;optimal allocations;free allocation;utility allocation;allocationswe;allocations;allocation;tractable fairness concepts;boolean allocation;indivisible items;convex allocation;knapsack instance;knapsack;allocation instance;fairness criterion;knapsack items;nash social welfare;envy;computational complexity;additive utility function;tractable computation;utility;polynomial time algorithm;nash welfare;utilitarian welfare;utilitarian tasks;fairness;utilitarianity"}, "346081161bdc8f18e2a4c4af7f51d35452b5cb01": {"ta_keywords": "annotated reasoning;reasoning strategies;sophisticated multihop reasoning;hop reasoning;benchmark;strategyqa;task;questions;strategies;best baseline;humans;question;wide range;ability", "pdf_keywords": "new strategy question generation method;reasoning strategies;question decompositions;annotated strategy questionswe;natural language inference;reasoningthe search;reasoning process;reasoning questions;reasoning steps;elicit quality strategy questions;machine reading comprehension;novel annotation pipeline;questions;strategy questions;simple reasoning questions;implicit reasoning;evidence annotations;annotators;annotations;annotates;multiple annotations;crowdsourcing data;annotator;rich lexical structure;natural language processing systems;strategyqa;natural language;comprehension;factorization;search corpus"}, "76df9c90d5359585f5501a4da1af1078c32be6d7": {"ta_keywords": "linguistic signature;text;linguistic structure;generative model;document;historical transcription;unsupervised;documents;search;important tasks;model;state;art solutions", "pdf_keywords": "text;generative probabilistic model;historical printing processes;typesetting;generative model;font structure;font;text recognition system;paper digitization program;historical documents;generative process;extensive ink usage;ink;lines;unsupervised fashion;language model;glyph;art baseline systems;novel statistical model;kerning;strong language model;glyph shape;images;documents;layout;transcription;paper;art baselineswe;line;semantic content"}, "88c3f221a6fc8aff014268b0efb5ff119ab40906": {"ta_keywords": "similarity;emojis;tweets;social media;signals;interesting connections;dataset;process;use;new method", "pdf_keywords": ""}, "ce2d6de9cec4a6d135c32bb8d2d02bba09928b33": {"ta_keywords": "text categorization;expert knowledge;algorithmic approach;recent algorithmic approach;accuracy;word;performance;terms;method;utility;efficiency;result", "pdf_keywords": ""}, "393c5c96e73dd3a82c175f9ab1f6c083830d3b82": {"ta_keywords": "market capitalization;book value;portfolio;stocks;structured data analysis;price history;market price;parameter reconstruction;financial output;inversion;reconstruction;return;simple strategy;company;novel method;significant improvement;method;proof", "pdf_keywords": "stock market prediction;future stock price;deep neural networks;neural network;stock market;future;stocks;clairvoyant model;deep learning approach;quantitative prediction;future fundamentals;predictions;future fundamental data;convolutional neural networks;naive predictions;future performance;portfolios;time series analysis;select stocks;china stock market;basic financial information;term memory;time frames;model;linear programming;inputs;network;data;feed;company"}, "e929c9b53c66d52ae5ea56f0dc2764aef4cc67f6": {"ta_keywords": "channel separation;robust separation;channel speech;same input data;input data;signal;intrinsic structure;arbitrary environment;novel method;method;variations;approach;use", "pdf_keywords": ""}, "be28821d510a99ffce40cdcf6860302def8533ef": {"ta_keywords": "facility location games;optimal strategy profile;strategy system;strategy model;strategy strategy model;strategy strategy;social welfare space;strategy;optimal use;model", "pdf_keywords": "social location game theory;social location games;facility location game;facility location games;optimal social cost;strategic content providers;optimal strategy location;strategic players;optimal strategy;location sharing;mediators;recommendation systems;optimal locations;optimal strategywe;optimal strategies;facility location models;optimal strategy profile;mediator;better social welfare;social choice theory;best social cost;social network;games;optimal payoff;players;social welfare;preferred strategy;optimal solutions;lower social cost;game"}, "3d2dece28f566792b6dd3a190aa345fc30fee1ff": {"ta_keywords": "ground transportation networks;anaheim transportation network model;mixed integer linear program;optimal location;global optimal solution;ground vehicles;mathematical program;linearized equilibria;linear constraints;ground nodes;hybrid air;mathematical model;equilibria;capacity;model;results", "pdf_keywords": "hybrid airground transportation network;static traffic equilibria model;static traffic equilibria;ground transportation networks;ground transportation network;anaheim transportation network model;traffic equilibria;air traffic flow;transportation networks;transportation network;traffic flow;traffic congestion;traffic demand;network model;network capacity;anaheim transportation network;level traffic assignment;new network model;mixed integer linear program;node capacity constraints;traffic vertiport;linear programs;linear program;integer linear program;optimal networks;congestion;traffic;bilinear equilibrium constraints;flow conservation constraints;network structure"}, "eadd73c3e1c20d16e32ee8656c4f954603b37450": {"ta_keywords": "clarinetist video;acoustic events;music videos;audio recordings;convolutional neural network;onsets;recognition;onset;salient points;precision;novel method;location", "pdf_keywords": "clarinetist videos;clarinet recording;onset detectors;visual onset detection;acoustic onsets;clarinetists;audiovisual dataset;cnntranscription;sound tracks;string recording;sound processing;convolutional neural model;convolutional neural networks;convolutional neural network;music videos;onset location;audio;temporal pooling;neural networks;music;onsets;string ensembles;onset;convolutional learning bias term;transcription process;same music training;multiple streams;strings;sound;3d"}, "a4e937f0b6e0688f7f3c4fcaebbabefa4a36da85": {"ta_keywords": "inference;training;unified framework", "pdf_keywords": "extended speech processing models;tegrable speech synthesis toolkit;tegrable speech model;quality speech output;scalable speech synthesis toolkit;speaker models;speech processing tasks;scale speech corpus;generative models;generative adversarial networks;gans;new speaker alignment model;fidelity speech;adversarial network;deep mixture density network;speech;deep mixture density networks;joint t2w models;speaker similarity;waveform models;noisy utterance;waveform;text prediction;corpus;joint textto;espnet2;t2w;network;models;new model"}, "2135c44087e06a6d95d04ad0afa400e926d37944": {"ta_keywords": "feature space normalization;feature spaces;spontaneous spontaneous spontaneous parity breaking;posteriori linear regression;normalization;model parameters;posteriori;estimation;maximum;adaptation;context;structural pes;ssbb;pes;problem;approach", "pdf_keywords": ""}, "99c80d608ba2aa638333f27bbe3f09cdc580a051": {"ta_keywords": "python library;shelf;version", "pdf_keywords": "torch audio toolkit;torch audio library;torch audio;large speech datasets;speech models library;audio;speech representation learning;speech models andwe;pytest8 framework;speech domain;pytest8we;root sound representation;large corpus;machine learning pipeline;audio signal;wavenet predictions;toolkit;tts representations;machine learning applications;available implementations;torch;neural network building blocks;accurate representations;several canonical machine learning models;available representations;datasets;benchmarks;generative networks;representations;implementations"}, "29bc6654abd34b2405f7a01341f790aed2aab9a4": {"ta_keywords": "storage codes;explicit codes;codes;repair;mds;data;design framework;constraints;maximum distance;small number;important settings;framework;smallest amount;solutions;rate;power;amount", "pdf_keywords": ""}, "b35ad59ce9a3ea01a0980c90bc750273d1f99e7a": {"ta_keywords": "heterogeneous databases;database integration;global domain;local names;similarity;space model;logic;vector", "pdf_keywords": ""}, "04e0fb8b3bb06e1200288e6d2a17d55773e97504": {"ta_keywords": "dependent opportunistic bandwidth sharing;optimal policy;cellular network;long run average reward process;timescale stochastic approximation;mobile users;step reward;instantaneous data volumes;algorithm;asymptotic convergence;location;linear combination;problem", "pdf_keywords": "fair bandwidth sharing;dependent opportunistic bandwidth sharing;optimal bandwidth allocation;cellular network;cellular networks;dynamic bandwidth allocation scheme;dynamic bandwidth sharing model;bandwidth allocation strategy;optimal bandwidth sharing;bandwidth allocation;dynamic bandwidth sharing paradigm;bandwidth sharing policies;novel bandwidth allocation strategy;mobile network;bandwidth sharing;wireless cellular network;stochastic learning algorithm;optimal policy;bandwidth sharing problem;stochastic fading process;available bandwidth;bandwidth;optimal throughput;mobile users;timescale stochastic approximations;cellular systems;timescale stochastic approximation;stochastic;stochastic decision process;stochastic network"}, "016d83091a60a6de67ba2395c063967686043380": {"ta_keywords": "variational probabilistic estimation;acoustic model adaptation;speech recognition;posterior value;model;parameter;novel method;method;problem", "pdf_keywords": ""}, "06708348b64e2e7b11a953389556c701bf3298da": {"ta_keywords": "estimation;random variables;large number;small number;number;problem", "pdf_keywords": ""}, "0098123efc851b67137c1028f7bac8d8bffbc8fd": {"ta_keywords": "language models;plms;signal;grounding;domain;presence;connection;novel approach;approach", "pdf_keywords": "downstream semantic parsing models;semantic parsing;machine reading comprehension;language models;knowledge extraction tasks;concept representation;interpretable machine learning;knowledge representation;language model;grounding models;groundingin;empirical grounding;latent grounding;grounding;linguistic function;ground;grounding capabilities ofplms;large corpus;concepts;novel weakly supervised approach;knowledge;concept;sql parsers;language;strong learning rate;quantitative learning framework;representations;token;granular models;models"}, "45f59bd3ef8e1d76474199c08c140675c04a728c": {"ta_keywords": "novel heterogeneous multiagent learning rules;heterogeneous multiagent learning;learning processes;agents;variational perspective;reactions;synthesis;conjectures;rules;framework", "pdf_keywords": ""}, "a13c580250af3644fe368b08a540f4ea65dac919": {"ta_keywords": "general compressive phase retrieval problem;fisher information;noisy intensity measurements;stochastic measurement scheme;stochastic measurement technique;efficient measurement;measurement matrix;noisy signal;unconstrained signals;complex signal;noise;key resource;row;novel method;form;problem;scheme;presence;use", "pdf_keywords": ""}, "0b2e9e978898b9fb1116ea964c8c470086ceed87": {"ta_keywords": "salient objects;informative depth cues;detection;spatial views;independent feature aggregation strategy;backbone;depth;gradient;backbone strategy;feature module;channel;field;agn;dem;bbs;presence;sde", "pdf_keywords": "salient object detection;saliency detection;novel saliency learning framework;saliency mappings;traditional multiscale saliency;saliency model;saliency fusion;salient object detection mechanism;saliency;saliency information;saliency map;novel saliency;final saliency map;salient features;informative depth cues;salient region representation;salient objects;depth contrast;salient object;depth features;initial saliency map;deep model;depth image;depth adapter module;rgb modalities;spatial attention;deep models;depth map;quality depth map;novel deep learning strategy"}, "0b8dbc4a899c836fe2b1a213b9dc064cdf62fd63": {"ta_keywords": "recent process comprehension benchmark;explanations;end task performance;explanation accuracy;good explanations;procedural text;end task;sentences;several strong baselines;benchmark;best baseline;surprising finding;points;effects;system;expense;perturbation;novel approach;fact", "pdf_keywords": "multihop reading comprehension;natural language inference task;natural language inference;novel procedural text explanation;natural language models;recent process comprehension benchmark;unstructured explanations;natural language processing;natural language;comprehension;procedural text;multitask learning problem;sensible explanation structure;linguistic structure;sentences;better understanding;explanations;text;commonsense reasoning;end task performance;context;tasks;good explanations;machine learning context;individual classification tasks;passage sentence;comprehensive understanding;passage;end task;traditional reasoning"}, "f07c5c540233b22f0ca154c80c713e2aed3c9606": {"ta_keywords": "music generation;gans;gan;discriminative metric;bert model;discriminator;optimization;likelihood;novel method;observed sequences;mesh;negative log;purpose;novel;use;experiments", "pdf_keywords": ""}, "d15a7d00897f58a94def2a58c0cb0311851f2968": {"ta_keywords": "automatic speech recognition;bidirectional long short term memory;microphones;mask estimation;neural network;asr;new baseline system;eigenvalue;time delay;free version;data;maximum mutual information;lds;lattice;system;tnn;augmented;paper", "pdf_keywords": "speech recognition;noisy automatic speech recognition;speech recognition framework;speech recognition system;deep learning;automatic speech recognition;deep learning approach;speech enhancement;deep learning networks;deep learning problem;speech processing communities;bidirectional long short short term memory;speech signal;speechwe;deep learning experiments;neural network;speech;nonlinear neural networks;recognition challenge;nonlinear neural network;data augmentation method;mask estimation;dnn;beamformer;rnwe;training;dnns;input signal;noise ratio;channel tracks"}, "04b44c518b145be625ff270af56cfd2e37900137": {"ta_keywords": "channel recordings;microphones;neural diarization;encoders;llerc encoders;aware realization;conventional eend;model adaptation method;spatio;co;novel method;end;method outperforms;types;method", "pdf_keywords": "speaker activity;speech recognition;speaker diarization system;speech processing;novel speaker diarization system;channel recordings;microphones;multichannel inputs;microphone settings;multichannel input;attention encoder;attention encoders;speaker interactions;channel attention;channel inputs;optimal speaker;speech problems;sensor encoders;channels;temporal encoder;neural diarization;encoders;encoder;speakers;attention weights;channel;neural diarization method;meeting transcription;attention;utterance"}, "a56dba9cabfc110df231051d7c9d6e439f6757dd": {"ta_keywords": "trainable pointwise ssp taggers;tagging;active learning;ssp tags;domains;predictors;domain;ssp;sequence;ssp transition tendencies;pointwise;paper;method;part;assumption;experiments", "pdf_keywords": ""}, "2ab9fd2be2bf82e0bbd558cc64c1c46728fc4f8a": {"ta_keywords": "speech characteristics;incremental adaptation;acoustic model;language model;macroscopic time scale;adaptation;time scale;conversation;time evolution system;posterior distributions;changes;model;robustness;novel method;method", "pdf_keywords": ""}, "f17e182fcb7fbbff2257824174ed6f7df512a42b": {"ta_keywords": "end speech recognition problem;relative word error rate;multidimensional end;models;model;relative reduction;cd;wer;approach", "pdf_keywords": "automatic speech recognition;encoder;novel encoder;encoders;like encoder;decoder;multidimensional pattern recognition;recognition;language model;pattern recognition paradigm;convolutional neural network;joint attention;multitask learning;dimensional multidimensional representations;segmentation model;speech;relative word error rate;attention;chime model;text;cnn;wavelength;joint network;downstream filtering;convolution layer;end system;signal;network architecture;reference network;amplitude filtering"}, "72b4ff7387223cf0398c298c3cc62ee07d9c0043": {"ta_keywords": "gram language models;simple language models;syntactic dependency tree;lexicalisation;sentence;task;probability;project;percentage points;highest accuracy;approach;new approach;date", "pdf_keywords": ""}, "57fec656119e82b5e70b1a654f6d87d8c1137ef4": {"ta_keywords": "new structured probabilistic topic model;biological figures;realistic figure generation scheme;information retrieval;efficient inference algorithm;life science systems;data mining tasks;visualization;gibbs;key infrared imaging;ir;program;representative application;engines;application;method;number", "pdf_keywords": ""}, "2797a36cd15b8c046683247995261546993c289d": {"ta_keywords": "speech recognition;temporal speech;noise modeling;dynamic variance adaptation;keyword recognition accuracy;audible quality;speech;presence;system", "pdf_keywords": ""}, "99bb811beb5d061d2b8fac5a1973b49cace93e2f": {"ta_keywords": "current purchase logs;real purchase logs;consumer purchase behavior;trends;interests;time;changes;new model;model;effectiveness;method", "pdf_keywords": ""}, "291b651654565cd88e4e56de5250219a71882a50": {"ta_keywords": "peer selection problem;weighted peer;robust selection;review system;selection;algorithm;nodes;agents;weighting scheme;new algorithm;iterative feedback;noisy assessments;accuracy;subset;presence;several examples", "pdf_keywords": "weighted peer nomination;novel strategyproof peer selection algorithm;peer selection algorithm;peer selection;peer nomination;underlying peer selection model;accurate peer;peer review paper;peer;reviewers;strategyproof assignment algorithm;individual ratings;reliability weights;selection algorithm;weighting scheme;simple weighting scheme;accurate evaluations;crowdsourcing;ratings;reviewee;popular algorithm;available reviews;optimal assignment;selection;empirical evaluation process;evaluations;reweighting methods andwe;inaccurate agents;accurate agents;algorithm"}, "9cf4609178e2739ed35f8d3e3d6efb7d5e2e1a41": {"ta_keywords": "traffic dynamics;traffic traffic;unstable eigenvalues;unstable behavior;anomaly feature;learned dynamics;dynamics;high queue lengths;detection;long sequences;method searches;accident;approach;method;case study day;feasibility;use;paper", "pdf_keywords": ""}, "74495e9735b601ce9060ac40ac27d196fdbf7462": {"ta_keywords": "storage;code construction;codes;large repair;flexible class;certain parameter regimes;regime", "pdf_keywords": ""}, "f75ba81828fd9d8c7fcba89dd98a0ee73d32dce6": {"ta_keywords": "magnetic properties;magnetic field;field;strength;effect;results;study", "pdf_keywords": ""}, "58fd3001c88e9784b0794eef06cb7c0eab0d8747": {"ta_keywords": "system representation;simulation;systems;ontology;synthesis;images;system;particles;simple model;model;novel approach;approach;simple", "pdf_keywords": ""}, "c9472731afe5fca98f362f49e26d17a9d5d0cc8e": {"ta_keywords": "black box problem;black box user;interpretability;algorithms;algorithm;existence;counterexample;machine;importance;widespread agreement;paper;user;ways", "pdf_keywords": "interpretability;machine learning suffers;machine learning algorithms;explanationthe definition;machine learning;causal explanations;algorithms;black box problem;constructive challenge;interpretation;explicability;inception;epistemological underpinnings;concept;features;belief formation;subtle problem;predictions;epistemological principle;unclarity;analytic epistemologists;mechanistic models;explanation;human brain;algorithm;attention;phenomena;notions;machine;belief"}, "b103bb1dc05a48796a3ff0804c11909bf68db11b": {"ta_keywords": "sentence instabilities;syntactic features;biomedical text;supervised manner;detection;cues;tasks;score;context;novel approach;performance;approach", "pdf_keywords": ""}, "83ac0851a8f6fa02f5db251b260f635907d7a01e": {"ta_keywords": "fast navigation;trajectory;unobserved environment;object;reference object;good approximation;novel approach;concept;approach;combined description", "pdf_keywords": "autonomous navigation;search trajectory;navigation task;visual navigation;ground truth trajectories;partial trajectories;navigation;best candidate trajectory;complete trajectory;planning;trajectory;optimal action;search space;natural language exploration strategy;greedy decoder;partial trajectory;unstructured world;visual environments;reinforcement learning;fast reinforcement learning agents;asynchronous beam search;movement;next node;action;unstructured landscape;usual greedy decodingwe;vector network;seq2seq;neural network representation;modern vision"}, "b02acb3d159b06b2319a164378e1e61c3983676f": {"ta_keywords": "complex queries;complex query models;combinatorial generalizability;benchmark;new benchmark;different operators;operators;normal forms;intersection;new dataset;forms;evaluation;projection;set;detailed study;impact", "pdf_keywords": "complexity query answering models;complex query answering models;benchmark query types;benchmark query typeswe;knowledge graphs;knowledge graph;generalizable benchmark;qa benchmark;quantum network;query types;qcd;combinatorial generalizabilitythis paper;queries;combinatorial generalizability;logical queries;query performance;query operator;qwe;new benchmark;benchmark results;comprehensive benchmark;computation models;novel benchmark;quadratures;models;entities;matrix models;benchmark;models thatwe;search"}, "eaa224ae5c969180503dda4972ab86d3a71c888c": {"ta_keywords": "polyphonic music recognition;optical music recognition;polyphonic music recognition problem;polyphonic file classification architecture;sheet music;image encoder architecture;large scale datasets;musescore forum;end;novel approach;novel formulations;previous work;approach;problem", "pdf_keywords": "polyphonic optical music recognition;polyphonic music recognition;optical music recognition;optical music;polyphonic music;resonance labeling;music systems;music discovery;simple representationpolyphonic music;encoder;vertical rnn decoder;available musescore sheet music;optical resonance;neural networks;sheet music;music;decoder;automatic parametric pattern recognition;symbol classifier;single symbol prediction;hashizer;end omr;datasets;symbol predictions;flagdecoder;musescore forum;segmentation;automatic data analysis;unstructured data;data sets"}, "baad427b45ac691763fe3de4ea3ac1bffd3c74e3": {"ta_keywords": "available crowd data;new visualization tool;geographic party affiliation;social events;geographic affiliation;different demographic backgrounds;affiliation;crowd;people;view;changes;person;time;users;other people;tool", "pdf_keywords": ""}, "64280761641d8f1eb285165160bd96efac0bb5f5": {"ta_keywords": "simultaneous recognition;deep neural network;environmental sounds;speech;dnn;respective noisy conditions;tasks;architectures;accuracy;techniques;problem;paper;study;possibility", "pdf_keywords": ""}, "2391e7446d47f681ad705c8e75d9d2ce1b92ad5f": {"ta_keywords": "rem project;israel science foundation;knowledge;reference corpus;literature;project;dm;resolution;evaluation;slacton collaboration;effort;work", "pdf_keywords": ""}, "ff7e60b8d336aef5ed974609a63610641085177e": {"ta_keywords": "random matrix model;image captioning;multiclass classification;distribution estimation;maximum likelihood;synthetic data;real data;distribution;temperature;approximation error;experiments;relation", "pdf_keywords": "softmax distribution;softmax approximation;softmax;reinforcement learning;effective reinforcement learning framework;specific reward;learning strategy;bayes decision boundary;rewards;learning process;maximum likelihood;novel probabilistic prediction function;bayes payoff;bayes decision rule;prediction function;model estimation process;gradient descent;deep learning model;machine learning;payoff distribution;exponentiated payoff distribution;distribution estimation;probabilistic prediction;distribution estimation framework;model prediction;neural models;novel empirical discovery;importance sampling;novel structured prediction model;conditional cost function"}, "4d991a83d6044b1aaed2c117b3d097ecd23cf6f4": {"ta_keywords": "end speaker diarization problem;speaker diarization;speaker diarization problem;neural network;diarization errors;neural model;attention;permutation;end;model;self;free objective function;novel approach;key", "pdf_keywords": "novel speaker diarization method;speaker diarization;speaker recognition;speaker diarization problem;axis speaker diarization;speaker embeddings;speaker segments;global speaker characteristics;speaker separation;speaker labels;speaker model;speaker assignments;speech mixtures;speech networks;audio mixtures;speaker positions;art xvector clustering;conventional speech recognition systems;speaker changes;clustering;speaker;acoustic processing;speaker overlaps;audio recording;new speaker;reference diarization results;reverberant speech;input features;random utterances;diarization error rate"}, "9f2cf7b35224aad3a8d261e4456fe2d65a5f5d3e": {"ta_keywords": "large dual encoders;domain retrieval tasks;dual encoders;dual encoder model;retrieval tasks;dense retrievers;npnppi database;sparse;performance;novel approach;variety;size", "pdf_keywords": "generalizable dense retrieval models;dense retrieval models;new dense retrieval model;popular neural retrieval models;retrieval models;vector retrieval model;dense retrieval problem;large dual encoders;retrieval tasks;dual encoder training approach;retrieval performance;high quality search corpus;information retrieval;dual encoders;dual encoder model;dual encoder;neural passage retrieval model;search datasets;product encoding model;dense embeddings;information retrieval iswe;search;mined corpus;generalizable t5 retriever;generic queries;high dimensionality;domain generalization;queries;passage retrieval process;unstructured entity"}, "d18d8d364bb18d66924919feebb2e892ebe6761c": {"ta_keywords": "neural machine translation;traditional statistical machine translation;translation quality;phrase;nmt;nmt output;different language;nmt outputs;cost;model;advantages;method", "pdf_keywords": "traditional spatial neural machine translation;neural machine translation;neural machine translation algorithm;traditional statistical machine translation;thatneural machine translation;several translation tasks;best translation output;aneural machine translation;translation model;translation system;translation accuracy;translation lexicon;attention;incorrect translation rules;phrase memory;translation length;decoding;translation;natural language;standard phrase;french tasks;unaligned words;correct translation;national electronic text processing network;sentences;target words;phrase;target word probability;new phrase;alignment"}, "1b96b89d5b3ba444126cebefdfc665d3866f14f0": {"ta_keywords": "online hiring processes;fair outcomes;modern hiring pipeline;fair treatment;algorithms;processes;challenges;definitions;set", "pdf_keywords": ""}, "c888022dec626171d243d2a056709b9b053a0ed9": {"ta_keywords": "end speech recognition system;speech recognition system;noisy speech benchmarks;speech enhancement;acoustic signal processing;joint encoder;encoder;decoder training;noise suppression;decoder framework;multichannel end;language components;architecture;novel architecture;conventional approach;possibility", "pdf_keywords": "end speech recognition system;end speech encoder;end speech recognition framework;end speech recognition objective;novel multichannel speech recognition architecture;end speech recognition problem;multichannel speech recognition;end speech recognition frameworkwe;speech enhancement;standard speech recognition system;neural beamformer;microphone array signal processing;speech recognition;acoustic encoding network;noisy speech benchmarks;speech recognition performance;speech mask;reference microphone;informative reference microphone;filterneural beamformers;noise suppression;noise mask;mask estimation network approach;filter estimation network;beamforming;encoder;novel mask estimation framework;decoder networks;deep neural network;deep learning"}, "2526c510610c7220ecc56e6b08d09c4cbaf58c3c": {"ta_keywords": "anaphora resolution;regular expressions;japanese language;recognition;human language;proper words;phrases;correct subjects;people;approach;method;context;problem", "pdf_keywords": ""}, "2a462e2b748d7e78f3af2621071265c1ad2683ea": {"ta_keywords": "optimal power flow problem;dispatchable wind energy;power flow efficiency;power system;iterative solution;efficiency;method;evaluation;problem;new method", "pdf_keywords": ""}, "b61c9799f10e4de9cd222dfd8e423bbd950a7c44": {"ta_keywords": "information content;corpus;text;simple event;information;extraction;uncoated text;questions;sci;proc;paris;comparative study;france;lond;acad;l127", "pdf_keywords": "knowledge extraction process;natural language data;answerable questions;extraction;natural language;un answer;natural language input;knowledge;content;questions;documents;entity;tasks;information;bert representation;task;large collection;context;text;new dataset;pnas;answer;monolinear machine learning;reading;mll;un;idk;particular type;ncl;proc"}, "4ebe5792fe2890590e7a5bf8ae0a29e0fb147ef9": {"ta_keywords": "semantic segmentation task;incomplete rewriting;several public datasets;global information;novel approach;state;art performance;approach;problem", "pdf_keywords": "utterances;style utterances;unconstrained utterances;incomplete utterance;edit operations;large corpus;semantic segmentation task;utterance;best sequence generation;level edit matrix;recurrent computation;natural language;semantic segmentation;deep interaction;faster inference speed;deep neural network;neural network context;convolutional neural network;conversation;contextual information;node dialogue;deep learning model;neural network;evaluation datasets;implicit context information;word relevance;domain conversation;novel kernel;word;computing"}, "e6fe601c44835d3654131d0312d65227d3523373": {"ta_keywords": "underlying syntactic relations;text;graph structure;extraction;supervised learning;coordinate term;random walks;coordinate;essential features;framework;state;art methods", "pdf_keywords": ""}, "5a4d4c0824b5e113c39105c71d42b93d3900d87e": {"ta_keywords": "entire crowdsourcing process;crowdsourcing;open source microtask engine;microtask;answer aggregation;task allocation;worker ranking;engine;architecture;quality control;implementation details;design;other means;such elements;paper", "pdf_keywords": ""}, "e2b6193a24cd6c9f736139aa66618d1b8bf2a60b": {"ta_keywords": "automatic manual transcription correction;transcription;speech transcripts;transcriber;highest error reduction;segmentation;constrained optimization strategy;segments;possible segmentations;tool;correction;errors;new tool;selection;time budget;traditional approaches;context", "pdf_keywords": ""}, "fc848789b557a7581c51c79fd01897dc5aa7e8a8": {"ta_keywords": "multimodal interoperability;multimodal knowledge resource;smart transformers;multimodal task;transformers;generative problem;adaptive models;model;novel model;model parameters;domain;class;set", "pdf_keywords": "visual language prediction;novel reasoning module;visual semantic matching;explicit knowledge extraction;multimodal task;explicit knowledge bases;retriever answer generation;explicit knowledge sources;external knowledge base;explicit knowledge;knowledge extraction;spontaneous spontaneous knowledge extraction;art answer generation;answer generation;knowledge base;joint knowledge;multiple structured knowledge bases;implicit knowledge;external knowledge;knowledge;language models;knowledge sources;language model;contrastive learning;vqa task;multimodal transformers;important knowledge sources;encoder;natural language processing;agnostic visiolinguistic representations"}, "db729f2f55a92465cf88682ba7917621fd4c000b": {"ta_keywords": "learning tasks;students;student;constrained self;linear algebraic formula;strategy;performance;lemma;self;proofs;time;effect", "pdf_keywords": ""}, "b2da0f022a48ebd10a23572b5310b7d7341b6448": {"ta_keywords": "orbit interaction;spin;orbit;coupled system;dynamics;enhancement;effect;suppression", "pdf_keywords": ""}, "004ddf5a39a735d0f8ec7547629c2bee65eb1f93": {"ta_keywords": "peer review;biases;testing;experiment;bidding;recent experiment;false alarm probability;results;blind manner;authors;sets;existence;setup;general framework;framework", "pdf_keywords": "bias testing;bias testing algorithms;peer review;bias testing method;independent reviewers;bias testing problem;peer review process;biases;significant bias;reviewers;possible biases;reviewers assignments;bias;statistical tests;arbitrary testing procedures;such biases;tests;testing;db reviewers;disagreement test;bias estimation;scholarly research;novel statistical experiment;conference experiment;reviewer;relative bias;ribbon reviewers;alternative hypotheses;reviewerswe;testing procedure"}, "1bd7d16340642948142d7608ef8f085d934d94a3": {"ta_keywords": "importance sampling;convex functions;unconstrained optimization;convex;point method;heavy ball momentum;standard stochastic;stochastic;spt;method;ssm;context;case", "pdf_keywords": "stochastic optimization;stochastic convexity optimization;stochastic convex optimization;stochastic momentum methods;stochastic coordinate descent optimization;stochastic search method;stochastic coordinate descent method;convexity optimization;stochastic coordinate descent;unconstrained minimization;convex search algorithm;standard convex optimization method;smooth objective function;momentum stochastic;free optimization algorithmsin;order momentum method;original optimization method;continuous control tasks;free random search method;stochastic;optimization problems;continuous control;iterations;convex functions;shape optimization;forward iterates;point method;convex distribution;optimal way;iterate"}, "db8376698c06d6a688a39bff0300780ef0383821": {"ta_keywords": "simultaneous control;local control unit;boards;board;mechanical properties;manufacture;systems;production;structure;method;face;new method;problem;use;new class;necessary input;variety", "pdf_keywords": ""}, "4b1555368fd2c5f1234eaac5e41296003481754a": {"ta_keywords": "deep eutectic electrolyte;oxygen battery;lithium;methylacetamide;dee;term operation;requirements;certain ratio", "pdf_keywords": ""}, "005879e6587eb6e05f56c20d345f784ee84a44c4": {"ta_keywords": "nonsyntactic language models;recurrent neural nets;dependency syntax;new generative models;different languages;models;trees;arabic;japanese;english;order;explicit independence assumptions;performance;significant improvement", "pdf_keywords": ""}, "f61862b286c9e4894302faf716eedb0eb60a2f5f": {"ta_keywords": "discussion forums;thread structure;discussions;messages;graph;temporal relationships;simple thresholdcut baseline;connections;simple baseline approach;variations;approaches;approach;results;novel approach;paper;representation;different ways", "pdf_keywords": ""}, "20819855b9517c927a1262850146e525c8083fb4": {"ta_keywords": "long short term memory;spoken language understanding;human dialog data;hotel reservation task;word sequences;utterance;rnns;neural networks;agent intentions;concept tags;lsd;sequence;human;tags;context;model;novel model;client", "pdf_keywords": ""}, "f74d4fa99ccedfea7a2662fe6944d99f34533912": {"ta_keywords": "fairness constraints;adaptive classification thresholds;fairness;machine learning;classification model;adaptive adaptation;demographic group;classification model output;optimization algorithm;novel algorithm;algorithm;accuracy;group;probability distribution;rigorous theoretical analysis;method;post;paper;convergence", "pdf_keywords": "optimal fairness model;fair classification problem;new fairness estimation method;fairness metrics;fairness regularization;fairness metric;multiple fairness constraints;optimal classification threshold;group fairness;fair representation learning constraint;classification threshold;adaptive classification thresholds;fairness notions;generalised fair representations;discrimination;fair representations;fairnessaccuracy;fairness;fairness trade;aware thresholds;adaptive thresholds;aware threshold adaptation method;aware threshold adaptation;aware classification;aware classification model;large classifiers;classification;classifier;bias sub;different sensitive groups"}, "25fec1e150a273b3bec3655ace0ff6b97c338a96": {"ta_keywords": "storage systems;storage;malicious adversaries;capacity;codes;bandwidth requirements;errors;class;respect;presence", "pdf_keywords": "resilient regenerating codes;resilient minimum bandwidth codes;resilient codes;arbitrary block errors;storage networks;erasure capacity;resilient productmatrix codes;storage systems;storage;erasures;efficient repair;malicious adversaries;blocks;such codes;modern communication systems;code;resiliency;coding;capacity;codes;bandwidth requirements;block;tight outer bounds;repair;errors;network;nodes;reconstruction;such systems;networks"}, "2875d6cac905c3dfec4c8a8e1d89a2a7e4d71d40": {"ta_keywords": "replication;wireless storage;information flow graphs;nodes;wireless medium;networks;storage;storage capacity;repair;node;broadcast nature;partial repair;cut;trade;problem;contents;set", "pdf_keywords": "storage nodes;optimal repair node;minimum repair bandwidth;clustered storage networks;storage networks;node repair data;single node repair;storage node;node repair;storage code;full node repair;storage network;repair bandwidths;optimal repair scheme;reliable storage;repair bandwidth;linear storage code;optimal storage points;nodesthe repair;broadcast repair model;minimum repair;multiple node codes;minimum storage capacity point;optimal repair;storage systems;minimum storage capacity;node failures;storage;network storage iswe;storage system"}, "190865e2c3d4908ff20bf9a31f5a2773d6fec5cb": {"ta_keywords": "question retrieval process;questions;translation;domain questions;large collection;featureless;data;latent step;new system;key feature;system;relevant information;model;problem;joint training;novel", "pdf_keywords": "question retrieval task;quantum information system;underlying question generation model;large iq datasets;retrieval scores;natural answer datasets;efficient iq approach;qed;iq;qa;retrieval datasets;structured querypassage corpus;iq approaches;questions;inference speed;exploratory search;memory;encoder;associated memory;iq approach;read models;natural language systems;massive memory;reader;knowledge;decoder model;task;simple encoder;high level;underlying knowledge"}, "4dc8ddef938699d0d8a0b685ad9f56d0b735a25d": {"ta_keywords": "concept learner;incremental abductive explanation;logical object;new algorithm;algorithm;object;model;performance;context;article;assumption;mistake", "pdf_keywords": ""}, "5677d2b565c8265fef1693a9be861739cb01bf2f": {"ta_keywords": "large vocabulary speech recognition systems;performance optimization;optimization;underlying hidden model;meta;parameters;simulation;underlying model;design;parallel;novel approach;evolution;strategy;experiments", "pdf_keywords": ""}, "3d2da57c2de69b02fa0fee5c12ace618718a3926": {"ta_keywords": "cells;biological system;graphical model;subfigures;graphs;labels;model;text;features;behavior;set;scheme;experimental results", "pdf_keywords": ""}, "d21a0e01514732f241b9c138eceb76ecaef17a27": {"ta_keywords": "better translation models;machine translation;active learning;human translators;short phrases;annotators;full sentences;unlabeled data;informative examples;statistical models;units;less effort;previous work;mt;framework;paper;pool", "pdf_keywords": ""}, "b4bc1a98eb79545f8da4385a6dfb643b0c62a07e": {"ta_keywords": "discrete syntax;simultaneous translation;syntax;discrete media;target sentence;methods;underlying underlying machine;predictions;method;constituents", "pdf_keywords": ""}, "425a4a9c0598e4101ca2f2b930f5c6986ce40a99": {"ta_keywords": "language models;search algorithms;privacy;memorization;joint optimization;utility;novel triplet;loss term;methods;accuracy;use;superior performance", "pdf_keywords": "privacy regularization methods;adversarial regularizer;adversarial training regularization scheme;optimal privacy;differential privacy;novel adversarial training technique;regularization;novel regularizations;privacy;private model training;regularizations;adversarial approach;users privacy;private data;neural language models;regularizer;regularizers;deep learning;link prediction;neural models;discriminator;sparse;linguistic representations;machine learning models;user patterns;networks;joint optimization;hidden dimension;neural network;utility"}, "4383e714f4535777ffb7b4f618d4ccede4b08bd3": {"ta_keywords": "allophone reconstruction;allophone model;allophones;transcription task;phonemes;transcriptes;languages;allo vera;speech;language;model;specific models;new resource", "pdf_keywords": "phonetic mappings;allophone representation;allophone speech recognition system;specific phoneme correspondences;broad phonetic representations;allodomain phoneme model;phonemic representations;standard multilingual speech recognition models;standard speech languages;diverse phonologies;multilingual speech recognition models;new multilingual speech recognition model;phonemic models;allophone model;speech data;language phones;multilingual speech recognition;allophones;acoustic phonological inventories;allophone;phone recognition task;allophonic maps;phonemes;allophonic resource;speech recognition systems;phoneme layer;untranscribed speech corpus;large scale utterances;language variety;allophonic rules"}, "c549b3f2d262efc1f68dfdd842174634f37519ed": {"ta_keywords": "novel incremental adaptation framework;incremental adaptation;incremental adaptation approach;incremental adaptation framework models;conventional incremental adaptation approach;speech recognition;acoustic model parameters;adaptive behavior;adaptation;macroscopic time scale;delay;posterior distributions;predictor;performance", "pdf_keywords": ""}, "40f4d7fe800810288a80f84cdb357a8f4c28e880": {"ta_keywords": "vision transformers;vision transformer;transformer architecture;spatial dimension conversion;transformer;spatial dimension reduction;original transformer architecture;cnns;convolutional neural networks;novel pooling;image;generalization performance;improved model capability;role;several tasks;novel pit;pit;effectiveness", "pdf_keywords": "convolutional architecture;vision transformer;convolutional network;convolutional neural network;convolutional neural networks;transformer architecture;spatial dimension conversion;convolutional networks;imagenet;spatial dimensional transformation;cnns;computer vision modeling;imagenet network;computer vision;dimension conversion;transformer;spatial dimension;available deep neural networks;convolution;neural network;cnnswe;spatial size;vision domain;convolutional generative models;object detection;transformer language model;neural networks;dimension reduction principle;transformers;transformation"}, "8a73eed98873d91086201f41c6e1f613fcdefe18": {"ta_keywords": "synthesized speech;attention context;language model;asr direction;hyperparameter;self;training;novel self;model;performance gap;advantages", "pdf_keywords": "unlabeled speech data;automatic speech recognition;adaptive adaptive mean field;end learning algorithm;neural networksin;machine learning;training;regularization term;unsupervision;speech;convolutional neural networks;signal processing;new models;utterance;speaker;level models;af model;model;accuracy;specaugment;text;new model;predictions;class;signal;fields;smoother way;snr;field approach;robustness ofwe"}, "42be8ed9973b3326a6e3d838c4742bc1d7704704": {"ta_keywords": "novel speech modification method;input speech signal;statistical feature mapping technique;articulatory parameters;unmodified articulatory movements;phonemic sounds;mixture model;manipulation method;inversion;production mapping;system;paper", "pdf_keywords": ""}, "7c0ada3511b05897fb4d75c5f657b5fbd953caa8": {"ta_keywords": "reputation bias;online voting;social influence;online platforms;biases;position bias;voting;majority;instrumental variable;impression;content;least squares method;validity;effects;impact;instruments;position;set", "pdf_keywords": "voter bias quantification;voter biases;voter bias;votingvoter biases;different voter biases;social influence bias;problemquantifying social influence bias;influence bias;reputation bias;social biases;onsocial influence biases;online voting;voting behavior;popular online platforms;networked social influence;bias;social influence;biases;sitesonline voting;anonymous voting process;social media platforms;aggregate vote;networked voting system;networked social influencewe;individual vote;cognitive biases;online platforms;many online platforms;cognitive bias;voting"}, "e79be3f9ce409f1a9b7084ef880298665e5212d0": {"ta_keywords": "novel contrastive learning algorithm;contrastive learning;multimodal representation learning;aware cascade learning;contrastive loss;multimodal fusion layers;text alignment;efficient loss estimation;cascade;text;syntactic classes;video;taco;hard negative examples;words;account;small set;method;approach;paper", "pdf_keywords": "current contrastive learning pipelines;new contrastive learning pipeline;aware contrastive loss;mpi contrastive learning pipeline;effective contrastive learning method;contrastive learning;effective contrastive loss;multimodal fusion;optimal contrastive loss;contrastive learning method;contrastive loss;attention layers;videotext alignment scores;conventional contrastive learning method;contrastive learning technique;multimodal fusion layers;attention layer;global videotext alignment;contrastive losses;good video representations;visual representation learning;good video representation;cascade learning strategy;video captioning;video representations;deep learning;structured meaning representations;scale video recognition;better alignment performance;video retrieval"}, "e40a5c25d39d0f9add6a26c82613cf29edbcccf5": {"ta_keywords": "empirical validation;erp;event;empirical data;potential;time;user;novel approach", "pdf_keywords": ""}, "92bd9e8a83e82dbbcafd8cde4f5a42d7bb4a5859": {"ta_keywords": "statistical machine translation;individuality;smt;concept;power;different ways;method;paper;technique", "pdf_keywords": ""}, "d1d23675d2e65cd734f2955c10ec1028b1139b5b": {"ta_keywords": "neural machine translation training system;neural machine translation systems;semantic similarity;better translations;new evaluation metric;disparate languages;human evaluation;metric results;english;sme;training;optimization procedure;analysis;recent work", "pdf_keywords": "neural machine translation model;neural machine translation;new machine translation model;machine translation;sentence similarity prediction;empirical sentence similarity;better translation accuracy;optimal translation accuracy;statistical machine translation;standard machine translation model;different translation models;semantic similarity;sentence similarity;machine translation output;translation model;translations;better translations;novel evaluation metric;similarity score;target language;sequence level training algorithm;empirical evaluation metrics;similarity;structured prediction training;disparate languages;nmt;sentences;recurrent neural networks;different languages;natural language"}, "a538a05864a23e2f80f9b003d5ecbdfb8025b954": {"ta_keywords": "target stance detection;twitter task;stance;twitter;tweet;machine learning pipeline;target;task;semeval;challenge;submission;way;paper", "pdf_keywords": ""}, "3376118362db3751cfbd88acd0c090b8a3897733": {"ta_keywords": "language models;semantic representations;best generalization;generalization capabilities;subwords;new words;complex words;derivational model;bert;vocabulary;input tokens;ppm;proper encoding;case", "pdf_keywords": "language models;standard word segmentation;complex words;rich linguistic structures;distributional semantics;word representations;word representation representation;complex wordswe;computational linguistics;underlying language;novel linguistic structures;linguistic representation;derivational morphology;word vectors;semantic classes;morphological validity;semantic dimensions;linguistic constraints;derivational segmentation;morphology;nonlinear languages;linguistic feature;morphological families;linguistic features;neural machine translation system;new derivational suffixes;words;vocabulary;natural language processing;new words"}, "1392df13a80a962057e979a294a850a50b7deb7e": {"ta_keywords": "automatic annotations;natural language;many uncertain labels;corpus;segmentation;human supervision;supervision efficiency;other segments;user models;segment;same chunk;utility;baselines;cost;method;problem;manner;noticeable gains", "pdf_keywords": ""}, "8df3f3f72eb239eb212bd3fc929bd754ce2e03d6": {"ta_keywords": "protein protein interaction networks;yeast proteins;topic coherence;important proteins;joint modeling;network;relationship;method;novel method;ability;performance", "pdf_keywords": ""}, "aae3d5e24d02ae538030ef3995a86118c5323ae1": {"ta_keywords": "molecule ion;purity ions;density storage medium;ions;storage medium;optical lattice;storage;probe;density;scalable method;speed;use", "pdf_keywords": ""}, "148bca569a0d2833f05df1297788f64bc6686fa8": {"ta_keywords": "temporal adaptation;natural language processing;nlp;temporal misalignment;pretraining;tasks;faceted model;performance;data;model;time period;suite;effects;gains;different domains", "pdf_keywords": "nonlinear programming models;nonlinear programming model;downstream tasks;short time delays;temporal domain adaptation;pipeline;temporal degradation;natural language models;degrade performance;training data diverges;temporal degradation score;tasks;certain tasks;task;training data;nonlinear text domains;several tasks;temporal misalignment;pretraining;unstructured text representations;temporal domain;evaluation timestep;nlps;document classification;temporal dynamics;downstream applications;regularization;natural language domains;entity type classification;modern nlpswe"}, "bb80f7d2269308c3e91da8c47b290645e9d3d7d5": {"ta_keywords": "dynamic human poses;dynamic poses;human poses;learned bases;projections;rotation;movements;original feature space;discovery;data;novel method;set;method;fact;problem", "pdf_keywords": "sparse rotationinvariant poses;pose bases;realistic human motions;bases poses;human poses;pose reconstruction;static poses;animators;pose;static human poses;human pose;pose shape;human motion;action dynamics inference;sports action sequences;rotation;sparse representation;model movement patterns;sports datasets;effective rotation;latent matrix factorization;sports dataset;activity classification;novel representation learning approach;invariant latent factor model;human action;dimensional projections;human activity patterns;sparse latent factor model;human actions"}, "ed1da1abf0f50ca758e422fbd945f891b6cda690": {"ta_keywords": "dialog retrieval;word representations;dialog;recursive neural network identification;example paraphrase;recursive autoencoders;user input query;sentences;subjective metrics;accuracy;arbitrary length;dynamic pooling;pair database;system performance;same meaning;method;robustness", "pdf_keywords": ""}, "7596030e0ce7a8df2f43e6ba4e9de5fa4a19240f": {"ta_keywords": "stochastic gradient descent step;stochastic gradients;double stepsize extragradient algorithm;exploration step;simple bilinear models;sharp convergence rates;update step;algorithm;method;modification;aggressive timescale;error;condition", "pdf_keywords": "novel stochastic extragradient method;stochastic extragradient method;stochastic extragradient;extragradient method converges;extragradient algorithm;extragradient methods;stochastic optimistic gradient;stochastic gradients;gradient descent;stochastic saddle;double stepsize extragradient;stochastic generalizations;better convergence guarantees;ascent schemes;stochastic payoff strategy;stochastic monotone problems;stochastic maxwell;stochastic bilinear saddle;gradient method;stochastic variational inequalities;stochastic feedback;vanilla extragradient;linear optimization methods;stochastic problems;explicit convergence rates;new iteration scheme;seg method converges;stochastic bilinear;optimizer;stochastic"}, "0d1cd3baad7d0734c9bbb008a33e2d10846968cd": {"ta_keywords": "supervised learning process;learning;physics;weights;method;powerful technique;random variables;variables;evaluation;problems;sum;use;set;sizes", "pdf_keywords": ""}, "739c2181f7894050a06b53b41ac5debe8ffc4829": {"ta_keywords": "gradient descent;gradient noise;deep neural networks;capacity metric;stochastic;sde;capacity;generalization error;driving process;metric;tail;trajectories;notion;number;theory;index;experiments", "pdf_keywords": "stochastic gradient descent;stochastic generalization;stochastic optimization;stochastic gradient descent method;stochastic learning algorithms;novel stochastic learning framework;stochastic optimization problems;stochastic optimization algorithms;stochastic;stochastic gradient monte carlo;stochasticity;stochastic optimization iswe;stochastic convergence rates;stochastic differential equation;discrete stochastic differential equations;stochastic processes;stochastic function;stochastic structure;underlying stochastic process;stochastic noise;stochastic differential equation method;stochastic process;stochastic processwe;corresponding stochastic process;gradient descent;stochastic models;simple stochastic process;stochastic partial differential equations;hencestochastic gradient descent;deep learning models"}, "94f8ef5944ecbdd0350d406bf3a16a7f2dff7349": {"ta_keywords": "sequence architecture;novel neural sequence;2mix corpus show;original seq2seq;novel curriculum;wsj1;recognition;architecture;system;model;performance;strategy;experiments", "pdf_keywords": "end multichannel speech recognition system;speech separation signals;sequence speech recognition model;end speech recognition;end speech recognition module;end speech recognition model;independent speech separation;end speech separation;novel speech recognition model;speech recognition;speech separation;neural beamformer;speech recognition system;novel neural beamforming algorithm;adaptive speech recognition;sequence encoder;input speech;novel neural sequence;neural beamforming mechanism;deep model;multiple speaker sources;noisy noisy speech;decoder network;curriculum learning;outputs text sequences;training data;decoder architecture;sequence;channels;novel neural end"}, "ab193c05bc447f368565c1ff37064b1c517a750f": {"ta_keywords": "dialogue systems;learning;agents;replay buffer;common exploration strategies;exploration;neural network;backprop;samples;bayes;new algorithm;algorithm;efficiency", "pdf_keywords": "dialogue systems;dialogue policies;dialogue interaction;dialogue agents;reinforcement learning;many reinforcement learning models;reinforcement learning models;novel reinforcement learning framework;simple replay buffer;large state spaces;unstructured learning;neural networks;learning process;generalized learning network;actions;action region;stochastic representation;underlying interaction structure;action;prior knowledge;action exchanges;backprop method;agent;exploration;rdqm;bayes;traditional exploration methods;interest;task;inherent stochasticity"}, "2b5d553cb2f298f36aff1a1519f7f2f6be4db5da": {"ta_keywords": "node attachment prediction task;taxonomy expansion model;natural supervision;query terms;feature representations;taxonomy;supervision signals;expansion;prediction;steam;node attachment task;self;natural self", "pdf_keywords": "novel taxonomy expansion method;taxonomy expansion task;taxonomy expansion model;taxonomy expansion;knowledge extraction;taxonomy construction benchmarkswe;supervised learning procedure;many knowledge ontologies;node attachment prediction task;new concept terms;new supervised learning model;query terms;taxonomies;supervised learning task;taxonomy;taxonomy expansion problem;unstructured taxonomy;available taxonomy data;supervised learning models;supervised approaches;supervised learning framework;supervised learning model;first supervised learning model;supervised learning performance;supervised supervised learning;supervised learning;query term;supervised supervised learning method;supervised learning method;topical hierarchy"}, "433b24d63146605d25c0c271062e129608462f03": {"ta_keywords": "feature space;feature spaces;nonlinear features;high dimensionality;dimensionality;linear models;basis vectors;log;efficient approximation;novel method;paper;method;experimental results;limited number", "pdf_keywords": ""}, "a0e0ce316ce0fdca2db61a52fdc7100e24906075": {"ta_keywords": "ensemble outlier detector;data streams;outlierness;outlier detection problem;stream;xstream;streaming;data sets;density;stationarity;granularities;row;variety;multiple scales", "pdf_keywords": ""}, "1a0d8dbd0252193abe9d64f72fc56cc1f05ed3eb": {"ta_keywords": "structured situational graph;situational graphs;situational reasoning end task;natural language;language model;reasoning;st graphs;st graph;graph;relevant consequences;qa;wiqa;input;points;method;accuracy", "pdf_keywords": "situational reasoning end task;agnostic situational reasoning framework;reasoning graph;situational reasoning;seq language models;graph generation tasks;reasoning task;seq approaches;qualitative counterfactual reasoning;graph generation;natural language;structured graph;language models;graph construction;seq approach;reasoning;language model;tasks;generative language model;seq;linguistic model;generative generation;st graphs;generative text;graphs;automated metrics;st graph;downstream tasks;graph;sequential sequential machine learning"}, "8529af634b443427d87d62d64467d2f1adfc230f": {"ta_keywords": "heterogeneous malware samples;malware samples;possible threat actors;different threat actors;threat actors;unsupervised setting;late integration;best method;results;method;better results;new method;intermediate integration;work", "pdf_keywords": ""}, "8424dd233577e3bd3fbd7ecdfd8b4d442531a20e": {"ta_keywords": "hidden model parameters;phylogenetic tree structure;discrete models;regression parameters;unsupervised learning;model;linear regression;parameters;probabilistic treatment;rich phenomenology;unified way;hyper;approach;paper;problem", "pdf_keywords": ""}, "7b72f79015a0a5e06cc019bae78f268b16a8e659": {"ta_keywords": "sequence discriminative training;deep neural network;automatic speech recognition;singular value decomposition;rank approximation;dnl;svd;model;novel method;method;problem;advantage;use;effectiveness", "pdf_keywords": ""}, "0ea90d783a76b7c119fb5471fc71b6bc2defa06d": {"ta_keywords": "storage system;node recovery;storage;minimum bandwidth setting;individual node;codes;data;lower bounds;system;problem;paper;download;read;model;amount", "pdf_keywords": "optimal storage code;storage nodes;minimum storage regeneration setting;optimal regenerating codes;optimal storage;storage systems;efficient recovery;storage system;storage;minimum download;optimal decoding;degraded reads;nodes;network protocols;node;minimum bandwidth setting;repair point;individual node;repair;multiple nodes;recovery;information theory;own data;random variables;data;matrix coding;code;node mustwe;common bit;failures"}, "2030551b2590fa70eb5132131e6627c93128f0a1": {"ta_keywords": "parametric utility learning;multiple utility functions;utility functions;social game experiment;heteroskedasticity;efficient behavior;energy;regression models;probabilistic interpretation;players;occupants;mixture;adaptation;data;method;performance;new method", "pdf_keywords": ""}, "b34f254083b012bafd9b5ebd6c27450b4213c984": {"ta_keywords": "captions;biomedical publications;novel learning techniques;content;accurate tool;methods;techniques;misunderstanding;task;hand;number;scheme", "pdf_keywords": ""}, "4a19211e077bc4ada685854245ba9fab381cbb06": {"ta_keywords": "novel framework", "pdf_keywords": "relation extraction;information extraction system;information extraction;knowledge base;unstructured text;information retrieval;relation tuples;knowledge representation;unstructured textwe;structured information;relations;different relation types;relation;relation triples;293k wikipedia articles;novel tagging scheme;natural entities;information processing;entities;contextual triples;joint extraction;document level generality;golden relation triples;entity;qa4ie;extraction;q4ie;arbitrary text;relationships;ground truth triples"}, "409280796e924bfe71421fe5bf4986bd3591ea72": {"ta_keywords": "similarity joins;heterogeneous databases;data integration;data mining;information representation language;data;word;system;context", "pdf_keywords": ""}, "4e3016617e5e254bafebcbd7e96c509f670bdd37": {"ta_keywords": "audio performance synthesis;sound synthesis;sound generative models;transformer encoder;music;decoder model;inputs;clear polyphony;listening test;novel system;model;harmonic structures;score;novel technique;conditioning;power;baseline", "pdf_keywords": "audio music synthesis;music synthesis;new violin dataset;piano dataset;unaligned musical scores;polyphonic inputs;audio waveform;musical score;novel alignment model;natural music performance;encoder;synthesis model;synthesized mel spectrograms;piano;transformer encoder;synthesized mel spectrogram;music;recordings;mel spectrograms;mel spectrogram;decoder model;decoder;wise positional encoding;deep neural network;novel mixer;generative network;waveform;alignment model;fast speech;convolutional neural network"}, "2a39a4f2d18e376ef8a6e2f45416e7b87957481e": {"ta_keywords": "task learning;historical text normalization;normalization;target task data;linear dynamics;more robust models;novel method;problems;field;method;set;traditional method;power", "pdf_keywords": ""}, "bd17620c6cb5ca97ef773499223d1509d123745f": {"ta_keywords": "interactive examples;learning;runnable code;game;parallel;self;integration;new approach;set;approach;problem", "pdf_keywords": "data processing;processing data;training data;data connection;data acquisition;learning process;real data;real world data;process data;complex learning processes;learning algorithm;machine learning model;learning model;signal processing;learning function;data;input data;raw data;machine learning system;data sets;machine learning program;machine learning algorithm;simple data;learning approach;learning process canwe;information inthe problem;data set;training function;artificial intelligence;machine learning context"}, "a30f912f8c5e2a2bfb06351d4578e1ba3fa37896": {"ta_keywords": "aware encoder;code tokens;code understanding;code semantics;novel encoder;novel identifier;identifiers;multitask learning;decoder model;developer;generation tasks;unified framework;prior methods;model;experiments", "pdf_keywords": "code generation tasks;code tokens;code generation;code search network;rich code semantics;code understanding model;code understanding;code translation task;encoder;code semantics;new encoder;benchmark code generation problem;decoder;bimodal dual generation task;identifier prediction;new code benchmark;unlabeled source code;encoded code;small code;code;source code;deobfuscation objective;codet5;novel identifieraware;codet5 model;encoderdecoder model;decoder model;available code;identifiers;identifier tagging"}, "70bc4dc0bc72816773006c71b56fa5885c729caa": {"ta_keywords": "generative music streams;generative adversarial networks;generative elliptic networks;gans;music;convolutions;generation;discriminators;bar;novel approach;preprocessing;data;new approach;set;form", "pdf_keywords": ""}, "78c9181abe18575925fbbb6e6d8c72d7bf90d06d": {"ta_keywords": "low mean delay;problem", "pdf_keywords": "delay optimality;random access protocol;random access channels;random access control;efficient contention mechanismwe;delay theorem;asynchronous transmission;delay optimality problem;delay loss;arbitrary mean arrival rates;many communication systems;random access network;small system regime;optimal polling;stochastic;symmetric scheduling policy;wide time synchronization;contention minis;cyclic polling;strong network traffic;exotic exotic exotic states;optimal schedulingwe;throughput optimal model;optimal scheduling;network evolution;markov process;exotic exotic states;strong contention window;new scheduling scheme;markov process models"}, "9bbdcc03d872987eef9165f4a63c3878a5b05189": {"ta_keywords": "novel dense encoder architecture;dense language model", "pdf_keywords": "dense retrieval;deep denoise;different attention structures;encoder;retrieval;contrastive model;memory representations;deep lexical systems;short sentence level tasks;dense denoise;deep learning environment;deep learning problem;natural language representations;encoder head;learning rate;denoise;deep bidirectional transformers;precision encoder;encoder backbone;language model;sentence similarity;text;web search retrieval;lexical retrieval problem;natural language;performance representations;salient aspects;representations;sparse;scalable learning framework"}, "2dd81061e0b11c828446f6a1843741ae51facbd2": {"ta_keywords": "neuronal dynamics;biological neurons;cortical micro;fast computation;cortical microcircuitry;convolutional architectures;networks;slow components;benchmark datasets;tractable model;membrane potential;realistic models;model;output;ability;competitive performance;new framework;applicability;respect", "pdf_keywords": "neural dynamics;neuron;neuronal network;neurons;neuronal networks;neuronal dynamics;synthetic neural networks;biological neurons;classical neuron;neuronal output;parallel neurons;constant energyneuronal dynamics;neural network;neuronal propagation;neuronal systems;neuronal plasticity;pyramidal neuron;convolutional architectures;neural update;neural networks;bi neuron;feedforward networks;fast computation;networks;neuronal integration;leaky integrator neurons;synaptic time;gradient descent;synaptic weight dynamics;neuronal mismatch energy"}, "87f42406de084e60d2365adac8a159ed3e455856": {"ta_keywords": "streaming graphs;graph detection approach;persistent anomalies;graphs;new similarity function;vector representation;local substructures;implementation;memory;short strings;relative frequency;novel approach;function;large amounts", "pdf_keywords": "graph streams;novel graph streaming framework;scalable graph streaming framework;streaming graphs;data streams;accurate graph detection;scalable graph decomposition;streaming applications;streamspot;scalable clustering;anomaly detection approach;stream spot;dynamic web graph;detects anomalies;scalable clustering algorithm;asynchronous streaming;scalable graph matching method;typed graphs;temporal graphs;clusters;stream;constantspace graph representation;graph structure;clustering;effective clustering;nodes;graph context;new streaming;shingle shingle cluster similarity;graphs"}, "4cc70dd760c2c8cfc0107921bade45fb5efe860e": {"ta_keywords": "art graph recommendation method;knowledge graphs;latent factorization;latent factor model;recommendations;graphs;large datasets;graph;types;links;models;entities;approaches;methods;strengths", "pdf_keywords": ""}, "fa2125641578934de12d7f792b094ffcfdf82ee2": {"ta_keywords": "text topicture synthesis system;text;linguistic representation;original text;design;symbols;pictures;system;technology;form;group;copies", "pdf_keywords": ""}, "c818f9be503a1ed94f991a2949c29e3ee477e8b8": {"ta_keywords": "uncertainty reduction methods;deep neural networks;enhanced feature vectors;input features;probabilistic representation;random interpolation coefficient;efficient dispersion;uncertainty;multiple recognition hypotheses;linear interpolation;empirical results;samples;training process;expectation;methods;effectiveness;method;paper;respect", "pdf_keywords": ""}, "da1d5dc331c2839dfc3e6a79ee17f3bdf2231a8b": {"ta_keywords": "partial list completion task;complete set;database;list;best subset;expansion;objects;method;output;steps;technique", "pdf_keywords": ""}, "eb28e82ca0bbc5d83e1cc07807da16874105d2fa": {"ta_keywords": "unsupervised semantic frame induction;graph clustering;crumpled graph;pattern recognition framework;cluster;triframes;graph;crumpled;structure;verb;objects;images;wtset1 algorithm;web;dependency triples;concept;supplemental material;available library;scale;novel approach;method;influence;state;approach;set", "pdf_keywords": ""}, "572535aff31c400578fdd75313c896c0650b2d4c": {"ta_keywords": "same neural network;denoise;dereverberation;speech challenge;simultaneous integration;novel architecture;architecture;resonance;better performance;first application;system;dr;power;paper", "pdf_keywords": ""}, "8495c1722e1f5107733c842839c2d298b9116921": {"ta_keywords": "conversational search;conversation history;conversational question;bert;history;convqa;approach;seamless integration;effective approach;effectiveness;mode;model;presence;problem;impact;different numbers;different settings", "pdf_keywords": "conversation history answer embeddings;conversational search;conversation history selection module;conversation history;machine comprehension;conversational question;bert;powerful language representation;deep analysis;search;leaderboard;unsupervised learning;prediction model;queries;recent years;popular topic;neural network;new insights;history;history answer;context;robust next generation generation;convqa;previous approaches;extensive experiments;evaluation;better training efficiency;strategy;current question;generator"}, "14f925098d57b0fa491a100fa73e52dbc764efa6": {"ta_keywords": "neuroscience;biological system;structure;brain;central topic;study;dynamics;behavior", "pdf_keywords": ""}, "32a2a8baf217d29f628d22d793cace95634f51d5": {"ta_keywords": "popular open source email client;thunderbird;emails;real email user study;content discovery;content;features;user study;novel approach;performance evaluation;purpose;use;preliminary results;new set;technique", "pdf_keywords": ""}, "1092da11519fe8427c8113b16a012f34f4a3fb6b": {"ta_keywords": "federated learning;learning model;privacy;nodes;dimensional graph;fairness;graph;accuracy;vertices;measures;novel approach;finite size;simple set;finite number", "pdf_keywords": "federated learning;differential privacy;differential privacy guarantees;fair learning;privacy;deep learning;local fairness;privacy budget;classifiers;training data;machine learning problems;machine learning;datasets;local learning;available datasets;fairness measure;learning;discrimination;big data;fairness;own attributes;data;dutch dataset;neural network;local learning rate;noisy data;prediction;learning rate;agents;fpfl"}, "ed913bed529d6bb7beac3b6086a853698abf627d": {"ta_keywords": "automatic speech recognition;digital home assistants;applications;pedagogical introduction;related applications;overview;systems;topic;recent developments;development;art;field;article;state;sections;powerful capabilities;first part", "pdf_keywords": ""}, "914626e2e13bd42a5f06c28ff02ba7c428e81ff1": {"ta_keywords": "turbulent fluid;parallel particles;propagation;particles;particle;direction;field;form;pair", "pdf_keywords": ""}, "e829ee7fe48f4b1e451378b6a21470b2f86c0aa6": {"ta_keywords": "erasure codes;private information retrieval;explicit erasure code;perfect privacy;replication;algorithm;algorithms;pir;requisite data;download;systems;paper;special cases;extra bit;size;contrast", "pdf_keywords": ""}, "31603b3339f4da5bdc6b7de4231bd1ddfb32a50a": {"ta_keywords": "dependent partial differential equations;arbitrary distributions;novel model;differential equation;distribution;data;datasets;model;time;analysis;linear combination;wide range;variety", "pdf_keywords": "irregular time series;regular time series;nonlinear dynamics;time time series models;alternative ode models;neural ode;neural model;time series;dynamics;neural process;noisy stochastic differential equation;neural networks;nonlinear diffusion processes;multivariate time series;odes;time series problems;neural network;neural networkswe;natural nonlinearity;rnns;underlying neural process;neural cde models;neural deep learning models;neural deep learning model;differential equations;stochastic models;models;neural information processing;novel model;recurrent cell"}, "5f563da2843e005c4b236f7889e7a22631b53210": {"ta_keywords": "conference review;conference publications;evaluation;new evaluation;conference;evaluation process;quality;results;improvements;consistency;budapest;hungary;paper;context", "pdf_keywords": "conference peer review;reviewer quality scores;reviewers scores;conference review process;peer review;conference neurips;reviewers;conference paper;peer review community;semantic science;citation counts;quality scores;review committee;citation count;neurips experiment;higher average citation count;citations;subjective scoring;quality reviews;paper scores;reviewing process;conference participants;standard quality score;conference proceedings;evaluation process;nips paper;accurate metrics;review;papers;acceptance rate"}, "dd5e54b08b2c1520d179e88cd524e9bd4fe1f6ab": {"ta_keywords": "optimal transport;divergences;sample complexity;optimizers;distance;regularizer;ot;variant;function", "pdf_keywords": "optimal transport;optimal transport problem;quantum information theory;empirical sinkhorn divergence;sample complexity;sample complexity algorithm;optimal potentials;tube operator;finite samples;quantum computer;sinkhorn divergence;tube tube operator;quantum systems;expectation maximization problem inwe;wasserstein distance result;probability measures;markov chainwe;transport plan;generic spin chain;regularization parameter;quantum system;spin chain;optimizers;measures;divergence;cost function;complexity;regularization parameterwe;minimal cost;empirical convergence"}, "8b192503f119a0b0cc30ef5179a00f231c20fb93": {"ta_keywords": "fake event epochs;rnn;events;deep neural network;negative evidence;observable events;datasets;novel nonparametric;world datasets;data set;baselines;experiments;presence;use;introduction;novel method;method", "pdf_keywords": "deep event models;fake event epochs;long short term memory;recurrent neural networks;recurrent neural network;event sequences;multivariate event stream;deep learning model;nonparametric neural model;proximal graphical event models;fake epochs;deep neural network;fake epoch;event stream;event sequence;model multivariate event data;events;spatiotemporal attention;fake epochs towe;multivariate event model;generic event model;event;neural layers;strict histories;dimensional attentions;observable events;time evolution;histories;neural network;continuoustime"}, "0ca2575a1ef73930dc2abe205b44e079eadc426c": {"ta_keywords": "multiple lanes;traffic flow;tolling schemes;lane road examples;theoretic lane;new macroscopic model;local decision;multiple populations;drivers;model;behavior", "pdf_keywords": ""}, "b5991b1018bb89b053a2c8229248f97956391bb5": {"ta_keywords": "iterative pronunciation;acoustic model adaptation;pronunciation variations;iterative estimation procedure;acoustic data;average accuracy;speech;approximation;improvement;method;occurrence frequency;standard;various levels;experiments", "pdf_keywords": ""}, "b19cba7bfe318c69d5e62f8322cb5d75228452f4": {"ta_keywords": "scale language models;specific weight matrices;parameterized hypercomplex multiplication;rank optimization;task;model;combination;novel method;adapters;previous work;method;art", "pdf_keywords": "scale language models;parameterized hypercomplex multiplication layers;rank language models;language models;soft computing;language model;generic language model;performance computing;nonlinear generative language models;performance memory;deep learning networks;deep network;scalable method;hypercomplex multiplications;bottleneck;aware reading model;efficient generation;memory;medical computing;layers;parameterization;linguistic complexity;fewer parameters;rank optimization;tuning;parameter complexity;models;weights;language;decoder model"}, "3ba013b8b56646d66aac8472fb90a5c029ef55a0": {"ta_keywords": "numerical solver;traditional numerical solvers;learning process;equations;optimal time;training;solutions;method;class;evolution;system;solution;utility", "pdf_keywords": "speed regularization;adaptive solvers;kutta solver;regularization;gradient descent;neural odes;regularization term;solver speed;standard numerical solvers;iterative optimization;ode solver;neural ode;step solvers;forward derivatives;autonomous ordinary differential equations;learning;order solver;solution trajectory;advanced learning;trajectory trajectories;odes;stochastic differential equations;supervised learning;trajectories;ordinary differential equations;regularizer;continuous normalizing flows;derivatives;trajectory;supervised learning model"}, "26299d5fdc5137291dc6a091573b3d18aba1d1c2": {"ta_keywords": "multilingual model;diverse languages;new language;entity recognition;novel adapter;task;adapter;novel invertible adapter architecture;strong baseline method;framework;representative set;size;problem;state;art", "pdf_keywords": "recent efficient language adapters architecture;multilingual models;multilingual model;multilingual task;language model;multilinguality;resource languages;machine translation model;reference language;languages;other languages;machine translation machine;adaptingwe;source language;language;computational linguistics;task adapters;arbitrary large corpora;vocabulary size;downstream task;linguistic specificity;arbitrary tasks;modular adapter framework;parallelizations;efficient transfer;adapters;efficient conversion;efficient generation;adapter;models"}, "cce8cf3d7f45113a4cba984b878802a5b16d5967": {"ta_keywords": "statistical phase transition;statistical properties", "pdf_keywords": ""}, "77ce1b8d425b7538c21ce0be976ee24a58e797c1": {"ta_keywords": "reconstruction basis functions;novel multiplicative update algorithm;basis functions;noise ratio;snr;nsf;optimization;novel method;generalization;signal;separate analysis;model;step;method;performance criteria", "pdf_keywords": ""}, "5e8c52ddbd3581320f7e536b7cd10d7263b81eb2": {"ta_keywords": "feature predicates;unsupervised grammar induction algorithm;student sample;agent;generation;novel approach;effort;level", "pdf_keywords": ""}, "4481244de2cc0c55d91cebbb152eec79a76386f3": {"ta_keywords": "reliable packaging technology;tsv integrations;non conductive film;logic device;3d;tsvs;reliable assembly process;3d structure;novel high density;substrates;target package;tier structure", "pdf_keywords": ""}, "d26254cf3ec537f37708afaaf7f5a76a7922d4a2": {"ta_keywords": "translation quality;word pairs;reorderings;different distances;distances;series;specific threshold;experiments", "pdf_keywords": ""}, "10e221c7d4636703c5c97b54f53b1cb57c25f3a6": {"ta_keywords": "unregularized linear networks;importance weighting;regularization;indefinite training;l2;performance;effect;paper;presence", "pdf_keywords": ""}, "6dfecb5915e8b10841abe224c5361bbda7100637": {"ta_keywords": "morphological parsers;parsers;tgrinya language;morphological segmentation;oromo languages;tigrinya;lemmatization;other languages;rule;simple rule;state models;paradigm;context;use", "pdf_keywords": ""}, "73e401dead436aabd0cd9c941f7b13bfdeda9861": {"ta_keywords": "point compressors;efficient communication;optimization;compressors;theoretical communication complexity;new efficient methods;practical efficiency;art error feedback mechanism;training process;aim;general approach;state;approach;methods;theoretical properties;new class;number;special case", "pdf_keywords": "communication compression;contractive compression operators;efficient compression scheme;linear communication algorithms;compression scheme;iterative communication method;geometric compression technique;geometric compression;biased compression algorithms;data compression;compression mappings;new compression model;compression;global compression;point compressors;biased compression algorithmswe;point compressor;points compressor;contractive compressors;compression mechanisms;contractive compressor;orthogonal approaches;communication algorithm;iterative minimization;theoretical communication complexity;efficient training;original compression;compressor;pc compressor;new efficient methods"}, "1c2c9e5d0588599516a78adda1fe3935dc5ae5d7": {"ta_keywords": "stochastic gradient play;stochastic derivative free play approach;convex games;unconstrained optimization;monotone games;optimization;efficiency;classical method;simple method;method;class", "pdf_keywords": ""}, "311381feeb6346bfcb2ba622bd8f713261a4075d": {"ta_keywords": "wikipedia revision histories;collaborative documents;user comments;hierarchical neural network;edit pairs;specific edit actions;edits;comment;document context;relationship;deletions;dataset;additions;novel method;approach", "pdf_keywords": ""}, "efcdb62b59e4dfb3f51b53850a81d6149ec3dfc8": {"ta_keywords": "interpretable machine learning;diagnostic workflow;iml methods;useful diagnostics;step workflow;iml;use cases;researchers;methods;cases;taxonomy;consumers", "pdf_keywords": "interpretable machine learning;technical objectives;iml methods;narrow technical objectives;machine learning;potential iml diagnostics;technical objective;machine learning models;diagnostics;ai explanations;feature attributions;machine learning applications;feature attribution;learning methods;accurate model predictions;features;evaluations;essential features;underlying predictive model;predictive models;iml;iml practitioners;taxonomy;usefulness;dataset poisoning;conventional explanations;artificial intelligence;existing methods;extensive discussion;deep neural networks"}, "null": {"ta_keywords": "hierarchical clustering;high quality hierarchies;hierarchies;large graph;tree;real world graphs;novel probabilistic model;graphs;quality metrics;continuous relaxation;model;end optimization;relaxed versions;efficient end;theory;connections;parameter", "pdf_keywords": ""}, "42fc352a0db1e742b0248a02b812db4aaf7b2cd3": {"ta_keywords": "neural machine translation;reranking;energy;mle;nmt;task measure;accuracy;model;samples;ebr;terms;method;problem;behavior", "pdf_keywords": "autoregressive neural machine translation;neural machine translation;machine translation;translation machine;translation model;resource machine translation datasets;translation tasks;translation performance;top translations;simple exponential translation model;lingual models;energy ranker;translation state;deep fusion;translation;polyakov decoder;generative model;neural machine;ranking;energy model;german tasks;decoder;parallel decoding;rank;encoder;energy;nonlinear models;mle;new conditional energy model;models"}, "740afdd1619d797145b056877865f941891e6a65": {"ta_keywords": "iteration complexity;gradient oracle;maker iterations;der gradient oracle;algorithms;iterations;loss function oracle;geometric domain;decision;maker;case;domain;problem;number", "pdf_keywords": "stochastic decision;bandit algorithms;performative prediction problem;dependent risk minimization;stochastic gradient methods;dynamic pricing;dependent learning problem;stochastic prediction function;stochastic gradient method;stochastic pricing;stochastic program;optimal rates;optimal prices;decision trees;first order gradient oracle;loss function oracle;stochastic algorithm;optimal price changes;optimal point;dynamic pricing experiment;stochastic representation;bandit feedback model;sample complexity guarantees;stochastic;geometric decay process;optimal occupancies;pricing parking spaces;bandit feedback;decision maker;dependent loss"}, "b9c026ab6e161a0f8c4b4db82ee8ad10792084cc": {"ta_keywords": "electrollarynx;statistical voice conversion;ngectomees;spectral subtraction;naturalness;experimental results;hybrid approach;intelligibility;method;significant improvements", "pdf_keywords": ""}, "066f2023b2b5ba5df61dc193c205785fa5e73fed": {"ta_keywords": "kernel clustering;spectral relaxation;spectral bounds;new linear kernel;continuous kernel;standard kernel;optimization algorithm;partition;solvers;joint energy;tasks;data;new approach;application;new criteria;approach;power;basic principles;variety", "pdf_keywords": ""}, "9578679e028777dd709881f938114aa59fbbf481": {"ta_keywords": "string distance metrics;entity names;similarity;empirical string;string;distance scheme;metrics;comparator scheme;terms;hybrid scheme;performance;method", "pdf_keywords": ""}, "ab627ba77dced941f9f45eeaee17bc6644308d89": {"ta_keywords": "new collective classification method;collective classification method;collective classification;act classification;email acts;email;baseline classifier;dependency;network;method;bag;significant interest;field;significant improvements;problem", "pdf_keywords": ""}, "3429d0529d3e77f9e4606f13b2d252d5d964abad": {"ta_keywords": "orbit coupling strength;orbit coupling;orbit interaction;spin;coupled system;orbit;system;effect;value", "pdf_keywords": ""}, "66b83f0801d0c2d4194ff60c5ef9c754b51ce521": {"ta_keywords": "image segmentation models;ct scans;pancreas;images;interpretability model;regions;analysis;downstream model performance;noise;downstream performance;method;new method", "pdf_keywords": "image segmentation tasks;image segmentation;accurate segmentation model;segmentation;interpretability models;input image;interpretability;interpretability model;images;neural networks;deep neural networks;image;occlusion map;interpretabilitywe;ct scans;explainability techniques;underlying noise model;pancreasct validation;occlusion sensitivity;large images;small neural network;noise model;pattern recognition;pancreas;pixel;learning;noise;cam;model;noise parameters"}, "568efa8d71d8f2a086c8debcdf547e7053269021": {"ta_keywords": "entanglement;quantum information processing;quantum system;quantum state;single quantum system;coherent state;measurement;stationary state;new method;key resource", "pdf_keywords": ""}, "4f0d485cbcde840533f23f0c8b0f3fa1ca2d74df": {"ta_keywords": "linear bandit problems;linear bandits;dependent lower bounds;transductive setting;algorithm;logarithmic factors;instance;new class", "pdf_keywords": "transductive linear bandit problem;general transductive bandit problem;transductive bandit setting;linear bandit problem;linear stochastic bandits;linear bandits;linear bandit;new optimal measurement allocation;optimal allocation;optimal strategy;regret minimization algorithm;bandit;bandits;optimal measurements;optimal static allocations;convex optimization cost;optimal convex optimization problems;optimal solutions;convex optimization problem;optimum;sample complexity;optimal design;optimization problems;least squares;optimum linear design;optimal arm;adaptive algorithm;rewards;single static allocation;adaptive phase allocation"}, "7b19e6540c786b80a3615a8ae2ef706242a1fa5b": {"ta_keywords": "compressive phase retrieval;graph codes;sample complexitys;sublinear scheme;measurement matrix;linear scheme;scheme;measurements;regime;number", "pdf_keywords": "noisy compressive phase retrieval problem;compressive phase retrieval;novel phase retrieval algorithm;noisy signal recovery;sparse phase;phase code algorithm;sublinear computational complexity;signal dimension;chaining pursuit method;measurement matrix;linear measurements;sparse bipartite graphs;noisy noisy signalwe;minimization;linear dimensional optimization problems;noisy signal;computational complexity;linear algorithm;convex programming;signal;additive noise;phase space;algorithms;phase;dimensional square lattice;algorithm;diffraction imaging;new algorithm;index matrices;encoding"}, "a0cbaf59f563580f68523ab6839a436e38b6db18": {"ta_keywords": "multilingual corpus;translation efficiency;resource translation;source sentence;performance;training;orders;novel strategy;strategy;magnitude", "pdf_keywords": "machine translation framework;multilingual data;multilingual translation;multilingual training;multilingual nmt system;best translation pattern;translation accuracy;best translation option;natural language;translation pattern;translation domain;language;target sentence;linguistic specificity;certain reference word;reference word;training data sets;machine learning;source sentence;learning;sentence;unconstrained context;baryonic units;best data selection method;neural networks;sampling framework;low resource;high dimensional adaptation;similar probability;scalable algorithm"}, "105146a7872835a52c8c5c55a3aae62c5d8852a1": {"ta_keywords": "machine translation;translation approach;many alignment strategy;character;phrase;traditional phrase;novel approach;accuracy;framework;approach;power;use;substantial gains", "pdf_keywords": ""}, "9d10bbd21f475d500c3a7e24052e02596e052e2e": {"ta_keywords": "speech recognition;neural networks;gram language;speech;strong signal;new approach;method;use;approach;presence", "pdf_keywords": ""}, "36d193c7a9523f55f9fe5ffd0730f248c241f5c7": {"ta_keywords": "bias;open problems;work;process;solutions;fair way;tutorial;collection;appendix", "pdf_keywords": ""}, "8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad": {"ta_keywords": "new commonsense inference task;adversarial dataset;new dataset;dataset;rich models;sentence;machine;hellaswag;novel;set", "pdf_keywords": "commonsense learning;commonsense inference;commonsense models;natural language inference challenges;commonsense models arewe;commonsense reasoning;commonsense;deep models;deep learning pipeline;neural machine translation;deep learning framework;adversarial filtering;natural language;language model;novel dataset collection;new dataset;text datasets;model selection biases;dataset;other dataset;section machine learning;human validation;machine learning;models;unstructured text accuratelywe;bert;future benchmarks;semantics;underlying text;unstructured text"}, "de7d0c87794c3de6f8ab2c753ecc398c18c26631": {"ta_keywords": "grammatical specification;universal interoperability project;expert annotations;languages;language;agreement;readable format;raw text;concise;rules;framework;promising results;interest", "pdf_keywords": "rule extractions;standard syntactic analysis;syntactic analysis;syntactic rules;rule extraction step;rule labeling;method extraction rules;grammatical agreement rules;linguisto framework;stable treebanks;automatic evaluation;evaluation ofagreement rules;linguistic features;verifying agreement rules;morphotactic text;computational linguistic complexity;linguist;expert annotations;goldstandard treebank;morphological features;crosslingual data analysis;rich morphology;linguists;method extraction process;agreement rules;linguistic similarities;several languages;languages;language;good agreement rules"}, "b709da495f15a4a9c1173192ecd755d1697dedf0": {"ta_keywords": "recommendation system;effective recommendation strategies;recommendation tasks;genome database;collaborative filtering approaches;random walk model;random walk;saccharomyces;edge paths;rich context information;path;graph;approaches;history;approach;restart;system;performance;experiments", "pdf_keywords": ""}, "bef4548a43fca8a7410734e4200157d50e257a29": {"ta_keywords": "adaptive span algorithm;adaptive span;automatic speech recognition;network learning concept;algorithm;data sets;sequence;appropriate sequence;new method;improved performance;several computer;method;task;use;problem;interest;results", "pdf_keywords": ""}, "dad121213a17c6cc977d2298c7a9927639ca58e6": {"ta_keywords": "random walks;random walk;random noise field;random environment;stationary state;method;application;simple method;large number;presence", "pdf_keywords": ""}, "9f832bdcbc9d9566f7ab07b7455364bee62086fb": {"ta_keywords": "", "pdf_keywords": ""}, "f79361dda56ee755fc56ab83cf0d9f12d42b2d5e": {"ta_keywords": "generative model;efficient identification;new method", "pdf_keywords": "approximate ranking;pairwise ranking;noisy pairwise comparisons;noisy pairwise comparison data;higher ranked item;pairwise comparisons;entire ranking;ranking;true ranking;noisy comparisons;optimal counting algorithm;optimal subset;rankings;pairwise observations;top pairwise;single pairwise win;pairwise win;underlying ordering;comparison probabilities;algorithms;top item;pairwise pairwise;algorithm;rank;random selection;recovery threshold;recovery probability;high probability;thresholds;items"}, "092687dc06b0b264a524c6d4ea151780ba85a02a": {"ta_keywords": "communication skills;communication;communication problems;brain;new training tool;context;method;video;form;nucl;modalities;people;article", "pdf_keywords": ""}, "2d8d51d483a50c6fbf16a0cc120465539f4055da": {"ta_keywords": "single microblog text;different languages;languages;other languages;multilingual study;entropy;information;tweet;larger character sets;much information;more information;character;results;theoretic approach;study;criterion", "pdf_keywords": ""}, "27ad78b72c3fb77a117b15855008b65e838314e8": {"ta_keywords": "orbit coupling;spin;dynamics;effect", "pdf_keywords": ""}, "51b2dd5cbec02f016c6fa716705ede9b3846a410": {"ta_keywords": "heisenberg antiferromagnet;spin dynamics;antiferromagnet;spin;orbit interaction;effect", "pdf_keywords": ""}, "3e86ecbd41ab55b90d3b45601aeb15d2e5c1c8f8": {"ta_keywords": "knockout tournament;optimal draw;draw;polynomial time;memoization;algorithm;outstanding open problem;player;natural cases;problem;previous approaches;np", "pdf_keywords": ""}, "7580df14bf01438e7174bbff260508a39a44df84": {"ta_keywords": "explicit erasure codes;storage code;possible storage channels;optimal codes;fast encoding;storage;codes;sparsity;minimum distortion;repair accuracy;spars;generator matrix;desirable properties;general classes;construction;ii;iii;transformation", "pdf_keywords": "first explicit sparse storage codes;storage codes;sparse codes;optimal optimal regenerating codes;erasure codes;systematic erasure codes;first explicit erasure code construction;matrix codes;storage systems;optimal storage;storage;encoding matrix;sparse matrix product;new encoding matrix;product codes;codesthis paper;storage length;first explicit code construction;maximum storage lengths;systematic mdr codes;sgr codes;new codes;codes;systematic msr codes;traditional matrix;code;sparse;minimal repair;matrix framework;encoding"}, "92fc770721e95249f8db01c5019d1cc4cf79ff00": {"ta_keywords": "natural language query system;command query language;hubble space telescope;proposal selection process;macro;language sentence;specification;implementation;procedure definition;system;variety;ways", "pdf_keywords": ""}, "6c5144872c259611dceb32fe4e4486a6865e6c42": {"ta_keywords": "robust speech recognition;speech recognition;speech recognition accuracy;extended kalman filter;residual component decomposition;bias component;residual component;estimation;algorithm;noise;bias;method;evaluation results;ed;paper;important step", "pdf_keywords": ""}, "cf6352c789ab51320fa7ca9b1440c685b57fd769": {"ta_keywords": "speaker diarization;diarization challenge;diarization;domain processing;challenges;upcoming jhu;task;first round;lessons;new approach;final results;use;combination;team;initial experiences", "pdf_keywords": ""}, "2e1a1588955a8a64ec618b3cc04be961ed0cb59c": {"ta_keywords": "electronic excitations;polymers;state energies;transfer learning models;excited state;efficient transfer;transferability;p3ht;oligomers;crystal;hexylthiopnene;solution phases;novel approach;relative locality;approach;information", "pdf_keywords": ""}, "9237d6efc603465765e80eb5ca1268c2bd7b5c24": {"ta_keywords": "machine translation;language generation tasks;linguistic labels;probabilistic models;log likelihoods;holistic analysis;source side data;comparison;compare;tool;results;mt;advanced features;systems;use;number", "pdf_keywords": "word translation accuracy;neural machine translation system;machine translation;machine translation output;machine translation data;translation accuracy;translation systems;other language generation systems;sentence accuracy;language generation systems;available open source comparison tool;word processing system;translation quality;gram analysis;holistic comparison;automatic error analysis;entire corpus;sentence accuracies;translation output;compare;sentences;linguistic domains;present compare;relative performance;toolkits;source tool;holistic analysis;new analysis tool;sentence;particular word type"}, "a6336fa1bcdeb7c84d2c4189728f0c1b2b7d0883": {"ta_keywords": "sequence learning;powerful neural networks;neural networks;handwriting recognition;image captioning;language translation;tasks;recent advances;synthesis;discovery;research", "pdf_keywords": "recurrent neural networks;recurrent neural network;recurrent neural network literature;machine learningwe review recurrent;neural machine;simple recurrent neural network;rnns;bidirectional recurrent neural network;popular bidirectional recurrent neural network;neural networks;neural network;powerful learning models;trainingneural networks;unsupervised machine learning tasks;long short memory;neural learning problems;complex algorithmic tasks;stochastic gradient descent;short short short memory;powerful computational systems;range time dependencies;machine learning;arbitrary computation;universal machines;bidirectional bidirectional neural network approach;powerful models;classical computers;models;speech recognition;time step"}, "7b7416c90e8d3fc9ad5c9fb3923a638f69294ed7": {"ta_keywords": "large text corpus;semiparametric representation;single model;novel method", "pdf_keywords": "dimensional mention encoders;entity mention encoder;entity mentions;encode knowledge;single neural language model;unstructured entity vocabulary;large text corpus;large corpus;knowledge bases;partial knowledge retrieval problem;simpler language model;generative entity;language model;knowledge base;novel retrieval model;corpus;several retrieval models;partial knowledge;unseen entity;entities;entity;generative retrieve;encoders;accurate retrieval;local mentions;factual knowledge;unstructured documents;dimensional memory layer;knowledge basis;unstructured unstructured text"}, "8be39979cb2eb1aeaba15b57e1e4bc712eb962cb": {"ta_keywords": "paragraph reconstruction;paragraph reconstruction objective;input paragraph;paragraph;art paragraph;sentence;zhang et al;novel state;powerful tool;method;state", "pdf_keywords": "paragraph embeddings;paragraph classification tasks;several standard paragraph classification tasks;sentence representations;sentence content;sentence content objective;word representations;downstream classification tasks;sentence content property;paragraph;accurate embedding;deconvolutional decoder;input paragraph;embeddings;convolutional encoder;downstream classification accuracieswe;downstream classification;sentences;sentence structures;encoder;accurate sentence structure;natural language;level encoder;text segments;many downstream tasks;art paragraph;downstream accuracy;standard cnn;dense vector representations;neural networks"}, "f9ee690d223beac6d893aedae13c09dbf0fb694e": {"ta_keywords": "clustering solution;clustering;maximum entropy estimation;clustering results;maximum entropy;maximum likelihood estimation;pairwise constraints;pairwise consistency condition;discovery;kuramoto;methods;siva compilation;method;variance;process;variety;results", "pdf_keywords": ""}, "7354b87a1b4c99ccd9cf25b7314927ced8b156f7": {"ta_keywords": "writing assistant;texts;text;generative dataset;generative output;generative model;author preference;language model;intent;users;scale user study;openmp;explicit means;linear", "pdf_keywords": "interactive writing assistant;text generation;novel generative text;explicit rhetorical directives;annotators;creative writing task;crowdsourced evaluations;rich lexical resources;author specifications;assistant tools;discourse markers;writing;syntactic instructions;texts;story writing;author intents;language model;computational linguistics;coherent text;neural language models;lexical resources;author intent;text;sentence fragments;important lexical resources;target author intent;explicit text equations;unstructured unstructured text;tunes language model;syntactic structures"}, "0f395654e69cd2e063a6ef221fb66fb46e68cefd": {"ta_keywords": "truthful classifiers;classifiers;traditional classification system;possible features;features;novel algorithm;novel characterization;algorithm;prior knowledge;presence;high number;convergent", "pdf_keywords": "classifiers;optimal classifier;optimal classifiers;classifier;possible truthful classifiers;truthful classifier;classification;linear classifiers;accurate classifiers;nonlinear classifiers;dimensional classifiers;climbing classifier;truthful classifier intheorems;nonlinear classifier;linear classifier;novel classifier;classification methods;linear classifier framework;learning problem;global classifier;machine learning;supervised learning problem;perturbative classifier;gaussian classifier;classifier accuracy;global classifier algorithm;mean classifier;supervised learning;feature classification;global classification"}, "60dd53fca1f538fabe18e4d6a9326b2f40e358dd": {"ta_keywords": "scalable implementation", "pdf_keywords": ""}, "084ddb77fce5a7f0b6418ef4e38dbb1bedf4ae78": {"ta_keywords": "electrollarynx control;electrollarynx;listenability;statistical f0 prediction;intelligibility;simulation;experimental results;method;naturalness;terms;significant improvements", "pdf_keywords": ""}, "5b2c5eeea9ac8f26908b9dfc8fd0a2d0e7aa5bb1": {"ta_keywords": "robust speech recognition;adaptive beamforming;beamforming;acoustic model;term memory recurrent;term memory;neural network architecture;multichannel;baseline systems;environmental noise;filter coefficients;nonstationary;source;system;novel system;absolute gain;dynamic nature;positions;time", "pdf_keywords": "deep lds acoustic model;speech enhancement module;adaptive beamforming network;deep network;speech recognition tasks;deep reinforcement learning;deep neural network;adaptively beamformer;adaptive beamformer;robust deep learning;speech processing;acoustic model;deep lld beamformer;low dimensional acoustic models;multichannel speech signal;accurate speech recognition;sum beamdeep;beamformer;deep lds;reinforcement learning;training data;deepwe;term memory;blind learning;snn;reinforcement learning paradigm;deep lld;ssm;temporal sequences;range dependencies"}, "d38b686b8b68d0b91b294fd8a55ac7dea191706f": {"ta_keywords": "popular summarization datasets;summarization framework;different summaries;highlighted sentences;external guidance;guidance;faithful summaries;different types;different kinds;input;framework;art performance;model;several different varieties;experiments;state", "pdf_keywords": "neural abstractive summarization framework;neural summarization;abstractive text summarization;abstractive summarization;inabstractive summarization;summarization;coherent summaries;summaries;accurate summaries;best summaries;natural language;annotations;novel recurrent;external guidance;guidance content;faithful summaries;popular sumneural networks;neural network;neural models;neural networks;text;summary;neural model;sentences;saliency;more novel words;decoder;guidance signals;neural network model;novel approach"}, "e114618157e025ed17b7e45684d67becd34a14f3": {"ta_keywords": "total variation distance;component mixtures;new lower bounds", "pdf_keywords": "distribution learning;high dimensional distributions;total variation distance;dimensional mixtures;component distributions;variational distance;dimensional distributions;dimensional mixture;gaussian mixtures;multivariate distributions;component mixtures;distributions;sample complexity;component gaussians;mixtures;dimensional gaussians;mixture;entanglement distribution;polynomial learning case;polynomial learning;generalization;identical particles;distribution;random distributions;tight lower bounds;generalized version;theoremstheorems;new theorems;close parameters;component variance"}, "3ed91aae1038b8b0130fb3974060a50b10de1345": {"ta_keywords": "automatic speech recognition;speech recognition;advanced machine learning algorithms;acoustic beamforming;asr;robust recovery;robust back;novel front end;tutorial article;field;distance;tutorial;end engine;recent developments;application;new class;dereverberation;approach;problem", "pdf_keywords": "advanced speech recognition pipeline;end speech recognition;advanced speech systems;end speech recognizer optimization;field speech recognition;advanced speech recognition;field speech processing;advanced speech processing;speech recognition;advanced speech recognition algorithms;speech recognition challenge;automatic speech recognition;robust speech recognition;speech recognition system;end speech enhancement;speech datasets;speech recordings;advanced acoustic systems;spoken systems;speech enhancement;advanced speech separation;deep learning;end speaker diarization problem;speaker task;acoustic beamforming;deep learning approach;speech source;sound processing;single speaker fromthe challenge;noisy single source speech channel"}, "addd2d86d19c1e7c8854e827fb2656a50c250440": {"ta_keywords": "summaries;article;novel method;method;set", "pdf_keywords": "aspect summarization process;aspect annotation;aspect classification;aspect dataset;aspect discovery process;novel aspect discovery;extractive summarization task;extractive summarization model;summarization;aspect classifier;aspects;summarization method;focused summaries;aspect classes;aspect;nssds summarization;wikipedia dataset;unstructured content;outputs summaries;unstructured unstructured unstructured text;relevant aspectswe;wikipedia articles;unstructured documents;relevant aspects;novel aspect;automatic characterization;articles;important sentences;sentences;domain labels"}, "0c3c4c88c7b07596221ac640c7b7102686e3eae3": {"ta_keywords": "particular proton interaction;proton targets;proton interaction;new statins;search;available mathematical dataset;new drugs;np", "pdf_keywords": "natural language inference;annotation corpus;deep knowledge extraction;biomedical text;natural language processing;biomedical text analysis;machine reading comprehension model;corpus;novel biomedical research dataset;annotators;nlp;neural machine translation model;deep machine;biomedical domain;novel biomedical question;training data;hard task;deep neural machine;deep neural machines;reasoning types;long answers;biomedicalwe;knowledge;strong baseline;task;question titles;topic distribution;questions;novel mathematical task;spontaneous electrocardiogramwe"}, "b7731a9b9142a6deb132e99bc55ddbe458a537a6": {"ta_keywords": "asymptotic regret;causal effects;causal effect;modern neuroscience;estimation;algorithms;many models;data source;functional;time;interest;framework;query;central problem", "pdf_keywords": "online moment selection;causal models;online moment selection problem;causal model;adaptive selection;graphical models;empirical heterogeneity;selection policy;empirical data;selection;empirical analysis;graph model;stationary models;models;stochastic bandit scenario;martingale theory;optimal action;structural assumptions;empirical consistency;qmd;graph vertexwe;empirical adjustment sets;causal effect;sample moments;step quantum;such processes;statistical analysis;machine learning algorithms;empirical adjustment;exponential weight matrix"}, "3ea5468e6d3007a94d4318932d7778693526145c": {"ta_keywords": "dynamic resource management mechanism;area grid computing;delay compensator;transient state performance;high steady state;main feature;paper", "pdf_keywords": ""}, "97906df07855b029b7aae7c2a1c6c5e8df1d531c": {"ta_keywords": "linguistic information;qualitative information;pipeline;level representations;information;network;level decisions;description;accuracy;essential features;model;basis", "pdf_keywords": "bert encoder;layer bert;language encoder;bert network;nonlinear linguistic tasks;contextual tasks;linguistic structure;linguistic information;encoder;downstream tasks;particular encoder;bert;joint knowledge representation;linguistic properties;natural language;level representations;coreference;coreference arewe;encodings;deep learning framework;tasks;encoding;deep bidirectional transformers;sentences;task;individual sentences;behavioral analysis;layers;important layers;pipeline"}, "e31a3f52890dcb68f596020e45f8c9718b700466": {"ta_keywords": "orbit interaction;magnetic field;spin;dynamics;interaction;field strength;presence;value", "pdf_keywords": ""}, "77c98b45c95121fc2a3d2ab4906fc00364cf381c": {"ta_keywords": "level systems;level system;separation;suitable control field;interaction;new method;method;application", "pdf_keywords": ""}, "11a28f9e6fb6581d0a01428dd27a3fb649454395": {"ta_keywords": "chymotrypsin andtrypsin;chymotrypsin;trypsin;atrypsin;active enzyme;super proton;tryptic hydrolysis;specific substrates;substrates;differences;special shape;form", "pdf_keywords": ""}, "6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4": {"ta_keywords": "quality image classification;visual concepts;images;dataset;natural language;image;models;scalable model;text;art;model;pairs;performance;state;approach;several important respects;ability", "pdf_keywords": "image supervision;transferable visual models;shot classification;cnn;image classification tasks;supervised natural language;shot image classifier;natural language supervision;imagenet dataset;shot classifier;object classification;convnets;shot detection;visual representations;supervised representation;deep learning;supervised image classification;fewshot benchmarking;shot clip;deep metric learning;learning;usual supervised training process;standard imagenet dataset;imagenet model;caption;supervised performance;visual concepts;sota image representations;shot evaluation;deep metric learning systems"}, "de0e1f9980afa7949df64d53b8ae7a2f59c55579": {"ta_keywords": "fewshot style transfer;style transfer;fewshot methods;paraphrases;sentences;target style;stylistic difference;inputs verbatim;single utterance;attribute;new method;context;difference;model;amount;work", "pdf_keywords": ""}, "1d79d055cf9711944f14e1388a9d054cbe81ddd0": {"ta_keywords": "discriminative language models;supervised training examples;text;perceptron algorithm;best training examples;word;similar results;baseline model;results;variant;half;case;work", "pdf_keywords": ""}, "8cfd299be05bf3df91e0bf656a7e2fb973056350": {"ta_keywords": "language family classification;language similarity approach;speech synthesis tasks;speech synthesis;acoustic cross;languages;wide range;hundreds;approach;effectiveness", "pdf_keywords": "multimodal speech encoder;language embeddings;language similarity;speech data;language family classification task;speech processing tasks;target language data;language similarity approach;speech synthesis;speech systems;multilingual tasks;speech recognition;different target languages;acoustic acoustic classification;source languages;spoken languages;multilingual sources;resource languages;speech synthesizer;acoustic models;acoustic information;language interfaces;languages;language families;language interface;speech recognition system;acousticwe;linguistic information;acoustic approaches;utterances"}, "3ba26e897d0085ecd8cb695e1728a083f9227447": {"ta_keywords": "statistical properties;statistical description;complex systems;particle motion;random environment;particle;properties;motion;system;observation;state;approach;problem;new approach", "pdf_keywords": ""}, "8f6763b339363216794f48895b9381d1a7caa88c": {"ta_keywords": "robustness;players actions;agent;players;disturbances;disturbance;existence;sufficient conditions;subset;properties;respect;number", "pdf_keywords": "quadratic games;quadratic game;learning dynamics;dynamic games;multiagent learning;optimal players;continuous games;finite horizon games;gradient play;discrete games;simultaneous play;game graphs;learning trajectory;optimal optimal strategies;quadratic cost functions;optimal strategies;gradient descent;player bilinear games;bilinear games;gradient disturbance;games;player games;individual player;optimal decision making;learning;dynamics;arbitrary disturbances;equilibria;player input;gradient"}, "51e5e7093e0183feab61b00ca6c3df61cd8c46de": {"ta_keywords": "gram language models;language modeling;machine translation;phrase tables;best lists;perceptron algorithm;strong baseline;phrasal;similar methods;output;training;largest gains;methods;experiments", "pdf_keywords": ""}, "9d03a125a9568af8af3fae5091752017d6abe59e": {"ta_keywords": "transfer learning setting;nonconventional regularities;regularities;data;methods;new ones;performance", "pdf_keywords": ""}, "7cbb56da008163df09d254f85b7165f11389f298": {"ta_keywords": "asynchronous asynchronous asynchronous protocol;asynchronous protocol;asynchronous asynchronous asynchronous asynchronous asynchronous regime;argument comprehension;process;computational investigation;results", "pdf_keywords": ""}, "3f16887a8e5c81aea554c7a266b08ea70dd2aa9a": {"ta_keywords": "cooperative environments;cooperative environment;reinforcement learning;art qed solver;mokshin algorithm;competitive behaviour;agents;mixed environments;kuramoto;environments;novel method;method;variation;state;variety;type", "pdf_keywords": "cooperative decision making;cooperative learning models;cooperative learning environment;selfish agents;cooperative policies;cooperative environments;cooperative cooperation;cooperative environment;cooperative learning environmentwe;optimal reinforcement learning strategy;exclusive learning;cooperation;selfish incentives;selfish payoffs;reward structure;reinforcement learning;common reward;optimal behavior;optimal behaviour;decentralized decision process;social rewards;natural selection rules;prosocial agents;reinforcement learning process;reinforcement learning experiment;training agents;critic framework;optimal strategy;policy gradient techniques;critic frameworks"}, "a6aed0c4e0f39a55edb407f492e41f178a62907f": {"ta_keywords": "rich knowledge graph;rich knowledge;text;robotic approach;generation;human;intrinsic properties;approach;gc;tg", "pdf_keywords": "unstructured knowledge representations;novel background knowledge graph;background knowledge graphs;unstructured text representations;contextual text attention;graph attention;text description generation;encode knowledge;unstructured text;new entity representation;more accurate link predictions;context representation;link prediction;natural language;natural language processing;biomedical entities;informative text;unstructured data;structured data;reference attention;linguistic structure;reference reference attention;text;entities;knowledge;entity;human papers;nlp;paper abstracts;paper robot"}, "5695847f8ffbb3da078842c3683ef74175eb59e5": {"ta_keywords": "japanese speech synthesis method;prosody correction method;synthetic speech;speaker individuality;incorrect phonetic sounds;phonetic sounds;multiple speakers;un consonants;various english prociency levels;correction method;spectrum;naturalness;method;preservation", "pdf_keywords": ""}, "37a0f28f6aa41028e64d0440001ff525d67c1305": {"ta_keywords": "multiagent resource allocation problems;resource allocation problem;constrained round robin;allocation;flexible algorithm;efficient algorithm;algorithm;uniform assignment;uniform welfare;welfare level;oracle;crr;performance comparison;original one", "pdf_keywords": "fair allocations;fair allocation;optimal allocation;maximal allocation;best possible allocation;fair allocation whenwe;minimal allocation;maximization allocation;robin allocation;common allocation;allocations;allocation;envy property;maximal utility;allocation problems;new allocation;allocation process;resource allocation;uniform sequential allocation;indivisible goods;possible resource allocation;maximum total utility;constrained round robin;envy;same capacity constraints;flexible algorithm;efficient crr algorithm;efficiency goal;fairness;agent system"}, "7bdb04ba2da682e4c0b19b5d61e999d648826edd": {"ta_keywords": "sparse covariance matrix;additive noise;measurement vector;ck measurements;small constant;components;algorithm;wi;simplifying assumptions;large fraction;message;problem", "pdf_keywords": ""}, "d8c1eb86cc4546e4355ed368d8400d7640926cee": {"ta_keywords": "novel probabilistic clustering method;new deterministic clustering algorithm;nonparametric hierarchical model;clusters;probabilistic model;underlying distribution;side information;data instance;information;variance asymptotics;side;model;strong results;discrete number;experiments;results;one", "pdf_keywords": "probabilistic clustering;novel probabilistic clustering algorithm;clustering;novel multivariate clustering algorithm;more general clustering structures;novel clustering model;novel clustering algorithm;clustering model;clustering results;pairwise cluster constraints;new deterministic clustering algorithm;empirical clustering problem;nonparametric probabilistic model;cluster;clustering problem;cluster components parameters;clustering clustering problem;clusters;discrete learning;probabilistic model;partial distribution models;dirichlet processes;datasets;multivariate data;dataset;data structure;side information;generalization;graphical representation;data instances"}, "fe0ec764813fbcb6b6fd77d82188e81826088103": {"ta_keywords": "discriminative objective functions;modified joint probabilities;strings;minimum error;minimum minimum minimum database;novel generalizations;difference measure;comprehensive relationship chart;negative exponential;analysis;conventional approach;common approaches;novel unified view;sum;term;observations;approach", "pdf_keywords": ""}, "659b476a10b0e676a031b1b17ebfe405c1904227": {"ta_keywords": "speech processing toolkit;end speech processing toolkit;advanced data augmentation;toolkit;new applications;novel applications;recent development;novel feature;unified architecture;integration;ability;conformer;development;art performance;end;paper;field;researchers;state", "pdf_keywords": "automatic speech separation;end speech processing toolkit;speech separation tasks;large scale speech networks;speech recognition;end speech recognition problem;speech processing tasks;end speech applications;speech separation;speech enhancement;speech processing applications;speech systems;scalable speech applications;advanced speech enhancement;automatic speech dereverberation;voice conversation;deep learning;deep neural architectures;speech translation;deep neural models;underlying speech systems;deep models;neural networks;speech;decoder architecture;neural network research network;decoder;encoder;recognition toolkit;recognition tasks"}, "f02948f2976991bb76419775f303c27fc8afb7b5": {"ta_keywords": "traxiition rule;algorithm;rules;generalization performance;generalization;sets;new method;methods;method;terms", "pdf_keywords": ""}, "62a5b47def8d21825d06f7407a505ff0b64ecb1a": {"ta_keywords": "semantic parsing;semantic parsing framework;free grammars;ambiguous input string;ungrammatical input;grammar;input sentence;synchronous context;output;meaning representation;multiple outputs;representation;scfg;scfg framework;new method", "pdf_keywords": ""}, "05b0c768ecd4a82e486923e83250ddd53bacbf67": {"ta_keywords": "pruning algorithm;algorithms;piecewise linear approximation;computational cost;approximation;tree;approaches;hybrid;public version", "pdf_keywords": "metric pruning rule;nearest neighbor search;generic pruning algorithms;nearest neighbors;generic pruning approaches;wise linear approximation pruning;pruning rules;pruning algorithm;new pruning algorithm;pruning rule;similarity search problem;distances;dimensional data sets;pruning problem;fast data indexing;dimensional data;similarity;metric data;search algorithm;fast indexing method;search data;distance;distance function;data structures;nn search;hierarchical partitioning;metrics;data points;trees;search point"}, "1d938731dfad31c09b2f58c365f630c640f2ca1a": {"ta_keywords": "active learning;strongest active learning;acoustic models;natural language processing;dynamic momentum;memory bank;training;novel framework;unlabeled data;self;al;framework;baselines;np;novel;approach", "pdf_keywords": "active learning;annotation;novel supervised learning model;language models;labeling;label efficiencywe;leverage label labeling;language modelswe;many natural language processing;label efficiency;classification tasks;aware sampling;training framework;training data;unlabeled data;learning efficiency;text classification;labels;leverage label propagation;nlp;training method;large scale datasets;target label;training paradigm;learning;robust selftraining;deep learning;aware framework;label set;learningwe"}, "ae82f831bda5681edfe40ec15de4e9d2096ea92f": {"ta_keywords": "lexical entailment;senses;clustering;entailment;multiple context vectors;words;word;algorithms;approach;new approach;effects;results;performance;use;problem;work", "pdf_keywords": "traditional entailment clustering;lexical entailment;word relatedness;tiered clustering approach;tiered clustering;clustering;word occurrence quality;clustering framework;traditional clustering methods;correlation clustering;word vectors;hierarchical clustering model;clustering algorithm;entailment definitions;word sense;word senses;similarity score;clusters;clustering problem;context vectors;collapsed gibbs sampler;multiple context vectors;other words;words;supervised method;occurrences;word;natural occurrences;terms;machine learning"}, "8c9069641876d025c66ab6800939c278b07f60a3": {"ta_keywords": "document hierarchies;document hierarchy;explicit hierarchies;hierarchy;graph data;document;documents;generative model;observed data;several groups;model", "pdf_keywords": ""}, "423044220d9642a2d5839cfb19e32171e8a16a83": {"ta_keywords": "bandit model;bandit setting;reward models;optimal policy;greedy policy;rewards;satiation dynamics;payoffs;offline world;algorithm;observation;data;arm;approach;new approach;cyclic order;problem", "pdf_keywords": "stochastic bandits model;bandits framework;stochastic agent;restless bandits;bandits;bandits literature;stochastic feedback loop;optimal strategy;reinforcement learning;stochastic dynamical system;optimal policy;deterministic dynamics;exploration strategy;reward models;backward exploration strategies;stochastic optimization problem;stochastic system;markov processes;markov process;natural selection strategy;strategy;dynamical systems;dependent rewards;greedy policy;identical deterministic dynamics;satiation dynamics;dependent reward function;satiation model;positive definite reward;biased feedback"}, "be0ad0710bfb09f6c875dd6cd834ac643713c93d": {"ta_keywords": "spin;orbit coupling strength;fermions;orbit coupling;zeeman field;substantial increase;effect", "pdf_keywords": ""}, "4a0f96bb17836b4c4d6e19627f176fba8fe05127": {"ta_keywords": "circuit breaker;circuit;relay circuit;incoming energy;energy;critical value;combination", "pdf_keywords": ""}, "1d5d170670889bd82364fbcc594dadcb5481e9e4": {"ta_keywords": "neural machine translation;translation examples;source sentences;target sentences;translation;sentence pairs;nmt;search engine;input sentence;source sides;words;process;effective method", "pdf_keywords": "neural machine translation model;neural machine translation;neural machine translation system;efficient translation;additionalneural machine translation;translation memory;translation optimization;translation system;translation process;standard translation systems;translation accuracy;translation examples;translation pieces;translation rules;theneural machine translation;translation words;translation rule;new translation pieces;source sentences;target sentences;sentence pairs;translation;neural machines;natural language;new source sentence;sentence similarities;nmt model;subword units;input sentence;text"}, "f32c67daa6a93281bd8645fc2fa423dca67aea00": {"ta_keywords": "conference peer review;reviewers;sharp minimax analysis;statistical accuracy objective;algorithm;papers;incremental max;flow procedure;statistical accuracy;score model;assignment;fairness;accuracy;correct recovery;problem;focus", "pdf_keywords": "peer review algorithm;accurate peer review assignments;fair fair peer review;new peer review algorithm;peer review4all algorithm;system assigns reviewers;paper assignment process;review algorithm;conference peer review;fairness algorithm;assignment algorithm;toronto paper matching system;peer review;paper assignment problem;total sum similarity assignment;reviewer pairs;min fairness objective;reviewer load constraints;fairir algorithm;reviewers similarities;arbitrary assignment;topic matching;peer review4all;peer review4;individual papers scores;fairness measure;reviewers;paper assignments;multiple candidate assignments;fair quality"}, "827e0def212f6834d615e4f3f25b55fe27f6460d": {"ta_keywords": "annotations;annotation;sensitive verb relations;large datasets;context;data;accuracy measure;novel approach;approach;third step;second step;step process;first step;fourth step", "pdf_keywords": ""}, "d8ad713ffde54d0a837e6a9cab4e70739d649d41": {"ta_keywords": "dialog retrieval;recursive autoencoders;recursive neural network identification technique;search;word representation;database;dynamic pooling algorithm;query;example;robustness;system performance;novel approach;user;approach;same answer;objective;possible answer", "pdf_keywords": ""}, "f45c777b29e0a00f7b1e1f33daa751853015724a": {"ta_keywords": "artificial intelligence;machine learning;beijing;workshop;economics;activities;technology;china;annual activity report;royal institute;brief summary;june;report;summary;entire event agenda;july;individual members;meeting;period", "pdf_keywords": ""}, "1f7fb2c16e51f207eb1816f4f3fc3e1649c364f0": {"ta_keywords": "robust speech recognition;speech recognition system;simulated noise levels;noise;simulation;performance;presence;enhancement;recent years;impact;section", "pdf_keywords": ""}, "83f648f01d858d02b20f9327bebb1d5e91d0b6a9": {"ta_keywords": "maximum mutual information;network optimization;conditional random fields;linear classification process;network;finite state transducer;crfs;wft;training technique;mmi;convergence;novel approach;novel method;method;effect", "pdf_keywords": ""}, "099346e2837c53ded931d98135edbb261039764a": {"ta_keywords": "computational social choice;social decision making;social choice;decision makers;algorithm;linear algebra;level system;group;machine;action;approach;ability;problem;power;new approach", "pdf_keywords": ""}, "cbdccaa4a5bceaae190f78b1ac0a0cf47391968d": {"ta_keywords": "new media;data platform;micro;new platform;data;platform;creation;development;new generation;meta;demonstration;ability", "pdf_keywords": ""}, "09fcc7ed1f867bcf9133ab12065ee7366cfaa652": {"ta_keywords": "recommendation models;fault tolerance;erasure;fault;tolerance architecture;dlrms;accuracy;efficient learning;training;xdl;system;source;scale", "pdf_keywords": "constant fault tolerance;fault tolerance framework;fault tolerance;checkpointing;fault tolerance system;fault tolerance approach;fault tolerance analysis;erasure coding;deep learning models;replication;deep learning pipeline;possible deep learning models;deep learning model;large data storage systems;erasure codes;simple erasure codes;deep learning;parallel storage;data storage;featureless storage mechanism;consistency;erasure;recommendation models;storage system;less memory;massive data;stateful optimizer;large models;accuracy guarantees;proactive redundancy"}, "8d939637b3a5ecf681130619cd35f295dbb9db03": {"ta_keywords": "venous thrombosis;single nucleotide polymorphism;healthy patients;rs710446;vvt;risk;effects;systematic investigation;vt;results;sample", "pdf_keywords": ""}, "0bcd8210e9b90b33ab8467b94fd9b9511aad0f86": {"ta_keywords": "best performance network architecture;ubiquitous multiprocessor;ubiquitous parallelism;processing architecture;pervasive applications;processing architectures;networking;network development;network;network transmission speed;processing;dmp;architecture;performance;novel architecture;technology;hybrid;experience;method", "pdf_keywords": ""}, "cece2d2f7cc38a512325122401f8aa658121b80e": {"ta_keywords": "deceptive partners;deceptive partner;deception;dialog;intelligent way;method;computer;task", "pdf_keywords": ""}, "4bc9d6596069c9277b57a7ee1e1127d231f28663": {"ta_keywords": "unsupervised parsing;financial market;autoencoder;soft argmax kernel;data;new algorithm;algorithm;tuning;performance;state;fine;art;combination", "pdf_keywords": ""}, "dbb4035111c12f4bce971bd4c8086e9d62c9eb97": {"ta_keywords": "convolutional network;mobile phone datasets;global poverty;graph;countries;different prediction tasks;use;algorithm;study;method", "pdf_keywords": "structured multilayer graph;dimensional network;graphs;input graph;graph topology;graph;convolutional network;wealth prediction;convolutional networks;manifold ranking;node classification;embedding;country datasets;scalable learning approach;prediction tasks;feature representation;prediction;consistent learning;mobile phone datasets;neighbors;classification tasks;nodes;machine learning;convolutional neural network;multiple views;data;human mobility;generalized node;neighborhood;layer network"}, "8376b18a4dd228ea4c33d606b32b081cee9bf80a": {"ta_keywords": "stochastic optimization problems;free algorithm;stochastic;stochasticity;complexity;gradient;algorithm;decision variable;dimension", "pdf_keywords": "stochastic convex optimization;free stochastic optimization;stochastic stochastic convex optimization framework;stochastic optimization;stochastic stochastic optimization;stochastic optimization problems;stochastic oracle;free convex optimization;stochastic optimization problem;stochastic gradient;convex optimization;free optimization;stochastic approximation;free optimization methods;stochastic algorithm;stochastic noise;stochastic function;stochastic version;stochastic problems;noisy oracles;stochastic;better complexity bounds;underlying optimization problem;additive noise;point feedback oracle;noisy objective value;stochastic differential equations;stochasticwe;stochastic process;optimal point"}, "1e5b826ddf0754f6e93234ba1260bd939c255e7f": {"ta_keywords": "knowledge distillation;best translation quality;distilled data;models;tnp model;complexity;optimal complexity;performance;capacity;strong correlation;data sets;output data;tnp;variations;several approaches;findings;effect", "pdf_keywords": "automatic machine translation models;neural machine translation model;adaptive translation;translation model;small sequence generation models;machine translation tasks;machine translation;ofneural machine translation;pretrainedwe study translation translation;autoregressive models;translation quality;adversarial defense framework;translation uncertainty model;parallel corpus training;translation protocol;autoregressive teacher models;translation uncertainty;unsupervised generation;generative flow model;knowledge distillation;machine learning tasks;models;training data;translation;iterative refinement;neural model;knowledge distillation process;model;neuralwe;knowledge"}, "41d4763792db8ea420efcfbd112a55deec971fee": {"ta_keywords": "commonsense knowledge;knowledge;relevance;available data;simple empirical process;observation;individuals;framework;large number;relationship", "pdf_keywords": ""}, "4cc97c3858b558b4fa80ad73a894fcc7df841114": {"ta_keywords": "proc;diagnostic;characterization;sci;acad;novel", "pdf_keywords": "group fairness definitions;group fairness notions;group fairness incompatibility;group fairness;linear group fairness definitions;fairness criterion;other group fairness notionswe;multiple fairness constraints;linear fairness constraints;fairness_confusion tensor;fairness notions;fairness optimality problem;fairness matrix;fairness gaps;fair classification;fairness;fair models;fairness trade;discrimination;predictive performance;disparate representations;favorable groups;different groups;independency constraints;performance metrics;groups;representative groups;bias optimization problem;individuals;equality"}, "72d89aa7cd77c3f22a667f2b0707758eb8d52a7a": {"ta_keywords": "new translation model;translation model;ambiguous words;attention;attention strategy;context;word;agreement;model;accuracy;strategy;novel strategy", "pdf_keywords": "aware translation models;aware translation accuracy;aware machine translation models;machine translation;aware translation;higher translation quality;neural machine translation;contrastive translations;novel translation model;human translators;translation quality;lower translation quality;kernelneural machine translation;professional translators;professional human translators;disambiguation system;correlatedour translation translation;machine translation techniques;powerful machine translation technique;conventional translation;translation metrics;ambiguous translations;14k translations;attention regularization influence alignment;translation;context words;attention regularization;correct disambiguation;attention regularizationwe;novel translation kernel"}, "d170bd486e4c0fe82601e322b0e9e0dde63ab299": {"ta_keywords": "neural language models;neural language modeling;softmax;adaptive representations;representations;several popular representations;variable capacity;benchmark;accuracy;performance;new class;grave et al;work;high degree;approach", "pdf_keywords": "adaptive word representations;neural language modeling;adaptive softmax output words;adaptive input representations;model words;adaptive softmax;ground state entropy;word models;attentional architecture;large vocabulary models;random matrix theory;word representations;ground state energy;adaptive embeddings;adaptive inputs;variable sized input words;adaptive input;meta meta metawe;meta metamaterials;representations;models;square lattices;square lattice;ground statewe;hamiltonian formalism;model;discrete memory;accurate modelswe;large embedment models;learning capacity"}, "3d3f01feee0dd3eea22e390c80deaadc6f11eb9a": {"ta_keywords": "end speech recognition system;single channel encoder;convolutional neural network;single channel end;multichannel;word error rate;end;mmi;lf;data;best baseline;experimental results;system;study", "pdf_keywords": "end speech recognition;end multichannel speech recognition system;cellular automaton speech recognition task;speech recognition tasks;speech recognition model;attention model;speech signals;end encoder;single channel endto;fifth chime challenge;convolutional neural network;audio features;multichannel input;deep convolutional neural network;encoder;multichannel;joint signal;channels;variance communication model;residual learning;attention;signal processing;joint ground state;parallel encoder;cnn;decoder;recognition;inputs;multichannel standard data base;minimum threshold encoder"}, "b2fd7297f7681f9e3ea860cecf1ec97b2cc8ccc3": {"ta_keywords": "harmonic trap;dimensional harmonic oscillator;harmonic oscillator model;harmonic field;oscillation;oscillator;amplitude;frequency;dynamics;study;predictions;results;agreement", "pdf_keywords": ""}, "ff783c4709a095cc581534fec58ef9515613ebc9": {"ta_keywords": "powerful model explanation paradigm;right reason", "pdf_keywords": "catastrophic forgetting;catastrophic forgetting phenomenon;continual learning;continual learner;continual learning process;incremental learning;episodic memory;gradient class activation;backpropagation;memory;vanilla backpropagation;machine learning tasks;neural networks;remember;explanation loss;deep learning model;model explanations;deep network;explainable artificial intelligence;deep neural network;learning problem;saliency;various popular explanation methods;saliency method;feature map activations;explanations;prediction;saliency maps;class activation mapping;new saliency method"}, "f2e7598464a0b9376771ffc4ba243233ee12c677": {"ta_keywords": "sememe knowledge base;external context information;sememes;hownet;internal character information;words;frequency words;large margin;novel framework;framework outperforms;art baselines;meaning;framework;robust performance;state;experiments;experiment", "pdf_keywords": "lexical sememe prediction;subword prediction framework;popular sememe annotation system;word embeddings;sememe prediction framework;sememe prediction;novel method sememe prediction;annotation efficiency;sememe prediction method;sememe embeddings;word structures;word vectors;sememe knowledge base;word similarity;internal character information;prediction accuracy;linguistic patterns;arbitrary word meanings;similar sememes;word occurrences;sememes;various word frequency scenarios;joint knowledge embeddings;prediction;frequency words;target word;external context information;words;character filtering;compound words"}, "48b18bf5c9cad0e4c36b2d885f380c5c637e1a09": {"ta_keywords": "variational autoencoders;generative models;novel probabilistic model;regularization;representations;latent space;constraints;sudakov;model;art methods;experiments;state;respect;superior performance", "pdf_keywords": "unsupervised disentanglement learning;supervised disentanglement;deep generative models;variational autoencoders;generative coding;generative models;generative model;conditional learning;novel generative model;deep learning;generative process;deep learning paradigm;optimal conditional priors;deep learning dataset;full disentanglement;deep learning environment;deep learning protocol;superior disentanglement performance;regularization;latent representations;unsupervised methods;optimal conditional;unsupervised setting;noisy datasets;latent representation;learning;large regularization terms;representations;unlabeled data;inductive biases"}, "ee5dc631a682696a4704b742ea087e8abb5df897": {"ta_keywords": "multimodal autonomous speech recognition;image processing pipeline;signal processing pipeline;self;novel framework;data conditions;framework;wide range;art;state;performance gap;combination", "pdf_keywords": ""}, "162c3cf78af48ddf826ec76a1a3767a88a730170": {"ta_keywords": "orbit interaction;spin;orbit;coupled system;dynamics;enhancement;effect;suppression", "pdf_keywords": ""}, "c8a5d05cb741b3448ec4106d2006ae24a7a401b4": {"ta_keywords": "sequence emission regularization;automatic speech recognition;latency regularization;sequence probability prediction;asr models;level emission regularization;latency;asr;sequence;fastemit;alignment information;model;novel method;variety;method", "pdf_keywords": "sequence transducer models;transducer regularization method;automatic speech recognition;transducer loss;speech recognition model;acoustic acoustic sensors;transducer;transducers;word error rate;lower emission latency;fast emission;backward propagation algorithm;sensors objective ofwe;sequence probability;level regularization method;signal processingwe;sensor;additional word alignment information;neural network;streaming advances;hypothesized words;fastemit;latency;emission delay;low latency scenario;sensors;recognition accuracy;signal processing;forward probabilities;pattern prediction"}, "9918ea4b68e90e1257953b6f2665b2ce29f2bc8b": {"ta_keywords": "nonlinear beamforming;nonlinear signal;nonlinearity;nonlinear systems;nls;challenge;front;challenges;american physical society;ability;eds;end;paper", "pdf_keywords": "nonlinear signal processing;nonlinear beamforming;nonlinear beamformer;nonlinear signal processing systems;beamforming enhancement;3d speech enhancement task;beamforming;speech enhancement;noisy speech signal;overall speech enhancement;beamformer;overall speech enhancement error;underlying beamformer;quantitative imaging system;scalable waveform source separation;parametric neural network;signal processing channel;speech recognition;signal processing tasks;nonlinearitywe;nonlinear complex spectral mapping;quantitative quantitative imaging;target signal misalignment;time signal processing;deep learning;signal processing process;noise ratio;nonlinear dynamical systems;dynamical nonlinearity framework;nonlinearity models"}, "30109a213aa10765486c676ecfa511db227ab543": {"ta_keywords": "neural machine translation;nmt;training process;simple shuffling;training;efficiency;models;length;crucial step;large effect;strategies;paper;choice;work;effect", "pdf_keywords": "neural machine translation;neural machine translation system;training corpus;translation tasks;translation process;neural machine;corpus;largescale machine learning;neural networks;reliably training models;translation;minimum training;nmt;sentences;language pairs;training process;training speed;sorting;sorting strategies;toolkits;larger miniwe;training;training curves;lamtram toolkit;machine learning system;machine;lamtram8;models;word;src"}, "5e6acc5c73f22c2dbbb4910f656a03cf40a2fe15": {"ta_keywords": "local interactions;interaction;dynamics;particles;strength;effect;system;finite number;choice", "pdf_keywords": ""}, "03e62d5f0265608c6ebdebba0870131b056b79a6": {"ta_keywords": "harmful online content;harm;severity;vulnerability;card;scale urgency;interviews;activities;agency experience;participants;analysis;perspectives;theoretical framework;types;course;months;sphere", "pdf_keywords": "harmful online content;online harms;online harm;harmful content;content moderation;content moderation approacheswe;upsetting content;online social media platforms;online harassment;online social media;social harm;online content;social media content;moderation practices;moderation efforts;harm mitigation;onsocial media platforms;appropriate moderation;moderation;online communities;social media;relational harm;harm;facebook;online platforms;social computing researchers;popular popular popular online platform;platform policy;social media domain;onlineharassment"}, "33ce3cd897a3473973f338c154f3fe5c1175643c": {"ta_keywords": "virtual knowledge bases;virtual vbf;prior vbf methods;language model;openicatedicate language;vkbs;domain icatedicate;open domain;external memory;text;tasks;domain question;opicatedicate;method", "pdf_keywords": "virtual large knowledge base;virtual knowledge base;knowledge embeddings;virtual knowledge;relation encoder;domain query language;relational knowledge;structured knowledge;domain queries;domain language model;natural language;relation mentions;structured supervision;natural language systems;large scale machine learning tasks;entities;relicative logic;external memory;encoder;database;topic entity;different query tasks;relation weights;knowledge;domain learning problem;relations;relation;text corpus;query;learning"}, "b00bc4dcce60e7c631a23d60894e51001de1c630": {"ta_keywords": "vesicular stomatitis virus;pseudotype virus;vv protein;protein pair;protein;discovery;functional form", "pdf_keywords": ""}, "2583e7e279e2969493c3290c8f300ab32da40bf9": {"ta_keywords": "entity mentions;knowledge base entries;languages;resource scenarios;machine learning;models;simple model;model;improvements;robustness;generation;disconnect;recent advances;problem;paper", "pdf_keywords": "crosslingual entity disambiguation entity;crosslingual entity;multilingual knowledge bases;disambiguation model;current entity generation methods;entity pairs;entity mentions;information extraction;low resource languages;disambiguation;disambiguation errors;entities;linking;entity names;entity;textual data;resource xel;source language;arbitrary knowledge bases;scientific literature;single entity;correct entity;linguistic structure;previous xel candidate generation models;reference text;same entity;wikipedia;linguistic model;translation accuracy;empirical compilation"}, "0bb30ed3340d2c34fe9f37c5002929bc5f458c23": {"ta_keywords": "bert architecture;biomedical literature;relation;neighbor tokens;novel sensor;extraction mechanism;novel neighbor;attention;novel method;novel network architecture;attenuatement mechanism;method;power", "pdf_keywords": "biomedical relation extraction tasks;relation extraction task;relation extraction;biomedical text;biomedical literature;biological text;bert architecture;biomedical knowledge;sentence representation;bert;large datasets;whole text;disease classification task;deep learning;global attention problem;novel graph;gene;sentence information;novel onal encoder;genomic variant information;biomedical object;ary relation informationin;gene expression;genes;relation;relations;graph information;attention mechanism;entities;novel approach"}, "9bbc8ca94810e8a21e4a6a55a5913c5b0b6c787f": {"ta_keywords": "line speech transcription;initial automatic transcript;smaller utterances;re speakers;confidence filtering;correction efficiency;supervision effort;method outperforms;speakers;method;terms;experiments", "pdf_keywords": ""}, "7e0570f498a5de4f2a861546d4e67ba208f71d12": {"ta_keywords": "speaker recognition benchmark", "pdf_keywords": ""}, "1410f7d9470a24fb4055c6685c2dda758b9d995f": {"ta_keywords": "such coevolving network game;adversarial fashion;games;agents;competitive settings;conservation laws;dynamics;polynomial time algorithm;average behavior;theoretic flavor;time;information;class", "pdf_keywords": "evolutionary game theory;evolutionary games;game theory;game dynamics;stochastic games;replicator dynamics game;polymatrix games;dimensional stochastic game;evolutionary dynamics;sum polymatrix games;evolutionary learning dynamic;polymatrix game;sum polymatrix gamestheorems;static polymatrix games;sum games;archetypal game;replicator game;evolutionary algorithms;more general dynamical systems;time evolution;novel evolutionary behaviors;typical static game formulation;strategy equilibrium;sum game;minmax games;network generalizations;dynamical systems;information theoretic;arbitrary strategy profileswe;replicator trajectories"}, "e8e62a80c7355bcf5dbc9fabafff4025e00cf540": {"ta_keywords": "linguistic structures;linguistic structure;plausible lexicons;novel nonparametric model;discrete structure;induction;model;equations;set;large number", "pdf_keywords": ""}, "4ef46d5daf6a7a9536e2ebe3c7aa2296bffcf43e": {"ta_keywords": "infinite hmm;stochastic markov chain model;tagging;unsupervised part;speech;pos process;pos;hmm;ihmm;context;infinite version;model;extension;performance;properties;simple experimental example", "pdf_keywords": ""}, "467b14cc8337dd7efe1d374f9a7feb90ae9d2c12": {"ta_keywords": "probability distribution;probabilities;distribution;distributions;method;calculation;new method;sum", "pdf_keywords": ""}, "63a604942f1238e9678aebd697a2379981e9a20a": {"ta_keywords": "student feedback;student performance;student learning;study;performance;simple model;results;effect;significant factor;use;paper;factors;impact", "pdf_keywords": ""}, "db392858262b17aa9c8ff8659738f68fbf832ebe": {"ta_keywords": "abstract syntax tree;arbitrary code;programming language;program;conditional probabilities;probability;nodes;product;model;new approach", "pdf_keywords": ""}, "040a1abdbef2a0e087a586d719259c32c95bfc78": {"ta_keywords": "belief propagation;computing;log;reward;particle;linear system;actions;model;novel approach;rules;set;approach;space;outcome;problem", "pdf_keywords": ""}, "e1a20480e4168d58deec743035b7ff02720672d7": {"ta_keywords": "automatic speech recognition;nonlinear dispersion models;connectionist temporal classification;open vocabulary end;hybrid attention;known words;asr;words;end;nl;nls;character;novel method;method;effectiveness", "pdf_keywords": ""}, "a13d9c8e5a2fc028ad597e2bd46a9c60aca0ede4": {"ta_keywords": "speech synthesis system;synthetic speech;expressive speech synthesis;prosody modification;prosody;target speaker;speech;novel method;generation;quality;medium;method;hmm;order;user;users;means;report", "pdf_keywords": ""}, "d26a7a86013b3be57acc0f5df73393cab7c302d9": {"ta_keywords": "stochastic particle methods;stochastic particle;diffusive systems;optimal interventions;deterministic framework;necessary controls;novel framework;shot;framework;solution", "pdf_keywords": "stochastic markov chain smoothing;markov chain smoothing;stochastic markov chain markov processes;stochastic control;markov chain smoothing method;stochastic stochastic markov processes;stochastic optimal control framework;robust stochastic control framework;stochastic optimal control;stochastic control problem;efficient stochastic controls;stochastic flows;stochastic optimal control problems;markovian stochastic systems;markov chain markov processes;markov chain approximation;stochastic optimal transport;stochastic dynamics;stochastic partial differential equations;stochastic differential equations;stochastic systems;stochastic particle dynamics;markovian models;stochastic models;stochastic principle;standard stochastic principle;hidden markov models;dependent drift;dependent scalar diffusions;optimal feedback control"}, "73c401e29cb83157bc6dfb33d5ce4364a7d2731b": {"ta_keywords": "domain generative models;shot generation problem;shot model;shot problem;domain distance consistency loss;source domain;realistic images;target domain;large domain;model;novel approach;essential features;cross;previous methods;different aspects;information", "pdf_keywords": "shot image generation;shot image distributions;arbitrary image domains;conditional generative adversarial networks;generative adversarial networks;generative adversarial network;corresponding images;hidden model learning;source models;realistic pictures;realistic images;real images;few training samples;adversarial network;source model;underlying source model;unrelated source domains;source domain;simple simulated images;low fidelity;large source dataset;gans;large source domain;arbitrary target distribution;images;target distributions;real caricatures;small target domain;training data;caricature domain"}, "4702bfd200ceb6de126a60afb4db9da5c413476e": {"ta_keywords": "acoustic models;acoustic model;acoustic score distribution;deep neural network;uncertainty;observation uncertainties;unscented transform;simultaneous propagation;simultaneous simultaneous propagation;accuracy;distortion;propagation;layers;novel framework;knowledge;framework", "pdf_keywords": ""}, "b946ce2c3405969bf615bedc623845b0d3d9b010": {"ta_keywords": "novel chunkwise attention;automatic speech recognition;attention;attentions;single chunk;decoder layers;asr;input sequence;online end;inference algorithms;mocha;training;end;system;novel approach;novel class;evaluations;approach;multiple connections;unique properties", "pdf_keywords": "global random access encoder;end speech recognition problem;dimensional quantum information processing;encoders;speech recognition systems;novel online decoder;monotonic chunkwise attention;noisy encoder;short speech sequence;decoder;encoders arewe;decoder layers;deep random matrix models;transformer decoder;stochastic language model;automatic speech recognition system;attention heads;attention;attentions;novel context inheritance mechanism;target attention;previous context block approach;nonlinear oscillators;random access patterns;input sequence;chunk;stage speech;corpus;input;pnas"}, "3ad287cf3b17cb109bf991731d2c0dcf8b7db2b1": {"ta_keywords": "morphological tagging;tag sets;syntactic traits;languages;neural network representations;factorial conditional random fields;language;neural network potentials;words;surface forms;overlap;paper;model;expressive power;information sharing;assumption;novel method;superficial differences;powerful technique", "pdf_keywords": "morphological tagging;morphological tags;morphologicalmorphological tagging;label languages;computational linguistics;tagging;natural language processing;corpus;natural language processing context;syntactic traits;low resource languages;languages;several languages;morphology;resource language;machine learning tasks;language;tags;tag sets;neural factorization;full tag sets;un tagging;generic language pair;languages bywe;deep learning;neural network framework;deep learning framework;random tag;syntax;labeling problem"}, "27e1dbe9f7c71cd6cc1b0357f49aef497e572d09": {"ta_keywords": "statistical machine translation;source code;sm;framework;method", "pdf_keywords": ""}, "1abd1efae8c3849e28de926e52d166b7800965a1": {"ta_keywords": "incomplete rank lists;aggregate aggregate rank lists;aggregation;tripadvisor review data;various real life data sets;accuracy;ties;best performance;empirical analysis;method;techniques;new method;paper", "pdf_keywords": ""}, "d513a3583bd168ee341ce3b26d54a4e4096da471": {"ta_keywords": "data storage systems;latency performance;copies;code;minimum number;case;variance", "pdf_keywords": ""}, "1d634d645bfe0b289fd6a2a0d9210b2a04c9237b": {"ta_keywords": "private learning;large nonlinear models;parallel learning;private baselines;models;modest memory cost;model;optimal nature;high accuracy;novel approach;depth;approach;problem", "pdf_keywords": "differential neural models;neural language models;novel differential neural model;private learning models;natural language models;large language models;deep learning models;neural language model;deep learning;large pretrained language models;sentence classification;private learning;natural language tasks;large neural networks;differential privacy condition;large generative generative models;modest privacy budgets;dense dense dense dialog generation task;language models;generative models;neural networks;gradient models;natural language processing;better private fine;private data;generative generative models;polynomial models;generative model;dense dense text;important hyperparameters"}, "5ee580fba44c6efb2a9b06f4c62de6b053db7784": {"ta_keywords": "review reviewers;lp losses;loss function;hyperparameters;aggregate mapping;reviews;social network;vectors;matrix;natural axiomatic properties;ijcai;framework;extension;only choice;standard class;approach", "pdf_keywords": "peer review;individual reviewers;reviewers opinions;reviewers;reviewer;rating system;several reviewers;recommendation function;reviews;review;aggregation methods;criteria scores;aggregation;aggregate score;scientific reviews;individual opinions;subjectivity;machine learning;aggregation process;recommendations;overall scores;aggregate mapping;consensus mapping;criteria;desirable outcomes;aggregate function;generalization;opinion;opinions;convex objective function"}, "45dcccef42ed09cfd2babb630c117e95136b35d1": {"ta_keywords": "universal dialogue system;example dialogue;semantics;schema elements;popular state tracking benchmarks;systems;system;new type;practice", "pdf_keywords": "popular dialogue state tracking benchmarks;dialogue state tracking system;domain dialogue state tracking model;natural language systems;natural language representation;conveying semantics;example dialogue;dialogue agents;simple natural language prompts;dialogue;utterances;schema representations;dialogue transformation;prompt descriptions;data mining tasks;seq2seq modeling;large language models;semantics;joint utterances;schema;schema elements;scalable schema;seq;single utterance;new service model;descriptions;service spaces;linguistic distance;data mining process;service developers"}, "6b9c3f82a0c0fd62f8ae527126b118890cfd452d": {"ta_keywords": "quantum spin;new method", "pdf_keywords": ""}, "1b06fe6ca5f4404e68b066cdea1a74a36e3e0e13": {"ta_keywords": "robotic model;unseen objects;objects;static object data;model;representation;scene;shape;presence;improvement;ability;method;success", "pdf_keywords": "robot manipulation;robot execution;egocentric manipulation data;robot action outcomes;robot;robot pairs;egocentric manipulator;robot classification;robot manipulator;robotic grasping problem;egocentric video game task;robot environments;objects;manipulator;object model;object ot;train object;object;unseen objects;auxiliary task;multiple objects;higher object data;supervised manner;mechanical turk annotators;camera;stack objects;detection;task;vision information;tools"}, "4cb3275ec95f4ad407f153aa9dc2d527bc2744e5": {"ta_keywords": "speech synthesis system;synthetic speech;synthesized speech;speech parameter generation module;input speech;own speech;target speaker;prosody;original functionality;f0 modification module;duration determination module;modules;main modules;last module;duration;system;users;paper", "pdf_keywords": ""}, "34c9e3152c9a14af711994230d8a3909daeaa7cf": {"ta_keywords": "empirical risk minimization;hyperparameters;reviews;final recommendations;different reviewers;aggregate mapping;scores;erm;natural axiomatic properties;only choice;ijcai;approach;framework", "pdf_keywords": ""}, "46f88a062df05673ae0731aa17f9f9cc9d3e87bf": {"ta_keywords": "dimensional harmonic oscillator;harmonic oscillator;second harmonic;flow;fluid", "pdf_keywords": ""}, "c4e83bfddb38642debb31097501aec8768f9020e": {"ta_keywords": "artificial intelligence models;physical systems;models;best features;new method;method;combination;problem;use", "pdf_keywords": ""}, "f5813bb0b398007cae10ffdddeab221d4b9b0dc7": {"ta_keywords": "dynamic simulation technique;simulation;dynamics;fluid;stationary state;stationary stationary state;dependent interaction;surface;time;technique;development;use;new type", "pdf_keywords": ""}, "27de5fb45af9799ed0020c978fe3a3080c60401e": {"ta_keywords": "morphokinetic annotations;fluid dynamics video;morphokinetic patterns;convolutional neural networks;neural networks;annotations;scale;use", "pdf_keywords": ""}, "bab35e88a510938d22cb28f2ecc6f6e189c3d8ea": {"ta_keywords": "2d electron gas;electron dynamics;electron gas;harmonic trap;electron density;2d;effect;variation;presence", "pdf_keywords": "end speech recognition;large vocabulary continuous speech recognition;human speech recognition;frequency speech recognition;speech recognition;automatic speech recognition;human speech data;dnn system;modular hmm;modular hybrid neural neural network;deep neural network;deep neural networks;acoustic transformer;end transformer;speech segment;linguistic transformer;e2e transformer architecture;transformer;neural network;neural networks;e2e transformer;performance transformer;encoder model;encoder;conventional modular segmentation system;efficient transformer;convolutional neural network;acoustic model;modular hidden model;sequence encoder"}, "0132cb4384c3a6402353d8f349f8dd450d8ea4a2": {"ta_keywords": "normalization task;historical documents;spelling;network;novel method;model;art performance;variety;cases;state", "pdf_keywords": "annotated text sequences;term memory networks;term memory model;linguistic annotation;neural machine;term memory units;modern german text;normalization;texts;text;annotations;deep nonlinear neural network architecture;normalization algorithms;character sequence;additional normalization data;body normalization task;historical text;embeddings;anselm texts;historical words;character alignment;early new high german;original text;level spelling;encoding;wordforms;baseline;order encoding;terms;novel approach"}, "39e734da43eb8c72e9549b42e96760545036f8e5": {"ta_keywords": "dialogs;reading comprehension architecture;text;crowd workers;content;model context context;questions;information;reference models;available datasets;number;14k;quantitative model;results;quantitative analysis;novel approach", "pdf_keywords": "simple reading comprehension dataset;dialogs;machine comprehension;14k dialogs;teacher dialog;annotated articles;dialog;frequent dialogs;dialog context;many annotations;wikipedia text;answerable questions;natural language;short excerpts;subsequent question collection;hidden wikipedia pages;interesting content;dialog acts;content;natural language rules;comprehension;text;questions;students;student;linguistic structure;next answerwe;task;people;teacher"}, "68b3905c2f82814294631f2ce29d5be4165e6b1f": {"ta_keywords": "wireless relay nodes;wireless relay network;optimal sequential deployment;optimal policy structures;random length;nodes;communication link;packet source;distribution;sink node;objectives;ii;line;person;establishment;problem;cases;illustrations", "pdf_keywords": "impromptu relay placement;wireless relay networks;wireless relay nodes;several sequential relay placement problems;relay nodes;relay placement;relay selection strategywe;wireless relay network;impromptu multihop wireless network;relay propagation;wireless networks;relay deployment;relay channel;relay;relay performance;optimal policies;wireless network;relays;optimal policy;network model;lone packet traffic;necessary transmit power;optimality;optimal placement;threshold policies;multihop relay;network parameters;optimal cost function satisfies;arbitrary propagation rates;optimal cost function"}, "dc8ebb6d9908542ae474dc2b21bfb6a14216f678": {"ta_keywords": "parallel translation models;large multilingual translation models;single single high performance;best available performance;graphics card;performance;scalable approach;parallel;competitive evaluation;input device;node;node approach;speed;art competitors;state;approach", "pdf_keywords": ""}, "010df54445ab5f47582eb668dc3488a3e46b55d3": {"ta_keywords": "unsupervised learning;unsupervised generation;generative model;large amplitude;models;states;state;novel approach;art approaches;method;approach", "pdf_keywords": "unsupervised hidden models;unsupervised learning;unsupervised tasks;generative models;supervised part;neural language modeling;generative information;tag induction;supervised learning;deep learning;novel probabilistic model;probabilistic learning;neuralization;neural networks;machine learning;generative random initialize distributions;noisy context;accurate representations;models;convolutional neural networks;predictors;neural network;markov model;novel probabilistic measure;linguistic structure;svm;simple model;model;machine;sentences"}, "90b9d19af75c86f42865052c21305c70f884b5fe": {"ta_keywords": "selfish routing game;congestion costs;congestion;urban transportation networks;equilibrium behavior;equilibria;uncertain users;uncertainties;user;cost;different estimates;distinct impacts;drivers;multiplicative factor;different directions;results", "pdf_keywords": ""}, "124385efee78010a4408329dffea4798f5a1ad47": {"ta_keywords": "translation unit segmentation;simultaneous speech translation;translation systems;translation;target languages;phrase table;english translation;english;japanese;less delay;sentence;delay;probabilities;information;points;parameter;relationship;method;source", "pdf_keywords": ""}, "0eac6cbd150b1a7e9d757ccc871eea2bf0d89e42": {"ta_keywords": "soft labels;supervised learning;labels;deep neural networks;standard machine learning tasks;learning;probability distributions;more complex relationships;metrics;relationships;representation;shelf data;novel method;practice;competitive performance;method;presence", "pdf_keywords": "soft ordinal label representation;deep learning;ordinal regression;cnn;associated soft ordinal label;deep neural networks;ordinal regression approaches;global recognition;deep learning framework;multivariate ordinal classification approach;deep neural network architecture;ordinal labels;standard ordinal classification algorithm;classification task;segmentation networks;ordinal relationships;convolutional neural network;recognition;discrete ordinal relationships;image classification framework;classification problems;rank likelihood;computer vision;soft probability distributions;different classification;category;pairordinal regression;simple quadratic learning architecture;higher rank;datasets"}, "59f3e3cad309eb4965d67773d68bc2f91b2e376f": {"ta_keywords": "speech translation corpus;endangered language;corpus;language documents;highland puebla nahuatl;central mexico;documentation;machine learning", "pdf_keywords": ""}, "2ac98a28fdae4c01a89f09393c736e72445a4c4e": {"ta_keywords": "quantum networks;quantum network;quantum computation;quantum information processing;quantum information;scalable architecture;communication;processing;concept", "pdf_keywords": ""}, "da660ca9e6fedefe815e305efd0dcd3bf9b4bb60": {"ta_keywords": "relation extraction process;entity recognizers;entity filters;salient features;novel approach;order;consistency;preciseness;performance;approach;test cases;set;magnitude", "pdf_keywords": ""}, "2078d466766b6876d73ac1981392fa8bd2b9520d": {"ta_keywords": "quantum system;quantum state;schrdinger equation;standard schrdinger equation;probability density;level atom;level atoms;particles;states;system;calculation;new method;method;exact solution;generalization", "pdf_keywords": ""}, "dcac1abd2ae5af180e51994a9c8334a6de915765": {"ta_keywords": "stochastic differential equations;constant learning rate;biased compression;convex objectives;sde;arbitrary compressions;exact optimum asymptotically;arbitrary sampling;quantization;convex;expectation;gradient;updates;complexity;general framework;variants;feedback;methods;new method;paper;error", "pdf_keywords": "stochastic gradient descent;stochastic gradient descent algorithm;stochastic methods;stochastic gradients;stochastic gradient;stochastic optimization;randstochastic gradient descent;stochastic method;stochastic optimization withwe;stochastic learning problem;stochastic differential equation method;elaborate gradient estimators;compressed communication;gradient estimators;communication compression;biased compression operators;generalized gradient approximation;stochastic differential equations;stochastic;stochastic stochastic noise model;optimal compression;arbitrary compressions;quadratic learning;quadratic learning time;iterative propagation;stochastic noise model;stochastic differential equation;sde methods;stochastic models;iterative solution"}, "6a9394e5d49c1251c0fb6d7fb0c0813d26c6a907": {"ta_keywords": "semantic parsing;semantic gaps;semantic benchmarks;scholar;data;strong performance;model;novel method", "pdf_keywords": "deep semantic parser;semantic parsers;semantic parser;semantic parsers map;language synthesis;shot parser;natural language processing;throughput paraphrases;semantic paraphrases;annotated utterances;canonical utterances;natural language utterances;compositional utterances;paraphrasing;level grammar;large corpus;syntactic constructs;arbitrary language;utterances;compositional generalization;semantic function;parallel learning pipeline;paraphrase accuracy;paraphrasers;grammar;laborious annotation efforts;deep context;supervised learning ofphrases;standard parallel corpus;meaning representations"}, "5446a8bbadc2ba2c575353b257f26abae27b3b2a": {"ta_keywords": "embedding;items;lowdimensional space;representation;comparisons;novel convex;item;users;simulations;set;point;range;form;approach;method;efficacy", "pdf_keywords": ""}, "f49ccfb32aad8e6893e8cbb037c1282572fe6e21": {"ta_keywords": "partial reconstruction model;graph characteristics;deep testing algorithm;deep confidence;graph;models;guide;novel method;method;mutation;paper", "pdf_keywords": ""}, "62dc7bdae6700c4409e6d9773d6ecb5c0fab75a4": {"ta_keywords": "approximate dictionary searching;russian dictionaries;dictionaries;natural language datasets;art indexing methods;frequent words;filtering methods;benchmark results;taxonomy;sequence;clueweb09;comprehensive survey;direct methods;methods;article;important cases;state", "pdf_keywords": ""}, "02aebef93baeef3396f3cb4468a7054067f190c6": {"ta_keywords": "canonical entities;entities;name structure;probabilistic approach;entity;nonparametric approach;database;matching;text;identification;prototype examples;model;method;set;problem", "pdf_keywords": ""}, "f262ef2f50dfcaf07dc6598f22fb9b2470b37cf1": {"ta_keywords": "information extraction;several information extraction tasks;span entities;dynamic span graphs;multiple datasets;enumeration;general framework;framework;art frameworks;different domains;state;good approach", "pdf_keywords": "relation extraction tasks;relation detection tasks;entity extraction tasks;entity recognition;relation extraction;entity extraction;entity extraction task;multiple information extraction tasks;entity mentions;different information extraction tasks;coreference links;joint entity;relation propagation layers;global contextual information;relation propagation;entity boundaries;dynamic span graph framework;relation graph;syntactic tools;entity;text spans;locallycontextualized vector space representation;dynamic span graph approach;text span;entity labels;joint extraction;dynamic span graph;span;span representations;richer span representations"}, "36906613dcef29263afe711f128da1fc916cbbee": {"ta_keywords": "random data;random sequence;relative sequence;data;shift;interest;new approach;approach;variant;invariant version;evatran package;experimental evaluation;problem", "pdf_keywords": "attention algorithm;attention architecture;speech recognition;attention block;speech recognition encoders;attention network;input features;attention;recognition performance;frame indexing technique;gaussian kernel;input feature;attentions;frame indexing;encoder;long sequence data;target attention;neural networks;kernel structure;relative position information;relative positional information;relative encoding;invariant kernel;invariant gaussian distribution;shift;relative positions;gaussian;novel feature selection method;convolutional layer;input data"}, "9a334566b79bc6c6906e2b5285d5ea50b9b99479": {"ta_keywords": "adversarial minimax game;invariant representations;invariant representation;independent image classification;free classification;better generalization;benchmark tasks;bias;task;lighting;interest;language;performance;improvement;independent generation;framework", "pdf_keywords": "adversarial minimax game;invariant representation learning framework;machine translation tasks;adversarial game;machine translation;machine translation model;invariant features;discriminative approaches;feature representation;invariant representation;representation learning problem;better representation;variational fair representation learning algorithm;unbiased representations;benchmark tasks;optimal encoder;better generalization;free classification;generative adversarial network;representations;representation;invariance;minimax game;minimax games;fair classification;encoder;ofmachine learning models;representation learning process;independent image classification;optimal prediction accuracy"}, "939dfa4dec88ca167e6572904c4ad2fcbf726f48": {"ta_keywords": "gradient flows;graphons;weighted graph;discrete time evolution;large graphs;graphs;discrete time behavior;nodes;weights;class;novel class;equivalence", "pdf_keywords": "generalized gradient flow;gradient flows;gradient flow;semiconvex functions;semiconvex functionswe;large graphs;initial measures;gradient flow curve;gradient flow inwe;natural semiconvexity;semiconvex function;exponential random graph model;random graphs;natural semiconvex;graphons;weak convergence;graphon;scalar entropy function;semiconvex;large deviation principle;stochasticwe;probability measures;asymptotic behavior;scaling limit;convergence rate;finite simple graph;quadratic random graph model;graphon family;discrete convex maps;weighted graph"}, "c377bf3ae52dee4075c1e807de9c5579d553de22": {"ta_keywords": "text recognition;european texts;dialogue conference;italy;evaluation;literature;paris;main topics;importance;significance;papers;conference;review;paper;self;best methods;september;majority", "pdf_keywords": ""}, "ea71f5f59727b63b8912c6db097ba811da41bf5b": {"ta_keywords": "stochastic propagation;nonlinear potential well;particle flow;particle velocity;particle density;particle interaction strength;particle;density;power law;function", "pdf_keywords": "stochastic particle dynamics;deterministic particle flow control;stochastic control;stochastic control systems;stochastic optimal control;deterministic particle dynamics;stochastic control frameworks;deterministic particle flow controlwe;stochastic particle networks;stochastic path integral cross entropy method;stochastic optimal control framework;stochastic systems;stochastic differential equations;stochastic path;stochastic control systems arewe;frameworkcontrolling stochastic systems;multidimensional stochastic differential equations;stochastic state transitions;arbitrary system dynamics;recent deterministic particle methods;stochastic differential equationswe;stochastic framework;stochastic system;underdamped stochastic thermodynamics;deterministic dynamical systems;stochastic;stochastic pdes;stochastic pde;stochastic stochastic markov chains;stochastic processes"}, "5a26eeda7c2ca58c2d56f1d580fbbae9eb1a19cd": {"ta_keywords": "small neural networks;elliptic pdes;neural network architecture;neural networks;gradient descent;coefficient networks;pde;boundary conditions;dirichlet;input dimension;coefficients;solution scale;parameters;solutions;representational power;iterate;parameter counts;proof technique", "pdf_keywords": "neural network approximations;small neural networks;dimensional linear elliptic partial differential equations;neural networks;gradient descent;linear elliptic partial differential equations;elliptic partial differential equations;coefficient networks;numerical approximation;neural network;linear partial differential equations;gradient descent bywe study;linear elliptic partial differential equation;partial differential equations;minimal partial differential equation;quantum networks;partial wave theorems;neurons;laplacian;approximation rates;pdes;numerical methods;approximatedwe;parametric complexity;input dimension;laplacian operator;pde;parameters;partial wave theorem;explicit formulas"}, "7647a06965d868a4f6451bef0818994100a142e8": {"ta_keywords": "aware neural language models;sequence labeling;benchmark datasets;nonlinear models;key knowledge;tasks;training process;character;novel approach;novel;framework;experiments;power;effectiveness;approach", "pdf_keywords": "neural language models;neural language model;entity recognition;novel sequence labeling framework;unannotated sequence data;sequence labeling framework;forward language model;neural language;concise language model;novel language model;language model;entity recognition problem;powerful language model;natural language;stable recurrent network;language model manner;relation extraction;underlying language structure;neural networks;level lstm unit unit;syntactic parsing;lap highway character;highway layers;machine learning tasks;sufficient annotations;highway highway;sequential learning approach;lap highway backward;lap highway nnp;unstructured word distribution"}, "e65676b43338e914ad77afd0fd6ce4bef87943a1": {"ta_keywords": "novel statistical singing voice conversion;converted singing voice waveforms;voice timbre;source singer;direct waveform modification;conversion accuracy;vocoder;target singer;conventional svc;spectrum differential;svc;technique;method;experimental results;paper", "pdf_keywords": ""}, "d29d33f3b92b447d6011606be41b64439a1da088": {"ta_keywords": "contextual bandit algorithm;novel online agent;behavioral constraints;agent;online setting;regret;constraints;decisions;feedback;guide;observation;real world data;set;case study;application domains", "pdf_keywords": "behavioral constrained thompson sampling;behavior constrained thompson sampling;modified contextual bandit algorithm;contextual bandit setting;bandit problems;bandit problem;bandit setting;bandits;behavioral constraints;constraint learning framework;computational regret problem;optimal strategy;online learning;behavioral agents;online decision making;constrained behavior;novel online agent;regret modeling;explicit constraints;learning process;generic online resource recommendation problem;constraints;online recommendation phase;model agent;exogenous constraints;agents;constraint;agents beliefs;strategy;teaching agent"}, "721d7c82b80f14246d353251837e1711824a9e60": {"ta_keywords": "speech recognition system;dereverberation module;dereverberation system;2mix corpus;dereverberation;frontend;reverb;joint separation;wsj1;framework;novel framework;performance;paper", "pdf_keywords": "end speech recognition system;end speech recognition;field speech recognition system;field speech recognition framework;unified convolutional beamformer;neural beamformer;speech recognition;wireless speech applications;end speech separation;automatic speech recognition systems;robust speech recognition;speech signal;advanced speechwe;beamforming;speech dereverberation;acoustic dereverberation;decoder output;decoder;advanced signal processing;advanced advanced beamforming;encoder;speaker signals;underlying speech model;convolutional neural network;speaker;end speech;maskformer;deep models;proton beamformer;reverberant wsj1"}, "a45c3120c077994409093771077a2d16f77674c5": {"ta_keywords": "efficient transfer learning methods;tuning methods;models;new parameter;previous methods;specific hidden states;art parameter;efficient fine;modifications;methods;tasks;unified framework;design dimensions;different methods;state;set;design;comparable results", "pdf_keywords": ""}, "62d17b6f6ad77fd71ef9954c7784700d5e316f1f": {"ta_keywords": "differential privacy;privacy;popular data protection techniques;public use;language models;protection methods;sanitization;text;social norm;meaningful notion;mismatch", "pdf_keywords": "privacywe describe;much private information;privacy preservation;privacy preserving;privacy guarantees;privacy;human privacy;private information;privacy risks;differential privacy;language models;anonymization;large machine learning models;language data;private informationdespite;natural language data;traditional privacy;large datasets;differential differential privacy;information islanguage models;large models;autonomous data sanitization;data sanitization;personal information;new training language;natural language;secret information evolves;privacy violation;generative language model;public use"}, "211a6838b9550d227ce81d0bec542ec5b70e290b": {"ta_keywords": "standard news text augmented approach;knowledge base augmented approach;news text;augmented version;augmented approach;accuracy;approach", "pdf_keywords": ""}, "cf9fa9ebbefab1877aa7a501c888a8a618c31abb": {"ta_keywords": "political blogs;blogs;blog posts;blog post;social media content;individual comments;polarity;topics;identification;models;method;results;set", "pdf_keywords": ""}, "42605dca59a3aafe2e5b33741a98dad9ba117395": {"ta_keywords": "graph walk performance;graph walk;random walks;similarity measure;personal information management;similarity;nodes;graph;links;represent objects;novel approach;objects;context;framework;form;problem;respect", "pdf_keywords": ""}, "2a94fa0de804b5efaae1a66f50c3ea96539c46b8": {"ta_keywords": "human conversation examples;best dialog example;rich dialog;natural language input;drama television;content;user query;human;query;system;dynamic fashion;design;user;users;goal;paper;experiments", "pdf_keywords": ""}, "d0895dccd61c567034d197eecfa5d7d59332061f": {"ta_keywords": "codes;optimal constructions;matrix framework;band;new product;constructions;values;paper", "pdf_keywords": "network coding;encoding matrix;minimum storage code;storage codes;storage code;matrix construction;message matrix;optimal constructions;message matrixtheorems;matrix framework;code generation;arbitrary nodes;storage network;common productmatrix framework;minimum storage;explicit matrix representation;code;matrix multiplication;message storage;codes;storage systems;generative coding;matrix nature;regeneration properties;storage;arbitrary message format;linear remapping;storage storage;matrix;partial transpose"}, "ffe6d7573bb2c4fbfac0cc474804b5b1734a1179": {"ta_keywords": "online movie recommendation agent;behavior constraints;behavioral constraints;interactive interface;constraints;online setting;observations;decisions;guide;novel system;system;set;degrading overall performance", "pdf_keywords": ""}, "40fc6e46f2921be346eacff86ce765ff5b28fbdd": {"ta_keywords": "funding rates;fit garch models;predictive model;predictive power;aforementioned aforementioned paper;relationship;behaviour;discussion;paper", "pdf_keywords": "heteroskedasticity process;heteroskedasticity;stock market;financial markets;stock market price;bitcoin derivative;futures contracts;funding rates;funding rate;spot price;stochastic volatility test;financial instruments;saddle;prices;stochastic volatility;conditional variance process;core currency;stochastic models;stochastic variable;stochasticity;stochastic processes;stochastic model;price;stochastic noise;stochastic gaussian noise;parabolic models;stochastic propagation;methodologyin;margin structure;methodology"}, "f7c9521dcd80127d6d4a72fb407e81a9c518ae8d": {"ta_keywords": "inductive learning;intensive inductive algorithms;effective concept definitions;free algorithms;knowledge;relevant knowledge;novel approach;rich literature;new approach;transformational framework;framework;paper;problem", "pdf_keywords": ""}, "737aff546a9112127d7a13a5b835e27a6e1e935e": {"ta_keywords": "speech recognition systems;input speech;unmasked context tokens;benchmark;tokens;network;novel method;variance;method;consideration", "pdf_keywords": ""}, "39fdea1c34832f9bb1644bff81f53fb8ce6b2679": {"ta_keywords": "spin dynamics;spin glass;spin;zeeman field;orbit interaction;particle;effect", "pdf_keywords": ""}, "b58e80ad8c6e6844c41535080ccbdef06bce3b6e": {"ta_keywords": "3d house simulator;simulator;navigation;manipulation;support", "pdf_keywords": "agent learning environment;3d house simulator;novel agent framework;natural language languagesreinforcement learning;object manipulation;structured environment;agent;agents;navigation;unity game development engine;complex environments;navigation system;environments;reinforcement learning;natural language;visual manipulation;arbitrary actions;object;realistic 3d;room layouts;new house;simple object;actions;environment;questions;representation;patterns;chalet;task;common household activities"}, "b0efb62aa2a435704a3412d592e73faf6be5ecea": {"ta_keywords": "unsupervised learning;characters;models;latent states;relationships;empirical data;sequences;way", "pdf_keywords": ""}, "8b48c55808636a52699b38869df3eba9c8b999d9": {"ta_keywords": "statistical voice conversion;voice;digital signal processing;digital digital signal;whisper;signal processing;output signal;conversion;input signal;signal;sdes;input;form;time;method", "pdf_keywords": ""}, "a0b47c7162d1a3b04b27e27c9fadd2eabc4dab0e": {"ta_keywords": "statistical machine translation system;translation system;data languages;decoder;standard language pairs;transition;complex languages;language;gw;data;system;wide range;problem", "pdf_keywords": ""}, "c3f9c1f702d0c3b35b99502674757b3d8e7dd352": {"ta_keywords": "synthesized speech;speech synthesis;voice conversion;speaker individuality;speaker;target speaker;native japanese speakers;naturalness;hidden model;method;paper;experimental results", "pdf_keywords": ""}, "89b2a1dc68a7232bc3c68eb4b3e597f99755f7fe": {"ta_keywords": "phrase level representations;recursive neural network;rnn;sentences;bowl quiz;simple text;questions;models;qanta;best human players;word;model;dataset;answer", "pdf_keywords": ""}, "0025b963134b1c0b64c1389af19610d038ab7072": {"ta_keywords": "preference functions;web search engine;search experts;specific query expansion strategy;pattern;simple pattern;algorithm;simple algorithm;good approximation;combination;approach;domain;experimental results", "pdf_keywords": "orbit coupling strengths;orbit coupling;orbit interaction;spin;interaction;dynamics;effect;value;wide range"}, "448406c38e739695b926d112b2b7aebd4e840322": {"ta_keywords": "new speaker diarization system;automatic speaker recognition;speaker diarization information;automatic speaker identification;online meeting recognizers;speaker;reverberation;background noise;input;system;performance;goal", "pdf_keywords": ""}, "06431546c21d7c2528aaa170c2e1078e0a82d12e": {"ta_keywords": "different transfer languages;target languages;future benchmark designs;benchmark problems;shot systems;tuning;models;performance;set;immediate impact;choice;study;finding;success;priori", "pdf_keywords": "translation tasks;neural machine translation;standard multilingual benchmarks;machine translation;multilingual optimization;multilingual translation;translation task;multilingual models;transferable language;translation process;multilingual systems;target languages;multilingual optimization iswe;finetuningmachine translation;other source languages;multiple source languages;crosslingual properties;translation;source language;different languages;shot transfer;crosslingual strength;languages;insufficient parallel training data;same corpus;underlying language;language;translation errors;resource tasks;computational linguistics"}, "7570afa31c68e24fce1342b7d67c591787219bc1": {"ta_keywords": "extractive summarization;abstractive mechanism;sentences;english sentences;source documents;coherent way;combination;model", "pdf_keywords": "abstractive sentence summarization;abstractive text summarization;extractive summarization;best possible summarization score;wikipedia extract;summarization accuracy;unstructured text;wikipedia text;neural abstractive model;recurrent neural network;attention layer;corpus;idf extractor;neural attention model;decoder;english wikipedia articles;extractor;cnn;extraction;wikipedia articles;extraction method;text;convolutional neural network;attention;sequence transduction;articles;structured data;simple abstractive model;ground truth text;abstractive model"}, "4f9a4afc0ba500d839f7ee245513af9b87add8be": {"ta_keywords": "contrastive learning;underlying video;modal discrimination;similarity structure;good representations;audio;similarity;learning approach;video;feature spaces;negative samples;self;multiple instances;method", "pdf_keywords": "audio representations;audio datasets;contrastive learning;contrastive learning framework;modal similarity;modality instance discrimination;visual instance discrimination;audio clips;audio data;modal representations;sound classification;sound recognition;visual representations;contrastive predictive coding;sound classificationwe;contrastive learning defines;unsupervised learning;deep representations;withinmodal discrimination;unconstrained video;visual representation;audio;spatiotemporal feature learning;audio instances;representation learning;multiple audios;contrastive estimation;unsupervised learning technique;unsupervised learning models;audio space"}, "2fb54dfcb1a62deac6565e82f2a87919d33074da": {"ta_keywords": "spin;random variable;coupling;particle;interaction;effect;same equation;one", "pdf_keywords": ""}, "5aa3c6ab6cc55c24bab224505e8ad5a4d9863706": {"ta_keywords": "high performance text processing;novel attention mechanism;decoder training process;novel encoder;decoder architecture;encoder;attention;phoneme dictionary;grapheme;mtl;model outperforms;architecture;model", "pdf_keywords": ""}, "86db47e228167439f15ee320a8a81d386f529a0c": {"ta_keywords": "environment manipulation tasks;arbitrary environments;framework;tasks;art results", "pdf_keywords": "environment manipulation tasks;environment manipulation;sequence generation task;same generative language model;novel execution;lem tasks;arbitrary environments;synthetic data;sequence generation problems;language model;executor generation;language;environments;linguistic representation;tasks;lem task;unstructured environments;sentence level;various environments;elliptic tasks;several exemplary tasks;only synthetic data;computational;pretraining;pretraining strategy;different environments;execution;environment;interaction;task"}, "2842c21e879ee581aa50704817454f21b539fc69": {"ta_keywords": "neutral languages;languages;sentiment;language;classifiers;preference;opinion;individual speakers;expression;study", "pdf_keywords": ""}, "a010b3aa83d7d80e52c84d5f239f940eb33df904": {"ta_keywords": "multimodal data augmentation network;automatic speech recognition;acoustic input;multimodal;traditional acoustic input;separate encoders;emphsymbolic input;asr;symbolic input;mmda;attention;decoder parameters;new end;architecture;addition", "pdf_keywords": "acoustic encoder;decoder networks;encoder;end speech recognition;novel encoder;multilingual training;data augmentation architecture;encoderwe;data augmentation;data augmentation schemes;standard encoder;end speech recognition problem;additional transcribed speech;new data augmentation scheme;deep learning;deep representations;language model;pronunciation lexicons;decoder network performance;low resource languages;natural language networks;attention;multilingual systems;decoder;pronunciations;additional training data;decoder architecture;neural network;languages;new language"}, "f784ab218692364b9c8a1f8064809e4524116f3a": {"ta_keywords": "byzantine attackers;byzantine;lebwohlert attacks;novel protocol;protocol;image classification;training;language modeling;communication efficiency;theoretical bounds;resistance;practical effectiveness;presence;scale experiments", "pdf_keywords": "untrusted gradients;byzantine attackers;federated learning systems;byzantine framework;massive networks;large neural networks;stochastic gradient descent;deep learning;collective attacks;byzantine faults;aggregation attacks;training protocol;malicious peers;networks;smirnov attacks;standard byzantine model;stochastic gradients;random attacks;malicious peer;attacks;decentralized algorithm;model gradients;attacker;stochastic gradient;parallel protocols;learning;security;certain attacks;training machine;randomization"}, "2dd1504d54f8d7e01e1323a9f876f35bb86356da": {"ta_keywords": "incentive policy;monetary incentives;transportation authority;novel simulation framework;cities;agent;level disutility;nashville area;effectiveness;model;local area;data;sensitivity;case study;effect;system;approach;analysis;landscape", "pdf_keywords": ""}, "c9d65eee1b5df8ccda87c024b88e1b620099b316": {"ta_keywords": "natural language commands;unrestricted natural language commands;robots;neural architectures;instruction sequences;meaningful training data;context;complex goal configurations;communication gap;humans;blocks world;framework;instructions;problem", "pdf_keywords": ""}, "f6db40e1f0477d27a34240b2e11d6893b9e85b7b": {"ta_keywords": "air purifiers;simple hydrogen filter;air pollution;air;simple device;dangerous contaminants;minimal invasive footprint;environment;affordable way;device;ideal tool;impact;power;quality", "pdf_keywords": ""}, "9a36d6b76b3b223aa877b4243e5fdfe5c998689e": {"ta_keywords": "repulsive interaction;dynamics;strong attraction;fluid", "pdf_keywords": ""}, "3f59122d4cac12f27ad6ae379deefd9f3fa81f29": {"ta_keywords": "natural language commands;robot;scenes;representations;intermediate goals;execution;framework;fidelity;sequence;crowd", "pdf_keywords": "robot actions;natural language commands;deep learning context;possible robot tasks;robot;deep learning;high level plan predictions;learned encoder;natural language input;deep neural network;high level actions;future prediction performance;scenes;natural language;generative neural networks;interpretable plans;agents;high level plan inference;intrinsic evaluations;representations;novel representation;tasks;novel robotic system;task;prediction;specific task;predictions;plans;language command;empirical pipeline"}, "0b2ff02ab23e5c9910b98fb87c4d58045dbe89ce": {"ta_keywords": "quantitative speech recognition;reverb;features;best features;challenge;quantitative analysis;recent publication;field;novel approach;following", "pdf_keywords": ""}, "d72a1579074a1a2bc500f257474144b1957d5166": {"ta_keywords": "neural network inference;neural networks;computation framework;computations;learning;computation;source prediction;localization;slowdowns;inference tasks;image classification;speech recognition;accurate reconstruction;impart resilience;evaluation results;system;promise;unavailable results;approach;variety", "pdf_keywords": ""}, "c5141ed9ed785a6a1df61b36883e6dfa19a59ff7": {"ta_keywords": "synthetic overlap datasets;channel speech;separation;robust models;datasets;kolmogorov;mixture;data;quality;smirnov;wide variety;ss;test;procedure;value", "pdf_keywords": "speech separation;channel speech separation evaluation;large corpus utterances;standard speech separation technologies;scale utterance mixtures;realistic conversational speech;corpus utterances;scale utterances;utterances;deep learning;synthetic overlap datasets;neural networks towe;speech science;noisy utterance;separation frameworks;better separation performance;neural networks;speech;corpus;source separation technologies;neural network architecture;neural network;speaker;speaker pairs;machine learning system;blind source;additional datasets;datasets;speakers;mixture"}, "77000dba4b0638bb8f4222efcd731e040938c846": {"ta_keywords": "driver interaction;driver intention;human machine interface;driver;cognitive load;interface;car;intention;prediction;next action;actions;examples;paper;possible ways;history;order", "pdf_keywords": ""}, "ab8be9e585e599db99d8451e63a2311d88ff9293": {"ta_keywords": "cache clusters;memory cache clusters;cache workloads;popularity distribution;production traces;twitter;traffic pattern;size distribution;density environments;comprehensive analysis", "pdf_keywords": ""}, "c612905cffc5a9aa9f0d8ac7ce1fd17f90413dab": {"ta_keywords": "argumentative dialogue;neural architecture;attention model;opinion holder;argument;interaction encoding;sss reasoning;sss;interplay;challenger;parts;components;model;vulnerable region detection;relationship;goal;oh", "pdf_keywords": "argumentative dialogue;simple argumentation model;argumentation;discussions;arguments;view opinion;neural architecture;novel argument;vulnerable sentences;complementary arguments;argument;view forum2;neural networks;opinion holder;interaction encoding;deep learning;view forum;reasoning;opposing claim;good representation;sentences;interaction information;prediction layer;attention mechanism;new insight;comments;view;interaction;architecture;talk"}, "5547eff5376c56358be56f8bcc3a4b6ce4600bb5": {"ta_keywords": "robust adiabatic speech processing;robust automatic speech recognition;asr;toolkits;advanced applications;recent advances;introduction;overview;use;strengths;field;purpose;art;chapter;state;variety", "pdf_keywords": ""}, "f430c43018f17cabccd3a2e9258aff3da508afe1": {"ta_keywords": "harmonic potential;wave function;particle;dynamics;motion", "pdf_keywords": ""}, "4b34a4cc5bc9defb0f530d61f9b0f843071e227c": {"ta_keywords": "menstrual hygiene;excessive vaginal bleeding;national family health survey;incidence;india;delivery;relationship;analysis;data;study;fact;results;strong association;fourth round;high focused states", "pdf_keywords": ""}, "39025112f6a40d8aae38f2e966bb27cbc35ea25d": {"ta_keywords": "dynamics", "pdf_keywords": ""}, "0db557c4315b1e08ef65ff15b96eb7630014bf72": {"ta_keywords": "dialogue system;unnecessary utterances;typical automatic summarization method;discussion;digressions;evaluation;detector;method;performance", "pdf_keywords": ""}, "09a169c853e24b3a5196eefeab4c94eaac744cda": {"ta_keywords": "political ideology;linguistic patterns;recursive neural network;words;dataset;patterns;analysis;model;approaches;novel method;order;magnitude", "pdf_keywords": ""}, "3a72f1346f3cd41e14b45c7fba5259bc77357ed4": {"ta_keywords": "polynomial determinate clauses;quadratic determinate clauses;nonrecursive determinate clauses;learning;linear;model", "pdf_keywords": "recursive logic programs;recursive programs;ary recursive clauses;linear recursive determinate clauses;linear recursive clauses;single linear recursive clause;arbitrary closed recursion;linear recursive clause;recursive clauses;learnability problem;logic programs;depth determinate programs;learnability;learnability model;deterministic prologs;determinate programs;logic program;depth determinate program;natural generalization;various natural generalizations;depth determinate clause;boolean functions;polynomial predictability model;linear program;nonrecursive clauses;subclaused determinate clause;boolean assignment;learning problem;computational learning theory;polynomial time"}, "b62d63580b81a2cbb20c3c1593dd62d118e4cb07": {"ta_keywords": "target similarity;shot examples;example selection method;code;world languages;sql;new framework;training bank;smcal programs;framework;lite visualizations;tst;output;discovery;generation", "pdf_keywords": "semantic programs;automatic parser generation process;semantic parsing;semantic language;large language models;natural language generation;free semantic constraints;language models;semantic processing;natural language descriptions;code generation;natural language;frozen language model;language model;program similarity;semantic documents;query synthesis;incremental parser;semantic rules;efficient program synthesis;performance programming language;program synthesis;relational encoding;target similarity tuning;simple relational search;unstructured relational data models;output language;string completion engine;optimal language;sentence sequenceswe"}, "af85c67a1f30f8359be1091234118492b511a088": {"ta_keywords": "turbulent medium;zeeman field;particle;propagation;stationary medium;vicinity;effect", "pdf_keywords": ""}, "cd9e1eac4c93a314254cf8a8682ed5f01b6a808f": {"ta_keywords": "knowledge base;reasoning accuracy;popular natural language processing benchmarks;memorization;triples;kb;specific information;significant improvement;use;novel approach;approach;novel;set;art", "pdf_keywords": "knowledge representation;knowledge bases;traditional logical inference systems;generalized knowledge base;traditional logical inference;entailment tasks;deductive reasoning;entailment questions;natural language entities;embeddings;unified knowledge base;relational filtering;entities;logical query;compositional representation;representation tasks;network representations;representations;queries;representation representation problem;data structure;set representation;representation;hash functions;novel query;relationin;relation;knowledge;qe systems;encoding"}, "9712ebfbc4f86c68403f64918463edad3e553ac6": {"ta_keywords": "centralized tracking;unknown parametric distribution;kolmogorov;distribution;process;algorithms;smirnov;parameter;high accuracy;minimum;paper deals;problem", "pdf_keywords": "dynamic sensor subset selection;centralized tracking;active sensor selection;active sensing;multiple sensors;single sensor;noisy noisy sensor networks;active sensors;noisy sensor network;efficient tracking mechanisms;sensor network framework;sensor activation process;tracking;centralized learning algorithm;sensor;new stochastic algorithm;simultaneous observation;stochastic approximation;sensors;stochastic version;generalized parameter learning;noisy sensorwe;noisy sensor;general bayesian boltzmann;sampling;markov chain;basic gibbs algorithm;optimal subset;markovian chain;global optimum"}, "873dff010c00f0601d6939324929eeabb1ddbd6e": {"ta_keywords": "secret sharing;shares;networks;network;participants;algorithm;exchange;randomness;messages;high degree", "pdf_keywords": "secret sharing;secret share;collective information exchange;secret data;sharing;social networks;secret information;secret messages;communication networks;secret;share;communication network;efficient communication;general communication network;sneak;network structure;complex networks;aggregate share;nodes;simple network;graph nodes;general network;other node;shares;network;spread;communication efficiencywe;collective behavior;efficient algorithm;node"}, "35cb2b9febada179689724c78dfe31d9fa3f74c4": {"ta_keywords": "local reconstruction codes;erasure;codes;lv;new set;new form", "pdf_keywords": "local reconstruction codes;erasure codingwe;decodable failure patterns;data storage systems;storage;data storage system;linear codes;windows storage;storage system;cloud storage system;data storage device;recovery property;new storage system;erasure;data fragment reconstruction;storage ring;linear code;storage transfer;codes;data fragments;digital data;code;complete computations;computing systems;high speed data;data;file scale;paxos storage ring;replication;computations"}, "35c6bdab35e8fd4e982302b5270da3c8098c58b1": {"ta_keywords": "natural language instructions;modularization;modular architecture;instruction;novel compositions;training;environments;generalization;advantages;power", "pdf_keywords": ""}, "32feca141fce06c6588b4014d27953a3fc25f19b": {"ta_keywords": "natural language summaries;natural language;language summaries;language model;text alternatives;physical systems;interaction;model;english sentence;humans;experiments;time", "pdf_keywords": "physical commonsense knowledge;language model;language models;language annotations;separate language model;natural language;language object;physical commonsense;language;commonsense knowledge;physical reasoning component;annotation;physical knowledge;language objectwe;linguisticwe;language transformer;physical world dynamics;linguistic form;encode language;interaction;knowledge;physical dynamics model;words;interface;conditional knowledge;text;novel objects;world dynamics;unified model;environment"}, "d9c2242e3aa17db649c92d7d4db46509f3d203db": {"ta_keywords": "robust reinforcement learning;robust reinforcement learning framework;stochastic decision problems;reinforcement learning;robust learning;auxiliary cost constraints;decision maker;policy;algorithm;upper confidence;framework;several examples;use", "pdf_keywords": "upper confidence reinforcement learning;safe reinforcement learning;optimal policy;reinforcement learning;robust linear programming;robust linear program;constrained learning;reinforcement learning algorithm;unstructured reinforcement learning task;robust policy;linear program;bandit states;robust optimization framework;robust learning;confidence bounds;reward function;probability guarantees;constraint costs;reward process;learning process;transition kernels;safety state constraints;transition kernel;optimal conditions;reward;robust algorithm;regret;cost functions;stochastic transition;robust formulation"}, "b990517fbbf4499861d7aa00407b0422874ab990": {"ta_keywords": "supervised sequence tag;words;fiel;proc;novel method;eds;method;environment;use", "pdf_keywords": "supervised sequence tagger;supervised sequence taggers;untranslated terms;art sequence tagging models;accurate translation accuracy;large corpus;stochastic translation;translation data;translators;annotated corpus;novel simultaneous interpreter;simultaneous translation;suitable translation sequence;optimal translation strategy;natural language;corpus;learns;corpora;term memory effects;simultaneous interpretation data;svm;bilingual dictionaries;snowpack;interpreter;snowpack snowpack;translation;interpreter data;annotated data;standard shallow shallow visual image processing;source word"}, "fb38451ff87254ac1ff15e79154ef958b4efb6a6": {"ta_keywords": "powerful computational graph formalism;nonlinear dynamics;multilayer perceptrons;simple classification models;computational power;advanced patterns;tutorial;problems;problem;outcome;wide range", "pdf_keywords": ""}, "e2d770b9ab691753a7ec1eb439185303118c8455": {"ta_keywords": "multilingual agent assistant;multilingual agent assistants;multilingual agents;new languages;machine learning architecture;machine learning tasks;language;machine learning;translation;data mining;machine;techniques;creation;approach;new generation;variety;use", "pdf_keywords": ""}, "c8f78575bfb642b2dab6ed542a683ade9527c17d": {"ta_keywords": "matching organ;organ donation;organ organs;organ;organs;tissue authority;selection;sided market;simple mechanism;axiomatic properties;health care industry;financial markets;new mechanism;process;complex algorithm;consideration;important implications;australia;lives;important part", "pdf_keywords": ""}, "4cc07c367e4a1f932e159678ef711e1802edf49f": {"ta_keywords": "decomposable spoken language tasks;decomposable task hierarchy;decomposable tasks;speech;wireless signal processing system;models;new performance splits;model;novel model;power;structure;case study;variety", "pdf_keywords": "spoken intent prediction;utterance subtasks;home voice assistant;decomposable sequence tasks;utterances;utterance lengths;intent systems;benchmarks;joint speech;decomposable tasks;utterance;end speech;spoken language;intents;tasks;spoken language languages;task;similar performance;speaker sets;robust test sets;speaker;deep bidirectional transformers;domain generalization scenarios;performance;end architectures;usual speaker;end translation framework;iterative running;accuracy differences;language"}, "8fa6b06cb96e5ae98dfff1c50f6940ef43af223f": {"ta_keywords": "novel storage system;hdfs;asynchronous asynchronous cascade code;asynchronous asynchronous cascade cascade cascade code;data;high performance;proton system;performance;hitchhiker;system", "pdf_keywords": ""}, "1941f5b053ccc80fa44980d38ac074145591b4ec": {"ta_keywords": "distinct semantic similarity evaluations;sentence embeddings;semantic similarity;parallel sentences;inference networks;novel variational probabilistic framework;source separation;benchmark datasets;data generating distributions;novel model;experiments;model;art evaluations;capacity transformers;large set;state;approach", "pdf_keywords": "semantic sentence embeddings;semantic embedding;semantic embeddings;embedting sentence embeddings;sentence embeddings;neural machine translation;latent semantic vectors;neural machine translation model;unsupervised semantic similarity evaluations;sentence representations;multilingual semantic similarity;machine translation;specific machine translation task;parallel sentences;semantic similarity;machine translation approaches;monolingual data;large corpus;lingual encoders;semantic encoders;translation baselines;novel generative model;input sentencesmachine translation;natural language sentences;inference networks;generative model;linguistic structure;text generation;sentences;semantic content"}, "553028f7f7c850371379c621e40d7d00e75303a6": {"ta_keywords": "multilingual models;negative interference;layers;learning algorithm;specific layers;systematic study;manner;results;parameters;specific parameters", "pdf_keywords": "sparse multilingual models;multilingual models;specificrobust multilingual models;bilingual models;multilingual neural machine translation tasks;multilingual language;same multilingual benchmarks;same multilingual benchmark;multilingual bert training;resource languages;crosslingual tasks;crosslingual translation;language conflicts;languages;similar languageswe;crosslingual representation;language tasks andwe;large scale machine translation system;large corpus;language;source language;high learning rates;meta learning task;ith language;negative interference;sparse model;models;translation;different models;parallel training signals"}, "1606dc1e966ad59dd96dc8e74722dca06b1f1a58": {"ta_keywords": "emergence;evolution;single initial state;phase;population;random variables;individuals;model;sequence;basis", "pdf_keywords": "evolutionary causal matrices;longitudinal influences;longitudinal outcomes;moral interventions;longitudinal outcome;educational interventions;voluntary service participation;voluntary service participants;computational models;longitudinal trend;future intervention outcomes;stochastic evolution;voluntary service participation rate;substantial longitudinal outcomes;adolescents;voluntary service activity;computational model;interventions;stochastic model;intervention frequency;prediction model;intervention outcomes;intervention type;simulation model;voluntary service;markov chain;intervention types;markov chain approach;transition matrix approach;elementary interventions"}, "e3480d9395e692833b722b2e957d51139985f310": {"ta_keywords": "machine learning;questions;macaw;minimal training effort;machine;variety;source;community;intuitive way;principles", "pdf_keywords": "language models;new language model;language model;language learning;natural languagewe;large language theory;other language modelwe;answers;natural language processing;insightful answers;macaw encoder;encoder;maximum learning;questions;learning;simple encoder;input;heterogenous dataset;macaw;text;nontrivial spatial reasoning;inputs;challenges;prediction;reasoning process;complicated questions;random input string;question interpretation;reasoning;current challenges"}, "6d19d73909ffaa6c94cae6a2535ed52d138cb63b": {"ta_keywords": "multi domain dialog system;dialog corpora;dialogue agent;dialog users;human conversation examples;appropriate dialog;statistical machine translation;twitter conversations;movie scripts;raw movie scripts;interaction;smt;evaluation;method;pair;design;paper", "pdf_keywords": ""}, "d1678032a9eee94ec0a9c54fb008e1addc7213d4": {"ta_keywords": "parametric utility learning;heteroskedasticity inference;social game experiment;utility;estimators;energy;efficient behavior;variance;least squares method;occupants;method;variant;performance;framework", "pdf_keywords": ""}, "33cd5965745dc2e8bb8d0400d0b3c18d4e6369d4": {"ta_keywords": "cache clusters;cache workloads;popularity distribution;traffic pattern;many distinct use cases;performance;multiple distinct use cases;business applications;size distribution;ttl;numerical approaches;time;context", "pdf_keywords": ""}, "0b79cb7fe16aa8b99d521989f39e49034394f701": {"ta_keywords": "human computation;human computation research;work;book;unique perspectives;future directions;goal;field;connections;research areas;comprehensive review;emphasis", "pdf_keywords": ""}, "61cd4ffdaf2c0daa3d432ff9fecdd064d6e72886": {"ta_keywords": "logic language model;order logic;interpretability;benchmark;novel benchmark;novel diagnostic method;generalization;method;weaknesses;accuracy;novel", "pdf_keywords": "human logic traceability;fol reasoning ability;reasoning ability;reasoning abilities;natural language processing;logic logic derivations;logics;specific logic types;logic derivations;natural language;different logicalin;reasoning process;logic network optimization method;abundant logical expressions;domain knowledge;logic;order logics;semantics;universal logic;logical chain;traceability;reasoning;machine learning;novel benchmark;mathematical inference;level learning;knowledge;novel datasets;fols;specific nlu abilities"}, "a829d65de0cc19da49ad6b4a294dd31545aed2bb": {"ta_keywords": "stochastic perturbation;stochastic response;perturbation;amplitude;terms;response;system;study;new approach", "pdf_keywords": ""}, "bc5e4b9fb3a40057df4994354a403202218d53a6": {"ta_keywords": "efficient algorithms;heuristic search procedure;heuristic search;algorithm;combinatorial problems;transformations;semantics;search;substantial improvements;program;search time;efficiency;sequence;method;metric;simple metric", "pdf_keywords": "natural language processing;faster dynamic programming algorithms;computational natural language processing;hierarchical hierarchical parsers;lexical programming;dynamic programming algorithms;natural language;monte carlo tree search;natural language theory;faster algorithms;tree search algorithm;program transformations;fast program;classic search algorithms;fast search;algorithms;nlp;search programs;runtime analysis;runtime;new algorithm;algorithm;beam search;program;novel algorithmic design;efficient pseudo;optimal search space refinements;simple algorithm;recent machine learning algorithm;data extraction"}, "ab17c315f7ee4fe69fde2f3d8ae0e30e4e2f3a2b": {"ta_keywords": "complex documents;iterative;complex questions;documents;questions;different questions;model;new model;different parts;experimental results", "pdf_keywords": "long structured documents;long structured document;conversational task;structured documents;novel reading comprehension model;hierarchical attention method;long document;complex queries;sentences;extractive questions;queries;lexical landscape;conversational techniques;document;conversational scenario;paragraph;tasks;conversationalqa;sharc benchmark;hierarchical part;questions;snippets;single query;text;query vector;extractive quantumqa task;dochopper;conversational setting;discourse entailwe;higher level"}, "e54ffc76d805c48660bb0fd20019ca82ac94ba0d": {"ta_keywords": "dimensional language models;low dimension reparameterization;low intrinsic dimension;training parameters;gradient descent;intrinsic dimension;models;tuning;model;other words;tune;large number;systematic approach;phenomenon;presence", "pdf_keywords": "low dimensional task representations;intrinsic dimension;intrinsic dimensionality;intrinsic dimensions;sentence prediction tasks;lower intrinsic dimensions;language models;higher generalization accuracies;natural language tasks;lower relative generalization gaps;bert pretraining algorithm;large scale training;intrinsic representation;higher dimensions;training accuracy;generalization bounds;prediction accuracy;intrinsic structure;classifiers;generalization;supervised learning;neural networks;compressible pretraining;intrinsicwe;models;full parameterization;text;large scale scalability;better evaluation performance;compressionwe"}, "2a82a16bdb793dc388391be57d6424f0d5090513": {"ta_keywords": "review scores;peer reviews;updates scores;scores;reviews;peer;synthetic datasets;papers;information;version;method;error;same manner;arbitrariness", "pdf_keywords": "peer review algorithm;available review scores;peer reviewers;underlying quality scores;world peer review data;peer review;optimal ranking;optimal scores;underlying quality score;rankings;ranking;peer review process;total rankings;partial ranking;final ranking set;reviewer assignments;continuous scores;underlying score;peer assessment problem;global ranking;scores;relative scores;reviewers assignments;reviewers information;global optimal score;peer review literature;input scores;reviewers;score;reviewer"}, "188fd1373aefdbf564e90a76fed43e1b8b7052dc": {"ta_keywords": "orbit interaction;spin;dynamics;interaction", "pdf_keywords": ""}, "69e2d1f5374918111432fae23212c2759b1357c2": {"ta_keywords": "sequential ranking algorithm;active ranking;ranking;pairwise comparisons;items;item;probability;logarithmic factors;set;problem", "pdf_keywords": "sequential ranking algorithm;noisy pairwise comparisons;complete ranking;active ranking;ranking;pairwise comparison probabilities;total ranking;active pairwise comparisons;ranking problem;pairwise comparisons;pairwise comparison model;full ranking;new bandit algorithm;selection process;pairwise comparison matrix;new stochastic algorithm;bandit models;rank;stochastic transitivity;sample complexity;comparisons;efficient algorithm;efficient identification;joint distributions;algorithms;bandits;top item;underlying distribution;optimality;novel algorithm"}, "926d827aef568ed97431a7845c9a8138930c80fd": {"ta_keywords": "social sharing behavior;affective responses;social network;affective reactions;relationship;network;messages;efficacy;people;studies;information;first study;second study", "pdf_keywords": ""}, "9b534639bcadc9ad232b338e760c523a4d74c8de": {"ta_keywords": "concise descriptions;linguistic phenomena;human evaluation;descriptions;language experts;evaluation;extraction;automatic framework;method;help", "pdf_keywords": "linguistical descriptions;linguistic exploration;natural language data;syntactic features;linguistical data;morphological agreement;linguistic questions;linguistic rules;interesting linguistic patterns;novel linguistic rules;corpus;linguistic structure;classification tasks;linguistic knowledge;rule extraction models;labeling words;linguistic systems;feature extraction;linguistic phenomena;relevant linguistic rules;linguistic framework;linguistic distinctions;treebanks;word order patterns;linguistical framework;linguistic problem;feature extractionwe;concise descriptions;other treebanks;linguistic standards"}, "987c5ad75d5092bed03e9f523aec00dc43bc17e4": {"ta_keywords": "urban air quality;road network density;air quality;road traffic;road area occupancy;bus emission reduction;bus network density;aerosol;bus route planning;intersections;intersection number;optical thickness;regression;impacts;greater impact;models;gwr;aod;characteristic parameters;geographical;main factors;other policy;results;timing;study", "pdf_keywords": ""}, "89c2b3bfcc309ce16c85d2ab0c8cac5295400715": {"ta_keywords": "entropy learners;sequential stacking;nonsequential base learners;algorithm;entropy method;comprehensive evaluation;new method;performance;version;problems", "pdf_keywords": ""}, "9b263129548dc09369e8bc34560fe5bb6047fcee": {"ta_keywords": "market rules;market choice;market microstructure;market;simulator;electricity;time simulation;greek;potential use;development;critical evaluation;processes;project;paper;scale;effectiveness;goal", "pdf_keywords": ""}, "9333d372ad3887e02029d2eab0dbc0c0478582c7": {"ta_keywords": "unsupervised learning methods;natural language processing tasks;generative part;evaluation;methods;context;accuracy;data;case study;art;state;choice;issues", "pdf_keywords": ""}, "35b376ad9e03e5e0b930c53a48817bfb5703108d": {"ta_keywords": "semantic similarity metrics;text style transfer problem;texts;test texts;semantics;efficient training strategies;content;training process;style;novel approach;quality;preservation;outputs;model;experiments;system;significant gains;set", "pdf_keywords": "unsupervised text style transfer;text style transfer model;neural machine translation models;text style transfer;neural machine translation model;novel paraphrastic similarity training algorithm;accuratelyconventional machine translation methods;sentence generation;machine translation agent;style transfer accuracy;style transfer;semantic similarity metrics;machine translation method;stochastic style transfer patterns;semantic similarity metric;automatic evaluation metrics;novel similarity similarity benchmark;novel similarity benchmark;semantic similarity metricwe;automatic evaluation;high style transfer accuracy;output texts;conditional generative adversarial network;language model;simple language model;automatic metrics;keystyle transfer;generative adversarial network;content reward;style classifier"}, "c96970cfb1c13ae6dccc30de482ce6b0d4414f2b": {"ta_keywords": "predicate invention;structured sparsity;structure;scale algorithms;performance;new version", "pdf_keywords": ""}, "4fee3d5d476568deb971768f8a5191eb627309d0": {"ta_keywords": "continuous games;game dynamics;differential equilibria;local equilibria;stable points;agent learning rates;stability;sum games;potential games;game;robustness;quadratic numerical range;player;variation;points;method;spectrum", "pdf_keywords": "learning dynamics;stable differential nash equilibrium;gradient learning dynamics;differential nash equilibria;differential nash equilibrium;game dynamics;stable equilibria;differential equilibria;continuous games;stable equilibrium;stability;stable games;dynamics;gradient;gradient model;optimal strategy;learning;equilibria;potential game;potential games;differential sde;instability;scalar action spaces;sum games;games;spectral properties;optimization landscape;differential system;sum game components;strategy"}, "5df0b8b80aecda1efdebac5d1ab7bcf94a88c68f": {"ta_keywords": "feature extractor;specific corpora;nearest neighbor analysis;biomedical literature;untagged information;probe;information;article;comparative study;performance;domain", "pdf_keywords": "contextual word embeddings;deep bidirectional language models;natural language inference;bio bert;biomedical corpora;embeddings;deep learningwe;biomedical text;deep learning;nlp;deep learning representation;embedding;biomedical articles;sentences;downstream task models;specific corpora;contextuality;sentence pair classification task;bio;gene mention;feature extractors;entities;bioelwe;bio elongation;linguistic properties;sentence structure;unstructured content;words;gene expression network;bioelmo"}, "5930efbf01efa8944258b1c0f7349111702f779e": {"ta_keywords": "novel nonlinear linguistic structures;natural language processing;feature extraction pipeline;novel feature extraction;nlp;nonlinear feature;extraction modules;selection;novel system;field;system;extensive set;creation;use;performance", "pdf_keywords": ""}, "44268b5a208e8f48a5883bb12e3e80a13101e752": {"ta_keywords": "median creatinine level;creatinine concentration;acute kidney injury;median age;free energy;risk factors;core;patients;ffi;age;data;comparative study;level;years;sample;public database", "pdf_keywords": ""}, "96ed7a7da69d654668b35b50344debd44e87c1a1": {"ta_keywords": "topic identification process;topic identification;contextual dependencies;contextual model;translation lexicons;resource languages;universal acoustic modeling;context;attention;independent models;selective exploration;dependent nature;cascade;model;selective manner;general purpose method", "pdf_keywords": "language topic classification;topic identification methods;topic identification;topic identification technique;novel topic identification method;topic id;speech networks;conventional speech classification methods;topics;unstructured audio;pronunciation model;contextual models;pronunciation information;automatic speaker systems;universal acoustic modeling;music segmentation techniques;contextual representation;attention modeling;topic;speech;neural networks;contextual data sets;languages;idf feature;attention model;neural network;convolutional neural network;transcriptions;translation lexicons;unusual pronunciations"}, "b9c3e87bc09c4c6167a03a835c30b1c23bef7a40": {"ta_keywords": "knowledge bases;contextual embedding;quality knowledge base dataset;novel bert;bert;questions;generalization;levels;novel approach;use;scale;approach;evaluation settings;utility", "pdf_keywords": "new knowledge bases;knowledge bases;arbitrary knowledge bases;generative knowledge base;natural language database;natural language questions;knowledge base;structured queries;natural language items;entity recognition;natural language;paraphrase dataset;natural search engine;large paraphrases;sql parsing;bert model;structured information;new compositional generalization;simple knowledge;questions;robust paraphrase;language model;entity surfaces;bert encoding;machine learning context;knowledge;paraphrases;mine canonical logical forms;diverse questions;new entities"}, "05b6be9aec266072669f6f287a846637eedf19b5": {"ta_keywords": "nematode canadensis;nematode community;experimental warming;experimental air warming;soil;invasion;chao;basal index;effects;structure indexes;study;results;maturity", "pdf_keywords": ""}, "449310e3538b08b43227d660227dfd2875c3c3c1": {"ta_keywords": "deep neural network models;neural network;generative model;residual network;parallel;latent variable model;models;learning process;depth;output;power;time;new family", "pdf_keywords": "deep neural network models;continuous backpropagation;direct backpropagation;neural ode;neural networks;adjoint sensitivity method;neural network;loss function;gradient;secondary neural network;gradients;ode solver;supervised learning;adaptive computation;latent ode model;adjoint method;standard adjoint method;sensitivity methods;constant memory cost;normalizing flow;learningwe;ode;learning process;loss;continuous normalizing flows;partial differential equationwe;generative model;models;derivatives;random matrix representations"}, "ab1e5a3c5521b6204dc7c6f1fa72b88000bc30ee": {"ta_keywords": "noise sources;noise;server;performance;variety;comparative study", "pdf_keywords": "textual responses;natural language processing tasks;user interaction;natural language;natural language processing;human annotators;input language;questions;input interface;input languages;natural keyboard errors;interfaces;queries;specific response;machine translation systems;input;interface noise;generative pretraining;natural language translation;neural machine translation system;linguistic features;generative pretraining system;machine translation system;keyboard;deep learning;human speech;text;spelling noise;language;machine translation pipeline"}, "5f609f252d8815c5fb660d83c0dc71af21ecf65d": {"ta_keywords": "noun phrases;extracted keywords;keywords;twitter;many events;event;knowledge;training data;feature vector;nps;data;base;np;approach;method;sample", "pdf_keywords": ""}, "33c691ca050e1806d44c08e55e63fcd7e555899a": {"ta_keywords": "unlabeled classification;unlabeled learning;neural network classifier;unlabeled sample;proportion estimation;negative data;proportions estimation;proportions;dedpul;novel method;paper;framework;difference;state;art;means", "pdf_keywords": ""}, "a8ea980b63deaf1404cd9f539a575b4e7135466e": {"ta_keywords": "parity models;erasure;median latency;decoder;prediction;same median latency;predictions;predictive model system;resilience;queries;inference;systems;new approach;form", "pdf_keywords": ""}, "fb7caddac20dca012f48c90b2e1e2383f7185051": {"ta_keywords": "misuse interpretability tools;interpretability tools;data scientists;machine learning models;tools;contextual inquiry;researchers;social science literature;trust;findings;tool designers;common issues;implications;participants;survey;output", "pdf_keywords": "interpretability tools;interpretability systems;interpretability;interpretability system;interpretmachine learning;interpretable feature attribution;interpretthe use;machine learning practitioners;machine learning models;explainable predictors;exploratory analyses;intelligibility;mental models;visualizations;interpretation;data scientists;interactive models;machine learning;visualizations output;machine learning system;explanations;datasets;data models;common explanations;machine learning experiment;data;understanding;preferred interpretation;tools;description"}, "b1d8c868e1d6d4980ee2f8c50e6fc5e4e7027ca2": {"ta_keywords": "story continuation;dialogue;unstructured data;characters;strong baselines;context;novel approach;evaluation;model;relationship;accuracy;trains;hypothesis", "pdf_keywords": "storytelling dataset;story continuation accuracy;story content;character prediction systems;emergent storytelling;character dialogue;story continuation;character prediction;character relationships;character relationship information;storytelling;characters;crowdsourcing;underlying story;character;stories;term memory;fantasy version;text;players;dungeons;unstructured text;agent;challenges;story;data;information;dragons;strong baselines;model"}, "e785441f5ccd6e4e29b3123e61121df5c65b88f7": {"ta_keywords": "variational autoencoder training;posterior collapse;inference network;inference lag;latent encoding;true posterior;model;dynamics;observation;initial stages;context;target;simple modification;result", "pdf_keywords": "variational autoencoder;deep generative models;variational inference network;variational inference;generative models;stochastic variational autoencoder;stable variational autoencoders;generative model;paralel inference network;convolutional generative model;specific variational inference approach;inference network;deep latent;variational learning technique;paralel inference;probabilistic inference;lagging inference network;input embeddings;variational distribution;probabilistic representations;bayesian inference;aggressive inference network updates;posterior posterior parameters;approximate posterior mimics;latent variable;autoregressive baselines;marginal data likelihood;models;autoregressive models;posterior collapse"}, "f7a2f2ae829545ee992a2214b3600cf914544e22": {"ta_keywords": "human students performance;student performance;student model;student data;human students;accuracy;study;correct performance;classes;model;predictions", "pdf_keywords": ""}, "9112be1801598125d463febb96a525227c32acc1": {"ta_keywords": "automatic differentiation;deep neural network;state transducers;traditional convolution;convolutional layer;level representations;latent decomposition;words;wfsts;replacement;training time;drop;interior;framework", "pdf_keywords": "new structured loss functions;automatic differentiation;various sequencelevel loss functions;state automata;learning operations;transition matrices;level loss functions;deep learning;stochastic generator;state transducers;automatic speechwe;dynamic inference;transition models;neural networks;deep neural network;convolutional neural network;neural networkwe;transitions;speech recognition;convolutional model;neural network;convolutional wfst layer;sentence recognition;finitestate transducer;machine learning models;weights;differentiable timedepth separable convolutional architecture;offline handwritten speech recognition;linear systems;graph transformer networks"}, "4cf633d0893a1d3af97723ce1f2fae33c2a30043": {"ta_keywords": "open information extraction models;knowledge bases;entity pairs;redundant relations;exact similarity;relations;similarity;conditional probability distributions;sampling;tasks;divergence;approach;good approximation;outputs;effective method;method;variety", "pdf_keywords": "relation similarity;relation extraction;relation extraction task;relation extraction dataset;relation extraction method;similar relations;relational classification tasks;relation prediction;knowledge bases;new similarity measure;novel similarity quantification approach;similarity score;similarity quantification approachwe;several information extraction tasks;relation prediction method;similarity;knowledge base completion;entity pairs;knowledge graph;tail entity pairs;tail entity pair distributions;knowledge graphs;disjoint relationships;same relations;redundant relations;relations;computed similarity score;relation structure;relation;information retrieval"}, "9f1059006e4ba303f8945114eddadd50d58a9f3e": {"ta_keywords": "large knowledge bases;soft symbolic databases;neural networks;neural network;learning;parallel network;gradient;large class;dms;tasks;framework;combination;power", "pdf_keywords": "soft symbolic databases;large knowledge bases;tensor network framework;nql framework;nql construction;sparse representations;general purpose nql framework;natural query language;tensor;many quantum information tasks;nql;entities;deep learning;learning systems;representations;relations;nonlinear quantum lyapunov;vector computations;relation;linearwe;dataflow language;natural language;mathematical objects;matrix;learning method;python language;gradient;recurrent models;framework;modern gradient"}, "02b932416751674dc25353620a1df4b53c3a5f6f": {"ta_keywords": "automatic speech recognition;sequence model;nonlinear partial differential equations;linguistic annotations;transcriptions;quality transcriptions;asr;datasets;ndes;model;novel method;method;experimental results;variety", "pdf_keywords": ""}, "811531c959b0543a8e7abe1e827770e36b96f817": {"ta_keywords": "level emphasis;speech system;emphasis;speech;target language;conditional random fields;translation;linear regression;word;mixture;performance;paper;measure;novel method;method;result", "pdf_keywords": ""}, "6c34b7b0441bff66cce2418d36acfd9776ad7bd2": {"ta_keywords": "algorithm irep;benchmark problems;rule;modi cations;error rates;arxiv;large collection;num ber", "pdf_keywords": ""}, "d723630c585aa0e4084fdd6e71bc6586cfa30e9d": {"ta_keywords": "joint pause prediction;pause information;syntactic information;dependency;data;model;more data;addition;paper;measure;further gains;results;experiments", "pdf_keywords": ""}, "81bc64ce5553798c058f25fe5bd537d4bed67aed": {"ta_keywords": "quantum dot;luminescence energy;dot;organometallic chemical vapour deposition;barrier;fabrication;shape;surface;mev;new type", "pdf_keywords": ""}, "a8c62c42509c45a708ba477b603ee3fb81c77056": {"ta_keywords": "false news detection;false news detection approaches;weibo;false information;social platform;powerful tool;user;competition;goal;time;propagation;development;process", "pdf_keywords": "first false news detection competition;false news detection;false news detection approaches;false news posts;news detection;news text detection;news image detection;falsenews;real news posts;validation competition;news posts;false news;validation;accurate rumor catalogs;open challenge;real news;ground truth;social media;competition;subtasks;first subtask;prediction performance;results;testing;noisy data;multiple modalities;rumor spread;participants;large dataset;xm"}, "44775500a5380be3776e876aedc43921d42d8de9": {"ta_keywords": "massive urban mobility data;urban dynamics;rich urban dynamics;urban states;whole city;semantics;population;model;sdd;interest;type;aspects;point;volume;method;frequency", "pdf_keywords": ""}, "470bfbde1dc0ed6ca989957dcd551213720657c0": {"ta_keywords": "dependency trees;translation quality;language pair;explicit syntax;trees;model;number", "pdf_keywords": "neural machine translation model;neural machine translation;bidirectional neural machine translation encoder;structured self attention encoder;attention models;machine translation;several translation models;structured self attention function;structured self attention;attention weights;translation translation performance;syntactic structure;syntactic information;attention probabilities;parses;attention mechanism;dependency trees;new seq2seq model;normal verb dependencies;linguistic structure;linguistic structures;projective dependency trees;hard attention mechanism;syntax;syntax syntax;explicit syntax;source syntax;natural language;neural network;neural networks"}, "bc494b9c6d9602a69b76ab9ea0e95d348a2fce19": {"ta_keywords": "summarization system;salient content;highlight;summaries;multiple annotators;manual evaluation;evaluation;less evaluation;source document;level reference;novel approach;dataset;approach;level", "pdf_keywords": "summarization evaluation;annotation evaluation;reference summaries;summarization framework;summarizemarization;referenceless evaluation;summarization;machine translation;document highlight annotation;content evaluation;short summary data;art summarization methods;abstractive summaries;annotation;highlights;manual evaluation;reference bias;summaries;expert annotators;novel evaluation approach;reference summary;highlight;human evaluation;salient content;comprehensive evaluation strategy;evaluations;summary;evaluation;system summaries;pertinent content"}, "e2ffd0ea7aa9cebaafba4afaee3cbe78070c8aa2": {"ta_keywords": "variational probabilistic model selection;state triphone model;state triphone parameters;speech recognition;state triphone;model selection;model;proper statistical basis;large data sets;choice;state", "pdf_keywords": ""}, "3321c947a4a399803592f26879927e58f587fd74": {"ta_keywords": "hour time baseline;behavior;participants;human people;orarresting;future;study;rate;orarrestings;findings;likelihood;sub;existence", "pdf_keywords": "crowdsourcing experiments;crowdsourcing studies;algorithmic risk;algorithmic risk assessment;human predictions;participantss predictions;algorithmic risk assessment instruments;human prediction;human risk assessment survey;probabilistic predictions;predictive performance;quantitative risk;predictive bias;human decision making;human risk;successful hypothesising algorithms;empirical validation;mechanical turk;risk estimates;human decision;risk estimation;human information;quantitative information;own predictions;binary predictions;prediction;risk assessments;predictive utility;risk assessment;higher predictive accuracy"}, "0554246ebb6c53e88d7ac2aabf0c96a91ad500a0": {"ta_keywords": "speech enhancement problem;strong speech enhancement capability;real speech recognition;domain model;simulation;performance discrepancy;frontend;asymmetric;time;novel approach;situations;paper;approach;preliminary experiments", "pdf_keywords": "domain speech enhancement;speech enhancement capability;speech enhancement;advanced speech enhancement;speech separation system;domain audio separation network;speech recognition performance discrepancy;channel encoder;speech source;chime4 corpus;overall speech recognition error;microphone;noise networks;encoder;broadcast;speech;channel;speech pattern;neural beamforming;snet;noise data;advanced signal processing models;advanced signal processing;signal processing;tasnet;wireless channel;noise;training schemes;network;acoustic dissipation model"}, "c2bd176f8f9c84f9ba52ffb8f8bd4e9299c0f0cf": {"ta_keywords": "quantile regression approach;multiple quantile regression;parking occupancy data;seattle;city;approach;idea;python package;results;problem", "pdf_keywords": ""}, "a9a7058b39768ece13608e31341cfb16c4faf2c3": {"ta_keywords": "fair machine learning;political philosophy;algorithms;broader problems;ideal approach;recent literature;shortcomings;connection", "pdf_keywords": "algorithmic injustice;algorithmic fairness;current fair machine learning;human fairness;current algorithmic fairness approaches;fairness metrics;new fair machine learning framework;evaluative justice;several fairness metrics;correct injustice;justice;fairness;human policy;fair machine;discrimination mechanism;fair metametrics;predictive framework;justicethe wealth;predictive models;consequential decisions;predictive modeling;evaluative approach;evaluative standard;fair metametrics project;policy;policies;algorithms;normative prescriptions;government;standard fair machine"}, "156323f4d87af6cf105c97bf29d324c9e3bc8f92": {"ta_keywords": "automatic speech translation;speech translation systems;translation quality;joint optimization;minimum error rate training;minimum variance training;optimization;batch margin;asr;synthesis;algorithm;performance;words;significant improvement;novel approach;mira;system;final result;approach;result;paper", "pdf_keywords": ""}, "3cd4797725ca9cf954946ed5309e15ebab80b92a": {"ta_keywords": "multimodal conditional image synthesis;unimodal conditional image synthesis approaches;multimodal multiscale projection discriminator;multimodal input;multimodal user;diversity;high quality;expert knowledge;novel approach;quality;approach;terms", "pdf_keywords": "multimodal conditional image synthesis;multimodal conditional image synthesis model;multimodal image synthesis;novel conditional generative framework;multiscale multimodal projection discriminator training;generative encoder;generative adversarial networks;generative models;generative structure;generative framework;multimodal image;deep convolutional generative adversarial networks;multimodal modalities;image synthesis;multimodal projection discriminator;multimodal projection discriminatorwe;conditional images;single generative model;multimodal user inputs;multimodal approach;contradictory multimodality inputs;gans;multimodal face;generative variance normalization;conditional image distribution;multiscale latent representation;diverse images;image translation;gan;novel generativegenerative extraction algorithm"}, "365e049ecb7299cc6925483127f2f2123a97c35f": {"ta_keywords": "change point detection;abrupt change points;novel kernel learning framework;generative model;auxiliary generative model;convolution operator formalism;benchmark datasets;world datasets;dynamical phase;set;approach;parameters;art approaches;framework", "pdf_keywords": "time series learning;novel probabilistic change point detection method;change point detection;new kernel learning framework;kernel learning framework;novel kernel learning framework;time series change;robust change detection;kernel learning approach;stable change detection method;robust change detection method;deep kernel learning;time series;deep kernel representation;time series applications;abrupt change;deep kernel parametrization;time series data;change points;change point;time model;kernel;compositional kernels;dynamical learning paradigm;stochastic learning approach;benchmark datasets;prediction;abnormal events;auxiliary generative model;generative model"}, "464a75c05a5ce709fc515a2577b43acc8e3d45ce": {"ta_keywords": "baseline methods;best performance;detailed analysis;evaluation;improvement;task;results;system", "pdf_keywords": ""}, "a6e61164e7b385cec0e12093bc270eafd3ef1dbc": {"ta_keywords": "unsupervised activity recognition;activities;recognition;participants;physical characteristics;height;end user;gender;user;presence;information;method;large number;problem", "pdf_keywords": ""}, "d7c1bdafb51fe1a757604f9daeaea812f124320f": {"ta_keywords": "technology forecasting systems;technology forecasting;government contracts;government contract;future state;interest;system;performance;possible parameter;parameter;research;new parameter", "pdf_keywords": ""}, "fba7c0a51a6301ca4086a5ce59b1f13af9acad7f": {"ta_keywords": "pointwise approach;data;analysis", "pdf_keywords": ""}, "5d07db93e6fbd9e10713a2f372131c777077062d": {"ta_keywords": "deep network;deep networks;reinforcement learning;minimal accuracy loss;bitwidths;nodes;network;discovery;computation;information;design;storage costs;novel approach;problem;approach", "pdf_keywords": ""}, "e63e1db25f33162cc6e498f983dc3f6e10c9867e": {"ta_keywords": "speech separation;noisy speech recordings;speaker information;speech sources;speakers;sequence;information;observation;high efficiency;novel approach;model;experiments;identities;conditions;work", "pdf_keywords": "time domain speaker extraction network;speaker extraction network;speaker extraction;conventional speech separation;independent speech separation;sound separation networks;speech extraction;speech separation;time domain speaker dereverberation network;novel speaker detection;speaker modeling;speech recordings;speech recognition;domain signal extraction module;speaker information;speaker lists;recurrent selective attention network;speech sources;speaker;speechwe;audio recordings;true speaker lists;speaker identities;novel attention model;decoder sequences;novel speaker;audio signal;multiple speakers;speakers;sequence model"}, "c3d2c60e70cad17ea37cb116ab30e1239405dbdd": {"ta_keywords": "linear zeeman field;orbit interaction;zeeman field;interaction strength;spin;field;factor;effect;strength", "pdf_keywords": ""}, "6a730aff0b3e23423a00cb3407eb04e7f6e83878": {"ta_keywords": "moses machine translation system;variational bayes;random matrices;bayesian approach;algorithm;likelihood;schrieffer;bcs;model;bardeen;data;cooper;performance;set;shape;terms", "pdf_keywords": ""}, "c06410ce8f9b1c941115d6d96780794e66b27eac": {"ta_keywords": "novel incremental adaptation framework;large vocabulary continuous speech recognition;utterance update;japanese broadcast news;line adaptation;macroscopic time evolution system;word accuracy;posterior distributions;700k vocabulary size;utterance;framework;paper", "pdf_keywords": ""}, "68167af17980a14ed5fa2514e61d76d5a6a9bed7": {"ta_keywords": "conspiracy theories;underlying conspiracy;social media content;mainstream news;public health responses;misinformation;social media;vaccination process;narratives;spread;effect;presence", "pdf_keywords": "topic cluster;social media posts;web articles;topic content;social media;external articles;pandemic vaccine conversation;social media platforms;twitter;information content;related websites;conspiracy theories;social sites;articles;several topics;legitimate sources;content;dubious sources;outbreaks;public discourse;external content;clustering;scientific community;fake news;available twitter dataset;cybersecurity context;coronavirus;topic topic;discussion;misinformation"}, "4a96b2b33786301d59670fa647f99e3dd807abb8": {"ta_keywords": "multiagent learning algorithms;nonuniform learning rates;continuous games;agents;individual gradient;convergence guarantees;equilibrium;gradient;games;convergence properties;numerical examples;vector field;analysis;oracle access;class;power;distortion;effects", "pdf_keywords": ""}, "d9b458d39e0912524032887aaaf922f0e950f0c1": {"ta_keywords": "large scale network;parallelization;neural network;parallel;supervised learning problem;large scale model;network;model;shape;size;problem;recent study;present;study;results;goal;summary", "pdf_keywords": ""}, "76f9f4bf8d97de5e95d2fd9dd8b50041524fb1cc": {"ta_keywords": "joint knowledge embeddings;joint entity alignment;knowledge graph completion performance;entity alignment;various knowledge graphs;entities;dimensional semantic space;relations;alignment performance;realworld datasets;parameter sharing method;small seed;novel approach;significant improvements;method;experiments", "pdf_keywords": ""}, "476ff888fe3917f92b221c522ffb7bfaa4e1861b": {"ta_keywords": "conversational search;functional search systems;retrieval question;reranker;retriever;large collection;answers;reader;extensive experiments;end system;evidence;further step;end", "pdf_keywords": "functional conversational search systems;conversational search;conversational system;retrieval retriever system;conversational search setting;novel retriever retriever retriever conversation;retrieval;retriever retrieval;retriever search;conversational learning framework;concat retrieval;retrieval module;conversations;conversation;retriever system;retriever generalization;retriever passage;retriever;retriever query;search question;retriever modeling;retriever recall;text documents;large collection;retriever module;retriever recall problem;time conversation;future retriever;answer span;question encoder"}, "b169c4b6c23efe8cbd4dc29eb97939cbcfba0f28": {"ta_keywords": "persuasive dialogue system;persuasive power;salespersons;dialogue;dialogue acts;professional salespersons;satisfaction;framing;factors;power;corpus;subjects;effective predictors;effective predictor", "pdf_keywords": ""}, "65d3575b1c380b1bcc14ec69ccf6989c04be9493": {"ta_keywords": "graph algorithms;many graph algorithms;easy parallelization;elimisation programming model;novel signal;optimal choice;model;elegant interpretation;convergence;essential features;problem", "pdf_keywords": ""}, "cf7e8f47ad1c57738dc586109dcf28a22ab67b72": {"ta_keywords": "peer review;papers;novel algorithm;paper;problem", "pdf_keywords": "new peer review bidding system;peer review;peer review process;review algorithm;peer review model;optimal paper ordering;iterative peer review process;competitive bidding system;individual reviewer;ranking items;review paper;bidding ordering;optimal ordering;bidding performance;review paper index;reviewers;reviewer;reviewer dissatisfaction;ranking;optimal papers;bidding processes;review;new paper ordering;crowdsourcing bidding;best bid;bidding process;paper ordering;minimum bid count;apeer review;final reviewer"}, "adc273bd25ab1e2a66543f23c7a801af0dd80e5b": {"ta_keywords": "simultaneous speaker diarization method;speaker diarization;simultaneous speech recognition;speaker embeddings;channel dialogue recordings;challenging dialogue recordings;speaker overlap ratio;word error rate;method", "pdf_keywords": "automatic speaker diarization;speaker diarization;simultaneous speaker diarization;iterative speaker diarization method;channel speech recognition system;automatic speech recognition;simultaneous speaker extraction;speech recognition problem;simultaneous speech recognition;speaker embeddings;oracle speaker embeddings;speaker information;channel dialogue recordings;dialogue recordings;coupled speaker encoders;speaker separation;speaker;real dialogue recordings;talk model;dialogue speech;target speaker;utterance;decoder;dependent interference speaker loss;entire utterance;auxiliary loss functions;special neural network;speakers;neural network;target speakers"}, "b20cadef0c59e80f7dfdf825b07442619d920fd5": {"ta_keywords": "nodes;network;novel approach;problem", "pdf_keywords": ""}, "16326359081a42c0b254ee6be39824fd2db07e48": {"ta_keywords": "pivot phrases;pivot language models;translation process;data;input;novel method;power;method", "pdf_keywords": ""}, "336ee50043b916c9e932338c02fd1abc87a6e849": {"ta_keywords": "cooperative neural modules;elementary particles;compositional generalization;novel reinforcement learning algorithm;novel approach;model;system;essential features;benchmark", "pdf_keywords": "novel compositional generalization model;compositional generalization model;compositional generalization ability;compositional generalization;neural compositionality;compositional task;sequence recurrent network;unstructured sentences;deep reinforcement learning;compositional rules;deep neural models;cooperative neural modules;seq2seq recurrent network;novel memory;learning model;neural networks;sequence network;neural model;reinforcement learning;natural language;neural network;essential linguistic structures;sentences;hierarchical reinforcement learning algorithm;hierarchical structure;biological neural networks;neuralwe;memory;reinforcement learning framework;remarkable linguistic diversity"}, "89ba434b30a3f1b61bcbcf917842899fe3d2eea4": {"ta_keywords": "statistical machine translation;parallel corpus;individuality;identical semantic content;function words;text;background text;manual evaluation;types;smt;smt construction techniques;limited set;system;different amounts;method;effectiveness;degree", "pdf_keywords": ""}, "63a35d8822a042f6d6cd919fd5d3c9e94df6ee18": {"ta_keywords": "novel change point detection framework;unsupervised change point detection methods;change point detection performance;true change point instances;change point instances;change points;sinkhorn divergences;ground metric;sample tests;supervision;windows;online manner;method", "pdf_keywords": "change point detection;novel change point detection framework;poor change point detection performance;change detection;change point detection method;true change point instances;metric learning approach;change points;learned linear metric;change point model;true change points;false change points;different change detection settings;supervised detection;supervised learning;weighted mean distances;abrupt changes;training sequences;supervised learningwe;wasserstein distance framework;detection;regularization;empirical sinkhorn divergence;complex sequential data;synthetic datasets;metric;metric enforces;shift;changes;small changes"}, "00717c695e4a33318fe5655e2b69e1ba8b61f981": {"ta_keywords": "speech segmentation challenge;deep learning;deep learning systems;npnpnp;joint recognition;language models;recognition;heterogeneous data;challenge;underlying language;contribution;goal;combination;system;paper;ability;novel approach;power;approach", "pdf_keywords": ""}, "ba201da15899e78629ee5471e8d336b6b2eb7279": {"ta_keywords": "mobility services;traffic congestion;routing models;monte carlo simulator framework;sustainable sharing;network;riders;algorithms;social structure;nashville;penetration;novel approach;case study;adoption;tennessee;overall efficacy;effects;different levels;effect;basis", "pdf_keywords": "optimal routing strategies;selfish routing;optimal congestion sharing;different routing strategies;mobility services;route users;optimal routing strategy;route plan;popular static routing games;public transportation network;transportation networks;different routing strategies increases;optimal transportation paths;public transportation;specific routing;traffic constraints;simple stochastic routing game;congested route;optimal transportation assignment;optimal transportation schedule;user bus network;network congestion;mobile agent network;preferred routewe;road traffic model;optimal transportation assignment problem;optimal user;congestion information;google maps systemin;transportation"}, "0af2ff552ab0555914dee90ccfae18297b2792c9": {"ta_keywords": "deep network model;end diarization models;speaker identification component;audio recordings;diarization performance;local convolutional network;diarization;attention module;model;end;global self;sequential approach;several components;system;data", "pdf_keywords": "binary speaker activity images;recursive speaker model;entropy speaker identification;speaker diarization;speaker detection;usual joint speaker detection;deep network model;discrete time speaker generators;channel audio recordings;end diarization models;speaker identification;speaker;diarization models;local convolutional network;diarization loss;diarization performance;attention module;diarization model;end diarization;speaker losses;speaker confusion;speaker loss;deep learning experiment;speakers;joint speaker loss;standard meeting models;neural network;attention;diarization;encoders"}, "12f3bc02d649645fa8734977e28b0ac839e56371": {"ta_keywords": "allowable congestion;queues;curbside parking;parking;crowded environment;faceted network;available server;drivers;density;constraints;number;strong correlation;new kind;method", "pdf_keywords": "parking demand;parking occupancy;parking prices;congestion constraints;parking data;parking policies;parking spots;parking forecasting;parking lots;parking sites;allowable congestion;parking;queue;curbside parking;parking lot;queues;minimum arrival rate;traffic;parking garages;parking meters;james webbs parking network;such queues;nodes;maximum occupancy;occupancy;convex optimization problem;observed occupancy;available servers;optimization problem;model city"}, "b7637d1148da569d2211b5dd9851bca82c6aac43": {"ta_keywords": "dependency parsing;imitation learning;feature selection;graph;languages;features;new framework;edges;paradigm;approach;method", "pdf_keywords": "dependency parsing;projective parsing;dependency parser;parsing;different parsing tasks;projective parser;general projective parser;dependency graph;structured prediction;dependency;sentences;cyclic dependency network;dynamic feature selection;novel graph;feature computation;feature library;projective trees;sentence;efficient training;tree;essential features;features;feature;trees;significant bottleneck;novel feature selection strategy;algorithms;large library;graph;language families"}, "5f8d2da91a6c4b9dd079ccb2706c31bda14ef320": {"ta_keywords": "automatic captioning tasks;speech recognition;joint modeling;extensive experimental evaluation;evaluation;improvements;approaches;several approaches;state;advantages;art methods;methods", "pdf_keywords": "synthetic speech;speech transcription;speech recognition;joint neural network modeling;speech contents;neural networks;neural networkswe;speech signal;37k speech samples;neural network;captioning algorithm;speech;captioning;audio;word sequences;synthetic dataset;captioning metrics;acoustic sources;new joint modeling framework;joint modeling;sound signals;sequence model;models;parametric decoder model;linguistic structure;existingneural networks;linguistic transition;scene generator;several models;art models"}, "298b72096b8a770b0cdb263dd53cf2463b8a1a1d": {"ta_keywords": "deep language models;embeddings;text;causal adjustment;content;dimensional embeddings;paper acceptance;forum post popularity;real data;performance;method", "pdf_keywords": "observational text data;causal identification;causal estimation;causal inference;lowdimensional document representations;natural language tasks;semisynthetic data;causal bert;text representation;causal effects;language corpus;sufficient embeddings;causal adjustment;level document embeddings;text;empirical evaluation;empirical data;effect estimation;prediction tasks;downstream prediction task;supervised dimensionality reduction;level embeddings;supervised dimensionality reduction canwe;information;efficient estimation;supervised dimensionality reduction approach;data;document;unmeasured data;unsupervised counterparts"}, "ffa07e4d7c8fade2ded5ffeea7265d22d8a0c0ab": {"ta_keywords": "accurate 3d model;generative neural network;neural network;objects;cut images;ground truth;object;images;simulation;ground truth difference;model;method;difference;pair", "pdf_keywords": ""}, "41e49fd3af628f1c8201942a659769f7cc21d812": {"ta_keywords": "", "pdf_keywords": "label propagation;graphs;novel graph;label propagation method;graph;labels;popular graph;label distribution;large label set;nodes;simple graph;parallel learning;parallel learning tasks;learning problem;learning method;skewed labels;label scores;scale parallel learning;clustering;small datasets;large datasets;edges;clustering structure;algorithms;node;sparse representation;community structure;hierarchical clustering structure;min sketches;algorithm"}, "3bfa808ce20b2736708c3fc0b9443635e3f133a7": {"ta_keywords": "neural networks;gnns;node;graph;bottleneck;neighbors;prediction task;hyperparameter tuning;messages;information;range information;state;novel problem;additional weights;mechanism;art results", "pdf_keywords": "isgraph neural networks;graph learningwe;graph representations;optimal gnns;large scale representations;prevents gnns;distant nodes;gnn model;graph models;graph kernels;nodes;generalized neural networks;graph kernel;neural networks;range information propagation;bottleneck;gnns;gnn;training data;graph;node;network;noisy data;final graph;lower bounds;noisy noisy data;long path;hidden size;theoretical lower bounds;regulatory networks"}, "9ab3622b3a801b90907f3ee399f881764db05d06": {"ta_keywords": "false data injection attack;attack detection probability;constrained optimization problem;optimal parameter values;optimization;stochastic approximation;attack;constraint;cyber;agent nodes;prespecified value;lagrange;novel approach;physical system;estimates;problem;design;combination;update;paper", "pdf_keywords": "attack detection constraint;attack algorithm;noisy quantum system;attack detection probability;optimal linear attack;attack design;secure estimation;polynomial time attack;discrete time attack;quantum communication;novel attack design scheme;efficient attack;online attack;quantum network model;quantum computation;attack scheme;linear injection attack;stochastic detection scheme;many quantum information processing;consensus filtering;attacks;networked cyber;false data injection attack;networked physical systems;multiple agent nodes;noisy markov chain;data injection attacks;new attack;stochastic detection;kalman filtering"}, "f2e544c5333125ee30c1c34b08936b6ef87c97dd": {"ta_keywords": "spoken language systems;neural network architectures;novel networks;network architecture;network parameters;empirical tuning;system parameters;best features;tuning;development;implementation;novel approach;design;combination;discovery;approaches;approach;method;state;art performance", "pdf_keywords": ""}, "ecfac0d377db229d58bc88698ad3bfd4b384ef37": {"ta_keywords": "automatic argument identification;annotated arguments;arguments;scientific publications;review;peer;different computer science conferences;process;decision process;models;community;novel approach;various use;field;other domains;transfer", "pdf_keywords": "argument mining;automatic argument identification;argumentative extraction;argumentative content;argumentative text;argumentative labeling;argumentative analysis;annotated arguments;argumentative process;argumentative structure;findargumentative sentence annotation;argument spans;arguments;heterogeneous text sources;argument;extensive empirical evaluation;scientific publications;annotators;strongest arguments;example review;scientific texts;scientific paper;weargumentative text;automatic mining;extensive evaluation;scientific research;document;nonlinear learning;document parts;documents"}, "6ec6fa4e34200e13d80ee79b95d1cc6ec0f6b424": {"ta_keywords": "unstructured tasks;central engine;level systems;engine;machine;task;execution;environment;pilot;pilot study;collection;follower;set", "pdf_keywords": "task description system;human dialogs;natural language tasks;agent task completion benchmark;task completion;agent tasks;task completion model;topological task description;human dialogue;dialogue actions;task information;many simple tasks;tasks;robot;human annotators;human annotator;task execution;dialogue;natural language representation;dialogue dialogue;visual simulation environment;task definition framework;unstructured media tasks;task;natural language;agent;unseen task edh benchmark;language grounding;utterances;human follower agents"}, "51b0609155e3a63afd1dd7dcc3034a5950f90ee0": {"ta_keywords": "model predictions;features;model outcomes;influence;dataset;paper;procedure", "pdf_keywords": "indirect feature influence audits;indirect influence audits;indirect influence audit;influence audits;global feature influence mechanism;indirect features;indirect influence measures;direct influence measures;indirect influence;indirect influence problem;fairness audits;classifier;disentangle representation information;classifier being;proxy features;direct influence problem;influence;classification;feature;direct features;features;input feature;individual features;linear classification;representation training data;representations;model information;models;feature vector;representation"}, "1a27c23453d3f718d854ac4b57dcf3e81ac51aa8": {"ta_keywords": "active learning;transferability;acquisition model;distinct successor model;datasets;models;data;cases", "pdf_keywords": ""}, "9c403ca58853fbb223f6e9fce446bb638f291692": {"ta_keywords": "new annotation approach;high quality annotations;entity mention annotations;consistent annotations;material science synthesis;material science synthesis procedures;new corpus;procedural texts;corpus;important entities;new label inventory;domain experts;model;independent models;data;consistency;speed;wide range", "pdf_keywords": ""}, "33e5e4b079535957d1275497f8870ea57762a03d": {"ta_keywords": "sentence attackability;attackability;online arguments;argumentation;sentence content;proposition types;sentences;attack;relevant characteristics;sentence;external knowledge source;tone;characteristics;useful information;scale analysis;reasons", "pdf_keywords": "attackable sentences;sentence attackability;attackability;sentence motivates attacks;argumentative nature;sentence characteristics;attacks;online arguments;common attacks;argumentation;sentence arousal;argumentative form;computational linguistics;sentences;attack;sentence qualification;computational corpus;attackthe ability;successful attack;machine learning models;online discussions;propositional types;arguments;corpus;strongest argument;proposition types;art attack prediction model;sentence sentence sentencewe;inference;sentiment"}, "81dfa45c568d7c1d9771ba2a1f07dad96558cff6": {"ta_keywords": "novel sequential pattern classifier;sequential pattern classifier;phoneme data;nonlinear classification;classification;kernel methods;conventional hmm;conventional hidden model;emission probability density functions;applications;pdfs;method;extension;paper;variety;steady improvements", "pdf_keywords": ""}, "be12a8d9ddb12c9ed292430c38d50093191dd442": {"ta_keywords": "clustering;new algorithm", "pdf_keywords": ""}, "90db4ddb08df23a4c587e6136e66cb388311473b": {"ta_keywords": "text categorization;several learning methods;feature selection method;generalization performance;features;filters;collaborative system;collaborative setting;direct transfer;transfer;more conventional evaluation settings;performance;data;several concrete proposals;beneficial variation", "pdf_keywords": ""}, "d409ff05d70f7b9787baf6431a84a178ad726e8d": {"ta_keywords": "soft constraints;constraints;constraint;human decision making;agents;agent;cognitive model;demonstrations;objectives;total reward;trajectory length;novel system architecture;new settings;mdft;method;novel method;number", "pdf_keywords": "inverse reinforcement learning;novel inverse reinforcement learning;reinforcement learning;reinforcement learning framework;simple reinforcement learning experiment;stochastic constraint learning;artificial agents;human decision making;effective artificial teams;stochastic constraint prediction;soft constraints;human decision;constrained environments;decision maker;deterministic decision;learning process;constraints;many agents;agents;agent;collective orchestration;orchestration;complex behaviors;diffusive constraints;constrained grid environments;constrained grid environmentswe;collective model;objectives;inverse;mdp"}, "f16c0699a873b0209a370e8e6301b0189785c614": {"ta_keywords": "constraint selection;active learning;best constraint;constraints;random selection;selection;uncertainty;problem;novel approach;set;previous works", "pdf_keywords": ""}, "363eb288abf76f7ab52d7789b30399b4b909dd5a": {"ta_keywords": "optimal bribery schemes;voting system;best possible election;vote;best candidate;combinatorial model;optimal procedure;linear combination;best possible outcome;best choices;process;problem", "pdf_keywords": ""}, "0d6a4e45acde6f47d704ed0752f17f7ab52223af": {"ta_keywords": "reinforcement learning tasks;natural language instructions;reinforcement learning;natural language properties;test tasks;unseen tasks;hierarchical model;novel strategy;novel approach;instructions;approach;large set", "pdf_keywords": "arbitrary tasks;imitation learning;task learning;hierarchical tasks;reinforcement learning;reinforcement;tasks;task robot;level task;new tasks;craft;reinforcement learning algorithms;interactive language acquisition task;human demonstrations;reinforcement learning framework;few tasks;generic reinforcement learning algorithm;difficult tasks;complex taskwe;supervised learning experiment;press craft;strategy training;task;agents;reinforcement learning approach;agent;reward signals;learning model;efficient learning;dimensional state reconstruction tasks"}, "88b66f705a329da8292e7b8aa4bfe26de4759cfa": {"ta_keywords": "machine translation;alignment;words;new approach;concept", "pdf_keywords": ""}, "a901185ee0710770420044cace33003109d478e3": {"ta_keywords": "informative rating systems;rating system;answer labels;online platforms;answer choices;rate;question phrasing;numeric interpretations;design;choice;true underlying distribution;problem;manner;convergence", "pdf_keywords": "effective rating systems;informative rating systems;rating systems;rating system;rating system designs;new rating system;robust rating system;rating scales;online ratings;true rating behavior;rating scale;standard rating systems;informative ratings;standard numeric rating systems;new rating scale;multiple rating scales;other ratings contexts;aggregate ratings;ratings;rating distribution;fewer ratings;rating;rating inflation;standard numeric rating scales;such ratings;average rating;level rating scale towe study;incorrect ratings;verbal rating scale;raters"}, "ee9f40f1c1e77b0b39b6e4a158208614fb4995c0": {"ta_keywords": "urban anomalies;traffic anomaly;urban datasets;individual anomaly;urban sensors;unexpected crowds;trajectory records;surveillance cameras;trip records;event records;comprehensive survey;social media;diverse devices;state;various types;main types;overview;art research", "pdf_keywords": "urban anomaly analytics;datadriven urban anomaly analysis frameworks;new urban anomaly analysis frameworks;urban anomaly analysis;urban anomaly detection;urban anomaly analysis frameworks;urban anomaly detection method;urban anomalies;urban big data;urban dynamics pattern mining;big urban data research;urban data;cityurban anomaly analysis;urban big data era;anomaly detection;anomaly occurrence;urban dynamics patterns;anomaly analysis;traffic anomalies;urban computing;urban dynamic pattern;anomaly;urban dynamics;spatial anomaly;anomaly types;spatiotemporal anomalies;abnormal spatiotemporal patterns;video anomaly detection methods;urban environments;anomalies"}, "5bcbc4554a68b38ff4a22b848fb0817b809608b2": {"ta_keywords": "harmonic laser pulse;harmonic frequency comb;harmonic oscillator;spectral density;frequency comb;mode;calculation;new method;method;combination", "pdf_keywords": ""}, "c159725940750adbad262ac946ce161bb68e41b5": {"ta_keywords": "automatic speech recognition;dynamic convolution;elementary particle;asr;model;elementary steps;attenuatement;end;self;process;art model;architecture;computation;steps;discrete sequence;state;context;order;form;novel approaches;better performance", "pdf_keywords": "end speech recognition;speech recognition;automatic speech recognition;robust speech recognition;advanced encoder;speech recognition problems;long short term memory;encoder;short term memory;convolutional layer training;convolution weights;input feature length;convolution layer;dynamic convolution;attention weight;convolutional layer;decoder;encoder side;convolutional layers;flexible convolutional networks;convolution;e2e;speech;attention;dimensional multidimensional pattern recognition;depthwise convolution;input;dot product;asr;vertex"}, "ec6499842d3e51b7dda94f5d0620d6df5c1a1b6d": {"ta_keywords": "unwritten speech languages;unwritten languages;unwritten multilayers systems;unwritten representation;speech;representation;building systems;text;feasibility;sufficiency;article;ii;paper investigates;need;aim;second step;approaches;first step", "pdf_keywords": ""}, "55faed1fbb1575ffa2609bdc4490586e30df441a": {"ta_keywords": "machine translation accuracy;machine translation;clqa;accuracy;manual analysis;data;words;frequency;result;factors;paper;relationship", "pdf_keywords": ""}, "e52115834ac7a529b1f4a7769dd538f143cf3eea": {"ta_keywords": "storage codes;general erasure codes;repair schemes;code;codes;general characterization;explicit constructions;classes;structure;framework;comparison;relevant parts", "pdf_keywords": "perfect bandwidth piggybacking codes;perfect bandwidth mds array codes;perfect bandwidth repair schemes;perfect bandwidth repair scheme;piggybacking codes;linear repair schemes;mds array codes;general piggybacking codes;scalar mds base codes;linear repair scheme;equivalent repair scheme;node mds array code;piggybacking framework;perfect bandwidth codes;perfect bandwidth code;valid repair scheme;array codes;repair schemes;piggybacking functions;repair matrices;storage;minimum storage;repair scheme;general linear codes;repair matrix;perfect linebacking codes;minimum repair;optimal repair;array code;achievable repair"}, "777d7b4141c9ce163de99b747e94c8d1db12e11e": {"ta_keywords": "machine learning experiment;prediction;fairness;input;extra features;outcome;novel framework;framework;subset;essential ones;rest;paper", "pdf_keywords": ""}, "89e53f116ef732d0abe81ee2218fa862ddc5ddce": {"ta_keywords": "speech translation toolkit;translation systems;benchmark datasets;toolkit;data preprocessing;pipelines;extraction;versions;wide range;training;recipes", "pdf_keywords": "speech translation toolkit;speech translation;end speech translation;high fidelity translation pipeline;machine translation models;machine translation;translation adaptation;deep neural signal processing pipeline;translation process;speech recognition;deep learning;resource speech;different automatic speech recognition;corpus;speech;inference pipelines;state corpora;biological speech;translation;conditional amplitude generation;corpora;translation process andwe;scale corpora;end feature extraction pipeline;multilingual textwe;multivariate neural signal;machine learning framework;sequence models;automatic source;conditional amplitudes"}, "6d654bab72d062d91f731331f16ea01d7cac0812": {"ta_keywords": "gender bias;narrative elements;biases;bias;literature;examples;occurrences;sample;study;elements;single feature;set", "pdf_keywords": "diverse gendered tropes;gender bias;tropes;social bias;bias;trope;diverse genres;author gender;genderedness;tvtropes;narratives;literature;gender;television;demographics;television series;media;popular novels;diversity;popular culture;single episode novels;novel dataset;classic television series;film;popular lists;female;examples;relevance;occurrences;datasets"}, "e79d1206292bc5e67ba19737d87d4b2ea4a37105": {"ta_keywords": "latent subword representations;subword tokenization module;subword tokenization end;deep model;new inductive bias;charformer;soft gradient;characters;gbst;byte level;part;model;extensive experiments;data;end;fashion", "pdf_keywords": "subword tokenization module;efficient subword tokenization;latent subword representations;subword tokenization end;stringon subword tokenization;latent subwords;subword features;subword blocks;subwords;subword length;subword;novel subword;deep transformer encoder;word recognition;soft probabilistic representation;lightweight tokenization method;deep signal representation;encoder;text classification tasks;natural language structures;gene transcription;character hash embeddings;level encoderdecoder;long document classification;charformer module;encoderdecoder stack;c4 corpus;charformer;charformer model;english tasks"}, "4cdd533963d8fb21fbf4bb3487bf6a6d60e14e93": {"ta_keywords": "cell image segmentation method;chinese restaurant process model;stochastic process;conditional iteration mode;random field;model;pk mode;paper;version", "pdf_keywords": ""}, "396e942542904dd32d0d70daa39613e5a27cc059": {"ta_keywords": "large streaming datasets;graphical learning;learning;memory cost;time;pass;methods;paper;conventional methods;terms", "pdf_keywords": ""}, "d7851e80f6072991bc99e2157f05515564f894f4": {"ta_keywords": "structured learning;wave;model;novel approach;method", "pdf_keywords": ""}, "254491f0d981fb5d796c374287d439d8d1967088": {"ta_keywords": "immunities;genotype;adolescents;primary health indicator;gsp;parent child;systematic study;effects;possibility;recent discovery;new class;better candidate", "pdf_keywords": ""}, "e68762a32ec91587d9761030fc75a8f5ee71c45b": {"ta_keywords": "topic tracking models;latent dirichlet;mixture modeling;unsupervised adaptation;speech recognition;topic;recognition output;uncertain observations;large uncertainties;input;improvements;different tasks;model;selection variable;recent approaches;problems;extension;problem;account", "pdf_keywords": ""}, "e66ade4e28d9f401277194ed8feea5c6e9f18253": {"ta_keywords": "terrorist groups;clusters;operational similarity;groups;similar behaviors;diversity;organizations;operational repertoires;novel computational framework;co;stability;last decade;difference;overall activity;factors", "pdf_keywords": "global terrorism database;terrorist organizations;operational similarity patterns;terror organizations;terrorist organization;terrorist groups;terrorist operations;operational similarity;global terrorism;terrorist group;clustering attacks;terrorist behaviors;similarity patterns;attack patterns;clustering;similarity;operational patterns;clusters;yearly clusters;collective behaviors;same cluster;operational profiles;similarities;ofterrorist organizations;outlier groupswe;groups;attacks;patterns;spatial patterns;complex networks"}, "82c4be27b0803c08c56bba4352669c1230a3ea19": {"ta_keywords": "repair;storage capacity;total repair;data reconstruction;helper nodes;erroneous nodes;codes;network;extensions;flexible parameter;dimakis et al;fundamental limits;number;time;certain number;presence", "pdf_keywords": "minimum repair bandwidth;regenerating codes;generalized minimal repair;minimal repair approximation;minimum repair;wireless networksa repair scheme;optimal exact repair;storage nodes;repair scheme;minimal repair;resilient repair procedure;node repair;perfect repair erasure;storage systems;omniscient repair mechanism;optimal storage capacity;various information storage schemeswe;error resiliency properties;optimal repair procedures;bandwidth adaptive;repair decoder;storage network;minimal storage capacity;arbitrary capacities;repair data;storage;storage system;repair requests;node storage capacity requirement;storage capacity"}, "4c42d6412c080fef23ad95b4469efe9cf321ae5d": {"ta_keywords": "unpaired speech;corpora;differentiable loss;loss;text modalities;text data;speech;extensive results;data quantity;impact;consistent gains;terms;techniques;method", "pdf_keywords": "speech encoder;end speech recognition problem;unpaired speech sentences;unpaired speech;memory tts encoder;encoder;differentiable speech;neural machine translation;conventional supervised training;probabilistic prediction;tts predictions;neural networks;neural network;corpus;neural state discrimination;sequence model;decoder model;supervised learning;cycle prediction;text data;sequence;end feedback loss;tamodystam;tthorayama;encoding;seq2seq models;memory;signal model;training;tts"}, "a714ca5254fb3cd7b06ead36d026c4eb154a7134": {"ta_keywords": "disparate features;disparate treatment;disparate treatment protocol;learning processes;nonsensitive features;disparate impact;processes;treatment;characteristic;family", "pdf_keywords": "quantum classification;discrimination mechanisms;motivated discrimination mechanisms;dynamic learning processes;classification;discrimination;machine learning;aware learning;classification problems;sensitive features;learning;fair representations;classification feature;fairness framework;treatment disparity;disparate treatment;optimal decision rules;empirical representation;global decision rule;disparate impact;disadvantaged group;bayesian representation;representations;representation;fairness;algorithms;empirical proxy variables;impact parity;fundamental distinction;new representation"}, "69320030be096e78380a097810554b648e7409c0": {"ta_keywords": "speaker clustering;speaker;statistical properties;mathematical model;mathematical physics;unified approach;class;method;problem;problems;information;novel method;unified framework;fact", "pdf_keywords": ""}, "0639cbb07ec3e03de7c8c1d828a90049c92cf5df": {"ta_keywords": "perovskite crystal chemistry;orthorhombic perovskites;perovskites;perovskite;chemical bonding;asno3;importance;detailed study", "pdf_keywords": ""}, "62924cef027a66a75b5465ebb7a926c06f95790f": {"ta_keywords": "domain adaptation;novel domain adaptation algorithm;adversarial alignment;adversarial algorithms;real datasets;standard domain;domain;robustness;algorithm;empirical benefits;theoretical assumptions;significant improvement;relaxed version;approach", "pdf_keywords": "adversarial distribution distance;adversarial training task;adversarial training;adversarial training strategy;target domain classification;adversarial learning;adversarial networks;domain adaptation approach;domain learning;domain classifier learning;synthetic dataset;source domains encodings;target representation support;relaxed distribution matching objective;source representation;underlying distribution alignment;relaxed distribution alignment;gan objective;source domain;adaptive domains;source domain support;dependent classifier;real datasets;relaxed distribution alignment condition;optimal distribution matching condition;classification;good target domain performance;generalized gradient approximations;target domain;target distribution"}, "9b5cf607f9cd3eb5ef47d3597bb9360ea6034264": {"ta_keywords": "peer review system;computational science;paper;performance;tutorial;collection;structure;framework;experiments;art;field;aim;goal;current state", "pdf_keywords": ""}, "9a7a4f125d8016e0fad9f6f5e9e0bca4e38b0784": {"ta_keywords": "scalable probabilistic logic framework;relational networks;order stochastic logic program;parameter learning;parallel stochastic gradient descent;inference;weight learning;queries;order theories;framework;novel;study", "pdf_keywords": ""}, "684e712f59f11d2bdc98be4c210824ab9e6f11f4": {"ta_keywords": "unlabeled relational graphs;latent relational graphs;novel transfer learning framework;different embeddings;unsupervised learning;graphs;glove embeddings;elmo embeddings;generic dependencies;data units;task;pairs;idea;framework", "pdf_keywords": ""}, "22655979df781d222eaf812b0d325fa9adf11594": {"ta_keywords": "knowledge bases;knowledge schemas;new dataset;questions;key features;documents;hotpotqa;strong supervision;level;explainable predictions", "pdf_keywords": "machine reasoning;machine reasoning systems;qa dataset;reasoning ability;knowledge base;natural language structures;knowledge schema;natural language;reasoning;new dataset;explainable reasoning;factoid comparison questions;questions;universe;unstructured text;dataset;sentence structures;intelligent systems;descriptive questions;complex questions;prior knowledge;recent discovery;test questions;facts;linguistic representations;explainable predictions;sentences;wiki context;language representations;galaxies"}, "0f726fcd676baff957574b223b99fd84163ebe6e": {"ta_keywords": "stacked graphical learning approach;graphical learning;base learner;tables;other related instances;features;algorithm;instance;thesis;problems;approach;predictions;performance;scheme;set;kind", "pdf_keywords": ""}, "4dfa9de9b3b2b222ddbdda934975bf608b8e1fda": {"ta_keywords": "conversation;dialogue;dialogues;novel annotation schema;annotated data;training classifiers;constructiveness;cues;usefulness;users;novel method;system;method;observation;appropriate set", "pdf_keywords": "group discussions;constructive conversations;collaborative discussions;group dialogues;conversations;conversation;dialogue data;scientific project meetings;discussions;participants;cognitive task;dialogue structure;14k utterances;study groups;simple cognitive task;negotiation dialogue agents;dialogue;group members;utterances;simple chat;chat;groups;cognitive psychology;negotiation dialogue;group collaborations;group;mechanical turk;task;annotation strategy;deliberation process"}, "bdf6ad58338279634d647447751442db8a6e2f77": {"ta_keywords": "neural networks;partial training;error surfaces;convergence properties;apparent convergence;critical points;weights;large movements;random initialization;nonconvex;weight space;loss;flat regions;symmetry", "pdf_keywords": ""}, "88051a6dce3b67541d8096647da2f6d31daa9e9a": {"ta_keywords": "language models;knowledge graph relations;entity spans;entities;language;document;joint distribution;relations;posterior probability;words;models;model;empirical improvements;new class", "pdf_keywords": "conditional language models;conditional language modeling;neural knowledge language models;knowledge graph information;structured knowledge graphs;knowledge graph;knowledge graph relations;knowledge graphs;baseline language model;wikipedia articles;natural language;topic entity;linguistic structure;structured knowledge;relational information;knowledge sources;relation generationwe;natural language generation;random text collections;art nlg models;posterior relation probability;structured data;conditional probability;nlg conditioning;bayesian framework;text span;knowledge;posterior probabilities;entities;vocabulary"}, "bc33c151a375d30d85a99d4e269185bad360b7bf": {"ta_keywords": "random medium;propagation;particle", "pdf_keywords": ""}, "72ae4bba9aaa30dfba45f6e7e076952a76e2d751": {"ta_keywords": "conversational models;conversational model;term memory language model;traditional language model;participant responses;participant role;context information;interoperability;available open access;model;experiments", "pdf_keywords": "conversational language model;conversation model;recurrent neural network language model;conversational structure;ubuntu conversation dataset;conversations;talk model;conversation;party conversations;language model;natural language systems;hierarchical recurrent;term memory;language generation model;speech recognition;natural language;topic feature;language model perwe;global topic information;sentences;context information;topic vector;neural network;word error rate;context;span context;participant role;ldm;participant roles;underlying context"}, "9b52f250376e07c2caddb5f43b8db8b2f300bb51": {"ta_keywords": "supersymmetric standard model;gaussian noise;noise;dynamics;effect", "pdf_keywords": ""}, "fd8b33299ce6ca81ce54e7d2de555a1a96ca96f1": {"ta_keywords": "structured discriminative models;speech recognition;natural language processing;structured sequences;structured models;nlp;classification;models;asr;field;problems;application;parameters;recent developments;article;approaches;recent work;variety;area", "pdf_keywords": ""}, "457e1c9476f08fa2c253982e3effcb364487073e": {"ta_keywords": "random process;random processes;certain quantum state;probability;certain state;nature;simple modification;similar modification;technology", "pdf_keywords": ""}, "b80ce55fbb4aa427439009985c0ce28a34324dc6": {"ta_keywords": "counterterms;simultaneous generation;counterterm;simple method;method;number;problem;functional form;assumption", "pdf_keywords": ""}, "e23c5dafc718f9e55ccf7729ce2d2834b650540a": {"ta_keywords": "speaker clustering systems;dirichlet process mixture model;structured utterance;novel speaker;speakers;estimation;method;case;large number;number;accuracy;experimental results", "pdf_keywords": ""}, "773e752ab6dc04b43aaf984bcbdd4895c9ab8c2f": {"ta_keywords": "continuous speech recognition;linear classifier;discriminative training;finite state transducer;acoustic models;perceptron algorithm;training method;linear models;linear graph;minimum phone error;wft;framework;scale;paper;scores;improvement;approach;experimental results", "pdf_keywords": ""}, "510aef8370d82c4c4ec50de0f645f34f11e549a7": {"ta_keywords": "recall protein entity recognition;protein entity recognition;dictionary hmms;conditional random fields;datasets;semicrfs;dictionary;new methods;performance;methods", "pdf_keywords": ""}, "7ddddea393c2cd70fe716e2dfc5d77daf58449c0": {"ta_keywords": "person content influence;influence content;influence;social networks;social media;content;people;others;article;person;power;method;novel method", "pdf_keywords": "social media influencers;person content influence;social network;social networks;social media;influence;influencewe;disinformation;ego network;heterogeneous influence;tweets;influential people;propagation;social dynamics;misinformation;causality;textual data;causality framework;impact;machine learning;spread;network;people heicks;certain individuals;certain topics;information;topics;classifier;alter network;individuals"}, "1890775da6ba2627a5d6c17a639e2dca7cdc388d": {"ta_keywords": "reversible switch switch switch switch switch;single switch switch;switch switch;single switch;switch", "pdf_keywords": ""}, "ccad27088b9098de4eaca8dc449b18766db4b3ab": {"ta_keywords": "style transfer sentences;style transfer;style transfer papers;world style transfer setting;automatic evaluations;diverse styles;unsupervised learning;language models;automatic metrics;art systems;tuning;variants;large dataset;outperforms state;simple method", "pdf_keywords": "style transfer tasks;text style transfer;simple unsupervised style transfer method;style sentences;style transfer datasets;style transfer;paraphrase generation problem;linguistic paraphrasing;novel style transfer model;text generation;sentence similarity;conventional style transfer systems;specific paraphrasers;level paraphrasing;style corpora;semantic textual similarity benchmarks;natural language processing;text prediction;specific paraphrases;recognizable stylistic features;natural language;language models;sentences;attribute transfer;new corpus;input sentences;semantic similarity;style distribution;syntactic survey;diverse styles"}, "703a8252585948a96f5815025f7f03d68033b8bf": {"ta_keywords": "agent system;bots;learning process;active users;play;self;user;presence;empirical example;large number;set;problem", "pdf_keywords": "automatic agents;conversational agents;conversational agent;human dialogs;dialogue systems;reinforcement learning methods;dialogue system;agent;dialogue self;agents;dialog;structured dialog;dialogue;simple dialog;negotiation;game;strategies;bot;strategy;learning;bots;good reward;human interaction;play;task;destination;booking;level;training;destinations"}, "fa6c76d466fef633df51745bad85e991c371622c": {"ta_keywords": "single laser pulse;laser pulse;single photon;laser field;laser;photon;parametric oscillator;frequency comb;generation;new method;method;application", "pdf_keywords": ""}, "41a47363d261459c594525ef330e5fccaa8518a0": {"ta_keywords": "authorship attribution;features;data;accuracy;art;relationship;continuous nature;type;new approach;state", "pdf_keywords": ""}, "98e6197e21ae530cd33eeff144ee556c5cf91dc8": {"ta_keywords": "cognitive learning;benchmark cognitive models;cognitive model;human domain expert;novel machine learning framework;student;study;feasibility;approach;quality;set;observed solutions", "pdf_keywords": ""}, "af679d69fcc1d0fcf0f039aba937853bcb50a8de": {"ta_keywords": "nested attention mechanism;context sequence modeling;long sequences;neural machine translation;language modeling;several benchmark examples;high accuracy;single linear;novel method;scalability;method", "pdf_keywords": "softmax attention;simple linear attention mechanism;nested attention mechanism;linear attention functions;regular attention function;attention operations;novel attention mechanism;sequence modeling tasks;neural machine translation tasks;several sequence modeling tasks;attention mechanism;context sequence modeling;neural machine translation;traditional attention mechanism;full attention;long range context model benchmark;sequence modeling;long sequences;neural machinewe;quadratic causal attention;causal attention;memory;sequence;language model;memory complexities;space complexity;scale language model;learning rate;tasks;luna recurrence algorithm"}, "682e69be87f181edcf71800b54083595874d4ec6": {"ta_keywords": "speaker trait prediction;classification accuracy;relative error reduction;hierarchical structure;social science;monte carlo simulations;predictions;model;features;novel algorithm;subtasks;approach;study;particular interest;problem", "pdf_keywords": "persuasive speaker traits;speaker trait prediction;intermediate classifiers;speaker traits;hierarchical hierarchical models;high level persuasion tasks;novel neural architecture;hierarchical model;passion networks;simple hierarchical model;neural network;artificial intelligence;hierarchical structure;prediction quality;supervised learning;different speaker traits;persuasiveness;prediction;prediction accuracy;persuasion agent;final classifier;speaker;traits;hierarchy;credibility;linguistic properties;persuasion;predictions;deep late fusion model;nodes"}, "7c8314e6138ce968f3b9f3bc55d5461ffbbec4aa": {"ta_keywords": "quantum information;global quantum network;quantum bus;quantum state;random variables;topological phase;nodes;network;local interactions;generation;local transformation;method;new method;sequence;form", "pdf_keywords": ""}, "97883f37c62b4b0e52cdc31dea1a375597db3804": {"ta_keywords": "single deep neural network;multiple tasks;multiple architectures;binary masks;individual networks;new task;task;least overhead;network;good performance;ability;accuracies;piggyback;applicability;method", "pdf_keywords": "imagenet classification;convolutional network;single deep neural network;new tasks;network quantization;large scale task;convolutional neural networks;convolutional neural network;new task;semantic segmentation;binary weight network;multiple tasks;tasks;networks;neural network;regular convolutional layer;mask weights;task;training;layers;learning;binary masks;simpler network;iterative learning;mask;supervised learning;task ordering;training process;weights;tune networks"}, "e4c8447e56fc9cc3867087748acc4b259b9efe19": {"ta_keywords": "recurrent neural networks;several text comprehension tasks;explicit memory;memory;directed acyclic subgraphs;encodes;sequence;edges;graph;benchmarks;model;distant elements;art results;new state", "pdf_keywords": "recurrent entity network;text comprehension models;recurrent neural networks;several text comprehension tasks;text comprehension;rnns;recurrent preprocessing;explicit memory;memory structure;natural language processing;linguistic representations;rnn;memory;deep learning;neural network;language model;explicit memory signal;neural networks;sequential data;linguistic data;cnn;sequence;sequences;term dependencies;symbolic knowledge;linguistic relations;nlp;next token;comprehension;representations"}, "be8d6a8d3dfe87a4d9171f25bf9a18d502498756": {"ta_keywords": "new algorithm;algorithm;global context;data;meta", "pdf_keywords": ""}, "bd1cf4279d834699db871e1451d289c49ff2b6de": {"ta_keywords": "dance dance revolution video game;step chart;steps;neural networks;chart;qualitative features;data;art approaches;method;novel method;vicinity;state", "pdf_keywords": "musical onset detection;choreographing;choreography;rhythm;raw audio track;musical recordings;music;dance;audio representation;onset detection;recurrent neural networks;dance dance revolution;deep recurrent neural networks;music signals;dance platform;convolutional neural networks;simple recurrent neural network;music onset signals;screen step charts;convolutional neural network;raw audio;electronic music;flat audio recordings;deep learning methods;onset detection problem;deep neural network;famous songs;rnn encoding;neural networks;convolutional encoding"}, "cf46ecac1cb1bdae153be2b909ff3e313034ac9e": {"ta_keywords": "social skills training aid;contextual differences;contextual information;verbal behaviour;modality;context", "pdf_keywords": ""}, "ef59f05a30972742a714b8903848e4b5dfc5cdaf": {"ta_keywords": "interpretable machine learning;use cases;workflow;cases;taxonomy;process;relevant information;methods;description;steps", "pdf_keywords": ""}, "9b9ee9a25fc4d9f8ad22c2923c49b8d5d0b83356": {"ta_keywords": "synonyms;hypernymy relationships;synsets;aware relationships;russian languages;sets;gold standard datasets;sense;method", "pdf_keywords": "word sense disambiguation procedure;distributional word representations;distributional semantics;hypernymy extraction;hypernymy extraction task;hypernymy datasets;word embeddings;aware hypernymy pairs;hypernymy relation;relevant hypernyms;synonyms;hypernyms;disambiguation;hypernym pairs;hypernymy relationships;corpus;mutually synonyms;word sense;wiktionary datasets;noisy hypernyms;hypernym;synset representations;reference words;natural language;dense synset representations;word senses;hypernywe;aware extraction;synsets;embeddings"}, "923ddc71f8a453c7995e97b0681a674224a5fc09": {"ta_keywords": "low quality translations;high quality translations;manual error analysis;features;models;methods;method;efficiency;work;experiments", "pdf_keywords": ""}, "407eacc5ade80b54126c300b57b81f4b4f411487": {"ta_keywords": "machine translation;strong machine translation systems;machine parity;recommendations;empirical findings;results;current best practices;set", "pdf_keywords": "human machine translation;neural machine translation systems;machine translation research;neural machine translation system;human translation outputs;machine translation;machine translation systems;machine machine translation systems;machine translation system;translation quality;different machine translations;human translation;language translation systems;human translations;different translation systems;expert translators;reference translations;translations;professional human translator;professional translators;different translation protocols;human evaluation;translation protocol;translationese texts;recent human evaluation studies;machine language;evaluation;source language;human language;machine parity"}, "4bf1ea102e1eb1246929bb77c11ebbd6b6d27500": {"ta_keywords": "emphsparse prototype support;strong language modeling performance;novel generative model;amortized variational inference;emph;prototype selection distribution;sparsity", "pdf_keywords": "good sparse prototypes;sparse prototypes;sparse neural editor;sparse language modeling;sparse prototype selection distribution;sparse prototype;sparse prototype distribution;novel generative model;large semantic variations;novel nonparametric generative model;generative models;sparse prototype support;neural language modeling;generative model;learnable prototype;sparse prototype set;better language modeling performance;amortized variational inference;prototype retrieval function;language models;large corpus;prototype selection distribution;stochastic variational inference;language modeling;sparse;strong language modeling performance;variational inference method;neural editor;prototype data;prototypes"}, "93a55f3341aa70bb42c0f76b112e2e8da27b3df2": {"ta_keywords": "entrainment;machine interaction;interoperability framework;dialogue acts;digital interoperability framework;european dialogue;eeg;selection;process;context;effect;choice;use", "pdf_keywords": ""}, "bed0452305633791340f80cb0be02f46e4a34b0d": {"ta_keywords": "voice conversion challenge;seq2seq;seq2seq models;sequence;speaker;identity;target;power;context;approach", "pdf_keywords": "voice conversion challenge;source speech processing toolkit;voice conversion;speech processing system;input speech;automatic speech recognition;voice;transcriptions;scale speech;vocoder;seq2seq baseline system;stage speech;sequence;speech;vocoder modeling;variational autoencoder;standard vocoder model;speaker interface;vector extractors;vocoder parameters;time waveform generation;target speaker;speaker;seq2seq;machine learning;oflingual data;tosequence;mandarin;pronunciations;asr"}, "ce97452d031a1a156212f038bab6f47a51575236": {"ta_keywords": "stance;prosodic features;spontaneous speech;new annotated corpus;speech;classifiers;fourway recognition;automatic recognition;binary detection;behavior;style;strong accuracies;elicit high densities;range;framework;paper investigates;different strengths", "pdf_keywords": ""}, "995f4e670c0cdcd5afdef08719c2528a682bff05": {"ta_keywords": "end speech translation model;memory resources;speed;naveveve model;corpora show;nave model;md;novel end;same computer;experimental evaluations", "pdf_keywords": "adaptive radar;intermediate automatic speech recognition;decoder;connectionist temporal classification;unstructured speech;encoder;end speech translation encoder;new training algorithm;term memory;automatic speech translation;asr;ctc;end speech translation task;convolutional neural network;speech;utterances;speed;md algorithm;noise ratio;adas;recent advanced network architecture;noise ratios;short distance limit;signal;nar;novel data structure;arbitrary message;decomposable sequence;best sequence;md"}, "e2198b039ee5bfa233cf06e65f26a9f3233ada9f": {"ta_keywords": "dialogue act;dialogue acts;specific dialogue acts;dialogue;corpus;entrainment;word choice;data", "pdf_keywords": ""}, "29da62b3f8aed3fe98b3f02bbfd436dd8e65a532": {"ta_keywords": "channel state;traditional channel state;wireless networks;optimal aggressiveness adaptation;collision avoidance;aggressiveness adaptation mechanism;network;throughput;message passing;synchronization;aggressiveness;nodes;principle;paper;technique;preliminary evaluation", "pdf_keywords": ""}, "4264599665522594d9ecb521dd2e1d002e85a961": {"ta_keywords": "fair matchings;paper matching;novel local fairness formulation;matching algorithms;fair fairness flow;fairness;new algorithms;fairir;algorithms;orders;formulation;critical issues;new formulation;magnitude", "pdf_keywords": "local fairness constraints;fairness algorithm;fairness constraints;novel local fairness formulation;relaxed local fairness formulation;local fairness formulation;common paper matching algorithms;paper matching algorithms;new paper matching algorithm;reviewers assignment;local fairness problem;peer review process;fair allocation problem;peer review;maximum reviewer;paper matching problem;paper matching;reviewer workloads;minimum paper score;techniquethe paper assignment problem;matchings;reviewers;fairflow;fairness;paper affinity;reviewer;new matching;fair assignments;reviewerswe;referee"}, "1578fba4a2b2ba819986e32c7da6ebbaf9aacf41": {"ta_keywords": "hierarchical neural conditional random field;lexical content;deep neural models;syntactic description;document;context;rcrg;novel training scheme;model;novel approach;morpho;changes;use", "pdf_keywords": "linguistical encoder;treebanks;crosslingual morphological tagging;most treebanks;linguistic data;hierarchical neural conditional random field model;linguistic constraints;syntactic description;natural language;theoretic representation representation representation representation;multiple languages;language clusters;languages;theoretic encoding;language;language family;morphosyntactic analysis;lingual feature selection problem;hierarchical neural model;morphological features;representation;source language;wise hierarchical neural model;encoder;crosslinguality;lemmatization;neural model;type encoder;attention;typology features"}, "6e2e7df21a5b5457ea4167133a40bc729028250d": {"ta_keywords": "ranking stacks;passage ranking;mean reciprocal rank;information retrieval tasks;preference judgments;top items;modern data sets;queries;datasets;relevant items;stacks;mrr;query;document;quality;tremendous improvements;number;observation;workers;approach", "pdf_keywords": "ranking tasks;top ranking;ranking;new top ranking;passage ranking;neural ranker iswe report;neural ranker;modern neural rankers;core information retrieval tasks;representative neural ranker;information retrieval;traditional information retrieval test collections;rankers;meta meta meta meta metagraph ranking;leaderboards;leaderboard;top query;mean reciprocal rank;marco passage retrieval development set;available evaluation queries;dense passage retrieval;evaluation queries;sparse label system;art ranker;benchmark problem;preferred query;accurate data evaluation;comparative assessment;top run;queries"}, "80257b7d02ad4d6a762ebc0d7f1560e0ef182354": {"ta_keywords": "automatic meaning preservation;polite sentences;stylistic attributes;sentence;source content;pipeline;target style;tag;meaning;new task;paper", "pdf_keywords": "polite style transfer;text style transfer;style transfer tasks;machine translationstyle transfer;linguistic transfer;linguistic style;sentiment transfer;style transfer;polite sentences;style transfer accuracy;politeness;style words;corpus style corpora;linguistic strategies;sentence classification;linguistic domains;encodes style words;free sentences;linguistic linguistic hierarchy;machine translation systems;automatic evaluation;underlying text;style attribute markers;corpus;content preservation;style coders;benchmark text;automatic transfer;different styles;style"}, "09093e29b1f705bb7a68ea2e9240b3f122efe92b": {"ta_keywords": "quantum system;quantum state;classical state;estimation;parameters;separation;method;new method;suitable choice", "pdf_keywords": ""}, "3edfccbe6adf18f5263cd2adf3d977bbc5811e0b": {"ta_keywords": "novel data augmentation method;encoder;asr encoder;speech signals;neural text;additional training data;speech;text;recognition;e2e;hidden states;sequence;characters;states;use;method;large amount", "pdf_keywords": "speech encoder;end speech recognition;neural machine translation model;automatic sequence recognition;automatic speech recognition;underlying speech encoder;asr encoder;neural text;novel data augmentation method;world speech dataset;encoder;neural textto;automatic speech recognition problem;translation model;encoder model;data augmentation method;encoder synthesis;classical encoder;language model;encoder network;additional training data;attention;training data;decoder model;signal translation;decoder;speech signals;deep learning representation;text;asr model"}, "e6aaac94df717786a467d057cb2157b9d49f0974": {"ta_keywords": "regret algorithms;arbitrary adversarial players;adversarial opponents;convergent opponents;regret;optimal response;prior tuning;game;knowledge;best response;ii;properties;family", "pdf_keywords": "regret algorithms;concave games;stochastic game;regret bounds;stochastic stochastic game;optimistic mirror descent;adaptive optimistic learning strategy;game theory;several different finite games;stable games;sum games;nash equilibrium;stable game;convex losses;game theory problem;optimal feedback rate;optimal learning;evolutionary game theory;learning strategy;convex actions;continuous game;players regularization;sum game;adaptive algorithms;stochastic payoffs;dynamic learning regime;adversarial players;individual regret;agent learning model;competitive games"}, "efaf07d40b9c5837639bed129794efc00f02e4c3": {"ta_keywords": "authorship attribution;gram features;continuous representations;neural network;datasets;features;art;model;paper;comparable results;experiments;state", "pdf_keywords": ""}, "9d9159026023f21e633f84fd61f3efad2e410214": {"ta_keywords": "order logic embeddings;knowledge base completion;latent continuous representations;embeddings;scalable matrix factorization approach;datasets;task;approach;problem;experiments;effectiveness", "pdf_keywords": ""}, "46f66dd37e6366ce102cfd97e718947151d5b1eb": {"ta_keywords": "basic fake news detectors;social media content;social media;misinformation;news post;propagation;environmental signals;observation;method;novel method;performance", "pdf_keywords": "fake news detection;basic fake news detectors;real news articles;news environment;fake news articles;external news environment;micro news environments;underlying news item;news items;current news items;recent mainstream news items;current news outlets;fake news;news reports;news report;social media;public attention;recent news;detection model;recent mainstream media opinion;false detection;social network;social networks;detection;best news;new framework;fact;representative datasets;early detection;empirical evidence"}, "8122eaeb63098e94416108df918c9669e9105e65": {"ta_keywords": "accelerated data storage;cluster cache;online erasure;erasure;erasure splitting;data stream;memory;processing;ii;objects;small amount;novel approach;problem;novel;combination;high degree;approach;control", "pdf_keywords": ""}, "8d64be0d3bb2650ff99a4c1ae8049eb5fece27a1": {"ta_keywords": "emotional speech recognition problem;emotional speech recognition;deep neural network;bottleneck features;bottleneck;layer;normal model;performance;system;method;new method;paper;improvement;sturucture", "pdf_keywords": ""}, "f8f17f32e651840531276423c7196856d27bcdd0": {"ta_keywords": "time stochastic model;time stochastic models;harmonic oscillator;mode;method;new method;class;construction;use", "pdf_keywords": ""}, "ee7af49291c030a3e29ad7a9cb5c1975d1b644f4": {"ta_keywords": "counterterms;simultaneous generation;counterterm;maximum number;simple model;simple method;model;method;generation;concept;functional", "pdf_keywords": ""}, "1cbb43b4d7f79d986a4a78ad3b53368c49e496ee": {"ta_keywords": "reverb challenge;deep neural network feature enhancement system;speech recognition;single channel front;art tandem;average word error rates;joint submission;back;system;real evaluation sets;paper;end;state;approach", "pdf_keywords": ""}, "d0fbae81d870bbfb34430654f70fd6a21e8bd1cc": {"ta_keywords": "law relationships;machine learning;neural network architecture;data;power;neuroscience;novel method;method;problems;variety", "pdf_keywords": "novel recurrent entity representation;recurrent neural network;recurrent layers;coreference annotations;recurrent layer;natural language processing;memory model;memory structure;entity mentionswe;powerful reading architecture;coreferent dependencies;rnn;nlp;memory;sequential recency;linguistic structure;entity;term dependencies;entities;coreferent recency;reading;writing tasks;multiple mentions;sequential antecedent;interested graph models;coreferent;text;unstructured query;convolutional neural network;layers"}, "071216d944bcd2f05deafdb94e657167cce148d9": {"ta_keywords": "method", "pdf_keywords": ""}, "1ebf54c0a8b38e8c26ed857cb9d4e565a8f17f17": {"ta_keywords": "structured data;similarity;random graph walks;objects;framework;problem", "pdf_keywords": ""}, "72a5c01afe276d06ca9179e24b1c925e206454f3": {"ta_keywords": "knowledge graphs;review;explanations;content;approach;power;new method", "pdf_keywords": "recommender system;recommender systems;knowledge graph;personalized algorithm;knowledge graph entities;ranked list;recommendations;personalized procedure;knowledge;big data analyst;new information;entities;explanations;entity;graph;new method;analytical tools;user;interaction;faster deployment;items;method;difficult task;crucial step;decision;applications;new domains;technique;training;combination"}, "f800f60db4427a51e564f1b875ae01d2c642fdce": {"ta_keywords": "functional repair;simple exact repair code;exact repair;storage;bandwidth tradeoff;separate tradeoff;case;existence", "pdf_keywords": "network coding;storage networks;single repair node;exact repair code;storage nodes;repair codes;storage network;repair code;replacement node;transfer codes;network node;single storage node;storage tradeoff;minimum storage;storage systems;bandwidth tradeoff;repair scenario;storage;nodes;problem ofthe storage;repair;network;functional repair;bandwidth tradeoff iswe;code;simple repair;bandwidth tradeoff curve;node;codes;networks"}, "b2f46145f2a50b609482a69d0581b218a6767cef": {"ta_keywords": "traditional knowledge integration systems;information integration;integration systems;information sources;retrieval methods;novel logic;integration;knowledge;logic;architecture;novel system;system;whirl;process", "pdf_keywords": ""}, "80fdacd50ba9ad2e594dd2ddb0b1fa0e591f37ea": {"ta_keywords": "structured prediction;event extraction;joint inference;extraction tasks;task;search;models;complex information;simple domain;adaptation method;accuracy;performance;paradigm;potential", "pdf_keywords": ""}, "d46ecbacf42748ac9ce1fecd9f1b4ed0b9e34980": {"ta_keywords": "email speech classification;email speech act prediction;email speech classification error rate;gram sequences;messages;contextual information;new open source toolkit;terms;novel representation;new approach", "pdf_keywords": ""}, "b350be3836c3d183464642815b26b061f24e8314": {"ta_keywords": "integer embeddings;mathematical world;embeddings;numerical methods;properties;set;combination", "pdf_keywords": "integer embeddings;number embeddings;embeddings;dimensional embeddings;integer representations;distributional semantics paradigm;mathematical sequence data;numerical reasoning tasks;mathematical knowledge;integer sequences;integers;language model;integer properties;classifiers;integer;learning model;integer value;corpus;sequence completion problems;linear regression classifiers;classifier;representations;new learning model;integer iswe;encodes;logistic regression;predictive ability;sample sequences;usefulness;vocabulary"}, "e602bde46bca5f424a3d53675c1275386544eb1e": {"ta_keywords": "harmonic trap;dimensional harmonic oscillator;harmonic oscillator;dynamics", "pdf_keywords": ""}, "7fcc2cc70498e409168a6c3dfd7c59652b1160c2": {"ta_keywords": "single feature transformation matrix frame;deep neural network;posteriori linear regression;multiple transformation matrices;feature;adaptation;matrices;dnl;structural maximum;space structure;frame;naxy;fsma;space;weighted sum;paper;method;experiments", "pdf_keywords": ""}, "dda3f2a2803c80e5b3332868bf86901d6239befc": {"ta_keywords": "unbiased compression;optimization methods;stochastic;iteration complexity;sgd;local updates;error compensation;methods;method;new theoretical frameworks;ones;reasonable assumptions;thesis;analysis", "pdf_keywords": "stochastic solverswe;stochastic solvers;stochastic reduction methods;stochastic generalization;stochastic methods;local stochastic solvers;stochastic reduction;stochastic optimizationin;stochastic differential equations;stochastic problems;stochastic learning;stochasticity theorywe;stochastic gradient methods;stochastic optimization problems;novel stochastic;stochastic version;stochastic realizations;nonlinear stochastic problems;stochastic optimization methods;local stochastic algorithms;large stochastic gradients;stochasticwe;stochastic gradient basedwe;stochastic gradientwe;local stochastic methods;simple stochastic gradient solver;stochastic approximation approach;stochastic stochastic gradients;stochastic optimization;new stochastic gradient"}, "9dc4a5284ecfd37ab8bc8990eddf1b39113e004b": {"ta_keywords": "machine translation;local context;translation;language;target domain mismatch;source;degradation;domains;metric;back;concept;effect", "pdf_keywords": "training machine translation systems;parallel translation data;neural machine translation system;machine translation systems;machine translation;translation data;machine translation system;low resource language pairs;domain adaptation;monolingual data;target language;backtranslation;translation;language pairs;target corpora;target domain mismatch;low resource data;source domain mismatch;same corpora;target domains;linguistic data;same language;corpora;particular language pair;different languages;social media data;different texts;new languages;parallel dataset;language"}, "128610c7df12bff1610949c551b6236cb350dcd9": {"ta_keywords": "language models;novel modality conversion mechanism;text representations;logographic languages;modality gap;speech;models;bert;model;nar;inference;target length;ctc branch;paper", "pdf_keywords": "end speech recognition;novel nar attention;speech recognition;automatic speech recognition;speech recognition system;bert encoders;language models;attention;nar models;attentionwe;joint attention;fast inference speed;new attention;recognition performance;speech;novel modality conversion mechanism;text modalities;text;nar;text modalities1;asr;recognition accuracy;bert;high accuracy;textin;models;modality gap;input;model;new model"}, "4e1d27c68a60bfd8393462107677469bf286f0f8": {"ta_keywords": "pragmatic program;program synthesis algorithm;program synthesis problem;program;new inductive bias;communication bias;bias;user participants;preference;notion;specifications;user;end;paper", "pdf_keywords": "pragmatic program synthesis system;pragmatic program synthesizer;program synthesis;pragmatic computation;recursive pragmatics;incremental pragmatic model;pragmatic speaker;pragmatic communication;recursive reasoning models;pragmatics;pragmatic interpretation;pragmatic response model;incremental probabilistic speaker pairs;informative listener pairs;program;natural language;output examples;interactive learning;naive implementation;simpler simpler speaker;probabilistic communication;like reference game;programs;small program;simpler speaker model;programwe;human speaker;listener pairs;machine listener;literal variant"}, "09e4e0eee756da5658c6d572871130d53a89c72b": {"ta_keywords": "optimal bic recommendation policy;novel probabilistic predictive model;probabilistic probabilistic predictive model;optimal policy;predictive model;decision subject;features;substantial computational gain;better outcomes;standard baselines;number;task", "pdf_keywords": "novel probabilistic persuasion model;bayesian persuasion framework;probabilistic decision rule;optimal decision maker;arbitrary decisions;optimal decision subject;decision maker utility;rational decision maker;persuasion techniques;probabilistic signaling policy;strategic decision subjects;action recommendation;persuasion;bandit exploration;decision decision decision;strategic learning;desirable actions;linear decision rules;optimal signaling policy;novel probabilistic recourse framework;optimal policy;decision maker;decision maker gains;decision makerthis article;new optimal signaling policy;decision subjects;signaling policy;decision subject;decision subject parameters;incentive"}, "933b03a81110676f4c61c449f1926ebd58bc47f7": {"ta_keywords": "blind user interface;dynamic touchscreens;open touchscreens;reverse engineering system;state diagrams;interfaces;statelens;style vision pipeline;design;state;art capabilities;users;crowd", "pdf_keywords": "user interfaces;engineering user interfaces;dynamic touchscreen interfaces;graphical touchpoint agent;touchpoint interface;user interaction information;user interaction;individual user interface research;user interface design;touchpoint interaction;user interactions;digital touchpoint agent;new digital touchpoint agent;dynamic touchscreens;touchpoint models;touchpoints;unfamiliar dynamic touchscreen devices;touchscreens;friendly interfaces;touchpoint feedback;touchpoint machine;touchpoint state transition;interactive guidance;unfamiliar touchscreen devices;blind users;interface;capacitive touchscreens;screen;physical interfaces;inaccessible dynamic touchscreen deviceswe"}, "d462eae8dd5c1415e03651b9fc1c2ca80a69521f": {"ta_keywords": "article selection task;student model;human student behavior;novel learning mechanism;background knowledge;student;selection;coverage;model;novel method;method;human;problem;case;large amounts", "pdf_keywords": ""}, "730e5e83586dd5784051f933e7bb82571cec4c94": {"ta_keywords": "speaker separation;speaker counting;speaker diarization;end speaker diarization;speech separation;separation masks;speakers;convolutional layers;scalable architecture;parallel architecture;architecture;number;set;framework;use;novel framework;end;post;novel", "pdf_keywords": "speech separation network;domain audio separation network;speech separation methods;domain audio separation;speech separation;end speaker diarization;speaker diarization;speaker counting;noise separation;automatic speech recognition;neural online source separation;speaker;speech activity;baseline separation models;speech activities;speaker level;separation;separation process;conference channel;speakers;separation masks;speech;convolutional module architecture;deep learning;joint diarization;convolutional layer model;convolutional layers;convolutional time;input signal;noisy representation"}, "1548142a6be92f41e45dcbde9ff8afd71134ac1d": {"ta_keywords": "lung cancer risk;polycyclic aromatic hydrocarbons;incremental lung cancer risk;mean concentrations;diagnostic ratios;median values;systematic study;ilcr;m3;analysis;pss;ng;ps;study", "pdf_keywords": ""}, "f48792e8a24e369c80e39a2a2b7451d108f02941": {"ta_keywords": "explainable answering;implementation;novel implementation;interpretation;interface;xqa;computational model;approaches;system;access;validity;user;end;position paper;hybrid;circulation", "pdf_keywords": ""}, "642c85d35b4a3cc9648b269e32fe9d0a18907c98": {"ta_keywords": "continuous speech separation;online processing framework;global modeling framework;path modeling framework;path transformer;models;local window dependencies;window;css;task;better evaluation;computation amount;relative cross;work", "pdf_keywords": "continuous speech separation;speech separation;channel speech separation;monaural speech separation;long recording separation;variancecontinuous speech separation;speech recognition;window processing;separation;speaker;path modeling framework;path models;path feature;novel feature extraction method;path modeling;short memory;machine learning tasks;global modeling framework;segmentation stage;long conversation;single acoustic wave;rnn;path transformer;signal processing;global feature;pipeline;free speech;cnn;path modeling stacks;new computational method"}, "acf0ccc8b67cc441c51d4281c305359073b9c7cc": {"ta_keywords": "speech translation;decoder models;encoder model;decoder;lipatov network;convolutional neural network;neural networks;kuraev;novel approach;combination", "pdf_keywords": ""}, "9cfc4e94e76d8025cd86d6652a641b1440681d28": {"ta_keywords": "linguistic structure;corpora;universe;structure;linear algebras;algorithm;new algorithm;characterization;convolution;terms;information;results;problem", "pdf_keywords": ""}, "d0a6b70c9dc1942169f48211d47843732c57a3a9": {"ta_keywords": "agnostic navigation representations;navigation;navigation model;navigation policy;agnostic learning;unseen environments;training;environments;environment;strategy;novel strategy;tasks;baselines;performance gap;extensive experiments", "pdf_keywords": "novel navigation tasks;supervised navigation;richer natural language guidance;generalized navigation model;functional navigation models;navigation;functional navigation model;navigation strategy;agnostic learning framework;level navigation;agnostic training;multitask history tasks;generalized environment representations;natural language;primary navigation metrics;agnostic agent;learning;intelligent robots;novel navigable direction model;structured representation;agent;multitask environment;training data;exploration;unstructured learning network;supervised translation representation;vision;free reinforcement learning approach;navigable direction model;reinforcement learning"}, "cdf17da4a7638985cb62a5dbf1161239b315eb85": {"ta_keywords": "topic models;entity link;mixed membership stochastic block models;protein interaction;protein;entities;entity;links;model;text;prediction;parameters;aspects;np parameters;ppi;ability", "pdf_keywords": ""}, "c0484ac1677b942e8b06ea0ac3cad5b01e52ced4": {"ta_keywords": "quantum system;harmonic trap;quantum state;single quantum;single quantum particle;dynamics;bath;system", "pdf_keywords": ""}, "092b80cc6250f74a2c1e0ba7820c31a8f0153c0a": {"ta_keywords": "literature variance;novel invisible cities;digital literature;literature;empirical representations;variance;new literature;unsupervised methods;cities;discovery;article;quantitative description;single author;computational methods;intrinsic structure;results;methods", "pdf_keywords": "novel cities;literary literature;word representations;literature;word representation;city descriptions;art text representation methods;linguistic similarity;imaginary cities;narrative texts;invisible cities;novel;imaginary city;embeddings;linguistic structure;representations;representation;linguistic structurewe;novel structure;natural language;empirical discovery;reader;similarities;scale language model;book;random representations;cities;books;clusters;natural language theory"}, "63cd8df0041638b0aa74834a81f99ff136951ff1": {"ta_keywords": "novel generative adversarial network;gan objectives;gan;binary neurons;output layer;generator;digits;predictions;network architectures;model;performance;different types;test time", "pdf_keywords": "generative adversarial network architecture;novel generative adversarial network;generative adversarial network;generative adversarial networks;binary neurons;end backpropagation;generative digits;gans;gan;generative bayesian network;multilayer perceptrons;multilayer perceptron;neurons;gan objectives andwe;convolutional neural network;convolutional neural networks;output layer;probabilistic network;generator;networks;wgan;binary digits;sigmoid;binary;gradients;stochastic model;discriminator;training;predictions;mlps"}, "655b842ae905756b2949758bd7e52e5fd32c3642": {"ta_keywords": "beam search;speech recognition;new continuous speech recognition;word speech recognition task;search error risk;search errors;search error;decoder;minimization;new heuristic;lps;new method;method;partial hypotheses;significant reduction;paper;prunes", "pdf_keywords": ""}, "28421c7f28adfb9ab8aeb56c196ac3ba326efdbb": {"ta_keywords": "orbit interaction;spin;evolution", "pdf_keywords": ""}, "7c72e63aa112193590861887c5d03b640ce90911": {"ta_keywords": "orbit coupling;orbit interaction;magnetic field;spin;dynamics;interaction;value;presence", "pdf_keywords": ""}, "3a6334953cd2775fab7a8e7b72ed63468c71dee7": {"ta_keywords": "human social skills training system;social skills training;social skills;audiovisual information;visual features;presence;pitch;method;simulation;users;experimental evaluation;novel method;results;paper;yaw;ratio", "pdf_keywords": ""}, "4e9328b2801e158647dff69606ed47d47045eca8": {"ta_keywords": "datalab;new data mining tools;data lab;unified data;data analysis tasks;data;different data processing operations;platform;framework;standardized interface;unified manner;development;users;concept;variety;open", "pdf_keywords": "data discovery platform;new data discovery framework;data features;data analysis platform;data discovery;large datasets;data discovery systemwe;datasets;itswe introduce data discovery;bioinformatics;dataset;novel interactive dataset analysis tool;empirical discovery;data mining;scale data mining;data management platform;comprehensive datasets;unified data;available datasets;machine learning;data search;datalab;large databases;data;short text datasets;data bias analysis;natural language data;data analysis;biological data;new data"}, "1e9771a264334c45020421b1c847f6bcd88adc60": {"ta_keywords": "inaccurate annotations;deep learning;deep networks;annotations;original annotations;network;inaccuracies;potential errors;topology;performance;method;approach;result", "pdf_keywords": "accurate annotations;microscopy images;deep networks;deep network;dimensional microscopy stacks;inaccurate annotations;annotations;annotation;biomedical image stacks;neuronal axons;annotated sub;structural annotations;dataannotated cell;neural networks;deformable contours;dimensional structured representations;neural network;annotated version;computer vision;supervised learning;supervised learning problems;axons;cell networks;network snakes;neuroscience;brain scans;network weights;simple neural network;dendrites;regularization"}, "80111013916dae3306316c34e13fe856cb08b87b": {"ta_keywords": "full intuitionistic logic;explicit default rules;default logic;intuitionistic logic;inheritance hierarchies;default rulebase;explicit exceptions;nonnormal defaults;general case;analogues;principle;language;failure;proof;case;combination", "pdf_keywords": ""}, "69e8c4327193af4549c06809c821c99deb4022cd": {"ta_keywords": "data storage;storage;nodes;data;subset;method;use;set;problem", "pdf_keywords": ""}, "a77643bff6f50ccc4f80ec081e4d078a2e788ae7": {"ta_keywords": "multilingual subword regularization;xtreme multilingual benchmark show;probabilistic segmentations;standard segmentation algorithms;predictors;consistency;inputs;novel method;results;points;method;new method;effectiveness", "pdf_keywords": "multilingual pre training;novel subword segmentation method;machine translation accuracy;multilingual representations;multilingual models;subword regularization methods;standard deterministic subword segmentation;subword regularization;machine translation;subword segmentation;multilingual data;language encoder;crosslingual transfer;language encoders;standard machine translation algorithm;language models;latin scripts;language model;language input;different languages;languages;translation;sentence classification;machine learning tasks;encoder;raw text;input segmentation;unstructured text input;segmentation;nernst classification task"}, "bf50833a46839d3932663b472d6145418f9d0bd6": {"ta_keywords": "stream attention;knowledge;tables;datasets;framework;characterization;proc;lond;sequence", "pdf_keywords": "microphone array speech recognition;hierarchical attention fusion;attention networks;coupled microphone arrays;end speech recognition problem;joint attention training;major speech recognition;speech recognition system;automatic speech recognition;hierarchical attention mechanism;attention;attention type;salient features;novel attention;multiple arrays;pattern recognition tasks;informative encoders;decoders;encoder structure;encoders;attention weight;relative word error reduction;decoder;array;parallel streams;classifier;acoustic signal generation;optic devices;signal amplitudes;advanced electro"}, "d6e21619df572d04b2b2d97b4c5d1fd604f185fb": {"ta_keywords": "tessellation datasets;parse;novel autoregressive algorithm;tessellation;autoregressive algorithm;tree;algorithm;unsupervised generation;higher accuracy;accuracy;input;baseline;experiments", "pdf_keywords": "neural machine translation model;neural machine translation;translation prediction;novel statistical translation model;autoregressive parser;fast neural decoding;autoregressive encoder;parsers;parse decoder;translation accuracy;translation quality;language generation;parse;syntactic structure;parse tree;token decoder;encoder;decoder;chunk sequences;chunk sequencewe;language pairs;chunk sequence;linguistic structure;decoder framework;computational linguistics;target sentence;syntax;latent transformer;corresponding target sequence;synst"}, "a5f42552b2368a587aea0a81175b4a79aa614601": {"ta_keywords": "web data;effective collaborative filtering;filtering;web pages;web;useful information;data;new features;concept;novel approach;collection;quality;approach;problem;set", "pdf_keywords": ""}, "0e61536550b7263d67b2928473355171dc37c0ae": {"ta_keywords": "harmonic trap;dimensional harmonic oscillator;harmonic oscillator;dynamics", "pdf_keywords": ""}, "7f0dbd30dc839fd95ea953a9229c879396ca11c0": {"ta_keywords": "symbolic knowledge bases;neural modules;model multihop inferences;original semantics;entities;large fpfs;novel representation;representation;tasks;fpfs;large numbers;variety", "pdf_keywords": "symbolic knowledge base;knowledge bases;symbolic knowledge;underlying knowledge base;encode knowledge;knowledge base;neural semantic parsing;semantic queriesknowledge bases;symbolic representations;knowledge graphs;matrix entity;semantic parsers;sparse matrix representations;neural inference modules;semantic parsing;large sparse matrices;original semantics;neural representation;neural representations;sparse matrices;relation vectors;entities;input entities;underlying reasoning process;many large data sets;denotations;representations;inference;natural language;sparsematrix"}, "c2c6c9947dc9d28bb4fc6f965310be517f4d8c57": {"ta_keywords": "shape synthesis;voxel;novel method;paper", "pdf_keywords": ""}, "81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85": {"ta_keywords": "language models;particular language model;corpora;accurate knowledge;knowledge;topics;better lps;lps;accuracy;profession;novel approach;approach", "pdf_keywords": "level knowledge bases;language models;knowledge retrieval;effective prompts;prompt generation;prompt candidates;natural language;diverse prompts;relational knowledge;different prompt generation methods;prompts;linguistic structure;linguistic specificity;large corpus;machine translation;linguistic properties;underlying language;language benchmark;knowledge;intriguing results;commonsense reasoning;manual prompts;paraphrasing;neural machine translation;prompt;lexical diversity;entities;inappropriate prompt;prompt weights;gold subject"}, "2c5a410b781f90c145efac05fea235c5c3e44861": {"ta_keywords": "source voice conversion;speech representation;s3prl;s3r;comprehensive evaluation;self;vc;a2;a2o;framework;results;context;one;series", "pdf_keywords": "source voice conversion;voice conversion challenge;standard speech synthesis;speech encoder;simple speaker encoder;new speaker encoder;speaker encoder;lingual voice conversion;input speaker information;unseen speaker information;unseen speaker informationwe;speaker information;input speaker;dependent synthesizer;neural vocoder;augmented speaker loop;reference encoder;encoder;wav2 vec;various s3r models;s3prl;wav2;speech;different s3r scenarios;s3r;speakers;fiber impulse approximation;vcc2020;vc;major fiber fiber fiber infrastructure"}, "3db9649f2ae986cac13f3e748375f8802f9b07fc": {"ta_keywords": "pruning;sparse;sparsity;training distribution;datasets;resources;nodes;resource;robustness;distribution shifts;certain tasks;novel strategy;power;strategy;number", "pdf_keywords": "machine translation;translation tasks;low resource resource language programs;translation techniques;low resource learning data;performance translation quality algorithm;deep convolutional neural machine translation;low resource resource learning;training corpus;compression;largest available parallel corpora;computational linguistics;translations;translation process;low resource constraints;resource machine learning;benchmark benchmark data;memorization models;low resource;languages;memorization;frequent sentences;sparse models;high computational efficiency;resource constraints;recurrent neural networks;data limitations;translation;german language;data bias"}, "12442420adf1c36887fafd108f4b7f4fc822ae60": {"ta_keywords": "neural sequence generation tasks;neural sequence generation;supervised baseline;training method;dataset;unlabeled data;prediction;self;training;novel self;input space;model;noisy version;noise;key idea", "pdf_keywords": "machine translation tasks;machine translation;sequence generation tasks;neural sequence generation;sequence generation task;translation performance;text summarization;supervised training task;unannotated data;sequence generation step;supervised learning tasks;deep learning aspect;deep learning models;deep learning structure;self;supervised model;deep learning problem;training procedure;training;new training;toy sum dataset;unlabeled data;initial baseline;flexible data;noisy self;data;machine learning framework;generalization;seq2seq tasks;kernel sum data"}, "299ab255f3d940a20891128dfa9e0736d74a936c": {"ta_keywords": "imitation learning;vision models;vision model;simulated item retrieval problem;vision;models;early fusion;iterations;goal;method;methods;set;end manner;end;domain", "pdf_keywords": "simulated object retrieval task;imitation learning;visual processing pipeline;robot;attention models;simple vision system;underlying object space;convolutional neural networks;search task;objects;deep reinforcement learning;attention;object detection;complex tasks;attention system;navigation task;object;joint learning;neural network;mobile robot;convolution channels;task;image;natural language commands;recognition algorithm;goal information;metasurfaces;early fusion;model;difficult task"}, "5c6ff5836639e87e8afeaad47e64d0e2234566e8": {"ta_keywords": "", "pdf_keywords": "fake news detection task;aware fake news detection;word attention layer;evidence attention;attention layers;document attention layer;word attention;textual claims;attention;attentive network;level attention;relevant semantic information;textual content;fake news;novel evidence;evidence;different semantic contributions;natural language;important words;semanticwe;embeddings;social media;social network;public datasets;fact;facts;neural neural network;convolutional learning model;politifact;text"}, "963c62b7c4b44ff1fe6aa1f45fa8a7d62b3d5051": {"ta_keywords": "generic textual entailment system;textual queries;questions;qa;support;system;domain;domain question;novel methodology;concept", "pdf_keywords": "generic textual entailment system;natural language query;natural language processing;conceptnet framework;arc challenge dataset;computational sentence inference;text queries;natural language;deep knowledge;novel entailment model;textual entailment;unstructured text;large corpus;computational tasks;novel open domain system;answer answer answer;questions;retrieval module;challenges;entailment module;end qa task;semantics;unstructured database;keywords;embeddings;biological knowledge;knowledge;natural language problem andwe;science exam questions;functionality"}, "e28b9bc26f5f7eb3b0532d823713400202372da2": {"ta_keywords": "critic game;novel reinforcement learning paradigm;stackelberg game;critic approach;critic;openai gym environments;sum game;follower structure;standard actor;interaction;dynamics;actor;player;leader;local convergence guarantee;refined update;experiments;update;framework", "pdf_keywords": "critic reinforcement learning algorithms;critic algorithms;critic algorithm;critic learning problem;reinforcement learning game;stackelberg game;critic functions;sum stackelberg game;critic framework;deep deterministic policy gradients;policy gradient problem;critic game;reinforcement learning tasks;reinforcement learning;quadratic gradient descent tasks;critic objective;reinforcement learning problems;stackelberg gradient;critic interaction;critic problem;reinforcement learning framework;quadratic gradient descent;novel stackelberg framework;critic policy;critic parameters;reinforcement learning problem;continuous reinforcement learning problem;critic;stackelberg framework;heavy quadratic learning cost"}, "b63bd17d4bb28ba90cc6ff66b51ba5b0377467bf": {"ta_keywords": "recurrent neural network language models;efficient minimum word error training;minimum word error training;speech recognition;rnns;lstm;type rnns;term memory;training method;gpus;standard elman;best lists;graphics processing units;advanced model", "pdf_keywords": ""}, "93d3e45395117e21214d404c8753b578c29266d1": {"ta_keywords": "textual open question;tabular;tables;textual units;evidence;new data;text;scale dataset;fusion;challenge;approaches;novel approach;group;approach;performance;variety;novel techniques;first technique", "pdf_keywords": "evidence retrieval challenging;dense retriever queries;unstructured data;unstructured text;retrieval method;rich content discovery;quality retrieval model;natural language data;structured data;sparse retriever reader;annotated questions;retriever query augmentation;interactive search;queries;dense retriever;rich content;text answer;search;structured structured structured structured data setwe;open question;generative entity linker;retriever blocks;retriever model;retriever;hybridqa dataset;annotated question;natural language;unstructured subset;qa;tablepassage retriever retrieval problem"}, "a0035379f93e0e95bdadd77a1d8eb27ba89dcf60": {"ta_keywords": "open collaborative storytelling;natural language annotations;open collaborative ecosystem storium;user experiments;models;novel platform;open access;evaluation;platform;novel perspective;available dataset;context;approach;dynamics;power;ability", "pdf_keywords": "open source story generator;loop story generation platform;loop story generation;story generation;narrative analytics;story text;narrative documents;story continuations;story descriptions;natural language annotations;annotated scene text;narrative stories;small scale annotations;narrators;narrative;annotations;text entries;language models;scene entry annotations;texts;long stories;longform stories;scene entries;storium language model;individual scene entries;neural topic model;topic model;intrinsic story;standard story;stories"}, "3e4d80e43346b9538504c0a7ee5562f3c6a09178": {"ta_keywords": "bandit algorithms;new class", "pdf_keywords": ""}, "740ecaa7fc1f4fc02116181b1757f03c815c7ea9": {"ta_keywords": "data mining;neural network;statistical techniques;clinical problem;novel method;method;performance;methods;data;problem;combination;concept;outcome;feasibility", "pdf_keywords": "recurrent neural network model;clinical time series;neural network;length time series;term memory;intensive care unit;diagnoses;supervised learning algorithm;hospital;novel feature;model;bacterial infections;data;phenotyping;shape;picu;bacterial nucleus;novel application;novel approach;architecture;ls;large collection;finland;linear;children;strong baseline;multiple types;systematic way;combination;la jolla"}, "191ef1408406569f0e9a69344add1ae350365431": {"ta_keywords": "fairness properties;recidivism prediction task;world credit;models;stakes settings;similar overall performance;task;framework;set", "pdf_keywords": "predictive fairness;predictive fairness properties;fairness framework;fair machine learning;algorithmic fairness;fairness properties;fair regression;predictive disparities;predictive disparity measure;discrimination task;predictive disparity models;disparities prediction functions;predictive disparity;fairness cost;disparities prediction function;recidivism prediction task;absolute predictive disparity;fairness;predictive disparity problem;fairness parity constraint;predictive performance;disparities;predictive;disparity measure;disparity;prediction;disparity inequality;predictive regression;threshold classifiers;classifiers"}, "9eeaeadc1e0e300337b47d867a314caeae5c10a9": {"ta_keywords": "magnetic phase;structural relaxation;magnetic moments;individual brain components;individual brain;brain;structural components;emergence;observation;model;degree", "pdf_keywords": ""}, "9813446d9545b600de9a4972c1382c5e3b22a351": {"ta_keywords": "intelligent student tutoring system;student modeling;student model;students;student;class;use;performance;help;paper", "pdf_keywords": ""}, "78dadbfb6710ac65f178b5e12bd975184aae62fe": {"ta_keywords": "harmonic oscillator;lorentz force;oscillator;particle;motion;force;effect", "pdf_keywords": ""}, "241e890c70f6d013de7fe5e174e061ff824dc5e9": {"ta_keywords": "learning environment;student performance;students;simple simple simple tasks;student;student list;study;task;computer system;project;simple problem;particular item;results;simple way;time", "pdf_keywords": ""}, "ca00ead4e5ddd14cbbbce03d89a57d14b430e320": {"ta_keywords": "smart grid;smart grid applications;privacy valuation process;smart grid devices;segmentation process;segmentation;privacy;customers;smart devices;approach;novel approach;process;account;breach;variety;use;construction;generation;risks", "pdf_keywords": "novel privacy metric;smart grid;nonintrusive load monitoring;privacy protection;smart grid operations;privacy;smart grid services;privacy insurance;energy disaggregation;private information;data disaggregation;privacy breach;energy consumer;successful privacy breach;adversary;utility;sensor;utility company;nilm algorithm;utility relationships;direct load control;utility relationship;consumers;risk consumers;asymmetric information;consumer;discrete component modeling;asymmetric information model;granularity data;data"}, "83c7335904002d2b7c7cb403f3538703c9a69025": {"ta_keywords": "silent speech;speaker;quiet environment;additional talk;talk;listener;signal;quality;method;hypothesis;paper;form;means", "pdf_keywords": ""}, "9f1a1d2cb6b278b7ee24e67d4c2ac38c1161fa1d": {"ta_keywords": "acoustic background;acoustic sound;acoustic acoustic acoustic sound;vowels;method;new method;determination;human beings;study;important aspects;problem;form", "pdf_keywords": ""}, "5bf7f468b763f181c31a5e1edc57bce9a6dbd00c": {"ta_keywords": "acute respiratory distress syndrome;consistent pde score;novel empirical approach;consistent empirical representation;consistent evaluation;underlying partial differential;pde;ill patients;partial differential;risk;score;novel method;method;elas;large public databases;use;self", "pdf_keywords": "viral pneumonia;severe acute respiratory distress syndrome;coronavirus disease;acute respiratory syndrome;pneumonia;coronavirus;unspeci ed ed pneumonia;intensive care unit;critical care patients;critical care;risk score;infectious diseases;risk score inthe inclusion;ill patients;rnv;intracranial hemorrhage;high risk group;high risk;severity;clinical data;cox model;risk group;mortality;risk groups;infections;associated survival;risk;new risk nomogram;clinical characteristics;decompensation indicators"}, "ea2b138583e587850153f2825fe9e4339aa5f5f9": {"ta_keywords": "neurons;dynamics;parallelization;network;simple model;model;study", "pdf_keywords": ""}, "ca1645abedae3b4caa3345aa8720c8b90f7c37db": {"ta_keywords": "voting rules;dependent scoring;new scoring rules;votes;static random variables;rank;static random variable;new rules;distribution;concept;case;presence;new family", "pdf_keywords": ""}, "b2c3d660aaefb80085fe72c80ce81c5fa71980e9": {"ta_keywords": "other pivot translation approaches;new pivot translation method;pivot language words;pivot translation method;pivot language;syntactic subtrees;parallel corpus;incorrect phrase combinations;syntactic roles;software;ieee international conference;annual meeting;computers;systems;results;method;united states;paper;th;experiments", "pdf_keywords": ""}, "b7d6829d9eccdbd3d3a5d6f5321a87158588033b": {"ta_keywords": "large scale datasets;videos;continuous traits;pairs;preference;time;novel method;ability;method", "pdf_keywords": ""}, "47adb249ce8f7f5f1e92112ba0f3757f8fbfbfc3": {"ta_keywords": "indirect associations;syntactic representations;encode direct term associations;texts;traversal;direct evidence;graphs;task;order methods;word;formalism;question", "pdf_keywords": ""}, "21ac57d41843ac5367e11b8b784aa57f2ef7a1fc": {"ta_keywords": "stochastic methods;generalized smooth objectives;convex problems;probability convergence results;continuous gradients;noise distribution;gradient clipping;novel stepsize rules;methods;extension;analysis", "pdf_keywords": "stochastic optimization;stochastic optimization problems;new stochastic gradient;stochastic gradient method;stochastic gradients;stochastic gradient;stochastic optimization problem;stochastic approximation;large scale learning process theorems;probability complexity guarantees;convex optimization;gradient descent methods;stochastic learning problem;stochastic approximation error;stochastic problems;stochastic noise;optimal stochastic value;probability complexity;generalized gradient learning;stochastic noise distribution;optimal methods;stochasticwe;learning process converges;stochastic;optimal estimation;stochasticity;complexity bounds;deep learning paradigm;gaussian gradient;strong convexity"}, "6cdff2505560390b28db5a96c2ae3070712077cf": {"ta_keywords": "policy gradient reinforcement learning;competitive gradient;certain online convex optimization algorithms;competitive agents;learning algorithms;gradient;bandits;sum games;learning;dynamical systems theory;potential games;agents;algorithms;wide breadth;general framework;behavior;non;lens", "pdf_keywords": "competitive stochastic gradient;competitive gradient;competitive multiagent games;differential nash equilibria;agent learning;multiagent reinforcement learning algorithm;global nash equilibria;differential nash equilibrium;many linear quadratic dynamic games;linear quadratic dynamic games;agents strategies;game dynamics;policy gradient;partial differential games;global nash equilibrium;gradient dynamics;classic policy gradient;local nash equilibria;deterministic gradient;local nash equilibrium;stochastic gradient algorithms;nash equilibria;sum games;theoretic games;continuous games;continuous time games;agents cost functions;stochastic gradient;learning scheme;gametheoretic notions"}, "7301c7aba3c0824b91f69747e7e50f4db56d7fc1": {"ta_keywords": "paralinguistic translation;digit translation task;neural network model;single neural network;model;method;similar results", "pdf_keywords": ""}, "8837530b23a2d51054d8752ae2f0ffef8998da8e": {"ta_keywords": "aware collective matrix factorization;differential privacy;location interaction;interaction data;domains;target domain;real locations;protection mechanism;auxiliary domain;confidence;cvf;criterion;user;novel approach;new method;assumption;paper", "pdf_keywords": ""}, "cb15c1c51e8a7da42d5b2ebac955bf1cd9dd4022": {"ta_keywords": "novel graph transforming encoder;encoder;art encoder;quality document sequences;information extraction system;graph;output;novel novel;generation;de methods;state", "pdf_keywords": "knowledge graph;novel graph encoder;knowledge graphs;knowledgegraphs;novel graph transforming encoder;natural language text;graph structure;unlabeled graph;text generation task;automatic information extraction systems;graphs;novel encoder;coherent knowledge;informative texts;graph transformer model;graph;encoder;novel text;scientific texts;nodes;better document structure;competitive encoder;new scientific domains;biological networks;encoder withwe;knowledge;decoder model;novel representations;networks;novel resource"}, "a54019645dd8e9cfd8d71ab60155449307de3d83": {"ta_keywords": "crowdsourcing;simple incentive;incentive;possible incentive;quality data;compatible payment mechanism;workers;questions;compatible mechanism;mechanism;compatible mechanisms;requirement;rest", "pdf_keywords": "crowdsourcing tasks;crowdsourcing results;crowdsourcing;scale crowdsourcing tasks;commercial crowdsourcing platform;possible incentivecompatible mechanisms;incentive mechanism;online crowdsourcing platform;crowdsourcing setting;typical crowdsourcing setups;incentive;incentive measures;amazon mechanical turk;general incentive mechanism;possible incentive;mechanical turk data;novel incentive;only feasible reward mechanism;incentivize workers;machine learning labels;autonomous workers;novel reward mechanism;lunch theorems;incentivizement;such inference algorithms;knowledgeable workers;tasks;inference algorithms;payment algorithms;arbitrary pairwise information exchange"}, "2cc7db7b17ee7349800334b3a154f708850c6410": {"ta_keywords": "queues;average job latency;decentralized versions;mds;extensive simulations;code;distance;same code;upper bounds;system;data;version;analysis;condition", "pdf_keywords": ""}, "0ff5b1e61bbebd2f077a4ef24c3afdb344e5b3d4": {"ta_keywords": "harmonic trap;dimensional lattice gas;homogeneous gas;dynamics;first order transition;initial state;transition;finite set;parameters;system;final state;effect;interplay", "pdf_keywords": ""}, "8e992116bbc8afb075577a30672de7a90fbeba78": {"ta_keywords": "novel streaming algorithm;synchronous beam search;other streaming approaches;neurons;encoders;classifier;stream;algorithm;parallel computer;feature;blockwise;arrival;performance;novel;several examples;efficacy", "pdf_keywords": ""}, "3163392f56cdffaa009fbc59f299989a1b8baec1": {"ta_keywords": "binary classification;unlabeled data;algorithms;algorithm;class;oc;list;pu;overall performance;state;performance;novel approach;novel modifications;various scenarios;art;approach", "pdf_keywords": "unlabeled learning;unreliable unlabeled data;supervised learning;binary classification;unlabeled dataset;predictive algorithms;classification;unlabeled data;supervised learning setup;best classification method;predictive machine;optimal classification method;novel class learning representation;convex hull learning;convex learning problem;machine learning;novelty detection;positive data;vector machines;nonnegative risk estimator;svm;machine learningwe;novelty detection problem;learning;detection;clustering;deep learning;robustness;cnn;benchmark datasets"}, "73472692b6090a72e36e03127bb99fc2e6bc8de0": {"ta_keywords": "communities;network modularity maximization;cultural cognitive mapping;community assignments;empirical data;socio;current unsupervised machine;nn;methodology;scm;novel methodology;techniques", "pdf_keywords": ""}, "2f6843f9345ca56af3fd9df5512daa1e7f80bedf": {"ta_keywords": "conditional random field classifier;conditional random field baseline;accurate prediction;structure;standard model;shape;data;novel method;method;system;use", "pdf_keywords": ""}, "faa4468f2ad1c7cedaf04bf56ebb20ae4b349952": {"ta_keywords": "long short term memory recurrent;neural network language models;covariance matrix adaptation evolution;global optimization;sdms;tuning process;benchmark problems;parameters;novel method;variety;study;effective method;method;results", "pdf_keywords": ""}, "872c2d9d8b27ff49367854a7cf67b5dff2010406": {"ta_keywords": "event detection process;unsupervised learning framework;event;independent way;domain;method;framework;novel method;presence;precision;problem;use", "pdf_keywords": ""}, "c6713071291729386955586c6309778b1637b852": {"ta_keywords": "stochastic pricing designs;adaptive energy management;linear quadratic game;buildings;zone building model;neighborhood;many agents;game;simplification;method;use", "pdf_keywords": ""}, "d5084f48212bed80e8c11e1e69669deea3ba2f83": {"ta_keywords": "pasta dish;novel parallel model;parallel tasks;simplicity;model;common ancestor;large class;problem;simple definitions;goal;notion;notations;ability", "pdf_keywords": ""}, "8451d8e20bb9a94c6a576e52ca1a63470f8d2390": {"ta_keywords": "optimization;free method", "pdf_keywords": ""}, "a6a9c06d138537002aaca79dba359cc320b951df": {"ta_keywords": "closed system;probability density;distribution;system;states;calculation;new method;method;analysis;number", "pdf_keywords": ""}, "03bbbaa03cb57413c2581cc8dc5cbfa532bbea15": {"ta_keywords": "empirical validation;erp;event;empirical data;potential;time;user;novel approach", "pdf_keywords": ""}, "73e1dcf5f0f3cf4e645b0bba62d9b1e2ef47b706": {"ta_keywords": "lexical class identification;free grammars;context;implementation;main algorithm;methods;paper;experimental results", "pdf_keywords": ""}, "122b75042daae44f93153dedda15b0fb11b3f279": {"ta_keywords": "bert;future datasets;datasets;single dataset;models;task;recommendations;question;experiments", "pdf_keywords": "machine reading comprehension systems;robust answers;bert;learning models;qa datasets;natural language processing;questions;informative probes;comprehension;lexical semantics;natural language;contexts;answers;linguistic structure;lexical aspects;human language;machine learning models;semantics;linguistic complexity;context;models;understanding reading;words;datasets;reading;new datasets;research datasets;entities;qa;iscomputational machine learning"}, "e6602786132e040e02df93f729f737f65a116677": {"ta_keywords": "digital home assistants;automatic speech recognition;key speech processing algorithms;free speech interaction;asr;commands;device;hands;recent advancements;distance;challenges;many other use cases;field;article", "pdf_keywords": ""}, "d8682a269523a868f2bc9714b00f0519aa0e931f": {"ta_keywords": "information retrieval;information sources;new integration system;integration system;structured collection;text;novel logic;small fragments;process", "pdf_keywords": ""}, "8cb74fe4f598699c9c24d88acd4906e2489267af": {"ta_keywords": "particles;single particle;dimensional motion;adaptive sensor;mapping;space;powerful method;large number;method;use", "pdf_keywords": ""}, "48aced0919e29722d6eed9544353d5507c541cfc": {"ta_keywords": "protein names;anaphoric linking;anaphoric techniques;specific domain;domain;thephoric;specific data;novel approach;use;combination", "pdf_keywords": ""}, "b3d9a0308ba6c4ca583a2b4e5be2b3eed466ccbc": {"ta_keywords": "observable decision process;relay;dynamic obstacles;stochastic aspect;communication;dynamic obstacle problem;pmdp;d2d;wave;signal;noise ratio;snr;approaches;framework;accurate description", "pdf_keywords": "relay selection;relaying network;best possible relay;relay;wireless relay links;optimal transmission policy;optimal communication policy;relay link;network delay;wireless scenario;packet delay;good relay;transmission policy;optimal threshold policy;current relay link;network;network node;communications;maximum throughputin;threshold policy;optimal policy;current relay link orwe;communication links;optimal policy checks;polynomial time approximation;short distance link;dynamic obstacles;d2d mm wave links;wireless sensor network;data packets towe"}, "fc8e226c20800c8ccc095bb6a3c0f8dcb637b683": {"ta_keywords": "lung metastases;initial metachronous lung metastases;lung cancer;multidisciplinary therapy;multidisciplinary treatment framework;metastases;ep2;consensus;guidelines;form;attempt;present work", "pdf_keywords": ""}, "6c0a3029afd65c83982b3fb96f623da382344286": {"ta_keywords": "topic models;tensor factorization theory;natural language processing;nlp;neural networks;accurate representations;matrix;population dynamics;context;optimization;tutorial;system;methods;information;recent applications;state", "pdf_keywords": ""}, "928f942baf03dd56aae662fa94d85d22b5600f83": {"ta_keywords": "new method", "pdf_keywords": ""}, "e30b22e692b3c7d2653832bf2901abd8a9375b6e": {"ta_keywords": "content placement algorithm;optimal content placement;popular content placement;independent content placement;content popularity;small cell networks;nodes;cache;gibbs;algorithm;ideas;knowledge;rate;improvement;download;backhaul;problem", "pdf_keywords": "popular content placement algorithm;practical cellular network base stations;cellular networks;cellular network;wireless cellular networks;small cell networks;small cell network scenarios;optimal content placement;content placement strategy;heuristiccontent caching;content distribution;heterogeneous networks;small cell network;novel cache update scheme;content popularities estimation;content request arrival process;content delivery systems;content delivery;content requests;popularity distribution;neighbouring base stations;content placement methods;wide virtual cache;decentralized network;multiple base stations;new content requests;simple sequential content update rules;cache update scheme;content popularities;content placement"}, "2406cf39805c70264c4226b7325a09b506c70921": {"ta_keywords": "neural sql query;arbitrary tables;sql queries;ofsql queries;table;synthetic corpus;unsupervised learning;pre;high accuracy;program;novel method;quadratic operator;execution;executor;model;form;method;framework;scale", "pdf_keywords": "sequence generation tasks;neural sql program;synthetic synthetic database;machine translation generator;unstructured table source;synthetic corpus;nonlinear semantic parsers;machine translation generatorwe;executable sql queries;large corpus;generative encoder;tables;machine translation machine;sql;encoder;decoder models;database;query sentences;unconstrained tables;natural language data;generative matrix representations;table;latent structured alignments;tablefv;natural language representation;natural language interface;generative fine;powerfulcomputational linguistics;random table model;novel execution"}, "3ee38da21d8cf9cb7d4077b729e57f68e9c8d671": {"ta_keywords": "accurate text generation;text generation;reinforcement learning;importance weighting;demonstrations;confidence;novel approach;weight;rl;depth;downweight;reference;model;quality;approaches;art methods;approach;state", "pdf_keywords": "supervised text generation tasks;text generation tasks;stochastic text generation tasks;conventional text generation tasks;quality text generation tasks;offline reinforcement learning;standard text generation tasks;text generation;natural language generation;reinforcement learning;deep reinforcement learning model;question generation;policy gradientwe;policy gradient;thedeep reinforcement learning;text;learning strategy;quality sentences;target text;quality text;random data generators;powerful machine translation method;policy data;text documents;generatewe;sentence quality;generative models;random words;structured prediction energy networks;expert demonstrations"}, "fdaad09b1a897c0a04b9a9579081d542e2b4546c": {"ta_keywords": "ionization time;electron gas;single ionization mode;wavevector;identification process;electron;electrons;identification;cell;2d;cross;technique;number", "pdf_keywords": ""}, "74276a37bfa50f90dfae37f767b2b67784bd402a": {"ta_keywords": "many multilingual benchmarks;multilingual text;languages;new common crawl;dataset;mt5;transfer;t5;training;model;art performance;state;design", "pdf_keywords": "large multilingual generative models;many multilingual benchmarks;multilingual model;machine translations;unified language model;lingual data;generative language model;multilinguality;large languages;mt5 languages;language mt5;translate training;new languages;target languages;target language;language model;new language;multilingual variant;resource languages;tuning language;mc4 language classification system;languages;large generative model;generative text classification problem;generative tasks;corpus;languagewe;language;linguistic models;languageswe"}, "00b2afaf5935b4dea41f134fe11a21a1ed56fa0e": {"ta_keywords": "behaviour", "pdf_keywords": ""}, "1bf36cb3453b51550ebadd904a840c75d59f171b": {"ta_keywords": "robust automatic speech recognition;robust arrndient systems;robust systems;robust arndient systems;deep learning;asr;systems;recent advances;recent developments;general background;introduction;field;overview;chapter", "pdf_keywords": ""}, "8b20173b98914f36302389e4c761c334fe867dcd": {"ta_keywords": "robust parsers;dependency treebanks;dependency parse;text generation systems;morphosyntax;text;language;noisy outputs;various rules;simple methodology;way;method", "pdf_keywords": "machine translation;natural language generation systems;standard machine translation;corpus;syntactic rule capturing;robust parsers;morphological taggers;robust parsing models;text generation systems;robust parser;grammatical taggers;parsers;entire corpus;noisy treebanks;natural language;new morphosyntactic rules;rich lexical structure;treebank;morphosyntactic rules;syntactic rules;grammar;large language model;corpora;standard linear gisin treebank;dependency parse trees;natural language processing applications;linguistic accuracy;readable grammatical description;dependency parse;dependency parses"}, "3dcba175248d0e8d2da44e3731e4adbfb9f00e97": {"ta_keywords": "novel open information extraction;relation tuples;individual sentences;supervision framework;local context information;quality extraction;world corpora;sentence;global structural signals;ie;effectiveness;system;different domains", "pdf_keywords": "novel open information extraction;open information extraction;entity phrase extraction task;high quality entity phrase pairs;domain information extraction;relation extraction tasks;domain information extraction systems;relation extraction;domain information extraction problem;entity phrase segmentation;entity phrase pairs;phrase extraction task;entity phrases;multiple entity phrases;meaningful relation phrase pairs;ective extraction;tuple extraction;sentence structures;text corpora;phrase boundary;recent phrasal segmentation problem;high quality phrase;corpus;positive entity tuples;sentence structure;semantic tokens;local contextual information;distant supervision task;extraction;positive entity pair"}, "96dbffb71e4d62a985f826197845623b1415c267": {"ta_keywords": "metacognitive learning;free grammar induction algorithm;future learning;human learning;synthetic student;deep features;probabilistic context;computational model;model;challenges;possible future studies", "pdf_keywords": ""}, "ce9919ffb9dab701babd67a945b1590917345789": {"ta_keywords": "multiple inconsistent explanation problem;training example;learning;abductive explanation;knowledge;explanation;negative examples;formalization;level explanation;techniques;possible explanations;ebl;certain type;set;extension;paper;convergence properties", "pdf_keywords": ""}, "53e161d4434576355fc5f63fe56afd8e135174b2": {"ta_keywords": "quality text;structure prediction;unstructured media;edge prediction;dataset;models;script;generation;proscript;scenarios;scenario;wide range;complementary tasks;new approach", "pdf_keywords": "narrative chain;natural language generation;narrative event;neural language models;script generation task;other narrative events;narratives;generative models;accurate generative models;neural language model;natural language;generative approach;manuscript generation;proscript generation task;scenarios;tasks;prediction;scripts;literature;quality scripts;events;similar predictions;prediction performance;computational linguistics;text;structured data;models;powerful machine learning;everyday scenarios;event"}, "6c59a6ad00d82ca9f76fef92232ff3e2f3c1acc8": {"ta_keywords": "global cost tensor matching;aware diarization outputs;several fusion datasets;diarization output;overlap;majority voting;noise ratio measurements;representation;graph;signal;method;strength", "pdf_keywords": "speaker diarization systems;speaker diarization;real multi speaker conversations;usual diarization algorithms;speaker classification;speech separation;speaker labels;diarization systems;graph matching;label voting;diarization task;incremental label mapping;overlap assignment;speech prediction;overlap awareness;diarization system;overall speaker confusion rate;recognition challenge;diarization;overlap;label mapping;incremental pair;common label space;multiple speakers;clustering;speaker;input speaker confusion;standard diarization model;speech;aware sum"}, "a5881560968963d0c845c468a273261fde0b7248": {"ta_keywords": "input text;candidate text;perturbations;simple perturbations;good quality metrics;experiments;impact;novel method;candidates", "pdf_keywords": "deep natural language processing;fragile explanations;fragile interpretations;word perturbations;simple word perturbations;natural language theory;linguistic constraints;nlp;text examples;deep learning framework;words;tokenizations;fit word vector;candidate perturbations;important words;neural networks;adversarial rules;arbitrary text;tokenization;adversarial copy;text classifier rank order correlation;sentence;perturbedwe;text;relative error;neural network;candidate adversaries;level word swaps;perturbations;interpretations"}, "3004a3e4d8969dc3c36c9274b0f76ecc874f2e6a": {"ta_keywords": "noisy spectrum;noise ratio;recognition signals;separation;speech;objective function;signal;new method;similar method;method;application;better results;experiments;use;results", "pdf_keywords": ""}, "e29e43d9c0772d44cff53044484970599db30d5f": {"ta_keywords": "domain adaptation;neural machine translation;other alternative adaptation strategies;domain differentials;consistent improvements;domains;target task;models;differences;related task;difference;multiple experimental settings;strategy;effectiveness;experimental results;framework;paper", "pdf_keywords": "neural machine translation models;neural machine translation;neural machine translation experiment;domain adaptation;machine translation;deep adaptation;translation strategy;spoken language domains;recurrent neural network;adaptation;domain models;medical adaptation;domain words;rnn;deep learning;neural machine;differential adaptation;corpus;deep network;several alternative adaptation strategies;deep transformers;parallel data;deep representation;nmt model;empirical corpora;adaptation process;sentences;nmt;deep learning andwe;2k training data"}, "f6eafb82d2450f28f668443b689c91e896a0d63e": {"ta_keywords": "linear bandit problems;bandit problem;linear reinforcement learning;optimism;uncertainty;rewards;confidence;algorithmic approach;small deviations;linear function;algorithm;principle;class;coefficients;ofu;ei;vector;paper;face;standard ei;variants;essential features", "pdf_keywords": ""}, "8225b047e0fe90c2d5f9bb77fd94396a9d0fd21e": {"ta_keywords": "entity recognition systems;gene identifier;ranking process;information content;article;graph;results;relation;individual systems;method;new method", "pdf_keywords": ""}, "3873e60de2d20aa33829e2d3d79221e716785546": {"ta_keywords": "telephone speech corpora;discriminative model;perceptron algorithm propagates confidences;perceptron algorithm;features;data;lds workshop;nir workshop;lds;nir;novel approach;effectiveness;form;experiments", "pdf_keywords": ""}, "229c0c13e5c2d8e189efccf77b8179ec16500212": {"ta_keywords": "machine translation engine;aavatar;controlledavatar;trees;engine", "pdf_keywords": ""}, "14fce3cfa503894f244fc6ea8a7a00fa0ddfd94e": {"ta_keywords": "dimensional quantum fluid;repulsive force;dynamics;simple model;model;system;new method", "pdf_keywords": ""}, "a54a3a7b02cacd92b3bc633be7ea54e4f365fa65": {"ta_keywords": "malware communities;malware;consensus graph representation;cognitive mapping;latent spatial domain;features;binary structure;temporal relationships;attributes;variation;scm;novel approach;insights;use", "pdf_keywords": ""}, "935c275868bec7301f4bd254159978d8ded138b9": {"ta_keywords": "proton cross section;neutron cross section;proton;expression;relative contributions;terms;literature", "pdf_keywords": ""}, "b9f0c7e99bcc94c2cd75fd8e1cef45188f51270e": {"ta_keywords": "temporal classification;joint speaker recognition;label transitions;supervised learning tasks;labels;neural network;graph;posteriors;gtc;model;state;task;extension;art approaches;variety", "pdf_keywords": "end speech recognition task;speech recognition;automatic speech recognition;temporal classification;speaker prediction;temporal classification method;speech languages;linguistic network;label transitions;neural network;label predictions;conditional speaker chain;neural networks;speaker mix dataset;gtc loss function;convolutional neural network;knowledge representation;speech;large amplitude speech separation;loss function;supervised search;classifier;speaker;linguistic token units;prediction;linguistic tokens;linguistic linguistic tokens;gtc;labels;languages"}, "dfa34a10e2ba861545549c3188ef245b1e69bcdf": {"ta_keywords": "event extraction;event sequences;biopolymers;ofwords;word;features;bag;results;bow;art results;use;bow method;novel method;method;state;set", "pdf_keywords": ""}, "2ea5b0f5e476ddc00ae4450f2888a51fa25dd1d3": {"ta_keywords": "novel task augmentation technique;unlabeled texts;task;training;task fine;strong data;strong base model;target;novel approach;tuning;data;self;variety;further fine;large amounts;settings;approach;presence", "pdf_keywords": "natural language inference;nlp benchmarks;natural language models;task augmentation;shot learning;sentence representations;many natural language;synthetic language models;linguistic tasks;data augmentation;machine learning tasks;domain training examples;simple learning tasks;language models;task training;shot training;scale language models;natural language;untask augmentation;nlp;language representations;training examples;tuning regression tasks;sentence similarity;deep learning;target task;linguistic domains;bert representation;many tasks;language model"}, "92f93c0014ba4da59180c4cd141ad0dcaad5803f": {"ta_keywords": "multilingual instance;transfer learning method;multilingual data;target languages;target language;transitive vocabulary overlaps;auxiliary languages;tasks;improvement;performance;data;method;use;pairs;effect", "pdf_keywords": "multilingual search;indirect vocabulary transfer;large corpus;large linguistic domains;transfer learning;multilingual domains;multilingual deeptrieval workshop;target language;unrelated language;target languages;next sentence prediction;language instance;corpus;neural transfer model;direct vocabulary overlap;linguistic features;direct vocabulary overlapwe;multiple languages;embeddings;sentence pairs;resource learning models;resource languages;auxiliary vocabularies;multilingual setting;original corpus;linguistic structure;new language;vocabulary items;auxiliary language;available wikipedia2 dataset"}, "4a9c80e263fd0a88ad8220aa076ede4a3e77fcc1": {"ta_keywords": "graph structures;deep network;deep networks;nodes;support vector method;discovery;structure;pattern;search;models;novel method;mutation;sdtm;method;experiments;study", "pdf_keywords": "adversarial deep learning model;adversarial attack;adversarial sample;adversarial samples;adversarial attacks;adversarial detection;adversarial examples;adversarial sample detection;deep network;deep learning models;deep learning modelswe;deep learning;deep learning community;deep learning model;new deep learning method;deep learning system;deep learning problem;systematic attacks;sparse graph;neural network;attack method;graph;detection;relational graphs;standard methoddeep learning;error propagation;relational graph;neurons;machine learning;nodes"}, "885fe11ed7ab81c8609ccddb3e10f62577c04ab9": {"ta_keywords": "rich dialogue systems;dialogue system;rich agents;neural networks;agents;search;backprop;sample space;exploration;bayes;new algorithm;algorithm;approaches;efficiency;use", "pdf_keywords": "dialogue systems;dialogue policies;dialogue interaction;dialogue agents;reinforcement learning;many reinforcement learning models;reinforcement learning models;novel reinforcement learning framework;simple replay buffer;large state spaces;unstructured learning;neural networks;learning process;generalized learning network;actions;action region;stochastic representation;underlying interaction structure;action;prior knowledge;action exchanges;backprop method;agent;exploration;rdqm;bayes;traditional exploration methods;interest;task;inherent stochasticity"}, "a1340029d8a5c57bee8a5995ac3beafd3d0ba96c": {"ta_keywords": "connected sensor subset selection;subset information;stochastic approximation;experimental data sets;novel algorithm;algorithms;algorithm;partial data;estimation;accuracy;key tools;significant improvement;variety;use", "pdf_keywords": ""}, "c6854064cb5053e67d23394eee6d1646108f6d56": {"ta_keywords": "novel textual entailment task;textual entailment;standard textual entailment;multiple premise sentences;trivial lexical inferences;multiple premise task;inference;several strong neural baselines;everyday events;knowledge;new dataset;challenging setting", "pdf_keywords": "novel textual entailment task;novel semantic textual entailment task;novel entailment task;textual entailment;entailment task;standard textual entailment;entailment challenge;trivial lexical inferences;premise entailment;realistic entailment problem;multiple premise sentences;multiple independent premise sentences;longer premise texts;many captions;multiple premise task;many texts;inference;complicated premises;several strong neural baselines;literature;label assignments;semantic development items;task;semantic phenomena;simple attention model;semantic phenomenawe;hypothesis towe;longer texts;premises;premise"}, "74b05adf1ec74849a4f7963fe3f17fd61b92af4b": {"ta_keywords": "users query structures;users query scenarios;ranking model;query scenarios;queries;fanda;multiple baselines;superiority;multiple metrics;account;study;novel approach;structures;experimental results", "pdf_keywords": "natural language interfaces;query languages;query dataset followup;query intents;query dataset;natural language input data;query analysis;natural language follow;queries;database query statements;up queries;semantic data;natural language;multiple queries;natural language utterances;databases;database;sequence query;dimensional semantic data;contextual information;subsequent query;parsing;knowledge representation;weakly supervised learning;sql database;parser;future query;sqlwe;sentence pattern;neural coreference resolution method"}, "a5f214e23b8cd35a370a182c155ef333d77c5bb2": {"ta_keywords": "natural speech;acoustic indicators;speaker;experts;stance;presentation;tasks;dyads;topic;joint work;groups;group;ability;preliminary results;attempt", "pdf_keywords": ""}, "b33caf27fe5584b9b773c75fc35ee0e8b1421864": {"ta_keywords": "potential game;equilibrium;suitable optimization;potential function;type game;populations;game;continuous space;suitable choice;problem", "pdf_keywords": ""}, "5ee96dd7e3395d8a53d6d3ceb62593477a4e0fe1": {"ta_keywords": "colorization;images;arbitrary shape;shape;objects;language;model;domain", "pdf_keywords": "automatic colorization;colorization;plausible colorizations;colorizations;quality colorizations;colorized images;deep activation maps;visual representation;colorized image;colorizationswe;convolutional models;colorization problem;convolutional networks;image color;visual attention;convolutional neural networks;visual objects;colors;captions;color;image datasets;natural language;color specifications;images;language;caption;different captions;image;single image;downstream prediction tasks"}, "53f1fb4dc887540ef134a8d08c152789c313aa5c": {"ta_keywords": "end speech recognition systems;independent high performance feature extraction;connectionist temporal classification;encoder;triphone;error feature extraction;decoder;attention;hybrid architecture;language;character;end;systems;scale signal;noise ratio;presence;purpose;paper investigates", "pdf_keywords": ""}, "7262bc3674c4c063526eaf4d2dcf54eecea7bf77": {"ta_keywords": "paraphrastic sentence embeddings;textual similarity competition;parallel machine learning algorithm;supervised systems;power", "pdf_keywords": "dataset paraphrastic sentence embeddings;paraphrastic sentence embeddings;semantic textual similarity competition;paraphrastic text;large textual entailment dataset;neural machine translation;large parallel corpus;semantic similarity tasks;word embeddings;sentence embeddings;parallel corpus;semantic sentence representations;paraphrastic sentence;machine translation;sentence representations;parallel text dataset;simple paraphrase lexicon;linguistic similarity;parallel corpora;novel machine translation techniques;machine translation output;sentence pairs;reference translations;parallel text;semantic text;sentences;simple translation strategy;corpora;english references;natural language processing"}, "6d2d86cf5e80b58a03360559095ea3603548248f": {"ta_keywords": "statistical seriation problem;efficient algorithms;gradient descent;new algorithm;same shape constraint;squares estimator;rows;matrix;columns;local minimum;permutation;prior work;goal;logarithmic factors", "pdf_keywords": ""}, "2d9769ce319a8acbe97438b45b0d381db2a538d1": {"ta_keywords": "infinite spin;spin;liquid equation;liquid;dynamics;numerical simulations;2d;simulation;motion;model;exact solution;solution;limit;good agreement", "pdf_keywords": ""}, "c143d2b09bdfc0dff784dce2668fd5657806dbf2": {"ta_keywords": "characterization;science;teams;world;evaluation;team;task;design;document;submission", "pdf_keywords": ""}, "2873053aa18059a61ead5880d449f5bccda2d213": {"ta_keywords": "interdependent scheduling games;equilibria;players;services;existence;set;model", "pdf_keywords": "interdependent scheduling games;scheduling model;optimal schedule;optimal schedules;welfare maximization;scheduling setting;game theory;game theory approach;pure nash equilibrium;optimality;game dynamics model;schedule;schedules;reward;symmetric services;own reward;reward function;uniform rewards;welfare;tasks;free schedules;services;games;power restoration;polynomial time;predecessor services;own services;game;agents;scale infrastructure restoration"}, "fee62123e1d2ac56065675983475b079e1e9106f": {"ta_keywords": "beam search;hamming loss;new training objective;cross entropy;sequence tasks;recognition;tagging;new training;final loss;objective yields;output;novel continuous approximation;sub;better results;experiments;procedure", "pdf_keywords": "continuous beam search;beam search generator;standard discontinuous beam search;soft beam search;beam search;direct loss optimization;beam searchwe;beam search problem;hamming loss;direct loss objective;structured prediction;training objective;beam expansion candidates;optimization;beam;new training objective;sequence learning;search;new algorithms;convex objective;natural machine learning tasks;detection;actual training loss;valid training criterion;optimization procedure;final evaluation loss;efficient algorithm;novel continuous approximation;iterative algorithms;stochastic"}, "a96e05353032cc6f3d72eb5eca192295beac065e": {"ta_keywords": "graphical learning;graphical models;base learner;gaussian model;features;instance;quadratic terms;fact;process;scheme", "pdf_keywords": ""}, "c52ac453e154953abdb06fc041023e327ea609a4": {"ta_keywords": "acoustic model construction;context range;model;explicit control;other popular approaches;new approach;previous approaches;others", "pdf_keywords": "acoustic sequence prediction;acoustic models;encode acoustic data;acoustic modeling;natural acoustic variability;attention model;acoustic signal;attention heads;attention;attention bias;term memory recurrent;context range;attention headswe;gaussian biasing approach;noisy noisy encoder;models;unstructured unstructured unstructured space representation;linguisticallyattention heads;important unstructured unstructured space representation;novel random noise approximation;neural network;prediction;utterance;unstructured unstructured unstructured spaces;unstructured unstructured unstructured unstructured spaces;local context;unstructured unstructured unstructured unstructured unstructured unstructured unstructured spaces;random noise;model;other representations"}, "7ac4227d0b4d38b16da27ed55bd53ce240a32404": {"ta_keywords": "automatic speech recognition;end speech translation;form utterances;asr;models;accuracy;nar;end;tasks;comparative study;further improvement;results;techniques;interesting findings;variety;experiments", "pdf_keywords": "end speech recognition;automatic speech recognition;robust speech recognition systems;unstructured utterances;end speech translation;input speech;neural networks;acoustic information;connectionist temporal classification;noisy reference sequence;acoustic feature;automatic scanning;advanced machine learning system;acoustic embeddings;nar models;advanced neural network forwe;utterances;form speech utteranceswe;threshold task;asr;speech;partial target sequence;term prediction;models;threshold;encoder states;nar;noise ratio;accuracy;utterancewe"}, "2f153172b92ea32f242d9cb6b94d162e52ef5f0b": {"ta_keywords": "representation;set;problem", "pdf_keywords": ""}, "4a348e4725a2bc677e4aa40aa63c1421e8f335c9": {"ta_keywords": "binary classification;classifier;optimal threshold;classifier parameters;optimal f1 score;recall;labels;precision;threshold;harmonic mean;f1 score;output;procedure;problem;context", "pdf_keywords": "optimal classifiers;classifier;binary classifier;classifier outputs;optimal decision regions;optimal prediction;probabilistic classifier;optimal predictions;data classification;maximum achievable f1 score;optimal thresholding;achievable f1 score;optimal threshold;optimal decision rules;f1 score;positive class;f1 metric;thresholding;prediction;optimal algorithms;present optimal algorithms;optimal optimal metric metric metric;random field classifier;performance metric;multilabel;maximum f1;useful label;term f1;f1;true positives"}, "c2a79e2a65b721d4de5f6d4806323174b9f8f393": {"ta_keywords": "label learning;fewshot inference;natural language processing;synthetic data;label;data generation;superglue benchmark;fewshot prompts;benchmark;udg;task;novel approach;specific models;results;art results;novel;state;approach;framework", "pdf_keywords": "unsupervised learning tasks;natural language models;label learning;label learning paradigm;shot inference;unsupervised generation;unsupervised data generation;unsupervised learning;natural language processing;fewshot inference;computational linguistics;language models;giant language models;standard text classification tasks;shot generation;training data creation procedure;synthetic data;language model;complex language understanding tasks;unsupervised manner;dataset;training;text;shot methods;unstructured unstructured data models;classification onwe;data;large tasks;human baselines;unstructured unstructured data"}, "3b00e642de51d0f8378c7c35eca89f2ecb6f3af8": {"ta_keywords": "microdeletion syndrome region;de novo microduplications;microduplications;theduplications;identification;region;individuals", "pdf_keywords": ""}, "7a6c61b57bac074f7cd85963fd13da8f3321e087": {"ta_keywords": "latent dirichlet allocation;topic discovery;hyperlinking;influential blog postings;hyperlinks;hyperlink;unsupervised model;probabilistic framework;topic;document;generative model;index;information;link;distribution;effectivenesswe;effectiveness;interest;terms;new model;framework;lazaral;user;degree", "pdf_keywords": ""}, "8c25e1c223fc70509172a32111c91fe4b9f86a56": {"ta_keywords": "cognitive wireless networking;programmable network architecture;programmable routers;cognitive radio;networking;network level;sdr;radio;level programmability solutions;sds;unified overview;software;device;dds;cw", "pdf_keywords": "future programmable wireless networking;future programmable wireless networks;programmable wireless networks;wireless programmable networks;programmable radio network;active networking paradigm;cognitive wireless networking;single programmable wireless network;active networking;programmable networking paradigm;wireless virtualization;configurable radio networks;programmable networking;cognitive radio network;inprogrammable wireless networks;future programmable wireless devices;cognitive radio networks;new wireless networks;cognitive wireless networks;wireless networking;programmable networking components;future wireless networks;programmable programmable network architecture;virtualizable wireless networksthis paper;programmable networks;programmable network architecture;wireless networks;wireless protocols;wireless communications;wireless network"}, "4a36a00db217fd98f1bd943aa2f2d6303adbc456": {"ta_keywords": "fair principal component analysis;smooth optimization;nonconvex optimization;manifold;dimensionality;fair pam;conditional distributions;maximum mean discrepancy;pam;classes;mhd;form;method;incorporation;novel method;problem;approach", "pdf_keywords": "fair principal component analysis;new fairness constraint;fairness constraint;classical fairness bounds;fair parameter;fair algorithm;sparse representation method;supervised learning tasks;fair representation;fairness;manifold optimization;empirical representation;unconstrained optimization problem;supervised classification task;novel locality assumption;hyperparameter;classifiers;underlying spectral decomposition;individual classifiers;heterogeneous datasets;exact penalty method;constraints;pca;local optimality guarantees;linear representations;finite dimensional matrix representations;local minimizer;linear sigma vectors;approximate local minimizers;intrinsic features"}, "101d619b5911e9c2fda6f02365c593ae61617cb6": {"ta_keywords": "cooperative cooperative dialogue systems;cooperative dialogue system;cooperative cooperative policy;cooperative systems;user simulator;framing;policy;construction;design;topic;experimental evaluation;use;experiments;great interest;method;real users", "pdf_keywords": ""}, "d4762619b55c65120307ceebe4a0646984f6045a": {"ta_keywords": "novel statistical machine translation;conditional translation probabilities;domain domain domain domains;tables;table;accurate modeling;optimal time;smt;model;top model;data;literature;context;framework;behavior;problem", "pdf_keywords": ""}, "50ec3d960ac458573a1e4a1556420c5e96d58609": {"ta_keywords": "large corpus;supervised learning;accurate evidence;likely evidence;par;iterations;theart methods;context;underlying model;novel approach;model;few simple questions;approach;state;singlehop", "pdf_keywords": "dense evidence search;evidence retrieval;dense retrieval methods;dense retrieval;large corpus;weakly supervised setting;answer labels;current dense training data;additional annotation cost;useful evidence;dense induction;weak supervision;corpus;natural language processing;supervised learning;more evidence;many search strategies;learning;learning model;retriever approach;evidence;supervised method;supervised learning process;deep learning;search;supervised supervised learning model;evidence forrecent advances;distant supervision;vertex learning;relevant evidence"}, "d59c7b1c85f8c459863762361f251575785347a8": {"ta_keywords": "porous membranes;molecular dynamics simulations;diffusion mechanism;cylindrical pores;hard spheres;permeability;small particles;nonequilibrium event;binary mixtures;minimal model;size exclusion;separation;model;linear relationship", "pdf_keywords": ""}, "35750f1908f405bb38b0708972f33fe07b378b64": {"ta_keywords": "interpretability logic;interpretationability logic;classic modal propositional logic;modal formulas;zagreb;system;overview;del;differential equations;main purpose;years;paper;version", "pdf_keywords": ""}, "90766546b29836eb96f54fe8fd70ec51a3e699ba": {"ta_keywords": "3d protein;protein folds;3d;1d kaplan lattice;deep convolutional neural network;kaplan lattice;cnn;prediction;pooling operation;irregular data representation;network;1d;architecture;novel", "pdf_keywords": "3d protein folds;protein structure prediction;protein networks;3d protein;protein folding;protein models;protein structures;protein protein structures;protein folds;protein protein protein structures;protein structure;protein structure accuracy;specific protein structures;3d macromolecular data;protein molecules;protein image;graph convolutional;molecular models;proteins;convolutional network architecture;fold recognition;protein model quality assessment;protein docking models;protein atoms;protein;graph representation;deep convolutional networks;convolutional neural networks;graph structures;convolutional neural network"}, "7181a5139301c8a407da75a105dd457bf03d7057": {"ta_keywords": "stochastic network optimization;emphmarkovian network optimization;such network optimization;optimization;heterogeneous planning time windows;dynamic programming;numerical algorithms;flow;dual method;dual pair;multicommodity;commercial software;powerful technique;art;variable amount;state", "pdf_keywords": ""}, "af553d6121d338fc74dbd5faa43d5383a222198d": {"ta_keywords": "conventional communication skill;time communication skill;time communication skills;training framework;training;people;number;fact", "pdf_keywords": ""}, "a8315b5d3ff1b834fb58420397b13b9d169efad1": {"ta_keywords": "name disambiguation mechanism;novel name disambiguation mechanism;citation records;soft clustering;clustering;hybrid clustering algorithm;different publication venues;information science;information theory;information;algorithm;study;standard tool;paper;efficiency;experimental results;first stage", "pdf_keywords": ""}, "83ddc47f6dd0434c12eff9e4e42b727217a200a8": {"ta_keywords": "learning dynamics;uniform learning rates;stable local equilibrium;convergence guarantees;continuous games;learning rates;convergence rates;stochastic case;deterministic case;convergence;agents;numerical examples;theory;effects", "pdf_keywords": "stochastic games;policy gradients;differential nash equilibria;differential nash equilibwe;discrete game theories;optimal learning rates;differential differential nash equilibria;differential nash equilibrium;stable local nash equilibrium;nonuniform learning rates;uniform learning rates;continuous games;optimal strategies;agents converges;stochastic gradient;generalized stochastic approximation algorithms;linear quadratic games;uniform learning rate;autonomous agents;nonconvex multiagent gradient problems;continuous game;gradient descent;learning rates;deterministic dynamics;standard linear quadratic games;time convergence guarantees;heterogeneous learning;gradient dynamics;stochastic approximation converges;stochastic approximation literature"}, "48ea80b65f42e9fbb96b856286d12347d1df52d2": {"ta_keywords": "novel commonsense reasoning dataset;dense annotations;language understanding tasks;machine reasoning process;trip;evidence;predictions;high end performance;evaluation;comparative study", "pdf_keywords": "novel commonsense reasoning dataset;physical commonsense reasoning;commonsense language understanding;verifiable language inference;linguistic task prediction pipeline;coherent reasoning;reasoning abilities;dense annotations;large language models;tiered language understanding;rich physical state annotations;language understanding;physical commonsense problems;physical commonsense;language models;language understanding system;machine coherence;end task prediction;language model;annotations;natural language processing;interpretability;plausibility prediction;physical state prediction;large datasets;language;level classification taskswe;knowledge;novel dataset;large dataset"}, "fa2657c0d66f048dee6b080536abbd1f947e822f": {"ta_keywords": "age estimation;structural brain mris;age;deep learning model;span evaluation dataset;cognitive functions;heterogeneous dataset;healthy population;life;independent life;analysis;available sources;test;scale;span;utility;hold;methods", "pdf_keywords": "brain aging;brain age;superior age estimation accuracy;age estimation performance;age estimation;age prediction;aging;brain imaging data;age activation map;even age distribution;neuromorphometric measures;longitudinal data;adult lifespan;deep convolutional neural network;brain activity;young chronological age;neuropsychological dataset;age diff age;brain imaging;age difference;deep learning model;structural neuroimaging;adult life span;different age groups;brain;convolutional neural network model;dimensional isotropic brain template;chronological age;age;structural brain imagings"}, "eb1b89751cac821792df36d3a1a2fb01dc4db2d1": {"ta_keywords": "orbit interaction;spin;orbit;coupled system;dynamics;enhancement;effect;suppression", "pdf_keywords": ""}, "120839995e64f8ed734b5249ab681328c4955f5d": {"ta_keywords": "congested stochastic game;optimal tolling threshold;game designer;tolling threshold;game;algorithms;players;threshold;designer;parties;problem", "pdf_keywords": ""}, "4d16a47fb6708704b155855045c9e5d2ea380bb0": {"ta_keywords": "sentiment analysis;sentiment;social media users;machine learning methods;movie reviews;product reviews;social media;machine;analysis;results;comprehensive research;quality solutions;domains;art;context;state", "pdf_keywords": ""}, "7212cca9be971997434c2b3a27411a163bbd89c3": {"ta_keywords": "ct inference;multipass conditioning;ct;improved conditioning;test sets;new method;dataset;context;paper;intermediates;performance;literature", "pdf_keywords": "connectionist temporal classification;several autoregressive encoder;unlabeled encoders;encoders;audio feature sequence;complete speech recognition;encoder input length;tractable conditioning;ctc network;intermediate prediction;label estimation;decoder benchmarks;latent representation;intermediate conditioning;new conditioning approaches;next inference;conditioning;ctc;search conditioning;machine learning;new conditioning;utterances;improved inference;input;output sequence;probabilistic model;representation;probabilistic formulation;inference;text"}, "6bb2b856d9a9b873259ba9dc48bc450c96eb3318": {"ta_keywords": "dimensional electron gas;electron gas;interacting system;dynamics;vicinity", "pdf_keywords": "automatic speech transcript;high quality speech transcripts;transcription cost model;speech transcripts;transcription prediction;transcriber segmentation;many transcription tasks;transcription task;transcription model;transcription segmentation;transcription;speech recognition;transcript;transcription problem;transcription transcription;transcriptthis paper;particular transcribers;quality transcribers;particular transcriber;annotation;transcription process;transcription run;annotation problem;transcribers;transcriber;predictive modelswe;speech;prediction models;transcriber working style;transcription transcription transcription transcription transcription transcription transcription transcriptionr"}, "79b8ef3905a42b771248719495a2117271906445": {"ta_keywords": "carbon footprint models;carbon footprint;carbon footprint reduction;energy usage;energy use;neural networks;transparent energy estimates;node;energy;large models;calculation;future research;future;potential;co;opportunities;need", "pdf_keywords": "energydeep learning;energy consumption;energy efficiency;energy usage;several recent large nonlinear machine learning;carbon footprint;model energy requirements;energy usages;efficient machine learning;energy costs;energy use;power consumption;energy scale;energy;ml workload scheduling;energy storage;training efficiency;other machine learning;large nonlinear models;carbon impact costs;deep learning;energy intensity;deep neural network architecture;solar energy;theydatacenter energy use;energythe power;large models;machine learning models;future energy applications;neural architectures"}, "1c709eef701d933af1383c790c13209f06806b60": {"ta_keywords": "sequential model predictions;sequential rationales;language modeling;machine translation;faithful rationales;algorithm;efficient algorithm;new dataset;problem", "pdf_keywords": "greedy rationalization;usual greedy rationalization;sequential rationales;greedy rationales;language models;sequence model;language modeling;natural language input;annotated rationales;greedy algorithm;sequences;training machine translation;interpretable predictions;computational linguistics;simple greedy algorithm;sequence;linguistic representation;novel computational linguistics benchmark;attention methods;denotatingthe rationales;classification rationale;interpretable predictive models;machine translation;natural language;rationalization;prediction;rationales;greedy assumption;rationale;dimensional transformer machine translation model"}, "0de86afbf91d0cf3e595a23a5b7a4d19deefb891": {"ta_keywords": "model bifurcations;bifurcation diagram;bifurcation measure;parameter regimes;parameter inference;parameters;optimisers;pitchfork diagrams;differential equations;gradients;minimal models;gradient;cost function;genetic toggle;error term;user;targets;approach;space;work", "pdf_keywords": "model bifurcations;bifurcations;bifurcation;bifurcation diagrams;bifurcationin;bifurcation points;bifurcation diagram;new bifurcations;bifurcation structure;bifurcation curve;bifurcation measure;cusp bifurcation;optimisation trajectories;optimisation equations;optimization methods;suitable cost function;differential equation models;parameter regimes;optimisers;nonlinear dynamical systems;dynamical system;optimisation;parameters;nonlinear models;solver;dynamical systems;oscillations;cost function;computation;kinetic parameters"}, "4f02d8775123624088a91fcfff20625463e5239a": {"ta_keywords": "novel collaborative filtering algorithm;collaborative filtering approach;large education data;logistic regression approach;prediction performance;interpretability;data;new model;outcomes", "pdf_keywords": ""}, "614dc4001ad68cac31484887f16542f04693eca4": {"ta_keywords": "bribery criteria;stochastic environment;models;evaluation criteria;model", "pdf_keywords": ""}, "3261728694c0a53a2e8f95326f94147a28e03a83": {"ta_keywords": "deep quantization;quantization;multiple quantization parameterization;novel sinusoidal regularization;deep networks;sinareq balance compute efficiency;sinareq;heterogeneous bitwidth assignment;neural networks;wrpn;sinusoidal properties;gradient;accuracy;training process;large variety;conjunction;dorefa", "pdf_keywords": "deep quantization;quantization;layer quantization bitwidth;linear quantization;wise quantization;heterogenous bitwidth quantization;sinusoidal regularization;quantization levels;quantization mapping;novel sinusoidal regularization term;weight quantization;regularization;deep network;deep neural network;deep networks;deep neural networks;wave quantum;neural network;wave quantum error correction;waveq;specialized training algorithms;regularizer;neural networks;quantizer step size;standard optimization;novel optimization objective;efficient learning technique;quantum;optimization;domain learning"}, "80ef8b8a1284790e0d8f7cbf9727c9e0b2a89332": {"ta_keywords": "black box shift estimation;distribution shift;test distribution;new statistical test;correct classifiers;data sets;correct predictions;predictions;dimensional data sets;test;large class;large set;variety;approach;efficacy;use", "pdf_keywords": "label shift detection;domain adaptation;label estimation;black box shift estimation;kernel mean matching;target label distribution;label distribution;label shift;supervised learning;shift detection;classifiers;machine learning;blackbox predictors;label;black box distribution;training data;binary classification;black box method;shift;human category learning;labeling error;empirical confusion matrices;learning process;elementary classifier;new kernel;estimation process;estimation;dimensional data;black boxwe;target distribution"}, "843966d4b567033abff9775c5958f7be4db5c0ad": {"ta_keywords": "gaussian noise;noise;gaussian field;dynamics;level system;effect", "pdf_keywords": ""}}